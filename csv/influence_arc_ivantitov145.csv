2020.acl-main.148,N19-1121,0,0.0492882,"Missing"
2020.acl-main.148,C18-1263,0,0.0463148,"al. (2017) exploited character-level inputs and adopted a shared encoder for many-to-one translation. Ha et al. (2016) and Johnson et al. (2017) further successfully trained a single NMT model for multilingual translation with a target language symbol guiding the translation direction. This approach serves as our baseline. Still, this paradigm forces different languages into one joint representation space, neglecting their linguistic diversity. Several subsequent studies have explored different strategies to mitigate this representation bottleneck, ranging from reorganizing parameter sharing (Blackwood et al., 2018; Sachan and Neubig, 2018; Lu et al., 2018; Wang et al., 2019c; Vázquez et al., 2019), designing language-specific parameter generators (Platanios et al., 2018), decoupling multilingual word encodings (Wang et al., 2019b) to language clustering (Tan et al., 2019). Our languagespecific modeling continues in this direction, but with a special focus on broadening normalization layers and encoder outputs. Multilingual NMT allows us to perform zeroshot translation, although the quality is not guaranteed (Firat et al., 2016b; Johnson et al., 2017). We observe that multilingual NMT often translates i"
2020.acl-main.148,P15-1166,0,0.154129,"Missing"
2020.acl-main.148,W18-6319,0,0.0621887,"ces already in OPUS-100. 6 6.1 Experiments Setup We perform one-to-many (English-X) and manyto-many (English-X ∪ X-English) translation on OPUS-100 (|T |is 100). We apply byte pair encoding (BPE) (Sennrich et al., 2016b; Kudo and Richardson, 2018) to handle multilingual words with a joint vocabulary size of 64k. We randomly 7 For efficiency, we only use 200 sentences per language pair for validation in our multilingual experiments. shuffle the training set to mix instances of different language pairs. We adopt BLEU (Papineni et al., 2002) for translation evaluation with the toolkit SacreBLEU (Post, 2018)8 . We employ the langdetect library9 to detect the language of translations, and measure the translation-language accuracy for zero-shot cases. Rather than providing numbers for each language pair, we report average BLEU over all 94 language pairs with test sets (BLEU94 ). We also show the win ratio (WR), counting the proportion where our approach outperforms its baseline. Apart from multilingual NMT, our baselines also involve bilingual NMT and pivot-based translation (only for zero-shot comparison). We select four typologically different target languages (German/De, Chinese/Zh, Breton/Br, T"
2020.acl-main.148,W18-6327,0,0.0797739,"racter-level inputs and adopted a shared encoder for many-to-one translation. Ha et al. (2016) and Johnson et al. (2017) further successfully trained a single NMT model for multilingual translation with a target language symbol guiding the translation direction. This approach serves as our baseline. Still, this paradigm forces different languages into one joint representation space, neglecting their linguistic diversity. Several subsequent studies have explored different strategies to mitigate this representation bottleneck, ranging from reorganizing parameter sharing (Blackwood et al., 2018; Sachan and Neubig, 2018; Lu et al., 2018; Wang et al., 2019c; Vázquez et al., 2019), designing language-specific parameter generators (Platanios et al., 2018), decoupling multilingual word encodings (Wang et al., 2019b) to language clustering (Tan et al., 2019). Our languagespecific modeling continues in this direction, but with a special focus on broadening normalization layers and encoder outputs. Multilingual NMT allows us to perform zeroshot translation, although the quality is not guaranteed (Firat et al., 2016b; Johnson et al., 2017). We observe that multilingual NMT often translates into the wrong target lang"
2020.acl-main.148,P16-1009,1,0.831948,"erformance, particularly compared to pivot-based models. Without access to parallel training data for zero-shot language pairs, multilingual models easily fall into the trap of offtarget translation where a model ignores the given target information and translates into a wrong language as shown in Table 1. To avoid such a trap, we propose the random online backtranslation (ROB T) algorithm. ROB T finetunes a pretrained multilingual NMT model for unseen training language pairs with pseudo parallel batches generated by back-translating the target-side training data.2 We perform backtranslation (Sennrich et al., 2016a) into randomly picked intermediate languages to ensure good coverage of ∼10,000 zero-shot directions. Although backtranslation has been successfully applied to zero-shot translation (Firat et al., 2016b; Gu et al., 2019; Lakew et al., 2019), whether it works in the massively multilingual set-up remained an open question and we investigate it in our work. For experiments, we collect OPUS-100, a massively multilingual dataset sampled from OPUS (Tiedemann, 2012). OPUS-100 consists of 55M English-centric sentence pairs covering 100 languages. As far as we know, no similar dataset is publicly ava"
2020.acl-main.148,P16-1162,1,0.656287,"erformance, particularly compared to pivot-based models. Without access to parallel training data for zero-shot language pairs, multilingual models easily fall into the trap of offtarget translation where a model ignores the given target information and translates into a wrong language as shown in Table 1. To avoid such a trap, we propose the random online backtranslation (ROB T) algorithm. ROB T finetunes a pretrained multilingual NMT model for unseen training language pairs with pseudo parallel batches generated by back-translating the target-side training data.2 We perform backtranslation (Sennrich et al., 2016a) into randomly picked intermediate languages to ensure good coverage of ∼10,000 zero-shot directions. Although backtranslation has been successfully applied to zero-shot translation (Firat et al., 2016b; Gu et al., 2019; Lakew et al., 2019), whether it works in the massively multilingual set-up remained an open question and we investigate it in our work. For experiments, we collect OPUS-100, a massively multilingual dataset sampled from OPUS (Tiedemann, 2012). OPUS-100 consists of 55M English-centric sentence pairs covering 100 languages. As far as we know, no similar dataset is publicly ava"
2020.acl-main.148,D19-1089,0,0.0451194,"Missing"
2020.acl-main.148,tiedemann-2012-parallel,0,0.463207,"uage pairs with pseudo parallel batches generated by back-translating the target-side training data.2 We perform backtranslation (Sennrich et al., 2016a) into randomly picked intermediate languages to ensure good coverage of ∼10,000 zero-shot directions. Although backtranslation has been successfully applied to zero-shot translation (Firat et al., 2016b; Gu et al., 2019; Lakew et al., 2019), whether it works in the massively multilingual set-up remained an open question and we investigate it in our work. For experiments, we collect OPUS-100, a massively multilingual dataset sampled from OPUS (Tiedemann, 2012). OPUS-100 consists of 55M English-centric sentence pairs covering 100 languages. As far as we know, no similar dataset is publicly available.3 We have released OPUS100 to facilitate future research.4 We adopt the Transformer model (Vaswani et al., 2017) and evaluate our approach under one-to-many and manyto-many translation settings. Our main findings are summarized as follows: • Increasing the capacity of multilingual NMT yields large improvements and narrows the performance gap with bilingual models. Lowresource translation benefits more from the increased capacity. • Language-specific mode"
2020.acl-main.148,W19-4305,0,0.0366729,"ne translation. Ha et al. (2016) and Johnson et al. (2017) further successfully trained a single NMT model for multilingual translation with a target language symbol guiding the translation direction. This approach serves as our baseline. Still, this paradigm forces different languages into one joint representation space, neglecting their linguistic diversity. Several subsequent studies have explored different strategies to mitigate this representation bottleneck, ranging from reorganizing parameter sharing (Blackwood et al., 2018; Sachan and Neubig, 2018; Lu et al., 2018; Wang et al., 2019c; Vázquez et al., 2019), designing language-specific parameter generators (Platanios et al., 2018), decoupling multilingual word encodings (Wang et al., 2019b) to language clustering (Tan et al., 2019). Our languagespecific modeling continues in this direction, but with a special focus on broadening normalization layers and encoder outputs. Multilingual NMT allows us to perform zeroshot translation, although the quality is not guaranteed (Firat et al., 2016b; Johnson et al., 2017). We observe that multilingual NMT often translates into the wrong target language on zero-shot directions (Table 1), resonating with the"
2020.acl-main.148,P19-1176,0,0.333792,"s and seek solutions that are capable of overcoming this capacity bottleneck. We propose language-aware layer normalization and linear transformation to relax the representation constraint in multilingual NMT models. The linear transformation is inserted in-between the encoder and the decoder so as to facilitate the induction of language-specific translation correspon1628 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1628–1639 c July 5 - 10, 2020. 2020 Association for Computational Linguistics dences. We also investigate deep NMT architectures (Wang et al., 2019a; Zhang et al., 2019) aiming at further reducing the performance gap with bilingual methods. Another pitfall of massively multilingual NMT is its poor zero-shot performance, particularly compared to pivot-based models. Without access to parallel training data for zero-shot language pairs, multilingual models easily fall into the trap of offtarget translation where a model ignores the given target information and translates into a wrong language as shown in Table 1. To avoid such a trap, we propose the random online backtranslation (ROB T) algorithm. ROB T finetunes a pretrained multilingual N"
2020.acl-main.148,P19-1117,0,0.469076,"s and seek solutions that are capable of overcoming this capacity bottleneck. We propose language-aware layer normalization and linear transformation to relax the representation constraint in multilingual NMT models. The linear transformation is inserted in-between the encoder and the decoder so as to facilitate the induction of language-specific translation correspon1628 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1628–1639 c July 5 - 10, 2020. 2020 Association for Computational Linguistics dences. We also investigate deep NMT architectures (Wang et al., 2019a; Zhang et al., 2019) aiming at further reducing the performance gap with bilingual methods. Another pitfall of massively multilingual NMT is its poor zero-shot performance, particularly compared to pivot-based models. Without access to parallel training data for zero-shot language pairs, multilingual models easily fall into the trap of offtarget translation where a model ignores the given target information and translates into a wrong language as shown in Table 1. To avoid such a trap, we propose the random online backtranslation (ROB T) algorithm. ROB T finetunes a pretrained multilingual N"
2020.acl-main.148,D19-1083,1,0.91379,"Missing"
2020.acl-main.148,D16-1050,1,0.707318,"lation for the first time. We release OPUS-100, a multilingual dataset from OPUS including 100 languages with around 55M sentence pairs for future study. Our experiments on this dataset show that the proposed approaches substantially increase translation performance, narrowing the performance gap with bilingual NMT models and pivot-based methods. In the future, we will develop lightweight alternatives to L A LT to reduce the number of model parameters. We will also exploit novel strategies to break the upper bound of ROB T and obtain larger zero-shot improvements, such as generative modeling (Zhang et al., 2016; Su et al., 2018; García et al., 2020; Zheng et al., 2020). Acknowledgments This project has received funding from the European Union’s Horizon 2020 Research and Innovation Programme under Grant Agreements 825460 (ELITR) and 825299 (GoURMET). This project has received support from Samsung Electronics Polska sp. z o.o. - Samsung R&D Institute Poland. Rico Sennrich acknowledges support of the Swiss National Science Foundation (MUTAMUR; no. 176727). References Roee Aharoni, Melvin Johnson, and Orhan Firat. 2019. Massively multilingual neural machine translation. In Proceedings of the 2019 Confer"
2020.acl-main.148,2020.wmt-1.63,0,0.014697,"gual dataset from OPUS including 100 languages with around 55M sentence pairs for future study. Our experiments on this dataset show that the proposed approaches substantially increase translation performance, narrowing the performance gap with bilingual NMT models and pivot-based methods. In the future, we will develop lightweight alternatives to L A LT to reduce the number of model parameters. We will also exploit novel strategies to break the upper bound of ROB T and obtain larger zero-shot improvements, such as generative modeling (Zhang et al., 2016; Su et al., 2018; García et al., 2020; Zheng et al., 2020). Acknowledgments This project has received funding from the European Union’s Horizon 2020 Research and Innovation Programme under Grant Agreements 825460 (ELITR) and 825299 (GoURMET). This project has received support from Samsung Electronics Polska sp. z o.o. - Samsung R&D Institute Poland. Rico Sennrich acknowledges support of the Swiss National Science Foundation (MUTAMUR; no. 176727). References Roee Aharoni, Melvin Johnson, and Orhan Firat. 2019. Massively multilingual neural machine translation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for C"
2020.acl-main.148,P02-1040,0,\N,Missing
2020.acl-main.148,Q17-1026,0,\N,Missing
2020.acl-main.148,D18-1039,0,\N,Missing
2020.acl-main.148,D18-1338,0,\N,Missing
2020.acl-main.148,W19-5301,0,\N,Missing
2020.acl-main.148,D19-1165,0,\N,Missing
2020.acl-main.148,2020.findings-emnlp.283,0,\N,Missing
2020.acl-main.461,P99-1071,0,0.258324,"oceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5151–5169 c July 5 - 10, 2020. 2020 Association for Computational Linguistics approaches have primarily focused on extractive summarization, i.e., producing summaries by copying parts of the input reviews. In this work, we instead consider abstractive summarization which involves generating new phrases, possibly rephrasing or using words that were not in the original text. Abstractive summaries are often preferable to extractive ones as they can synthesize content across documents avoiding redundancy (Barzilay et al., 1999; Carenini and Cheung, 2008; Di Fabbrizio et al., 2014). In addition, we focus on the unsupervised setting and do not use any summaries for training. Unlike aspect-based summarization (Liu, 2012), which rewards the diversity of opinions, we aim to generate summaries that represent consensus (i.e., dominant opinons in reviews). We argue that such summaries can be useful for quick decision making, and to get an overall feel for a product or business (see the example in Table 1). More specifically, we assume we are provided with a large collection of reviews for various products and businesses an"
2020.acl-main.461,D17-1223,0,0.0313091,"lidis and Lapata, 2018; Medhat et al., 2014). Although there has been significant progress recently in summarizing non-subjective context (Rush et al., 2015; Nallapati et al., 2016; Paulus et al., 2017; See et al., 2017; Liu et al., 2018), modern deep learning methods rely on large amounts of annotated data that are not readily available in the opinion-summarization domain and expensive to produce. Moreover, annotation efforts would have to be undertaken for multiple domains as online reviews are inherently multi-domain (Blitzer et al., 2007) and summarization systems highly domain-sensitive (Isonuma et al., 2017). Thus, perhaps unsurprisingly, there is a long history of applying unsupervised and weakly-supervised methods to opinion summarization (e.g., Mei et al. 2007; Titov and McDonald 2008; Angelidis and Lapata 2018), however, these 5151 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5151–5169 c July 5 - 10, 2020. 2020 Association for Computational Linguistics approaches have primarily focused on extractive summarization, i.e., producing summaries by copying parts of the input reviews. In this work, we instead consider abstractive summarization which"
2020.acl-main.461,K16-1028,0,0.0981339,"Missing"
2020.acl-main.461,E17-2025,0,0.017462,"chanical Turk (AMT) workers, who summarized 8 input reviews. We created a new test for Amazon reviews following a similar procedure (see Appendix A.6 for details). We sampled 60 products and 8 reviews for each product, and they were shown to AMT workers who were asked to write a summary. We collected three summaries per product, 28 products were used for development and 32 for testing. 4.2 Experimental Details We used GRUs (Cho et al., 2014) for sequential encoding and decoding we used GRUs. We randomly initialized word embeddings that were shared across the model as a form of regularization (Press and Wolf, 2017). Further, optimization was performed using Adam (Kingma and Ba, 2014). In order to overcome the “posterior collapse” (Bowman et al., 2016), both for our model and the vanilla VAE baseline, we applied cyclical annealing (Fu et al., 2019). The reported ROUGE scores are based on F1 (see Appendix A.3 for details on hyperparameters). 5155 Copycat MeanSum LexRank Opinosis VAE Clustroid Lead Random Oracle R1 0.2947 0.2846 0.2501 0.2488 0.2542 0.2628 0.2634 0.2304 0.2907 R2 0.0526 0.0366 0.0362 0.0278 0.0311 0.0348 0.0372 0.0244 0.0527 RL 0.1809 0.1557 0.1467 0.1409 0.1504 0.1536 0.1386 0.1344 0.1863"
2020.emnlp-main.14,N18-1202,0,0.450491,"iption length evaluates ‘the amount of effort’ needed to achieve the quality. This amount of effort characterizes either (i) size of a probing model, or (ii) the amount of data needed to achieve the high quality. We consider two methods for estimating MDL which can be easily implemented on top of the standard probing pipelines: variational coding and online coding. We show that these methods agree in results and are more informative and stable than the standard probes.1 1 Figure 1: Illustration of the idea behind MDL probes. Introduction To estimate to what extent representations (e.g., ELMo (Peters et al., 2018) or BERT (Devlin et al., 2019)) capture a linguistic property, most previous 1 We release code at https://github.com/ lena-voita/description-length-probing. work uses ‘probing tasks’ (aka ‘probes’ and ‘diagnostic classifiers’); see Belinkov and Glass (2019) for a comprehensive review. These classifiers are trained to predict a linguistic property from ‘frozen’ representations, and accuracy of the classifier is used to measure how well these representations encode the property. Despite widespread adoption of such probes, they fail to adequately reflect differences in representations. This is cl"
2020.emnlp-main.14,D19-1250,0,0.0349274,"rs of the randomly initialized model is the same. Moreover, Table 5 shows that not only total codelength but data and model components of the code for random model layers are also identical. For the trained model, this is not the case: LAYER 2 is worse than LAYER 1 for all tasks. This is one more illustration of the general process explained in Voita et al. (2019a): the way representations evolve between layers is defined by the training objective. For the randomly initialized model, since no training objective has been optimized, no evolution happens. or in QA settings (Radford et al., 2019; Petroni et al., 2019; Poerner et al., 2019; Jiang et al., 2019). An information-theoretic view on analysis of NLP models has been previously attempted in Voita et al. (2019a) when explaining how representations in the Transformer evolve between layers under different training objectives. In context of probing, Pimentel et al. (2020) attempted to formalize probing for linguistic structure from the information-theoretic perspective. The authors measure mutual information between representations and labels, and argue the importance of defining and taking into account “ease of extraction”, though they do not formaliz"
2020.emnlp-main.14,D16-1123,0,0.0293224,"curves have also been used before by Yogatama et al. (2019) to evaluate how quickly a model learns a new task, and by Talmor et al. (2019) to understand whether the performance of a LM on a task should be attributed to the pre-trained representations or to the process of fine-tuning on the task data. Other methods for analyzing NLP models include (i) inspecting the mechanisms a model uses to encode information, e.g. attention weights (Voita et al., 2018; Raganato and Tiedemann, 2018; Voita et al., 2019b; Clark et al., 2019; Kovaleva et al., 2019) or individual neurons (Karpathy et al., 2015; Pham et al., 2016; Bau et al., 2019), (ii) looking at model predictions using manually defined templates, either evaluating sensitivity to specific grammatical errors (Linzen et al., 2016; Gulordava et al., 2018; Tran et al., 2018; Marvin and Linzen, 2018) or understanding what language models know when applying them as knowledge bases Conclusions We propose information-theoretic probing which measures minimum description length (MDL) of labels given representations. We show that MDL naturally characterizes not only probe quality, but also ‘the amount of effort’ needed to achieve it (or, intuitively, strength"
2020.emnlp-main.14,D18-1503,0,0.0700715,"Missing"
2020.emnlp-main.14,2020.acl-main.420,0,0.208772,"f the general process explained in Voita et al. (2019a): the way representations evolve between layers is defined by the training objective. For the randomly initialized model, since no training objective has been optimized, no evolution happens. or in QA settings (Radford et al., 2019; Petroni et al., 2019; Poerner et al., 2019; Jiang et al., 2019). An information-theoretic view on analysis of NLP models has been previously attempted in Voita et al. (2019a) when explaining how representations in the Transformer evolve between layers under different training objectives. In context of probing, Pimentel et al. (2020) attempted to formalize probing for linguistic structure from the information-theoretic perspective. The authors measure mutual information between representations and labels, and argue the importance of defining and taking into account “ease of extraction”, though they do not formalize this notion. That work can serve as an additional motivation for using MDL. Namely, minimum description length is the sum of (i) the data codelength, which is related to mutual information, and (ii) the model codelength, which measures “the amount of effort” needed to extract labels from representations; in Pim"
2020.emnlp-main.14,D19-1448,1,0.939411,"epresentations from the trained model. This shows that gain from using context for the randomly initialized model is at least twice smaller than for the trained model. Note also that randomly initialized layers do not evolve: for all tasks, MDL for layers of the randomly initialized model is the same. Moreover, Table 5 shows that not only total codelength but data and model components of the code for random model layers are also identical. For the trained model, this is not the case: LAYER 2 is worse than LAYER 1 for all tasks. This is one more illustration of the general process explained in Voita et al. (2019a): the way representations evolve between layers is defined by the training objective. For the randomly initialized model, since no training objective has been optimized, no evolution happens. or in QA settings (Radford et al., 2019; Petroni et al., 2019; Poerner et al., 2019; Jiang et al., 2019). An information-theoretic view on analysis of NLP models has been previously attempted in Voita et al. (2019a) when explaining how representations in the Transformer evolve between layers under different training objectives. In context of probing, Pimentel et al. (2020) attempted to formalize probing"
2020.emnlp-main.14,P17-2018,0,0.0134759,"how that, in contrast to accuracy, codelength is stable across settings. Task: part of speech. Control tasks were designed for two tasks: part-of-speech (PoS) tagging and dependency edge prediction. In this work, we 187 Experimental Results Results are shown in Table 1.6 Different compression methods, similar results. First, we see that both compression methods show similar trends in codelength. For the linguistic task, the best layer is the first one. For the control task, codes become larger as we move up from the embedding layer; this is expected since the control 5 As given by the code of Qi and Manning (2017) at https://github.com/qipeng/arc-swift. 6 Accuracies can differ from the ones reported in Hewitt and Liang (2019): we report accuracy on the test set, while they – on the development set. Since the development set is used for stopping criteria, we believe that test scores are more reliable. Accuracy 0 1 LAYER 2 LAYER LAYER 93.7 / 96.3 97.5 / 91.9 97.3 / 89.4 Variational code codelength compression 163 / 267 85 / 470 103 / 612 31.32 / 19.09 59.76 / 10.85 49.67 / 8.33 Online code codelength compression 173 / 302 96 / 515 115 / 717 29.5 / 16.87 53.06 / 9.89 44.3 / 7.11 Table 1: Experimental resu"
2020.emnlp-main.14,W18-5431,0,0.0387557,"by Saphra and Lopez (2019), who show that probes are not suitable for analyzing learning dynamics. In addition to task performance, learning curves have also been used before by Yogatama et al. (2019) to evaluate how quickly a model learns a new task, and by Talmor et al. (2019) to understand whether the performance of a LM on a task should be attributed to the pre-trained representations or to the process of fine-tuning on the task data. Other methods for analyzing NLP models include (i) inspecting the mechanisms a model uses to encode information, e.g. attention weights (Voita et al., 2018; Raganato and Tiedemann, 2018; Voita et al., 2019b; Clark et al., 2019; Kovaleva et al., 2019) or individual neurons (Karpathy et al., 2015; Pham et al., 2016; Bau et al., 2019), (ii) looking at model predictions using manually defined templates, either evaluating sensitivity to specific grammatical errors (Linzen et al., 2016; Gulordava et al., 2018; Tran et al., 2018; Marvin and Linzen, 2018) or understanding what language models know when applying them as knowledge bases Conclusions We propose information-theoretic probing which measures minimum description length (MDL) of labels given representations. We show that MDL"
2020.emnlp-main.14,N19-1329,0,0.104251,"nal motivation for using MDL. Namely, minimum description length is the sum of (i) the data codelength, which is related to mutual information, and (ii) the model codelength, which measures “the amount of effort” needed to extract labels from representations; in Pimentel et al. (2020), this is referred to as “ease of extraction”. 5 6 Related work Probing classifiers are the most common approach for associating representations with linguistic properties (see Belinkov and Glass (2019) for a survey). Among the works highlighting limitations of standard probes (not mentioned above) is the work by Saphra and Lopez (2019), who show that probes are not suitable for analyzing learning dynamics. In addition to task performance, learning curves have also been used before by Yogatama et al. (2019) to evaluate how quickly a model learns a new task, and by Talmor et al. (2019) to understand whether the performance of a LM on a task should be attributed to the pre-trained representations or to the process of fine-tuning on the task data. Other methods for analyzing NLP models include (i) inspecting the mechanisms a model uses to encode information, e.g. attention weights (Voita et al., 2018; Raganato and Tiedemann, 20"
2020.emnlp-main.14,P18-1117,1,0.794383,"above) is the work by Saphra and Lopez (2019), who show that probes are not suitable for analyzing learning dynamics. In addition to task performance, learning curves have also been used before by Yogatama et al. (2019) to evaluate how quickly a model learns a new task, and by Talmor et al. (2019) to understand whether the performance of a LM on a task should be attributed to the pre-trained representations or to the process of fine-tuning on the task data. Other methods for analyzing NLP models include (i) inspecting the mechanisms a model uses to encode information, e.g. attention weights (Voita et al., 2018; Raganato and Tiedemann, 2018; Voita et al., 2019b; Clark et al., 2019; Kovaleva et al., 2019) or individual neurons (Karpathy et al., 2015; Pham et al., 2016; Bau et al., 2019), (ii) looking at model predictions using manually defined templates, either evaluating sensitivity to specific grammatical errors (Linzen et al., 2016; Gulordava et al., 2018; Tran et al., 2018; Marvin and Linzen, 2018) or understanding what language models know when applying them as knowledge bases Conclusions We propose information-theoretic probing which measures minimum description length (MDL) of labels given rep"
2020.emnlp-main.14,P19-1580,1,0.943829,"epresentations from the trained model. This shows that gain from using context for the randomly initialized model is at least twice smaller than for the trained model. Note also that randomly initialized layers do not evolve: for all tasks, MDL for layers of the randomly initialized model is the same. Moreover, Table 5 shows that not only total codelength but data and model components of the code for random model layers are also identical. For the trained model, this is not the case: LAYER 2 is worse than LAYER 1 for all tasks. This is one more illustration of the general process explained in Voita et al. (2019a): the way representations evolve between layers is defined by the training objective. For the randomly initialized model, since no training objective has been optimized, no evolution happens. or in QA settings (Radford et al., 2019; Petroni et al., 2019; Poerner et al., 2019; Jiang et al., 2019). An information-theoretic view on analysis of NLP models has been previously attempted in Voita et al. (2019a) when explaining how representations in the Transformer evolve between layers under different training objectives. In context of probing, Pimentel et al. (2020) attempted to formalize probing"
2020.emnlp-main.14,J93-2004,0,\N,Missing
2020.emnlp-main.14,D18-1151,0,\N,Missing
2020.emnlp-main.14,Q19-1004,0,\N,Missing
2020.emnlp-main.14,W18-5448,0,\N,Missing
2020.emnlp-main.14,N19-1423,0,\N,Missing
2020.emnlp-main.14,D19-1445,0,\N,Missing
2020.emnlp-main.14,D19-1275,0,\N,Missing
2020.emnlp-main.262,P19-1284,1,0.901642,"Missing"
2020.emnlp-main.262,Q19-1004,0,0.159133,"in et al., 2019). Introduction Deep neural networks have become standard tools in NLP demonstrating impressive improvements over traditional approaches on many tasks (Goldberg, 2017). Their power typically comes at the expense of interpretability, which may prevent users from trusting predictions (Kim, 2015; Ribeiro et al., 2016), makes it hard to detect model or data deficiencies (Gururangan et al., 2018; Kaushik and 1 Source code available at https://github.com/ nicola-decao/diffmask These challenges have motivated work on interpretability, both in NLP and generally in machine learning; see Belinkov and Glass (2019) and Jacovi and Goldberg (2020) for reviews. In this work, we study post hoc interpretability where the goal is to explain the prediction of a trained model and to reveal how the model arrives at the decision. This goal is usually approached with attribution methods (Bach et al., 2015; Shrikumar et al., 2017; Sundararajan et al., 2017), which explain the behavior of a model by assigning relevance to inputs. One way to perform attribution is to use erasure where a subset of features (e.g., input tokens) is considered irrelevant if it can be removed without affecting the model prediction (Li et"
2020.emnlp-main.262,D14-1179,0,0.0240881,"Missing"
2020.emnlp-main.262,N19-1240,1,0.898731,"Missing"
2020.emnlp-main.262,N19-1423,0,0.20494,") and sometimes evaluated using erasure as ground-truth (Serrano and Smith, 2019; Jain and Wallace, 2019). Despite its conceptual simplicity, subset erasure is not commonly used in practice. First, it is generally intractable, and beam search (Feng et al., 2018) or leave-one-out estimates (Zintgraf et al., 2017) are typically used instead. These approximations may be inaccurate. For example, leaveone-out can underestimate the contribution of features due to saturation (Shrikumar et al., 2017). More importantly, even these approximations remain very expensive with modern deep (e.g., BERTbased; Devlin et al., 2019) models, as they require multiple computation passes through the model. Second, the method is susceptible to the hindsight bias: the fact that a feature can be dropped does not mean that the model ‘knows’ that it can be dropped and that the feature is not used by the model when processing the example. This results in over-aggressive pruning that does not reflect what information the model uses to arrive at the decision. The issue is pronounced in NLP tasks (see Figure 2d and Feng et al., 2018), though it is easier to see on an artificial example (Figure 3a). A model is asked to predict if ther"
2020.emnlp-main.262,D18-1407,0,0.0381253,"Missing"
2020.emnlp-main.262,N18-2017,0,0.0644358,"Missing"
2020.emnlp-main.262,D19-1424,0,0.0258942,"n but use the Gumbel softmax trick (Maddison et al., 2017; Jang et al., 2017) to approximate minimal subset selection. They assume that the subset contains exactly k elements where k is a hyperparameter. Moreover, their explainer is a separate model predicting input subsets, rather than a ‘probe’ on top of the model’s hidden layers, and hence cannot be used to reveal how decisions are formed across layers. A large body of literature analyzed BERT and Transformed-based models. For example, Tenney et al. (2019) and van Aken et al. (2019) probed BERT layers for a range of linguistic tasks, while Hao et al. (2019) analyzed the optimization surface. Rogers et al. (2020) provides a comprehensive overview of recent BERT analysis papers. There is a stream of work on learning interpretable models by means of extracting latent rationales (Lei et al., 2016; Bastings et al., 2019). Some of the techniques underlying D IFF M ASK are related to that line of work. They employ stochastic masks to learn an interpretable model, which they train by minimizing a downstream loss subject to constraints on L0 , whereas we employ stochastic masks to interpret an existing model, and for that, we minimize L0 subject to const"
2020.emnlp-main.262,2020.acl-main.386,0,0.0691378,"Deep neural networks have become standard tools in NLP demonstrating impressive improvements over traditional approaches on many tasks (Goldberg, 2017). Their power typically comes at the expense of interpretability, which may prevent users from trusting predictions (Kim, 2015; Ribeiro et al., 2016), makes it hard to detect model or data deficiencies (Gururangan et al., 2018; Kaushik and 1 Source code available at https://github.com/ nicola-decao/diffmask These challenges have motivated work on interpretability, both in NLP and generally in machine learning; see Belinkov and Glass (2019) and Jacovi and Goldberg (2020) for reviews. In this work, we study post hoc interpretability where the goal is to explain the prediction of a trained model and to reveal how the model arrives at the decision. This goal is usually approached with attribution methods (Bach et al., 2015; Shrikumar et al., 2017; Sundararajan et al., 2017), which explain the behavior of a model by assigning relevance to inputs. One way to perform attribution is to use erasure where a subset of features (e.g., input tokens) is considered irrelevant if it can be removed without affecting the model prediction (Li et al., 2016; Feng et al., 2018)."
2020.emnlp-main.262,N19-1357,0,0.0199656,"mostly to the answer span itself (underlined). Our method (d) reveals that the model pays attention to other named entities and the predicate ‘practice’ in both sentences. Predictions of the path-based methods (a) are more spread-out. Exact search (e) as well as approximate search (f) leads to pathological attributions. bution (Rockt¨aschel et al., 2016; Serrano and Smith, 2019; Vashishth et al., 2019) or back-propagation methods (Bach et al., 2015; Shrikumar et al., 2017; Sundararajan et al., 2017). These approaches received much scrutiny in recent years (Nie et al., 2018; Sixt et al., 2020; Jain and Wallace, 2019), as they cannot guarantee that the network is ignoring low-scored features. They are often motivated as approximations of erasure (Baehrens et al., 2010; Simonyan et al., 2014; Feng et al., 2018) and sometimes evaluated using erasure as ground-truth (Serrano and Smith, 2019; Jain and Wallace, 2019). Despite its conceptual simplicity, subset erasure is not commonly used in practice. First, it is generally intractable, and beam search (Feng et al., 2018) or leave-one-out estimates (Zintgraf et al., 2017) are typically used instead. These approximations may be inaccurate. For example, leaveone-o"
2020.emnlp-main.262,D18-1546,0,0.0525125,"Missing"
2020.emnlp-main.262,D19-1445,0,0.0416496,"Missing"
2020.emnlp-main.262,D16-1011,0,0.0708883,"Missing"
2020.emnlp-main.262,D17-1159,1,0.837947,"lying D IFF M ASK are related to that line of work. They employ stochastic masks to learn an interpretable model, which they train by minimizing a downstream loss subject to constraints on L0 , whereas we employ stochastic masks to interpret an existing model, and for that, we minimize L0 subject to constraints on that model’s output distribution. In our very recent work Schlichtkrull et al. (2020), we also employ stochastic masks and L0 regularization for analyzing graph neural networks. We learn which edges are relevant in multi-hop question answering and graph-based semantic role labeling (Marcheggiani and Titov, 2017; De Cao et al., 2019). 5 Conclusion We have introduced a new post hoc interpretation method which learns to completely remove subsets of inputs or hidden states through masking. We circumvent an intractable search by learning an end-to-end differentiable prediction model. To overcome the hindsight bias problem, we probe the model’s hidden states at different depths and amortize predictions over the training set. Faithfulness is validated in a controlled experiment pointing more clearly to some flaws of other attribution methods. We used our method to study BERT-based models on sentiment class"
2020.emnlp-main.262,D16-1264,0,0.0188232,"ectively) receive more attention than the rest.6 Attributions by Schulz et al. (2020) and Guan et al. (2019) assign slightly higher importance to hidden states corresponding to ‘highly’ and ‘enjoyable’, whereas it is hard to see any informative patterns provided by integrated gradient. Notice that for D IFF M ASK, a near-zero attribution has a very clear interpretation: such a state is not used for prediction since in expectation it is dropped (not gated). 3.3 Question Answering We turn now to QA where we analyse a fine-tuned BERTLARGE model on the Stanford Question Answering Dataset (SQ UAD; Rajpurkar et al., 2016). Analysis We start by asking D IFF M ASK which tokens does the model keep? We do a similar analysis as for sentiment classification of POS tags over the entire validation set. We summarize the 6 Voita et al. (2019b) and Michel et al. (2019) pointed out that many Transformer heads play no or minor role. [CLS] A fast , funny , highly enjoyable movie . [SEP] E 3 6 9 12 (b) Sundararajan et al. (2017). [CLS] A fast , funny , highly enjoyable movie . [SEP] E 3 6 9 12 (d) Guan et al. (2019). [CLS] A fast , funny , highly enjoyable movie . [SEP] E 3 6 9 12 (f) D IFF M ASK on hidden. Figure 6: Sentime"
2020.emnlp-main.262,N16-3020,0,0.12846,"o mask the input and re-compute the forward pass (bottom). The classifier g is trained to mask the input as much as possible without changing the output (minimizing a divergence D? ). Lipton, 2018) or verify that a model is fair and does not exhibit harmful biases (Sun et al., 2019; Holstein et al., 2019). Introduction Deep neural networks have become standard tools in NLP demonstrating impressive improvements over traditional approaches on many tasks (Goldberg, 2017). Their power typically comes at the expense of interpretability, which may prevent users from trusting predictions (Kim, 2015; Ribeiro et al., 2016), makes it hard to detect model or data deficiencies (Gururangan et al., 2018; Kaushik and 1 Source code available at https://github.com/ nicola-decao/diffmask These challenges have motivated work on interpretability, both in NLP and generally in machine learning; see Belinkov and Glass (2019) and Jacovi and Goldberg (2020) for reviews. In this work, we study post hoc interpretability where the goal is to explain the prediction of a trained model and to reveal how the model arrives at the decision. This goal is usually approached with attribution methods (Bach et al., 2015; Shrikumar et al., 2"
2020.emnlp-main.262,2020.tacl-1.54,0,0.0159067,"2017; Jang et al., 2017) to approximate minimal subset selection. They assume that the subset contains exactly k elements where k is a hyperparameter. Moreover, their explainer is a separate model predicting input subsets, rather than a ‘probe’ on top of the model’s hidden layers, and hence cannot be used to reveal how decisions are formed across layers. A large body of literature analyzed BERT and Transformed-based models. For example, Tenney et al. (2019) and van Aken et al. (2019) probed BERT layers for a range of linguistic tasks, while Hao et al. (2019) analyzed the optimization surface. Rogers et al. (2020) provides a comprehensive overview of recent BERT analysis papers. There is a stream of work on learning interpretable models by means of extracting latent rationales (Lei et al., 2016; Bastings et al., 2019). Some of the techniques underlying D IFF M ASK are related to that line of work. They employ stochastic masks to learn an interpretable model, which they train by minimizing a downstream loss subject to constraints on L0 , whereas we employ stochastic masks to interpret an existing model, and for that, we minimize L0 subject to constraints on that model’s output distribution. In our very"
2020.emnlp-main.262,P19-1282,0,0.0226684,"ford University and stayed at the Santa Clara Marriott . (e) Our D IFF M ASK. (f) Our D IFF M ASK non-amortized. Figure 2: Question Answering token attribution: (b) and (c), are misleading (i.e., not faithful) as they attribute the prediction mostly to the answer span itself (underlined). Our method (d) reveals that the model pays attention to other named entities and the predicate ‘practice’ in both sentences. Predictions of the path-based methods (a) are more spread-out. Exact search (e) as well as approximate search (f) leads to pathological attributions. bution (Rockt¨aschel et al., 2016; Serrano and Smith, 2019; Vashishth et al., 2019) or back-propagation methods (Bach et al., 2015; Shrikumar et al., 2017; Sundararajan et al., 2017). These approaches received much scrutiny in recent years (Nie et al., 2018; Sixt et al., 2020; Jain and Wallace, 2019), as they cannot guarantee that the network is ignoring low-scored features. They are often motivated as approximations of erasure (Baehrens et al., 2010; Simonyan et al., 2014; Feng et al., 2018) and sometimes evaluated using erasure as ground-truth (Serrano and Smith, 2019; Jain and Wallace, 2019). Despite its conceptual simplicity, subset erasure is no"
2020.emnlp-main.262,D13-1170,0,0.00302756,"thods do not allow for this type of inspection. These observations are consistent across the entire test set. For attribution to hidden states (i.e., the output of the feed-forward layer) we can compare methods in terms of how much their attributions resemble the ground-truth across the test set. Table 1 shows how the different approaches deviate from the gold-truth in terms of Kullback-Leibler (DKL ) and Jensen–Shannon (DJS ) divergences.5 3.2 Sentiment Classification We turn now to a real task and analyze models finetuned for sentiment classification on the Stanford Sentiment Treebank (SST; Socher et al., 2013). Erasure search as learning masks Before diving into an analysis of a BERT sentiment model, we would like to demonstrate that we can approximate the result of erasure well through our differentiable relaxations. For that, we train a singlelayer GRU sentiment classifier and compare the analyses by D IFF M ASK to solutions provided by 4 To enable comparison across methods, the attributions in this Section are normalized between 0 and 1. 5 We use DKL [pkq] and DJS [pkq] where p is the groundtruth distribution and q is the predicted attribution distribution. 3247 Metric Precision Recall F1 Optima"
2020.emnlp-main.262,P19-1159,0,0.0298372,"tmaps but also analyze how decisions are formed across network layers. We use D IFF M ASK to study BERT models on sentiment classification and question answering.1 1 Model with gated input Gated input Figure 1: D IFF M ASK: hidden states up to layer ` from a model (top) are fed to a classifier g that predicts a mask z. We use this to mask the input and re-compute the forward pass (bottom). The classifier g is trained to mask the input as much as possible without changing the output (minimizing a divergence D? ). Lipton, 2018) or verify that a model is fair and does not exhibit harmful biases (Sun et al., 2019; Holstein et al., 2019). Introduction Deep neural networks have become standard tools in NLP demonstrating impressive improvements over traditional approaches on many tasks (Goldberg, 2017). Their power typically comes at the expense of interpretability, which may prevent users from trusting predictions (Kim, 2015; Ribeiro et al., 2016), makes it hard to detect model or data deficiencies (Gururangan et al., 2018; Kaushik and 1 Source code available at https://github.com/ nicola-decao/diffmask These challenges have motivated work on interpretability, both in NLP and generally in machine learni"
2020.emnlp-main.262,D19-1448,1,0.785224,"rd to see any informative patterns provided by integrated gradient. Notice that for D IFF M ASK, a near-zero attribution has a very clear interpretation: such a state is not used for prediction since in expectation it is dropped (not gated). 3.3 Question Answering We turn now to QA where we analyse a fine-tuned BERTLARGE model on the Stanford Question Answering Dataset (SQ UAD; Rajpurkar et al., 2016). Analysis We start by asking D IFF M ASK which tokens does the model keep? We do a similar analysis as for sentiment classification of POS tags over the entire validation set. We summarize the 6 Voita et al. (2019b) and Michel et al. (2019) pointed out that many Transformer heads play no or minor role. [CLS] A fast , funny , highly enjoyable movie . [SEP] E 3 6 9 12 (b) Sundararajan et al. (2017). [CLS] A fast , funny , highly enjoyable movie . [SEP] E 3 6 9 12 (d) Guan et al. (2019). [CLS] A fast , funny , highly enjoyable movie . [SEP] E 3 6 9 12 (f) D IFF M ASK on hidden. Figure 6: Sentiment classification: comparison between attribution method for hidden layers w.r.t. the predicted label. All plots are normalized per-layer by the largest attribution. Attention heatmap is obtained max pooling over h"
2020.emnlp-main.262,P19-1580,1,0.761029,"rd to see any informative patterns provided by integrated gradient. Notice that for D IFF M ASK, a near-zero attribution has a very clear interpretation: such a state is not used for prediction since in expectation it is dropped (not gated). 3.3 Question Answering We turn now to QA where we analyse a fine-tuned BERTLARGE model on the Stanford Question Answering Dataset (SQ UAD; Rajpurkar et al., 2016). Analysis We start by asking D IFF M ASK which tokens does the model keep? We do a similar analysis as for sentiment classification of POS tags over the entire validation set. We summarize the 6 Voita et al. (2019b) and Michel et al. (2019) pointed out that many Transformer heads play no or minor role. [CLS] A fast , funny , highly enjoyable movie . [SEP] E 3 6 9 12 (b) Sundararajan et al. (2017). [CLS] A fast , funny , highly enjoyable movie . [SEP] E 3 6 9 12 (d) Guan et al. (2019). [CLS] A fast , funny , highly enjoyable movie . [SEP] E 3 6 9 12 (f) D IFF M ASK on hidden. Figure 6: Sentiment classification: comparison between attribution method for hidden layers w.r.t. the predicted label. All plots are normalized per-layer by the largest attribution. Attention heatmap is obtained max pooling over h"
2020.emnlp-main.322,S07-1018,0,0.135729,"inimize the negative conditional log-likelihood: 80 L=− N X log P (y|x, p) 75 0-10 j 11-20 21-30 31-40 41-300 Sentence length (tokens) where p is the predicate position for the training example j. 5 Figure 3: CoNLL-2005 F1 score as a function of sentence length. Experiments 5.1 Data and setting We experiment on the CoNLL-2005 and CoNLL2012 (OntoNotes) datasets and use the CoNLL 2005 evaluation script for evaluation. We also apply our approach to FrameNet 1.5 with the data split of Das et al. (2014) and follow the official evaluation set-up from the SemEval07 Task 19 on frame-semantic parsing (Baker et al., 2007). We train the self-attentive constituency parser of Kitaev and Klein (2018)8 on the training data of the CoNLL-2005 dataset (Penn Treebank) and parse the development and test sets of CoNLL2005 dataset. We apply the same procedure for the CoNLL-2012 dataset. We perform 10-fold jackknifing to obtain syntactic predictions for the training set of CoNLL-2005 and CoNLL-2012. For FrameNet, we parse the entire corpus with the parser trained on the training set of CoNLL-2005. All hyperparameters are reported in Appendix A. 5.2 Importance of syntax and ablations Before comparing our full model to state"
2020.emnlp-main.322,P98-1013,0,0.900737,"-A1 I-A1 I-A1 I-A1 I-A1 Figure 1: An example with semantic-role annotation and its reduction to the sequence labeling problem (BIO labels): the argument structure for predicates appeal and limit are shown in blue and red, respectively. Introduction The task of semantic role labeling (SRL) involves predicting the predicate-argument structure of a sentence. More formally, for every predicate, the SRL model must identify all argument spans and label them with their semantic roles (see Figure 1). The most popular resources for estimating SRL models are PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998). In both cases, annotations are made on top of syntactic constituent structures. Earlier work on SRL hinged ∗ Research conducted when the author was at the University of Amsterdam. on constituent syntactic structure, using the trees to derive features and constraints on role assignments (Gildea and Jurafsky, 2002; Pradhan et al., 2005; Punyakanok et al., 2008). In contrast, modern SRL systems largely ignore treebank syntax (He et al., 2018a, 2017; Marcheggiani et al., 2017; Zhou and Xu, 2015) and instead use powerful feature extractors, for example, LSTM sentence encoders. There have been rec"
2020.emnlp-main.322,Q19-1022,0,0.0152602,"to force the self-attention model to predict syntactic edges. Roth and Lapata (2016) encoded dependency paths between predicates and arguments using an LSTM. Li et al. (2018) analysed different ways of encoding syntactic dependencies for dependency-based SRL, while He et al. (2018b) and He et al. (2019) proposed an argument pruning technique which calculates promising candidate arguments. Recently, Wang et al. (2019) used syntax linearizaton approaches of G´omez-Rodr´ıguez and Vilares (2018) and employed this information as a 3922 word-level feature in a SRL model. Swayamdipta et al. (2018); Cai and Lapata (2019) used multi-task learning to produce syntactically-informed word representation, with a sentence encoder shared between SRL and an auxiliary syntax-related task. In earlier work, Naradowsky et al. (2012) used graphical models to encode syntactic structures while Moschitti et al. (2008) applied tree kernels for encoding constituency trees. Many methods cast the problem of SRL as a span classification problem. FitzGerald et al. (2015) used hand-crafted features to represent spans, while He et al. (2018a) and Ouchi et al. (2018) adopted a BiLSTM feature extractor. In principle, SpanGCN can be use"
2020.emnlp-main.322,W05-0620,0,0.128701,"Missing"
2020.emnlp-main.322,N16-1012,0,0.0379906,"datasets: CoNLL-2005 (Carreras and M`arquez, 2005) and CoNLL-2012 (Pradhan et al., 2012) with PropBank-style (Palmer et al., 2005) annotation and on FrameNet 1.5 (Baker et al., 1998) 3 . By empirically comparing SpanGCN to GCNs over dependency structures, we confirm our intuition that constituents yield more informative features for the SRL task. 4 SpanGCN may be beneficial in other NLP tasks, where neural sentence encoders are already effective and syntactic structure can provide an additional inductive bias, e.g., logical semantic parsing (Dong and Lapata, 2016) or sentence simplification (Chopra et al., 2016). Moreover, in principle, SpanGCN can be applied to other forms of span2 Recently, Wang et al. (2019) proposed different ways of encoding dependency and constituency syntax based on the linearization approaches of G´omez-Rodr´ıguez and Vilares (2018). 3 Although we tested the model on English datasets, SpanGCN can be applied to constituent trees in any language. 4 Code available at https://github.com/diegma/ span-gcn. Constituent decomposition Constituent to constituent GCN Constituent composition (S) (VP) Mary eats an apple (NP) (NP) Mary eats an apple Figure 2: SpanGCN encoder. First, for ea"
2020.emnlp-main.322,J14-1002,0,0.0937786,"2005 development set. SpanGCN DepGCN DepGCN w/o LSTM 90 85 F1 between BIO labels. The entire model is trained to minimize the negative conditional log-likelihood: 80 L=− N X log P (y|x, p) 75 0-10 j 11-20 21-30 31-40 41-300 Sentence length (tokens) where p is the predicate position for the training example j. 5 Figure 3: CoNLL-2005 F1 score as a function of sentence length. Experiments 5.1 Data and setting We experiment on the CoNLL-2005 and CoNLL2012 (OntoNotes) datasets and use the CoNLL 2005 evaluation script for evaluation. We also apply our approach to FrameNet 1.5 with the data split of Das et al. (2014) and follow the official evaluation set-up from the SemEval07 Task 19 on frame-semantic parsing (Baker et al., 2007). We train the self-attentive constituency parser of Kitaev and Klein (2018)8 on the training data of the CoNLL-2005 dataset (Penn Treebank) and parse the development and test sets of CoNLL2005 dataset. We apply the same procedure for the CoNLL-2012 dataset. We perform 10-fold jackknifing to obtain syntactic predictions for the training set of CoNLL-2005 and CoNLL-2012. For FrameNet, we parse the entire corpus with the parser trained on the training set of CoNLL-2005. All hyperpa"
2020.emnlp-main.322,P18-1249,0,0.151087,"p) 75 0-10 j 11-20 21-30 31-40 41-300 Sentence length (tokens) where p is the predicate position for the training example j. 5 Figure 3: CoNLL-2005 F1 score as a function of sentence length. Experiments 5.1 Data and setting We experiment on the CoNLL-2005 and CoNLL2012 (OntoNotes) datasets and use the CoNLL 2005 evaluation script for evaluation. We also apply our approach to FrameNet 1.5 with the data split of Das et al. (2014) and follow the official evaluation set-up from the SemEval07 Task 19 on frame-semantic parsing (Baker et al., 2007). We train the self-attentive constituency parser of Kitaev and Klein (2018)8 on the training data of the CoNLL-2005 dataset (Penn Treebank) and parse the development and test sets of CoNLL2005 dataset. We apply the same procedure for the CoNLL-2012 dataset. We perform 10-fold jackknifing to obtain syntactic predictions for the training set of CoNLL-2005 and CoNLL-2012. For FrameNet, we parse the entire corpus with the parser trained on the training set of CoNLL-2005. All hyperparameters are reported in Appendix A. 5.2 Importance of syntax and ablations Before comparing our full model to state-of-the-art SRL systems, we show that our model genuinely benefits from inco"
2020.emnlp-main.322,D14-1081,0,0.0607492,"Missing"
2020.emnlp-main.322,D18-1262,0,0.0681372,"Zuidema, 2014; Teng and Zhang, 2017). Second, these previous models do a global pass over the tree, whereas GCNs consider only small fragments of the graph. This may make GCNs more robust when using noisy, predicted syntactic structures. In SRL, dependency syntax has gained a lot of attention. Similarly to this work, Marcheggiani and Titov (2017) encoded dependency structures using GCNs. Strubell et al. (2018) used a multitask objective to force the self-attention model to predict syntactic edges. Roth and Lapata (2016) encoded dependency paths between predicates and arguments using an LSTM. Li et al. (2018) analysed different ways of encoding syntactic dependencies for dependency-based SRL, while He et al. (2018b) and He et al. (2019) proposed an argument pruning technique which calculates promising candidate arguments. Recently, Wang et al. (2019) used syntax linearizaton approaches of G´omez-Rodr´ıguez and Vilares (2018) and employed this information as a 3922 word-level feature in a SRL model. Swayamdipta et al. (2018); Cai and Lapata (2019) used multi-task learning to produce syntactically-informed word representation, with a sentence encoder shared between SRL and an auxiliary syntax-relate"
2020.emnlp-main.322,N19-1112,0,0.140216,", (2) where σ is the sigmoid activation function, whereas u ˆTc (u,v) ∈ Rm and ˆbTf (u,v) ∈ R are edge-typespecific parameters. Now, we show how to compose GCN and LSTM layers to produce a syntactically-informed encoder. 2.3 SpanGCN Our model is shown in Figure 2. It is composed of three modules: constituent composition, constituent GCN, and constituent decomposition. Note that there is no parameter sharing across these components. Constituent composition The model takes as input word representations which can either be static word embeddings or contextual word vectors (Peters et al., 2018a; Liu et al., 2019b; Devlin et al., 2019). The sentence is first encoded with a BiLSTM to obtain a context-aware representation of each word. A constituency tree is composed of words (Vw ) and constituents (Vc ).5 We add representations (initially zero vectors) for each constituent in the tree; they are shown as green blocks in Figure 2. Each constituent representation is computed using GCN updates (Equation 1), relying on the word representation corresponding to the beginning of its span and the representation corresponding to the end of its span. The coarse-grained types Tc (u, v) here are binary, distinguish"
2020.emnlp-main.322,2021.ccl-1.108,0,0.108082,"Missing"
2020.emnlp-main.322,W08-1301,0,0.0832476,"Missing"
2020.emnlp-main.322,J08-2003,0,0.0601961,"and He et al. (2019) proposed an argument pruning technique which calculates promising candidate arguments. Recently, Wang et al. (2019) used syntax linearizaton approaches of G´omez-Rodr´ıguez and Vilares (2018) and employed this information as a 3922 word-level feature in a SRL model. Swayamdipta et al. (2018); Cai and Lapata (2019) used multi-task learning to produce syntactically-informed word representation, with a sentence encoder shared between SRL and an auxiliary syntax-related task. In earlier work, Naradowsky et al. (2012) used graphical models to encode syntactic structures while Moschitti et al. (2008) applied tree kernels for encoding constituency trees. Many methods cast the problem of SRL as a span classification problem. FitzGerald et al. (2015) used hand-crafted features to represent spans, while He et al. (2018a) and Ouchi et al. (2018) adopted a BiLSTM feature extractor. In principle, SpanGCN can be used as a syntactic feature extractor within this class of models. 7 Conclusions In this paper, we introduced SpanGCN, a novel neural architecture for encoding constituency syntax at the word level. We applied SpanGCN to SRL, on PropBank and FrameNet. We observed substantial improvements"
2020.emnlp-main.322,D12-1074,0,0.079829,"Missing"
2020.emnlp-main.322,D18-1191,0,0.0609876,"3922 word-level feature in a SRL model. Swayamdipta et al. (2018); Cai and Lapata (2019) used multi-task learning to produce syntactically-informed word representation, with a sentence encoder shared between SRL and an auxiliary syntax-related task. In earlier work, Naradowsky et al. (2012) used graphical models to encode syntactic structures while Moschitti et al. (2008) applied tree kernels for encoding constituency trees. Many methods cast the problem of SRL as a span classification problem. FitzGerald et al. (2015) used hand-crafted features to represent spans, while He et al. (2018a) and Ouchi et al. (2018) adopted a BiLSTM feature extractor. In principle, SpanGCN can be used as a syntactic feature extractor within this class of models. 7 Conclusions In this paper, we introduced SpanGCN, a novel neural architecture for encoding constituency syntax at the word level. We applied SpanGCN to SRL, on PropBank and FrameNet. We observed substantial improvements from using constituent syntax on both datasets, and also in the realistic out-of-domain setting. By comparing to dependency GCN, we observed that for SRL constituent structures yield more informative features that the dependency ones. Given that"
2020.emnlp-main.322,D13-1170,0,0.0145791,"Missing"
2020.emnlp-main.322,P19-1529,0,0.385234,"-style (Palmer et al., 2005) annotation and on FrameNet 1.5 (Baker et al., 1998) 3 . By empirically comparing SpanGCN to GCNs over dependency structures, we confirm our intuition that constituents yield more informative features for the SRL task. 4 SpanGCN may be beneficial in other NLP tasks, where neural sentence encoders are already effective and syntactic structure can provide an additional inductive bias, e.g., logical semantic parsing (Dong and Lapata, 2016) or sentence simplification (Chopra et al., 2016). Moreover, in principle, SpanGCN can be applied to other forms of span2 Recently, Wang et al. (2019) proposed different ways of encoding dependency and constituency syntax based on the linearization approaches of G´omez-Rodr´ıguez and Vilares (2018). 3 Although we tested the model on English datasets, SpanGCN can be applied to constituent trees in any language. 4 Code available at https://github.com/diegma/ span-gcn. Constituent decomposition Constituent to constituent GCN Constituent composition (S) (VP) Mary eats an apple (NP) (NP) Mary eats an apple Figure 2: SpanGCN encoder. First, for each constituent, an initial representation is produced by composing the start and end tokens’ BiLSTM s"
2020.emnlp-main.322,D17-1128,0,0.128843,"e the notation by referring to nonterminals as constituents: part-of-speech tags (typically ‘preterminals’) are stripped off from our trees. 3917 includes the label of the constituent sending the message. For example, consider the computation of the VP constituent in Figure 2. It receives a message from the S constituent, this is a parent-to-child message, and the ‘sender’ is S; these two factors determine Tf (u, v) and, as a result, the parameters used in computing the corresponding message. As in previous work we compare to, we assume to have access to gold frames (Swayamdipta et al., 2018; Yang and Mitchell, 2017). Constituent decomposition At this point, we ’infuse’ words with information coming from constituents. The graph here is the inverse of the one used in the composition stage: the constituents pass the information to the first and the last words in their spans. As in the composition stage, Tc (u, v) is binary, distinguishing messages to start and end tokens. The fine-grained edge types, as before, additionally include the constituent label. To spread syntactic information across the sentence, we use a further BiLSTM layer. Note that residual connections indicated in grey in Figure 2, let the m"
2020.emnlp-main.322,P15-1109,0,0.16467,"he most popular resources for estimating SRL models are PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998). In both cases, annotations are made on top of syntactic constituent structures. Earlier work on SRL hinged ∗ Research conducted when the author was at the University of Amsterdam. on constituent syntactic structure, using the trees to derive features and constraints on role assignments (Gildea and Jurafsky, 2002; Pradhan et al., 2005; Punyakanok et al., 2008). In contrast, modern SRL systems largely ignore treebank syntax (He et al., 2018a, 2017; Marcheggiani et al., 2017; Zhou and Xu, 2015) and instead use powerful feature extractors, for example, LSTM sentence encoders. There have been recent successful attempts to improve neural SRL models using syntax (Marcheggiani and Titov, 2017; Strubell et al., 2018; He et al., 2018b). Nevertheless, they have relied on syntactic dependency representations rather than constituent trees. In these methods, information from dependency trees is injected into word representations using graph convolutional networks (GCN) (Kipf and Welling, 2017) or self-attention mechanisms (Vaswani et al., 2017). Since SRL annotations are done on top of syntact"
2020.emnlp-main.322,D18-1548,0,0.343693,"SRL hinged ∗ Research conducted when the author was at the University of Amsterdam. on constituent syntactic structure, using the trees to derive features and constraints on role assignments (Gildea and Jurafsky, 2002; Pradhan et al., 2005; Punyakanok et al., 2008). In contrast, modern SRL systems largely ignore treebank syntax (He et al., 2018a, 2017; Marcheggiani et al., 2017; Zhou and Xu, 2015) and instead use powerful feature extractors, for example, LSTM sentence encoders. There have been recent successful attempts to improve neural SRL models using syntax (Marcheggiani and Titov, 2017; Strubell et al., 2018; He et al., 2018b). Nevertheless, they have relied on syntactic dependency representations rather than constituent trees. In these methods, information from dependency trees is injected into word representations using graph convolutional networks (GCN) (Kipf and Welling, 2017) or self-attention mechanisms (Vaswani et al., 2017). Since SRL annotations are done on top of syntactic constituents,1 we argue that exploiting constituency syntax, rather than dependency one, is more natural and may yield more predictive features for semantic roles. For example, even though constituent boundaries could"
2020.emnlp-main.322,W08-2121,0,0.098063,"Missing"
2020.emnlp-main.322,D18-1412,0,0.489599,"ionally 5 We slightly abuse the notation by referring to nonterminals as constituents: part-of-speech tags (typically ‘preterminals’) are stripped off from our trees. 3917 includes the label of the constituent sending the message. For example, consider the computation of the VP constituent in Figure 2. It receives a message from the S constituent, this is a parent-to-child message, and the ‘sender’ is S; these two factors determine Tf (u, v) and, as a result, the parameters used in computing the corresponding message. As in previous work we compare to, we assume to have access to gold frames (Swayamdipta et al., 2018; Yang and Mitchell, 2017). Constituent decomposition At this point, we ’infuse’ words with information coming from constituents. The graph here is the inverse of the one used in the composition stage: the constituents pass the information to the first and the last words in their spans. As in the composition stage, Tc (u, v) is binary, distinguishing messages to start and end tokens. The fine-grained edge types, as before, additionally include the constituent label. To spread syntactic information across the sentence, we use a further BiLSTM layer. Note that residual connections indicated in g"
2020.emnlp-main.322,P15-1150,0,0.203203,"Missing"
2020.emnlp-main.322,Q17-1012,0,0.0463966,"Missing"
2020.emnlp-main.337,D17-1223,0,0.0604145,"Missing"
2020.emnlp-main.337,W04-1013,0,0.0893905,"ator can directly attend to the word vacuum in the source reviews to increase its prediction probability. Additionally, we condition on partial information about the target review ri using an oracle 2 Both the code and datasets are available at: https:// github.com/abrazinskas/FewSum q(ri , r−i ) as shown in Eq. 2. M N 1 XX j j log Gθ (rij |Eθ (r−i ), q(rij , r−i )) (2) MN j=1 i=1 We refer to this partial information as properties (Ficler and Goldberg, 2017), which correspond to text characteristics of ri or relations between ri and r−i . For example, one such property can be the ROUGE score (Lin, 2004) between ri and r−i , which indicates the degree of overlap between ri and r−i . In Fig. 1, a high ROUGE value can signal to the generator to attend the word vacuum in the source reviews instead of predicting it based on language statistics. Intuitively, while the model observes a wide distribution of ROUGE scores during training on reviews, during summarization in test time we can achieve a high degree of input-output text overlap by setting the property to a high value. We considered three types of properties. Content Coverage: ROUGE-1, ROUGE-2, and ROUGE-L between ri and r−i signals to Gθ h"
2020.emnlp-main.337,K16-1028,0,0.0518863,"ment to the input reviews. The reviews are truncated, and delimited with the symbol ‘||’. generation (Hu and Liu, 2004; Medhat et al., 2014; Angelidis and Lapata, 2018). Introduction Summarization of user opinions expressed in online resources, such as blogs, reviews, social media, or internet forums, has drawn much attention due to its potential for various information access applications, such as creating digests, search, and report Although significant progress has been observed in supervised summarization in non-subjective single-document context, such as news articles (Rush et al., 2015; Nallapati et al., 2016; Paulus et al., 2017; See et al., 2017; Liu et al., 2018), modern deep learning methods rely on large amounts of annotated data that are not readily available in the opinion-summarization domain and expensive to produce. A key obstacle making data annotation expensive is that annotators need to consider multiple input texts when writing a summary, which is 4119 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4119–4135, c November 16–20, 2020. 2020 Association for Computational Linguistics time-consuming. Moreover, annotation would have to be under"
2020.emnlp-main.337,P17-1099,0,0.0665184,"uncated, and delimited with the symbol ‘||’. generation (Hu and Liu, 2004; Medhat et al., 2014; Angelidis and Lapata, 2018). Introduction Summarization of user opinions expressed in online resources, such as blogs, reviews, social media, or internet forums, has drawn much attention due to its potential for various information access applications, such as creating digests, search, and report Although significant progress has been observed in supervised summarization in non-subjective single-document context, such as news articles (Rush et al., 2015; Nallapati et al., 2016; Paulus et al., 2017; See et al., 2017; Liu et al., 2018), modern deep learning methods rely on large amounts of annotated data that are not readily available in the opinion-summarization domain and expensive to produce. A key obstacle making data annotation expensive is that annotators need to consider multiple input texts when writing a summary, which is 4119 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4119–4135, c November 16–20, 2020. 2020 Association for Computational Linguistics time-consuming. Moreover, annotation would have to be undertaken for multiple domains as online re"
2020.emnlp-main.337,P16-1162,0,0.00773267,"480 human-written summaries (180 for Amazon and 300 for Yelp) for 8 reviews each, using Amazon Mechanical Turk (AMT). Each product/business received 3 summaries, and averaged ROUGE scores are reported in the following sections. Also, we reserved approximately 13 for testing and the rest for training and validation. The details are in Appendix 9.2. 4.2 Experimental Details For the main model, we used the Transformer architecture (Vaswani et al., 2017) with trainable length embeddings and shared parameters between the encoder and generator (Raffel et al., 2019). Subwords were obtained with BPE (Sennrich et al., 2016) using 32000 merges. Subword embeddings were shared across the model as a form of regularization (Press and Wolf, 2017). For a fair comparison, we approximately matched the number of parameters to C OPYCAT (Braˇzinskas et al., 2020). We randomly initialized all parameters with Glorot (Glorot and Bengio, 2010). For the plug-in network, we employed a multi-layer feed-forward network with multi-head attention modules over encoded states of the source review. After the last layer, we performed a linear projection to compute property values. Further, parameter optimization was performed using Adam"
2020.emnlp-main.337,P07-1056,0,\N,Missing
2020.emnlp-main.337,P19-1213,0,\N,Missing
2020.emnlp-main.337,N16-1095,0,\N,Missing
2020.emnlp-main.354,2020.emnlp-main.703,0,0.12132,"vEæ ILLC, University of Amsterdam ititov@inf.ed.ac.uk Klein and Manning, 2002). Recently, deep learning models have been shown very effective across NLP tasks and have also been applied to grammar induction, greatly advancing the area (Shen et al., 2018, 2019; Kim et al., 2019a,b; Jin et al., 2019). These neural grammar-induction approaches have been generally limited to relying on text, without considering learning signals from other modalities. In contrast, the crucial aspect of natural language learning is that it is grounded in perceptual experiences (Barsalou, 1999; Fincher-Kiefer, 2001; Bisk et al., 2020). We thus anticipate improved language understanding by leveraging grounded learning. Promising results from grounded learning have been emerging in areas such as representation learning (Bruni et al., 2014; Kiela et al., 2018; Bordes et al., 2019). Typically, they use visual images as perceptual groundings of language and aim at improving continuous vector representations of language (e.g., word or sentence embeddings). In this work, we consider a more challenging problem: can visual groundings help us induce syntactic structure? We refer to this problem as visually grounded grammar induction"
2020.emnlp-main.354,P18-2003,0,0.0251761,". Take a span ci,j = wi . . . wj (0 < i < j ≤ n): j X 1 ck = fk ( hl ) . j−i+1 (7) l=i The BiLSTM encoding model operates at the span level and encodes semantics of a span. Unlike using a single sentence-level (Bi)LSTM encoder, it guarantees that no information from words outside of the span leaks into its representations. More importantly, it can run in O(n) for a sentence of length n with a parallel implementation. While the produced representation does not reflect the structural decisions made by the parser, it can be sensitive to word order and may be affected by its syntactic structure (Blevins et al., 2018). In order to compute the representation of unlabeled constituent c, we average the label-specific span representation ck under the distribution of labels defined by the parser: c= K X p(k|c, w)ck , (8) k=1 where p(k|c, w) is the probability that the span c has label k, conditioned on having this constituent span in the tree. To further reduce computation we estimate the matching loss only using the n(n−1) shortest spans 4 for a sentence of length n. Thus the image-text alignment loss will focus on small constituents. 3 Intuitively, the key learning signal for the parser in our model comes thr"
2020.emnlp-main.354,D19-1064,0,0.0937273,"., 2018, 2019; Kim et al., 2019a,b; Jin et al., 2019). These neural grammar-induction approaches have been generally limited to relying on text, without considering learning signals from other modalities. In contrast, the crucial aspect of natural language learning is that it is grounded in perceptual experiences (Barsalou, 1999; Fincher-Kiefer, 2001; Bisk et al., 2020). We thus anticipate improved language understanding by leveraging grounded learning. Promising results from grounded learning have been emerging in areas such as representation learning (Bruni et al., 2014; Kiela et al., 2018; Bordes et al., 2019). Typically, they use visual images as perceptual groundings of language and aim at improving continuous vector representations of language (e.g., word or sentence embeddings). In this work, we consider a more challenging problem: can visual groundings help us induce syntactic structure? We refer to this problem as visually grounded grammar induction. Shi et al. (2019) propose a visually grounded neural syntax learner (VG-NSL) to tackle the task. Specifically, they learn a parser from aligned imagesentence pairs (e.g., image-caption data), where each sentence describes visual content of the co"
2020.emnlp-main.354,W01-0713,0,0.432452,"Missing"
2020.emnlp-main.354,N09-1009,0,0.0224738,"see that VC-PCFG identifies most NPs but makes mistakes in PP attachement and consequently fails to identify the VP. 5 apple Related work Grammar Induction has a long history in computational linguistics. Following observations that direct optimization of log-likelihood with the Expectation Maximization algorithm (Lari and Young, 1990) is not effective at producing effective grammars, a number of approaches have been developed, emboding various inductive biases or assumption about the language structure and its relation to surface realizations (Klein and Manning, 2002; Smith and Eisner, 2005; Cohen and Smith, 2009; Spitkovsky et al., 2010). The recent advances in the area have been brought by flexible neural models (Jin et al., 2019; Kim et al., 2019a,b; Drozdov et al., 2019). All these methods, with the exception of Shi et al. (2019), rely solely on text. Visually grounded learning is motivated by the observation that natural language is grounded in perceptual experiences (Steels, 1998; Barsalou, 1999; Fincher-Kiefer, 2001; Roy, 2002; Bisk et al., 2020). It has been shown effective in word representation learning (Bruni et al., 2014; Silberer and Lapata, 2014; Lazaridou et al., 2015) and sentence repr"
2020.emnlp-main.354,N19-1116,0,0.296061,"erentiation (Eisner, 2016). 3.2 Span representation Estimation of the expected image-text matching scores relies on span representations. Ideally, a span representation should encode semantics of a span with its computation guided by its syntactic structure (Socher et al., 2013). The reliance on the predicted tree structure will result in propagating learning signals derived from the alignment of 4372 images and sentences back to the parser. To realize this desideratum, we could follow the inside algorithm and recursively compose span representations (Le and Zuidema, 2015; Stern et al., 2017; Drozdov et al., 2019), which is, however, time- and memory-inefficient in practice. Instead, we produce span representations largely independently of the parser, as we will explain below. The only way the parser model influences this representation is through the predicted constituent label: we use its distribution to compute the representation.3 Specificially, as a trade-off for a better training efficiency, we adopt a single-layer BiLSTM to encode spans. A mean-pooling layer is applied over the hidden states h of the BiLSTM and followed by a label-specific affine transformation fk (·) to produce a label-specific"
2020.emnlp-main.354,W16-5901,0,0.398379,"tribution and use end-to-end contrastive learning (Section 3.1). Since it is inefficient to compute constituent representations relying on the chart, we will introduce an additional textual representation model to encode constituents (Section 3.2). Moreover, VC-PCFGs let us additionally optimize a language modeling objective, complementing the visually grounded contrastive learning (Section 3.3). = X p(c|w)h(c, v) , (6) c∈w where p(c|w) is the conditional probability (i.e., marginal) of the span c given w. It can be efficiently computed with the inside algorithm and automatic differentiation (Eisner, 2016). 3.2 Span representation Estimation of the expected image-text matching scores relies on span representations. Ideally, a span representation should encode semantics of a span with its computation guided by its syntactic structure (Socher et al., 2013). The reliance on the predicted tree structure will result in propagating learning signals derived from the alignment of 4372 images and sentences back to the parser. To realize this desideratum, we could follow the inside algorithm and recursively compose span representations (Le and Zuidema, 2015; Stern et al., 2017; Drozdov et al., 2019), whi"
2020.emnlp-main.354,N10-1115,0,0.0168319,"e and maximize the evidence lower bound (ELBO): log pθ (w) ≥ ELBO(w; φ, θ) = (1) Eqφ (z|w) [log pθ (w|z)] − KL[qφ (z|w)||p(z)] , where qφ (z|w) is a variational posterior, a neural network parameterized with φ. The expected loglikelihood term is estimated via the reparameterization trick (Kingma et al., 2014); the KL term can be computed analytically when p(z) and qφ (z|w) are normally distributed. 2.2 Visually grounded neural syntax learner The visually grounded neural syntax learner (VGNSL) comprises a parsing model and an image-text matching model. The parsing model is an easyfirst parser (Goldberg and Elhadad, 2010). It builds a parse greedily in a bottom-up manner while at the same time producing a semantic representation for each constituent in the parse (i.e., its ‘embedding’). The parser is optimized through R EIN FORCE (Williams, 1992). The reward encourages merging two adjacent constituents if the merge results in a constituent that is concrete, i.e., if its semantic representations is predictive of the corresponding image, as measured with a matching function. We omit details of the parser and how the semantic representations of constituents are computed, as they are not relevant to our approach,"
2020.emnlp-main.354,N19-1115,0,0.0367926,"Missing"
2020.emnlp-main.354,P19-1234,0,0.275624,"1 Introduction Grammar induction is a task of finding latent hierarchical structure of language. As a fundamental problem in computational linguistics, it has been extensively studied for decades (Lari and Young, 1990; Carroll and Charniak, 1992; Clark, 2001; 1 Our code is available at https://git.io/JU0JJ. Ivan TitovEæ ILLC, University of Amsterdam ititov@inf.ed.ac.uk Klein and Manning, 2002). Recently, deep learning models have been shown very effective across NLP tasks and have also been applied to grammar induction, greatly advancing the area (Shen et al., 2018, 2019; Kim et al., 2019a,b; Jin et al., 2019). These neural grammar-induction approaches have been generally limited to relying on text, without considering learning signals from other modalities. In contrast, the crucial aspect of natural language learning is that it is grounded in perceptual experiences (Barsalou, 1999; Fincher-Kiefer, 2001; Bisk et al., 2020). We thus anticipate improved language understanding by leveraging grounded learning. Promising results from grounded learning have been emerging in areas such as representation learning (Bruni et al., 2014; Kiela et al., 2018; Bordes et al., 2019). Typically, they use visual imag"
2020.emnlp-main.354,N18-1038,0,0.0776128,"the area (Shen et al., 2018, 2019; Kim et al., 2019a,b; Jin et al., 2019). These neural grammar-induction approaches have been generally limited to relying on text, without considering learning signals from other modalities. In contrast, the crucial aspect of natural language learning is that it is grounded in perceptual experiences (Barsalou, 1999; Fincher-Kiefer, 2001; Bisk et al., 2020). We thus anticipate improved language understanding by leveraging grounded learning. Promising results from grounded learning have been emerging in areas such as representation learning (Bruni et al., 2014; Kiela et al., 2018; Bordes et al., 2019). Typically, they use visual images as perceptual groundings of language and aim at improving continuous vector representations of language (e.g., word or sentence embeddings). In this work, we consider a more challenging problem: can visual groundings help us induce syntactic structure? We refer to this problem as visually grounded grammar induction. Shi et al. (2019) propose a visually grounded neural syntax learner (VG-NSL) to tackle the task. Specifically, they learn a parser from aligned imagesentence pairs (e.g., image-caption data), where each sentence describes vi"
2020.emnlp-main.354,P14-2135,0,0.0275555,"ptual experience of language and exploits visual semantics derived from images to improve continuous vector representatios of language. In contrast, we induce structured representations, discrete the words on it words on it in chinese in chinese apple has Figure 3: Upper: A parse output by the best run of VC-PCFG. Bottom: The corresponding gold tree. tree structure of language, by using visual groundings. We propose a model for the task within the contrastive learning framework. Learning involves estimating concreteness of spans, which generalizes word-level concreteness (Turney et al., 2011; Kiela et al., 2014). In the vision and machine learning community, unsupervised induction of structured image representations (aka scene graphs or world models) has been receiving increasing attention (Eslami et al., 2016; Burgess et al., 2019; Kipf et al., 2020). However, they typically rely solely on visual signal. An interesting extension of our work would be to consider joint induction of structured representations of images and text while guiding learning by an alignment loss. 6 Conclusion We have presented visually-grounded compound PCFGs (VC-PCFGs) that use compound PCFGs and generalize the visually groun"
2020.emnlp-main.354,P19-1228,0,0.49569,"Missing"
2020.emnlp-main.354,N19-1114,0,0.408793,"ecific inductive bias to obtain more informative reward signals. Another issue with VG-NSL is that the parser does not admit tractable estimation of the partition function and the posterior probabilities for constituent boundaries needed to compute the expected reward in closed form. Instead, VG-NSL relies on Monte Carlo policy gradients, potentially suffering from high variance. To alleviate the first issue, we propose to complement the image-text alignment-based loss with a loss defined on unlabeled text (i.e., its loglikelihood). As re-confirmed with neural models in Shen et al. (2019) and Kim et al. (2019a), text itself can drive induction of rich syntactic knowledge, so additionally optimizing the parser on raw text can be beneficial and complementary to visual grounded learning. To resolve the second issue, we resort to an extension of probabilistic contextfree grammar (PCFG) parsing model, compound PCFG (Kim et al., 2019a). It admits tractable estimation of the posteriors, needed in the alignment loss, with dynamical programming and leads to a fully-differentiable end-to-end visually grounded learning. More importantly, the PCFG parser lets us complement the alignment loss with a language m"
2020.emnlp-main.354,P18-1249,0,0.0267339,"is the mean vector of the variational posterior qφ (z|w). As δ(·) has zero mass everywhere but at the mode µφ (w), it is equivalently solving argmaxt pθ (t|w, µφ (w)). 4373 4 Experiments 4.1 Datasets and evaluation Datasets: We use MSCOCO (Lin et al., 2014). It consists of 82,783 training images, 1,000 validation images, and 1,000 test images. Each image is associated with 5 caption sentences. We encode images into 2048-dimensional vectors using the pre-trained ResNet-101 (He et al., 2016). At test time, only captions are used. We follow Shi et al. (2019) and parse test captions with Benepar (Kitaev and Klein, 2018). We use the same data preprocessing4 as in Shen et al. (2019) and Kim et al. (2019a), where punctuation is removed from all data, and the top 10,000 frequent words in training sentences are kept as the vocabulary. Evaluation: We mainly compare VC-PCFGs with VG-NSL (Shi et al., 2019). To verify the effectiveness of the use of visual groundings, we also compare our model with a C-PCFG trained only on the training captions. All models are run four times with different random seeds and for at most 15 epochs with early stopping (i.e., the image-caption loss / perplexity on the validation captions"
2020.emnlp-main.354,P02-1017,0,0.38033,"ree predicted by the best run of VC-PCFG. We can see that VC-PCFG identifies most NPs but makes mistakes in PP attachement and consequently fails to identify the VP. 5 apple Related work Grammar Induction has a long history in computational linguistics. Following observations that direct optimization of log-likelihood with the Expectation Maximization algorithm (Lari and Young, 1990) is not effective at producing effective grammars, a number of approaches have been developed, emboding various inductive biases or assumption about the language structure and its relation to surface realizations (Klein and Manning, 2002; Smith and Eisner, 2005; Cohen and Smith, 2009; Spitkovsky et al., 2010). The recent advances in the area have been brought by flexible neural models (Jin et al., 2019; Kim et al., 2019a,b; Drozdov et al., 2019). All these methods, with the exception of Shi et al. (2019), rely solely on text. Visually grounded learning is motivated by the observation that natural language is grounded in perceptual experiences (Steels, 1998; Barsalou, 1999; Fincher-Kiefer, 2001; Roy, 2002; Bisk et al., 2020). It has been shown effective in word representation learning (Bruni et al., 2014; Silberer and Lapata,"
2020.emnlp-main.354,2020.acl-main.234,0,0.155782,"Missing"
2020.emnlp-main.354,P15-1027,0,0.0245968,"h and Eisner, 2005; Cohen and Smith, 2009; Spitkovsky et al., 2010). The recent advances in the area have been brought by flexible neural models (Jin et al., 2019; Kim et al., 2019a,b; Drozdov et al., 2019). All these methods, with the exception of Shi et al. (2019), rely solely on text. Visually grounded learning is motivated by the observation that natural language is grounded in perceptual experiences (Steels, 1998; Barsalou, 1999; Fincher-Kiefer, 2001; Roy, 2002; Bisk et al., 2020). It has been shown effective in word representation learning (Bruni et al., 2014; Silberer and Lapata, 2014; Lazaridou et al., 2015) and sentence representation learning (Kiela et al., 2018; Bordes et al., 2019). All this work uses visual images as perceptual experience of language and exploits visual semantics derived from images to improve continuous vector representatios of language. In contrast, we induce structured representations, discrete the words on it words on it in chinese in chinese apple has Figure 3: Upper: A parse output by the best run of VC-PCFG. Bottom: The corresponding gold tree. tree structure of language, by using visual groundings. We propose a model for the task within the contrastive learning frame"
2020.emnlp-main.354,D15-1137,0,0.0115436,"inside algorithm and automatic differentiation (Eisner, 2016). 3.2 Span representation Estimation of the expected image-text matching scores relies on span representations. Ideally, a span representation should encode semantics of a span with its computation guided by its syntactic structure (Socher et al., 2013). The reliance on the predicted tree structure will result in propagating learning signals derived from the alignment of 4372 images and sentences back to the parser. To realize this desideratum, we could follow the inside algorithm and recursively compose span representations (Le and Zuidema, 2015; Stern et al., 2017; Drozdov et al., 2019), which is, however, time- and memory-inefficient in practice. Instead, we produce span representations largely independently of the parser, as we will explain below. The only way the parser model influences this representation is through the predicted constituent label: we use its distribution to compute the representation.3 Specificially, as a trade-off for a better training efficiency, we adopt a single-layer BiLSTM to encode spans. A mean-pooling layer is applied over the hidden states h of the BiLSTM and followed by a label-specific affine transf"
2020.emnlp-main.354,2020.acl-demos.38,0,0.202745,"Missing"
2020.emnlp-main.354,P19-1180,0,0.484081,"dily in a bottom-up manner while at the same time producing a semantic representation for each constituent in the parse (i.e., its ‘embedding’). The parser is optimized through R EIN FORCE (Williams, 1992). The reward encourages merging two adjacent constituents if the merge results in a constituent that is concrete, i.e., if its semantic representations is predictive of the corresponding image, as measured with a matching function. We omit details of the parser and how the semantic representations of constituents are computed, as they are not relevant to our approach, and refer the reader to Shi et al. (2019). However, as we will extend their image-text matching model, we explain this component of their approach more formally. In their work, this loss is used to learn the textual and visual representations. For every constituent c(i) of a sentence w(i) , they define the following triplet hinge loss: h i h(c(i) , v(i) ) = Ec0 m(c0 , v(i) ) − m(c(i) , v(i) ) +  + h i (i) 0 (i) (i) + Ev0 m(c , v )−m(c , v )+ , (2) + where [·]+ = max(0, ·),  is a positive margin, m(c, v) , cos(c, v) is the matching function measuring similarity between the constituent representation c and the image representation v"
2020.emnlp-main.354,P17-1076,0,0.0134622,"m and automatic differentiation (Eisner, 2016). 3.2 Span representation Estimation of the expected image-text matching scores relies on span representations. Ideally, a span representation should encode semantics of a span with its computation guided by its syntactic structure (Socher et al., 2013). The reliance on the predicted tree structure will result in propagating learning signals derived from the alignment of 4372 images and sentences back to the parser. To realize this desideratum, we could follow the inside algorithm and recursively compose span representations (Le and Zuidema, 2015; Stern et al., 2017; Drozdov et al., 2019), which is, however, time- and memory-inefficient in practice. Instead, we produce span representations largely independently of the parser, as we will explain below. The only way the parser model influences this representation is through the predicted constituent label: we use its distribution to compute the representation.3 Specificially, as a trade-off for a better training efficiency, we adopt a single-layer BiLSTM to encode spans. A mean-pooling layer is applied over the hidden states h of the BiLSTM and followed by a label-specific affine transformation fk (·) to p"
2020.emnlp-main.354,D11-1063,0,0.0106333,"isual images as perceptual experience of language and exploits visual semantics derived from images to improve continuous vector representatios of language. In contrast, we induce structured representations, discrete the words on it words on it in chinese in chinese apple has Figure 3: Upper: A parse output by the best run of VC-PCFG. Bottom: The corresponding gold tree. tree structure of language, by using visual groundings. We propose a model for the task within the contrastive learning framework. Learning involves estimating concreteness of spans, which generalizes word-level concreteness (Turney et al., 2011; Kiela et al., 2014). In the vision and machine learning community, unsupervised induction of structured image representations (aka scene graphs or world models) has been receiving increasing attention (Eslami et al., 2016; Burgess et al., 2019; Kipf et al., 2020). However, they typically rely solely on visual signal. An interesting extension of our work would be to consider joint induction of structured representations of images and text while guiding learning by an alignment loss. 6 Conclusion We have presented visually-grounded compound PCFGs (VC-PCFGs) that use compound PCFGs and generali"
2020.emnlp-main.354,Q18-1019,0,0.0644167,"Missing"
2020.emnlp-main.354,P14-1068,0,0.0486843,"ein and Manning, 2002; Smith and Eisner, 2005; Cohen and Smith, 2009; Spitkovsky et al., 2010). The recent advances in the area have been brought by flexible neural models (Jin et al., 2019; Kim et al., 2019a,b; Drozdov et al., 2019). All these methods, with the exception of Shi et al. (2019), rely solely on text. Visually grounded learning is motivated by the observation that natural language is grounded in perceptual experiences (Steels, 1998; Barsalou, 1999; Fincher-Kiefer, 2001; Roy, 2002; Bisk et al., 2020). It has been shown effective in word representation learning (Bruni et al., 2014; Silberer and Lapata, 2014; Lazaridou et al., 2015) and sentence representation learning (Kiela et al., 2018; Bordes et al., 2019). All this work uses visual images as perceptual experience of language and exploits visual semantics derived from images to improve continuous vector representatios of language. In contrast, we induce structured representations, discrete the words on it words on it in chinese in chinese apple has Figure 3: Upper: A parse output by the best run of VC-PCFG. Bottom: The corresponding gold tree. tree structure of language, by using visual groundings. We propose a model for the task within the c"
2020.emnlp-main.354,P13-1045,0,0.0318763,"). Moreover, VC-PCFGs let us additionally optimize a language modeling objective, complementing the visually grounded contrastive learning (Section 3.3). = X p(c|w)h(c, v) , (6) c∈w where p(c|w) is the conditional probability (i.e., marginal) of the span c given w. It can be efficiently computed with the inside algorithm and automatic differentiation (Eisner, 2016). 3.2 Span representation Estimation of the expected image-text matching scores relies on span representations. Ideally, a span representation should encode semantics of a span with its computation guided by its syntactic structure (Socher et al., 2013). The reliance on the predicted tree structure will result in propagating learning signals derived from the alignment of 4372 images and sentences back to the parser. To realize this desideratum, we could follow the inside algorithm and recursively compose span representations (Le and Zuidema, 2015; Stern et al., 2017; Drozdov et al., 2019), which is, however, time- and memory-inefficient in practice. Instead, we produce span representations largely independently of the parser, as we will explain below. The only way the parser model influences this representation is through the predicted const"
2020.emnlp-main.354,W10-2902,0,0.1109,"fies most NPs but makes mistakes in PP attachement and consequently fails to identify the VP. 5 apple Related work Grammar Induction has a long history in computational linguistics. Following observations that direct optimization of log-likelihood with the Expectation Maximization algorithm (Lari and Young, 1990) is not effective at producing effective grammars, a number of approaches have been developed, emboding various inductive biases or assumption about the language structure and its relation to surface realizations (Klein and Manning, 2002; Smith and Eisner, 2005; Cohen and Smith, 2009; Spitkovsky et al., 2010). The recent advances in the area have been brought by flexible neural models (Jin et al., 2019; Kim et al., 2019a,b; Drozdov et al., 2019). All these methods, with the exception of Shi et al. (2019), rely solely on text. Visually grounded learning is motivated by the observation that natural language is grounded in perceptual experiences (Steels, 1998; Barsalou, 1999; Fincher-Kiefer, 2001; Roy, 2002; Bisk et al., 2020). It has been shown effective in word representation learning (Bruni et al., 2014; Silberer and Lapata, 2014; Lazaridou et al., 2015) and sentence representation learning (Kiela"
2020.emnlp-main.616,N19-1314,0,0.0185138,"training (Belinkov et al., 2019). Adversarial attacks have recently been extended as an effective model analysis tool from vision to language tasks (Samanta and Mehta, 2017; Alzantot et al., 2018; Glockner et al., 2018; Zhang et al., 2019), including NMT (Cheng et al., 2018, 2019), where the focus so far has been on strategies requiring direct access to the victim model’s loss gradient or output distribution. Recent surveys suggested that state-of-the-art attacks often yield ungrammatical and meaning-destroying samples, thus diminishing their usefulness for the evaluation of model robustness (Michel et al., 2019; Morris et al., 2020). Targeted attacks on WSD abilities of translation models have so far remained unexplored. 6 Conclusion We conducted an initial investigation into leveraging data artifacts for the prediction of WSD errors in machine translation and proposed a simple adversarial attack strategy based on the presented insights. Our results show that WSD is not yet a solved problem in NMT, and while the general performance of popular model architectures is high, we can identify or create sentences where models are more likely to fail due to data biases. The effectiveness of our methods owes"
2020.emnlp-main.616,2020.findings-emnlp.341,0,0.0228425,"al., 2019). Adversarial attacks have recently been extended as an effective model analysis tool from vision to language tasks (Samanta and Mehta, 2017; Alzantot et al., 2018; Glockner et al., 2018; Zhang et al., 2019), including NMT (Cheng et al., 2018, 2019), where the focus so far has been on strategies requiring direct access to the victim model’s loss gradient or output distribution. Recent surveys suggested that state-of-the-art attacks often yield ungrammatical and meaning-destroying samples, thus diminishing their usefulness for the evaluation of model robustness (Michel et al., 2019; Morris et al., 2020). Targeted attacks on WSD abilities of translation models have so far remained unexplored. 6 Conclusion We conducted an initial investigation into leveraging data artifacts for the prediction of WSD errors in machine translation and proposed a simple adversarial attack strategy based on the presented insights. Our results show that WSD is not yet a solved problem in NMT, and while the general performance of popular model architectures is high, we can identify or create sentences where models are more likely to fail due to data biases. The effectiveness of our methods owes to neural models stru"
2020.emnlp-main.616,P16-1162,1,0.257586,"Missing"
2020.emnlp-main.616,P19-1164,0,0.0214994,"resolution capabilities of translation models (Rios et al., 2017; Liu et al., 2018). To our knowledge, no study so far has examined the interaction between training data artifacts and WSD performance in detail. Dataset artifacts, on the other hand, have previously been shown to enable models to make correct predictions based on incorrect or insufficient information (McCoy et al., 2019; Gururangan et al., 2018) by over-relying on spurious correlations present in the training data. Within NMT, models were found to exhibit gender-bias, reinforcing harmful stereotypes (Vanmassenhove et al., 2018; Stanovsky et al., 2019). As a response, strategies have been proposed for de-biasing the training data (Li and Vasconcelos, 2019; Le Bras et al., 2020), as well as for making models more robust to data biases through adversarial training (Belinkov et al., 2019). Adversarial attacks have recently been extended as an effective model analysis tool from vision to language tasks (Samanta and Mehta, 2017; Alzantot et al., 2018; Glockner et al., 2018; Zhang et al., 2019), including NMT (Cheng et al., 2018, 2019), where the focus so far has been on strategies requiring direct access to the victim model’s loss gradient or ou"
2020.emnlp-main.616,D19-1149,1,0.864039,"Missing"
2020.emnlp-main.616,P10-1023,0,0.0259456,"errors, while minimizing the models’ ability to disambiguate based on syntactic cues. English homographs are collected from web resources4 , excluding those that do not satisfy the above criteria. Refer to appendix A.2 for the full homograph list. We next compile a parallel lexicon of homograph translations, prioritizing a high coverage of all possible senses. Similar to (Raganato et al., 2019), 2 http://opus.nlpl.eu http://statmt.org/wmt19 4 http://7esl.com/homographs http://en.wikipedia.org/wiki/List_of_ English_homographs 3 we obtain sense-specific translations from crosslingual BabelNet (Navigli and Ponzetto, 2010) synsets. Since BabelNet entries vary in their granularity, we iteratively merge related synsets as long as they have at least three German translations in common or share at least one definition.5 This leaves us with multiple sense clusters of semantically related German translations per homograph. To further improve the quality of the lexicon, we manually clean and extend each homograph entry to address the noise inherent in BabelNet and its incomplete coverage.6 Appendix A.7 provides examples of the final sense clusters. In order to identify sentence contexts specific to each homograph sens"
2020.emnlp-main.616,D18-1334,0,0.0235428,"9), or to improve ambiguity resolution capabilities of translation models (Rios et al., 2017; Liu et al., 2018). To our knowledge, no study so far has examined the interaction between training data artifacts and WSD performance in detail. Dataset artifacts, on the other hand, have previously been shown to enable models to make correct predictions based on incorrect or insufficient information (McCoy et al., 2019; Gururangan et al., 2018) by over-relying on spurious correlations present in the training data. Within NMT, models were found to exhibit gender-bias, reinforcing harmful stereotypes (Vanmassenhove et al., 2018; Stanovsky et al., 2019). As a response, strategies have been proposed for de-biasing the training data (Li and Vasconcelos, 2019; Le Bras et al., 2020), as well as for making models more robust to data biases through adversarial training (Belinkov et al., 2019). Adversarial attacks have recently been extended as an effective model analysis tool from vision to language tasks (Samanta and Mehta, 2017; Alzantot et al., 2018; Glockner et al., 2018; Zhang et al., 2019), including NMT (Cheng et al., 2018, 2019), where the focus so far has been on strategies requiring direct access to the victim mo"
2020.emnlp-main.616,N19-4009,0,0.0258583,"Missing"
2020.emnlp-main.616,W18-6319,0,0.0121862,"ctors identified for the different senses of the homograph spring in the OS18 training set. Intuitively, if an NMT model disproportionately relies on simple surface-level correlations when resolving lexical ambiguity, it is more likely to make WSD errors when translating sentences that contain 5 A manual inspection found the clusters to be meaningful. The lexicon is released as part of our experimental code: http://github.com/demelin/detecting_ wsd_biases_for_nmt. 6 7636 season water source device summer winter come hot water find like back thing regime and hyper-parameter choices. SacreBLEU (Post, 2018) scores reported in Table 2 indicate that all models are reasonably competent. WMT Table 1: Examples of attractors for spring. strong attractors towards a wrong sense. To test this, we collect attractors from the extracted parallel sentences, quantifying their disambiguation bias (DB) using two metrics: Raw co-occurrence frequency (FREQ) and positive point-wise mutual information (PPMI) between attractors and homograph senses. FREQ is defined in Eqn.1, while Eqn.2 describes PPMI, with w ∈ V denoting an attractor term in the source vocabulary7 , and sc ∈ SC denoting a sense cluster in the set o"
2020.findings-emnlp.230,N18-1008,0,0.0196101,"ng studies on ST used a cascade of separately trained ASR and MT systems (Ney, 1999). Despite its simplicity, this approach inevitably suffers from mistakes made by ASR models, and is error prone. Research in this direction often focuses on strategies capable of mitigating the mismatch between ASR output and MT input, such as representing ASR outputs with lattices (Saleem et al., 2004; Mathias and Byrne, 2006; Zhang et al., 2019a; Beck et al., 2019), injecting synthetic ASR errors for robust MT (Tsvetkov et al., 2014; Cheng et al., 2018) and differentiable cascade modeling (Kano et al., 2017; Anastasopoulos and Chiang, 2018; Sperber et al., 2019). In contrast to cascading, another option is to perform direct speech-to-text translation. Duong et al. (2016) and B´erard et al. (2016) employ the attentional encoder-decoder model (Bahdanau et al., 2015) for E2E ST without accessing any intermediate transcriptions. E2E ST opens the way to bridging the modality gap directly, but it is data-hungry, sample-inefficient and often underperforms cascade models especially in low-resource settings (Bansal et al., 2018). This led researchers to explore solutions ranging from efficient neural architecture design (Karita et al.,"
2020.findings-emnlp.230,N19-1006,0,0.0749518,"mproves model convergence; feature selection makes training more stable. Compared to other models, the curve of ST with AFS is much smoother, suggesting its better regularization effect. We then investigate the effect of training data size, and show the results in Figure 7. Overall, we do not observe higher data efficiency by feature selection on low-resource settings. But instead, our results suggest that feature selection delivers larger performance improvement when more training data is available. With respect to data efficiency, ASR pretraining seems to be more important (Figure 7, left) (Bansal et al., 2019; Stoian et al., 2020). Com2538 10 17.5 # Selected Features 20 BLEU BLEU 15.0 12.5 10.0 7.5 18 16 ST + ASR-PT ST + Fixed Rate ST+AFSt ST + AFSt,f 14 ST ST + ASR-PT 5.0 2.5 50000 12 50000 6 4 2 100000 150000 200000 # Training Samples MuST-C En-De. We split the original training data into non-overlapped five subsets, and train different models with accumulated subsets. Results are reported on the test set. Note that we perform ASR pretraining on the original dataset. λ = 0.5, k = 6. 2500 Frequency 6000 4000 2000 0.5 1.0 1.5 Attention Weight 2.0 0.6 AFSt AFSt,f Fixed Rate 1.5 2.0 0 10 20 30 40 50"
2020.findings-emnlp.230,P19-1284,1,0.874511,"Missing"
2020.findings-emnlp.230,D19-5304,0,0.0916558,"in tokenization and letter case. To ease future cross-paper comparison, we provide SacreBLEU (Post, 2018)4 for our models. 5 Related Work Speech Translation Pioneering studies on ST used a cascade of separately trained ASR and MT systems (Ney, 1999). Despite its simplicity, this approach inevitably suffers from mistakes made by ASR models, and is error prone. Research in this direction often focuses on strategies capable of mitigating the mismatch between ASR output and MT input, such as representing ASR outputs with lattices (Saleem et al., 2004; Mathias and Byrne, 2006; Zhang et al., 2019a; Beck et al., 2019), injecting synthetic ASR errors for robust MT (Tsvetkov et al., 2014; Cheng et al., 2018) and differentiable cascade modeling (Kano et al., 2017; Anastasopoulos and Chiang, 2018; Sperber et al., 2019). In contrast to cascading, another option is to perform direct speech-to-text translation. Duong et al. (2016) and B´erard et al. (2016) employ the attentional encoder-decoder model (Bahdanau et al., 2015) for E2E ST without accessing any intermediate transcriptions. E2E ST opens the way to bridging the modality gap directly, but it is data-hungry, sample-inefficient and often underperforms casc"
2020.findings-emnlp.230,P18-1163,0,0.0603518,"LEU (Post, 2018)4 for our models. 5 Related Work Speech Translation Pioneering studies on ST used a cascade of separately trained ASR and MT systems (Ney, 1999). Despite its simplicity, this approach inevitably suffers from mistakes made by ASR models, and is error prone. Research in this direction often focuses on strategies capable of mitigating the mismatch between ASR output and MT input, such as representing ASR outputs with lattices (Saleem et al., 2004; Mathias and Byrne, 2006; Zhang et al., 2019a; Beck et al., 2019), injecting synthetic ASR errors for robust MT (Tsvetkov et al., 2014; Cheng et al., 2018) and differentiable cascade modeling (Kano et al., 2017; Anastasopoulos and Chiang, 2018; Sperber et al., 2019). In contrast to cascading, another option is to perform direct speech-to-text translation. Duong et al. (2016) and B´erard et al. (2016) employ the attentional encoder-decoder model (Bahdanau et al., 2015) for E2E ST without accessing any intermediate transcriptions. E2E ST opens the way to bridging the modality gap directly, but it is data-hungry, sample-inefficient and often underperforms cascade models especially in low-resource settings (Bansal et al., 2018). This led researchers"
2020.findings-emnlp.230,D19-1223,0,0.0367555,"Missing"
2020.findings-emnlp.230,N19-1202,0,0.284852,"Missing"
2020.findings-emnlp.230,N16-1109,0,0.62118,"−0.4 play P L is EY1 IH1 Z not NAA1 just JH IH0 S T CH child ’s AY1 L DT S games G EY1 M Z Figure 1: Example illustrating our motivation. We plot the amplitude and frequency spectrum of an audio segment (top), paired with its time-aligned words and phonemes (bottom). Information inside an audio stream is not uniformly distributed. We propose to dynamically capture speech features corresponding to informative signals (red rectangles) to improve ST. Introduction End-to-end (E2E) speech translation (ST), a paradigm that directly maps audio to a foreign text, has been gaining popularity recently (Duong et al., 2016; B´erard et al., 2016; Bansal et al., 2018; Di Gangi et al., 2019; Wang et al., 2019). Based on the attentional encoder-decoder framework (Bahdanau et al., 2015), it optimizes model parameters under direct translation supervision. This end-toend paradigm avoids the problem of error propagation that is inherent in cascade models where an automatic speech recognition (ASR) model and 1 We release our source code at https://github. com/bzhangGo/zero. a machine translation (MT) model are chained together. Nonetheless, previous work still reports that E2E ST delivers inferior performance compared t"
2020.findings-emnlp.230,L18-1001,0,0.276493,"hat correspond to git > 0, and pass them similarly as done with word embeddings to the ST encoder. We employ sinusoidal positional encoding to distinguish features at different positions. Except for the input to the ST encoder, our E2E ST follows the standard encoder-decoder translation model (MST in Eq. 10) and is optimized with LMLE alone as in Eq. 9. Intuitively, AFS bridges the gap between ASR output and MT input by selecting transcriptaligned speech features. 4 Experiments Datasets and Preprocessing We experiment with two benchmarks: the Augmented LibriSpeech dataset (LibriSpeech En-Fr) (Kocabiyikoglu et al., 2018) and the multilingual MuST-C dataset (MuSTC) (Di Gangi et al., 2019). LibriSpeech En-Fr is 3 Other candidate gating models, like linear mapping upon mean-pooled encoder outputs, delivered worse performance in our preliminary experiments. collected by aligning e-books in French with English utterances of LibriSpeech, further augmented with French translations offered by Google Translate. We use the 100 hours clean training set for training, including 47K utterances to train ASR models and double the size for ST models after concatenation with the Google translations. We report results on the te"
2020.findings-emnlp.230,P07-2045,0,0.0118427,"mon test set, whose size ranges from 2502 (Es) to 2641 (De) utterances. For all datasets, we extract 40-dimensional logMel filterbanks with a step size of 10ms and window size of 25ms as the acoustic features. We expand these features with their first and second-order derivatives, and stabilize them using mean subtraction and variance normalization. We stack the features corresponding to three consecutive frames without overlapping to the left, resulting in the final 360-dimensional acoustic input. For transcriptions and translations, we tokenize and truecase all the text using Moses scripts (Koehn et al., 2007). We train subword models (Sennrich et al., 2016) on each dataset with a joint vocabulary size of 16K to handle rare words, and share the model for ASR, MT and ST. We train all models without removing punctuation. Model Settings and Baselines We adopt the Transformer architecture (Vaswani et al., 2017) for all tasks, including MASR (Eq. 6), MAFS (Eq. 8) and MST (Eq. 10). The encoder and decoder consist of 6 identical layers, each including a self-attention sublayer, a cross-attention sublayer (decoder alone) and a feedforward sublayer. We employ the base setting for experiments: hidden size d"
2020.findings-emnlp.230,P02-1040,0,0.106095,"Missing"
2020.findings-emnlp.230,W18-6319,0,0.0501918,"Missing"
2020.findings-emnlp.230,P16-1162,1,0.24597,"to 2641 (De) utterances. For all datasets, we extract 40-dimensional logMel filterbanks with a step size of 10ms and window size of 25ms as the acoustic features. We expand these features with their first and second-order derivatives, and stabilize them using mean subtraction and variance normalization. We stack the features corresponding to three consecutive frames without overlapping to the left, resulting in the final 360-dimensional acoustic input. For transcriptions and translations, we tokenize and truecase all the text using Moses scripts (Koehn et al., 2007). We train subword models (Sennrich et al., 2016) on each dataset with a joint vocabulary size of 16K to handle rare words, and share the model for ASR, MT and ST. We train all models without removing punctuation. Model Settings and Baselines We adopt the Transformer architecture (Vaswani et al., 2017) for all tasks, including MASR (Eq. 6), MAFS (Eq. 8) and MST (Eq. 10). The encoder and decoder consist of 6 identical layers, each including a self-attention sublayer, a cross-attention sublayer (decoder alone) and a feedforward sublayer. We employ the base setting for experiments: hidden size d = 512, attention head 8 and feedforward size 2048"
2020.findings-emnlp.230,Q19-1020,0,0.0126597,"f separately trained ASR and MT systems (Ney, 1999). Despite its simplicity, this approach inevitably suffers from mistakes made by ASR models, and is error prone. Research in this direction often focuses on strategies capable of mitigating the mismatch between ASR output and MT input, such as representing ASR outputs with lattices (Saleem et al., 2004; Mathias and Byrne, 2006; Zhang et al., 2019a; Beck et al., 2019), injecting synthetic ASR errors for robust MT (Tsvetkov et al., 2014; Cheng et al., 2018) and differentiable cascade modeling (Kano et al., 2017; Anastasopoulos and Chiang, 2018; Sperber et al., 2019). In contrast to cascading, another option is to perform direct speech-to-text translation. Duong et al. (2016) and B´erard et al. (2016) employ the attentional encoder-decoder model (Bahdanau et al., 2015) for E2E ST without accessing any intermediate transcriptions. E2E ST opens the way to bridging the modality gap directly, but it is data-hungry, sample-inefficient and often underperforms cascade models especially in low-resource settings (Bansal et al., 2018). This led researchers to explore solutions ranging from efficient neural architecture design (Karita et al., 2019; Di Gangi et al.,"
2020.findings-emnlp.230,E14-1065,0,0.0202094,"ison, we provide SacreBLEU (Post, 2018)4 for our models. 5 Related Work Speech Translation Pioneering studies on ST used a cascade of separately trained ASR and MT systems (Ney, 1999). Despite its simplicity, this approach inevitably suffers from mistakes made by ASR models, and is error prone. Research in this direction often focuses on strategies capable of mitigating the mismatch between ASR output and MT input, such as representing ASR outputs with lattices (Saleem et al., 2004; Mathias and Byrne, 2006; Zhang et al., 2019a; Beck et al., 2019), injecting synthetic ASR errors for robust MT (Tsvetkov et al., 2014; Cheng et al., 2018) and differentiable cascade modeling (Kano et al., 2017; Anastasopoulos and Chiang, 2018; Sperber et al., 2019). In contrast to cascading, another option is to perform direct speech-to-text translation. Duong et al. (2016) and B´erard et al. (2016) employ the attentional encoder-decoder model (Bahdanau et al., 2015) for E2E ST without accessing any intermediate transcriptions. E2E ST opens the way to bridging the modality gap directly, but it is data-hungry, sample-inefficient and often underperforms cascade models especially in low-resource settings (Bansal et al., 2018)."
2020.findings-emnlp.230,2020.acl-main.344,0,0.240923,"ription. To improve speech encoding, we apply logarithmic penalty on attention to enforce short-range dependency (Di Gangi et al., 2019) and use trainable positional embedding with a maximum length of 2048. Apart from LMLE , we augment the training objective with the connectionist temporal classification (Graves et al., 2006, CTC) loss LCTC as in Eq. 5. Note η = 1 − γ. The CTC loss is applied to the encoder outputs, guiding them to align with their corresponding transcription (sub)words and improving the encoder’s robustness (Karita et al., 2019). Following previous work (Karita et al., 2019; Wang et al., 2020), we set γ to 0.3. (7)  3. Train ST model with pretrained and frozen ASR and AFS submodules until convergence:  ing encoder and decoder respectively. F (·) denotes the AFS approach, and F E means freezing the ASR encoder and the AFS module during training. Note that our framework puts no constraint on the architecture of the encoder and decoder in any task, although we adopt the multi-head dot-product attention network (Vaswani et al., 2017) for our experiments. Note that our model only requires pair-wise training corpora, (X, Y ) for ASR, and (X, Z) for ST. Lt0 (X) = n X i=1 1 − p(git = 0|"
2020.findings-emnlp.230,2021.findings-acl.255,1,0.952716,"Missing"
2020.findings-emnlp.230,P19-1649,0,0.290045,"zero. a machine translation (MT) model are chained together. Nonetheless, previous work still reports that E2E ST delivers inferior performance compared to cascade methods (Niehues et al., 2019). We study one reason for the difficulty of training E2E ST models, namely the uneven spread of information in the speech signal, as visualized in Figure 1, and the consequent difficulty of extracting informative features. Features corresponding to uninformative signals, such as pauses or noise, increase the input length and bring in unmanageable noise for ST. This increases the difficulty of learning (Zhang et al., 2019b; Na et al., 2019) and reduces translation performance. In this paper, we propose adaptive feature selection (AFS) for ST to explicitly eliminate uninformative features. Figure 2 shows the overall architecture. We employ a pretrained ASR encoder to induce contextual speech features, followed by an ST encoder bridging the gap between speech and translation modalities. AFS is inserted in-between them to select a subset of features for ST encoding (see red rectangles in Figure 1). To ensure that the selected features are well-aligned to transcriptions, we pretrain AFS on ASR. AFS estimates the i"
2020.iwpt-1.7,D18-1001,1,0.933625,"t to a server which performs potentially computationally intensive operations and returns a new data, still encrypted, which only the client can decipher. All of this is done without the server itself ever being exposed to the actual content of the encrypted input data. While solutions for generic homomorphic encryption have been discovered, they are either computationally inefficient (Gentry, 2010) or have strong limitations in regards to the depth and complexity of computation they permit (Bos et al., 2013). x y server eavesdropper Figure 2: General setting illustration (figure adapted from Coavoux et al. 2018). An NLP client encrypts an x into y through obfuscation and y is sent to an NLP server. The NLP server (potentially even a legacy one) does not need to be modified to de-obfuscate y. An eavesdropper (a possibly malicious channel listener) only has access to y which is needed to be deobfuscated to gain any information about x. In this paper, we consider a softer version of homomorphic encryption in the form of obfuscation for natural language. Our goal is to identify an efficient function that stochastically transforms a given natural language input (such as a sentence) into another input whic"
2020.iwpt-1.7,N19-1423,0,0.02218,"ttacker experiments, we assume that it is known which words in the sentence are obfuscated. As such, the results we provide for attacking our obfuscation are an upper bound. In practice, an attacker would also have to identify which words were substituted for new words, which may lead to a small decrease in its accuracy. 5.1 6 In this section, we describe our experiments with our obfuscation model. We first describe the experimental setting and then turn to the results.6 6.1 Trained Attacker Pretrained Attacker In addition to a trained attacker, we also use a conditional language model, BERT (Devlin et al., 2019).5 BERT is based on the Transformer model of Vaswani et al. (2017), and uses a bidirectional encoder to obtain “contextual” embeddings for each word in a given sentence. We use the BERT model by masking out each obfuscated word, and then predicting the masked word similar to the “masked language task” that is mentioned by Devlin et al. (2019). This means that the embeddings in each position are fed into a softmax function to predict the missing word. We use the 5 We use the implementation available https://github.com/huggingface/ pytorch-pretrained-BERT. Experimental Setting In our experiments"
2020.iwpt-1.7,D18-1002,0,0.0210957,"rlap is 1. There was a stark difference between the two averages of the overlap sizes. For the random baseline model, the average was 1.46 (over 5,680 tokens) and for the neural model the average was 1.80. The difference between these two averages is statistically significant with p-value < 0.05 in a one-sided t-test. 7 Related Work There has been a significant increase in interest in the topic of privacy in the NLP community in recent years. For example, Reddy and Knight (2016) focused on obfuscation of gender features from social media text, while Li et al. (2018), Coavoux et al. (2018) and Elazar and Goldberg (2018) focused on the removal of private information from neural representations such as named entities and demographic information. Unlike the latter work, we are interested in preserving the privacy of the inputs themselves, while requiring no extra work from deployed NLP software which processes these 12 The verbs were lemmatized first using the WordNet lemmatizer available in NLTK. 69 inputs. Marujo et al. (2015), for example, perform multi-document summarization on an approximate version of the original documents. Differential privacy (Dwork, 2008) which aims to protect the privacy of informati"
2020.iwpt-1.7,kingsbury-palmer-2002-treebank,0,0.307791,"on NN . . . . . . . . . . . . strength professionalism direction NN elsewhere near even RB . . . . Table 3: Example of five sentences obfuscated with the random and neural models. Words in italics are the ones being substituted (or the substitutes). The obfuscated terms are named entities, nouns, adjectives, verbs and adverbs. and its substituted version beyond them having been seen in the training data with the same partof-speech tag. To further test whether the neural model preserves other syntactic similarities between the original and obfuscated sentences, we took all verbs from Propbank (Kingsbury and Palmer, 2002) and created a signature for each one: the list of argument types it can appear with. For example, the signature for yield is 01,012, which means that “yield” appears with two frames in Propbank, one with two arguments and the other with three arguments. We then calculated for each verb12 that appears in the original sentence the overlap between its signature and the signature of the verb in the obfuscated sentence (neural or random). This overlap is counted as the size of the intersection of the frame signatures of the two verbs. For example, the signature of advocate might be 012 while the s"
2020.iwpt-1.7,P18-2005,0,0.124044,"test the efficiency of our obfuscation model by developing two independent attacker models. Their goal is to recover the original words by inspecting only the obfuscated sentence. The attacker models may have access to all data that the parser and the obfuscator models were trained and developed on. This is perhaps unlike other scenarios in which the training set is assumed to be inaccessible to any attacker. We note that ideally, we would want to show that our obfuscation model retains privacy universally for any attacker. However, this is quite a difficult task, and we follow Coavoux et al. (2018) in presenting two strong attackers which may represent possible universal attackers. In our attacker experiments, we assume that it is known which words in the sentence are obfuscated. As such, the results we provide for attacking our obfuscation are an upper bound. In practice, an attacker would also have to identify which words were substituted for new words, which may lead to a small decrease in its accuracy. 5.1 6 In this section, we describe our experiments with our obfuscation model. We first describe the experimental setting and then turn to the results.6 6.1 Trained Attacker Pretraine"
2020.iwpt-1.7,W18-2501,0,0.0221199,"Missing"
2020.iwpt-1.7,J93-2004,0,0.0702364,"training, the constituency parser that is included in the AllenNLP software package (Gardner et al., 2018) was used.7 For our dependency parser, we follow the canonical setting of using pre-trained word embedding, 1D convolutional character level embedding and POS tag embedding, each of 100 dimensions as the input feature. We also use a three-layer bidirectional LSTM with Bayesian dropout (Gal and Ghahramani, 2016) as the encoder. We use the biaffine attention mechanism to obtain the prediction for each head, and also the prediction for the edge labels. We use the English Penn Treebank (PTB; Marcus et al. 1993) version 3.0 converted using Stanford dependencies for training the dependency parser. We follow the standard parsing split for training (sections 01–21), development (section 22) and test sets (section 23). The training set portion of the PTB data is also used to train our neural obfuscator model. We also create a spectrum over the POS tags to decide on the set P for each of our experiments (see Section 3.1). This spectrum is described in Table 1. Our first attacker works by first encoding the obfuscated sentence with a BiLSTM network. We then try to predict original words by using a feedforw"
2020.iwpt-1.7,W16-5603,0,0.0277543,"gnatures of the two verbs. For example, the signature of advocate might be 012 while the signature of affect is 012,01. Therefore, their overlap is 1. There was a stark difference between the two averages of the overlap sizes. For the random baseline model, the average was 1.46 (over 5,680 tokens) and for the neural model the average was 1.80. The difference between these two averages is statistically significant with p-value < 0.05 in a one-sided t-test. 7 Related Work There has been a significant increase in interest in the topic of privacy in the NLP community in recent years. For example, Reddy and Knight (2016) focused on obfuscation of gender features from social media text, while Li et al. (2018), Coavoux et al. (2018) and Elazar and Goldberg (2018) focused on the removal of private information from neural representations such as named entities and demographic information. Unlike the latter work, we are interested in preserving the privacy of the inputs themselves, while requiring no extra work from deployed NLP software which processes these 12 The verbs were lemmatized first using the WordNet lemmatizer available in NLTK. 69 inputs. Marujo et al. (2015), for example, perform multi-document summa"
2020.wmt-1.62,P19-1122,0,0.296453,"great speedup, albeit at a high cost to generation quality. Follow-up studies often seek solutions to recover the performance (Libovick´y and Helcl, 2018; Guo et al., 2019; Shao et al., 2020; Ghazvininejad et al., 2020; Ran et al., 2020), but also reveal the trade-off between the quality and speed in terms of autoregressiveness. This motivates researchers to discover the optimal balance by resorting to semi-autoregressive modeling (Wang et al., 2018; Stern et al., 2018), iterative refinement (Lee et al., 2018; Stern et al., 2019; Ghazvininejad et al., 2019) or in-between (Kaiser et al., 2018; Akoury et al., 2019). We hypothesize that generation order affects the felicity of independence assumptions made in semiautoregressive modelling. Unlike generation with flexible orders (Emelianenko et al., 2019; Stern et al., 2019; Gu et al., 2019a), we employ deterministic generation order for model simplicity and training efficiency, specifically focusing on bidirectional decoding. The study of bidirectional modeling dates back to the era of phase-based statistical machine translation (Watanabe and Sumita, 2002; Finch and Sumita, 2009) and recently gained popularity in neural machine translation (Liu et al., 3"
2020.wmt-1.62,D09-1117,0,0.0391274,"n et al., 2019; Ghazvininejad et al., 2019) or in-between (Kaiser et al., 2018; Akoury et al., 2019). We hypothesize that generation order affects the felicity of independence assumptions made in semiautoregressive modelling. Unlike generation with flexible orders (Emelianenko et al., 2019; Stern et al., 2019; Gu et al., 2019a), we employ deterministic generation order for model simplicity and training efficiency, specifically focusing on bidirectional decoding. The study of bidirectional modeling dates back to the era of phase-based statistical machine translation (Watanabe and Sumita, 2002; Finch and Sumita, 2009) and recently gained popularity in neural machine translation (Liu et al., 3 Autoregressive Transformer Transformer (Vaswani et al., 2017), the state-of-theart neural sequence generation model, follows the autoregressive factorization as in Eq. 1. To handle the dependency of target word yt on previous target words y<t , Transformer relies on a masked selfattention network in the decoder: ! l Kl T Q √ + M Vl (3) ATT(Yl , M) = f d where Ql , Kl , Vl = Wql Yl , Wkl Yl , Wvl Yl ∈ Rn×d , f (·) denotes softmax operation, d is model dimension and l is layer depth. Wq , Wk , Wv ∈ Rd×d are trainable pa"
2020.wmt-1.62,D19-1633,0,0.059964,"tion where target words are predicted independently, leading to great speedup, albeit at a high cost to generation quality. Follow-up studies often seek solutions to recover the performance (Libovick´y and Helcl, 2018; Guo et al., 2019; Shao et al., 2020; Ghazvininejad et al., 2020; Ran et al., 2020), but also reveal the trade-off between the quality and speed in terms of autoregressiveness. This motivates researchers to discover the optimal balance by resorting to semi-autoregressive modeling (Wang et al., 2018; Stern et al., 2018), iterative refinement (Lee et al., 2018; Stern et al., 2019; Ghazvininejad et al., 2019) or in-between (Kaiser et al., 2018; Akoury et al., 2019). We hypothesize that generation order affects the felicity of independence assumptions made in semiautoregressive modelling. Unlike generation with flexible orders (Emelianenko et al., 2019; Stern et al., 2019; Gu et al., 2019a), we employ deterministic generation order for model simplicity and training efficiency, specifically focusing on bidirectional decoding. The study of bidirectional modeling dates back to the era of phase-based statistical machine translation (Watanabe and Sumita, 2002; Finch and Sumita, 2009) and recently gained"
2020.wmt-1.62,Q19-1042,0,0.332934,"2020), but also reveal the trade-off between the quality and speed in terms of autoregressiveness. This motivates researchers to discover the optimal balance by resorting to semi-autoregressive modeling (Wang et al., 2018; Stern et al., 2018), iterative refinement (Lee et al., 2018; Stern et al., 2019; Ghazvininejad et al., 2019) or in-between (Kaiser et al., 2018; Akoury et al., 2019). We hypothesize that generation order affects the felicity of independence assumptions made in semiautoregressive modelling. Unlike generation with flexible orders (Emelianenko et al., 2019; Stern et al., 2019; Gu et al., 2019a), we employ deterministic generation order for model simplicity and training efficiency, specifically focusing on bidirectional decoding. The study of bidirectional modeling dates back to the era of phase-based statistical machine translation (Watanabe and Sumita, 2002; Finch and Sumita, 2009) and recently gained popularity in neural machine translation (Liu et al., 3 Autoregressive Transformer Transformer (Vaswani et al., 2017), the state-of-theart neural sequence generation model, follows the autoregressive factorization as in Eq. 1. To handle the dependency of target word yt on previous t"
2020.wmt-1.62,D19-5601,0,0.018062,"In contrast to Zhou et al. (2019), we only make minimal changes to the standard Transformer decoder, which benefits efficiency during training and inference, and makes our method easy to implement. We also find improvements in both decoding speed and translation quality compared to (Wang et al., 2018; Zhou et al., 2019). Related Work Efforts on fast sequence generation come along with the rapid development of encoder-decoder models (Vaswani et al., 2017). A straightforward way is to reduce the amount of computation. Methods in this category range from teacherstudent model (Kim and Rush, 2016; Hayashi et al., 2019), constrained softmax prediction (Hu et al., 2015), beam search cube pruning (Zhang et al., 2018c), float-point quantization (Wu et al., 2016; Bhandare et al., 2019), model pruning (See et al., 2016), to simplified decoder architectures, such as lightweight recurrent models (Zhang et al., 2018b; Zhang and Sennrich, 2019; Kim et al., 2019), average attention network (Zhang et al., 2018a), merged attention network (Zhang et al., 2019a), dynamic convolution (Wu et al., 2019), and hybrid attentions (Shazeer, 2019; Wang et al., 2019), .etc. Nonetheless, the above methods still suffer from the infer"
2020.wmt-1.62,2015.mtsummit-papers.23,0,0.0459221,"changes to the standard Transformer decoder, which benefits efficiency during training and inference, and makes our method easy to implement. We also find improvements in both decoding speed and translation quality compared to (Wang et al., 2018; Zhou et al., 2019). Related Work Efforts on fast sequence generation come along with the rapid development of encoder-decoder models (Vaswani et al., 2017). A straightforward way is to reduce the amount of computation. Methods in this category range from teacherstudent model (Kim and Rush, 2016; Hayashi et al., 2019), constrained softmax prediction (Hu et al., 2015), beam search cube pruning (Zhang et al., 2018c), float-point quantization (Wu et al., 2016; Bhandare et al., 2019), model pruning (See et al., 2016), to simplified decoder architectures, such as lightweight recurrent models (Zhang et al., 2018b; Zhang and Sennrich, 2019; Kim et al., 2019), average attention network (Zhang et al., 2018a), merged attention network (Zhang et al., 2019a), dynamic convolution (Wu et al., 2019), and hybrid attentions (Shazeer, 2019; Wang et al., 2019), .etc. Nonetheless, the above methods still suffer from the inference bottleneck caused by the sequential nature of"
2020.wmt-1.62,P02-1040,0,0.106272,"Missing"
2020.wmt-1.62,D16-1139,0,0.311654,"ropose a modified beam search algorithm. 504 We extensively experiment on five machine translation tasks and two document summarization tasks, with an in-depth analysis studying the impact of batch size, beam size and sequence length on the decoding speed. We close our analysis by examining the capacity of our model in handling long-range dependencies. On these tasks, IBDecoder yields ∼2× speedup against Transformer at inference, and reaches 4×–11× after pairing it with SA. Still, the overall generation quality is comparable. When we pair our method with sequence-level knowledge distillation (Kim and Rush, 2016), we outperform a Transformer baseline on 6 out of 7 tasks. Our contributions are summarized below: • We propose IBDecoder, following a bidirectional factorization of the conditional probability, for fast sequence generation. IBDecoder retains the training efficiency and is easy to implement. • We extend IBDecoder to enable multi-word simultaneous generation by investigating integration with IMDecoder and SA. Results show that IBDecoder + SA performs better than IMDecoder. • We propose a modified beam search algorithm to support step-wise parallel generation. • On several sequence generation b"
2020.wmt-1.62,W18-6319,0,0.0117233,"beam search algorithm as in Algorithm 1. For each partial hyExperiments Setup We test our model on machine translation (MT) and document summarization. We train MT models on five different language pairs: WMT14 English-German (En-De, Bojar et al., 2014), WMT14 English-French (En-Fr, Bojar et al., 2014), WMT16 Romanian-English (Ro-En, Bojar et al., 2016), WMT18 English-Russian (EnRu, Bojar et al., 2018) and WAT17 Small-NMT English-Japanese (En-Ja, Nakazawa et al., 2017). Translation quality is measured by BLEU (Papineni et al., 2002), and we report detokenized BLEU using the toolkit sacreBLEU (Post, 2018)4 except for En-Ja. Following Gu et al. (2019b), we segment Japanese text with KyTea5 and compute tokenized BLEU. We train document summarization models on two benchmark datasets: the non-anonymized version of the CNN/Daily Mail dataset (CDMail, Hermann et al., 2015) and the Annotated English Gigaword (Gigaword, Rush et al., 2015). We evaluate the summarization quality using ROUGEL (Lin, 2004). We provide details of data preprocessing and model settings in Appendix A. We perform thorough analysis of our model on WMT14 En-De. We also report results improved by knowledge distillation (KD, Kim an"
2020.wmt-1.62,2020.acl-main.277,0,0.0288259,"Zhang et al., 2019a), dynamic convolution (Wu et al., 2019), and hybrid attentions (Shazeer, 2019; Wang et al., 2019), .etc. Nonetheless, the above methods still suffer from the inference bottleneck caused by the sequential nature of autoregressive models. Instead, Gu et al. (2018) propose non-autoregressive generation where target words are predicted independently, leading to great speedup, albeit at a high cost to generation quality. Follow-up studies often seek solutions to recover the performance (Libovick´y and Helcl, 2018; Guo et al., 2019; Shao et al., 2020; Ghazvininejad et al., 2020; Ran et al., 2020), but also reveal the trade-off between the quality and speed in terms of autoregressiveness. This motivates researchers to discover the optimal balance by resorting to semi-autoregressive modeling (Wang et al., 2018; Stern et al., 2018), iterative refinement (Lee et al., 2018; Stern et al., 2019; Ghazvininejad et al., 2019) or in-between (Kaiser et al., 2018; Akoury et al., 2019). We hypothesize that generation order affects the felicity of independence assumptions made in semiautoregressive modelling. Unlike generation with flexible orders (Emelianenko et al., 2019; Stern et al., 2019; Gu et"
2020.wmt-1.62,D18-1149,0,0.019092,"018) propose non-autoregressive generation where target words are predicted independently, leading to great speedup, albeit at a high cost to generation quality. Follow-up studies often seek solutions to recover the performance (Libovick´y and Helcl, 2018; Guo et al., 2019; Shao et al., 2020; Ghazvininejad et al., 2020; Ran et al., 2020), but also reveal the trade-off between the quality and speed in terms of autoregressiveness. This motivates researchers to discover the optimal balance by resorting to semi-autoregressive modeling (Wang et al., 2018; Stern et al., 2018), iterative refinement (Lee et al., 2018; Stern et al., 2019; Ghazvininejad et al., 2019) or in-between (Kaiser et al., 2018; Akoury et al., 2019). We hypothesize that generation order affects the felicity of independence assumptions made in semiautoregressive modelling. Unlike generation with flexible orders (Emelianenko et al., 2019; Stern et al., 2019; Gu et al., 2019a), we employ deterministic generation order for model simplicity and training efficiency, specifically focusing on bidirectional decoding. The study of bidirectional modeling dates back to the era of phase-based statistical machine translation (Watanabe and Sumita,"
2020.wmt-1.62,D15-1044,0,0.0498232,"sh (Ro-En, Bojar et al., 2016), WMT18 English-Russian (EnRu, Bojar et al., 2018) and WAT17 Small-NMT English-Japanese (En-Ja, Nakazawa et al., 2017). Translation quality is measured by BLEU (Papineni et al., 2002), and we report detokenized BLEU using the toolkit sacreBLEU (Post, 2018)4 except for En-Ja. Following Gu et al. (2019b), we segment Japanese text with KyTea5 and compute tokenized BLEU. We train document summarization models on two benchmark datasets: the non-anonymized version of the CNN/Daily Mail dataset (CDMail, Hermann et al., 2015) and the Annotated English Gigaword (Gigaword, Rush et al., 2015). We evaluate the summarization quality using ROUGEL (Lin, 2004). We provide details of data preprocessing and model settings in Appendix A. We perform thorough analysis of our model on WMT14 En-De. We also report results improved by knowledge distillation (KD, Kim and Rush, 2016). 507 4 5 Signature BLEU+c.mixed+#.1+s.exp+tok.13a+v.1.4.3 http://www.phontron.com/kytea/ ID Model B h c BLEU↑ +KD↑ Latency↓ Speedup↑ Train↑ 1 Transformer 4 1 1 1 26.9 26.0 27.3 26.8 387 294 1.00× 1.32× 1.00× 2 IBDecoder 4 1 2 1 26.2 25.0 27.1 26.8 204 166 1.90× 2.33× 0.98× 3 2 + SA 4 1 2 2 23.0 21.7 26.3 26.0 117 89"
2020.wmt-1.62,D18-1336,0,0.0462902,"Missing"
2020.wmt-1.62,K16-1029,0,0.0688829,"Missing"
2020.wmt-1.62,W04-1013,0,0.0540656,"., 2018) and WAT17 Small-NMT English-Japanese (En-Ja, Nakazawa et al., 2017). Translation quality is measured by BLEU (Papineni et al., 2002), and we report detokenized BLEU using the toolkit sacreBLEU (Post, 2018)4 except for En-Ja. Following Gu et al. (2019b), we segment Japanese text with KyTea5 and compute tokenized BLEU. We train document summarization models on two benchmark datasets: the non-anonymized version of the CNN/Daily Mail dataset (CDMail, Hermann et al., 2015) and the Annotated English Gigaword (Gigaword, Rush et al., 2015). We evaluate the summarization quality using ROUGEL (Lin, 2004). We provide details of data preprocessing and model settings in Appendix A. We perform thorough analysis of our model on WMT14 En-De. We also report results improved by knowledge distillation (KD, Kim and Rush, 2016). 507 4 5 Signature BLEU+c.mixed+#.1+s.exp+tok.13a+v.1.4.3 http://www.phontron.com/kytea/ ID Model B h c BLEU↑ +KD↑ Latency↓ Speedup↑ Train↑ 1 Transformer 4 1 1 1 26.9 26.0 27.3 26.8 387 294 1.00× 1.32× 1.00× 2 IBDecoder 4 1 2 1 26.2 25.0 27.1 26.8 204 166 1.90× 2.33× 0.98× 3 2 + SA 4 1 2 2 23.0 21.7 26.3 26.0 117 89 3.31× 4.35× 0.98× 4 IMDecoder 4 1 4 1 21.5 19.7 24.6 24.1 102 85"
2020.wmt-1.62,E17-2060,1,0.847341,"erformance drop w/ and w/o KD. Further analysis reveals that, as the dependency between predicted target words weakens, our model suffers from more serious over-translation issue, yielding larger OTEM (Yang et al., 2018). Although n-gram deduplication slightly improves quality8 , it does not explain the whole performance drop, echoing with Wang et al. (2018). We recommend using c = 2 for a good balance. In addition, the reduction of OTEM by KD in Figure 4 partially clarifies its improvement on quality. Analysis on Long-range Dependency We adopt the subject-verb agreement task from Lingeval97 (Sennrich, 2017) for analysis. We can see from the results in Figure 5 that IBDecoder 8 we only applied deduplication for results in Figure 4. Comparison to Previous Work Results in Table 5 show that our model outperforms SynST (Akoury et al., 2019) in quality, and slightly surpasses the Levenshtein Transformer (Gu et al., 2019b) in speed. Particularly, our model (27.50† /2.33×) surpasses SAT (Wang et al., 2018) (26.09† /2.07×) and SBSG (Zhou et al., 2019) (27.22† /1.61×) in terms of both quality and speed. Our model doesn’t heavily rely on extra linguistic knowledge (Akoury et al., 2019), neither requires co"
2020.wmt-1.62,N16-1046,0,0.0374675,"Missing"
2020.wmt-1.62,W17-5701,0,0.0491763,"Missing"
2020.wmt-1.62,P16-1162,1,0.259035,"al probability, for fast sequence generation. IBDecoder retains the training efficiency and is easy to implement. • We extend IBDecoder to enable multi-word simultaneous generation by investigating integration with IMDecoder and SA. Results show that IBDecoder + SA performs better than IMDecoder. • We propose a modified beam search algorithm to support step-wise parallel generation. • On several sequence generation benchmarks, IBDecoder yields ∼2× speedup against Transformer at inference, and reaches 4×–11× after pairing it with SA. Still, the overall generation quality is comparable. 2 2016; Sennrich et al., 2016a; Zhang et al., 2019c,b; Zheng et al., 2019). Unfortunately, these methods either design complex neural decoders, which hurts training efficiency, and/or perform the left-to-right and right-to-left inference separately followed by rescoring, which slows down decoding. By contrast, our model speeds up inference while maintaining training speed. Our work is closely related to SA (Wang et al., 2018) and synchronous bidirectional generation (Zhou et al., 2019). IBDecoder extends SA to incorporate information from different directions. In contrast to Zhou et al. (2019), we only make minimal change"
2020.wmt-1.62,Q19-1002,0,0.0134224,"in parallel are in fact likely to be inter-dependent. We hypothesize that there are groups of words that are less likely to be strongly inter-dependent than neighbouring words, which will allow for better parallelization. Inspired by bidirectional modeling (Zhang et al., 2019b, 2020), we resort to an alternative probabilistic factorization: dn/2e BD p (y|x) = Y −→ ←−−  − p → yt , ← y− t0 |y<t , y>t0 , x , (2) t=1 Introduction Neural sequence generation aided by encoderdecoder models (Bahdanau et al., 2015; Vaswani et al., 2017) has achieved great success in recent years (Bojar et al., 2018; Song et al., 2019; Raffel et al., 2019; Karita et al., 2019), but still suffers from slow inference. One crucial bottleneck 1 Source code is released at https://github.com/ bzhangGo/zero. Introducing an independence assumption between t and t0 = n − t + 1 allows for parallel word pre−−−−−−−→ ←−−−−−−− diction from both the left-to-right and right-to-left directions. Based on this factorization, Zhou et al. (2019) propose synchronous bidirectional translation using a dedicated interactive decoder, and report quality improvements compared to left-toright semi-autoregressive decoding (Wang et al., 2018, SA) in tra"
2020.wmt-1.62,D18-1044,0,0.033297,"Missing"
2020.wmt-1.62,P19-1149,1,0.831445,"19). Related Work Efforts on fast sequence generation come along with the rapid development of encoder-decoder models (Vaswani et al., 2017). A straightforward way is to reduce the amount of computation. Methods in this category range from teacherstudent model (Kim and Rush, 2016; Hayashi et al., 2019), constrained softmax prediction (Hu et al., 2015), beam search cube pruning (Zhang et al., 2018c), float-point quantization (Wu et al., 2016; Bhandare et al., 2019), model pruning (See et al., 2016), to simplified decoder architectures, such as lightweight recurrent models (Zhang et al., 2018b; Zhang and Sennrich, 2019; Kim et al., 2019), average attention network (Zhang et al., 2018a), merged attention network (Zhang et al., 2019a), dynamic convolution (Wu et al., 2019), and hybrid attentions (Shazeer, 2019; Wang et al., 2019), .etc. Nonetheless, the above methods still suffer from the inference bottleneck caused by the sequential nature of autoregressive models. Instead, Gu et al. (2018) propose non-autoregressive generation where target words are predicted independently, leading to great speedup, albeit at a high cost to generation quality. Follow-up studies often seek solutions to recover the performanc"
2020.wmt-1.62,D19-1083,1,0.894205,"Missing"
2020.wmt-1.62,P18-1166,1,0.934783,"bidirectional decoder (IBDecoder) for fast generation. As shown in Figure 1a, we interleave target words from the left-to-right and right-to-left directions and separate their positions to support reusing any standard unidirectional decoders, such as the Transformer decoder (Vaswani et al., 2017). We reorganize the self-attention mask to enable inter- and intra-direction interaction (Figure 1c) following SA. Unlike SA, we show through experiments that distant tokens from different directions are less inter-dependent, providing a guarantee for better performance. Compared to previous studies (Zhang et al., 2018d, 2019b, 2020; Zhou et al., 2019), our approach has no extra model parameters and brings in little overhead at training and decoding. IBDecoder is speedup-bounded at 2×. To push this ceiling up, we explore strategies for multiword simultaneous generation, including multidirectional decoding (IMDecoder, Figure 1d) and SA (Figure 1b). The former extends Eq. 2 by inserting more generation directions, while the latter allows each direction to produce multiple target words (Wang et al., 2018). These strategies offer us a chance to aggressively improve the decoding speed albeit at the risk of degen"
2020.wmt-1.62,D18-1459,1,0.940765,"bidirectional decoder (IBDecoder) for fast generation. As shown in Figure 1a, we interleave target words from the left-to-right and right-to-left directions and separate their positions to support reusing any standard unidirectional decoders, such as the Transformer decoder (Vaswani et al., 2017). We reorganize the self-attention mask to enable inter- and intra-direction interaction (Figure 1c) following SA. Unlike SA, we show through experiments that distant tokens from different directions are less inter-dependent, providing a guarantee for better performance. Compared to previous studies (Zhang et al., 2018d, 2019b, 2020; Zhou et al., 2019), our approach has no extra model parameters and brings in little overhead at training and decoding. IBDecoder is speedup-bounded at 2×. To push this ceiling up, we explore strategies for multiword simultaneous generation, including multidirectional decoding (IMDecoder, Figure 1d) and SA (Figure 1b). The former extends Eq. 2 by inserting more generation directions, while the latter allows each direction to produce multiple target words (Wang et al., 2018). These strategies offer us a chance to aggressively improve the decoding speed albeit at the risk of degen"
2020.wmt-1.62,C02-1050,0,0.216363,"ent (Lee et al., 2018; Stern et al., 2019; Ghazvininejad et al., 2019) or in-between (Kaiser et al., 2018; Akoury et al., 2019). We hypothesize that generation order affects the felicity of independence assumptions made in semiautoregressive modelling. Unlike generation with flexible orders (Emelianenko et al., 2019; Stern et al., 2019; Gu et al., 2019a), we employ deterministic generation order for model simplicity and training efficiency, specifically focusing on bidirectional decoding. The study of bidirectional modeling dates back to the era of phase-based statistical machine translation (Watanabe and Sumita, 2002; Finch and Sumita, 2009) and recently gained popularity in neural machine translation (Liu et al., 3 Autoregressive Transformer Transformer (Vaswani et al., 2017), the state-of-theart neural sequence generation model, follows the autoregressive factorization as in Eq. 1. To handle the dependency of target word yt on previous target words y<t , Transformer relies on a masked selfattention network in the decoder: ! l Kl T Q √ + M Vl (3) ATT(Yl , M) = f d where Ql , Kl , Vl = Wql Yl , Wkl Yl , Wvl Yl ∈ Rn×d , f (·) denotes softmax operation, d is model dimension and l is layer depth. Wq , Wk , W"
2020.wmt-1.62,D18-1460,0,0.28935,"bidirectional decoder (IBDecoder) for fast generation. As shown in Figure 1a, we interleave target words from the left-to-right and right-to-left directions and separate their positions to support reusing any standard unidirectional decoders, such as the Transformer decoder (Vaswani et al., 2017). We reorganize the self-attention mask to enable inter- and intra-direction interaction (Figure 1c) following SA. Unlike SA, we show through experiments that distant tokens from different directions are less inter-dependent, providing a guarantee for better performance. Compared to previous studies (Zhang et al., 2018d, 2019b, 2020; Zhou et al., 2019), our approach has no extra model parameters and brings in little overhead at training and decoding. IBDecoder is speedup-bounded at 2×. To push this ceiling up, we explore strategies for multiword simultaneous generation, including multidirectional decoding (IMDecoder, Figure 1d) and SA (Figure 1b). The former extends Eq. 2 by inserting more generation directions, while the latter allows each direction to produce multiple target words (Wang et al., 2018). These strategies offer us a chance to aggressively improve the decoding speed albeit at the risk of degen"
2020.wmt-1.62,D19-1086,0,0.0159326,"BDecoder retains the training efficiency and is easy to implement. • We extend IBDecoder to enable multi-word simultaneous generation by investigating integration with IMDecoder and SA. Results show that IBDecoder + SA performs better than IMDecoder. • We propose a modified beam search algorithm to support step-wise parallel generation. • On several sequence generation benchmarks, IBDecoder yields ∼2× speedup against Transformer at inference, and reaches 4×–11× after pairing it with SA. Still, the overall generation quality is comparable. 2 2016; Sennrich et al., 2016a; Zhang et al., 2019c,b; Zheng et al., 2019). Unfortunately, these methods either design complex neural decoders, which hurts training efficiency, and/or perform the left-to-right and right-to-left inference separately followed by rescoring, which slows down decoding. By contrast, our model speeds up inference while maintaining training speed. Our work is closely related to SA (Wang et al., 2018) and synchronous bidirectional generation (Zhou et al., 2019). IBDecoder extends SA to incorporate information from different directions. In contrast to Zhou et al. (2019), we only make minimal changes to the standard Transformer decoder, which"
2021.acl-long.200,N18-1118,1,0.930138,"translation cohesion and coherence has long been posited (Hardmeier et al., 2012; Xiong and Zhang, 2013). With the rapid development of neural MT and also available document-level textual datasets, research in this direction gained great popularity. Recent efforts often focus on either advanced contextual neural architecture development (Tiedemann and Scherrer, 2017; Kuang et al., 2018; Miculicich et al., 2018; Zhang et al., 2018, 2020c; Kang et al., 2020; Chen et al., 2020; Ma et al., 2020a; Zheng et al., 2020) and/or improved analysis and evaluation targeted at specific discourse phenomena (Bawden et al., 2018; L¨aubli et al., 2018; Guillou et al., 2018; Voita et al., 2019; Kim et al., 2019; Cai and Xiong, 2020). We follow this research line, and adapt the concatenation-based contextual model (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Lopes et al., 2020) to ST. Our main interest lies in exploring the impact of context on ST. Developing dedicated contextual models for ST is beyond the scope of this study, which we leave to future work. Context-aware ST extends the sentence-level ST towards streaming ST which allows models to access unlimited previous audio inputs. Instead of improving conte"
2021.acl-long.200,2020.iwdp-1.3,0,0.0248615,"). With the rapid development of neural MT and also available document-level textual datasets, research in this direction gained great popularity. Recent efforts often focus on either advanced contextual neural architecture development (Tiedemann and Scherrer, 2017; Kuang et al., 2018; Miculicich et al., 2018; Zhang et al., 2018, 2020c; Kang et al., 2020; Chen et al., 2020; Ma et al., 2020a; Zheng et al., 2020) and/or improved analysis and evaluation targeted at specific discourse phenomena (Bawden et al., 2018; L¨aubli et al., 2018; Guillou et al., 2018; Voita et al., 2019; Kim et al., 2019; Cai and Xiong, 2020). We follow this research line, and adapt the concatenation-based contextual model (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Lopes et al., 2020) to ST. Our main interest lies in exploring the impact of context on ST. Developing dedicated contextual models for ST is beyond the scope of this study, which we leave to future work. Context-aware ST extends the sentence-level ST towards streaming ST which allows models to access unlimited previous audio inputs. Instead of improving contextual modeling, many studies on streaming ST aim at developing better sentence/word segmentation policie"
2021.acl-long.200,2020.autosimtrans-1.5,0,0.0295807,"rk is inspired by pioneer studies on contextaware textual MT. Context beyond the current sentence carries information whose importance for translation cohesion and coherence has long been posited (Hardmeier et al., 2012; Xiong and Zhang, 2013). With the rapid development of neural MT and also available document-level textual datasets, research in this direction gained great popularity. Recent efforts often focus on either advanced contextual neural architecture development (Tiedemann and Scherrer, 2017; Kuang et al., 2018; Miculicich et al., 2018; Zhang et al., 2018, 2020c; Kang et al., 2020; Chen et al., 2020; Ma et al., 2020a; Zheng et al., 2020) and/or improved analysis and evaluation targeted at specific discourse phenomena (Bawden et al., 2018; L¨aubli et al., 2018; Guillou et al., 2018; Voita et al., 2019; Kim et al., 2019; Cai and Xiong, 2020). We follow this research line, and adapt the concatenation-based contextual model (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Lopes et al., 2020) to ST. Our main interest lies in exploring the impact of context on ST. Developing dedicated contextual models for ST is beyond the scope of this study, which we leave to future work. Context-aware ST"
2021.acl-long.200,N19-1202,0,0.0590194,"Missing"
2021.acl-long.200,N13-1073,0,0.0189828,".29 62.08 Table 1: Case-sensitive tokenized BLEU and APT for different models and settings on MuST-C En-De test set. Numbers in bracket denote document-based BLEU. lp: the length penalty for beam search decoding. “w/o Cyn ”: models that are trained without target-side context. Best results are highlighted in bold. Note C = 2, λ = 0.5 and lp = 0.6 by default. erwise specified. We use APT (Miculicich Werlen and Popescu-Belis, 2017), the accuracy of pronoun translation, as an approximate proxy for documentlevel evaluation. Word alignment required by APT is automatically extracted via fast align (Dyer et al., 2013) with the strategy “grow-diag-final-and”. 5.2 Results on MuST-C En-De Does context improve translation? Yes, but the decoding method matters for context-aware ST. Table 1 summarizes the results. Our model with IMED outperforms Baseline by +0.48 BLEU (significant at p &lt; 0.05)6 and +1.79 APT (1→5), clearly showing the benefits from contextual modeling. Although SWBD-Cons yields worse sentencebased BLEU (-0.27, 1→4), it still beats Baseline in document-based BLEU (+0.58) and pronoun translation (+0.17 APT). The reason behind this inferior BLEU partially lies in misaligned translation (see Table 8"
2021.acl-long.200,D14-1140,0,0.0403613,"Missing"
2021.acl-long.200,W18-6435,0,0.0268638,"Missing"
2021.acl-long.200,D12-1108,0,0.0351747,"gs in textual MT (Miculicich et al., 2018; Huo et al., 2020). In addition, context also improves the translation of homophones. • ST models with contexts suffer less from (artificial) audio segmentation errors. • Contextual modeling improves translation quality and reduces latency and flicker for simultaneous translation under re-translation strategy (Arivazhagan et al., 2020a). 2 Related Work Our work is inspired by pioneer studies on contextaware textual MT. Context beyond the current sentence carries information whose importance for translation cohesion and coherence has long been posited (Hardmeier et al., 2012; Xiong and Zhang, 2013). With the rapid development of neural MT and also available document-level textual datasets, research in this direction gained great popularity. Recent efforts often focus on either advanced contextual neural architecture development (Tiedemann and Scherrer, 2017; Kuang et al., 2018; Miculicich et al., 2018; Zhang et al., 2018, 2020c; Kang et al., 2020; Chen et al., 2020; Ma et al., 2020a; Zheng et al., 2020) and/or improved analysis and evaluation targeted at specific discourse phenomena (Bawden et al., 2018; L¨aubli et al., 2018; Guillou et al., 2018; Voita et al., 2"
2021.acl-long.200,2020.wmt-1.71,0,0.109365,"textaware ST model here for both types of translation – that’s why we call it in-model ensemble. We adopt Transformer (Vaswani et al., 2017) for experiments with the MuST-C dataset (Di Gangi et al., 2019). We study the impact of context on translation in different settings. Our results demonstrate the effectiveness of contextual modeling. Our main findings are summarized below: • Incorporating context improves overall translation quality (+0.18-2.61 BLEU) and benefits pronoun translation across different language pairs, resonating with previous findings in textual MT (Miculicich et al., 2018; Huo et al., 2020). In addition, context also improves the translation of homophones. • ST models with contexts suffer less from (artificial) audio segmentation errors. • Contextual modeling improves translation quality and reduces latency and flicker for simultaneous translation under re-translation strategy (Arivazhagan et al., 2020a). 2 Related Work Our work is inspired by pioneer studies on contextaware textual MT. Context beyond the current sentence carries information whose importance for translation cohesion and coherence has long been posited (Hardmeier et al., 2012; Xiong and Zhang, 2013). With the rap"
2021.acl-long.200,2020.emnlp-main.206,0,0.0710008,"Missing"
2021.acl-long.200,2020.emnlp-main.175,0,0.0284592,"Related Work Our work is inspired by pioneer studies on contextaware textual MT. Context beyond the current sentence carries information whose importance for translation cohesion and coherence has long been posited (Hardmeier et al., 2012; Xiong and Zhang, 2013). With the rapid development of neural MT and also available document-level textual datasets, research in this direction gained great popularity. Recent efforts often focus on either advanced contextual neural architecture development (Tiedemann and Scherrer, 2017; Kuang et al., 2018; Miculicich et al., 2018; Zhang et al., 2018, 2020c; Kang et al., 2020; Chen et al., 2020; Ma et al., 2020a; Zheng et al., 2020) and/or improved analysis and evaluation targeted at specific discourse phenomena (Bawden et al., 2018; L¨aubli et al., 2018; Guillou et al., 2018; Voita et al., 2019; Kim et al., 2019; Cai and Xiong, 2020). We follow this research line, and adapt the concatenation-based contextual model (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Lopes et al., 2020) to ST. Our main interest lies in exploring the impact of context on ST. Developing dedicated contextual models for ST is beyond the scope of this study, which we leave to future wor"
2021.acl-long.200,D19-6503,0,0.0268863,"Missing"
2021.acl-long.200,C18-1050,0,0.0644363,"Missing"
2021.acl-long.200,D18-1512,1,0.89201,"Missing"
2021.acl-long.200,2020.eamt-1.24,0,0.178603,"fforts often focus on either advanced contextual neural architecture development (Tiedemann and Scherrer, 2017; Kuang et al., 2018; Miculicich et al., 2018; Zhang et al., 2018, 2020c; Kang et al., 2020; Chen et al., 2020; Ma et al., 2020a; Zheng et al., 2020) and/or improved analysis and evaluation targeted at specific discourse phenomena (Bawden et al., 2018; L¨aubli et al., 2018; Guillou et al., 2018; Voita et al., 2019; Kim et al., 2019; Cai and Xiong, 2020). We follow this research line, and adapt the concatenation-based contextual model (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Lopes et al., 2020) to ST. Our main interest lies in exploring the impact of context on ST. Developing dedicated contextual models for ST is beyond the scope of this study, which we leave to future work. Context-aware ST extends the sentence-level ST towards streaming ST which allows models to access unlimited previous audio inputs. Instead of improving contextual modeling, many studies on streaming ST aim at developing better sentence/word segmentation policies to avoid segmentation errors that greatly hurt translation (Matusov et al., 2007; Rangarajan Sridhar et al., 2013; IranzoS´anchez et al., 2020; Zhang an"
2021.acl-long.200,2020.acl-main.321,0,0.0156967,"ioneer studies on contextaware textual MT. Context beyond the current sentence carries information whose importance for translation cohesion and coherence has long been posited (Hardmeier et al., 2012; Xiong and Zhang, 2013). With the rapid development of neural MT and also available document-level textual datasets, research in this direction gained great popularity. Recent efforts often focus on either advanced contextual neural architecture development (Tiedemann and Scherrer, 2017; Kuang et al., 2018; Miculicich et al., 2018; Zhang et al., 2018, 2020c; Kang et al., 2020; Chen et al., 2020; Ma et al., 2020a; Zheng et al., 2020) and/or improved analysis and evaluation targeted at specific discourse phenomena (Bawden et al., 2018; L¨aubli et al., 2018; Guillou et al., 2018; Voita et al., 2019; Kim et al., 2019; Cai and Xiong, 2020). We follow this research line, and adapt the concatenation-based contextual model (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Lopes et al., 2020) to ST. Our main interest lies in exploring the impact of context on ST. Developing dedicated contextual models for ST is beyond the scope of this study, which we leave to future work. Context-aware ST extends the sent"
2021.acl-long.200,D18-1325,0,0.111413,"that we use the same contextaware ST model here for both types of translation – that’s why we call it in-model ensemble. We adopt Transformer (Vaswani et al., 2017) for experiments with the MuST-C dataset (Di Gangi et al., 2019). We study the impact of context on translation in different settings. Our results demonstrate the effectiveness of contextual modeling. Our main findings are summarized below: • Incorporating context improves overall translation quality (+0.18-2.61 BLEU) and benefits pronoun translation across different language pairs, resonating with previous findings in textual MT (Miculicich et al., 2018; Huo et al., 2020). In addition, context also improves the translation of homophones. • ST models with contexts suffer less from (artificial) audio segmentation errors. • Contextual modeling improves translation quality and reduces latency and flicker for simultaneous translation under re-translation strategy (Arivazhagan et al., 2020a). 2 Related Work Our work is inspired by pioneer studies on contextaware textual MT. Context beyond the current sentence carries information whose importance for translation cohesion and coherence has long been posited (Hardmeier et al., 2012; Xiong and Zhang,"
2021.acl-long.200,W17-4802,0,0.0152439,") 22.97 (28.29) 22.94 (28.11) 61.89 63.51 62.76 11 12 3 w/o Cyn 5 w/o Cyn 21.12 (26.17) 20.72 (25.43) 59.51 58.18 13 14 3 w/o Baseline Initial. 5 w/o Baseline Initial. 21.75 (27.15) 21.97 (27.20) 62.29 62.08 Table 1: Case-sensitive tokenized BLEU and APT for different models and settings on MuST-C En-De test set. Numbers in bracket denote document-based BLEU. lp: the length penalty for beam search decoding. “w/o Cyn ”: models that are trained without target-side context. Best results are highlighted in bold. Note C = 2, λ = 0.5 and lp = 0.6 by default. erwise specified. We use APT (Miculicich Werlen and Popescu-Belis, 2017), the accuracy of pronoun translation, as an approximate proxy for documentlevel evaluation. Word alignment required by APT is automatically extracted via fast align (Dyer et al., 2013) with the strategy “grow-diag-final-and”. 5.2 Results on MuST-C En-De Does context improve translation? Yes, but the decoding method matters for context-aware ST. Table 1 summarizes the results. Our model with IMED outperforms Baseline by +0.48 BLEU (significant at p &lt; 0.05)6 and +1.79 APT (1→5), clearly showing the benefits from contextual modeling. Although SWBD-Cons yields worse sentencebased BLEU (-0.27, 1→4"
2021.acl-long.200,P02-1040,0,0.109788,"Missing"
2021.acl-long.200,W18-6319,0,0.0237995,"Missing"
2021.acl-long.200,N13-1023,0,0.194178,"nd Scherrer, 2017; Bawden et al., 2018; Lopes et al., 2020) to ST. Our main interest lies in exploring the impact of context on ST. Developing dedicated contextual models for ST is beyond the scope of this study, which we leave to future work. Context-aware ST extends the sentence-level ST towards streaming ST which allows models to access unlimited previous audio inputs. Instead of improving contextual modeling, many studies on streaming ST aim at developing better sentence/word segmentation policies to avoid segmentation errors that greatly hurt translation (Matusov et al., 2007; Rangarajan Sridhar et al., 2013; IranzoS´anchez et al., 2020; Zhang and Zhang, 2020; Arivazhagan et al., 2020b). Very recently, Ma et al. (2020b) proposed a memory augmented Transformer encoder for streaming ST, where the previous audio features are summarized into a growing continuous memory to improve the model’s context awareness. Despite its success, this method ignores the target-side context, which turns out to have significant positive impact on ST in our experiments. Our study still relies on oracle sentence segmentation of the audio. The most related work to ours is (Gaido et al., 2020), which also investigated con"
2021.acl-long.200,W17-4702,1,0.848263,"re ST. yn denotes the n-th target sentence in a document; xn denotes the speech encodings extracted from the n-th audio segment. We use dashed gray box to indicate the concatenation operation. “&lt;s>”: sentence separator symbol. Document-level context often offers extra informative clues that could improve the understanding of individual sentences. Such clues have been proven effective for textual machine translation (MT), particularly in handling translation errors specific to discourse phenomena, such as inaccurate coreference of pronouns (Guillou, 2016) and mistranslation of ambiguous words (Rios et al., 2017). Besides, ensuring consistency in translation is virtually impossible without document-level context as well (Voita et al., 2019). Analogous to MT, speech translation (ST) also suffers from these translation issues, and super-sentential context could in fact be more valuable to ST because 1) homophones and acoustic noise bring additional ambiguity to ST, and 2) a common use case in ST is simultaneous translation, where the system has to output translations of sentence fragments, and may have to predict future input to account for word order differences between the source and target language ("
2021.acl-long.200,P16-1162,1,0.404175,"Missing"
2021.acl-long.200,W17-4811,0,0.0254226,"and flicker for simultaneous translation under re-translation strategy (Arivazhagan et al., 2020a). 2 Related Work Our work is inspired by pioneer studies on contextaware textual MT. Context beyond the current sentence carries information whose importance for translation cohesion and coherence has long been posited (Hardmeier et al., 2012; Xiong and Zhang, 2013). With the rapid development of neural MT and also available document-level textual datasets, research in this direction gained great popularity. Recent efforts often focus on either advanced contextual neural architecture development (Tiedemann and Scherrer, 2017; Kuang et al., 2018; Miculicich et al., 2018; Zhang et al., 2018, 2020c; Kang et al., 2020; Chen et al., 2020; Ma et al., 2020a; Zheng et al., 2020) and/or improved analysis and evaluation targeted at specific discourse phenomena (Bawden et al., 2018; L¨aubli et al., 2018; Guillou et al., 2018; Voita et al., 2019; Kim et al., 2019; Cai and Xiong, 2020). We follow this research line, and adapt the concatenation-based contextual model (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Lopes et al., 2020) to ST. Our main interest lies in exploring the impact of context on ST. Developing dedicat"
2021.acl-long.200,P19-1116,1,0.896393,"Missing"
2021.acl-long.200,P18-1117,1,0.841621,"wer this question by studying the impact of incorrect context on our model. We replace the correct source context with some random audio segments from the same document, and randomly select the target context from previous translations during decoding. Intuitively, the performance of our model should be intact if it ignores the context. Note that we trained our model with correct contexts but test it with random contexts here. Results in Table 2 show that the randomized context, either source- or target-side, hurts the performance of our model in both BLEU and APT, similar to the findings in (Voita et al., 2018), and the translation of pronouns suffers more (> -1.6 APT). Compared to SWBD, the incorrect context has more negative impact on IMED, resulting in worse performance than Baseline (Table 1), although IMED also uses sentence-level translation. We ascribe this to the target prefix constraint in IMED which makes translation errors at early decoding much easier to propagate. We observe that the incorrect target context acts similarly to its source counterpart under IMED, albeit its selection scope is much smaller (only limited to the translated segments), and combining both contexts leads to a sli"
2021.acl-long.200,2020.acl-main.344,0,0.0208695,"ty to ST, and 2) a common use case in ST is simultaneous translation, where the system has to output translations of sentence fragments, and may have to predict future input to account for word order differences between the source and target language (Grissom II et al., 2014). Both for ambiguity from the acoustic signal, and operating on small sentence fragments, we hypothesize that access to extra context2 will be beneficial. Although recent studies on ST have achieved promising results with end-to-end (E2E) models (Anastasopoulos and Chiang, 2018; Di Gangi et al., 2019; Zhang et al., 2020a; Wang et al., 2020; Dong et al., 2020), nevertheless, they mainly focus on sentence-level translation. One practical challenge when scaling up sentence-level E2E ST to the document-level is the encoding of very long audio segments, which can easily hit the computational bottleneck, especially with Transformers (Vaswani et al., 2017). So far, the research question of whether and how contextual information benefits E2E ST has received little attention. In this paper, we answer this question through extensive experiments by exploring a concatenation1 Source code is available at https://github.com/ bzhangGo/zero. 2"
2021.acl-long.200,2020.findings-emnlp.230,1,0.889467,"ng additional ambiguity to ST, and 2) a common use case in ST is simultaneous translation, where the system has to output translations of sentence fragments, and may have to predict future input to account for word order differences between the source and target language (Grissom II et al., 2014). Both for ambiguity from the acoustic signal, and operating on small sentence fragments, we hypothesize that access to extra context2 will be beneficial. Although recent studies on ST have achieved promising results with end-to-end (E2E) models (Anastasopoulos and Chiang, 2018; Di Gangi et al., 2019; Zhang et al., 2020a; Wang et al., 2020; Dong et al., 2020), nevertheless, they mainly focus on sentence-level translation. One practical challenge when scaling up sentence-level E2E ST to the document-level is the encoding of very long audio segments, which can easily hit the computational bottleneck, especially with Transformers (Vaswani et al., 2017). So far, the research question of whether and how contextual information benefits E2E ST has received little attention. In this paper, we answer this question through extensive experiments by exploring a concatenation1 Source code is available at https://github.c"
2021.acl-long.200,2021.findings-acl.255,1,0.835314,"Missing"
2021.acl-long.200,D18-1049,0,0.0359043,"Missing"
2021.acl-long.200,2020.emnlp-main.81,0,0.0507902,"Missing"
2021.acl-long.200,2020.autosimtrans-1.1,0,0.196645,"., 2020) to ST. Our main interest lies in exploring the impact of context on ST. Developing dedicated contextual models for ST is beyond the scope of this study, which we leave to future work. Context-aware ST extends the sentence-level ST towards streaming ST which allows models to access unlimited previous audio inputs. Instead of improving contextual modeling, many studies on streaming ST aim at developing better sentence/word segmentation policies to avoid segmentation errors that greatly hurt translation (Matusov et al., 2007; Rangarajan Sridhar et al., 2013; IranzoS´anchez et al., 2020; Zhang and Zhang, 2020; Arivazhagan et al., 2020b). Very recently, Ma et al. (2020b) proposed a memory augmented Transformer encoder for streaming ST, where the previous audio features are summarized into a growing continuous memory to improve the model’s context awareness. Despite its success, this method ignores the target-side context, which turns out to have significant positive impact on ST in our experiments. Our study still relies on oracle sentence segmentation of the audio. The most related work to ours is (Gaido et al., 2020), which also investigated contextualized translation and showed that contextaware"
2021.acl-long.200,2021.acl-demo.7,0,0.0628604,"Missing"
2021.acl-long.258,P18-2003,0,0.0284265,"ist fundamental differences between cognitive and connectionist architectures that makes compositional generalization by the latter unlikely (Fodor and Pylyshyn, 1988). However, recent work has shown these models’ capacity for learning some syntactic properties. Hupkes et al. (2018) show how some architectures can handle hierarchy in an algebraic context and generalize in a limited way to unseen depths and lengths. Work looking at the latent representations learned by deep machine translation systems show how these models seem to extract constituency and syntactic class information from data (Blevins et al., 2018; Belinkov et al., 2018). These results, and the more general fact that neural models perform a variety of NLP tasks with high fidelity (eg. Vaswani et al., 2017; Dong and Lapata, 2016), suggest these models have some sensitivity to syntactic structure and by extension may be able to learn to generalize compositionally. Recently there have been a number of datasets designed to more formally assess connectionist models’ aptitude for compositional generalization (Kim and Linzen, 2020; Lake and Baroni, 2018; Hupkes et al., 2019). These datasets frame the problem of compositional generalization as"
2021.acl-long.258,2020.acl-main.407,0,0.0284757,"2020). Another line of work utilizes data augmentation via fixed rules (Andreas, 2020) or a learned network (Akyürek et al., 2020) in an effort to transform the out-of-distribution compositional generalization task into an in-distribution one. Our work follows an orthogonal direction, injecting compositional bias using a specialized training algorithm. A related area of research looks at the emergence of compositional languages, often showing that languages which seem to lack natural-language like compositional structure may still be able to generalize to novel concepts (Kottur et al., 2017; Chaabouni et al., 2020). This may help to explain the ways in which models can generalize robustly on in-distribution data unseen during training while still struggling on tasks specifically targeting compositionality. 6 Conclusion Our work highlights the importance of training objectives that select for robust generalization strategies. The meta-learning augmented approach to supervised learning used here allows for the specification of different constraints on learning through the design of the meta-tasks. Our similarity-driven task design improved on baseline performance on two different compositional generalizat"
2021.acl-long.258,P16-1002,0,0.051892,"ee-MAML to COGS; Lev-MAML and StrMAML make use of the natural language sentences while Tree-MAML uses the dependency structures reconstructed from the logical forms. 3.2 Baselines In general, our method is model-agnostic and can be coupled with any semantic parser to improve its compositional generalization. Additionally LevMAML, and Str-MAML are dataset agnostic provided the dataset has a natural language input. In this work, we apply our methods on two widely used sequence-to-sequences models. 4 LSTM-based Seq2Seq has been the backbone of many neural semantic parsers (Dong and Lapata, 2016; Jia and Liang, 2016). It utilizes Transformer-based Seq2Seq also follows the encoder-decoder framework, but it uses Transformers (Vaswani et al., 2017) to replace the LSTM for encoding and decoding. It has proved successful in many NLP tasks e.g., machine translation. Recently, it has been adapted for semantic parsing (Wang et al., 2020b) with superior performance. We try to see whether our MAML training can improve the compositional generalization of contemporary semantic parsers, compared with standard supervised learning. Moreover, we include a meta-baseline, referred to as Uni-MAML, that constructs meta-train"
2021.acl-long.258,P19-1589,0,0.0224501,"aset agnostic as well requiring only that the input be a natural language sentence. Our methods are simple to implement compared with other approaches to improving compositional generalization, and we look forward to their use in combination with other techniques to further improve models’ compositional ability. Acknowledgements Meta-Learning for NLP Meta-learning methods (Vinyals et al., 2016; Ravi and Larochelle, 2016; Finn et al., 2017b) that are widely used for few-shot learning, have been adapted for NLP applications like machine translation (Gu et al., 2018) and relation classification (Obamuyide and Vlachos, 2019). In this work, we extend the conventional MAML (Finn et al., 2017b) algorithm, which was initially proposed for few-shot learning, as a tool to inject inductive bias, inspired by Li et al. (2018); Wang et al. (2020a). For compositional generalization, Lake (2019) proposes a meta-learning procedure to train a memory-augmented neural model. However, its meta-learning algorithm is specialized for the SCAN dataset (Lake and Baroni, 2018) and not suitable to more realistic datasets. This work was supported in part by the UKRI Centre for Doctoral Training in Natural Language Processing, funded by t"
2021.acl-long.258,D19-1438,0,0.138375,"Missing"
2021.acl-long.258,D15-1166,0,0.029311,"Missing"
2021.acl-long.258,2021.naacl-main.33,1,0.82693,"Missing"
2021.acl-long.258,2020.acl-main.677,1,0.248884,"there may be a large number of generalization strategies that are compatible with the data, only some of which will perform well outside of training (D’Amour et al., 2020). It seems connectionist models do not reliably extract the strategies from their training data that generalize well outside of the training distribution. Here we focus on an approach that tries to to introduce a bias during training such that the model arrives at a more robust strategy. To do this we implement a variant of the model agnostic meta-learning algorithm (MAML, Finn et al., 2017a). The approach used here follows Wang et al. (2020a) which implements an objective function that explicitly optimizes for out-ofdistribution generalization in line with Li et al. (2018). Wang et al. (2020a) creates pairs of tasks for each batch (which here we call meta-train and meta-test) by sub-sampling the existing training data. Each meta-train, meta-test task pair is designed to simulate the divergence between training and testing: meta-train is designed to resemble the training distribution, and meta-test to resemble the test distribution. The training objective then requires that update steps taken on meta-train are also beneficial for"
2021.acl-long.91,P17-1080,0,0.0267114,"utions when conditioning on different types of prefixes, when varying the training objective or the amount of training data, and during the training process. We find that models trained with more data tend to rely on source information more and to have more sharp token contributions; the training process is non-monotonic with several stages of different nature.1 1 Introduction With the success of neural approaches to natural language processing, analysis of NLP models has become an important and active topic of research. In NMT, approaches to analysis include probing for linguistic structure (Belinkov et al., 2017; Conneau et al., 2018), evaluating via contrastive translation pairs (Sennrich, 2017; Burlot and Yvon, 2017; Rios Gonzales et al., 2017; Tang 1 We release the code at https://github.com/ lena-voita/the-story-of-heads. et al., 2018), inspecting model components, such as attention (Ghader and Monz, 2017; Voita et al., 2018; Tang et al., 2018; Raganato and Tiedemann, 2018; Voita et al., 2019) or neurons (Dalvi et al., 2019; Bau et al., 2019), among others. Unfortunately, although a lot of work on model analysis has been done, a question of how the NMT predictions are formed remains largely open."
2021.acl-long.91,W19-5361,0,0.0226197,"rol the influence of source and target leads to improvement for both RNN (Tu et al., 2017; Wang et al., 2018) and Transfomer (Li et al., 2020) models. A more popular example is a model’s tendency to generate hallucinations (fluent but inadequate translations); it is usually attributed to the inappropriately strong influence of target context. Several works observed that, when hallucinating, a model fails to properly use source: it produces a deficient attention matrix, where almost all the probability mass is concentrated on uninformative source tokens (EOS and punctuation) (Lee et al., 2018; Berard et al., 2019). We argue that a natural way to estimate how the source and target contexts contribute to generation is to apply Layerwise Relevance Propagation (LRP) (Bach et al., 2015) to NMT models. LRP redistributes the information used for a prediction between all input elements keeping the total contribution constant. This ‘conservation principle’ makes relevance propagation unique: differently from other methods estimating influence of individual tokens (Alvarez-Melis and Jaakkola, 2017; He 1126 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th Intern"
2021.acl-long.91,P17-1106,0,0.080576,"yer l by summing all messages from neurons at layer (l + 1): X (l,l+1) (l) Ri = Ri←j . (3) j Layer-wise relevance propagation is a framework which decomposes the prediction of a deep neural network computed over an instance, e.g. an image or sentence, into relevance scores for single input dimensions of the sample such as subpixels of an image or neurons of input token embeddings. The Equations (2) and (3) define the propagation of relevance from layer l+1 to layer l. The only thing that is missing is specific formulas for computing the 2 Previous work applying one of the LRP variants to NMT (Ding et al., 2017; Voita et al., 2019) do not describe extensions beyond the original LRP rules (Bach et al., 2015). 1127 (l,l+1) (l,l+1) messages Ri←j . Usually, the message Ri←j has the following structure: (l,l+1) Ri←j (l+1) = vij Rj , X vij = 1. (4) i Several versions of LRP satisfying equation (4) (and, therefore, the conservation principle) have been introduced: LRP-ε, LRP-αβ and LRPγ (Bach et al., 2015; Binder et al., 2016; Montavon et al., 2019). We use LRP-αβ (Bach et al., 2015; Binder et al., 2016), which defines relevances at each step in such a way that they are positive. Rule for relevance propaga"
2021.acl-long.91,N18-1033,0,0.0334514,"Missing"
2021.acl-long.91,I17-1004,0,0.0187699,"ss is non-monotonic with several stages of different nature.1 1 Introduction With the success of neural approaches to natural language processing, analysis of NLP models has become an important and active topic of research. In NMT, approaches to analysis include probing for linguistic structure (Belinkov et al., 2017; Conneau et al., 2018), evaluating via contrastive translation pairs (Sennrich, 2017; Burlot and Yvon, 2017; Rios Gonzales et al., 2017; Tang 1 We release the code at https://github.com/ lena-voita/the-story-of-heads. et al., 2018), inspecting model components, such as attention (Ghader and Monz, 2017; Voita et al., 2018; Tang et al., 2018; Raganato and Tiedemann, 2018; Voita et al., 2019) or neurons (Dalvi et al., 2019; Bau et al., 2019), among others. Unfortunately, although a lot of work on model analysis has been done, a question of how the NMT predictions are formed remains largely open. Namely, the generation of a target token is defined by two types of context, source and target, but there is no method which explicitly evaluates the relative contribution of source and target to a given prediction. The ability to measure this relative contribution is important for model understanding"
2021.acl-long.91,D19-1088,0,0.0815687,"for the model 6 Random prefixes come from the same evaluation set, but with shuffled target sentences. 1130 to rely on the source more and to be more confident when choosing relevant source tokens. Reference vs random prefixes. Results for random sentence prefixes are given in Figures 3c, 3d. The reaction to random prefixes helps us study the self-recovery ability of NMT models. Previous work has found that models can fall into a hallucination mode where “the decoder ignores context from the encoder and samples from its language mode” (Koehn and Knowles, 2017; Lee et al., 2018). In contrast, He et al. (2019b) found that a language model is able to recover from artificially distorted history input and generate reasonable samples. Our results show evidence for both. At the beginning of the generation process, the model tends to rely more on the source context when given a random prefix compared to the reference prefix, indicating a self-recovery mode. However, when the prefix becomes longer, the model choice shifts towards ignoring the source and relying more on the target: Figure 3c shows a large drop of source influence for later positions. Figure 3d also shows that with a random prefix, the ent"
2021.acl-long.91,kocmi-bojar-2017-curriculum,0,0.0180957,"that earlier positions are the ones that change most actively: at these positions, we see the largest decrease at the first stage and the largest following increase at the subsequent stages. If we look at how accuracy for each position changes in training (Figure 10), we see that at the end of the first stage, early tokens have the highest accuracy.7 This is not surprising: one could expect early positions to train faster because they are observed more frequently in training. Previously such intuition motivated the usage of sentence length as one of the criteria for curriculum learning (e.g., Kocmi and Bojar (2017)). 7.1 Relation to Previous Work Interestingly, our stages in Figure 7 agree with the ones found by Frankle et al. (2020) for ResNet-20 trained on CIFAR-10 when investigating, among other things, the lottery ticket hypothesis (Frankle and Carbin, 2019). Their stages were defined based on the changes in gradient magnitude, in the weight space, in the performance, and in the effectiveness of rewinding in search of the ‘winning’ subnetwork (for more details on the lottery ticket hypothesis 7 Accuracy is the proportion of cases where the correct token is the most probable choice. 1133 not using so"
2021.acl-long.91,W17-3204,0,0.0232433,"experiments, these simpler model-generated prefixes allow for the model 6 Random prefixes come from the same evaluation set, but with shuffled target sentences. 1130 to rely on the source more and to be more confident when choosing relevant source tokens. Reference vs random prefixes. Results for random sentence prefixes are given in Figures 3c, 3d. The reaction to random prefixes helps us study the self-recovery ability of NMT models. Previous work has found that models can fall into a hallucination mode where “the decoder ignores context from the encoder and samples from its language mode” (Koehn and Knowles, 2017; Lee et al., 2018). In contrast, He et al. (2019b) found that a language model is able to recover from artificially distorted history input and generate reasonable samples. Our results show evidence for both. At the beginning of the generation process, the model tends to rely more on the source context when given a random prefix compared to the reference prefix, indicating a self-recovery mode. However, when the prefix becomes longer, the model choice shifts towards ignoring the source and relying more on the target: Figure 3c shows a large drop of source influence for later positions. Figure"
2021.acl-long.91,2020.acl-main.757,0,0.0321549,"he generation of a target token is defined by two types of context, source and target, but there is no method which explicitly evaluates the relative contribution of source and target to a given prediction. The ability to measure this relative contribution is important for model understanding since previous work showed that NMT models often fail to effectively control information flow from source and target contexts. For example, adding context gates to dynamically control the influence of source and target leads to improvement for both RNN (Tu et al., 2017; Wang et al., 2018) and Transfomer (Li et al., 2020) models. A more popular example is a model’s tendency to generate hallucinations (fluent but inadequate translations); it is usually attributed to the inappropriately strong influence of target context. Several works observed that, when hallucinating, a model fails to properly use source: it produces a deficient attention matrix, where almost all the probability mass is concentrated on uninformative source tokens (EOS and punctuation) (Lee et al., 2018; Berard et al., 2019). We argue that a natural way to estimate how the source and target contexts contribute to generation is to apply Layerwis"
2021.acl-long.91,W18-5431,0,0.036824,"1 Introduction With the success of neural approaches to natural language processing, analysis of NLP models has become an important and active topic of research. In NMT, approaches to analysis include probing for linguistic structure (Belinkov et al., 2017; Conneau et al., 2018), evaluating via contrastive translation pairs (Sennrich, 2017; Burlot and Yvon, 2017; Rios Gonzales et al., 2017; Tang 1 We release the code at https://github.com/ lena-voita/the-story-of-heads. et al., 2018), inspecting model components, such as attention (Ghader and Monz, 2017; Voita et al., 2018; Tang et al., 2018; Raganato and Tiedemann, 2018; Voita et al., 2019) or neurons (Dalvi et al., 2019; Bau et al., 2019), among others. Unfortunately, although a lot of work on model analysis has been done, a question of how the NMT predictions are formed remains largely open. Namely, the generation of a target token is defined by two types of context, source and target, but there is no method which explicitly evaluates the relative contribution of source and target to a given prediction. The ability to measure this relative contribution is important for model understanding since previous work showed that NMT models often fail to effectively"
2021.acl-long.91,W17-4702,1,0.826388,"the training process. We find that models trained with more data tend to rely on source information more and to have more sharp token contributions; the training process is non-monotonic with several stages of different nature.1 1 Introduction With the success of neural approaches to natural language processing, analysis of NLP models has become an important and active topic of research. In NMT, approaches to analysis include probing for linguistic structure (Belinkov et al., 2017; Conneau et al., 2018), evaluating via contrastive translation pairs (Sennrich, 2017; Burlot and Yvon, 2017; Rios Gonzales et al., 2017; Tang 1 We release the code at https://github.com/ lena-voita/the-story-of-heads. et al., 2018), inspecting model components, such as attention (Ghader and Monz, 2017; Voita et al., 2018; Tang et al., 2018; Raganato and Tiedemann, 2018; Voita et al., 2019) or neurons (Dalvi et al., 2019; Bau et al., 2019), among others. Unfortunately, although a lot of work on model analysis has been done, a question of how the NMT predictions are formed remains largely open. Namely, the generation of a target token is defined by two types of context, source and target, but there is no method which explicitly"
2021.acl-long.91,E17-2060,1,0.800681,"or the amount of training data, and during the training process. We find that models trained with more data tend to rely on source information more and to have more sharp token contributions; the training process is non-monotonic with several stages of different nature.1 1 Introduction With the success of neural approaches to natural language processing, analysis of NLP models has become an important and active topic of research. In NMT, approaches to analysis include probing for linguistic structure (Belinkov et al., 2017; Conneau et al., 2018), evaluating via contrastive translation pairs (Sennrich, 2017; Burlot and Yvon, 2017; Rios Gonzales et al., 2017; Tang 1 We release the code at https://github.com/ lena-voita/the-story-of-heads. et al., 2018), inspecting model components, such as attention (Ghader and Monz, 2017; Voita et al., 2018; Tang et al., 2018; Raganato and Tiedemann, 2018; Voita et al., 2019) or neurons (Dalvi et al., 2019; Bau et al., 2019), among others. Unfortunately, although a lot of work on model analysis has been done, a question of how the NMT predictions are formed remains largely open. Namely, the generation of a target token is defined by two types of context, source"
2021.acl-long.91,P16-1162,1,0.434072,"Missing"
2021.acl-long.91,P16-1159,0,0.0171603,"get history, and show that Minimum Risk Training (MRT), which does not suffer from exposure bias, reduces hallucinations. However, they did not directly measure this overreliance on target history. Our method is able to directly test whether there is indeed an over-reliance on the target history with MLE-trained models, and more robust inclusion of source context with MRT. We also consider a simpler heuristic, word dropout, which we hypothesize to have a similar effect. (a) (b) Figure 4: Contribution of source (a) and entropy of source (b) with model-generated prefixes. Minimum Risk Training (Shen et al., 2016) is a sentence-level objective that inherently avoids exposure bias. It minimises the expected loss (‘risk’) with respect to the posterior distribution: X X R(θ) = P (˜ y |x, θ)∆(˜ y , y), (x,y) y˜∈Y(x) where Y(x) is a set of candidate translations for x, ∆(˜ y , y) is the discrepancy between the model prediction y˜ and the gold translation y (e.g., a negative smoothed sentence-level BLEU). More details on the method can be found in Shen et al. (2016) or Edunov et al. (2018); training details for our models are in the appendix. Word Dropout is a simple data augmentation technique. During train"
2021.acl-long.91,W11-2102,0,0.0996528,"anguage.6 As in previous experiments, we evaluate relevance for top-1 logit predicted by the model. Reference vs model prefixes. When feeding model-generated prefixes, the model uses source more (Figure 3(a)) and has more focused source contributions (lower entropy in Figure 3(b)) than when generating the reference. This may be because model-generated translations are ‘easier’ than references. For example, beam search translations contain fewer rare tokens (Burlot and Yvon, 2018; Ott et al., 2018), are simpler syntactically (Burlot and Yvon, 2018) and, according to the fuzzy reordering score (Talbot et al., 2011), model translations have significantly less reordering compared to the real parallel sentences (Zhou et al., 2020). As we see from our experiments, these simpler model-generated prefixes allow for the model 6 Random prefixes come from the same evaluation set, but with shuffled target sentences. 1130 to rely on the source more and to be more confident when choosing relevant source tokens. Reference vs random prefixes. Results for random sentence prefixes are given in Figures 3c, 3d. The reaction to random prefixes helps us study the self-recovery ability of NMT models. Previous work has found"
2021.acl-long.91,D18-1458,1,0.849335,"different nature.1 1 Introduction With the success of neural approaches to natural language processing, analysis of NLP models has become an important and active topic of research. In NMT, approaches to analysis include probing for linguistic structure (Belinkov et al., 2017; Conneau et al., 2018), evaluating via contrastive translation pairs (Sennrich, 2017; Burlot and Yvon, 2017; Rios Gonzales et al., 2017; Tang 1 We release the code at https://github.com/ lena-voita/the-story-of-heads. et al., 2018), inspecting model components, such as attention (Ghader and Monz, 2017; Voita et al., 2018; Tang et al., 2018; Raganato and Tiedemann, 2018; Voita et al., 2019) or neurons (Dalvi et al., 2019; Bau et al., 2019), among others. Unfortunately, although a lot of work on model analysis has been done, a question of how the NMT predictions are formed remains largely open. Namely, the generation of a target token is defined by two types of context, source and target, but there is no method which explicitly evaluates the relative contribution of source and target to a given prediction. The ability to measure this relative contribution is important for model understanding since previous work showed that NMT mo"
2021.acl-long.91,W18-6304,1,0.8452,"different nature.1 1 Introduction With the success of neural approaches to natural language processing, analysis of NLP models has become an important and active topic of research. In NMT, approaches to analysis include probing for linguistic structure (Belinkov et al., 2017; Conneau et al., 2018), evaluating via contrastive translation pairs (Sennrich, 2017; Burlot and Yvon, 2017; Rios Gonzales et al., 2017; Tang 1 We release the code at https://github.com/ lena-voita/the-story-of-heads. et al., 2018), inspecting model components, such as attention (Ghader and Monz, 2017; Voita et al., 2018; Tang et al., 2018; Raganato and Tiedemann, 2018; Voita et al., 2019) or neurons (Dalvi et al., 2019; Bau et al., 2019), among others. Unfortunately, although a lot of work on model analysis has been done, a question of how the NMT predictions are formed remains largely open. Namely, the generation of a target token is defined by two types of context, source and target, but there is no method which explicitly evaluates the relative contribution of source and target to a given prediction. The ability to measure this relative contribution is important for model understanding since previous work showed that NMT mo"
2021.acl-long.91,Q17-1007,0,0.0170471,"edictions are formed remains largely open. Namely, the generation of a target token is defined by two types of context, source and target, but there is no method which explicitly evaluates the relative contribution of source and target to a given prediction. The ability to measure this relative contribution is important for model understanding since previous work showed that NMT models often fail to effectively control information flow from source and target contexts. For example, adding context gates to dynamically control the influence of source and target leads to improvement for both RNN (Tu et al., 2017; Wang et al., 2018) and Transfomer (Li et al., 2020) models. A more popular example is a model’s tendency to generate hallucinations (fluent but inadequate translations); it is usually attributed to the inappropriately strong influence of target context. Several works observed that, when hallucinating, a model fails to properly use source: it produces a deficient attention matrix, where almost all the probability mass is concentrated on uninformative source tokens (EOS and punctuation) (Lee et al., 2018; Berard et al., 2019). We argue that a natural way to estimate how the source and target c"
2021.acl-long.91,P18-1117,1,0.828772,"h several stages of different nature.1 1 Introduction With the success of neural approaches to natural language processing, analysis of NLP models has become an important and active topic of research. In NMT, approaches to analysis include probing for linguistic structure (Belinkov et al., 2017; Conneau et al., 2018), evaluating via contrastive translation pairs (Sennrich, 2017; Burlot and Yvon, 2017; Rios Gonzales et al., 2017; Tang 1 We release the code at https://github.com/ lena-voita/the-story-of-heads. et al., 2018), inspecting model components, such as attention (Ghader and Monz, 2017; Voita et al., 2018; Tang et al., 2018; Raganato and Tiedemann, 2018; Voita et al., 2019) or neurons (Dalvi et al., 2019; Bau et al., 2019), among others. Unfortunately, although a lot of work on model analysis has been done, a question of how the NMT predictions are formed remains largely open. Namely, the generation of a target token is defined by two types of context, source and target, but there is no method which explicitly evaluates the relative contribution of source and target to a given prediction. The ability to measure this relative contribution is important for model understanding since previous work"
2021.acl-long.91,P19-1580,1,0.901558,"s of neural approaches to natural language processing, analysis of NLP models has become an important and active topic of research. In NMT, approaches to analysis include probing for linguistic structure (Belinkov et al., 2017; Conneau et al., 2018), evaluating via contrastive translation pairs (Sennrich, 2017; Burlot and Yvon, 2017; Rios Gonzales et al., 2017; Tang 1 We release the code at https://github.com/ lena-voita/the-story-of-heads. et al., 2018), inspecting model components, such as attention (Ghader and Monz, 2017; Voita et al., 2018; Tang et al., 2018; Raganato and Tiedemann, 2018; Voita et al., 2019) or neurons (Dalvi et al., 2019; Bau et al., 2019), among others. Unfortunately, although a lot of work on model analysis has been done, a question of how the NMT predictions are formed remains largely open. Namely, the generation of a target token is defined by two types of context, source and target, but there is no method which explicitly evaluates the relative contribution of source and target to a given prediction. The ability to measure this relative contribution is important for model understanding since previous work showed that NMT models often fail to effectively control information"
2021.acl-long.91,2020.acl-main.326,1,0.919069,"longer, the model choice shifts towards ignoring the source and relying more on the target: Figure 3c shows a large drop of source influence for later positions. Figure 3d also shows that with a random prefix, the entropy of source contributions is high and is roughly constant. 5 Exposure Bias and Source Contributions The results in the previous section agree with some observations made in previous work studying selfrecovery and hallucinations. In this section, we illustrate more explicitly how our methodology can be used to shed light on the effects of exposure bias and training objectives. Wang and Sennrich (2020) empirically link the hallucination mode to exposure bias (Ranzato et al., 2016), i.e. the mismatch between the gold history seen at training time, and the (potentially erroneous) model-generated prefixes at test time. The authors hypothesize that exposure bias leads to an over-reliance on target history, and show that Minimum Risk Training (MRT), which does not suffer from exposure bias, reduces hallucinations. However, they did not directly measure this overreliance on target history. Our method is able to directly test whether there is indeed an over-reliance on the target history with MLE-"
2021.acl-long.91,C18-1124,0,0.0223897,"med remains largely open. Namely, the generation of a target token is defined by two types of context, source and target, but there is no method which explicitly evaluates the relative contribution of source and target to a given prediction. The ability to measure this relative contribution is important for model understanding since previous work showed that NMT models often fail to effectively control information flow from source and target contexts. For example, adding context gates to dynamically control the influence of source and target leads to improvement for both RNN (Tu et al., 2017; Wang et al., 2018) and Transfomer (Li et al., 2020) models. A more popular example is a model’s tendency to generate hallucinations (fluent but inadequate translations); it is usually attributed to the inappropriately strong influence of target context. Several works observed that, when hallucinating, a model fails to properly use source: it produces a deficient attention matrix, where almost all the probability mass is concentrated on uninformative source tokens (EOS and punctuation) (Lee et al., 2018; Berard et al., 2019). We argue that a natural way to estimate how the source and target contexts contribute t"
2021.adaptnlp-1.17,P19-1228,0,0.181737,"Missing"
2021.adaptnlp-1.17,H94-1020,0,0.240069,"approximated by: Z ∗ t ≈ argmax pθ (t|w, z)δ(z − µφ (w)) dz , z where δ(·) is the Dirac delta function and µφ (w) is the mean vector of the variational posterior. Similarly to C-PCFGs, neural PCFGs (NPCFGs) also use neural networks to parameterize PCFGs, but their parameterization does not rely on the sentence-dependent z. In the following discussion, we will refer to z as ‘sentence embedding’. 3 Experimental setup Datasets: We investigate the parsing performance of C-PCFGs across ten languages. Specifically, we conduct experiments on the Wall Street Journal (WSJ) corpus of the Penn Treebank (Marcus et al., 1994) for English, the Penn Chinese Treebank 5.1 (CTB) (Xue et al., 2005) for Chinese, and eight additional treebanks from the SPMRL 2014 shared task (Seddah et al., 2014) for the other eight languages (Basque, German, French, Hebrew, Hungarian, Korean, Polish, Swedish). We use the standard data splits for each treebank. Following Kim et al. (2019), punctuation is removed from all data; the top 10000 frequent tokens in the training data of each treebank are kept as the vocabulary.3 Unless otherwise specified, we train C-PCFGs on sentences no longer than 40 tokens. Model hyperparameters and evaluati"
2021.adaptnlp-1.17,2020.acl-demos.38,0,0.0677599,"ue et al., 2005) for Chinese, and eight additional treebanks from the SPMRL 2014 shared task (Seddah et al., 2014) for the other eight languages (Basque, German, French, Hebrew, Hungarian, Korean, Polish, Swedish). We use the standard data splits for each treebank. Following Kim et al. (2019), punctuation is removed from all data; the top 10000 frequent tokens in the training data of each treebank are kept as the vocabulary.3 Unless otherwise specified, we train C-PCFGs on sentences no longer than 40 tokens. Model hyperparameters and evaluation: We re-implement C-PCFGs relying on TorchStruct (Rush, 2020) and adopt the same hyperparameter settings as in Kim et al. (2019). We train C-PCFGs for each language separately. On each treebank we run C-PCFGs four times with different random seeds and for 30 epochs. The best model in each run is selected according to the perplexity on the validation data. At test time, trivial spans, such as single-word and sentence-level spans, are ignored. We report average corpus- and sentence-level F1 numbers as well as the unbiased standard deviations. 4 4.1 Results and discussion Main results We compare C-PCFGs against three trivial baselines (left- / right-branch"
2021.adaptnlp-1.17,W14-6111,0,0.135662,"Missing"
2021.adaptnlp-1.17,2020.emnlp-main.354,1,0.77571,"Missing"
2021.emnlp-main.522,2020.emnlp-main.262,1,0.874608,"Missing"
2021.emnlp-main.522,N19-1423,0,0.051768,"Missing"
2021.emnlp-main.522,2020.emnlp-main.400,0,0.0149476,"ilar to Elastic Weight Consolidation (Kirkpatrick et al., 2017), a technique developed for preventing catastrophic forgetting in neural network models. reach the prediction quality of alternatives that retrieve and use context. Approaches that incentivize memorization of factual knowledge show to be beneficial for many downstream tasks suggesting that research on methods that effectively edit the memory of a model is indeed important (Zhang et al., 2019; Sun et al., 2019, 2020). Some recent hybrid approaches that use both implicit and explicit memory show some benefits for question answering (Févry et al., 2020; Verga et al., 2020). Notably, language models that only rely on internal implicit memory are state-of-the-art for (multilingual-) Entity Linking (De Cao et al., 2021a,b). An effective mechanism for editing LM’s implicit memory may be applicable in all these settings. Causal Interventions Identification of minimal changes to neural networks needed to achieve a certain behaviour has been studied in the context of research in interpreting neural networks (Lakretz et al., 2019; Vig et al., 2020; Elazar et al., 2021; Csordás et al., 2021). The components which need to be updated can be interprete"
2021.emnlp-main.522,2020.tacl-1.28,0,0.0368313,"et al., 2017)—i.e., a neural network that predicts the parameters of another network. Since the task requires every other prediction to stay the same—except the one we desire to change—we cast the learning task as a constrained optimization problem. Knowledge in Language Models Petroni et al. (2019) show that pre-trained language models recall factual knowledge without fine-tuning, which they do by feeding specific prompts to LMs. Handcrafted prompts have been found not to be the best option to extract knowledge from LMs, and various solutions have been proposed to understand what LMs ‘know’ (Jiang et al., 2020; Shin et al., 2020; Liu et al., 2021). Additionally, Roberts et al. (2020) show that large models can be fine-tuned to access their internal memories to answer questions in natural language without any additional context and with surprisingly high accuracy—a setting they Optimization For an input x, changing the prereferred to as closed-book question answering. Al- diction of a model f (·; θ) to a corresponds to minthough performing quite well, these models cannot imizing the loss L(θ; x, a) incurred when a is the 6494 target. Preserving the rest of the knowledge corresponds to constraining t"
2021.emnlp-main.522,P17-1147,0,0.0305987,"Missing"
2021.emnlp-main.522,P18-4020,0,0.0239798,"Missing"
2021.emnlp-main.522,Q19-1026,0,0.0230255,"Missing"
2021.emnlp-main.522,N19-1002,0,0.0210426,"2020). Some recent hybrid approaches that use both implicit and explicit memory show some benefits for question answering (Févry et al., 2020; Verga et al., 2020). Notably, language models that only rely on internal implicit memory are state-of-the-art for (multilingual-) Entity Linking (De Cao et al., 2021a,b). An effective mechanism for editing LM’s implicit memory may be applicable in all these settings. Causal Interventions Identification of minimal changes to neural networks needed to achieve a certain behaviour has been studied in the context of research in interpreting neural networks (Lakretz et al., 2019; Vig et al., 2020; Elazar et al., 2021; Csordás et al., 2021). The components which need to be updated can be interpreted as controlling or encoding the corresponding phenomena (e.g., subject-verb agreement). Much of this research focused on modifying neuron activations rather than weights and on sparse interventions (e.g., modifying one or a handful of neurons). While far from our goals, there are interesting connections with our work. For example, our analysis of updates in Section 6.4, though very limited, may shed some light on how factual knowledge is encoded in the parameters of a model"
2021.emnlp-main.522,2020.fever-1.5,0,0.0782322,"Missing"
2021.emnlp-main.522,K17-1034,0,0.0182328,"e report these baselines fine-tuning all parameters or just a subset of them. We limit the search to selecting entire layers and base our decision on performance on a subset of the validation set. Note that selecting a subset of parameters for update requires an extensive search, which K NOWLEDGE E DITOR dispenses with by automatically learning it. 5.2 Models and data on a task with a more complex output space: closedbook question answering (QA). For that we finetune a BART base model (Lewis et al., 2020) with a standard seq2seq objective on the Zero-Shot Relation Extraction (zsRE) dataset by Levy et al. (2017). We evaluate on this dataset because it is annotated with human-generated question paraphrases that we can use to measure our model’s robustness to semantically equivalent inputs. We create alternative predictions for FC simply flipping the labels, whereas for QA we pick all hypotheses enumerated via beam search except the top-1. The latter ensures high-probability outcomes under the model distribution. We generate semantically equivalent inputs with back-translation. See Appendix B for technical details on models and data collection. 6 Results Table 1 reports the main results for fact-checki"
2021.emnlp-main.522,2020.acl-main.703,0,0.128836,"tual knowledge, neural models implicitly memorize facts in their parameters. One cannot easily access and interpret their computation and memories (Ribeiro et al., 2016; Belinkov and Glass, 2019; Voita et al., 2019; De Cao et al., 2020), thus, modifying their knowledge is a challenging problem. Motivated by practical con1 Introduction siderations, we formulate the following desiderata Using pre-trained transformer-based Language for a method aimed at tackling this problem (see Models (LMs; Vaswani et al., 2017; Devlin et al., Section 2 for a more formal treatment): 2019; Radford et al., 2019; Lewis et al., 2020; Raf• Generality: be able to modify a model that fel et al., 2020; Brown et al., 2020) has recently was not specifically trained to be editable (i.e., 1 no need for special pre-training of LMs, such Source code available at https://github.com/ nicola-decao/KnowledgeEditor as using meta-learning); 6491 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6491–6506 c November 7–11, 2021. 2021 Association for Computational Linguistics Semantically equivalent What is the capital of Namibia? Answers Scores Namibia -0.43 Nigeria -0.69 Nibia -0.89 Namibia -1."
2021.emnlp-main.522,D19-1448,1,0.833456,"ver time (e.g., not reflecting changes of heads of states or country populations). Developing reliable and computationally efficient methods for bug-fixing models without the need for expensive re-training would be beneficial. See Figure 2 for an example of revising the memory of a model that initially misremembered Namibia’s capital. Unlike conventional Knowledge Bases (KBs) that explicitly store factual knowledge, neural models implicitly memorize facts in their parameters. One cannot easily access and interpret their computation and memories (Ribeiro et al., 2016; Belinkov and Glass, 2019; Voita et al., 2019; De Cao et al., 2020), thus, modifying their knowledge is a challenging problem. Motivated by practical con1 Introduction siderations, we formulate the following desiderata Using pre-trained transformer-based Language for a method aimed at tackling this problem (see Models (LMs; Vaswani et al., 2017; Devlin et al., Section 2 for a more formal treatment): 2019; Radford et al., 2019; Lewis et al., 2020; Raf• Generality: be able to modify a model that fel et al., 2020; Brown et al., 2020) has recently was not specifically trained to be editable (i.e., 1 no need for special pre-training of LMs, s"
2021.emnlp-main.522,P18-1042,0,0.0415729,"Missing"
2021.emnlp-main.523,2020.emnlp-main.42,0,0.0859537,"reflects linguistic properties. native solution in this direction is to develop sparsified softmax alternatives, such as sparse- In machine translation, the encoder-decoder attenmax (Martins and Astudillo, 2016; Malaviya et al., tion captures the source-target word alignment to 2018), entmax (Peters et al., 2019; Correia et al., a certain degree (Ghader and Monz, 2017), with recent work further strengthening this via specific 2019), fusedmax (Niculae and Blondel, 2017), and induction methods (Ding et al., 2019; Kobayashi 2 Note that sparsified softmax variants also use some form et al., 2020; Chen et al., 2020). We apply analyof ReLU to achieve sparsity, but they stick to the probabilistic constraint which demands extra complexity. sis techniques from previous work to analyze our 6508 models. 3 Background: Attention in Transformer Many variants of attention mechanism have been developed since its first proposal (Bahdanau et al., 2015; Luong et al., 2015). In this paper, we focus on the one used by Transformer, namely multihead scaled dot-product attention (MHATT), in an encoder-decoder setup. Given query inputs X ∈ Rn×d and a sequence of context items Y ∈ Rm×d , each head in MHATT summarizes query-r"
2021.emnlp-main.523,D15-1166,0,0.0686734,"Ghader and Monz, 2017), with recent work further strengthening this via specific 2019), fusedmax (Niculae and Blondel, 2017), and induction methods (Ding et al., 2019; Kobayashi 2 Note that sparsified softmax variants also use some form et al., 2020; Chen et al., 2020). We apply analyof ReLU to achieve sparsity, but they stick to the probabilistic constraint which demands extra complexity. sis techniques from previous work to analyze our 6508 models. 3 Background: Attention in Transformer Many variants of attention mechanism have been developed since its first proposal (Bahdanau et al., 2015; Luong et al., 2015). In this paper, we focus on the one used by Transformer, namely multihead scaled dot-product attention (MHATT), in an encoder-decoder setup. Given query inputs X ∈ Rn×d and a sequence of context items Y ∈ Rm×d , each head in MHATT summarizes query-relevant context information as follows: SMATT (X, Y) = αV, with α = Softmax f Q, KT  , (1) with Q = XWq ; K, V = YWk , YWv , where n and m are the query and context length, respectively; d and dh are the model and head dimension, respectively; W∗ ∈ Rd×dh denotes trainable model parameters. α ∈ Rn×m is the attention weight, which estimates the deg"
2021.emnlp-main.523,P18-2059,0,0.0360198,"Missing"
2021.emnlp-main.523,P00-1056,0,0.642798,"ayer attention (bottom figures). Results in Table 5 further show that the behaviour of ReLA-g is more alignment-like than the baselines we consider. Head Diversity We evaluate head diversity with a generalization of Jensen-Shannon divergence following (Correia et al., 2019) to reflect disCross Attention vs. Word Alignment We ex- agreements between heads. For ReLA-g, we reperiment with the publicly available De-En eval- normalize its attention scores via softmax, and reuation set9 and evaluate the alignment quality gard the null attention as a special one-hot distribuwith alignment error rate (Och and Ney, 2000, tion putting all probability mass to a dummy zero AER). We study normal attention and shifted atten- vector, i.e. entropy of 0. tion following previous work (Chen et al., 2020; Figure 5 shows the results. We observe that the Kobayashi et al., 2020). The former explores at- heads of the encoder self-attention exhibit much tention weights corresponding to decoder outputs higher disagreement than those of the other two (i.e. α in Eq. 1 and 3); the latter, by contrast, skips attention types for all sparsified attention models. the weights at the first decoding step, i.e. α[1 :], to Overall, head"
2021.emnlp-main.523,P02-1040,0,0.109315,"aky rectified linear unit (Xu et al., 2015). Baseline: Transformer. “-”: optimization failed, where training loss didn’t decrease. Higher BLEU indicates better result. 2018, Zh-En, 25M), and WMT16 RomanianEnglish (Bojar et al., 2016, Ro-En, 608K). We evaluate on the official test set from the corresponding year (e.g. newstest2014 for WMT14), and regard the previous year’s test set as the development set (e.g. newstest2013 for WMT14). We preprocess all datasets using the byte pair encoding algorithm (Sennrich et al., 2016) with 32K merging operations. We report detokenized case-sensitive BLEU (Papineni et al., 2002) implemented by SacreBLEU (Post, 2018),5 and also show tokenized case-sensitive BLEU with multi-bleu.perl for ablation studies. Model Configuration We use the Transformer base setting for experiments: model dimension d = 512, head number H = 8, head dimension dh = 64, 6 layers and FFN size of 2048 (Vaswani et al., 2017). We apply dropout to the residual connections and attention weights, with a rate of 0.1. We tune model parameters using Adam (Kingma and Ba, 2015, β1 = 0.9, β2 = 0.98) with label smoothing of 0.1. We schedule the learning rate following Vaswani et al. (2017) with a warmup step"
2021.emnlp-main.523,P19-1146,0,0.737498,"tes the analysis of the information flow in the model, and has led researchers to study sparse alternatives, which often lead to improved model performance and/or interpretabil1 Introduction ity (Correia et al., 2019). Efforts in this category Attention models (Bahdanau et al., 2015) have include designing fixed sparsity patterns (Raganato been hugely successful recently, with Trans- et al., 2020; Child et al., 2019) and creating sparsiformer (Vaswani et al., 2017) in particular, advanc- fied softmax variants (Martins and Astudillo, 2016; ing state of the art on various tasks, such as machine Peters et al., 2019). However, these methods also translation (Bojar et al., 2018), document summa- have drawbacks. Fixed sparsity patterns lack flexrization (Liu and Lapata, 2019) and speech process- ibility and generalize poorly across tasks. Sparing (Chiu et al., 2018), and delivering a large im- sified softmax variants often depend on complex pact on a broad range of NLP tasks via large-scale inference algorithms (e.g., requiring the sorting self-supervised pretraining (Devlin et al., 2019). operation), which reduces their efficiency. 1 In this paper, we propose rectified linear attenSource code is available"
2021.emnlp-main.523,W18-6319,0,0.0286511,"line: Transformer. “-”: optimization failed, where training loss didn’t decrease. Higher BLEU indicates better result. 2018, Zh-En, 25M), and WMT16 RomanianEnglish (Bojar et al., 2016, Ro-En, 608K). We evaluate on the official test set from the corresponding year (e.g. newstest2014 for WMT14), and regard the previous year’s test set as the development set (e.g. newstest2013 for WMT14). We preprocess all datasets using the byte pair encoding algorithm (Sennrich et al., 2016) with 32K merging operations. We report detokenized case-sensitive BLEU (Papineni et al., 2002) implemented by SacreBLEU (Post, 2018),5 and also show tokenized case-sensitive BLEU with multi-bleu.perl for ablation studies. Model Configuration We use the Transformer base setting for experiments: model dimension d = 512, head number H = 8, head dimension dh = 64, 6 layers and FFN size of 2048 (Vaswani et al., 2017). We apply dropout to the residual connections and attention weights, with a rate of 0.1. We tune model parameters using Adam (Kingma and Ba, 2015, β1 = 0.9, β2 = 0.98) with label smoothing of 0.1. We schedule the learning rate following Vaswani et al. (2017) with a warmup step of 4K. Each training batch contains ar"
2021.emnlp-main.523,2020.findings-emnlp.49,0,0.01865,"eries, even allowing for null attention (all attention scores are zero) for some queries. We provide experimental results for ReLA with Transformer on five machine translation tasks, along with an in-depth analysis on WMT14 English-German task. Our contributions are summarized below: hashing/clustering-based variants (Roy et al., 2020; Kitaev et al., 2020). These models often require dedicated algorithms for forward and backward propagation, at the cost of a significant computational overhead. Another strategy is to manually define sparse patterns inspired by task-specific attention analysis. Raganato et al. (2020) corroborated the feasibility of fixed patterns for Transformer encoder in translation. Child et al. (2019) introduced local and strided patterns to scale SMATT up to very long inputs. Unlike data-driven approaches, whether these patterns could generalize to different tasks and settings is still an open question. • We propose ReLA, a drop-in SMATT alternative, that learns sparse attention automatically with high flexibility and efficiency. A different type of linear attention model is proposed by Katharopoulos et al. (2020) and Choromanski et al. (2020), who aim at reducing the O(n2 ) complexi"
2021.emnlp-main.523,P16-1162,1,0.360181,"s on WMT14 En-De. GeLU: Gaussian error linear unit (Hendrycks and Gimpel, 2016); Leaky ReLU: leaky rectified linear unit (Xu et al., 2015). Baseline: Transformer. “-”: optimization failed, where training loss didn’t decrease. Higher BLEU indicates better result. 2018, Zh-En, 25M), and WMT16 RomanianEnglish (Bojar et al., 2016, Ro-En, 608K). We evaluate on the official test set from the corresponding year (e.g. newstest2014 for WMT14), and regard the previous year’s test set as the development set (e.g. newstest2013 for WMT14). We preprocess all datasets using the byte pair encoding algorithm (Sennrich et al., 2016) with 32K merging operations. We report detokenized case-sensitive BLEU (Papineni et al., 2002) implemented by SacreBLEU (Post, 2018),5 and also show tokenized case-sensitive BLEU with multi-bleu.perl for ablation studies. Model Configuration We use the Transformer base setting for experiments: model dimension d = 512, head number H = 8, head dimension dh = 64, 6 layers and FFN size of 2048 (Vaswani et al., 2017). We apply dropout to the residual connections and attention weights, with a rate of 0.1. We tune model parameters using Adam (Kingma and Ba, 2015, β1 = 0.9, β2 = 0.98) with label smoo"
2021.emnlp-main.523,P18-1117,1,0.846856,"y ensuring the sparse property of the attention weight α ∈ Rn×m . Besides, ReLA allows for null attention, where it assigns zero scores to all context items (i.e. some rows of α are zero vectors), effectively switching off the corresponding attention head for certain queries. Nevertheless, the outputs of ReLU in Eq. 3 are often of different scales and varied variance, causing gradient instability and also optimization failure. Stabilization with Normalization A common strategy in deep learning to stabilize neuron activations is to apply layer normalization LN(·) (Ba 3 As an anecdotal example, Voita et al. (2018) performed an analysis of attention to previous sentences in MT, and found that the model has learned to generally attend to the end-ofsentence symbol as a way to ignore context. While this might be an effective strategy for instances where context matters little, this reduces the interpretability of attention. We argue that the use of the softmax function in SMATT (Eq. 1) has two undesirable consequences: 6509 et al., 2016). We follow this strategy and normalize each representation z ∈ Rdh in the attention outputs (αV) with root mean square layer normalization (Zhang and Sennrich, 2019, RMSNo"
2021.emnlp-main.523,P19-1580,1,0.83075,"ose of the other two (i.e. α in Eq. 1 and 3); the latter, by contrast, skips attention types for all sparsified attention models. the weights at the first decoding step, i.e. α[1 :], to Overall, heads in ReLA-g are in less agreement offset the left padding to the decoder inputs made than with the sparsified softmax alternatives, in for auto-regressive generation in Transformer. most cases across different attention types. This Figure 4 shows the results. Regardless of the indicates that ReLA-g is capable of inducing heads attention type (normal or shifted), attention re- with different roles (Voita et al., 2019). 9 Note we convert the attention scores of ReLA-g https://www-i6.informatik.rwth-aachen. de/goldAlignment/ into categorical distribution via softmax for diver6513 JS Divergence Encoder Attention Decoder Attention Cross Attention softmax sparsemax 1.5-entmax ReLA-g 1.0 0.5 1 2 3 4 5 6 1 2 3 4 5 6 1 2 3 4 5 6 Layer Index Figure 5: Jensen-Shannon (JS) Divergence over heads at each layer for different attention models and types on the WMT14 En-De test set. Higher JS Divergence indicates higher head diversity. Encoder Attention Decoder Attention Cross Attention 0.6 Null Rate 0.5 0.4 0.3 0.2 0.1 1"
2021.emnlp-main.523,D19-1002,0,0.039206,"Missing"
2021.emnlp-main.604,2020.emnlp-main.630,0,0.0241356,"of-the-art approaches on the standard English AIDA dataset. 2 Background to contextualized mention representations. Martins et al. (2019) also explore joint learning of Named Entity Recognition (NER) and EL showing that the two tasks benefit from joint training, while Li et al. (2020) approach EL specifically for questions. In this work, we focus on monolingual EL in English while there is a line of work that explores cross-lingual entity linking (McNamee et al., 2011; Ji et al., 2015), that is linking from any source language to a standard one (e.g. English), and multilingual entity linking (Botha et al., 2020) that is a generalization of both. Autoregressive Linking The GENRE model by De Cao et al. (2021a) departs from framing EL as matching in vector space, and instead frames it as a sequence-to-sequence problem. GENRE tackles MD and ED for all mention-entity pairs jointly by autoregressively generating a version of the input markup-annotated with the entities’ unique identifiers expressed in natural language. Although we focus on EL, GENRE was also applied to ED alone as well as to page-level document retrieval for fact-checking, open-domain question Related work EL is typically decomposed in ans"
2021.emnlp-main.604,C10-1032,0,0.0424516,"ctive entities. This necestimes faster and more accurate than the presitates using an autoregressive decoder, precluding vious generative method, outperforming stateparallelism across mentions. Generation also has a of-the-art approaches on the standard English 1 high computational cost due to relying on a comdataset AIDA-CoNLL. plex and deep Transformer (Vaswani et al., 2017) decoder. Transformers are state-less and their mem1 Introduction ory footprint scales with sequence length, making Entity Linking (EL; Bunescu and Pa¸sca, 2006; them memory-consuming when generating long Cucerzan, 2007; Dredze et al., 2010; Hoffart et al., sequences. Additionally, Transformers-based de2011; Le and Titov, 2018) is a fundamental task in coders are notably data-hungry, and their effective NLP employed as a building block for text under- training requires large amounts of data. For examstanding (Févry et al., 2020a; Verga et al., 2020), ple, De Cao et al. (2021a) had to pre-train their question answering (Nie et al., 2019; Asai et al., model on Wikipedia abstracts. 2020; De Cao et al., 2019), dialog modeling (DiIn this work, we revisit the generative approach nan et al., 2019; Sevegnani et al., 2021), and in- to EL"
2021.emnlp-main.604,2020.emnlp-main.400,0,0.0175661,"relying on a comdataset AIDA-CoNLL. plex and deep Transformer (Vaswani et al., 2017) decoder. Transformers are state-less and their mem1 Introduction ory footprint scales with sequence length, making Entity Linking (EL; Bunescu and Pa¸sca, 2006; them memory-consuming when generating long Cucerzan, 2007; Dredze et al., 2010; Hoffart et al., sequences. Additionally, Transformers-based de2011; Le and Titov, 2018) is a fundamental task in coders are notably data-hungry, and their effective NLP employed as a building block for text under- training requires large amounts of data. For examstanding (Févry et al., 2020a; Verga et al., 2020), ple, De Cao et al. (2021a) had to pre-train their question answering (Nie et al., 2019; Asai et al., model on Wikipedia abstracts. 2020; De Cao et al., 2019), dialog modeling (DiIn this work, we revisit the generative approach nan et al., 2019; Sevegnani et al., 2021), and in- to EL and generate mention-entity pairs conditionformation extraction (Sarawagi, 2008; Martinez- ally independently given the input. This allows Rodriguez et al., 2020), to name a few. Popular ear- for parallelism across mentions, which we exploit lier methods address the Mention Detection (MD) by"
2021.emnlp-main.604,D17-1277,0,0.0487857,"Missing"
2021.emnlp-main.604,D11-1072,0,0.0619097,"hastic gradient descent (SGD; Robbins and Monro, 1951; Kiefer and Wolfowitz, 1952; Bottou, 2012). For the language model component, we employ length normalization (Sutskever et al., 2011, 2014) and label smoothing (Szegedy et al., 2016). All components are further regularized with dropout (Srivastava et al., 2014). The classification loss is the negative logarithm of Equation 4, and we approximate the normalization constant via negative sampling, with samples drawn from a candidate set specific to each training instance. 4 4.1 Experiments Setting We use the standard English AIDA-CoNLL splits (Hoffart et al., 2011) for training, validation (i.e., for doing model selection), and test. See Table 1 for statistics of this dataset. AIDA provides full supervision for both MD and ED. We only link mentions that have a valid gold KB entity, a setting 3 We limit the maximum number of tokens per span to referred to as InKB evaluation (Röder et al., 2018). 15 to avoid memory overhead (in the training set there is no mention with more than 12 tokens). This is in line with many previous models (Luo 7664 Split Documents Mentions 942 216 230 18,540 4,791 4,485 Training Validation Test Table 1: Statistics of the AIDA-Co"
2021.emnlp-main.604,P11-1055,0,0.0605715,"are Schmidhuber, 1997) as an encoder and then local full cross-encoders of context and entity since deand global scoring functions to link mentions. They coders can use attention to context. Bi-encoders exploit pre-computed entity embeddings by Ganea solutions may be sub-optimal and memory ineffiand Hofmann (2017) and match the embeddings cient although memory-efficient dense retrieval has 2 https://www.wikidata.org/wiki/Q60 recently received attention (Izacard et al., 2020; 7663 Task Entity Linking (EL) is the task of predicting a set Y of mention-entity pairs contained in some input text x (Hoffmann et al., 2011). Each mention m is a pair of start and end positions hms , me i indicating a span in x. Each mention m refers to an entity e in a fixed Knowledge Base (KB)—note that entities can be referred to with multiple ambiguous surface forms (e.g. in Wikidata “NYC"" and “New York"" both refers to the entity “New York City""2 ). Min et al., 2021; Lewis et al., 2021). A caveat of joint modeling all mention-entity pairs with an autoregressive model (i.e., without any independence assumptions) is the lack of parallelism, which makes GENRE extremely slow for the complete task of EL. In addition, generation of"
2021.emnlp-main.604,2021.eacl-main.40,0,0.0567059,"Missing"
2021.emnlp-main.604,K18-1050,0,0.0601994,"t et al., 2011) dataset. et al., 2015; Ganea and Hofmann, 2017; Yamada et al., 2016) and all systems we compare to. As in several previous approaches, for linking we assume the availability of a pre-computed set of candidates instead of considering the whole KB. For that, we use the candidates by Pershina et al. (2015). We also use these candidates to provide negative samples for the discriminative loss during training (see Equation 4). 4.2 Architecture details Method Micro-F1 Hoffart et al. (2011) Steinmetz and Sack (2013) Daiber et al. (2013) Moro et al. (2014) Piccinno and Ferragina (2014) Kolitsas et al. (2018) Peters et al. (2019) Broscheit (2019) Martins et al. (2019) van Hulst et al. (2020)† Févry et al. (2020b) De Cao et al. (2021a) Kannan Ravi et al. (2021) 72.8 42.3 57.8 48.5 73.0 82.4 73.7 79.3 81.9 80.5 76.7 83.7 83.1 Ours 85.5 Ablations (ours) LM score only Classifier score only 81.5 81.7 As the document encoder, we use a LongBeam Search w/ candidates 84.9 former (Beltagy et al., 2020). A Longformer is Beam Search w/o candidates 49.4* a RoBERTa (Liu et al., 2019) model with a limited attention window (we use 128 tokens). It Table 2: Results (InKB) on the AIDA test set and some has 12 layers"
2021.emnlp-main.604,P18-1148,1,0.848643,"sive decoder, precluding vious generative method, outperforming stateparallelism across mentions. Generation also has a of-the-art approaches on the standard English 1 high computational cost due to relying on a comdataset AIDA-CoNLL. plex and deep Transformer (Vaswani et al., 2017) decoder. Transformers are state-less and their mem1 Introduction ory footprint scales with sequence length, making Entity Linking (EL; Bunescu and Pa¸sca, 2006; them memory-consuming when generating long Cucerzan, 2007; Dredze et al., 2010; Hoffart et al., sequences. Additionally, Transformers-based de2011; Le and Titov, 2018) is a fundamental task in coders are notably data-hungry, and their effective NLP employed as a building block for text under- training requires large amounts of data. For examstanding (Févry et al., 2020a; Verga et al., 2020), ple, De Cao et al. (2021a) had to pre-train their question answering (Nie et al., 2019; Asai et al., model on Wikipedia abstracts. 2020; De Cao et al., 2019), dialog modeling (DiIn this work, we revisit the generative approach nan et al., 2019; Sevegnani et al., 2021), and in- to EL and generate mention-entity pairs conditionformation extraction (Sarawagi, 2008; Martine"
2021.emnlp-main.604,2020.emnlp-main.522,0,0.0120542,"oregressive entity linking that retains the advantages of being generative while being &gt;70 times faster than a previous generative formulation and as fast as non-generative models. We optimize for the correctness of the decoder’s ranking with a discriminative loss to improve autoregressive EL further. The model outperforms state-of-the-art approaches on the standard English AIDA dataset. 2 Background to contextualized mention representations. Martins et al. (2019) also explore joint learning of Named Entity Recognition (NER) and EL showing that the two tasks benefit from joint training, while Li et al. (2020) approach EL specifically for questions. In this work, we focus on monolingual EL in English while there is a line of work that explores cross-lingual entity linking (McNamee et al., 2011; Ji et al., 2015), that is linking from any source language to a standard one (e.g. English), and multilingual entity linking (Botha et al., 2020) that is a generalization of both. Autoregressive Linking The GENRE model by De Cao et al. (2021a) departs from framing EL as matching in vector space, and instead frames it as a sequence-to-sequence problem. GENRE tackles MD and ED for all mention-entity pairs join"
2021.emnlp-main.604,2021.ccl-1.108,0,0.0540938,"Missing"
2021.emnlp-main.604,D15-1104,0,0.0739129,"Missing"
2021.emnlp-main.604,P19-2026,0,0.0534211,"encoder (Beltagy et al., 2020) designed to support long sequences. Figure 1 outlines our model. Contributions We propose a highly parallel model for autoregressive entity linking that retains the advantages of being generative while being &gt;70 times faster than a previous generative formulation and as fast as non-generative models. We optimize for the correctness of the decoder’s ranking with a discriminative loss to improve autoregressive EL further. The model outperforms state-of-the-art approaches on the standard English AIDA dataset. 2 Background to contextualized mention representations. Martins et al. (2019) also explore joint learning of Named Entity Recognition (NER) and EL showing that the two tasks benefit from joint training, while Li et al. (2020) approach EL specifically for questions. In this work, we focus on monolingual EL in English while there is a line of work that explores cross-lingual entity linking (McNamee et al., 2011; Ji et al., 2015), that is linking from any source language to a standard one (e.g. English), and multilingual entity linking (Botha et al., 2020) that is a generalization of both. Autoregressive Linking The GENRE model by De Cao et al. (2021a) departs from framin"
2021.emnlp-main.604,I11-1029,0,0.0213794,"optimize for the correctness of the decoder’s ranking with a discriminative loss to improve autoregressive EL further. The model outperforms state-of-the-art approaches on the standard English AIDA dataset. 2 Background to contextualized mention representations. Martins et al. (2019) also explore joint learning of Named Entity Recognition (NER) and EL showing that the two tasks benefit from joint training, while Li et al. (2020) approach EL specifically for questions. In this work, we focus on monolingual EL in English while there is a line of work that explores cross-lingual entity linking (McNamee et al., 2011; Ji et al., 2015), that is linking from any source language to a standard one (e.g. English), and multilingual entity linking (Botha et al., 2020) that is a generalization of both. Autoregressive Linking The GENRE model by De Cao et al. (2021a) departs from framing EL as matching in vector space, and instead frames it as a sequence-to-sequence problem. GENRE tackles MD and ED for all mention-entity pairs jointly by autoregressively generating a version of the input markup-annotated with the entities’ unique identifiers expressed in natural language. Although we focus on EL, GENRE was also app"
2021.emnlp-main.604,Q14-1019,0,0.0524117,"Missing"
2021.emnlp-main.604,D19-1258,0,0.0290497,"Missing"
2021.emnlp-main.604,N15-1026,0,0.0605548,"Missing"
2021.emnlp-main.604,D19-1005,0,0.0441375,"Missing"
2021.emnlp-main.604,2021.naacl-main.200,1,0.772611,"Missing"
2021.emnlp-main.604,2021.acl-long.194,0,0.0167169,"ng Cucerzan, 2007; Dredze et al., 2010; Hoffart et al., sequences. Additionally, Transformers-based de2011; Le and Titov, 2018) is a fundamental task in coders are notably data-hungry, and their effective NLP employed as a building block for text under- training requires large amounts of data. For examstanding (Févry et al., 2020a; Verga et al., 2020), ple, De Cao et al. (2021a) had to pre-train their question answering (Nie et al., 2019; Asai et al., model on Wikipedia abstracts. 2020; De Cao et al., 2019), dialog modeling (DiIn this work, we revisit the generative approach nan et al., 2019; Sevegnani et al., 2021), and in- to EL and generate mention-entity pairs conditionformation extraction (Sarawagi, 2008; Martinez- ally independently given the input. This allows Rodriguez et al., 2020), to name a few. Popular ear- for parallelism across mentions, which we exploit lier methods address the Mention Detection (MD) by employing a shallow LSTM-based decoder. To and Entity Disambiguation (ED) stages of EL sep- optimize more explicitly the generator’s ranking, arately (Ceccarelli et al., 2013; Daiber et al., 2013; we use a discriminative correction term that pushes Steinmetz and Sack, 2013; Piccinno and Fer"
2021.emnlp-main.604,2020.emnlp-main.519,0,0.0231975,"nerating a version of the input markup-annotated with the entities’ unique identifiers expressed in natural language. Although we focus on EL, GENRE was also applied to ED alone as well as to page-level document retrieval for fact-checking, open-domain question Related work EL is typically decomposed in answering, slot filling, and dialog (Petroni et al., Mention Detection (MD, i.e., the task of finding 2021). mGENRE (De Cao et al., 2021b) is the mention spans in text) and Entity Disambiguation multilingual extension of GENRE. (ED, i.e., the task of disambiguating a mention Modern techniques (Wu et al., 2020; Botha et al., to its respective entity). Many methods (Hoffart 2020) are based on a dense retriever module that et al., 2011; Piccinno and Ferragina, 2014; Steinuses maximum inner-product search (MIPS) to metz and Sack, 2013) treat these sub-tasks sepamatch mention vectors to entity embeddings. In rately, training different modules. More modern apcontrast with MIPS for linking, generative models proaches – known as end-to-end EL – instead use a i) exploit knowledge learned during pre-training, ii) shared (typically neural) architecture. Kolitsas et al. are memory-efficient as they do not nee"
2021.emnlp-main.604,K16-1025,0,0.0178117,"istics of this dataset. AIDA provides full supervision for both MD and ED. We only link mentions that have a valid gold KB entity, a setting 3 We limit the maximum number of tokens per span to referred to as InKB evaluation (Röder et al., 2018). 15 to avoid memory overhead (in the training set there is no mention with more than 12 tokens). This is in line with many previous models (Luo 7664 Split Documents Mentions 942 216 230 18,540 4,791 4,485 Training Validation Test Table 1: Statistics of the AIDA-CoNLL standard splits (Hoffart et al., 2011) dataset. et al., 2015; Ganea and Hofmann, 2017; Yamada et al., 2016) and all systems we compare to. As in several previous approaches, for linking we assume the availability of a pre-computed set of candidates instead of considering the whole KB. For that, we use the candidates by Pershina et al. (2015). We also use these candidates to provide negative samples for the discriminative loss during training (see Equation 4). 4.2 Architecture details Method Micro-F1 Hoffart et al. (2011) Steinmetz and Sack (2013) Daiber et al. (2013) Moro et al. (2014) Piccinno and Ferragina (2014) Kolitsas et al. (2018) Peters et al. (2019) Broscheit (2019) Martins et al. (2019) v"
2021.emnlp-main.667,W18-6315,0,0.344008,"ining 1/3 as a held-out set for analysis (see Section 3.3). More details on hyperparameters, preprocessing, and training can be found in the appendix. 3.2 Target-Side LM Scores For each of the models, we train 2-, 3-, 4- and 5gram KenLM (Heafield, 2011)2 language models on target sides of the corresponding training data (segmented with BPE). We report KenLM scores for the translations of the development sets. 3.3 Monotonicity of Alignments To measure how the relative ordering of words in the source and its translation changes during training, we use two different scores used in previous work (Burlot and Yvon, 2018; Zhou et al., 2020). We evaluate the scores for two permutations of the source: the trivial monotonic alignment and the alignment inferred for the generated translation. Fuzzy Reordering Score (Talbot et al., 2011) counts the number of chunks of contiguously aligned words and, intuitively, it is based on the number of times a reader would need to jump in order to read one reordering in the order proposed by the other. The score is between 0 and 1, where a larger score indicates more monotonic alignments. Kendall tau distance (Kendall, 1938) is also called bubble-sort distance since it is equi"
2021.emnlp-main.667,2016.amta-researchers.10,0,0.017897,"translation by guiding teacher model selection. 1 Introduction competences within the same network over the course of training. Even though previous work shows how to improve some of the competences in NMT, e.g., by using lexical translation probabilities, phrase memories, target-side LM, alignment information (Arthur et al., 2016; He et al., 2016; Tang et al., 2016; Wang et al., 2017; Zhang et al., 2017a; Dahlmann et al., 2017; Gülçehre et al., 2015; Gülçehre et al., 2017; He et al., 2016; Sriram et al., 2017; Dahlmann et al., 2017; Stahlberg et al., 2018; Mi et al., 2016b; Liu et al., 2016; Chen et al., 2016; Alkhouli et al., 2016; Alkhouli and Ney, 2017; Park and Tsvetkov, 2019; Song et al., 2020a among others), it is still not clear how and when NMT acquires these competences during training. For example, are there any stages where NMT focuses on different aspects of translation, e.g., fluency (agreement on the target side) or adequacy (i.e. connection to the source), or does it improve everything at the same rate? Does it learn word-by-word translation first and more complicated patterns later, or is there a different behavior? This is especially interesting in light of a recent work analyzing"
2021.emnlp-main.667,2020.emnlp-main.42,0,0.0125496,"er resources due to only partial training of the teachers. 7 Additional Related Work Other work connecting neural and traditional approaches include modeling modifications, such as modeling coverage and/or fertility (Tu et al., 2016; Mi et al., 2016a; Cohn et al., 2016; Feng et al., 2016) and several other modifications (Zhang et al., 2017b; Stahlberg et al., 2017; Huang et al., 2018), analysis of the relation between attention and word alignments (Ghader and Monz, 2017), and word alignment induction from NMT models (Li et al., 2019; Garg et al., 2019; Song et al., 2020b; Zenkel et al., 2020; Chen et al., 2020). Previous analysis of NMT learning dynamics include analyzing how the trainable parameters affect an NMT model (Zhu et al., 2020) and looking at the speed of learning specific discourse phenomena in context-aware NMT (Voita et al., 2019b,a). 8 Conclusions We analyze how NMT acquires different competencies during training and look at the competencies related to three core SMT components. We find 6 We used the code and the data from https:// that NMT first focuses on learning target-side langithub.com/pytorch/fairseq/tree/master/ examples/nonautoregressive_translation. guage modeling, then impr"
2021.emnlp-main.667,N16-1102,0,0.0223665,"lysis can be useful in the settings where data complexity matters and, therefore, limit ourselves to only using different teacher checkpoints. Future work, however, can investigate possible combinations with other approaches. For example, to further improve quality, our method can be combined with the Born-Again networks while still requiring fewer resources due to only partial training of the teachers. 7 Additional Related Work Other work connecting neural and traditional approaches include modeling modifications, such as modeling coverage and/or fertility (Tu et al., 2016; Mi et al., 2016a; Cohn et al., 2016; Feng et al., 2016) and several other modifications (Zhang et al., 2017b; Stahlberg et al., 2017; Huang et al., 2018), analysis of the relation between attention and word alignments (Ghader and Monz, 2017), and word alignment induction from NMT models (Li et al., 2019; Garg et al., 2019; Song et al., 2020b; Zenkel et al., 2020; Chen et al., 2020). Previous analysis of NMT learning dynamics include analyzing how the trainable parameters affect an NMT model (Zhu et al., 2020) and looking at the speed of learning specific discourse phenomena in context-aware NMT (Voita et al., 2019b,a). 8 Conclu"
2021.emnlp-main.667,W11-2123,0,0.0230619,"En-Ru – 2.5m sentence pairs (parallel training data excluding UN and Paracrawl). Since our observations are similar for both languages, in the main text we show figures for one of them and in the appendix – for the other. vocabulary of about 50k tokens for LM-style models. For each experiment, we randomly choose 2/3 of the dataset for training and use the remaining 1/3 as a held-out set for analysis (see Section 3.3). More details on hyperparameters, preprocessing, and training can be found in the appendix. 3.2 Target-Side LM Scores For each of the models, we train 2-, 3-, 4- and 5gram KenLM (Heafield, 2011)2 language models on target sides of the corresponding training data (segmented with BPE). We report KenLM scores for the translations of the development sets. 3.3 Monotonicity of Alignments To measure how the relative ordering of words in the source and its translation changes during training, we use two different scores used in previous work (Burlot and Yvon, 2018; Zhou et al., 2020). We evaluate the scores for two permutations of the source: the trivial monotonic alignment and the alignment inferred for the generated translation. Fuzzy Reordering Score (Talbot et al., 2011) counts the numbe"
2021.emnlp-main.667,D16-1139,0,0.0235228,"eously. This is possible only with an underlying assumption that the output tokens are independent from each other, which is unrealistic for natural language. Fortunately, while this independence assumption is unrealistic for real references, it might be more plausible for simpler sequences, e.g. artificially generated translations. That is why targets for NAT models are usually not references but beam search translations of the standard autoregressive NMT (which, as we already mentioned above, are simpler than references in many aspects). This is called sequence-level knowledge distillation (Kim and Rush, 2016), and it is currently one of the de-facto standard parts of the NAT training pipelines (Gu et al. (2018); Lee et al. (2018); Ghazvininejad et al. (2019) to name a few). Recently Zhou et al. (2020) showed that the quality of a NAT model strongly depends on the We showed that during a large part of the training, complexity of the distilled data, and changing this the translation quality (e.g., BLEU) changes little, complexity can improve the model. Since distilled but the alignments become less monotonic. Intu- data consists of translations from a standard auitively, the translations become more"
2021.emnlp-main.667,D18-1149,0,0.020969,"unrealistic for natural language. Fortunately, while this independence assumption is unrealistic for real references, it might be more plausible for simpler sequences, e.g. artificially generated translations. That is why targets for NAT models are usually not references but beam search translations of the standard autoregressive NMT (which, as we already mentioned above, are simpler than references in many aspects). This is called sequence-level knowledge distillation (Kim and Rush, 2016), and it is currently one of the de-facto standard parts of the NAT training pipelines (Gu et al. (2018); Lee et al. (2018); Ghazvininejad et al. (2019) to name a few). Recently Zhou et al. (2020) showed that the quality of a NAT model strongly depends on the We showed that during a large part of the training, complexity of the distilled data, and changing this the translation quality (e.g., BLEU) changes little, complexity can improve the model. Since distilled but the alignments become less monotonic. Intu- data consists of translations from a standard auitively, the translations become more complicated toregressive teacher, our analysis gives a very simwhile their quality remains roughly the same. ple way of mo"
2021.emnlp-main.667,P19-1124,0,0.0181308,"method can be combined with the Born-Again networks while still requiring fewer resources due to only partial training of the teachers. 7 Additional Related Work Other work connecting neural and traditional approaches include modeling modifications, such as modeling coverage and/or fertility (Tu et al., 2016; Mi et al., 2016a; Cohn et al., 2016; Feng et al., 2016) and several other modifications (Zhang et al., 2017b; Stahlberg et al., 2017; Huang et al., 2018), analysis of the relation between attention and word alignments (Ghader and Monz, 2017), and word alignment induction from NMT models (Li et al., 2019; Garg et al., 2019; Song et al., 2020b; Zenkel et al., 2020; Chen et al., 2020). Previous analysis of NMT learning dynamics include analyzing how the trainable parameters affect an NMT model (Zhu et al., 2020) and looking at the speed of learning specific discourse phenomena in context-aware NMT (Voita et al., 2019b,a). 8 Conclusions We analyze how NMT acquires different competencies during training and look at the competencies related to three core SMT components. We find 6 We used the code and the data from https:// that NMT first focuses on learning target-side langithub.com/pytorch/fairse"
2021.emnlp-main.667,C16-1291,0,0.0204142,"ve neural machine translation by guiding teacher model selection. 1 Introduction competences within the same network over the course of training. Even though previous work shows how to improve some of the competences in NMT, e.g., by using lexical translation probabilities, phrase memories, target-side LM, alignment information (Arthur et al., 2016; He et al., 2016; Tang et al., 2016; Wang et al., 2017; Zhang et al., 2017a; Dahlmann et al., 2017; Gülçehre et al., 2015; Gülçehre et al., 2017; He et al., 2016; Sriram et al., 2017; Dahlmann et al., 2017; Stahlberg et al., 2018; Mi et al., 2016b; Liu et al., 2016; Chen et al., 2016; Alkhouli et al., 2016; Alkhouli and Ney, 2017; Park and Tsvetkov, 2019; Song et al., 2020a among others), it is still not clear how and when NMT acquires these competences during training. For example, are there any stages where NMT focuses on different aspects of translation, e.g., fluency (agreement on the target side) or adequacy (i.e. connection to the source), or does it improve everything at the same rate? Does it learn word-by-word translation first and more complicated patterns later, or is there a different behavior? This is especially interesting in light of a re"
2021.emnlp-main.667,D16-1096,0,0.114135,"la nonautoregressive neural machine translation by guiding teacher model selection. 1 Introduction competences within the same network over the course of training. Even though previous work shows how to improve some of the competences in NMT, e.g., by using lexical translation probabilities, phrase memories, target-side LM, alignment information (Arthur et al., 2016; He et al., 2016; Tang et al., 2016; Wang et al., 2017; Zhang et al., 2017a; Dahlmann et al., 2017; Gülçehre et al., 2015; Gülçehre et al., 2017; He et al., 2016; Sriram et al., 2017; Dahlmann et al., 2017; Stahlberg et al., 2018; Mi et al., 2016b; Liu et al., 2016; Chen et al., 2016; Alkhouli et al., 2016; Alkhouli and Ney, 2017; Park and Tsvetkov, 2019; Song et al., 2020a among others), it is still not clear how and when NMT acquires these competences during training. For example, are there any stages where NMT focuses on different aspects of translation, e.g., fluency (agreement on the target side) or adequacy (i.e. connection to the source), or does it improve everything at the same rate? Does it learn word-by-word translation first and more complicated patterns later, or is there a different behavior? This is especially interesti"
2021.emnlp-main.667,D16-1249,0,0.102271,"la nonautoregressive neural machine translation by guiding teacher model selection. 1 Introduction competences within the same network over the course of training. Even though previous work shows how to improve some of the competences in NMT, e.g., by using lexical translation probabilities, phrase memories, target-side LM, alignment information (Arthur et al., 2016; He et al., 2016; Tang et al., 2016; Wang et al., 2017; Zhang et al., 2017a; Dahlmann et al., 2017; Gülçehre et al., 2015; Gülçehre et al., 2017; He et al., 2016; Sriram et al., 2017; Dahlmann et al., 2017; Stahlberg et al., 2018; Mi et al., 2016b; Liu et al., 2016; Chen et al., 2016; Alkhouli et al., 2016; Alkhouli and Ney, 2017; Park and Tsvetkov, 2019; Song et al., 2020a among others), it is still not clear how and when NMT acquires these competences during training. For example, are there any stages where NMT focuses on different aspects of translation, e.g., fluency (agreement on the target side) or adequacy (i.e. connection to the source), or does it improve everything at the same rate? Does it learn word-by-word translation first and more complicated patterns later, or is there a different behavior? This is especially interesti"
2021.emnlp-main.667,N19-4009,0,0.0257403,"g Following previous work (Zhou et al., 2020), we train the same NAT model on their preprocessed dataset6 and vary only distilled targets. Model. The model is the re-implemented by Zhou et al. (2020) version of the vanilla NAT by Gu et al. (2018). For more details, see appendix. Dataset. The dataset is WMT14 English-German (En-De) with newstest2013 as the validation set and newstest2014 as the test set, and BPE vocabulary of 37,000. We use the preprocessed dataset and the vocabularies released by Zhou et al. (2020). Distilled targets. The teacher is the standard Transformer-base from fairseq (Ott et al., 2019). For the baseline distilled dataset, we use the fully converged model (in this case, the model after 200k updates). For other datasets, we use earlier checkpoints. Evaluation. We average the last 10 checkpoints. 6.3 Experiments Figure 8c shows the BLEU scores for NAT models trained with distilled data obtained from different teacher’s checkpoints; the baseline is the fully converged model (200k iterations). We see that by taking an earlier checkpoint, after 40k iterations, we improve NAT quality by 1.1 BLEU. For this checkpoint, the teacher’s BLEU score is not much lower than that of the fina"
2021.emnlp-main.667,D19-5626,0,0.0145721,"petences within the same network over the course of training. Even though previous work shows how to improve some of the competences in NMT, e.g., by using lexical translation probabilities, phrase memories, target-side LM, alignment information (Arthur et al., 2016; He et al., 2016; Tang et al., 2016; Wang et al., 2017; Zhang et al., 2017a; Dahlmann et al., 2017; Gülçehre et al., 2015; Gülçehre et al., 2017; He et al., 2016; Sriram et al., 2017; Dahlmann et al., 2017; Stahlberg et al., 2018; Mi et al., 2016b; Liu et al., 2016; Chen et al., 2016; Alkhouli et al., 2016; Alkhouli and Ney, 2017; Park and Tsvetkov, 2019; Song et al., 2020a among others), it is still not clear how and when NMT acquires these competences during training. For example, are there any stages where NMT focuses on different aspects of translation, e.g., fluency (agreement on the target side) or adequacy (i.e. connection to the source), or does it improve everything at the same rate? Does it learn word-by-word translation first and more complicated patterns later, or is there a different behavior? This is especially interesting in light of a recent work analyzing how NMT balances the two different types of context: the source and pre"
2021.emnlp-main.667,P16-1162,1,0.275811,"ere 0 indicates the monotonic alignment. The main difference between the scores is that the first one takes into account only the number of jumps, while the second also considers their distance. For a formal description of the scores and their differences, see the appendix. Our setting. For each of the considered model checkpoints, we obtain datasets where the sources come from the held-out 1/3 of the original dataset, and targets are their translations. For these datasets, we infer alignments using fast_align (Dyer et al., 2013)3 . Preprocessing. The data is lowercased and encoded using BPE (Sennrich et al., 2016). We use separate source and target vocabularies of about 32k tokens for encoder-decoder models, and a joint 8480 2 3 https://github.com/kpu/kenlm https://github.com/clab/fast_align (a) (b) Figure 2: (a) KenLM scores (horizontal dashed lines are the scores for the references); (b) proportion of tokens of different frequency ranks in model translations. En-Ru. Figure 3: Translations at different steps during training. En-De. 4 Transformer Training Stages In this section, we discuss the standard encoderdecoder Transformer. In the next section, we mention differences with several other models. We"
2021.emnlp-main.667,W18-6321,0,0.0237275,"be used to improve vanilla nonautoregressive neural machine translation by guiding teacher model selection. 1 Introduction competences within the same network over the course of training. Even though previous work shows how to improve some of the competences in NMT, e.g., by using lexical translation probabilities, phrase memories, target-side LM, alignment information (Arthur et al., 2016; He et al., 2016; Tang et al., 2016; Wang et al., 2017; Zhang et al., 2017a; Dahlmann et al., 2017; Gülçehre et al., 2015; Gülçehre et al., 2017; He et al., 2016; Sriram et al., 2017; Dahlmann et al., 2017; Stahlberg et al., 2018; Mi et al., 2016b; Liu et al., 2016; Chen et al., 2016; Alkhouli et al., 2016; Alkhouli and Ney, 2017; Park and Tsvetkov, 2019; Song et al., 2020a among others), it is still not clear how and when NMT acquires these competences during training. For example, are there any stages where NMT focuses on different aspects of translation, e.g., fluency (agreement on the target side) or adequacy (i.e. connection to the source), or does it improve everything at the same rate? Does it learn word-by-word translation first and more complicated patterns later, or is there a different behavior? This is esp"
2021.emnlp-main.667,E17-2058,0,0.0612973,"Missing"
2021.emnlp-main.667,W11-2102,0,0.0290503,", 4- and 5gram KenLM (Heafield, 2011)2 language models on target sides of the corresponding training data (segmented with BPE). We report KenLM scores for the translations of the development sets. 3.3 Monotonicity of Alignments To measure how the relative ordering of words in the source and its translation changes during training, we use two different scores used in previous work (Burlot and Yvon, 2018; Zhou et al., 2020). We evaluate the scores for two permutations of the source: the trivial monotonic alignment and the alignment inferred for the generated translation. Fuzzy Reordering Score (Talbot et al., 2011) counts the number of chunks of contiguously aligned words and, intuitively, it is based on the number of times a reader would need to jump in order to read one reordering in the order proposed by the other. The score is between 0 and 1, where a larger score indicates more monotonic alignments. Kendall tau distance (Kendall, 1938) is also called bubble-sort distance since it is equivalent to the number of swaps that the bubble sort algorithm would take to place one list in the same order as the other list. We evaluate the normalized distance: it is between 0 and 1, where 0 indicates the monoto"
2021.emnlp-main.667,W19-6627,0,0.0318272,"Missing"
2021.emnlp-main.667,P16-1008,0,0.0292613,"ts mainly to illustrate how our analysis can be useful in the settings where data complexity matters and, therefore, limit ourselves to only using different teacher checkpoints. Future work, however, can investigate possible combinations with other approaches. For example, to further improve quality, our method can be combined with the Born-Again networks while still requiring fewer resources due to only partial training of the teachers. 7 Additional Related Work Other work connecting neural and traditional approaches include modeling modifications, such as modeling coverage and/or fertility (Tu et al., 2016; Mi et al., 2016a; Cohn et al., 2016; Feng et al., 2016) and several other modifications (Zhang et al., 2017b; Stahlberg et al., 2017; Huang et al., 2018), analysis of the relation between attention and word alignments (Ghader and Monz, 2017), and word alignment induction from NMT models (Li et al., 2019; Garg et al., 2019; Song et al., 2020b; Zenkel et al., 2020; Chen et al., 2020). Previous analysis of NMT learning dynamics include analyzing how the trainable parameters affect an NMT model (Zhu et al., 2020) and looking at the speed of learning specific discourse phenomena in context-aware"
2021.emnlp-main.667,D19-1081,1,0.900622,"Missing"
2021.emnlp-main.667,P19-1116,1,0.896375,"Missing"
2021.emnlp-main.667,2021.acl-long.91,1,0.927702,"thers), it is still not clear how and when NMT acquires these competences during training. For example, are there any stages where NMT focuses on different aspects of translation, e.g., fluency (agreement on the target side) or adequacy (i.e. connection to the source), or does it improve everything at the same rate? Does it learn word-by-word translation first and more complicated patterns later, or is there a different behavior? This is especially interesting in light of a recent work analyzing how NMT balances the two different types of context: the source and prefix of the target sentence (Voita et al., 2021). As it turns out, changes in NMT training are non-monotonic and form several distinct stages (e.g., stages changing direction from decreasing influence of source to increasing), which hints that the NMT training consists of stages with qualitatively different changes. In the last couple of decades, the two main machine translation paradigms have been statistical and neural MT. Statistical MT (SMT) decomposes the translation task into several components (e.g., lexical translation probabilities, alignment probabilities, target-side language model, etc.) which are learned separately and then com"
2021.emnlp-main.667,D17-1149,0,0.0184086,"veral models and language pairs. Additionally, we explain how such an understanding of the training process can be useful in practice and, as an example, show how it can be used to improve vanilla nonautoregressive neural machine translation by guiding teacher model selection. 1 Introduction competences within the same network over the course of training. Even though previous work shows how to improve some of the competences in NMT, e.g., by using lexical translation probabilities, phrase memories, target-side LM, alignment information (Arthur et al., 2016; He et al., 2016; Tang et al., 2016; Wang et al., 2017; Zhang et al., 2017a; Dahlmann et al., 2017; Gülçehre et al., 2015; Gülçehre et al., 2017; He et al., 2016; Sriram et al., 2017; Dahlmann et al., 2017; Stahlberg et al., 2018; Mi et al., 2016b; Liu et al., 2016; Chen et al., 2016; Alkhouli et al., 2016; Alkhouli and Ney, 2017; Park and Tsvetkov, 2019; Song et al., 2020a among others), it is still not clear how and when NMT acquires these competences during training. For example, are there any stages where NMT focuses on different aspects of translation, e.g., fluency (agreement on the target side) or adequacy (i.e. connection to the source),"
2021.emnlp-main.667,2020.acl-main.146,0,0.0201941,"e still requiring fewer resources due to only partial training of the teachers. 7 Additional Related Work Other work connecting neural and traditional approaches include modeling modifications, such as modeling coverage and/or fertility (Tu et al., 2016; Mi et al., 2016a; Cohn et al., 2016; Feng et al., 2016) and several other modifications (Zhang et al., 2017b; Stahlberg et al., 2017; Huang et al., 2018), analysis of the relation between attention and word alignments (Ghader and Monz, 2017), and word alignment induction from NMT models (Li et al., 2019; Garg et al., 2019; Song et al., 2020b; Zenkel et al., 2020; Chen et al., 2020). Previous analysis of NMT learning dynamics include analyzing how the trainable parameters affect an NMT model (Zhu et al., 2020) and looking at the speed of learning specific discourse phenomena in context-aware NMT (Voita et al., 2019b,a). 8 Conclusions We analyze how NMT acquires different competencies during training and look at the competencies related to three core SMT components. We find 6 We used the code and the data from https:// that NMT first focuses on learning target-side langithub.com/pytorch/fairseq/tree/master/ examples/nonautoregressive_translation. guage"
2021.emnlp-main.667,P17-1139,0,0.117211,"nguage pairs. Additionally, we explain how such an understanding of the training process can be useful in practice and, as an example, show how it can be used to improve vanilla nonautoregressive neural machine translation by guiding teacher model selection. 1 Introduction competences within the same network over the course of training. Even though previous work shows how to improve some of the competences in NMT, e.g., by using lexical translation probabilities, phrase memories, target-side LM, alignment information (Arthur et al., 2016; He et al., 2016; Tang et al., 2016; Wang et al., 2017; Zhang et al., 2017a; Dahlmann et al., 2017; Gülçehre et al., 2015; Gülçehre et al., 2017; He et al., 2016; Sriram et al., 2017; Dahlmann et al., 2017; Stahlberg et al., 2018; Mi et al., 2016b; Liu et al., 2016; Chen et al., 2016; Alkhouli et al., 2016; Alkhouli and Ney, 2017; Park and Tsvetkov, 2019; Song et al., 2020a among others), it is still not clear how and when NMT acquires these competences during training. For example, are there any stages where NMT focuses on different aspects of translation, e.g., fluency (agreement on the target side) or adequacy (i.e. connection to the source), or does it improve e"
2021.emnlp-main.667,I17-1016,0,0.0954765,"nguage pairs. Additionally, we explain how such an understanding of the training process can be useful in practice and, as an example, show how it can be used to improve vanilla nonautoregressive neural machine translation by guiding teacher model selection. 1 Introduction competences within the same network over the course of training. Even though previous work shows how to improve some of the competences in NMT, e.g., by using lexical translation probabilities, phrase memories, target-side LM, alignment information (Arthur et al., 2016; He et al., 2016; Tang et al., 2016; Wang et al., 2017; Zhang et al., 2017a; Dahlmann et al., 2017; Gülçehre et al., 2015; Gülçehre et al., 2017; He et al., 2016; Sriram et al., 2017; Dahlmann et al., 2017; Stahlberg et al., 2018; Mi et al., 2016b; Liu et al., 2016; Chen et al., 2016; Alkhouli et al., 2016; Alkhouli and Ney, 2017; Park and Tsvetkov, 2019; Song et al., 2020a among others), it is still not clear how and when NMT acquires these competences during training. For example, are there any stages where NMT focuses on different aspects of translation, e.g., fluency (agreement on the target side) or adequacy (i.e. connection to the source), or does it improve e"
2021.emnlp-main.714,W13-2322,0,0.13195,"of its semantic role ARG1. Intuitively, this subgraph needs to be aligned to the token ‘opinion’. Similarly, ‘(b / boy)’ should be aligned to the token ‘boy’. Given such an alignment and segmentation, it is straightforward to construct a simple parser: parsing can be framed as tagging input tokens with subgraphs (including empty subgraphs), followed by predicting relations between the subgraphs. The key obstacle to training such an AMR parser is that the segmentation and alignment between AMR subgraphs and words are latent, i.e. not annotated in the data. Abstract Meaning Representation (AMR; Banarescu et al. 2013) is a broad-coverage semantic Most previous work adopts a pipeline approach formalism which represents sentence meaning as to handling this obstacle. They rely on a prerooted labeled directed acyclic graphs. The rep- learned aligner (e.g., (Pourdamghani et al., 2014)) resentations have been exploited in a wide range to produce the alignment, and apply a rule sysof tasks, including text summarization (Liu et al., tem to segment the AMR subgraph (Flanigan et al., 2015; Dohare and Karnick, 2017; Hardy and Vla- 2014; Werling et al., 2015; Damonte et al., 2017; chos, 2018), machine translation (Jon"
2021.emnlp-main.714,L18-1266,0,0.0222361,"Missing"
2021.emnlp-main.714,2020.lrec-1.86,0,0.161565,"., 9075 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9075–9091 c November 7–11, 2021. 2021 Association for Computational Linguistics 2013; Damonte and Cohen, 2018) and AMR banks for individual languages substantially diverge from English AMR. For example, Spanish AMR represents pronouns and ellipsis differently from the English one (Migueles-Abraira et al., 2018). As new AMR sembanks in languages other than English are being developed (Anchiêta and Pardo, 2018; Song et al., 2020), domain-specific AMR extensions get developed (Bonn et al., 2020; Bonial et al., 2020), and extra constructions are getting introduced to AMRs (Bonial et al., 2018), eliminating the need for rules while learning graph segmentation from scratch is becoming an important problem to solve. We propose to optimize a graph-based parser that treats the alignment and graph segmentation as latent variables. The graph-based parser consists of two parts: concept identification and relation identification. The concept identification model generates the AMR nodes, and the relation identification component decides on the labeled edges. During training, both components rely on latent alignment"
2021.emnlp-main.714,2020.lrec-1.601,0,0.0162014,"ua (Banarescu et al., 9075 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9075–9091 c November 7–11, 2021. 2021 Association for Computational Linguistics 2013; Damonte and Cohen, 2018) and AMR banks for individual languages substantially diverge from English AMR. For example, Spanish AMR represents pronouns and ellipsis differently from the English one (Migueles-Abraira et al., 2018). As new AMR sembanks in languages other than English are being developed (Anchiêta and Pardo, 2018; Song et al., 2020), domain-specific AMR extensions get developed (Bonn et al., 2020; Bonial et al., 2020), and extra constructions are getting introduced to AMRs (Bonial et al., 2018), eliminating the need for rules while learning graph segmentation from scratch is becoming an important problem to solve. We propose to optimize a graph-based parser that treats the alignment and graph segmentation as latent variables. The graph-based parser consists of two parts: concept identification and relation identification. The concept identification model generates the AMR nodes, and the relation identification component decides on the labeled edges. During training, both components re"
2021.emnlp-main.714,D19-1393,0,0.0242465,"Missing"
2021.emnlp-main.714,2020.acl-main.119,0,0.568672,"identification and relation identification. The concept identification model generates the AMR nodes, and the relation identification component decides on the labeled edges. During training, both components rely on latent alignment and segmentation, which are being induced simultaneously. Importantly, at test time, the parser simply tags the input with the subgraphs and predicts the relations, so there is no test-time overhead from using the latent-structure apparatus. An extra benefit of this approach, in contrast to encoder-decoder AMR models (Konstas et al., 2017; van Noord and Bos, 2017; Cai and Lam, 2020) is its transparency, as one can readily see which input token triggers each subgraph.1 To develop our parser, we frame the alignment and segmentation problems as choosing a generation order of concept nodes, as we explain in Section 2.2. As marginalization over the latent generation orders is infeasible, we adopt the variational auto-encoder (VAE) framework (Kingma and Welling, 2014). Intuitively, a trainable neural module (an encoder in the VAE) is used to sample a plausible generation order (i.e., a segmentation plus an alignment), which is then used to train the parser (a decoder in the VA"
2021.emnlp-main.714,P13-2131,0,0.333006,"Missing"
2021.emnlp-main.714,N18-1104,1,0.853323,"on rules are relvidually aligned to sentence tokens (Flanigan et al., atively complex — e.g., the rules of Lyu and Titov 2014). In Figure 1, each dashed box represents (2018) targeted 40 different AMR subgraph types — the boundary of a single semantic subgraph. Red and language-dependent. AMR has never been inarrows represent the alignment between subgraphs tended to be used as an interlingua (Banarescu et al., 9075 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9075–9091 c November 7–11, 2021. 2021 Association for Computational Linguistics 2013; Damonte and Cohen, 2018) and AMR banks for individual languages substantially diverge from English AMR. For example, Spanish AMR represents pronouns and ellipsis differently from the English one (Migueles-Abraira et al., 2018). As new AMR sembanks in languages other than English are being developed (Anchiêta and Pardo, 2018; Song et al., 2020), domain-specific AMR extensions get developed (Bonn et al., 2020; Bonial et al., 2020), and extra constructions are getting introduced to AMRs (Bonial et al., 2018), eliminating the need for rules while learning graph segmentation from scratch is becoming an important problem t"
2021.emnlp-main.714,E17-1051,1,0.936262,"bstract Meaning Representation (AMR; Banarescu et al. 2013) is a broad-coverage semantic Most previous work adopts a pipeline approach formalism which represents sentence meaning as to handling this obstacle. They rely on a prerooted labeled directed acyclic graphs. The rep- learned aligner (e.g., (Pourdamghani et al., 2014)) resentations have been exploited in a wide range to produce the alignment, and apply a rule sysof tasks, including text summarization (Liu et al., tem to segment the AMR subgraph (Flanigan et al., 2015; Dohare and Karnick, 2017; Hardy and Vla- 2014; Werling et al., 2015; Damonte et al., 2017; chos, 2018), machine translation (Jones et al., 2012; Ballesteros and Al-Onaizan, 2017; Peng et al., Song et al., 2019), paraphrase detection (Issa et al., 2015; Artzi et al., 2015; Groschwitz et al., 2018). 2018) and question answering (Mitra and Baral, While Lyu and Titov (2018) jointly optimize the 2016). parser and the alignment model, the rules handling An AMR graph can be regarded as consisting specific constructions still needed to be crafted to of multiple concept subgraphs, which can be indi- segment the graph. The segmentation rules are relvidually aligned to sentence tokens (Flani"
2021.emnlp-main.714,P14-1134,0,0.0233221,".8 71 72 78.2 Smatch 71.0 74.4 77.0 75.5 73.2 76.8 80.2 + 88.1 74.2 80.2 88.1 74.5 78.7 87.4 78.9 80.2 - 87.5 ± 0.1 71.3 ±0.1 75.2 ± 0.1 + 88.7 ± 0.2 73.6± 0.2 76.8 ± 0.4 - 88.3 ± 0.3 73.0 ± 0.2 76.1 ± 0.2 rized in Table 3. As expected, the full model performs the best, demonstrating that it is important to learn both alignments and segmentation. Interestingly, both ‘segmentation learned’ and ‘alignment learned’ obtain reasonable performance, but the ‘nothing learned’ model fails badly. 7 Related Work A wide range of approaches for AMR parsing have been explored, including graph-based models (Flanigan et al., 2014; Werling et al., 2015; Lyu and Titov, 2018; Zhang et al., 2019a), transitionbased models (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017), grammar-based modTable 1: Scores with standard deviation on the AMR els (Peng et al., 2015; Artzi et al., 2015; Groschwitz 2.0 test set. digits. The columns ’R’ indicate if handet al., 2018; Lindemann et al., 2020) and neural crafted rules are used for segmentation, ♦ indicates that autoregressive models (Konstas et al., 2017; van the system used specialized pretraining or self-training. Noord and Bos, 2017; Zhang et al., 2019b; Cai and Our results"
2021.emnlp-main.714,W18-2501,0,0.0242931,"Missing"
2021.emnlp-main.714,W17-6810,0,0.0189296,"system used specialized pretraining or self-training. Noord and Bos, 2017; Zhang et al., 2019b; Cai and Our results are averaged over 4 runs. Lam, 2020; Xu et al., 2020b). The majority of strong parsers rely on explicit Metric Concept SRL Smatch graph segmentation in training. Typically, the seggreedy 87.0 71.5 74.8 mentation is dealt with hand-crafted rules, with rule 88.0 72.6 75.8 rule templates developed by studying training set full 87.8 72.9 75.6 statistics and ensuring the necessary level of coverTable 2: AMR 3.0 test set, averaged over 2 runs. age. Alternatively, Artzi et al. (2015); Groschwitz et al. (2017, 2018); Lindemann et al. (2020); Peng Concept SRL Smatch et al. (2015) using existing grammar formalisms to nothing learned 81.7 62.6 61.9 segment the AMR graphs. Astudillo et al. (2020); segmentation learned 86.0 69.1 70.5 Lee et al. (2020) - while not not relying on graph alignment learned 87.6 71.1 74.4 recategorization rules - use a rule system to ‘pack’ full (all learned) 88.3 73.0 76.1 and ‘unpack’ nodes. In recent work, strong results were obtained without using any explicit segmentaTable 3: Scores with different versions of latent segtion and alignment, relying on sequence-sequence me"
2021.emnlp-main.714,P18-1170,0,0.0597585,"cle. They rely on a prerooted labeled directed acyclic graphs. The rep- learned aligner (e.g., (Pourdamghani et al., 2014)) resentations have been exploited in a wide range to produce the alignment, and apply a rule sysof tasks, including text summarization (Liu et al., tem to segment the AMR subgraph (Flanigan et al., 2015; Dohare and Karnick, 2017; Hardy and Vla- 2014; Werling et al., 2015; Damonte et al., 2017; chos, 2018), machine translation (Jones et al., 2012; Ballesteros and Al-Onaizan, 2017; Peng et al., Song et al., 2019), paraphrase detection (Issa et al., 2015; Artzi et al., 2015; Groschwitz et al., 2018). 2018) and question answering (Mitra and Baral, While Lyu and Titov (2018) jointly optimize the 2016). parser and the alignment model, the rules handling An AMR graph can be regarded as consisting specific constructions still needed to be crafted to of multiple concept subgraphs, which can be indi- segment the graph. The segmentation rules are relvidually aligned to sentence tokens (Flanigan et al., atively complex — e.g., the rules of Lyu and Titov 2014). In Figure 1, each dashed box represents (2018) targeted 40 different AMR subgraph types — the boundary of a single semantic subgraph. Red"
2021.emnlp-main.714,D18-1086,0,0.480202,"Missing"
2021.emnlp-main.714,N18-1041,1,0.871721,"Missing"
2021.emnlp-main.714,P17-1014,0,0.153553,"aph-based parser consists of two parts: concept identification and relation identification. The concept identification model generates the AMR nodes, and the relation identification component decides on the labeled edges. During training, both components rely on latent alignment and segmentation, which are being induced simultaneously. Importantly, at test time, the parser simply tags the input with the subgraphs and predicts the relations, so there is no test-time overhead from using the latent-structure apparatus. An extra benefit of this approach, in contrast to encoder-decoder AMR models (Konstas et al., 2017; van Noord and Bos, 2017; Cai and Lam, 2020) is its transparency, as one can readily see which input token triggers each subgraph.1 To develop our parser, we frame the alignment and segmentation problems as choosing a generation order of concept nodes, as we explain in Section 2.2. As marginalization over the latent generation orders is infeasible, we adopt the variational auto-encoder (VAE) framework (Kingma and Welling, 2014). Intuitively, a trainable neural module (an encoder in the VAE) is used to sample a plausible generation order (i.e., a segmentation plus an alignment), which is then"
2021.emnlp-main.714,2020.emnlp-main.323,0,0.628777,"n system of Lyu and Titov (2018), or, more specifically, its re-implementation by Zhang et al. (2019a). Arguably, this can be thought of as an upper bound for how well an induction method can do. This fixed segmentation can be incorporated into our latent-generation-order framework, so that the alignment between concept nodes and the tokens will still be induced. This is achieved by fixing S, while still inducing A. details about the strategy. Results In Table 1, we compare our models with recent AMR parsers (Xu et al., 2020a; Cai and Lam, 2020, 2019; Zhang et al., 2019a; Naseem et al., 2019; Lindemann et al., 2020; Lee et al., 2020), as well as (Lyu and Titov, 2018), which we build on, and (van Noord and Bos, 2017), the earliest model which does not exploit any rules. Overall, our model (‘full’) performs competitively, but lags behind scores reported by some of the very recent parsers.7 However, except for a no-rule version of Cai and Lam (2020), all these models either use rules (Lee et al., 2020) (see Section 7) or specialized pretraining (Xu et al., 2020a). Both our VAE model and the rule-based segmentation achieve high concept identification scores (Damonte et al., 2017). The relation identificatio"
2021.emnlp-main.714,N15-1114,0,0.0647233,"Missing"
2021.emnlp-main.714,Q18-1005,0,0.0244773,"ying on sequence-sequence mentation on the AMR 2.0 test set, averaged over 2 models (Xu et al., 2020b; Cai and Lam, 2020), still runs the rules appear useful even with these strong modmethod is able to induce relatively accurate align- els (Cai and Lam, 2020). ments, and joint induction of alignments with segMore generally, outside of AMR parsing, difmentation may be beneficial, or, at the very least, ferentiable relaxations of latent structure reprenot detrimental to alignment quality. sentations have received attention in NLP (Kim Ablations To reconfirm that it is important to et al., 2017; Liu and Lapata, 2018), including prelearn the segmentation and alignment, rather than vious applications of the perturb-and-MAP frameto sample it randomly, we perform further ablations. work (Corro and Titov, 2019). From a more genIn our parameterization, discussed in Section 4.2.2, eral goal perspective – inducing a segmentation it is possible to set Araw = 0 and/or Sraw = 0, of a linguistic structure – our work is related to which corresponds to sampling from the prior in tree-substitution grammar induction (Sima’an et al., training (i.e. quasi-uniformly while respecting the 1995; Cohn et al., 2010), the DOP par"
2021.emnlp-main.714,2021.ccl-1.108,0,0.0234037,"Missing"
2021.emnlp-main.714,D19-1099,1,0.882105,"Missing"
2021.emnlp-main.714,P18-1037,1,0.377359,". In contrast, we treat both alignment and segmentation as latent variables in our model and induce them as part of end-to-end training. As marginalizing over the structured latent variables is infeasible, we use the variational autoencoding framework. To ensure end-to-end differentiable optimization, we introduce a differentiable relaxation of the segmentation and alignment problems. We observe that inducing segmentation yields substantial gains over using a ‘greedy’ segmentation heuristic. The performance of our method also approaches that of a model that relies on the segmentation rules of Lyu and Titov (2018), which were hand-crafted to handle individual AMR constructions. 1 Introduction 1 thing 0 The ARG1 opinion 0 opine-01 1 of 2 ARG0 2 boy the 3 4 boy Figure 1: An example of AMR, the dashed red arrows mark latent alignment. Dashed blue boxes represent the latent segmentation. and tokens. For example, ‘(o / opine-01: ARG1 (t / thing))’ refers to a combination of the predicate ‘opine-01’ and a filler of its semantic role ARG1. Intuitively, this subgraph needs to be aligned to the token ‘opinion’. Similarly, ‘(b / boy)’ should be aligned to the token ‘boy’. Given such an alignment and segmentation"
2021.emnlp-main.714,K15-1004,0,0.0190254,"ull model performs the best, demonstrating that it is important to learn both alignments and segmentation. Interestingly, both ‘segmentation learned’ and ‘alignment learned’ obtain reasonable performance, but the ‘nothing learned’ model fails badly. 7 Related Work A wide range of approaches for AMR parsing have been explored, including graph-based models (Flanigan et al., 2014; Werling et al., 2015; Lyu and Titov, 2018; Zhang et al., 2019a), transitionbased models (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017), grammar-based modTable 1: Scores with standard deviation on the AMR els (Peng et al., 2015; Artzi et al., 2015; Groschwitz 2.0 test set. digits. The columns ’R’ indicate if handet al., 2018; Lindemann et al., 2020) and neural crafted rules are used for segmentation, ♦ indicates that autoregressive models (Konstas et al., 2017; van the system used specialized pretraining or self-training. Noord and Bos, 2017; Zhang et al., 2019b; Cai and Our results are averaged over 4 runs. Lam, 2020; Xu et al., 2020b). The majority of strong parsers rely on explicit Metric Concept SRL Smatch graph segmentation in training. Typically, the seggreedy 87.0 71.5 74.8 mentation is dealt with hand-crafte"
2021.emnlp-main.714,D14-1162,0,0.0861259,"Missing"
2021.emnlp-main.714,2020.lrec-1.362,0,0.0353183,"the alignment between subgraphs tended to be used as an interlingua (Banarescu et al., 9075 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9075–9091 c November 7–11, 2021. 2021 Association for Computational Linguistics 2013; Damonte and Cohen, 2018) and AMR banks for individual languages substantially diverge from English AMR. For example, Spanish AMR represents pronouns and ellipsis differently from the English one (Migueles-Abraira et al., 2018). As new AMR sembanks in languages other than English are being developed (Anchiêta and Pardo, 2018; Song et al., 2020), domain-specific AMR extensions get developed (Bonn et al., 2020; Bonial et al., 2020), and extra constructions are getting introduced to AMRs (Bonial et al., 2018), eliminating the need for rules while learning graph segmentation from scratch is becoming an important problem to solve. We propose to optimize a graph-based parser that treats the alignment and graph segmentation as latent variables. The graph-based parser consists of two parts: concept identification and relation identification. The concept identification model generates the AMR nodes, and the relation identification component"
2021.emnlp-main.714,Q19-1002,0,0.0543064,"ne approach formalism which represents sentence meaning as to handling this obstacle. They rely on a prerooted labeled directed acyclic graphs. The rep- learned aligner (e.g., (Pourdamghani et al., 2014)) resentations have been exploited in a wide range to produce the alignment, and apply a rule sysof tasks, including text summarization (Liu et al., tem to segment the AMR subgraph (Flanigan et al., 2015; Dohare and Karnick, 2017; Hardy and Vla- 2014; Werling et al., 2015; Damonte et al., 2017; chos, 2018), machine translation (Jones et al., 2012; Ballesteros and Al-Onaizan, 2017; Peng et al., Song et al., 2019), paraphrase detection (Issa et al., 2015; Artzi et al., 2015; Groschwitz et al., 2018). 2018) and question answering (Mitra and Baral, While Lyu and Titov (2018) jointly optimize the 2016). parser and the alignment model, the rules handling An AMR graph can be regarded as consisting specific constructions still needed to be crafted to of multiple concept subgraphs, which can be indi- segment the graph. The segmentation rules are relvidually aligned to sentence tokens (Flanigan et al., atively complex — e.g., the rules of Lyu and Titov 2014). In Figure 1, each dashed box represents (2018) targ"
2021.emnlp-main.714,2020.findings-emnlp.199,1,0.83745,"Missing"
2021.emnlp-main.743,P18-1063,0,0.0193774,"spark of interest in unsupervised abstractive opinion summarization. Such models include M EAN S UM (Chu and Liu, 2019), C OPYCAT (Bražinskas et al., 2020b), D E An alternative to review subsets selection are NOISE S UM (Amplayo and Lapata, 2020), O PIN more memory and computationally efficient attenION D IGEST (Suhara et al., 2020), and C ONDA tion mechanisms (Beltagy et al., 2020; Pasunuru S UM (Amplayo and Lapata, 2021b). et al., 2021). However, it is unclear what relationOur work is related to the extractive-abstractive ship exists between attention weights and model summarization model (Chen and Bansal, 2018) that outputs (Jain and Wallace, 2019), thus, making it selects salient sentences from an input document harder to offer evidence for generated summaries. using reinforcement learning. They assume one-to- In our case, the summarizer relies only on a seone mapping between extracted and summary sen- lected subset and generates summaries faithful to 9431 its content. In general, in news summarization, which is a more mature branch, large datasets are commonly obtained from online resources (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018; Narayan et al., 2018; Fabbri et al., 2019). The"
2021.emnlp-main.743,W14-4408,0,0.0606172,"Missing"
2021.emnlp-main.743,P19-1102,0,0.0192641,"el (Chen and Bansal, 2018) that outputs (Jain and Wallace, 2019), thus, making it selects salient sentences from an input document harder to offer evidence for generated summaries. using reinforcement learning. They assume one-to- In our case, the summarizer relies only on a seone mapping between extracted and summary sen- lected subset and generates summaries faithful to 9431 its content. In general, in news summarization, which is a more mature branch, large datasets are commonly obtained from online resources (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018; Narayan et al., 2018; Fabbri et al., 2019). The most relevant dataset is M ULTI N EWS Fabbri et al. (2019), where journalist-written summaries are linked to multiple news articles. The most similar opinion summarization dataset S PACE (Angelidis et al., 2020) contains 1050 summaries produced for 50 hotels by crowdsourcing. 7 Conclusions In this work, we introduce the largest multidocument abstractive dataset for opinion summarization. The dataset consists of verdicts, pros and cons, written by professional writers for more than 31,000 Amazon products. Each product is linked to more than 320 customer reviews, on average. As standard en"
2021.emnlp-main.743,2020.emnlp-main.751,0,0.0394514,"Missing"
2021.emnlp-main.743,W18-2706,0,0.0207669,"tails can be found in Appendix 8.4. Hardware All experiments were conducted on 4 x GeForce RTX 2080 Ti. 5 5.1 Results Automatic Evaluation The results in Table 4 suggest that the supervised models substantially outperform the unsupervised ones. Also, all supervised abstractive summarizers outperform E XT S UM, suggesting recombining information from reviews into fluent text is beneficial. Among the summarizers with the review selectors, S EL S UM yields the best results on verdicts and cons. Although, we noticed that S EL S UM generates shorter pros than R1 TOP - K, which may harm its scores (Fan et al., 2018)5 . Further, when random reviews were used both in training and testing (R AND S EL), the results are substantially lower. On the other hand, when review subsets were produced by S EL S UM and summarized by R AND S EL (marked with ‘*’), we observed a substantial increase in all the scores. This suggests the importance of deliberate review selection in test time. In general, all models yield higher scores on pros than cons, which is expected as most reviews are positive (on average 4.32/5) and it is harder for the model to find negative points in input reviews. 5.2 Content Support Generating in"
2021.emnlp-main.743,C10-1039,0,0.0583512,"estimated the mutual information (MI) (Kraskov et al., 2004; Ross, 2014) between the posterior input features and the binary decision to select a review, as in Sec. 3.3. We found that besides review-vssummary ROUGE-1 and -2 scores, the posterior uses fine-grained aspect features, and review-vs-allreviews ROUGE scores (quantifying the uniqueness of each review). See also Appendix 8.7. 6 Related Work Due to a lack of annotated data, extractive weaklysupervised opinion summarization has been the dominant paradigm. L EX R ANK (Erkan and Radev, 2004) is an unsupervised extractive model. O PINOSIS (Ganesan et al., 2010) does not use any supervision and relies on POS tags and redundancies to generate short opinions. Although, it can 5 recombine fragments of input text, it cannot genR1 TOP - K and S EL S UM generate 31.95 and 27.14 words on average, respectively. erate novel words and phrases and thus produce 9430 O RACLE R ANDOM L EX R ANK M EAN S UM C OPYCAT E XT S UM R AND S EL R AND S EL * R1 TOP - K S EL S UM R1 38.14 13.12 15.12 13.78 17.05 18.74 23.25 23.95 23.43 24.33 Verdict R2 11.76 0.82 1.84 0.93 1.78 3.01 4.75 5.16 4.94 5.29 RL 31.50 10.85 12.60 11.70 14.50 15.74 17.82 18.49 18.52 18.84 R1 37.22 14"
2021.emnlp-main.743,D18-1443,0,0.0560584,"Missing"
2021.emnlp-main.743,D14-1168,0,0.0273338,"g a differentiable loss. Also, our model is related to the unsupervised paraphrasing MARGE model (Lewis et al., 2020a), where the decoder has a modified attention mechanism accounting for the target-source document similarity. However, in their approach, the actual selection of relevant documents is performed offline via heuristics. This, in turn, makes it non-differentiable and over-reliant on the modified attention mechanism. We, however, learn the selector (posterior) jointly with summarizer, and select reviews in the online regime. coherent abstractive summaries. Other earlier approaches (Gerani et al., 2014; Di Fabbrizio et al., 2014) relied on text planners and templates, which restrict the output text. A more recent method of Angelidis and Lapata (2018) applies multiple specialized models to produce extractive summaries. More recently, there has been a spark of interest in unsupervised abstractive opinion summarization. Such models include M EAN S UM (Chu and Liu, 2019), C OPYCAT (Bražinskas et al., 2020b), D E An alternative to review subsets selection are NOISE S UM (Amplayo and Lapata, 2020), O PIN more memory and computationally efficient attenION D IGEST (Suhara et al., 2020), and C ONDA"
2021.emnlp-main.743,N18-1065,0,0.0479485,"Missing"
2021.emnlp-main.743,2020.acl-main.703,0,0.159914,"AND S EL). Here, review subsets were re-sampled at each training epoch. ROUGE-1 top-k We produced review subsets based on review-summary ROUGE-1 R scores (R1 TOP - K ) for training.3 Specifically, we computed the scores for each pair, and then selected K reviews with highest scores to form the subset. To select reviews in test time, we trained a selector as in Sec. 3.3. 4.4 Experimental details Below we briefly describe model details; more information can be found in Appendix 8.5. Summarizer We used the Transformer encoderdecoder architecture (Vaswani et al., 2017) initialized with base BART (Lewis et al., 2020b), 140M parameters in total. Reviews were independently encoded and concatenated states of product reviews were attended by the decoder to predict the summary as in Bražinskas et al. (2020a). We used ROUGE-L as the stopping criterion. Summary generation was performed via the beam search of size 5 and with 3-gram blocking (Paulus et al., 2017). Posterior For the inference network in Sec. 3.2.1, we used a simple non-linear two-layer feed-forward network with 250 hidden dimensions. The model consisted of 95k parameters. The network inputs 23 pre-computed features. For instance, ROUGE-1 and -2 sc"
2021.emnlp-main.743,2020.acl-main.45,0,0.0629,"Missing"
2021.emnlp-main.743,W04-1013,0,0.0205053,"esigned for pros and cons generation. Therefore, we used a separately trained classifier to split each summary to pros and cons. Extractive summarizer We used a pre-trained BART encoder, and 100 hidden states for 1 layer score feed-forward network with ReLU, and 0.1 dropout. The contextualizer had one layer, and the final score feed-forward had 100 hidden dimensions, 0.1 dropout, with layer normalization before logits are computed. We trained the model for 5 epochs, with 1e-05 learning rate. Automatic evaluation We separately evaluated verdicts, pros, and cons with the standard ROUGE package (Lin, 2004)4 , and report F1 scores. Human evaluation To assess content support, we randomly sampled 50 products, generated summaries, and hired 3 workers on Amazon Mechanical Turk (AMT) for each HIT. To ensure high qual3 We tried but were not able to obtain better results by turning the scores into a distribution and sampling from it, so we used the deterministic strategy in the main experiments. 4 We used a wrapper over the package https:// github.com/pltrdy/files2rouge. 9429 ity submissions, we used qualification tasks and filters. More details can be found in Appendix 8.4. Hardware All experiments we"
2021.emnlp-main.743,D19-1387,1,0.851138,"rvised abstractive summarization model which treats a summary as a structured latent state of an autoencoder trained to reconstruct reviews of a product. C OPYCAT (Bražinskas et al., 2020b) is the state-of-the-art unsupervised abstractive summarizer with hierarchical continuous latent representations to model products and individual reviews. R ANDOM: here we split all N reviews by sentences, and randomly selected 3, 7, 4 sentences for verdicts, pros, and cons, respectively. E XT S UM: we created an extractive summarizer trained on our data. First, we used the same ROUGE greedy heuristic as in Liu and Lapata (2019) to sequentially select summarizing verdict, pro, and con sentences from the full set of reviews using the actual gold summary (O RACLE). Further, we trained a model, with the same architecture as the prior in Sec. 3.3, to predict sentence classes. More details can be found in Appendix 8.2. 4.3 Alternative Review Selectors To better understand the role of review selection, we trained the same encoder-decoder summarizer as in S EL S UM but with two alternative selectors. Random reviews We trained and tested on random review subsets (R AND S EL). Here, review subsets were re-sampled at each trai"
2021.emnlp-main.743,2020.acl-main.173,0,0.0807035,". via Monte Carlo (MC). log E rˆ1:K ∼p(ˆ r1:K |r1:N ) E rˆ1:K ∼p(ˆ r1:K |r1:N ) [pθ (s|ˆ r1:K )] ≥ [log pθ (s|ˆ r1:K )] (2) Here the latent subset rˆ1:K is sampled from a prior categorical distribution agnostic of the summary. From the theoretical perspective, it can lead to a large gap between the log-likelihood and the lower bound, contributing to poor performance (Deng et al., 2018). From the practical perspective, it can result in the input reviews not covering the summary content, thus forcing the decoder in training to predict ‘novel’ content. Consequently, this leads to hallucinations (Maynez et al., 2020) in test time, as we empirically demonstrate in Sec. 5.2. 3.2 Model 3.2.1 To address the previously mentioned problems, we leverage amortized inference reducing the gap (Kingma and Welling, 2013; Cremer et al., 2018). And re-formulate the lower bound using weighted sampling as shown in Eq. 3. log view subsets selected by the approximate posterior qφ (ˆ r1:K |r1:N , s). Unlike the prior, it selects reviews relevant to the summary s, thus providing a better content coverage of the summary. Hence, it reduces the amount of ‘novel’ content the decoder needs to predict. As we empirically demonstrate"
2021.emnlp-main.743,K16-1028,0,0.0612886,"Missing"
2021.emnlp-main.743,D18-1206,1,0.837831,"odel summarization model (Chen and Bansal, 2018) that outputs (Jain and Wallace, 2019), thus, making it selects salient sentences from an input document harder to offer evidence for generated summaries. using reinforcement learning. They assume one-to- In our case, the summarizer relies only on a seone mapping between extracted and summary sen- lected subset and generates summaries faithful to 9431 its content. In general, in news summarization, which is a more mature branch, large datasets are commonly obtained from online resources (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018; Narayan et al., 2018; Fabbri et al., 2019). The most relevant dataset is M ULTI N EWS Fabbri et al. (2019), where journalist-written summaries are linked to multiple news articles. The most similar opinion summarization dataset S PACE (Angelidis et al., 2020) contains 1050 summaries produced for 50 hotels by crowdsourcing. 7 Conclusions In this work, we introduce the largest multidocument abstractive dataset for opinion summarization. The dataset consists of verdicts, pros and cons, written by professional writers for more than 31,000 Amazon products. Each product is linked to more than 320 customer reviews, on a"
2021.emnlp-main.743,D19-1018,0,0.0488964,"Missing"
2021.emnlp-main.743,2021.naacl-main.380,0,0.0407238,"Missing"
2021.emnlp-main.743,E17-2025,0,0.0486887,"Missing"
2021.findings-acl.207,D18-1316,0,0.0174526,"n return x(t) else return None end if This helps to determine the word substituting order in the proposed method. In this work, we use a combination of the changes found in the unlabelled attachment score (UAS) and in the labelled attachment score (LAS) to measure word importance. Specifically, the importance of a word xi in sentence x is computed as 2.3.1 Generating Process We collect substitutes from the following methods: BERT-Based Method: We use BERT to generate candidates for each target word from its context. This method generates only single subwords. Embedding-Based Method: Following Alzantot et al. (2018), we use word embeddings of Mrkˇsi´c et al. (2016)2 to compute the N nearest neighbours of each target word according to their cosine similarity and use them as candidates. Sememe-Based Method: The sememes of a word represent its core meaning (Dong and Dong, 2006). Following Zang et al. (2020), we collect the substitutes of the target word x based on the rule that one of the substitutes the senses of x∗ must have the same sememe annotations as one of senses of x. Synonym-Based Method: We use WordNet3 to extract synonyms of each target word as candidates. 2.3.2 Filtering Process Generation of S"
2021.findings-acl.207,N19-1423,0,0.0124486,"Preserving the syntactic structure enables us to use the gold syntactic structure of the original sentence in the evaluation process. While preserving the fluency ensures that ungrammatical adversarial examples, which not only fool the target model but also confuse humans, will not be considered valid. Therefore in this paper, we evaluate the quality of an adversarial example in two aspects, namely the fluency and syntactic structure preservation. Recently, Zheng et al. (2020) proposed the first dependency parser attacking algorithm based on word-substitution which depended entirely on BERT (Devlin et al., 2019) to generate candidate substitutes. The rational was that the use of the pre-trained language model will ensure fluency of the adversarial examples. However, we find that using BERT alone is far from enough to preserve fluency. Therefore, in this paper, we propose a method to generate better adversarial examples for dependency parsing with four types of candidate generators and filters. Specifically, our method consists of three steps: (i) determining the substitution order, (ii) generating and filtering candidate substitutes for each word, (iii) searching for the best possible combination of"
2021.findings-acl.207,2020.emnlp-main.182,0,0.356771,"operties of adversarial attacks. We find that (i) the introduction of out-of-vocabulary (OOV, words not in the embedding’s vocabulary) and out-of-training (OOT, words not in the training set of the parser) words in adversarial examples are two main factors that harm models’ performance; (ii) adversarial examples generated against a parser strongly depend on the type of the parser, the token embeddings and even the random seed. Adversarial training (Goodfellow et al., 2015), where adversarial examples are added in the training stage, has been commonly used in previous work (Zheng et al., 2020; Han et al., 2020) to improve a parser’s robustness. Only a limited number of adversarial examples have been used in such cases, and Zheng et al. (2020) argued that overuse of them may lead to a performance drop on the clean data. However, we show that with improvement in the quality of adversarial examples produced in our method, more adversarial examples can be used in the training stage to further improve the parsing models’ robustness without producing any apparent harm in their performance on the clean data. Inspired by our second finding, we propose to improve the parsers’ robustness by combining models t"
2021.findings-acl.207,2020.iwpt-1.7,1,0.754643,"or, and we propose to generate better examples by using more generators and stricter filters. Han et al. (2020) proposed an approach to attack structured prediction models with a seq2seq model (Wang et al., 2016) and evaluated this model on dependency parsing. They used two reference parsers in addition to the victim parser to supervise the training of the adversarial example generator, and found that the three parsers produce better results when they have different inductive biases embedded to make the attack successful. This finding is quite close in spirit to our conclusion in Section 4.5. Hu et al. (2020) also put forth efforts to modify the text in syntactic tasks while preserving the original syntactic structure. However, their goal is to preserve privacy via the modification of words that could disclose sensitive information. 6 Conclusion In this paper, we propose a method for generating high-quality adversarial examples for dependency parsing and show its effectiveness based on automatic and human evaluation. We investigate This work was supported by the National Key R&D Program of China via grant 2020AAA0106501 and the National Natural Science Foundation of China (NSFC) via grant 61976072"
2021.findings-acl.207,N16-1082,0,0.0336606,"ii) the true dependency tree of x∗ should be the same as that of x. In this paper, these two constraints are ensured through the use of various filters (see Section 2.3) and are used to evaluate the quality of adversarial examples (see details on fluency and syntactic structure preservation in Section 3.3). 2.2 Word Importance Ranking Word importance ranking in our model is based on the observation that some words have a stronger influence on model prediction than others. Such word importance is typically computed by setting each word to unknown and examining the changes in their predictions (Li et al., 2016; Ren et al., 2019). 2345 Algorithm 1 Dependency Parsing Attack generation methods, then apply filters to discard inappropriate substitutes, ensuring both diversity and quality of the generated candidates. Input: Sentence example x(0) = {x1 , x2 , . . . , xN }, maximum percentage of words allowed to be modified γ Output: Adversarial example x(i) 1: for i = 1 to N do 2: Compute word importance I(x(0) , xi ) via Eq. 1 3: end for 4: Create a set W of all words xi ∈ x(0) sorted by the descending order of their importance I(x(0) , xi ). 5: t = 0 6: for each word xj in W do 7: Build candidate set Cj"
2021.findings-acl.207,2021.ccl-1.108,0,0.0377807,"Missing"
2021.findings-acl.207,P18-1130,0,0.0126623,"rds in the sentence exceeds a threshold γ, we stop the process. Otherwise, we search for a substitute for the next target word. 3 3.1 Experimental Setup Target Parsers and Token Embeddings We choose the following two strong and commonly used English parsers, one graph-based, the other transition-based, as target models, both of which achieve performance close to the state-of-the-art. Deep Biaffine Parser (Dozat and Manning, 2017) is a graph-based parser that scores each candidate arc independently and relies on a decoding algorithm to search for the highest-scoring tree. Stack-Pointer Parser (Ma et al., 2018) is a transition-based parser that incrementally builds the dependency tree with pre-defined operations. We used the following four types of token embeddings to study their influence on each parsers’ robustness. To focus on the influence of the embeddings, we use only the embeddings as input to the parsers: GloVe (Pennington et al., 2014) is a frequently used static word embedding. RoBERTa (Liu et al., 2019) is a pre-trained language model based on a masked language modelling object, which learns to predict a randomly masked token based on its context. It produces contextualised word piece emb"
2021.findings-acl.207,de-marneffe-etal-2006-generating,0,0.0517672,"Missing"
2021.findings-acl.207,2020.findings-emnlp.341,0,0.0200122,"model (i.e., a random seed). We use these insights to improve the robustness of English parsing models, relying on adversarial training and model ensembling.1 1 Introduction Neural network-based models have achieved great successes in a wide range of NLP tasks. However, recent work has shown that their performance can be easily undermined with adversarial examples that would pose no confusion for humans (Zhang et al., 2020). As an increasing number of successful adversarial attackers have been developed for NLP tasks, the quality of the adversarial examples they generate has been questioned (Morris et al., 2020). The definition of a valid successful adversarial example differs across target tasks. In semantic tasks such as sentiment analysis (Zhang et al., 2019) and textual entailment (Jin et al., 2020), a valid successful adversarial example needs to be able to alter the prediction of the target model while ∗ Work partially done while at the University of Edinburgh. 1 Our code is available at: https://github.com/ WangYuxuan93/DepAttacker.git preserving the semantic content and fluency of the original text. In contrast, in the less explored field of attacking syntactic tasks, the syntactic structure,"
2021.findings-acl.207,N16-1018,0,0.0609948,"Missing"
2021.findings-acl.207,D14-1162,0,0.0855441,"ich achieve performance close to the state-of-the-art. Deep Biaffine Parser (Dozat and Manning, 2017) is a graph-based parser that scores each candidate arc independently and relies on a decoding algorithm to search for the highest-scoring tree. Stack-Pointer Parser (Ma et al., 2018) is a transition-based parser that incrementally builds the dependency tree with pre-defined operations. We used the following four types of token embeddings to study their influence on each parsers’ robustness. To focus on the influence of the embeddings, we use only the embeddings as input to the parsers: GloVe (Pennington et al., 2014) is a frequently used static word embedding. RoBERTa (Liu et al., 2019) is a pre-trained language model based on a masked language modelling object, which learns to predict a randomly masked token based on its context. It produces contextualised word piece embeddings. ELECTRA (Clark et al., 2020) is a pre-trained language model based on a replaced token detection object, which learns to predict whether each token in the corrupted input has been replaced. It produces contextualised word piece embeddings. ELMo (Peters et al., 2018) is a pre-trained language representation model based on characte"
2021.findings-acl.207,N18-1202,0,0.0174184,", we use only the embeddings as input to the parsers: GloVe (Pennington et al., 2014) is a frequently used static word embedding. RoBERTa (Liu et al., 2019) is a pre-trained language model based on a masked language modelling object, which learns to predict a randomly masked token based on its context. It produces contextualised word piece embeddings. ELECTRA (Clark et al., 2020) is a pre-trained language model based on a replaced token detection object, which learns to predict whether each token in the corrupted input has been replaced. It produces contextualised word piece embeddings. ELMo (Peters et al., 2018) is a pre-trained language representation model based on character embeddings and bidirectional language modelling. 3.2 Datasets and Experimental Settings We train the target parsers and evaluate the proposed method on the English Penn Treebank (PTB) dataset,7 converted into Stanford dependencies using version 3.3.0 of the Stanford dependency converter (de Marneffe et al., 2006) (PTB-SD-3.3.0). We follow the standard PTB split, using section 2-21 for training, section 22 as a development set and 23 as a test set. It is important to note that when converting PTB into Stanford dependencies, Zhen"
2021.findings-acl.207,P19-1103,1,0.805871,"ndency tree of x∗ should be the same as that of x. In this paper, these two constraints are ensured through the use of various filters (see Section 2.3) and are used to evaluate the quality of adversarial examples (see details on fluency and syntactic structure preservation in Section 3.3). 2.2 Word Importance Ranking Word importance ranking in our model is based on the observation that some words have a stronger influence on model prediction than others. Such word importance is typically computed by setting each word to unknown and examining the changes in their predictions (Li et al., 2016; Ren et al., 2019). 2345 Algorithm 1 Dependency Parsing Attack generation methods, then apply filters to discard inappropriate substitutes, ensuring both diversity and quality of the generated candidates. Input: Sentence example x(0) = {x1 , x2 , . . . , xN }, maximum percentage of words allowed to be modified γ Output: Adversarial example x(i) 1: for i = 1 to N do 2: Compute word importance I(x(0) , xi ) via Eq. 1 3: end for 4: Create a set W of all words xi ∈ x(0) sorted by the descending order of their importance I(x(0) , xi ). 5: t = 0 6: for each word xj in W do 7: Build candidate set Cj for xj following t"
2021.findings-acl.207,D16-1058,0,0.0368721,"Morris et al. (2020) reported that quite a number of these techniques introduce grammatical errors. In syntactic tasks, Zheng et al. (2020) recently proposed the first dependency parser attacking method which depends entirely on BERT to generate candidates. However, we show that the quality of adversarial examples generated by their method is relatively low due to the limitation of the BERTbased generator, and we propose to generate better examples by using more generators and stricter filters. Han et al. (2020) proposed an approach to attack structured prediction models with a seq2seq model (Wang et al., 2016) and evaluated this model on dependency parsing. They used two reference parsers in addition to the victim parser to supervise the training of the adversarial example generator, and found that the three parsers produce better results when they have different inductive biases embedded to make the attack successful. This finding is quite close in spirit to our conclusion in Section 4.5. Hu et al. (2020) also put forth efforts to modify the text in syntactic tasks while preserving the original syntactic structure. However, their goal is to preserve privacy via the modification of words that could"
2021.findings-acl.207,2020.acl-main.540,0,0.0323383,"e importance of a word xi in sentence x is computed as 2.3.1 Generating Process We collect substitutes from the following methods: BERT-Based Method: We use BERT to generate candidates for each target word from its context. This method generates only single subwords. Embedding-Based Method: Following Alzantot et al. (2018), we use word embeddings of Mrkˇsi´c et al. (2016)2 to compute the N nearest neighbours of each target word according to their cosine similarity and use them as candidates. Sememe-Based Method: The sememes of a word represent its core meaning (Dong and Dong, 2006). Following Zang et al. (2020), we collect the substitutes of the target word x based on the rule that one of the substitutes the senses of x∗ must have the same sememe annotations as one of senses of x. Synonym-Based Method: We use WordNet3 to extract synonyms of each target word as candidates. 2.3.2 Filtering Process Generation of Substitute Candidates We apply the following four types of filters to discard candidates which are likely inappropriate, either in terms of syntactic preservation or fluency. POS Filter: We first filter out substitutes with different part-of-speech (POS) tags from the original word.4 This filte"
2021.findings-acl.207,P19-1559,0,0.0216978,"ing.1 1 Introduction Neural network-based models have achieved great successes in a wide range of NLP tasks. However, recent work has shown that their performance can be easily undermined with adversarial examples that would pose no confusion for humans (Zhang et al., 2020). As an increasing number of successful adversarial attackers have been developed for NLP tasks, the quality of the adversarial examples they generate has been questioned (Morris et al., 2020). The definition of a valid successful adversarial example differs across target tasks. In semantic tasks such as sentiment analysis (Zhang et al., 2019) and textual entailment (Jin et al., 2020), a valid successful adversarial example needs to be able to alter the prediction of the target model while ∗ Work partially done while at the University of Edinburgh. 1 Our code is available at: https://github.com/ WangYuxuan93/DepAttacker.git preserving the semantic content and fluency of the original text. In contrast, in the less explored field of attacking syntactic tasks, the syntactic structure, rather than the semantic content, must be preserved while also maintaining the fluency. Preserving the syntactic structure enables us to use the gold sy"
2021.findings-acl.207,2020.acl-main.590,0,0.151193,"tacking syntactic tasks, the syntactic structure, rather than the semantic content, must be preserved while also maintaining the fluency. Preserving the syntactic structure enables us to use the gold syntactic structure of the original sentence in the evaluation process. While preserving the fluency ensures that ungrammatical adversarial examples, which not only fool the target model but also confuse humans, will not be considered valid. Therefore in this paper, we evaluate the quality of an adversarial example in two aspects, namely the fluency and syntactic structure preservation. Recently, Zheng et al. (2020) proposed the first dependency parser attacking algorithm based on word-substitution which depended entirely on BERT (Devlin et al., 2019) to generate candidate substitutes. The rational was that the use of the pre-trained language model will ensure fluency of the adversarial examples. However, we find that using BERT alone is far from enough to preserve fluency. Therefore, in this paper, we propose a method to generate better adversarial examples for dependency parsing with four types of candidate generators and filters. Specifically, our method consists of three steps: (i) determining the su"
2021.findings-acl.255,P19-1284,1,0.915029,"017). In this work, we hypothesize that encoder outputs are compressible and we can force Seq2Seq model to route information through their subset. Figure 2 illustrates our intuition as well as the difference with existing work (Vaswani et al., 2017; Correia et al., 2019). Instead of dynamically sparsifying attention weights for individual decoder steps (Correia et al., 2019), we aim at detecting uninformative source encodings and dropping them to shorten the encoding sequence before generation. To this end, we build on recent work on sparsifying weights (Louizos et al., 2018) and activations (Bastings et al., 2019) of neural networks. Specifically, we insert a differentiable neural sparsity layer (L0 D ROP) in-between the encoder and the decoder. The layer can be regarded as providing a multiplicative scalar gate for every encoder output. The gate is a random variable and, unlike standard attention, can be exactly zero, effectively masking out the corresponding source encodings. The sparsity is promoted by introducing an extra term to the learning objective, i.e. an expected value of the sparsity-inducing L0 penalty. By varying the coefficient for the regularizer, we can obtain different levels of spars"
2021.findings-acl.255,D18-1461,0,0.0148816,"ch differs from the linear-time model of Wang et al. (2019), although both can accelerate decoding. Our study of rule-based sparsity patterns is in line with the sparse Transformer (Child et al., 2019) though we also explore the use of external linguistic information (POS tag) in our sparsification rules, and focus on encoder outputs instead of self-attention. Character-based translation gained increasing popularity due to its capability of handling out-ofvocabulary issues while avoiding tokenization and subword segmentation (Ling et al., 2015; Costajuss`a and Fonollosa, 2016; Sennrich, 2017; Cherry et al., 2018). Recent efforts often focus on closing the performance gap against its subword-level counterpart (Libovick´y and Fraser, 2020; Gao et al., 2020), but little study explores solutions to improve its inefficient inference resulted from the long character sequences. In this respect, Cherry et al. (2018) proposed to use conditional computation to dynamically compress encoder states. Similar to our results, they also observed a trade-off between the translation quality and the degree of compression. 3 Background: Transformer We take Transformer (Vaswani et al., 2017) as our testbed. Transformer use"
2021.findings-acl.255,P16-2058,0,0.0496229,"Missing"
2021.findings-acl.255,D19-1361,0,0.0221361,"codings with rulebased sparse patterns is feasible, and confirms information-theoretic expectations, although rule-based patterns do not generalize well across tasks. 2 Related Work Approaches to compression in Seq2Seq models fall into the category of model parameter compression (See et al., 2016), sequential knowledge dis2889 tillation (Kim and Rush, 2016) or sparse attention induction that ranges from modeling hard attention (Wu et al., 2018) to developing differentiable sparse softmax functions or regularizing attention weights for sparsity (Niculae and Blondel, 2017; Correia et al., 2019; Cui et al., 2019; Zhang et al., 2019). Unfortunately, the success of all these studies builds upon the access to all source encodings in training and decoding. Learning which encoder outputs to prune in Seq2Seq models, to the best of our knowledge, has never been investigated before. Sukhbaatar et al. (2019) learn attention spans in self-attention and discard information from states outside of the span; this method is not directly applicable to encoder-decoder attention. We use the differentiable L0 -relaxation which was first introduced by Louizos et al. (2018) in the context of pruning individual neural net"
2021.findings-acl.255,2020.acl-main.145,0,0.0174848,"line with the sparse Transformer (Child et al., 2019) though we also explore the use of external linguistic information (POS tag) in our sparsification rules, and focus on encoder outputs instead of self-attention. Character-based translation gained increasing popularity due to its capability of handling out-ofvocabulary issues while avoiding tokenization and subword segmentation (Ling et al., 2015; Costajuss`a and Fonollosa, 2016; Sennrich, 2017; Cherry et al., 2018). Recent efforts often focus on closing the performance gap against its subword-level counterpart (Libovick´y and Fraser, 2020; Gao et al., 2020), but little study explores solutions to improve its inefficient inference resulted from the long character sequences. In this respect, Cherry et al. (2018) proposed to use conditional computation to dynamically compress encoder states. Similar to our results, they also observed a trade-off between the translation quality and the degree of compression. 3 Background: Transformer We take Transformer (Vaswani et al., 2017) as our testbed. Transformer uses the dot-product attention network as its backbone to handle intra- and intersequence dependencies:   QKT √ V, (1) ATT(H, M) = AV = S M d wher"
2021.findings-acl.255,D18-1443,0,0.063663,"Missing"
2021.findings-acl.255,D16-1139,0,0.0135276,", verbs and nouns). • L0 D ROP can improve decoding efficiency particularly for lengthy source inputs. We achieve a decoding speedup of up to 1.65× on document summarization tasks and 1.20× on character-based machine translation task. • Filtering out source encodings with rulebased sparse patterns is feasible, and confirms information-theoretic expectations, although rule-based patterns do not generalize well across tasks. 2 Related Work Approaches to compression in Seq2Seq models fall into the category of model parameter compression (See et al., 2016), sequential knowledge dis2889 tillation (Kim and Rush, 2016) or sparse attention induction that ranges from modeling hard attention (Wu et al., 2018) to developing differentiable sparse softmax functions or regularizing attention weights for sparsity (Niculae and Blondel, 2017; Correia et al., 2019; Cui et al., 2019; Zhang et al., 2019). Unfortunately, the success of all these studies builds upon the access to all source encodings in training and decoding. Learning which encoder outputs to prune in Seq2Seq models, to the best of our knowledge, has never been investigated before. Sukhbaatar et al. (2019) learn attention spans in self-attention and disca"
2021.findings-acl.255,D19-1223,0,0.188797,"Missing"
2021.findings-acl.255,Q17-1026,0,0.0281559,"tention weight, and the white blocks denote an attention weight of 0. The source words whose encoding is pruned by L0 D ROP (receiving zero weight) are highlighted in red. across words, for example, it is negatively correlated with event frequency (Shannon, 1948; Zipf, 1949). When moving from word-level to characterlevel processing, the notion of encoding information and computing attention on the level of characters also seems excessive. Previous work has proposed hierarchical architectures where characterlevel encodings are compressed into word-level or span-level states (Ling et al., 2015; Lee et al., 2017). In this work, we hypothesize that encoder outputs are compressible and we can force Seq2Seq model to route information through their subset. Figure 2 illustrates our intuition as well as the difference with existing work (Vaswani et al., 2017; Correia et al., 2019). Instead of dynamically sparsifying attention weights for individual decoder steps (Correia et al., 2019), we aim at detecting uninformative source encodings and dropping them to shorten the encoding sequence before generation. To this end, we build on recent work on sparsifying weights (Louizos et al., 2018) and activations (Bast"
2021.findings-acl.255,2020.emnlp-main.203,0,0.0279484,"Missing"
2021.findings-acl.255,W04-1013,0,0.117013,"rtened source sequence X across decoder layers and steps. L0 D ROP changes the dependency of the encoder-decoder attention on source sequence from O(N M ) to O(N 0 M ), and allows for efficiency gains even with moderate sparsity, especially for large L, N and M . 5 Experimental Setup We study L0 D ROP on machine translation tasks (WMT14 English-German (En-De) (Bojar et al., 2014) and WMT18 Chinese-English (Zh-En) (Bojar et al., 2018)5 ) and document summarization tasks (CNN/Daily Mail (Hermann et al., 2015)6 and WikiSum (Liu et al., 2018)7 ). We adopt BLEU (Papineni et al., 2002) and ROUGE-L (Lin, 2004) to evaluate the translation and summarization quality, respectively. Other details, including model settings, are given in the Appendix A. For characterbased translation, we employ the same model architecture and hyperparameters for training and decoding as specified in Appendix A, except that we adopt larger-batch training (∼85K characters) and encourage longer sequence decoding (length penalty of 1.0). 6 Results and Analysis How much can encoder outputs be sparsified? We answer this question by analyzing the impact of pruning source encodings on the generation quality. We first train a base"
2021.findings-acl.255,P19-1500,0,0.0448246,"Missing"
2021.findings-acl.255,P02-1040,0,0.117752,"eriments. ¯ L is reused that the shortened source sequence X across decoder layers and steps. L0 D ROP changes the dependency of the encoder-decoder attention on source sequence from O(N M ) to O(N 0 M ), and allows for efficiency gains even with moderate sparsity, especially for large L, N and M . 5 Experimental Setup We study L0 D ROP on machine translation tasks (WMT14 English-German (En-De) (Bojar et al., 2014) and WMT18 Chinese-English (Zh-En) (Bojar et al., 2018)5 ) and document summarization tasks (CNN/Daily Mail (Hermann et al., 2015)6 and WikiSum (Liu et al., 2018)7 ). We adopt BLEU (Papineni et al., 2002) and ROUGE-L (Lin, 2004) to evaluate the translation and summarization quality, respectively. Other details, including model settings, are given in the Appendix A. For characterbased translation, we employ the same model architecture and hyperparameters for training and decoding as specified in Appendix A, except that we adopt larger-batch training (∼85K characters) and encourage longer sequence decoding (length penalty of 1.0). 6 Results and Analysis How much can encoder outputs be sparsified? We answer this question by analyzing the impact of pruning source encodings on the generation qualit"
2021.findings-acl.255,W18-6319,0,0.0512902,"Missing"
2021.findings-acl.255,K16-1029,0,0.0534137,"Missing"
2021.findings-acl.255,E17-2060,1,0.833484,"e-dependent, which differs from the linear-time model of Wang et al. (2019), although both can accelerate decoding. Our study of rule-based sparsity patterns is in line with the sparse Transformer (Child et al., 2019) though we also explore the use of external linguistic information (POS tag) in our sparsification rules, and focus on encoder outputs instead of self-attention. Character-based translation gained increasing popularity due to its capability of handling out-ofvocabulary issues while avoiding tokenization and subword segmentation (Ling et al., 2015; Costajuss`a and Fonollosa, 2016; Sennrich, 2017; Cherry et al., 2018). Recent efforts often focus on closing the performance gap against its subword-level counterpart (Libovick´y and Fraser, 2020; Gao et al., 2020), but little study explores solutions to improve its inefficient inference resulted from the long character sequences. In this respect, Cherry et al. (2018) proposed to use conditional computation to dynamically compress encoder states. Similar to our results, they also observed a trade-off between the translation quality and the degree of compression. 3 Background: Transformer We take Transformer (Vaswani et al., 2017) as our te"
2021.findings-acl.255,P16-1162,1,0.505932,"Missing"
2021.findings-acl.255,P19-1032,0,0.0272384,"et al., 2016), sequential knowledge dis2889 tillation (Kim and Rush, 2016) or sparse attention induction that ranges from modeling hard attention (Wu et al., 2018) to developing differentiable sparse softmax functions or regularizing attention weights for sparsity (Niculae and Blondel, 2017; Correia et al., 2019; Cui et al., 2019; Zhang et al., 2019). Unfortunately, the success of all these studies builds upon the access to all source encodings in training and decoding. Learning which encoder outputs to prune in Seq2Seq models, to the best of our knowledge, has never been investigated before. Sukhbaatar et al. (2019) learn attention spans in self-attention and discard information from states outside of the span; this method is not directly applicable to encoder-decoder attention. We use the differentiable L0 -relaxation which was first introduced by Louizos et al. (2018) in the context of pruning individual neural network parameters. It was previously used to prune heads in multi-head attention (Voita et al., 2019). Our work is more similar in spirit to Bastings et al. (2019) where they used the L0 relaxations to construct interpretable classifiers, i.e. models that can reveal which words they rely on whe"
2021.findings-acl.255,N03-1033,0,0.194987,"efforts. By scheduling λ linearly with training steps, we can train models with L0 D ROP (Eq. (12)) from scratch, and obtain a BLEU score of 27.03 (λ = 0.2, warmup step of 200K) on WMT14 En-De (using subwords), comparable to using finetuning (27.04). What types of source encoding are required for generation? Our goal here it to understand encodings of which types of tokens are retained. For each source encoding, we regard the POS of its corresponding word as its type. We take WMT14 EnDe as our benchmark, where we annotate POS for source sentences in the test set using the Stanford POS tagger (Toutanova et al., 2003). We handle subwords separately by labeling its first piece as BPEH while the others as BPEO, regardless of the POS of its unsegmented form. We group different POS tags into 6 categories for the sake of analysis: BPEH, BPEO, function words, content words, punctuation and the rest.9 Figure 5 shows how the sparsity rate of each encoding type changes as a function of the overall sparsity rate. We find that L0 D ROP first choose to eliminate the encoding of punctuation, followed by that of function words. These words often signal structural and grammatical relationships that, while important to bu"
2021.findings-acl.255,P19-1580,1,0.823602,"on the access to all source encodings in training and decoding. Learning which encoder outputs to prune in Seq2Seq models, to the best of our knowledge, has never been investigated before. Sukhbaatar et al. (2019) learn attention spans in self-attention and discard information from states outside of the span; this method is not directly applicable to encoder-decoder attention. We use the differentiable L0 -relaxation which was first introduced by Louizos et al. (2018) in the context of pruning individual neural network parameters. It was previously used to prune heads in multi-head attention (Voita et al., 2019). Our work is more similar in spirit to Bastings et al. (2019) where they used the L0 relaxations to construct interpretable classifiers, i.e. models that can reveal which words they rely on when predicting a class. In their approach, the information from dropped words is lost rather than rerouted into the states of retained words, as desirable for interpretability but problematic in the text generation set-up. The number of the source encodings selected by L0 D ROP is sentence-dependent, which differs from the linear-time model of Wang et al. (2019), although both can accelerate decoding. Our"
2021.findings-acl.255,D19-1074,0,0.0231365,"ed to prune heads in multi-head attention (Voita et al., 2019). Our work is more similar in spirit to Bastings et al. (2019) where they used the L0 relaxations to construct interpretable classifiers, i.e. models that can reveal which words they rely on when predicting a class. In their approach, the information from dropped words is lost rather than rerouted into the states of retained words, as desirable for interpretability but problematic in the text generation set-up. The number of the source encodings selected by L0 D ROP is sentence-dependent, which differs from the linear-time model of Wang et al. (2019), although both can accelerate decoding. Our study of rule-based sparsity patterns is in line with the sparse Transformer (Child et al., 2019) though we also explore the use of external linguistic information (POS tag) in our sparsification rules, and focus on encoder outputs instead of self-attention. Character-based translation gained increasing popularity due to its capability of handling out-ofvocabulary issues while avoiding tokenization and subword segmentation (Ling et al., 2015; Costajuss`a and Fonollosa, 2016; Sennrich, 2017; Cherry et al., 2018). Recent efforts often focus on closing"
2021.findings-acl.255,D18-1473,0,0.050686,"Missing"
2021.findings-acl.261,K16-1002,0,0.100823,"Missing"
2021.findings-acl.261,W18-6315,0,0.0204696,", unlike masking. Introduction Neural machine translation (NMT) is notoriously data-hungry (Koehn and Knowles, 2017). To learn a strong model it requires large, high-quality and in-domain parallel data, which exist only for a few language-pairs. The most successful approach for improving low-resource NMT is backtranslation (Sennrich et al., 2016), that exploits abundant monolingual corpora to augment the parallel with synthetic data. However, in low-resource settings, it may fail to improve or even degrade translation quality if the initial model is not strong enough (Imankulova et al., 2017; Burlot and Yvon, 2018). Unsupervised pretraining is a complementary technique, that has revolutionized many natural language understanding (NLU) tasks (Wang et al., 2019). The dominant approach is to train a (large) model on a lot of unlabeled data using the masked language modeling (MLM; Devlin et al. (2019)) objective and then finetune it on a downstream task. Besides improving generalization, good initialization drastically reduces the need for labelled data. This paradigm has been applied recently to NMT yielding impressive results in low-resource settings, with models such as XLM (Conneau and Lample, 2019), MA"
2021.findings-acl.261,N19-1423,0,0.153631,"ow-resource NMT is backtranslation (Sennrich et al., 2016), that exploits abundant monolingual corpora to augment the parallel with synthetic data. However, in low-resource settings, it may fail to improve or even degrade translation quality if the initial model is not strong enough (Imankulova et al., 2017; Burlot and Yvon, 2018). Unsupervised pretraining is a complementary technique, that has revolutionized many natural language understanding (NLU) tasks (Wang et al., 2019). The dominant approach is to train a (large) model on a lot of unlabeled data using the masked language modeling (MLM; Devlin et al. (2019)) objective and then finetune it on a downstream task. Besides improving generalization, good initialization drastically reduces the need for labelled data. This paradigm has been applied recently to NMT yielding impressive results in low-resource settings, with models such as XLM (Conneau and Lample, 2019), MASS (Song et al., 2019) and BART/mBART (Lewis et al., 2020b; Liu et al., 2020), that adapt MLM to sequence-to-sequence architectures. Although pretraining alone is not enough to outperform backtranslation, it helps the initial model to produce synthetic data of sufficient quality, and com"
2021.findings-acl.261,D18-1045,0,0.0708412,"Missing"
2021.findings-acl.261,P17-2090,0,0.0168307,"on all permutations of other tokens in a sentence and Song et al. (2020) extend this to sequence-level pretraining for NLU. MARGE (Lewis et al., 2020a) explores multi-lingual pretraining for document-level NMT, by reconstructing texts from a set of retrieved relevant documents. Clark et al. (2020) propose the replaced token detection (RTD) objective for pretraining text encoders. They replace tokens with samples from a MLM and train the encoder as a discriminator to predict whether each word is real or fake. Similar ideas have been previously explored in NMT with contextual data augmentation (Fadaee et al., 2017; Kobayashi, 2018; Gao et al., 2019). 2957 1 Code at github.com/cbaziotis/nmt-pretraining-objectives 3 Pretraining Our pretraining model is a multilingual denoising sequence autoencoder, based on the Transformer (Vaswani et al., 2017). We assume access to a corpus of unpaired data, containing text in two languages A, B. Given a text sequence of N tokens x = hx1 , x2 , ..., xN i we first add noise to it and obtain its corrupted version x0 . An encoder transforms x0 into a sequence of contextualized representations h(x0 ) = hh1 , h2 , ..., hN i, which are given as input to the decoder, that prod"
2021.findings-acl.261,P19-1555,0,0.0152663,"a sentence and Song et al. (2020) extend this to sequence-level pretraining for NLU. MARGE (Lewis et al., 2020a) explores multi-lingual pretraining for document-level NMT, by reconstructing texts from a set of retrieved relevant documents. Clark et al. (2020) propose the replaced token detection (RTD) objective for pretraining text encoders. They replace tokens with samples from a MLM and train the encoder as a discriminator to predict whether each word is real or fake. Similar ideas have been previously explored in NMT with contextual data augmentation (Fadaee et al., 2017; Kobayashi, 2018; Gao et al., 2019). 2957 1 Code at github.com/cbaziotis/nmt-pretraining-objectives 3 Pretraining Our pretraining model is a multilingual denoising sequence autoencoder, based on the Transformer (Vaswani et al., 2017). We assume access to a corpus of unpaired data, containing text in two languages A, B. Given a text sequence of N tokens x = hx1 , x2 , ..., xN i we first add noise to it and obtain its corrupted version x0 . An encoder transforms x0 into a sequence of contextualized representations h(x0 ) = hh1 , h2 , ..., hN i, which are given as input to the decoder, that produces a reconstruction of x. The reco"
2021.findings-acl.261,D19-1632,0,0.0466467,"Missing"
2021.findings-acl.261,W17-5704,0,0.0232334,"h resemble real sentences, unlike masking. Introduction Neural machine translation (NMT) is notoriously data-hungry (Koehn and Knowles, 2017). To learn a strong model it requires large, high-quality and in-domain parallel data, which exist only for a few language-pairs. The most successful approach for improving low-resource NMT is backtranslation (Sennrich et al., 2016), that exploits abundant monolingual corpora to augment the parallel with synthetic data. However, in low-resource settings, it may fail to improve or even degrade translation quality if the initial model is not strong enough (Imankulova et al., 2017; Burlot and Yvon, 2018). Unsupervised pretraining is a complementary technique, that has revolutionized many natural language understanding (NLU) tasks (Wang et al., 2019). The dominant approach is to train a (large) model on a lot of unlabeled data using the masked language modeling (MLM; Devlin et al. (2019)) objective and then finetune it on a downstream task. Besides improving generalization, good initialization drastically reduces the need for labelled data. This paradigm has been applied recently to NMT yielding impressive results in low-resource settings, with models such as XLM (Conne"
2021.findings-acl.261,2020.emnlp-main.75,0,0.0801411,"Missing"
2021.findings-acl.261,2021.naacl-main.7,0,0.0768002,"Missing"
2021.naacl-main.219,D13-1160,0,0.0392973,"select restaurant where star_rating = 3 select restaurant where star_rating = 3 and cuisine = thai Gold Exe 7 7 7 7 7 3 3 3 Figure 1: Candidate programs for an utterance can be classified by executability (Exe); note that the gold program is always in the set of executable programs. We propose to ultilize the weak yet freely available signal of executablility for learning. ronment against which they are executed (e.g., a knowledge base, a relational database). An alternative to annotation is to collect answers (or denotations) of programs, rather than programs themselves (Liang et al., 2013; Berant et al., 2013). In this work, we focus on the more extreme setting where there are no annotations available for a large number of utterances. This setting resembles a common real-life scenario where massive numbers of user utterances can be collected when deploying a semantic parser (Iyer et al., 2017). Effectively utilizing the unlabeled data makes it possible for a semantic parser to improve over time without human involvement. Our key observation is that not all candidate programs for an utterance will be semantically valid. This implies that only some candidate programs can be executed and obtain non-em"
2021.naacl-main.219,N18-1016,0,0.0652319,"Missing"
2021.naacl-main.219,P16-1004,1,0.885665,"Missing"
2021.naacl-main.219,P17-1097,0,0.015149,"ograms. The objective to minimize consists of two parts: N M 1 X 1 X l l Lsup (xi , yi ) + λ Lunsup (xi ) J = N M i=1 j=1 grams. Specifically, they are defined as follows: Lsup (x, y) = − log p(y|x, θ) X Lunsup (x) = − log R(y)p(y|x, θ) (2) (3) y where R(y) is a binary reward function that returns 1 if y is executable and 0 otherwise. In practice, this function is implemented by running a task-specific executor, e.g., a SQL executor. Another alternative to unsupervised loss is REINFORCE (Sutton et al., 1999), i.e., maximize the expected R(y) with respect to p(y|x, θ). However, as presented in Guu et al. (2017), this objective usually underperforms MML, which is consistent with our initial experiments.2 3.2 Self-Training and Top-K MML MML in Equation (3) requires marginalizing over all executable programs which is intractable. Conventionally, we resort to beam search to explore the space of programs and collect executable ones. To illustrate, we can divide the space of programs into four parts based on whether they are executable and observed, as shown in Figure 2a. For example, programs in PSE ∪ PSN are seen in the sense that they are retrieved by beam search. Programs in PSE ∪ PUE are all executab"
2021.naacl-main.219,P17-2098,0,0.0388284,"Missing"
2021.naacl-main.219,P17-1089,0,0.0183457,"pose to ultilize the weak yet freely available signal of executablility for learning. ronment against which they are executed (e.g., a knowledge base, a relational database). An alternative to annotation is to collect answers (or denotations) of programs, rather than programs themselves (Liang et al., 2013; Berant et al., 2013). In this work, we focus on the more extreme setting where there are no annotations available for a large number of utterances. This setting resembles a common real-life scenario where massive numbers of user utterances can be collected when deploying a semantic parser (Iyer et al., 2017). Effectively utilizing the unlabeled data makes it possible for a semantic parser to improve over time without human involvement. Our key observation is that not all candidate programs for an utterance will be semantically valid. This implies that only some candidate programs can be executed and obtain non-empty execution results.1 As illustrated in Figure 1, executability is a weak signal that can differentiate between semantically valid and invalid programs. On unlabeled utterances, we can encourage a parser to only focus on executable programs ignoring non-executable ones. Moreover, the ex"
2021.naacl-main.219,P16-1002,0,0.366429,"mpty execution results.1 As illustrated in Figure 1, executability is a weak signal that can differentiate between semantically valid and invalid programs. On unlabeled utterances, we can encourage a parser to only focus on executable programs ignoring non-executable ones. Moreover, the executability of a program Semantic parsing is the task of mapping natural language (NL) utterances to meaning representations (aka programs) that can be executed against a real-world environment such as a knowledge base or a relational database. While neural sequence-to-sequence models (Dong and Lapata, 2016; Jia and Liang, 2016a) have achieved much success in this task in recent years, they usually require a large amount of labeled data (i.e., utteranceprogram pairs) for training. However, annotating utterances with programs is expensive as it 1 In the rest of this paper, we extend the meaning of ‘exerequires expert knowledge of meaning representa- cutability’, and use it to refer to the case where a program is tions (e.g., lambda calculus, SQLs) and the envi- executable and obtains non-empty results. 2747 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguisti"
2021.naacl-main.219,D16-1116,0,0.0409265,"Missing"
2021.naacl-main.219,D17-1160,0,0.0169834,"pproxima- and Lapata, 2016; Jia and Liang, 2016a) treat protions of MML for learning from executions. They grams as sequences, ignoring their internal strucare designed to complement Self-Training and Top- ture. As a result, the well-formedness of generated K MML via discouraging seen non-executable pro- programs cannot be guaranteed. Grammar-based grams and introducing sparsity. In the following decoders aim to remedy this issue. For text-to-LF sections, we will empirically show that they are parsing, we use the type-constrained decoder prosuperior to Self-Training and Top-K MML for posed by Krishnamurthy et al. (2017); for text-tosemi-supervised semantic parsing. The approxi- SQL parsing, we use an AST (abstract syntax tree) mations we proposed may also be beneficial for based decoder following Yin and Neubig (2018). learning from denotations (Liang et al., 2013; Be- Note that grammar-based decoding can only ensure rant et al., 2013) and weakly supervised question the syntactic correctness of generated programs. answering (Min et al., 2019), but we leave this to Executable programs are additionally semantically future work. correct. For example, all programs in Figure 1 are 2752 Model Lower bound Self-Trai"
2021.naacl-main.219,J13-2005,0,0.168547,"nt where cuisine > 3 select restaurant where star_rating = 3 select restaurant where star_rating = 3 and cuisine = thai Gold Exe 7 7 7 7 7 3 3 3 Figure 1: Candidate programs for an utterance can be classified by executability (Exe); note that the gold program is always in the set of executable programs. We propose to ultilize the weak yet freely available signal of executablility for learning. ronment against which they are executed (e.g., a knowledge base, a relational database). An alternative to annotation is to collect answers (or denotations) of programs, rather than programs themselves (Liang et al., 2013; Berant et al., 2013). In this work, we focus on the more extreme setting where there are no annotations available for a large number of utterances. This setting resembles a common real-life scenario where massive numbers of user utterances can be collected when deploying a semantic parser (Iyer et al., 2017). Effectively utilizing the unlabeled data makes it possible for a semantic parser to improve over time without human involvement. Our key observation is that not all candidate programs for an utterance will be semantically valid. This implies that only some candidate programs can be exec"
2021.naacl-main.219,D19-1284,0,0.0874913,"ly seen nonexecutable (PSN ) programs are discouraged. 4.3 Sparse MML Sparse MML is based on the intuition that in most cases there is only one or few correct pro4.2 Repulsion MML and Gentle MML grams among all executable programs. As menAs mentioned previously, Self-Training and Toptioned in Section 2, spurious programs that are exeK MML should be reasonable approximations in cutable, but do not reflect the semantics of an uttercases where gold programs are retrieved, i.e., they ance are harmful. One empirical evidence from preare in the seen executable subset (PSE in Figure 2a). vious work (Min et al., 2019) is that Self-Training However, if a parser is uncertain, i.e., beam search outperforms Top-K MML for weakly-supervised cannot retrieve the gold programs, exclusively exquestion answering. Hence, exploiting all seen ploiting PSE programs is undesirable. Hence, we executable programs can be sub-optimal. Followconsider ways of taking unseen executable proing recent work on sparse distributions (Martins grams (PUE in Figure 2a) into account. Since we and Astudillo, 2016; Niculae et al., 2018), we pronever directly observe unseen programs (PUE or pose to encourage sparsity of the ‘soft label’ q. P"
2021.naacl-main.219,D17-1127,0,0.0347042,"Missing"
2021.naacl-main.219,2020.acl-main.677,1,0.801936,"s defined as follows:  t+1 qsparse = SparseMaxy∈PSE log p(y|x, θ t ) t+1 occupies the middle ground beIntuitively, qsparse tween Self-Training (uses y ∗ only) and Top-K MML (uses all PSE programs). With the help of sparsity of q introduced by SparseMax, the M-step will only promote a subset of PSE programs. 5 Semantic Parsers In principle, our X-PR framework is modelagnostic, i.e., it can be coupled with any semantic parser for semi-supervised learning. In this work, we use a neural parser that achieves state-of-the-art performance across semantic parsing tasks. Specifically, we use RAT-SQL (Wang et al., 2020) which features a relation-aware encoder and a grammarbased decoder. The parser was originally developed for text-to-SQL parsing, and we adapt it to text-to-LF parsing. In this section, we briefly review the encoder and decoder of this parser. For more details, please refer to Wang et al. (2020). 5.1 Relation-Aware Encoding Relation-aware encoding is originally designed to handle schema encoding and schema linking for text-to-SQL parsing. We generalize these two notions for both text-to-LF and text-to-SQL parsing as follows: • enviroment encoding: encoding enviroments, i.e., a knowledge base c"
2021.naacl-main.33,P19-1444,0,0.386011,"zation. earlier work has primarily focused on evaluating parsers in-domain (e.g., tables or databases) and Conventional supervised learning simply asoften with the same programs as those provided sumes that source- and target-domain data origin training (Finegan-Dollak et al., 2018). A much inate from the same distribution, and as a result more challenging goal is achieving domain gener- struggles to capture this notion of domain generalization, i.e., building parsers which can be suc- alization for zero-shot semantic parsing. Previous cessfully applied to new domains and are able approaches (Guo et al., 2019b; Wang et al., 2020; to produce complex unseen programs. Achiev- Herzig and Berant, 2018) facilitate domain genering this generalization goal would, in principle, alization by incorporating inductive biases in the let users query arbitrary (semi-)structured data on model, e.g., designing linking features or functions the Web and reduce the annotation effort required which should be invariant under domain shifts. In to build multi-domain NL interfaces (e.g., Apple this work, we take a different direction and improve 366 Proceedings of the 2021 Conference of the North American Chapter of the As"
2021.naacl-main.33,D18-1190,0,0.0766582,"tables or databases) and Conventional supervised learning simply asoften with the same programs as those provided sumes that source- and target-domain data origin training (Finegan-Dollak et al., 2018). A much inate from the same distribution, and as a result more challenging goal is achieving domain gener- struggles to capture this notion of domain generalization, i.e., building parsers which can be suc- alization for zero-shot semantic parsing. Previous cessfully applied to new domains and are able approaches (Guo et al., 2019b; Wang et al., 2020; to produce complex unseen programs. Achiev- Herzig and Berant, 2018) facilitate domain genering this generalization goal would, in principle, alization by incorporating inductive biases in the let users query arbitrary (semi-)structured data on model, e.g., designing linking features or functions the Web and reduce the annotation effort required which should be invariant under domain shifts. In to build multi-domain NL interfaces (e.g., Apple this work, we take a different direction and improve 366 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 366–379 June 6"
2021.naacl-main.33,2020.acl-main.398,0,0.0158909,"ing training. One line of work tries to incorporate inductive biases, e.g., domain-invariant n-gram matching features (Guo et al., 2019b; Wang et al., 2020), cross-domain alignment functions (Herzig and Berant, 2018), or auxiliary linking tasks (Chang et al., 2020) to improve schema linking. However, in the cross-lingual setting of Chinese Spider (Min et al., 2019), where questions and schemas are not in the same language, it is not obvious how to design such inductive biases like n-gram matching features. Another line of work relies on large-scale unsupervised pre-training on massive tables (Herzig et al., 2020; Yin et al., 2020) to obtain better representations for both questions and database schemas. Our work is orthogonal to these approaches and can be easily coupled with them. As an example, • We handle zero-shot semantic parsing by apwe show in Section 5 that our training procedure plying a meta-learning objective that directly can improve the performance of a parser already enoptimizes for domain generalization. hanced with n-gram matching features (Guo et al., • We propose an approximation of the meta- 2019b; Wang et al., 2020). Our work is similar in spirit to Givoli and Relearning objective"
2021.naacl-main.33,2020.findings-emnlp.438,0,0.350531,"Missing"
2021.naacl-main.33,D19-1377,0,0.0459501,"d in a question. For example, a parser would decide to select the column Status because of the word statuses in Figure 1. However, in the setting of zero-shot parsing, columns or tables might be mentioned in a question without ever being observed during training. One line of work tries to incorporate inductive biases, e.g., domain-invariant n-gram matching features (Guo et al., 2019b; Wang et al., 2020), cross-domain alignment functions (Herzig and Berant, 2018), or auxiliary linking tasks (Chang et al., 2020) to improve schema linking. However, in the cross-lingual setting of Chinese Spider (Min et al., 2019), where questions and schemas are not in the same language, it is not obvious how to design such inductive biases like n-gram matching features. Another line of work relies on large-scale unsupervised pre-training on massive tables (Herzig et al., 2020; Yin et al., 2020) to obtain better representations for both questions and database schemas. Our work is orthogonal to these approaches and can be easily coupled with them. As an example, • We handle zero-shot semantic parsing by apwe show in Section 5 that our training procedure plying a meta-learning objective that directly can improve the per"
2021.naacl-main.33,P19-1589,0,0.152758,"Missing"
2021.naacl-main.33,P15-1142,0,0.0817549,"Missing"
2021.naacl-main.33,D14-1162,0,0.0876107,"ial encoder provides initial representations, denoted as Qinit and Sinit for the question and the schema, respectively. A relation-aware transformer (RAT) module then takes the initial representations and further computes context-aware representations Qenc and Senc for the question and the schema, respectively. Finally, a decoder generates a sequence of production rules that constitute the abstract syntax tree T based on Qenc and Senc . To obtain Qinit and Sinit , the initial encoder could either be 1) LSTMs (Hochreiter and Schmidhuber, 1997) on top of pre-trained word embeddings, like GloVe (Pennington et al., 2014), or 2) pre-trained contextual embeddings like BERT (Devlin et al., 2 We re-implemented RAT-SQL, and added a component for value prediction so that our base parsers can be evaluated by execution accuracy. 370 2019). In our work, we will test the effectiveness of our method for both variants. As shown in Wang et al. (2020), the encodings Qenc and Senc , which are the output of the RAT module, heavily rely on schema-linking features. These features are extracted from a heuristic function that links question words to columns and tables based on n-gram matching, and they are readily available in t"
2021.naacl-main.33,2020.acl-demos.14,0,0.0271746,"Missing"
2021.naacl-main.33,N18-2028,0,0.034793,"Missing"
2021.naacl-main.33,2020.acl-main.742,0,0.146303,"n n-gram matching, and they are readily available in the conventional mono-lingual setting of the Spider dataset. However, we hypothesize that the parser’s over-reliance on these features is specific to Spider, where annotators were shown the database schema and asked to formulate queries. As a result, they were prone to re-using terms from the schema verbatim in their questions. This would not be the case in a real-world application where users are unfamiliar with the structure of the underlying database and free to use arbitrary terms which would not necessarily match column or table names (Suhr et al., 2020). Hence, we will also evaluate our parser in the cross-lingual setting where Q and S are not in the same language, and such features would not be available. 5 Experiments To evaluate DG-MAML, we integrate it with a base parser and test it on zero-shot text-to-SQL tasks. By designing an in-domain benchmark, we also show that the out-of-domain improvement does not come at the cost of in-domain performance. We also present some analysis to show how DGMAML affects domain generalization. 5.1 Datasets and Metrics We evaluate DG-MAML on two zero-shot textto-SQL benchmarks, namely, (English) Spider (Y"
2021.naacl-main.33,W00-1317,0,0.491671,"Missing"
2021.naacl-main.33,2020.acl-main.677,1,0.36471,"k has primarily focused on evaluating parsers in-domain (e.g., tables or databases) and Conventional supervised learning simply asoften with the same programs as those provided sumes that source- and target-domain data origin training (Finegan-Dollak et al., 2018). A much inate from the same distribution, and as a result more challenging goal is achieving domain gener- struggles to capture this notion of domain generalization, i.e., building parsers which can be suc- alization for zero-shot semantic parsing. Previous cessfully applied to new domains and are able approaches (Guo et al., 2019b; Wang et al., 2020; to produce complex unseen programs. Achiev- Herzig and Berant, 2018) facilitate domain genering this generalization goal would, in principle, alization by incorporating inductive biases in the let users query arbitrary (semi-)structured data on model, e.g., designing linking features or functions the Web and reduce the annotation effort required which should be invariant under domain shifts. In to build multi-domain NL interfaces (e.g., Apple this work, we take a different direction and improve 366 Proceedings of the 2021 Conference of the North American Chapter of the Association for Comput"
2021.naacl-main.33,P15-1129,0,0.0624881,"Missing"
2021.naacl-main.33,D18-2002,0,0.0544053,"n be computed as: ∇θ Lτ (θ) =∇θ θ 0 ∇θ0 LBt (θ 0 ) + ∇θ LBs (θ)  = I − α∇2θ LBs (θ) ∇θ0 LBt (θ 0 ) (5) + ∇θ LBs (θ) Semantic Parser In general, DG-MAML is model-agnostic and can be coupled with any semantic parser to improve its domain generalization. In this work, we use a base parser that is based on RAT-SQL (Wang et al., 2020), which currently achieves state-of-theart performance on Spider.2 Formally, RAT-SQL takes as input question Q and schema S of its corresponding database. Then it produces a program which is represented as an abstract syntax tree T in the context-free grammar of SQL (Yin and Neubig, 2018). RAT-SQL adopts the encoder-decoder framework for text-to-SQL parsing. It has three components: an initial encoder, a transformer-based encoder and an LSTM-based decoder. The initial encoder provides initial representations, denoted as Qinit and Sinit for the question and the schema, respectively. A relation-aware transformer (RAT) module then takes the initial representations and further computes context-aware representations Qenc and Senc for the question and the schema, respectively. Finally, a decoder generates a sequence of production rules that constitute the abstract syntax tree T base"
2021.naacl-main.33,2020.acl-main.745,0,0.0107787,"e of work tries to incorporate inductive biases, e.g., domain-invariant n-gram matching features (Guo et al., 2019b; Wang et al., 2020), cross-domain alignment functions (Herzig and Berant, 2018), or auxiliary linking tasks (Chang et al., 2020) to improve schema linking. However, in the cross-lingual setting of Chinese Spider (Min et al., 2019), where questions and schemas are not in the same language, it is not obvious how to design such inductive biases like n-gram matching features. Another line of work relies on large-scale unsupervised pre-training on massive tables (Herzig et al., 2020; Yin et al., 2020) to obtain better representations for both questions and database schemas. Our work is orthogonal to these approaches and can be easily coupled with them. As an example, • We handle zero-shot semantic parsing by apwe show in Section 5 that our training procedure plying a meta-learning objective that directly can improve the performance of a parser already enoptimizes for domain generalization. hanced with n-gram matching features (Guo et al., • We propose an approximation of the meta- 2019b; Wang et al., 2020). Our work is similar in spirit to Givoli and Relearning objective that is more effic"
2021.naacl-main.33,D18-1193,0,0.0964776,"). Hence, we will also evaluate our parser in the cross-lingual setting where Q and S are not in the same language, and such features would not be available. 5 Experiments To evaluate DG-MAML, we integrate it with a base parser and test it on zero-shot text-to-SQL tasks. By designing an in-domain benchmark, we also show that the out-of-domain improvement does not come at the cost of in-domain performance. We also present some analysis to show how DGMAML affects domain generalization. 5.1 Datasets and Metrics We evaluate DG-MAML on two zero-shot textto-SQL benchmarks, namely, (English) Spider (Yu et al., 2018b) and Chinese Spider (Min et al., 2019). Chinese Spider is a Chinese version of Spider that translates all NL questions from English to Chinese and keeps the original English database. It introduces the additional challenge of encoding crosslingual correspondences between Chinese and English.3 In both datasets, we report exact set match accuracy, following Yu et al. (2018b). We also report execution accuracy in the Spider dataset. schema-linking features (as mentioned in Section 4) and pre-trained emebddings such as BERT. To show that our method can still achieve additional improvements, we c"
2021.naacl-main.33,D18-1425,0,0.0902285,"Missing"
2021.naacl-main.33,2020.emnlp-main.558,0,0.318575,"Missing"
C12-1089,N09-1003,0,0.0667245,"enforce a form of crosslingual agreement in the induced representations. However, atomic cluster labels arguably are not capable of encoding multiple factors or views on the syntactic and semantic properties of words, and, consequently, may be less informative for many applications. For a detailed comparison of properties of distributed representations and Brown clustering we refer the reader to Turian et al. (2010). Construction of crosslingual representations and similarity functions has also been considered in the related area of distributional semantics (van der Plas and Tiedemann, 2006; Agirre et al., 2009) where a word is represented as a vector and each of its components encodes the strength of co-occurrence with a specific lexical or syntactic context (Rapp, 1995). These representations again have very different properties from the ones considered here: for example, they are typically very highly dimensional and, consequently, may be less useful as features in 1469 DistribReps MT Glossed Majority-Class en → de 77.6 68.1 65.1 46.8 de → en 71.1 67.4 68.6 46.8 Table 2: Classification accuracy for training on English and German with 1000 labeled examples. classifiers. Also they generally cannot b"
C12-1089,W12-3102,0,0.0134022,"Missing"
C12-1089,W02-1001,0,0.0334611,"aining,5 we learn on a subset of training sequences choosing the 3,000 most frequent words in en and de for their en de output vocabularies Vout and Vout , respectively. The representations were induced from our subset of RCV1/RCV2 dataset using word alignments from Europarl v7 (see Section 5.1). We ran the learning procedure for 40 iterations, which took about 10 CPU days and is linearly parallelizable. Learning rate was set to 0.005 and was reduced when the training data likelihood went up, as is common when training neural networks. We used the averaged version of the perceptron algorithm (Collins, 2002) to train a multiclass document classifier, so that we do not need to tune any parameters, with the exception of the the number of epochs, which we set to 10 in all experiments (the results were not sensitive to this parameter). Our goal is to train a classifier in one language and test it on data in another, so we compared the following classifiers: • A classifier which used features based on the crosslingual representations we induced (DistribReps) and was trained on supervised training data in one language and directly tested on documents in the other. We represent each document as an avera"
C12-1089,P07-1071,0,0.0126209,"re informative by using them for crosslingual document classification, where classifiers trained on these representations substantially outperform strong baselines (e.g. machine translation) when applied to a new language. KEYWORDS: distributed representations, multilingual learning, direct transfer of annotation. Proceedings of COLING 2012: Technical Papers, pages 1459–1474, COLING 2012, Mumbai, December 2012. 1459 1 Introduction Word representations induced to capture syntactic and semantic properties of words have been extremely useful for numerous natural language processing applications (Collobert and Weston, 2007; Turian et al., 2010). Their primary appeal is that they can be induced using abundant unsupervised data and then used directly or as additional features to alleviate the data sparsity problem common in the supervised learning scenario. Most of the prior work on inducing these representations has focused on a single language, English, which enjoys the largest repository of available annotated resources. In this work, we focus on a single representation for a pair of languages such that semantically similar words are closer to one another in the induced representation irrespective of the langu"
C12-1089,P12-1092,0,0.110889,"ters, with the exception of the the number of epochs, which we set to 10 in all experiments (the results were not sensitive to this parameter). Our goal is to train a classifier in one language and test it on data in another, so we compared the following classifiers: • A classifier which used features based on the crosslingual representations we induced (DistribReps) and was trained on supervised training data in one language and directly tested on documents in the other. We represent each document as an average of ddimensional representations of all of its tokens weighted by their idf score (Huang et al., 2012). • A classifier with word count features which was trained and tested on the second language documents translated into the original language. Translations are done by replacing each word in a test document by the word most frequently aligned to it in the parallel data (Glossed). Unaligned words were left as is. • Using a machine translation system instead of simple glossing would provide a natural baseline (Fortuna and Shawe-Taylor, 2005; Shi et al., 2010). So, another baseline (MT) is similar to the previous with the exception that the second language documents were translated by the standar"
C12-1089,E12-1014,1,0.459675,"language that are similar to each other end up being “close” in the joint representation. However, since aligned resources may not be available for a given language pair, an investigation of robustness of our setup to the amount of parallel data as well as using alternative resources to define A is an interesting future direction. Distributed representations of multi-word expressions (phrases) have recently been shown very useful for sentiment analysis (Socher et al., 2011b). Inducing these representations in multiple languages is likely to benefit tasks like low-resource machine translation (Klementiev et al., 2012) where it could potentially be used to both induce phrase tables and score them with little parallel data. We showed that crosslingual representations are very informative for crosslingual document classification, where they can be used to directly apply a classifier trained on data in one language to test data in another. Classification accuracy is likely to improve if we were to learn these representations specifically for the task, which would require a small change to the learning objective. Applying our framework with the specific goal of building a state-of-the-art classifier is also an"
C12-1089,2005.mtsummit-papers.11,0,0.121865,"in another language where no annotation is available. Note that our goal is not to induce a state-of-the-art classifier, but rather to examine the informativeness of the induced representations.4 Thus, we keep the classification experiments simple: we chose a learning algorithm requiring no parameter tuning and used simple features. 5.1 Data In our experiments, we induce crosslingual embeddings and use them for multilinigual document classification for the English-German language pair. We use the following resources: • English (en) and German (de) sections of the Europarl v7 parallel corpus (Koehn, 2005) to induce our baseline systems and to compute the interaction matrix A (see Section 4). We used GIZA++ (Och and Ney, 2003) to induce word alignments, keeping only bidirectional alignments. In the context of our model, parallel data is only used to estimate the interaction matrix A. When constructing A, we discard word pairs aligned only once in order to reduce the number of effective updates during gradient descent (see equation (6)). • A subset of the English and German sections of the Reuters RCV1/RCV2 corpora (Lewis et al., 2004) to induce crosslingual embeddings and for the crosslingual d"
C12-1089,P07-2045,0,0.0179981,"which was trained and tested on the second language documents translated into the original language. Translations are done by replacing each word in a test document by the word most frequently aligned to it in the parallel data (Glossed). Unaligned words were left as is. • Using a machine translation system instead of simple glossing would provide a natural baseline (Fortuna and Shawe-Taylor, 2005; Shi et al., 2010). So, another baseline (MT) is similar to the previous with the exception that the second language documents were translated by the standard phrase-based machine translation model (Koehn et al., 2007) using default parameters and a 5-gram language model trained on Europarl v7 data. 5 • For reference, we also include majority class predictions (Majority Class). In particular, computing the normalization in the softmax function, is linear in |Vout |. 1467 january de januar februar november april august märz juni dezember juli september president en de president präsident king präsidenten hun minister areas staatspräsident saddam hun minister vorsitzenden advisers us-präsident prince könig representative berichteten institutional außenminister said en de said sagte reported erklärte stated sa"
C12-1089,D09-1092,0,0.0703015,"and testing on German documents (left) and vice versa (right). 6 Additional Related Work In the last decade crosslingual methods have attracted a lot of attention both in NLP and closely related communities such as information retrieval (Lavrenko et al., 2002) and information management (Frederking et al., 2001). Much of this work has focused on techniques for porting methods and resources from one language to another (see, e.g., crosslingual document classification (Fortuna and Shawe-Taylor, 2005; Shi et al., 2010)). Development of crosslingual models (e.g., topic models (Zhang et al., 2010; Mimno et al., 2009)) has also attracted some attention. However, these approaches either do not induce representations of individual words and, as such, may not be very useful for methods dealing with richer linguistic structures (such as syntactic parsing or semantic role labeling) or they focus on porting a specific method (e.g., named entity recognizer (Steinberger and Pouliquen, 2007)). This contrasts significantly with our objective: inducing fine-grain distributed word representation useful in virtually arbitrary NLP problems. Possibly the most related work to ours is the method for inducing crosslingual B"
C12-1089,J03-1002,0,0.0681579,"st crosslingual distributed representation induction as a multitask learning problem by treating each word w in our languages’ vocabularies as a separate task. The set of related tasks for each w are then the possible translations of the word in the other language. When encoding relatedness and defining an interaction matrix A, we make use of parallel data (a set of sentences and their translations). These resources are available for many language pairs and include large volumes of multilingual parliamentary proceedings, book translations, etc. Standard Machine Translation tools (e.g. GIZA++ (Och and Ney, 2003)) can be used to induce alignments between words on both sides of the bitext. Assuming that word alignments are available, we first define a complete undirected bipartite weighted graph H with two disjoint sets of vertices corresponding to the input vocabularies Vin of the two languages, and edges labeled with the number of alignments between each pair of words in the two sets. The edge weights indicate the fit of a pair of words as translations, and thus encode the degree of relatedness between the two corresponding tasks. We can now ˜ as the directly apply the definition of the interaction m"
C12-1089,P95-1050,0,0.159462,"he syntactic and semantic properties of words, and, consequently, may be less informative for many applications. For a detailed comparison of properties of distributed representations and Brown clustering we refer the reader to Turian et al. (2010). Construction of crosslingual representations and similarity functions has also been considered in the related area of distributional semantics (van der Plas and Tiedemann, 2006; Agirre et al., 2009) where a word is represented as a vector and each of its components encodes the strength of co-occurrence with a specific lexical or syntactic context (Rapp, 1995). These representations again have very different properties from the ones considered here: for example, they are typically very highly dimensional and, consequently, may be less useful as features in 1469 DistribReps MT Glossed Majority-Class en → de 77.6 68.1 65.1 46.8 de → en 71.1 67.4 68.6 46.8 Table 2: Classification accuracy for training on English and German with 1000 labeled examples. classifiers. Also they generally cannot be created with a specific application in mind, whereas word representations can be learned to be useful for a specific problem (Collobert and Weston, 2007). 7 Conc"
C12-1089,D10-1103,0,0.0155011,"in the other. We represent each document as an average of ddimensional representations of all of its tokens weighted by their idf score (Huang et al., 2012). • A classifier with word count features which was trained and tested on the second language documents translated into the original language. Translations are done by replacing each word in a test document by the word most frequently aligned to it in the parallel data (Glossed). Unaligned words were left as is. • Using a machine translation system instead of simple glossing would provide a natural baseline (Fortuna and Shawe-Taylor, 2005; Shi et al., 2010). So, another baseline (MT) is similar to the previous with the exception that the second language documents were translated by the standard phrase-based machine translation model (Koehn et al., 2007) using default parameters and a 5-gram language model trained on Europarl v7 data. 5 • For reference, we also include majority class predictions (Majority Class). In particular, computing the normalization in the softmax function, is linear in |Vout |. 1467 january de januar februar november april august märz juni dezember juli september president en de president präsident king präsidenten hun min"
C12-1089,D11-1014,0,0.106414,"define a signal for aligning the latent representations in both languages as we induce them. In MTL terminology, we treat words as individual tasks; words that are likely to be translations of one another (based on bitext statistics) are treated as related tasks and effectively help to align representations in both languages during learning. We use a variant of a neural network language model of Bengio et al. (2003) to induce the latent representations in individual languages. These models learn a lower-dimensional embedding of words arguably capturing their syntactic and semantic properties (Socher et al., 2011a). In sum, the contributions of this work are: • we frame induction of crosslingual distributed word representations as joint induction and alignment of distributed representations in individual languages; • we apply our framework to the neural network language modeling approach of Bengio et al. (2003); • although our goal is not to beat the state of the art in crosslingual document classification, we use this task to show that the crosslingual embeddings we induce enable us to transfer a classifier trained on one language to another without any adaptation. The crosslingual representation ind"
C12-1089,N12-1052,0,0.110906,"ed some attention. However, these approaches either do not induce representations of individual words and, as such, may not be very useful for methods dealing with richer linguistic structures (such as syntactic parsing or semantic role labeling) or they focus on porting a specific method (e.g., named entity recognizer (Steinberger and Pouliquen, 2007)). This contrasts significantly with our objective: inducing fine-grain distributed word representation useful in virtually arbitrary NLP problems. Possibly the most related work to ours is the method for inducing crosslingual Brown clusterings (Täckström et al., 2012). They also use multi-lingual parallel data to enforce a form of crosslingual agreement in the induced representations. However, atomic cluster labels arguably are not capable of encoding multiple factors or views on the syntactic and semantic properties of words, and, consequently, may be less informative for many applications. For a detailed comparison of properties of distributed representations and Brown clustering we refer the reader to Turian et al. (2010). Construction of crosslingual representations and similarity functions has also been considered in the related area of distributional"
C12-1089,P11-1007,1,0.726455,"Missing"
C12-1089,P10-1040,0,0.785082,"for crosslingual document classification, where classifiers trained on these representations substantially outperform strong baselines (e.g. machine translation) when applied to a new language. KEYWORDS: distributed representations, multilingual learning, direct transfer of annotation. Proceedings of COLING 2012: Technical Papers, pages 1459–1474, COLING 2012, Mumbai, December 2012. 1459 1 Introduction Word representations induced to capture syntactic and semantic properties of words have been extremely useful for numerous natural language processing applications (Collobert and Weston, 2007; Turian et al., 2010). Their primary appeal is that they can be induced using abundant unsupervised data and then used directly or as additional features to alleviate the data sparsity problem common in the supervised learning scenario. Most of the prior work on inducing these representations has focused on a single language, English, which enjoys the largest repository of available annotated resources. In this work, we focus on a single representation for a pair of languages such that semantically similar words are closer to one another in the induced representation irrespective of the language. Learning with the"
C12-1089,P06-2111,0,0.00673623,"Missing"
C12-1089,P10-1115,0,0.00610724,"training on English and testing on German documents (left) and vice versa (right). 6 Additional Related Work In the last decade crosslingual methods have attracted a lot of attention both in NLP and closely related communities such as information retrieval (Lavrenko et al., 2002) and information management (Frederking et al., 2001). Much of this work has focused on techniques for porting methods and resources from one language to another (see, e.g., crosslingual document classification (Fortuna and Shawe-Taylor, 2005; Shi et al., 2010)). Development of crosslingual models (e.g., topic models (Zhang et al., 2010; Mimno et al., 2009)) has also attracted some attention. However, these approaches either do not induce representations of individual words and, as such, may not be very useful for methods dealing with richer linguistic structures (such as syntactic parsing or semantic role labeling) or they focus on porting a specific method (e.g., named entity recognizer (Steinberger and Pouliquen, 2007)). This contrasts significantly with our objective: inducing fine-grain distributed word representation useful in virtually arbitrary NLP problems. Possibly the most related work to ours is the method for in"
C12-1161,P09-1004,0,0.556833,"els according to their underlying semantic role. For example, in the following sentences: (a) [A0 Mary] opened [A1 the door]. (b) [A1 The door] opened. (c) [A1 The door] was opened [A0 by Mary]. Mary always takes an agent role (A0 in the PropBank notation (Palmer et al., 2005)) for the predicate open, and door is always a patient (A1). 2637 In this work we focus on the labeling stage of semantic role labeling. Identification, though an important problem, can be tackled with heuristics (Lang and Lapata, 2011a; Grenager and Manning, 2006; de Marneffe et al., 2006), with unsupervised techniques (Abend et al., 2009) or potentially by using a supervised classifier trained on a small amount of data. In our experiments we use the heuristic identifier of Lang and Lapata (2011a). Also, as in much of the previous work on supervised and unsupervised SRL, we rely on automatically generated syntactic dependency trees. In the labeling stage, semantic roles are represented by clusters of arguments, and labeling a particular argument corresponds to deciding on its role cluster. However, instead of dealing with argument occurrences directly, in BayesSRL they are represented as predicate-specific syntactic signatures,"
C12-1161,P98-1013,0,0.184332,"otated sentences. Our semi-supervised method outperforms a strong supervised baseline when only a small amount of labeled data is available. KEYWORDS: semantic role labeling, semi-supervised learning, shallow semantics, Bayesian model. Proceedings of COLING 2012: Technical Papers, pages 2635–2652, COLING 2012, Mumbai, December 2012. 2635 1 Introduction Shallow representations of meaning, and semantic role labels in particular, have a long history in linguistics (Fillmore, 1968). More recently, with the emergence of large annotated resources such as PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998), automatic semantic role labeling (SRL) has attracted a lot of attention (Gildea and Jurafsky, 2002; Carreras and Màrquez, 2005; Surdeanu et al., 2008; Hajiˇc et al., 2009). SRL representations encode the underlying predicate-argument structure of sentences, or, more specifically, for every predicate in a sentence they identify a set of arguments and associate each argument with an underlying semantic role, such as an agent (an initiator or doer of the action) or a patient (an affected entity). SRL representations have many potential applications in NLP and have been shown to benefit question"
C12-1161,W05-0620,0,0.0771967,"data is available. KEYWORDS: semantic role labeling, semi-supervised learning, shallow semantics, Bayesian model. Proceedings of COLING 2012: Technical Papers, pages 2635–2652, COLING 2012, Mumbai, December 2012. 2635 1 Introduction Shallow representations of meaning, and semantic role labels in particular, have a long history in linguistics (Fillmore, 1968). More recently, with the emergence of large annotated resources such as PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998), automatic semantic role labeling (SRL) has attracted a lot of attention (Gildea and Jurafsky, 2002; Carreras and Màrquez, 2005; Surdeanu et al., 2008; Hajiˇc et al., 2009). SRL representations encode the underlying predicate-argument structure of sentences, or, more specifically, for every predicate in a sentence they identify a set of arguments and associate each argument with an underlying semantic role, such as an agent (an initiator or doer of the action) or a patient (an affected entity). SRL representations have many potential applications in NLP and have been shown to benefit question answering (Shen and Lapata, 2007; Kaisser and Webber, 2007), textual entailment (Sammons et al., 2009), machine translation (Wu"
C12-1161,P08-1090,0,0.0263381,"to SRL discussed in the introduction, semisupervised and weakly-supervised techniques have also been explored for other types of semantic representations but these studies have mostly focused on restricted domains (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Goldwasser et al., 2011; Liang et al., 2011). Similarly, unsupervised induction for other shallow semantic formalisms include Poon and Domingos (2009, 2010) and Titov and Klementiev (2011). A related problem of inducing script knowledge, or narrative event chains, has recently received a considerable attention (Chambers and Jurafsky, 2008; Manshadi et al., 2008; Chambers and Jurafsky, 2009; Regneri et al., 2010, 2011) with approaches mostly considering unsupervised or weakly-supervised setting due to scarcity of labeled data. Though in this paper we focus on the labeling of arguments the complementary task of unsupervised argument identification was considered in Abend et al. (2009). Unsupervised learning has been one of the central paradigms for the closely-related area of relation extraction, where several techniques have been proposed to cluster semantically similar verbalizations of relations (Lin and Pantel, 2001; Banko e"
C12-1161,P09-1068,0,0.0212224,"and weakly-supervised techniques have also been explored for other types of semantic representations but these studies have mostly focused on restricted domains (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Goldwasser et al., 2011; Liang et al., 2011). Similarly, unsupervised induction for other shallow semantic formalisms include Poon and Domingos (2009, 2010) and Titov and Klementiev (2011). A related problem of inducing script knowledge, or narrative event chains, has recently received a considerable attention (Chambers and Jurafsky, 2008; Manshadi et al., 2008; Chambers and Jurafsky, 2009; Regneri et al., 2010, 2011) with approaches mostly considering unsupervised or weakly-supervised setting due to scarcity of labeled data. Though in this paper we focus on the labeling of arguments the complementary task of unsupervised argument identification was considered in Abend et al. (2009). Unsupervised learning has been one of the central paradigms for the closely-related area of relation extraction, where several techniques have been proposed to cluster semantically similar verbalizations of relations (Lin and Pantel, 2001; Banko et al., 2007). Similarly to SRL, semisupervised appro"
C12-1161,J81-4005,0,0.787162,"Missing"
C12-1161,de-marneffe-etal-2006-generating,0,0.0108172,"Missing"
C12-1161,D09-1003,0,0.122344,"ervised and unsupervised learning. The existing semi-supervised approaches to SRL can largely be regarded as extensions to supervised techniques, as they use supervised learning as sub-routines in the estimation process. These include self-training and co-training methods (He and Gildea, 2006b; Lee et al., 2007; Kaljahi and Samad, 2010), mono-lingual and cross-lingual annotation projection (Fürstenau and Lapata, 2009; Pado and Lapata, 2009; van der Plas et al., 2011), and methods which exploit or induce word representations to reduce the sparsity of lexicalized features (He and Gildea, 2006a; Deschacht and Moens, 2009; Collobert et al., 2011). Most of these approaches, especially the bootstrapping-style methods (He and Gildea, 2006b; Lee et al., 2007; Kaljahi and Samad, 2010; Fürstenau and Lapata, 2009), have achieved minimal or even no improvement from using unlabeled data. Consequently, the development of effective semi-supervised techniques remains an important and largely unresolved problem. Another vein of research exploiting unlabeled data for shallow semantic parsing has focused on purely unsupervised set-ups (Swier and Stevenson, 2004; Grenager and Manning, 2006; Lang and Lapata, 2010, 2011a,b; Tit"
C12-1161,D09-1002,0,0.124862,"rformance of these models tends to degrade substantially (Pradhan et al., 2008). Scarcity of annotated data has motivated the research into techniques capable of exploiting unlabeled data, that is, semi-supervised and unsupervised learning. The existing semi-supervised approaches to SRL can largely be regarded as extensions to supervised techniques, as they use supervised learning as sub-routines in the estimation process. These include self-training and co-training methods (He and Gildea, 2006b; Lee et al., 2007; Kaljahi and Samad, 2010), mono-lingual and cross-lingual annotation projection (Fürstenau and Lapata, 2009; Pado and Lapata, 2009; van der Plas et al., 2011), and methods which exploit or induce word representations to reduce the sparsity of lexicalized features (He and Gildea, 2006a; Deschacht and Moens, 2009; Collobert et al., 2011). Most of these approaches, especially the bootstrapping-style methods (He and Gildea, 2006b; Lee et al., 2007; Kaljahi and Samad, 2010; Fürstenau and Lapata, 2009), have achieved minimal or even no improvement from using unlabeled data. Consequently, the development of effective semi-supervised techniques remains an important and largely unresolved problem. Another v"
C12-1161,S12-1026,0,0.0394686,", especially the bootstrapping-style methods (He and Gildea, 2006b; Lee et al., 2007; Kaljahi and Samad, 2010; Fürstenau and Lapata, 2009), have achieved minimal or even no improvement from using unlabeled data. Consequently, the development of effective semi-supervised techniques remains an important and largely unresolved problem. Another vein of research exploiting unlabeled data for shallow semantic parsing has focused on purely unsupervised set-ups (Swier and Stevenson, 2004; Grenager and Manning, 2006; Lang and Lapata, 2010, 2011a,b; Titov and Klementiev, 2012; Garg and Henderson, 2012; Fürstenau and Rambow, 2012). The unsupervised setting is important in itself, and the development of these methods arguably provides interesting insights into modeling implicit supervision signals present in unlabeled data. However, given that small amounts of labeled data are often easy to obtain, it is surprising that no previous work that we are aware of looked into integration of labeled data into unsupervised SRL systems.1 Moreover, due to the inherent difference in the clustering metrics used for unsupervised SRL and the labeled accuracy scores used to evaluate supervised SRL methods, they have so far never been p"
C12-1161,P11-2051,0,0.0599104,"sentations encode the underlying predicate-argument structure of sentences, or, more specifically, for every predicate in a sentence they identify a set of arguments and associate each argument with an underlying semantic role, such as an agent (an initiator or doer of the action) or a patient (an affected entity). SRL representations have many potential applications in NLP and have been shown to benefit question answering (Shen and Lapata, 2007; Kaisser and Webber, 2007), textual entailment (Sammons et al., 2009), machine translation (Wu and Fung, 2009; Liu and Gildea, 2010; Wu et al., 2011; Gao and Vogel, 2011), and dialogue systems (Basili et al., 2009; van der Plas et al., 2009), among others. Most of the current statistical approaches to SRL are supervised, requiring large quantities of human annotated data to estimate model parameters. However, such resources are expensive to create and only available for a small number of languages and domains. Moreover, when moved to a new domain, performance of these models tends to degrade substantially (Pradhan et al., 2008). Scarcity of annotated data has motivated the research into techniques capable of exploiting unlabeled data, that is, semi-supervised"
C12-1161,P12-2029,0,0.0714618,". Most of these approaches, especially the bootstrapping-style methods (He and Gildea, 2006b; Lee et al., 2007; Kaljahi and Samad, 2010; Fürstenau and Lapata, 2009), have achieved minimal or even no improvement from using unlabeled data. Consequently, the development of effective semi-supervised techniques remains an important and largely unresolved problem. Another vein of research exploiting unlabeled data for shallow semantic parsing has focused on purely unsupervised set-ups (Swier and Stevenson, 2004; Grenager and Manning, 2006; Lang and Lapata, 2010, 2011a,b; Titov and Klementiev, 2012; Garg and Henderson, 2012; Fürstenau and Rambow, 2012). The unsupervised setting is important in itself, and the development of these methods arguably provides interesting insights into modeling implicit supervision signals present in unlabeled data. However, given that small amounts of labeled data are often easy to obtain, it is surprising that no previous work that we are aware of looked into integration of labeled data into unsupervised SRL systems.1 Moreover, due to the inherent difference in the clustering metrics used for unsupervised SRL and the labeled accuracy scores used to evaluate supervised SRL methods,"
C12-1161,P11-1149,0,0.0210271,"g semi-supervised SRL by exploiting labeled data in unsupervised methods is a promising research direction. Existing state-of-the-art methods can already be used for languages and domains for which little or no annotated data is available. 5 Additional Related Work Additionally to the semi-supervised approaches to SRL discussed in the introduction, semisupervised and weakly-supervised techniques have also been explored for other types of semantic representations but these studies have mostly focused on restricted domains (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Goldwasser et al., 2011; Liang et al., 2011). Similarly, unsupervised induction for other shallow semantic formalisms include Poon and Domingos (2009, 2010) and Titov and Klementiev (2011). A related problem of inducing script knowledge, or narrative event chains, has recently received a considerable attention (Chambers and Jurafsky, 2008; Manshadi et al., 2008; Chambers and Jurafsky, 2009; Regneri et al., 2010, 2011) with approaches mostly considering unsupervised or weakly-supervised setting due to scarcity of labeled data. Though in this paper we focus on the labeling of arguments the complementary task of unsupe"
C12-1161,W06-1601,0,0.510181,"lized features (He and Gildea, 2006a; Deschacht and Moens, 2009; Collobert et al., 2011). Most of these approaches, especially the bootstrapping-style methods (He and Gildea, 2006b; Lee et al., 2007; Kaljahi and Samad, 2010; Fürstenau and Lapata, 2009), have achieved minimal or even no improvement from using unlabeled data. Consequently, the development of effective semi-supervised techniques remains an important and largely unresolved problem. Another vein of research exploiting unlabeled data for shallow semantic parsing has focused on purely unsupervised set-ups (Swier and Stevenson, 2004; Grenager and Manning, 2006; Lang and Lapata, 2010, 2011a,b; Titov and Klementiev, 2012; Garg and Henderson, 2012; Fürstenau and Rambow, 2012). The unsupervised setting is important in itself, and the development of these methods arguably provides interesting insights into modeling implicit supervision signals present in unlabeled data. However, given that small amounts of labeled data are often easy to obtain, it is surprising that no previous work that we are aware of looked into integration of labeled data into unsupervised SRL systems.1 Moreover, due to the inherent difference in the clustering metrics used for unsu"
C12-1161,W09-1201,0,0.197321,"Missing"
C12-1161,D08-1008,0,0.445123,"led data into learning of a generative model would amount to maximizing joint probability of labeled and unlabeled data. However, due to hard constraints in the BayesSRL model and the great disbalance between the amount of labeled and unlabeled data, we argue that a different approach is preferred. Namely, we use labeled data to construct an informed prior over the potential semantic representations and also modify the model to integrate the labels as soft constraints on admissible semantic structures. We compare the semi-supervised approach we propose to a state-of-the-art supervised method (Johansson and Nugues, 2008a). Though the BayesSRL model exploits a cross-predicate representation, it does not align roles across predicates which prevents us from using supervised evaluation metrics. Consequently, we evaluate the methods using clustering measures: the harmonic mean of purity and collocation, a common metric for unsupervised SRL evaluation (Lang and Lapata, 2010), and the information-theoretic V-Measure (Rosenberg and Hirschberg, 2007). The semi-supervised method outperforms its supervised counterpart when the amount of labeled data is small. Unsurprisingly, it does not fare as well when the amount of"
C12-1161,W07-1206,0,0.0953408,"(SRL) has attracted a lot of attention (Gildea and Jurafsky, 2002; Carreras and Màrquez, 2005; Surdeanu et al., 2008; Hajiˇc et al., 2009). SRL representations encode the underlying predicate-argument structure of sentences, or, more specifically, for every predicate in a sentence they identify a set of arguments and associate each argument with an underlying semantic role, such as an agent (an initiator or doer of the action) or a patient (an affected entity). SRL representations have many potential applications in NLP and have been shown to benefit question answering (Shen and Lapata, 2007; Kaisser and Webber, 2007), textual entailment (Sammons et al., 2009), machine translation (Wu and Fung, 2009; Liu and Gildea, 2010; Wu et al., 2011; Gao and Vogel, 2011), and dialogue systems (Basili et al., 2009; van der Plas et al., 2009), among others. Most of the current statistical approaches to SRL are supervised, requiring large quantities of human annotated data to estimate model parameters. However, such resources are expensive to create and only available for a small number of languages and domains. Moreover, when moved to a new domain, performance of these models tends to degrade substantially (Pradhan et a"
C12-1161,P10-3016,0,0.018456,"small number of languages and domains. Moreover, when moved to a new domain, performance of these models tends to degrade substantially (Pradhan et al., 2008). Scarcity of annotated data has motivated the research into techniques capable of exploiting unlabeled data, that is, semi-supervised and unsupervised learning. The existing semi-supervised approaches to SRL can largely be regarded as extensions to supervised techniques, as they use supervised learning as sub-routines in the estimation process. These include self-training and co-training methods (He and Gildea, 2006b; Lee et al., 2007; Kaljahi and Samad, 2010), mono-lingual and cross-lingual annotation projection (Fürstenau and Lapata, 2009; Pado and Lapata, 2009; van der Plas et al., 2011), and methods which exploit or induce word representations to reduce the sparsity of lexicalized features (He and Gildea, 2006a; Deschacht and Moens, 2009; Collobert et al., 2011). Most of these approaches, especially the bootstrapping-style methods (He and Gildea, 2006b; Lee et al., 2007; Kaljahi and Samad, 2010; Fürstenau and Lapata, 2009), have achieved minimal or even no improvement from using unlabeled data. Consequently, the development of effective semi-su"
C12-1161,N10-1137,0,0.352264,"a, 2006a; Deschacht and Moens, 2009; Collobert et al., 2011). Most of these approaches, especially the bootstrapping-style methods (He and Gildea, 2006b; Lee et al., 2007; Kaljahi and Samad, 2010; Fürstenau and Lapata, 2009), have achieved minimal or even no improvement from using unlabeled data. Consequently, the development of effective semi-supervised techniques remains an important and largely unresolved problem. Another vein of research exploiting unlabeled data for shallow semantic parsing has focused on purely unsupervised set-ups (Swier and Stevenson, 2004; Grenager and Manning, 2006; Lang and Lapata, 2010, 2011a,b; Titov and Klementiev, 2012; Garg and Henderson, 2012; Fürstenau and Rambow, 2012). The unsupervised setting is important in itself, and the development of these methods arguably provides interesting insights into modeling implicit supervision signals present in unlabeled data. However, given that small amounts of labeled data are often easy to obtain, it is surprising that no previous work that we are aware of looked into integration of labeled data into unsupervised SRL systems.1 Moreover, due to the inherent difference in the clustering metrics used for unsupervised SRL and the la"
C12-1161,P11-1112,0,0.150344,"rediction of predicate argument structure, i.e. both identification of arguments as well as assignment of labels according to their underlying semantic role. For example, in the following sentences: (a) [A0 Mary] opened [A1 the door]. (b) [A1 The door] opened. (c) [A1 The door] was opened [A0 by Mary]. Mary always takes an agent role (A0 in the PropBank notation (Palmer et al., 2005)) for the predicate open, and door is always a patient (A1). 2637 In this work we focus on the labeling stage of semantic role labeling. Identification, though an important problem, can be tackled with heuristics (Lang and Lapata, 2011a; Grenager and Manning, 2006; de Marneffe et al., 2006), with unsupervised techniques (Abend et al., 2009) or potentially by using a supervised classifier trained on a small amount of data. In our experiments we use the heuristic identifier of Lang and Lapata (2011a). Also, as in much of the previous work on supervised and unsupervised SRL, we rely on automatically generated syntactic dependency trees. In the labeling stage, semantic roles are represented by clusters of arguments, and labeling a particular argument corresponds to deciding on its role cluster. However, instead of dealing with"
C12-1161,D11-1122,0,0.161039,"rediction of predicate argument structure, i.e. both identification of arguments as well as assignment of labels according to their underlying semantic role. For example, in the following sentences: (a) [A0 Mary] opened [A1 the door]. (b) [A1 The door] opened. (c) [A1 The door] was opened [A0 by Mary]. Mary always takes an agent role (A0 in the PropBank notation (Palmer et al., 2005)) for the predicate open, and door is always a patient (A1). 2637 In this work we focus on the labeling stage of semantic role labeling. Identification, though an important problem, can be tackled with heuristics (Lang and Lapata, 2011a; Grenager and Manning, 2006; de Marneffe et al., 2006), with unsupervised techniques (Abend et al., 2009) or potentially by using a supervised classifier trained on a small amount of data. In our experiments we use the heuristic identifier of Lang and Lapata (2011a). Also, as in much of the previous work on supervised and unsupervised SRL, we rely on automatically generated syntactic dependency trees. In the labeling stage, semantic roles are represented by clusters of arguments, and labeling a particular argument corresponds to deciding on its role cluster. However, instead of dealing with"
C12-1161,P11-1060,0,0.0210029,"exploiting labeled data in unsupervised methods is a promising research direction. Existing state-of-the-art methods can already be used for languages and domains for which little or no annotated data is available. 5 Additional Related Work Additionally to the semi-supervised approaches to SRL discussed in the introduction, semisupervised and weakly-supervised techniques have also been explored for other types of semantic representations but these studies have mostly focused on restricted domains (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Goldwasser et al., 2011; Liang et al., 2011). Similarly, unsupervised induction for other shallow semantic formalisms include Poon and Domingos (2009, 2010) and Titov and Klementiev (2011). A related problem of inducing script knowledge, or narrative event chains, has recently received a considerable attention (Chambers and Jurafsky, 2008; Manshadi et al., 2008; Chambers and Jurafsky, 2009; Regneri et al., 2010, 2011) with approaches mostly considering unsupervised or weakly-supervised setting due to scarcity of labeled data. Though in this paper we focus on the labeling of arguments the complementary task of unsupervised argument ident"
C12-1161,P09-1011,0,0.0317139,"ver, our results strongly suggest that approaching semi-supervised SRL by exploiting labeled data in unsupervised methods is a promising research direction. Existing state-of-the-art methods can already be used for languages and domains for which little or no annotated data is available. 5 Additional Related Work Additionally to the semi-supervised approaches to SRL discussed in the introduction, semisupervised and weakly-supervised techniques have also been explored for other types of semantic representations but these studies have mostly focused on restricted domains (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Goldwasser et al., 2011; Liang et al., 2011). Similarly, unsupervised induction for other shallow semantic formalisms include Poon and Domingos (2009, 2010) and Titov and Klementiev (2011). A related problem of inducing script knowledge, or narrative event chains, has recently received a considerable attention (Chambers and Jurafsky, 2008; Manshadi et al., 2008; Chambers and Jurafsky, 2009; Regneri et al., 2010, 2011) with approaches mostly considering unsupervised or weakly-supervised setting due to scarcity of labeled data. Though in this paper we focus on the"
C12-1161,C10-1081,0,0.0426271,", 2008; Hajiˇc et al., 2009). SRL representations encode the underlying predicate-argument structure of sentences, or, more specifically, for every predicate in a sentence they identify a set of arguments and associate each argument with an underlying semantic role, such as an agent (an initiator or doer of the action) or a patient (an affected entity). SRL representations have many potential applications in NLP and have been shown to benefit question answering (Shen and Lapata, 2007; Kaisser and Webber, 2007), textual entailment (Sammons et al., 2009), machine translation (Wu and Fung, 2009; Liu and Gildea, 2010; Wu et al., 2011; Gao and Vogel, 2011), and dialogue systems (Basili et al., 2009; van der Plas et al., 2009), among others. Most of the current statistical approaches to SRL are supervised, requiring large quantities of human annotated data to estimate model parameters. However, such resources are expensive to create and only available for a small number of languages and domains. Moreover, when moved to a new domain, performance of these models tends to degrade substantially (Pradhan et al., 2008). Scarcity of annotated data has motivated the research into techniques capable of exploiting un"
C12-1161,J93-2004,0,0.0390146,"Missing"
C12-1161,J05-1004,0,0.735362,"ction to better accommodate for annotated sentences. Our semi-supervised method outperforms a strong supervised baseline when only a small amount of labeled data is available. KEYWORDS: semantic role labeling, semi-supervised learning, shallow semantics, Bayesian model. Proceedings of COLING 2012: Technical Papers, pages 2635–2652, COLING 2012, Mumbai, December 2012. 2635 1 Introduction Shallow representations of meaning, and semantic role labels in particular, have a long history in linguistics (Fillmore, 1968). More recently, with the emergence of large annotated resources such as PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998), automatic semantic role labeling (SRL) has attracted a lot of attention (Gildea and Jurafsky, 2002; Carreras and Màrquez, 2005; Surdeanu et al., 2008; Hajiˇc et al., 2009). SRL representations encode the underlying predicate-argument structure of sentences, or, more specifically, for every predicate in a sentence they identify a set of arguments and associate each argument with an underlying semantic role, such as an agent (an initiator or doer of the action) or a patient (an affected entity). SRL representations have many potential applications in NLP and h"
C12-1161,D09-1001,0,0.0337001,"the-art methods can already be used for languages and domains for which little or no annotated data is available. 5 Additional Related Work Additionally to the semi-supervised approaches to SRL discussed in the introduction, semisupervised and weakly-supervised techniques have also been explored for other types of semantic representations but these studies have mostly focused on restricted domains (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Goldwasser et al., 2011; Liang et al., 2011). Similarly, unsupervised induction for other shallow semantic formalisms include Poon and Domingos (2009, 2010) and Titov and Klementiev (2011). A related problem of inducing script knowledge, or narrative event chains, has recently received a considerable attention (Chambers and Jurafsky, 2008; Manshadi et al., 2008; Chambers and Jurafsky, 2009; Regneri et al., 2010, 2011) with approaches mostly considering unsupervised or weakly-supervised setting due to scarcity of labeled data. Though in this paper we focus on the labeling of arguments the complementary task of unsupervised argument identification was considered in Abend et al. (2009). Unsupervised learning has been one of the central paradi"
C12-1161,P10-1031,0,0.0462471,"Missing"
C12-1161,J08-2006,0,0.114367,"ebber, 2007), textual entailment (Sammons et al., 2009), machine translation (Wu and Fung, 2009; Liu and Gildea, 2010; Wu et al., 2011; Gao and Vogel, 2011), and dialogue systems (Basili et al., 2009; van der Plas et al., 2009), among others. Most of the current statistical approaches to SRL are supervised, requiring large quantities of human annotated data to estimate model parameters. However, such resources are expensive to create and only available for a small number of languages and domains. Moreover, when moved to a new domain, performance of these models tends to degrade substantially (Pradhan et al., 2008). Scarcity of annotated data has motivated the research into techniques capable of exploiting unlabeled data, that is, semi-supervised and unsupervised learning. The existing semi-supervised approaches to SRL can largely be regarded as extensions to supervised techniques, as they use supervised learning as sub-routines in the estimation process. These include self-training and co-training methods (He and Gildea, 2006b; Lee et al., 2007; Kaljahi and Samad, 2010), mono-lingual and cross-lingual annotation projection (Fürstenau and Lapata, 2009; Pado and Lapata, 2009; van der Plas et al., 2011),"
C12-1161,P10-1100,0,0.0157944,"ques have also been explored for other types of semantic representations but these studies have mostly focused on restricted domains (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Goldwasser et al., 2011; Liang et al., 2011). Similarly, unsupervised induction for other shallow semantic formalisms include Poon and Domingos (2009, 2010) and Titov and Klementiev (2011). A related problem of inducing script knowledge, or narrative event chains, has recently received a considerable attention (Chambers and Jurafsky, 2008; Manshadi et al., 2008; Chambers and Jurafsky, 2009; Regneri et al., 2010, 2011) with approaches mostly considering unsupervised or weakly-supervised setting due to scarcity of labeled data. Though in this paper we focus on the labeling of arguments the complementary task of unsupervised argument identification was considered in Abend et al. (2009). Unsupervised learning has been one of the central paradigms for the closely-related area of relation extraction, where several techniques have been proposed to cluster semantically similar verbalizations of relations (Lin and Pantel, 2001; Banko et al., 2007). Similarly to SRL, semisupervised approaches in this area are"
C12-1161,R11-1064,0,0.0238259,"Missing"
C12-1161,D07-1043,0,0.0589163,"del to integrate the labels as soft constraints on admissible semantic structures. We compare the semi-supervised approach we propose to a state-of-the-art supervised method (Johansson and Nugues, 2008a). Though the BayesSRL model exploits a cross-predicate representation, it does not align roles across predicates which prevents us from using supervised evaluation metrics. Consequently, we evaluate the methods using clustering measures: the harmonic mean of purity and collocation, a common metric for unsupervised SRL evaluation (Lang and Lapata, 2010), and the information-theoretic V-Measure (Rosenberg and Hirschberg, 2007). The semi-supervised method outperforms its supervised counterpart when the amount of labeled data is small. Unsurprisingly, it does not fare as well when the amount of data increases. We believe that this is primarily due to the overly coarse modeling of the syntax-semantics interface, as it is optimized for the unsupervised setting. Nevertheless, these results strongly suggest that approaching the semi-supervised learning setting for SRL from an unsupervised perspective is a promising research direction and that the existing unsupervised SRL methods are already mature enough to be useful fo"
C12-1161,P07-1076,0,0.0213571,"e to scarcity of labeled data. Though in this paper we focus on the labeling of arguments the complementary task of unsupervised argument identification was considered in Abend et al. (2009). Unsupervised learning has been one of the central paradigms for the closely-related area of relation extraction, where several techniques have been proposed to cluster semantically similar verbalizations of relations (Lin and Pantel, 2001; Banko et al., 2007). Similarly to SRL, semisupervised approaches in this area are also typically based on bootstrapping techniques (e.g., (Agichtein and Gravano, 2000; Rosenfeld and Feldman, 2007)) and often achieve impressive results. However, their set-up is arguably different from ours as relation extractors are generally more precision-oriented, focus primarily on binary relations and can partially sidestep the complexity of language. 6 Conclusions In this work, we demonstrated that unsupervised techniques can be improved by exploiting small amounts of labeled data yielding SRL parsers competitive with supervised approaches in a low resource setting. We also uncovered some of the deficiencies of the existing unsupervised approaches; namely, overly coarse modeling of syntax-semantic"
C12-1161,D07-1002,0,0.137781,"semantic role labeling (SRL) has attracted a lot of attention (Gildea and Jurafsky, 2002; Carreras and Màrquez, 2005; Surdeanu et al., 2008; Hajiˇc et al., 2009). SRL representations encode the underlying predicate-argument structure of sentences, or, more specifically, for every predicate in a sentence they identify a set of arguments and associate each argument with an underlying semantic role, such as an agent (an initiator or doer of the action) or a patient (an affected entity). SRL representations have many potential applications in NLP and have been shown to benefit question answering (Shen and Lapata, 2007; Kaisser and Webber, 2007), textual entailment (Sammons et al., 2009), machine translation (Wu and Fung, 2009; Liu and Gildea, 2010; Wu et al., 2011; Gao and Vogel, 2011), and dialogue systems (Basili et al., 2009; van der Plas et al., 2009), among others. Most of the current statistical approaches to SRL are supervised, requiring large quantities of human annotated data to estimate model parameters. However, such resources are expensive to create and only available for a small number of languages and domains. Moreover, when moved to a new domain, performance of these models tends to degrade"
C12-1161,W08-2121,0,0.299616,": semantic role labeling, semi-supervised learning, shallow semantics, Bayesian model. Proceedings of COLING 2012: Technical Papers, pages 2635–2652, COLING 2012, Mumbai, December 2012. 2635 1 Introduction Shallow representations of meaning, and semantic role labels in particular, have a long history in linguistics (Fillmore, 1968). More recently, with the emergence of large annotated resources such as PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998), automatic semantic role labeling (SRL) has attracted a lot of attention (Gildea and Jurafsky, 2002; Carreras and Màrquez, 2005; Surdeanu et al., 2008; Hajiˇc et al., 2009). SRL representations encode the underlying predicate-argument structure of sentences, or, more specifically, for every predicate in a sentence they identify a set of arguments and associate each argument with an underlying semantic role, such as an agent (an initiator or doer of the action) or a patient (an affected entity). SRL representations have many potential applications in NLP and have been shown to benefit question answering (Shen and Lapata, 2007; Kaisser and Webber, 2007), textual entailment (Sammons et al., 2009), machine translation (Wu and Fung, 2009; Liu an"
C12-1161,P11-1145,1,0.858373,"for languages and domains for which little or no annotated data is available. 5 Additional Related Work Additionally to the semi-supervised approaches to SRL discussed in the introduction, semisupervised and weakly-supervised techniques have also been explored for other types of semantic representations but these studies have mostly focused on restricted domains (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Goldwasser et al., 2011; Liang et al., 2011). Similarly, unsupervised induction for other shallow semantic formalisms include Poon and Domingos (2009, 2010) and Titov and Klementiev (2011). A related problem of inducing script knowledge, or narrative event chains, has recently received a considerable attention (Chambers and Jurafsky, 2008; Manshadi et al., 2008; Chambers and Jurafsky, 2009; Regneri et al., 2010, 2011) with approaches mostly considering unsupervised or weakly-supervised setting due to scarcity of labeled data. Though in this paper we focus on the labeling of arguments the complementary task of unsupervised argument identification was considered in Abend et al. (2009). Unsupervised learning has been one of the central paradigms for the closely-related area of rel"
C12-1161,E12-1003,1,0.301847,"009; Collobert et al., 2011). Most of these approaches, especially the bootstrapping-style methods (He and Gildea, 2006b; Lee et al., 2007; Kaljahi and Samad, 2010; Fürstenau and Lapata, 2009), have achieved minimal or even no improvement from using unlabeled data. Consequently, the development of effective semi-supervised techniques remains an important and largely unresolved problem. Another vein of research exploiting unlabeled data for shallow semantic parsing has focused on purely unsupervised set-ups (Swier and Stevenson, 2004; Grenager and Manning, 2006; Lang and Lapata, 2010, 2011a,b; Titov and Klementiev, 2012; Garg and Henderson, 2012; Fürstenau and Rambow, 2012). The unsupervised setting is important in itself, and the development of these methods arguably provides interesting insights into modeling implicit supervision signals present in unlabeled data. However, given that small amounts of labeled data are often easy to obtain, it is surprising that no previous work that we are aware of looked into integration of labeled data into unsupervised SRL systems.1 Moreover, due to the inherent difference in the clustering metrics used for unsupervised SRL and the labeled accuracy scores used to evaluat"
C12-1161,P10-1098,1,0.84739,"ongly suggest that approaching semi-supervised SRL by exploiting labeled data in unsupervised methods is a promising research direction. Existing state-of-the-art methods can already be used for languages and domains for which little or no annotated data is available. 5 Additional Related Work Additionally to the semi-supervised approaches to SRL discussed in the introduction, semisupervised and weakly-supervised techniques have also been explored for other types of semantic representations but these studies have mostly focused on restricted domains (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Goldwasser et al., 2011; Liang et al., 2011). Similarly, unsupervised induction for other shallow semantic formalisms include Poon and Domingos (2009, 2010) and Titov and Klementiev (2011). A related problem of inducing script knowledge, or narrative event chains, has recently received a considerable attention (Chambers and Jurafsky, 2008; Manshadi et al., 2008; Chambers and Jurafsky, 2009; Regneri et al., 2010, 2011) with approaches mostly considering unsupervised or weakly-supervised setting due to scarcity of labeled data. Though in this paper we focus on the labeling of arguments the com"
C12-1161,N09-2032,0,0.156489,"Missing"
C12-1161,P11-2052,0,0.0751272,"Missing"
C12-1161,N09-2004,0,0.0215398,"05; Surdeanu et al., 2008; Hajiˇc et al., 2009). SRL representations encode the underlying predicate-argument structure of sentences, or, more specifically, for every predicate in a sentence they identify a set of arguments and associate each argument with an underlying semantic role, such as an agent (an initiator or doer of the action) or a patient (an affected entity). SRL representations have many potential applications in NLP and have been shown to benefit question answering (Shen and Lapata, 2007; Kaisser and Webber, 2007), textual entailment (Sammons et al., 2009), machine translation (Wu and Fung, 2009; Liu and Gildea, 2010; Wu et al., 2011; Gao and Vogel, 2011), and dialogue systems (Basili et al., 2009; van der Plas et al., 2009), among others. Most of the current statistical approaches to SRL are supervised, requiring large quantities of human annotated data to estimate model parameters. However, such resources are expensive to create and only available for a small number of languages and domains. Moreover, when moved to a new domain, performance of these models tends to degrade substantially (Pradhan et al., 2008). Scarcity of annotated data has motivated the research into techniques ca"
C12-1161,N07-1070,0,\N,Missing
C12-1161,C98-1013,0,\N,Missing
C12-1161,J02-3001,0,\N,Missing
C18-1151,P17-1151,0,0.0328616,"Missing"
C18-1151,W11-2501,0,0.0835167,"Missing"
C18-1151,P15-1077,0,0.0794181,"Missing"
C18-1151,D09-1046,0,0.0744186,"Missing"
C18-1151,P14-5004,0,0.0606016,"Missing"
C18-1151,C14-1137,0,0.0612503,"Missing"
C18-1151,P16-1193,0,0.035142,"Missing"
C18-1151,P12-1092,0,0.148742,"Missing"
C18-1151,N15-1098,0,0.0889386,"Missing"
C18-1151,D15-1200,0,0.0849929,"Missing"
C18-1151,S07-1009,0,0.0612768,"Missing"
C18-1151,W15-1501,0,0.0847621,"Missing"
C18-1151,K16-1006,0,0.0477803,"Missing"
C18-1151,D16-1089,0,0.066608,"Missing"
C18-1151,D14-1113,0,0.0827983,"Missing"
C18-1151,D14-1162,0,0.102333,"Missing"
C18-1151,N18-1202,0,0.121266,"Missing"
C18-1151,N10-1013,0,0.0547952,"Missing"
C18-1151,J17-4004,0,0.0313491,"Missing"
C18-1151,C14-1212,0,0.111434,"Missing"
C18-1151,E12-1004,0,0.0693131,"Missing"
D07-1099,P05-1023,1,0.872299,"Missing"
D07-1099,N03-1014,1,0.923782,"he parse history, the ISBN graphical model can specify conditional dependency edges between latent variables which are arbitrarily far apart in the parse history. The source state of such an edge is determined by the partial parse structure built at the time of the destination state, thereby allowing the conditional dependency edges to be appropriate for the structural nature of the parsing problem. In particular, they allow conditional dependencies to be local in the parse structure, not just local in the history sequence. In this they are similar to the class of neural networks proposed in (Henderson, 2003) for constituent parsing. In fact, in (Titov and Henderson, 2007a) it was shown that this neural network can be viewed as a coarse approximation to the corresponding ISBN model. Traditional statistical parsing models also condition on features which are local in the parse structure, but these features need to be explicitly defined before learning, and require careful feature selection. This is especially difficult for languages unknown to the parser developer, since the number of possible features grows exponentially with the structural distance considered. The ISBN model uses an alternative a"
D07-1099,P04-1013,1,0.915627,"Missing"
D07-1099,W07-2416,0,0.0672376,"Missing"
D07-1099,J93-2004,0,0.0350868,"Missing"
D07-1099,W06-2932,0,0.037245,"N dependency parser in the multilingual shared task setup and achieved competitive accuracy on every language, and the third best average score overall. The proposed model requires minimal design effort because it relies mostly on automatic feature induction, which is highly desirable when using new treebanks or languages. The parsing time needed to achieve high accuracy is also quite small, making this model a good candidate for use in practical applications. The fact that our model defines a probability model over parse trees, unlike the previous stateof-the-art methods (Nivre et al., 2006; McDonald et al., 2006), makes it easier to use this model in applications which require probability estimates, such as in language processing pipelines or for language modeling. Also, as with any generative model, it should be easy to improve the parser’s accuracy with discriminative reranking, such as discriminative retraining techniques (Henderson, 2004) or data-defined kernels (Henderson and Titov, 2005), with or even without the introduction of any additional linguistic features. Acknowledgments This work was funded by Swiss NSF grant 200020109685, UK EPSRC grant EP/E019501/1, and EU FP6 grant 507802 for projec"
D07-1099,P05-1013,0,0.0177838,"al Dufour CH-1211 Gen`eve 4, Switzerland ivan.titov@cui.unige.ch James Henderson University of Edinburgh 2 Buccleuch Place Edinburgh EH8 9LW, United Kingdom james.henderson@ed.ac.uk Abstract CoNLL-X shared task (Buchholz and Marsi, 2006). This parser employs a latent variable model, Incremental Sigmoid Belief Networks (ISBNs), to define a generative history-based model of projective parsing. We used the pseudo-projective transformation introduced in (Nivre and Nilsson, 2005) to cast non-projective parsing tasks as projective. Following (Nivre et al., 2006), the encoding scheme called HEAD in (Nivre and Nilsson, 2005) was used to encode the original non-projective dependencies in the labels of the projectivized dependency tree. In the following sections we will briefly discuss our modifications to the ISBN parser, experimental setup, and achieved results. We use a generative history-based model to predict the most likely derivation of a dependency parse. Our probabilistic model is based on Incremental Sigmoid Belief Networks, a recently proposed class of latent variable models for structure prediction. Their ability to automatically induce features results in multilingual parsing which is robust enough to"
D07-1099,W04-2407,0,0.162111,"(Nivre et al., 2007) considers dependency parsing of texts written in different languages. It requires use of a single dependency parsing model for the entire set of languages; model parameters are estimated individually for each language on the basis of provided training sets. We use a recently proposed dependency parser (Titov and Henderson, 2007b)1 which has demonstrated state-of-theart performance on a selection of languages from the 1 The ISBN parser will be soon made downloadable from the authors’ web-page. The Probability Model Our probability model uses the parsing order proposed in (Nivre et al., 2004), but instead of performing deterministic parsing as in (Nivre et al., 2004), this ordering is used to define a generative historybased model, by adding word prediction to the Shift parser action. We also decomposed some parser actions into sub-sequences of decisions. We split arc prediction decisions (Left-Arcr and Right-Arcr ) each into two elementary decisions: first the parser creates the corresponding arc, then it assigns a relation r to the arc. Similarly, we decompose the decision to shift a word into a decision to shift and a prediction of the word. We used part-of-speech tags and fine"
D07-1099,W06-2933,0,0.206262,"ble Model Ivan Titov University of Geneva 24, rue G´en´eral Dufour CH-1211 Gen`eve 4, Switzerland ivan.titov@cui.unige.ch James Henderson University of Edinburgh 2 Buccleuch Place Edinburgh EH8 9LW, United Kingdom james.henderson@ed.ac.uk Abstract CoNLL-X shared task (Buchholz and Marsi, 2006). This parser employs a latent variable model, Incremental Sigmoid Belief Networks (ISBNs), to define a generative history-based model of projective parsing. We used the pseudo-projective transformation introduced in (Nivre and Nilsson, 2005) to cast non-projective parsing tasks as projective. Following (Nivre et al., 2006), the encoding scheme called HEAD in (Nivre and Nilsson, 2005) was used to encode the original non-projective dependencies in the labels of the projectivized dependency tree. In the following sections we will briefly discuss our modifications to the ISBN parser, experimental setup, and achieved results. We use a generative history-based model to predict the most likely derivation of a dependency parse. Our probabilistic model is based on Incremental Sigmoid Belief Networks, a recently proposed class of latent variable models for structure prediction. Their ability to automatically induce featu"
D07-1099,P07-1080,1,0.698229,"e labeled attachment score in the task, despite using no discriminative methods. We also demonstrate that the parser is quite fast, and can provide even faster parsing times without much loss of accuracy. 1 2 Introduction The multilingual track of the CoNLL-2007 shared task (Nivre et al., 2007) considers dependency parsing of texts written in different languages. It requires use of a single dependency parsing model for the entire set of languages; model parameters are estimated individually for each language on the basis of provided training sets. We use a recently proposed dependency parser (Titov and Henderson, 2007b)1 which has demonstrated state-of-theart performance on a selection of languages from the 1 The ISBN parser will be soon made downloadable from the authors’ web-page. The Probability Model Our probability model uses the parsing order proposed in (Nivre et al., 2004), but instead of performing deterministic parsing as in (Nivre et al., 2004), this ordering is used to define a generative historybased model, by adding word prediction to the Shift parser action. We also decomposed some parser actions into sub-sequences of decisions. We split arc prediction decisions (Left-Arcr and Right-Arcr ) e"
D07-1099,W07-2218,1,0.787169,"e labeled attachment score in the task, despite using no discriminative methods. We also demonstrate that the parser is quite fast, and can provide even faster parsing times without much loss of accuracy. 1 2 Introduction The multilingual track of the CoNLL-2007 shared task (Nivre et al., 2007) considers dependency parsing of texts written in different languages. It requires use of a single dependency parsing model for the entire set of languages; model parameters are estimated individually for each language on the basis of provided training sets. We use a recently proposed dependency parser (Titov and Henderson, 2007b)1 which has demonstrated state-of-theart performance on a selection of languages from the 1 The ISBN parser will be soon made downloadable from the authors’ web-page. The Probability Model Our probability model uses the parsing order proposed in (Nivre et al., 2004), but instead of performing deterministic parsing as in (Nivre et al., 2004), this ordering is used to define a generative historybased model, by adding word prediction to the Shift parser action. We also decomposed some parser actions into sub-sequences of decisions. We split arc prediction decisions (Left-Arcr and Right-Arcr ) e"
D07-1099,W06-2920,0,\N,Missing
D07-1099,D07-1096,0,\N,Missing
D13-1134,W11-2815,1,0.839165,"ounding process, and should ideally respond to the user’s observed behavior in real time. For instance, if the user of a pedestrian navigation system takes a wrong turn, the system should interpret this as evidence of misunderstanding and bring the user back on track. We focus here on the problem of predicting how the user has resolved a referring expression (RE) that was generated by the system, i.e. a noun phrase that is intended to identify some object uniquely to the listener. A number of authors have recently offered statistical models for parts of this problem. Golland et al. (2010) and Garoufi and Koller (2011) have presented log-linear models for predicting how the listener will resolve a given RE in a given scene; however, these models do not update the probability model based on observing the user’s reactions. Nakano et al. (2007), Buschmeier and Kopp (2012), and Koller et al. (2012) all predict what the listener understood based on their behavior, but do not consider the RE itself in the model. The models of Frank and Goodman (2012) and Vogel et al. (2013) aim at explaining the effect of implicatures on the listener’s RE resolution process in terms of hypothesized interactions, but do not actual"
D13-1134,D10-1040,0,0.0392935,"a user must model this grounding process, and should ideally respond to the user’s observed behavior in real time. For instance, if the user of a pedestrian navigation system takes a wrong turn, the system should interpret this as evidence of misunderstanding and bring the user back on track. We focus here on the problem of predicting how the user has resolved a referring expression (RE) that was generated by the system, i.e. a noun phrase that is intended to identify some object uniquely to the listener. A number of authors have recently offered statistical models for parts of this problem. Golland et al. (2010) and Garoufi and Koller (2011) have presented log-linear models for predicting how the listener will resolve a given RE in a given scene; however, these models do not update the probability model based on observing the user’s reactions. Nakano et al. (2007), Buschmeier and Kopp (2012), and Koller et al. (2012) all predict what the listener understood based on their behavior, but do not consider the RE itself in the model. The models of Frank and Goodman (2012) and Vogel et al. (2013) aim at explaining the effect of implicatures on the listener’s RE resolution process in terms of hypothesized i"
D13-1134,W12-1604,1,0.820151,"the problem of predicting how the user has resolved a referring expression (RE) that was generated by the system, i.e. a noun phrase that is intended to identify some object uniquely to the listener. A number of authors have recently offered statistical models for parts of this problem. Golland et al. (2010) and Garoufi and Koller (2011) have presented log-linear models for predicting how the listener will resolve a given RE in a given scene; however, these models do not update the probability model based on observing the user’s reactions. Nakano et al. (2007), Buschmeier and Kopp (2012), and Koller et al. (2012) all predict what the listener understood based on their behavior, but do not consider the RE itself in the model. The models of Frank and Goodman (2012) and Vogel et al. (2013) aim at explaining the effect of implicatures on the listener’s RE resolution process in terms of hypothesized interactions, but do not actually support a realtime interaction between a system and a user. In this paper, we show how to predict how the listener has resolved an RE by combining a statistical model of RE resolution based on the RE itself with a statistical model of RE resolution based on the listener’s behav"
D13-1134,P07-2031,0,0.0217636,"ng and bring the user back on track. We focus here on the problem of predicting how the user has resolved a referring expression (RE) that was generated by the system, i.e. a noun phrase that is intended to identify some object uniquely to the listener. A number of authors have recently offered statistical models for parts of this problem. Golland et al. (2010) and Garoufi and Koller (2011) have presented log-linear models for predicting how the listener will resolve a given RE in a given scene; however, these models do not update the probability model based on observing the user’s reactions. Nakano et al. (2007), Buschmeier and Kopp (2012), and Koller et al. (2012) all predict what the listener understood based on their behavior, but do not consider the RE itself in the model. The models of Frank and Goodman (2012) and Vogel et al. (2013) aim at explaining the effect of implicatures on the listener’s RE resolution process in terms of hypothesized interactions, but do not actually support a realtime interaction between a system and a user. In this paper, we show how to predict how the listener has resolved an RE by combining a statistical model of RE resolution based on the RE itself with a statistica"
D13-1134,P13-2014,0,0.0120824,"uely to the listener. A number of authors have recently offered statistical models for parts of this problem. Golland et al. (2010) and Garoufi and Koller (2011) have presented log-linear models for predicting how the listener will resolve a given RE in a given scene; however, these models do not update the probability model based on observing the user’s reactions. Nakano et al. (2007), Buschmeier and Kopp (2012), and Koller et al. (2012) all predict what the listener understood based on their behavior, but do not consider the RE itself in the model. The models of Frank and Goodman (2012) and Vogel et al. (2013) aim at explaining the effect of implicatures on the listener’s RE resolution process in terms of hypothesized interactions, but do not actually support a realtime interaction between a system and a user. In this paper, we show how to predict how the listener has resolved an RE by combining a statistical model of RE resolution based on the RE itself with a statistical model of RE resolution based on the listener’s behavior. To our knowledge, this is the first approach to combine two such models explicitly. We consider the RE grounding problem in the context of interactive, situated natural lan"
D13-1134,W10-4233,1,\N,Missing
D13-1134,W11-2845,1,\N,Missing
D17-1159,C10-3009,0,0.150488,"Missing"
D17-1159,W09-1206,0,0.0353084,"Missing"
D17-1159,Q16-1026,0,0.0569738,"s are kept fixed. The final word representation is given by x = xre xpe xpos xle , where represents the concatenation operator. 4.2 Bidirectional LSTM layer One of the most popular and effective ways to represent sequences, such as sentences (Mikolov et al., 2010), is to use recurrent neural networks 4 We drop the index i from the notation for the sake of brevity. (RNN) (Elman, 1990). In particular their gated versions, Long Short-Term Memory (LSTM) networks (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Units (GRU) (Cho et al., 2014), have proven effective in modeling long sequences (Chiu and Nichols, 2016; Sutskever et al., 2014). Formally, an LSTM can be defined as a function LST M✓ (x1:i ) that takes as input the sequence x1:i and returns a hidden state hi 2 Rdh . This state can be regarded as a representation of the sentence from the start to the position i, or, in other words, it encodes the word at position i along with its left context. However, the right context is also important, so Bidirectional LSTMs (Graves, 2008) use two LSTMs: one for the forward pass, and another for the backward pass, LST MF and LST MB , respectively. By concatenating the states of both LSTMs, we create a comple"
D17-1159,P15-1033,0,0.0657373,"proposed SRL model on the English and Chinese CoNLL-2009 dataset with standard splits into training, test and development sets. The predicted POS tags for both languages were provided by the CoNLL-2009 shared-task organizers. For the predicate disambiguator we used the ones from Roth and Lapata (2016) for English and from Bj¨orkelund et al. (2009) for Chinese. We parsed English sentences with the BIST Parser (Kiperwasser and Goldberg, 2016), whereas for Chinese we used automatically predicted parses provided by the CoNLL-2009 shared-task organizers. For English, we used external embeddings of Dyer et al. (2015), learned using the structured skip n-gram approach of Ling et al. (2015). For Chinese we used external embeddings produced with the neural language model of Bengio et al. (2003). We used edge dropout in GCN: when P R F1 LSTMs LSTMs + GCNs (K=1) LSTMs + GCNs (K=2) LSTMs + GCNs (K=1), no gates 84.3 85.2 84.1 84.7 81.1 81.6 81.4 81.4 82.7 83.3 82.7 83.0 GCNs (no LSTMs), K=1 GCNs (no LSTMs), K=2 GCNs (no LSTMs), K=3 GCNs (no LSTMs), K=4 79.9 83.4 83.6 82.7 70.4 74.6 75.8 76.0 74.9 78.7 79.5 79.2 Table 1: SRL results without predicate disambiguation on the English development set. (k) computing hv"
D17-1159,P17-2012,0,0.0121086,"LSTM representations of syntactic paths between arguments and predicates; Lei et al. (2015) who relied on low-rank tensor factorizations for modeling syntax. Also Foland and Martin (2015) used (nongraph) convolutional networks and provided syntactic features as input. A very different line of research, but with similar goals to ours (i.e. integrating syntax with minimal feature engineering), used tree kernels (Moschitti et al., 2008). Beyond SRL, there have been many proposals on how to incorporate syntactic information in RNN models, for example, in the context of neural machine translation (Eriguchi et al., 2017; Sennrich and Haddow, 2016). One of the most popular and attractive approaches is to use treestructured recursive neural networks (Socher et al., 2013; Le and Zuidema, 2014; Dyer et al., 2015), including stacking them on top of a sequential BiLSTM (Miwa and Bansal, 2016). An approach of Mou et al. (2015) to sentiment analysis and question classification, introduced even before GCNs became popular in the machine learning community, is related to graph convolution. However, it is inherently single-layer and tree-specific, uses bottom-up computations, does not share parameters across syntactic f"
D17-1159,D15-1112,0,0.362146,"Missing"
D17-1159,S15-1033,0,0.198886,".3 79.9 78.7 78.2 72.3 74.4 74.0 74.8 75.2 77.1 76.2 76.5 GCNs (no LSTMs), K=1 GCNs (no LSTMs), K=2 GCNs (no LSTMs), K=3 GCNs (no LSTMs), K=4 78.7 79.7 76.8 79.1 58.5 62.7 66.8 63.5 67.1 70.1 71.4 70.4 System Zhao et al. (2009) (global) Bj¨orkelund et al. (2009) (global) Roth and Lapata (2016) (global) Ours (local) P R F1 Lei et al. (2015) (local) - 86.6 FitzGerald et al. (2015) (local) - 86.7 Roth and Lapata (2016) (local) 88.1 85.3 86.7 Marcheggiani et al. (2017) (local) 88.7 86.8 87.7 Ours (local) 89.1 86.8 88.0 Bj¨orkelund et al. (2010) (global) 88.6 85.2 FitzGerald et al. (2015) (global) Foland and Martin (2015) (global) Swayamdipta et al. (2016) (global) Roth and Lapata (2016) (global) 90.0 85.5 R F1 80.4 82.4 83.2 84.6 75.2 75.1 75.9 80.4 77.7 78.6 79.4 82.5 Table 4: Results on the Chinese test set. Table 2: SRL results without predicate disambiguation on the Chinese development set. System P 86.9 87.3 86.0 85.0 87.7 FitzGerald et al. (2015) (ensemble) - 87.7 Roth and Lapata (2016) (ensemble) 90.3 85.7 87.9 Ours (ensemble 3x) 90.5 87.7 89.1 Table 3: Results on the test set for English. off-the-shelf disambiguator for all versions of the model, in Table 1 and 2 we report SRL-only scores (i.e., predi"
D17-1159,W09-1205,1,0.72113,"quently, the predicted syntax for the out-ofdomain test set is of lower quality, which negatively affects the quality of GCN embeddings. However, our model works surprisingly well on out-of-domain data (Table 5), substantially outperforming all the previous syntax-aware models. This suggests that our model is fairly robust to mistakes in syntax. As expected though, our model does not outperform the syntax-agnostic model of Marcheggiani et al. (2017). 6 Related Work Perhaps the earliest methods modeling syntaxsemantics interface with RNNs are due to (Henderson et al., 2008; Titov et al., 2009; Gesmundo et al., 2009), they used shift-reduce parsers for joint SRL and syntactic parsing, and relied on RNNs to model statistical dependencies across syntactic and semantic parsing actions. A more 9 To compare to previous work, we report combined scores which also include predicate disambiguation. As we use disambiguators from previous work (see Section 5.1), actual gains in argument identification and labeling are even larger. modern (e.g., based on LSTMs) and effective reincarnation of this line of research has been proposed in Swayamdipta et al. (2016). Other recent work which considered incorporation of synta"
D17-1159,J02-3001,0,0.0712097,"producing latent feature representations of words in a sentence. We observe that GCN layers are complementary to LSTM ones: when we stack both GCN and LSTM layers, we obtain a substantial improvement over an already state-of-theart LSTM SRL model, resulting in the best reported scores on the standard benchmark (CoNLL-2009) both for Chinese and English. 1 A1 A0 Sequa make.01 makes A1 and COORD SBJ repair.01 repairs CONJ jet engine.01 engines. NMOD OBJ ROOT Figure 1: An example sentence annotated with semantic (top) and syntactic dependencies (bottom). Introduction Semantic role labeling (SRL) (Gildea and Jurafsky, 2002) can be informally described as the task of discovering who did what to whom. For example, consider an SRL dependency graph shown above the sentence in Figure 1. Formally, the task includes (1) detection of predicates (e.g., makes); (2) labeling the predicates with a sense from a sense inventory (e.g., make.01); (3) identifying and assigning arguments to semantic roles (e.g., Sequa is A0, i.e., an agent / ‘doer’ for the corresponding predicate, and engines is A1, i.e., a patient / ‘an affected entity’). SRL is often regarded as an important step in the standard NLP pipeline, providing informat"
D17-1159,W08-2122,1,0.663696,"hey are applied to out-of-domain data. Consequently, the predicted syntax for the out-ofdomain test set is of lower quality, which negatively affects the quality of GCN embeddings. However, our model works surprisingly well on out-of-domain data (Table 5), substantially outperforming all the previous syntax-aware models. This suggests that our model is fairly robust to mistakes in syntax. As expected though, our model does not outperform the syntax-agnostic model of Marcheggiani et al. (2017). 6 Related Work Perhaps the earliest methods modeling syntaxsemantics interface with RNNs are due to (Henderson et al., 2008; Titov et al., 2009; Gesmundo et al., 2009), they used shift-reduce parsers for joint SRL and syntactic parsing, and relied on RNNs to model statistical dependencies across syntactic and semantic parsing actions. A more 9 To compare to previous work, we report combined scores which also include predicate disambiguation. As we use disambiguators from previous work (see Section 5.1), actual gains in argument identification and labeling are even larger. modern (e.g., based on LSTMs) and effective reincarnation of this line of research has been proposed in Swayamdipta et al. (2016). Other recent"
D17-1159,P82-1020,0,0.877305,"Missing"
D17-1159,C08-1050,0,0.0538083,"closely related to syntactic ones, even though the syntaxsemantics interface is far from trivial (Levin, 1993). For example, one can observe that many arcs in the syntactic dependency graph (shown in black below the sentence in Figure 1) are mirrored in the semantic dependency graph. Given these similarities and also because of availability of accurate syntactic parsers for many languages, it seems natural to exploit syntactic information when predicting semantics. Though historically most SRL approaches did rely on syntax (Thompson et al., 2003; Pradhan et al., 2005; Punyakanok et al., 2008; Johansson and Nugues, 2008), the last generation of SRL models put syntax aside in favor of neural sequence models, namely LSTMs (Zhou and Xu, 2015; Marcheggiani et al., 2017), and outperformed syntactically-driven methods on standard benchmarks. We believe that one of the reasons for this radical choice is the lack of simple and effective methods for incorporating syntactic information into sequential neural networks (namely, at the level of words). In this paper we 1506 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1506–1515 c Copenhagen, Denmark, September 7–11, 2017. 2"
D17-1159,D15-1279,0,0.0152645,"th similar goals to ours (i.e. integrating syntax with minimal feature engineering), used tree kernels (Moschitti et al., 2008). Beyond SRL, there have been many proposals on how to incorporate syntactic information in RNN models, for example, in the context of neural machine translation (Eriguchi et al., 2017; Sennrich and Haddow, 2016). One of the most popular and attractive approaches is to use treestructured recursive neural networks (Socher et al., 2013; Le and Zuidema, 2014; Dyer et al., 2015), including stacking them on top of a sequential BiLSTM (Miwa and Bansal, 2016). An approach of Mou et al. (2015) to sentiment analysis and question classification, introduced even before GCNs became popular in the machine learning community, is related to graph convolution. However, it is inherently single-layer and tree-specific, uses bottom-up computations, does not share parameters across syntactic functions and does not use gates. Gates have been previously used in GCNs (Li et al., 2016) but between GCN layers rather than for individual edges. Previous approaches to integrating syntactic information in neural models are mainly designed to induce representations of sentences or syntactic constituents"
D17-1159,Q16-1023,0,0.0527386,"od representation for roles associated with infrequent predicates. As our training objective we use the categorical cross-entropy. 5 Experiments 5.1 Datasets and parameters We tested the proposed SRL model on the English and Chinese CoNLL-2009 dataset with standard splits into training, test and development sets. The predicted POS tags for both languages were provided by the CoNLL-2009 shared-task organizers. For the predicate disambiguator we used the ones from Roth and Lapata (2016) for English and from Bj¨orkelund et al. (2009) for Chinese. We parsed English sentences with the BIST Parser (Kiperwasser and Goldberg, 2016), whereas for Chinese we used automatically predicted parses provided by the CoNLL-2009 shared-task organizers. For English, we used external embeddings of Dyer et al. (2015), learned using the structured skip n-gram approach of Ling et al. (2015). For Chinese we used external embeddings produced with the neural language model of Bengio et al. (2003). We used edge dropout in GCN: when P R F1 LSTMs LSTMs + GCNs (K=1) LSTMs + GCNs (K=2) LSTMs + GCNs (K=1), no gates 84.3 85.2 84.1 84.7 81.1 81.6 81.4 81.4 82.7 83.3 82.7 83.0 GCNs (no LSTMs), K=1 GCNs (no LSTMs), K=2 GCNs (no LSTMs), K=3 GCNs (no"
D17-1159,W05-0634,0,0.0230195,"on answering. The semantic representations are closely related to syntactic ones, even though the syntaxsemantics interface is far from trivial (Levin, 1993). For example, one can observe that many arcs in the syntactic dependency graph (shown in black below the sentence in Figure 1) are mirrored in the semantic dependency graph. Given these similarities and also because of availability of accurate syntactic parsers for many languages, it seems natural to exploit syntactic information when predicting semantics. Though historically most SRL approaches did rely on syntax (Thompson et al., 2003; Pradhan et al., 2005; Punyakanok et al., 2008; Johansson and Nugues, 2008), the last generation of SRL models put syntax aside in favor of neural sequence models, namely LSTMs (Zhou and Xu, 2015; Marcheggiani et al., 2017), and outperformed syntactically-driven methods on standard benchmarks. We believe that one of the reasons for this radical choice is the lack of simple and effective methods for incorporating syntactic information into sequential neural networks (namely, at the level of words). In this paper we 1506 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 15"
D17-1159,J08-2005,0,0.147309,"ntic representations are closely related to syntactic ones, even though the syntaxsemantics interface is far from trivial (Levin, 1993). For example, one can observe that many arcs in the syntactic dependency graph (shown in black below the sentence in Figure 1) are mirrored in the semantic dependency graph. Given these similarities and also because of availability of accurate syntactic parsers for many languages, it seems natural to exploit syntactic information when predicting semantics. Though historically most SRL approaches did rely on syntax (Thompson et al., 2003; Pradhan et al., 2005; Punyakanok et al., 2008; Johansson and Nugues, 2008), the last generation of SRL models put syntax aside in favor of neural sequence models, namely LSTMs (Zhou and Xu, 2015; Marcheggiani et al., 2017), and outperformed syntactically-driven methods on standard benchmarks. We believe that one of the reasons for this radical choice is the lack of simple and effective methods for incorporating syntactic information into sequential neural networks (namely, at the level of words). In this paper we 1506 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1506–1515 c Copenhagen, Den"
D17-1159,D14-1081,0,0.0202425,"(2015) used (nongraph) convolutional networks and provided syntactic features as input. A very different line of research, but with similar goals to ours (i.e. integrating syntax with minimal feature engineering), used tree kernels (Moschitti et al., 2008). Beyond SRL, there have been many proposals on how to incorporate syntactic information in RNN models, for example, in the context of neural machine translation (Eriguchi et al., 2017; Sennrich and Haddow, 2016). One of the most popular and attractive approaches is to use treestructured recursive neural networks (Socher et al., 2013; Le and Zuidema, 2014; Dyer et al., 2015), including stacking them on top of a sequential BiLSTM (Miwa and Bansal, 2016). An approach of Mou et al. (2015) to sentiment analysis and question classification, introduced even before GCNs became popular in the machine learning community, is related to graph convolution. However, it is inherently single-layer and tree-specific, uses bottom-up computations, does not share parameters across syntactic functions and does not use gates. Gates have been previously used in GCNs (Li et al., 2016) but between GCN layers rather than for individual edges. Previous approaches to in"
D17-1159,N15-1121,0,0.136561,"Missing"
D17-1159,N15-1142,0,0.0197962,"ndard splits into training, test and development sets. The predicted POS tags for both languages were provided by the CoNLL-2009 shared-task organizers. For the predicate disambiguator we used the ones from Roth and Lapata (2016) for English and from Bj¨orkelund et al. (2009) for Chinese. We parsed English sentences with the BIST Parser (Kiperwasser and Goldberg, 2016), whereas for Chinese we used automatically predicted parses provided by the CoNLL-2009 shared-task organizers. For English, we used external embeddings of Dyer et al. (2015), learned using the structured skip n-gram approach of Ling et al. (2015). For Chinese we used external embeddings produced with the neural language model of Bengio et al. (2003). We used edge dropout in GCN: when P R F1 LSTMs LSTMs + GCNs (K=1) LSTMs + GCNs (K=2) LSTMs + GCNs (K=1), no gates 84.3 85.2 84.1 84.7 81.1 81.6 81.4 81.4 82.7 83.3 82.7 83.0 GCNs (no LSTMs), K=1 GCNs (no LSTMs), K=2 GCNs (no LSTMs), K=3 GCNs (no LSTMs), K=4 79.9 83.4 83.6 82.7 70.4 74.6 75.8 76.0 74.9 78.7 79.5 79.2 Table 1: SRL results without predicate disambiguation on the English development set. (k) computing hv , we ignore each node v 2 N (v) with probability . Adam (Kingma and Ba,"
D17-1159,Q16-1037,0,0.0216408,"Missing"
D17-1159,K17-1041,1,0.867534,"y arcs in the syntactic dependency graph (shown in black below the sentence in Figure 1) are mirrored in the semantic dependency graph. Given these similarities and also because of availability of accurate syntactic parsers for many languages, it seems natural to exploit syntactic information when predicting semantics. Though historically most SRL approaches did rely on syntax (Thompson et al., 2003; Pradhan et al., 2005; Punyakanok et al., 2008; Johansson and Nugues, 2008), the last generation of SRL models put syntax aside in favor of neural sequence models, namely LSTMs (Zhou and Xu, 2015; Marcheggiani et al., 2017), and outperformed syntactically-driven methods on standard benchmarks. We believe that one of the reasons for this radical choice is the lack of simple and effective methods for incorporating syntactic information into sequential neural networks (namely, at the level of words). In this paper we 1506 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1506–1515 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics propose one way how to address this limitation. Specifically, we rely on graph convolutional networks"
D17-1159,P16-1105,0,0.0593489,"very different line of research, but with similar goals to ours (i.e. integrating syntax with minimal feature engineering), used tree kernels (Moschitti et al., 2008). Beyond SRL, there have been many proposals on how to incorporate syntactic information in RNN models, for example, in the context of neural machine translation (Eriguchi et al., 2017; Sennrich and Haddow, 2016). One of the most popular and attractive approaches is to use treestructured recursive neural networks (Socher et al., 2013; Le and Zuidema, 2014; Dyer et al., 2015), including stacking them on top of a sequential BiLSTM (Miwa and Bansal, 2016). An approach of Mou et al. (2015) to sentiment analysis and question classification, introduced even before GCNs became popular in the machine learning community, is related to graph convolution. However, it is inherently single-layer and tree-specific, uses bottom-up computations, does not share parameters across syntactic functions and does not use gates. Gates have been previously used in GCNs (Li et al., 2016) but between GCN layers rather than for individual edges. Previous approaches to integrating syntactic information in neural models are mainly designed to induce representations of s"
D17-1159,P16-1113,0,0.525863,"in the neural network may need to be down-weighted. In order to address the above issues, inspired by recent literature (van den Oord et al., 2016; Dauphin et al., 2016), we calculate for each edge node pair a scalar gate of the form ⇣ ⌘ (k) (k) ˆb(k) gu,v = h(k) · v ˆ + (3) u dir(u,v) L(u,v) , A1 dobj nsubj (4) u2N (v) 3.3 Complementarity of GCNs and LSTMs The inability of GCNs to capture dependencies between nodes far away from each other in the graph may seem like a serious problem, especially in the context of SRL: paths between predicates and arguments often include many dependency arcs (Roth and Lapata, 2016). However, when graph convolution is performed on top of LSTM (1) states (i.e., LSTM states serve as input xv = hv to GCN) rather than static word embeddings, GCN may not need to capture more than a couple of hops. To elaborate on this, let us speculate what role GCNs would play when used in combinations with LSTMs, given that LSTMs have already been shown very effective for SRL (Zhou and Xu, 2015; Marcheggiani et al., 2017). Though LSTMs are capable of capturing at least some degree of syntax (Linzen et al., 2016) without explicit syntactic supervision, SRL datasets are moderately sized, so L"
D17-1159,W16-2209,0,0.00848594,"syntactic paths between arguments and predicates; Lei et al. (2015) who relied on low-rank tensor factorizations for modeling syntax. Also Foland and Martin (2015) used (nongraph) convolutional networks and provided syntactic features as input. A very different line of research, but with similar goals to ours (i.e. integrating syntax with minimal feature engineering), used tree kernels (Moschitti et al., 2008). Beyond SRL, there have been many proposals on how to incorporate syntactic information in RNN models, for example, in the context of neural machine translation (Eriguchi et al., 2017; Sennrich and Haddow, 2016). One of the most popular and attractive approaches is to use treestructured recursive neural networks (Socher et al., 2013; Le and Zuidema, 2014; Dyer et al., 2015), including stacking them on top of a sequential BiLSTM (Miwa and Bansal, 2016). An approach of Mou et al. (2015) to sentiment analysis and question classification, introduced even before GCNs became popular in the machine learning community, is related to graph convolution. However, it is inherently single-layer and tree-specific, uses bottom-up computations, does not share parameters across syntactic functions and does not use ga"
D17-1159,D13-1170,0,0.067449,"node in the graph (in our case a word in a sentence), GCN encodes relevant information about its neighborhood as a real-valued feature vector. GCNs have been studied largely in the context of undirected unlabeled graphs. We introduce a version of GCNs for modeling syntactic dependency structures and generally applicable to labeled directed graphs. One layer GCN encodes only information about immediate neighbors and K layers are needed to encode K-order neighborhoods (i.e., information about nodes at most K hops aways). This contrasts with recurrent and recursive neural networks (Elman, 1990; Socher et al., 2013) which, at least in theory, can capture statistical dependencies across unbounded paths in a trees or in a sequence. However, as we will further discuss in Section 3.3, this is not a serious limitation when GCNs are used in combination with encoders based on recurrent networks (LSTMs). When we stack GCNs on top of LSTM layers, we obtain a substantial improvement over an already state-of-the-art LSTM SRL model, resulting in the best reported scores on the standard benchmark (CoNLL-2009), both for English and Chinese.1 Interestingly, again unlike recursive neural networks, GCNs do not constrain"
D17-1159,K16-1019,0,0.121957,"4.8 75.2 77.1 76.2 76.5 GCNs (no LSTMs), K=1 GCNs (no LSTMs), K=2 GCNs (no LSTMs), K=3 GCNs (no LSTMs), K=4 78.7 79.7 76.8 79.1 58.5 62.7 66.8 63.5 67.1 70.1 71.4 70.4 System Zhao et al. (2009) (global) Bj¨orkelund et al. (2009) (global) Roth and Lapata (2016) (global) Ours (local) P R F1 Lei et al. (2015) (local) - 86.6 FitzGerald et al. (2015) (local) - 86.7 Roth and Lapata (2016) (local) 88.1 85.3 86.7 Marcheggiani et al. (2017) (local) 88.7 86.8 87.7 Ours (local) 89.1 86.8 88.0 Bj¨orkelund et al. (2010) (global) 88.6 85.2 FitzGerald et al. (2015) (global) Foland and Martin (2015) (global) Swayamdipta et al. (2016) (global) Roth and Lapata (2016) (global) 90.0 85.5 R F1 80.4 82.4 83.2 84.6 75.2 75.1 75.9 80.4 77.7 78.6 79.4 82.5 Table 4: Results on the Chinese test set. Table 2: SRL results without predicate disambiguation on the Chinese development set. System P 86.9 87.3 86.0 85.0 87.7 FitzGerald et al. (2015) (ensemble) - 87.7 Roth and Lapata (2016) (ensemble) 90.3 85.7 87.9 Ours (ensemble 3x) 90.5 87.7 89.1 Table 3: Results on the test set for English. off-the-shelf disambiguator for all versions of the model, in Table 1 and 2 we report SRL-only scores (i.e., predicate disambiguation is not evaluate"
D17-1159,W09-1209,0,0.282909,"Missing"
D17-1159,P15-1109,0,0.245213,"an observe that many arcs in the syntactic dependency graph (shown in black below the sentence in Figure 1) are mirrored in the semantic dependency graph. Given these similarities and also because of availability of accurate syntactic parsers for many languages, it seems natural to exploit syntactic information when predicting semantics. Though historically most SRL approaches did rely on syntax (Thompson et al., 2003; Pradhan et al., 2005; Punyakanok et al., 2008; Johansson and Nugues, 2008), the last generation of SRL models put syntax aside in favor of neural sequence models, namely LSTMs (Zhou and Xu, 2015; Marcheggiani et al., 2017), and outperformed syntactically-driven methods on standard benchmarks. We believe that one of the reasons for this radical choice is the lack of simple and effective methods for incorporating syntactic information into sequential neural networks (namely, at the level of words). In this paper we 1506 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1506–1515 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics propose one way how to address this limitation. Specifically, we rely on g"
D17-1159,J08-2003,0,0.0132015,"ude: FitzGerald et al. (2015) who use standard syntactic features within an MLP calculating potentials of a CRF model; Roth and Lapata (2016) who enriched standard features for SRL with LSTM representations of syntactic paths between arguments and predicates; Lei et al. (2015) who relied on low-rank tensor factorizations for modeling syntax. Also Foland and Martin (2015) used (nongraph) convolutional networks and provided syntactic features as input. A very different line of research, but with similar goals to ours (i.e. integrating syntax with minimal feature engineering), used tree kernels (Moschitti et al., 2008). Beyond SRL, there have been many proposals on how to incorporate syntactic information in RNN models, for example, in the context of neural machine translation (Eriguchi et al., 2017; Sennrich and Haddow, 2016). One of the most popular and attractive approaches is to use treestructured recursive neural networks (Socher et al., 2013; Le and Zuidema, 2014; Dyer et al., 2015), including stacking them on top of a sequential BiLSTM (Miwa and Bansal, 2016). An approach of Mou et al. (2015) to sentiment analysis and question classification, introduced even before GCNs became popular in the machine"
D17-1209,N16-1024,0,0.0410979,"hanism. Stahlberg et al. (2016) use a pruned lattice from a hierarchical phrase-based model (hiero) to constrain NMT. Hiero trees are not syntactically-aware, but instead constrained by symmetrized word alignments. Aharoni and Goldberg (2017) propose neural string-to-tree by predicting linearized parse trees. Multi-task Learning. Sharing NMT parameters with a syntactic parser is a popular approach to obtaining syntactically-aware representations. Luong et al. (2015a) predict linearized constituency parses as an additional task. Eriguchi et al. (2017) multi-task with a target-side RNNG parser (Dyer et al., 2016) and improve on various language pairs with English on the target side. Nadejde et al. (2017) multi-task with CCG tagging, and also integrate syntax on the target side by predicting a sequence of words interleaved with CCG supertags. Latent structure. Hashimoto and Tsuruoka (2017) add a syntax-inspired encoder on top of a BiLSTM layer. They encode source words as a learned average of potential parents emulating a relaxed dependency tree. While their model is trained purely on translation data, they also experiment with pre-training the encoder using treebank annotation and report modest improv"
D17-1209,P17-2021,0,0.19234,"ures and/or constraints. Sennrich and Haddow (2016) embed features such as POS-tags, lemmas and dependency labels and feed these into the network along with word embeddings. Eriguchi et al. (2016) parse English sentences with an HPSG parser and use a Tree-LSTM to encode the internal nodes of the tree. In the decoder, word and node representations compete under the same attention mechanism. Stahlberg et al. (2016) use a pruned lattice from a hierarchical phrase-based model (hiero) to constrain NMT. Hiero trees are not syntactically-aware, but instead constrained by symmetrized word alignments. Aharoni and Goldberg (2017) propose neural string-to-tree by predicting linearized parse trees. Multi-task Learning. Sharing NMT parameters with a syntactic parser is a popular approach to obtaining syntactically-aware representations. Luong et al. (2015a) predict linearized constituency parses as an additional task. Eriguchi et al. (2017) multi-task with a target-side RNNG parser (Dyer et al., 2016) and improve on various language pairs with English on the target side. Nadejde et al. (2017) multi-task with CCG tagging, and also integrate syntax on the target side by predicting a sequence of words interleaved with CCG s"
D17-1209,P10-1146,0,0.0329964,"riguchi et al., 2017; Hashimoto and Tsuruoka, 2017)) or may be too restrictive in modeling the interface between syntax and the translation task (e.g., learning representations of linguistic phrases (Eriguchi et al., 2016)). Our goal is to provide the encoder with access to rich syntactic information but let it decide which aspects of syntax are beneficial for MT, without placing rigid constraints on the interaction between syntax and the translation task. This goal is in line with claims that rigid syntactic constraints typically hurt MT (Zollmann and Venugopal, 2006; Smith and Eisner, 2006; Chiang, 2010), and, though these claims have been made in the context of traditional MT systems, we believe they are no less valid for NMT. Attention-based NMT systems (Bahdanau et al., 2015; Luong et al., 2015b) represent source sentence words as latent-feature vectors in the encoder and use these vectors when generating a translation. Our goal is to automatically incorporate information about syntactic neighborhoods of source words into these feature vectors, and, thus, potentially improve quality of the translation output. Since vectors correspond to words, it is natural for us to use dependency syntax."
D17-1209,W14-4012,0,0.196706,"Missing"
D17-1209,D14-1179,0,0.140705,"Missing"
D17-1209,P16-1078,0,0.267892,"y we have not seen much benefit from using syntactic information in NMT is the lack of simple and effective methods for incorporating structured information in neural encoders, including RNNs. Despite some successes, techniques explored so far either incorporate syntactic information in NMT models in a relatively indirect way (e.g., multi-task learning (Luong et al., 2015a; Nadejde et al., 2017; Eriguchi et al., 2017; Hashimoto and Tsuruoka, 2017)) or may be too restrictive in modeling the interface between syntax and the translation task (e.g., learning representations of linguistic phrases (Eriguchi et al., 2016)). Our goal is to provide the encoder with access to rich syntactic information but let it decide which aspects of syntax are beneficial for MT, without placing rigid constraints on the interaction between syntax and the translation task. This goal is in line with claims that rigid syntactic constraints typically hurt MT (Zollmann and Venugopal, 2006; Smith and Eisner, 2006; Chiang, 2010), and, though these claims have been made in the context of traditional MT systems, we believe they are no less valid for NMT. Attention-based NMT systems (Bahdanau et al., 2015; Luong et al., 2015b) represent"
D17-1209,P17-2012,0,0.225287,"ystems rely on sequential encoderdecoders (Sutskever et al., 2014; Bahdanau et al., 2015) and lack any explicit modeling of syntax or any hierarchical structure of language. One potential reason for why we have not seen much benefit from using syntactic information in NMT is the lack of simple and effective methods for incorporating structured information in neural encoders, including RNNs. Despite some successes, techniques explored so far either incorporate syntactic information in NMT models in a relatively indirect way (e.g., multi-task learning (Luong et al., 2015a; Nadejde et al., 2017; Eriguchi et al., 2017; Hashimoto and Tsuruoka, 2017)) or may be too restrictive in modeling the interface between syntax and the translation task (e.g., learning representations of linguistic phrases (Eriguchi et al., 2016)). Our goal is to provide the encoder with access to rich syntactic information but let it decide which aspects of syntax are beneficial for MT, without placing rigid constraints on the interaction between syntax and the translation task. This goal is in line with claims that rigid syntactic constraints typically hurt MT (Zollmann and Venugopal, 2006; Smith and Eisner, 2006; Chiang, 2010), and,"
D17-1209,D17-1012,0,0.137824,"Missing"
D17-1209,D14-1080,0,0.0742857,"Missing"
D17-1209,D13-1176,0,0.0955223,", unlike recursive neural networks (Socher et al., 2013), GCNs do not require the graphs to be trees. However, in this work we solely focus on dependency syntax and leave more general investigation for future work. Our main contributions can be summarized as follows: • we show that incorporating structure is beneficial for machine translation on EnglishCzech and English-German. 2 Background Notation. We use x for vectors, x1:t for a sequence of t vectors, and X for matrices. The i-th value of vector x is denoted by xi . We use ◦ for vector concatenation. 2.1 Neural Machine Translation In NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014b), given example translation pairs from a parallel corpus, a neural network is trained to directly estimate the conditional distribution p(y1:Ty |x1:Tx ) of translating a source sentence x1:Tx (a sequence of Tx words) into a target sentence y1:Ty . NMT models typically consist of an encoder, a decoder and some method for conditioning the decoder on the encoder, for example, an attention mechanism. We will now briefly describe the components that we use in this paper. 2.1.1 Encoders An encoder is a function that takes as input the source sentence and p"
D17-1209,Q16-1037,0,0.200605,"pturing syntactic properties specifically relevant to the translation task. Though GCNs can take word embeddings as input, we will see that they are more effective when used as layers on top of recurrent neural network (RNN) or convolutional neural network (CNN) encoders (Gehring et al., 2016), enriching their states with syntactic information. A comparison to RNNs is the most challenging test for GCNs, as it has been shown that RNNs (e.g., LSTMs) are able to capture certain syntactic phenomena (e.g., subject-verb agreement) reasonably well on their own, without explicit treebank supervision (Linzen et al., 2016; Shi et al., 2016). Nevertheless, GCNs appear beneficial even in this challenging set-up: we obtain +1.2 and +0.7 BLEU point improvements from using syntactic GCNs on top of bidirectional RNNs for EnglishGerman and English-Czech, respectively. In principle, GCNs are flexible enough to incorporate any linguistic structure as long as they can be represented as graphs (e.g., dependency-based semantic-role labeling representations (Surdeanu et al., 2008), AMR semantic graphs (Banarescu et al., 2012) and co-reference chains). For example, unlike recursive neural networks (Socher et al., 2013), GCN"
D17-1209,W16-2209,0,0.0464331,"N <=10 11-20 21-30 31-40 Sentence length 41+ Figure 4: Validation BLEU per sentence length. Discussion. Results suggest that the syntaxaware representations provided by GCNs consistently lead to improved translation performance as measured by BLEU4 (as well as TER and BEER). Consistent gains in terms of Kendall tau and BLEU1 indicate that improvements correlate with better word order and lexical/BPE selection, two phenomena that depend crucially on syntax. 5 Related Work We review various accounts to syntax in NMT as well as other convolutional encoders. Syntactic features and/or constraints. Sennrich and Haddow (2016) embed features such as POS-tags, lemmas and dependency labels and feed these into the network along with word embeddings. Eriguchi et al. (2016) parse English sentences with an HPSG parser and use a Tree-LSTM to encode the internal nodes of the tree. In the decoder, word and node representations compete under the same attention mechanism. Stahlberg et al. (2016) use a pruned lattice from a hierarchical phrase-based model (hiero) to constrain NMT. Hiero trees are not syntactically-aware, but instead constrained by symmetrized word alignments. Aharoni and Goldberg (2017) propose neural string-t"
D17-1209,W16-2323,0,0.105372,"as output, so they can easily be incorporated as layers into standard encoders (e.g., on top of bidirectional RNNs or convolutional neural networks). We evaluate their effectiveness with English-German and English-Czech translation experiments for different types of encoders and observe substantial improvements over their syntax-agnostic versions in all the considered setups. 1 Introduction Neural machine translation (NMT) is one of success stories of deep learning in natural language processing, with recent NMT systems outperforming traditional phrase-based approaches on many language pairs (Sennrich et al., 2016a). State-ofthe-art NMT systems rely on sequential encoderdecoders (Sutskever et al., 2014; Bahdanau et al., 2015) and lack any explicit modeling of syntax or any hierarchical structure of language. One potential reason for why we have not seen much benefit from using syntactic information in NMT is the lack of simple and effective methods for incorporating structured information in neural encoders, including RNNs. Despite some successes, techniques explored so far either incorporate syntactic information in NMT models in a relatively indirect way (e.g., multi-task learning (Luong et al., 2015"
D17-1209,P16-1162,0,0.532548,"as output, so they can easily be incorporated as layers into standard encoders (e.g., on top of bidirectional RNNs or convolutional neural networks). We evaluate their effectiveness with English-German and English-Czech translation experiments for different types of encoders and observe substantial improvements over their syntax-agnostic versions in all the considered setups. 1 Introduction Neural machine translation (NMT) is one of success stories of deep learning in natural language processing, with recent NMT systems outperforming traditional phrase-based approaches on many language pairs (Sennrich et al., 2016a). State-ofthe-art NMT systems rely on sequential encoderdecoders (Sutskever et al., 2014; Bahdanau et al., 2015) and lack any explicit modeling of syntax or any hierarchical structure of language. One potential reason for why we have not seen much benefit from using syntactic information in NMT is the lack of simple and effective methods for incorporating structured information in neural encoders, including RNNs. Despite some successes, techniques explored so far either incorporate syntactic information in NMT models in a relatively indirect way (e.g., multi-task learning (Luong et al., 2015"
D17-1209,D16-1159,0,0.0446397,"perties specifically relevant to the translation task. Though GCNs can take word embeddings as input, we will see that they are more effective when used as layers on top of recurrent neural network (RNN) or convolutional neural network (CNN) encoders (Gehring et al., 2016), enriching their states with syntactic information. A comparison to RNNs is the most challenging test for GCNs, as it has been shown that RNNs (e.g., LSTMs) are able to capture certain syntactic phenomena (e.g., subject-verb agreement) reasonably well on their own, without explicit treebank supervision (Linzen et al., 2016; Shi et al., 2016). Nevertheless, GCNs appear beneficial even in this challenging set-up: we obtain +1.2 and +0.7 BLEU point improvements from using syntactic GCNs on top of bidirectional RNNs for EnglishGerman and English-Czech, respectively. In principle, GCNs are flexible enough to incorporate any linguistic structure as long as they can be represented as graphs (e.g., dependency-based semantic-role labeling representations (Surdeanu et al., 2008), AMR semantic graphs (Banarescu et al., 2012) and co-reference chains). For example, unlike recursive neural networks (Socher et al., 2013), GCNs do not require th"
D17-1209,W06-3104,0,0.0297664,"Nadejde et al., 2017; Eriguchi et al., 2017; Hashimoto and Tsuruoka, 2017)) or may be too restrictive in modeling the interface between syntax and the translation task (e.g., learning representations of linguistic phrases (Eriguchi et al., 2016)). Our goal is to provide the encoder with access to rich syntactic information but let it decide which aspects of syntax are beneficial for MT, without placing rigid constraints on the interaction between syntax and the translation task. This goal is in line with claims that rigid syntactic constraints typically hurt MT (Zollmann and Venugopal, 2006; Smith and Eisner, 2006; Chiang, 2010), and, though these claims have been made in the context of traditional MT systems, we believe they are no less valid for NMT. Attention-based NMT systems (Bahdanau et al., 2015; Luong et al., 2015b) represent source sentence words as latent-feature vectors in the encoder and use these vectors when generating a translation. Our goal is to automatically incorporate information about syntactic neighborhoods of source words into these feature vectors, and, thus, potentially improve quality of the translation output. Since vectors correspond to words, it is natural for us to use dep"
D17-1209,D15-1166,0,0.52241,"nrich et al., 2016a). State-ofthe-art NMT systems rely on sequential encoderdecoders (Sutskever et al., 2014; Bahdanau et al., 2015) and lack any explicit modeling of syntax or any hierarchical structure of language. One potential reason for why we have not seen much benefit from using syntactic information in NMT is the lack of simple and effective methods for incorporating structured information in neural encoders, including RNNs. Despite some successes, techniques explored so far either incorporate syntactic information in NMT models in a relatively indirect way (e.g., multi-task learning (Luong et al., 2015a; Nadejde et al., 2017; Eriguchi et al., 2017; Hashimoto and Tsuruoka, 2017)) or may be too restrictive in modeling the interface between syntax and the translation task (e.g., learning representations of linguistic phrases (Eriguchi et al., 2016)). Our goal is to provide the encoder with access to rich syntactic information but let it decide which aspects of syntax are beneficial for MT, without placing rigid constraints on the interaction between syntax and the translation task. This goal is in line with claims that rigid syntactic constraints typically hurt MT (Zollmann and Venugopal, 2006"
D17-1209,2006.amta-papers.25,0,0.0452057,"Missing"
D17-1209,D17-1159,1,0.838954,"tional Linguistics det The dobj nsubj monkey • we introduce a method for incorporating structure into NMT using syntactic GCNs; det eats a banana • we show that GCNs can be used along with RNN and CNN encoders; Figure 1: A dependency tree for the example sentence: “The monkey eats a banana.” th order neighborhood (i.e. nodes at most k hops aways from the node) (Gilmer et al., 2017). They are generally simple and computationally inexpensive. We use Syntactic GCNs, a version of GCN operating on top of syntactic dependency trees, recently shown effective in the context of semantic role labeling (Marcheggiani and Titov, 2017). Since syntactic GCNs produce representations at word level, it is straightforward to use them as encoders within the attention-based encoderdecoder framework. As NMT systems are trained end-to-end, GCNs end up capturing syntactic properties specifically relevant to the translation task. Though GCNs can take word embeddings as input, we will see that they are more effective when used as layers on top of recurrent neural network (RNN) or convolutional neural network (CNN) encoders (Gehring et al., 2016), enriching their states with syntactic information. A comparison to RNNs is the most challe"
D17-1209,P02-1040,0,0.0986164,"using the syntaxnet/demo.sh shell script. 9 https://github.com/moses-smt/mosesdecoder 1962 8 English-German English-German (full) English-Czech Source Target 37824 50000 33786 8099 (BPE) 16000 (BPE) 8116 (BPE) Table 2: Vocabulary sizes. lent to the dimensionality of their input. We report results for 2-layer GCNs, as we find them most effective (see ablation studies below). Baselines. We provide three baselines, each with a different encoder: a bag-of-words encoder, a convolutional encoder with window size w = 5, and a BiRNN. See §2.1.1 for details. Evaluation. We report (cased) BLEU results (Papineni et al., 2002) using multi-bleu, as well as Kendall τ reordering scores.10 4.2.1 Results English-German. Table 3 shows test results on English-German. Unsurprisingly, the bag-ofwords baseline performs the worst. We expected the BoW+GCN model to make easy gains over this baseline, which is indeed what happens. The CNN baseline reaches a higher BLEU4 score than the BoW models, but interestingly its BLEU1 score is lower than the BoW+GCN model. The CNN+GCN model improves over the CNN baseline by +1.9 and +1.1 for BLEU1 and BLEU4 , respectively. The BiRNN, the strongest baseline, reaches a BLEU4 of 14.9. Interes"
D17-1209,D13-1170,0,0.00748132,"sion (Linzen et al., 2016; Shi et al., 2016). Nevertheless, GCNs appear beneficial even in this challenging set-up: we obtain +1.2 and +0.7 BLEU point improvements from using syntactic GCNs on top of bidirectional RNNs for EnglishGerman and English-Czech, respectively. In principle, GCNs are flexible enough to incorporate any linguistic structure as long as they can be represented as graphs (e.g., dependency-based semantic-role labeling representations (Surdeanu et al., 2008), AMR semantic graphs (Banarescu et al., 2012) and co-reference chains). For example, unlike recursive neural networks (Socher et al., 2013), GCNs do not require the graphs to be trees. However, in this work we solely focus on dependency syntax and leave more general investigation for future work. Our main contributions can be summarized as follows: • we show that incorporating structure is beneficial for machine translation on EnglishCzech and English-German. 2 Background Notation. We use x for vectors, x1:t for a sequence of t vectors, and X for matrices. The i-th value of vector x is denoted by xi . We use ◦ for vector concatenation. 2.1 Neural Machine Translation In NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014;"
D17-1209,P16-2049,0,0.03803,"better word order and lexical/BPE selection, two phenomena that depend crucially on syntax. 5 Related Work We review various accounts to syntax in NMT as well as other convolutional encoders. Syntactic features and/or constraints. Sennrich and Haddow (2016) embed features such as POS-tags, lemmas and dependency labels and feed these into the network along with word embeddings. Eriguchi et al. (2016) parse English sentences with an HPSG parser and use a Tree-LSTM to encode the internal nodes of the tree. In the decoder, word and node representations compete under the same attention mechanism. Stahlberg et al. (2016) use a pruned lattice from a hierarchical phrase-based model (hiero) to constrain NMT. Hiero trees are not syntactically-aware, but instead constrained by symmetrized word alignments. Aharoni and Goldberg (2017) propose neural string-to-tree by predicting linearized parse trees. Multi-task Learning. Sharing NMT parameters with a syntactic parser is a popular approach to obtaining syntactically-aware representations. Luong et al. (2015a) predict linearized constituency parses as an additional task. Eriguchi et al. (2017) multi-task with a target-side RNNG parser (Dyer et al., 2016) and improve"
D17-1209,D14-1025,1,0.89451,"Missing"
D17-1209,W06-3119,0,0.0765112,"learning (Luong et al., 2015a; Nadejde et al., 2017; Eriguchi et al., 2017; Hashimoto and Tsuruoka, 2017)) or may be too restrictive in modeling the interface between syntax and the translation task (e.g., learning representations of linguistic phrases (Eriguchi et al., 2016)). Our goal is to provide the encoder with access to rich syntactic information but let it decide which aspects of syntax are beneficial for MT, without placing rigid constraints on the interaction between syntax and the translation task. This goal is in line with claims that rigid syntactic constraints typically hurt MT (Zollmann and Venugopal, 2006; Smith and Eisner, 2006; Chiang, 2010), and, though these claims have been made in the context of traditional MT systems, we believe they are no less valid for NMT. Attention-based NMT systems (Bahdanau et al., 2015; Luong et al., 2015b) represent source sentence words as latent-feature vectors in the encoder and use these vectors when generating a translation. Our goal is to automatically incorporate information about syntactic neighborhoods of source words into these feature vectors, and, thus, potentially improve quality of the translation output. Since vectors correspond to words, it is n"
D17-1209,W08-2121,0,\N,Missing
D17-1209,P11-2031,0,\N,Missing
D17-1209,P17-1012,0,\N,Missing
D17-3004,D07-1002,0,\N,Missing
D17-3004,lo-wu-2010-evaluating,0,\N,Missing
D17-3004,W05-0635,0,\N,Missing
D17-3004,W05-0628,0,\N,Missing
D17-3004,W04-3212,0,\N,Missing
D17-3004,W05-0634,0,\N,Missing
D17-3004,P10-1040,0,\N,Missing
D17-3004,J05-1004,0,\N,Missing
D17-3004,D14-1162,0,\N,Missing
D17-3004,W04-2705,0,\N,Missing
D17-3004,N07-1069,0,\N,Missing
D17-3004,W13-3520,0,\N,Missing
D17-3004,W08-0804,0,\N,Missing
D17-3004,J93-2004,0,\N,Missing
D17-3004,D12-1074,0,\N,Missing
D17-3004,de-marneffe-etal-2014-universal,0,\N,Missing
D17-3004,W08-2121,0,\N,Missing
D17-3004,D09-1059,0,\N,Missing
D17-3004,W09-1205,1,\N,Missing
D17-3004,J92-4003,0,\N,Missing
D17-3004,W09-1201,0,\N,Missing
D17-3004,petrov-etal-2012-universal,0,\N,Missing
D17-3004,P08-1063,0,\N,Missing
D17-3004,W09-1208,0,\N,Missing
D17-3004,Q15-1034,1,\N,Missing
D17-3004,P14-1111,1,\N,Missing
D17-3004,W13-5503,0,\N,Missing
D17-3004,D16-1177,1,\N,Missing
D19-1081,W16-2378,0,0.027085,"in Figure 6. Figure 5 shows how consistency scores are changing in training.6 For deixis, the model achieves the final quality quite quickly; for the rest, it needs a large number of training steps to converge. 8 (b) Figure 5: Consistency scores progression in training. 8.1 Automatic post-editing Our model can be regarded as an automatic postediting system – a system designed to fix systematic MT errors that is decoupled from the main MT system. Automatic post-editing has a long history, including rule-based (Knight and Chander, 1994), statistical (Simard et al., 2007) and neural approaches (Junczys-Dowmunt and Grundkiewicz, 2016; Pal et al., 2016; Freitag et al., 2019). In terms of architectures, modern approaches use neural sequence-to-sequence models, either multi-source architectures that consider both the original source and the baseline translation (Junczys-Dowmunt and Grundkiewicz, 2016; Pal et al., 2016), or monolingual repair systems, as in Freitag et al. (2019), which is concurrent work to ours. True post-editing datasets are typically small and expensive to create (Specia et al., 2017), hence synthetic training data has been created that uses original monolingual data as output for the sequence-to-sequence"
D19-1081,C18-1050,0,0.280125,"rsity of Edinburgh, Scotland 4 University of Zurich, Switzerland lena-voita@yandex-team.ru sennrich@cl.uzh.ch ititov@inf.ed.ac.uk Abstract human over machine translation (Läubli et al., 2018). These findings emphasize the need to shift towards context-aware machine translation both from modeling and evaluation perspective. Most previous work on context-aware NMT assumed that either all the bilingual data is available at the document level (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Agrawal et al., 2018; Kuang et al., 2018; Miculicich et al., 2018) or at least its fraction (Voita et al., 2019). But in practical scenarios, document-level parallel data is often scarce, which is one of the challenges when building a contextaware system. We introduce an approach to context-aware machine translation using only monolingual document-level data. In our setting, a separate monolingual sequence-to-sequence model (DocRepair) is used to correct sentence-level translations of adjacent sentences. The key idea is to use monolingual data to imitate typical inconsistencies between context-agnostic translations of isolated sente"
D19-1081,D18-1512,1,0.752714,"Missing"
D19-1081,N18-1118,1,0.881056,"ennrich4,3 Ivan Titov3,2 1 3 Yandex, Russia 2 University of Amsterdam, Netherlands University of Edinburgh, Scotland 4 University of Zurich, Switzerland lena-voita@yandex-team.ru sennrich@cl.uzh.ch ititov@inf.ed.ac.uk Abstract human over machine translation (Läubli et al., 2018). These findings emphasize the need to shift towards context-aware machine translation both from modeling and evaluation perspective. Most previous work on context-aware NMT assumed that either all the bilingual data is available at the document level (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Agrawal et al., 2018; Kuang et al., 2018; Miculicich et al., 2018) or at least its fraction (Voita et al., 2019). But in practical scenarios, document-level parallel data is often scarce, which is one of the challenges when building a contextaware system. We introduce an approach to context-aware machine translation using only monolingual document-level data. In our setting, a separate monolingual sequence-to-sequence model (DocRepair) is used to correct sentence-level translations of adjacent sentences. The key idea is to use monolingual data to"
D19-1081,L18-1275,0,0.0705509,"t statistics are shown in Table 1. The suites for deixis and lexical cohesion are split into development and test sets, with 500 examples from each used for validation purposes and the rest for testing. Convergence of both consistency scores on these development sets and BLEU score on a general development set are used as early stopping criteria in models training. For ellipsis, there is no dedicated development set, so we evaluate on all the ellipsis data and do not use it for development. 3.2 4 4.1 Experimental Setup Data preprocessing We use the publicly available OpenSubtitles2018 corpus (Lison et al., 2018) for English and Russian. For a fair comparison with previous work, we train the baseline MT system on the data released by Voita et al. (2019). Namely, our MT system is trained on 6m instances. These are sentence pairs with a relative time overlap of subtitle frames between source and target language subtitles of at least 0.9. We gathered 30m groups of 4 consecutive sentences as our monolingual data. We used only documents not containing groups of sentences from general development and test sets as well as from contrastive test sets. The main results we report Phenomena overview Deixis Deicti"
D19-1081,P18-1118,0,0.262537,"ia 2 University of Amsterdam, Netherlands University of Edinburgh, Scotland 4 University of Zurich, Switzerland lena-voita@yandex-team.ru sennrich@cl.uzh.ch ititov@inf.ed.ac.uk Abstract human over machine translation (Läubli et al., 2018). These findings emphasize the need to shift towards context-aware machine translation both from modeling and evaluation perspective. Most previous work on context-aware NMT assumed that either all the bilingual data is available at the document level (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Agrawal et al., 2018; Kuang et al., 2018; Miculicich et al., 2018) or at least its fraction (Voita et al., 2019). But in practical scenarios, document-level parallel data is often scarce, which is one of the challenges when building a contextaware system. We introduce an approach to context-aware machine translation using only monolingual document-level data. In our setting, a separate monolingual sequence-to-sequence model (DocRepair) is used to correct sentence-level translations of adjacent sentences. The key idea is to use monolingual data to imitate typical inconsistencies between conte"
D19-1081,P12-1000,0,0.240227,"Missing"
D19-1081,P19-1116,1,0.829458,"Missing"
D19-1081,D18-1325,0,0.340288,"Scotland 4 University of Zurich, Switzerland lena-voita@yandex-team.ru sennrich@cl.uzh.ch ititov@inf.ed.ac.uk Abstract human over machine translation (Läubli et al., 2018). These findings emphasize the need to shift towards context-aware machine translation both from modeling and evaluation perspective. Most previous work on context-aware NMT assumed that either all the bilingual data is available at the document level (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Agrawal et al., 2018; Kuang et al., 2018; Miculicich et al., 2018) or at least its fraction (Voita et al., 2019). But in practical scenarios, document-level parallel data is often scarce, which is one of the challenges when building a contextaware system. We introduce an approach to context-aware machine translation using only monolingual document-level data. In our setting, a separate monolingual sequence-to-sequence model (DocRepair) is used to correct sentence-level translations of adjacent sentences. The key idea is to use monolingual data to imitate typical inconsistencies between context-agnostic translations of isolated sentences. The DocRepair model"
D19-1081,P18-1117,1,0.894452,"3,2 1 3 Yandex, Russia 2 University of Amsterdam, Netherlands University of Edinburgh, Scotland 4 University of Zurich, Switzerland lena-voita@yandex-team.ru sennrich@cl.uzh.ch ititov@inf.ed.ac.uk Abstract human over machine translation (Läubli et al., 2018). These findings emphasize the need to shift towards context-aware machine translation both from modeling and evaluation perspective. Most previous work on context-aware NMT assumed that either all the bilingual data is available at the document level (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Agrawal et al., 2018; Kuang et al., 2018; Miculicich et al., 2018) or at least its fraction (Voita et al., 2019). But in practical scenarios, document-level parallel data is often scarce, which is one of the challenges when building a contextaware system. We introduce an approach to context-aware machine translation using only monolingual document-level data. In our setting, a separate monolingual sequence-to-sequence model (DocRepair) is used to correct sentence-level translations of adjacent sentences. The key idea is to use monolingual data to imitate typical inco"
D19-1081,P16-2046,0,0.0681467,"Missing"
D19-1081,D17-1301,0,0.21286,"Neural Machine Translation Elena Voita1,2 Rico Sennrich4,3 Ivan Titov3,2 1 3 Yandex, Russia 2 University of Amsterdam, Netherlands University of Edinburgh, Scotland 4 University of Zurich, Switzerland lena-voita@yandex-team.ru sennrich@cl.uzh.ch ititov@inf.ed.ac.uk Abstract human over machine translation (Läubli et al., 2018). These findings emphasize the need to shift towards context-aware machine translation both from modeling and evaluation perspective. Most previous work on context-aware NMT assumed that either all the bilingual data is available at the document level (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Agrawal et al., 2018; Kuang et al., 2018; Miculicich et al., 2018) or at least its fraction (Voita et al., 2019). But in practical scenarios, document-level parallel data is often scarce, which is one of the challenges when building a contextaware system. We introduce an approach to context-aware machine translation using only monolingual document-level data. In our setting, a separate monolingual sequence-to-sequence model (DocRepair) is used to correct sentence-level translations of adjacent sen"
D19-1081,P16-1009,1,0.944061,"ered 30m groups of 4 consecutive sentences as our monolingual data. We used only documents not containing groups of sentences from general development and test sets as well as from contrastive test sets. The main results we report Phenomena overview Deixis Deictic words or phrases, are referential expressions whose denotation depends on context. 879 are for the model trained on all 30m fragments. We use the tokenization provided by the corpus and use multi-bleu.perl3 on lowercased data to compute BLEU score. We use beam search with a beam of 4. Sentences were encoded using byte-pair encoding (Sennrich et al., 2016b), with source and target vocabularies of about 32000 tokens. Translation pairs were batched together by approximate sequence length. Each training batch contained a set of translation pairs containing approximately 150004 source tokens. It has been shown that Transformer’s performance depends heavily on batch size (Popel and Bojar, 2018), and we chose a large batch size to ensure the best performance. In training context-aware models, for early stopping we use both convergence in BLEU score on the general development set and scores on the consistency development sets. After training, we aver"
D19-1081,P16-1162,1,0.737112,"ered 30m groups of 4 consecutive sentences as our monolingual data. We used only documents not containing groups of sentences from general development and test sets as well as from contrastive test sets. The main results we report Phenomena overview Deixis Deictic words or phrases, are referential expressions whose denotation depends on context. 879 are for the model trained on all 30m fragments. We use the tokenization provided by the corpus and use multi-bleu.perl3 on lowercased data to compute BLEU score. We use beam search with a beam of 4. Sentences were encoded using byte-pair encoding (Sennrich et al., 2016b), with source and target vocabularies of about 32000 tokens. Translation pairs were batched together by approximate sequence length. Each training batch contained a set of translation pairs containing approximately 150004 source tokens. It has been shown that Transformer’s performance depends heavily on batch size (Popel and Bojar, 2018), and we chose a large batch size to ensure the best performance. In training context-aware models, for early stopping we use both convergence in BLEU score on the general development set and scores on the consistency development sets. After training, we aver"
D19-1081,W17-4811,0,0.157059,"nslation Elena Voita1,2 Rico Sennrich4,3 Ivan Titov3,2 1 3 Yandex, Russia 2 University of Amsterdam, Netherlands University of Edinburgh, Scotland 4 University of Zurich, Switzerland lena-voita@yandex-team.ru sennrich@cl.uzh.ch ititov@inf.ed.ac.uk Abstract human over machine translation (Läubli et al., 2018). These findings emphasize the need to shift towards context-aware machine translation both from modeling and evaluation perspective. Most previous work on context-aware NMT assumed that either all the bilingual data is available at the document level (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Agrawal et al., 2018; Kuang et al., 2018; Miculicich et al., 2018) or at least its fraction (Voita et al., 2019). But in practical scenarios, document-level parallel data is often scarce, which is one of the challenges when building a contextaware system. We introduce an approach to context-aware machine translation using only monolingual document-level data. In our setting, a separate monolingual sequence-to-sequence model (DocRepair) is used to correct sentence-level translations of adjacent sentences. The key idea is to use"
D19-1083,D14-1179,0,0.0233531,"Missing"
D19-1083,D18-1338,0,0.701433,"as convolutional networks (Gehring et al., 2017) and selfattention networks (Vaswani et al., 2017). The Transformer represents the current SOTA in NMT. It heavily relies on the combination of residual connections (He et al., 2015) and layer normalization (Ba et al., 2016) for convergence. Nevertheless, simply extending this model with more layers results in gradient vanishing due to the interaction of RC and LN (see Section 4). Recent work has proposed methods to train deeper 899 Transformer models, including a rescheduling of RC and LN (Vaswani et al., 2018), the transparent attention model (Bapna et al., 2018) and the stochastic residual connection (Pham et al., 2019). In contrast to these work, we identify the large output variance of RC as the source of gradient vanishing, and employ scaled initialization to mitigate it without any structure adjustment. The effect of careful initialization on boosting convergence was also investigated and verified in previous work (Zhang et al., 2019; Child et al., 2019; Devlin et al., 2019; Radford et al., 2018). The merged attention network falls into the category of simplifying the Transformer so as to shorten training and/or decoding time. Methods to improve"
D19-1083,N19-1423,0,0.520782,"ayers. Solid lines indicate the vanilla Transformer, and dashed lines denote our proposed method. During back-propagation, gradients in Transformer gradually vanish from high layers to low layers. Introduction The capability of deep neural models of handling complex dependencies has benefited various artificial intelligence tasks, such as image recognition where test error was reduced by scaling VGG nets (Simonyan and Zisserman, 2015) up to hundreds of convolutional layers (He et al., 2015). In NLP, deep self-attention networks have enabled large-scale pretrained language models such as BERT (Devlin et al., 2019) and GPT (Radford et al., 2018) to boost state-of-the-art (SOTA) performance on downstream applications. By contrast, though neural machine translation (NMT) gained encouraging improvement when shifting from a shallow architecture (Bahdanau et al., 2015) to deeper ones (Zhou et al., 2016; Wu et al., 2016; Zhang et al., 2018; Chen et al., 2018), the Transformer (Vaswani et al., 2017), a currently SOTA architecture, achieves best results with merely 6 encoder and decoder layers, and no gains were reported by Vaswani et al. (2017) from 1 Source code for reproduction is available at https:// githu"
D19-1083,D17-1151,0,0.023642,"he encoder-decoder attention into a single sublayer, allowing for parallel computation. • We conduct extensive experiments and verify that deep Transformers with DS-Init and MAtt improve translation quality while preserving decoding efficiency. 2 Related Work Our work aims at improving translation quality by increasing model depth. Compared with the single-layer NMT system (Bahdanau et al., 2015), deep NMT models are typically more capable of handling complex language variations and translation relationships via stacking multiple encoder and decoder layers (Zhou et al., 2016; Wu et al., 2016; Britz et al., 2017; Chen et al., 2018), and/or multiple attention layers (Zhang et al., 2018). One common problem for the training of deep neural models are vanishing or exploding gradients. Existing methods mainly focus on developing novel network architectures so as to stabilize gradient back-propagation, such as the fast-forward connection (Zhou et al., 2016), the linear associative unit (Wang et al., 2017), or gated recurrent network variants (Hochreiter and Schmidhuber, 1997; Gers and Schmidhuber, 2001; Cho et al., 2014; Di Gangi and Federico, 2018). In contrast to the above recurrent network based NMT mod"
D19-1083,P18-1008,0,0.11932,"recognition where test error was reduced by scaling VGG nets (Simonyan and Zisserman, 2015) up to hundreds of convolutional layers (He et al., 2015). In NLP, deep self-attention networks have enabled large-scale pretrained language models such as BERT (Devlin et al., 2019) and GPT (Radford et al., 2018) to boost state-of-the-art (SOTA) performance on downstream applications. By contrast, though neural machine translation (NMT) gained encouraging improvement when shifting from a shallow architecture (Bahdanau et al., 2015) to deeper ones (Zhou et al., 2016; Wu et al., 2016; Zhang et al., 2018; Chen et al., 2018), the Transformer (Vaswani et al., 2017), a currently SOTA architecture, achieves best results with merely 6 encoder and decoder layers, and no gains were reported by Vaswani et al. (2017) from 1 Source code for reproduction is available at https:// github.com/bzhangGo/zero 898 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 898–909, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Our contributions are summarized as follows: further incr"
D19-1083,W18-2716,0,0.0383618,"Missing"
D19-1083,W18-6301,0,0.0786423,"Missing"
D19-1083,P02-1040,0,0.104645,"putation except for two linear projections: h i SA AN(Sl−1 ) = Ma (Sl−1 Wv ) Wo , (16) 902 Dataset WMT14 En-De WMT14 En-Fr WMT18 En-Fi WMT18 Zh-En IWSLT14 De-En #Src 116M 1045M 73M 510M 3.0M #Tgt 110M 1189M 54M 576M 3.2M #Sent 4.5M 36M 3.3M 25M 159K #BPE 32K 32K 32K 32K 30K evaluate our model on all WMT14-18 test sets for WMT14 En-De translation. The settings for IWSLT14 De-En are as in Ranzato et al. (2016), with 7584 sentence pairs for development, and the concatenated dev sets for IWSLT 2014 as test set (tst2010, tst2011, tst2012, dev2010, dev2012). We report tokenized case-sensitive BLEU (Papineni et al., 2002) for WMT14 En-De and WMT14 En-Fr, and provide detokenized casesensitive BLEU for WMT14 En-De, WMT18 EnFi and Zh-En with sacreBLEU (Post, 2018)5 . We also report chrF score for En-Fi translation which was found correlated better with human evaluation (Bojar et al., 2018). Following previous work (Wu et al., 2019), we evaluate IWSLT14 DeEn with tokenized case-insensitive BLEU. Table 2: Statistics for different training datasets. #Src and #Tgt denote the number of source and target tokens respectively. #Sent: the number of bilingual sentences. #BPE: the number of merge operations in BPE. M: milli"
D19-1083,P17-1013,0,0.0287158,"015), deep NMT models are typically more capable of handling complex language variations and translation relationships via stacking multiple encoder and decoder layers (Zhou et al., 2016; Wu et al., 2016; Britz et al., 2017; Chen et al., 2018), and/or multiple attention layers (Zhang et al., 2018). One common problem for the training of deep neural models are vanishing or exploding gradients. Existing methods mainly focus on developing novel network architectures so as to stabilize gradient back-propagation, such as the fast-forward connection (Zhou et al., 2016), the linear associative unit (Wang et al., 2017), or gated recurrent network variants (Hochreiter and Schmidhuber, 1997; Gers and Schmidhuber, 2001; Cho et al., 2014; Di Gangi and Federico, 2018). In contrast to the above recurrent network based NMT models, recent work focuses on feed-forward alternatives with more smooth gradient flow, such as convolutional networks (Gehring et al., 2017) and selfattention networks (Vaswani et al., 2017). The Transformer represents the current SOTA in NMT. It heavily relies on the combination of residual connections (He et al., 2015) and layer normalization (Ba et al., 2016) for convergence. Nevertheless,"
D19-1083,P19-1176,0,0.171746,"ly. Deep Transformer with our method outperforms its 6-layer counterpart by over 0.4 points on newstest2014 and around 0.1 point on newstest2014∼newstest2018. Our model outperforms the transparent model (Bapna et al., 2018) (+1.58 BLEU), an approach for the deep encoder. Our model performs on par with current SOTA, the dynamic convolution model (DCNN) (Wu et al., 2019). In particular, though DCNN achieves encouraging performance on newstest2014, it falls behind the baseline on other test sets. By contrast, our model obtains more consistent performance improvements. In work concurrent to ours, Wang et al. (2019) discuss how the placement of layer normalization affects deep Transformers, and compare the original post-norm (which we consider our baseline) and a pre-norm layout (which we call T2T). Their results also show that pre-norm allows training of deeper Transformers. Our results show that deep post-norm Transformers are also trainable with appropriate initialization, and tend to give slightly better results. 27.75 27.50 10 15 20 25 30 Model Depth Figure 3: Test BLEU score on newstest2014 with respect to model depth for Transformer+DS-Init+MAtt. Model Vaswani et al. (2017) Chen et al. (2018) Ott"
D19-1083,W18-6319,0,0.030342,"4 De-En #Src 116M 1045M 73M 510M 3.0M #Tgt 110M 1189M 54M 576M 3.2M #Sent 4.5M 36M 3.3M 25M 159K #BPE 32K 32K 32K 32K 30K evaluate our model on all WMT14-18 test sets for WMT14 En-De translation. The settings for IWSLT14 De-En are as in Ranzato et al. (2016), with 7584 sentence pairs for development, and the concatenated dev sets for IWSLT 2014 as test set (tst2010, tst2011, tst2012, dev2010, dev2012). We report tokenized case-sensitive BLEU (Papineni et al., 2002) for WMT14 En-De and WMT14 En-Fr, and provide detokenized casesensitive BLEU for WMT14 En-De, WMT18 EnFi and Zh-En with sacreBLEU (Post, 2018)5 . We also report chrF score for En-Fi translation which was found correlated better with human evaluation (Bojar et al., 2018). Following previous work (Wu et al., 2019), we evaluate IWSLT14 DeEn with tokenized case-insensitive BLEU. Table 2: Statistics for different training datasets. #Src and #Tgt denote the number of source and target tokens respectively. #Sent: the number of bilingual sentences. #BPE: the number of merge operations in BPE. M: million, K: thousand. where Ma denotes the average mask matrix for parallel computation (Zhang et al., 2018). This new model is then combined with"
D19-1083,P16-1162,1,0.515604,"0.6. Decoding is implemented with cache to save redundant computations. Other settings for specific translation tasks are explained in the individual subsections. Experiments 7.1 Model Settings Datasets and Evaluation We take WMT14 English-German translation (En-De) (Bojar et al., 2014) as our benchmark for model analysis, and examine the generalization of our approach on four other tasks: WMT14 English-French (En-Fr), IWSLT14 German-English (De-En) (Cettolo et al., 2014), WMT18 English-Finnish (En-Fi) and WMT18 Chinese-English (Zh-En) (Bojar et al., 2018). Byte pair encoding algorithm (BPE) (Sennrich et al., 2016) is used in preprocessing to handle low frequency words. Statistics of different datasets are listed in Table 2. Except for IWSLT14 De-En task, we collect subword units independently on the source and target side of training data. We directly use the preprocessed training data from the WMT18 website3 for En-Fi and Zh-En tasks, and use newstest2017 as our development set, newstest2018 as our test set. Our training data for WMT14 EnDe and WMT14 En-Fr is identical to previous setups (Vaswani et al., 2017; Wu et al., 2019). We use newstest2013 as development set for WMT14 En-De and newstest2012+20"
D19-1083,P18-1166,1,0.692132,"asks, such as image recognition where test error was reduced by scaling VGG nets (Simonyan and Zisserman, 2015) up to hundreds of convolutional layers (He et al., 2015). In NLP, deep self-attention networks have enabled large-scale pretrained language models such as BERT (Devlin et al., 2019) and GPT (Radford et al., 2018) to boost state-of-the-art (SOTA) performance on downstream applications. By contrast, though neural machine translation (NMT) gained encouraging improvement when shifting from a shallow architecture (Bahdanau et al., 2015) to deeper ones (Zhou et al., 2016; Wu et al., 2016; Zhang et al., 2018; Chen et al., 2018), the Transformer (Vaswani et al., 2017), a currently SOTA architecture, achieves best results with merely 6 encoder and decoder layers, and no gains were reported by Vaswani et al. (2017) from 1 Source code for reproduction is available at https:// github.com/bzhangGo/zero 898 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 898–909, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Our contributions are summarized as f"
D19-1083,W18-1819,0,0.402365,"rward alternatives with more smooth gradient flow, such as convolutional networks (Gehring et al., 2017) and selfattention networks (Vaswani et al., 2017). The Transformer represents the current SOTA in NMT. It heavily relies on the combination of residual connections (He et al., 2015) and layer normalization (Ba et al., 2016) for convergence. Nevertheless, simply extending this model with more layers results in gradient vanishing due to the interaction of RC and LN (see Section 4). Recent work has proposed methods to train deeper 899 Transformer models, including a rescheduling of RC and LN (Vaswani et al., 2018), the transparent attention model (Bapna et al., 2018) and the stochastic residual connection (Pham et al., 2019). In contrast to these work, we identify the large output variance of RC as the source of gradient vanishing, and employ scaled initialization to mitigate it without any structure adjustment. The effect of careful initialization on boosting convergence was also investigated and verified in previous work (Zhang et al., 2019; Child et al., 2019; Devlin et al., 2019; Radford et al., 2018). The merged attention network falls into the category of simplifying the Transformer so as to shor"
D19-1083,D16-1050,1,0.869028,"ayer. Experimental results show that deep models trained with these techniques clearly outperform a vanilla Transformer with 6 layers in terms of BLEU, and outperforms other solutions to train deep Transformers (Bapna et al., 2018; Vaswani et al., 2018). Thanks to the more efficient merged attention sublayer, we achieve these quality improvements while matching the decoding speed of the baseline model. In the future, we would like to extend our model to other sequence-to-sequence tasks, such as summarization and dialogue generation, as well as adapt the idea to other generative architectures (Zhang et al., 2016, 2018). We have trained models with up to 30 layers each for the encoder and decoder, and while training was successful and improved over shallower counterparts, gains are relatively small beyond 12 layers. An open question is whether there are other structural issues that limit the benefits of increasing the depth of the Transformer architecture, or whether the benefit of very deep models is greater for other tasks and dataset. 0.6 0.4 0.2 0.0 (b) Decoder 1.2 Transformer Transformer Transformer Transformer Gradient Norm 1.0 L1 L18 + DS-Init L1 + DS-Init L18 0.8 0.6 0.4 0.2 0.0 0 10 20 30 40"
D19-1083,Q16-1027,0,0.0956212,"ed various artificial intelligence tasks, such as image recognition where test error was reduced by scaling VGG nets (Simonyan and Zisserman, 2015) up to hundreds of convolutional layers (He et al., 2015). In NLP, deep self-attention networks have enabled large-scale pretrained language models such as BERT (Devlin et al., 2019) and GPT (Radford et al., 2018) to boost state-of-the-art (SOTA) performance on downstream applications. By contrast, though neural machine translation (NMT) gained encouraging improvement when shifting from a shallow architecture (Bahdanau et al., 2015) to deeper ones (Zhou et al., 2016; Wu et al., 2016; Zhang et al., 2018; Chen et al., 2018), the Transformer (Vaswani et al., 2017), a currently SOTA architecture, achieves best results with merely 6 encoder and decoder layers, and no gains were reported by Vaswani et al. (2017) from 1 Source code for reproduction is available at https:// github.com/bzhangGo/zero 898 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 898–909, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics"
D19-1099,Q17-1010,0,0.00637774,"y loss. 6 Experiments Datasets We conduct experiments on CoNLL2009 (Hajiˇc et al., 2009) data set for all languages, including Catalan (Ca), Chinese (Zh), Czech (Cz), English (En), German (De), Japanese (Jp) and Spanish (Es). We use the predicted part-of-speech tags, dependency labels, and pre-identified predicate, provided with the dataset. The statistics of datasets are shown in Table 2. 4 A more canonical way of controlling stochasticity is to use the temperature but we prefer not to scale the gradient. Hyperparameters We use ELMo (Peters et al., 2018) for English, and FastText embeddings (Bojanowski et al., 2017; Grave et al., 2018) for all other languages. We train and run the refinement networks for two iterations. All other hyperparameters are the same for all languages, except BiLSTMs for English is larger than others. Training Details Training the refinement network takes roughly 2 times more time than the baseline models, as it requires running BiLSTMs. The extra computation for the structured refinement network is minimal. For English, training the iterative refinement model for 1 epoch takes about 6 minutes on one 1080ti GPU. Adam is used as the optimizer (Kingma and Ba, 2015), with the learn"
D19-1099,C18-1233,0,0.455788,"Missing"
D19-1099,D19-1544,1,0.609629,"ll A0 A1 A2 Baseline predicated role Figure 2: Confusion matrix for the baseline model, and a correction matrix where the errors were corrected by the refinement network. Only Null, A0, A1, A2 are presented here. can take an inspiration from either declarative constraints used in the previous work (Punyakanok et al., 2008) or from literature on lexical semantics of verbs, studying patterns of event and argument realization (e.g., Levin 1993). Indeed, the unique role constraint as a declarative constraint is one of the motivation for the concurrent work on modeling argument interaction in SRL (Chen et al., 2019). That work relies on capsule networks (Sabour et al., 2017) and focuses primarily on enforcing the role uniqueness constraint. The framework can be extended to other tasks. For example, in syntactic dependency parsing: the refinement network can rely on representations of grandparent nodes, siblings and children to propose a correction. In general, structure refinement networks should allow domain experts to incorporate prior knowledge about output dependencies and improve model performance. Acknowledgments We thank the anonymous reviewers for their suggestions. The project was supported by t"
D19-1099,S12-1029,0,0.0238771,"ated over the entire sentences but not the information what the other arguments are. It is a coarse compressed representation of the prediction, yet it represents long-distance information not readily available within the factorized base model. While this is not the only possible design, we believe that the empirical gains from using this simple refinement network, demonstrate the viability of our general framework of iterative refinement with restricted inference networks. They also suggest that intuitions underlying declarative constraints used in early work on SRL (Punyakanok et al., 2008; Das et al., 2012) can be revisited but now encoded in a flexible soft way to provide induction biases for the refinement networks. We leave this for future work. We consider the CoNLL-2009 dataset (Hajiˇc et al., 2009). We start with a strong factorized baseline model, which already achieves state-ofthe-art results on a subset of the languages. Then, using our structure refinement network, we improve on this baseline on all 7 CoNLL-2009 languages. The model achieves best-reported results in 5 languages, including English. We also observe improvements on out-of-domain test sets, confirming the robustness of our"
D19-1099,P18-2077,0,0.0596169,"ze n × r such that each row sums to 1, corresponding to a probability distribution over roles. In particular Ri,0 is the probability of i-th word not being an argument of the predicate. We index role label and sense predictions from different refinement iterations (‘time steps’) with t, i.e. Pt and Rt . The index t ranges from 0 to T , and P0 and R0 denotes the predictions from the factorized baseline model. Details (e.g., hyperparameters) are provided in the appendix. 1073 3.2 Factorized Model Similarly to recent approaches to SRL and semantic graph parsing (He et al., 2017; Li et al., 2018; Dozat and Manning, 2018), our factorized baseline model starts with concatenated embeddings x. Then, we encode the sentence with a BiLSTM, further extract features with an MLP (multilayer perceptron) and apply a bi-affine classifier to the resulting features to label the words with roles. We also use a predicate-dependent linear layer for sense disambiguation. More formally, we start with getting a sentence representation by concatenating embeddings. We have xw ∈ Rn×dw , xdep ∈ Rn×dδ , xpos ∈ Rn×dp for words, dependency labels and part-of-speech tags, respectively. We concatenate them to form a sentence representatio"
D19-1099,D15-1112,0,0.101519,"Missing"
D19-1099,P17-1044,0,0.202681,"s are labeled with their senses from a given sense inventory (e.g., SATISFY.01 in the example). Before the rise of deep learning methods, the most accurate SRL methods relied on modeling high-order interactions in the output space (e.g., between arguments or arguments and predicates) (Watanabe et al., 2010; Toutanova et al., 2008). Earlier neural methods can model such output interactions through a transition system, and achieve competitive performance (Henderson et al., 2013). However, current state-ofthe-art SRL systems use powerful sentence encoders (e.g., layers of LSTMs (Li et al., 2018; He et al., 2017) or multi-head self-attention (Strubell et al., 2018)) and factorize over small fragments of the predicted structures. Specifically, most modern models process individual arguments and perform predicate disambiguation independently. The trend towards more factorizable models is not unique to dependency-based SRL but common for most structured prediction tasks in NLP (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017, 2018). The only major exception is language generation tasks, especially machine translation and language modeling, where larger amounts of text are typically used in traini"
D19-1099,J13-4006,1,0.927424,"ans (see the graph in red in Figure 1). Edges in the dependency graphs are annotated with semantic roles (e.g., A0: PLEASER) and the predicates are labeled with their senses from a given sense inventory (e.g., SATISFY.01 in the example). Before the rise of deep learning methods, the most accurate SRL methods relied on modeling high-order interactions in the output space (e.g., between arguments or arguments and predicates) (Watanabe et al., 2010; Toutanova et al., 2008). Earlier neural methods can model such output interactions through a transition system, and achieve competitive performance (Henderson et al., 2013). However, current state-ofthe-art SRL systems use powerful sentence encoders (e.g., layers of LSTMs (Li et al., 2018; He et al., 2017) or multi-head self-attention (Strubell et al., 2018)) and factorize over small fragments of the predicted structures. Specifically, most modern models process individual arguments and perform predicate disambiguation independently. The trend towards more factorizable models is not unique to dependency-based SRL but common for most structured prediction tasks in NLP (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017, 2018). The only major exception is lan"
D19-1099,W18-2501,0,0.0389846,"Missing"
D19-1099,P00-1065,0,0.337018,", University of Edinburgh 2 ILLC, University of Amsterdam chunchuan.lv@gmail.com scohen@inf.ed.ac.uk ititov@inf.ed.ac.uk entity fulfilled (A1) satisfy.01 pleaser (A0) An easier standard for An An easier standard for easier standard for a state to satisfy to to satisfy satisfy satisfy.01 entity fulfilled (A1) a a state state ... Figure 1: An example of structured refinement, the sentence fragment is from CoNLL-2009: the initial prediction by the factorized model in blue, the refined one (identical to the gold standard) in red. Introduction Semantic role labeling (SRL), originally introduced by Gildea and Jurafsky (2000), involves the prediction of predicate-argument structure, i.e., identification of arguments and their assignment to underlying semantic roles. Semantic-role representations have been shown to be beneficial in many NLP applications, including question answering (Shen and Lapata, 2007), information extraction (Christensen et al., 2011) and machine translation (Marcheggiani et al., 2018). In this work, we focus on dependency-based SRL (Hajiˇc et al., 2009), a popular version of the task which involves identifying syntactic heads of arguments rather than marking entire argument spans (see the gra"
D19-1099,N10-1112,0,0.0412181,"is drawn from the standard Gumbel distribution (Maddison et al., 2017; Jang et al., 2017), and λg is a hyper-parameter controlling decoding stochasticity.4 5.3 Loss for Iterative Refinement Let us denote gold-standard labels for roles and predicates as R∗ and P∗ . We use two separate losses Lbase (R∗ , P∗ , x) and Lrefine (R∗ , P∗ , x) for our two-stage training. We define losses for predictions from each refinement iteration and sum them up: Lbase (R∗ , P∗ , x) = L(R∗ , R0 ) + L(P∗ , P0 ) (31) Lrefine (R∗ , P∗ , x) = T X L(R∗ , Rt ) + L(P∗ , Pt ) t=1 (32) We adopt the Softmax-Margin loss (Gimpel and Smith, 2010; Blondel et al., 2019) for individual L. Effectively, we subtract 1 from the logit of the gold label, and apply the cross entropy loss. 6 Experiments Datasets We conduct experiments on CoNLL2009 (Hajiˇc et al., 2009) data set for all languages, including Catalan (Ca), Chinese (Zh), Czech (Cz), English (En), German (De), Japanese (Jp) and Spanish (Es). We use the predicted part-of-speech tags, dependency labels, and pre-identified predicate, provided with the dataset. The statistics of datasets are shown in Table 2. 4 A more canonical way of controlling stochasticity is to use the temperature"
D19-1099,L18-1550,0,0.0145407,"asets We conduct experiments on CoNLL2009 (Hajiˇc et al., 2009) data set for all languages, including Catalan (Ca), Chinese (Zh), Czech (Cz), English (En), German (De), Japanese (Jp) and Spanish (Es). We use the predicted part-of-speech tags, dependency labels, and pre-identified predicate, provided with the dataset. The statistics of datasets are shown in Table 2. 4 A more canonical way of controlling stochasticity is to use the temperature but we prefer not to scale the gradient. Hyperparameters We use ELMo (Peters et al., 2018) for English, and FastText embeddings (Bojanowski et al., 2017; Grave et al., 2018) for all other languages. We train and run the refinement networks for two iterations. All other hyperparameters are the same for all languages, except BiLSTMs for English is larger than others. Training Details Training the refinement network takes roughly 2 times more time than the baseline models, as it requires running BiLSTMs. The extra computation for the structured refinement network is minimal. For English, training the iterative refinement model for 1 epoch takes about 6 minutes on one 1080ti GPU. Adam is used as the optimizer (Kingma and Ba, 2015), with the learning rate of 3e-4. We"
D19-1099,Q16-1023,0,0.0202311,"tions through a transition system, and achieve competitive performance (Henderson et al., 2013). However, current state-ofthe-art SRL systems use powerful sentence encoders (e.g., layers of LSTMs (Li et al., 2018; He et al., 2017) or multi-head self-attention (Strubell et al., 2018)) and factorize over small fragments of the predicted structures. Specifically, most modern models process individual arguments and perform predicate disambiguation independently. The trend towards more factorizable models is not unique to dependency-based SRL but common for most structured prediction tasks in NLP (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017, 2018). The only major exception is language generation tasks, especially machine translation and language modeling, where larger amounts of text are typically used in training. Powerful encoders, in principle, can capture long-distance dependencies and hence alleviate the need for modeling high-order interactions in 1071 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1071–1082, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computatio"
D19-1099,D18-1149,0,0.149851,"LFILLED, as in ‘a sweet tooth to satisfy’), instrument (A2: METHOD, as in ‘a little dessert to satisfy your sweet tooth’) and agent (A0: PLEASER, as in our actual example). The basic factorized model got it wrong, assigning A1 to the argument ‘state’. However, taking into account other arguments, the model can correct the label. The configuration ‘A1 to satisfy’ is more likely when an agent (A0) is present in the sentence. The lack of an agent boosts the score for the correct configuration ‘A0 to satisfy’. Our iterative refinement approach encodes the above intuition. In iterative refinement (Lee et al., 2018), a refinement network repeatedly takes previous output as input and produces its refined version. Formally, we have y t+1 = Refine(x, y t ). Naturally, such refinement strategy also requires an initial prediction y 0 , which is produced by a (‘base’) factorized model. Refinement strategies have been successful in machine translation (Lee et al., 2018; Novak et al., 2017; Xia et al., 2017; Hassan et al., 2018), but their effectiveness in other NLP tasks is yet to be demonstrated.1 We conjecture that this discrepancy is due to differences in data availability. Given larger amounts of training d"
D19-1099,N18-2078,1,0.775886,"e fragment is from CoNLL-2009: the initial prediction by the factorized model in blue, the refined one (identical to the gold standard) in red. Introduction Semantic role labeling (SRL), originally introduced by Gildea and Jurafsky (2000), involves the prediction of predicate-argument structure, i.e., identification of arguments and their assignment to underlying semantic roles. Semantic-role representations have been shown to be beneficial in many NLP applications, including question answering (Shen and Lapata, 2007), information extraction (Christensen et al., 2011) and machine translation (Marcheggiani et al., 2018). In this work, we focus on dependency-based SRL (Hajiˇc et al., 2009), a popular version of the task which involves identifying syntactic heads of arguments rather than marking entire argument spans (see the graph in red in Figure 1). Edges in the dependency graphs are annotated with semantic roles (e.g., A0: PLEASER) and the predicates are labeled with their senses from a given sense inventory (e.g., SATISFY.01 in the example). Before the rise of deep learning methods, the most accurate SRL methods relied on modeling high-order interactions in the output space (e.g., between arguments or arg"
D19-1099,K17-1041,1,0.824981,"Missing"
D19-1099,D07-1002,0,0.0748708,"ntity fulfilled (A1) a a state state ... Figure 1: An example of structured refinement, the sentence fragment is from CoNLL-2009: the initial prediction by the factorized model in blue, the refined one (identical to the gold standard) in red. Introduction Semantic role labeling (SRL), originally introduced by Gildea and Jurafsky (2000), involves the prediction of predicate-argument structure, i.e., identification of arguments and their assignment to underlying semantic roles. Semantic-role representations have been shown to be beneficial in many NLP applications, including question answering (Shen and Lapata, 2007), information extraction (Christensen et al., 2011) and machine translation (Marcheggiani et al., 2018). In this work, we focus on dependency-based SRL (Hajiˇc et al., 2009), a popular version of the task which involves identifying syntactic heads of arguments rather than marking entire argument spans (see the graph in red in Figure 1). Edges in the dependency graphs are annotated with semantic roles (e.g., A0: PLEASER) and the predicates are labeled with their senses from a given sense inventory (e.g., SATISFY.01 in the example). Before the rise of deep learning methods, the most accurate SRL"
D19-1099,P18-2106,0,0.039378,"Missing"
D19-1099,S15-2153,0,0.217243,"Missing"
D19-1099,N18-1202,0,0.0516561,"rom the logit of the gold label, and apply the cross entropy loss. 6 Experiments Datasets We conduct experiments on CoNLL2009 (Hajiˇc et al., 2009) data set for all languages, including Catalan (Ca), Chinese (Zh), Czech (Cz), English (En), German (De), Japanese (Jp) and Spanish (Es). We use the predicted part-of-speech tags, dependency labels, and pre-identified predicate, provided with the dataset. The statistics of datasets are shown in Table 2. 4 A more canonical way of controlling stochasticity is to use the temperature but we prefer not to scale the gradient. Hyperparameters We use ELMo (Peters et al., 2018) for English, and FastText embeddings (Bojanowski et al., 2017; Grave et al., 2018) for all other languages. We train and run the refinement networks for two iterations. All other hyperparameters are the same for all languages, except BiLSTMs for English is larger than others. Training Details Training the refinement network takes roughly 2 times more time than the baseline models, as it requires running BiLSTMs. The extra computation for the structured refinement network is minimal. For English, training the iterative refinement model for 1 epoch takes about 6 minutes on one 1080ti GPU. Adam"
D19-1099,J08-2005,0,0.664079,"previous iteration aggregated over the entire sentences but not the information what the other arguments are. It is a coarse compressed representation of the prediction, yet it represents long-distance information not readily available within the factorized base model. While this is not the only possible design, we believe that the empirical gains from using this simple refinement network, demonstrate the viability of our general framework of iterative refinement with restricted inference networks. They also suggest that intuitions underlying declarative constraints used in early work on SRL (Punyakanok et al., 2008; Das et al., 2012) can be revisited but now encoded in a flexible soft way to provide induction biases for the refinement networks. We leave this for future work. We consider the CoNLL-2009 dataset (Hajiˇc et al., 2009). We start with a strong factorized baseline model, which already achieves state-ofthe-art results on a subset of the languages. Then, using our structure refinement network, we improve on this baseline on all 7 CoNLL-2009 languages. The model achieves best-reported results in 5 languages, including English. We also observe improvements on out-of-domain test sets, confirming th"
D19-1099,P16-1113,0,0.151582,"., 2018). 6.1 Results and Discussions Test Results Results for all CoNLL-2009 languages on the standard (in-domain) datasets are presented in Table 1. We compare our best model to the best previous single model for the corresponding language (excluding ensemble ones). Most research has focused on English, but we include results of recent models which were evaluated on at least 3 languages. When compared to the previous models, both our models are very competitive, with the exception of German. On the German dataset, Mulcaire et al. (2018) also report a relatively weak result, when compared to Roth and Lapata (2016). The German dataset is the smallest one in terms of the number of predicates. Syntactic information used by Roth and Lapata (2016) may be very beneficial in this setting and may be the reason for this discrepancy. Our structured refinement approach improves over the best previous results on 5 out of 7 languages. Note that hyper-parameters of the refinement network are not tuned for individual languages, suggesting that the proposed method is robust and may be easy to apply to new languages and/or new base models. The only case where the refinement network was not effective is Chinese, where i"
D19-1099,D18-1548,0,0.173638,"Missing"
D19-1099,J08-2002,0,0.124802,"Missing"
D19-1099,N19-1335,0,0.0308413,"his work. While they improve over their baseline model, their baseline model used multilayer perceptron to encode local factors, thus the encoder power is limited. Moreover their refined model performs worse in the out-of-domain setting than their baseline model, indicating overfitting (Belanger et al., 2017). In the follow-up work, Tu and Gimpel (2018, 2019) introduce inference networks to replace gradient descent. Their inference networks directly refine the output. Improvements over competitive baselines are reported on part-of-speech tagging, named entity recognition and CCG supertagging (Tu and Gimpel, 2019). However, their inference networks are distilling knowledge from a tractable linear-chain conditional random field (CRF) model. Thus, these methods do not provide direct performance gains. More importantly, the interactions captured in these models are likely local, as they learn to mimic Markov CRFs. Denoising autoencoders (Vincent et al., 2008) can also be used to refine structure. Indeed, image segmentation can be improved through iterative inference with denoising autoencoders (Romero et al., 2017; Drozdzal et al., 2018). Their framework is very similar to ours, albeit we are working in a"
D19-1099,P19-1454,0,0.0388933,"zal et al., 2018). Their framework is very similar to ours, albeit we are working in a discrete domain. One other difference is that by using a convolutional architecture in the refinement network, they are still modeling only local interactions. At a more conceptual level, Bengio et al. (2013) argued that a denoising autoencoder should not be too robust to the input variations as to ignore the input. This indicates that we should not expect refinement networks to correct all the errors, even in theory, and hence, the refinement networks do not need to be particularly powerful. Very recently, Wang et al. (2019) used high order statistical model for Semantic Dependency Parsing (Oepen et al., 2015), and obtain improvements over strong baseline using BiLSTM. They attempted loopy belief propagation and mean field variational inference for inference, and train the model end to end. Such inference steps are well motivated. This work is similar to energy network approach (Belanger and McCallum, 2016), while a global score function is provided, and approximate inference steps are used. Comparing to ours, the inference can also be regarded as iterative structure refinement. Yet, we do not provide a global sc"
D19-1099,P10-2018,0,0.200576,"on dependency-based SRL (Hajiˇc et al., 2009), a popular version of the task which involves identifying syntactic heads of arguments rather than marking entire argument spans (see the graph in red in Figure 1). Edges in the dependency graphs are annotated with semantic roles (e.g., A0: PLEASER) and the predicates are labeled with their senses from a given sense inventory (e.g., SATISFY.01 in the example). Before the rise of deep learning methods, the most accurate SRL methods relied on modeling high-order interactions in the output space (e.g., between arguments or arguments and predicates) (Watanabe et al., 2010; Toutanova et al., 2008). Earlier neural methods can model such output interactions through a transition system, and achieve competitive performance (Henderson et al., 2013). However, current state-ofthe-art SRL systems use powerful sentence encoders (e.g., layers of LSTMs (Li et al., 2018; He et al., 2017) or multi-head self-attention (Strubell et al., 2018)) and factorize over small fragments of the predicted structures. Specifically, most modern models process individual arguments and perform predicate disambiguation independently. The trend towards more factorizable models is not unique t"
D19-1099,W09-1209,0,0.412489,"t. Out-of-Domain Results on the out-of-domain 1076 Model Ca Cz De En Ja Es Zh Avg. Roth and Lapata (2016) Marcheggiani et al. (2017) Mulcaire et al. (2018)* Previous best single model Baseline model Structured refinement 79.45 80.32 80.69 80.91 86.00 85.14 86.02 87.30 87.62 80.10 69.97 80.10 75.06 75.87 86.7 87.7 87.24 90.40 90.65 90.99 76.00 78.69 81.97 82.54 80.20 80.30 77.32 80.50 79.87 80.53 79.4 81.2 81.89 84.30 83.26 83.31 79.57 82.90 82.69 83.11 Table 1: Labeled F1 score (including senses) for all languages on the CoNLL-2009 in-domain test set. For previous best result, Catalan is from Zhao et al. (2009), Japanese is from Watanabe et al. (2010), Czech is from Henderson et al. (2013), German and Spanish are from Roth and Lapata (2016), English is from Li et al. (2018) and Chinese is from Cai et al. (2018). We report the best testing results from Mulcaire et al. (2018). Ca Cz De En Ja Es Zh #sent 13200 38727 36020 39279 4393 14329 22277 #pred 37444 414133 17400 179014 25712 43828 102827 #pred/#sent 2.84 10.69 0.48 4.56 5.85 3.06 4.62 accurate predictions. A potential alternative explanation is that our refinement network is restricted to simple interactions, resulting in the fixed point reachab"
D19-1391,D13-1160,0,0.23937,"slot and a complete program is executed against a table to obtain a denotation. Introduction Semantic parsing is the task of translating natural language to machine interpretable meaning representations. Typically, it requires mapping a natural language utterance onto a program, which is executed against a knowledge base to obtain an answer or a denotation. Most previous work (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Lu et al., 2008; Jia and Liang, 2016) has focused on the supervised setting where a model is learned from question-program pairs. Weakly supervised semantic parsing (Berant et al., 2013; Liang et al., 2011) reduces the burden of annotating programs by learning from questions paired with their answers (or denotations). Two major challenges arise when learning from denotations: 1) training of the semantic parser requires exploring a large search space of possible programs to find those which are consistent, and execute to correct denotations; 2) the parser should be robust to spurious programs which accidentally execute to correct denotations, but do not reflect the semantics of the question. In this paper, we propose a weakly-supervised neural semantic parser that features st"
D19-1391,N19-1273,0,0.535378,"rs were based on AllenNLP (Gardner et al., 2017).5 4.2 Baselines Aside from comparing our model against previously published approaches, we also implemented the following baselines: Typed Seq2Seq Programs were generated using a sequence-to-sequence model with attention (Dong and Lapata, 2016). Similarly to Krishnamurthy et al. (2017), we constrained the decod5 Please refer to the Appendix for the full list of hyperparameters used in our experiments. Supervised by Denotations Dev. Test Pasupat and Liang (2015) Neelakantan et al. (2017) Haug et al. (2018) Zhang et al. (2017) Liang et al. (2018) Dasigi et al. (2019) Agarwal et al. (2019) 37.0 34.1 — 40.4 42.3 42.1 43.2 37.1 34.2 34.8 43.7 43.1 43.9 44.1 Typed Seq2Seq Abstract Programs f.w. standard attention f.w. structured attention 37.3 38.3 39.4 43.7 41.4 44.5 Table 1: Results on W IKI TABLE Q UESTIONS. f.w. stands for slots filled with. ing process so that only well-formed programs are predicted. This baseline can be viewed as merging the two stages of our model into one stage where generation of abstract programs and their instantiations are performed with a shared decoder. Standard Attention The aligned representation of slot s in Equation (6) is c"
D19-1391,P16-1004,1,0.944039,"on scores were computed based on the dot product between two vectors. Each MLP is a one-hidden-layer perceptron with ReLU as the activation function. Dropout (Srivastava et al., 2014) was applied to prevent overfitting. All models were trained with Adam (Kingma and Ba, 2015). Implementations of abstract and instantiation grammars were based on AllenNLP (Gardner et al., 2017).5 4.2 Baselines Aside from comparing our model against previously published approaches, we also implemented the following baselines: Typed Seq2Seq Programs were generated using a sequence-to-sequence model with attention (Dong and Lapata, 2016). Similarly to Krishnamurthy et al. (2017), we constrained the decod5 Please refer to the Appendix for the full list of hyperparameters used in our experiments. Supervised by Denotations Dev. Test Pasupat and Liang (2015) Neelakantan et al. (2017) Haug et al. (2018) Zhang et al. (2017) Liang et al. (2018) Dasigi et al. (2019) Agarwal et al. (2019) 37.0 34.1 — 40.4 42.3 42.1 43.2 37.1 34.2 34.8 43.7 43.1 43.9 44.1 Typed Seq2Seq Abstract Programs f.w. standard attention f.w. structured attention 37.3 38.3 39.4 43.7 41.4 44.5 Table 1: Results on W IKI TABLE Q UESTIONS. f.w. stands for slots fille"
D19-1391,P18-1068,1,0.821412,"ically, we decompose semantic parsing into two steps: 1) a natural language utterance is first mapped to an abstract program which is a composition of high-level operations; and 2) the abstract program is then instantiated with low-level operations that usually involve relations and entities specific to the knowledge base at hand. This decomposition is motivated by the observation that only a small number of sensible abstract programs can be instantiated into consistent programs. Similar ideas of using abstract meaning representations have been explored with fully-supervised semantic parsers (Dong and Lapata, 2018; Catherine Finegan-Dollak and Radev, 2018) and in other related tasks (Goldman et al., 2018; Herzig and Berant, 2018; Nye et al., 2019). For a knowledge base in tabular format, we abstract two basic operations of row selection and column selection from programs: these are handled in the second (instantiation) stage. As shown in Figure 1, the question is first mapped to the abstract program “select (#row slot, #column slot)” whose two slots are subsequently instantiated with filter conditions (row slot) and a column name (column slot). During the instantiation of abstract programs, each slot s"
D19-1391,P18-1168,0,0.081132,"Missing"
D19-1391,D10-1119,0,0.0340866,"o models which enforce type constraints (Yin and Neubig, 2017; Rabinovich et al., 2017; Krishnamurthy et al., 2017) so as to restrict the vast search space of potential programs. We use both methods as baselines to show that the structured bias introduced by our model can help our parser handle spurious programs in the setting of learning from denotations. Note that our alignment model can also be applied in the supervised case in order to help the parser rule out incorrect programs. Earlier work has used lexicon mappings (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2010) to model correspondences between programs and natural language. However, these methods cannot generalize to unseen tables where new relations and entities appear. To address this issue, Pasupat and Liang (2015) propose a floating parser which allows partial programs to be generated without being anchored to question tokens. In the same spirit, we use a sequence-to-sequence model to generate abstract programs while relying on explicit alignments to instantiate them. Besides semantic parsing, treating alignments as discrete latent variables has proved effective in other tasks like sequence tran"
D19-1391,P11-1060,0,0.0663622,"rogram is executed against a table to obtain a denotation. Introduction Semantic parsing is the task of translating natural language to machine interpretable meaning representations. Typically, it requires mapping a natural language utterance onto a program, which is executed against a knowledge base to obtain an answer or a denotation. Most previous work (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Lu et al., 2008; Jia and Liang, 2016) has focused on the supervised setting where a model is learned from question-program pairs. Weakly supervised semantic parsing (Berant et al., 2013; Liang et al., 2011) reduces the burden of annotating programs by learning from questions paired with their answers (or denotations). Two major challenges arise when learning from denotations: 1) training of the semantic parser requires exploring a large search space of possible programs to find those which are consistent, and execute to correct denotations; 2) the parser should be robust to spurious programs which accidentally execute to correct denotations, but do not reflect the semantics of the question. In this paper, we propose a weakly-supervised neural semantic parser that features structured latent align"
D19-1391,D08-1082,0,0.180738,"rating an abstract program for a question, our parser finds alignments between slots (with prefix #) and question spans. Based on the alignment, it instantiates each slot and a complete program is executed against a table to obtain a denotation. Introduction Semantic parsing is the task of translating natural language to machine interpretable meaning representations. Typically, it requires mapping a natural language utterance onto a program, which is executed against a knowledge base to obtain an answer or a denotation. Most previous work (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Lu et al., 2008; Jia and Liang, 2016) has focused on the supervised setting where a model is learned from question-program pairs. Weakly supervised semantic parsing (Berant et al., 2013; Liang et al., 2011) reduces the burden of annotating programs by learning from questions paired with their answers (or denotations). Two major challenges arise when learning from denotations: 1) training of the semantic parser requires exploring a large search space of possible programs to find those which are consistent, and execute to correct denotations; 2) the parser should be robust to spurious programs which accidental"
D19-1391,D15-1166,0,0.0114607,"imilarly to Xu et al. (2017) and Catherine Finegan-Dollak and Radev (2018), we generate them with a seq2seq model. Although templatebased approaches would be more efficient in practice, a seq2seq model is more general since it could generate unseen abstract programs which fixed templates could not otherwise handle. Our goal here is to generate a sequence of production rules that lead to abstract programs. During decoding, the hidden state gj of the jth timestep is computed based on the previous production rule, which is mapped to an embedding aj−1 . We also incorporate an attention mechanism (Luong et al., 2015) to compute a contextual vector bj . Finally, a score vector sj is computed by feeding the concatenation of the hidden state and context vector to a multilayer perceptron (MLP): gj = LSTM(gj−1 , aj−1 ) bj = Attention(gj , l) sj = MLP1 ([gj ; bj ]) (3) p(aj |x, t, a<j ) = softmaxaj (sj ) 3.4 Instantiating Abstract Programs To instantiate an abstract program, each slot must obtain its specific semantics from the question. We model this process by an alignment model which learns the correspondence between slots and question spans. Formally, we use a binary alignment matrix A with size m × n × n,"
D19-1391,P18-1037,1,0.715409,"nd natural language. However, these methods cannot generalize to unseen tables where new relations and entities appear. To address this issue, Pasupat and Liang (2015) propose a floating parser which allows partial programs to be generated without being anchored to question tokens. In the same spirit, we use a sequence-to-sequence model to generate abstract programs while relying on explicit alignments to instantiate them. Besides semantic parsing, treating alignments as discrete latent variables has proved effective in other tasks like sequence transduction (Yu et al., 2016) and AMR parsing (Lyu and Titov, 2018). Learning from Denotations To improve the efficiency of searching for consistent programs, Zhang et al. (2017) use a macro grammar induced from cached consistent programs. Unlike Zhang et al. (2017) who abstract entities and relations from logical forms, we take a step further and abstract the computation of row and column selection. Our work also differs from Pasupat and Liang (2016) who resort to manual annotations to alleviate spuriousness. Instead, we equip our parser with an inductive bias to rule out spurious programs during training. Recently, reinforcement learning based methods addre"
D19-1391,P17-1097,0,0.0468624,"and relations from logical forms, we take a step further and abstract the computation of row and column selection. Our work also differs from Pasupat and Liang (2016) who resort to manual annotations to alleviate spuriousness. Instead, we equip our parser with an inductive bias to rule out spurious programs during training. Recently, reinforcement learning based methods address the computational challenge by using a memory buffer (Liang et al., 2018) which stores consistent programs and an auxiliary reward function (Agarwal et al., 2019) which provides feedback to deal with spurious programs. Guu et al. (2017) employ various strategies to encourage even distributions over consistent programs in cases where the parser has been misled by spurious programs. Dasigi et al. (2019) use coverage of lexicon-like rules to guide the search of consistent programs. 6 Conclusions In this paper, we proposed a neural semantic parser that learns from denotations using abstract programs and latent structured alignments. Our parser achieves state-of-the-art performance on two benchmarks, W IKI TABLE Q UESTIONS and W IKI SQL. Empirical analysis shows that the inductive bias introduced by the alignment model helps our"
D19-1391,D18-1190,0,0.0255333,"Missing"
D19-1391,N18-2115,0,0.0366449,"Missing"
D19-1391,P16-1002,0,0.112553,"t program for a question, our parser finds alignments between slots (with prefix #) and question spans. Based on the alignment, it instantiates each slot and a complete program is executed against a table to obtain a denotation. Introduction Semantic parsing is the task of translating natural language to machine interpretable meaning representations. Typically, it requires mapping a natural language utterance onto a program, which is executed against a knowledge base to obtain an answer or a denotation. Most previous work (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Lu et al., 2008; Jia and Liang, 2016) has focused on the supervised setting where a model is learned from question-program pairs. Weakly supervised semantic parsing (Berant et al., 2013; Liang et al., 2011) reduces the burden of annotating programs by learning from questions paired with their answers (or denotations). Two major challenges arise when learning from denotations: 1) training of the semantic parser requires exploring a large search space of possible programs to find those which are consistent, and execute to correct denotations; 2) the parser should be robust to spurious programs which accidentally execute to correct"
D19-1391,P15-1142,0,0.552994,"e, we could replace the uniqueness constraint with modeling the number of slots aligned to a span, or favor sparse alignment distributions. Crucially, the two-stage framework makes it easier to inject prior knowledge about datasets and formalisms while maintaining efficiency. 2 Background Given knowledge base t, our task is to map a natural utterance x to program z, which is then executed against a knowledge base to obtain denotation [[z]]t = d. We train our parser only based on d without access to correct programs z ∗ . Our experiments focus on two benchmarks, namely W IK I TABLE Q UESTIONS (Pasupat and Liang, 2015) and W IKI SQL (Zhong et al., 2017) where each question is paired with a Wikipedia table and a denotation. Figure 1 shows a simplified example taken from W IKI TABLE Q UESTIONS. 2.1 Grammars Executable programs z that can query tables are defined according to a language. Specifically, the search space of programs is constrained by grammar rules so that it can be explored efficiently. We adopt the variable-free language of Liang et al. (2018) and define an abstract grammar and an instantiation grammar which decompose the generation of a program in two stages.3 The first stage involves the gener"
D19-1391,P16-1003,0,0.0138843,"n explicit alignments to instantiate them. Besides semantic parsing, treating alignments as discrete latent variables has proved effective in other tasks like sequence transduction (Yu et al., 2016) and AMR parsing (Lyu and Titov, 2018). Learning from Denotations To improve the efficiency of searching for consistent programs, Zhang et al. (2017) use a macro grammar induced from cached consistent programs. Unlike Zhang et al. (2017) who abstract entities and relations from logical forms, we take a step further and abstract the computation of row and column selection. Our work also differs from Pasupat and Liang (2016) who resort to manual annotations to alleviate spuriousness. Instead, we equip our parser with an inductive bias to rule out spurious programs during training. Recently, reinforcement learning based methods address the computational challenge by using a memory buffer (Liang et al., 2018) which stores consistent programs and an auxiliary reward function (Agarwal et al., 2019) which provides feedback to deal with spurious programs. Guu et al. (2017) employ various strategies to encourage even distributions over consistent programs in cases where the parser has been misled by spurious programs. D"
D19-1391,D14-1162,0,0.0820315,"sets, tables are extracted from Wikipedia and cover a wide range of domains. Entity extraction is important during parsing since entities are used as values in filter conditions during instantiation. String entities are extracted by string matching utterance spans and table cells. In W IKI TABLE Q UESTIONS, numbers and dates are extracted from the CoreNLP annotations released with the dataset. W IKI SQL does not have entities for dates, and we use string-based normalization to deal with numbers. Implementation We obtained word embeddings by a linear projection of GloVe pre-trained embeddings (Pennington et al., 2014) which were fixed during training. Attention scores were computed based on the dot product between two vectors. Each MLP is a one-hidden-layer perceptron with ReLU as the activation function. Dropout (Srivastava et al., 2014) was applied to prevent overfitting. All models were trained with Adam (Kingma and Ba, 2015). Implementations of abstract and instantiation grammars were based on AllenNLP (Gardner et al., 2017).5 4.2 Baselines Aside from comparing our model against previously published approaches, we also implemented the following baselines: Typed Seq2Seq Programs were generated using a s"
D19-1391,P17-1105,0,0.0207745,"programs cannot be correctly instantiated. Table 4 shows the proportion of errors attested by the two attention models. We observe that structured attention suffers less from instantiation errors compared against the standard attention baseline, which points to the benefits of the structured alignment model. 3781 5 Related Work Neural Semantic Parsing We follow the line of work that applies sequence-to-sequence models (Sutskever et al., 2014) to semantic parsing (Jia and Liang, 2016; Dong and Lapata, 2016). Our work also relates to models which enforce type constraints (Yin and Neubig, 2017; Rabinovich et al., 2017; Krishnamurthy et al., 2017) so as to restrict the vast search space of potential programs. We use both methods as baselines to show that the structured bias introduced by our model can help our parser handle spurious programs in the setting of learning from denotations. Note that our alignment model can also be applied in the supervised case in order to help the parser rule out incorrect programs. Earlier work has used lexicon mappings (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2010) to model correspondences between programs and natural langua"
D19-1391,D17-1160,0,0.0686532,"ctly instantiated. Table 4 shows the proportion of errors attested by the two attention models. We observe that structured attention suffers less from instantiation errors compared against the standard attention baseline, which points to the benefits of the structured alignment model. 3781 5 Related Work Neural Semantic Parsing We follow the line of work that applies sequence-to-sequence models (Sutskever et al., 2014) to semantic parsing (Jia and Liang, 2016; Dong and Lapata, 2016). Our work also relates to models which enforce type constraints (Yin and Neubig, 2017; Rabinovich et al., 2017; Krishnamurthy et al., 2017) so as to restrict the vast search space of potential programs. We use both methods as baselines to show that the structured bias introduced by our model can help our parser handle spurious programs in the setting of learning from denotations. Note that our alignment model can also be applied in the supervised case in order to help the parser rule out incorrect programs. Earlier work has used lexicon mappings (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2010) to model correspondences between programs and natural language. However, these methods ca"
D19-1391,P18-1034,0,0.123129,"Missing"
D19-1391,Q15-1003,0,0.0749439,"Missing"
D19-1391,P07-1121,0,0.367466,"s: Figure 1: After generating an abstract program for a question, our parser finds alignments between slots (with prefix #) and question spans. Based on the alignment, it instantiates each slot and a complete program is executed against a table to obtain a denotation. Introduction Semantic parsing is the task of translating natural language to machine interpretable meaning representations. Typically, it requires mapping a natural language utterance onto a program, which is executed against a knowledge base to obtain an answer or a denotation. Most previous work (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Lu et al., 2008; Jia and Liang, 2016) has focused on the supervised setting where a model is learned from question-program pairs. Weakly supervised semantic parsing (Berant et al., 2013; Liang et al., 2011) reduces the burden of annotating programs by learning from questions paired with their answers (or denotations). Two major challenges arise when learning from denotations: 1) training of the semantic parser requires exploring a large search space of possible programs to find those which are consistent, and execute to correct denotations; 2) the parser should be robust to spurious programs"
D19-1391,P17-1041,0,0.0241277,"d and linked, abstract programs cannot be correctly instantiated. Table 4 shows the proportion of errors attested by the two attention models. We observe that structured attention suffers less from instantiation errors compared against the standard attention baseline, which points to the benefits of the structured alignment model. 3781 5 Related Work Neural Semantic Parsing We follow the line of work that applies sequence-to-sequence models (Sutskever et al., 2014) to semantic parsing (Jia and Liang, 2016; Dong and Lapata, 2016). Our work also relates to models which enforce type constraints (Yin and Neubig, 2017; Rabinovich et al., 2017; Krishnamurthy et al., 2017) so as to restrict the vast search space of potential programs. We use both methods as baselines to show that the structured bias introduced by our model can help our parser handle spurious programs in the setting of learning from denotations. Note that our alignment model can also be applied in the supervised case in order to help the parser rule out incorrect programs. Earlier work has used lexicon mappings (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2010) to model correspondences between pr"
D19-1391,D16-1138,0,0.0167158,"correspondences between programs and natural language. However, these methods cannot generalize to unseen tables where new relations and entities appear. To address this issue, Pasupat and Liang (2015) propose a floating parser which allows partial programs to be generated without being anchored to question tokens. In the same spirit, we use a sequence-to-sequence model to generate abstract programs while relying on explicit alignments to instantiate them. Besides semantic parsing, treating alignments as discrete latent variables has proved effective in other tasks like sequence transduction (Yu et al., 2016) and AMR parsing (Lyu and Titov, 2018). Learning from Denotations To improve the efficiency of searching for consistent programs, Zhang et al. (2017) use a macro grammar induced from cached consistent programs. Unlike Zhang et al. (2017) who abstract entities and relations from logical forms, we take a step further and abstract the computation of row and column selection. Our work also differs from Pasupat and Liang (2016) who resort to manual annotations to alleviate spuriousness. Instead, we equip our parser with an inductive bias to rule out spurious programs during training. Recently, rein"
D19-1391,N18-2093,0,0.0780745,"Missing"
D19-1391,D07-1071,0,0.0807273,"g (Jia and Liang, 2016; Dong and Lapata, 2016). Our work also relates to models which enforce type constraints (Yin and Neubig, 2017; Rabinovich et al., 2017; Krishnamurthy et al., 2017) so as to restrict the vast search space of potential programs. We use both methods as baselines to show that the structured bias introduced by our model can help our parser handle spurious programs in the setting of learning from denotations. Note that our alignment model can also be applied in the supervised case in order to help the parser rule out incorrect programs. Earlier work has used lexicon mappings (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2010) to model correspondences between programs and natural language. However, these methods cannot generalize to unseen tables where new relations and entities appear. To address this issue, Pasupat and Liang (2015) propose a floating parser which allows partial programs to be generated without being anchored to question tokens. In the same spirit, we use a sequence-to-sequence model to generate abstract programs while relying on explicit alignments to instantiate them. Besides semantic parsing, treating alignments as discrete late"
D19-1391,D17-1125,0,0.782666,"ons of abstract and instantiation grammars were based on AllenNLP (Gardner et al., 2017).5 4.2 Baselines Aside from comparing our model against previously published approaches, we also implemented the following baselines: Typed Seq2Seq Programs were generated using a sequence-to-sequence model with attention (Dong and Lapata, 2016). Similarly to Krishnamurthy et al. (2017), we constrained the decod5 Please refer to the Appendix for the full list of hyperparameters used in our experiments. Supervised by Denotations Dev. Test Pasupat and Liang (2015) Neelakantan et al. (2017) Haug et al. (2018) Zhang et al. (2017) Liang et al. (2018) Dasigi et al. (2019) Agarwal et al. (2019) 37.0 34.1 — 40.4 42.3 42.1 43.2 37.1 34.2 34.8 43.7 43.1 43.9 44.1 Typed Seq2Seq Abstract Programs f.w. standard attention f.w. structured attention 37.3 38.3 39.4 43.7 41.4 44.5 Table 1: Results on W IKI TABLE Q UESTIONS. f.w. stands for slots filled with. ing process so that only well-formed programs are predicted. This baseline can be viewed as merging the two stages of our model into one stage where generation of abstract programs and their instantiations are performed with a shared decoder. Standard Attention The aligned repr"
D19-1448,P17-1080,0,0.0460681,"gure 11c) parts of a sentence. It can be clearly seen that LM first accumulates information about the left part of CCG, understanding the syntactic structure of the past. Then this information gets dismissed while forming information about future. Figure 12 shows how representations of different occurrences of the token “is” get reordered in the space according to CCG tags (colors correspond to tags). 7 Additional related work Previous work analyzed representations of MT and/or LM models by using probing tasks. Different levels of linguistic analysis have been considered including morphology (Belinkov et al., 2017a; Dalvi et al., 2017; Bisazza and Tump, 2018), syntax (Shi et al., 2016; Tenney et al., 2019b) and semantics (Hill et al., 2017; Belinkov et al., 2017b; Raganato and Tiedemann, 2018; Tenney et al., 2019b). Our work complements this 5 To derive CCG supertags, we use Yoshikawa et al. (2017) tagger, the latest version with ELMO: https://github. com/masashi-y/depccg. 6 In their experiments, representations are pooled across layers with the scalar mixing technique similar to the one used in the ELMO model (Peters et al., 2018). The probing classifier is trained jointly with the mixing weights, and"
D19-1448,I17-1001,0,0.0439048,"gure 11c) parts of a sentence. It can be clearly seen that LM first accumulates information about the left part of CCG, understanding the syntactic structure of the past. Then this information gets dismissed while forming information about future. Figure 12 shows how representations of different occurrences of the token “is” get reordered in the space according to CCG tags (colors correspond to tags). 7 Additional related work Previous work analyzed representations of MT and/or LM models by using probing tasks. Different levels of linguistic analysis have been considered including morphology (Belinkov et al., 2017a; Dalvi et al., 2017; Bisazza and Tump, 2018), syntax (Shi et al., 2016; Tenney et al., 2019b) and semantics (Hill et al., 2017; Belinkov et al., 2017b; Raganato and Tiedemann, 2018; Tenney et al., 2019b). Our work complements this 5 To derive CCG supertags, we use Yoshikawa et al. (2017) tagger, the latest version with ELMO: https://github. com/masashi-y/depccg. 6 In their experiments, representations are pooled across layers with the scalar mixing technique similar to the one used in the ELMO model (Peters et al., 2018). The probing classifier is trained jointly with the mixing weights, and"
D19-1448,D18-1313,0,0.0516911,"arly seen that LM first accumulates information about the left part of CCG, understanding the syntactic structure of the past. Then this information gets dismissed while forming information about future. Figure 12 shows how representations of different occurrences of the token “is” get reordered in the space according to CCG tags (colors correspond to tags). 7 Additional related work Previous work analyzed representations of MT and/or LM models by using probing tasks. Different levels of linguistic analysis have been considered including morphology (Belinkov et al., 2017a; Dalvi et al., 2017; Bisazza and Tump, 2018), syntax (Shi et al., 2016; Tenney et al., 2019b) and semantics (Hill et al., 2017; Belinkov et al., 2017b; Raganato and Tiedemann, 2018; Tenney et al., 2019b). Our work complements this 5 To derive CCG supertags, we use Yoshikawa et al. (2017) tagger, the latest version with ELMO: https://github. com/masashi-y/depccg. 6 In their experiments, representations are pooled across layers with the scalar mixing technique similar to the one used in the ELMO model (Peters et al., 2018). The probing classifier is trained jointly with the mixing weights, and the learned coefficients are used to estimate"
D19-1448,P18-2003,0,0.133599,"at the top MLM layers. 1 Introduction Deep (i.e. multi-layered) neural networks have become the standard approach for many natural language processing (NLP) tasks, and their analysis has been an active topic of research. One popular approach for analyzing representations of neural models is to evaluate how informative they are for various linguistic tasks, so-called “probing tasks”. Previous work has made some interesting observations regarding these representations; for example, Zhang and Bowman (2018) show that untrained LSTMs outperform trained ones on a word identity prediction task; and Blevins et al. (2018) show that up to a certain layer performance of representations obtained from a deep LM improves on a constituent labeling task, but then decreases, while with representations obtained from an MT encoder performance continues to improve up to the highest layer. These observations have, however, been somewhat anecdotal and an explanation of the process behind such behavior has been lacking. In this paper, we attempt to explain more generally why such behavior is observed. Rather than measuring the quality of representations obtained from a particular model on some auxiliary task, we characteriz"
D19-1448,I17-1015,0,0.0126937,"tence. It can be clearly seen that LM first accumulates information about the left part of CCG, understanding the syntactic structure of the past. Then this information gets dismissed while forming information about future. Figure 12 shows how representations of different occurrences of the token “is” get reordered in the space according to CCG tags (colors correspond to tags). 7 Additional related work Previous work analyzed representations of MT and/or LM models by using probing tasks. Different levels of linguistic analysis have been considered including morphology (Belinkov et al., 2017a; Dalvi et al., 2017; Bisazza and Tump, 2018), syntax (Shi et al., 2016; Tenney et al., 2019b) and semantics (Hill et al., 2017; Belinkov et al., 2017b; Raganato and Tiedemann, 2018; Tenney et al., 2019b). Our work complements this 5 To derive CCG supertags, we use Yoshikawa et al. (2017) tagger, the latest version with ELMO: https://github. com/masashi-y/depccg. 6 In their experiments, representations are pooled across layers with the scalar mixing technique similar to the one used in the ELMO model (Peters et al., 2018). The probing classifier is trained jointly with the mixing weights, and the learned coeffici"
D19-1448,P14-5010,0,0.00471906,"Missing"
D19-1448,N18-1202,0,0.0802825,"vels of linguistic analysis have been considered including morphology (Belinkov et al., 2017a; Dalvi et al., 2017; Bisazza and Tump, 2018), syntax (Shi et al., 2016; Tenney et al., 2019b) and semantics (Hill et al., 2017; Belinkov et al., 2017b; Raganato and Tiedemann, 2018; Tenney et al., 2019b). Our work complements this 5 To derive CCG supertags, we use Yoshikawa et al. (2017) tagger, the latest version with ELMO: https://github. com/masashi-y/depccg. 6 In their experiments, representations are pooled across layers with the scalar mixing technique similar to the one used in the ELMO model (Peters et al., 2018). The probing classifier is trained jointly with the mixing weights, and the learned coefficients are used to estimate the contribution of different layers to a particular task. Figure 12: t-SNE of different occurrences of the token “is”, CCG tag is in color (intensity of a color is a token position). On the x-axis are layers. line of research by analyzing how word representations evolve between layers and gives insights into how models trained on different tasks come to represent different information. Canonical correlation analysis has been previously used to investigate learning dynamics of"
D19-1448,W18-5431,0,0.0280036,"this information gets dismissed while forming information about future. Figure 12 shows how representations of different occurrences of the token “is” get reordered in the space according to CCG tags (colors correspond to tags). 7 Additional related work Previous work analyzed representations of MT and/or LM models by using probing tasks. Different levels of linguistic analysis have been considered including morphology (Belinkov et al., 2017a; Dalvi et al., 2017; Bisazza and Tump, 2018), syntax (Shi et al., 2016; Tenney et al., 2019b) and semantics (Hill et al., 2017; Belinkov et al., 2017b; Raganato and Tiedemann, 2018; Tenney et al., 2019b). Our work complements this 5 To derive CCG supertags, we use Yoshikawa et al. (2017) tagger, the latest version with ELMO: https://github. com/masashi-y/depccg. 6 In their experiments, representations are pooled across layers with the scalar mixing technique similar to the one used in the ELMO model (Peters et al., 2018). The probing classifier is trained jointly with the mixing weights, and the learned coefficients are used to estimate the contribution of different layers to a particular task. Figure 12: t-SNE of different occurrences of the token “is”, CCG tag is in c"
D19-1448,N19-1329,0,0.0659016,"On the x-axis are layers. line of research by analyzing how word representations evolve between layers and gives insights into how models trained on different tasks come to represent different information. Canonical correlation analysis has been previously used to investigate learning dynamics of CNNs and RNNs, to measure the intrinsic dimensionality of layers in CNNs and compare representations of networks which memorize and generalize (Raghu et al., 2017; Morcos et al., 2018). Bau et al. (2019) used SVCCA as one of the methods used for identifying important individual neurons in NMT models. Saphra and Lopez (2019) used SVCCA to investigate how representations of linguistic structure are learned over time in LMs. 8 Conclusions In this work, we analyze how the learning objective determines the information flow in the model. We propose to view the evolution of a token representation between layers from the compression/prediction trade-off perspective. We conduct a series of experiments supporting this view and propose a possible explanation for superior performance of MLM over LM for pretraining. We relate our findings to observations previously made in the context of probing tasks. Acknowledgments We wou"
D19-1448,P16-1162,1,0.356275,"Missing"
D19-1448,D16-1159,0,0.0223667,"es information about the left part of CCG, understanding the syntactic structure of the past. Then this information gets dismissed while forming information about future. Figure 12 shows how representations of different occurrences of the token “is” get reordered in the space according to CCG tags (colors correspond to tags). 7 Additional related work Previous work analyzed representations of MT and/or LM models by using probing tasks. Different levels of linguistic analysis have been considered including morphology (Belinkov et al., 2017a; Dalvi et al., 2017; Bisazza and Tump, 2018), syntax (Shi et al., 2016; Tenney et al., 2019b) and semantics (Hill et al., 2017; Belinkov et al., 2017b; Raganato and Tiedemann, 2018; Tenney et al., 2019b). Our work complements this 5 To derive CCG supertags, we use Yoshikawa et al. (2017) tagger, the latest version with ELMO: https://github. com/masashi-y/depccg. 6 In their experiments, representations are pooled across layers with the scalar mixing technique similar to the one used in the ELMO model (Peters et al., 2018). The probing classifier is trained jointly with the mixing weights, and the learned coefficients are used to estimate the contribution of diffe"
D19-1448,P17-1026,0,0.0240237,"ifferent occurrences of the token “is” get reordered in the space according to CCG tags (colors correspond to tags). 7 Additional related work Previous work analyzed representations of MT and/or LM models by using probing tasks. Different levels of linguistic analysis have been considered including morphology (Belinkov et al., 2017a; Dalvi et al., 2017; Bisazza and Tump, 2018), syntax (Shi et al., 2016; Tenney et al., 2019b) and semantics (Hill et al., 2017; Belinkov et al., 2017b; Raganato and Tiedemann, 2018; Tenney et al., 2019b). Our work complements this 5 To derive CCG supertags, we use Yoshikawa et al. (2017) tagger, the latest version with ELMO: https://github. com/masashi-y/depccg. 6 In their experiments, representations are pooled across layers with the scalar mixing technique similar to the one used in the ELMO model (Peters et al., 2018). The probing classifier is trained jointly with the mixing weights, and the learned coefficients are used to estimate the contribution of different layers to a particular task. Figure 12: t-SNE of different occurrences of the token “is”, CCG tag is in color (intensity of a color is a token position). On the x-axis are layers. line of research by analyzing how"
D19-1448,W18-5448,0,0.173809,"g the token identity and producing a more generalized token representation. The token identity then gets recreated at the top MLM layers. 1 Introduction Deep (i.e. multi-layered) neural networks have become the standard approach for many natural language processing (NLP) tasks, and their analysis has been an active topic of research. One popular approach for analyzing representations of neural models is to evaluate how informative they are for various linguistic tasks, so-called “probing tasks”. Previous work has made some interesting observations regarding these representations; for example, Zhang and Bowman (2018) show that untrained LSTMs outperform trained ones on a word identity prediction task; and Blevins et al. (2018) show that up to a certain layer performance of representations obtained from a deep LM improves on a constituent labeling task, but then decreases, while with representations obtained from an MT encoder performance continues to improve up to the highest layer. These observations have, however, been somewhat anecdotal and an explanation of the process behind such behavior has been lacking. In this paper, we attempt to explain more generally why such behavior is observed. Rather than"
D19-1448,P19-1452,0,0.430652,"re we primarily want to understand what representation in each layer ‘focuses’ on. We evaluate to what extent a certain property is important for defining a token representation at each layer by (1) selecting a large number of token occurrences and taking their representations; (2) validating if a value of the property is the same for token occurrences corresponding to the closest representations. Though our approach is different from probing tasks, we choose the properties which will enable us to relate to other works reporting similar behaviour (Zhang and Bowman, 2018; Blevins et al., 2018; Tenney et al., 2019a). The properties we consider are token identity, position in a sentence, neighboring tokens and CCG supertags. 6.1 Methodology For our analysis, we take 100 random word types from the top 5,000 in our vocabulary. For each word type, we gather 1,000 different occurrences along with the representations from all three models. For each representation, we take the closest neighbors among representations at each layer and evaluate the percentage of neighbors with the same value of the property. 6.2 Preserving token identity and position In this section, we track the loss of information about token"
D19-1448,P19-1580,1,0.84886,"ers. In contrast, rare tokens are not the most influencing ones at the lower layers of MLM. We hypothesize that the training procedure of MLM, with masking out some tokens or replacing them with random ones, teaches the model not to over-rely on these tokens before their context is well understood. To test our hypothesis, we additionally trained MT and LM models with token dropout on the input side (Figure 6). As we expected, there is no extreme influence of rare tokens when using this regularization, supporting the above interpretation. Interestingly, our earlier study of the MT Transformer (Voita et al., 2019) shows how this influence of rare tokens is implemented by the model. In that work, we observed that, for any considered language pair, there is one dedicated attention head in the first encoder layer which tends to point to the least frequent tokens in every sentence. The above analysis suggest that this phenomenon is likely due to overfitting. We also analyzed the extent of change and influence splitting tokens according to their part of speech; see appendix for details. 6 What does a layer represent? Whereas in the previous section we were interested in quantifying the amount of information"
D19-1544,W13-2322,0,0.0681914,"Missing"
D19-1544,S12-1029,0,0.109017,"at most one agent. Similarly, depending on a verb class, only certain subcategorization patterns are licensed. Nevertheless, rather than modeling the interaction between argument labeling decisions, state-of-the-art semantic role labeling models (Marcheggiani and Titov, 2017; Cai et al., 2018; Li et al., 2019b) rely on powerful sentence encoders (e.g., multi-layer BiLSTMs (Zhou and Xu, 2015; Qian et al., 2017; Tan et al., 2018)). This contrasts with earlier work on SRL, which hinged on global declarative constraints on the labeling decisions (FitzGerald et al., 2015; T¨ackstr¨om et al., 2015; Das et al., 2012). Modern SRL systems are much more accurate and hence enforcing such hard and often approximate constraints is unlikely to be as beneficial (see our experiments in Section 7.2). 5415 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5415–5425, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Instead of using hard constraints, we propose to use a simple iterative structure-refinement procedure. It starts with independent predictions and refi"
D19-1544,W18-5302,0,0.0194457,"both the identification of arguments and labeling them with underlying semantic roles. The shallow semantic structures have been shown beneficial in many natural language processing (NLP) applications, including information extracCode: https://github.com/DalstonChen/CapNetSRL. document (A1) signer (A0) sign.01 document (A1) signer (A0) An injury prevented Hiti from signing the contract Figure 1: An example predicate-argument structure: green and red arcs denote correct and wrong predictions, respectively. Introduction 1 sign.01 signer (A0) tion (Christensen et al., 2011), question answering (Eckert and Neves, 2018) and machine translation (Marcheggiani et al., 2018). In this work, we focus on the dependency version of the SRL task (Surdeanu et al., 2008). An example of a dependency semantic-role structure is shown in Figure 1. Edges in the graph are marked with semantic roles, whereas predicates (typically verbs or nominalized verbs) are annotated with their senses. Intuitively, there are many restrictions on potential predicate-argument structures. Consider the ‘role uniqueness’ constraint: each role is typically, but not always, realized at most once. For example, predicates have at most one agent. Si"
D19-1544,D15-1112,0,0.212768,"Missing"
D19-1544,S15-1033,0,0.0462449,"Missing"
D19-1544,C18-1232,0,0.0289367,"2009), Spanish (Es) and German (De) are from Roth and Lapata (2016), Czech (Cz) is from Henderson et al. (2013), Chinese (Zh) is from Cai et al. (2018) and English (En) is from Li et al. (2019b). log space q˜ = sign(q) log(|q|). 8 As we expected, for both models, the majority of changes are correct, leading to better overall performance. We can again see the same trends. CapsuleNet without the global node tends to filter our arguments by changing them to “None”. The reverse is true for the full model. The capsule networks have been recently applied to a number of NLP tasks (Xiao et al., 2018; Gong et al., 2018). In particular, Yang et al. (2018) represent text classification labels by a layer of capsules, and take the capsule actions as the classification probability. Using a similar method, Xia et al. (2018) perform intent detection with the capsule networks. Wang et al. (2018) and Li et al. (2019a) use capsule networks to capture rich features for machine translation. More closely to our work, Zhang et al. (2018) and Zhang et al. (2019) adopt the capsule networks for relation extraction. The previous models apply the capsule networks to problems that have a fixed number of components in the output"
D19-1544,L18-1550,0,0.0316245,"of exact match on propositions. (18) where η is a discount coefficient. 7 Experiments 7.1 Datasets & Configuration we conduct experiments on CoNLL-2009 (Hajiˇc et al., 2009) for all languages, including Catalan (Ca), Chinese (Zh), Czech (Cz), English (En), German (De), Japanese (Jp) and Spanish (Es). As standard we do not consider predicate identification: the predicate position is provided as an input feature. We use the labeled ‘semantic’ F1 which jointly scores sense prediction and role labeling. We use ELMo (Peters et al., 2018) (dimension de as 1024) for English, and FastText embeddings (Grave et al., 2018) (dimension de as 300) for all other languages on both the baseline model and the proposed CapsuleNet. LSTM state dimension dl is 500. Capsule size K is 16. Batch size is 32. The coefficient for the regularization term λ is 0.0004. We employ Adam (Kingma and Ba, 2015) as the optimizer and the initial learning rate α is set to 0.0001. Syntactic information is not utilized in our model. 7.2 Model Selection Table 1 shows the performance of our model trained with loss L∗ (θ) for different values of discount coefficient η on the English development set. The model achieves the best performance when"
D19-1544,P18-1192,0,0.461015,"Missing"
D19-1544,J13-4006,1,0.876302,"respectively. The numbers are transformed into log space. Model Previous Best Single Model Baseline Model CapsuleNet SRL (This Work) Ja Es Ca De Cz Zh En Avg. 78.69 80.12 81.26 80.50 81.00 81.32 80.32 81.39 81.65 80.10 76.01 76.44 86.02 87.79 88.08 84.30 81.05 81.65 90.40 90.49 91.06 82.90 82.55 83.07 Table 5: Labeled F1 score (including senses) for all languages on the CoNLL-2009 in-domain test sets. For previous best result, Japanese (Ja) is from Watanabe et al. (2010), Catalan (Ca) is from Zhao et al. (2009), Spanish (Es) and German (De) are from Roth and Lapata (2016), Czech (Cz) is from Henderson et al. (2013), Chinese (Zh) is from Cai et al. (2018) and English (En) is from Li et al. (2019b). log space q˜ = sign(q) log(|q|). 8 As we expected, for both models, the majority of changes are correct, leading to better overall performance. We can again see the same trends. CapsuleNet without the global node tends to filter our arguments by changing them to “None”. The reverse is true for the full model. The capsule networks have been recently applied to a number of NLP tasks (Xiao et al., 2018; Gong et al., 2018). In particular, Yang et al. (2018) represent text classification labels by a layer of capsul"
D19-1544,D18-1149,0,0.0279567,"Missing"
D19-1544,N15-1121,0,0.258268,"Missing"
D19-1544,N19-1359,0,0.0980712,"s predicates (typically verbs or nominalized verbs) are annotated with their senses. Intuitively, there are many restrictions on potential predicate-argument structures. Consider the ‘role uniqueness’ constraint: each role is typically, but not always, realized at most once. For example, predicates have at most one agent. Similarly, depending on a verb class, only certain subcategorization patterns are licensed. Nevertheless, rather than modeling the interaction between argument labeling decisions, state-of-the-art semantic role labeling models (Marcheggiani and Titov, 2017; Cai et al., 2018; Li et al., 2019b) rely on powerful sentence encoders (e.g., multi-layer BiLSTMs (Zhou and Xu, 2015; Qian et al., 2017; Tan et al., 2018)). This contrasts with earlier work on SRL, which hinged on global declarative constraints on the labeling decisions (FitzGerald et al., 2015; T¨ackstr¨om et al., 2015; Das et al., 2012). Modern SRL systems are much more accurate and hence enforcing such hard and often approximate constraints is unlikely to be as beneficial (see our experiments in Section 7.2). 5415 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Internation"
D19-1544,N18-1202,0,0.0542808,"oefficients η of loss L∗ (θ) on the English development set. EM denotes the ratio of exact match on propositions. (18) where η is a discount coefficient. 7 Experiments 7.1 Datasets & Configuration we conduct experiments on CoNLL-2009 (Hajiˇc et al., 2009) for all languages, including Catalan (Ca), Chinese (Zh), Czech (Cz), English (En), German (De), Japanese (Jp) and Spanish (Es). As standard we do not consider predicate identification: the predicate position is provided as an input feature. We use the labeled ‘semantic’ F1 which jointly scores sense prediction and role labeling. We use ELMo (Peters et al., 2018) (dimension de as 1024) for English, and FastText embeddings (Grave et al., 2018) (dimension de as 300) for all other languages on both the baseline model and the proposed CapsuleNet. LSTM state dimension dl is 500. Capsule size K is 16. Batch size is 32. The coefficient for the regularization term λ is 0.0004. We employ Adam (Kingma and Ba, 2015) as the optimizer and the initial learning rate α is set to 0.0001. Syntactic information is not utilized in our model. 7.2 Model Selection Table 1 shows the performance of our model trained with loss L∗ (θ) for different values of discount coefficien"
D19-1544,W17-4305,0,0.0254901,"ere are many restrictions on potential predicate-argument structures. Consider the ‘role uniqueness’ constraint: each role is typically, but not always, realized at most once. For example, predicates have at most one agent. Similarly, depending on a verb class, only certain subcategorization patterns are licensed. Nevertheless, rather than modeling the interaction between argument labeling decisions, state-of-the-art semantic role labeling models (Marcheggiani and Titov, 2017; Cai et al., 2018; Li et al., 2019b) rely on powerful sentence encoders (e.g., multi-layer BiLSTMs (Zhou and Xu, 2015; Qian et al., 2017; Tan et al., 2018)). This contrasts with earlier work on SRL, which hinged on global declarative constraints on the labeling decisions (FitzGerald et al., 2015; T¨ackstr¨om et al., 2015; Das et al., 2012). Modern SRL systems are much more accurate and hence enforcing such hard and often approximate constraints is unlikely to be as beneficial (see our experiments in Section 7.2). 5415 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5415–5425, c Hong Kong, China, November 3–7,"
D19-1544,P16-1113,0,0.15932,"t and wrong role transitions have been made respectively. The numbers are transformed into log space. Model Previous Best Single Model Baseline Model CapsuleNet SRL (This Work) Ja Es Ca De Cz Zh En Avg. 78.69 80.12 81.26 80.50 81.00 81.32 80.32 81.39 81.65 80.10 76.01 76.44 86.02 87.79 88.08 84.30 81.05 81.65 90.40 90.49 91.06 82.90 82.55 83.07 Table 5: Labeled F1 score (including senses) for all languages on the CoNLL-2009 in-domain test sets. For previous best result, Japanese (Ja) is from Watanabe et al. (2010), Catalan (Ca) is from Zhao et al. (2009), Spanish (Es) and German (De) are from Roth and Lapata (2016), Czech (Cz) is from Henderson et al. (2013), Chinese (Zh) is from Cai et al. (2018) and English (En) is from Li et al. (2019b). log space q˜ = sign(q) log(|q|). 8 As we expected, for both models, the majority of changes are correct, leading to better overall performance. We can again see the same trends. CapsuleNet without the global node tends to filter our arguments by changing them to “None”. The reverse is true for the full model. The capsule networks have been recently applied to a number of NLP tasks (Xiao et al., 2018; Gong et al., 2018). In particular, Yang et al. (2018) represent tex"
D19-1544,D18-1262,0,0.27734,"Missing"
D19-1544,W08-2121,0,0.0658614,"Missing"
D19-1544,K16-1019,0,0.0689934,"Missing"
D19-1544,N18-2078,1,0.790627,"g them with underlying semantic roles. The shallow semantic structures have been shown beneficial in many natural language processing (NLP) applications, including information extracCode: https://github.com/DalstonChen/CapNetSRL. document (A1) signer (A0) sign.01 document (A1) signer (A0) An injury prevented Hiti from signing the contract Figure 1: An example predicate-argument structure: green and red arcs denote correct and wrong predictions, respectively. Introduction 1 sign.01 signer (A0) tion (Christensen et al., 2011), question answering (Eckert and Neves, 2018) and machine translation (Marcheggiani et al., 2018). In this work, we focus on the dependency version of the SRL task (Surdeanu et al., 2008). An example of a dependency semantic-role structure is shown in Figure 1. Edges in the graph are marked with semantic roles, whereas predicates (typically verbs or nominalized verbs) are annotated with their senses. Intuitively, there are many restrictions on potential predicate-argument structures. Consider the ‘role uniqueness’ constraint: each role is typically, but not always, realized at most once. For example, predicates have at most one agent. Similarly, depending on a verb class, only certain sub"
D19-1544,K17-1041,1,0.705651,"LL2019. Compared with the non-refinement baseline model, we observe substantial improvements from using the iterative procedure. The model achieves state-of-the-art performance in 5 languages, including English. 2 Base Dependency SRL Model In dependency SRL, for each predicate p of a given sentence x = {x1 , x2 , · · · , xn } with n words, the model needs to predict roles y = {y1 , y2 , · · · , yn } for every word. The role can be none, signifying that the corresponding word is not an argument of the predicate. We start with describing the factorized baseline model which is similar to that of Marcheggiani et al. (2017). It consists of three components: (1) an embedding layer; (2) an encoding layer and (3) an inference layer. 2.1 Embedding Layer The first step is to map symbolic sentence x and predicate p into continuous embedded space: ei = Lookup(xi ), (1) p = Lookup(p), (2) where ei ∈ Rde and p ∈ Rde . 5416 Semantic Role Logits Coupling Coeffients Word Capsule Layer Role Capsule Layer The probability P (y|x, p) is then computed as Y P (y|x, p) = P (yi |x, i, p), (5) Bi-LSTM Layer … 2.3 Inference Layer … where W ∈ Rdl ×de is a trainable parameter and bj|i ∈ R is a scalar representing the logit of role j fo"
D19-1544,Q15-1003,0,0.0593726,"Missing"
D19-1544,D17-1159,1,0.927706,"Missing"
D19-1544,P10-2018,0,0.112906,"mes are listed. “None” type denotes “NOT an argument”. Green and red nodes denote the numbers of correct and wrong role transitions have been made respectively. The numbers are transformed into log space. Model Previous Best Single Model Baseline Model CapsuleNet SRL (This Work) Ja Es Ca De Cz Zh En Avg. 78.69 80.12 81.26 80.50 81.00 81.32 80.32 81.39 81.65 80.10 76.01 76.44 86.02 87.79 88.08 84.30 81.05 81.65 90.40 90.49 91.06 82.90 82.55 83.07 Table 5: Labeled F1 score (including senses) for all languages on the CoNLL-2009 in-domain test sets. For previous best result, Japanese (Ja) is from Watanabe et al. (2010), Catalan (Ca) is from Zhao et al. (2009), Spanish (Es) and German (De) are from Roth and Lapata (2016), Czech (Cz) is from Henderson et al. (2013), Chinese (Zh) is from Cai et al. (2018) and English (En) is from Li et al. (2019b). log space q˜ = sign(q) log(|q|). 8 As we expected, for both models, the majority of changes are correct, leading to better overall performance. We can again see the same trends. CapsuleNet without the global node tends to filter our arguments by changing them to “None”. The reverse is true for the full model. The capsule networks have been recently applied to a numb"
D19-1544,P18-2106,0,0.0974038,"Missing"
D19-1544,D18-1348,0,0.0266022,"ce q˜ = sign(q) log(|q|). 8 As we expected, for both models, the majority of changes are correct, leading to better overall performance. We can again see the same trends. CapsuleNet without the global node tends to filter our arguments by changing them to “None”. The reverse is true for the full model. The capsule networks have been recently applied to a number of NLP tasks (Xiao et al., 2018; Gong et al., 2018). In particular, Yang et al. (2018) represent text classification labels by a layer of capsules, and take the capsule actions as the classification probability. Using a similar method, Xia et al. (2018) perform intent detection with the capsule networks. Wang et al. (2018) and Li et al. (2019a) use capsule networks to capture rich features for machine translation. More closely to our work, Zhang et al. (2018) and Zhang et al. (2019) adopt the capsule networks for relation extraction. The previous models apply the capsule networks to problems that have a fixed number of components in the output. Their approach cannot be directly applied to our setting. 7.7 Multilingual Results Table 5 gives the results of the proposed CapsuleNet SRL (with global node) on the in-domain test sets of all languag"
D19-1544,D18-1486,0,0.0152966,"from Zhao et al. (2009), Spanish (Es) and German (De) are from Roth and Lapata (2016), Czech (Cz) is from Henderson et al. (2013), Chinese (Zh) is from Cai et al. (2018) and English (En) is from Li et al. (2019b). log space q˜ = sign(q) log(|q|). 8 As we expected, for both models, the majority of changes are correct, leading to better overall performance. We can again see the same trends. CapsuleNet without the global node tends to filter our arguments by changing them to “None”. The reverse is true for the full model. The capsule networks have been recently applied to a number of NLP tasks (Xiao et al., 2018; Gong et al., 2018). In particular, Yang et al. (2018) represent text classification labels by a layer of capsules, and take the capsule actions as the classification probability. Using a similar method, Xia et al. (2018) perform intent detection with the capsule networks. Wang et al. (2018) and Li et al. (2019a) use capsule networks to capture rich features for machine translation. More closely to our work, Zhang et al. (2018) and Zhang et al. (2019) adopt the capsule networks for relation extraction. The previous models apply the capsule networks to problems that have a fixed number of comp"
D19-1544,D18-1350,0,0.014814,"are from Roth and Lapata (2016), Czech (Cz) is from Henderson et al. (2013), Chinese (Zh) is from Cai et al. (2018) and English (En) is from Li et al. (2019b). log space q˜ = sign(q) log(|q|). 8 As we expected, for both models, the majority of changes are correct, leading to better overall performance. We can again see the same trends. CapsuleNet without the global node tends to filter our arguments by changing them to “None”. The reverse is true for the full model. The capsule networks have been recently applied to a number of NLP tasks (Xiao et al., 2018; Gong et al., 2018). In particular, Yang et al. (2018) represent text classification labels by a layer of capsules, and take the capsule actions as the classification probability. Using a similar method, Xia et al. (2018) perform intent detection with the capsule networks. Wang et al. (2018) and Li et al. (2019a) use capsule networks to capture rich features for machine translation. More closely to our work, Zhang et al. (2018) and Zhang et al. (2019) adopt the capsule networks for relation extraction. The previous models apply the capsule networks to problems that have a fixed number of components in the output. Their approach cannot be directly"
D19-1544,D18-1120,0,0.0228097,"nds to filter our arguments by changing them to “None”. The reverse is true for the full model. The capsule networks have been recently applied to a number of NLP tasks (Xiao et al., 2018; Gong et al., 2018). In particular, Yang et al. (2018) represent text classification labels by a layer of capsules, and take the capsule actions as the classification probability. Using a similar method, Xia et al. (2018) perform intent detection with the capsule networks. Wang et al. (2018) and Li et al. (2019a) use capsule networks to capture rich features for machine translation. More closely to our work, Zhang et al. (2018) and Zhang et al. (2019) adopt the capsule networks for relation extraction. The previous models apply the capsule networks to problems that have a fixed number of components in the output. Their approach cannot be directly applied to our setting. 7.7 Multilingual Results Table 5 gives the results of the proposed CapsuleNet SRL (with global node) on the in-domain test sets of all languages from CoNLL-2009. As shown in Table 5, the proposed model consistently outperforms the non-refinement baseline model and achieves state-of-the-art performance on Catalan (Ca), Czech (Cz), English (En), Japane"
D19-1544,W09-1209,0,0.302992,"rgument”. Green and red nodes denote the numbers of correct and wrong role transitions have been made respectively. The numbers are transformed into log space. Model Previous Best Single Model Baseline Model CapsuleNet SRL (This Work) Ja Es Ca De Cz Zh En Avg. 78.69 80.12 81.26 80.50 81.00 81.32 80.32 81.39 81.65 80.10 76.01 76.44 86.02 87.79 88.08 84.30 81.05 81.65 90.40 90.49 91.06 82.90 82.55 83.07 Table 5: Labeled F1 score (including senses) for all languages on the CoNLL-2009 in-domain test sets. For previous best result, Japanese (Ja) is from Watanabe et al. (2010), Catalan (Ca) is from Zhao et al. (2009), Spanish (Es) and German (De) are from Roth and Lapata (2016), Czech (Cz) is from Henderson et al. (2013), Chinese (Zh) is from Cai et al. (2018) and English (En) is from Li et al. (2019b). log space q˜ = sign(q) log(|q|). 8 As we expected, for both models, the majority of changes are correct, leading to better overall performance. We can again see the same trends. CapsuleNet without the global node tends to filter our arguments by changing them to “None”. The reverse is true for the full model. The capsule networks have been recently applied to a number of NLP tasks (Xiao et al., 2018; Gong"
D19-1544,P15-1109,0,0.0179041,"es. Intuitively, there are many restrictions on potential predicate-argument structures. Consider the ‘role uniqueness’ constraint: each role is typically, but not always, realized at most once. For example, predicates have at most one agent. Similarly, depending on a verb class, only certain subcategorization patterns are licensed. Nevertheless, rather than modeling the interaction between argument labeling decisions, state-of-the-art semantic role labeling models (Marcheggiani and Titov, 2017; Cai et al., 2018; Li et al., 2019b) rely on powerful sentence encoders (e.g., multi-layer BiLSTMs (Zhou and Xu, 2015; Qian et al., 2017; Tan et al., 2018)). This contrasts with earlier work on SRL, which hinged on global declarative constraints on the labeling decisions (FitzGerald et al., 2015; T¨ackstr¨om et al., 2015; Das et al., 2012). Modern SRL systems are much more accurate and hence enforcing such hard and often approximate constraints is unlikely to be as beneficial (see our experiments in Section 7.2). 5415 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5415–5425, c Hong Kong, C"
E12-1003,P09-1004,0,0.236863,"e role induction problem is formulated as a graph partitioning problem: each vertex in the graph corresponds to a predicate occurrence and edges represent lexical and syntactic similarities between the occurrences. Unsupervised induction of semantics has also been studied in Poon and Domingos (2009) and Titov and Klementiev (2010) but the induced representations are not entirely compatible with the PropBank-style annotations and they have been evaluated only on a question answering task for the biomedical domain. Also, the related task of unsupervised argument identification was considered in Abend et al. (2009). 9 Conclusions In this work we introduced two Bayesian models for unsupervised role induction. They treat the task as a family of related clustering problems, one for each predicate. The first factored model induces each clustering independently, whereas the second model couples them by exploiting a novel technique for sharing clustering preferences across a family of clusterings. Both methods achieve state-of-the-art results with the coupled model outperforming the factored counterpart in all regimes. Acknowledgements The authors acknowledge the support of the MMCI Cluster of Excellence, and"
E12-1003,J92-4003,0,0.0768219,"Bank annotations, it has dependency structures generated automatically by the MaltParser (Nivre et al., 2007). We vary our experimental setup as follows: • We evaluate our models on gold and automatically generated parses, and use either gold PropBank annotations or the heuristic from Section 2 to identify arguments, resulting in four experimental regimes. • In order to reduce the sparsity of predicate argument fillers we consider replacing lemmas of their syntactic heads with word clusters induced by a clustering algorithm as a preprocessing step. In particular, we use Brown (Br) clustering (Brown et al., 1992) induced over RCV1 corpus (Turian et al., 2010). Although the clustering is hierarchical, we only use a cluster at the lowest level of the hierarchy for each word. We use the purity (PU) and collocation (CO) metrics as well as their harmonic mean (F1) to measure the quality of the resulting clusters. Purity measures the degree to which each cluster contains arguments sharing the same gold role: 1 X PU = max |Gj ∩ Ci | j N i where if Ci is the set of arguments in the i-th induced cluster, Gj is the set of arguments in the jth 10 The coupled model without discounting still outperforms the factor"
E12-1003,W05-0620,0,0.241621,"Missing"
E12-1003,D09-1003,0,0.187069,"antic roles for adverbials do not normally depend on whether the construction is passive or active). 8 Related Work Most of SRL research has focused on the supervised setting (Carreras and M`arquez, 2005; Surdeanu et al., 2008), however, lack of annotated resources for most languages and insufficient coverage provided by the existing resources motivates the need for using unlabeled data or other forms of weak supervision. This work includes methods based on graph alignment between labeled and unlabeled data (F¨urstenau and Lapata, 2009), using unlabeled data to improve lexical generalization (Deschacht and Moens, 2009), and projection of annotation across languages (Pado and Lapata, 2009; van der Plas et al., 2011). Semi-supervised and weakly-supervised techniques have also been explored for other types of semantic representations but these studies have mostly focused on restricted domains (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Goldwasser et al., 2011; Liang et al., 2011). Unsupervised learning has been one of the central paradigms for the closely-related area of relation extraction, where several techniques have been proposed to cluster semantically similar verbalizations"
E12-1003,D09-1002,0,0.355054,"Missing"
E12-1003,P11-2051,0,0.0722852,"the following sentences: (a) [A0 Mary] opened [A1 the door]. (b) [A0 Mary] is expected to open [A1 the door]. (c) [A1 The door] opened. (d) [A1 The door] was opened [A0 by Mary]. Mary always takes an agent role (A0) for the predicate open, and door is always a patient (A1). SRL representations have many potential applications in natural language processing and have recently been shown to be beneficial in question answering (Shen and Lapata, 2007; Kaisser and Webber, 2007), textual entailment (Sammons et al., 2009), machine translation (Wu and Fung, 2009; Liu and Gildea, 2010; Wu et al., 2011; Gao and Vogel, 2011), and dialogue systems (Basili et al., 2009; van der Plas et al., 2011), among others. Though syntactic representations are often predictive of semantic roles (Levin, 1993), the interface between syntactic and semantic representations is far from trivial. The lack of simple deterministic rules for mapping syntax to shallow semantics motivates the use of statistical methods. Although current statistical approaches have been successful in predicting shallow semantic representations, they typically require large amounts of annotated data to estimate model parameters. These resources are scarce an"
E12-1003,P11-1149,0,0.0238149,"nlabeled data or other forms of weak supervision. This work includes methods based on graph alignment between labeled and unlabeled data (F¨urstenau and Lapata, 2009), using unlabeled data to improve lexical generalization (Deschacht and Moens, 2009), and projection of annotation across languages (Pado and Lapata, 2009; van der Plas et al., 2011). Semi-supervised and weakly-supervised techniques have also been explored for other types of semantic representations but these studies have mostly focused on restricted domains (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Goldwasser et al., 2011; Liang et al., 2011). Unsupervised learning has been one of the central paradigms for the closely-related area of relation extraction, where several techniques have been proposed to cluster semantically similar verbalizations of relations (Lin and Pantel, 2001; Banko et al., 2007). Early unsupervised approaches to the SRL problem include the work by Swier and Stevenson (2004), where the VerbNet verb lexicon was used to guide unsupervised learning, and a generative model of Grenager and Manning (2006) which exploits linguistic priors on syntactic-semantic interface. More recently, the role ind"
E12-1003,W06-1601,0,0.767612,"of the European Chapter of the Association for Computational Linguistics, pages 12–22, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics cient hierarchical Bayesian models for this task. It is natural to split the SRL task into two stages: the identification of arguments (the identification stage) and the assignment of semantic roles (the labeling stage). In this and in much of the previous work on unsupervised techniques, the focus is on the labeling stage. Identification, though an important problem, can be tackled with heuristics (Lang and Lapata, 2011a; Grenager and Manning, 2006) or, potentially, by using a supervised classifier trained on a small amount of data. We follow (Lang and Lapata, 2011a), and regard the labeling stage as clustering of syntactic signatures of argument realizations for every predicate. In our first model, as in most of the previous work on unsupervised SRL, we define an independent model for each predicate. We use the Chinese Restaurant Process (CRP) (Ferguson, 1973) as a prior for the clustering of syntactic signatures. The resulting model achieves state-of-the-art results, substantially outperforming previous methods evaluated in the same se"
E12-1003,W07-1206,0,0.146166,"gument structure, i.e. both identification of arguments as well as assignment of labels according to their underlying semantic role. For example, in the following sentences: (a) [A0 Mary] opened [A1 the door]. (b) [A0 Mary] is expected to open [A1 the door]. (c) [A1 The door] opened. (d) [A1 The door] was opened [A0 by Mary]. Mary always takes an agent role (A0) for the predicate open, and door is always a patient (A1). SRL representations have many potential applications in natural language processing and have recently been shown to be beneficial in question answering (Shen and Lapata, 2007; Kaisser and Webber, 2007), textual entailment (Sammons et al., 2009), machine translation (Wu and Fung, 2009; Liu and Gildea, 2010; Wu et al., 2011; Gao and Vogel, 2011), and dialogue systems (Basili et al., 2009; van der Plas et al., 2011), among others. Though syntactic representations are often predictive of semantic roles (Levin, 1993), the interface between syntactic and semantic representations is far from trivial. The lack of simple deterministic rules for mapping syntax to shallow semantics motivates the use of statistical methods. Although current statistical approaches have been successful in predicting shal"
E12-1003,N10-1137,0,0.835683,"didate argument to iteratively discard non-arguments from the list of all words in a sentence. Note that inducing these rules for a new language would require some linguistic expertise. One alternative may be to annotate a small number of arguments and train a classifier with nonlexicalized features instead. In the argument labeling stage, semantic roles are represented by clusters of arguments, and labeling a particular argument corresponds to deciding on its role cluster. However, instead of deal1 Although it provides a strong baseline which is difficult to beat (Grenager and Manning, 2006; Lang and Lapata, 2010; Lang and Lapata, 2011a). 13 ing with argument occurrences directly, we represent them as predicate specific syntactic signatures, and refer to them as argument keys. This representation aids our models in inducing high purity clusters (of argument keys) while reducing their granularity. We follow (Lang and Lapata, 2011a) and use the following syntactic features to form the argument key representation: • Active or passive verb voice (ACT/PASS). • Argument position relative to predicate (LEFT/RIGHT). • Syntactic relation to its governor. • Preposition used for argument realization. In the exam"
E12-1003,P11-1112,0,0.363518,"of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 12–22, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics cient hierarchical Bayesian models for this task. It is natural to split the SRL task into two stages: the identification of arguments (the identification stage) and the assignment of semantic roles (the labeling stage). In this and in much of the previous work on unsupervised techniques, the focus is on the labeling stage. Identification, though an important problem, can be tackled with heuristics (Lang and Lapata, 2011a; Grenager and Manning, 2006) or, potentially, by using a supervised classifier trained on a small amount of data. We follow (Lang and Lapata, 2011a), and regard the labeling stage as clustering of syntactic signatures of argument realizations for every predicate. In our first model, as in most of the previous work on unsupervised SRL, we define an independent model for each predicate. We use the Chinese Restaurant Process (CRP) (Ferguson, 1973) as a prior for the clustering of syntactic signatures. The resulting model achieves state-of-the-art results, substantially outperforming previous me"
E12-1003,D11-1122,0,0.355122,"of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 12–22, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics cient hierarchical Bayesian models for this task. It is natural to split the SRL task into two stages: the identification of arguments (the identification stage) and the assignment of semantic roles (the labeling stage). In this and in much of the previous work on unsupervised techniques, the focus is on the labeling stage. Identification, though an important problem, can be tackled with heuristics (Lang and Lapata, 2011a; Grenager and Manning, 2006) or, potentially, by using a supervised classifier trained on a small amount of data. We follow (Lang and Lapata, 2011a), and regard the labeling stage as clustering of syntactic signatures of argument realizations for every predicate. In our first model, as in most of the previous work on unsupervised SRL, we define an independent model for each predicate. We use the Chinese Restaurant Process (CRP) (Ferguson, 1973) as a prior for the clustering of syntactic signatures. The resulting model achieves state-of-the-art results, substantially outperforming previous me"
E12-1003,P09-1011,0,0.0244737,"existing resources motivates the need for using unlabeled data or other forms of weak supervision. This work includes methods based on graph alignment between labeled and unlabeled data (F¨urstenau and Lapata, 2009), using unlabeled data to improve lexical generalization (Deschacht and Moens, 2009), and projection of annotation across languages (Pado and Lapata, 2009; van der Plas et al., 2011). Semi-supervised and weakly-supervised techniques have also been explored for other types of semantic representations but these studies have mostly focused on restricted domains (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Goldwasser et al., 2011; Liang et al., 2011). Unsupervised learning has been one of the central paradigms for the closely-related area of relation extraction, where several techniques have been proposed to cluster semantically similar verbalizations of relations (Lin and Pantel, 2001; Banko et al., 2007). Early unsupervised approaches to the SRL problem include the work by Swier and Stevenson (2004), where the VerbNet verb lexicon was used to guide unsupervised learning, and a generative model of Grenager and Manning (2006) which exploits linguistic priors on syn"
E12-1003,P11-1060,0,0.0128193,"rms of weak supervision. This work includes methods based on graph alignment between labeled and unlabeled data (F¨urstenau and Lapata, 2009), using unlabeled data to improve lexical generalization (Deschacht and Moens, 2009), and projection of annotation across languages (Pado and Lapata, 2009; van der Plas et al., 2011). Semi-supervised and weakly-supervised techniques have also been explored for other types of semantic representations but these studies have mostly focused on restricted domains (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Goldwasser et al., 2011; Liang et al., 2011). Unsupervised learning has been one of the central paradigms for the closely-related area of relation extraction, where several techniques have been proposed to cluster semantically similar verbalizations of relations (Lin and Pantel, 2001; Banko et al., 2007). Early unsupervised approaches to the SRL problem include the work by Swier and Stevenson (2004), where the VerbNet verb lexicon was used to guide unsupervised learning, and a generative model of Grenager and Manning (2006) which exploits linguistic priors on syntactic-semantic interface. More recently, the role induction problem has be"
E12-1003,C10-1081,0,0.0846742,"erlying semantic role. For example, in the following sentences: (a) [A0 Mary] opened [A1 the door]. (b) [A0 Mary] is expected to open [A1 the door]. (c) [A1 The door] opened. (d) [A1 The door] was opened [A0 by Mary]. Mary always takes an agent role (A0) for the predicate open, and door is always a patient (A1). SRL representations have many potential applications in natural language processing and have recently been shown to be beneficial in question answering (Shen and Lapata, 2007; Kaisser and Webber, 2007), textual entailment (Sammons et al., 2009), machine translation (Wu and Fung, 2009; Liu and Gildea, 2010; Wu et al., 2011; Gao and Vogel, 2011), and dialogue systems (Basili et al., 2009; van der Plas et al., 2011), among others. Though syntactic representations are often predictive of semantic roles (Levin, 1993), the interface between syntactic and semantic representations is far from trivial. The lack of simple deterministic rules for mapping syntax to shallow semantics motivates the use of statistical methods. Although current statistical approaches have been successful in predicting shallow semantic representations, they typically require large amounts of annotated data to estimate model pa"
E12-1003,C10-2107,0,0.0395726,", among others. Though syntactic representations are often predictive of semantic roles (Levin, 1993), the interface between syntactic and semantic representations is far from trivial. The lack of simple deterministic rules for mapping syntax to shallow semantics motivates the use of statistical methods. Although current statistical approaches have been successful in predicting shallow semantic representations, they typically require large amounts of annotated data to estimate model parameters. These resources are scarce and expensive to create, and even the largest of them have low coverage (Palmer and Sporleder, 2010). Moreover, these models are domain-specific, and their performance drops substantially when they are used in a new domain (Pradhan et al., 2008). Such domain specificity is arguably unavoidable for a semantic analyzer, as even the definitions of semantic roles are typically predicate specific, and different domains can have radically different distributions of predicates (and their senses). The necessity for a large amounts of human-annotated data for every language and domain is one of the major obstacles to the wide-spread adoption of semantic role representations. These challenges motivate"
E12-1003,J05-1004,0,0.22551,"ling task and discuss some specifics of the unsupervised setting. In Section 3, we describe CRPs and dd-CRPs, the key components of our models. In Sections 4 – 6, we describe our factored and coupled models and the inference method. Section 7 provides both evaluation and analysis. Finally, additional related work is presented in Section 8. 2 Task Definition In this work, instead of assuming the availability of role annotated data, we rely only on automatically generated syntactic dependency graphs. While we cannot expect that syntactic structure can trivially map to a semantic representation (Palmer et al., 2005)1 , we can use syntactic cues to help us in both stages of unsupervised SRL. Before defining our task, let us consider the two stages separately. In the argument identification stage, we implement a heuristic proposed in (Lang and Lapata, 2011a) comprised of a list of 8 rules, which use nonlexicalized properties of syntactic paths between a predicate and a candidate argument to iteratively discard non-arguments from the list of all words in a sentence. Note that inducing these rules for a new language would require some linguistic expertise. One alternative may be to annotate a small number of"
E12-1003,D09-1001,0,0.0821299,"here it has been reformulated as a problem of detecting alterations and mapping non-standard linkings to the canonical ones. Later, Lang and Lapata (2011a) proposed an algorithmic approach to clustering argument signatures which achieves higher accuracy and outperforms the syntactic baseline. In Lang and Lapata (2011b), the role induction problem is formulated as a graph partitioning problem: each vertex in the graph corresponds to a predicate occurrence and edges represent lexical and syntactic similarities between the occurrences. Unsupervised induction of semantics has also been studied in Poon and Domingos (2009) and Titov and Klementiev (2010) but the induced representations are not entirely compatible with the PropBank-style annotations and they have been evaluated only on a question answering task for the biomedical domain. Also, the related task of unsupervised argument identification was considered in Abend et al. (2009). 9 Conclusions In this work we introduced two Bayesian models for unsupervised role induction. They treat the task as a family of related clustering problems, one for each predicate. The first factored model induces each clustering independently, whereas the second model couples"
E12-1003,J08-2006,0,0.502038,"resentations is far from trivial. The lack of simple deterministic rules for mapping syntax to shallow semantics motivates the use of statistical methods. Although current statistical approaches have been successful in predicting shallow semantic representations, they typically require large amounts of annotated data to estimate model parameters. These resources are scarce and expensive to create, and even the largest of them have low coverage (Palmer and Sporleder, 2010). Moreover, these models are domain-specific, and their performance drops substantially when they are used in a new domain (Pradhan et al., 2008). Such domain specificity is arguably unavoidable for a semantic analyzer, as even the definitions of semantic roles are typically predicate specific, and different domains can have radically different distributions of predicates (and their senses). The necessity for a large amounts of human-annotated data for every language and domain is one of the major obstacles to the wide-spread adoption of semantic role representations. These challenges motivate the need for unsupervised methods which, instead of relying on labeled data, can exploit large amounts of unlabeled texts. In this paper, we pro"
E12-1003,D07-1002,0,0.20067,"diction of predicate argument structure, i.e. both identification of arguments as well as assignment of labels according to their underlying semantic role. For example, in the following sentences: (a) [A0 Mary] opened [A1 the door]. (b) [A0 Mary] is expected to open [A1 the door]. (c) [A1 The door] opened. (d) [A1 The door] was opened [A0 by Mary]. Mary always takes an agent role (A0) for the predicate open, and door is always a patient (A1). SRL representations have many potential applications in natural language processing and have recently been shown to be beneficial in question answering (Shen and Lapata, 2007; Kaisser and Webber, 2007), textual entailment (Sammons et al., 2009), machine translation (Wu and Fung, 2009; Liu and Gildea, 2010; Wu et al., 2011; Gao and Vogel, 2011), and dialogue systems (Basili et al., 2009; van der Plas et al., 2011), among others. Though syntactic representations are often predictive of semantic roles (Levin, 1993), the interface between syntactic and semantic representations is far from trivial. The lack of simple deterministic rules for mapping syntax to shallow semantics motivates the use of statistical methods. Although current statistical approaches have been su"
E12-1003,W08-2121,0,0.262047,"Missing"
E12-1003,P11-1145,1,0.64672,"ne role r is drawn from the uniform Bernoulli distribution. If 0 is drawn then the semantic role is not realized for the given occurrence, otherwise the number of additional roles r is drawn from the geometric distribution Geom(ψp,r ). The Beta priors over ψ 4 For prepositional phrases, we take as head the head noun of the object noun phrase as it encodes crucial lexical information. However, the preposition is not ignored but rather encoded in the corresponding argument key, as explained in Section 2. 5 Alternatively, the clustering of arguments could be induced within the model, as done in (Titov and Klementiev, 2011). 15 Clustering of argument keys: Factored model: for each predicate p = 1, 2, . . . : Bp ∼ CRP (α) Coupled model: D ∼ N onInf orm for each predicate p = 1, 2, . . . : Bp ∼ dd-CRP (α, D) [partition of arg keys] [similarity graph] [partition of arg keys] Parameters: for each predicate p = 1, 2, . . . : for each role r ∈ Bp : θp,r ∼ DP (β, H (A) ) [distrib of arg fillers] ψp,r ∼ Beta(η0 , η1 ) [geom distr for dup roles] Data Generation: for each predicate p = 1, 2, . . . : for each occurrence l of p: for every role r ∈ Bp : if [n ∼ U nif (0, 1)] = 1: [role appears at least once] GenArgument(p, r"
E12-1003,P10-1098,1,0.856098,"otivates the need for using unlabeled data or other forms of weak supervision. This work includes methods based on graph alignment between labeled and unlabeled data (F¨urstenau and Lapata, 2009), using unlabeled data to improve lexical generalization (Deschacht and Moens, 2009), and projection of annotation across languages (Pado and Lapata, 2009; van der Plas et al., 2011). Semi-supervised and weakly-supervised techniques have also been explored for other types of semantic representations but these studies have mostly focused on restricted domains (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Goldwasser et al., 2011; Liang et al., 2011). Unsupervised learning has been one of the central paradigms for the closely-related area of relation extraction, where several techniques have been proposed to cluster semantically similar verbalizations of relations (Lin and Pantel, 2001; Banko et al., 2007). Early unsupervised approaches to the SRL problem include the work by Swier and Stevenson (2004), where the VerbNet verb lexicon was used to guide unsupervised learning, and a generative model of Grenager and Manning (2006) which exploits linguistic priors on syntactic-semantic interface. Mo"
E12-1003,P10-1040,0,0.0195094,"generated automatically by the MaltParser (Nivre et al., 2007). We vary our experimental setup as follows: • We evaluate our models on gold and automatically generated parses, and use either gold PropBank annotations or the heuristic from Section 2 to identify arguments, resulting in four experimental regimes. • In order to reduce the sparsity of predicate argument fillers we consider replacing lemmas of their syntactic heads with word clusters induced by a clustering algorithm as a preprocessing step. In particular, we use Brown (Br) clustering (Brown et al., 1992) induced over RCV1 corpus (Turian et al., 2010). Although the clustering is hierarchical, we only use a cluster at the lowest level of the hierarchy for each word. We use the purity (PU) and collocation (CO) metrics as well as their harmonic mean (F1) to measure the quality of the resulting clusters. Purity measures the degree to which each cluster contains arguments sharing the same gold role: 1 X PU = max |Gj ∩ Ci | j N i where if Ci is the set of arguments in the i-th induced cluster, Gj is the set of arguments in the jth 10 The coupled model without discounting still outperforms the factored counterpart in our experiments. 18 gold clus"
E12-1003,P11-2052,0,0.0857279,"Missing"
E12-1003,N09-2004,0,0.0807512,"ording to their underlying semantic role. For example, in the following sentences: (a) [A0 Mary] opened [A1 the door]. (b) [A0 Mary] is expected to open [A1 the door]. (c) [A1 The door] opened. (d) [A1 The door] was opened [A0 by Mary]. Mary always takes an agent role (A0) for the predicate open, and door is always a patient (A1). SRL representations have many potential applications in natural language processing and have recently been shown to be beneficial in question answering (Shen and Lapata, 2007; Kaisser and Webber, 2007), textual entailment (Sammons et al., 2009), machine translation (Wu and Fung, 2009; Liu and Gildea, 2010; Wu et al., 2011; Gao and Vogel, 2011), and dialogue systems (Basili et al., 2009; van der Plas et al., 2011), among others. Though syntactic representations are often predictive of semantic roles (Levin, 1993), the interface between syntactic and semantic representations is far from trivial. The lack of simple deterministic rules for mapping syntax to shallow semantics motivates the use of statistical methods. Although current statistical approaches have been successful in predicting shallow semantic representations, they typically require large amounts of annotated dat"
E12-1003,W04-3213,0,\N,Missing
E12-1003,N07-1070,0,\N,Missing
E12-1003,W09-1201,0,\N,Missing
E12-1003,J02-3001,0,\N,Missing
E12-1003,D07-1096,0,\N,Missing
E14-1006,P98-1013,0,0.23749,"Missing"
E14-1006,W12-1901,1,0.846802,"rior knowledge as defined above enables the application of our model to small data sets, we emphasize that the model is generally widely applicable for two reasons. First, the data, collected using crowdsourcing, is comparatively easy and cheap to extend. Secondly, our model is domain independent and can be applied to scenario descriptions from any domain without any modification. Note that parameters were tuned on held-out scenarios, and no scenario-specific tuning was performed. 2 A related task, unsupervised frame induction, has also been considered in the past (Titov and Klementiev, 2011; Modi et al., 2012; O’Connor, 2012); the frame representations encode events and participants but ignore the temporal aspect of script knowledge. We model temporal constraints on event type orderings with the Generalized Mallows Model (GMM; Mallows (1957); Fligner and Verducci (1986); Klementiev et al. (2008)), a statistical model over permutations. The GMM is a flexible model which can specify item-specific sensitivity to perturbation from the item’s position in the canonical permutation. With the GMM we are thus able to model event type-specific temporal flexibility – a feature of scripts that MSA cannot capt"
E14-1006,P08-1090,0,0.840503,"Work In the 1970s, scripts were introduced as a way to equip AI systems with world knowledge (Schank and Abelson, 1975; Barr and Feigenbaum, 1986). Task-specific script databases were developed manually. FrameNet (Baker et al., 1998) follows a similar idea, in defining verb frames together with argument types that can fill the verbs’ argument slots. Frames can then be combined into “scenario frames”. Manual composition of such databases, is arguably expensive and does not scale well. This paper follows a series of more recent work which aims to infer script knowledge automatically from data. Chambers and Jurafsky (2008) The GMM has been successfully applied to modeling ordering constraints in NLP tasks. Chen et al. (2009) augment classical topic models with a GMM, under the assumption that topics in structured domains (e.g., biographies in Wikipedia) tend to follow an underlying canonical ordering, an assumption which matches well our data (the annotators were asked to follow the temporal or50 tion in an ESD with one event type e. We specify the number of possible event types E a priori as a number exceeding the number of event types in all the scripts considered. The model will select an effective subset of"
E14-1006,P09-1068,0,0.567259,"v models. We are not aware of previous work on modeling events with GMMs. Conversely, MMs were considered in the very recent work of Cheung et al. (2013) in the context of script induction from news corpora where the Markovian assumption is much more natural. There exists a body of work for learning participant types involved in scripts. Regneri et al. (2011) extend their work by inducing participant types on the basis of the TSG, using structural information about participant mentions in the TSG as well as WordNet similarity, which they then combine into an Integer Linear Program. Similarly, Chambers and Jurafsky (2009) extend their work on narrative chains, presenting a system with which they jointly learn event types and semantic roles of the participants involved, but do not consider event orderings. We include participant types as a latent feature in our model, assuming that participant mentions in an event description are a predictive feature for the corresponding event type. One way of alleviating the problem of small data sets is incorporating informed prior knowledge. Raina et al. (2006) encode word correlations in a variance-covariance matrix of a multivariate normal distribution (MVN), and sample p"
E14-1006,P10-1100,1,0.849007,"iversity of Edinburgh, United Kingdom 2 ILLC, University of Amsterdam, Netherlands 3 Department of Computational Linguistics, Saarland University, Germany Abstract natural text (cf. (Chambers and Jurafsky, 2008)), as not all types of scripts are elaborated in natural text – being left implicit because of assumed readers’ world knowledge. Our model, operating on data obtained in a cheap way by crowdsourcing, is applicable to any kind of script and can fill this gap. We follow work in inducing script knowledge from explicit instantiations of scripts, socalled event sequence descriptions (ESDs) (Regneri et al., 2010). Our data consists of sets of ESDs, each set describing a well-known situation we will call scenario (e.g., “washing laundry”). An ESD consists of a sequence of events, each describing an action defining part of the scenario (e.g., “place the laundry in the washing machine”). We refer to descriptions of the same event across ESDs as event types. We refer to entities involved in a scenario as participants (e.g., a “washing machine” or a “detergent”), and to sets of participant descriptions describing the same entity as participant types. For each type of scenario, our model clusters descriptio"
E14-1006,R11-1064,1,0.951101,"umber of possible event types E a priori as a number exceeding the number of event types in all the scripts considered. The model will select an effective subset of those types. Assume a scenario-specific corpus c, consisting of D ESDs, c = {d1 , ..., dD }. Each ESD di consists of Nd event descriptions di = {di,1 , ..., di,Ni }. Boundaries between descriptions of single events are marked in the data. For each event description di,n a bag of participant descriptions is extracted. Each participant description corresponds to one noun phrase as identified automatically by a dependency parser (cf. Regneri et al. (2011)). We also associate participant types with participant descriptions, these types are latent and induced at the inference stage. Given such a corpus of ESDs, our model assigns each event description di,n in an ESD di one event type zdi,n = e, where e ∈ {1, ..., E}. Assuming that all ESDs are generated from the same underlying set of event types, our objective is to assign the same event type to equivalent event descriptions across all ESDs in the corpus. We furthermore assume that there exists a canonical temporal ordering of event types for each scenario type, and that events in observed scen"
E14-1006,P11-1145,1,0.710389,"t. While we will show that prior knowledge as defined above enables the application of our model to small data sets, we emphasize that the model is generally widely applicable for two reasons. First, the data, collected using crowdsourcing, is comparatively easy and cheap to extend. Secondly, our model is domain independent and can be applied to scenario descriptions from any domain without any modification. Note that parameters were tuned on held-out scenarios, and no scenario-specific tuning was performed. 2 A related task, unsupervised frame induction, has also been considered in the past (Titov and Klementiev, 2011; Modi et al., 2012; O’Connor, 2012); the frame representations encode events and participants but ignore the temporal aspect of script knowledge. We model temporal constraints on event type orderings with the Generalized Mallows Model (GMM; Mallows (1957); Fligner and Verducci (1986); Klementiev et al. (2008)), a statistical model over permutations. The GMM is a flexible model which can specify item-specific sensitivity to perturbation from the item’s position in the canonical permutation. With the GMM we are thus able to model event type-specific temporal flexibility – a feature of scripts t"
E14-1006,C98-1013,0,\N,Missing
E14-1006,J82-1004,0,\N,Missing
J13-4006,W06-2922,0,0.0164481,"anar graphs belong to two conceptual groups: those that manipulate the graph, either by pre-processing or by post-processing (Hall and Novak 2005; McDonald and Pereira 2006), and those that adapt the algorithm to deal with non-planarity. Among the approaches that, like ours, devise an algorithm to deal with non-planarity, Yngve (1960) proposed a limited manipulation of registers to handle discontinuous constituents, which guaranteed that parsing/generation could be performed with a stack of very limited depth. An approach to non-planar parsing that is more similar to ours has been proposed in Attardi (2006). Attardi’s dependency parsing algorithm adds six new actions that allow this algorithm to parse any type of non-planar tree. Our Swap action is related to Attardi’s actions Left2 and Right2, which create dependency arcs between the second element on the stack and the front of the input queue. In the Attardi algorithm, every attachment to an element below the top of the stack requires the use of one of the new actions, whose frequency is much lower than the normal attachment actions, and therefore harder to learn. This contrasts with the Swap action, which handles reordering with a single acti"
J13-4006,P98-1013,0,0.0854963,"Missing"
J13-4006,P93-1005,0,0.21067,"e use latent variables to model the interaction between syntax and semantics. Latent variables serve as an interface between semantics and syntax, capturing properties of both structures relevant to the prediction of semantics given syntax and, conversely, syntax given semantics. Unlike hand-crafted features, latent variables are induced automatically from data, thereby avoiding a priori hard independence assumptions. Instead, the structure of the latent variable model is used to encode soft biases towards learning the types of features we expect to be useful. We define a history-based model (Black et al. 1993) for joint parsing of semantic and syntactic structures. History-based models map structured representations to sequences of derivation steps, and model the probability of each step conditioned on the entire sequence of previous steps. There are standard shift-reduce algorithms (Nivre, Hall, and Nilsson 2004) for mapping a syntactic dependency graph to a derivation sequence, and similar algorithms can be defined for mapping a semantic dependency graph to a derivation sequence, as discussed subsequently. But defining a joint syntactic–semantic derivation presents a challenge. Namely, given the"
J13-4006,D12-1133,0,0.0438463,"2013 Association for Computational Linguistics Computational Linguistics Volume 39, Number 4 nature of the approach, this extension of the ISBN architecture to weakly synchronized syntacticsemantic derivations is also an exemplification of its applicability to other problems where two independent, but related, representations are being learned. 1. Introduction Success in statistical syntactic parsing based on supervised techniques trained on a large corpus of syntactic trees—both constituency-based (Collins 1999; Charniak 2000; Henderson 2003) and dependency-based (McDonald 2006; Nivre 2006; Bohnet and Nivre 2012; Hatori et al. 2012)—has paved the way to applying statistical approaches to the more ambitious goals of recovering semantic representations, such as the logical form of a sentence (Ge and Mooney 2005; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Ge and Mooney 2009; Kwiatkowski et al. 2011) or learning the propositional argument-structure of its main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of representation of language and text has many potential applications in q"
J13-4006,burchardt-etal-2006-salsa,0,0.097902,"participants, or representations of objects involving their properties. The participants and properties in a frame are designated with a set of semantic roles called frame elements. One example is the MOTION DIRECTIONAL frame, and its associated frame elements include the THEME (the moving object), the GOAL (the ultimate destination), the SOURCE, and the PATH. The collection of sentences used to exemplify frames in the English FrameNet has been sampled to produce informative lexicographic examples, but no attempt has been made to produce representative distributions. The German SALSA corpus (Burchardt et al. 2006), however, has been annotated with FrameNet annotation. This extension to exhaustive corpus coverage and a new language has only required a few novel frames, demonstrating the cross-linguistic validity of this annotation scheme. FrameNets for other languages, Spanish and Japanese, are also under construction. Another semantically annotated corpus—the one we use in this work for experiments on English—is called Proposition Bank (PropBank) (Palmer, Gildea, and Kingsbury 2005). PropBank is based on the assumption that the lexicon is not a list of irregularities, but that systematic correlations c"
J13-4006,W05-0620,0,0.103932,"Missing"
J13-4006,A00-2018,0,0.0291314,"mber 2012; accepted for publication: 1 November 2012. doi:10.1162/COLI a 00158 © 2013 Association for Computational Linguistics Computational Linguistics Volume 39, Number 4 nature of the approach, this extension of the ISBN architecture to weakly synchronized syntacticsemantic derivations is also an exemplification of its applicability to other problems where two independent, but related, representations are being learned. 1. Introduction Success in statistical syntactic parsing based on supervised techniques trained on a large corpus of syntactic trees—both constituency-based (Collins 1999; Charniak 2000; Henderson 2003) and dependency-based (McDonald 2006; Nivre 2006; Bohnet and Nivre 2012; Hatori et al. 2012)—has paved the way to applying statistical approaches to the more ambitious goals of recovering semantic representations, such as the logical form of a sentence (Ge and Mooney 2005; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Ge and Mooney 2009; Kwiatkowski et al. 2011) or learning the propositional argument-structure of its main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a se"
J13-4006,W08-2134,0,0.0835302,"tions separately, with a pipeline of state-of-the-art systems, and then reranks the joint representation in a Table 7 Scores of the fully connected model on the final testing sets of the CoNLL-2008 shared task (percentages). WSJ Brown WSJ+Brown Syntactic LAS P 88.4 80.4 87.5 79.9 65.9 78.4 Semantic R F1 75.5 60.8 73.9 77.6 63.3 76.1 P Macro R F1 84.2 73.1 83.0 82.0 70.6 80.7 83.0 71.8 81.8 981 Computational Linguistics Volume 39, Number 4 Table 8 Comparison with other models on the CoNLL-2008 test set (percentages). C O NLL M EASURES M ODEL Johansson and Nugues (2008b) Ciaramita et al. (2008) Che et al. (2008) Zhao and Kit (2008) This article Henderson et al. (2008) Llu´ıs and M`arquez (2008) C ROSSING A RCS Synt LAS Semantic F1 Macro F1 P 89.3 87.4 86.7 87.7 87.5 87.6 85.8 81.6 78.0 78.5 76.7 76.1 73.1 70.3 85.5 82.7 82.7 82.2 81.8 80.5 78.1 67.0 59.9 56.9 58.5 62.1 72.6 53.8 Semantics R F1 44.5 34.2 32.4 36.1 29.4 1.7 19.2 53.5 43.5 41.3 44.6 39.9 3.3 28.3 final step (Johansson and Nugues 2008b). Similarly, Che et al. (2008) also implement a pipeline consisting of state-of-the-art components where the final inference stage is performed using Integer Linear Programming to ensure global coherence o"
J13-4006,W08-2139,0,0.0541097,"Missing"
J13-4006,P05-1033,0,0.175384,"forms a linguistically meaningful chunk in that it includes all the decisions about the arcs on the left side of the associated word, both its parents and its children. Thus, synchronizing the syntactic and semantic subsequences according to their associated word places together subsequences that are likely to be correlated. Note that such pairs of syntactic and semantic subsequences will, in general, have different numbers of steps on each side and these numbers of steps are, in general, unbounded. Therefore, instead of defining atomic synchronized rules as in synchronous grammars (Wu 1997; Chiang 2005), we resort to parametrized models that exploit the internal structure of the paired subsequences. This derivational, joint approach to handling these complex representations leads to a new proposal on how to learn them, which avoids extensive and complex feature engineering, as discussed in the following. 1.2 Joint Learning of Syntax and Semantics Our probabilistic model is learned using Incremental Sigmoid Belief Networks (ISBNs) (Henderson and Titov 2010), a recent development of an early latent variable model 952 Henderson et al. Joint Syntactic and Semantic Parsing for syntactic structure"
J13-4006,W10-1811,0,0.0444172,"Missing"
J13-4006,W08-2138,0,0.114405,"Missing"
J13-4006,cmejrek-etal-2004-prague,0,0.0485572,"Missing"
J13-4006,J81-4005,0,0.781848,"Missing"
J13-4006,P07-1071,0,0.0242847,"Missing"
J13-4006,N04-1035,0,0.0110215,"trivial to develop systems that actually succeed in exploiting this intuitively obvious 988 Henderson et al. Joint Syntactic and Semantic Parsing correlation. Li, Zhou, and Ng’s approach is also different from ours in that they do not attempt to induce common representations useful for both tasks or for many languages, and as such cannot be regarded as multi-task, nor as multilingual, learning. Synchronous grammars provide an elegant way to handle multiple levels of representation. They have received much attention because of their applications in syntaxbased statistical machine translation (Galley et al. 2004; Chiang 2005; Nesson and Shieber 2008) and semantic parsing (Wong and Mooney 2006, 2007). Results indicate that these techniques are among the best both in machine translation and in the database query domain. Our method differs from those techniques that use a synchronous grammar, because we do not rewrite pairs of synchronized non-terminals, but instead synchronize chunks of derivation sequences. This difference is in part motivated by the fact that the strings for our two structures are perfectly aligned (being the same string), so synchronizing on the chunks of derivations associated with"
J13-4006,P11-2051,0,0.0133446,"main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of representation of language and text has many potential applications in question answering and information extraction (Surdeanu et al. 2003; Moschitti et al. 2007), and has recently been argued to be useful in machine translation and its evaluation (Wu and Fung 2009; Liu and Gildea 2010; Lo and Wu 2011; Wu et al. 2011), dialogue systems (Basili et al. 2009; Van der Plas, Henderson, and Merlo 2009), automatic data generation (Gao and Vogel 2011; Van der Plas, Merlo, and Henderson 2011) and authorship attribution (Hedegaard and Simonsen 2011), among others. The recovery of the full meaning of text requires structured analyses of both its grammar and its semantics. These two forms of linguistic knowledge are usually thought to be at least partly independent, as demonstrated by speakers’ ability to understand the meaning of ungrammatical text or speech and to assign grammatical categories and structures to unknown words and nonsense sentences. These two levels of representation of language, however, are closely correlated. From a lingu"
J13-4006,P11-2003,1,0.92649,"used to estimate the probability of this chosen parser action, also shown in red. The edges to the state that is used to make this decision are specified by identifying the most recent previous state that shares some property with this state. In Figure 8, these edges are labeled with the property, such as having the same word on the top of the stack (S=S) or the top of the stack being the same as the current leftmost child of the top of the stack (S=LS). The argument for the incremental specification of model structure can be applied to any Bayesian network architecture, not just SBNs (e.g., Garg and Henderson 2011). We focus on ISBNs because, as shown in Section 4.1.5, they are closely related to the empirically successful neural network models of Henderson (2003), and they have achieved very good results on the sub-problem of parsing syntactic dependencies (Titov and Henderson 2007d). 4.1.4 ISBNs for Derivations of Structures. The general form of ISBN models that have been proposed for modeling derivations of structures is illustrated in Figure 9. Figure 9 illustrates a situation where we are given a derivation history preceding the elementary decision dik in decision Di , and we wish to compute a prob"
J13-4006,P09-1069,0,0.00946418,"where two independent, but related, representations are being learned. 1. Introduction Success in statistical syntactic parsing based on supervised techniques trained on a large corpus of syntactic trees—both constituency-based (Collins 1999; Charniak 2000; Henderson 2003) and dependency-based (McDonald 2006; Nivre 2006; Bohnet and Nivre 2012; Hatori et al. 2012)—has paved the way to applying statistical approaches to the more ambitious goals of recovering semantic representations, such as the logical form of a sentence (Ge and Mooney 2005; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Ge and Mooney 2009; Kwiatkowski et al. 2011) or learning the propositional argument-structure of its main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of representation of language and text has many potential applications in question answering and information extraction (Surdeanu et al. 2003; Moschitti et al. 2007), and has recently been argued to be useful in machine translation and its evaluation (Wu and Fung 2009; Liu and Gildea 2010; Lo and Wu 2011; Wu et al. 2011), dialogue systems (Basili"
J13-4006,W05-0602,0,0.0203331,"ations is also an exemplification of its applicability to other problems where two independent, but related, representations are being learned. 1. Introduction Success in statistical syntactic parsing based on supervised techniques trained on a large corpus of syntactic trees—both constituency-based (Collins 1999; Charniak 2000; Henderson 2003) and dependency-based (McDonald 2006; Nivre 2006; Bohnet and Nivre 2012; Hatori et al. 2012)—has paved the way to applying statistical approaches to the more ambitious goals of recovering semantic representations, such as the logical form of a sentence (Ge and Mooney 2005; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Ge and Mooney 2009; Kwiatkowski et al. 2011) or learning the propositional argument-structure of its main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of representation of language and text has many potential applications in question answering and information extraction (Surdeanu et al. 2003; Moschitti et al. 2007), and has recently been argued to be useful in machine translation and its evaluation (Wu and Fung 2009; Liu an"
J13-4006,W09-1205,1,0.929971,"Missing"
J13-4006,J02-3001,0,0.528478,"supervised techniques trained on a large corpus of syntactic trees—both constituency-based (Collins 1999; Charniak 2000; Henderson 2003) and dependency-based (McDonald 2006; Nivre 2006; Bohnet and Nivre 2012; Hatori et al. 2012)—has paved the way to applying statistical approaches to the more ambitious goals of recovering semantic representations, such as the logical form of a sentence (Ge and Mooney 2005; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Ge and Mooney 2009; Kwiatkowski et al. 2011) or learning the propositional argument-structure of its main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of representation of language and text has many potential applications in question answering and information extraction (Surdeanu et al. 2003; Moschitti et al. 2007), and has recently been argued to be useful in machine translation and its evaluation (Wu and Fung 2009; Liu and Gildea 2010; Lo and Wu 2011; Wu et al. 2011), dialogue systems (Basili et al. 2009; Van der Plas, Henderson, and Merlo 2009), automatic data generation (Gao and Vogel 2011; Van der Plas, Merlo, and Henderson 2011)"
J13-4006,P10-1151,0,0.037242,"Missing"
J13-4006,W09-1201,0,0.0282323,"Missing"
J13-4006,W05-1505,0,0.0299227,"rcs, rather than to change the order of the target string. The switching of elements of the semantic structure used in Wong and Mooney (2007) is more similar to the word reordering technique of Hajiˇcov´a et al. (2004) than to our Swap operation, because the reordering occurs before, rather than during, the derivation. The notion of planarity has been widely discussed in many works cited herein, and in the dependency parsing literature. Approaches to dealing with non-planar graphs belong to two conceptual groups: those that manipulate the graph, either by pre-processing or by post-processing (Hall and Novak 2005; McDonald and Pereira 2006), and those that adapt the algorithm to deal with non-planarity. Among the approaches that, like ours, devise an algorithm to deal with non-planarity, Yngve (1960) proposed a limited manipulation of registers to handle discontinuous constituents, which guaranteed that parsing/generation could be performed with a stack of very limited depth. An approach to non-planar parsing that is more similar to ours has been proposed in Attardi (2006). Attardi’s dependency parsing algorithm adds six new actions that allow this algorithm to parse any type of non-planar tree. Our S"
J13-4006,P12-1110,0,0.0280159,"omputational Linguistics Computational Linguistics Volume 39, Number 4 nature of the approach, this extension of the ISBN architecture to weakly synchronized syntacticsemantic derivations is also an exemplification of its applicability to other problems where two independent, but related, representations are being learned. 1. Introduction Success in statistical syntactic parsing based on supervised techniques trained on a large corpus of syntactic trees—both constituency-based (Collins 1999; Charniak 2000; Henderson 2003) and dependency-based (McDonald 2006; Nivre 2006; Bohnet and Nivre 2012; Hatori et al. 2012)—has paved the way to applying statistical approaches to the more ambitious goals of recovering semantic representations, such as the logical form of a sentence (Ge and Mooney 2005; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Ge and Mooney 2009; Kwiatkowski et al. 2011) or learning the propositional argument-structure of its main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of representation of language and text has many potential applications in question answering and"
J13-4006,P11-2012,0,0.0126839,"; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of representation of language and text has many potential applications in question answering and information extraction (Surdeanu et al. 2003; Moschitti et al. 2007), and has recently been argued to be useful in machine translation and its evaluation (Wu and Fung 2009; Liu and Gildea 2010; Lo and Wu 2011; Wu et al. 2011), dialogue systems (Basili et al. 2009; Van der Plas, Henderson, and Merlo 2009), automatic data generation (Gao and Vogel 2011; Van der Plas, Merlo, and Henderson 2011) and authorship attribution (Hedegaard and Simonsen 2011), among others. The recovery of the full meaning of text requires structured analyses of both its grammar and its semantics. These two forms of linguistic knowledge are usually thought to be at least partly independent, as demonstrated by speakers’ ability to understand the meaning of ungrammatical text or speech and to assign grammatical categories and structures to unknown words and nonsense sentences. These two levels of representation of language, however, are closely correlated. From a linguistic point of view, the assumption that syntactic distributions will be predictive of semantic rol"
J13-4006,N03-1014,1,0.346312,"pted for publication: 1 November 2012. doi:10.1162/COLI a 00158 © 2013 Association for Computational Linguistics Computational Linguistics Volume 39, Number 4 nature of the approach, this extension of the ISBN architecture to weakly synchronized syntacticsemantic derivations is also an exemplification of its applicability to other problems where two independent, but related, representations are being learned. 1. Introduction Success in statistical syntactic parsing based on supervised techniques trained on a large corpus of syntactic trees—both constituency-based (Collins 1999; Charniak 2000; Henderson 2003) and dependency-based (McDonald 2006; Nivre 2006; Bohnet and Nivre 2012; Hatori et al. 2012)—has paved the way to applying statistical approaches to the more ambitious goals of recovering semantic representations, such as the logical form of a sentence (Ge and Mooney 2005; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Ge and Mooney 2009; Kwiatkowski et al. 2011) or learning the propositional argument-structure of its main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of r"
J13-4006,W08-2122,1,0.960335,"her drop of recall on non-planar dependencies. Applying the same planarization approach to semantic dependency structures is not trivial and would require a novel planarization algorithm, because semantic dependency graphs are highly disconnected structures, and direct application of any planarization algorithm, such as the one proposed in Nivre and Nilsson (2005), is unlikely to be appropriate. For instance, a method that extends the planarization method to semantic predicate-argument structures by exploiting the connectedness of the corresponding syntactic dependency trees has been tried in Henderson et al. (2008). Experimental results reported in Section 6 indicate that the method that we will illustrate in the following paragraphs yields better performance. A different way to tackle non-planarity is to extend the set of parsing actions to a more complex set that can parse any type of non-planarity (Attardi 2006). This approach is discussed in more detail in Section 7. We adopt a conservative version of this approach 3 Note that this planarity definition is stricter than the definition normally used in graph theory where the entire plane is used. Some parsing algorithms require projectivity: this is a"
J13-4006,W07-2416,0,0.0276781,"re needed because nominal long distance dependencies are not captured under the Penn Treebank’s system of empty categories. They are used for all those cases in which the nominal argument is outside the noun phrase. For example, in a support verb construction, such as Mary took dozens of walks, the arcs linking walks to of , of to dozens, and dozens to took are all marked as support. The data we use for English are the output of an automatic process of conversion of the original PTB, PropBank, and NomBank into dependency structures, performed by the algorithm described in Johansson and Nugues (2007). These are the data provided to participants to the CoNLL-2008 and CoNLL-2009 shared tasks (http://ifarm.nl/signll/conll/). An example is shown in Figure 3. This representation encodes both the grammatical functions and the semantic labels that describe the sentence. Argument labels in PropBank and NomBank are assigned to constituents, as shown in Figure 2. After the conversion to dependency the PropBank and NomBank labels Figure 3 An example from the PropBank corpus of verbal predicates and their semantic roles (lower half) paired with syntactic dependencies derived from the Penn Treebank. 9"
J13-4006,D08-1008,0,0.0432658,"tic representations is central for any system that taps into the meaning of text. Standard approaches to automatic semantic role labeling use hand-crafted features of syntactic and semantic representations within linear models trained with supervised learning. For example, Gildea and Jurafsky (2002) formulate the shallow semantic task of semantic role labeling (SRL) as a classification problem, where the semantic role to be assigned to each constituent is inferred on the basis of its co-occurrence counts with syntactic features extracted from parse trees. More recent and accurate SRL methods (Johansson and Nugues 2008a; Punyakanok, Roth, and Yih 2008) use complex sets of lexico-syntactic features and declarative constraints to infer the semantic structure. Whereas supervised learning is more flexible, general, and adaptable than hand-crafted systems, linear models require complex features and the number of these features grows with the complexity of the task. To keep the number of features tractable, model designers impose hard constraints on the possible interactions within the semantic or syntactic structures, such as conditioning on grandparents but not great-great-grandparents. Likewise, hard constrain"
J13-4006,W08-2123,0,0.0264032,"tic representations is central for any system that taps into the meaning of text. Standard approaches to automatic semantic role labeling use hand-crafted features of syntactic and semantic representations within linear models trained with supervised learning. For example, Gildea and Jurafsky (2002) formulate the shallow semantic task of semantic role labeling (SRL) as a classification problem, where the semantic role to be assigned to each constituent is inferred on the basis of its co-occurrence counts with syntactic features extracted from parse trees. More recent and accurate SRL methods (Johansson and Nugues 2008a; Punyakanok, Roth, and Yih 2008) use complex sets of lexico-syntactic features and declarative constraints to infer the semantic structure. Whereas supervised learning is more flexible, general, and adaptable than hand-crafted systems, linear models require complex features and the number of these features grows with the complexity of the task. To keep the number of features tractable, model designers impose hard constraints on the possible interactions within the semantic or syntactic structures, such as conditioning on grandparents but not great-great-grandparents. Likewise, hard constrain"
J13-4006,kawahara-etal-2002-construction,0,0.0253574,"Missing"
J13-4006,D11-1140,0,0.0212437,"nt, but related, representations are being learned. 1. Introduction Success in statistical syntactic parsing based on supervised techniques trained on a large corpus of syntactic trees—both constituency-based (Collins 1999; Charniak 2000; Henderson 2003) and dependency-based (McDonald 2006; Nivre 2006; Bohnet and Nivre 2012; Hatori et al. 2012)—has paved the way to applying statistical approaches to the more ambitious goals of recovering semantic representations, such as the logical form of a sentence (Ge and Mooney 2005; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Ge and Mooney 2009; Kwiatkowski et al. 2011) or learning the propositional argument-structure of its main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of representation of language and text has many potential applications in question answering and information extraction (Surdeanu et al. 2003; Moschitti et al. 2007), and has recently been argued to be useful in machine translation and its evaluation (Wu and Fung 2009; Liu and Gildea 2010; Lo and Wu 2011; Wu et al. 2011), dialogue systems (Basili et al. 2009; Van der Plas"
J13-4006,P11-1112,0,0.0168707,"sequence, as discussed subsequently. But defining a joint syntactic–semantic derivation presents a challenge. Namely, given the complex nature of correspondences between the structures, it is not obvious how to synchronize individual semantic– syntactic steps in the derivation. Previous joint statistical models of dependency syntax and SRL have either ignored semantic arcs not corresponding to single syntactic arcs (Thompson, Levy, and Manning 2003; Titov and Klementiev 2011) or resorted to pre-/post-processing strategies that modify semantic or syntactic structures (Llu´ıs and M`arquez 2008; Lang and Lapata 2011; Titov and Klementiev 2012). In a constituency setting, Li, Zhou, and Ng (2010) explore different levels of coupling of syntax and semantics, and find that only explicit interleaving or explicit feature selection yield improvements in performance. Instead of synchronizing individual steps, we (1) decompose both the syntactic derivation and the semantic derivation into subsequences, where each subsequence corresponds to a single word in the sentence, and then (2) synchronize syntactic and semantic subsequences corresponding to the same word with each other. To decide which steps correspond to"
J13-4006,P10-1113,0,0.0686563,"Missing"
J13-4006,D07-1072,0,0.0138803,"Missing"
J13-4006,C10-1081,0,0.00983689,"y 2005; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Ge and Mooney 2009; Kwiatkowski et al. 2011) or learning the propositional argument-structure of its main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of representation of language and text has many potential applications in question answering and information extraction (Surdeanu et al. 2003; Moschitti et al. 2007), and has recently been argued to be useful in machine translation and its evaluation (Wu and Fung 2009; Liu and Gildea 2010; Lo and Wu 2011; Wu et al. 2011), dialogue systems (Basili et al. 2009; Van der Plas, Henderson, and Merlo 2009), automatic data generation (Gao and Vogel 2011; Van der Plas, Merlo, and Henderson 2011) and authorship attribution (Hedegaard and Simonsen 2011), among others. The recovery of the full meaning of text requires structured analyses of both its grammar and its semantics. These two forms of linguistic knowledge are usually thought to be at least partly independent, as demonstrated by speakers’ ability to understand the meaning of ungrammatical text or speech and to assign grammatical"
J13-4006,W08-2124,0,0.466359,"Missing"
J13-4006,P11-1023,0,0.0121846,"ey 2007; Zettlemoyer and Collins 2007; Ge and Mooney 2009; Kwiatkowski et al. 2011) or learning the propositional argument-structure of its main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of representation of language and text has many potential applications in question answering and information extraction (Surdeanu et al. 2003; Moschitti et al. 2007), and has recently been argued to be useful in machine translation and its evaluation (Wu and Fung 2009; Liu and Gildea 2010; Lo and Wu 2011; Wu et al. 2011), dialogue systems (Basili et al. 2009; Van der Plas, Henderson, and Merlo 2009), automatic data generation (Gao and Vogel 2011; Van der Plas, Merlo, and Henderson 2011) and authorship attribution (Hedegaard and Simonsen 2011), among others. The recovery of the full meaning of text requires structured analyses of both its grammar and its semantics. These two forms of linguistic knowledge are usually thought to be at least partly independent, as demonstrated by speakers’ ability to understand the meaning of ungrammatical text or speech and to assign grammatical categories and s"
J13-4006,J93-2004,0,0.048808,"Missing"
J13-4006,J08-2001,0,0.0213094,"Missing"
J13-4006,P05-1010,0,0.0303616,"Missing"
J13-4006,E06-1011,0,0.0289654,"er of the target string. The switching of elements of the semantic structure used in Wong and Mooney (2007) is more similar to the word reordering technique of Hajiˇcov´a et al. (2004) than to our Swap operation, because the reordering occurs before, rather than during, the derivation. The notion of planarity has been widely discussed in many works cited herein, and in the dependency parsing literature. Approaches to dealing with non-planar graphs belong to two conceptual groups: those that manipulate the graph, either by pre-processing or by post-processing (Hall and Novak 2005; McDonald and Pereira 2006), and those that adapt the algorithm to deal with non-planarity. Among the approaches that, like ours, devise an algorithm to deal with non-planarity, Yngve (1960) proposed a limited manipulation of registers to handle discontinuous constituents, which guaranteed that parsing/generation could be performed with a stack of very limited depth. An approach to non-planar parsing that is more similar to ours has been proposed in Attardi (2006). Attardi’s dependency parsing algorithm adds six new actions that allow this algorithm to parse any type of non-planar tree. Our Swap action is related to Att"
J13-4006,W08-2101,1,0.933675,"Missing"
J13-4006,J01-3003,1,0.121086,"orrelated. From a linguistic point of view, the assumption that syntactic distributions will be predictive of semantic role assignments is based on linking theory (Levin 1986). Linking theory assumes the existence of a ranking of semantic roles that are mapped by default on a ranking of grammatical functions and syntactic positions, and it attempts to predict the mapping of the underlying semantic component of a predicate’s meaning onto the syntactic structure. For example, Agents are always mapped in syntactically higher positions than Themes. Linking theory has been confirmed statistically (Merlo and Stevenson 2001). It is currently common to represent the syntactic and semantic role structures of a sentence in terms of dependencies, as illustrated in Figure 1. The complete graph of both the syntax and the semantics of the sentences is composed of two half graphs, which Figure 1 A semantic dependency graph labeled with semantic roles (lower half) paired with a syntactic dependency tree labeled with grammatical relations. 950 Henderson et al. Joint Syntactic and Semantic Parsing share all their vertices—namely, the words. Internally, these two half graphs exhibit different properties. The syntactic graph"
J13-4006,W04-2705,0,0.0149074,"y express consistent semantic roles across verbs, whereas arguments receiving an AM-X label are supposed to be adjuncts, and the roles they express are consistent across all verbs. A0 and A1 arguments are annotated based on the proto-role theory presented in Dowty (1991) and correspond to proto-agents and proto-patients, respectively. Although PropBank, unlike FrameNet, does not attempt to group different predicates evoking the same prototypical situation, it does distinguish between different senses of polysemous verbs, resulting in multiple framesets for such predicates. NomBank annotation (Meyers et al. 2004) extends the PropBank framework to annotate arguments of nouns. Only the subset of nouns that take arguments are annotated in NomBank and only a subset of the non-argument siblings of nouns are marked as ARG-M. The most notable specificity of NomBank is the use of support chains, marked as SU. Support chains are needed because nominal long distance dependencies are not captured under the Penn Treebank’s system of empty categories. They are used for all those cases in which the nominal argument is outside the noun phrase. For example, in a support verb construction, such as Mary took dozens of"
J13-4006,A00-2030,0,0.0353462,"Missing"
J13-4006,R09-1051,0,0.195166,"Missing"
J13-4006,P07-1098,0,0.0109335,"oaches to the more ambitious goals of recovering semantic representations, such as the logical form of a sentence (Ge and Mooney 2005; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Ge and Mooney 2009; Kwiatkowski et al. 2011) or learning the propositional argument-structure of its main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of representation of language and text has many potential applications in question answering and information extraction (Surdeanu et al. 2003; Moschitti et al. 2007), and has recently been argued to be useful in machine translation and its evaluation (Wu and Fung 2009; Liu and Gildea 2010; Lo and Wu 2011; Wu et al. 2011), dialogue systems (Basili et al. 2009; Van der Plas, Henderson, and Merlo 2009), automatic data generation (Gao and Vogel 2011; Van der Plas, Merlo, and Henderson 2011) and authorship attribution (Hedegaard and Simonsen 2011), among others. The recovery of the full meaning of text requires structured analyses of both its grammar and its semantics. These two forms of linguistic knowledge are usually thought to be at least partly independen"
J13-4006,W05-1509,1,0.808134,"ub-problems needed to be solved to find a solution for these primary tasks, then one would expect an improvement from inducing shared representations. Multi-task learning methods have been shown to be beneficial in many domains, including natural language processing (Ando and Zhang 2005a, 2005b; Argyriou, Evgeniou, and Pontil 2006; Collobert and Weston 2008). Their application in the context of syntactic-semantic parsing has been very limited, however. The only other such successful multi-task learning approach we are aware of targets a similar, but more restricted, task of function labeling (Musillo and Merlo 2005). Musillo and Merlo (2005) conclusively show that jointly learning functional and syntactic information can significantly improve syntax. Our joint learning approach is an example of a multi-task learning approach in that the induced representations in the vectors of latent variables can capture hidden sub-problems relevant to predicting both syntactic and semantic structures. The rest of this article will first describe the data that are used in this work and their relevant properties. We then present our probabilistic model of joint syntactic parsing 953 Computational Linguistics Volume 39,"
J13-4006,N06-2026,1,0.911364,"Missing"
J13-4006,P08-2054,1,0.840744,"st, our ISBN latent variable models do not require heuristics to control the complexity of the augmented grammars or to search for predictive latent representations. Furthermore, probabilistic context-free grammars augmented with latent annotations do impose context-free independence assumptions between the latent labels, contrary to our models. Finally, our ISBN models have been successfully applied to both phrase-structure and dependency parsing. State-of-the-art results on unlexicalized dependency parsing have recently been achieved with latent variable probabilistic context-free grammars (Musillo and Merlo 2008; Musillo 2010). These latent variable grammars are compact and interpretable from a linguistic perspective, and they integrate grammar transforms that constrain the flow of latent information, thereby drastically limiting the space of latent annotations. For example, they encode the notion of X-bar projection in their constrained latent variables. 8. Conclusions and Future Work The proposed joint model achieves competitive performance on both syntactic and semantic dependency parsing for several languages. Our experiments also demonstrate the benefit of joint learning of syntax and semantics."
J13-4006,P09-1040,0,0.219321,"Missing"
J13-4006,W06-2933,0,0.0211291,"those used in previous dependency parsing work. No independence assumptions are made in the probability decomposition itself. This allows the probability estimation technique (discussed in Section 4) to make maximal use of its latent variables to learn correlations between the different parser actions, both within and between structures. 3.1 Synchronized Derivations We first specify the syntactic and semantic derivations separately, before specifying how they are synchronized in a joint generative model. The derivations for syntactic dependency trees are based on a shift-reduce style parser (Nivre et al. 2006; Titov and Henderson 2007d). The derivations use a stack and an input queue. There are actions for creating a leftward or rightward arc between the top of the stack and the front of the queue, for popping a word from the stack, and for shifting a word from the queue to the stack. A syntactic configuration of the parser is defined by the current stack, the queue of remaining input words, and the partial labeled dependency structure constructed by previous parser actions. The parser starts with an empty stack and terminates when it 957 Computational Linguistics Volume 39, Number 4 reaches a con"
J13-4006,W09-3811,0,0.0251302,"Missing"
J13-4006,P05-1013,0,0.029498,"Missing"
J13-4006,J05-1004,0,0.311193,"Missing"
J13-4006,P06-1055,0,0.0132839,"rchitecture to design a joint model of syntactic–semantic dependency parsing. In traditional fully supervised parsing models, designing a joint syntactic–semantic parsing model would require extensive feature engineering. These features pick out parts of the corpus annotation that are relevant to predicting other parts of the corpus annotation. If features are missing then predicting the annotation cannot be done accurately, and if there are too many features then the model cannot be learned accurately. Latent variable models, such as ISBNs and Latent PCFGs (Matsuzaki, Miyao, and Tsujii 2005; Petrov et al. 2006), have the advantage that the model can induce new, more predictive, features by composing elementary features, or propagate information to include predictive but non-local features. These latent annotations are induced during learning, allowing the model to both predict them from other parts of the annotation and use them to predict the desired corpus annotation. In ISBNs, we use latent variables to induce features of the parse history D1 , . . . , Di−1 that are used to predict future parser decisions Di , . . . , Dm . The main difference between ISBNs and Latent PCFGs is that ISBNs have vect"
J13-4006,W05-1512,0,0.0551746,"Missing"
J13-4006,J08-2005,0,0.301645,"Missing"
J13-4006,P03-1002,0,0.0145641,"lying statistical approaches to the more ambitious goals of recovering semantic representations, such as the logical form of a sentence (Ge and Mooney 2005; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Ge and Mooney 2009; Kwiatkowski et al. 2011) or learning the propositional argument-structure of its main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of representation of language and text has many potential applications in question answering and information extraction (Surdeanu et al. 2003; Moschitti et al. 2007), and has recently been argued to be useful in machine translation and its evaluation (Wu and Fung 2009; Liu and Gildea 2010; Lo and Wu 2011; Wu et al. 2011), dialogue systems (Basili et al. 2009; Van der Plas, Henderson, and Merlo 2009), automatic data generation (Gao and Vogel 2011; Van der Plas, Merlo, and Henderson 2011) and authorship attribution (Hedegaard and Simonsen 2011), among others. The recovery of the full meaning of text requires structured analyses of both its grammar and its semantics. These two forms of linguistic knowledge are usually thought to be at"
J13-4006,W08-2121,0,0.0315325,"Missing"
J13-4006,taule-etal-2008-ancora,0,0.0186112,"Missing"
J13-4006,P07-1080,1,0.904311,"re needed because nominal long distance dependencies are not captured under the Penn Treebank’s system of empty categories. They are used for all those cases in which the nominal argument is outside the noun phrase. For example, in a support verb construction, such as Mary took dozens of walks, the arcs linking walks to of , of to dozens, and dozens to took are all marked as support. The data we use for English are the output of an automatic process of conversion of the original PTB, PropBank, and NomBank into dependency structures, performed by the algorithm described in Johansson and Nugues (2007). These are the data provided to participants to the CoNLL-2008 and CoNLL-2009 shared tasks (http://ifarm.nl/signll/conll/). An example is shown in Figure 3. This representation encodes both the grammatical functions and the semantic labels that describe the sentence. Argument labels in PropBank and NomBank are assigned to constituents, as shown in Figure 2. After the conversion to dependency the PropBank and NomBank labels Figure 3 An example from the PropBank corpus of verbal predicates and their semantic roles (lower half) paired with syntactic dependencies derived from the Penn Treebank. 9"
J13-4006,D07-1099,1,0.919153,"Missing"
J13-4006,W07-2218,1,0.414946,"uences. This derivational, joint approach to handling these complex representations leads to a new proposal on how to learn them, which avoids extensive and complex feature engineering, as discussed in the following. 1.2 Joint Learning of Syntax and Semantics Our probabilistic model is learned using Incremental Sigmoid Belief Networks (ISBNs) (Henderson and Titov 2010), a recent development of an early latent variable model 952 Henderson et al. Joint Syntactic and Semantic Parsing for syntactic structure prediction (Henderson 2003), which has shown very good performance for both constituency (Titov and Henderson 2007a) and dependency parsing (Titov and Henderson 2007d). Instead of hand-crafting features of the previous parsing decisions, as is standard in history-based models, ISBNs estimate the probability of the next parsing actions conditioned on a vector of latent-variable features of the parsing history. These features are induced automatically to maximize the likelihood of the syntactic–semantics graphs given in the training set, and therefore they encode important correlations between syntactic and semantic decisions. This makes joint learning of syntax and semantics a crucial component of our appr"
J13-4006,P11-1145,1,0.410843,"yntactic dependency graph to a derivation sequence, and similar algorithms can be defined for mapping a semantic dependency graph to a derivation sequence, as discussed subsequently. But defining a joint syntactic–semantic derivation presents a challenge. Namely, given the complex nature of correspondences between the structures, it is not obvious how to synchronize individual semantic– syntactic steps in the derivation. Previous joint statistical models of dependency syntax and SRL have either ignored semantic arcs not corresponding to single syntactic arcs (Thompson, Levy, and Manning 2003; Titov and Klementiev 2011) or resorted to pre-/post-processing strategies that modify semantic or syntactic structures (Llu´ıs and M`arquez 2008; Lang and Lapata 2011; Titov and Klementiev 2012). In a constituency setting, Li, Zhou, and Ng (2010) explore different levels of coupling of syntax and semantics, and find that only explicit interleaving or explicit feature selection yield improvements in performance. Instead of synchronizing individual steps, we (1) decompose both the syntactic derivation and the semantic derivation into subsequences, where each subsequence corresponds to a single word in the sentence, and t"
J13-4006,E12-1003,1,0.526699,"subsequently. But defining a joint syntactic–semantic derivation presents a challenge. Namely, given the complex nature of correspondences between the structures, it is not obvious how to synchronize individual semantic– syntactic steps in the derivation. Previous joint statistical models of dependency syntax and SRL have either ignored semantic arcs not corresponding to single syntactic arcs (Thompson, Levy, and Manning 2003; Titov and Klementiev 2011) or resorted to pre-/post-processing strategies that modify semantic or syntactic structures (Llu´ıs and M`arquez 2008; Lang and Lapata 2011; Titov and Klementiev 2012). In a constituency setting, Li, Zhou, and Ng (2010) explore different levels of coupling of syntax and semantics, and find that only explicit interleaving or explicit feature selection yield improvements in performance. Instead of synchronizing individual steps, we (1) decompose both the syntactic derivation and the semantic derivation into subsequences, where each subsequence corresponds to a single word in the sentence, and then (2) synchronize syntactic and semantic subsequences corresponding to the same word with each other. To decide which steps correspond to a given word, we use a simpl"
J13-4006,J08-2002,0,0.369439,"Missing"
J13-4006,D09-1088,0,0.0527065,"Missing"
J13-4006,N09-2032,1,0.888657,"Missing"
J13-4006,P11-2052,1,0.891556,"Missing"
J13-4006,N06-1056,0,0.0444389,"obvious 988 Henderson et al. Joint Syntactic and Semantic Parsing correlation. Li, Zhou, and Ng’s approach is also different from ours in that they do not attempt to induce common representations useful for both tasks or for many languages, and as such cannot be regarded as multi-task, nor as multilingual, learning. Synchronous grammars provide an elegant way to handle multiple levels of representation. They have received much attention because of their applications in syntaxbased statistical machine translation (Galley et al. 2004; Chiang 2005; Nesson and Shieber 2008) and semantic parsing (Wong and Mooney 2006, 2007). Results indicate that these techniques are among the best both in machine translation and in the database query domain. Our method differs from those techniques that use a synchronous grammar, because we do not rewrite pairs of synchronized non-terminals, but instead synchronize chunks of derivation sequences. This difference is in part motivated by the fact that the strings for our two structures are perfectly aligned (being the same string), so synchronizing on the chunks of derivations associated with individual words eliminates any further alignment issues. We have also proposed n"
J13-4006,P07-1121,0,0.099988,"emplification of its applicability to other problems where two independent, but related, representations are being learned. 1. Introduction Success in statistical syntactic parsing based on supervised techniques trained on a large corpus of syntactic trees—both constituency-based (Collins 1999; Charniak 2000; Henderson 2003) and dependency-based (McDonald 2006; Nivre 2006; Bohnet and Nivre 2012; Hatori et al. 2012)—has paved the way to applying statistical approaches to the more ambitious goals of recovering semantic representations, such as the logical form of a sentence (Ge and Mooney 2005; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Ge and Mooney 2009; Kwiatkowski et al. 2011) or learning the propositional argument-structure of its main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of representation of language and text has many potential applications in question answering and information extraction (Surdeanu et al. 2003; Moschitti et al. 2007), and has recently been argued to be useful in machine translation and its evaluation (Wu and Fung 2009; Liu and Gildea 2010; Lo and"
J13-4006,J97-3002,0,0.0302763,"bsequence forms a linguistically meaningful chunk in that it includes all the decisions about the arcs on the left side of the associated word, both its parents and its children. Thus, synchronizing the syntactic and semantic subsequences according to their associated word places together subsequences that are likely to be correlated. Note that such pairs of syntactic and semantic subsequences will, in general, have different numbers of steps on each side and these numbers of steps are, in general, unbounded. Therefore, instead of defining atomic synchronized rules as in synchronous grammars (Wu 1997; Chiang 2005), we resort to parametrized models that exploit the internal structure of the paired subsequences. This derivational, joint approach to handling these complex representations leads to a new proposal on how to learn them, which avoids extensive and complex feature engineering, as discussed in the following. 1.2 Joint Learning of Syntax and Semantics Our probabilistic model is learned using Incremental Sigmoid Belief Networks (ISBNs) (Henderson and Titov 2010), a recent development of an early latent variable model 952 Henderson et al. Joint Syntactic and Semantic Parsing for synta"
J13-4006,N09-2004,0,0.0248142,"Missing"
J13-4006,C00-2137,0,0.0196533,"ntic measures in the table). For the CoNLL-2008 scores the predicate sense labeling includes predicate identification, but for the CoNLL-2009 scores predicate identification was given in the task input. The syntactic LAS and the semantic F1 are then averaged with equal weight to produce an overall score called Macro F1 .10 When we evaluate the impact of the Swap action on crossing arcs, we also calculate precision, recall, and F-measure on pairs of crossing arcs.11 In our experiments, the statistical significance levels we report are all computed using a stratified shuffling test (Cohen 1995; Yeh 2000) with 10,000 randomized trials. 6.1 Monolingual Experimental Set-up We start by describing the monolingual English experiments. We train and evaluate our English models on data provided for the CoNLL-2008 shared task on joint learning of syntactic and semantic dependencies. The data is derived by merging a dependency transformation of the Penn Treebank with PropBank and NomBank (Surdeanu et al. 2008). An illustrative example of the kind of labeled structures that we need to parse is given in Figure 3. Training, development, and test data follow the usual partition as sections 02–21, 24, and 23"
J13-4006,D07-1071,0,0.0291049,"pplicability to other problems where two independent, but related, representations are being learned. 1. Introduction Success in statistical syntactic parsing based on supervised techniques trained on a large corpus of syntactic trees—both constituency-based (Collins 1999; Charniak 2000; Henderson 2003) and dependency-based (McDonald 2006; Nivre 2006; Bohnet and Nivre 2012; Hatori et al. 2012)—has paved the way to applying statistical approaches to the more ambitious goals of recovering semantic representations, such as the logical form of a sentence (Ge and Mooney 2005; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Ge and Mooney 2009; Kwiatkowski et al. 2011) or learning the propositional argument-structure of its main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of representation of language and text has many potential applications in question answering and information extraction (Surdeanu et al. 2003; Moschitti et al. 2007), and has recently been argued to be useful in machine translation and its evaluation (Wu and Fung 2009; Liu and Gildea 2010; Lo and Wu 2011; Wu et al. 2011), dial"
J13-4006,W08-2127,0,0.19821,"Missing"
J13-4006,W04-2407,0,\N,Missing
J13-4006,J03-4003,0,\N,Missing
J13-4006,C98-1013,0,\N,Missing
J13-4006,P05-1001,0,\N,Missing
J13-4006,D07-1096,0,\N,Missing
J13-4006,W06-2303,1,\N,Missing
J14-3007,W06-3814,0,0.0491604,"Missing"
J14-3007,S07-1002,0,0.0519592,"Missing"
J14-3007,H05-1004,0,0.136661,"Missing"
J14-3007,W09-2419,0,0.0864926,"H(c) and H(k). As a result, the V-measure will be positively biased, and this bias would be especially high for systems predicting a large number of clusters. This phenomenon has been previously noticed (Manandhar et al. 2010) but no satisfactory explanation has been given. The shortcomings of the ML estimator are especially easy to see on the example of a baseline system that assigns every instance in the testing set to an individual cluster. This baseline, when averaged over the 100 target words, outperforms all the participants’ systems of the SemEval-2010 task on the standard testing set (Manandhar and Klapaftis 2009). Though we cannot compute the true bias for any real system, the computation is trivial for this baseline. The true V-measure is equal to 0, 1 V-measure can be expressed via entropies in a number of different ways, although, for ML estimation they are all equivalent. For some more complex estimators, including some of the ones considered here, the resulting estimates will be somewhat different depending on the decomposition. We will focus on the symmetric form presented here. 673 Computational Linguistics Volume 40, Number 3 as the baseline can be regarded as a limiting case of a stochastic s"
J14-3007,S10-1011,0,0.389128,"4286 Trier, Germany. E-mail: sporledc@uni-trier.de. Submission received: 14 March 2013; revised version received: 13 September 2013; accepted for publication: 20 November 2013 doi:10.1162/COLI a 00196 © 2014 Association for Computational Linguistics Computational Linguistics Volume 40, Number 3 proposed but information theoretic measures have been among the most successful and widely used techniques. One example is the normalized mutual information, also known as V-measure (Strehl and Gosh 2002; Rosenberg and Hirschberg 2007), which has, for example, been adopted in the SemEval 2010 WSI task (Manandhar et al. 2010). All information theoretic measures of cluster quality essentially rely on samplebased estimates of entropy. For instance, the mutual information I(c, k) between a gold standard class c and an output cluster k can be written H(c) + H(k) − H(k, c), where H(c) and H(k) are the marginal entropies of c and k, respectively, and H(k, c) is their joint entropy. The most standard estimator is the maximum-likelihood (ML) estimator, which substitutes the probability of each event (cluster, classes, or cluster-class pair occurrence) with its normalized empirical frequency. Entropy estimators, even thoug"
J14-3007,W04-2406,0,0.0405453,"ly to the setting of WSI evaluation where a ground-truth cluster sense number arguably does not exist and the standard evaluation scenarios use a small number of instances of each word to compute the score. We describe more accurate entropy estimators and analyze their performance both in simulations and on evaluation of WSI systems. 1. Introduction The task of word sense induction (WSI) has grown in popularity recently. WSI has the advantage of not assuming a predefined inventory of senses. Rather, senses are induced in an unsupervised fashion on the basis of corpus evidence (Schutze ¨ 1998; Purandare and Pedersen 2004). WSI systems can therefore better adapt to different target domains that may require sense inventories of different granularities. However, the fact that WSI systems do not rely on fixed inventories also makes it notoriously difficult to evaluate and compare their performance. WSI evaluation is a type of cluster evaluation problem. Although cluster evaluation has received much attention (see, e.g., Dom 2001; Strehl and Gosh 2002; Meila 2007), it is still not a solved problem. Finding a good way to score partially incorrect clusters is particularly difficult. Several solutions have been ∗ Micr"
J14-3007,D07-1043,0,0.375199,"tion. E-mail: titov@uva.nl. † Computational Linguistics and Digital Humanities, Trier University, 54286 Trier, Germany. E-mail: sporledc@uni-trier.de. Submission received: 14 March 2013; revised version received: 13 September 2013; accepted for publication: 20 November 2013 doi:10.1162/COLI a 00196 © 2014 Association for Computational Linguistics Computational Linguistics Volume 40, Number 3 proposed but information theoretic measures have been among the most successful and widely used techniques. One example is the normalized mutual information, also known as V-measure (Strehl and Gosh 2002; Rosenberg and Hirschberg 2007), which has, for example, been adopted in the SemEval 2010 WSI task (Manandhar et al. 2010). All information theoretic measures of cluster quality essentially rely on samplebased estimates of entropy. For instance, the mutual information I(c, k) between a gold standard class c and an output cluster k can be written H(c) + H(k) − H(k, c), where H(c) and H(k) are the marginal entropies of c and k, respectively, and H(k, c) is their joint entropy. The most standard estimator is the maximum-likelihood (ML) estimator, which substitutes the probability of each event (cluster, classes, or cluster-cla"
J14-3007,J98-1004,0,0.734873,"Missing"
K17-1039,D13-1203,0,0.335242,", v0 are real scalars. Unlike Wiseman et al. (2015b), where the maxmargin loss is used, we define a probabilistic model. The probability3 that mi and mj are coreferent is given by The outline of our paper is as follows: we introduce our neural resolver baseline and the B3 and LEA metrics in Section 2. Our method to turn a mention ranking resolver into an entity-centric resolver is presented in Section 3, and the proposed differentiable relaxations in Section 4. Section 5 shows our experimental results. 2 if j < i p(ai = j) = Pi Background exp{s(ai = j)} j 0 =1 exp{s(ai = j 0 )} (1) Following Durrett and Klein (2013) we use the following softmax-margin (Gimpel and Smith, 2010) loss function: Neural mention ranking In this section we introduce neural mention ranking, the framework which underpins current stateof-the-art models (Clark and Manning, 2016a). Specifically, we consider a probabilistic version of the method proposed by Wiseman et al. (2015b). In experiments we will use it as our baseline. L(Θ) = − n X i=1 log X  p0 (ai = j) +λ||Θ||1 , j∈C(mi ) where Θ are model parameters, C(mi ) is the set of the indices of correct antecedents of mi , and p0 (ai = j) ∝ p(ai = j)e∆(j,C(mi )) . ∆ is a cost functi"
K17-1039,N10-1112,0,0.0340471,"he maxmargin loss is used, we define a probabilistic model. The probability3 that mi and mj are coreferent is given by The outline of our paper is as follows: we introduce our neural resolver baseline and the B3 and LEA metrics in Section 2. Our method to turn a mention ranking resolver into an entity-centric resolver is presented in Section 3, and the proposed differentiable relaxations in Section 4. Section 5 shows our experimental results. 2 if j < i p(ai = j) = Pi Background exp{s(ai = j)} j 0 =1 exp{s(ai = j 0 )} (1) Following Durrett and Klein (2013) we use the following softmax-margin (Gimpel and Smith, 2010) loss function: Neural mention ranking In this section we introduce neural mention ranking, the framework which underpins current stateof-the-art models (Clark and Manning, 2016a). Specifically, we consider a probabilistic version of the method proposed by Wiseman et al. (2015b). In experiments we will use it as our baseline. L(Θ) = − n X i=1 log X  p0 (ai = j) +λ||Θ||1 , j∈C(mi ) where Θ are model parameters, C(mi ) is the set of the indices of correct antecedents of mi , and p0 (ai = j) ∝ p(ai = j)e∆(j,C(mi )) . ∆ is a cost function used to manipulate the contribution of different error typ"
K17-1039,D17-1210,0,0.0458125,"Missing"
K17-1039,N10-1061,0,0.0339784,") but much stronger thanks to deeper neural networks and “better mention detection, more effective, hyperparameters, and more epochs of training”. Furthermore, using reward rescaling they achieve the best performance in the literature on the English and Chinese portions of the CoNLL 2012 dataset. Our work is built upon mention ranking by turning a mentionranking model into an entity-centric one. It is worth noting that although we use the model proposed by Wiseman et al. (2015b), any mentionranking models can be employed. Entity centricity (Wellner and McCallum, 2003; Poon and Domingos, 2008; Haghighi and Klein, 2010; Ma et al., 2014a; Clark and Manning, 2016b), on the other hand, incorporates entitylevel information to solve the problem. The approach can be top-down as in Haghighi and Klein (2010) where they propose a generative model. It can also be bottom-up by merging smaller clusters into bigger ones as in Clark and Manning (2016b). The method proposed by Ma et al. (2014a) greedily and incrementally adds mentions to previously built clusters using a prune-and-score technique. Importantly, employing imitation learning these two methods can optimize the resolvers directly on evaluation metrics. Our wor"
K17-1039,D16-1245,0,0.212269,"losses are only indirectly related to the metrics. One way to deal with this challenge is to optimize directly the non-differentiable metrics using reinforcement learning (RL), for example, relying on the REINFORCE policy gradient algorithm (Williams, 1992). However, this approach has not been very successful, which, as suggested by Clark and Manning (2016a), is possibly due to the discrepancy between sampling decisions at training time and choosing the highest ranking ones at test time. A more successful alternative is using a ‘roll-out’ stage to associate cost with possible decisions, as in Clark and Manning (2016a), but it is computationally expensive. Imitation learning (Ma et al., 2014b; Clark and Manning, 2015), though also exploiting metrics, requires access to an expert policy, with exact policies not directly computable for the metrics of interest. In this work, we aim at combining the best of both worlds by proposing a simple method that can turn popular coreference evaluation metrics into differentiable functions of model parameters. As we show, this function can be computed recursively using scores of individual local decisions, resulting in a simple and efficient estimation procedure. The ke"
K17-1039,P16-1061,0,0.0635332,"losses are only indirectly related to the metrics. One way to deal with this challenge is to optimize directly the non-differentiable metrics using reinforcement learning (RL), for example, relying on the REINFORCE policy gradient algorithm (Williams, 1992). However, this approach has not been very successful, which, as suggested by Clark and Manning (2016a), is possibly due to the discrepancy between sampling decisions at training time and choosing the highest ranking ones at test time. A more successful alternative is using a ‘roll-out’ stage to associate cost with possible decisions, as in Clark and Manning (2016a), but it is computationally expensive. Imitation learning (Ma et al., 2014b; Clark and Manning, 2015), though also exploiting metrics, requires access to an expert policy, with exact policies not directly computable for the metrics of interest. In this work, we aim at combining the best of both worlds by proposing a simple method that can turn popular coreference evaluation metrics into differentiable functions of model parameters. As we show, this function can be computed recursively using scores of individual local decisions, resulting in a simple and efficient estimation procedure. The ke"
K17-1039,H05-1004,0,0.059475,"e reward difference between the best decision according to the current model and the decision leading to the highest metric score. 2 This slightly deviates from the definition of antecedents in linguistics (Crystal, 1997). 3 For the sake of readability, we do not explicitly mark in our notation that all the probabilities are conditioned on the document (e.g., the mentions) and dependent on model parameters. 391 2.2 Evaluation Metrics m1 We use five most popular metrics4 , mu mu+1 mi mention ... ... entity ... ... • MUC (Vilain et al., 1995), • B3 (Bagga and Baldwin, 1998), E1 • CEAFm , CEAFe (Luo, 2005), • LEA (Moosavi and Strube, 2016). for evaluation. However, because MUC is the least discriminative metric (Moosavi and Strube, 2016), whereas CEAF is slow to compute, out of the five most popular metrics we incorporate into our loss only B3 . In addition, we integrate LEA, as it has been shown to provide a good balance between discriminativity and interpretability. Let G = {G1 , G2 , ..., GN } and S = {S1 , S2 , ..., SM } be the gold-standard entity set and an entity set given by a resolver. Recall that an entity is a set of mentions. The recall and precision of the B3 metric is computed by:"
K17-1039,P15-1136,0,0.0626163,"directly the non-differentiable metrics using reinforcement learning (RL), for example, relying on the REINFORCE policy gradient algorithm (Williams, 1992). However, this approach has not been very successful, which, as suggested by Clark and Manning (2016a), is possibly due to the discrepancy between sampling decisions at training time and choosing the highest ranking ones at test time. A more successful alternative is using a ‘roll-out’ stage to associate cost with possible decisions, as in Clark and Manning (2016a), but it is computationally expensive. Imitation learning (Ma et al., 2014b; Clark and Manning, 2015), though also exploiting metrics, requires access to an expert policy, with exact policies not directly computable for the metrics of interest. In this work, we aim at combining the best of both worlds by proposing a simple method that can turn popular coreference evaluation metrics into differentiable functions of model parameters. As we show, this function can be computed recursively using scores of individual local decisions, resulting in a simple and efficient estimation procedure. The key idea is to replace nondifferentiable indicator functions (e.g. the member function I(m ∈ S)) with the"
K17-1039,P14-2005,0,0.0431003,"Missing"
K17-1039,D14-1225,0,0.0397992,"Missing"
K17-1039,Q15-1029,0,0.016189,"case that the optimal β on one set can be suboptimal on the other set. Second, in our experiments we fix T = 1, meaning that the relaxations might not be close to the true evaluation metrics enough. Our future work, to confirm/reject this, is to use annealing, i.e., gradually decreasing T down to (but larger than) 0. Table 1 shows that the difference between Lβ,B 3 and Lβ,LEA in terms of accuracy is not substan6 Related work Mention ranking and entity centricity are two main streams in the coreference resolution literature. Mention ranking (Denis and Baldridge, 2007; Durrett and Klein, 2013; Martschat and Strube, 2015; Wiseman et al., 2015a) considers local and independent decisions when choosing a correct antecedent for a mention. This approach is computationally efficient and currently dominant with state-of-the-art performance (Wiseman et al., 2016; Clark and Manning, 2016a). Wiseman et al. (2015b) propose to use simple neural 396 Figure 4: Recall, precision, F1 (average of MUC, B3 , CEAFe ), on the development set when training with Lβ,B 3 (left) and Lβ,LEA (right). Higher values of β yield lower precisions but higher recalls. However, different from both Ma et al. (2014a); Clark and Manning (2016b), o"
K17-1039,P15-1137,0,0.144354,"a clustering problem: each cluster corresponds to a single entity and consists of all its mentions in a given text. Consequently, it is natural to evaluate predicted clusters by comparing them with the ones annotated by human experts, and this is exactly what the standard metrics (e.g., MUC, B3 , CEAF) do. In contrast, most state-of-theart systems are optimized to make individual co390 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 390–399, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics model of Wiseman et al. (2015b), which on its own outperforms the original one and achieves similar performance to its global version (Wiseman et al., 2016). Importantly when we use the introduced differentiable relaxations in training, we observe a substantial gain in performance over our probabilistic baseline. Interestingly, the absolute improvement (+0.52) is higher than the one reported in Clark and Manning (2016a) using RL (+0.05) and the one using reward rescaling1 (+0.37). This suggests that our method provides a viable alternative to using RL and reward rescaling. (mi , mj ). The scoring function is defined by: #"
K17-1039,P16-1060,0,0.0683648,"between the best decision according to the current model and the decision leading to the highest metric score. 2 This slightly deviates from the definition of antecedents in linguistics (Crystal, 1997). 3 For the sake of readability, we do not explicitly mark in our notation that all the probabilities are conditioned on the document (e.g., the mentions) and dependent on model parameters. 391 2.2 Evaluation Metrics m1 We use five most popular metrics4 , mu mu+1 mi mention ... ... entity ... ... • MUC (Vilain et al., 1995), • B3 (Bagga and Baldwin, 1998), E1 • CEAFm , CEAFe (Luo, 2005), • LEA (Moosavi and Strube, 2016). for evaluation. However, because MUC is the least discriminative metric (Moosavi and Strube, 2016), whereas CEAF is slow to compute, out of the five most popular metrics we incorporate into our loss only B3 . In addition, we integrate LEA, as it has been shown to provide a good balance between discriminativity and interpretability. Let G = {G1 , G2 , ..., GN } and S = {S1 , S2 , ..., SM } be the gold-standard entity set and an entity set given by a resolver. Recall that an entity is a set of mentions. The recall and precision of the B3 metric is computed by: 3 |Gv ∩Su |2 u=1 |Gv | PN v=1 |Gv"
K17-1039,N16-1114,0,0.155907,"ly, it is natural to evaluate predicted clusters by comparing them with the ones annotated by human experts, and this is exactly what the standard metrics (e.g., MUC, B3 , CEAF) do. In contrast, most state-of-theart systems are optimized to make individual co390 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 390–399, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics model of Wiseman et al. (2015b), which on its own outperforms the original one and achieves similar performance to its global version (Wiseman et al., 2016). Importantly when we use the introduced differentiable relaxations in training, we observe a substantial gain in performance over our probabilistic baseline. Interestingly, the absolute improvement (+0.52) is higher than the one reported in Clark and Manning (2016a) using RL (+0.05) and the one using reward rescaling1 (+0.37). This suggests that our method provides a viable alternative to using RL and reward rescaling. (mi , mj ). The scoring function is defined by: #  ""  ha (mi ) uT + u0 s(ai = j) = hp (mi , mj )   T v ha (mi ) + v0 2.1 if j = i where ha (mi ) = tanh(Wa φa (mi ) + ba )"
K17-1039,D08-1068,0,0.0344271,"in Wiseman et al. (2015b) but much stronger thanks to deeper neural networks and “better mention detection, more effective, hyperparameters, and more epochs of training”. Furthermore, using reward rescaling they achieve the best performance in the literature on the English and Chinese portions of the CoNLL 2012 dataset. Our work is built upon mention ranking by turning a mentionranking model into an entity-centric one. It is worth noting that although we use the model proposed by Wiseman et al. (2015b), any mentionranking models can be employed. Entity centricity (Wellner and McCallum, 2003; Poon and Domingos, 2008; Haghighi and Klein, 2010; Ma et al., 2014a; Clark and Manning, 2016b), on the other hand, incorporates entitylevel information to solve the problem. The approach can be top-down as in Haghighi and Klein (2010) where they propose a generative model. It can also be bottom-up by merging smaller clusters into bigger ones as in Clark and Manning (2016b). The method proposed by Ma et al. (2014a) greedily and incrementally adds mentions to previously built clusters using a prune-and-score technique. Importantly, employing imitation learning these two methods can optimize the resolvers directly on e"
K17-1039,P14-2006,0,0.0431862,"Missing"
K17-1039,M95-1005,0,0.628084,"nique that computes error values for a heuristic loss function based on the reward difference between the best decision according to the current model and the decision leading to the highest metric score. 2 This slightly deviates from the definition of antecedents in linguistics (Crystal, 1997). 3 For the sake of readability, we do not explicitly mark in our notation that all the probabilities are conditioned on the document (e.g., the mentions) and dependent on model parameters. 391 2.2 Evaluation Metrics m1 We use five most popular metrics4 , mu mu+1 mi mention ... ... entity ... ... • MUC (Vilain et al., 1995), • B3 (Bagga and Baldwin, 1998), E1 • CEAFm , CEAFe (Luo, 2005), • LEA (Moosavi and Strube, 2016). for evaluation. However, because MUC is the least discriminative metric (Moosavi and Strube, 2016), whereas CEAF is slow to compute, out of the five most popular metrics we incorporate into our loss only B3 . In addition, we integrate LEA, as it has been shown to provide a good balance between discriminativity and interpretability. Let G = {G1 , G2 , ..., GN } and S = {S1 , S2 , ..., SM } be the gold-standard entity set and an entity set given by a resolver. Recall that an entity is a set of men"
K17-1041,D15-1112,0,0.738701,"Missing"
K17-1041,S15-1033,0,0.15906,"et al., 2005) as well as declarative constraints (Punyakanok et al., 2008; Roth and Yih, 2005). Neural SRL models instead exploited feature induction capabilities of neural networks, largely eliminating the need for complex hand-crafted features. Initially achieving stateof-the-art results only in the multilingual setting, where careful feature engineering is not practical (Gesmundo et al., 2009; Titov et al., 2009), neural SRL models now also outperform their traditional counterparts on standard benchmarks for English (FitzGerald et al., 2015; Roth and Lapata, 2016; Swayamdipta et al., 2016; Foland and Martin, 2015). Introduction The task of semantic role labeling (SRL), pioneered by Gildea and Jurafsky (2002), involves the prediction of predicate argument structure, i.e., both identification of arguments as well as their assignment to an underlying semantic role. These representations have been shown to be beneficial in many NLP applications, including question answering (Shen and Lapata, 2007) and information extraction (Christensen et al., 2011). Semantic banks (e.g., PropBank (Palmer et al., 2005)) often represent arguments as syntactic constituents or, more generally, text spans (Baker Recently, it"
K17-1041,W09-1205,1,0.924597,"s used for predicate makes in this sentence) and edge labels are semantic roles (e.g., A0 is a proto-agent, ‘doer’). Until recently, state-of-the-art SRL systems relied on complex sets of lexico-syntactic features (Pradhan et al., 2005) as well as declarative constraints (Punyakanok et al., 2008; Roth and Yih, 2005). Neural SRL models instead exploited feature induction capabilities of neural networks, largely eliminating the need for complex hand-crafted features. Initially achieving stateof-the-art results only in the multilingual setting, where careful feature engineering is not practical (Gesmundo et al., 2009; Titov et al., 2009), neural SRL models now also outperform their traditional counterparts on standard benchmarks for English (FitzGerald et al., 2015; Roth and Lapata, 2016; Swayamdipta et al., 2016; Foland and Martin, 2015). Introduction The task of semantic role labeling (SRL), pioneered by Gildea and Jurafsky (2002), involves the prediction of predicate argument structure, i.e., both identification of arguments as well as their assignment to an underlying semantic role. These representations have been shown to be beneficial in many NLP applications, including question answering (Shen and"
K17-1041,J02-3001,0,0.714125,". Neural SRL models instead exploited feature induction capabilities of neural networks, largely eliminating the need for complex hand-crafted features. Initially achieving stateof-the-art results only in the multilingual setting, where careful feature engineering is not practical (Gesmundo et al., 2009; Titov et al., 2009), neural SRL models now also outperform their traditional counterparts on standard benchmarks for English (FitzGerald et al., 2015; Roth and Lapata, 2016; Swayamdipta et al., 2016; Foland and Martin, 2015). Introduction The task of semantic role labeling (SRL), pioneered by Gildea and Jurafsky (2002), involves the prediction of predicate argument structure, i.e., both identification of arguments as well as their assignment to an underlying semantic role. These representations have been shown to be beneficial in many NLP applications, including question answering (Shen and Lapata, 2007) and information extraction (Christensen et al., 2011). Semantic banks (e.g., PropBank (Palmer et al., 2005)) often represent arguments as syntactic constituents or, more generally, text spans (Baker Recently, it has been shown that an accurate span-based SRL model can be constructed without relying on synta"
K17-1041,P14-1111,0,0.0452345,"Missing"
K17-1041,P98-1013,0,0.415501,"Missing"
K17-1041,W09-1201,0,0.0317489,"Missing"
K17-1041,C10-3009,0,0.41938,"Missing"
K17-1041,W08-2122,1,0.852604,"Missing"
K17-1041,W09-1206,0,0.0206759,"Missing"
K17-1041,P82-1020,0,0.792352,"Missing"
K17-1041,P15-1162,0,0.0815172,"Missing"
K17-1041,Q16-1023,0,0.11092,"sentation component that from a word wi in a sentence w build a word representation xi ; 2.3 Predicate-specific encoding As we will show in the ablation studies in Section 3, encoding a sentence with a bidirectional LSTM in one shot and using it to predict the entire semantic dependency graph does not result in competitive SRL performance. Instead, similarly to Zhou and Xu (2015), we produce predicatespecific encodings of a sentence and use them to predict arguments of the corresponding predicate. This contrasts with most other applications of LSTM encoders (for example, in syntactic parsing (Kiperwasser and Goldberg, 2016; Cross and Huang, 2016) or machine translation (Sutskever et al., 2014)), where sentences are typically encoded once and then used to predict the entire structured output (e.g., a syntactic tree or a target sentence). Specifically, when identifying arguments of a given predicate, we add a predicatespecific feature to the representation of each word in the sentence by concatenating a binary flag to the word representation of Section 2.1. The flag is set to 1 for the word corresponding to the currently considered predicate, it is set to 0 otherwise. In this way, sentences with more than one pre"
K17-1041,P16-2006,0,0.0176382,"word wi in a sentence w build a word representation xi ; 2.3 Predicate-specific encoding As we will show in the ablation studies in Section 3, encoding a sentence with a bidirectional LSTM in one shot and using it to predict the entire semantic dependency graph does not result in competitive SRL performance. Instead, similarly to Zhou and Xu (2015), we produce predicatespecific encodings of a sentence and use them to predict arguments of the corresponding predicate. This contrasts with most other applications of LSTM encoders (for example, in syntactic parsing (Kiperwasser and Goldberg, 2016; Cross and Huang, 2016) or machine translation (Sutskever et al., 2014)), where sentences are typically encoded once and then used to predict the entire structured output (e.g., a syntactic tree or a target sentence). Specifically, when identifying arguments of a given predicate, we add a predicatespecific feature to the representation of each word in the sentence by concatenating a binary flag to the word representation of Section 2.1. The flag is set to 1 for the word corresponding to the currently considered predicate, it is set to 0 otherwise. In this way, sentences with more than one predicate will be re-encode"
K17-1041,N15-1121,0,0.508069,"Missing"
K17-1041,P15-1033,0,0.0188839,"ds i and p in the sentence, so it is quite natural to exploit hidden states corresponding to both endpoints.4 Since we use predicate information within the classifier, it may seem that predicate-specific sentence encoding (Section 2.3) is not needed anymore. Moreover, predicting dependency edges relying on LSTM states of endpoints was shown effective in the context of syntactic dependency 3 Experiments We applied our model to the English, Chinese, Czech and Spanish CoNLL-2009 datasets with the standard split into training, test and development sets. For English, we used external embeddings of Dyer et al. (2015) learned using the structured skip n-gram approach of Ling et al. (2015), for Chinese, we used external embeddings produced with the neural language model of Bengio et al. (2003). For Czech and Spanish, we used embeddings created with the model proposed by Bojanowski et al. (2016). Similarly to Kiperwasser and Goldberg (2016) we used word dropout (Iyyer et al., 2015); we replaced a word with the UNK token with probabilα ity f r(w)+α , where α is an hyper-parameter and 3 Since they considered span-based SRL, they used BIO encoding (Ramshaw and Marcus, 1995) and ensured the consistency of B, I a"
K17-1041,N15-1142,0,0.0389421,"tes corresponding to both endpoints.4 Since we use predicate information within the classifier, it may seem that predicate-specific sentence encoding (Section 2.3) is not needed anymore. Moreover, predicting dependency edges relying on LSTM states of endpoints was shown effective in the context of syntactic dependency 3 Experiments We applied our model to the English, Chinese, Czech and Spanish CoNLL-2009 datasets with the standard split into training, test and development sets. For English, we used external embeddings of Dyer et al. (2015) learned using the structured skip n-gram approach of Ling et al. (2015), for Chinese, we used external embeddings produced with the neural language model of Bengio et al. (2003). For Czech and Spanish, we used embeddings created with the model proposed by Bojanowski et al. (2016). Similarly to Kiperwasser and Goldberg (2016) we used word dropout (Iyyer et al., 2015); we replaced a word with the UNK token with probabilα ity f r(w)+α , where α is an hyper-parameter and 3 Since they considered span-based SRL, they used BIO encoding (Ramshaw and Marcus, 1995) and ensured the consistency of B, I and O labels with a 1-order Markov CRF. For dependency SRL both BIO encod"
K17-1041,Q16-1037,0,0.0672347,"Missing"
K17-1041,D12-1074,0,0.0282166,"Missing"
K17-1041,J08-2002,0,0.346099,"Missing"
K17-1041,J05-1004,0,0.734425,"rd benchmarks for English (FitzGerald et al., 2015; Roth and Lapata, 2016; Swayamdipta et al., 2016; Foland and Martin, 2015). Introduction The task of semantic role labeling (SRL), pioneered by Gildea and Jurafsky (2002), involves the prediction of predicate argument structure, i.e., both identification of arguments as well as their assignment to an underlying semantic role. These representations have been shown to be beneficial in many NLP applications, including question answering (Shen and Lapata, 2007) and information extraction (Christensen et al., 2011). Semantic banks (e.g., PropBank (Palmer et al., 2005)) often represent arguments as syntactic constituents or, more generally, text spans (Baker Recently, it has been shown that an accurate span-based SRL model can be constructed without relying on syntactic features (Zhou and Xu, 2015). 411 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 411–420, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics Nevertheless, the situation with dependency-based SRL has not changed: even recent state-of-the-art methods for this task heavily rely on syntactic features ("
K17-1041,W09-1209,0,0.733772,"α is an hyper-parameter and 3 Since they considered span-based SRL, they used BIO encoding (Ramshaw and Marcus, 1995) and ensured the consistency of B, I and O labels with a 1-order Markov CRF. For dependency SRL both BIO encoding and the 1-order Markov CRF would be useless. 4 We abuse the notation and refer as p both to the predicate word and to its position in the sentence. 414 f r(w) is the frequency of the word w. The predicted POS tags were provided by the CoNLL2009 shared-task organizers. We used the same predicate disambiguator as in Roth and Lapata (2016) for English, the one used in Zhao et al. (2009) for Czech and Spanish, and the one used in Bj¨orkelund et al. (2009) for Chinese. The training objective was the categorical cross-entropy, and we optimized it with Adam (Kingma and Ba, 2015). The hyperparameter tuning and all model selection was performed on the English development set; the chosen values are shown in Table 1. System Lei et al. (2015) (local) FitzGerald et al. (2015) (local) Roth and Lapata (2016) (local) Ours (local) F1 - 86.6 - 86.7 88.1 85.3 86.7 88.7 86.8 87.7 86.9 87.3 86.0 85.0 87.7 FitzGerald et al. (2015) (ensemble) - 87.7 Roth and Lapata (2016) (ensemble) 90.3 85.7 8"
K17-1041,W05-0634,0,0.0814859,"hared tasks (Surdeanu et al., 2008; Hajic et al., 2009) popularized dependency-based semantic role labeling where the goal is to identify syntactic heads of arguments rather than entire constituents. Figure 1 shows an example of such a dependency-based representation: node labels are senses of predicates (e.g., “01” indicates that the first sense from the PropBank sense repository is used for predicate makes in this sentence) and edge labels are semantic roles (e.g., A0 is a proto-agent, ‘doer’). Until recently, state-of-the-art SRL systems relied on complex sets of lexico-syntactic features (Pradhan et al., 2005) as well as declarative constraints (Punyakanok et al., 2008; Roth and Yih, 2005). Neural SRL models instead exploited feature induction capabilities of neural networks, largely eliminating the need for complex hand-crafted features. Initially achieving stateof-the-art results only in the multilingual setting, where careful feature engineering is not practical (Gesmundo et al., 2009; Titov et al., 2009), neural SRL models now also outperform their traditional counterparts on standard benchmarks for English (FitzGerald et al., 2015; Roth and Lapata, 2016; Swayamdipta et al., 2016; Foland and Ma"
K17-1041,P15-1109,0,0.748091,"prediction of predicate argument structure, i.e., both identification of arguments as well as their assignment to an underlying semantic role. These representations have been shown to be beneficial in many NLP applications, including question answering (Shen and Lapata, 2007) and information extraction (Christensen et al., 2011). Semantic banks (e.g., PropBank (Palmer et al., 2005)) often represent arguments as syntactic constituents or, more generally, text spans (Baker Recently, it has been shown that an accurate span-based SRL model can be constructed without relying on syntactic features (Zhou and Xu, 2015). 411 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 411–420, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics Nevertheless, the situation with dependency-based SRL has not changed: even recent state-of-the-art methods for this task heavily rely on syntactic features (Roth and Lapata, 2016; FitzGerald et al., 2015; Lei et al., 2015; Roth and Woodsend, 2014; Swayamdipta et al., 2016). In particular, Roth and Lapata (2016) argue that syntactic features are necessary for the dependency-based SRL and s"
K17-1041,J08-2005,0,0.783513,"pularized dependency-based semantic role labeling where the goal is to identify syntactic heads of arguments rather than entire constituents. Figure 1 shows an example of such a dependency-based representation: node labels are senses of predicates (e.g., “01” indicates that the first sense from the PropBank sense repository is used for predicate makes in this sentence) and edge labels are semantic roles (e.g., A0 is a proto-agent, ‘doer’). Until recently, state-of-the-art SRL systems relied on complex sets of lexico-syntactic features (Pradhan et al., 2005) as well as declarative constraints (Punyakanok et al., 2008; Roth and Yih, 2005). Neural SRL models instead exploited feature induction capabilities of neural networks, largely eliminating the need for complex hand-crafted features. Initially achieving stateof-the-art results only in the multilingual setting, where careful feature engineering is not practical (Gesmundo et al., 2009; Titov et al., 2009), neural SRL models now also outperform their traditional counterparts on standard benchmarks for English (FitzGerald et al., 2015; Roth and Lapata, 2016; Swayamdipta et al., 2016; Foland and Martin, 2015). Introduction The task of semantic role labeling"
K17-1041,W95-0107,0,0.0705039,"For English, we used external embeddings of Dyer et al. (2015) learned using the structured skip n-gram approach of Ling et al. (2015), for Chinese, we used external embeddings produced with the neural language model of Bengio et al. (2003). For Czech and Spanish, we used embeddings created with the model proposed by Bojanowski et al. (2016). Similarly to Kiperwasser and Goldberg (2016) we used word dropout (Iyyer et al., 2015); we replaced a word with the UNK token with probabilα ity f r(w)+α , where α is an hyper-parameter and 3 Since they considered span-based SRL, they used BIO encoding (Ramshaw and Marcus, 1995) and ensured the consistency of B, I and O labels with a 1-order Markov CRF. For dependency SRL both BIO encoding and the 1-order Markov CRF would be useless. 4 We abuse the notation and refer as p both to the predicate word and to its position in the sentence. 414 f r(w) is the frequency of the word w. The predicted POS tags were provided by the CoNLL2009 shared-task organizers. We used the same predicate disambiguator as in Roth and Lapata (2016) for English, the one used in Zhao et al. (2009) for Czech and Spanish, and the one used in Bj¨orkelund et al. (2009) for Chinese. The training obje"
K17-1041,P16-1113,0,0.652995,"mplex sets of lexico-syntactic features (Pradhan et al., 2005) as well as declarative constraints (Punyakanok et al., 2008; Roth and Yih, 2005). Neural SRL models instead exploited feature induction capabilities of neural networks, largely eliminating the need for complex hand-crafted features. Initially achieving stateof-the-art results only in the multilingual setting, where careful feature engineering is not practical (Gesmundo et al., 2009; Titov et al., 2009), neural SRL models now also outperform their traditional counterparts on standard benchmarks for English (FitzGerald et al., 2015; Roth and Lapata, 2016; Swayamdipta et al., 2016; Foland and Martin, 2015). Introduction The task of semantic role labeling (SRL), pioneered by Gildea and Jurafsky (2002), involves the prediction of predicate argument structure, i.e., both identification of arguments as well as their assignment to an underlying semantic role. These representations have been shown to be beneficial in many NLP applications, including question answering (Shen and Lapata, 2007) and information extraction (Christensen et al., 2011). Semantic banks (e.g., PropBank (Palmer et al., 2005)) often represent arguments as syntactic constituents"
K17-1041,D14-1045,0,0.0200821,"Missing"
K17-1041,D07-1002,0,0.105903,"al., 2009; Titov et al., 2009), neural SRL models now also outperform their traditional counterparts on standard benchmarks for English (FitzGerald et al., 2015; Roth and Lapata, 2016; Swayamdipta et al., 2016; Foland and Martin, 2015). Introduction The task of semantic role labeling (SRL), pioneered by Gildea and Jurafsky (2002), involves the prediction of predicate argument structure, i.e., both identification of arguments as well as their assignment to an underlying semantic role. These representations have been shown to be beneficial in many NLP applications, including question answering (Shen and Lapata, 2007) and information extraction (Christensen et al., 2011). Semantic banks (e.g., PropBank (Palmer et al., 2005)) often represent arguments as syntactic constituents or, more generally, text spans (Baker Recently, it has been shown that an accurate span-based SRL model can be constructed without relying on syntactic features (Zhou and Xu, 2015). 411 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 411–420, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics Nevertheless, the situation with dependency-based"
K17-1041,K16-1019,0,0.453786,"ntactic features (Pradhan et al., 2005) as well as declarative constraints (Punyakanok et al., 2008; Roth and Yih, 2005). Neural SRL models instead exploited feature induction capabilities of neural networks, largely eliminating the need for complex hand-crafted features. Initially achieving stateof-the-art results only in the multilingual setting, where careful feature engineering is not practical (Gesmundo et al., 2009; Titov et al., 2009), neural SRL models now also outperform their traditional counterparts on standard benchmarks for English (FitzGerald et al., 2015; Roth and Lapata, 2016; Swayamdipta et al., 2016; Foland and Martin, 2015). Introduction The task of semantic role labeling (SRL), pioneered by Gildea and Jurafsky (2002), involves the prediction of predicate argument structure, i.e., both identification of arguments as well as their assignment to an underlying semantic role. These representations have been shown to be beneficial in many NLP applications, including question answering (Shen and Lapata, 2007) and information extraction (Christensen et al., 2011). Semantic banks (e.g., PropBank (Palmer et al., 2005)) often represent arguments as syntactic constituents or, more generally, text"
K17-1041,C98-1013,0,\N,Missing
N15-1001,P09-1004,0,0.0181926,"ion of arguments and assignment of labels according to their underlying semantic role. For example, in the following sentences: (a) [Agent Mary] opened [P atient the door]. (b) [P atient The door] opened. (c) [P atient The door] was opened [Agent by Mary]. Mary always takes an agent role for the predicate open, and door is always a patient. In this work we focus on the labeling stage of semantic role labeling. Identification, though an important problem, can be tackled with heuristics (Lang and Lapata, 2011a; Grenager and Manning, 2006; de Marneffe et al., 2006), with unsupervised techniques (Abend et al., 2009) or potentially by using a supervised classifier trained on a small amount of data. 3 Approach At the core of our approach is a statistical model encoding an interdependence between a semantic role structure and its realization in a sentence. In the unsupervised learning setting, sentences, their syntactic representations and argument positions (denoted by x) are observable whereas the associated semantic roles r are latent and need to be induced by the model. The idea which underlines much of latent variable modeling is that a good latent representation is the one which helps us to reconstruc"
N15-1001,P98-1013,0,0.376071,"ntly to minimize errors in argument reconstruction, the induced roles largely correspond to roles defined in annotated resources. Our method performs on par with most accurate role induction methods on English and German, even though, unlike these previous approaches, we do not incorporate any prior linguistic knowledge about the languages. 1 Introduction Shallow semantic representations, and semantic role labels in particular, have a long history in linguistics (Fillmore, 1968). More recently, with an emergence of large annotated resources such as PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998), automatic semantic role labeling (SRL) has attracted a lot of attention (Gildea and Jurafsky, 2002; Carreras and M`arquez, 2005; Surdeanu et al., 2008; Hajiˇc et al., 2009; Das et al., 2010). Semantic role representations encode the underlying predicate-argument structure of sentences, or, more specifically, for every predicate in a sentence they identify a set of arguments and associate each argument with an underlying semantic role, such Most current statistical approaches to SRL are supervised, requiring large quantities of human annotated data to estimate model parameters. However, such"
N15-1001,burchardt-etal-2006-salsa,0,0.0309327,"for model development. In our experiments, we relied on gold standard syntax and gold standard argument identification, as this set-up allows us to evaluate against much of the previous work. We refer the reader to Lang and Lapata (2010) for details of the experimental set-up. There has not been much work on unsupervised induction of roles for languages other than English, perhaps primarily because of the above-mentioned model limitations. For German, we replicate the set-up considered in Titov and Klementiev (2012b). They used the CoNLL 2009 version (Hajiˇc et al., 2009) of the SALSA corpus (Burchardt et al., 2006). Instead of using syntactic parses provided in the CoNLL dataset, they re-parsed it with the MALT dependency parser (Nivre et al., 2004). Similarly, rather than relying on gold standard annotations for argument identification, they used a supervised classifier to predict argument positions. Details of the preprocessing can be found in Titov and Klementiev (2012b). As in most previous work on unsupervised SRL, we evaluate our model using purity, collocation and their harmonic mean F1. Purity (PU) measures the 6 PU = 1 X max |Gj ∩ Ci | j N i CO = 1 X max |Gj ∩ Ci | i N j We compute the aggregat"
N15-1001,W05-0620,0,0.174304,"Missing"
N15-1001,P11-1144,0,0.0199916,"lyannotated resources. One natural direction is semi-supervised role labeling, where both annotated and unannotated data is used to estimate a model. Previous semisupervised approaches to SRL can be mostly regarded as extensions to supervised learning by either incorporating word features induced from unnannoted texts (Collobert and Weston, 2008; Deschacht and Moens, 2009) or creating some form of ‘surrogate’ supervision (He and Gildea, 2006; F¨urstenau and Lapata, 2009). Benefits from using unlabeled data were moderate, and more significant for the harder SRL version, frame-semantic parsing (Das and Smith, 2011). 8 Another important direction includes crosslingual approaches (Pado and Lapata, 2009; van der Plas et al., 2011; Kozhevnikov and Titov, 2013) which leverage resources from resource-rich languages, as well as parallel data, to produce annotation or models for resource-poor languages. However, both translation shifts and noise in word alignments harm the performance of cross-lingual methods. Nevertheless, even joint unsupervised induction across languages appears to be beneficial (Titov and Klementiev, 2012b). Unsupervised learning has also been one of the central paradigms for the closely-re"
N15-1001,N10-1138,0,0.253008,"hods on English and German, even though, unlike these previous approaches, we do not incorporate any prior linguistic knowledge about the languages. 1 Introduction Shallow semantic representations, and semantic role labels in particular, have a long history in linguistics (Fillmore, 1968). More recently, with an emergence of large annotated resources such as PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998), automatic semantic role labeling (SRL) has attracted a lot of attention (Gildea and Jurafsky, 2002; Carreras and M`arquez, 2005; Surdeanu et al., 2008; Hajiˇc et al., 2009; Das et al., 2010). Semantic role representations encode the underlying predicate-argument structure of sentences, or, more specifically, for every predicate in a sentence they identify a set of arguments and associate each argument with an underlying semantic role, such Most current statistical approaches to SRL are supervised, requiring large quantities of human annotated data to estimate model parameters. However, such resources are expensive to create and only available for a small number of languages. Moreover, when moved to a new domain (e.g., from news corpora to blogs or biomedical texts), the performan"
N15-1001,de-marneffe-etal-2006-generating,0,0.0483274,"Missing"
N15-1001,D09-1003,0,0.0236433,"higher collocation scores. 5 Additional Related Work In recent years, unsupervised approaches to semantic role induction have attracted considerable attention. However, there exist other ways to address lack of coverage provided by existing semanticallyannotated resources. One natural direction is semi-supervised role labeling, where both annotated and unannotated data is used to estimate a model. Previous semisupervised approaches to SRL can be mostly regarded as extensions to supervised learning by either incorporating word features induced from unnannoted texts (Collobert and Weston, 2008; Deschacht and Moens, 2009) or creating some form of ‘surrogate’ supervision (He and Gildea, 2006; F¨urstenau and Lapata, 2009). Benefits from using unlabeled data were moderate, and more significant for the harder SRL version, frame-semantic parsing (Das and Smith, 2011). 8 Another important direction includes crosslingual approaches (Pado and Lapata, 2009; van der Plas et al., 2011; Kozhevnikov and Titov, 2013) which leverage resources from resource-rich languages, as well as parallel data, to produce annotation or models for resource-poor languages. However, both translation shifts and noise in word alignments harm t"
N15-1001,erk-pado-2006-shalmaneser,0,0.0315718,"family of joint models p(x, y|θ) and estimate the parameters θ by, for example, maximizing the likelihood. Generative models of semantics (Titov and Klementiev, 2012a; Titov and Klementiev, 2011; Modi et al., 2012; O’Connor, 2013; Kawahara et al., 2014) necessarily make very strong independence assumptions (e.g., arguments are conditionally independent of each other given the predicate) and use simplistic features of x and y. Thus, they cannot meet the desiderata stated above. Importantly, they are also much more simplistic in their assumptions than state-of-the-art supervised role labelers (Erk and Pado, 2006; Johansson and Nugues, 2008; Das et al., 2010). 3.2 Reconstruction error minimization Generative modeling is not the only way to learn latent representations. One alternative, popular in the neural network community, is to instead use autoencoders and optimize the reconstruction error (Hinton, 1989; Vincent et al., 2008). In autoencoders, a latent representation y (their hidden layer) is predicted from x by an encoding model and then this y is used to recover x ˜ with a reconstruction model (see Figure 1(a)). Parameters of the encoding and reconstruction components are chosen so as to minimiz"
N15-1001,D09-1002,0,0.0465352,"Missing"
N15-1001,S12-1026,0,0.0258267,"Missing"
N15-1001,P11-2051,0,0.0648136,"Missing"
N15-1001,P12-2029,0,0.168109,"ate model parameters. However, such resources are expensive to create and only available for a small number of languages. Moreover, when moved to a new domain (e.g., from news corpora to blogs or biomedical texts), the performance of these models tends to degrade substantially (Pradhan et al., 2008). The scarcity of annotated data has motivated the research into unsupervised learning of semantic representations (Swier and Stevenson, 2004; Grenager and Manning, 2006; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2012a; F¨urstenau and Rambow, 2012; Garg and Henderson, 2012). The existing methods have a number of serious shortcomings. First, they make very strong assumptions, for example, assuming that arguments are conditionally independent of each other given the predicate. Second, unlike state-ofthe-art supervised parsers, they rely on a very simplistic set of features of a sentence. These factors lead to models being insufficiently expressive to capture the syntax-semantics interface, inadequate 1 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1–10, c Denver, Colorado, May 31 – June 5, 2015. 2015 Associ"
N15-1001,W06-1601,0,0.554584,"with an underlying semantic role, such Most current statistical approaches to SRL are supervised, requiring large quantities of human annotated data to estimate model parameters. However, such resources are expensive to create and only available for a small number of languages. Moreover, when moved to a new domain (e.g., from news corpora to blogs or biomedical texts), the performance of these models tends to degrade substantially (Pradhan et al., 2008). The scarcity of annotated data has motivated the research into unsupervised learning of semantic representations (Swier and Stevenson, 2004; Grenager and Manning, 2006; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2012a; F¨urstenau and Rambow, 2012; Garg and Henderson, 2012). The existing methods have a number of serious shortcomings. First, they make very strong assumptions, for example, assuming that arguments are conditionally independent of each other given the predicate. Second, unlike state-ofthe-art supervised parsers, they rely on a very simplistic set of features of a sentence. These factors lead to models being insufficiently expressive to capture the syntax-semantics interface, inadequate 1 Human La"
N15-1001,W09-1201,0,0.0897965,"Missing"
N15-1001,W08-2123,0,0.18867,"els p(x, y|θ) and estimate the parameters θ by, for example, maximizing the likelihood. Generative models of semantics (Titov and Klementiev, 2012a; Titov and Klementiev, 2011; Modi et al., 2012; O’Connor, 2013; Kawahara et al., 2014) necessarily make very strong independence assumptions (e.g., arguments are conditionally independent of each other given the predicate) and use simplistic features of x and y. Thus, they cannot meet the desiderata stated above. Importantly, they are also much more simplistic in their assumptions than state-of-the-art supervised role labelers (Erk and Pado, 2006; Johansson and Nugues, 2008; Das et al., 2010). 3.2 Reconstruction error minimization Generative modeling is not the only way to learn latent representations. One alternative, popular in the neural network community, is to instead use autoencoders and optimize the reconstruction error (Hinton, 1989; Vincent et al., 2008). In autoencoders, a latent representation y (their hidden layer) is predicted from x by an encoding model and then this y is used to recover x ˜ with a reconstruction model (see Figure 1(a)). Parameters of the encoding and reconstruction components are chosen so as to minimize some form of the reconstru"
N15-1001,W07-1206,0,0.0641984,"Missing"
N15-1001,E14-1007,0,0.0126049,"ndproduct of learning: it will be used to process new sentences, and it will be compared to existing methods in our evaluation. 3.1 Shortcomings of generative modeling The above paragraph can be regarded as our desiderata; now we discuss how to achieve them. The standard way to approach latent variable modeling is to use the generative framework: that is to define a family of joint models p(x, y|θ) and estimate the parameters θ by, for example, maximizing the likelihood. Generative models of semantics (Titov and Klementiev, 2012a; Titov and Klementiev, 2011; Modi et al., 2012; O’Connor, 2013; Kawahara et al., 2014) necessarily make very strong independence assumptions (e.g., arguments are conditionally independent of each other given the predicate) and use simplistic features of x and y. Thus, they cannot meet the desiderata stated above. Importantly, they are also much more simplistic in their assumptions than state-of-the-art supervised role labelers (Erk and Pado, 2006; Johansson and Nugues, 2008; Das et al., 2010). 3.2 Reconstruction error minimization Generative modeling is not the only way to learn latent representations. One alternative, popular in the neural network community, is to instead use"
N15-1001,P13-1117,1,0.848885,"mate a model. Previous semisupervised approaches to SRL can be mostly regarded as extensions to supervised learning by either incorporating word features induced from unnannoted texts (Collobert and Weston, 2008; Deschacht and Moens, 2009) or creating some form of ‘surrogate’ supervision (He and Gildea, 2006; F¨urstenau and Lapata, 2009). Benefits from using unlabeled data were moderate, and more significant for the harder SRL version, frame-semantic parsing (Das and Smith, 2011). 8 Another important direction includes crosslingual approaches (Pado and Lapata, 2009; van der Plas et al., 2011; Kozhevnikov and Titov, 2013) which leverage resources from resource-rich languages, as well as parallel data, to produce annotation or models for resource-poor languages. However, both translation shifts and noise in word alignments harm the performance of cross-lingual methods. Nevertheless, even joint unsupervised induction across languages appears to be beneficial (Titov and Klementiev, 2012b). Unsupervised learning has also been one of the central paradigms for the closely-related area of relation extraction (RE), where several techniques have been proposed to cluster semantically similar verbalizations of relations"
N15-1001,N10-1137,0,0.594713,"role, such Most current statistical approaches to SRL are supervised, requiring large quantities of human annotated data to estimate model parameters. However, such resources are expensive to create and only available for a small number of languages. Moreover, when moved to a new domain (e.g., from news corpora to blogs or biomedical texts), the performance of these models tends to degrade substantially (Pradhan et al., 2008). The scarcity of annotated data has motivated the research into unsupervised learning of semantic representations (Swier and Stevenson, 2004; Grenager and Manning, 2006; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2012a; F¨urstenau and Rambow, 2012; Garg and Henderson, 2012). The existing methods have a number of serious shortcomings. First, they make very strong assumptions, for example, assuming that arguments are conditionally independent of each other given the predicate. Second, unlike state-ofthe-art supervised parsers, they rely on a very simplistic set of features of a sentence. These factors lead to models being insufficiently expressive to capture the syntax-semantics interface, inadequate 1 Human Language Technologies: Th"
N15-1001,P11-1112,0,0.373135,"statistical approaches to SRL are supervised, requiring large quantities of human annotated data to estimate model parameters. However, such resources are expensive to create and only available for a small number of languages. Moreover, when moved to a new domain (e.g., from news corpora to blogs or biomedical texts), the performance of these models tends to degrade substantially (Pradhan et al., 2008). The scarcity of annotated data has motivated the research into unsupervised learning of semantic representations (Swier and Stevenson, 2004; Grenager and Manning, 2006; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2012a; F¨urstenau and Rambow, 2012; Garg and Henderson, 2012). The existing methods have a number of serious shortcomings. First, they make very strong assumptions, for example, assuming that arguments are conditionally independent of each other given the predicate. Second, unlike state-ofthe-art supervised parsers, they rely on a very simplistic set of features of a sentence. These factors lead to models being insufficiently expressive to capture the syntax-semantics interface, inadequate 1 Human Language Technologies: The 2015 Annual Conferenc"
N15-1001,D11-1122,0,0.203984,"statistical approaches to SRL are supervised, requiring large quantities of human annotated data to estimate model parameters. However, such resources are expensive to create and only available for a small number of languages. Moreover, when moved to a new domain (e.g., from news corpora to blogs or biomedical texts), the performance of these models tends to degrade substantially (Pradhan et al., 2008). The scarcity of annotated data has motivated the research into unsupervised learning of semantic representations (Swier and Stevenson, 2004; Grenager and Manning, 2006; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2012a; F¨urstenau and Rambow, 2012; Garg and Henderson, 2012). The existing methods have a number of serious shortcomings. First, they make very strong assumptions, for example, assuming that arguments are conditionally independent of each other given the predicate. Second, unlike state-ofthe-art supervised parsers, they rely on a very simplistic set of features of a sentence. These factors lead to models being insufficiently expressive to capture the syntax-semantics interface, inadequate 1 Human Language Technologies: The 2015 Annual Conferenc"
N15-1001,J14-3006,0,0.565638,"Missing"
N15-1001,C10-1081,0,0.0594468,"Missing"
N15-1001,W12-1901,1,0.808468,"nts. The role labeler will be the endproduct of learning: it will be used to process new sentences, and it will be compared to existing methods in our evaluation. 3.1 Shortcomings of generative modeling The above paragraph can be regarded as our desiderata; now we discuss how to achieve them. The standard way to approach latent variable modeling is to use the generative framework: that is to define a family of joint models p(x, y|θ) and estimate the parameters θ by, for example, maximizing the likelihood. Generative models of semantics (Titov and Klementiev, 2012a; Titov and Klementiev, 2011; Modi et al., 2012; O’Connor, 2013; Kawahara et al., 2014) necessarily make very strong independence assumptions (e.g., arguments are conditionally independent of each other given the predicate) and use simplistic features of x and y. Thus, they cannot meet the desiderata stated above. Importantly, they are also much more simplistic in their assumptions than state-of-the-art supervised role labelers (Erk and Pado, 2006; Johansson and Nugues, 2008; Das et al., 2010). 3.2 Reconstruction error minimization Generative modeling is not the only way to learn latent representations. One alternative, popular in the neur"
N15-1001,W04-2407,0,0.0151906,"us to evaluate against much of the previous work. We refer the reader to Lang and Lapata (2010) for details of the experimental set-up. There has not been much work on unsupervised induction of roles for languages other than English, perhaps primarily because of the above-mentioned model limitations. For German, we replicate the set-up considered in Titov and Klementiev (2012b). They used the CoNLL 2009 version (Hajiˇc et al., 2009) of the SALSA corpus (Burchardt et al., 2006). Instead of using syntactic parses provided in the CoNLL dataset, they re-parsed it with the MALT dependency parser (Nivre et al., 2004). Similarly, rather than relying on gold standard annotations for argument identification, they used a supervised classifier to predict argument positions. Details of the preprocessing can be found in Titov and Klementiev (2012b). As in most previous work on unsupervised SRL, we evaluate our model using purity, collocation and their harmonic mean F1. Purity (PU) measures the 6 PU = 1 X max |Gj ∩ Ci | j N i CO = 1 X max |Gj ∩ Ci | i N j We compute the aggregate PU, CO, and F1 scores over all predicates in the same way as Lang and Lapata (2010): we weight the scores for each predicate by the num"
N15-1001,J05-1004,0,0.680454,"en the components are estimated jointly to minimize errors in argument reconstruction, the induced roles largely correspond to roles defined in annotated resources. Our method performs on par with most accurate role induction methods on English and German, even though, unlike these previous approaches, we do not incorporate any prior linguistic knowledge about the languages. 1 Introduction Shallow semantic representations, and semantic role labels in particular, have a long history in linguistics (Fillmore, 1968). More recently, with an emergence of large annotated resources such as PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998), automatic semantic role labeling (SRL) has attracted a lot of attention (Gildea and Jurafsky, 2002; Carreras and M`arquez, 2005; Surdeanu et al., 2008; Hajiˇc et al., 2009; Das et al., 2010). Semantic role representations encode the underlying predicate-argument structure of sentences, or, more specifically, for every predicate in a sentence they identify a set of arguments and associate each argument with an underlying semantic role, such Most current statistical approaches to SRL are supervised, requiring large quantities of human annotated data to estimat"
N15-1001,D14-1162,0,0.0801379,"ater) cannon. On the contrary, if the Patient is cat then the Agent is more likely to be dog than police. In turn, the dot product (Cv,ri uai )T Cv,rj uaj is large if these expectations are met for the argument pair (ai , aj ), and small otherwise. Intuitively, this objective corresponds to scoring argument tuples according to X T h(a, r, v, C, u) = uTai Cv,r C u , (2) i v,rj aj i6=j hinting at connections to (coupled) tensor and matrix factorization methods (Nickel et al., 2011; Yılmaz et al., 2011; Bordes et al., 2011; Riedel et al., 2013) and distributional semantics (Mikolov et al., 2013; Pennington et al., 2014). Note also that the reconstruction model does not have access to any features of the sentence (e.g., argument order or syntax), forcing the roles to convey all the necessary information. This factorization can be thought of as a generalization of the notion of selection preferences. Selectional preferences characterize the set of arguments licensed for a given role of a given predicate: for example, Agent for the predicate charge can be police or dog but not table or idea. In our generalization, we model soft restrictions imposed not only by the role itself but also by other arguments and the"
N15-1001,J08-2006,0,0.108652,"ng predicate-argument structure of sentences, or, more specifically, for every predicate in a sentence they identify a set of arguments and associate each argument with an underlying semantic role, such Most current statistical approaches to SRL are supervised, requiring large quantities of human annotated data to estimate model parameters. However, such resources are expensive to create and only available for a small number of languages. Moreover, when moved to a new domain (e.g., from news corpora to blogs or biomedical texts), the performance of these models tends to degrade substantially (Pradhan et al., 2008). The scarcity of annotated data has motivated the research into unsupervised learning of semantic representations (Swier and Stevenson, 2004; Grenager and Manning, 2006; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2012a; F¨urstenau and Rambow, 2012; Garg and Henderson, 2012). The existing methods have a number of serious shortcomings. First, they make very strong assumptions, for example, assuming that arguments are conditionally independent of each other given the predicate. Second, unlike state-ofthe-art supervised parsers, they rely on a ver"
N15-1001,N13-1008,0,0.04557,"lice, and the role Instrument is filled by the word baton or perhaps (a water) cannon. On the contrary, if the Patient is cat then the Agent is more likely to be dog than police. In turn, the dot product (Cv,ri uai )T Cv,rj uaj is large if these expectations are met for the argument pair (ai , aj ), and small otherwise. Intuitively, this objective corresponds to scoring argument tuples according to X T h(a, r, v, C, u) = uTai Cv,r C u , (2) i v,rj aj i6=j hinting at connections to (coupled) tensor and matrix factorization methods (Nickel et al., 2011; Yılmaz et al., 2011; Bordes et al., 2011; Riedel et al., 2013) and distributional semantics (Mikolov et al., 2013; Pennington et al., 2014). Note also that the reconstruction model does not have access to any features of the sentence (e.g., argument order or syntax), forcing the roles to convey all the necessary information. This factorization can be thought of as a generalization of the notion of selection preferences. Selectional preferences characterize the set of arguments licensed for a given role of a given predicate: for example, Agent for the predicate charge can be police or dog but not table or idea. In our generalization, we model soft restric"
N15-1001,D07-1002,0,0.0776223,"Missing"
N15-1001,W08-2121,0,0.206963,"Missing"
N15-1001,P11-1145,1,0.884489,"errors in recovering arguments. The role labeler will be the endproduct of learning: it will be used to process new sentences, and it will be compared to existing methods in our evaluation. 3.1 Shortcomings of generative modeling The above paragraph can be regarded as our desiderata; now we discuss how to achieve them. The standard way to approach latent variable modeling is to use the generative framework: that is to define a family of joint models p(x, y|θ) and estimate the parameters θ by, for example, maximizing the likelihood. Generative models of semantics (Titov and Klementiev, 2012a; Titov and Klementiev, 2011; Modi et al., 2012; O’Connor, 2013; Kawahara et al., 2014) necessarily make very strong independence assumptions (e.g., arguments are conditionally independent of each other given the predicate) and use simplistic features of x and y. Thus, they cannot meet the desiderata stated above. Importantly, they are also much more simplistic in their assumptions than state-of-the-art supervised role labelers (Erk and Pado, 2006; Johansson and Nugues, 2008; Das et al., 2010). 3.2 Reconstruction error minimization Generative modeling is not the only way to learn latent representations. One alternative,"
N15-1001,E12-1003,1,0.810652,"equiring large quantities of human annotated data to estimate model parameters. However, such resources are expensive to create and only available for a small number of languages. Moreover, when moved to a new domain (e.g., from news corpora to blogs or biomedical texts), the performance of these models tends to degrade substantially (Pradhan et al., 2008). The scarcity of annotated data has motivated the research into unsupervised learning of semantic representations (Swier and Stevenson, 2004; Grenager and Manning, 2006; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2012a; F¨urstenau and Rambow, 2012; Garg and Henderson, 2012). The existing methods have a number of serious shortcomings. First, they make very strong assumptions, for example, assuming that arguments are conditionally independent of each other given the predicate. Second, unlike state-ofthe-art supervised parsers, they rely on a very simplistic set of features of a sentence. These factors lead to models being insufficiently expressive to capture the syntax-semantics interface, inadequate 1 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1–1"
N15-1001,P12-1068,1,0.705929,"equiring large quantities of human annotated data to estimate model parameters. However, such resources are expensive to create and only available for a small number of languages. Moreover, when moved to a new domain (e.g., from news corpora to blogs or biomedical texts), the performance of these models tends to degrade substantially (Pradhan et al., 2008). The scarcity of annotated data has motivated the research into unsupervised learning of semantic representations (Swier and Stevenson, 2004; Grenager and Manning, 2006; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2012a; F¨urstenau and Rambow, 2012; Garg and Henderson, 2012). The existing methods have a number of serious shortcomings. First, they make very strong assumptions, for example, assuming that arguments are conditionally independent of each other given the predicate. Second, unlike state-ofthe-art supervised parsers, they rely on a very simplistic set of features of a sentence. These factors lead to models being insufficiently expressive to capture the syntax-semantics interface, inadequate 1 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1–1"
N15-1001,N09-2032,0,0.0246087,"Missing"
N15-1001,P11-2052,0,0.059039,"Missing"
N15-1001,N09-2004,0,0.0851416,"Missing"
N15-1001,D11-1135,0,0.042755,"h languages, as well as parallel data, to produce annotation or models for resource-poor languages. However, both translation shifts and noise in word alignments harm the performance of cross-lingual methods. Nevertheless, even joint unsupervised induction across languages appears to be beneficial (Titov and Klementiev, 2012b). Unsupervised learning has also been one of the central paradigms for the closely-related area of relation extraction (RE), where several techniques have been proposed to cluster semantically similar verbalizations of relations (Lin and Pantel, 2001; Banko et al., 2007; Yao et al., 2011). Similarly to SRL, unsupervised methods for RE mostly rely on generative modeling and agglomerative clustering. From the learning perspective, methods which use the reconstruction-error objective to estimate linear models (Ammar et al., 2014; Daum´e III, 2009) are certainly related. However, they do not consider learning factorization models, and they also do not deal with semantics. Tensor factorization methods used in the context of modeling knoweldge bases (e.g., (Bordes et al., 2011)) are also close in spirit. However, they do not deal with inducing semantics but rather factorize existing"
N15-1001,N07-1070,0,\N,Missing
N15-1001,C98-1013,0,\N,Missing
N15-1001,J02-3001,0,\N,Missing
N16-1160,N12-1095,0,0.104572,"Multilingual models. The research on using multilingual information in the learning of multi-sense embedding models is scarce. Guo et al. (2014) perform a sense induction step based on clustering translations prior to learning word embeddings. Once the translations are clustered, they are mapped to a source corpus using WSD heuristics, after which a recurrent neural network is trained to obtain sense-specific representations. Unlike in our work, the sense induction and embedding learning components are entirely separated, without a possibility for one to influence another. In a similar vein, Bansal et al. (2012) use bilingual corpora to perform soft word clustering, extending the previous work on the monolingual case of Lin and Wu (2009). Single-sense representations in the multilingual context have been studied more extensively (Lu et al., 2015; Faruqui and Dyer, 2014b; Hill et al., 2014a; Zhang et al., 2014; Faruqui and Dyer, 2013; Zou et al., 2013), with a goal of bringing the representations in the same semantic space. A related line of work concerns the crosslingual setting, where one tries to leverage training data in one language to build models for typically lower-resource languages (Hermann"
N16-1160,P14-1023,0,0.0189679,"s based on the chosen sense. The two components are estimated jointly. We observe that the word representations induced from bilingual data outperform the monolingual counterparts across a range of evaluation tasks, even though crosslingual information is not available at test time. 1 Introduction Approaches to learning word embeddings (i.e. realvalued vectors) relying on word context have received much attention in recent years, and the induced representations have been shown to capture syntactic and semantic properties of words. They have been evaluated intrinsically (Mikolov et al., 2013a; Baroni et al., 2014; Levy and Goldberg, 2014) and have also been used in concrete NLP applications to deal with word sparsity and improve generalization (Turian et al., 2010; Collobert et al., 2011; Bansal et al., 2014; Passos et al., 2014). While most work to date has focused on developing embedding models which represent a word with a single vector, some researchers have attempted to capture polysemy explicitly and have encoded properties of each word with multiple vectors (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Chen et al., 2014; Li and Jurafsky, 2015). Gertjan van Noord University o"
N16-1160,P91-1034,0,0.563356,"m another language can also be leveraged to yield better firstlanguage embeddings (Guo et al., 2014). Our paper falls in the latter, much less explored category. We adhere to the view of multilingual learning as a means of language grounding (Faruqui and Dyer, 2014b; Zou et al., 2013; Titov and Klementiev, 2012; Snyder and Barzilay, 2010; Naseem et al., 2009). Intuitively, polysemy in one language can be at least partially resolved by looking at the translation of the word and its context in another language (Kaji, 2003; Ng et al., 2003; Diab and Resnik, 2002; Ide, 2000; Dagan and Itai, 1994; Brown et al., 1991). Better sense assignment can then lead to better sense-specific word embeddings. We propose a model that uses second-language embeddings as a supervisory signal in learning multisense representations in the first language. This supervision is easy to obtain for many language pairs as numerous parallel corpora exist nowadays. Our model, which can be seen as an autoencoder with a discrete hidden layer encoding word senses, leverages bilingual data in its encoding part, while the decoder predicts the surrounding words relying on the 1346 Proceedings of NAACL-HLT 2016, pages 1346–1356, c San Dieg"
N16-1160,P12-1015,0,0.0297571,"n step based on the sentential context provided for each word in the pair. The other benchmarks we use provide the ratings for the word pairs without context. WS-353 contains 353 human-rated word pairs (Finkelstein et al., 2001), while Agirre et al. (2009) separate this benchmark for similarity (WS-SIM) and relatedness (WS-REL). The RG-65 (Rubenstein and Goodenough, 1965) and the MC-30 (Miller and Charles, 1991) benchmarks contain nouns only. The MTurk-287 (Radinsky et al., 2011) and MTurk-771 (Halawi et al., 2012) include word pairs whose similarity was crowdsourced from AMT. Similarly, MEN (Bruni et al., 2012) is an AMT-annotated dataset of 3000 word pairs. The YP130 (Yang and Powers, 2006) and Verb-143 (Baker et al., 2014) measure verb similarity. Rare-Word (Luong et al., 2013) contains 2034 rare-word pairs. Finally, SimLex-999 (Hill et al., 2014b) is intended to measure pure similarity as opposed to relatedness. For these benchmarks, we prepare the word representations by taking a uniform average of all sense embeddings per word. The evaluation is carried out using the tool described in Faruqui and Dyer (2014a). Due to space constraints, we report the results by averaging over all benchmarks (Sim"
N16-1160,D14-1110,0,0.147602,"been evaluated intrinsically (Mikolov et al., 2013a; Baroni et al., 2014; Levy and Goldberg, 2014) and have also been used in concrete NLP applications to deal with word sparsity and improve generalization (Turian et al., 2010; Collobert et al., 2011; Bansal et al., 2014; Passos et al., 2014). While most work to date has focused on developing embedding models which represent a word with a single vector, some researchers have attempted to capture polysemy explicitly and have encoded properties of each word with multiple vectors (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Chen et al., 2014; Li and Jurafsky, 2015). Gertjan van Noord University of Groningen Netherlands g.j.m.van.noord@rug.nl In parallel to this work on multi-sense word embeddings, another line of research has investigated integrating multilingual data, with largely two distinct goals in mind. The first goal has been to obtain representations for several languages in the same semantic space, which then enables the transfer of a model (e.g., a syntactic parser) trained on annotated training data in one language to another language lacking this annotation (Klementiev et al., 2012; Hermann and Blunsom, 2014; Gouws et"
N16-1160,J81-4005,0,0.699143,"Missing"
N16-1160,J94-4003,0,0.27129,"ondly, information from another language can also be leveraged to yield better firstlanguage embeddings (Guo et al., 2014). Our paper falls in the latter, much less explored category. We adhere to the view of multilingual learning as a means of language grounding (Faruqui and Dyer, 2014b; Zou et al., 2013; Titov and Klementiev, 2012; Snyder and Barzilay, 2010; Naseem et al., 2009). Intuitively, polysemy in one language can be at least partially resolved by looking at the translation of the word and its context in another language (Kaji, 2003; Ng et al., 2003; Diab and Resnik, 2002; Ide, 2000; Dagan and Itai, 1994; Brown et al., 1991). Better sense assignment can then lead to better sense-specific word embeddings. We propose a model that uses second-language embeddings as a supervisory signal in learning multisense representations in the first language. This supervision is easy to obtain for many language pairs as numerous parallel corpora exist nowadays. Our model, which can be seen as an autoencoder with a discrete hidden layer encoding word senses, leverages bilingual data in its encoding part, while the decoder predicts the surrounding words relying on the 1346 Proceedings of NAACL-HLT 2016, pages"
N16-1160,W12-3131,0,0.0419226,"Missing"
N16-1160,P14-1129,0,0.00942358,"are not entirely peaked, which makes weighting beneficial. ∗ Unlike in training, the sense prediction step here does not use the crosslingual context Ci′ since it is not available in the evaluation tasks. In this work, instead of marginalizing out the unobservable crosslingual context, we simply ignore it in computation. Sometimes, even the first-language context is missing, as is the situation in many word similarity tasks.PIn that case, we just use the uniform average, 1/|S| s∈S ϕi,s . 3 Word affiliation from alignments In defining the crosslingual signal we draw on a heuristic inspired by Devlin et al. (2014). The secondlanguage context words are taken to be the multiset of words around and including the pivot affiliated to xi : (6) Ci′ = {x′ai −m , ..., x′ai , ..., x′ai +m }, where x′ai is the word affiliated to xi and the parameter m regulates the context window size. By choosing m = 0, only the affiliated word is used as l′ context, and by choosing m = ∞, the l′ context is the entire sentence (≈uniform alignment). To obtain the index ai , we use the following: 1) If xi aligns to exactly one second-language word, ai is the index of the word it aligns to. 2) If xi aligns to multiple words, ai is"
N16-1160,P02-1033,0,0.365828,"14; Chandar A P et al., 2014). Secondly, information from another language can also be leveraged to yield better firstlanguage embeddings (Guo et al., 2014). Our paper falls in the latter, much less explored category. We adhere to the view of multilingual learning as a means of language grounding (Faruqui and Dyer, 2014b; Zou et al., 2013; Titov and Klementiev, 2012; Snyder and Barzilay, 2010; Naseem et al., 2009). Intuitively, polysemy in one language can be at least partially resolved by looking at the translation of the word and its context in another language (Kaji, 2003; Ng et al., 2003; Diab and Resnik, 2002; Ide, 2000; Dagan and Itai, 1994; Brown et al., 1991). Better sense assignment can then lead to better sense-specific word embeddings. We propose a model that uses second-language embeddings as a supervisory signal in learning multisense representations in the first language. This supervision is easy to obtain for many language pairs as numerous parallel corpora exist nowadays. Our model, which can be seen as an autoencoder with a discrete hidden layer encoding word senses, leverages bilingual data in its encoding part, while the decoder predicts the surrounding words relying on the 1346 Proc"
N16-1160,P10-4002,0,0.0503289,"re x′ai is the word affiliated to xi and the parameter m regulates the context window size. By choosing m = 0, only the affiliated word is used as l′ context, and by choosing m = ∞, the l′ context is the entire sentence (≈uniform alignment). To obtain the index ai , we use the following: 1) If xi aligns to exactly one second-language word, ai is the index of the word it aligns to. 2) If xi aligns to multiple words, ai is the index of the aligned word in the middle (and rounding down when necessary). 3) If xi is unaligned, Ci′ is empty, therefore no l′ context is used. We use the cdec aligner (Dyer et al., 2010) to wordalign the parallel corpora. 4 Parameters and Set-up their default values can be examined in the source code available online. 4.2 Bilingual data In a large body of work on multilingual word representations, Europarl (Koehn, 2005) is the preferred source of parallel data. However, the domain of Europarl is rather constrained, whereas we would like to obtain word representations of more general language, also to carry out an effective evaluation on semantic similarity datasets where domains are usually broader. We therefore use the following parallel corpora: News Commentary (Bojar et al"
N16-1160,P13-2136,0,0.0224997,"D heuristics, after which a recurrent neural network is trained to obtain sense-specific representations. Unlike in our work, the sense induction and embedding learning components are entirely separated, without a possibility for one to influence another. In a similar vein, Bansal et al. (2012) use bilingual corpora to perform soft word clustering, extending the previous work on the monolingual case of Lin and Wu (2009). Single-sense representations in the multilingual context have been studied more extensively (Lu et al., 2015; Faruqui and Dyer, 2014b; Hill et al., 2014a; Zhang et al., 2014; Faruqui and Dyer, 2013; Zou et al., 2013), with a goal of bringing the representations in the same semantic space. A related line of work concerns the crosslingual setting, where one tries to leverage training data in one language to build models for typically lower-resource languages (Hermann and Blunsom, 2014; Gouws et al., 2014; Chandar A P et al., 2014; Soyer et al., 2014; Klementiev et al., 2012; T¨ackstr¨om et al., 2012). The recent works of Kawakami and Dyer (2015) and Nalisnick and Ravi (2015) are also of interest. The latter work on the infinite Skip-Gram model in which the embedding dimensionality is stoc"
N16-1160,P14-5004,0,0.0786572,"representations for several languages in the same semantic space, which then enables the transfer of a model (e.g., a syntactic parser) trained on annotated training data in one language to another language lacking this annotation (Klementiev et al., 2012; Hermann and Blunsom, 2014; Gouws et al., 2014; Chandar A P et al., 2014). Secondly, information from another language can also be leveraged to yield better firstlanguage embeddings (Guo et al., 2014). Our paper falls in the latter, much less explored category. We adhere to the view of multilingual learning as a means of language grounding (Faruqui and Dyer, 2014b; Zou et al., 2013; Titov and Klementiev, 2012; Snyder and Barzilay, 2010; Naseem et al., 2009). Intuitively, polysemy in one language can be at least partially resolved by looking at the translation of the word and its context in another language (Kaji, 2003; Ng et al., 2003; Diab and Resnik, 2002; Ide, 2000; Dagan and Itai, 1994; Brown et al., 1991). Better sense assignment can then lead to better sense-specific word embeddings. We propose a model that uses second-language embeddings as a supervisory signal in learning multisense representations in the first language. This supervision is ea"
N16-1160,E14-1049,0,0.293144,"representations for several languages in the same semantic space, which then enables the transfer of a model (e.g., a syntactic parser) trained on annotated training data in one language to another language lacking this annotation (Klementiev et al., 2012; Hermann and Blunsom, 2014; Gouws et al., 2014; Chandar A P et al., 2014). Secondly, information from another language can also be leveraged to yield better firstlanguage embeddings (Guo et al., 2014). Our paper falls in the latter, much less explored category. We adhere to the view of multilingual learning as a means of language grounding (Faruqui and Dyer, 2014b; Zou et al., 2013; Titov and Klementiev, 2012; Snyder and Barzilay, 2010; Naseem et al., 2009). Intuitively, polysemy in one language can be at least partially resolved by looking at the translation of the word and its context in another language (Kaji, 2003; Ng et al., 2003; Diab and Resnik, 2002; Ide, 2000; Dagan and Itai, 1994; Brown et al., 1991). Better sense assignment can then lead to better sense-specific word embeddings. We propose a model that uses second-language embeddings as a supervisory signal in learning multisense representations in the first language. This supervision is ea"
N16-1160,N15-1003,0,0.0138046,"which would allow to weight the contribution of different senses more accurately for the multi-sense models. Why, then, does simply averaging the M U and B I M U embeddings lead to better results than when using the S G embeddings? We hypothesize that the single-sense model tends to over-represent the dominant sense with its generic, one-vector-per-word representation, whereas the uniformly averaged embeddings yielded by the multisense models better encode the range of potential senses. Similar observations have been made in the context of selectional preference modeling of polysemous verbs (Greenberg et al., 2015). In POS tagging, the relationship between M U and B I M U models is similar as discussed above. Overall, however, neither of the multi-sense models outperforms the S G embeddings. The neural network tagger may be able to implicitly perform disambiguation on top of single-sense S G embeddings, similarly to what has been argued in Li and Jurafsky (2015). The tagging accuracies obtained with M U on CZ-EN and FR-EN are similar to the one obtained by Li and Jurafsky with their multi-sense model (93.8), while the accuracy of S G is more competitive in our case (around 94.0 compared to 92.5), althou"
N16-1160,C14-1048,0,0.53625,"embeddings, another line of research has investigated integrating multilingual data, with largely two distinct goals in mind. The first goal has been to obtain representations for several languages in the same semantic space, which then enables the transfer of a model (e.g., a syntactic parser) trained on annotated training data in one language to another language lacking this annotation (Klementiev et al., 2012; Hermann and Blunsom, 2014; Gouws et al., 2014; Chandar A P et al., 2014). Secondly, information from another language can also be leveraged to yield better firstlanguage embeddings (Guo et al., 2014). Our paper falls in the latter, much less explored category. We adhere to the view of multilingual learning as a means of language grounding (Faruqui and Dyer, 2014b; Zou et al., 2013; Titov and Klementiev, 2012; Snyder and Barzilay, 2010; Naseem et al., 2009). Intuitively, polysemy in one language can be at least partially resolved by looking at the translation of the word and its context in another language (Kaji, 2003; Ng et al., 2003; Diab and Resnik, 2002; Ide, 2000; Dagan and Itai, 1994; Brown et al., 1991). Better sense assignment can then lead to better sense-specific word embeddings."
N16-1160,P14-1006,0,0.130951,"tan et al., 2014; Chen et al., 2014; Li and Jurafsky, 2015). Gertjan van Noord University of Groningen Netherlands g.j.m.van.noord@rug.nl In parallel to this work on multi-sense word embeddings, another line of research has investigated integrating multilingual data, with largely two distinct goals in mind. The first goal has been to obtain representations for several languages in the same semantic space, which then enables the transfer of a model (e.g., a syntactic parser) trained on annotated training data in one language to another language lacking this annotation (Klementiev et al., 2012; Hermann and Blunsom, 2014; Gouws et al., 2014; Chandar A P et al., 2014). Secondly, information from another language can also be leveraged to yield better firstlanguage embeddings (Guo et al., 2014). Our paper falls in the latter, much less explored category. We adhere to the view of multilingual learning as a means of language grounding (Faruqui and Dyer, 2014b; Zou et al., 2013; Titov and Klementiev, 2012; Snyder and Barzilay, 2010; Naseem et al., 2009). Intuitively, polysemy in one language can be at least partially resolved by looking at the translation of the word and its context in another language (Kaji, 2003;"
N16-1160,P12-1092,0,0.943861,"to capture syntactic and semantic properties of words. They have been evaluated intrinsically (Mikolov et al., 2013a; Baroni et al., 2014; Levy and Goldberg, 2014) and have also been used in concrete NLP applications to deal with word sparsity and improve generalization (Turian et al., 2010; Collobert et al., 2011; Bansal et al., 2014; Passos et al., 2014). While most work to date has focused on developing embedding models which represent a word with a single vector, some researchers have attempted to capture polysemy explicitly and have encoded properties of each word with multiple vectors (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Chen et al., 2014; Li and Jurafsky, 2015). Gertjan van Noord University of Groningen Netherlands g.j.m.van.noord@rug.nl In parallel to this work on multi-sense word embeddings, another line of research has investigated integrating multilingual data, with largely two distinct goals in mind. The first goal has been to obtain representations for several languages in the same semantic space, which then enables the transfer of a model (e.g., a syntactic parser) trained on annotated training data in one language to another language lacking this annotati"
N16-1160,J98-1001,0,0.107447,"Missing"
N16-1160,N03-1015,0,0.0604604,"unsom, 2014; Gouws et al., 2014; Chandar A P et al., 2014). Secondly, information from another language can also be leveraged to yield better firstlanguage embeddings (Guo et al., 2014). Our paper falls in the latter, much less explored category. We adhere to the view of multilingual learning as a means of language grounding (Faruqui and Dyer, 2014b; Zou et al., 2013; Titov and Klementiev, 2012; Snyder and Barzilay, 2010; Naseem et al., 2009). Intuitively, polysemy in one language can be at least partially resolved by looking at the translation of the word and its context in another language (Kaji, 2003; Ng et al., 2003; Diab and Resnik, 2002; Ide, 2000; Dagan and Itai, 1994; Brown et al., 1991). Better sense assignment can then lead to better sense-specific word embeddings. We propose a model that uses second-language embeddings as a supervisory signal in learning multisense representations in the first language. This supervision is easy to obtain for many language pairs as numerous parallel corpora exist nowadays. Our model, which can be seen as an autoencoder with a discrete hidden layer encoding word senses, leverages bilingual data in its encoding part, while the decoder predicts the su"
N16-1160,C12-1089,1,0.672633,"an et al., 2014; Neelakantan et al., 2014; Chen et al., 2014; Li and Jurafsky, 2015). Gertjan van Noord University of Groningen Netherlands g.j.m.van.noord@rug.nl In parallel to this work on multi-sense word embeddings, another line of research has investigated integrating multilingual data, with largely two distinct goals in mind. The first goal has been to obtain representations for several languages in the same semantic space, which then enables the transfer of a model (e.g., a syntactic parser) trained on annotated training data in one language to another language lacking this annotation (Klementiev et al., 2012; Hermann and Blunsom, 2014; Gouws et al., 2014; Chandar A P et al., 2014). Secondly, information from another language can also be leveraged to yield better firstlanguage embeddings (Guo et al., 2014). Our paper falls in the latter, much less explored category. We adhere to the view of multilingual learning as a means of language grounding (Faruqui and Dyer, 2014b; Zou et al., 2013; Titov and Klementiev, 2012; Snyder and Barzilay, 2010; Naseem et al., 2009). Intuitively, polysemy in one language can be at least partially resolved by looking at the translation of the word and its context in an"
N16-1160,2005.mtsummit-papers.11,0,0.0362331,"o obtain the index ai , we use the following: 1) If xi aligns to exactly one second-language word, ai is the index of the word it aligns to. 2) If xi aligns to multiple words, ai is the index of the aligned word in the middle (and rounding down when necessary). 3) If xi is unaligned, Ci′ is empty, therefore no l′ context is used. We use the cdec aligner (Dyer et al., 2010) to wordalign the parallel corpora. 4 Parameters and Set-up their default values can be examined in the source code available online. 4.2 Bilingual data In a large body of work on multilingual word representations, Europarl (Koehn, 2005) is the preferred source of parallel data. However, the domain of Europarl is rather constrained, whereas we would like to obtain word representations of more general language, also to carry out an effective evaluation on semantic similarity datasets where domains are usually broader. We therefore use the following parallel corpora: News Commentary (Bojar et al., 2013) (NC), Yandex-1M4 (RU-EN), CzEng 1.0 (Bojar et al., 2012) (CZ-EN) from which we exclude the EU legislation texts, and GigaFrEn (Callison-Burch et al., 2009) (FR-EN). The sizes of the corpora are reported in Table 1. The word repr"
N16-1160,W14-1618,0,0.0178525,"sense. The two components are estimated jointly. We observe that the word representations induced from bilingual data outperform the monolingual counterparts across a range of evaluation tasks, even though crosslingual information is not available at test time. 1 Introduction Approaches to learning word embeddings (i.e. realvalued vectors) relying on word context have received much attention in recent years, and the induced representations have been shown to capture syntactic and semantic properties of words. They have been evaluated intrinsically (Mikolov et al., 2013a; Baroni et al., 2014; Levy and Goldberg, 2014) and have also been used in concrete NLP applications to deal with word sparsity and improve generalization (Turian et al., 2010; Collobert et al., 2011; Bansal et al., 2014; Passos et al., 2014). While most work to date has focused on developing embedding models which represent a word with a single vector, some researchers have attempted to capture polysemy explicitly and have encoded properties of each word with multiple vectors (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Chen et al., 2014; Li and Jurafsky, 2015). Gertjan van Noord University of Groningen Netherlands g."
N16-1160,D15-1200,0,0.371076,"rinsically (Mikolov et al., 2013a; Baroni et al., 2014; Levy and Goldberg, 2014) and have also been used in concrete NLP applications to deal with word sparsity and improve generalization (Turian et al., 2010; Collobert et al., 2011; Bansal et al., 2014; Passos et al., 2014). While most work to date has focused on developing embedding models which represent a word with a single vector, some researchers have attempted to capture polysemy explicitly and have encoded properties of each word with multiple vectors (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Chen et al., 2014; Li and Jurafsky, 2015). Gertjan van Noord University of Groningen Netherlands g.j.m.van.noord@rug.nl In parallel to this work on multi-sense word embeddings, another line of research has investigated integrating multilingual data, with largely two distinct goals in mind. The first goal has been to obtain representations for several languages in the same semantic space, which then enables the transfer of a model (e.g., a syntactic parser) trained on annotated training data in one language to another language lacking this annotation (Klementiev et al., 2012; Hermann and Blunsom, 2014; Gouws et al., 2014; Chandar A P"
N16-1160,P09-1116,0,0.0258784,"et al. (2014) perform a sense induction step based on clustering translations prior to learning word embeddings. Once the translations are clustered, they are mapped to a source corpus using WSD heuristics, after which a recurrent neural network is trained to obtain sense-specific representations. Unlike in our work, the sense induction and embedding learning components are entirely separated, without a possibility for one to influence another. In a similar vein, Bansal et al. (2012) use bilingual corpora to perform soft word clustering, extending the previous work on the monolingual case of Lin and Wu (2009). Single-sense representations in the multilingual context have been studied more extensively (Lu et al., 2015; Faruqui and Dyer, 2014b; Hill et al., 2014a; Zhang et al., 2014; Faruqui and Dyer, 2013; Zou et al., 2013), with a goal of bringing the representations in the same semantic space. A related line of work concerns the crosslingual setting, where one tries to leverage training data in one language to build models for typically lower-resource languages (Hermann and Blunsom, 2014; Gouws et al., 2014; Chandar A P et al., 2014; Soyer et al., 2014; Klementiev et al., 2012; T¨ackstr¨om et al."
N16-1160,N15-1028,0,0.0147469,". Once the translations are clustered, they are mapped to a source corpus using WSD heuristics, after which a recurrent neural network is trained to obtain sense-specific representations. Unlike in our work, the sense induction and embedding learning components are entirely separated, without a possibility for one to influence another. In a similar vein, Bansal et al. (2012) use bilingual corpora to perform soft word clustering, extending the previous work on the monolingual case of Lin and Wu (2009). Single-sense representations in the multilingual context have been studied more extensively (Lu et al., 2015; Faruqui and Dyer, 2014b; Hill et al., 2014a; Zhang et al., 2014; Faruqui and Dyer, 2013; Zou et al., 2013), with a goal of bringing the representations in the same semantic space. A related line of work concerns the crosslingual setting, where one tries to leverage training data in one language to build models for typically lower-resource languages (Hermann and Blunsom, 2014; Gouws et al., 2014; Chandar A P et al., 2014; Soyer et al., 2014; Klementiev et al., 2012; T¨ackstr¨om et al., 2012). The recent works of Kawakami and Dyer (2015) and Nalisnick and Ravi (2015) are also of interest. The"
N16-1160,W13-3512,0,0.0151987,"ns 353 human-rated word pairs (Finkelstein et al., 2001), while Agirre et al. (2009) separate this benchmark for similarity (WS-SIM) and relatedness (WS-REL). The RG-65 (Rubenstein and Goodenough, 1965) and the MC-30 (Miller and Charles, 1991) benchmarks contain nouns only. The MTurk-287 (Radinsky et al., 2011) and MTurk-771 (Halawi et al., 2012) include word pairs whose similarity was crowdsourced from AMT. Similarly, MEN (Bruni et al., 2012) is an AMT-annotated dataset of 3000 word pairs. The YP130 (Yang and Powers, 2006) and Verb-143 (Baker et al., 2014) measure verb similarity. Rare-Word (Luong et al., 2013) contains 2034 rare-word pairs. Finally, SimLex-999 (Hill et al., 2014b) is intended to measure pure similarity as opposed to relatedness. For these benchmarks, we prepare the word representations by taking a uniform average of all sense embeddings per word. The evaluation is carried out using the tool described in Faruqui and Dyer (2014a). Due to space constraints, we report the results by averaging over all benchmarks (Similarity), and include the individual results in the online repository. 5.2 Supersense similarity We also evaluate on a task measuring the similarity between the embeddings—"
N16-1160,Q16-1017,1,0.554196,"art et al., 1986; Bengio et 1347 al., 2013). Autoencoders are trained to reproduce their input by first mapping their input to a (lower dimensional) hidden layer and then predicting an approximation of the input relying on this hidden layer. In our case, the hidden layer is not a real-valued vector, but is a categorical variable encoding the sense of a word. Discrete-state autoencoders have been successful in several natural language processing applications, including POS tagging and word alignment (Ammar et al., 2014), semantic role induction (Titov and Khoddam, 2015) and relation discovery (Marcheggiani and Titov, 2016). More formally, our model consists of two components: an encoding part which assigns a sense to a pivot word, and a reconstruction (decoding) part recovering context words based on the pivot word and its sense. As predictions are probabilistic (‘soft’), the reconstruction step involves summation over all potential word senses. The goal is to find embedding parameters which minimize the error in recovering context words based on the pivot word and the sense assignment. Parameters of both encoding and reconstruction are jointly optimized. Intuitively, a good sense assignment should make the rec"
N16-1160,D14-1113,0,0.791023,"erties of words. They have been evaluated intrinsically (Mikolov et al., 2013a; Baroni et al., 2014; Levy and Goldberg, 2014) and have also been used in concrete NLP applications to deal with word sparsity and improve generalization (Turian et al., 2010; Collobert et al., 2011; Bansal et al., 2014; Passos et al., 2014). While most work to date has focused on developing embedding models which represent a word with a single vector, some researchers have attempted to capture polysemy explicitly and have encoded properties of each word with multiple vectors (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Chen et al., 2014; Li and Jurafsky, 2015). Gertjan van Noord University of Groningen Netherlands g.j.m.van.noord@rug.nl In parallel to this work on multi-sense word embeddings, another line of research has investigated integrating multilingual data, with largely two distinct goals in mind. The first goal has been to obtain representations for several languages in the same semantic space, which then enables the transfer of a model (e.g., a syntactic parser) trained on annotated training data in one language to another language lacking this annotation (Klementiev et al., 2012; Hermann and Blun"
N16-1160,P03-1058,0,0.112642,"Gouws et al., 2014; Chandar A P et al., 2014). Secondly, information from another language can also be leveraged to yield better firstlanguage embeddings (Guo et al., 2014). Our paper falls in the latter, much less explored category. We adhere to the view of multilingual learning as a means of language grounding (Faruqui and Dyer, 2014b; Zou et al., 2013; Titov and Klementiev, 2012; Snyder and Barzilay, 2010; Naseem et al., 2009). Intuitively, polysemy in one language can be at least partially resolved by looking at the translation of the word and its context in another language (Kaji, 2003; Ng et al., 2003; Diab and Resnik, 2002; Ide, 2000; Dagan and Itai, 1994; Brown et al., 1991). Better sense assignment can then lead to better sense-specific word embeddings. We propose a model that uses second-language embeddings as a supervisory signal in learning multisense representations in the first language. This supervision is easy to obtain for many language pairs as numerous parallel corpora exist nowadays. Our model, which can be seen as an autoencoder with a discrete hidden layer encoding word senses, leverages bilingual data in its encoding part, while the decoder predicts the surrounding words r"
N16-1160,W14-1609,0,0.0102159,"though crosslingual information is not available at test time. 1 Introduction Approaches to learning word embeddings (i.e. realvalued vectors) relying on word context have received much attention in recent years, and the induced representations have been shown to capture syntactic and semantic properties of words. They have been evaluated intrinsically (Mikolov et al., 2013a; Baroni et al., 2014; Levy and Goldberg, 2014) and have also been used in concrete NLP applications to deal with word sparsity and improve generalization (Turian et al., 2010; Collobert et al., 2011; Bansal et al., 2014; Passos et al., 2014). While most work to date has focused on developing embedding models which represent a word with a single vector, some researchers have attempted to capture polysemy explicitly and have encoded properties of each word with multiple vectors (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Chen et al., 2014; Li and Jurafsky, 2015). Gertjan van Noord University of Groningen Netherlands g.j.m.van.noord@rug.nl In parallel to this work on multi-sense word embeddings, another line of research has investigated integrating multilingual data, with largely two distinct goals in mind. The"
N16-1160,N10-1013,0,0.34779,"rical study. We comment here briefly on other choices of k ∈ {2, 4, 5}. We have found k = 2 to be a good choice on the RU-EN and FR-EN corpora (but not on CZ-EN), with an around 0.2-point improvement over k = 3 on SCWS and in POS tagging. With the larger values of k, the performance tends to degrade. For example, on RU-EN, the k = 5 score on SCWS is about 0.6 point below our default setting. 7 Additional Related Work Multi-sense models. One line of research has dealt with sense induction as a separate, clustering problem that is followed by an embedding learning component (Huang et al., 2012; Reisinger and Mooney, 2010). In another, the sense assignment and the embeddings are trained jointly (Neelakantan et al., 2014; Tian et al., 2014; Li and Jurafsky, 2015; Bartunov et al., 2015). Neelakantan et al. (2014) propose an extension of Skip-Gram (Mikolov et al., 2013a) by introducing sense-specific parameters together with the k-means-inspired ‘centroid’ vectors that keep track of the contexts in which word senses have occurred. They explore two model variants, one in which the number of senses is the same for all words, and another in which a threshold value determines the number of senses for each word. The re"
N16-1160,N12-1052,0,0.0135432,"Missing"
N16-1160,C14-1016,0,0.411031,"c and semantic properties of words. They have been evaluated intrinsically (Mikolov et al., 2013a; Baroni et al., 2014; Levy and Goldberg, 2014) and have also been used in concrete NLP applications to deal with word sparsity and improve generalization (Turian et al., 2010; Collobert et al., 2011; Bansal et al., 2014; Passos et al., 2014). While most work to date has focused on developing embedding models which represent a word with a single vector, some researchers have attempted to capture polysemy explicitly and have encoded properties of each word with multiple vectors (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Chen et al., 2014; Li and Jurafsky, 2015). Gertjan van Noord University of Groningen Netherlands g.j.m.van.noord@rug.nl In parallel to this work on multi-sense word embeddings, another line of research has investigated integrating multilingual data, with largely two distinct goals in mind. The first goal has been to obtain representations for several languages in the same semantic space, which then enables the transfer of a model (e.g., a syntactic parser) trained on annotated training data in one language to another language lacking this annotation (Klementiev et a"
N16-1160,N15-1001,1,0.835683,"eneral structure from neural autoencoders (Rumelhart et al., 1986; Bengio et 1347 al., 2013). Autoencoders are trained to reproduce their input by first mapping their input to a (lower dimensional) hidden layer and then predicting an approximation of the input relying on this hidden layer. In our case, the hidden layer is not a real-valued vector, but is a categorical variable encoding the sense of a word. Discrete-state autoencoders have been successful in several natural language processing applications, including POS tagging and word alignment (Ammar et al., 2014), semantic role induction (Titov and Khoddam, 2015) and relation discovery (Marcheggiani and Titov, 2016). More formally, our model consists of two components: an encoding part which assigns a sense to a pivot word, and a reconstruction (decoding) part recovering context words based on the pivot word and its sense. As predictions are probabilistic (‘soft’), the reconstruction step involves summation over all potential word senses. The goal is to find embedding parameters which minimize the error in recovering context words based on the pivot word and the sense assignment. Parameters of both encoding and reconstruction are jointly optimized. In"
N16-1160,P12-1068,1,0.786917,"he same semantic space, which then enables the transfer of a model (e.g., a syntactic parser) trained on annotated training data in one language to another language lacking this annotation (Klementiev et al., 2012; Hermann and Blunsom, 2014; Gouws et al., 2014; Chandar A P et al., 2014). Secondly, information from another language can also be leveraged to yield better firstlanguage embeddings (Guo et al., 2014). Our paper falls in the latter, much less explored category. We adhere to the view of multilingual learning as a means of language grounding (Faruqui and Dyer, 2014b; Zou et al., 2013; Titov and Klementiev, 2012; Snyder and Barzilay, 2010; Naseem et al., 2009). Intuitively, polysemy in one language can be at least partially resolved by looking at the translation of the word and its context in another language (Kaji, 2003; Ng et al., 2003; Diab and Resnik, 2002; Ide, 2000; Dagan and Itai, 1994; Brown et al., 1991). Better sense assignment can then lead to better sense-specific word embeddings. We propose a model that uses second-language embeddings as a supervisory signal in learning multisense representations in the first language. This supervision is easy to obtain for many language pairs as numerou"
N16-1160,D15-1243,0,0.0275842,"we prepare the word representations by taking a uniform average of all sense embeddings per word. The evaluation is carried out using the tool described in Faruqui and Dyer (2014a). Due to space constraints, we report the results by averaging over all benchmarks (Similarity), and include the individual results in the online repository. 5.2 Supersense similarity We also evaluate on a task measuring the similarity between the embeddings—in our case uniformly averaged in the case of multi-sense embeddings—and a matrix of supersense features extracted from the English SemCor, using the Qvec tool (Tsvetkov et al., 2015). We choose this method because it has been shown to output scores that correlate well with extrinsic tasks, e.g. text classification and sentiment analysis. We believe that this, in combination with word similarity tasks from the previous section, can give a reliable picture of the generic quality of word embeddings studied in this work. 5.3 POS tagging As our downstream evaluation task, we use the learned word representations to initialize the embedding layer of a neural network tagging model. We use the same convolutional architecture as Li and Juraf1350 sky (2015): an input layer taking a"
N16-1160,P10-1040,0,0.00925279,"monolingual counterparts across a range of evaluation tasks, even though crosslingual information is not available at test time. 1 Introduction Approaches to learning word embeddings (i.e. realvalued vectors) relying on word context have received much attention in recent years, and the induced representations have been shown to capture syntactic and semantic properties of words. They have been evaluated intrinsically (Mikolov et al., 2013a; Baroni et al., 2014; Levy and Goldberg, 2014) and have also been used in concrete NLP applications to deal with word sparsity and improve generalization (Turian et al., 2010; Collobert et al., 2011; Bansal et al., 2014; Passos et al., 2014). While most work to date has focused on developing embedding models which represent a word with a single vector, some researchers have attempted to capture polysemy explicitly and have encoded properties of each word with multiple vectors (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Chen et al., 2014; Li and Jurafsky, 2015). Gertjan van Noord University of Groningen Netherlands g.j.m.van.noord@rug.nl In parallel to this work on multi-sense word embeddings, another line of research has investigated integrat"
N16-1160,P14-1011,0,0.0182934,"urce corpus using WSD heuristics, after which a recurrent neural network is trained to obtain sense-specific representations. Unlike in our work, the sense induction and embedding learning components are entirely separated, without a possibility for one to influence another. In a similar vein, Bansal et al. (2012) use bilingual corpora to perform soft word clustering, extending the previous work on the monolingual case of Lin and Wu (2009). Single-sense representations in the multilingual context have been studied more extensively (Lu et al., 2015; Faruqui and Dyer, 2014b; Hill et al., 2014a; Zhang et al., 2014; Faruqui and Dyer, 2013; Zou et al., 2013), with a goal of bringing the representations in the same semantic space. A related line of work concerns the crosslingual setting, where one tries to leverage training data in one language to build models for typically lower-resource languages (Hermann and Blunsom, 2014; Gouws et al., 2014; Chandar A P et al., 2014; Soyer et al., 2014; Klementiev et al., 2012; T¨ackstr¨om et al., 2012). The recent works of Kawakami and Dyer (2015) and Nalisnick and Ravi (2015) are also of interest. The latter work on the infinite Skip-Gram model in which the embeddin"
N16-1160,D13-1141,0,0.114152,"ral languages in the same semantic space, which then enables the transfer of a model (e.g., a syntactic parser) trained on annotated training data in one language to another language lacking this annotation (Klementiev et al., 2012; Hermann and Blunsom, 2014; Gouws et al., 2014; Chandar A P et al., 2014). Secondly, information from another language can also be leveraged to yield better firstlanguage embeddings (Guo et al., 2014). Our paper falls in the latter, much less explored category. We adhere to the view of multilingual learning as a means of language grounding (Faruqui and Dyer, 2014b; Zou et al., 2013; Titov and Klementiev, 2012; Snyder and Barzilay, 2010; Naseem et al., 2009). Intuitively, polysemy in one language can be at least partially resolved by looking at the translation of the word and its context in another language (Kaji, 2003; Ng et al., 2003; Diab and Resnik, 2002; Ide, 2000; Dagan and Itai, 1994; Brown et al., 1991). Better sense assignment can then lead to better sense-specific word embeddings. We propose a model that uses second-language embeddings as a supervisory signal in learning multisense representations in the first language. This supervision is easy to obtain for ma"
N16-1160,J15-4004,0,\N,Missing
N16-1160,W09-0401,0,\N,Missing
N16-1160,N09-1003,0,\N,Missing
N16-1160,D14-1034,0,\N,Missing
N16-1160,bojar-etal-2012-joy,0,\N,Missing
N16-1160,P14-2131,0,\N,Missing
N16-1160,W13-2201,0,\N,Missing
N18-2078,P16-1231,0,0.0829039,"Missing"
N18-2078,P17-1012,0,0.0222946,"into standard attentionbased NMT models (Bahdanau et al., 2015). We consider PropBank-style (Palmer et al., 2005) semantic role structures, or more specifi1 We slightly abuse the terminology: formally these are syntactic heads of arguments rather than arguments. 486 Proceedings of NAACL-HLT 2018, pages 486–492 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics the source sentence. Traditionally, the encoder is parametrized as a Recurrent Neural Network (RNN), but other architectures have also been successful, such as Convolutional Neural Networks (CNN) (Gehring et al., 2017) and hierarchical selfattention models (Vaswani et al., 2017), among others. In this paper we experiment with RNN and CNN encoders. We explore the benefits of incorporating information about semantic-role structures into such encoders. More formally, RNNs (Elman, 1990) can be defined as a function RNN(x1:t ) that calculates the hidden representation ht of a word xt based on its left context. Bidirectional RNNs use two RNNs: one runs in the forward direction and another one in the backward direction. The forward RNN(x1:t ) represents the left context of word xt , whereas the backward RNN(xn:t )"
N18-2078,W11-2136,0,0.294152,"Missing"
N18-2078,J12-2006,0,0.0234545,"s and thus Introduction It has long been argued that semantic representations may provide a useful linguistic bias to machine translation systems (Weaver, 1955; BarHillel, 1960). Semantic representations provide an abstraction which can generalize over different surface realizations of the same underlying ‘meaning’. Providing this information to a machine translation system, can, in principle, improve meaning preservation and boost generalization performance. Though incorporation of semantic information into traditional statistical machine translation has been an active research topic (e.g., (Baker et al., 2012; Liu and Gildea, 2010; Wu and Fung, 2009; Bazrafshan and Gildea, 2013; Aziz et al., 2011; Jones et al., 2012)), we are not aware of any previous work considering semantic structures in neural machine translation (NMT). In this work, we aim to fill this gap by showing how information about predicate-argument structure of source sentences can be integrated into standard attentionbased NMT models (Bahdanau et al., 2015). We consider PropBank-style (Palmer et al., 2005) semantic role structures, or more specifi1 We slightly abuse the terminology: formally these are syntactic heads of arguments ra"
N18-2078,W09-1201,0,0.0862604,"Missing"
N18-2078,D17-1209,1,0.367367,"the two sentences. We hypothesize that semantic roles can be especially beneficial in NMT, as ‘argument switching’ (flipping arguments corresponding to different roles) is one of frequent and severe mistakes made by NMT systems (Isabelle et al., 2017). There is a limited amount of work on incorporating graph structures into neural sequence models. Though, unlike semantics in NMT, syntactically-aware NMT has been a relatively hot topic recently, with a number of approaches claiming improvements from using treebank syntax (Sennrich and Haddow, 2016; Eriguchi et al., 2016; Nadejde et al., 2017; Bastings et al., 2017; Aharoni and Goldberg, 2017), our graphs are different from syntactic structures. Unlike syntactic dependency graphs, they are not trees and thus Introduction It has long been argued that semantic representations may provide a useful linguistic bias to machine translation systems (Weaver, 1955; BarHillel, 1960). Semantic representations provide an abstraction which can generalize over different surface realizations of the same underlying ‘meaning’. Providing this information to a machine translation system, can, in principle, improve meaning preservation and boost generalization performance."
N18-2078,P13-2074,0,0.0177105,"representations may provide a useful linguistic bias to machine translation systems (Weaver, 1955; BarHillel, 1960). Semantic representations provide an abstraction which can generalize over different surface realizations of the same underlying ‘meaning’. Providing this information to a machine translation system, can, in principle, improve meaning preservation and boost generalization performance. Though incorporation of semantic information into traditional statistical machine translation has been an active research topic (e.g., (Baker et al., 2012; Liu and Gildea, 2010; Wu and Fung, 2009; Bazrafshan and Gildea, 2013; Aziz et al., 2011; Jones et al., 2012)), we are not aware of any previous work considering semantic structures in neural machine translation (NMT). In this work, we aim to fill this gap by showing how information about predicate-argument structure of source sentences can be integrated into standard attentionbased NMT models (Bahdanau et al., 2015). We consider PropBank-style (Palmer et al., 2005) semantic role structures, or more specifi1 We slightly abuse the terminology: formally these are syntactic heads of arguments rather than arguments. 486 Proceedings of NAACL-HLT 2018, pages 486–492"
N18-2078,D17-1263,0,0.0417528,"Missing"
N18-2078,D14-1179,0,0.0347707,"Missing"
N18-2078,C12-1083,0,0.023744,"Missing"
N18-2078,P02-1040,0,0.100846,"these experiments, we use newstest2015 and newstest2016 as a validation and test set, respectively. We parsed the English partitions of these datasets with a syntactic dependency parser (Andor et al., 2016) and dependency-based semantic role labeler (Marcheggiani et al., 2017). We constructed the English vocabulary by taking all words with frequency higher than three, while for German we used byte-pair encodings (BPE) (Sennrich et al., 2016). All hyperparameter selection was performed on the validation set (see Appendix ??). We measured the performance of the models with (cased) BLEU scores (Papineni et al., 2002). The settings and the framework (Neural Monkey (Helcl and Libovick´y, 2017)) used for experiments are the ones used in Bastings et al. (2017), which we use as baselines. As RNNs, we use GRUs (Cho et al., 2014). We now discuss the impact that different architectures and linguistic information have on the translation quality. u∈N (v) where N (v) is the set of neighbors of v, Wdir(u,v) ∈ Rd×d is a direction-specific parameter matrix. There are three possible directions (dir(u, v) ∈ {in, out, loop}): self-loop edges were added in order to ensure that the initial representation of node hv directly"
N18-2078,W16-2209,0,0.0210827,"nice present to his wonderful wife”, despite different surface forms of the two sentences. We hypothesize that semantic roles can be especially beneficial in NMT, as ‘argument switching’ (flipping arguments corresponding to different roles) is one of frequent and severe mistakes made by NMT systems (Isabelle et al., 2017). There is a limited amount of work on incorporating graph structures into neural sequence models. Though, unlike semantics in NMT, syntactically-aware NMT has been a relatively hot topic recently, with a number of approaches claiming improvements from using treebank syntax (Sennrich and Haddow, 2016; Eriguchi et al., 2016; Nadejde et al., 2017; Bastings et al., 2017; Aharoni and Goldberg, 2017), our graphs are different from syntactic structures. Unlike syntactic dependency graphs, they are not trees and thus Introduction It has long been argued that semantic representations may provide a useful linguistic bias to machine translation systems (Weaver, 1955; BarHillel, 1960). Semantic representations provide an abstraction which can generalize over different surface realizations of the same underlying ‘meaning’. Providing this information to a machine translation system, can, in principle,"
N18-2078,P16-1162,0,0.0412888,"WMT16 dataset (∼4.5 million sentence pairs for training). We use its subset, News Commentary v11, for development and additional experiments (∼226.000 sentence pairs). For all these experiments, we use newstest2015 and newstest2016 as a validation and test set, respectively. We parsed the English partitions of these datasets with a syntactic dependency parser (Andor et al., 2016) and dependency-based semantic role labeler (Marcheggiani et al., 2017). We constructed the English vocabulary by taking all words with frequency higher than three, while for German we used byte-pair encodings (BPE) (Sennrich et al., 2016). All hyperparameter selection was performed on the validation set (see Appendix ??). We measured the performance of the models with (cased) BLEU scores (Papineni et al., 2002). The settings and the framework (Neural Monkey (Helcl and Libovick´y, 2017)) used for experiments are the ones used in Bastings et al. (2017), which we use as baselines. As RNNs, we use GRUs (Cho et al., 2014). We now discuss the impact that different architectures and linguistic information have on the translation quality. u∈N (v) where N (v) is the set of neighbors of v, Wdir(u,v) ∈ Rd×d is a direction-specific parame"
N18-2078,Q16-1037,0,0.0242759,"ly predicted (hence noisy) syntactic dependency graphs by Marcheggiani and Titov (2017). Representations of nodes (i.e. words in a sentence) in GCNs are directly influenced by representations of their neighbors in the graph. The form of influence (e.g., transition matrices and parameters of gates) are learned in such a way as to benefit the end task (i.e. translation). These linguistically-aware word representations are used within a neural encoder. Although recent research has shown that neural architectures are able to learn some linguistic phenomena without explicit linguistic supervision (Linzen et al., 2016; Vaswani et al., 2017), informing word representations with linguistic structures can provide a useful inductive bias. We apply GCNs to the semantic dependency graphs and experiment on the English–German language pair (WMT16). We observe an improvement over the semantics-agnostic baseline (a BiRNN encoder; 23.3 vs 24.5 BLEU). As we use exactly the same modeling approach as in the syntactic method of Bastings et al. (2017), we can easily compare the influence of the types of linguistic structures (i.e., syntax vs. semantics). We observe that when using full WMT data we obtain better results wi"
N18-2078,C10-1081,0,0.0443147,"ion It has long been argued that semantic representations may provide a useful linguistic bias to machine translation systems (Weaver, 1955; BarHillel, 1960). Semantic representations provide an abstraction which can generalize over different surface realizations of the same underlying ‘meaning’. Providing this information to a machine translation system, can, in principle, improve meaning preservation and boost generalization performance. Though incorporation of semantic information into traditional statistical machine translation has been an active research topic (e.g., (Baker et al., 2012; Liu and Gildea, 2010; Wu and Fung, 2009; Bazrafshan and Gildea, 2013; Aziz et al., 2011; Jones et al., 2012)), we are not aware of any previous work considering semantic structures in neural machine translation (NMT). In this work, we aim to fill this gap by showing how information about predicate-argument structure of source sentences can be integrated into standard attentionbased NMT models (Bahdanau et al., 2015). We consider PropBank-style (Palmer et al., 2005) semantic role structures, or more specifi1 We slightly abuse the terminology: formally these are syntactic heads of arguments rather than arguments. 4"
N18-2078,K17-1041,1,0.899713,"0v ∈ Rd of a node v while relying on representations hu of its neighbors: X  gu,v Wdir(u,v) hu + blab(u,v) , h0v=ρ Experiments We experimented with the English-to-German WMT16 dataset (∼4.5 million sentence pairs for training). We use its subset, News Commentary v11, for development and additional experiments (∼226.000 sentence pairs). For all these experiments, we use newstest2015 and newstest2016 as a validation and test set, respectively. We parsed the English partitions of these datasets with a syntactic dependency parser (Andor et al., 2016) and dependency-based semantic role labeler (Marcheggiani et al., 2017). We constructed the English vocabulary by taking all words with frequency higher than three, while for German we used byte-pair encodings (BPE) (Sennrich et al., 2016). All hyperparameter selection was performed on the validation set (see Appendix ??). We measured the performance of the models with (cased) BLEU scores (Papineni et al., 2002). The settings and the framework (Neural Monkey (Helcl and Libovick´y, 2017)) used for experiments are the ones used in Bastings et al. (2017), which we use as baselines. As RNNs, we use GRUs (Cho et al., 2014). We now discuss the impact that different arc"
N18-2078,D17-1159,1,0.897432,"Missing"
N18-2078,N09-2004,0,0.0499694,"rgued that semantic representations may provide a useful linguistic bias to machine translation systems (Weaver, 1955; BarHillel, 1960). Semantic representations provide an abstraction which can generalize over different surface realizations of the same underlying ‘meaning’. Providing this information to a machine translation system, can, in principle, improve meaning preservation and boost generalization performance. Though incorporation of semantic information into traditional statistical machine translation has been an active research topic (e.g., (Baker et al., 2012; Liu and Gildea, 2010; Wu and Fung, 2009; Bazrafshan and Gildea, 2013; Aziz et al., 2011; Jones et al., 2012)), we are not aware of any previous work considering semantic structures in neural machine translation (NMT). In this work, we aim to fill this gap by showing how information about predicate-argument structure of source sentences can be integrated into standard attentionbased NMT models (Bahdanau et al., 2015). We consider PropBank-style (Palmer et al., 2005) semantic role structures, or more specifi1 We slightly abuse the terminology: formally these are syntactic heads of arguments rather than arguments. 486 Proceedings of N"
N18-2078,W17-4707,0,0.0121998,"erent surface forms of the two sentences. We hypothesize that semantic roles can be especially beneficial in NMT, as ‘argument switching’ (flipping arguments corresponding to different roles) is one of frequent and severe mistakes made by NMT systems (Isabelle et al., 2017). There is a limited amount of work on incorporating graph structures into neural sequence models. Though, unlike semantics in NMT, syntactically-aware NMT has been a relatively hot topic recently, with a number of approaches claiming improvements from using treebank syntax (Sennrich and Haddow, 2016; Eriguchi et al., 2016; Nadejde et al., 2017; Bastings et al., 2017; Aharoni and Goldberg, 2017), our graphs are different from syntactic structures. Unlike syntactic dependency graphs, they are not trees and thus Introduction It has long been argued that semantic representations may provide a useful linguistic bias to machine translation systems (Weaver, 1955; BarHillel, 1960). Semantic representations provide an abstraction which can generalize over different surface realizations of the same underlying ‘meaning’. Providing this information to a machine translation system, can, in principle, improve meaning preservation and boost gener"
N18-2078,W08-2121,0,\N,Missing
N18-2078,J05-1004,0,\N,Missing
N19-1173,D07-1101,0,0.0263053,"i = j ¯ ij = Lij + exp(ˆ L Lij otherwise −1 ¯ eij = (1 − δ1,j )Aij [L ]jj ¯ −1 ]ji − (1 − δi,1 )Aij [L ¯ −1 ]i1 ri = exp(ˆ ri )[L return ri , eij access to labels for the roots (aka summary sentences), while tree edges are latent and learned without an explicit training signal. And as previous work (Liu and Lapata, 2017) has shown, a single application of TMT leads to shallow tree structures. Secondly, the calculation of r˜i and e˜ij would be based on first-order features alone, however, higher-order information pertaining to siblings and grandchildren has proved useful in discourse parsing (Carreras, 2007). We address these issues with an inference algorithm which iteratively infers latent trees. In contrast to multi-layer neural network architectures like the Transformer or Recursive Neural Networks (Tai et al., 2015) where word representations are updated at every layer based on the output of previous layers, we refine only the tree structure during each iteration, word representations are not passed across multiple layers. Empirically, at early iterations, the model learns shallow and 1748 simple trees, and information propagates mostly between neighboring nodes; as the structure gets more r"
N19-1173,P14-1048,0,0.10089,"Missing"
N19-1173,D14-1168,0,0.0867573,"e-like document representations obtained by a parser trained on discourse annotated corpora (Carlson et al., 2003; Prasad et al., 2008). For instance, Marcu (1999) argues that a good summary can be generated by traversing the RST discourse tree structure top-down, following nucleus nodes (discourse units in RST are characterized regarding their text importance; nuclei denote central units, whereas satellites denote peripheral ones). Other work (Hirao et al., 2013; Yoshida et al., 2014) extends this idea by transforming RST trees into dependency trees and generating summaries by tree trimming. Gerani et al. (2014) summarize product reviews; their system aggregates RST trees rep1745 Proceedings of NAACL-HLT 2019, pages 1745–1755 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics 1. One wily coyote traveled a bit too far from home, and its resulting adventure through Harlem had alarmed residents doing a double take and scampering to get out of its way Wednesday morning. Police say frightened New Yorkers reported the coyote sighting around 9:30 a.m., and an emergency service unit was dispatched to find the animal. The little troublemaker was caught and tranquil"
N19-1173,N18-1150,0,0.0583725,"Missing"
N19-1173,D13-1158,0,0.0555327,"Missing"
N19-1173,P16-1046,1,0.938134,"ually sentences) in a document. Recent 1 Our code is publicly available at https://github. com/nlpyang/SUMO. approaches to (single-document) extractive summarization frame the task as a sequence labeling problem taking advantage of the success of neural network architectures (Bahdanau et al., 2015). The idea is to predict a label for each sentence specifying whether it should be included in the summary. Existing systems mostly rely on recurrent neural networks (Hochreiter and Schmidhuber, 1997) to model the document and obtain a vector representation for each sentence (Nallapati et al., 2017; Cheng and Lapata, 2016). Intersentential relations are captured in a sequential manner, without taking the structure of the document into account, although the latter has been shown to correlate with what readers perceive as important in a text (Marcu, 1999). Another problem in neural-based extractive models is the lack of interpretability. While capable of identifying summary sentences, these models are not able to rationalize their predictions (e.g., a sentence is in the summary because it describes important content upon which other related sentences elaborate). The summarization literature offers examples of mod"
N19-1173,D14-1179,0,0.0207797,"Missing"
N19-1173,J10-3005,1,0.827546,"doc-att) outperform L EAD -3 across metrics. S UMO (3-layer) is competitive or better than stateof-the-art approaches. Examples of system output are shown in Table 4. Finally, we should point out that S UMO is superior to Marcu (1999) even though the latter employs linguistically informed document representations. 3.4 Human Evaluation In addition to automatic evaluation, we also assessed system performance by eliciting human judgments. Our first evaluation quantified the degree to which summarization models retain key information from the document following a question-answering (QA) paradigm (Clarke and Lapata, 2010; Narayan et al., 2018). We created a set of questions based on the gold summary under the assumption that it highlights the most important document content. We then examined whether participants were able to answer these questions by reading system summaries alone without access to the article. The more questions a system can answer, the better it is at summarizing the document as a whole. We randomly selected 20 documents from the CNN/DailyMail and NYT datasets, respectively and wrote multiple question-answer pairs for each gold summary. We created 71 questions in total varying from two to s"
N19-1173,D18-1409,0,0.0886611,"Missing"
N19-1173,P16-1188,0,0.493253,"luated S UMO on two benchmark datasets, namely the CNN/DailyMail news highlights dataset (Hermann et al., 2015) and the New York Times Annotated Corpus (NYT; Sandhaus 2008). The CNN/DailyMail dataset contains news articles and associated highlights, i.e., a few bullet points giving a brief overview of the article. We used the standard splits of Hermann et al. (2015) for training, validation, and testing (90,266/1,220/1,093 CNN documents and 196,961/12,148/10,397 DailyMail documents). We did not anonymize entities. The NYT dataset contains 110,540 articles with abstractive summaries. Following Durrett et al. (2016), we split these into 100,834 training and 9,706 test examples, based on date of publication (test is all articles published on January 1, 2007 or later). We also followed their filtering procedure, documents with summaries that are shorter than 50 words were removed from the raw dataset. The 1749 Model R-1 L EAD-3 29.2 Narayan et al. (2018) 30.4 25.6 Marcu (1999) Durrett et al. (2016) — See et al. (2017) — Celikyilmaz et al. (2018) — Transformer (no doc-att) 29.2 Transformer (1-layer doc-att) 29.5 Transformer (3-layer doc-att) 29.6 S UMO (1-layer) 29.5 S UMO (3-layer) 29.7 CNN R-2 R-L 11.2 26"
N19-1173,P17-2074,0,0.0457368,"Missing"
N19-1173,D07-1015,0,0.723746,"Missing"
N19-1173,D18-1149,0,0.0136327,"tability in the summarization process by helping explain how document content contributes to the model’s decisions. We design a new iterative structure refinement algorithm, which learns to induce document-level structures through repeatedly refining the trees predicted by previous iterations and allows the model to infer complex trees which go beyond simple parent-child relations (Liu and Lapata, 2018; Kim et al., 2017). The idea of structure refinement is conceptually related to recently proposed models for solving iterative inference problems (Marino et al., 2018; Putzky and Welling, 2017; Lee et al., 2018). It is also related to structured prediction energy networks (Belanger et al., 2017) which approach structured prediction as iterative miminization of an energy function. However, we are not aware of any previous work considering structure refinement for tree induction problems. Our contributions in this work are three-fold: a novel conceptualization of extractive summarization as a tree induction problem; a model which capitalizes on the notion of structured attention to learn document representations based on iterative structure refinement; and large-scale evaluation studies (both automatic"
N19-1173,W04-1013,0,0.0496642,"was set to 30K. We used 300D word embeddings which were initialized randomly from N (0, 0.01). The sentence-level Transformer has 6 layers and the hidden size of FFN was set to 512. The number of heads in MHAtt was set to 4. Adam was used for training (β1 = 0.9, β2 = 0.999). We adopted the learning rate schedule from Vaswani et al. (2017) with warming-up on the first 8,000 steps. S UMO and related Transformer models produced 3-sentence summaries for each document at test time (for both CNN/DailyMail and NYT datasets). 3.3 Automatic Evaluation We evaluated summarization quality using ROUGE F1 (Lin, 2004). We report unigram and bigram overlap (ROUGE-1 and ROUGE-2) as a means of assessing informativeness and the longest common subsequence (ROUGE-L) as a means of assessing fluency. Table 1 summarizes our results. We evaluated two variants of S UMO, with one and three structured-attention layers. We compared against a baseline which simply selects the first three sentences in each document (L EAD-3) and several incarnations of the basic Transformer model introduced in Section 2.1. These include a Transformer without document-level self-attention and two variants with document-level self attention"
N19-1173,Q18-1005,1,0.934844,"tractive summary is generated. Despite the intuitive appeal of discourse structure for the summarization task, the reliance on a parser which is both expensive to obtain (since it must be trained on labeled data) and error prone, presents a major obstacle to its widespread use. Recognizing the merits of structure-aware representations for various NLP tasks, recent efforts have focused on learning latent structures (e.g., parse trees) while optimizing a neural network model for a down-stream task. Various methods impose structural constraints on the basic attention mechanism (Kim et al., 2017; Liu and Lapata, 2018), formulate structure learning as a reinforcement learning problem (Yogatama et al., 2017; Williams et al., 2018), or sparsify the set of possible structures (Niculae et al., 2018). Although latent structures are mostly induced for individual sentences, Liu and Lapata (2018) induce dependency-like structures for entire documents. Drawing inspiration from this work and existing discourse-informed summarization models (Marcu, 1999; Hirao et al., 2013), we frame extractive summarization as a tree induction problem. Our model represents documents as multiroot dependency trees where each root node"
N19-1173,W01-0100,0,0.421629,"summarizer1 performs competitively against state-of-the-art methods. 1 Introduction Single-document summarization is the task of automatically generating a shorter version of a document while retaining its most important information. The task has received much attention in the natural language processing community due to its potential for various information access applications. Examples include tools which digest textual content (e.g., news, social media, reviews), answer questions, or provide recommendations. Of the many summarization paradigms that have been identified over the years (see Mani 2001 and Nenkova and McKeown 2011 for comprehensive overviews), two have consistently attracted attention. In abstractive summarization, various text rewriting operations generate summaries using words or phrases that were not in the original text, while extractive approaches form summaries by copying and concatenating the most important spans (usually sentences) in a document. Recent 1 Our code is publicly available at https://github. com/nlpyang/SUMO. approaches to (single-document) extractive summarization frame the task as a sequence labeling problem taking advantage of the success of neural n"
N19-1173,D17-1159,1,0.817496,"i and cli represent parent and child vectors, respectively, while vector zil is updated with contextual information at hop l. At the final iteration (lines 9 and 10), the top sentence embeddings v K−1 are used to calculate the final root probabilities rK . We define the model’s loss function as the summation of the losses of all iterations: L= K X [y log(rk ) + (1 − y) log(1 − rk )] (12) k=1 S UMO uses the root probabilities of the top layer as the scores for summary sentences. The k-Hop-Propagation function resembles the computation used in Graph Convolution Networks (Kipf and Welling, 2017; Marcheggiani and Titov, 2017). GCNs have been been recently applied to latent trees (Corro and Titov, 2019), however not in combination with iterative refinement. 3 Experiments In this section we present our experimental setup, describe the summarization datasets we used, discuss implementation details, our evaluation protocol, and analyze our results. Algorithm 2: Structured Summarization Model Input: Document d Output: Root probabilities r K after K iterations 1 Calculate sentence vectors s using sentence-level Transformer TS 0 2 v ←s 3 for k ← 1 to K − 1 do 4 Calculate unnormalized root scores: r˜ik = Wrk vik−1 5 Calcu"
N19-1173,J93-2004,0,0.0707446,"Missing"
N19-1173,N18-1158,1,0.785996,"of Hermann et al. (2015) for training, validation, and testing (90,266/1,220/1,093 CNN documents and 196,961/12,148/10,397 DailyMail documents). We did not anonymize entities. The NYT dataset contains 110,540 articles with abstractive summaries. Following Durrett et al. (2016), we split these into 100,834 training and 9,706 test examples, based on date of publication (test is all articles published on January 1, 2007 or later). We also followed their filtering procedure, documents with summaries that are shorter than 50 words were removed from the raw dataset. The 1749 Model R-1 L EAD-3 29.2 Narayan et al. (2018) 30.4 25.6 Marcu (1999) Durrett et al. (2016) — See et al. (2017) — Celikyilmaz et al. (2018) — Transformer (no doc-att) 29.2 Transformer (1-layer doc-att) 29.5 Transformer (3-layer doc-att) 29.6 S UMO (1-layer) 29.5 S UMO (3-layer) 29.7 CNN R-2 R-L 11.2 26.0 11.7 26.9 6.10 19.5 — — — — — — 11.1 25.6 11.4 26.0 11.8 26.3 11.6 26.2 12.0 26.5 R-1 40.7 41.0 31.9 — — — 40.5 41.5 41.7 41.6 42.0 DM R-2 R-L 18.3 37.2 18.8 37.7 12.4 23.5 — — — — — — 18.1 36.8 18.7 38.0 18.8 38.0 18.8 37.6 19.1 38.0 CNN+DM R-1 R-2 R-L 39.6 17.7 36.2 40.0 18.2 36.6 26.5 9.80 20.4 — — — 39.5 17.3 36.4 41.7 19.5 37.9 39.7"
N19-1173,D18-1108,0,0.0129512,"must be trained on labeled data) and error prone, presents a major obstacle to its widespread use. Recognizing the merits of structure-aware representations for various NLP tasks, recent efforts have focused on learning latent structures (e.g., parse trees) while optimizing a neural network model for a down-stream task. Various methods impose structural constraints on the basic attention mechanism (Kim et al., 2017; Liu and Lapata, 2018), formulate structure learning as a reinforcement learning problem (Yogatama et al., 2017; Williams et al., 2018), or sparsify the set of possible structures (Niculae et al., 2018). Although latent structures are mostly induced for individual sentences, Liu and Lapata (2018) induce dependency-like structures for entire documents. Drawing inspiration from this work and existing discourse-informed summarization models (Marcu, 1999; Hirao et al., 2013), we frame extractive summarization as a tree induction problem. Our model represents documents as multiroot dependency trees where each root node is a summary sentence, and the subtrees attached to it are sentences whose content is related to and covered by the summary sentence. An example of a document and its corresponding"
N19-1173,prasad-etal-2008-penn,0,0.0815454,"le of identifying summary sentences, these models are not able to rationalize their predictions (e.g., a sentence is in the summary because it describes important content upon which other related sentences elaborate). The summarization literature offers examples of models which exploit the structure of the underlying document, inspired by existing theories of discourse such as Rhetorical Structure Theory (RST; Mann and Thompson 1988). Most approaches produce summaries based on tree-like document representations obtained by a parser trained on discourse annotated corpora (Carlson et al., 2003; Prasad et al., 2008). For instance, Marcu (1999) argues that a good summary can be generated by traversing the RST discourse tree structure top-down, following nucleus nodes (discourse units in RST are characterized regarding their text importance; nuclei denote central units, whereas satellites denote peripheral ones). Other work (Hirao et al., 2013; Yoshida et al., 2014) extends this idea by transforming RST trees into dependency trees and generating summaries by tree trimming. Gerani et al. (2014) summarize product reviews; their system aggregates RST trees rep1745 Proceedings of NAACL-HLT 2019, pages 1745–175"
N19-1173,P17-1099,0,0.725316,"66/1,220/1,093 CNN documents and 196,961/12,148/10,397 DailyMail documents). We did not anonymize entities. The NYT dataset contains 110,540 articles with abstractive summaries. Following Durrett et al. (2016), we split these into 100,834 training and 9,706 test examples, based on date of publication (test is all articles published on January 1, 2007 or later). We also followed their filtering procedure, documents with summaries that are shorter than 50 words were removed from the raw dataset. The 1749 Model R-1 L EAD-3 29.2 Narayan et al. (2018) 30.4 25.6 Marcu (1999) Durrett et al. (2016) — See et al. (2017) — Celikyilmaz et al. (2018) — Transformer (no doc-att) 29.2 Transformer (1-layer doc-att) 29.5 Transformer (3-layer doc-att) 29.6 S UMO (1-layer) 29.5 S UMO (3-layer) 29.7 CNN R-2 R-L 11.2 26.0 11.7 26.9 6.10 19.5 — — — — — — 11.1 25.6 11.4 26.0 11.8 26.3 11.6 26.2 12.0 26.5 R-1 40.7 41.0 31.9 — — — 40.5 41.5 41.7 41.6 42.0 DM R-2 R-L 18.3 37.2 18.8 37.7 12.4 23.5 — — — — — — 18.1 36.8 18.7 38.0 18.8 38.0 18.8 37.6 19.1 38.0 CNN+DM R-1 R-2 R-L 39.6 17.7 36.2 40.0 18.2 36.6 26.5 9.80 20.4 — — — 39.5 17.3 36.4 41.7 19.5 37.9 39.7 17.0 35.9 40.6 18.1 36.7 40.6 18.1 36.9 40.5 18.0 36.8 41.0 18.4"
N19-1173,P15-1150,0,0.168463,"Missing"
N19-1173,Q18-1019,0,0.0619806,"Missing"
N19-1173,D14-1196,0,0.0219041,"ries of discourse such as Rhetorical Structure Theory (RST; Mann and Thompson 1988). Most approaches produce summaries based on tree-like document representations obtained by a parser trained on discourse annotated corpora (Carlson et al., 2003; Prasad et al., 2008). For instance, Marcu (1999) argues that a good summary can be generated by traversing the RST discourse tree structure top-down, following nucleus nodes (discourse units in RST are characterized regarding their text importance; nuclei denote central units, whereas satellites denote peripheral ones). Other work (Hirao et al., 2013; Yoshida et al., 2014) extends this idea by transforming RST trees into dependency trees and generating summaries by tree trimming. Gerani et al. (2014) summarize product reviews; their system aggregates RST trees rep1745 Proceedings of NAACL-HLT 2019, pages 1745–1755 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics 1. One wily coyote traveled a bit too far from home, and its resulting adventure through Harlem had alarmed residents doing a double take and scampering to get out of its way Wednesday morning. Police say frightened New Yorkers reported the coyote sighting"
N19-1173,D18-1088,1,0.863752,"f-the-art methods while being able to rationalize model predictions. 2 Model Description Let d denote a document containing several sentences [sent1 , sent2 , · · · , sentm ], where senti is the i-th sentence in the document. Extractive summarization can be defined as the task of assigning a label yi ∈ {0, 1} to each senti , indicating whether the sentence should be included in the summary. It is assumed that summary sentences represent the most important content of the document. 1746 2.1 Baseline Model Most extractive models frame summarization as a classification problem. Recent approaches (Zhang et al., 2018; Dong et al., 2018; Nallapati et al., 2017; Cheng and Lapata, 2016) incorporate a neural network-based encoder to build representations for sentences and apply a binary classifier over these representations to predict whether the sentences should be included in the summary. Given predicted scores r and gold labels y, the loss function can be defined as: L=− m X For our extractive summarization task, the baseline system is composed of a sentence-level Transformer (TS ) and a document-level Transformer (TD ), which have the same structure. For each sentence si = [wi1 , wi2 , · · · , win ] in th"
N19-1173,D17-1225,0,0.0143628,"attention instantiated with one and three layers. Several stateof-the-art models are also included in Table 1, both extractive and abstractive. R EFRESH (Narayan et al., 2018) is an extractive summarization system trained by globally optimizing the ROUGE metric with reinforcement learning. The system of Marcu (1999) is another extractive summarizer based on RST parsing. It uses discourse structures and RST’s notion of nuclearity to score document sentences in terms of their importance and selects the most important ones as the summary. Our re-implementation of Marcu (1999) used the parser of Zhao and Huang (2017) to obtain RST trees. Durrett et al. (2016) develop a summarization system which integrates a compression model that enforces grammaticality and coherence. See et al. (2017) present an abstractive summarization system based on 1750 an encoder-decoder architecture. Celikyilmaz et al.’s (2018) system is state-of-the-art in abstractive summarization using multiple agents to represent the document as well a hierarchical attention mechanism over the agents for decoding. As far as S UMO is concerned, we observe that it outperforms a simple Transformer model without any document attention as well as"
N19-1240,D17-1209,1,0.857368,"tch at test time. CorefGRU, similarly to us, encodes relations between entity mentions in the document. Instead of using graph neural network layers, as we do, they augment RNNs with jump links corresponding to pairs of corefereed mentions. MHPGM uses a multi-attention mechanism in combination with external commonsense relations to perform multiple hops of reasoning. Weaver is a deep coencoding model that uses several alternating biLSTMs to process the concatenated documents and the query. Graph neural networks have been shown successful on a number of NLP tasks (Marcheggiani and Titov, 2017; Bastings et al., 2017; Zhang et al., 2313 2018a), including those involving document level modeling (Peng et al., 2017). They have also been applied in the context of asking questions about knowledge contained in a knowledge base (Zhang et al., 2018b). In Schlichtkrull et al. (2018), GCNs are used to capture reasoning chains in a knowledge base. Our work and unpublished concurrent work by Song et al. (2018) are the first to study graph neural networks in the context of multidocument QA. Besides differences in the architecture, Song et al. (2018) propose to train a combination of a graph recurrent network and an RN"
N19-1240,D18-1454,0,0.122338,"Missing"
N19-1240,N18-2007,0,0.53592,"ttention. The methods reported by Welbl et al. (2018) approach the task by merely concatenating all documents into a single long text and training a standard RNN-based reading comprehension model, namely, BiDAF (Seo et al., 2016) and FastQA (Weissenborn et al., 2017). Document concatenation in this setting is also used in Weaver (Raison et al., 2018) and MHPGM (Bauer et al., 2018). The only published paper which 2306 Proceedings of NAACL-HLT 2019, pages 2306–2317 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics goes beyond concatenation is due to Dhingra et al. (2018), where they augment RNNs with jump-links corresponding to co-reference edges. Though these edges provide a structural bias, the RNN states are still tasked with passing the information across the document and performing multihop reasoning. Instead, we frame question answering as an inference problem on a graph representing the document collection. Nodes in this graph correspond to named entities in a document whereas edges encode relations between them (e.g., crossand within-document coreference links or simply co-occurrence in a document). We assume that reasoning chains can be captured by p"
N19-1240,P17-1147,0,0.0654028,"2016) and CNN/Daily Mail (Hermann et al., 2015), enabling end-to-end training of neural models (Seo et al., 2016; Xiong et al., 2016; Shen et al., 2017). These systems, given a text and a question, need to answer the query relying on the given document. Recently, it has been observed that most questions in these datasets do not require reasoning across the document, but they can be answered relying on information contained in a single sentence (Weissenborn et al., 2017). The last generation of large-scale reading comprehension datasets, such as a NarrativeQA (Kocisky et al., 2018), TriviaQA (Joshi et al., 2017), and RACE (Lai et al., 2017), have been created in such a way as to address this shortcoming and to ensure that systems relying only on local information cannot achieve competitive performance. Even though these new datasets are challenging and require reasoning within documents, many question answering and search applications require aggregation of information across multiple documents. The W IKI H OP dataset (Welbl et al., 2018) was explicitly created to facilitate the development of systems dealing with these scenarios. Each example in W IKI H OP consists of a collection of documents, a qu"
N19-1240,Q18-1023,0,0.0288008,"such as SQuAD (Rajpurkar et al., 2016) and CNN/Daily Mail (Hermann et al., 2015), enabling end-to-end training of neural models (Seo et al., 2016; Xiong et al., 2016; Shen et al., 2017). These systems, given a text and a question, need to answer the query relying on the given document. Recently, it has been observed that most questions in these datasets do not require reasoning across the document, but they can be answered relying on information contained in a single sentence (Weissenborn et al., 2017). The last generation of large-scale reading comprehension datasets, such as a NarrativeQA (Kocisky et al., 2018), TriviaQA (Joshi et al., 2017), and RACE (Lai et al., 2017), have been created in such a way as to address this shortcoming and to ensure that systems relying only on local information cannot achieve competitive performance. Even though these new datasets are challenging and require reasoning within documents, many question answering and search applications require aggregation of information across multiple documents. The W IKI H OP dataset (Welbl et al., 2018) was explicitly created to facilitate the development of systems dealing with these scenarios. Each example in W IKI H OP consists of"
N19-1240,D17-1082,0,0.0373422,"ann et al., 2015), enabling end-to-end training of neural models (Seo et al., 2016; Xiong et al., 2016; Shen et al., 2017). These systems, given a text and a question, need to answer the query relying on the given document. Recently, it has been observed that most questions in these datasets do not require reasoning across the document, but they can be answered relying on information contained in a single sentence (Weissenborn et al., 2017). The last generation of large-scale reading comprehension datasets, such as a NarrativeQA (Kocisky et al., 2018), TriviaQA (Joshi et al., 2017), and RACE (Lai et al., 2017), have been created in such a way as to address this shortcoming and to ensure that systems relying only on local information cannot achieve competitive performance. Even though these new datasets are challenging and require reasoning within documents, many question answering and search applications require aggregation of information across multiple documents. The W IKI H OP dataset (Welbl et al., 2018) was explicitly created to facilitate the development of systems dealing with these scenarios. Each example in W IKI H OP consists of a collection of documents, a query and a set of candidate an"
N19-1240,D17-1018,0,0.0235344,"n and across supporting documents. For a given query q = hs, r, ?i, we identify mentions in Sq of the entities in Cq ∪ {s} and create one node per mention. This process is based on the following heuristic: 1. we consider mentions spans in Sq exactly matching an element of Cq ∪ {s}. Admittedly, this is a rather simple strategy which may suffer from low recall. 2. we use predictions from a coreference resolution system to add mentions of elements in Cq ∪ {s} beyond exact matching (including both noun phrases and anaphoric pronouns). In particular, we use the end-to-end coreference resolution by Lee et al. (2017). 3. we discard mentions which are ambiguously resolved to multiple coreference chains; this may sacrifice recall, but avoids propagating ambiguity. Figure 2: Supporting documents (dashed ellipses) organized as a graph where nodes are mentions of either candidate entities or query entities. Nodes with the same color indicates they refer to the same entity (exact match, coreference or both). Nodes are connected by three simple relations: one indicating co-occurrence in the same document (solid edges), another connecting mentions that exactly match (dashed edges), and a third one indicating a co"
N19-1240,D17-1159,1,0.839662,"cument and evaluating exact match at test time. CorefGRU, similarly to us, encodes relations between entity mentions in the document. Instead of using graph neural network layers, as we do, they augment RNNs with jump links corresponding to pairs of corefereed mentions. MHPGM uses a multi-attention mechanism in combination with external commonsense relations to perform multiple hops of reasoning. Weaver is a deep coencoding model that uses several alternating biLSTMs to process the concatenated documents and the query. Graph neural networks have been shown successful on a number of NLP tasks (Marcheggiani and Titov, 2017; Bastings et al., 2017; Zhang et al., 2313 2018a), including those involving document level modeling (Peng et al., 2017). They have also been applied in the context of asking questions about knowledge contained in a knowledge base (Zhang et al., 2018b). In Schlichtkrull et al. (2018), GCNs are used to capture reasoning chains in a knowledge base. Our work and unpublished concurrent work by Song et al. (2018) are the first to study graph neural networks in the context of multidocument QA. Besides differences in the architecture, Song et al. (2018) propose to train a combination of a graph recu"
N19-1240,Q17-1008,0,0.0355064,"t. Instead of using graph neural network layers, as we do, they augment RNNs with jump links corresponding to pairs of corefereed mentions. MHPGM uses a multi-attention mechanism in combination with external commonsense relations to perform multiple hops of reasoning. Weaver is a deep coencoding model that uses several alternating biLSTMs to process the concatenated documents and the query. Graph neural networks have been shown successful on a number of NLP tasks (Marcheggiani and Titov, 2017; Bastings et al., 2017; Zhang et al., 2313 2018a), including those involving document level modeling (Peng et al., 2017). They have also been applied in the context of asking questions about knowledge contained in a knowledge base (Zhang et al., 2018b). In Schlichtkrull et al. (2018), GCNs are used to capture reasoning chains in a knowledge base. Our work and unpublished concurrent work by Song et al. (2018) are the first to study graph neural networks in the context of multidocument QA. Besides differences in the architecture, Song et al. (2018) propose to train a combination of a graph recurrent network and an RNN encoder. We do not train any RNN document encoders in this work. 6 Conclusion We designed a grap"
N19-1240,D14-1162,0,0.0823299,"icted probability since a candidate answer is realized in multiple locations via different nodes. 2.3 Node Annotations Keeping in mind we want an efficient model, we encode words in supporting documents and in the query using only a pre-trained model for contextualized word representations rather than training our own encoder. Specifically, we use ELMo2 (Peters et al., 2018), a pre-trained bi-directional language model that relies on character-based input representation. ELMo representations, differently from other pre-trained word-based models (e.g., word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014)), are contextualized since each token representation depends on the entire text excerpt (i.e., the whole sentence). We choose not to fine tune nor propagate gradients through the ELMo architecture, as it would have defied the goal of not having specialized RNN encoders. In the experiments, we will also ablate the use of ELMo showing how our model behaves using non-contextualized word representations (we use GloVe). Documents pre-processing ELMo encodings are used to produce a set of representations D {xi }N i=1 , where xi ∈ R denotes the ith candidate mention in context. Note that these repre"
N19-1240,N18-1202,0,0.0625823,"fo ([q, hi ]) , i∈Mc (1) where fo is a parameterized affine transformation, and Mc is the set of node indices such that i ∈ Mc only if node vi is a mention of c. The max operator in Equation 1 is necessary to select the node with highest predicted probability since a candidate answer is realized in multiple locations via different nodes. 2.3 Node Annotations Keeping in mind we want an efficient model, we encode words in supporting documents and in the query using only a pre-trained model for contextualized word representations rather than training our own encoder. Specifically, we use ELMo2 (Peters et al., 2018), a pre-trained bi-directional language model that relies on character-based input representation. ELMo representations, differently from other pre-trained word-based models (e.g., word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014)), are contextualized since each token representation depends on the entire text excerpt (i.e., the whole sentence). We choose not to fine tune nor propagate gradients through the ELMo architecture, as it would have defied the goal of not having specialized RNN encoders. In the experiments, we will also ablate the use of ELMo showing how our model beh"
N19-1240,D16-1264,0,0.139682,".ed.ac.uk Stockholm is the capital of Sweden and the most populous city in [..] query: country Thorildsplan candidates: {Denmark, Finland, Sweden, Italy, ...} answer: Sweden Figure 1: A sample from W IKI H OP where multi-step reasoning and information combination from different documents is necessary to infer the correct answer. Introduction The long-standing goal of natural language understanding is the development of systems which can acquire knowledge from text collections. Fresh interest in reading comprehension tasks was sparked by the availability of large-scale datasets, such as SQuAD (Rajpurkar et al., 2016) and CNN/Daily Mail (Hermann et al., 2015), enabling end-to-end training of neural models (Seo et al., 2016; Xiong et al., 2016; Shen et al., 2017). These systems, given a text and a question, need to answer the query relying on the given document. Recently, it has been observed that most questions in these datasets do not require reasoning across the document, but they can be answered relying on information contained in a single sentence (Weissenborn et al., 2017). The last generation of large-scale reading comprehension datasets, such as a NarrativeQA (Kocisky et al., 2018), TriviaQA (Joshi"
N19-1240,K17-1028,0,0.496915,"t collections. Fresh interest in reading comprehension tasks was sparked by the availability of large-scale datasets, such as SQuAD (Rajpurkar et al., 2016) and CNN/Daily Mail (Hermann et al., 2015), enabling end-to-end training of neural models (Seo et al., 2016; Xiong et al., 2016; Shen et al., 2017). These systems, given a text and a question, need to answer the query relying on the given document. Recently, it has been observed that most questions in these datasets do not require reasoning across the document, but they can be answered relying on information contained in a single sentence (Weissenborn et al., 2017). The last generation of large-scale reading comprehension datasets, such as a NarrativeQA (Kocisky et al., 2018), TriviaQA (Joshi et al., 2017), and RACE (Lai et al., 2017), have been created in such a way as to address this shortcoming and to ensure that systems relying only on local information cannot achieve competitive performance. Even though these new datasets are challenging and require reasoning within documents, many question answering and search applications require aggregation of information across multiple documents. The W IKI H OP dataset (Welbl et al., 2018) was explicitly creat"
N19-1240,Q18-1021,0,0.164649,"paragraphs. We introduce a neural model which integrates and reasons relying on information spread within documents and across multiple documents. We frame it as an inference problem on a graph. Mentions of entities are nodes of this graph while edges encode relations between different mentions (e.g., within- and crossdocument coreference). Graph convolutional networks (GCNs) are applied to these graphs and trained to perform multi-step reasoning. Our Entity-GCN method is scalable and compact, and it achieves state-of-the-art results on a multi-document question answering dataset, W IKI H OP (Welbl et al., 2018). 1 Ivan Titov University of Edinburgh University of Amsterdam ititov@inf.ed.ac.uk Stockholm is the capital of Sweden and the most populous city in [..] query: country Thorildsplan candidates: {Denmark, Finland, Sweden, Italy, ...} answer: Sweden Figure 1: A sample from W IKI H OP where multi-step reasoning and information combination from different documents is necessary to infer the correct answer. Introduction The long-standing goal of natural language understanding is the development of systems which can acquire knowledge from text collections. Fresh interest in reading comprehension tasks"
N19-1240,D18-1244,0,0.0500509,"mentions. MHPGM uses a multi-attention mechanism in combination with external commonsense relations to perform multiple hops of reasoning. Weaver is a deep coencoding model that uses several alternating biLSTMs to process the concatenated documents and the query. Graph neural networks have been shown successful on a number of NLP tasks (Marcheggiani and Titov, 2017; Bastings et al., 2017; Zhang et al., 2313 2018a), including those involving document level modeling (Peng et al., 2017). They have also been applied in the context of asking questions about knowledge contained in a knowledge base (Zhang et al., 2018b). In Schlichtkrull et al. (2018), GCNs are used to capture reasoning chains in a knowledge base. Our work and unpublished concurrent work by Song et al. (2018) are the first to study graph neural networks in the context of multidocument QA. Besides differences in the architecture, Song et al. (2018) propose to train a combination of a graph recurrent network and an RNN encoder. We do not train any RNN document encoders in this work. 6 Conclusion We designed a graph neural network that operates over a compact graph representation of a set of documents where nodes are mentions to entities and"
P05-1023,E03-1005,0,0.023571,"Missing"
P05-1023,A00-2018,0,0.0254505,". These weights define a normalized exponential model, with the network’s hidden layer as the input features. When we tried using the complete set of weights in some small scale experiments, training the classifier was more computationally expensive, and actually performed slightly worse than just using the output weights. Using just the output weights also allows us to make some approximations in the TOP reranking kernel which makes the classifier learning algorithm more efficient. 3.1 A History-Based Probability Model As with many other statistical parsers (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2000), Henderson (2003) uses a history-based model of parsing. He defines the mapping from phrase structure trees to parse sequences using a form of left-corner parsing strategy (see (Henderson, 2003) for more details). The parser actions include: introducing a new constituent with a specified label, attaching one constituent to another, and predicting the next word of the sentence. A complete parse consists of a sequence of these actions, d1 ,..., dm , such that performing d1 ,..., dm results in a complete phrase structure tree. Because this mapping to parse sequences is one-to-one, and the word p"
P05-1023,P02-1034,0,0.592804,"ize measures related directly to expected testing performance (i.e. “large margin” methods), rather than the probabilistic measures used in statistical models, which are only indirectly related to expected testing performance. Work on kernel methods in natural Ivan Titov Department of Computer Science University of Geneva 24, rue G´en´eral Dufour CH-1211 Gen`eve 4, Switzerland ivan.titov@cui.unige.ch language has focussed on the definition of appropriate kernels for natural language tasks. In particular, most of the work on parsing with kernel methods has focussed on kernels over parse trees (Collins and Duffy, 2002; Shen and Joshi, 2003; Shen et al., 2003; Collins and Roark, 2004). These kernels have all been hand-crafted to try reflect properties of parse trees which are relevant to discriminating correct parse trees from incorrect ones, while at the same time maintaining the tractability of learning. Some work in machine learning has taken an alternative approach to defining kernels, where the kernel is derived from a probabilistic model of the task (Jaakkola and Haussler, 1998; Tsuda et al., 2002). This way of defining kernels has two advantages. First, linguistic knowledge about parsing is reflected"
P05-1023,P04-1015,0,0.0403825,". “large margin” methods), rather than the probabilistic measures used in statistical models, which are only indirectly related to expected testing performance. Work on kernel methods in natural Ivan Titov Department of Computer Science University of Geneva 24, rue G´en´eral Dufour CH-1211 Gen`eve 4, Switzerland ivan.titov@cui.unige.ch language has focussed on the definition of appropriate kernels for natural language tasks. In particular, most of the work on parsing with kernel methods has focussed on kernels over parse trees (Collins and Duffy, 2002; Shen and Joshi, 2003; Shen et al., 2003; Collins and Roark, 2004). These kernels have all been hand-crafted to try reflect properties of parse trees which are relevant to discriminating correct parse trees from incorrect ones, while at the same time maintaining the tractability of learning. Some work in machine learning has taken an alternative approach to defining kernels, where the kernel is derived from a probabilistic model of the task (Jaakkola and Haussler, 1998; Tsuda et al., 2002). This way of defining kernels has two advantages. First, linguistic knowledge about parsing is reflected in the design of the probabilistic model, not directly in the kern"
P05-1023,N03-1014,1,0.783763,"g. 1 For example, see (Henderson, 2004) for a discussion of why generative models are better than models parameterized to estimate the a posteriori probability directly. 181 Proceedings of the 43rd Annual Meeting of the ACL, pages 181–188, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics In this paper, we propose a new method for deriving a kernel from a probabilistic model which is specifically tailored to reranking tasks, and we apply this method to natural language parsing. For the probabilistic model, we use a state-of-the-art neural network based statistical parser (Henderson, 2003). The resulting kernel is then used with the Voted Perceptron algorithm (Freund and Schapire, 1998) to reranking the top 20 parses from the probabilistic model. This method achieves a significant improvement over the accuracy of the probabilistic model alone. 2 Kernels Derived from Probabilistic Models In recent years, several methods have been proposed for constructing kernels from trained probabilistic models. As usual, these kernels are then used with linear classifiers to learn the desired task. As well as some empirical successes, these methods are motivated by theoretical results which s"
P05-1023,P04-1013,1,0.825686,"tly well understood, both in terms of reflecting generalizations and controlling computational cost. Because many NLP problems are unbounded in size and complexity, it is hard to specify all possible relevant kernel features without having so many features that the computations become intractable and/or the data becomes too sparse.1 Second, the kernel is defined using the trained parameters of the probabilistic model. Thus the kernel is in part determined by the training data, and is automatically tailored to reflect properties of parse trees which are relevant to parsing. 1 For example, see (Henderson, 2004) for a discussion of why generative models are better than models parameterized to estimate the a posteriori probability directly. 181 Proceedings of the 43rd Annual Meeting of the ACL, pages 181–188, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics In this paper, we propose a new method for deriving a kernel from a probabilistic model which is specifically tailored to reranking tasks, and we apply this method to natural language parsing. For the probabilistic model, we use a state-of-the-art neural network based statistical parser (Henderson, 2003). The resulting kernel"
P05-1023,J93-2004,0,0.0296727,"mputation and the parameters of the mappings h(d1 ,..., di−1 ). With multilayered networks such as SSNs, this training is not guaranteed to converge to a global optimum, but in practice a network whose criteria value is close to the optimum can be found. 4 Large-Margin Optimization Once we have defined a kernel over parse trees, general techniques for linear classifier optimization can be used to learn the given task. The most sophisticated of these techniques (such as Support Vector Machines) are unfortunately too computationally expensive to be used on large datasets like the Penn Treebank (Marcus et al., 1993). Instead we use a method which has often been shown to be virtually as good, the Voted Perceptron (VP) (Freund and Schapire, 1998) algorithm. The VP algorithm was originally applied to parse reranking in (Collins and Duffy, 2002) with the Tree kernel. We modify the perceptron training algorithm to make it more suitable for parsing, where zero-one classification loss is not the evaluation measure usually employed. We also develop a variant of the kernel defined in section 2.3, which is more efficient when used with the VP algorithm. Given a list of candidate trees, we train the classifier to s"
P05-1023,W96-0213,0,0.182419,"are present in the candidate derivation for the sentence, and thus in the first vector component. We have applied this technique to the TOP reranking kernel, the result of which we will call the efficient TOP reranking kernel. 5 The Experimental Results We used the Penn Treebank WSJ corpus (Marcus et al., 1993) to perform empirical experiments on the proposed parsing models. In each case the input to the network is a sequence of tag-word pairs.2 We report results for two different vocabulary sizes, varying in the frequency with which tag-word pairs must 2 We used a publicly available tagger (Ratnaparkhi, 1996) to provide the tags. occur in the training set in order to be included explicitly in the vocabulary. A frequency threshold of 200 resulted in a vocabulary of 508 tag-word pairs (including tag-unknown word pairs) and a threshold of 20 resulted in 4215 tag-word pairs. We denote the probabilistic model trained with the vocabulary of 508 by the SSN-Freq≥200, the model trained with the vocabulary of 4215 by the SSN-Freq≥20. Testing the probabilistic parser requires using a beam search through the space of possible parses. We used a form of beam search which prunes the search after the prediction o"
P05-1023,W03-0402,0,0.551335,"ctly to expected testing performance (i.e. “large margin” methods), rather than the probabilistic measures used in statistical models, which are only indirectly related to expected testing performance. Work on kernel methods in natural Ivan Titov Department of Computer Science University of Geneva 24, rue G´en´eral Dufour CH-1211 Gen`eve 4, Switzerland ivan.titov@cui.unige.ch language has focussed on the definition of appropriate kernels for natural language tasks. In particular, most of the work on parsing with kernel methods has focussed on kernels over parse trees (Collins and Duffy, 2002; Shen and Joshi, 2003; Shen et al., 2003; Collins and Roark, 2004). These kernels have all been hand-crafted to try reflect properties of parse trees which are relevant to discriminating correct parse trees from incorrect ones, while at the same time maintaining the tractability of learning. Some work in machine learning has taken an alternative approach to defining kernels, where the kernel is derived from a probabilistic model of the task (Jaakkola and Haussler, 1998; Tsuda et al., 2002). This way of defining kernels has two advantages. First, linguistic knowledge about parsing is reflected in the design of the"
P05-1023,W03-1012,0,0.0588209,"ng performance (i.e. “large margin” methods), rather than the probabilistic measures used in statistical models, which are only indirectly related to expected testing performance. Work on kernel methods in natural Ivan Titov Department of Computer Science University of Geneva 24, rue G´en´eral Dufour CH-1211 Gen`eve 4, Switzerland ivan.titov@cui.unige.ch language has focussed on the definition of appropriate kernels for natural language tasks. In particular, most of the work on parsing with kernel methods has focussed on kernels over parse trees (Collins and Duffy, 2002; Shen and Joshi, 2003; Shen et al., 2003; Collins and Roark, 2004). These kernels have all been hand-crafted to try reflect properties of parse trees which are relevant to discriminating correct parse trees from incorrect ones, while at the same time maintaining the tractability of learning. Some work in machine learning has taken an alternative approach to defining kernels, where the kernel is derived from a probabilistic model of the task (Jaakkola and Haussler, 1998; Tsuda et al., 2002). This way of defining kernels has two advantages. First, linguistic knowledge about parsing is reflected in the design of the probabilistic model"
P05-1023,W04-3201,0,0.0508062,"he position in the list ordered by the F1 score. We expect that an improvement could be achieved by combining our approach of scaling updates by the F1 loss with the all pairs approach of (Shen and Joshi, 2004). Use of the F1 loss function during training demonstrated 187 better performance comparing to the 0-1 loss function when applied to a structured classification task (Tsochantaridis et al., 2004). All the described kernel methods are limited to the reranking of candidates from an existing parser due to the complexity of finding the best parse given a kernel (i.e. the decoding problem). (Taskar et al., 2004) suggested a method for maximal margin parsing which employs the dynamic programming approach to decoding and parameter estimation problems. The efficiency of dynamic programming means that the entire space of parses can be considered, not just a candidate list. However, not all kernels are suitable for this method. The dynamic programming approach requires the feature vector of a tree to be decomposable into a sum over parts of the tree. In particular, this is impossible with the TOP and Fisher kernels derived from the SSN model. Also, it isn’t clear whether the algorithm remains tractable fo"
P05-1023,C00-2137,0,0.222983,"Missing"
P05-1023,J03-4003,0,\N,Missing
P07-1080,P05-1010,0,0.0330288,"ite accurate parsing models. The main drawback of our proposed mean field approach is the relative computational complexity of the numerical procedure used to maximize Lt,k V . But this approximation has succeeded in showing that a more accurate approximation of ISBNs results in a more accurate parser. We believe this provides strong justification for more accurate approximations of ISBNs for parsing. 5 Related Work There has not been much previous work on graphical models for full parsing, although recently several latent variable models for parsing have been proposed (Koo and Collins, 2005; Matsuzaki et al., 2005; Riezler et al., 2002). In (Koo and Collins, 2005), an undirected graphical model is used for parse reranking. Dependency parsing with dynamic Bayesian networks was considered in (Peshkin and Savova, 2005), with limited success. Their model is very different from ours. Roughly, it considered the whole sentence at a time, with the graphical model being used to decide which words correspond to leaves of the tree. The chosen words are then removed from the sentence and the model is recursively applied to the reduced sentence. Undirected graphical models, in particular Condi3 We measured signific"
P07-1080,P05-1022,0,0.0572489,"ion of the graph, as would be necessary for an undirected graphical model. 634 Figure 1: Illustration of an ISBN. 3 The Probabilistic Model of Parsing In this section we present our framework for syntactic parsing with dynamic Sigmoid Belief Networks. We first specify the form of SBN we propose, namely ISBNs, and then two methods for approximating the inference problems required for parsing. We only consider generative models of parsing, since generative probability models are simpler and we are focused on probability estimation, not decision making. Although the most accurate parsing models (Charniak and Johnson, 2005; Henderson, 2004; Collins, 2000) are discriminative, all the most accurate discriminative models make use of a generative model. More accurate generative models should make the discriminative models which use them more accurate as well. Also, there are some applications, such as language modeling, which require generative models. 3.1 The Graphical Model In ISBNs, we use a history-based model, which decomposes the probability of the parse as: P (T ) = P (D 1 , ..., Dm ) = Y P (Dt |D1 , . . . , Dt−1 ), t where T is the parse tree and D 1 , . . . , Dm is its equivalent sequence of parser decisio"
P07-1080,A00-2018,0,0.928079,"e. However, they can be approximated sufficiently well to build fast and accurate statistical parsers which induce features during training. We use SBNs in a generative history-based model of constituent structure parsing. The probability of an unbounded structure is decomposed into a sequence of probabilities for individual derivation decisions, each decision conditioned on the unbounded history of previous decisions. The most common approach to handling the unbounded nature of the histories is to choose a pre-defined set of features which can be unambiguously derived from the history (e.g. (Charniak, 2000; Collins, 1999)). Decision probabilities are then assumed to be independent of all information not represented by this finite set of features. Another previous approach is to use neural networks to compute a compressed representation of the history and condition decisions on this representation (Henderson, 2003; Henderson, 2004). It is possible that an unbounded amount of information is encoded in the compressed representation via its continuous values, but it is not clear whether this is actually happening due to the lack of any principled interpretation for these continuous values. Like the"
P07-1080,W96-0213,0,0.338312,"Missing"
P07-1080,P02-1035,0,0.0191114,"els. The main drawback of our proposed mean field approach is the relative computational complexity of the numerical procedure used to maximize Lt,k V . But this approximation has succeeded in showing that a more accurate approximation of ISBNs results in a more accurate parser. We believe this provides strong justification for more accurate approximations of ISBNs for parsing. 5 Related Work There has not been much previous work on graphical models for full parsing, although recently several latent variable models for parsing have been proposed (Koo and Collins, 2005; Matsuzaki et al., 2005; Riezler et al., 2002). In (Koo and Collins, 2005), an undirected graphical model is used for parse reranking. Dependency parsing with dynamic Bayesian networks was considered in (Peshkin and Savova, 2005), with limited success. Their model is very different from ours. Roughly, it considered the whole sentence at a time, with the graphical model being used to decide which words correspond to leaves of the tree. The chosen words are then removed from the sentence and the model is recursively applied to the reduced sentence. Undirected graphical models, in particular Condi3 We measured significance of all the experim"
P07-1080,P05-1023,1,0.877504,"Missing"
P07-1080,N03-1014,1,0.518164,"for individual derivation decisions, each decision conditioned on the unbounded history of previous decisions. The most common approach to handling the unbounded nature of the histories is to choose a pre-defined set of features which can be unambiguously derived from the history (e.g. (Charniak, 2000; Collins, 1999)). Decision probabilities are then assumed to be independent of all information not represented by this finite set of features. Another previous approach is to use neural networks to compute a compressed representation of the history and condition decisions on this representation (Henderson, 2003; Henderson, 2004). It is possible that an unbounded amount of information is encoded in the compressed representation via its continuous values, but it is not clear whether this is actually happening due to the lack of any principled interpretation for these continuous values. Like the former approach, we assume that there are a finite set of features which encode the relevant information about the parse history. But unlike that approach, we allow feature values to be ambiguous, and represent each feature as a distribution over (binary) values. In other words, these history features are treat"
P07-1080,N03-1028,0,0.0548333,"as considered in (Peshkin and Savova, 2005), with limited success. Their model is very different from ours. Roughly, it considered the whole sentence at a time, with the graphical model being used to decide which words correspond to leaves of the tree. The chosen words are then removed from the sentence and the model is recursively applied to the reduced sentence. Undirected graphical models, in particular Condi3 We measured significance of all the experiments in this paper with the randomized significance test (Yeh, 2000). 638 tional Random Fields, are the standard tools for shallow parsing (Sha and Pereira, 2003). However, shallow parsing is effectively a sequence labeling problem and therefore differs significantly from full parsing. As discussed in section 2.2, undirected graphical models do not seem to be suitable for historybased full parsing models. Sigmoid Belief Networks were used originally for character recognition tasks, but later a dynamic modification of this model was applied to the reinforcement learning task (Sallans, 2002). However, their graphical model, approximation method, and learning method differ significantly from those of this paper. 6 Conclusions This paper proposes a new gen"
P07-1080,P04-1013,1,0.948169,"rivation decisions, each decision conditioned on the unbounded history of previous decisions. The most common approach to handling the unbounded nature of the histories is to choose a pre-defined set of features which can be unambiguously derived from the history (e.g. (Charniak, 2000; Collins, 1999)). Decision probabilities are then assumed to be independent of all information not represented by this finite set of features. Another previous approach is to use neural networks to compute a compressed representation of the history and condition decisions on this representation (Henderson, 2003; Henderson, 2004). It is possible that an unbounded amount of information is encoded in the compressed representation via its continuous values, but it is not clear whether this is actually happening due to the lack of any principled interpretation for these continuous values. Like the former approach, we assume that there are a finite set of features which encode the relevant information about the parse history. But unlike that approach, we allow feature values to be ambiguous, and represent each feature as a distribution over (binary) values. In other words, these history features are treated as latent varia"
P07-1080,W04-3201,0,0.0503689,"Missing"
P07-1080,P06-1110,0,0.0245139,"Missing"
P07-1080,C00-2137,0,0.0119761,"l is used for parse reranking. Dependency parsing with dynamic Bayesian networks was considered in (Peshkin and Savova, 2005), with limited success. Their model is very different from ours. Roughly, it considered the whole sentence at a time, with the graphical model being used to decide which words correspond to leaves of the tree. The chosen words are then removed from the sentence and the model is recursively applied to the reduced sentence. Undirected graphical models, in particular Condi3 We measured significance of all the experiments in this paper with the randomized significance test (Yeh, 2000). 638 tional Random Fields, are the standard tools for shallow parsing (Sha and Pereira, 2003). However, shallow parsing is effectively a sequence labeling problem and therefore differs significantly from full parsing. As discussed in section 2.2, undirected graphical models do not seem to be suitable for historybased full parsing models. Sigmoid Belief Networks were used originally for character recognition tasks, but later a dynamic modification of this model was applied to the reinforcement learning task (Sallans, 2002). However, their graphical model, approximation method, and learning met"
P07-1080,H05-1064,0,0.0190351,"n, allow us to build quite accurate parsing models. The main drawback of our proposed mean field approach is the relative computational complexity of the numerical procedure used to maximize Lt,k V . But this approximation has succeeded in showing that a more accurate approximation of ISBNs results in a more accurate parser. We believe this provides strong justification for more accurate approximations of ISBNs for parsing. 5 Related Work There has not been much previous work on graphical models for full parsing, although recently several latent variable models for parsing have been proposed (Koo and Collins, 2005; Matsuzaki et al., 2005; Riezler et al., 2002). In (Koo and Collins, 2005), an undirected graphical model is used for parse reranking. Dependency parsing with dynamic Bayesian networks was considered in (Peshkin and Savova, 2005), with limited success. Their model is very different from ours. Roughly, it considered the whole sentence at a time, with the graphical model being used to decide which words correspond to leaves of the tree. The chosen words are then removed from the sentence and the model is recursively applied to the reduced sentence. Undirected graphical models, in particular Con"
P07-1080,J93-2004,0,\N,Missing
P07-1080,J04-4004,0,\N,Missing
P07-1080,J03-4003,0,\N,Missing
P08-1036,P08-1031,0,0.0513063,"Missing"
P08-1036,E06-1039,0,0.101719,".g., topics in blogs, numerical product and service ratings in user reviews, and helpfulness rankings in online discussion forums. Many previous studies on user generated content have attempted to predict these labels automatically from the associated text. However, these labels are often present in the data already, which opens another interesting line of research: designing models leveraging these labelings to improve a wide variety of applications. In this study, we look at the problem of aspectbased sentiment summarization (Hu and Liu, 2004a; Popescu and Etzioni, 2005; Gamon et al., 2005; Carenini et al., 2006; Zhuang et al., 2006).1 An aspect-based summarization system takes as input a set of user reviews for a specific product or service and produces a set of relevant aspects, the aggregated sentiment for each aspect, and supporting textual evidence. For example, figure 1 summarizes a restaurant using aspects food, decor, service, and value plus a numeric rating out of 5. Standard aspect-based summarization consists of two problems. The first is aspect identification and mention extraction. Here the goal is to find the set of relevant aspects for a rated entity and extract all textual mentions th"
P08-1036,W02-1011,0,0.0441681,"set of relevant aspects for a rated entity and extract all textual mentions that are associated with each. Aspects can be fine-grained, e.g., fish, lamb, calamari, or coarse-grained, e.g., food, decor, service. Similarly, extracted text can range from a single word to phrases and sentences. The second problem is sentiment classification. Once all the relevant aspects and associated pieces of texts are extracted, the system should aggregate sentiment over each aspect to provide the user with an average numeric or symbolic rating. Sentiment classification is a well studied problem (Wiebe, 2000; Pang et al., 2002; Turney, 2002) and in many domains users explicitly 1 We use the term aspect to denote properties of an object that can be rated by a user as in Snyder and Barzilay (2007). Other studies use the term feature (Hu and Liu, 2004b). 308 Proceedings of ACL-08: HLT, pages 308–316, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics Food: 5; Decor: 5; Service: 5; Value: 5 The chicken was great. On top of that our service was excellent and the price was right. Can’t wait to go back! Food: 2; Decor: 1; Service: 3; Value: 2 We went there for our anniversary. My soup was col"
P08-1036,H05-1043,0,0.956418,"creation of an abundance of labeled content, e.g., topics in blogs, numerical product and service ratings in user reviews, and helpfulness rankings in online discussion forums. Many previous studies on user generated content have attempted to predict these labels automatically from the associated text. However, these labels are often present in the data already, which opens another interesting line of research: designing models leveraging these labelings to improve a wide variety of applications. In this study, we look at the problem of aspectbased sentiment summarization (Hu and Liu, 2004a; Popescu and Etzioni, 2005; Gamon et al., 2005; Carenini et al., 2006; Zhuang et al., 2006).1 An aspect-based summarization system takes as input a set of user reviews for a specific product or service and produces a set of relevant aspects, the aggregated sentiment for each aspect, and supporting textual evidence. For example, figure 1 summarizes a restaurant using aspects food, decor, service, and value plus a numeric rating out of 5. Standard aspect-based summarization consists of two problems. The first is aspect identification and mention extraction. Here the goal is to find the set of relevant aspects for a rated"
P08-1036,N07-1038,0,0.942091,"or coarse-grained, e.g., food, decor, service. Similarly, extracted text can range from a single word to phrases and sentences. The second problem is sentiment classification. Once all the relevant aspects and associated pieces of texts are extracted, the system should aggregate sentiment over each aspect to provide the user with an average numeric or symbolic rating. Sentiment classification is a well studied problem (Wiebe, 2000; Pang et al., 2002; Turney, 2002) and in many domains users explicitly 1 We use the term aspect to denote properties of an object that can be rated by a user as in Snyder and Barzilay (2007). Other studies use the term feature (Hu and Liu, 2004b). 308 Proceedings of ACL-08: HLT, pages 308–316, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics Food: 5; Decor: 5; Service: 5; Value: 5 The chicken was great. On top of that our service was excellent and the price was right. Can’t wait to go back! Food: 2; Decor: 1; Service: 3; Value: 2 We went there for our anniversary. My soup was cold and expensive plus it felt like they hadn’t painted since 1980. Food: 3; Decor: 5; Service: 4; Value: 5 The food is only mediocre, but well worth the cost. Wait staff was"
P08-1036,P02-1053,0,0.0327733,"ects for a rated entity and extract all textual mentions that are associated with each. Aspects can be fine-grained, e.g., fish, lamb, calamari, or coarse-grained, e.g., food, decor, service. Similarly, extracted text can range from a single word to phrases and sentences. The second problem is sentiment classification. Once all the relevant aspects and associated pieces of texts are extracted, the system should aggregate sentiment over each aspect to provide the user with an average numeric or symbolic rating. Sentiment classification is a well studied problem (Wiebe, 2000; Pang et al., 2002; Turney, 2002) and in many domains users explicitly 1 We use the term aspect to denote properties of an object that can be rated by a user as in Snyder and Barzilay (2007). Other studies use the term feature (Hu and Liu, 2004b). 308 Proceedings of ACL-08: HLT, pages 308–316, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics Food: 5; Decor: 5; Service: 5; Value: 5 The chicken was great. On top of that our service was excellent and the price was right. Can’t wait to go back! Food: 2; Decor: 1; Service: 3; Value: 2 We went there for our anniversary. My soup was cold and expensive"
P08-1036,H05-2017,0,\N,Missing
P10-1098,W02-1022,0,0.0331915,"ning. There are many directions we plan on investigating in the future for the problem of learning semantics with non-contradictory relations. A promising and challenging possibility is to consider models which induce full semantic representations of meaning. Another direction would be to investigate purely unsupervised set-up, though it would make evaluation of the resulting method much more complex. One potential alternative would be to replace the initial supervision with a set of posterior constraints (Graca et al., 2008) or generalized expectation criteria (McCallum et al., 2007). sages (Barzilay and Lee, 2002), however, in their work all the passages were annotated with unaligned semantic expressions. Also, they assumed that the passages are paraphrases of each other, which is stronger than our non-contradiction assumption. Sentence and text alignment has also been considered in the related context of paraphrase extraction (see, e.g., (Dolan et al., 2004; Barzilay and Lee, 2003)) but this prior work did not focus on inducing or learning semantic representations. Similarly, in information extraction, there have been approaches for pattern discovery using comparable monolingual corpora (Shinyama and"
P10-1098,N03-1003,0,0.23311,"aspect of this algorithm is that unlike usual greedy inference, the remaining (‘future’) texts do affect the choice of meaning representations made on the earlier stages. As soon as semantics m?k are inferred for every k, we find ourselves in the set-up of learning with unaligned semantic states considered in (Liang et al., 2009). The induced alignments a1 ,..., aK of semantics m? to texts w1 ,..., wK at the same time induce alignments between the texts. The problem of producing multiple sequence alignment, especially in the context of sentence alignments, has been extensively studied in NLP (Barzilay and Lee, 2003). In this paper, we use semantic structures as a pivot for finding the best alignment in the hope that presence of meaningful text alignments will improve the quality of the resulting semantic structures by enforcing a form of agreement between them. m?k = arg max P (wk |mk )P (mk |m?&lt;k ). mk Here, and in further discussion, we assume that the above search problem can be efficiently solved, exactly or approximately. However, a major weakness of this algorithm is that decisions about components of the composite semantic representation (e.g., argument values) are made only on the basis of a sing"
P10-1098,W05-0620,0,0.289818,"Missing"
P10-1098,D09-1001,0,0.57248,"earch has focused on supervised methods requiring large amounts of labeled data. The supervision was either given in the form of meaning representations aligned with sentences (Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Mooney, 2007) or in a somewhat more relaxed form, such as lists of candidate meanings for each sentence (Kate and Mooney, 2007; Chen and Mooney, 2008) or formal representations of the described world state for each text (Liang et al., 2009). Such annotated resources are scarce and expensive to create, motivating the need for unsupervised or semi-supervised techniques (Poon and Domingos, 2009). However, unsupervised methods have their own challenges: they are not always able to discover semantic equivalences of lexical entries or logical forms or, on the contrary, cluster semantically different or even opposite expressions (Poon and Domingos, 1 This view on this form of supervision is evocative of cotraining (Blum and Mitchell, 1998) which, roughly, exploits the fact that the same example can be ‘easy’ for one model but ‘hard’ for another one. 958 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 958–967, c Uppsala, Sweden, 11-16 July 20"
P10-1098,W00-1009,0,0.018905,"hat the passages are paraphrases of each other, which is stronger than our non-contradiction assumption. Sentence and text alignment has also been considered in the related context of paraphrase extraction (see, e.g., (Dolan et al., 2004; Barzilay and Lee, 2003)) but this prior work did not focus on inducing or learning semantic representations. Similarly, in information extraction, there have been approaches for pattern discovery using comparable monolingual corpora (Shinyama and Sekine, 2003) but they generally focused only on discovery of a single pattern from a pair of sentences or texts. Radev (2000) considered types of potential relations between documents, including contradiction, and studied how this information can be exploited in NLP. However, this work considered primarily multi-document summarization and question answering problems. Another related line of research in machine learning is clustering or classification with constraints (Basu et al., 2004), where supervision is given in the form of constraints. Constraints declare which pairs of instances are required to be assigned to the same class (or required to be assigned to different classes). However, we are not aware of any pr"
P10-1098,W03-1609,0,0.12325,"ference to other time periods, underlying the need for modeling alignment between grouped texts and their latent meaning representation. As much of the human knowledge is redescribed multiple times, we believe that noncontradictory and semantically overlapping texts are often easy to obtain. For example, consider semantic analysis of news articles or biographies. In both cases we can find groups of documents referring to the same events or persons, and though they will probably focus on different aspects and have different subjective passages, they are likely to agree on the core information (Shinyama and Sekine, 2003). Alternatively, if such groupings are not available, it may still be easier to give each semantic representation (or a state) to multiple annotators and ask each of them to provide a textual description, instead of annotating texts with semantic expressions. The state can be communi959 garded as defining the probability distribution of meaning m and its alignment a with the given text w, P (m, a, w) = P (a, w|m)P (m). The semantics m can be represented either as a logical formula (see, e.g., (Poon and Domingos, 2009)) or as a set of field values if database records are used as a meaning repre"
P10-1098,C04-1051,0,0.0231935,"ation of the resulting method much more complex. One potential alternative would be to replace the initial supervision with a set of posterior constraints (Graca et al., 2008) or generalized expectation criteria (McCallum et al., 2007). sages (Barzilay and Lee, 2002), however, in their work all the passages were annotated with unaligned semantic expressions. Also, they assumed that the passages are paraphrases of each other, which is stronger than our non-contradiction assumption. Sentence and text alignment has also been considered in the related context of paraphrase extraction (see, e.g., (Dolan et al., 2004; Barzilay and Lee, 2003)) but this prior work did not focus on inducing or learning semantic representations. Similarly, in information extraction, there have been approaches for pattern discovery using comparable monolingual corpora (Shinyama and Sekine, 2003) but they generally focused only on discovery of a single pattern from a pair of sentences or texts. Radev (2000) considered types of potential relations between documents, including contradiction, and studied how this information can be exploited in NLP. However, this work considered primarily multi-document summarization and question"
P10-1098,W05-0602,0,0.0955384,"ive semantics-text correspondence model (Liang et al., 2009) and demonstrate that exploiting the noncontradiction relation between texts leads to substantial improvements over natural baselines on a problem of analyzing human-written weather forecasts. 1 Introduction In recent years, there has been increasing interest in statistical approaches to semantic parsing. However, most of this research has focused on supervised methods requiring large amounts of labeled data. The supervision was either given in the form of meaning representations aligned with sentences (Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Mooney, 2007) or in a somewhat more relaxed form, such as lists of candidate meanings for each sentence (Kate and Mooney, 2007; Chen and Mooney, 2008) or formal representations of the described world state for each text (Liang et al., 2009). Such annotated resources are scarce and expensive to create, motivating the need for unsupervised or semi-supervised techniques (Poon and Domingos, 2009). However, unsupervised methods have their own challenges: they are not always able to discover semantic equivalences of lexical entries or logical forms or, on the contrary, cluster semantically differe"
P10-1098,J05-4002,0,0.0198768,"Missing"
P10-1098,P09-1011,0,0.0751051,"exts, where one text mentions “2.2 bn GBP decrease in profit”, whereas another one includes a passage “profit fell by 2.2 billion pounds”. Even if the model has not observed We argue that groups of unannotated texts with overlapping and non-contradictory semantics represent a valuable source of information for learning semantic representations. A simple and efficient inference method recursively induces joint semantic representations for each group and discovers correspondence between lexical entries and latent semantic concepts. We consider the generative semantics-text correspondence model (Liang et al., 2009) and demonstrate that exploiting the noncontradiction relation between texts leads to substantial improvements over natural baselines on a problem of analyzing human-written weather forecasts. 1 Introduction In recent years, there has been increasing interest in statistical approaches to semantic parsing. However, most of this research has focused on supervised methods requiring large amounts of labeled data. The supervision was either given in the form of meaning representations aligned with sentences (Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Mooney, 2007) or in a somewhat more rel"
P10-2028,P01-1064,0,0.518821,"hese strategies may result in poor segmentation and alignment quality. To address this problem, we construct a nonparametric Bayesian model for joint segmentation and alignment of parallel parts. In comparison with the discussed pipeline approaches, our method has two important advantages: (1) it leverages the lexical cohesion phenomenon (Halliday and Hasan, 1976) in modeling the parallel parts of documents, and (2) ensures that the effective number of segments can grow adaptively. Lexical cohesion is an idea that topicallycoherent segments display compact lexical distributions (Hearst, 1994; Utiyama and Isahara, 2001; Eisenstein and Barzilay, 2008). We hypothesize that not only isolated fragments but also each group of linked fragments displays a compact and consistent lexical distribution, and our generative model leverages this inter-part cohesion assumption. In this paper, we consider the dataset of “English as a second language” (ESL) podcast1 , where each episode consists of two parallel parts: a story (an example monologue or dialogue) and an explanatory lecture discussing the meaning and usage of English expressions appearing in the story. Fig. 1 presents an example episode, consisting of two paral"
P10-2028,W04-3216,0,0.0594615,"Missing"
P10-2028,P06-1039,0,0.0415145,"Missing"
P10-2028,D08-1035,0,0.497884,"in poor segmentation and alignment quality. To address this problem, we construct a nonparametric Bayesian model for joint segmentation and alignment of parallel parts. In comparison with the discussed pipeline approaches, our method has two important advantages: (1) it leverages the lexical cohesion phenomenon (Halliday and Hasan, 1976) in modeling the parallel parts of documents, and (2) ensures that the effective number of segments can grow adaptively. Lexical cohesion is an idea that topicallycoherent segments display compact lexical distributions (Hearst, 1994; Utiyama and Isahara, 2001; Eisenstein and Barzilay, 2008). We hypothesize that not only isolated fragments but also each group of linked fragments displays a compact and consistent lexical distribution, and our generative model leverages this inter-part cohesion assumption. In this paper, we consider the dataset of “English as a second language” (ESL) podcast1 , where each episode consists of two parallel parts: a story (an example monologue or dialogue) and an explanatory lecture discussing the meaning and usage of English expressions appearing in the story. Fig. 1 presents an example episode, consisting of two parallel parts, and their hidden topi"
P10-2028,P94-1002,0,0.857425,"experiments, these strategies may result in poor segmentation and alignment quality. To address this problem, we construct a nonparametric Bayesian model for joint segmentation and alignment of parallel parts. In comparison with the discussed pipeline approaches, our method has two important advantages: (1) it leverages the lexical cohesion phenomenon (Halliday and Hasan, 1976) in modeling the parallel parts of documents, and (2) ensures that the effective number of segments can grow adaptively. Lexical cohesion is an idea that topicallycoherent segments display compact lexical distributions (Hearst, 1994; Utiyama and Isahara, 2001; Eisenstein and Barzilay, 2008). We hypothesize that not only isolated fragments but also each group of linked fragments displays a compact and consistent lexical distribution, and our generative model leverages this inter-part cohesion assumption. In this paper, we consider the dataset of “English as a second language” (ESL) podcast1 , where each episode consists of two parallel parts: a story (an example monologue or dialogue) and an explanatory lecture discussing the meaning and usage of English expressions appearing in the story. Fig. 1 presents an example episo"
P10-2028,J02-4006,0,0.0277398,"cument, depending on their size and structure. To tackle this problem, we use Dirichlet processes (DP) (Ferguson, 1973) to deRelated Work Discourse segmentation has been an active area of research (Hearst, 1994; Utiyama and Isahara, 2001; Galley et al., 2003; Malioutov and Barzilay, 2006). Our work extends the Bayesian segmentation model (Eisenstein and Barzilay, 2008) for isolated texts, to the problem of segmenting parallel parts of documents. The task of aligning each sentence of an abstract to one or more sentences of the body has been studied in the context of summarization (Marcu, 1999; Jing, 2002; Daum´e and Marcu, 2004). Our work is different in that we do not try to extract the most relevant sentence but rather aim to find coherent fragments with maximally overlapping lexical distributions. Similarly, the query-focused summarization (e.g., (Daum´e and Marcu, 2006)) is also related but it focuses on sentence extraction rather than on joint segmentation. We are aware of only one previous work on joint segmentation and alignment of multiple texts (Sun et al., 2007) but their approach is based on similarity functions rather than on modeling lexical cohesion in the generative framework."
P10-2028,P06-1004,0,0.315658,"ot to discover collection-level topics (as e.g. in (Blei et al., 2003)), but to perform joint discourse segmentation and alignment. Unlike (Eisenstein and Barzilay, 2008), we cannot make an assumption that the number of segments is known a-priori, as the effective number of part-specific segments can vary significantly from document to document, depending on their size and structure. To tackle this problem, we use Dirichlet processes (DP) (Ferguson, 1973) to deRelated Work Discourse segmentation has been an active area of research (Hearst, 1994; Utiyama and Isahara, 2001; Galley et al., 2003; Malioutov and Barzilay, 2006). Our work extends the Bayesian segmentation model (Eisenstein and Barzilay, 2008) for isolated texts, to the problem of segmenting parallel parts of documents. The task of aligning each sentence of an abstract to one or more sentences of the body has been studied in the context of summarization (Marcu, 1999; Jing, 2002; Daum´e and Marcu, 2004). Our work is different in that we do not try to extract the most relevant sentence but rather aim to find coherent fragments with maximally overlapping lexical distributions. Similarly, the query-focused summarization (e.g., (Daum´e and Marcu, 2006)) is"
P10-2028,P03-1071,0,\N,Missing
P11-1007,W06-1615,0,0.420646,"algorithms operate under assumption that the learning data originates from the same distribution as the test data, though in practice this assumption is often violated. This difference in the data distributions normally results in a significant drop in accuracy. To address In this paper we focus on a more challenging and arguably more realistic version of the domainadaptation problem where only unlabeled data is available for the target domain. One of the most promising research directions on domain adaptation for this setting is based on the idea of inducing a shared feature representation (Blitzer et al., 2006), that is mapping from the initial feature representation to a new representation such that (1) examples from both domains ‘look similar’ and (2) an accurate classifier can be trained in this new representation. Blitzer et al. (2006) use auxiliary tasks based on unlabeled data for both domains (called pivot features) and a dimensionality reduction technique to induce such shared representation. The success of their domain-adaptation method (Structural Correspondence Learning, SCL) crucially depends on the choice of the auxiliary tasks, and defining them can be a non-trivial engineering problem"
P11-1007,P07-1056,0,0.601589,"les are predictive of the label on the source domain. The danger is that these predictive clusters will consist of features specific to the source domain only and, consequently, a classifier relying on such clusters would perform badly on the target domain. We introduce a constraint enforcing that marginal distributions of each cluster (i.e., each latent variable) do not vary significantly across domains. We show that this constraint is effective on the sentiment classification task (Pang et al., 2002), resulting in scores similar to the ones obtained by the structural correspondence methods (Blitzer et al., 2007) without the need to engineer auxiliary tasks. 1 Introduction Supervised learning methods have become a standard tool in natural language processing, and large training sets have been annotated for a wide variety of tasks. However, most learning algorithms operate under assumption that the learning data originates from the same distribution as the test data, though in practice this assumption is often violated. This difference in the data distributions normally results in a significant drop in accuracy. To address In this paper we focus on a more challenging and arguably more realistic version"
P11-1007,W04-3237,0,0.0936552,"performs semi-supervised learning with posterior regularization (Ganchev et al., 2010). Our approach differs from theirs in many respects. First, they do not focus on the domain-adaptation setting and do not attempt to define constraints to prevent the model from learning domain-specific information. Second, their expectation constraints are estimated from labeled data, whereas we are trying to match expectations computed on unlabeled data for two domains. This approach bears some similarity to the adaptation methods standard for the setting where labelled data is available for both domains (Chelba and Acero, 2004; Daum´e and Marcu, 2006). However, instead of ensuring that the classifier parameters are similar across domains, we favor models resulting in similar marginal distributions of latent variables. 7 Discussion and Conclusions In this paper we presented a domain-adaptation method based on semi-supervised learning with distributed representations coupled with constraints favoring domain-independence of modeled phenomena. Our approach results in competitive domainadaptation performance on the sentiment classification task, rivalling that of the state-of-the-art SCL method (Blitzer et al., 2007). B"
P11-1007,W09-1205,1,0.842395,"it labeled data from the target domain. as well as different types of Deep Belief Networks (Hinton and Salakhutdinov, 2006). The power of these methods is in their ability to automatically construct new features from elementary ones provided by the model designer. This feature induction capability is especially desirable for problems where engineering features is a labor-intensive process (e.g., multilingual syntactic parsing (Titov and Henderson, 2007b)), or for multitask learning problems where the nature of interactions between the tasks is not fully understood (Collobert and Weston, 2008; Gesmundo et al., 2009). In this paper we consider classification tasks, namely prediction of sentiment polarity of a user review (Pang et al., 2002), and model the joint distribution of the binary sentiment label y ∈ {0, 1} and the multiset of text features x, xi ∈ X . The hidden variable vector z (zi ∈ {0, 1}, i = 1, . . . , m) encodes statistical dependencies between components of x and also dependencies between the label y and the features x. Intuitively, the model can be regarded as a logistic regression classifier with latent features. The model assumes that the features and the latent variable vector are gene"
P11-1007,P09-1056,0,0.038216,"rvised in-domain classifier considered in Blitzer et al. (2007), otherwise, the computed drop would be larger. are word-based and lexicons are very different for different domains, therefore such assumptions are likely to be overly restrictive. Various semi-supervised techniques for domainadaptation have also been considered, one example being self-training (McClosky et al., 2006). However, their behavior in the domain-adaptation setting is not well-understood. Semi-supervised learning with distributed representations and its application to domain adaptation has previously been considered in (Huang and Yates, 2009), but no attempt has been made to address problems specific to the domain-adaptation setting. Similar approaches has also been considered in the context of topic models (Xue et al., 2008), however the preference towards induction of domain-independent topics was not explicitly encoded in the learning objective or model priors. A closely related method to ours is that of (Druck and McCallum, 2010) which performs semi-supervised learning with posterior regularization (Ganchev et al., 2010). Our approach differs from theirs in many respects. First, they do not focus on the domain-adaptation setti"
P11-1007,P07-1034,0,0.449063,"set of auxiliary tasks is still an open problem. 6 Related Work There is a growing body of work on domain adaptation. In this paper, we focus on the class of methods which induce a shared feature representation. Another popular class of domain-adaptation techniques assume that the input distributions P (x) for the source and the target domain share support, that is every example x which has a non-zero probability on the target domain must have also a non-zero probability on the source domain, and vice-versa. Such methods tackle domain adaptation by instance re-weighting (Bickel et al., 2007; Jiang and Zhai, 2007), or, similarly, by feature re-weighting (Satpal and Sarawagi, 2007). In NLP, most features 5 The drop in accuracy for the SCL method in Table 1 is is computed with respect to the less accurate supervised in-domain classifier considered in Blitzer et al. (2007), otherwise, the computed drop would be larger. are word-based and lexicons are very different for different domains, therefore such assumptions are likely to be overly restrictive. Various semi-supervised techniques for domainadaptation have also been considered, one example being self-training (McClosky et al., 2006). However, their be"
P11-1007,P06-1043,0,0.0526461,"ickel et al., 2007; Jiang and Zhai, 2007), or, similarly, by feature re-weighting (Satpal and Sarawagi, 2007). In NLP, most features 5 The drop in accuracy for the SCL method in Table 1 is is computed with respect to the less accurate supervised in-domain classifier considered in Blitzer et al. (2007), otherwise, the computed drop would be larger. are word-based and lexicons are very different for different domains, therefore such assumptions are likely to be overly restrictive. Various semi-supervised techniques for domainadaptation have also been considered, one example being self-training (McClosky et al., 2006). However, their behavior in the domain-adaptation setting is not well-understood. Semi-supervised learning with distributed representations and its application to domain adaptation has previously been considered in (Huang and Yates, 2009), but no attempt has been made to address problems specific to the domain-adaptation setting. Similar approaches has also been considered in the context of topic models (Xue et al., 2008), however the preference towards induction of domain-independent topics was not explicitly encoded in the learning objective or model priors. A closely related method to ours"
P11-1007,W02-1011,0,0.0170942,"domains. Such a model would cluster features in both domains and ensure that at least some of the latent variables are predictive of the label on the source domain. The danger is that these predictive clusters will consist of features specific to the source domain only and, consequently, a classifier relying on such clusters would perform badly on the target domain. We introduce a constraint enforcing that marginal distributions of each cluster (i.e., each latent variable) do not vary significantly across domains. We show that this constraint is effective on the sentiment classification task (Pang et al., 2002), resulting in scores similar to the ones obtained by the structural correspondence methods (Blitzer et al., 2007) without the need to engineer auxiliary tasks. 1 Introduction Supervised learning methods have become a standard tool in natural language processing, and large training sets have been annotated for a wide variety of tasks. However, most learning algorithms operate under assumption that the learning data originates from the same distribution as the test data, though in practice this assumption is often violated. This difference in the data distributions normally results in a signifi"
P11-1007,E09-3005,0,0.296663,"m the initial feature representation to a new representation such that (1) examples from both domains ‘look similar’ and (2) an accurate classifier can be trained in this new representation. Blitzer et al. (2006) use auxiliary tasks based on unlabeled data for both domains (called pivot features) and a dimensionality reduction technique to induce such shared representation. The success of their domain-adaptation method (Structural Correspondence Learning, SCL) crucially depends on the choice of the auxiliary tasks, and defining them can be a non-trivial engineering problem for many NLP tasks (Plank, 2009). In this paper, we investigate methods which do not use auxiliary tasks to induce a shared feature representation. We use generative latent variable models (LVMs) learned on all the available data: unlabeled data for both domains and on the labeled data for the source domain. Our LVMs use vectors of latent features 62 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 62–71, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics to represent examples. The latent variables encode regularities observed on unlabeled data f"
P11-1007,P07-1080,1,0.801664,"on 6 with further examination of the related work. 2 The Latent Variable Model The adaptation method advocated in this paper is applicable to any joint probabilistic model which uses distributed representations, i.e. vectors of latent variables, to abstract away from hand-crafted features. These models, for example, include Restricted Boltzmann Machines (Smolensky, 1986; Hinton, 2002) and Sigmoid Belief Networks (SBNs) (Saul et al., 1996) for classification and regression tasks, Factorial HMMs (Ghahramani and Jordan, 1997) for sequence labeling problems, Incremental SBNs for parsing problems (Titov and Henderson, 2007a), 1 Among the versions which do not exploit labeled data from the target domain. as well as different types of Deep Belief Networks (Hinton and Salakhutdinov, 2006). The power of these methods is in their ability to automatically construct new features from elementary ones provided by the model designer. This feature induction capability is especially desirable for problems where engineering features is a labor-intensive process (e.g., multilingual syntactic parsing (Titov and Henderson, 2007b)), or for multitask learning problems where the nature of interactions between the tasks is not ful"
P11-1007,D07-1099,1,0.852948,"on 6 with further examination of the related work. 2 The Latent Variable Model The adaptation method advocated in this paper is applicable to any joint probabilistic model which uses distributed representations, i.e. vectors of latent variables, to abstract away from hand-crafted features. These models, for example, include Restricted Boltzmann Machines (Smolensky, 1986; Hinton, 2002) and Sigmoid Belief Networks (SBNs) (Saul et al., 1996) for classification and regression tasks, Factorial HMMs (Ghahramani and Jordan, 1997) for sequence labeling problems, Incremental SBNs for parsing problems (Titov and Henderson, 2007a), 1 Among the versions which do not exploit labeled data from the target domain. as well as different types of Deep Belief Networks (Hinton and Salakhutdinov, 2006). The power of these methods is in their ability to automatically construct new features from elementary ones provided by the model designer. This feature induction capability is especially desirable for problems where engineering features is a labor-intensive process (e.g., multilingual syntactic parsing (Titov and Henderson, 2007b)), or for multitask learning problems where the nature of interactions between the tasks is not ful"
P11-1145,P09-1004,0,0.194803,"f relation extraction has focused on unsupervised or minimally supervised methods (see, for example, (Lin and Pantel, 2001; Yates and Etzioni, 2009)). These approaches cluster semantically equivalent verbalizations of relations, often relying on syntactic fragments as features for relation extraction and clustering (Lin and Pantel, 2001; Banko et al., 2007). The success of these methods suggests that semantic parsing can also be tackled as clustering of syntactic realizations of predicate-argument relations. While a similar direction has been previously explored in (Swier and Stevenson, 2004; Abend et al., 2009; Lang and Lapata, 2010), the recent work of (Poon and Domingos, 2009) takes it one step further by not only predicting predicate-argument structure of a sentence but also assigning sentence fragments to clusters of semantically similar expressions. For example, for a pair of sentences on Figure 1, in addition to inducing predicate-argument structure, they aim to assign expressions “Steelers” and “the Pittsburgh team” to the same semantic class Steelers, and group expressions “defeated” and “secured the victory over”. Such semantic representation can be useful for entailment or question answer"
P11-1145,W05-0620,0,0.320281,"Missing"
P11-1145,P09-1068,0,0.020042,"ion used is a lexicon providing a small number of examples for every logical symbol. A form of self-training is then used to bootstrap the model. Unsupervised semantic role labeling with a generative model has also been considered (Grenager and Manning, 2006), however, they do not attempt to discover frames and deal only with isolated pred1453 icates. Another generative model for SRL has been proposed in (Thompson et al., 2003), but the parameters were estimated from fully annotated data. The unsupervised setting has also been considering for the related problem of learning narrative schemas (Chambers and Jurafsky, 2009). However, their approach is quite different from our Bayesian model as it relies on similarity functions. Though in this work we focus solely on the unsupervised setting, there has been some successful work on semi-supervised semantic-role labeling, including the Framenet version of the problem (F¨urstenau and Lapata, 2009). Their method exploits graph alignments between labeled and unlabeled examples, and, therefore, crucially relies on the availability of labeled examples. 9 Conclusions and Future Work In this work, we introduced a non-parametric Bayesian model for the semantic parsing prob"
P11-1145,W10-2903,0,0.0233423,"method we use in our experiments. Though it is likely that tuning and different heuristics may result in better scores, we chose not to perform excessive tuning, as the evaluation dataset is fairly small. 8 Related Work There is a growing body of work on statistical learning for different versions of the semantic parsing problem (e.g., (Gildea and Jurafsky, 2002; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Mooney, 2007)), however, most of these methods rely on human annotation, or some weaker forms of supervision (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Clarke et al., 2010) and very little research has considered the unsupervised setting. In addition to the MLN model (Poon and Domingos, 2009), another unsupervised method has been proposed in (Goldwasser et al., 2011). In that work, the task is to predict a logical formula, and the only supervision used is a lexicon providing a small number of examples for every logical symbol. A form of self-training is then used to bootstrap the model. Unsupervised semantic role labeling with a generative model has also been considered (Grenager and Manning, 2006), however, they do not attempt to discover frames and deal only w"
P11-1145,N09-1062,0,0.0189067,"in Figure 2. We associate with each semantic class c, c = 1, 2, . . . , a distribution of its syntactic realizations φc . For example, for the frame WinPrize illustrated in Figure 1 this distribution would concentrate at syntactic fragments corresponding to lexical items “defeated”, “secured the victory” and “won”. The distribution is drawn from DP (w(C) , H (C) ), where H (C) is a base measure over syntactic subtrees. We use a simple generative process to define the probability of a subtree, the underlying model is similar to the base measures used in the Bayesian tree-substitution grammars (Cohn et al., 2009). We start by generating a word w uniformly from the treebank distribution, then we decide on the number of dependents of w using the geometric distribution Geom(q (C) ). For every dependent we generate a dependency relation r and a lexical form w0 from P (r|w)P (w0 |r), where probabilities P are based on add-0.1 smoothed treebank counts. The process is continued recursively. The smaller the parameter q (C) , the lower is the probability assigned to larger sub-trees. + Parameters ψc,t and ψc,t , t = 1, . . . , T , define a distribution over vectors (m1 , m2 , . . . , mT ) where mt is the numbe"
P11-1145,D09-1100,0,0.0159489,"nd clusterization moves such that the resulting model also obeys both of these constraints. Third, as in (Poon and Domingos, 2009), we do not model polysemy as we assume 1 Semantic classes correspond to lambda-form clusters in (Poon and Domingos, 2009) terminology. that each syntactic fragment corresponds to a single semantic class. This is not a model assumption and is only used at inference as it reduces mixing time of the Markov chain. It is not likely to be restrictive for the biomedical domain studied in our experiments. As in some of the recent work on learning semantic representations (Eisenstein et al., 2009; Poon and Domingos, 2009), we assume that dependency structures are provided for every sentence. This assumption allows us to construct models of semantics not Markovian within a sequence of words (see for an example a model described in (Liang et al., 2009)), but rather Markovian within a dependency tree. Though we include generation of the syntactic structure in our model, we would not expect that this syntactic component would result in an accurate syntactic model, even if trained in a supervised way, as the chosen independence assumptions are oversimplistic. In this way, we can use a simp"
P11-1145,D09-1002,0,0.251292,"Missing"
P11-1145,W05-0602,0,0.183109,"ns of predicates and those of their arguments, as well as the clusters of their syntactic realizations. We develop a modification of the MetropolisHastings split-merge sampler, resulting in an efficient inference algorithm for the model. The method is experimentally evaluated by using the induced semantic representation for the question answering task in the biomedical domain. 1 Introduction Statistical approaches to semantic parsing have recently received considerable attention. While some methods focus on predicting a complete formal representation of meaning (Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Mooney, 2007), others consider more shallow forms of representation (Carreras and M`arquez, 2005; Liang et al., 2009). However, most of this research has concentrated on supervised methods requiring large amounts of labeled data. Such annotated resources are scarce, expensive to create and even the largest of them tend to have Conversely, research in the closely related task of relation extraction has focused on unsupervised or minimally supervised methods (see, for example, (Lin and Pantel, 2001; Yates and Etzioni, 2009)). These approaches cluster semantically equivalent verbalizations of r"
P11-1145,P11-1149,0,0.0336904,"irly small. 8 Related Work There is a growing body of work on statistical learning for different versions of the semantic parsing problem (e.g., (Gildea and Jurafsky, 2002; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Mooney, 2007)), however, most of these methods rely on human annotation, or some weaker forms of supervision (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Clarke et al., 2010) and very little research has considered the unsupervised setting. In addition to the MLN model (Poon and Domingos, 2009), another unsupervised method has been proposed in (Goldwasser et al., 2011). In that work, the task is to predict a logical formula, and the only supervision used is a lexicon providing a small number of examples for every logical symbol. A form of self-training is then used to bootstrap the model. Unsupervised semantic role labeling with a generative model has also been considered (Grenager and Manning, 2006), however, they do not attempt to discover frames and deal only with isolated pred1453 icates. Another generative model for SRL has been proposed in (Thompson et al., 2003), but the parameters were estimated from fully annotated data. The unsupervised setting ha"
P11-1145,W06-1601,0,0.527645,"Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Clarke et al., 2010) and very little research has considered the unsupervised setting. In addition to the MLN model (Poon and Domingos, 2009), another unsupervised method has been proposed in (Goldwasser et al., 2011). In that work, the task is to predict a logical formula, and the only supervision used is a lexicon providing a small number of examples for every logical symbol. A form of self-training is then used to bootstrap the model. Unsupervised semantic role labeling with a generative model has also been considered (Grenager and Manning, 2006), however, they do not attempt to discover frames and deal only with isolated pred1453 icates. Another generative model for SRL has been proposed in (Thompson et al., 2003), but the parameters were estimated from fully annotated data. The unsupervised setting has also been considering for the related problem of learning narrative schemas (Chambers and Jurafsky, 2009). However, their approach is quite different from our Bayesian model as it relies on similarity functions. Though in this work we focus solely on the unsupervised setting, there has been some successful work on semi-supervised sema"
P11-1145,N07-1018,0,0.0165284,"models of semantics. Additional benefits generally expected from Bayesian modeling include the ability to encode prior linguistic knowledge in the form of hyperpriors and the potential for more reliable modeling of smaller datasets. More detailed discussion of relation between the Markov Logic Network (MLN) approach of (Poon and Domingos, 2009) and our non-parametric method is presented in Section 3. Hierarchical Pitman-Yor processes (or their special case, hierarchical Dirichlet processes) have previously been used in NLP, for example, in the context of syntactic parsing (Liang et al., 2007; Johnson et al., 2007). However, in all these cases the effective size of the state space (i.e., the number of sub-symbols in the infinite PCFG (Liang et al., 2007), or the number of adapted productions in the adaptor grammar (Johnson et al., 2007)) was not very large. In our case, the state space size equals 1446 the total number of distinct semantic clusters, and, thus, is expected to be exceedingly large even for moderate datasets: for example, the MLN model induces 18,543 distinct clusters from 18,471 sentences of the GENIA corpus (Poon and Domingos, 2009). This suggests that standard inference methods for hier"
P11-1145,N10-1137,0,0.344024,"n has focused on unsupervised or minimally supervised methods (see, for example, (Lin and Pantel, 2001; Yates and Etzioni, 2009)). These approaches cluster semantically equivalent verbalizations of relations, often relying on syntactic fragments as features for relation extraction and clustering (Lin and Pantel, 2001; Banko et al., 2007). The success of these methods suggests that semantic parsing can also be tackled as clustering of syntactic realizations of predicate-argument relations. While a similar direction has been previously explored in (Swier and Stevenson, 2004; Abend et al., 2009; Lang and Lapata, 2010), the recent work of (Poon and Domingos, 2009) takes it one step further by not only predicting predicate-argument structure of a sentence but also assigning sentence fragments to clusters of semantically similar expressions. For example, for a pair of sentences on Figure 1, in addition to inducing predicate-argument structure, they aim to assign expressions “Steelers” and “the Pittsburgh team” to the same semantic class Steelers, and group expressions “defeated” and “secured the victory over”. Such semantic representation can be useful for entailment or question answering tasks, as an entailm"
P11-1145,D07-1072,0,0.036925,"nalize more complex models of semantics. Additional benefits generally expected from Bayesian modeling include the ability to encode prior linguistic knowledge in the form of hyperpriors and the potential for more reliable modeling of smaller datasets. More detailed discussion of relation between the Markov Logic Network (MLN) approach of (Poon and Domingos, 2009) and our non-parametric method is presented in Section 3. Hierarchical Pitman-Yor processes (or their special case, hierarchical Dirichlet processes) have previously been used in NLP, for example, in the context of syntactic parsing (Liang et al., 2007; Johnson et al., 2007). However, in all these cases the effective size of the state space (i.e., the number of sub-symbols in the infinite PCFG (Liang et al., 2007), or the number of adapted productions in the adaptor grammar (Johnson et al., 2007)) was not very large. In our case, the state space size equals 1446 the total number of distinct semantic clusters, and, thus, is expected to be exceedingly large even for moderate datasets: for example, the MLN model induces 18,543 distinct clusters from 18,471 sentences of the GENIA corpus (Poon and Domingos, 2009). This suggests that standard inf"
P11-1145,P09-1011,0,0.55895,"dification of the MetropolisHastings split-merge sampler, resulting in an efficient inference algorithm for the model. The method is experimentally evaluated by using the induced semantic representation for the question answering task in the biomedical domain. 1 Introduction Statistical approaches to semantic parsing have recently received considerable attention. While some methods focus on predicting a complete formal representation of meaning (Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Mooney, 2007), others consider more shallow forms of representation (Carreras and M`arquez, 2005; Liang et al., 2009). However, most of this research has concentrated on supervised methods requiring large amounts of labeled data. Such annotated resources are scarce, expensive to create and even the largest of them tend to have Conversely, research in the closely related task of relation extraction has focused on unsupervised or minimally supervised methods (see, for example, (Lin and Pantel, 2001; Yates and Etzioni, 2009)). These approaches cluster semantically equivalent verbalizations of relations, often relying on syntactic fragments as features for relation extraction and clustering (Lin and Pantel, 2001"
P11-1145,C10-2107,0,0.0899362,"Missing"
P11-1145,D09-1001,0,0.267318,"upervised methods (see, for example, (Lin and Pantel, 2001; Yates and Etzioni, 2009)). These approaches cluster semantically equivalent verbalizations of relations, often relying on syntactic fragments as features for relation extraction and clustering (Lin and Pantel, 2001; Banko et al., 2007). The success of these methods suggests that semantic parsing can also be tackled as clustering of syntactic realizations of predicate-argument relations. While a similar direction has been previously explored in (Swier and Stevenson, 2004; Abend et al., 2009; Lang and Lapata, 2010), the recent work of (Poon and Domingos, 2009) takes it one step further by not only predicting predicate-argument structure of a sentence but also assigning sentence fragments to clusters of semantically similar expressions. For example, for a pair of sentences on Figure 1, in addition to inducing predicate-argument structure, they aim to assign expressions “Steelers” and “the Pittsburgh team” to the same semantic class Steelers, and group expressions “defeated” and “secured the victory over”. Such semantic representation can be useful for entailment or question answering tasks, as an entailment model can abstract away from specifics of"
P11-1145,P10-1044,0,0.01211,"dobj pp_over Opponent nmod Steelers Ravens�secured�the�victory�over�the�Pittsburgh�team Figure 1: An example of two different syntactic trees with a common semantic representation WinPrize(Ravens, Steelers). From the statistical modeling point of view, joint learning of predicate-argument structure and discovery of semantic clusters of expressions can also be beneficial, because it results in a more compact model of selectional preference, less prone to the data-sparsity problem (Zapirain et al., 2010). In this respect our model is similar to recent LDA-based models of selectional preference (Ritter et al., 2010; S´eaghdha, 2010), and can even be regarded as their recursive and non-parametric extension. In this paper, we adopt the above definition of unsupervised semantic parsing and propose a Bayesian non-parametric approach which uses hierarchical Pitman-Yor (PY) processes (Pitman, 2002) to model statistical dependencies between predicate and argument clusters, as well as distributions over syntactic and lexical realizations of each cluster. Our non-parametric model automatically discovers granularity of clustering appropriate for the dataset, unlike the parametric method of (Poon and Domingos, 200"
P11-1145,P10-1045,0,0.0821964,"Missing"
P11-1145,P06-1124,0,0.0150463,". (1) j +α where φ1 , . . . , φK are K values assigned to x1 , x2 , . . . , xj . The number of times φk was asPK signed is denoted jk , so that j = k=1 jk . The parameter β < 1 controls how heavy the tail of the distribution is: when it approaches 1, a new value is assigned to every draw, when β = 0 the PY process reduces to DP. The expected value of K scales as O(αnβ ) with the number of draws n, while it scales only logarithmically for DP processes. PY processes are expected to be more appropriate for many NLP problems, as they model power-law type distributions common for natural language (Teh, 2006). Hierarchical Dirichlet Processes (HDP) or hierarchical PY processes are used if the goal is to draw several related probability measures for the same set S. For example, they can be used to generate transition distributions of a Markov model, HDPHMM (Teh et al., 2006; Beal et al., 2002). For such a HMM, the top-level state proportions are drawn from the top-level stick breaking construction γ ∼ GEM (α, β), and then the individual transition distributions for every state z = 1, 2, . . . φz are drawn from P Y (γ, α0 , β 0 ). The parameters α0 and β 0 control how similar the individual transiti"
P11-1145,P10-1098,1,0.824245,"to be too coarse for the QA method we use in our experiments. Though it is likely that tuning and different heuristics may result in better scores, we chose not to perform excessive tuning, as the evaluation dataset is fairly small. 8 Related Work There is a growing body of work on statistical learning for different versions of the semantic parsing problem (e.g., (Gildea and Jurafsky, 2002; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Mooney, 2007)), however, most of these methods rely on human annotation, or some weaker forms of supervision (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Clarke et al., 2010) and very little research has considered the unsupervised setting. In addition to the MLN model (Poon and Domingos, 2009), another unsupervised method has been proposed in (Goldwasser et al., 2011). In that work, the task is to predict a logical formula, and the only supervision used is a lexicon providing a small number of examples for every logical symbol. A form of self-training is then used to bootstrap the model. Unsupervised semantic role labeling with a generative model has also been considered (Grenager and Manning, 2006), however, they do not attempt to discover"
P11-1145,N10-1058,0,0.0679849,"Missing"
P11-1145,W04-3213,0,\N,Missing
P11-1145,J02-3001,0,\N,Missing
P12-1068,P09-1004,0,0.0226264,"ole induction problem is formulated as a graph partitioning problem: each vertex in the graph corresponds to a predicate occurrence and edges represent lexical and syntactic similarities between the occurrences. Unsupervised induction of semantics has also been studied in Poon and Domingos (2009) and Titov and Klementiev (2011) but the induced representations are not entirely compatible with the PropBank-style annotations and they have been evaluated only on a question answering task for the biomedical domain. Also, a related task of unsupervised argument identification has been considered in Abend et al. (2009). 8 Conclusions This work adds unsupervised semantic role labeling to the list of NLP tasks benefiting from the crosslingual induction setting. We show that an agreement signal extracted from parallel data provides indirect supervision capable of substantially improving a state-of-the-art model for semantic role induction. Although in this work we focused primarily on improving performance for each individual language, cross-lingual semantic representation could be extracted by a simple post-processing step. In future work, we would like to model cross-lingual semantics explicitly. Acknowledge"
P12-1068,burchardt-etal-2006-salsa,0,0.0642777,"Missing"
P12-1068,P07-1036,0,0.0194532,"Missing"
P12-1068,de-marneffe-etal-2006-generating,0,0.0140545,"Missing"
P12-1068,D09-1003,0,0.0260963,"t of monolingual weakly-parallel data was studied in Titov and Kozhevnikov (2010) but their setting was semisupervised and they experimented only on a restricted domain. Most of the SRL research has focused on the supervised setting, however, lack of annotated resources for most languages and insufficient coverage provided by the existing resources motivates the need for using unlabeled data or other forms of weak supervision. This includes methods based on graph alignment between labeled and unlabeled data (F¨urstenau and Lapata, 2009), using unlabeled data to improve lexical generalization (Deschacht and Moens, 2009), and projection of annotation across languages (Pado and Lapata, 2009; van der Plas et al., 2011). Semi-supervised and weaklysupervised techniques have also been explored for other types of semantic representations but these studies again have mostly focused on restricted domains (Kate and Mooney, 2007; Liang et al., 2009; Goldwasser et al., 2011; Liang et al., 2011). 654 Early unsupervised approaches to the SRL task include (Swier and Stevenson, 2004), where the VerbNet verb lexicon was used to guide unsupervised learning, and a generative model of Grenager and Manning (2006) which exploits"
P12-1068,D09-1002,0,0.0327818,"Missing"
P12-1068,P11-2051,0,0.0443446,"afsky, 2002) involves predicting predicate argument structure, i.e. both the identification of arguments the arguments ‘Peter’, ‘Mary’, and ‘planning a theft’ of the predicate ‘blame’ take the agent (A0), patient (A1) and reason (A2) roles, respectively. In this work, we focus on predicting argument roles. SRL representations have many potential applications in NLP and have recently been shown to benefit question answering (Shen and Lapata, 2007; Kaisser and Webber, 2007), textual entailment (Sammons et al., 2009), machine translation (Wu and Fung, 2009; Liu and Gildea, 2010; Wu et al., 2011; Gao and Vogel, 2011), and dialogue systems (Basili et al., 2009; van der Plas et al., 2011), among others. Though syntactic representations are often predictive of semantic roles (Levin, 1993), the interface between syntactic and semantic representations is far from trivial. Lack of simple deterministic rules for mapping syntax to shallow semantics motivates the use of statistical methods. Most of the current statistical approaches to SRL are supervised, requiring large quantities of human annotated data to estimate model parameters. However, such resources are expensive to create and only available for a small n"
P12-1068,P11-1149,0,0.0134443,"otivates the need for using unlabeled data or other forms of weak supervision. This includes methods based on graph alignment between labeled and unlabeled data (F¨urstenau and Lapata, 2009), using unlabeled data to improve lexical generalization (Deschacht and Moens, 2009), and projection of annotation across languages (Pado and Lapata, 2009; van der Plas et al., 2011). Semi-supervised and weaklysupervised techniques have also been explored for other types of semantic representations but these studies again have mostly focused on restricted domains (Kate and Mooney, 2007; Liang et al., 2009; Goldwasser et al., 2011; Liang et al., 2011). 654 Early unsupervised approaches to the SRL task include (Swier and Stevenson, 2004), where the VerbNet verb lexicon was used to guide unsupervised learning, and a generative model of Grenager and Manning (2006) which exploits linguistic priors on syntactic-semantic interface. More recently, the role induction problem has been studied in Lang and Lapata (2010) where it has been reformulated as a problem of detecting alternations and mapping non-standard linkings to the canonical ones. Later, Lang and Lapata (2011a) proposed an algorithmic approach to clustering argument"
P12-1068,W06-1601,0,0.393615,"efinition of the crosslingual semantic role induction task we address in this paper. In Section 3, we describe the base monolingual model, and in Section 4 we propose an extension for the crosslingual setting. In Section 5, we describe our inference procedure. Section 6 provides both evaluation and analysis. Finally, additional related work is presented in Section 7. 2 Problem Definition As we mentioned in the introduction, in this work we focus on the labeling stage of semantic role labeling. Identification, though an important problem, can be tackled with heuristics (Lang and Lapata, 2011a; Grenager and Manning, 2006; de Marneffe et al., 2006) or potentially by using a supervised classifier trained on a small amount of data. Instead of assuming the availability of role annotated data, we rely only on automatically generated syntactic dependency graphs in both languages. While we cannot expect that syntactic structure can trivially map to a semantic representation1 , we can make use of syntactic cues. In the labeling stage, semantic roles are represented by clusters of arguments, and labeling a particular argument corresponds to deciding on its role cluster. However, instead of dealing with argument occurr"
P12-1068,D08-1008,0,0.0253397,"Missing"
P12-1068,W07-1206,0,0.0213555,"k is to show that parallel data is useful in unsupervised induction of shallow semantic representations. Semantic role labeling (SRL) (Gildea and Jurafsky, 2002) involves predicting predicate argument structure, i.e. both the identification of arguments the arguments ‘Peter’, ‘Mary’, and ‘planning a theft’ of the predicate ‘blame’ take the agent (A0), patient (A1) and reason (A2) roles, respectively. In this work, we focus on predicting argument roles. SRL representations have many potential applications in NLP and have recently been shown to benefit question answering (Shen and Lapata, 2007; Kaisser and Webber, 2007), textual entailment (Sammons et al., 2009), machine translation (Wu and Fung, 2009; Liu and Gildea, 2010; Wu et al., 2011; Gao and Vogel, 2011), and dialogue systems (Basili et al., 2009; van der Plas et al., 2011), among others. Though syntactic representations are often predictive of semantic roles (Levin, 1993), the interface between syntactic and semantic representations is far from trivial. Lack of simple deterministic rules for mapping syntax to shallow semantics motivates the use of statistical methods. Most of the current statistical approaches to SRL are supervised, requiring large q"
P12-1068,2005.mtsummit-papers.11,0,0.403731,"nd semantics, encoded as a clustering of syntactic signatures of predicate arguments. The clustering implicitly defines the set of permissible alternations. For predicates present in both sides of a bitext, we guide models in both languages to prefer clusterings which maximize agreement between predicate argument structures predicted for each aligned predicate pair. We experimentally show the effectiveness of the crosslingual learning on the English-German language pair. Our model admits efficient inference: the estimation time on CoNLL 2009 data (Hajiˇc et al., 2009) and Europarl v.6 bitext (Koehn, 2005) does not exceed 5 hours on a single processor and the inference algorithm is highly parallelizable, reducing in648 ference time down to less than half an hour on multiple processors. This suggests that the models scale to much larger corpora, which is an important property for a successful unsupervised learning method, as unlabeled data is abundant. In summary, our contributions are as follows. • This work is the first to consider the crosslingual setting for unsupervised SRL. • We propose a form of agreement penalty and show its efficacy on English-German language pair when used in conjuncti"
P12-1068,P04-1060,0,0.149232,"which results in efficient inference. When applied to German-English parallel data, our method obtains a substantial improvement over a model trained without using the agreement signal, when both are tested on non-parallel sentences. 1 (a) [A0 Peter] blamed [A1 Mary] [A2 for planning a theft]. (b) [A0 Peter] blamed [A2 planning a theft] [A1 on Mary]. (c) [A1 Mary] was blamed [A2 for planning a theft] [A0 by Peter] Introduction Learning in the context of multiple languages simultaneously has been shown to be beneficial to a number of NLP tasks from morphological analysis to syntactic parsing (Kuhn, 2004; Snyder and Barzilay, 2010; McDonald et al., 2011). The goal of this work is to show that parallel data is useful in unsupervised induction of shallow semantic representations. Semantic role labeling (SRL) (Gildea and Jurafsky, 2002) involves predicting predicate argument structure, i.e. both the identification of arguments the arguments ‘Peter’, ‘Mary’, and ‘planning a theft’ of the predicate ‘blame’ take the agent (A0), patient (A1) and reason (A2) roles, respectively. In this work, we focus on predicting argument roles. SRL representations have many potential applications in NLP and have r"
P12-1068,N10-1137,0,0.459253,"stead of assuming the availability of role annotated data, we rely only on automatically generated syntactic dependency graphs in both languages. While we cannot expect that syntactic structure can trivially map to a semantic representation1 , we can make use of syntactic cues. In the labeling stage, semantic roles are represented by clusters of arguments, and labeling a particular argument corresponds to deciding on its role cluster. However, instead of dealing with argument occurrences directly, 1 Although it provides a strong baseline which is difficult to beat (Grenager and Manning, 2006; Lang and Lapata, 2010; Lang and Lapata, 2011a). we represent them as predicate-specific syntactic signatures, and refer to them as argument keys. This representation aids our models in inducing high purity clusters (of argument keys) while reducing their granularity. We follow (Lang and Lapata, 2011a) and use the following syntactic features for English to form the argument key representation: • • • • Active or passive verb voice (ACT/PASS). Arg. position relative to predicate (LEFT/RIGHT). Syntactic relation to its governor. Preposition used for argument realization. In the example sentences in Section 1, the arg"
P12-1068,P11-1112,0,0.400338,"ection 2 begins with a definition of the crosslingual semantic role induction task we address in this paper. In Section 3, we describe the base monolingual model, and in Section 4 we propose an extension for the crosslingual setting. In Section 5, we describe our inference procedure. Section 6 provides both evaluation and analysis. Finally, additional related work is presented in Section 7. 2 Problem Definition As we mentioned in the introduction, in this work we focus on the labeling stage of semantic role labeling. Identification, though an important problem, can be tackled with heuristics (Lang and Lapata, 2011a; Grenager and Manning, 2006; de Marneffe et al., 2006) or potentially by using a supervised classifier trained on a small amount of data. Instead of assuming the availability of role annotated data, we rely only on automatically generated syntactic dependency graphs in both languages. While we cannot expect that syntactic structure can trivially map to a semantic representation1 , we can make use of syntactic cues. In the labeling stage, semantic roles are represented by clusters of arguments, and labeling a particular argument corresponds to deciding on its role cluster. However, instead of"
P12-1068,D11-1122,0,0.224447,"ection 2 begins with a definition of the crosslingual semantic role induction task we address in this paper. In Section 3, we describe the base monolingual model, and in Section 4 we propose an extension for the crosslingual setting. In Section 5, we describe our inference procedure. Section 6 provides both evaluation and analysis. Finally, additional related work is presented in Section 7. 2 Problem Definition As we mentioned in the introduction, in this work we focus on the labeling stage of semantic role labeling. Identification, though an important problem, can be tackled with heuristics (Lang and Lapata, 2011a; Grenager and Manning, 2006; de Marneffe et al., 2006) or potentially by using a supervised classifier trained on a small amount of data. Instead of assuming the availability of role annotated data, we rely only on automatically generated syntactic dependency graphs in both languages. While we cannot expect that syntactic structure can trivially map to a semantic representation1 , we can make use of syntactic cues. In the labeling stage, semantic roles are represented by clusters of arguments, and labeling a particular argument corresponds to deciding on its role cluster. However, instead of"
P12-1068,P09-1011,0,0.0120106,"existing resources motivates the need for using unlabeled data or other forms of weak supervision. This includes methods based on graph alignment between labeled and unlabeled data (F¨urstenau and Lapata, 2009), using unlabeled data to improve lexical generalization (Deschacht and Moens, 2009), and projection of annotation across languages (Pado and Lapata, 2009; van der Plas et al., 2011). Semi-supervised and weaklysupervised techniques have also been explored for other types of semantic representations but these studies again have mostly focused on restricted domains (Kate and Mooney, 2007; Liang et al., 2009; Goldwasser et al., 2011; Liang et al., 2011). 654 Early unsupervised approaches to the SRL task include (Swier and Stevenson, 2004), where the VerbNet verb lexicon was used to guide unsupervised learning, and a generative model of Grenager and Manning (2006) which exploits linguistic priors on syntactic-semantic interface. More recently, the role induction problem has been studied in Lang and Lapata (2010) where it has been reformulated as a problem of detecting alternations and mapping non-standard linkings to the canonical ones. Later, Lang and Lapata (2011a) proposed an algorithmic approa"
P12-1068,P11-1060,0,0.0169971,"ng unlabeled data or other forms of weak supervision. This includes methods based on graph alignment between labeled and unlabeled data (F¨urstenau and Lapata, 2009), using unlabeled data to improve lexical generalization (Deschacht and Moens, 2009), and projection of annotation across languages (Pado and Lapata, 2009; van der Plas et al., 2011). Semi-supervised and weaklysupervised techniques have also been explored for other types of semantic representations but these studies again have mostly focused on restricted domains (Kate and Mooney, 2007; Liang et al., 2009; Goldwasser et al., 2011; Liang et al., 2011). 654 Early unsupervised approaches to the SRL task include (Swier and Stevenson, 2004), where the VerbNet verb lexicon was used to guide unsupervised learning, and a generative model of Grenager and Manning (2006) which exploits linguistic priors on syntactic-semantic interface. More recently, the role induction problem has been studied in Lang and Lapata (2010) where it has been reformulated as a problem of detecting alternations and mapping non-standard linkings to the canonical ones. Later, Lang and Lapata (2011a) proposed an algorithmic approach to clustering argument signatures which ach"
P12-1068,C10-1081,0,0.0241886,"tic role labeling (SRL) (Gildea and Jurafsky, 2002) involves predicting predicate argument structure, i.e. both the identification of arguments the arguments ‘Peter’, ‘Mary’, and ‘planning a theft’ of the predicate ‘blame’ take the agent (A0), patient (A1) and reason (A2) roles, respectively. In this work, we focus on predicting argument roles. SRL representations have many potential applications in NLP and have recently been shown to benefit question answering (Shen and Lapata, 2007; Kaisser and Webber, 2007), textual entailment (Sammons et al., 2009), machine translation (Wu and Fung, 2009; Liu and Gildea, 2010; Wu et al., 2011; Gao and Vogel, 2011), and dialogue systems (Basili et al., 2009; van der Plas et al., 2011), among others. Though syntactic representations are often predictive of semantic roles (Levin, 1993), the interface between syntactic and semantic representations is far from trivial. Lack of simple deterministic rules for mapping syntax to shallow semantics motivates the use of statistical methods. Most of the current statistical approaches to SRL are supervised, requiring large quantities of human annotated data to estimate model parameters. However, such resources are expensive to"
P12-1068,J93-2004,0,0.0396894,"Missing"
P12-1068,D11-1006,0,0.0140176,"When applied to German-English parallel data, our method obtains a substantial improvement over a model trained without using the agreement signal, when both are tested on non-parallel sentences. 1 (a) [A0 Peter] blamed [A1 Mary] [A2 for planning a theft]. (b) [A0 Peter] blamed [A2 planning a theft] [A1 on Mary]. (c) [A1 Mary] was blamed [A2 for planning a theft] [A0 by Peter] Introduction Learning in the context of multiple languages simultaneously has been shown to be beneficial to a number of NLP tasks from morphological analysis to syntactic parsing (Kuhn, 2004; Snyder and Barzilay, 2010; McDonald et al., 2011). The goal of this work is to show that parallel data is useful in unsupervised induction of shallow semantic representations. Semantic role labeling (SRL) (Gildea and Jurafsky, 2002) involves predicting predicate argument structure, i.e. both the identification of arguments the arguments ‘Peter’, ‘Mary’, and ‘planning a theft’ of the predicate ‘blame’ take the agent (A0), patient (A1) and reason (A2) roles, respectively. In this work, we focus on predicting argument roles. SRL representations have many potential applications in NLP and have recently been shown to benefit question answering (S"
P12-1068,J03-1002,0,0.00664751,"Missing"
P12-1068,D09-1001,0,0.0330562,"ere it has been reformulated as a problem of detecting alternations and mapping non-standard linkings to the canonical ones. Later, Lang and Lapata (2011a) proposed an algorithmic approach to clustering argument signatures which achieves higher accuracy and outperforms the syntactic baseline. In Lang and Lapata (2011b), the role induction problem is formulated as a graph partitioning problem: each vertex in the graph corresponds to a predicate occurrence and edges represent lexical and syntactic similarities between the occurrences. Unsupervised induction of semantics has also been studied in Poon and Domingos (2009) and Titov and Klementiev (2011) but the induced representations are not entirely compatible with the PropBank-style annotations and they have been evaluated only on a question answering task for the biomedical domain. Also, a related task of unsupervised argument identification has been considered in Abend et al. (2009). 8 Conclusions This work adds unsupervised semantic role labeling to the list of NLP tasks benefiting from the crosslingual induction setting. We show that an agreement signal extracted from parallel data provides indirect supervision capable of substantially improving a state"
P12-1068,J08-2006,0,0.235936,"predictive of semantic roles (Levin, 1993), the interface between syntactic and semantic representations is far from trivial. Lack of simple deterministic rules for mapping syntax to shallow semantics motivates the use of statistical methods. Most of the current statistical approaches to SRL are supervised, requiring large quantities of human annotated data to estimate model parameters. However, such resources are expensive to create and only available for a small number of languages and domains. Moreover, when moved to a new domain, performance of these models tends to degrade substantially (Pradhan et al., 2008). Sparsity of annotated data motivates the need to look to alternative 647 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 647–656, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics resources. In this work, we make use of unsupervised data along with parallel texts and learn to induce semantic structures in two languages simultaneously. As does most of the recent work on unsupervised SRL, we assume that our data is annotated with automatically-predicted syntactic dependency parses and aim to induce a model o"
P12-1068,D07-1002,0,0.0859375,"). The goal of this work is to show that parallel data is useful in unsupervised induction of shallow semantic representations. Semantic role labeling (SRL) (Gildea and Jurafsky, 2002) involves predicting predicate argument structure, i.e. both the identification of arguments the arguments ‘Peter’, ‘Mary’, and ‘planning a theft’ of the predicate ‘blame’ take the agent (A0), patient (A1) and reason (A2) roles, respectively. In this work, we focus on predicting argument roles. SRL representations have many potential applications in NLP and have recently been shown to benefit question answering (Shen and Lapata, 2007; Kaisser and Webber, 2007), textual entailment (Sammons et al., 2009), machine translation (Wu and Fung, 2009; Liu and Gildea, 2010; Wu et al., 2011; Gao and Vogel, 2011), and dialogue systems (Basili et al., 2009; van der Plas et al., 2011), among others. Though syntactic representations are often predictive of semantic roles (Levin, 1993), the interface between syntactic and semantic representations is far from trivial. Lack of simple deterministic rules for mapping syntax to shallow semantics motivates the use of statistical methods. Most of the current statistical approaches to SRL are su"
P12-1068,P08-1084,0,0.0354071,"see Table 1. The relatively low expressivity and limited purity of our argument keys (see discussion in Section 4) are likely to limit potential improvements when using them in crosslingual learning. The natural next step would be to consider crosslingual learning with a more expressive model of the syntactic frame and syntax-semantics linking. 7 Related Work Unsupervised learning in crosslingual setting has been an active area of research in recent years. However, most of this research has focused on induction of syntactic structures (Kuhn, 2004; Snyder et al., 2009) or morphologic analysis (Snyder and Barzilay, 2008) and we are not aware of any previous work on induction of semantic representations in the crosslingual setting. Learning of semantic representations in the context of monolingual weakly-parallel data was studied in Titov and Kozhevnikov (2010) but their setting was semisupervised and they experimented only on a restricted domain. Most of the SRL research has focused on the supervised setting, however, lack of annotated resources for most languages and insufficient coverage provided by the existing resources motivates the need for using unlabeled data or other forms of weak supervision. This i"
P12-1068,D08-1109,0,0.0108127,"n unsupervised induction of linguistic structures, we rely on automatically produced word alignments. In Section 6, we describe how we use word alignment to decide if two arguments are aligned; for now, we assume that (noisy) argument alignments are given. Intuitively, when two arguments are aligned in parallel data, we expect them to be labeled with the same semantic role in both languages. This correspondence is simpler than the one expected in multilingual induction of syntax and morphology where systematic but unknown relation between structures in two language is normally assumed (e.g., (Snyder et al., 2008)). A straightforward implementation of this idea would require us to maintain one-to-one mapping between semantic roles across languages. Instead of assuming this correspondence, we penalize for the lack of isomorphism between the sets of roles in aligned predicates with the penalty dependent on the degree of violation. This softer approach is more appropriate in our setting, as individual argument keys do not always deterministically map to gold standard roles4 and strict penalization would result in the propagation of the corresponding overcoarse clusters to the other language. Empirically,"
P12-1068,P09-1009,0,0.0127398,"ctic parses. ods do not always improve on it, see Table 1. The relatively low expressivity and limited purity of our argument keys (see discussion in Section 4) are likely to limit potential improvements when using them in crosslingual learning. The natural next step would be to consider crosslingual learning with a more expressive model of the syntactic frame and syntax-semantics linking. 7 Related Work Unsupervised learning in crosslingual setting has been an active area of research in recent years. However, most of this research has focused on induction of syntactic structures (Kuhn, 2004; Snyder et al., 2009) or morphologic analysis (Snyder and Barzilay, 2008) and we are not aware of any previous work on induction of semantic representations in the crosslingual setting. Learning of semantic representations in the context of monolingual weakly-parallel data was studied in Titov and Kozhevnikov (2010) but their setting was semisupervised and they experimented only on a restricted domain. Most of the SRL research has focused on the supervised setting, however, lack of annotated resources for most languages and insufficient coverage provided by the existing resources motivates the need for using unlab"
P12-1068,W08-2121,0,0.0580191,"Missing"
P12-1068,P11-1145,1,0.944736,"cluster to reassign it to. This is done by considering all clusters (including creating a new one) and choosing the most probable one. Instead of choosing argument keys randomly at the first stage, we order them by corpus frequency. This ordering is beneficial as getting clustering right for frequent argument keys is more important and the corresponding decisions should be made earlier.5 We used a single iteration in our experiments, as we have not noticed any benefit from using multiple iterations. 5 This has been explored before for shallow semantic representations (Lang and Lapata, 2011a; Titov and Klementiev, 2011). 5.2 Incorporating the Alignment Penalty Inference in the monolingual setting is done independently for each predicate, as the model factorizes over the predicates. The role alignment penalty introduces interdependencies between the objectives for each bilingual predicate pair chosen by the assignment algorithm as discussed in Section 4. For each pair of predicates, we search for clusterings to maximize the sum of the log-probability and the negated penalty term. At first glance it may seem that the alignment penalty can be easily integrated into the greedy MAP search algorithm: instead of co"
P12-1068,E12-1003,1,0.709203,"y based on monolingual data, though possible, may be tricky as selectional preferences of the roles are not particularly restrictive; similar restrictions for patient and agent roles may further complicate the process. However, both sentences (a) and (b) are likely to be translated in German as ‘[A0 Peter] beschuldigte [A1 Mary] [A2 einen Diebstahl zu planen]’. Maximizing agreement between the roles predicted for both languages would provide a strong signal for inducing the proper linkings in our examples. In this work, we begin with a state-of-the-art monolingual unsupervised Bayesian model (Titov and Klementiev, 2012) and focus on improving its performance in the crosslingual setting. It induces a linking between syntax and semantics, encoded as a clustering of syntactic signatures of predicate arguments. The clustering implicitly defines the set of permissible alternations. For predicates present in both sides of a bitext, we guide models in both languages to prefer clusterings which maximize agreement between predicate argument structures predicted for each aligned predicate pair. We experimentally show the effectiveness of the crosslingual learning on the English-German language pair. Our model admits e"
P12-1068,P10-1098,1,0.844806,"crosslingual learning with a more expressive model of the syntactic frame and syntax-semantics linking. 7 Related Work Unsupervised learning in crosslingual setting has been an active area of research in recent years. However, most of this research has focused on induction of syntactic structures (Kuhn, 2004; Snyder et al., 2009) or morphologic analysis (Snyder and Barzilay, 2008) and we are not aware of any previous work on induction of semantic representations in the crosslingual setting. Learning of semantic representations in the context of monolingual weakly-parallel data was studied in Titov and Kozhevnikov (2010) but their setting was semisupervised and they experimented only on a restricted domain. Most of the SRL research has focused on the supervised setting, however, lack of annotated resources for most languages and insufficient coverage provided by the existing resources motivates the need for using unlabeled data or other forms of weak supervision. This includes methods based on graph alignment between labeled and unlabeled data (F¨urstenau and Lapata, 2009), using unlabeled data to improve lexical generalization (Deschacht and Moens, 2009), and projection of annotation across languages (Pado a"
P12-1068,P11-2052,0,0.0705933,"Missing"
P12-1068,N09-2004,0,0.0460525,"resentations. Semantic role labeling (SRL) (Gildea and Jurafsky, 2002) involves predicting predicate argument structure, i.e. both the identification of arguments the arguments ‘Peter’, ‘Mary’, and ‘planning a theft’ of the predicate ‘blame’ take the agent (A0), patient (A1) and reason (A2) roles, respectively. In this work, we focus on predicting argument roles. SRL representations have many potential applications in NLP and have recently been shown to benefit question answering (Shen and Lapata, 2007; Kaisser and Webber, 2007), textual entailment (Sammons et al., 2009), machine translation (Wu and Fung, 2009; Liu and Gildea, 2010; Wu et al., 2011; Gao and Vogel, 2011), and dialogue systems (Basili et al., 2009; van der Plas et al., 2011), among others. Though syntactic representations are often predictive of semantic roles (Levin, 1993), the interface between syntactic and semantic representations is far from trivial. Lack of simple deterministic rules for mapping syntax to shallow semantics motivates the use of statistical methods. Most of the current statistical approaches to SRL are supervised, requiring large quantities of human annotated data to estimate model parameters. However, such resou"
P12-1068,W04-3213,0,\N,Missing
P12-1068,N07-1070,0,\N,Missing
P12-1068,W09-1201,0,\N,Missing
P12-1068,J02-3001,0,\N,Missing
P12-1068,D07-1096,0,\N,Missing
P13-1117,P09-1004,0,0.0806552,"Missing"
P13-1117,W09-1206,0,0.0989703,"Missing"
P13-1117,P07-1036,0,0.039053,"l., 2008). The feature model was modified to accommodate the cross-lingual cluster features and the reranker component was not used. We do not model the interaction between different argument roles in the same predicate. While this has been found useful, in the cross-lingual setup one has to be careful with the assumptions made. For example, modeling the sequence of roles using a Markov chain (Thompson et al., 2003) may not work well in the present setting, especially between distant languages, as the order or arguments is not necessarily preserved. Most constraints that prove useful for SRL (Chang et al., 2007) also require customization when applied to a new language, and some rely on languagespecific resources, such as a valency lexicon. Taking into account the interaction between different arguments of a predicate is likely to improve the performance of the transferred model, but this is outside the scope of this work. 3.4 Feature Selection Compatibility of feature representations is necessary but not sufficient for successful model transfer. We have to make sure that the features we use are predictive of similar outcomes in the two languages as well. Depending on the pair of languages in questio"
P13-1117,I11-1021,0,0.0179144,"in terms of the metric considered. This is likely due to the fact that we consider only the core roles, which can usually be predicted with high accuracy based on syntactic information alone. 6 Related Work Development of robust statistical models for core NLP tasks is a challenging problem, and adaptation of existing models to new languages presents a viable alternative to exhaustive annotation for each language. Although the models thus obtained are generally imperfect, they can be further refined for a particular language and domain using techniques such as active learning (Settles, 2010; Chen et al., 2011). Cross-lingual annotation projection (Yarowsky et al., 2001) approaches have been applied extensively to a variety of tasks, including POS tagging (Xi and Hwa, 2005; Das and Petrov, 2011), morphology segmentation (Snyder and Barzilay, 2008), verb classification (Merlo et al., 2002), mention detection (Zitouni and Florian, 2008), LFG parsing (Wr´oblewska and Frank, 2009), information extraction (Kim et al., 2010), SRL (Pad´o and Lapata, 2009; van der Plas et al., 2011; Annesi and Basili, 2010; Tonelli and Pianta, 2008), dependency parsing (Naseem et al., 2012; Ganchev et al., 2009; Smith and E"
P13-1117,P11-1061,0,0.0624042,"alone. 6 Related Work Development of robust statistical models for core NLP tasks is a challenging problem, and adaptation of existing models to new languages presents a viable alternative to exhaustive annotation for each language. Although the models thus obtained are generally imperfect, they can be further refined for a particular language and domain using techniques such as active learning (Settles, 2010; Chen et al., 2011). Cross-lingual annotation projection (Yarowsky et al., 2001) approaches have been applied extensively to a variety of tasks, including POS tagging (Xi and Hwa, 2005; Das and Petrov, 2011), morphology segmentation (Snyder and Barzilay, 2008), verb classification (Merlo et al., 2002), mention detection (Zitouni and Florian, 2008), LFG parsing (Wr´oblewska and Frank, 2009), information extraction (Kim et al., 2010), SRL (Pad´o and Lapata, 2009; van der Plas et al., 2011; Annesi and Basili, 2010; Tonelli and Pianta, 2008), dependency parsing (Naseem et al., 2012; Ganchev et al., 2009; Smith and Eisner, 2009; Hwa et al., 2005) or temporal relation prediction (Spreyer and Frank, 2008). Interestingly, it has also been used to propagate morphosyntactic information between old and mode"
P13-1117,D12-1001,0,0.0315236,"an, 2008), LFG parsing (Wr´oblewska and Frank, 2009), information extraction (Kim et al., 2010), SRL (Pad´o and Lapata, 2009; van der Plas et al., 2011; Annesi and Basili, 2010; Tonelli and Pianta, 2008), dependency parsing (Naseem et al., 2012; Ganchev et al., 2009; Smith and Eisner, 2009; Hwa et al., 2005) or temporal relation prediction (Spreyer and Frank, 2008). Interestingly, it has also been used to propagate morphosyntactic information between old and modern versions of the same language (Meyer, 2011). Cross-lingual model transfer methods (McDonald et al., 2011; Zeman and Resnik, 2008; Durrett et al., 2012; Søgaard, 2011; Lopez et al., 2008) have also been receiving much attention recently. The basic idea behind model transfer is similar to that of cross-lingual annotation projection, as we can see from the way parallel data is used in, for example, McDonald et al. (2011). A crucial component of direct transfer approaches is the unified feature representation. There are at least two such representations of lexical information (Klementiev et al., 2012; T¨ackstr¨om et al., 2012), but both work on word level. This makes it hard to account for phenomena that are expressed differently in the languag"
P13-1117,eisele-chen-2010-multiun,0,0.0154946,"t-of-speech tags in all datasets were replaced with the universal POS tags of Petrov et al. (2012). For Czech, we have augmented the mappings to account for the tags that were not present in the datasets from which the original mappings were derived. Namely, tag “t” is mapped to “VERB” and “Y” – to “PRON”. We use parallel data to construct a bilingual dictionary used in word mapping, as well as in the projection baseline. For English-Czech 1 see http://www.ml4nlp.de/code-and-data/treex2conll and English-French, the data is drawn from Europarl (Koehn, 2005), for English-Chinese – from MultiUN (Eisele and Chen, 2010). The word alignments were obtained using GIZA++ (Och and Ney, 2003) and the intersection heuristic. 4.2 Syntactic Transfer In the low-resource setting, we cannot always rely on the availability of an accurate dependency parser for the target language. If one is not available, the natural solution would be to use crosslingual model transfer to obtain it. Unfortunately, the models presented in the previous work, such as Zeman and Resnik (2008), McDonald et al. (2011) and T¨ackstr¨om et al. (2012), were not made available, so we reproduced the direct transfer algorithm of McDonald et al. (2011),"
P13-1117,P09-1042,0,0.0612864,"Settles, 2010; Chen et al., 2011). Cross-lingual annotation projection (Yarowsky et al., 2001) approaches have been applied extensively to a variety of tasks, including POS tagging (Xi and Hwa, 2005; Das and Petrov, 2011), morphology segmentation (Snyder and Barzilay, 2008), verb classification (Merlo et al., 2002), mention detection (Zitouni and Florian, 2008), LFG parsing (Wr´oblewska and Frank, 2009), information extraction (Kim et al., 2010), SRL (Pad´o and Lapata, 2009; van der Plas et al., 2011; Annesi and Basili, 2010; Tonelli and Pianta, 2008), dependency parsing (Naseem et al., 2012; Ganchev et al., 2009; Smith and Eisner, 2009; Hwa et al., 2005) or temporal relation prediction (Spreyer and Frank, 2008). Interestingly, it has also been used to propagate morphosyntactic information between old and modern versions of the same language (Meyer, 2011). Cross-lingual model transfer methods (McDonald et al., 2011; Zeman and Resnik, 2008; Durrett et al., 2012; Søgaard, 2011; Lopez et al., 2008) have also been receiving much attention recently. The basic idea behind model transfer is similar to that of cross-lingual annotation projection, as we can see from the way parallel data is used in, for exampl"
P13-1117,P11-2051,0,0.0410822,"g competitive performance as compared to a state-of-the-art unsupervised SRL system and a cross-lingual annotation projection baseline. We also consider the contribution of different aspects of the feature representation to the performance of the model and discuss practical applicability of this method. 1 Background and Motivation Semantic role labeling has proven useful in many natural language processing tasks, such as question answering (Shen and Lapata, 2007; Kaisser and Webber, 2007), textual entailment (Sammons et al., 2009), machine translation (Wu and Fung, 2009; Liu and Gildea, 2010; Gao and Vogel, 2011) and dialogue systems (Basili et al., 2009; van der Plas et al., 2009). Multiple models have been designed to automatically predict semantic roles, and a considerable amount of data has been annotated to train these models, if only for a few more popular languages. As the annotation is costly, one would like to leverage existing resources to minimize the human effort required to construct a model for a new language. A number of approaches to the construction of semantic role labeling models for new languages have been proposed. On one end of the scale is unsupervised SRL, such as Grenager and"
P13-1117,W06-1601,0,0.127753,"Vogel, 2011) and dialogue systems (Basili et al., 2009; van der Plas et al., 2009). Multiple models have been designed to automatically predict semantic roles, and a considerable amount of data has been annotated to train these models, if only for a few more popular languages. As the annotation is costly, one would like to leverage existing resources to minimize the human effort required to construct a model for a new language. A number of approaches to the construction of semantic role labeling models for new languages have been proposed. On one end of the scale is unsupervised SRL, such as Grenager and Manning (2006), which requires some expert knowledge, but no labeled data. It clusters together arguments that should bear the same semantic role, but does not assign a particular role to each cluster. On the other end is annotating a new dataset from scratch. There are also intermediate options, which often make use of similarities between languages. This way, if an accurate model exists for one language, it should help simplify the construction of a model for another, related language. The approaches in this third group often use parallel data to bridge the gap between languages. Cross-lingual annotation"
P13-1117,W02-2231,0,0.0657153,"Missing"
P13-1117,hajic-etal-2012-announcing,0,0.27893,"r, they are very sensitive to the quality of parallel data, as well as the accuracy of a sourcelanguage model on it. An alternative approach, known as cross-lingual model transfer, or cross-lingual model adaptation, consists of modifying a source-language model to make it directly applicable to a new language. This usually involves constructing a shared feature representation across the two languages. McDonald et al. (2011) successfully apply this idea to the transfer of dependency parsers, using part-ofspeech tags as the shared representation of words. A later extension of T¨ackstr¨om et al. (2012) enriches this representation with cross-lingual word clusters, considerably improving the performance. In the case of SRL, a shared representation that is purely syntactic is likely to be insufficient, since structures with different semantics may be realized by the same syntactic construct, for example “in August” vs “in Britain”. However with the help of recently introduced cross-lingual word represen1190 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1190–1200, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics"
P13-1117,D08-1008,0,0.0787868,"Missing"
P13-1117,W07-1206,0,0.0584184,"one language to another using a shared feature representation. This approach is then evaluated on three language pairs, demonstrating competitive performance as compared to a state-of-the-art unsupervised SRL system and a cross-lingual annotation projection baseline. We also consider the contribution of different aspects of the feature representation to the performance of the model and discuss practical applicability of this method. 1 Background and Motivation Semantic role labeling has proven useful in many natural language processing tasks, such as question answering (Shen and Lapata, 2007; Kaisser and Webber, 2007), textual entailment (Sammons et al., 2009), machine translation (Wu and Fung, 2009; Liu and Gildea, 2010; Gao and Vogel, 2011) and dialogue systems (Basili et al., 2009; van der Plas et al., 2009). Multiple models have been designed to automatically predict semantic roles, and a considerable amount of data has been annotated to train these models, if only for a few more popular languages. As the annotation is costly, one would like to leverage existing resources to minimize the human effort required to construct a model for a new language. A number of approaches to the construction of semanti"
P13-1117,C10-1064,0,0.0348397,". Although the models thus obtained are generally imperfect, they can be further refined for a particular language and domain using techniques such as active learning (Settles, 2010; Chen et al., 2011). Cross-lingual annotation projection (Yarowsky et al., 2001) approaches have been applied extensively to a variety of tasks, including POS tagging (Xi and Hwa, 2005; Das and Petrov, 2011), morphology segmentation (Snyder and Barzilay, 2008), verb classification (Merlo et al., 2002), mention detection (Zitouni and Florian, 2008), LFG parsing (Wr´oblewska and Frank, 2009), information extraction (Kim et al., 2010), SRL (Pad´o and Lapata, 2009; van der Plas et al., 2011; Annesi and Basili, 2010; Tonelli and Pianta, 2008), dependency parsing (Naseem et al., 2012; Ganchev et al., 2009; Smith and Eisner, 2009; Hwa et al., 2005) or temporal relation prediction (Spreyer and Frank, 2008). Interestingly, it has also been used to propagate morphosyntactic information between old and modern versions of the same language (Meyer, 2011). Cross-lingual model transfer methods (McDonald et al., 2011; Zeman and Resnik, 2008; Durrett et al., 2012; Søgaard, 2011; Lopez et al., 2008) have also been receiving much attentio"
P13-1117,C12-1089,1,0.934479,"of SRL, a shared representation that is purely syntactic is likely to be insufficient, since structures with different semantics may be realized by the same syntactic construct, for example “in August” vs “in Britain”. However with the help of recently introduced cross-lingual word represen1190 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1190–1200, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics tations, such as the cross-lingual clustering mentioned above or cross-lingual distributed word representations of Klementiev et al. (2012), we may be able to transfer models of shallow semantics in a similar fashion. In this work we construct a shared feature representation for a pair of languages, employing crosslingual representations of syntactic and lexical information, train a semantic role labeling model on one language and apply it to the other one. This approach yields an SRL model for a new language at a very low cost, effectively requiring only a source language model and parallel data. We evaluate on five (directed) language pairs – EN - ZH , ZH - EN, EN - CZ, CZ - EN and EN - FR , where EN , FR , CZ and ZH denote Eng"
P13-1117,2005.mtsummit-papers.11,0,0.249319,"jiˇc, 2002) and an inventory of semantic roles (or functors), most of which are interpretable across various predicates. Also note that the syntactic annotation of English and Czech in PCEDT 2.0 is quite similar (to the extent permitted by the difference in the structure of the two languages) and we can use the dependency relations in our experiments. For English-French, the English CoNLL-ST dataset was used as a source and the model was evaluated on the manually annotated dataset from van der Plas et al. (2011). The latter contains one thousand sentences from the French part of the Europarl (Koehn, 2005) corpus, annotated with semantic roles following an adapted version of PropBank (Palmer et al., 2005) guidelines. The authors perform annotation projection from English to French, using a joint model of syntax and semantics and employing heuristics for filtering. We use a model trained on the output of this projection system as one of the baselines. The evaluation dataset is relatively small in this case, so we perform the transfer only one-way, from English to French. The part-of-speech tags in all datasets were replaced with the universal POS tags of Petrov et al. (2012). For Czech, we have"
P13-1117,N10-1137,0,0.193129,"– the decisions regarding the classification of different arguments are made independently of each other. With respect to the use of syntactic annotation we consider two options: using an existing dependency parser for the target language and obtaining one by means of cross-lingual transfer (see section 4.2). Following McDonald et al. (2011), we assume that a part-of-speech tagger is available for the target language. 2.2 SRL in the Low-resource Setting Several approaches have been proposed to obtain an SRL model for a new language with little or no manual annotation. Unsupervised SRL models (Lang and Lapata, 2010) cluster the arguments of predicates in a given corpus according to their semantic roles. The performance of such models can be impressive, especially for those languages where semantic roles correlate strongly with syntactic relation of the argument to its predicate. However, assigning meaningful role labels to the resulting clusters requires additional effort and the model’s parameters generally need some adjustment for every language. If the necessary resources are already available for a closely related language, they can be utilized to facilitate the construction of a model for the target"
P13-1117,P11-1112,0,0.294116,"e use the F1 measure as a metric for the argument identification stage and accuracy as an aggregate measure of argument classification performance. When comparing to the unsupervised SRL system the clustering evaluation measures are used instead. These are purity and collocation 1191 the word in the source language it is most often aligned to. 1 X PU = max |Gj ∩ Ci | j N i CO = 1 X max |Gj ∩ Ci |, i N j where Ci is the set of arguments in the i-th induced cluster, Gj is the set of arguments in the jth gold cluster and N is the total number of arguments. We report the harmonic mean of the two (Lang and Lapata, 2011) and denote it F1c to avoid confusing it with the supervised metric. 3 3.2 Model Transfer The idea of this work is to abstract the model away from the particular source language and apply it to a new one. This setup requires that we use the same feature representation for both languages, for example part-of-speech tags and dependency relation labels should be from the same inventory. Some features are not applicable to certain languages because the corresponding phenomena are absent in them. For example, consider a strongly inflected language and an analytic one. While the latter can usually c"
P13-1117,C10-1081,0,0.0367706,"ge pairs, demonstrating competitive performance as compared to a state-of-the-art unsupervised SRL system and a cross-lingual annotation projection baseline. We also consider the contribution of different aspects of the feature representation to the performance of the model and discuss practical applicability of this method. 1 Background and Motivation Semantic role labeling has proven useful in many natural language processing tasks, such as question answering (Shen and Lapata, 2007; Kaisser and Webber, 2007), textual entailment (Sammons et al., 2009), machine translation (Wu and Fung, 2009; Liu and Gildea, 2010; Gao and Vogel, 2011) and dialogue systems (Basili et al., 2009; van der Plas et al., 2009). Multiple models have been designed to automatically predict semantic roles, and a considerable amount of data has been annotated to train these models, if only for a few more popular languages. As the annotation is costly, one would like to leverage existing resources to minimize the human effort required to construct a model for a new language. A number of approaches to the construction of semantic role labeling models for new languages have been proposed. On one end of the scale is unsupervised SRL,"
P13-1117,I08-3008,0,0.372997,"nd Frank, 2009), information extraction (Kim et al., 2010), SRL (Pad´o and Lapata, 2009; van der Plas et al., 2011; Annesi and Basili, 2010; Tonelli and Pianta, 2008), dependency parsing (Naseem et al., 2012; Ganchev et al., 2009; Smith and Eisner, 2009; Hwa et al., 2005) or temporal relation prediction (Spreyer and Frank, 2008). Interestingly, it has also been used to propagate morphosyntactic information between old and modern versions of the same language (Meyer, 2011). Cross-lingual model transfer methods (McDonald et al., 2011; Zeman and Resnik, 2008; Durrett et al., 2012; Søgaard, 2011; Lopez et al., 2008) have also been receiving much attention recently. The basic idea behind model transfer is similar to that of cross-lingual annotation projection, as we can see from the way parallel data is used in, for example, McDonald et al. (2011). A crucial component of direct transfer approaches is the unified feature representation. There are at least two such representations of lexical information (Klementiev et al., 2012; T¨ackstr¨om et al., 2012), but both work on word level. This makes it hard to account for phenomena that are expressed differently in the languages considered, for example the synta"
P13-1117,W12-3404,0,0.0415517,"Missing"
P13-1117,D11-1006,0,0.510005,"rallel data to bridge the gap between languages. Cross-lingual annotation projection systems (Pad´o and Lapata, 2009), for example, propagate information directly via word alignment links. However, they are very sensitive to the quality of parallel data, as well as the accuracy of a sourcelanguage model on it. An alternative approach, known as cross-lingual model transfer, or cross-lingual model adaptation, consists of modifying a source-language model to make it directly applicable to a new language. This usually involves constructing a shared feature representation across the two languages. McDonald et al. (2011) successfully apply this idea to the transfer of dependency parsers, using part-ofspeech tags as the shared representation of words. A later extension of T¨ackstr¨om et al. (2012) enriches this representation with cross-lingual word clusters, considerably improving the performance. In the case of SRL, a shared representation that is purely syntactic is likely to be insufficient, since structures with different semantics may be realized by the same syntactic construct, for example “in August” vs “in Britain”. However with the help of recently introduced cross-lingual word represen1190 Proceedin"
P13-1117,P02-1027,0,0.0787163,"ng problem, and adaptation of existing models to new languages presents a viable alternative to exhaustive annotation for each language. Although the models thus obtained are generally imperfect, they can be further refined for a particular language and domain using techniques such as active learning (Settles, 2010; Chen et al., 2011). Cross-lingual annotation projection (Yarowsky et al., 2001) approaches have been applied extensively to a variety of tasks, including POS tagging (Xi and Hwa, 2005; Das and Petrov, 2011), morphology segmentation (Snyder and Barzilay, 2008), verb classification (Merlo et al., 2002), mention detection (Zitouni and Florian, 2008), LFG parsing (Wr´oblewska and Frank, 2009), information extraction (Kim et al., 2010), SRL (Pad´o and Lapata, 2009; van der Plas et al., 2011; Annesi and Basili, 2010; Tonelli and Pianta, 2008), dependency parsing (Naseem et al., 2012; Ganchev et al., 2009; Smith and Eisner, 2009; Hwa et al., 2005) or temporal relation prediction (Spreyer and Frank, 2008). Interestingly, it has also been used to propagate morphosyntactic information between old and modern versions of the same language (Meyer, 2011). Cross-lingual model transfer methods (McDonald"
P13-1117,P12-1066,0,0.0222569,"as active learning (Settles, 2010; Chen et al., 2011). Cross-lingual annotation projection (Yarowsky et al., 2001) approaches have been applied extensively to a variety of tasks, including POS tagging (Xi and Hwa, 2005; Das and Petrov, 2011), morphology segmentation (Snyder and Barzilay, 2008), verb classification (Merlo et al., 2002), mention detection (Zitouni and Florian, 2008), LFG parsing (Wr´oblewska and Frank, 2009), information extraction (Kim et al., 2010), SRL (Pad´o and Lapata, 2009; van der Plas et al., 2011; Annesi and Basili, 2010; Tonelli and Pianta, 2008), dependency parsing (Naseem et al., 2012; Ganchev et al., 2009; Smith and Eisner, 2009; Hwa et al., 2005) or temporal relation prediction (Spreyer and Frank, 2008). Interestingly, it has also been used to propagate morphosyntactic information between old and modern versions of the same language (Meyer, 2011). Cross-lingual model transfer methods (McDonald et al., 2011; Zeman and Resnik, 2008; Durrett et al., 2012; Søgaard, 2011; Lopez et al., 2008) have also been receiving much attention recently. The basic idea behind model transfer is similar to that of cross-lingual annotation projection, as we can see from the way parallel data"
P13-1117,J08-4003,0,0.0130348,"s were obtained using GIZA++ (Och and Ney, 2003) and the intersection heuristic. 4.2 Syntactic Transfer In the low-resource setting, we cannot always rely on the availability of an accurate dependency parser for the target language. If one is not available, the natural solution would be to use crosslingual model transfer to obtain it. Unfortunately, the models presented in the previous work, such as Zeman and Resnik (2008), McDonald et al. (2011) and T¨ackstr¨om et al. (2012), were not made available, so we reproduced the direct transfer algorithm of McDonald et al. (2011), using Malt parser (Nivre, 2008) and the same set of features. We did not reimplement the projected transfer algorithm, however, and used the default training procedure instead of perceptron-based learning. The dependency structure thus obtained is, of course, only a rough approximation – even a much more sophisticated algorithm may not perform well when transferring syntax between such languages as Czech and English, given the inherent difference in their structure. The scores are shown in table 2. We will henceforth refer to the syntactic annotations that were provided with the datasets as original, as opposed to the annot"
P13-1117,J03-1002,0,0.00570173,"gs of Petrov et al. (2012). For Czech, we have augmented the mappings to account for the tags that were not present in the datasets from which the original mappings were derived. Namely, tag “t” is mapped to “VERB” and “Y” – to “PRON”. We use parallel data to construct a bilingual dictionary used in word mapping, as well as in the projection baseline. For English-Czech 1 see http://www.ml4nlp.de/code-and-data/treex2conll and English-French, the data is drawn from Europarl (Koehn, 2005), for English-Chinese – from MultiUN (Eisele and Chen, 2010). The word alignments were obtained using GIZA++ (Och and Ney, 2003) and the intersection heuristic. 4.2 Syntactic Transfer In the low-resource setting, we cannot always rely on the availability of an accurate dependency parser for the target language. If one is not available, the natural solution would be to use crosslingual model transfer to obtain it. Unfortunately, the models presented in the previous work, such as Zeman and Resnik (2008), McDonald et al. (2011) and T¨ackstr¨om et al. (2012), were not made available, so we reproduced the direct transfer algorithm of McDonald et al. (2011), using Malt parser (Nivre, 2008) and the same set of features. We di"
P13-1117,J05-1004,0,0.362938,"across various predicates. Also note that the syntactic annotation of English and Czech in PCEDT 2.0 is quite similar (to the extent permitted by the difference in the structure of the two languages) and we can use the dependency relations in our experiments. For English-French, the English CoNLL-ST dataset was used as a source and the model was evaluated on the manually annotated dataset from van der Plas et al. (2011). The latter contains one thousand sentences from the French part of the Europarl (Koehn, 2005) corpus, annotated with semantic roles following an adapted version of PropBank (Palmer et al., 2005) guidelines. The authors perform annotation projection from English to French, using a joint model of syntax and semantics and employing heuristics for filtering. We use a model trained on the output of this projection system as one of the baselines. The evaluation dataset is relatively small in this case, so we perform the transfer only one-way, from English to French. The part-of-speech tags in all datasets were replaced with the universal POS tags of Petrov et al. (2012). For Czech, we have augmented the mappings to account for the tags that were not present in the datasets from which the o"
P13-1117,petrov-etal-2012-universal,0,0.279379,"sentations in this space. Word mapping. The first option is simply to use the source language words as the shared representation. Here every source language word would have itself as its representation and every target word would map into a source word that corresponds to it. In other words, we supply the model with a gloss of the target sentence. The mapping (bilingual dictionary) we use is derived from a word-aligned parallel corpus, by identifying, for each word in the target language, Syntactic Information Part-of-speech Tags. We map part-of-speech tags into the universal tagset following Petrov et al. (2012). This may have a negative effect on the performance of a monolingual model, since most part-of-speech tagsets are more fine-grained than the universal POS tags considered here. For example Penn Treebank inventory contains 36 tags and the universal POS tagset – only 12. Since the finergrained POS tags often reflect more languagespecific phenomena, however, they would only be useful for very closely related languages in the cross-lingual setting. The universal part-of-speech tags used in evaluation are derived from gold-standard annotation for all languages except French, where predicted ones h"
P13-1117,D07-1002,0,0.0786053,"ring an SRL model from one language to another using a shared feature representation. This approach is then evaluated on three language pairs, demonstrating competitive performance as compared to a state-of-the-art unsupervised SRL system and a cross-lingual annotation projection baseline. We also consider the contribution of different aspects of the feature representation to the performance of the model and discuss practical applicability of this method. 1 Background and Motivation Semantic role labeling has proven useful in many natural language processing tasks, such as question answering (Shen and Lapata, 2007; Kaisser and Webber, 2007), textual entailment (Sammons et al., 2009), machine translation (Wu and Fung, 2009; Liu and Gildea, 2010; Gao and Vogel, 2011) and dialogue systems (Basili et al., 2009; van der Plas et al., 2009). Multiple models have been designed to automatically predict semantic roles, and a considerable amount of data has been annotated to train these models, if only for a few more popular languages. As the annotation is costly, one would like to leverage existing resources to minimize the human effort required to construct a model for a new language. A number of approaches to"
P13-1117,D09-1086,0,0.0380716,"al., 2011). Cross-lingual annotation projection (Yarowsky et al., 2001) approaches have been applied extensively to a variety of tasks, including POS tagging (Xi and Hwa, 2005; Das and Petrov, 2011), morphology segmentation (Snyder and Barzilay, 2008), verb classification (Merlo et al., 2002), mention detection (Zitouni and Florian, 2008), LFG parsing (Wr´oblewska and Frank, 2009), information extraction (Kim et al., 2010), SRL (Pad´o and Lapata, 2009; van der Plas et al., 2011; Annesi and Basili, 2010; Tonelli and Pianta, 2008), dependency parsing (Naseem et al., 2012; Ganchev et al., 2009; Smith and Eisner, 2009; Hwa et al., 2005) or temporal relation prediction (Spreyer and Frank, 2008). Interestingly, it has also been used to propagate morphosyntactic information between old and modern versions of the same language (Meyer, 2011). Cross-lingual model transfer methods (McDonald et al., 2011; Zeman and Resnik, 2008; Durrett et al., 2012; Søgaard, 2011; Lopez et al., 2008) have also been receiving much attention recently. The basic idea behind model transfer is similar to that of cross-lingual annotation projection, as we can see from the way parallel data is used in, for example, McDonald et al. (2011"
P13-1117,P11-2120,0,0.0283472,"(Wr´oblewska and Frank, 2009), information extraction (Kim et al., 2010), SRL (Pad´o and Lapata, 2009; van der Plas et al., 2011; Annesi and Basili, 2010; Tonelli and Pianta, 2008), dependency parsing (Naseem et al., 2012; Ganchev et al., 2009; Smith and Eisner, 2009; Hwa et al., 2005) or temporal relation prediction (Spreyer and Frank, 2008). Interestingly, it has also been used to propagate morphosyntactic information between old and modern versions of the same language (Meyer, 2011). Cross-lingual model transfer methods (McDonald et al., 2011; Zeman and Resnik, 2008; Durrett et al., 2012; Søgaard, 2011; Lopez et al., 2008) have also been receiving much attention recently. The basic idea behind model transfer is similar to that of cross-lingual annotation projection, as we can see from the way parallel data is used in, for example, McDonald et al. (2011). A crucial component of direct transfer approaches is the unified feature representation. There are at least two such representations of lexical information (Klementiev et al., 2012; T¨ackstr¨om et al., 2012), but both work on word level. This makes it hard to account for phenomena that are expressed differently in the languages considered,"
P13-1117,E12-1003,1,0.868647,"certain word may be indicated by a preposition, inflection or word order, depending on the language. Accurate representation of such information would require an extra level of abstraction (Hajiˇc, 2002). A side-effect of using adaptation methods is that we are forced to use the same annotation scheme for the task in question (SRL, in our case), which in turn simplifies the development of cross-lingual tools for downstream tasks. Such representations are also likely to be useful in machine translation. Unsupervised semantic role labeling methods (Lang and Lapata, 2010; Lang and Lapata, 2011; Titov and Klementiev, 2012a; Lorenzo and Cerisara, 2012) also constitute an alternative to cross-lingual model transfer. For an overview of of semi-supervised approaches we refer the reader to Titov and Klementiev (2012b). 7 Conclusion We have considered the cross-lingual model transfer approach as applied to the task of semantic role labeling and observed that for closely related languages it performs comparably to annotation projection approaches. It allows one to quickly construct an SRL model for a new language without manual annotation or language-specific heuristics, provided an accurate model is available for on"
P13-1117,C12-1161,1,0.838001,"certain word may be indicated by a preposition, inflection or word order, depending on the language. Accurate representation of such information would require an extra level of abstraction (Hajiˇc, 2002). A side-effect of using adaptation methods is that we are forced to use the same annotation scheme for the task in question (SRL, in our case), which in turn simplifies the development of cross-lingual tools for downstream tasks. Such representations are also likely to be useful in machine translation. Unsupervised semantic role labeling methods (Lang and Lapata, 2010; Lang and Lapata, 2011; Titov and Klementiev, 2012a; Lorenzo and Cerisara, 2012) also constitute an alternative to cross-lingual model transfer. For an overview of of semi-supervised approaches we refer the reader to Titov and Klementiev (2012b). 7 Conclusion We have considered the cross-lingual model transfer approach as applied to the task of semantic role labeling and observed that for closely related languages it performs comparably to annotation projection approaches. It allows one to quickly construct an SRL model for a new language without manual annotation or language-specific heuristics, provided an accurate model is available for on"
P13-1117,tonelli-pianta-2008-frame,0,0.0653132,"cular language and domain using techniques such as active learning (Settles, 2010; Chen et al., 2011). Cross-lingual annotation projection (Yarowsky et al., 2001) approaches have been applied extensively to a variety of tasks, including POS tagging (Xi and Hwa, 2005; Das and Petrov, 2011), morphology segmentation (Snyder and Barzilay, 2008), verb classification (Merlo et al., 2002), mention detection (Zitouni and Florian, 2008), LFG parsing (Wr´oblewska and Frank, 2009), information extraction (Kim et al., 2010), SRL (Pad´o and Lapata, 2009; van der Plas et al., 2011; Annesi and Basili, 2010; Tonelli and Pianta, 2008), dependency parsing (Naseem et al., 2012; Ganchev et al., 2009; Smith and Eisner, 2009; Hwa et al., 2005) or temporal relation prediction (Spreyer and Frank, 2008). Interestingly, it has also been used to propagate morphosyntactic information between old and modern versions of the same language (Meyer, 2011). Cross-lingual model transfer methods (McDonald et al., 2011; Zeman and Resnik, 2008; Durrett et al., 2012; Søgaard, 2011; Lopez et al., 2008) have also been receiving much attention recently. The basic idea behind model transfer is similar to that of cross-lingual annotation projection,"
P13-1117,N09-2032,0,0.133935,"Missing"
P13-1117,P11-2052,0,0.276739,"Missing"
P13-1117,2009.eamt-1.30,0,0.0135443,"ted on three language pairs, demonstrating competitive performance as compared to a state-of-the-art unsupervised SRL system and a cross-lingual annotation projection baseline. We also consider the contribution of different aspects of the feature representation to the performance of the model and discuss practical applicability of this method. 1 Background and Motivation Semantic role labeling has proven useful in many natural language processing tasks, such as question answering (Shen and Lapata, 2007; Kaisser and Webber, 2007), textual entailment (Sammons et al., 2009), machine translation (Wu and Fung, 2009; Liu and Gildea, 2010; Gao and Vogel, 2011) and dialogue systems (Basili et al., 2009; van der Plas et al., 2009). Multiple models have been designed to automatically predict semantic roles, and a considerable amount of data has been annotated to train these models, if only for a few more popular languages. As the annotation is costly, one would like to leverage existing resources to minimize the human effort required to construct a model for a new language. A number of approaches to the construction of semantic role labeling models for new languages have been proposed. On one end of the scal"
P13-1117,H05-1107,0,0.187032,"tactic information alone. 6 Related Work Development of robust statistical models for core NLP tasks is a challenging problem, and adaptation of existing models to new languages presents a viable alternative to exhaustive annotation for each language. Although the models thus obtained are generally imperfect, they can be further refined for a particular language and domain using techniques such as active learning (Settles, 2010; Chen et al., 2011). Cross-lingual annotation projection (Yarowsky et al., 2001) approaches have been applied extensively to a variety of tasks, including POS tagging (Xi and Hwa, 2005; Das and Petrov, 2011), morphology segmentation (Snyder and Barzilay, 2008), verb classification (Merlo et al., 2002), mention detection (Zitouni and Florian, 2008), LFG parsing (Wr´oblewska and Frank, 2009), information extraction (Kim et al., 2010), SRL (Pad´o and Lapata, 2009; van der Plas et al., 2011; Annesi and Basili, 2010; Tonelli and Pianta, 2008), dependency parsing (Naseem et al., 2012; Ganchev et al., 2009; Smith and Eisner, 2009; Hwa et al., 2005) or temporal relation prediction (Spreyer and Frank, 2008). Interestingly, it has also been used to propagate morphosyntactic informati"
P13-1117,I08-1064,0,0.0168277,"proaches have been applied extensively to a variety of tasks, including POS tagging (Xi and Hwa, 2005; Das and Petrov, 2011), morphology segmentation (Snyder and Barzilay, 2008), verb classification (Merlo et al., 2002), mention detection (Zitouni and Florian, 2008), LFG parsing (Wr´oblewska and Frank, 2009), information extraction (Kim et al., 2010), SRL (Pad´o and Lapata, 2009; van der Plas et al., 2011; Annesi and Basili, 2010; Tonelli and Pianta, 2008), dependency parsing (Naseem et al., 2012; Ganchev et al., 2009; Smith and Eisner, 2009; Hwa et al., 2005) or temporal relation prediction (Spreyer and Frank, 2008). Interestingly, it has also been used to propagate morphosyntactic information between old and modern versions of the same language (Meyer, 2011). Cross-lingual model transfer methods (McDonald et al., 2011; Zeman and Resnik, 2008; Durrett et al., 2012; Søgaard, 2011; Lopez et al., 2008) have also been receiving much attention recently. The basic idea behind model transfer is similar to that of cross-lingual annotation projection, as we can see from the way parallel data is used in, for example, McDonald et al. (2011). A crucial component of direct transfer approaches is the unified feature r"
P13-1117,H01-1035,0,0.345167,". The performance of such models can be impressive, especially for those languages where semantic roles correlate strongly with syntactic relation of the argument to its predicate. However, assigning meaningful role labels to the resulting clusters requires additional effort and the model’s parameters generally need some adjustment for every language. If the necessary resources are already available for a closely related language, they can be utilized to facilitate the construction of a model for the target language. This can be achieved either by means of cross-lingual annotation projection (Yarowsky et al., 2001) or by cross-lingual model transfer (Zeman and Resnik, 2008). This last approach is the one we are considering in this work, and the other two options are treated as baselines. The unsupervised model will be further referred to as U NSUP and the projection baseline as P ROJ. 2.3 Evaluation Measures We use the F1 measure as a metric for the argument identification stage and accuracy as an aggregate measure of argument classification performance. When comparing to the unsupervised SRL system the clustering evaluation measures are used instead. These are purity and collocation 1191 the word in th"
P13-1117,N12-1052,0,0.109786,"Missing"
P13-1117,D08-1063,0,0.0197563,"odels to new languages presents a viable alternative to exhaustive annotation for each language. Although the models thus obtained are generally imperfect, they can be further refined for a particular language and domain using techniques such as active learning (Settles, 2010; Chen et al., 2011). Cross-lingual annotation projection (Yarowsky et al., 2001) approaches have been applied extensively to a variety of tasks, including POS tagging (Xi and Hwa, 2005; Das and Petrov, 2011), morphology segmentation (Snyder and Barzilay, 2008), verb classification (Merlo et al., 2002), mention detection (Zitouni and Florian, 2008), LFG parsing (Wr´oblewska and Frank, 2009), information extraction (Kim et al., 2010), SRL (Pad´o and Lapata, 2009; van der Plas et al., 2011; Annesi and Basili, 2010; Tonelli and Pianta, 2008), dependency parsing (Naseem et al., 2012; Ganchev et al., 2009; Smith and Eisner, 2009; Hwa et al., 2005) or temporal relation prediction (Spreyer and Frank, 2008). Interestingly, it has also been used to propagate morphosyntactic information between old and modern versions of the same language (Meyer, 2011). Cross-lingual model transfer methods (McDonald et al., 2011; Zeman and Resnik, 2008; Durrett e"
P13-1117,W09-1201,0,\N,Missing
P13-1160,S07-1002,0,0.0124771,"ed in our annotation scheme (see Table 2). The hyperpriors were chosen in a qualitative experiment over a subset of our dataset by manually inspecting the produced languages models. The resulting values are: α = 10−3 , β = 5 ∗ 10−4 , τ = 5 ∗ 10−4 , η = 10−3 , ν4 = 103 , ν¯4 = 10−4 , ωθ = 85 and ωθ0 = ωψ = ωψ0 = 5. 5.1 Direct clustering evaluation Our labels encoding aspect and sentiment level can be regarded as clusters. Consequently we can apply techniques developed in the context of clustering evaluation. We use a version of the standard metrics considered for the word sense induction task (Agirre and Soroa, 2007) where a clustering is converted to a classification problem. This is achieved by splitting the gold standard into two subsets; the training portion is used to choose oneto-one correspondence from the gold classes to the induced clusters and then the chosen mapping is applied to the testing portion. We perform 10-fold cross validation and report precision, recall and F1 score. Our dataset is very skewed and the majority class (rest) is arguably the least important, so we use macro-averaging over labels and then average those across folds to arrive to the reported numbers. We compare the discou"
P13-1160,C08-2002,0,0.0857708,"earns those polarity shifters relying on sentence-level sentiment labels. Though successful, this model is unlikely to capture intra-sentence non-local phenomena such as effect of discourse connectives, unless it is provided with syntactic information as an input. This may be problematic for the noisy sentiment-analysis domain and especially for poor-resource languages. Similar to our work, others have focused on modeling interactions between phrases and sentences. However, this has been achieved by either using a subset of relations that can be found in discourse theories (Zhou et al., 2011; Asher et al., 2008; Snyder and Barzilay, 2007) or by using directly (Taboada et al., 2008) the output of discourse parsers (Soricut and Marcu, 2003). Discourse cues as predictive features of topic boundaries have also been considered in Eisenstein and Barzilay (2008). This work was extended by Trivedi and Eisenstein (2013), where discourse connectors are used as features for modeling subjectivity transitions. Another related line of research was presented in Somasundaran et al. (2009) where a domainspecific discourse scheme is considered. Similarly to our set-up, discourse relations enforce constraints on senti"
P13-1160,D08-1035,0,0.0100928,"ic information as an input. This may be problematic for the noisy sentiment-analysis domain and especially for poor-resource languages. Similar to our work, others have focused on modeling interactions between phrases and sentences. However, this has been achieved by either using a subset of relations that can be found in discourse theories (Zhou et al., 2011; Asher et al., 2008; Snyder and Barzilay, 2007) or by using directly (Taboada et al., 2008) the output of discourse parsers (Soricut and Marcu, 2003). Discourse cues as predictive features of topic boundaries have also been considered in Eisenstein and Barzilay (2008). This work was extended by Trivedi and Eisenstein (2013), where discourse connectors are used as features for modeling subjectivity transitions. Another related line of research was presented in Somasundaran et al. (2009) where a domainspecific discourse scheme is considered. Similarly to our set-up, discourse relations enforce constraints on sentiment polarity of associated sentiment expressions. Somasundaran et al. (2009) show that gold-standard discourse information encoded in this way provides a useful signal for prediction of sentiment, but they leave automatic discourse relation predict"
P13-1160,E12-1021,0,0.148328,"Missing"
P13-1160,N10-1120,0,0.453483,"on for Computational Linguistics, pages 1630–1639, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics count for these discourse phenomena and cannot rely solely on local lexical information. These issues have not gone unnoticed to the research community. Consequently, there has recently been an increased interest in models that leverage content and discourse structure in sentiment analysis tasks. However, discourse-level information is typically incorporated in a pipeline architecture, either in the form of sentiment polarity shifters (Polanyi and Zaenen, 2006; Nakagawa et al., 2010) that operate on the lexical level or by using discourse relations (Taboada et al., 2008; Zhou et al., 2011) that comply with discourse theories like Rhetorical Structure Theory (RST) (Mann and Thompson, 1988). Such approaches have a number of disadvantages. First, they require additional resources, such as lists of polarity shifters or discourse connectives which signal specific relations. These resources are available only for a handful of languages. Second, relying on a generic discourse analysis step that is carried out before sentiment analysis may introduce additional noise and lead to e"
P13-1160,prasad-etal-2008-penn,0,0.0143278,"(or negative) is also vaguely defined. To gain insight into our model, we conducted an experiment similar to the one presented in Somasundaran et al. (2009). We divide the dataset in two subsets; one containing all EDUs starting with a discourse cue (“marked”) and one containing the remaining EDUs (“unmarked”). We hypothesize that the effect of the discourse-aware model should be stronger on the first subset, since the presence of the connective indicates the possibility of a discourse relation with the previous EDU. The set of discourse connectives is taken from the Penn Discourse Treebank (Prasad et al., 2008), thus creating a list of 240 potential connectives. Table 5 presents a subset of “marked” EDUs for which trying to assign the sentiment and aspect out of context (i.e. without the previous EDU) is a difficult task. In examples 1-3 there is no explicit mention of the aspect. However, there is an anaphoric expression (marked in bold) which 1635 refers to a mention of the aspect in some previous EDU. On the other hand, in examples 4 and 5 there is an ambiguity in the choice of aspect; in example 5, tea making facilities can refer to a breakfast at the hotel (label food) or to facilities in the r"
P13-1160,sadamitsu-etal-2008-sentiment,0,0.0271863,"e beneficial. 6 Related Work Recently, there has been significant interest in leveraging content structure for a number of NLP tasks (Webber et al., 2011). Sentiment analysis has not been an exception to this and discourse has been used in order to enforce constraints on the assignment of polarity labels at several granularity levels, ranging from the lexical level (Polanyi and Zaenen, 2006) to the review level (Taboada et al., 2011). One way to deal with this problem is to model the interactions by using a precompiled set of polarity shifters (Nakagawa et al., 2010; Polanyi and Zaenen, 2006; Sadamitsu et al., 2008). Socher et al. (2011) defined a recurrent neural network model, which, in essence, learns those polarity shifters relying on sentence-level sentiment labels. Though successful, this model is unlikely to capture intra-sentence non-local phenomena such as effect of discourse connectives, unless it is provided with syntactic information as an input. This may be problematic for the noisy sentiment-analysis domain and especially for poor-resource languages. Similar to our work, others have focused on modeling interactions between phrases and sentences. However, this has been achieved by either usi"
P13-1160,N07-1038,0,0.110215,"shifters relying on sentence-level sentiment labels. Though successful, this model is unlikely to capture intra-sentence non-local phenomena such as effect of discourse connectives, unless it is provided with syntactic information as an input. This may be problematic for the noisy sentiment-analysis domain and especially for poor-resource languages. Similar to our work, others have focused on modeling interactions between phrases and sentences. However, this has been achieved by either using a subset of relations that can be found in discourse theories (Zhou et al., 2011; Asher et al., 2008; Snyder and Barzilay, 2007) or by using directly (Taboada et al., 2008) the output of discourse parsers (Soricut and Marcu, 2003). Discourse cues as predictive features of topic boundaries have also been considered in Eisenstein and Barzilay (2008). This work was extended by Trivedi and Eisenstein (2013), where discourse connectors are used as features for modeling subjectivity transitions. Another related line of research was presented in Somasundaran et al. (2009) where a domainspecific discourse scheme is considered. Similarly to our set-up, discourse relations enforce constraints on sentiment polarity of associated"
P13-1160,D11-1014,0,0.0612942,"ork Recently, there has been significant interest in leveraging content structure for a number of NLP tasks (Webber et al., 2011). Sentiment analysis has not been an exception to this and discourse has been used in order to enforce constraints on the assignment of polarity labels at several granularity levels, ranging from the lexical level (Polanyi and Zaenen, 2006) to the review level (Taboada et al., 2011). One way to deal with this problem is to model the interactions by using a precompiled set of polarity shifters (Nakagawa et al., 2010; Polanyi and Zaenen, 2006; Sadamitsu et al., 2008). Socher et al. (2011) defined a recurrent neural network model, which, in essence, learns those polarity shifters relying on sentence-level sentiment labels. Though successful, this model is unlikely to capture intra-sentence non-local phenomena such as effect of discourse connectives, unless it is provided with syntactic information as an input. This may be problematic for the noisy sentiment-analysis domain and especially for poor-resource languages. Similar to our work, others have focused on modeling interactions between phrases and sentences. However, this has been achieved by either using a subset of relatio"
P13-1160,C08-1101,0,0.0120621,"ven better) or both (the room was nice but the breakfast was awful). Therefore, we do not explicitly model 3 The ‘explanation’ relation, for example, can occur with a polarity change (We were upgraded to a really nice room because the hotel made a terrible blunder with our booking) but does not have to (The room was really nice because the hotel was newly renovated). 1631 Name AltSame SameAlt AltAlt Description different polarity, same aspect same polarity, different aspect different polarity and aspect Table 1: Discourse relations generic discourse relations; instead, inspired by the work of Somasundaran et al. (2008), we define three very general relations which encode how polarity and aspect change (Table 1). Note that we do not have a discourse relation SameSame since we do not expect to have strong linguistic evidence which states that an EDU contains the same sentiment information as the previous one.4 However, we assume that the sentiment and topic flow is fairly smooth in general. In other words, for two adjacent EDUs not connected by any of the above three relations, the prior probability of staying at the same topic and sentiment level is higher than picking a new topic and sentiment level (i.e. w"
P13-1160,D09-1018,0,0.725925,"st, they require additional resources, such as lists of polarity shifters or discourse connectives which signal specific relations. These resources are available only for a handful of languages. Second, relying on a generic discourse analysis step that is carried out before sentiment analysis may introduce additional noise and lead to error propagation. Furthermore, these techniques will not necessarily be able to induce discourse relations informative for the sentiment analysis domain (Voll and Taboada, 2007). An alternative approach is to define a taskspecific scheme of discourse relations (Somasundaran et al., 2009). This previous work showed that task-specific discourse relations are helpful in predicting sentiment, however, in doing so they relied on gold-standard discourse annotation at test time rather than predicting it automatically or inducing it jointly with sentiment polarity. We take a different approach and induce discourse and sentiment information jointly in an unsupervised (or weakly supervised) manner. This has the advantage of not having to pre-specify a mapping from discourse cues to discourse relations; our model induces this automatically, which makes it portable to new domains and lan"
P13-1160,N03-1030,0,0.0509325,"re intra-sentence non-local phenomena such as effect of discourse connectives, unless it is provided with syntactic information as an input. This may be problematic for the noisy sentiment-analysis domain and especially for poor-resource languages. Similar to our work, others have focused on modeling interactions between phrases and sentences. However, this has been achieved by either using a subset of relations that can be found in discourse theories (Zhou et al., 2011; Asher et al., 2008; Snyder and Barzilay, 2007) or by using directly (Taboada et al., 2008) the output of discourse parsers (Soricut and Marcu, 2003). Discourse cues as predictive features of topic boundaries have also been considered in Eisenstein and Barzilay (2008). This work was extended by Trivedi and Eisenstein (2013), where discourse connectors are used as features for modeling subjectivity transitions. Another related line of research was presented in Somasundaran et al. (2009) where a domainspecific discourse scheme is considered. Similarly to our set-up, discourse relations enforce constraints on sentiment polarity of associated sentiment expressions. Somasundaran et al. (2009) show that gold-standard discourse information encode"
P13-1160,J11-2001,0,0.0477964,"EDUs are higher than the ones for the “marked” dataset clearly suggests that the latter constitute a hard case for sentiment analysis, in which exploiting discourse signal proves to be beneficial. 6 Related Work Recently, there has been significant interest in leveraging content structure for a number of NLP tasks (Webber et al., 2011). Sentiment analysis has not been an exception to this and discourse has been used in order to enforce constraints on the assignment of polarity labels at several granularity levels, ranging from the lexical level (Polanyi and Zaenen, 2006) to the review level (Taboada et al., 2011). One way to deal with this problem is to model the interactions by using a precompiled set of polarity shifters (Nakagawa et al., 2010; Polanyi and Zaenen, 2006; Sadamitsu et al., 2008). Socher et al. (2011) defined a recurrent neural network model, which, in essence, learns those polarity shifters relying on sentence-level sentiment labels. Though successful, this model is unlikely to capture intra-sentence non-local phenomena such as effect of discourse connectives, unless it is provided with syntactic information as an input. This may be problematic for the noisy sentiment-analysis domain"
P13-1160,P11-2100,0,0.140484,"Missing"
P13-1160,P08-1036,1,0.665334,"the wheat from the chaff, it is necessary to structure the available information. In the review domain, this is done in aspectbased sentiment analysis which aims at identifying text fragments in which opinions are expressed about ratable aspects of products, such as ‘room quality’ or ‘service quality’. Such fine-grained analysis can serve as the first step in aspect-based sentiment summarization (Hu and Liu, 2004), a task with many practical applications. Aspect-based summarization is an active research area for which various techniques have been developed, both statistical (Mei et al., 2007; Titov and McDonald, 2008b) and not (Hu and Liu, 2004), and relying on different types of supervision sources, such as sentiment-annotated texts or polarity lexica (Turney and Littman, 2002). Most methods rely on local information (bag-of-words, short ngrams or elementary syntactic fragments) and do not attempt to account for more complex interactions. However, these local lexical representations by themselves are often not sufficient to infer a sentiment or aspect for a fragment of text. For instance, in the following example taken from a TripAdvisor1 review: Example 1. The room was nice but let’s not talk about the"
P13-1160,N13-1100,0,0.137549,"e noisy sentiment-analysis domain and especially for poor-resource languages. Similar to our work, others have focused on modeling interactions between phrases and sentences. However, this has been achieved by either using a subset of relations that can be found in discourse theories (Zhou et al., 2011; Asher et al., 2008; Snyder and Barzilay, 2007) or by using directly (Taboada et al., 2008) the output of discourse parsers (Soricut and Marcu, 2003). Discourse cues as predictive features of topic boundaries have also been considered in Eisenstein and Barzilay (2008). This work was extended by Trivedi and Eisenstein (2013), where discourse connectors are used as features for modeling subjectivity transitions. Another related line of research was presented in Somasundaran et al. (2009) where a domainspecific discourse scheme is considered. Similarly to our set-up, discourse relations enforce constraints on sentiment polarity of associated sentiment expressions. Somasundaran et al. (2009) show that gold-standard discourse information encoded in this way provides a useful signal for prediction of sentiment, but they leave automatic discourse relation prediction for future work. They use an integer linear programmi"
P13-1160,D11-1015,0,0.432291,"utational Linguistics count for these discourse phenomena and cannot rely solely on local lexical information. These issues have not gone unnoticed to the research community. Consequently, there has recently been an increased interest in models that leverage content and discourse structure in sentiment analysis tasks. However, discourse-level information is typically incorporated in a pipeline architecture, either in the form of sentiment polarity shifters (Polanyi and Zaenen, 2006; Nakagawa et al., 2010) that operate on the lexical level or by using discourse relations (Taboada et al., 2008; Zhou et al., 2011) that comply with discourse theories like Rhetorical Structure Theory (RST) (Mann and Thompson, 1988). Such approaches have a number of disadvantages. First, they require additional resources, such as lists of polarity shifters or discourse connectives which signal specific relations. These resources are available only for a handful of languages. Second, relying on a generic discourse analysis step that is carried out before sentiment analysis may introduce additional noise and lead to error propagation. Furthermore, these techniques will not necessarily be able to induce discourse relations i"
P13-1160,H05-2017,0,\N,Missing
P13-1160,H05-1043,0,\N,Missing
P13-1160,N10-1122,0,\N,Missing
P13-1160,D10-1006,0,\N,Missing
P13-1160,P09-2020,0,\N,Missing
P14-2095,W09-1206,0,0.133266,"Missing"
P14-2095,C10-1011,0,0.0311416,"e the size of the latter dataset is relatively small – one thousand sentences – we reserve the whole dataset for testing and only evaluate transfer from English to French, but not the other way around. Datasets for other languages are sufficiently large, so we take 30 thousand samples for testing and use the rest as training data. The validation set in each experiment is withheld from the corresponding training corpus and contains 10 thousand samples. Parallel data for both language pairs is derived from Europarl (Koehn, 2005), which we preprocess using MATE - TOOLS (Bj¨orkelund et al., 2009; Bohnet, 2010). 3 FRP AP DT 2 5 10 20 50 Number of parallel instances, ×103 Figure 2: English-Czech transfer results Error FRP AP DT 0.40 0.38 0.36 0.34 0.32 2 5 10 20 50 Number of parallel instances, ×103 Figure 3: Czech-English transfer results Results The classification error of FRP and the baselines given varying amount of parallel data is reported in figures 2, 3 and 4. The training set for each language is fixed. We denote the two baselines AP (annotation projection) and DT (direct transfer). The number of parallel instances in these experiments is shown on a logarithmic scale, the values considered a"
P14-2095,hajic-etal-2012-announcing,0,0.0615005,"Missing"
P14-2095,W05-0620,0,0.135818,"Missing"
P14-2095,W06-1615,0,0.0757181,"ing between certain elements of syntactic structure, for example a known part-of-speech tag mapping. The available parallel data itself may also be used more comprehensively – aligned arguments of aligned predicates, for example, constitute only a small part of it, while the mapping of vector representations of individual tokens is likely to be the same for all aligned words. Additional Related Work Apart from the work on direct/projected transfer and annotation projection mentioned above, the proposed method can be seen as a more explicit kind of domain adaptation, similar to Titov (2011) or Blitzer et al. (2006). It is also somewhat similar in spirit to Mikolov et al. (2013b), where a small number of word translation pairs are used to estimate a mapping between distributed representations of words in two different languages and build a word translation model. 5 Conclusions In this paper we propose a new method of crosslingual model transfer, report initial evaluation results and highlight directions for its further development. We observe that the performance of this method is competitive with that of established crosslingual transfer approaches and its application requires very little manual adjustm"
P14-2095,cyrus-2006-building,0,0.0343425,"d, does not require any changes to the feature representation. Instead, it operates on translation pairs, usually on sentence level, applying the available source-side model to the source sentence and transferring the resulting annotations through the word alignment links to the target one. The quality of predictions on source sentences depends heavily on the quality of parallel data and the domain it belongs to (or, rather, the similarity between this domain and that of the corpus the source-language model was trained on). The transfer itself also introduces errors due to translation shifts (Cyrus, 2006) and word alignment errors, which may lead to inaccurate predictions. These issues are generally handled using heuristics (Pad´o and Lapata, 2006) and filtering, for example based on alignment coverage (van der Plas et al., 2011). We propose a novel approach to crosslingual model transfer based on feature representation projection. First, a compact feature representation relevant for the task in question is constructed for either language independently and then the mapping between the two representations is determined using parallel data. The target instance can then be mapped into the source-"
P14-2095,C12-1089,1,0.850287,"trained on the source-language data directly applied to the target language. If parallel data is available, it can be further used to enforce model agreement on this data to adjust for discrepancies between the two languages, for example by means of projected transfer (McDonald et al., 2011). The shared feature representation depends on the task in question, but most often each aspect of the original feature representation is handled separately. Word types, for example, may be replaced by cross-lingual word clusters (T¨ackstr¨om et al., 2012) or cross-lingual distributed word representations (Klementiev et al., 2012). Part-ofspeech tags, which are often language-specific, can be converted into universal part-of-speech tags (Petrov et al., 2012) and morpho-syntactic information can also be represented in a unified way (Zeman et al., 2012; McDonald et al., 2013; Tsarfaty, 2013). Unfortunately, the design of such representations and corresponding conversion procedures is by no means trivial. Annotation projection, on the other hand, does not require any changes to the feature representation. Instead, it operates on translation pairs, usually on sentence level, applying the available source-side model to the"
P14-2095,P11-1061,0,0.052449,"interesting directions for further research. 1 Ivan Titov ILLC, University of Amsterdam Amsterdam, Netherlands titov@uva.nl Introduction Cross-lingual model transfer approaches are concerned with creating statistical models for various tasks for languages poor in annotated resources, utilising resources or models available for these tasks in other languages. That includes approaches such as direct model transfer (Zeman and Resnik, 2008) and annotation projection (Yarowsky et al., 2001). Such methods have been successfully applied to a variety of tasks, including POS tagging (Xi and Hwa, 2005; Das and Petrov, 2011; T¨ackstr¨om et al., 2013), syntactic parsing (Ganchev et al., 2009; Smith and Eisner, 2009; Hwa et al., 2005; Durrett et al., 2012; Søgaard, 2011), semantic role labeling (Pad´o and Lapata, 2009; Annesi and Basili, 2010; Tonelli and Pianta, 2008; Kozhevnikov and Titov, 2013) and others. Direct model transfer attempts to find a shared feature representation for samples from the two languages, usually generalizing and abstracting away from language-specific representations. 579 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 579–585"
P14-2095,2005.mtsummit-papers.11,0,0.00751901,"the manually corrected dataset from van der Plas et al. (2011) for French. Since the size of the latter dataset is relatively small – one thousand sentences – we reserve the whole dataset for testing and only evaluate transfer from English to French, but not the other way around. Datasets for other languages are sufficiently large, so we take 30 thousand samples for testing and use the rest as training data. The validation set in each experiment is withheld from the corresponding training corpus and contains 10 thousand samples. Parallel data for both language pairs is derived from Europarl (Koehn, 2005), which we preprocess using MATE - TOOLS (Bj¨orkelund et al., 2009; Bohnet, 2010). 3 FRP AP DT 2 5 10 20 50 Number of parallel instances, ×103 Figure 2: English-Czech transfer results Error FRP AP DT 0.40 0.38 0.36 0.34 0.32 2 5 10 20 50 Number of parallel instances, ×103 Figure 3: Czech-English transfer results Results The classification error of FRP and the baselines given varying amount of parallel data is reported in figures 2, 3 and 4. The training set for each language is fixed. We denote the two baselines AP (annotation projection) and DT (direct transfer). The number of parallel instan"
P14-2095,D12-1001,0,0.0146938,"tion Cross-lingual model transfer approaches are concerned with creating statistical models for various tasks for languages poor in annotated resources, utilising resources or models available for these tasks in other languages. That includes approaches such as direct model transfer (Zeman and Resnik, 2008) and annotation projection (Yarowsky et al., 2001). Such methods have been successfully applied to a variety of tasks, including POS tagging (Xi and Hwa, 2005; Das and Petrov, 2011; T¨ackstr¨om et al., 2013), syntactic parsing (Ganchev et al., 2009; Smith and Eisner, 2009; Hwa et al., 2005; Durrett et al., 2012; Søgaard, 2011), semantic role labeling (Pad´o and Lapata, 2009; Annesi and Basili, 2010; Tonelli and Pianta, 2008; Kozhevnikov and Titov, 2013) and others. Direct model transfer attempts to find a shared feature representation for samples from the two languages, usually generalizing and abstracting away from language-specific representations. 579 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 579–585, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Figure 1: Dependency-based semantic ro"
P14-2095,P13-1117,1,0.88985,"ated resources, utilising resources or models available for these tasks in other languages. That includes approaches such as direct model transfer (Zeman and Resnik, 2008) and annotation projection (Yarowsky et al., 2001). Such methods have been successfully applied to a variety of tasks, including POS tagging (Xi and Hwa, 2005; Das and Petrov, 2011; T¨ackstr¨om et al., 2013), syntactic parsing (Ganchev et al., 2009; Smith and Eisner, 2009; Hwa et al., 2005; Durrett et al., 2012; Søgaard, 2011), semantic role labeling (Pad´o and Lapata, 2009; Annesi and Basili, 2010; Tonelli and Pianta, 2008; Kozhevnikov and Titov, 2013) and others. Direct model transfer attempts to find a shared feature representation for samples from the two languages, usually generalizing and abstracting away from language-specific representations. 579 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 579–585, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Figure 1: Dependency-based semantic role labeling example. The top arcs depict dependency relations, the bottom ones – semantic role structure. Rendered with https://code.google.com/p"
P14-2095,D11-1006,0,0.0409738,"s of words in two different languages and build a word translation model. 5 Conclusions In this paper we propose a new method of crosslingual model transfer, report initial evaluation results and highlight directions for its further development. We observe that the performance of this method is competitive with that of established crosslingual transfer approaches and its application requires very little manual adjustment – no heuristics or filtering and no explicit shared feature representation design. It also retains compatibility with any refinement procedures similar to projected transfer (McDonald et al., 2011) that may have been designed to work in conjunction with direct model transfer. 6 Multi-source Transfer One of the strong points of direct model transfer is that it naturally fits the multi-source transfer setting. There are several possible ways of adapting FRP to such a setting. It remains to be seen which one would produce the best results and how multi-source feature representation projection would compare to, for example, multi-source projected transfer (McDonald et al., 2011). Acknowledgements The authors would like to acknowledge the support of MMCI Cluster of Excellence and Saarbr¨ucke"
P14-2095,P09-1042,0,0.0524422,"ersity of Amsterdam Amsterdam, Netherlands titov@uva.nl Introduction Cross-lingual model transfer approaches are concerned with creating statistical models for various tasks for languages poor in annotated resources, utilising resources or models available for these tasks in other languages. That includes approaches such as direct model transfer (Zeman and Resnik, 2008) and annotation projection (Yarowsky et al., 2001). Such methods have been successfully applied to a variety of tasks, including POS tagging (Xi and Hwa, 2005; Das and Petrov, 2011; T¨ackstr¨om et al., 2013), syntactic parsing (Ganchev et al., 2009; Smith and Eisner, 2009; Hwa et al., 2005; Durrett et al., 2012; Søgaard, 2011), semantic role labeling (Pad´o and Lapata, 2009; Annesi and Basili, 2010; Tonelli and Pianta, 2008; Kozhevnikov and Titov, 2013) and others. Direct model transfer attempts to find a shared feature representation for samples from the two languages, usually generalizing and abstracting away from language-specific representations. 579 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 579–585, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for"
P14-2095,P13-2017,0,0.0254122,"Missing"
P14-2095,P11-1007,1,0.850171,"n about the mapping between certain elements of syntactic structure, for example a known part-of-speech tag mapping. The available parallel data itself may also be used more comprehensively – aligned arguments of aligned predicates, for example, constitute only a small part of it, while the mapping of vector representations of individual tokens is likely to be the same for all aligned words. Additional Related Work Apart from the work on direct/projected transfer and annotation projection mentioned above, the proposed method can be seen as a more explicit kind of domain adaptation, similar to Titov (2011) or Blitzer et al. (2006). It is also somewhat similar in spirit to Mikolov et al. (2013b), where a small number of word translation pairs are used to estimate a mapping between distributed representations of words in two different languages and build a word translation model. 5 Conclusions In this paper we propose a new method of crosslingual model transfer, report initial evaluation results and highlight directions for its further development. We observe that the performance of this method is competitive with that of established crosslingual transfer approaches and its application requires v"
P14-2095,tonelli-pianta-2008-frame,0,0.0623483,"or languages poor in annotated resources, utilising resources or models available for these tasks in other languages. That includes approaches such as direct model transfer (Zeman and Resnik, 2008) and annotation projection (Yarowsky et al., 2001). Such methods have been successfully applied to a variety of tasks, including POS tagging (Xi and Hwa, 2005; Das and Petrov, 2011; T¨ackstr¨om et al., 2013), syntactic parsing (Ganchev et al., 2009; Smith and Eisner, 2009; Hwa et al., 2005; Durrett et al., 2012; Søgaard, 2011), semantic role labeling (Pad´o and Lapata, 2009; Annesi and Basili, 2010; Tonelli and Pianta, 2008; Kozhevnikov and Titov, 2013) and others. Direct model transfer attempts to find a shared feature representation for samples from the two languages, usually generalizing and abstracting away from language-specific representations. 579 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 579–585, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Figure 1: Dependency-based semantic role labeling example. The top arcs depict dependency relations, the bottom ones – semantic role structure. Rendered"
P14-2095,P13-2103,0,0.0315322,"onald et al., 2011). The shared feature representation depends on the task in question, but most often each aspect of the original feature representation is handled separately. Word types, for example, may be replaced by cross-lingual word clusters (T¨ackstr¨om et al., 2012) or cross-lingual distributed word representations (Klementiev et al., 2012). Part-ofspeech tags, which are often language-specific, can be converted into universal part-of-speech tags (Petrov et al., 2012) and morpho-syntactic information can also be represented in a unified way (Zeman et al., 2012; McDonald et al., 2013; Tsarfaty, 2013). Unfortunately, the design of such representations and corresponding conversion procedures is by no means trivial. Annotation projection, on the other hand, does not require any changes to the feature representation. Instead, it operates on translation pairs, usually on sentence level, applying the available source-side model to the source sentence and transferring the resulting annotations through the word alignment links to the target one. The quality of predictions on source sentences depends heavily on the quality of parallel data and the domain it belongs to (or, rather, the similarity b"
P14-2095,P06-1146,0,0.0633505,"Missing"
P14-2095,P11-2052,0,0.0631739,"Missing"
P14-2095,H05-1107,0,0.0840017,"tion and suggests interesting directions for further research. 1 Ivan Titov ILLC, University of Amsterdam Amsterdam, Netherlands titov@uva.nl Introduction Cross-lingual model transfer approaches are concerned with creating statistical models for various tasks for languages poor in annotated resources, utilising resources or models available for these tasks in other languages. That includes approaches such as direct model transfer (Zeman and Resnik, 2008) and annotation projection (Yarowsky et al., 2001). Such methods have been successfully applied to a variety of tasks, including POS tagging (Xi and Hwa, 2005; Das and Petrov, 2011; T¨ackstr¨om et al., 2013), syntactic parsing (Ganchev et al., 2009; Smith and Eisner, 2009; Hwa et al., 2005; Durrett et al., 2012; Søgaard, 2011), semantic role labeling (Pad´o and Lapata, 2009; Annesi and Basili, 2010; Tonelli and Pianta, 2008; Kozhevnikov and Titov, 2013) and others. Direct model transfer attempts to find a shared feature representation for samples from the two languages, usually generalizing and abstracting away from language-specific representations. 579 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short"
P14-2095,petrov-etal-2012-universal,0,0.208486,"enforce model agreement on this data to adjust for discrepancies between the two languages, for example by means of projected transfer (McDonald et al., 2011). The shared feature representation depends on the task in question, but most often each aspect of the original feature representation is handled separately. Word types, for example, may be replaced by cross-lingual word clusters (T¨ackstr¨om et al., 2012) or cross-lingual distributed word representations (Klementiev et al., 2012). Part-ofspeech tags, which are often language-specific, can be converted into universal part-of-speech tags (Petrov et al., 2012) and morpho-syntactic information can also be represented in a unified way (Zeman et al., 2012; McDonald et al., 2013; Tsarfaty, 2013). Unfortunately, the design of such representations and corresponding conversion procedures is by no means trivial. Annotation projection, on the other hand, does not require any changes to the feature representation. Instead, it operates on translation pairs, usually on sentence level, applying the available source-side model to the source sentence and transferring the resulting annotations through the word alignment links to the target one. The quality of pred"
P14-2095,J08-2005,0,0.0494351,"nd their arguments in sentences and assigning each argument a semantic role with respect to its predicate (see figure 1). Note that only a single word – the syntactic head of the argument phrase – is marked as an argument in this case, as opposed to constituent- or span-based SRL (Carreras and M`arquez, 2005). We focus on the assignment of semantic roles to identified arguments. For the sake of simplicity we cast it as a multiclass classification problem, ignoring the interaction between different arguments in a predicate. It is well known that such interaction plays an important part in SRL (Punyakanok et al., 2008), but it is not well understood which kinds of interactions are preserved across languages and which are not. Also, should one like to apply constraints on the set of semantic roles in a given predicate, or, for example, use a reranker (Bj¨orkelund et al., 2009), this can be done using a factorized model obtained by cross-lingual transfer. In our setting, each instance includes the word type and part-of-speech and morphological tags (if any) of argument token, its parent and corresponding predicate token, as well as their dependency relations to their respective parents. This representation is"
P14-2095,H01-1035,0,0.135724,"formance on model transfer for semantic role labeling when compared to direct model transfer and annotation projection and suggests interesting directions for further research. 1 Ivan Titov ILLC, University of Amsterdam Amsterdam, Netherlands titov@uva.nl Introduction Cross-lingual model transfer approaches are concerned with creating statistical models for various tasks for languages poor in annotated resources, utilising resources or models available for these tasks in other languages. That includes approaches such as direct model transfer (Zeman and Resnik, 2008) and annotation projection (Yarowsky et al., 2001). Such methods have been successfully applied to a variety of tasks, including POS tagging (Xi and Hwa, 2005; Das and Petrov, 2011; T¨ackstr¨om et al., 2013), syntactic parsing (Ganchev et al., 2009; Smith and Eisner, 2009; Hwa et al., 2005; Durrett et al., 2012; Søgaard, 2011), semantic role labeling (Pad´o and Lapata, 2009; Annesi and Basili, 2010; Tonelli and Pianta, 2008; Kozhevnikov and Titov, 2013) and others. Direct model transfer attempts to find a shared feature representation for samples from the two languages, usually generalizing and abstracting away from language-specific represen"
P14-2095,D09-1086,0,0.039876,"sterdam, Netherlands titov@uva.nl Introduction Cross-lingual model transfer approaches are concerned with creating statistical models for various tasks for languages poor in annotated resources, utilising resources or models available for these tasks in other languages. That includes approaches such as direct model transfer (Zeman and Resnik, 2008) and annotation projection (Yarowsky et al., 2001). Such methods have been successfully applied to a variety of tasks, including POS tagging (Xi and Hwa, 2005; Das and Petrov, 2011; T¨ackstr¨om et al., 2013), syntactic parsing (Ganchev et al., 2009; Smith and Eisner, 2009; Hwa et al., 2005; Durrett et al., 2012; Søgaard, 2011), semantic role labeling (Pad´o and Lapata, 2009; Annesi and Basili, 2010; Tonelli and Pianta, 2008; Kozhevnikov and Titov, 2013) and others. Direct model transfer attempts to find a shared feature representation for samples from the two languages, usually generalizing and abstracting away from language-specific representations. 579 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 579–585, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistic"
P14-2095,I08-3008,0,0.442313,"-side model. This approach displays competitive performance on model transfer for semantic role labeling when compared to direct model transfer and annotation projection and suggests interesting directions for further research. 1 Ivan Titov ILLC, University of Amsterdam Amsterdam, Netherlands titov@uva.nl Introduction Cross-lingual model transfer approaches are concerned with creating statistical models for various tasks for languages poor in annotated resources, utilising resources or models available for these tasks in other languages. That includes approaches such as direct model transfer (Zeman and Resnik, 2008) and annotation projection (Yarowsky et al., 2001). Such methods have been successfully applied to a variety of tasks, including POS tagging (Xi and Hwa, 2005; Das and Petrov, 2011; T¨ackstr¨om et al., 2013), syntactic parsing (Ganchev et al., 2009; Smith and Eisner, 2009; Hwa et al., 2005; Durrett et al., 2012; Søgaard, 2011), semantic role labeling (Pad´o and Lapata, 2009; Annesi and Basili, 2010; Tonelli and Pianta, 2008; Kozhevnikov and Titov, 2013) and others. Direct model transfer attempts to find a shared feature representation for samples from the two languages, usually generalizing an"
P14-2095,P11-2120,0,0.0346754,"el transfer approaches are concerned with creating statistical models for various tasks for languages poor in annotated resources, utilising resources or models available for these tasks in other languages. That includes approaches such as direct model transfer (Zeman and Resnik, 2008) and annotation projection (Yarowsky et al., 2001). Such methods have been successfully applied to a variety of tasks, including POS tagging (Xi and Hwa, 2005; Das and Petrov, 2011; T¨ackstr¨om et al., 2013), syntactic parsing (Ganchev et al., 2009; Smith and Eisner, 2009; Hwa et al., 2005; Durrett et al., 2012; Søgaard, 2011), semantic role labeling (Pad´o and Lapata, 2009; Annesi and Basili, 2010; Tonelli and Pianta, 2008; Kozhevnikov and Titov, 2013) and others. Direct model transfer attempts to find a shared feature representation for samples from the two languages, usually generalizing and abstracting away from language-specific representations. 579 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 579–585, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Figure 1: Dependency-based semantic role labeling exam"
P14-2095,zeman-etal-2012-hamledt,0,0.0282962,"Missing"
P14-2095,N12-1052,0,0.0720701,"Missing"
P14-2095,W09-1201,0,\N,Missing
P14-2095,Q13-1001,0,\N,Missing
P18-1037,P13-2009,0,0.0281691,"cation and only used at pre-processing. Treating alignment as discrete variables has been successful in some sequence transduction tasks with neural models (Yu et al., 2017, 2016). Our work is similar in that we also train discrete alignments jointly but the tasks, the inference framework and the decoders are very different. Acknowledgments The discrete alignment modeling framework has been developed in the context of traditional (i.e. non-neural) statistical machine translation (Brown et al., 1993). Such translation models have also been successfully applied to semantic parsing tasks (e.g., (Andreas et al., 2013)), where they rivaled specialized semantic parsers from that period. However, they are considerably less accurate than current state-of-the-art parsers applied to the same datasets (e.g., (Dong and Lapata, 2016)). We thank Marco Damonte, Shay Cohen, Diego Marcheggiani and Wilker Aziz for helpful discussions as well as anonymous reviewers for their suggestions. The project was supported by the European Research Council (ERC StG BroadSem 678254) and the Dutch National Science Foundation (NWO VIDI 639.022.518). References For AMR parsing, another way to avoid using pre-trained aligners is to use"
P18-1037,D15-1198,0,0.254837,"Missing"
P18-1037,W13-2322,0,0.759949,"Missing"
P18-1037,C12-1083,0,0.0869055,"ptimal for this task. Introduction Abstract meaning representations (AMRs) (Banarescu et al., 2013) are broad-coverage sentencelevel semantic representations. AMR encodes, among others, information about semantic relations, named entities, co-reference, negation and modality. The semantic representations can be regarded as rooted labeled directed acyclic graphs (see Figure 1). As AMR abstracts away from details of surface realization, it is potentially beneficial in many semantic related NLP tasks, including text summarization (Liu et al., 2015; Dohare and Karnick, 2017), machine translation (Jones et al., 2012) and question answering (Mitra and Baral, 2016). In this work, we demonstrate that the alignments can be treated as latent variables in a joint probabilistic model and induced in such a way as to be beneficial for AMR parsing. Intuitively, in our probabilistic model, every node in a graph is assumed to be aligned to a word in a sentence: each concept is predicted based on the corresponding RNN state. Similarly, graph edges (i.e. relations) are predicted based on representations of concepts and aligned words (see Figure 2). As alignments are latent, exact inference requires marginalizing over l"
P18-1037,J93-2003,0,0.123413,"al., 2015; Wang and Xue, 2017). However, those aligners are trained independently of concept and relation identification and only used at pre-processing. Treating alignment as discrete variables has been successful in some sequence transduction tasks with neural models (Yu et al., 2017, 2016). Our work is similar in that we also train discrete alignments jointly but the tasks, the inference framework and the decoders are very different. Acknowledgments The discrete alignment modeling framework has been developed in the context of traditional (i.e. non-neural) statistical machine translation (Brown et al., 1993). Such translation models have also been successfully applied to semantic parsing tasks (e.g., (Andreas et al., 2013)), where they rivaled specialized semantic parsers from that period. However, they are considerably less accurate than current state-of-the-art parsers applied to the same datasets (e.g., (Dong and Lapata, 2016)). We thank Marco Damonte, Shay Cohen, Diego Marcheggiani and Wilker Aziz for helpful discussions as well as anonymous reviewers for their suggestions. The project was supported by the European Research Council (ERC StG BroadSem 678254) and the Dutch National Science Foun"
P18-1037,S17-2157,0,0.248696,"Missing"
P18-1037,E17-1051,0,0.347758,"gure 1: An example of AMR, the dashed lines denote latent alignments, obligate-01 is the root. Numbers indicate depth-first traversal order. AMR parsing has recently received a lot of attention (e.g., (Flanigan et al., 2014; Artzi et al., 2015; Konstas et al., 2017)). One distinctive aspect of AMR annotation is the lack of explicit alignments between nodes in the graph (concepts) and words in the sentences. Though this arguably simplified the annotation process (Banarescu et al., 2013), it is not straightforward to produce an effective parser without relying on an alignment. Most AMR parsers (Damonte et al., 2017; Flanigan et al., 2016; Werling et al., 2015; Wang and Xue, 2017; Foland and Martin, 2017) use a pipeline where the aligner training stage precedes training a parser. The aligners are not directly informed by the AMR parsing objective and may produce alignments suboptimal for this task. Introduction Abstract meaning representations (AMRs) (Banarescu et al., 2013) are broad-coverage sentencelevel semantic representations. AMR encodes, among others, information about semantic relations, named entities, co-reference, negation and modality. The semantic representations can be regarded as rooted l"
P18-1037,P17-1014,0,0.583487,"Relation identification: predicting a relation between boy and go-02 relying on the two concepts and corresponding RNN states. that each word corresponds to at most one concept (if any). This encourages competition: alignments are mutually-repulsive. In our example, obligate is not lexically similar to the word must and may be hard to align. However, given that other concepts are easy to predict, alignment candidates other than must and the will be immediately ruled out. We believe that these are the key reasons for why attention-based neural models do not achieve competitive results on AMR (Konstas et al., 2017) and why state-of-the-art models rely on aligners. Our goal is to combine best of two worlds: to use alignments (as in state-of-the-art AMR methods) and to induce them while optimizing for the end goal (similarly to the attention component of encoder-decoder models). Our model consists of three parts: (1) the concept identification model Pθ (c|a, w); (2) the relation identification model Pφ (R|a, w, c) and (3) the alignment model Qψ (a|c, R, w).4 Formally, (1) and (2) together with the uniform prior over alignments P (a) form the generative model of AMR graphs. In contrast, the alignment model"
P18-1037,P16-1004,0,0.0352587,"e also train discrete alignments jointly but the tasks, the inference framework and the decoders are very different. Acknowledgments The discrete alignment modeling framework has been developed in the context of traditional (i.e. non-neural) statistical machine translation (Brown et al., 1993). Such translation models have also been successfully applied to semantic parsing tasks (e.g., (Andreas et al., 2013)), where they rivaled specialized semantic parsers from that period. However, they are considerably less accurate than current state-of-the-art parsers applied to the same datasets (e.g., (Dong and Lapata, 2016)). We thank Marco Damonte, Shay Cohen, Diego Marcheggiani and Wilker Aziz for helpful discussions as well as anonymous reviewers for their suggestions. The project was supported by the European Research Council (ERC StG BroadSem 678254) and the Dutch National Science Foundation (NWO VIDI 639.022.518). References For AMR parsing, another way to avoid using pre-trained aligners is to use seq2seq models (Konstas et al., 2017; van Noord and Bos, 2017). In particular, van Noord and Bos (2017) used character level seq2seq model and achieved the previous state-of-the-art result. However, their model"
P18-1037,N15-1114,0,0.109237,"ormed by the AMR parsing objective and may produce alignments suboptimal for this task. Introduction Abstract meaning representations (AMRs) (Banarescu et al., 2013) are broad-coverage sentencelevel semantic representations. AMR encodes, among others, information about semantic relations, named entities, co-reference, negation and modality. The semantic representations can be regarded as rooted labeled directed acyclic graphs (see Figure 1). As AMR abstracts away from details of surface realization, it is potentially beneficial in many semantic related NLP tasks, including text summarization (Liu et al., 2015; Dohare and Karnick, 2017), machine translation (Jones et al., 2012) and question answering (Mitra and Baral, 2016). In this work, we demonstrate that the alignments can be treated as latent variables in a joint probabilistic model and induced in such a way as to be beneficial for AMR parsing. Intuitively, in our probabilistic model, every node in a graph is assumed to be aligned to a word in a sentence: each concept is predicted based on the corresponding RNN state. Similarly, graph edges (i.e. relations) are predicted based on representations of concepts and aligned words (see Figure 2). As"
P18-1037,W02-0109,0,0.171285,"Missing"
P18-1037,S16-1186,0,0.284987,"AMR, the dashed lines denote latent alignments, obligate-01 is the root. Numbers indicate depth-first traversal order. AMR parsing has recently received a lot of attention (e.g., (Flanigan et al., 2014; Artzi et al., 2015; Konstas et al., 2017)). One distinctive aspect of AMR annotation is the lack of explicit alignments between nodes in the graph (concepts) and words in the sentences. Though this arguably simplified the annotation process (Banarescu et al., 2013), it is not straightforward to produce an effective parser without relying on an alignment. Most AMR parsers (Damonte et al., 2017; Flanigan et al., 2016; Werling et al., 2015; Wang and Xue, 2017; Foland and Martin, 2017) use a pipeline where the aligner training stage precedes training a parser. The aligners are not directly informed by the AMR parsing objective and may produce alignments suboptimal for this task. Introduction Abstract meaning representations (AMRs) (Banarescu et al., 2013) are broad-coverage sentencelevel semantic representations. AMR encodes, among others, information about semantic relations, named entities, co-reference, negation and modality. The semantic representations can be regarded as rooted labeled directed acyclic"
P18-1037,P15-1002,0,0.0335064,"d node) conditioned on the aligned word k or decides that no concept should be introduced (i.e. returns NULL). Though it can be modeled with a softmax classifier, it would not be effective in handling rare or unseen words. First, we split the decision into estimating the probability of concept category τ (c) ∈ T (e.g. ‘number’, ’frame’) and estimating the probability of the specific concept within the chosen category. Second, based on a lemmatizer and training data5 we prepare one candidate concept ek for each word k in vocabulary (e.g., it would propose want if the word is wants). Similar to Luong et al. (2015), our model can then either copy the candidate ek or rely on the softmax over potential concepts of category τ . Formally, the concept prediction model is defined as Pθ (c|hk , wk ) = P (τ (c)|hk , wk )× T h ) + exp(vT h ) [[ek = c]] × exp(vcopy k c k , Z(hk , θ) where the first multiplicative term is a softmax classifier over categories (including NULL); vcopy , vc ∈ Rd (for c ∈ C) are model parameters; [[. . .]] denotes the indicator function and equals 1 if its argument is true and 0, otherwise; Z(h, θ) is the partition function ensuring that the scores sum to 1. 2.4 2.5 Recall that the ali"
P18-1037,P14-1134,0,0.297519,"at most one argument for each relation r ∈ R; (3) the graph should be connected. Constraint (1) is addressed by keeping only the highest scoring neighbor. In order to satisfy the last two constraints we use a simple greedy procedure. First, for each edge, we pick-up the highest scoring relation and edge (possibly NULL). If the constraint (2) is violated, we simply keep the highest scoring edge among the duplicates and drop the rest. If the graph is not connected (i.e. constraint (3) is violated), we greedily choose edges linking the connected components until the graph gets connected (MSCG in Flanigan et al. (2014)). As we will show in our experiments, a softer version of the loss is even more effective: logPθ (ci |ˆ ai , w) n X ≈ log (ˆ aik Pθ (ci |ai = k, w))α , Such ‘primary’ concepts get encoded in the category of the concept (the set of categories is τ , see also section 2.3). In Figure 3, the re-categorized concept thing(opinion) is produced from thing and opine-01. We use concept as the dummy category type. There are 8 templates in our system which extract re-categorizations for fixed phrases (e.g. thing(opinion)), and a deterministic system for grouping lexically flexible, but structurally stabl"
P18-1037,P17-1043,0,0.26472,"e root. Numbers indicate depth-first traversal order. AMR parsing has recently received a lot of attention (e.g., (Flanigan et al., 2014; Artzi et al., 2015; Konstas et al., 2017)). One distinctive aspect of AMR annotation is the lack of explicit alignments between nodes in the graph (concepts) and words in the sentences. Though this arguably simplified the annotation process (Banarescu et al., 2013), it is not straightforward to produce an effective parser without relying on an alignment. Most AMR parsers (Damonte et al., 2017; Flanigan et al., 2016; Werling et al., 2015; Wang and Xue, 2017; Foland and Martin, 2017) use a pipeline where the aligner training stage precedes training a parser. The aligners are not directly informed by the AMR parsing objective and may produce alignments suboptimal for this task. Introduction Abstract meaning representations (AMRs) (Banarescu et al., 2013) are broad-coverage sentencelevel semantic representations. AMR encodes, among others, information about semantic relations, named entities, co-reference, negation and modality. The semantic representations can be regarded as rooted labeled directed acyclic graphs (see Figure 1). As AMR abstracts away from details of surfac"
P18-1037,P14-5010,0,0.01069,"Missing"
P18-1037,D14-1048,0,0.245577,"Missing"
P18-1037,K17-1041,1,0.833933,"sistent AMR graphs, so instead we search for the highest scoring valid graph (see Section 3.2). Note that the alignment model Qψ is not used at test time and only necessary to train accurate concept and relation identification models. 2.3 Concept identification model In the above discussion, we assumed that BiLSTM encodes a sentence once and the BiLSTM states are then used to predict concepts and relations. In semantic role labeling, the task closely related to the relation identification stage of AMR parsing, a slight modification of this approach was shown more effective (Zhou and Xu, 2015; Marcheggiani et al., 2017). In that previous work, the sentence was encoded by a BiLSTM once per each predicate (i.e. verb) and the encoding was in turn used to identify arguments of that predicate. The only difference across the re-encoding passes was a binary flag used as input to the BiLSTM encoder at each word position. The flag was set to 1 for the word corresponding to the predicate and to 0 for all other words. In that way, BiLSTM was encoding the sentence specifically for predicting arguments of a given predicate. Inspired by this approach, when predicting label rij for j ∈ {1, . . . m}, we input binary flags p"
P18-1037,D17-1159,1,0.841641,"is indeed beneficial. We believe that the proposed approach may be extended to other parsing tasks where alignments are latent (e.g., parsing to logical form (Liang, 2016)). Another promising direction is integrating character seq2seq to substitute the copy function. This should also improve the handling of negation and rare words. Though our parsing model does not use any linearization of the graph, we relied on LSTMs and somewhat arbitrary linearization (depth-first traversal) to encode the AMR graph in our alignment model. A better alternative would be to use graph convolutional networks (Marcheggiani and Titov, 2017; Kipf and Welling, 2017): neighborhoods in the graph are likely to be more informative for predicting alignments than the neighborhoods in the graph traversal. Additional Related Work Alignment performance has been previously identified as a potential bottleneck affecting AMR parsing (Damonte et al., 2017; Foland and Martin, 2017). Some recent work has focused on building aligners specifically for training their parsers (Werling et al., 2015; Wang and Xue, 2017). However, those aligners are trained independently of concept and relation identification and only used at pre-processing. Treating"
P18-1037,S16-1181,0,0.433359,"Missing"
P18-1037,D17-1129,0,0.633763,"s, obligate-01 is the root. Numbers indicate depth-first traversal order. AMR parsing has recently received a lot of attention (e.g., (Flanigan et al., 2014; Artzi et al., 2015; Konstas et al., 2017)). One distinctive aspect of AMR annotation is the lack of explicit alignments between nodes in the graph (concepts) and words in the sentences. Though this arguably simplified the annotation process (Banarescu et al., 2013), it is not straightforward to produce an effective parser without relying on an alignment. Most AMR parsers (Damonte et al., 2017; Flanigan et al., 2016; Werling et al., 2015; Wang and Xue, 2017; Foland and Martin, 2017) use a pipeline where the aligner training stage precedes training a parser. The aligners are not directly informed by the AMR parsing objective and may produce alignments suboptimal for this task. Introduction Abstract meaning representations (AMRs) (Banarescu et al., 2013) are broad-coverage sentencelevel semantic representations. AMR encodes, among others, information about semantic relations, named entities, co-reference, negation and modality. The semantic representations can be regarded as rooted labeled directed acyclic graphs (see Figure 1). As AMR abstracts a"
P18-1037,P15-1095,0,0.245685,"Missing"
P18-1037,D16-1138,0,0.0837929,"Missing"
P18-1037,P15-1109,0,0.0180529,"to generating inconsistent AMR graphs, so instead we search for the highest scoring valid graph (see Section 3.2). Note that the alignment model Qψ is not used at test time and only necessary to train accurate concept and relation identification models. 2.3 Concept identification model In the above discussion, we assumed that BiLSTM encodes a sentence once and the BiLSTM states are then used to predict concepts and relations. In semantic role labeling, the task closely related to the relation identification stage of AMR parsing, a slight modification of this approach was shown more effective (Zhou and Xu, 2015; Marcheggiani et al., 2017). In that previous work, the sentence was encoded by a BiLSTM once per each predicate (i.e. verb) and the encoding was in turn used to identify arguments of that predicate. The only difference across the re-encoding passes was a binary flag used as input to the BiLSTM encoder at each word position. The flag was set to 1 for the word corresponding to the predicate and to 0 for all other words. In that way, BiLSTM was encoding the sentence specifically for predicting arguments of a given predicate. Inspired by this approach, when predicting label rij for j ∈ {1, . . ."
P18-1037,E17-1035,0,0.103362,"thing and opine-01. We use concept as the dummy category type. There are 8 templates in our system which extract re-categorizations for fixed phrases (e.g. thing(opinion)), and a deterministic system for grouping lexically flexible, but structurally stable sub-graphs (e.g., named entities, have-rel-role91 and have-org-role-91 concepts). Pre- and post-pocessing Re-Categorization AMR parsers often rely on a pre-processing stage, where specific subgraphs of AMR are grouped together and assigned to a single node with a new compound category (e.g., Werling et al. (2015); Foland and Martin (2017); Peng et al. (2017)); this transformation is reversed at the post-processing stage. Our approach is very similar to the Factored Concept Label system of Wang and Xue (2017), with one important difference that we unpack our concepts before the relation identification stage, so the relations are predicted between original concepts (all nodes in each group share the same alignment distributions to the RNN states). Intuitively, the goal is to ensure that concepts rarely lexically triggered (e.g., thing in Figure 3) get grouped together with lexically triggered nodes. Finally, we need to select a root node. Similarly"
P18-1117,N18-1118,1,0.741682,"reasing lexical consistency (Carpuat, 2009; Tiedemann, 2010; Gong et al., 2011), or topic adaptation (Su et al., 2012; Hasler et al., 2014), with special-purpose features engineered to model these phenomena. However, with traditional statistical machine translation being largely supplanted with neural machine translation (NMT) models trained in an end-toend fashion, an alternative is to directly provide additional context to an NMT system at training time and hope that it will succeed in inducing relevant predictive features (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018). While the latter approach, using context-aware NMT models, has demonstrated to yield performance improvements, it is still not clear what kinds of discourse phenomena are successfully handled by the NMT systems and, importantly, how they are modeled. Understanding this would inform development of future discourse-aware NMT models, as it will suggest what kind of inductive biases need to be encoded in the architecture or which linguistic features need to be exploited. In our work we aim to enhance our understanding of the modelling of selected discourse phenomena in NMT. To this end, we const"
P18-1117,W09-2404,0,0.13593,"99; Hardmeier, 2012). Using extended context, beyond the single source sentence, should in principle be beneficial in ambiguous cases and also ensure that generated translations are coherent. Nevertheless, machine translation systems typically ignore discourse phenomena and translate sentences in isolation. Earlier research on this topic focused on handling specific phenomena, such as translating pronouns (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Hardmeier et al., 2015), discourse connectives (Meyer et al., 2012), verb tense (Gong et al., 2012), increasing lexical consistency (Carpuat, 2009; Tiedemann, 2010; Gong et al., 2011), or topic adaptation (Su et al., 2012; Hasler et al., 2014), with special-purpose features engineered to model these phenomena. However, with traditional statistical machine translation being largely supplanted with neural machine translation (NMT) models trained in an end-toend fashion, an alternative is to directly provide additional context to an NMT system at training time and hope that it will succeed in inducing relevant predictive features (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018). While the latter app"
P18-1117,D12-1026,0,0.029012,"se phenomena is important in translation (Mitkov, 1999; Hardmeier, 2012). Using extended context, beyond the single source sentence, should in principle be beneficial in ambiguous cases and also ensure that generated translations are coherent. Nevertheless, machine translation systems typically ignore discourse phenomena and translate sentences in isolation. Earlier research on this topic focused on handling specific phenomena, such as translating pronouns (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Hardmeier et al., 2015), discourse connectives (Meyer et al., 2012), verb tense (Gong et al., 2012), increasing lexical consistency (Carpuat, 2009; Tiedemann, 2010; Gong et al., 2011), or topic adaptation (Su et al., 2012; Hasler et al., 2014), with special-purpose features engineered to model these phenomena. However, with traditional statistical machine translation being largely supplanted with neural machine translation (NMT) models trained in an end-toend fashion, an alternative is to directly provide additional context to an NMT system at training time and hope that it will succeed in inducing relevant predictive features (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2"
P18-1117,D11-1084,0,0.33727,"nded context, beyond the single source sentence, should in principle be beneficial in ambiguous cases and also ensure that generated translations are coherent. Nevertheless, machine translation systems typically ignore discourse phenomena and translate sentences in isolation. Earlier research on this topic focused on handling specific phenomena, such as translating pronouns (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Hardmeier et al., 2015), discourse connectives (Meyer et al., 2012), verb tense (Gong et al., 2012), increasing lexical consistency (Carpuat, 2009; Tiedemann, 2010; Gong et al., 2011), or topic adaptation (Su et al., 2012; Hasler et al., 2014), with special-purpose features engineered to model these phenomena. However, with traditional statistical machine translation being largely supplanted with neural machine translation (NMT) models trained in an end-toend fashion, an alternative is to directly provide additional context to an NMT system at training time and hope that it will succeed in inducing relevant predictive features (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018). While the latter approach, using context-aware NMT models"
P18-1117,2010.iwslt-papers.10,0,0.64861,"nation of the context and source sentences (+0.6). 1 Introduction It has long been argued that handling discourse phenomena is important in translation (Mitkov, 1999; Hardmeier, 2012). Using extended context, beyond the single source sentence, should in principle be beneficial in ambiguous cases and also ensure that generated translations are coherent. Nevertheless, machine translation systems typically ignore discourse phenomena and translate sentences in isolation. Earlier research on this topic focused on handling specific phenomena, such as translating pronouns (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Hardmeier et al., 2015), discourse connectives (Meyer et al., 2012), verb tense (Gong et al., 2012), increasing lexical consistency (Carpuat, 2009; Tiedemann, 2010; Gong et al., 2011), or topic adaptation (Su et al., 2012; Hasler et al., 2014), with special-purpose features engineered to model these phenomena. However, with traditional statistical machine translation being largely supplanted with neural machine translation (NMT) models trained in an end-toend fashion, an alternative is to directly provide additional context to an NMT system at training time and hope that it will succeed in i"
P18-1117,P14-5010,0,0.0941084,"are relatively sparse in a general-purpose test set, and previous work has designed targeted evaluation of pronoun translation (Hardmeier et al., 2015; Miculicich Werlen and Popescu-Belis, 2017; Bawden et al., 2018). However, we note that in Russian, grammatical gender is not only marked on pronouns, but also on adjectives and verbs. Rather than using a pronoun-specific evaluation, we present results with BLEU on test sets where we hypothesize context to be relevant, specifically sentences containing co-referential pronouns. We feed Stanford CoreNLP open-source coreference resolution system (Manning et al., 2014a) with pairs of sentences to find examples where there is a link between one of the pronouns under consideration and the context. We focus on anaphoric instances of “it” (this excludes, among others, pleonastic uses of ”it”), and instances of the pronouns “I”, “you”, and “yours” that are coreferent with an expression in the previous sentence. All these pronouns express ambiguity in the translation into Russian, and the model has learned to attend to context for their translation (Table 2). To combat data sparsity, the test sets are extracted from large amounts of held-out data of OpenSubtitle"
P18-1117,D13-1037,0,0.0258483,"cts anaphora while CoreNLP fails. Nevertheless, there is room for improvement, and improving the attention component is likely to boost translation performance. 6 Related work Our analysis focuses on how our context-aware neural model implicitly captures anaphora. Early work on anaphora phenomena in statistical machine translation has relied on external systems for coreference resolution (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010). Results 1271 were mixed, and the low performance of coreference resolution systems was identified as a problem for this type of system. Later work by Hardmeier et al. (2013) has shown that cross-lingual pronoun prediction systems can implicitly learn to resolve coreference, but this work still relied on external feature extraction to identify anaphora candidates. Our experiments show that a contextaware neural machine translation system can implicitly learn coreference phenomena without any feature engineering. Tiedemann and Scherrer (2017) and Bawden et al. (2018) analyze the attention weights of context-aware NMT models. Tiedemann and Scherrer (2017) find some evidence for aboveaverage attention on contextual history for the translation of pronouns, and our ana"
P18-1117,2012.amta-papers.20,0,0.0268581,"been argued that handling discourse phenomena is important in translation (Mitkov, 1999; Hardmeier, 2012). Using extended context, beyond the single source sentence, should in principle be beneficial in ambiguous cases and also ensure that generated translations are coherent. Nevertheless, machine translation systems typically ignore discourse phenomena and translate sentences in isolation. Earlier research on this topic focused on handling specific phenomena, such as translating pronouns (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Hardmeier et al., 2015), discourse connectives (Meyer et al., 2012), verb tense (Gong et al., 2012), increasing lexical consistency (Carpuat, 2009; Tiedemann, 2010; Gong et al., 2011), or topic adaptation (Su et al., 2012; Hasler et al., 2014), with special-purpose features engineered to model these phenomena. However, with traditional statistical machine translation being largely supplanted with neural machine translation (NMT) models trained in an end-toend fashion, an alternative is to directly provide additional context to an NMT system at training time and hope that it will succeed in inducing relevant predictive features (Jean et al., 2017; Wang et al.,"
P18-1117,E14-1035,0,0.0790868,"Missing"
P18-1117,D17-1195,0,0.0494454,"Missing"
P18-1117,W10-1737,0,0.304292,"Missing"
P18-1117,L18-1275,0,0.2103,"e phenomena are successfully handled by the NMT systems and, importantly, how they are modeled. Understanding this would inform development of future discourse-aware NMT models, as it will suggest what kind of inductive biases need to be encoded in the architecture or which linguistic features need to be exploited. In our work we aim to enhance our understanding of the modelling of selected discourse phenomena in NMT. To this end, we construct a simple discourse-aware model, demonstrate that it achieves improvements over the discourse-agnostic baseline on an English-Russian subtitles dataset (Lison et al., 2018) and study which context information is being captured in the model. Specifically, we start with the Trans1264 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1264–1274 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics former (Vaswani et al., 2017), a state-of-the-art model for context-agnostic NMT, and modify it in such way that it can handle additional context. In our model, a source sentence and a context sentence are first encoded independently, and then a single attention layer, in a comb"
P18-1117,W17-4802,0,0.148428,"short sentences, context does not seem disproportionally more useful for these sentences. 5.3 Analysis of pronoun translation The analysis of the attention model indicates that the model attends heavily to the contextual history for the translation of some pronouns. Here, we investigate whether this context-aware modelling results in empirical improvements in translation Ambiguous pronouns and translation quality Ambiguous pronouns are relatively sparse in a general-purpose test set, and previous work has designed targeted evaluation of pronoun translation (Hardmeier et al., 2015; Miculicich Werlen and Popescu-Belis, 2017; Bawden et al., 2018). However, we note that in Russian, grammatical gender is not only marked on pronouns, but also on adjectives and verbs. Rather than using a pronoun-specific evaluation, we present results with BLEU on test sets where we hypothesize context to be relevant, specifically sentences containing co-referential pronouns. We feed Stanford CoreNLP open-source coreference resolution system (Manning et al., 2014a) with pairs of sentences to find examples where there is a link between one of the pronouns under consideration and the context. We focus on anaphoric instances of “it” (th"
P18-1117,W05-0908,0,0.021397,"Missing"
P18-1117,P16-1162,1,0.553356,"nce) context encoder (next sentence) context encoder (random context) sentences, to let the shared layers know whether it is encoding a source or a context sentence. 4 Experiments 4.1 Data and setting We use the publicly available OpenSubtitles2018 corpus (Lison et al., 2018) for English and Russian.1 As described in the appendix, we apply data cleaning and randomly choose 2 million training instances from the resulting data. For development and testing, we randomly select two subsets of 10000 instances from movies not encountered in training.2 Sentences were encoded using byte-pair encoding (Sennrich et al., 2016), with source and target vocabularies of about 32000 tokens. We generally used the same parameters and optimizer as in the original Transformer (Vaswani et al., 2017). The hyperparameters, preprocessing and training details are provided in the supplementary material. 5 Results and analysis We start by experiments motivating the setting and verifying that the improvements are indeed genuine, i.e. they come from inducing predictive features of the context. In subsequent section 5.2, we analyze the features induced by the context encoder and perform error analysis. 5.1 Overall performance We use"
P18-1117,P12-1048,0,0.0435972,"ntence, should in principle be beneficial in ambiguous cases and also ensure that generated translations are coherent. Nevertheless, machine translation systems typically ignore discourse phenomena and translate sentences in isolation. Earlier research on this topic focused on handling specific phenomena, such as translating pronouns (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Hardmeier et al., 2015), discourse connectives (Meyer et al., 2012), verb tense (Gong et al., 2012), increasing lexical consistency (Carpuat, 2009; Tiedemann, 2010; Gong et al., 2011), or topic adaptation (Su et al., 2012; Hasler et al., 2014), with special-purpose features engineered to model these phenomena. However, with traditional statistical machine translation being largely supplanted with neural machine translation (NMT) models trained in an end-toend fashion, an alternative is to directly provide additional context to an NMT system at training time and hope that it will succeed in inducing relevant predictive features (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018). While the latter approach, using context-aware NMT models, has demonstrated to yield performanc"
P18-1117,W10-2602,0,0.126031,"2012). Using extended context, beyond the single source sentence, should in principle be beneficial in ambiguous cases and also ensure that generated translations are coherent. Nevertheless, machine translation systems typically ignore discourse phenomena and translate sentences in isolation. Earlier research on this topic focused on handling specific phenomena, such as translating pronouns (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Hardmeier et al., 2015), discourse connectives (Meyer et al., 2012), verb tense (Gong et al., 2012), increasing lexical consistency (Carpuat, 2009; Tiedemann, 2010; Gong et al., 2011), or topic adaptation (Su et al., 2012; Hasler et al., 2014), with special-purpose features engineered to model these phenomena. However, with traditional statistical machine translation being largely supplanted with neural machine translation (NMT) models trained in an end-toend fashion, an alternative is to directly provide additional context to an NMT system at training time and hope that it will succeed in inducing relevant predictive features (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018). While the latter approach, using cont"
P18-1117,W17-4811,0,0.50249,"tense (Gong et al., 2012), increasing lexical consistency (Carpuat, 2009; Tiedemann, 2010; Gong et al., 2011), or topic adaptation (Su et al., 2012; Hasler et al., 2014), with special-purpose features engineered to model these phenomena. However, with traditional statistical machine translation being largely supplanted with neural machine translation (NMT) models trained in an end-toend fashion, an alternative is to directly provide additional context to an NMT system at training time and hope that it will succeed in inducing relevant predictive features (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018). While the latter approach, using context-aware NMT models, has demonstrated to yield performance improvements, it is still not clear what kinds of discourse phenomena are successfully handled by the NMT systems and, importantly, how they are modeled. Understanding this would inform development of future discourse-aware NMT models, as it will suggest what kind of inductive biases need to be encoded in the architecture or which linguistic features need to be exploited. In our work we aim to enhance our understanding of the modelling of selected discourse phenomena in NMT."
P18-1117,D17-1301,0,0.115635,"t al., 2012), verb tense (Gong et al., 2012), increasing lexical consistency (Carpuat, 2009; Tiedemann, 2010; Gong et al., 2011), or topic adaptation (Su et al., 2012; Hasler et al., 2014), with special-purpose features engineered to model these phenomena. However, with traditional statistical machine translation being largely supplanted with neural machine translation (NMT) models trained in an end-toend fashion, an alternative is to directly provide additional context to an NMT system at training time and hope that it will succeed in inducing relevant predictive features (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018). While the latter approach, using context-aware NMT models, has demonstrated to yield performance improvements, it is still not clear what kinds of discourse phenomena are successfully handled by the NMT systems and, importantly, how they are modeled. Understanding this would inform development of future discourse-aware NMT models, as it will suggest what kind of inductive biases need to be encoded in the architecture or which linguistic features need to be exploited. In our work we aim to enhance our understanding of the modelling of select"
P18-1117,N16-1114,0,0.0253566,"ubstantial improvements in performance on these subsets. By comparing attention distributions induced by our model against co-reference links, we conclude that the model implicitly captures coreference phenomena, even without having any kind of specialized features which could help it in this subtask. These observations also suggest potential directions for future work. For example, effective co-reference systems go beyond relying simply on embeddings of contexts. One option would be to integrate ‘global’ features summarizing properties of groups of mentions predicted as linked in a document (Wiseman et al., 2016), or to use latent relations to trace en• we introduce a context-aware neural model, which is effective and has a sufficiently simple and interpretable interface between the context and the rest of the translation model; • we analyze the flow of information from the context and identify pronoun translation as the key phenomenon captured by the model; • by comparing to automatically predicted or human-annotated coreference relations, we observe that the model implicitly captures anaphora. Neural Machine Translation Given a source sentence x = (x1 , x2 , . . . , xS ) and a target sentence y = (y"
P18-1148,E06-1002,0,0.378301,"Missing"
P18-1148,D13-1184,0,0.538133,"1 are coreferent and thus should be assigned to the same entity. Besides coreference, there are many other relations between entities which constrain or favor certain alignment configurations. For example, consider relation participant in in Figure 1: if “World Cup” is aligned to the entity FIFA W ORLD C UP then we expect the second “England” to refer to a football team rather than a basketball one. NEL methods typically consider only coreference, relying either on off-the-shelf systems or some simple heuristics (Lazic et al., 2015), and exploit them in a pipeline fashion, though some (e.g., Cheng and Roth (2013); Ren et al. (2017)) additionally exploit a range of syntactic-semantic relations such as apposition and possessives. Another line of work ignores relations altogether and models the predicted sequence of KB entities as a bag (Globerson et al., 2016; Yamada et al., 2016; Ganea and Hofmann, 2017). Though they are able to capture some degree of coherence (e.g., preference towards entities from the same general domain) and are generally empirically successful, the underlying assumption is too coarse. For example, they would favor assigning all the occurrences of “England” in Figure 1 to the same"
P18-1148,Q15-1011,0,0.0675553,"oose potential candidates, eliminating options which are highly unlikely. This preprocessing step is called candidate selection. The task of a statistical model is thus reduced to choosing the best option among a smaller list of candidates Ci = (ei1 , ..., eili ). In what follows, we will discuss two classes of approaches tackling this problem: local and global modeling. (1) arg max n ∑ E∈C1 ×...×Cn i=1 Ψ(ei , ci ) + Φ(E, D) where E = (e1 , ..., en ). The coherence score function, in the simplest form, is a sum over all pairwise scores Φ(ei , ej , D) (Ratinov et al., 2011; Huang et al., 2015; Chisholm and Hachey, 2015; Ganea et al., 2016; Guo and Barbosa, 2016; Globerson et al., 2016; Yamada et al., 2016), resulting in: E∗ = arg max n ∑ E∈C1 ×...×Cn i=1 ∑ Ψ(ei , ci )+ Φ(ei , ej , D) (2) i=j A disadvantage of global models is that exact decoding (Equation 2) is NP-hard (Wainwright et al., 2008). Ganea and Hofmann (2017) overcome this using loopy belief propagation (LBP), 1596 an approximate inference method based on message passing (Murphy et al., 1999). Globerson et al. (2016) propose a star model which approximates the decoding problem in Equation 2 by approximately decomposing it into n decoding problem"
P18-1148,Q14-1037,0,0.0479319,"relations is in itself a challenging problem. In Cheng and Roth (2013), relations between mentions are extracted using a labor-intensive approach, requiring a set of hand-crafted rules and a KB containing relations between entities. This approach is difficult to generalize to languages and domains which do not have such KBs or the settings where no experts are available to design the rules. We, in contrast, focus on automating the process using representation learning. Most of these methods relied on relations predicted by external tools, usually a coreference system. One notable exception is Durrett and Klein (2014): they use a joint model of entity linking and coreference resolution. Nevertheless their coreference component is still supervised, whereas our relations are latent even at training time. Representation learning How can we define local score functions Ψ and pairwise score functions Φ? Previous approaches employ a wide spectrum of techniques. At one extreme, extensive feature engineering was used to define useful features. For example, Ratinov et al. (2011) use cosine similarities between Wikipedia titles and local contexts as a feature when computing the local scores. For pairwise scores they"
P18-1148,D17-1277,0,0.702506,"Missing"
P18-1148,P16-1059,0,0.782395,"e 1: if “World Cup” is aligned to the entity FIFA W ORLD C UP then we expect the second “England” to refer to a football team rather than a basketball one. NEL methods typically consider only coreference, relying either on off-the-shelf systems or some simple heuristics (Lazic et al., 2015), and exploit them in a pipeline fashion, though some (e.g., Cheng and Roth (2013); Ren et al. (2017)) additionally exploit a range of syntactic-semantic relations such as apposition and possessives. Another line of work ignores relations altogether and models the predicted sequence of KB entities as a bag (Globerson et al., 2016; Yamada et al., 2016; Ganea and Hofmann, 2017). Though they are able to capture some degree of coherence (e.g., preference towards entities from the same general domain) and are generally empirically successful, the underlying assumption is too coarse. For example, they would favor assigning all the occurrences of “England” in Figure 1 to the same entity. We hypothesize that relations useful for NEL can be induced without (or only with little) domain expertise. In order to prove this, we encode relations as latent variables and induce them by optimizing the entity-linking model in an end-to-e"
P18-1148,P13-2006,0,0.117866,"inference method based on message passing (Murphy et al., 1999). Globerson et al. (2016) propose a star model which approximates the decoding problem in Equation 2 by approximately decomposing it into n decoding problems, one per each ei . 2.3 Related work Our work focuses on modeling pairwise score functions Φ and is related to previous approaches in the two following aspects. At the other extreme, feature engineering is almost completely replaced by representation learning. These approaches rely on pretrained embeddings of words (Mikolov et al., 2013; Pennington et al., 2014) and entities (He et al., 2013; Yamada et al., 2017; Ganea and Hofmann, 2017) and often do not use virtually any other hand-crafted features. Ganea and Hofmann (2017) showed that such an approach can yield SOTA accuracy on a standard benchmark (AIDA-CoNLL dataset). Their local and pairwise score functions are Ψ(ei , ci ) = eTi Bf (ci ) 1 Φ(ei , ej , D) = eT Rej n−1 i Relations between mentions A relation widely used by NEL systems is coreference: two mentions are coreferent if they refer to the same entity. Though, as we discussed in Section 1, other linguistic relations constrain entity assignments, only a few approaches"
P18-1148,D11-1072,0,0.935177,"Missing"
P18-1148,P11-1055,0,0.0593486,"(AIDACoNLL) and substantially outperforms its relation-agnostic version. Its training also converges much faster, suggesting that the injected structural bias helps to explain regularities in the training data. 1 Introduction Named entity linking (NEL) is the task of assigning entity mentions in a text to corresponding entries in a knowledge base (KB). For example, consider Figure 1 where a mention “World Cup” refers to a KB entity FIFA W ORLD C UP. NEL is often regarded as crucial for natural language understanding and commonly used as preprocessing for tasks such as information extraction (Hoffmann et al., 2011) and question answering (Yih et al., 2015). Potential assignments of mentions to entities are regulated by semantic and discourse constraints. For example, the second and third occurrences of mention “England” in Figure 1 are coreferent and thus should be assigned to the same entity. Besides coreference, there are many other relations between entities which constrain or favor certain alignment configurations. For example, consider relation participant in in Figure 1: if “World Cup” is aligned to the entity FIFA W ORLD C UP then we expect the second “England” to refer to a football team rather"
P18-1148,Q15-1036,0,0.640468,"ts. For example, the second and third occurrences of mention “England” in Figure 1 are coreferent and thus should be assigned to the same entity. Besides coreference, there are many other relations between entities which constrain or favor certain alignment configurations. For example, consider relation participant in in Figure 1: if “World Cup” is aligned to the entity FIFA W ORLD C UP then we expect the second “England” to refer to a football team rather than a basketball one. NEL methods typically consider only coreference, relying either on off-the-shelf systems or some simple heuristics (Lazic et al., 2015), and exploit them in a pipeline fashion, though some (e.g., Cheng and Roth (2013); Ren et al. (2017)) additionally exploit a range of syntactic-semantic relations such as apposition and possessives. Another line of work ignores relations altogether and models the predicted sequence of KB entities as a bag (Globerson et al., 2016; Yamada et al., 2016; Ganea and Hofmann, 2017). Though they are able to capture some degree of coherence (e.g., preference towards entities from the same general domain) and are generally empirically successful, the underlying assumption is too coarse. For example, th"
P18-1148,D14-1162,0,0.0856078,"propagation (LBP), 1596 an approximate inference method based on message passing (Murphy et al., 1999). Globerson et al. (2016) propose a star model which approximates the decoding problem in Equation 2 by approximately decomposing it into n decoding problems, one per each ei . 2.3 Related work Our work focuses on modeling pairwise score functions Φ and is related to previous approaches in the two following aspects. At the other extreme, feature engineering is almost completely replaced by representation learning. These approaches rely on pretrained embeddings of words (Mikolov et al., 2013; Pennington et al., 2014) and entities (He et al., 2013; Yamada et al., 2017; Ganea and Hofmann, 2017) and often do not use virtually any other hand-crafted features. Ganea and Hofmann (2017) showed that such an approach can yield SOTA accuracy on a standard benchmark (AIDA-CoNLL dataset). Their local and pairwise score functions are Ψ(ei , ci ) = eTi Bf (ci ) 1 Φ(ei , ej , D) = eT Rej n−1 i Relations between mentions A relation widely used by NEL systems is coreference: two mentions are coreferent if they refer to the same entity. Though, as we discussed in Section 1, other linguistic relations constrain entity assig"
P18-1148,P15-1137,0,0.0268051,"other transformation (e.g., a flexible neural model) which can then disregard this signal when it is useless. This is not possible within our model, as it simply uses αijk to weight the bilinear terms without any extra transformation. Luckily, there is an easy way to circumvent this problem. We add to each document a padding mention mpad linked to a padding entity epad . In this way, the model can use the padding mention to damp the probability mass that the other mentions receive. This method is similar to the way some mention-ranking coreference models deal with non-anaphoric mentions (e.g. Wiseman et al. (2015)). 3.4 Implementation Following Ganea and Hofmann (2017) we use Equation 2 to define a conditional random field (CRF). We use the local score function identical to theirs and the pairwise scores are defined as explained above:   n ∑  ∑ q(E|D) ∝ exp Ψ(ei , ci ) + Φ(ei , ej , D)   i=1 i=j We also use max-product loopy belief propagation (LBP) to estimate the max-marginal probability qˆi (ei |D) ≈ max q(E|D) e1 ,...,ei−1 ei+1 ,...,en for each mention mi . The final score function for mi is given by: ρi (e) = g(ˆ qi (e|D), pˆ(e|mi )) where g is a two-layer neural network and pˆ(e|mi ) is th"
P18-1148,K16-1025,0,0.81478,"ligned to the entity FIFA W ORLD C UP then we expect the second “England” to refer to a football team rather than a basketball one. NEL methods typically consider only coreference, relying either on off-the-shelf systems or some simple heuristics (Lazic et al., 2015), and exploit them in a pipeline fashion, though some (e.g., Cheng and Roth (2013); Ren et al. (2017)) additionally exploit a range of syntactic-semantic relations such as apposition and possessives. Another line of work ignores relations altogether and models the predicted sequence of KB entities as a bag (Globerson et al., 2016; Yamada et al., 2016; Ganea and Hofmann, 2017). Though they are able to capture some degree of coherence (e.g., preference towards entities from the same general domain) and are generally empirically successful, the underlying assumption is too coarse. For example, they would favor assigning all the occurrences of “England” in Figure 1 to the same entity. We hypothesize that relations useful for NEL can be induced without (or only with little) domain expertise. In order to prove this, we encode relations as latent variables and induce them by optimizing the entity-linking model in an end-to-end fashion. In this w"
P18-1148,Q17-1028,0,0.589014,"omain) and are generally empirically successful, the underlying assumption is too coarse. For example, they would favor assigning all the occurrences of “England” in Figure 1 to the same entity. We hypothesize that relations useful for NEL can be induced without (or only with little) domain expertise. In order to prove this, we encode relations as latent variables and induce them by optimizing the entity-linking model in an end-to-end fashion. In this way, relations between mentions in documents will be induced in such a way as to be beneficial for NEL. As with other recent approaches to NEL (Yamada et al., 2017; Ganea and Hofmann, 2017), we rely on representation learning and learn embeddings of mentions, contexts and relations. This further reduces the amount of human expertise required to construct the system and, in principle, may make it more portable across languages and domains. Our multi-relational neural model achieves an 1595 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1595–1604 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics FIFA_World_Cup FIBA_Basketball_ World_Cup ... West_Germany"
P18-1148,P15-1128,0,0.0451472,"relation-agnostic version. Its training also converges much faster, suggesting that the injected structural bias helps to explain regularities in the training data. 1 Introduction Named entity linking (NEL) is the task of assigning entity mentions in a text to corresponding entries in a knowledge base (KB). For example, consider Figure 1 where a mention “World Cup” refers to a KB entity FIFA W ORLD C UP. NEL is often regarded as crucial for natural language understanding and commonly used as preprocessing for tasks such as information extraction (Hoffmann et al., 2011) and question answering (Yih et al., 2015). Potential assignments of mentions to entities are regulated by semantic and discourse constraints. For example, the second and third occurrences of mention “England” in Figure 1 are coreferent and thus should be assigned to the same entity. Besides coreference, there are many other relations between entities which constrain or favor certain alignment configurations. For example, consider relation participant in in Figure 1: if “World Cup” is aligned to the entity FIFA W ORLD C UP then we expect the second “England” to refer to a football team rather than a basketball one. NEL methods typical"
P18-1148,P11-1138,0,0.941387,", it is standard to use an heuristic to choose potential candidates, eliminating options which are highly unlikely. This preprocessing step is called candidate selection. The task of a statistical model is thus reduced to choosing the best option among a smaller list of candidates Ci = (ei1 , ..., eili ). In what follows, we will discuss two classes of approaches tackling this problem: local and global modeling. (1) arg max n ∑ E∈C1 ×...×Cn i=1 Ψ(ei , ci ) + Φ(E, D) where E = (e1 , ..., en ). The coherence score function, in the simplest form, is a sum over all pairwise scores Φ(ei , ej , D) (Ratinov et al., 2011; Huang et al., 2015; Chisholm and Hachey, 2015; Ganea et al., 2016; Guo and Barbosa, 2016; Globerson et al., 2016; Yamada et al., 2016), resulting in: E∗ = arg max n ∑ E∈C1 ×...×Cn i=1 ∑ Ψ(ei , ci )+ Φ(ei , ej , D) (2) i=j A disadvantage of global models is that exact decoding (Equation 2) is NP-hard (Wainwright et al., 2008). Ganea and Hofmann (2017) overcome this using loopy belief propagation (LBP), 1596 an approximate inference method based on message passing (Murphy et al., 1999). Globerson et al. (2016) propose a star model which approximates the decoding problem in Equation 2 by appro"
P19-1116,N18-1118,1,0.72825,"(a) red – V-form, blue – T-form; (b) red – feminine, blue – masculine. pronouns refer to the same person, the pronouns, as well as verbs that agree with them, should be translated using the same form. See Figure 1(a) for an example translation that violates T-V consistency. Figure 1(b) shows an example of inconsistent first person gender (marked on the verb), although the speaker is clearly the same. Anaphora are a form of deixis that received a lot of attention in MT research, both from the perspective of modelling (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Jean et al., 2017b; Bawden et al., 2018; Voita et al., 2018, among others) and targeted evaluation (Hardmeier et al., 2015; Guillou and Hardmeier, 2016; Müller et al., 2018), and we list anaphora errors separately, and will not further focus on them. 2.2.2 Ellipsis Ellipsis is the omission from a clause of one or more words that are nevertheless understood in the context of the remaining elements. In machine translation, elliptical constructions in the source language pose a problem if the target language does not allow the same types of ellipsis (requiring the elided material to be predicted from context), or if the elided materia"
P19-1116,W12-3156,0,0.0411155,"ocument-level context, and train all components jointly on this data, we focus on an asymmetric scenario where we have a large amount of sentence-level data, used to train our first-pass model, and a smaller amount of document-level data, used to train our secondpass decoder, keeping the first-pass model fixed. Automatic evaluation of the discourse phenomena we consider is challenging. For lexical cohesion, Wong and Kit (2012) count the ratio between the number of repeated and lexically similar content words over the total number of content words in a target document. However, Guillou (2013); Carpuat and Simard (2012) find that translations generated by a machine translation system tend to be similarly or more lexically consistent, as measured by a similar metric, than human ones. This even holds for sentence-level systems, where the increased consistency is not due to improved cohesion, but accidental – Ott et al. (2018) show that beam search introduces a bias towards frequent words, which could be one factor explaining this finding. This means that a higher repetition rate does not mean that a translation system is in fact more cohesive, and we find that even our baseline is more repetitive than the huma"
P19-1116,D11-1084,0,0.114119,"ensively in MT research.3 We classified ellipsis examples which lead to errors in sentence-level translations by the type of error they cause. Results are provided in Table 4. It can be seen that the most frequent problems related to ellipsis that we find in our annotated corpus are wrong morphological forms, followed by wrongly predicted verbs in case of verb phrase ellipsis in English, which does not exist in Russian, thus requiring the prediction of the verb in the Russian translation (Figure 2(b)). 2.2.3 Lexical cohesion Lexical cohesion has been studied previously in MT (Tiedemann, 2010; Gong et al., 2011; Wong and Kit, 2012; Kuang et al., 2018; Miculicich et al., 2018, among others). There are various cohesion devices (Morris and Hirst, 1991), and a good translation should exhibit lexical cohesion beyond the sentence level. We 3 Exceptions include (Yamamoto and Sumita, 1998), and work on the related phenomenon of pronoun dropping (Russo et al., 2012; Wang et al., 2016; Rios and Tuggener, 2017). (b) EN But that’s not what I’m talking about. I’m talking about your future. RU Но я говорю не об этом. Речь о твоём будущем. RU No ya govoryu ne ob etom. Rech’ o tvoyom budushchem. Figure 3: Examples"
P19-1116,W13-3302,0,0.0331035,"ining data has document-level context, and train all components jointly on this data, we focus on an asymmetric scenario where we have a large amount of sentence-level data, used to train our first-pass model, and a smaller amount of document-level data, used to train our secondpass decoder, keeping the first-pass model fixed. Automatic evaluation of the discourse phenomena we consider is challenging. For lexical cohesion, Wong and Kit (2012) count the ratio between the number of repeated and lexically similar content words over the total number of content words in a target document. However, Guillou (2013); Carpuat and Simard (2012) find that translations generated by a machine translation system tend to be similarly or more lexically consistent, as measured by a similar metric, than human ones. This even holds for sentence-level systems, where the increased consistency is not due to improved cohesion, but accidental – Ott et al. (2018) show that beam search introduces a bias towards frequent words, which could be one factor explaining this finding. This means that a higher repetition rate does not mean that a translation system is in fact more cohesive, and we find that even our baseline is mo"
P19-1116,L16-1100,0,0.0269998,", the pronouns, as well as verbs that agree with them, should be translated using the same form. See Figure 1(a) for an example translation that violates T-V consistency. Figure 1(b) shows an example of inconsistent first person gender (marked on the verb), although the speaker is clearly the same. Anaphora are a form of deixis that received a lot of attention in MT research, both from the perspective of modelling (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Jean et al., 2017b; Bawden et al., 2018; Voita et al., 2018, among others) and targeted evaluation (Hardmeier et al., 2015; Guillou and Hardmeier, 2016; Müller et al., 2018), and we list anaphora errors separately, and will not further focus on them. 2.2.2 Ellipsis Ellipsis is the omission from a clause of one or more words that are nevertheless understood in the context of the remaining elements. In machine translation, elliptical constructions in the source language pose a problem if the target language does not allow the same types of ellipsis (requiring the elided material to be predicted from context), or if the elided material affects the syntax of the sentence; for example, the grammatical function of a noun phrase and thus its inflec"
P19-1116,W04-3250,0,0.33155,"es context on the target side and performed best in a contrastive evaluation of pronoun translation (Müller et al., 2018). 6.2 General results BLEU scores for our model and the baselines are given in Table 6.5 For context-aware models, all sentences in a group were translated, and then only the current sentence is evaluated. We also report BLEU for the context-agnostic baseline trained only on 1.5m dataset to show how the performance is influenced by the amount of data. We observe that our model is no worse in BLEU than the baseline despite the second-pass model 5 We use bootstrap resampling (Koehn, 2004) for significance testing. 1204 model baseline (1.5m) baseline (6m) concat s-hier-to-2.tied CADec BLEU 29.10 32.40 31.56 26.68 32.38 Table 6: BLEU scores. CADec trained with p = 0.5. Scores for CADec are not statistically different from the baseline (6m). being trained only on a fraction of the data. In contrast, the concatenation baseline, trained on a mixture of data with and without context is about 1 BLEU below the context-agnostic baseline and our model when using all 3 context sentences. CADec’s performance remains the same independently from the number of context sentences (1, 2 or 3) a"
P19-1116,P07-2045,0,0.00694205,"as the proportion of times the true translation is preferred over the contrastive ones. Test set statistics are shown in Table 5. 3.1 Deixis From Table 3, we see that the most frequent error category related to deixis in our annotated corpus is the inconsistency of T-V forms when translating second person pronouns. The test set we 1201 total deixis lex. cohesion ellipsis (infl.) ellipsis (VP) 3000 2000 500 500 the same inflection. Verbs which are used to construct such contrastive translations are the top-10 lemmas of translations of the verb “do” which we get from the lexical table of Moses (Koehn et al., 2007) induced from the training data. latest relevant context 1st 2nd 3rd 1000 855 1000 630 1000 515 3.3 Table 5: Size of test sets: total number of test instances and with regard to the latest context sentence with politeness indication or with the named entity under consideration. For ellipsis, we distinguish whether model has to predict correct noun phrase inflection, or correct verb sense (VP ellipsis). construct for this category tests the ability of a machine translation system to produce translations with consistent level of politeness. We semi-automatically identify sets of consecutive sent"
P19-1116,C18-1050,0,0.290311,"ellipsis examples which lead to errors in sentence-level translations by the type of error they cause. Results are provided in Table 4. It can be seen that the most frequent problems related to ellipsis that we find in our annotated corpus are wrong morphological forms, followed by wrongly predicted verbs in case of verb phrase ellipsis in English, which does not exist in Russian, thus requiring the prediction of the verb in the Russian translation (Figure 2(b)). 2.2.3 Lexical cohesion Lexical cohesion has been studied previously in MT (Tiedemann, 2010; Gong et al., 2011; Wong and Kit, 2012; Kuang et al., 2018; Miculicich et al., 2018, among others). There are various cohesion devices (Morris and Hirst, 1991), and a good translation should exhibit lexical cohesion beyond the sentence level. We 3 Exceptions include (Yamamoto and Sumita, 1998), and work on the related phenomenon of pronoun dropping (Russo et al., 2012; Wang et al., 2016; Rios and Tuggener, 2017). (b) EN But that’s not what I’m talking about. I’m talking about your future. RU Но я говорю не об этом. Речь о твоём будущем. RU No ya govoryu ne ob etom. Rech’ o tvoyom budushchem. Figure 3: Examples of lack of lexical cohesion in MT. (a) N"
P19-1116,W17-4806,0,0.122204,"introduce a model that is suitable for this scenario and demonstrate major gains over a context-agnostic baseline on our new benchmarks without sacrificing performance as measured with BLEU.1 1 Introduction With the recent rapid progress of neural machine translation (NMT), translation mistakes and inconsistencies due to the lack of extra-sentential context are becoming more and more noticeable among otherwise adequate translations produced by standard context-agnostic NMT systems (Läubli et al., 2018). Though this problem has recently triggered a lot of attention to contextaware translation (Jean et al., 2017a; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden 1 We release code and data sets https://github.com/lena-voita/ good-translation-wrong-in-context. at et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Agrawal et al., 2018; Miculicich et al., 2018; Zhang et al., 2018), the progress and widespread adoption of the new paradigm is hampered by several important problems. Firstly, it is highly non-trivial to design metrics which would reliably trace the progress and guide model design. Standard machine translation metrics (e.g., BLEU) do not appear appropriate as they do not suffic"
P19-1116,P18-1118,0,0.231045,"the recent rapid progress of neural machine translation (NMT), translation mistakes and inconsistencies due to the lack of extra-sentential context are becoming more and more noticeable among otherwise adequate translations produced by standard context-agnostic NMT systems (Läubli et al., 2018). Though this problem has recently triggered a lot of attention to contextaware translation (Jean et al., 2017a; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden 1 We release code and data sets https://github.com/lena-voita/ good-translation-wrong-in-context. at et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Agrawal et al., 2018; Miculicich et al., 2018; Zhang et al., 2018), the progress and widespread adoption of the new paradigm is hampered by several important problems. Firstly, it is highly non-trivial to design metrics which would reliably trace the progress and guide model design. Standard machine translation metrics (e.g., BLEU) do not appear appropriate as they do not sufficiently differentiate between consistent and inconsistent translations (Wong and Kit, 2012).2 For example, if multiple translations of a name are possible, forcing consistency is essentially as likely to make all occur"
P19-1116,D18-1325,0,0.564624,"anslation (NMT), translation mistakes and inconsistencies due to the lack of extra-sentential context are becoming more and more noticeable among otherwise adequate translations produced by standard context-agnostic NMT systems (Läubli et al., 2018). Though this problem has recently triggered a lot of attention to contextaware translation (Jean et al., 2017a; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden 1 We release code and data sets https://github.com/lena-voita/ good-translation-wrong-in-context. at et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Agrawal et al., 2018; Miculicich et al., 2018; Zhang et al., 2018), the progress and widespread adoption of the new paradigm is hampered by several important problems. Firstly, it is highly non-trivial to design metrics which would reliably trace the progress and guide model design. Standard machine translation metrics (e.g., BLEU) do not appear appropriate as they do not sufficiently differentiate between consistent and inconsistent translations (Wong and Kit, 2012).2 For example, if multiple translations of a name are possible, forcing consistency is essentially as likely to make all occurrences of the name match the reference translat"
P19-1116,J91-1002,0,0.468219,"cause. Results are provided in Table 4. It can be seen that the most frequent problems related to ellipsis that we find in our annotated corpus are wrong morphological forms, followed by wrongly predicted verbs in case of verb phrase ellipsis in English, which does not exist in Russian, thus requiring the prediction of the verb in the Russian translation (Figure 2(b)). 2.2.3 Lexical cohesion Lexical cohesion has been studied previously in MT (Tiedemann, 2010; Gong et al., 2011; Wong and Kit, 2012; Kuang et al., 2018; Miculicich et al., 2018, among others). There are various cohesion devices (Morris and Hirst, 1991), and a good translation should exhibit lexical cohesion beyond the sentence level. We 3 Exceptions include (Yamamoto and Sumita, 1998), and work on the related phenomenon of pronoun dropping (Russo et al., 2012; Wang et al., 2016; Rios and Tuggener, 2017). (b) EN But that’s not what I’m talking about. I’m talking about your future. RU Но я говорю не об этом. Речь о твоём будущем. RU No ya govoryu ne ob etom. Rech’ o tvoyom budushchem. Figure 3: Examples of lack of lexical cohesion in MT. (a) Name translation inconsistency. (b) Inconsistent translation. Using either of the highlighted translat"
P19-1116,W18-6307,1,0.907879,"rbs that agree with them, should be translated using the same form. See Figure 1(a) for an example translation that violates T-V consistency. Figure 1(b) shows an example of inconsistent first person gender (marked on the verb), although the speaker is clearly the same. Anaphora are a form of deixis that received a lot of attention in MT research, both from the perspective of modelling (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Jean et al., 2017b; Bawden et al., 2018; Voita et al., 2018, among others) and targeted evaluation (Hardmeier et al., 2015; Guillou and Hardmeier, 2016; Müller et al., 2018), and we list anaphora errors separately, and will not further focus on them. 2.2.2 Ellipsis Ellipsis is the omission from a clause of one or more words that are nevertheless understood in the context of the remaining elements. In machine translation, elliptical constructions in the source language pose a problem if the target language does not allow the same types of ellipsis (requiring the elided material to be predicted from context), or if the elided material affects the syntax of the sentence; for example, the grammatical function of a noun phrase and thus its inflection in Russian may de"
P19-1116,E17-2104,0,0.0159519,"which does not exist in Russian, thus requiring the prediction of the verb in the Russian translation (Figure 2(b)). 2.2.3 Lexical cohesion Lexical cohesion has been studied previously in MT (Tiedemann, 2010; Gong et al., 2011; Wong and Kit, 2012; Kuang et al., 2018; Miculicich et al., 2018, among others). There are various cohesion devices (Morris and Hirst, 1991), and a good translation should exhibit lexical cohesion beyond the sentence level. We 3 Exceptions include (Yamamoto and Sumita, 1998), and work on the related phenomenon of pronoun dropping (Russo et al., 2012; Wang et al., 2016; Rios and Tuggener, 2017). (b) EN But that’s not what I’m talking about. I’m talking about your future. RU Но я говорю не об этом. Речь о твоём будущем. RU No ya govoryu ne ob etom. Rech’ o tvoyom budushchem. Figure 3: Examples of lack of lexical cohesion in MT. (a) Name translation inconsistency. (b) Inconsistent translation. Using either of the highlighted translations consistently would be good. focus on repetition with two frequent cases in our annotated corpus being reiteration of named entities (Figure 3(a)) and reiteration of more general phrase types for emphasis (Figure 3(b)) or in clarification questions. 3"
P19-1116,E12-3010,0,0.0228523,"ase of verb phrase ellipsis in English, which does not exist in Russian, thus requiring the prediction of the verb in the Russian translation (Figure 2(b)). 2.2.3 Lexical cohesion Lexical cohesion has been studied previously in MT (Tiedemann, 2010; Gong et al., 2011; Wong and Kit, 2012; Kuang et al., 2018; Miculicich et al., 2018, among others). There are various cohesion devices (Morris and Hirst, 1991), and a good translation should exhibit lexical cohesion beyond the sentence level. We 3 Exceptions include (Yamamoto and Sumita, 1998), and work on the related phenomenon of pronoun dropping (Russo et al., 2012; Wang et al., 2016; Rios and Tuggener, 2017). (b) EN But that’s not what I’m talking about. I’m talking about your future. RU Но я говорю не об этом. Речь о твоём будущем. RU No ya govoryu ne ob etom. Rech’ o tvoyom budushchem. Figure 3: Examples of lack of lexical cohesion in MT. (a) Name translation inconsistency. (b) Inconsistent translation. Using either of the highlighted translations consistently would be good. focus on repetition with two frequent cases in our annotated corpus being reiteration of named entities (Figure 3(a)) and reiteration of more general phrase types for emphasis (F"
P19-1116,P16-1162,1,0.549441,"Missing"
P19-1116,W10-2602,0,0.0527347,"been studied extensively in MT research.3 We classified ellipsis examples which lead to errors in sentence-level translations by the type of error they cause. Results are provided in Table 4. It can be seen that the most frequent problems related to ellipsis that we find in our annotated corpus are wrong morphological forms, followed by wrongly predicted verbs in case of verb phrase ellipsis in English, which does not exist in Russian, thus requiring the prediction of the verb in the Russian translation (Figure 2(b)). 2.2.3 Lexical cohesion Lexical cohesion has been studied previously in MT (Tiedemann, 2010; Gong et al., 2011; Wong and Kit, 2012; Kuang et al., 2018; Miculicich et al., 2018, among others). There are various cohesion devices (Morris and Hirst, 1991), and a good translation should exhibit lexical cohesion beyond the sentence level. We 3 Exceptions include (Yamamoto and Sumita, 1998), and work on the related phenomenon of pronoun dropping (Russo et al., 2012; Wang et al., 2016; Rios and Tuggener, 2017). (b) EN But that’s not what I’m talking about. I’m talking about your future. RU Но я говорю не об этом. Речь о твоём будущем. RU No ya govoryu ne ob etom. Rech’ o tvoyom budushchem."
P19-1116,W17-4811,0,0.282935,"this scenario and demonstrate major gains over a context-agnostic baseline on our new benchmarks without sacrificing performance as measured with BLEU.1 1 Introduction With the recent rapid progress of neural machine translation (NMT), translation mistakes and inconsistencies due to the lack of extra-sentential context are becoming more and more noticeable among otherwise adequate translations produced by standard context-agnostic NMT systems (Läubli et al., 2018). Though this problem has recently triggered a lot of attention to contextaware translation (Jean et al., 2017a; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden 1 We release code and data sets https://github.com/lena-voita/ good-translation-wrong-in-context. at et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Agrawal et al., 2018; Miculicich et al., 2018; Zhang et al., 2018), the progress and widespread adoption of the new paradigm is hampered by several important problems. Firstly, it is highly non-trivial to design metrics which would reliably trace the progress and guide model design. Standard machine translation metrics (e.g., BLEU) do not appear appropriate as they do not sufficiently differentiate between consistent and incons"
P19-1116,P18-1117,1,0.867087,"1 Introduction With the recent rapid progress of neural machine translation (NMT), translation mistakes and inconsistencies due to the lack of extra-sentential context are becoming more and more noticeable among otherwise adequate translations produced by standard context-agnostic NMT systems (Läubli et al., 2018). Though this problem has recently triggered a lot of attention to contextaware translation (Jean et al., 2017a; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden 1 We release code and data sets https://github.com/lena-voita/ good-translation-wrong-in-context. at et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Agrawal et al., 2018; Miculicich et al., 2018; Zhang et al., 2018), the progress and widespread adoption of the new paradigm is hampered by several important problems. Firstly, it is highly non-trivial to design metrics which would reliably trace the progress and guide model design. Standard machine translation metrics (e.g., BLEU) do not appear appropriate as they do not sufficiently differentiate between consistent and inconsistent translations (Wong and Kit, 2012).2 For example, if multiple translations of a name are possible, forcing consistency is essentially as"
P19-1116,D17-1301,0,0.232651,"at is suitable for this scenario and demonstrate major gains over a context-agnostic baseline on our new benchmarks without sacrificing performance as measured with BLEU.1 1 Introduction With the recent rapid progress of neural machine translation (NMT), translation mistakes and inconsistencies due to the lack of extra-sentential context are becoming more and more noticeable among otherwise adequate translations produced by standard context-agnostic NMT systems (Läubli et al., 2018). Though this problem has recently triggered a lot of attention to contextaware translation (Jean et al., 2017a; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden 1 We release code and data sets https://github.com/lena-voita/ good-translation-wrong-in-context. at et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Agrawal et al., 2018; Miculicich et al., 2018; Zhang et al., 2018), the progress and widespread adoption of the new paradigm is hampered by several important problems. Firstly, it is highly non-trivial to design metrics which would reliably trace the progress and guide model design. Standard machine translation metrics (e.g., BLEU) do not appear appropriate as they do not sufficiently differentiate"
P19-1116,N16-1113,0,0.0317856,"llipsis in English, which does not exist in Russian, thus requiring the prediction of the verb in the Russian translation (Figure 2(b)). 2.2.3 Lexical cohesion Lexical cohesion has been studied previously in MT (Tiedemann, 2010; Gong et al., 2011; Wong and Kit, 2012; Kuang et al., 2018; Miculicich et al., 2018, among others). There are various cohesion devices (Morris and Hirst, 1991), and a good translation should exhibit lexical cohesion beyond the sentence level. We 3 Exceptions include (Yamamoto and Sumita, 1998), and work on the related phenomenon of pronoun dropping (Russo et al., 2012; Wang et al., 2016; Rios and Tuggener, 2017). (b) EN But that’s not what I’m talking about. I’m talking about your future. RU Но я говорю не об этом. Речь о твоём будущем. RU No ya govoryu ne ob etom. Rech’ o tvoyom budushchem. Figure 3: Examples of lack of lexical cohesion in MT. (a) Name translation inconsistency. (b) Inconsistent translation. Using either of the highlighted translations consistently would be good. focus on repetition with two frequent cases in our annotated corpus being reiteration of named entities (Figure 3(a)) and reiteration of more general phrase types for emphasis (Figure 3(b)) or in c"
P19-1116,D12-1097,0,0.508088,"code and data sets https://github.com/lena-voita/ good-translation-wrong-in-context. at et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Agrawal et al., 2018; Miculicich et al., 2018; Zhang et al., 2018), the progress and widespread adoption of the new paradigm is hampered by several important problems. Firstly, it is highly non-trivial to design metrics which would reliably trace the progress and guide model design. Standard machine translation metrics (e.g., BLEU) do not appear appropriate as they do not sufficiently differentiate between consistent and inconsistent translations (Wong and Kit, 2012).2 For example, if multiple translations of a name are possible, forcing consistency is essentially as likely to make all occurrences of the name match the reference translation as making them all different from the reference. Second, most previous work on context-aware NMT has made the assumption that all the bilingual data is available at the document level. However, isolated parallel sentences are a lot easier to acquire and hence only a fraction of the parallel data will be at the document level in any practical scenario. In other words, a context-aware model trained only on documentlevel"
P19-1116,P98-2233,0,0.0378362,"tated corpus are wrong morphological forms, followed by wrongly predicted verbs in case of verb phrase ellipsis in English, which does not exist in Russian, thus requiring the prediction of the verb in the Russian translation (Figure 2(b)). 2.2.3 Lexical cohesion Lexical cohesion has been studied previously in MT (Tiedemann, 2010; Gong et al., 2011; Wong and Kit, 2012; Kuang et al., 2018; Miculicich et al., 2018, among others). There are various cohesion devices (Morris and Hirst, 1991), and a good translation should exhibit lexical cohesion beyond the sentence level. We 3 Exceptions include (Yamamoto and Sumita, 1998), and work on the related phenomenon of pronoun dropping (Russo et al., 2012; Wang et al., 2016; Rios and Tuggener, 2017). (b) EN But that’s not what I’m talking about. I’m talking about your future. RU Но я говорю не об этом. Речь о твоём будущем. RU No ya govoryu ne ob etom. Rech’ o tvoyom budushchem. Figure 3: Examples of lack of lexical cohesion in MT. (a) Name translation inconsistency. (b) Inconsistent translation. Using either of the highlighted translations consistently would be good. focus on repetition with two frequent cases in our annotated corpus being reiteration of named entitie"
P19-1116,W15-2501,0,\N,Missing
P19-1116,W10-1737,0,\N,Missing
P19-1116,2010.iwslt-papers.10,0,\N,Missing
P19-1116,W05-0908,0,\N,Missing
P19-1116,C98-2228,0,\N,Missing
P19-1116,L18-1275,0,\N,Missing
P19-1187,P07-1036,0,0.14589,"Missing"
P19-1187,Y15-1010,0,0.0588417,"Missing"
P19-1187,D17-1277,0,0.296092,"Missing"
P19-1187,P16-1059,0,0.308632,"dia). For instance, in Figure 1 we link mention “Trump” to Wikipedia entity Donald Trump. Entity linking enables aggregation of information across multiple mentions of the same entity which is crucial in many natural language processing applications such as question answering (Hoffmann et al., 2011; Welbl et al., 2018), information extraction (Hoffmann et al., 2011) or multi-document summarization (Nenkova, 2008). While traditionally entity linkers relied mostly on Wikipedia and heuristics (Milne and Witten, 2008; Ratinov et al., 2011a; Cheng and Roth, 2013), the recent generation of methods (Globerson et al., 2016; Guo and Barbosa, 2016; Yamada et al., 2016; Ganea and Hofmann, 2017; Le and Titov, 2018) approached the task as supervised learning on a collection of documents specifically annotated for the entity linking problem (e.g., relying on AIDA CoNLL (Hoffart et al., 2011)). While they substantially outperform the traditional methods, such human-annotated resources are scarce (e.g., available mostly for English) and expensive to create. Moreover, the resulting models end up being domain-specific: their performance drops substantially when they are used in a new domain.1 We will refer to these syste"
P19-1187,D11-1072,0,0.757705,"Missing"
P19-1187,P11-1055,0,0.0588095,"The correct entities are marked with (*). We automatically extract likely candidates (red bold) and likely negative examples (non-bold red). These are used to train our weakly-supervised model. Introduction Named entity linking is the task of linking a mention to the corresponding entity in a knowledge base (e.g., Wikipedia). For instance, in Figure 1 we link mention “Trump” to Wikipedia entity Donald Trump. Entity linking enables aggregation of information across multiple mentions of the same entity which is crucial in many natural language processing applications such as question answering (Hoffmann et al., 2011; Welbl et al., 2018), information extraction (Hoffmann et al., 2011) or multi-document summarization (Nenkova, 2008). While traditionally entity linkers relied mostly on Wikipedia and heuristics (Milne and Witten, 2008; Ratinov et al., 2011a; Cheng and Roth, 2013), the recent generation of methods (Globerson et al., 2016; Guo and Barbosa, 2016; Yamada et al., 2016; Ganea and Hofmann, 2017; Le and Titov, 2018) approached the task as supervised learning on a collection of documents specifically annotated for the entity linking problem (e.g., relying on AIDA CoNLL (Hoffart et al., 2011)). While"
P19-1187,Q15-1036,0,0.568335,"e 1). 1935 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1935–1945 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics languages and covers many domains. Though Wikipedia information is often used within entity linking pipelines, previous systems relying on Wikipedia are substantially less accurate than modern fully-supervised systems (e.g., Cheng and Roth (2013), Ratinov at al. (2011a)). This is also true of the only other method which, like ours, uses a combination of Wikipedia data and unlabeled texts (Lazic et al., 2015). We will refer to approaches using this form of supervision, including our approach, as Wikipedia-based linkers. Wikipedia articles have a specific rigid structure (Chen et al., 2009), often dictated by the corresponding templates, and mentions in them are only linked once (when first mentioned). For these reasons, Wikipedia pages were not regarded as suitable for training document-level models (Globerson et al., 2016; Ganea and Hofmann, 2017), whereas state-of-the-art fully supervised methods rely on document-level modeling. We will show that, by exploiting unlabeled documents and estimating"
P19-1187,D13-1184,0,0.593409,"he corresponding entity in a knowledge base (e.g., Wikipedia). For instance, in Figure 1 we link mention “Trump” to Wikipedia entity Donald Trump. Entity linking enables aggregation of information across multiple mentions of the same entity which is crucial in many natural language processing applications such as question answering (Hoffmann et al., 2011; Welbl et al., 2018), information extraction (Hoffmann et al., 2011) or multi-document summarization (Nenkova, 2008). While traditionally entity linkers relied mostly on Wikipedia and heuristics (Milne and Witten, 2008; Ratinov et al., 2011a; Cheng and Roth, 2013), the recent generation of methods (Globerson et al., 2016; Guo and Barbosa, 2016; Yamada et al., 2016; Ganea and Hofmann, 2017; Le and Titov, 2018) approached the task as supervised learning on a collection of documents specifically annotated for the entity linking problem (e.g., relying on AIDA CoNLL (Hoffart et al., 2011)). While they substantially outperform the traditional methods, such human-annotated resources are scarce (e.g., available mostly for English) and expensive to create. Moreover, the resulting models end up being domain-specific: their performance drops substantially when th"
P19-1187,P18-1148,1,0.89197,"king enables aggregation of information across multiple mentions of the same entity which is crucial in many natural language processing applications such as question answering (Hoffmann et al., 2011; Welbl et al., 2018), information extraction (Hoffmann et al., 2011) or multi-document summarization (Nenkova, 2008). While traditionally entity linkers relied mostly on Wikipedia and heuristics (Milne and Witten, 2008; Ratinov et al., 2011a; Cheng and Roth, 2013), the recent generation of methods (Globerson et al., 2016; Guo and Barbosa, 2016; Yamada et al., 2016; Ganea and Hofmann, 2017; Le and Titov, 2018) approached the task as supervised learning on a collection of documents specifically annotated for the entity linking problem (e.g., relying on AIDA CoNLL (Hoffart et al., 2011)). While they substantially outperform the traditional methods, such human-annotated resources are scarce (e.g., available mostly for English) and expensive to create. Moreover, the resulting models end up being domain-specific: their performance drops substantially when they are used in a new domain.1 We will refer to these systems as fully-supervised. Our goal is to show that an accurate entity linker can be created"
P19-1187,Q15-1011,0,0.0473453,"ch are not ambiguous enough. To break ties, we chose a mention which is ranked higher in the first step. 1939 github.com/dalab/deep-ed code.google.com/archive/p/word2vec/ 8 nlp.stanford.edu/projects/glove/ development and test sets in our training set. In addition, we did not use any articles appearing in WIKI to compute rwiki . We rely on SpaCy9 to extract named entity mentions. We compare our model to those systems which were trained on Wikipedia or on Wikipedia plus unlabeled documents. They are: Milne and Witten (2008), Ratinov et al. (2011a), Hoffart et al. (2011), Cheng and Roth (2013), Chisholm and Hachey (2015), Lazic et al. (2015). Note that we are aware of only Lazic et al. (2015) which relied on learning from a combination of Wikipedia and unlabeled documents. They use semi-supervised learning and exploit only local context (i.e. coherence with other entities is not modeled). We also compare to recent state-of-the-art systems trained supervisedly on Wikipedia and extra supervision or on AIDA CoNLL: Chisholm and Hachey (2015), Guo and Barbosa (2016), Globerson et al. (2016), Yamada et al. (2016), Ganea and Hofmann (2017), Le and Titov (2018). Chisholm and Hachey (2015) used supervision in the form"
P19-1187,J05-1003,0,0.0927496,"igure 2, A ∈ Rdc ×dc is a diagonal matrix. A similar attention model was used in the supervised linkers of Le and Titov (2018) and Globerson et al. (2016). Previous supervised methods such as Ganea and Hofmann (2017) additionally exploited a simple extra feature pwiki (ei |mi ): the normalized frequency of mention mi being used as an anchor text for entity ei in Wikipedia articles and YAGO. We combine this score with the model score s(ei |D) using a one-layer neural network to yield sˆ(ei |D). At test time, we use our model to select entities from the candidate list. As standard in reranking (Collins and Koo, 2005), we linearly combine 2 For each ei , you create its own graphical model: keep only edges connecting ei to all other entities; what you obtain is a star-shaped graph with ei at its center. 1937 sˆ(ei |D) with the score sc (ei |D) from the candidate generator, defined below (Section 3.3).3 The hyper-parameters are chosen using a development set. Additional details are provided in the appendix. 2.3 XXh D mi δ + max sˆ(e− i |D) − e− i ∈Ei i − max sˆ(e+ i |D) + e+ i ∈Ei + where Θ is the set of model parameters, δ is a margin, and [x]+ = max{0, x}. Producing Weak Supervision We rely primarily on Wi"
P19-1187,P09-1113,0,0.108348,"ifiers’) has been a popular line of research both for named entity linking (Cheng and Roth, 2013; Milne and Witten, 2008) and generally entity disambiguation tasks (Ratinov et al., 2011b). How11 Note that we do not use NER types in our system. The weakly- or semi-supervised set-up, which we use, is not common for entity linking. The only other approach which uses a combination of Wikipedia and unlabeled data, as far as we are aware of, is by Lazic et al. (2015). We discussed it and compared to in previous sections. Our setup is inspired by distantly-supervised learning in relation extraction (Mintz et al., 2009). In distant learning, the annotation is automatically (and noisily) induced relying on a knowledge base instead of annotating the data by hand. Fan, Zhou, and Zheng (2015) learned a Freebase linker using distance supervision. Their evaluation is nonstandard. They also do not attempt to learn a disambiguation model but directly train their system to replicate noisy projected annotations. Wang et al. (2015) refer to their approach as unsupervised, as they do not use unlabeled data. However, their method does not involve any learning and relies on matching heuristics. Some aspects of their appro"
P19-1187,K16-1025,0,0.493904,"“Trump” to Wikipedia entity Donald Trump. Entity linking enables aggregation of information across multiple mentions of the same entity which is crucial in many natural language processing applications such as question answering (Hoffmann et al., 2011; Welbl et al., 2018), information extraction (Hoffmann et al., 2011) or multi-document summarization (Nenkova, 2008). While traditionally entity linkers relied mostly on Wikipedia and heuristics (Milne and Witten, 2008; Ratinov et al., 2011a; Cheng and Roth, 2013), the recent generation of methods (Globerson et al., 2016; Guo and Barbosa, 2016; Yamada et al., 2016; Ganea and Hofmann, 2017; Le and Titov, 2018) approached the task as supervised learning on a collection of documents specifically annotated for the entity linking problem (e.g., relying on AIDA CoNLL (Hoffart et al., 2011)). While they substantially outperform the traditional methods, such human-annotated resources are scarce (e.g., available mostly for English) and expensive to create. Moreover, the resulting models end up being domain-specific: their performance drops substantially when they are used in a new domain.1 We will refer to these systems as fully-supervised. Our goal is to show"
P19-1187,P11-1138,0,0.298197,"Missing"
P19-1187,D12-1042,0,0.104364,"Missing"
P19-1187,D15-1081,0,0.0197393,"a, as far as we are aware of, is by Lazic et al. (2015). We discussed it and compared to in previous sections. Our setup is inspired by distantly-supervised learning in relation extraction (Mintz et al., 2009). In distant learning, the annotation is automatically (and noisily) induced relying on a knowledge base instead of annotating the data by hand. Fan, Zhou, and Zheng (2015) learned a Freebase linker using distance supervision. Their evaluation is nonstandard. They also do not attempt to learn a disambiguation model but directly train their system to replicate noisy projected annotations. Wang et al. (2015) refer to their approach as unsupervised, as they do not use unlabeled data. However, their method does not involve any learning and relies on matching heuristics. Some aspects of their approach (e.g., using Wikipedia link statitics) resemble our candidate generation stage. So, in principle, their approach could be compared to the ‘no-disambiguation’ baselines (sc ) in Table 3. Their evaluation set-up is not standard. Our model (but not the estimation method) bears similarities to the approaches of Le and Titov (2018) and Globerson at al. (2016). Both these supervised approaches are global and"
P19-1187,Q18-1021,0,0.024322,"e marked with (*). We automatically extract likely candidates (red bold) and likely negative examples (non-bold red). These are used to train our weakly-supervised model. Introduction Named entity linking is the task of linking a mention to the corresponding entity in a knowledge base (e.g., Wikipedia). For instance, in Figure 1 we link mention “Trump” to Wikipedia entity Donald Trump. Entity linking enables aggregation of information across multiple mentions of the same entity which is crucial in many natural language processing applications such as question answering (Hoffmann et al., 2011; Welbl et al., 2018), information extraction (Hoffmann et al., 2011) or multi-document summarization (Nenkova, 2008). While traditionally entity linkers relied mostly on Wikipedia and heuristics (Milne and Witten, 2008; Ratinov et al., 2011a; Cheng and Roth, 2013), the recent generation of methods (Globerson et al., 2016; Guo and Barbosa, 2016; Yamada et al., 2016; Ganea and Hofmann, 2017; Le and Titov, 2018) approached the task as supervised learning on a collection of documents specifically annotated for the entity linking problem (e.g., relying on AIDA CoNLL (Hoffart et al., 2011)). While they substantially ou"
P19-1284,D17-1042,0,0.100006,"Missing"
P19-1284,D15-1003,0,0.0234473,"those areas. Interpretability. Machine learning research has been focusing more and more on interpretability (Gilpin et al., 2018). However, there are many nuances to interpretability (Lipton, 2016), and amongst them we focus on model transparency. One strategy is to extract a simpler, interpretable model from a neural network, though this comes at the cost of performance. For example, Thrun (1995) extract if-then rules, while Craven and Shavlik (1996) extract decision trees. There is also work on making word vectors more interpretable. Faruqui et al. (2015) make word vectors more sparse, and Herbelot and Vecchi (2015) learn to map distributional word vectors to model-theoretic semantic vectors. Similarly to Lei et al. (2016), Titov and McDonald (2008) extract informative fragments of text by jointly training a classifier and a model predicting a stochastic mask, while relying on Gibbs sampling to do so. Their focus is on using the sentiment labels as a weak supervision signal for opinion summarization rather than on rationalizing classifier predictions. There are also related approaches that aim to interpret an already-trained model, in contrast to Lei et al. (2016) and our approach where the rationale is"
P19-1284,D15-1075,0,0.0339015,"Missing"
P19-1284,P16-1139,0,0.0232646,"Missing"
P19-1284,P17-1152,0,0.016633,"0 0 man 0 0 77 21 0 0 0 0 walking 0 0 0 0 88 0 0 0 dog 0 0 0 0 0 0 86 0 Natural Language Inference In Natural language inference (NLI), given a premise sentence x(p) and a hypothesis sentence x(h) , the goal is to predict their relation y which can be contradiction, entailment, or neutral. As our dataset we use the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015). Baseline. We use the Decomposable Attention model (DA) of Parikh et al. (2016).9 DA does not make use of LSTMs, but rather uses attention to find connections between the premise and the hy9 Better results e.g. Chen et al. (2017) and data sets for NLI exist, but are not the focus of this paper. Figure 6: Example of HardKuma attention between a premise (rows) and hypothesis (columns) in SNLI (cell values shown in multiples of 10−2 ). 2969 7 Related Work This work has connections with work on interpretability, learning from rationales, sparse structures, and rectified distributions. We discuss each of those areas. Interpretability. Machine learning research has been focusing more and more on interpretability (Gilpin et al., 2018). However, there are many nuances to interpretability (Lipton, 2016), and amongst them we fo"
P19-1284,P18-1031,0,0.0163499,"outhfeel is quite strong in the sense that you can get a good taste of it before you even swallow . Rationale Extractor pours a dark amber color with decent head that does not recede much . it ’s a tad too dark to see the carbonation , but fairs well . smells of roasted malts and mouthfeel is quite strong in the sense that you can get a good taste of it before you even swallow . Figure 1: Rationale extraction for a beer review. know why a prediction was made, we do not know if we can trust it. Introduction Neural networks are bringing incredible performance gains on text classification tasks (Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2019). However, this power comes hand in hand with a desire for more interpretability, even though its definition may differ (Lipton, 2016). While it is useful to obtain high classification accuracy, with more data available than ever before it also becomes increasingly important to justify predictions. Imagine having to classify a large collection of documents, while verifying that the classifications make sense. It would be extremely time-consuming to read each document to evaluate the results. Moreover, if we do not What if the model could provide us th"
P19-1284,D18-1108,0,0.0624407,"ftmax itself, but in particular, they target relaxations such as sparsemax (Martins and Astudillo, 2016) which, unlike softmax, are sparse (i.e. produce vectors of probability values with components that evaluate to exactly 0). Their activation functions are themselves solutions to convex optimization problems, to which they provide efficient forward and backward passes. The technique can be seen as a deterministic sparsely activated layer which they use as a drop-in replacement to standard attention mechanisms. In contrast, in this paper we focus on binary outcomes rather than K-valued ones. Niculae et al. (2018) extend the framework to structured discrete spaces where they learn sparse parameterizations of discrete latent models. In this context, parameter estimation requires exact marginalization of discrete variables or gradient estimation via REINFORCE. They show that oftentimes distributions are sparse enough to enable exact marginal inference. Peng et al. (2018) propose SPIGOT, a proxy gradient to the non-differentiable arg max operator. This proxy requires an arg max solver (e.g. Viterbi for structured prediction) and, like the straight-through estimator (Bengio et al., 2013), is a biased estim"
P19-1284,D16-1244,0,0.101556,"Missing"
P19-1284,P18-1173,0,0.0743277,"Missing"
P19-1284,N18-1202,0,0.0135614,"in the sense that you can get a good taste of it before you even swallow . Rationale Extractor pours a dark amber color with decent head that does not recede much . it ’s a tad too dark to see the carbonation , but fairs well . smells of roasted malts and mouthfeel is quite strong in the sense that you can get a good taste of it before you even swallow . Figure 1: Rationale extraction for a beer review. know why a prediction was made, we do not know if we can trust it. Introduction Neural networks are bringing incredible performance gains on text classification tasks (Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2019). However, this power comes hand in hand with a desire for more interpretability, even though its definition may differ (Lipton, 2016). While it is useful to obtain high classification accuracy, with more data available than ever before it also becomes increasingly important to justify predictions. Imagine having to classify a large collection of documents, while verifying that the classifications make sense. It would be extremely time-consuming to read each document to evaluate the results. Moreover, if we do not What if the model could provide us the most important part"
P19-1284,N16-3020,0,0.706564,"tributional word vectors to model-theoretic semantic vectors. Similarly to Lei et al. (2016), Titov and McDonald (2008) extract informative fragments of text by jointly training a classifier and a model predicting a stochastic mask, while relying on Gibbs sampling to do so. Their focus is on using the sentiment labels as a weak supervision signal for opinion summarization rather than on rationalizing classifier predictions. There are also related approaches that aim to interpret an already-trained model, in contrast to Lei et al. (2016) and our approach where the rationale is jointly modeled. Ribeiro et al. (2016) make any classifier interpretable by approximating it locally with a linear proxy model in an approach called LIME, and Alvarez-Melis and Jaakkola (2017) propose a framework that returns input-output pairs that are causally related. Learning from rationales. Our work is different from approaches that aim to improve classification using rationales as an additional input (Zaidan et al., 2007; Zaidan and Eisner, 2008; Zhang et al., 2016). Instead, our rationales are latent and we are interested in uncovering them. We only use annotated rationales for evaluation. Sparse layers. Also arguing for e"
P19-1284,D08-1004,0,\N,Missing
P19-1284,P15-1144,0,\N,Missing
P19-1284,N07-1033,0,\N,Missing
P19-1284,P08-1036,1,\N,Missing
P19-1284,D13-1170,0,\N,Missing
P19-1284,N19-1423,0,\N,Missing
P19-1400,charton-etal-2014-improving,0,0.434526,"L-ND performs worse than MIL-ND, as we expected, because there are no noisy data points at test time. What is wrong with the candidate selector? The above results show that candidate selection is a bottleneck and that the used selector is far from perfect. We found two cases where the selector is problematic: (i) the mention or the entity name is in an abbreviated form, such as ‘U.N.’ rather than ‘United Nations’, (ii) the mention and the entity’s name only fuzzily match, such as ‘[English] county’ and E NGLAND (country). We can overcome these problems via extending our surface matching as in Charton et al. (2014); Usbeck et al. (2014) or using word embeddings. Even in some cases when the selector does not have any problems with surface matching, the number of candidates may be too large. For instance, consider ‘[Simpson] killed his wife...’, there are more than 1,500 entities in the knowledge base containing the word ‘Simpson’. It is unlikely that our entity disambiguation model can deal with such large lists. We may need a stronger mechanism for reducing the number of candidates. For example, we could use document-level information to discard highly unlikely entities. 6 Related work High performance"
P19-1400,Q15-1011,0,0.123494,"link entities, greatly outperforms the surface matching baseline. For a subset of entity categories, it even approaches the performance of supervised learning. 1 Introduction Entity linking (EL) is the task of linking potentially ambiguous textual mentions to the corresponding entities in a knowledge base. Accurate entity linking is crucial in many natural language processing tasks, including information extraction (Hoffart et al., 2011) and question answering (Yih et al., 2015). Though there has been significant progress in entity linking recently (Ratinov et al., 2011; Hoffart et al., 2011; Chisholm and Hachey, 2015; Globerson et al., 2016; Yamada et al., 2017; Ganea and Hofmann, 2017; Le and Titov, 2018), previous work has focused on supervised learning. Annotated data necessary for supervised learning is available for certain knowledge bases and domains. For example, one can directly use webpages linking to Wikipedia to learn a Wikipedia linker. Similarly, there exist domain-specific sets of manually annotated documents (e.g., AIDACoNLL news dataset for YAGO (Hoffart et al., 2011)). However, for many ontologies and domains annotation is not available or limited (e.g., law). Our goal is to develop a met"
P19-1400,Y15-1010,0,0.335821,"tion frameworks (Ratinov et al., 2011) are introduced: local which resolves mentions independently, and global which makes use of coherence modeling at the document level. Though we experimented with local models, the local-global distinction is largely orthogonal as we can directly integrate coherence modeling components in our DL approach. Different types of supervision have been considered in previous work: full supervision (Yamada et al., 2017; Ganea and Hofmann, 2017; Le and Titov, 2018), using combinations of labeled and unlabeled data (Lazic et al., 2015), and even distant supervision (Fan et al., 2015). The approach of Fan et al. (2015) is heavily Wikipediabased: they rely on a heuristic mapping from Freebase entities to Wikipedia entities, and learn features from Wikipedia articles. Unlike ours, their approach cannot be generalized to set-ups where no documents are available for entities. 4088 7 Conclusions We introduced the first approach to entity linking which neither uses annotated texts, nor assumes that entities are associated with textual documents (e.g., Wikipedia articles). We learn the model using the MIL paradigm, and introduce a novel component, a noise detecting classifier, es"
P19-1400,D17-1277,0,0.0248221,"Missing"
P19-1400,P16-1059,0,0.0828233,"erforms the surface matching baseline. For a subset of entity categories, it even approaches the performance of supervised learning. 1 Introduction Entity linking (EL) is the task of linking potentially ambiguous textual mentions to the corresponding entities in a knowledge base. Accurate entity linking is crucial in many natural language processing tasks, including information extraction (Hoffart et al., 2011) and question answering (Yih et al., 2015). Though there has been significant progress in entity linking recently (Ratinov et al., 2011; Hoffart et al., 2011; Chisholm and Hachey, 2015; Globerson et al., 2016; Yamada et al., 2017; Ganea and Hofmann, 2017; Le and Titov, 2018), previous work has focused on supervised learning. Annotated data necessary for supervised learning is available for certain knowledge bases and domains. For example, one can directly use webpages linking to Wikipedia to learn a Wikipedia linker. Similarly, there exist domain-specific sets of manually annotated documents (e.g., AIDACoNLL news dataset for YAGO (Hoffart et al., 2011)). However, for many ontologies and domains annotation is not available or limited (e.g., law). Our goal is to develop a method which does not rely"
P19-1400,D11-1072,0,0.0963529,"Missing"
P19-1400,Q15-1036,0,0.0182781,"n entity from a candidate list, two main disambiguation frameworks (Ratinov et al., 2011) are introduced: local which resolves mentions independently, and global which makes use of coherence modeling at the document level. Though we experimented with local models, the local-global distinction is largely orthogonal as we can directly integrate coherence modeling components in our DL approach. Different types of supervision have been considered in previous work: full supervision (Yamada et al., 2017; Ganea and Hofmann, 2017; Le and Titov, 2018), using combinations of labeled and unlabeled data (Lazic et al., 2015), and even distant supervision (Fan et al., 2015). The approach of Fan et al. (2015) is heavily Wikipediabased: they rely on a heuristic mapping from Freebase entities to Wikipedia entities, and learn features from Wikipedia articles. Unlike ours, their approach cannot be generalized to set-ups where no documents are available for entities. 4088 7 Conclusions We introduced the first approach to entity linking which neither uses annotated texts, nor assumes that entities are associated with textual documents (e.g., Wikipedia articles). We learn the model using the MIL paradigm, and introduce a"
P19-1400,P18-1148,1,0.897533,"even approaches the performance of supervised learning. 1 Introduction Entity linking (EL) is the task of linking potentially ambiguous textual mentions to the corresponding entities in a knowledge base. Accurate entity linking is crucial in many natural language processing tasks, including information extraction (Hoffart et al., 2011) and question answering (Yih et al., 2015). Though there has been significant progress in entity linking recently (Ratinov et al., 2011; Hoffart et al., 2011; Chisholm and Hachey, 2015; Globerson et al., 2016; Yamada et al., 2017; Ganea and Hofmann, 2017; Le and Titov, 2018), previous work has focused on supervised learning. Annotated data necessary for supervised learning is available for certain knowledge bases and domains. For example, one can directly use webpages linking to Wikipedia to learn a Wikipedia linker. Similarly, there exist domain-specific sets of manually annotated documents (e.g., AIDACoNLL news dataset for YAGO (Hoffart et al., 2011)). However, for many ontologies and domains annotation is not available or limited (e.g., law). Our goal is to develop a method which does not rely on any training data besides unlabeled texts and a knowledge base."
P19-1400,P14-5010,0,0.0046689,"Missing"
P19-1400,P09-1113,0,0.108963,"robability is then used to weight the corresponding term in the objective function of the MIL model. By jointly training the MIL model and the noise detection classifier, we effectively let the MIL model choose which examples to use for training. As we will see in our experimental analysis, this joint learning method leads to a substantial improvement in performance. We also confirm that the noise detection model is generally able to identify and exclude wrong candidate lists by comparing its predictions to the gold standard. DL is the mainstream approach to learning relation extractors (RE) (Mintz et al., 2009; Riedel et al., 2010), a problem related to entity linking. However, the two instantiations of the DL framework are very different. For RE, a bag of sentences is assigned to a categorical label (a relation). For EL, we assign a bag of entities, conditioned on the mention, to a positive class (correct) or a negative class (incorrect). We evaluate our approach on the news domain for English as, having gold standard annotation (AIDA CoNLL), we can both assess performance and compute the upper bound, given by supervised learning. Nevertheless, we expect that our methodology is applicable to a wid"
P19-1400,D14-1162,0,0.0833263,"r, we introduce the following terms. Definition 1. A data point is a tuple hm, c, E + , E − i of mention m, context c, positive set E + , and negative set E − . In testing, E − = ∅. Definition 2. A data point hm, c, E + , E − i is noisy if E + does not contain the correct entity for mention m. If a data point is not noisy, we will refer to it as valid. 3 Models We introduce two approaches. The first one directly applies MIL, disregarding the fact that many data points are noisy. The second one addresses this shortcoming by integrating a noise detection component. 3.1 Model 1: MIL from GloVe2 (Pennington et al., 2014). Forward fi and backward bi states of BiLSTM are fed into the classifier described below. Entity embeddings In this work, we use a simple and scalable approach which involves computing entity embeddings on the fly using associated types. For instance, the TV episode B ILL C LINTON is associated with several types including BASE . TYPE ONTOLOGY. NON AGENT and TV. TV SERIES EPISODE. Specifically, in order to produce an entity embedding, each type t is assigned a vector t ∈ Rdt . We then compute a vector for entity e as e = ReLU(We 1 X t + be ), |Te | t∈Te where Te is the set of e’s types, and W"
P19-1400,P11-1138,0,0.685285,"ethod, jointly learning to detect noise and link entities, greatly outperforms the surface matching baseline. For a subset of entity categories, it even approaches the performance of supervised learning. 1 Introduction Entity linking (EL) is the task of linking potentially ambiguous textual mentions to the corresponding entities in a knowledge base. Accurate entity linking is crucial in many natural language processing tasks, including information extraction (Hoffart et al., 2011) and question answering (Yih et al., 2015). Though there has been significant progress in entity linking recently (Ratinov et al., 2011; Hoffart et al., 2011; Chisholm and Hachey, 2015; Globerson et al., 2016; Yamada et al., 2017; Ganea and Hofmann, 2017; Le and Titov, 2018), previous work has focused on supervised learning. Annotated data necessary for supervised learning is available for certain knowledge bases and domains. For example, one can directly use webpages linking to Wikipedia to learn a Wikipedia linker. Similarly, there exist domain-specific sets of manually annotated documents (e.g., AIDACoNLL news dataset for YAGO (Hoffart et al., 2011)). However, for many ontologies and domains annotation is not available or"
P19-1400,Q17-1028,0,0.225009,"hing baseline. For a subset of entity categories, it even approaches the performance of supervised learning. 1 Introduction Entity linking (EL) is the task of linking potentially ambiguous textual mentions to the corresponding entities in a knowledge base. Accurate entity linking is crucial in many natural language processing tasks, including information extraction (Hoffart et al., 2011) and question answering (Yih et al., 2015). Though there has been significant progress in entity linking recently (Ratinov et al., 2011; Hoffart et al., 2011; Chisholm and Hachey, 2015; Globerson et al., 2016; Yamada et al., 2017; Ganea and Hofmann, 2017; Le and Titov, 2018), previous work has focused on supervised learning. Annotated data necessary for supervised learning is available for certain knowledge bases and domains. For example, one can directly use webpages linking to Wikipedia to learn a Wikipedia linker. Similarly, there exist domain-specific sets of manually annotated documents (e.g., AIDACoNLL news dataset for YAGO (Hoffart et al., 2011)). However, for many ontologies and domains annotation is not available or limited (e.g., law). Our goal is to develop a method which does not rely on any training data"
P19-1400,P15-1128,0,0.0358095,"model: it lets the model detect and disregard examples which are likely to be noisy. Our method, jointly learning to detect noise and link entities, greatly outperforms the surface matching baseline. For a subset of entity categories, it even approaches the performance of supervised learning. 1 Introduction Entity linking (EL) is the task of linking potentially ambiguous textual mentions to the corresponding entities in a knowledge base. Accurate entity linking is crucial in many natural language processing tasks, including information extraction (Hoffart et al., 2011) and question answering (Yih et al., 2015). Though there has been significant progress in entity linking recently (Ratinov et al., 2011; Hoffart et al., 2011; Chisholm and Hachey, 2015; Globerson et al., 2016; Yamada et al., 2017; Ganea and Hofmann, 2017; Le and Titov, 2018), previous work has focused on supervised learning. Annotated data necessary for supervised learning is available for certain knowledge bases and domains. For example, one can directly use webpages linking to Wikipedia to learn a Wikipedia linker. Similarly, there exist domain-specific sets of manually annotated documents (e.g., AIDACoNLL news dataset for YAGO (Hof"
P19-1551,P15-2047,0,0.0525893,"ask. Ablation studies emphasize the importance of both stochasticity and constraining latent structures to be projective trees. 1 Introduction Discrete structures are ubiquitous in the study of natural languages, for example in morphology, syntax and discourse analysis. In natural language processing, they are often used to inject linguistic prior knowledge into statistical models. For examples, syntactic structures have been shown beneficial in question answering (Cui et al., 2005), sentiment analysis (Socher et al., 2013), machine translation (Bastings et al., 2017) and relation extraction (Liu et al., 2015), among others. However, linguistic tools producing these structured representations (e.g., syntactic parsers) are not available for many languages and not robust when applied outside of the domain they were trained on (Petrov et al., 2010; Foster et al., 2011). Moreover, linguistic structures do not always seem suitable in downstream applications, with simpler alternatives sometimes yielding better performance (Wang et al., 2018). Indeed, a parallel line of work focused on inducing task-specific structured representations of language (Naradowsky et al., 2012; Yogatama et al., 2017; Kim et al."
P19-1551,P14-5010,0,0.00876284,"Missing"
P19-1551,W18-5452,0,0.0377044,"related to ours. Differentiable structured layers in neural networks have been explored for semi-supervised parsing, for example by learning an auxiliary task on unlabelled data (Peng et al., 2018) or using a variational autoencoder (Corro and Titov, 2019). Besides research focused on inducing taskspecific structures, another line of work, grammar induction, focused on unsupervised induction of linguistic structures. These methods typically rely on unlabeled texts and are evaluated by comparing the induced structures to actual syntactic annotation (Klein and Manning, 2005; Shen et al., 2018; Htut et al., 2018). 8 Conclusions We introduced a novel approach to latent tree learning: a relaxed version of stochastic differentiable dynamic programming which allows for efficient sampling of projective dependency trees and enables end-to-end differentiation. We demonstrate effectiveness of our approach on both synthetic and real tasks. The analyses confirm importance of the tree constraint. Future work will investigate constituency structures and new neural architectures for latent structure incorporation. Acknowledgments We thank Maximin Coavoux and Serhii Havrylov for their comments and suggestions. We a"
P19-1551,D17-1159,1,0.900762,"Missing"
P19-1551,N18-4013,0,0.109327,"is that we stochastically sample global structures and do it in a differentiable fashion. For example, the structured attention method (Kim et al., 2017; Liu and Lapata, 2018) does not sample entire trees but rather computes arc marginals, and hence does not faithfully represent higher-order statistics. Much of other previous work relies either on reinforce5508 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5508–5521 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics ment learning (Yogatama et al., 2017; Nangia and Bowman, 2018; Williams et al., 2018a) or does not treat the latent structure as a random variable (Peng et al., 2018). Niculae et al. (2018) marginalizes over latent structures, however, this necessitates strong sparsity assumptions on the posterior distributions which may inject undesirable biases in the model. Overall, differential dynamic programming has not been actively studied in the task-specific tree induction context. Most previous work also focused on constituent trees rather than dependency ones. We study properties of our approach on a synthetic structure induction task and experiment on senti"
P19-1551,Q18-1005,0,0.0685494,"others. However, linguistic tools producing these structured representations (e.g., syntactic parsers) are not available for many languages and not robust when applied outside of the domain they were trained on (Petrov et al., 2010; Foster et al., 2011). Moreover, linguistic structures do not always seem suitable in downstream applications, with simpler alternatives sometimes yielding better performance (Wang et al., 2018). Indeed, a parallel line of work focused on inducing task-specific structured representations of language (Naradowsky et al., 2012; Yogatama et al., 2017; Kim et al., 2017; Liu and Lapata, 2018; Niculae et al., 2018). In these approaches, no syntactic or semantic annotation is needed for training: representation is induced from scratch in an end-to-end fashion, in such a way as to benefit a given downstream task. In other words, these approaches provide an inductive bias specifying that (hierarchical) structures are appropriate for representing a natural language, but do not make any further assumptions regarding what the structures represent. Structures induced in this way, though useful for the task, tend not to resemble any accepted syntactic or semantic formalisms (Williams et a"
P19-1551,D12-1074,0,0.0324642,"s et al., 2017) and relation extraction (Liu et al., 2015), among others. However, linguistic tools producing these structured representations (e.g., syntactic parsers) are not available for many languages and not robust when applied outside of the domain they were trained on (Petrov et al., 2010; Foster et al., 2011). Moreover, linguistic structures do not always seem suitable in downstream applications, with simpler alternatives sometimes yielding better performance (Wang et al., 2018). Indeed, a parallel line of work focused on inducing task-specific structured representations of language (Naradowsky et al., 2012; Yogatama et al., 2017; Kim et al., 2017; Liu and Lapata, 2018; Niculae et al., 2018). In these approaches, no syntactic or semantic annotation is needed for training: representation is induced from scratch in an end-to-end fashion, in such a way as to benefit a given downstream task. In other words, these approaches provide an inductive bias specifying that (hierarchical) structures are appropriate for representing a natural language, but do not make any further assumptions regarding what the structures represent. Structures induced in this way, though useful for the task, tend not to resemb"
P19-1551,D18-1108,0,0.334197,"istic tools producing these structured representations (e.g., syntactic parsers) are not available for many languages and not robust when applied outside of the domain they were trained on (Petrov et al., 2010; Foster et al., 2011). Moreover, linguistic structures do not always seem suitable in downstream applications, with simpler alternatives sometimes yielding better performance (Wang et al., 2018). Indeed, a parallel line of work focused on inducing task-specific structured representations of language (Naradowsky et al., 2012; Yogatama et al., 2017; Kim et al., 2017; Liu and Lapata, 2018; Niculae et al., 2018). In these approaches, no syntactic or semantic annotation is needed for training: representation is induced from scratch in an end-to-end fashion, in such a way as to benefit a given downstream task. In other words, these approaches provide an inductive bias specifying that (hierarchical) structures are appropriate for representing a natural language, but do not make any further assumptions regarding what the structures represent. Structures induced in this way, though useful for the task, tend not to resemble any accepted syntactic or semantic formalisms (Williams et al., 2018a). Our approac"
P19-1551,D16-1244,0,0.0980072,"Missing"
P19-1551,P18-1173,0,0.262522,"Missing"
P19-1551,D10-1069,0,0.0915584,"y, syntax and discourse analysis. In natural language processing, they are often used to inject linguistic prior knowledge into statistical models. For examples, syntactic structures have been shown beneficial in question answering (Cui et al., 2005), sentiment analysis (Socher et al., 2013), machine translation (Bastings et al., 2017) and relation extraction (Liu et al., 2015), among others. However, linguistic tools producing these structured representations (e.g., syntactic parsers) are not available for many languages and not robust when applied outside of the domain they were trained on (Petrov et al., 2010; Foster et al., 2011). Moreover, linguistic structures do not always seem suitable in downstream applications, with simpler alternatives sometimes yielding better performance (Wang et al., 2018). Indeed, a parallel line of work focused on inducing task-specific structured representations of language (Naradowsky et al., 2012; Yogatama et al., 2017; Kim et al., 2017; Liu and Lapata, 2018; Niculae et al., 2018). In these approaches, no syntactic or semantic annotation is needed for training: representation is induced from scratch in an end-to-end fashion, in such a way as to benefit a given down"
P19-1551,Q18-1019,0,0.124189,"l., 2017; Liu and Lapata, 2018) does not sample entire trees but rather computes arc marginals, and hence does not faithfully represent higher-order statistics. Much of other previous work relies either on reinforce5508 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5508–5521 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics ment learning (Yogatama et al., 2017; Nangia and Bowman, 2018; Williams et al., 2018a) or does not treat the latent structure as a random variable (Peng et al., 2018). Niculae et al. (2018) marginalizes over latent structures, however, this necessitates strong sparsity assumptions on the posterior distributions which may inject undesirable biases in the model. Overall, differential dynamic programming has not been actively studied in the task-specific tree induction context. Most previous work also focused on constituent trees rather than dependency ones. We study properties of our approach on a synthetic structure induction task and experiment on sentiment classification (Socher et al., 2013) and natural language inference (Bowman et al., 2015). Our experiments confirm that the"
P19-1551,N18-1101,0,0.055705,"Lapata, 2018; Niculae et al., 2018). In these approaches, no syntactic or semantic annotation is needed for training: representation is induced from scratch in an end-to-end fashion, in such a way as to benefit a given downstream task. In other words, these approaches provide an inductive bias specifying that (hierarchical) structures are appropriate for representing a natural language, but do not make any further assumptions regarding what the structures represent. Structures induced in this way, though useful for the task, tend not to resemble any accepted syntactic or semantic formalisms (Williams et al., 2018a). Our approach falls under this category. In our method, projective dependency trees (see Figure 3 for examples) are treated as latent variables within a probabilistic model. We rely on differentiable dynamic programming (Mensch and Blondel, 2018) which allows for efficient sampling of dependency trees (Corro and Titov, 2019). Intuitively, sampling a tree involves stochastically perturbing dependency weights and then running a relaxed form of the Eisner dynamic programming algortihm (Eisner, 1996). A sampled tree (or its continuous relaxation) can then be straightforwardly integrated in a ne"
P19-1551,D13-1170,0,0.0269819,"language inference tasks. We also study its properties on a synthetic structure induction task. Ablation studies emphasize the importance of both stochasticity and constraining latent structures to be projective trees. 1 Introduction Discrete structures are ubiquitous in the study of natural languages, for example in morphology, syntax and discourse analysis. In natural language processing, they are often used to inject linguistic prior knowledge into statistical models. For examples, syntactic structures have been shown beneficial in question answering (Cui et al., 2005), sentiment analysis (Socher et al., 2013), machine translation (Bastings et al., 2017) and relation extraction (Liu et al., 2015), among others. However, linguistic tools producing these structured representations (e.g., syntactic parsers) are not available for many languages and not robust when applied outside of the domain they were trained on (Petrov et al., 2010; Foster et al., 2011). Moreover, linguistic structures do not always seem suitable in downstream applications, with simpler alternatives sometimes yielding better performance (Wang et al., 2018). Indeed, a parallel line of work focused on inducing task-specific structured"
P19-1551,D18-1509,0,0.0682415,"wn beneficial in question answering (Cui et al., 2005), sentiment analysis (Socher et al., 2013), machine translation (Bastings et al., 2017) and relation extraction (Liu et al., 2015), among others. However, linguistic tools producing these structured representations (e.g., syntactic parsers) are not available for many languages and not robust when applied outside of the domain they were trained on (Petrov et al., 2010; Foster et al., 2011). Moreover, linguistic structures do not always seem suitable in downstream applications, with simpler alternatives sometimes yielding better performance (Wang et al., 2018). Indeed, a parallel line of work focused on inducing task-specific structured representations of language (Naradowsky et al., 2012; Yogatama et al., 2017; Kim et al., 2017; Liu and Lapata, 2018; Niculae et al., 2018). In these approaches, no syntactic or semantic annotation is needed for training: representation is induced from scratch in an end-to-end fashion, in such a way as to benefit a given downstream task. In other words, these approaches provide an inductive bias specifying that (hierarchical) structures are appropriate for representing a natural language, but do not make any further"
P19-1551,D15-1075,0,\N,Missing
P19-1551,C96-1058,0,\N,Missing
P19-1551,D18-1544,0,\N,Missing
P19-1580,P17-1080,0,0.0432762,"U, WMT) retained more readily in the lower layers, while decoder-encoder attention heads are retained in the higher layers. This suggests that lower layers of the Transformer’s decoder are mostly responsible for language modeling, while higher layers are mostly responsible for conditioning on the source sentence. These observations are similar for both datasets we use. 7 Related work One popular approach to the analysis of NMT representations is to evaluate how informative they are for various linguistic tasks. Different levels of linguistic analysis have been considered including morphology (Belinkov et al., 2017a; Dalvi et al., 2017; Bisazza and Tump, 2018), syntax (Shi et al., 2016) and semantics (Hill et al., 2017; Belinkov et al., 2017b; Raganato and Tiedemann, 2018). Bisazza and Tump (2018) showed that the target language determines which information gets encoded. This agrees with our results for different domains on the English-Russian translation task in Section 5.2.2. There we observed that attention heads are more likely to track syntactic relations requiring more complex agreement in the target language (in this case the subject-verb relation). An alternative method to study the ability of l"
P19-1580,I17-1004,0,0.047812,"age (in this case the subject-verb relation). An alternative method to study the ability of language models and machine translation models to capture hierarchical information is to test their sensitivity to specific grammatical errors (Linzen et al., 2016; Gulordava et al., 2018; Tran et al., 2018; Sennrich, 2017; Tang et al., 2018). While this line of work has shown that NMT models, including the Transformer, do learn some syntactic structures, our work provides further insight into the role of multi-head attention. There are several works analyzing attention weights of different NMT models (Ghader and Monz, 2017; Voita et al., 2018; Tang et al., 2018; 5804 Raganato and Tiedemann, 2018). Raganato and Tiedemann (2018) use the self-attention weights of the Transformer’s encoder to induce a tree structure for each sentence and compute the unlabeled attachment score of these trees. However they do not evaluate specific syntactic relations (i.e. labeled attachment scores) or consider how different heads specialize to specific dependency relations. Recently Bau et al. (2019) proposed a method for identifying important individual neurons in NMT models. They show that similar important neurons emerge in diffe"
P19-1580,N18-1108,0,0.0148612,"isazza and Tump (2018) showed that the target language determines which information gets encoded. This agrees with our results for different domains on the English-Russian translation task in Section 5.2.2. There we observed that attention heads are more likely to track syntactic relations requiring more complex agreement in the target language (in this case the subject-verb relation). An alternative method to study the ability of language models and machine translation models to capture hierarchical information is to test their sensitivity to specific grammatical errors (Linzen et al., 2016; Gulordava et al., 2018; Tran et al., 2018; Sennrich, 2017; Tang et al., 2018). While this line of work has shown that NMT models, including the Transformer, do learn some syntactic structures, our work provides further insight into the role of multi-head attention. There are several works analyzing attention weights of different NMT models (Ghader and Monz, 2017; Voita et al., 2018; Tang et al., 2018; 5804 Raganato and Tiedemann, 2018). Raganato and Tiedemann (2018) use the self-attention weights of the Transformer’s encoder to induce a tree structure for each sentence and compute the unlabeled attachment score of"
P19-1580,Q16-1037,0,0.0407968,"d Tiedemann, 2018). Bisazza and Tump (2018) showed that the target language determines which information gets encoded. This agrees with our results for different domains on the English-Russian translation task in Section 5.2.2. There we observed that attention heads are more likely to track syntactic relations requiring more complex agreement in the target language (in this case the subject-verb relation). An alternative method to study the ability of language models and machine translation models to capture hierarchical information is to test their sensitivity to specific grammatical errors (Linzen et al., 2016; Gulordava et al., 2018; Tran et al., 2018; Sennrich, 2017; Tang et al., 2018). While this line of work has shown that NMT models, including the Transformer, do learn some syntactic structures, our work provides further insight into the role of multi-head attention. There are several works analyzing attention weights of different NMT models (Ghader and Monz, 2017; Voita et al., 2018; Tang et al., 2018; 5804 Raganato and Tiedemann, 2018). Raganato and Tiedemann (2018) use the self-attention weights of the Transformer’s encoder to induce a tree structure for each sentence and compute the unlabe"
P19-1580,L18-1275,0,0.0155653,", German and French. For each language pair, we use the same number of sentence pairs from WMT data to control for the amount of training data and train Transformer models with the same numbers of parameters. We use 2.5m sentence pairs, corresponding to the amount of English–Russian parallel training data (excluding UN and Paracrawl). In Section 5.2 we use the same held-out data for all language pairs; these are 50k English sentences taken from the WMT EN-FR data not used in training. For English-Russian, we perform additional experiments using the publicly available OpenSubtitles2018 corpus (Lison et al., 2018) to evaluate the impact of domains on our results. In Section 6 we concentrate on English-Russian and two domains: WMT and OpenSubtitles. Model hyperparameters, preprocessing and training details are provided in appendix B. 4 Identifying Important Heads Previous work analyzing how representations are formed by the Transformer’s multi-head attention mechanism focused on either the average or the maximum attention weights over all heads (Voita et al., 2018; Tang et al., 2018), but neither method explicitly takes into account the varying importance of different heads. Also, this obscures the role"
P19-1580,P14-5010,0,0.00588199,"Missing"
P19-1580,W18-5431,0,0.278397,"ws the encoderdecoder framework using stacked multi-head selfattention and fully connected layers. Multi-head attention was shown to make more efficient use of the model’s capacity: performance of the model with 8 heads is almost 1 BLEU point higher than that of a model of the same size with single-head attention (Vaswani et al., 2017). The Transformer achieved state-of-the-art results in recent shared translation tasks (Bojar et al., 2018; Niehues et al., 2018). Despite the model’s widespread adoption and recent attempts to investigate the kinds of information learned by the model’s encoder (Raganato and Tiedemann, 2018), the analysis of multi-head attention and its importance 1 We release code at https://github.com/ lena-voita/the-story-of-heads. • Can we significantly reduce the number of attention heads while preserving translation quality? We start by identifying the most important heads in each encoder layer using layer-wise relevance propagation (Ding et al., 2017). For heads judged to be important, we then attempt to characterize the roles they perform. We observe the following types of role: positional (heads attending to an adjacent token), syntactic (heads attending to tokens in a specific syntactic"
P19-1580,E17-2060,1,0.860043,"et language determines which information gets encoded. This agrees with our results for different domains on the English-Russian translation task in Section 5.2.2. There we observed that attention heads are more likely to track syntactic relations requiring more complex agreement in the target language (in this case the subject-verb relation). An alternative method to study the ability of language models and machine translation models to capture hierarchical information is to test their sensitivity to specific grammatical errors (Linzen et al., 2016; Gulordava et al., 2018; Tran et al., 2018; Sennrich, 2017; Tang et al., 2018). While this line of work has shown that NMT models, including the Transformer, do learn some syntactic structures, our work provides further insight into the role of multi-head attention. There are several works analyzing attention weights of different NMT models (Ghader and Monz, 2017; Voita et al., 2018; Tang et al., 2018; 5804 Raganato and Tiedemann, 2018). Raganato and Tiedemann (2018) use the self-attention weights of the Transformer’s encoder to induce a tree structure for each sentence and compute the unlabeled attachment score of these trees. However they do not ev"
P19-1580,P16-1162,1,0.561712,"Missing"
P19-1580,D16-1159,0,0.0459911,"ntion heads are retained in the higher layers. This suggests that lower layers of the Transformer’s decoder are mostly responsible for language modeling, while higher layers are mostly responsible for conditioning on the source sentence. These observations are similar for both datasets we use. 7 Related work One popular approach to the analysis of NMT representations is to evaluate how informative they are for various linguistic tasks. Different levels of linguistic analysis have been considered including morphology (Belinkov et al., 2017a; Dalvi et al., 2017; Bisazza and Tump, 2018), syntax (Shi et al., 2016) and semantics (Hill et al., 2017; Belinkov et al., 2017b; Raganato and Tiedemann, 2018). Bisazza and Tump (2018) showed that the target language determines which information gets encoded. This agrees with our results for different domains on the English-Russian translation task in Section 5.2.2. There we observed that attention heads are more likely to track syntactic relations requiring more complex agreement in the target language (in this case the subject-verb relation). An alternative method to study the ability of language models and machine translation models to capture hierarchical inf"
P19-1580,D18-1458,1,0.942299,"Rest Can Be Pruned Elena Voita1,2 David Talbot1 Fedor Moiseev1,5 Rico Sennrich3,4 Ivan Titov3,2 1 Yandex, Russia 2 University of Amsterdam, Netherlands 3 University of Edinburgh, Scotland 4 University of Zurich, Switzerland 5 Moscow Institute of Physics and Technology, Russia {lena-voita, talbot, femoiseev}@yandex-team.ru rico.sennrich@ed.ac.uk ititov@inf.ed.ac.uk Abstract for translation is challenging. Previous analysis of multi-head attention considered the average of attention weights over all heads at a given position or focused only on the maximum attention weights (Voita et al., 2018; Tang et al., 2018), but neither method explicitly takes into account the varying importance of different heads. Also, this obscures the roles played by individual heads which, as we show, influence the generated translations to differing extents. We attempt to answer the following questions: Multi-head self-attention is a key component of the Transformer, a state-of-the-art architecture for neural machine translation. In this work we evaluate the contribution made by individual attention heads in the encoder to the overall performance of the model and analyze the roles played by them. We find that the most impo"
P19-1580,W18-6304,1,0.947178,"Rest Can Be Pruned Elena Voita1,2 David Talbot1 Fedor Moiseev1,5 Rico Sennrich3,4 Ivan Titov3,2 1 Yandex, Russia 2 University of Amsterdam, Netherlands 3 University of Edinburgh, Scotland 4 University of Zurich, Switzerland 5 Moscow Institute of Physics and Technology, Russia {lena-voita, talbot, femoiseev}@yandex-team.ru rico.sennrich@ed.ac.uk ititov@inf.ed.ac.uk Abstract for translation is challenging. Previous analysis of multi-head attention considered the average of attention weights over all heads at a given position or focused only on the maximum attention weights (Voita et al., 2018; Tang et al., 2018), but neither method explicitly takes into account the varying importance of different heads. Also, this obscures the roles played by individual heads which, as we show, influence the generated translations to differing extents. We attempt to answer the following questions: Multi-head self-attention is a key component of the Transformer, a state-of-the-art architecture for neural machine translation. In this work we evaluate the contribution made by individual attention heads in the encoder to the overall performance of the model and analyze the roles played by them. We find that the most impo"
P19-1580,D18-1503,0,0.0632653,"Missing"
P19-1580,P18-1117,1,0.937646,"e Heavy Lifting, the Rest Can Be Pruned Elena Voita1,2 David Talbot1 Fedor Moiseev1,5 Rico Sennrich3,4 Ivan Titov3,2 1 Yandex, Russia 2 University of Amsterdam, Netherlands 3 University of Edinburgh, Scotland 4 University of Zurich, Switzerland 5 Moscow Institute of Physics and Technology, Russia {lena-voita, talbot, femoiseev}@yandex-team.ru rico.sennrich@ed.ac.uk ititov@inf.ed.ac.uk Abstract for translation is challenging. Previous analysis of multi-head attention considered the average of attention weights over all heads at a given position or focused only on the maximum attention weights (Voita et al., 2018; Tang et al., 2018), but neither method explicitly takes into account the varying importance of different heads. Also, this obscures the roles played by individual heads which, as we show, influence the generated translations to differing extents. We attempt to answer the following questions: Multi-head self-attention is a key component of the Transformer, a state-of-the-art architecture for neural machine translation. In this work we evaluate the contribution made by individual attention heads in the encoder to the overall performance of the model and analyze the roles played by them. We fin"
Q16-1008,D11-1033,0,0.0565742,"t translation task is used as a form of prior knowledge. Various techniques can then be used for adaptation. For example, one approach is to combine a system trained on the in-domain data with another general-domain system trained on the rest of the data (e.g., see Koehn and Schroeder (2007), Foster et al. (2010), Bisazza et al. (2011), Sennrich (2012b), Razmara et al. (2012), Sennrich et al. (2013), Haddow (2013), Joty et al. (2015)). Rather than using the entire training data, it is also common to combine the in-domain system with a system trained on a selected subset of the data (e.g., see Axelrod et al. (2011), Koehn and Haddow (2012), Duh et al. (2013), Kirchhoff and Bilmes (2014), Cuong and Sima’an (2014b)). In some other cases, the prior knowledge lies in meta-information about the training data. This could be document-annotated training information (Eidelman et al., 2012; Hu et al., 2014; Hasler et al., 2014; Su et al., 2015; Zhang et al., 2014), and domainannotated sub-corpora (Chiang et al., 2011; Sennrich, 2012b; Chen et al., 2013b; Carpuat et al., 2014; Cuong and Sima’an, 2015). Some recent approaches perform adaptation by exploiting a target domain development, or even only the source side"
Q16-1008,2011.iwslt-evaluation.18,0,0.138052,"specialized phrases are to individual induced sub-domains; (3) estimating feature weights on out-of-domain data (rather than on the target domain). We conduct experiments on three language pairs and a number of different domains. We observe consistent improvements over a baseline which does not explicitly reward domain invariance. 1 Introduction Mismatch in phrase translation distributions between test data (target domain) and train data is known to harm performance of statistical translation systems (Irvine et al., 2013; Carpuat et al., 2014). Domain-adaptation methods (Foster et al., 2010; Bisazza et al., 2011; Sennrich, 2012b; Razmara et al., 2012; Sennrich et al., 2013; Haddow, 2013; Joty et al., 2015) aim to specialize a system estimated on out-of-domain training data to a target domain represented by a small data sample. In practice, however, the target domain may not be known When the target domain is unknown at training time, the system could be trained to make safer choices, preferring translations which are likely to work across different domains. For example, when translating from English to Russian, the most natural translation for the word ‘code’ would be highly dependent on the domain ("
Q16-1008,J93-2003,0,0.085355,"P (z) z 1 2 P (e|z) X P (f , a|e, z) a  X 1 + P (f |z) P (e, a0 |f , z) . 2 0 3. if the e-to-f direction is chosen then generate the pair relying on P (e |z)P (f |e, z); 4. otherwise, use P (f |z)P (e |f , z). 2 As we aim for a simple approach, our TMs are computed through the introduction of hidden alignments a and a0 in f -to-e and e-to-f P directions respectively, in which P (f | e, z) = a P (f , a |e, z) P 0 and P (e |f , z) = a0 P (e, a |f , z). To make the marginalization of alignments tractable, we restrict P (f , a |e, z) and P (e, a0 |f , z) to the same assumptions as IBM Model 1 (Brown et al., 1993) (i.e., a multiplication of translation of lexical probabilities with respect to latent subdomains). 1. generate the domain z from the prior P (z); 2. choose the generation direction: f -to-e or e-tof , with equal probability; 1 (4) a As there is no closed-form solution, we use the expectation-maximization (EM) algorithm (Dempster et al., 1977). In the E-step, we compute the posterior distribu2 Note that we effectively average between them which is reasonable, as there is no reason to give preference to any of them. tions P (a, z |e, f ) and P (a0 , z |e, f ) as follows  P (a, z |e, f ) ∝ P"
Q16-1008,P13-1141,0,0.026014,"3), Kirchhoff and Bilmes (2014), Cuong and Sima’an (2014b)). In some other cases, the prior knowledge lies in meta-information about the training data. This could be document-annotated training information (Eidelman et al., 2012; Hu et al., 2014; Hasler et al., 2014; Su et al., 2015; Zhang et al., 2014), and domainannotated sub-corpora (Chiang et al., 2011; Sennrich, 2012b; Chen et al., 2013b; Carpuat et al., 2014; Cuong and Sima’an, 2015). Some recent approaches perform adaptation by exploiting a target domain development, or even only the source side of the development set (Sennrich, 2012a; Carpuat et al., 2013; Carpuat et al., 2014; Mansour and Ney, 2014). Recently, there was some research on adapting simultaneously to multiple domains, the goal related to ours (Clark et al., 2012; Sennrich, 2012a). For instance, Clark et al. (2012) augment a phrase-based MT system with various domain indicator features to build a single system that performs well across a range of domains. Sennrich (2012a) proposed to cluster training data in an unsupervised fashion to build mixture models that yield good performance on multiple test domains. However, their approaches are very different from ours, that is minimizin"
Q16-1008,W14-3363,0,0.210242,"from the training data only; (2) introducing features which measure how specialized phrases are to individual induced sub-domains; (3) estimating feature weights on out-of-domain data (rather than on the target domain). We conduct experiments on three language pairs and a number of different domains. We observe consistent improvements over a baseline which does not explicitly reward domain invariance. 1 Introduction Mismatch in phrase translation distributions between test data (target domain) and train data is known to harm performance of statistical translation systems (Irvine et al., 2013; Carpuat et al., 2014). Domain-adaptation methods (Foster et al., 2010; Bisazza et al., 2011; Sennrich, 2012b; Razmara et al., 2012; Sennrich et al., 2013; Haddow, 2013; Joty et al., 2015) aim to specialize a system estimated on out-of-domain training data to a target domain represented by a small data sample. In practice, however, the target domain may not be known When the target domain is unknown at training time, the system could be trained to make safer choices, preferring translations which are likely to work across different domains. For example, when translating from English to Russian, the most natural tra"
Q16-1008,N13-1114,0,0.221895,"ned on a development set. Importantly, we show that there is no noteworthy benefit from tuning the weights on a sample from the target domain. It is enough to tune them on a mixed-domain dataset sufficiently different from the training data. We attribute this attractive property to the fact that our features, unlike the ones typically considered in standard domain-adaptation work, are generic and only affect the amount of risk our system takes. In contrast, for example, in Eidelman et al. (2012), Chiang et al. (2011), Hu et al. (2014), Hasler et al. (2014), Su et al. (2015), Sennrich (2012b), Chen et al. (2013b), and Carpuat et al. (2014), features capture similarities between a target domain and each of the training subdomains. Clearly, domain adaptation with such rich features, though potentially more powerful, would not be possible without a development set closely matching the target domain. We conduct our experiments on three language pairs and explore adaptation to 9 domain adaptation tasks in total. We observe significant and consistent performance improvements over the baseline domain-agnostic systems. This result confirms that our two features, and the latent subdomains they are computed f"
Q16-1008,P13-1126,0,0.225624,"ned on a development set. Importantly, we show that there is no noteworthy benefit from tuning the weights on a sample from the target domain. It is enough to tune them on a mixed-domain dataset sufficiently different from the training data. We attribute this attractive property to the fact that our features, unlike the ones typically considered in standard domain-adaptation work, are generic and only affect the amount of risk our system takes. In contrast, for example, in Eidelman et al. (2012), Chiang et al. (2011), Hu et al. (2014), Hasler et al. (2014), Su et al. (2015), Sennrich (2012b), Chen et al. (2013b), and Carpuat et al. (2014), features capture similarities between a target domain and each of the training subdomains. Clearly, domain adaptation with such rich features, though potentially more powerful, would not be possible without a development set closely matching the target domain. We conduct our experiments on three language pairs and explore adaptation to 9 domain adaptation tasks in total. We observe significant and consistent performance improvements over the baseline domain-agnostic systems. This result confirms that our two features, and the latent subdomains they are computed f"
Q16-1008,N12-1047,0,0.0394474,"of-the-art phrase-based system. The Baseline system includes MOSES (Koehn et al., 2007) baseline feature functions, plus eight hierarchical lexicalized reordering model feature functions (Galley and Manning, 2008). The training data is first word-aligned using GIZA++ (Och and Ney, 2003) and then symmetrized with grow(-diag)final-and (Koehn et al., 2003). We limit the phrase length to the maximum of seven words. The lan104 In practice, we found that using this hard version leads to better performance.5 4.3 Alternative tuning scenarios In order to tune all systems, we use the k-best batch MIRA (Cherry and Foster, 2012). We report the translation accuracy with three metrics - BLEU 5 A more principled alternative would be to use posterior regularization (Ganchev et al., 2009). Task System BLEU↑ /∆ English-French Baseline 21.4 Professional & Business Services Our System 21.5/+0.1 Baseline 39.9 Leisure, Tourism and Arts Our System 40.8/+0.9 English-Spanish Baseline 32.5 Financials Our System 32.8/+0.3 Baseline 24.4 Professional & Business Services Our System 24.8/+0.4 Baseline 33.3 Legal Services Our System 33.8/+0.5 English-German Baseline 22.8 Computer Software Our System 23.1/+0.3 Baseline 20.5 Computer Hard"
Q16-1008,P11-2080,0,0.0930478,"likely safer to use. 100 Weights for these features, alongside all other standard features, are tuned on a development set. Importantly, we show that there is no noteworthy benefit from tuning the weights on a sample from the target domain. It is enough to tune them on a mixed-domain dataset sufficiently different from the training data. We attribute this attractive property to the fact that our features, unlike the ones typically considered in standard domain-adaptation work, are generic and only affect the amount of risk our system takes. In contrast, for example, in Eidelman et al. (2012), Chiang et al. (2011), Hu et al. (2014), Hasler et al. (2014), Su et al. (2015), Sennrich (2012b), Chen et al. (2013b), and Carpuat et al. (2014), features capture similarities between a target domain and each of the training subdomains. Clearly, domain adaptation with such rich features, though potentially more powerful, would not be possible without a development set closely matching the target domain. We conduct our experiments on three language pairs and explore adaptation to 9 domain adaptation tasks in total. We observe significant and consistent performance improvements over the baseline domain-agnostic sys"
Q16-1008,P11-2031,0,0.052164,"Missing"
Q16-1008,2012.amta-papers.4,0,0.0206664,"nnotated training information (Eidelman et al., 2012; Hu et al., 2014; Hasler et al., 2014; Su et al., 2015; Zhang et al., 2014), and domainannotated sub-corpora (Chiang et al., 2011; Sennrich, 2012b; Chen et al., 2013b; Carpuat et al., 2014; Cuong and Sima’an, 2015). Some recent approaches perform adaptation by exploiting a target domain development, or even only the source side of the development set (Sennrich, 2012a; Carpuat et al., 2013; Carpuat et al., 2014; Mansour and Ney, 2014). Recently, there was some research on adapting simultaneously to multiple domains, the goal related to ours (Clark et al., 2012; Sennrich, 2012a). For instance, Clark et al. (2012) augment a phrase-based MT system with various domain indicator features to build a single system that performs well across a range of domains. Sennrich (2012a) proposed to cluster training data in an unsupervised fashion to build mixture models that yield good performance on multiple test domains. However, their approaches are very different from ours, that is minimizing risk associated with choosing domain-specific translations. Moreover, the present work deviates radically from earlier work in that it explores the scenario where no prior"
Q16-1008,P13-1077,0,0.0214107,"i−n , z) and P (f |z) = = i P (ei | Q j−1 Here, the notation ei−1 j P (fj |fj−n , z). i−n j−1 and fj−n is used to denote the history of length n for the source and target words ei and fj , respectively. Training. For training, we maximize the loglikelihood L of the data L= Formally, it is a uniform mixture of the generative processes for the two potential translation di1 Doing that requires incorporating into the model additional hidden variables encoding phrase segmentation (DeNero et al., 2006). This would significantly complicate inference (Mylonakis and Sima’an, 2008; Neubig et al., 2011; Cohn and Haffari, 2013). 102 X e,f log X P (z) z 1 2 P (e|z) X P (f , a|e, z) a  X 1 + P (f |z) P (e, a0 |f , z) . 2 0 3. if the e-to-f direction is chosen then generate the pair relying on P (e |z)P (f |e, z); 4. otherwise, use P (f |z)P (e |f , z). 2 As we aim for a simple approach, our TMs are computed through the introduction of hidden alignments a and a0 in f -to-e and e-to-f P directions respectively, in which P (f | e, z) = a P (f , a |e, z) P 0 and P (e |f , z) = a0 P (e, a |f , z). To make the marginalization of alignments tractable, we restrict P (f , a |e, z) and P (e, a0 |f , z) to the same assumptio"
Q16-1008,D14-1062,1,0.846718,"Missing"
Q16-1008,C14-1182,1,0.889066,"Missing"
Q16-1008,N15-1043,1,0.880913,"Missing"
Q16-1008,W06-3105,0,0.03583,"3) + P (f |z)P (e |f , z) . 2 We use standard nth -order Markov model for P Q(e |z) andi−1P (f |z), in which P (e |z) ei−n , z) and P (f |z) = = i P (ei | Q j−1 Here, the notation ei−1 j P (fj |fj−n , z). i−n j−1 and fj−n is used to denote the history of length n for the source and target words ei and fj , respectively. Training. For training, we maximize the loglikelihood L of the data L= Formally, it is a uniform mixture of the generative processes for the two potential translation di1 Doing that requires incorporating into the model additional hidden variables encoding phrase segmentation (DeNero et al., 2006). This would significantly complicate inference (Mylonakis and Sima’an, 2008; Neubig et al., 2011; Cohn and Haffari, 2013). 102 X e,f log X P (z) z 1 2 P (e|z) X P (f , a|e, z) a  X 1 + P (f |z) P (e, a0 |f , z) . 2 0 3. if the e-to-f direction is chosen then generate the pair relying on P (e |z)P (f |e, z); 4. otherwise, use P (f |z)P (e |f , z). 2 As we aim for a simple approach, our TMs are computed through the introduction of hidden alignments a and a0 in f -to-e and e-to-f P directions respectively, in which P (f | e, z) = a P (f , a |e, z) P 0 and P (e |f , z) = a0 P (e, a |f , z). T"
Q16-1008,W11-2107,0,0.114396,"Missing"
Q16-1008,P13-2119,0,0.157235,"owledge. Various techniques can then be used for adaptation. For example, one approach is to combine a system trained on the in-domain data with another general-domain system trained on the rest of the data (e.g., see Koehn and Schroeder (2007), Foster et al. (2010), Bisazza et al. (2011), Sennrich (2012b), Razmara et al. (2012), Sennrich et al. (2013), Haddow (2013), Joty et al. (2015)). Rather than using the entire training data, it is also common to combine the in-domain system with a system trained on a selected subset of the data (e.g., see Axelrod et al. (2011), Koehn and Haddow (2012), Duh et al. (2013), Kirchhoff and Bilmes (2014), Cuong and Sima’an (2014b)). In some other cases, the prior knowledge lies in meta-information about the training data. This could be document-annotated training information (Eidelman et al., 2012; Hu et al., 2014; Hasler et al., 2014; Su et al., 2015; Zhang et al., 2014), and domainannotated sub-corpora (Chiang et al., 2011; Sennrich, 2012b; Chen et al., 2013b; Carpuat et al., 2014; Cuong and Sima’an, 2015). Some recent approaches perform adaptation by exploiting a target domain development, or even only the source side of the development set (Sennrich, 2012a; Ca"
Q16-1008,P12-2023,0,0.107089,"he latent subdomains is likely safer to use. 100 Weights for these features, alongside all other standard features, are tuned on a development set. Importantly, we show that there is no noteworthy benefit from tuning the weights on a sample from the target domain. It is enough to tune them on a mixed-domain dataset sufficiently different from the training data. We attribute this attractive property to the fact that our features, unlike the ones typically considered in standard domain-adaptation work, are generic and only affect the amount of risk our system takes. In contrast, for example, in Eidelman et al. (2012), Chiang et al. (2011), Hu et al. (2014), Hasler et al. (2014), Su et al. (2015), Sennrich (2012b), Chen et al. (2013b), and Carpuat et al. (2014), features capture similarities between a target domain and each of the training subdomains. Clearly, domain adaptation with such rich features, though potentially more powerful, would not be possible without a development set closely matching the target domain. We conduct our experiments on three language pairs and explore adaptation to 9 domain adaptation tasks in total. We observe significant and consistent performance improvements over the baseli"
Q16-1008,D10-1044,0,0.130054,"res which measure how specialized phrases are to individual induced sub-domains; (3) estimating feature weights on out-of-domain data (rather than on the target domain). We conduct experiments on three language pairs and a number of different domains. We observe consistent improvements over a baseline which does not explicitly reward domain invariance. 1 Introduction Mismatch in phrase translation distributions between test data (target domain) and train data is known to harm performance of statistical translation systems (Irvine et al., 2013; Carpuat et al., 2014). Domain-adaptation methods (Foster et al., 2010; Bisazza et al., 2011; Sennrich, 2012b; Razmara et al., 2012; Sennrich et al., 2013; Haddow, 2013; Joty et al., 2015) aim to specialize a system estimated on out-of-domain training data to a target domain represented by a small data sample. In practice, however, the target domain may not be known When the target domain is unknown at training time, the system could be trained to make safer choices, preferring translations which are likely to work across different domains. For example, when translating from English to Russian, the most natural translation for the word ‘code’ would be highly dep"
Q16-1008,D08-1089,0,0.0721658,"P (z = i |e˜) ∝ P (z = i |f˜) ∝ Table 2: Data and adaptation tasks. to the training data. In this way, we test the stability of our results across a wide range of target domains. X he,f i X c(i; zˆhe,f i )δ(˜ e; e), he,f i c(i; zˆhe,f i )δ(f˜; f ). Here, zˆhe, f i is the “winning&quot; latent subdomain for sentence pair he, f i: zˆhe, f i = argmax P (z = i |e, f ) i∈{1, ..., K} 4.2 Systems We use a standard state-of-the-art phrase-based system. The Baseline system includes MOSES (Koehn et al., 2007) baseline feature functions, plus eight hierarchical lexicalized reordering model feature functions (Galley and Manning, 2008). The training data is first word-aligned using GIZA++ (Och and Ney, 2003) and then symmetrized with grow(-diag)final-and (Koehn et al., 2003). We limit the phrase length to the maximum of seven words. The lan104 In practice, we found that using this hard version leads to better performance.5 4.3 Alternative tuning scenarios In order to tune all systems, we use the k-best batch MIRA (Cherry and Foster, 2012). We report the translation accuracy with three metrics - BLEU 5 A more principled alternative would be to use posterior regularization (Ganchev et al., 2009). Task System BLEU↑ /∆ English-"
Q16-1008,N13-1035,0,0.0887923,"eights on out-of-domain data (rather than on the target domain). We conduct experiments on three language pairs and a number of different domains. We observe consistent improvements over a baseline which does not explicitly reward domain invariance. 1 Introduction Mismatch in phrase translation distributions between test data (target domain) and train data is known to harm performance of statistical translation systems (Irvine et al., 2013; Carpuat et al., 2014). Domain-adaptation methods (Foster et al., 2010; Bisazza et al., 2011; Sennrich, 2012b; Razmara et al., 2012; Sennrich et al., 2013; Haddow, 2013; Joty et al., 2015) aim to specialize a system estimated on out-of-domain training data to a target domain represented by a small data sample. In practice, however, the target domain may not be known When the target domain is unknown at training time, the system could be trained to make safer choices, preferring translations which are likely to work across different domains. For example, when translating from English to Russian, the most natural translation for the word ‘code’ would be highly dependent on the domain (and the corresponding word sense). The Russian words ‘xifr’, ‘zakon’ or ‘pro"
Q16-1008,E14-1035,0,0.490374,"se features, alongside all other standard features, are tuned on a development set. Importantly, we show that there is no noteworthy benefit from tuning the weights on a sample from the target domain. It is enough to tune them on a mixed-domain dataset sufficiently different from the training data. We attribute this attractive property to the fact that our features, unlike the ones typically considered in standard domain-adaptation work, are generic and only affect the amount of risk our system takes. In contrast, for example, in Eidelman et al. (2012), Chiang et al. (2011), Hu et al. (2014), Hasler et al. (2014), Su et al. (2015), Sennrich (2012b), Chen et al. (2013b), and Carpuat et al. (2014), features capture similarities between a target domain and each of the training subdomains. Clearly, domain adaptation with such rich features, though potentially more powerful, would not be possible without a development set closely matching the target domain. We conduct our experiments on three language pairs and explore adaptation to 9 domain adaptation tasks in total. We observe significant and consistent performance improvements over the baseline domain-agnostic systems. This result confirms that our two"
Q16-1008,P13-2121,0,0.0218392,"https://www.taus.net/. English Professional Dev &Business Test Services Leisure, Tourism and Arts Dev Test Professional Dev &Business Test Services Dev Legal Test Dev Financials Test Professional Dev &Business Services Test Dev Legal Test Computer Software Computer Hardware Dev Test Dev Test Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words French guage models are interpolated 5-grams with KneserNey smoothing, estimated by KenLM (Heafield et al., 2013) from a large monolingual corpus of nearly 2.1B English words collected within the WMT 2015 MT Shared Task. Finally, we use MOSES as a decoder (Koehn et al., 2007). Our system is exactly the same as the baseline, plus three additional feature functions induced for the translation rules: two features for domainspecificity of phrases (both for the source side ~ (Dα (f˜)) and the target side (Dα (~e˜)), and one feature for source-target coherence across subdomains ~ (D(~e˜, f˜)). For the projection, we use K=12. We 2K 74.16K 83.85K 5K 92.84K 105.05K 2K 107.45K 117.16K 5K 101.82K 114.76K English S"
Q16-1008,P14-1110,0,0.0873844,"00 Weights for these features, alongside all other standard features, are tuned on a development set. Importantly, we show that there is no noteworthy benefit from tuning the weights on a sample from the target domain. It is enough to tune them on a mixed-domain dataset sufficiently different from the training data. We attribute this attractive property to the fact that our features, unlike the ones typically considered in standard domain-adaptation work, are generic and only affect the amount of risk our system takes. In contrast, for example, in Eidelman et al. (2012), Chiang et al. (2011), Hu et al. (2014), Hasler et al. (2014), Su et al. (2015), Sennrich (2012b), Chen et al. (2013b), and Carpuat et al. (2014), features capture similarities between a target domain and each of the training subdomains. Clearly, domain adaptation with such rich features, though potentially more powerful, would not be possible without a development set closely matching the target domain. We conduct our experiments on three language pairs and explore adaptation to 9 domain adaptation tasks in total. We observe significant and consistent performance improvements over the baseline domain-agnostic systems. This result"
Q16-1008,Q13-1035,0,0.0901055,"ng latent subdomains from the training data only; (2) introducing features which measure how specialized phrases are to individual induced sub-domains; (3) estimating feature weights on out-of-domain data (rather than on the target domain). We conduct experiments on three language pairs and a number of different domains. We observe consistent improvements over a baseline which does not explicitly reward domain invariance. 1 Introduction Mismatch in phrase translation distributions between test data (target domain) and train data is known to harm performance of statistical translation systems (Irvine et al., 2013; Carpuat et al., 2014). Domain-adaptation methods (Foster et al., 2010; Bisazza et al., 2011; Sennrich, 2012b; Razmara et al., 2012; Sennrich et al., 2013; Haddow, 2013; Joty et al., 2015) aim to specialize a system estimated on out-of-domain training data to a target domain represented by a small data sample. In practice, however, the target domain may not be known When the target domain is unknown at training time, the system could be trained to make safer choices, preferring translations which are likely to work across different domains. For example, when translating from English to Russia"
Q16-1008,D07-1031,0,0.0299789,"80.49K 85.08K 5K 79.75K 85.28K 2K 50.54K 45.99K 5K 124.93K 111.70K 2K 40.24K 38.31K 5K 102.71K 101.12K 2K 37.40K 36.98K 5K 103.29K 98.04K also explored different values for K, but have not observed significant difference in the scores. In our experiments we do one iteration of EM with parallel LMs (as described in Section 3), before continuing with the full model for three more iterations. We did not observe a significant improvement from running EM any longer. Finally, we use hard EM, as it has been found to yield better models than the standard soft EM on a number of different task (e.g., (Johnson, 2007)). In other words, instead of standard ‘soft’ EM updates with phrase counts weighted according to the posterior P (z = i |e, f ), we use the ‘winner-takes-all’ approach: P (z = i |e˜) ∝ P (z = i |f˜) ∝ Table 2: Data and adaptation tasks. to the training data. In this way, we test the stability of our results across a wide range of target domains. X he,f i X c(i; zˆhe,f i )δ(˜ e; e), he,f i c(i; zˆhe,f i )δ(f˜; f ). Here, zˆhe, f i is the “winning&quot; latent subdomain for sentence pair he, f i: zˆhe, f i = argmax P (z = i |e, f ) i∈{1, ..., K} 4.2 Systems We use a standard state-of-the-art phrase-"
Q16-1008,D15-1147,0,0.0599944,"of-domain data (rather than on the target domain). We conduct experiments on three language pairs and a number of different domains. We observe consistent improvements over a baseline which does not explicitly reward domain invariance. 1 Introduction Mismatch in phrase translation distributions between test data (target domain) and train data is known to harm performance of statistical translation systems (Irvine et al., 2013; Carpuat et al., 2014). Domain-adaptation methods (Foster et al., 2010; Bisazza et al., 2011; Sennrich, 2012b; Razmara et al., 2012; Sennrich et al., 2013; Haddow, 2013; Joty et al., 2015) aim to specialize a system estimated on out-of-domain training data to a target domain represented by a small data sample. In practice, however, the target domain may not be known When the target domain is unknown at training time, the system could be trained to make safer choices, preferring translations which are likely to work across different domains. For example, when translating from English to Russian, the most natural translation for the word ‘code’ would be highly dependent on the domain (and the corresponding word sense). The Russian words ‘xifr’, ‘zakon’ or ‘programma’ would perhap"
Q16-1008,D14-1014,0,0.0191632,"chniques can then be used for adaptation. For example, one approach is to combine a system trained on the in-domain data with another general-domain system trained on the rest of the data (e.g., see Koehn and Schroeder (2007), Foster et al. (2010), Bisazza et al. (2011), Sennrich (2012b), Razmara et al. (2012), Sennrich et al. (2013), Haddow (2013), Joty et al. (2015)). Rather than using the entire training data, it is also common to combine the in-domain system with a system trained on a selected subset of the data (e.g., see Axelrod et al. (2011), Koehn and Haddow (2012), Duh et al. (2013), Kirchhoff and Bilmes (2014), Cuong and Sima’an (2014b)). In some other cases, the prior knowledge lies in meta-information about the training data. This could be document-annotated training information (Eidelman et al., 2012; Hu et al., 2014; Hasler et al., 2014; Su et al., 2015; Zhang et al., 2014), and domainannotated sub-corpora (Chiang et al., 2011; Sennrich, 2012b; Chen et al., 2013b; Carpuat et al., 2014; Cuong and Sima’an, 2015). Some recent approaches perform adaptation by exploiting a target domain development, or even only the source side of the development set (Sennrich, 2012a; Carpuat et al., 2013; Carpuat e"
Q16-1008,W12-3139,0,0.0253064,"sed as a form of prior knowledge. Various techniques can then be used for adaptation. For example, one approach is to combine a system trained on the in-domain data with another general-domain system trained on the rest of the data (e.g., see Koehn and Schroeder (2007), Foster et al. (2010), Bisazza et al. (2011), Sennrich (2012b), Razmara et al. (2012), Sennrich et al. (2013), Haddow (2013), Joty et al. (2015)). Rather than using the entire training data, it is also common to combine the in-domain system with a system trained on a selected subset of the data (e.g., see Axelrod et al. (2011), Koehn and Haddow (2012), Duh et al. (2013), Kirchhoff and Bilmes (2014), Cuong and Sima’an (2014b)). In some other cases, the prior knowledge lies in meta-information about the training data. This could be document-annotated training information (Eidelman et al., 2012; Hu et al., 2014; Hasler et al., 2014; Su et al., 2015; Zhang et al., 2014), and domainannotated sub-corpora (Chiang et al., 2011; Sennrich, 2012b; Chen et al., 2013b; Carpuat et al., 2014; Cuong and Sima’an, 2015). Some recent approaches perform adaptation by exploiting a target domain development, or even only the source side of the development set ("
Q16-1008,W07-0733,0,0.0351621,"in SMT can be regarded as injecting prior knowledge about the target translation task into the learning process. Various approaches have so far been exploited in the literature. They can be loosely categorized according to the type of prior knowledge exploited for adaptation. Often, a seed 109 in-domain corpus exemplifying the target translation task is used as a form of prior knowledge. Various techniques can then be used for adaptation. For example, one approach is to combine a system trained on the in-domain data with another general-domain system trained on the rest of the data (e.g., see Koehn and Schroeder (2007), Foster et al. (2010), Bisazza et al. (2011), Sennrich (2012b), Razmara et al. (2012), Sennrich et al. (2013), Haddow (2013), Joty et al. (2015)). Rather than using the entire training data, it is also common to combine the in-domain system with a system trained on a selected subset of the data (e.g., see Axelrod et al. (2011), Koehn and Haddow (2012), Duh et al. (2013), Kirchhoff and Bilmes (2014), Cuong and Sima’an (2014b)). In some other cases, the prior knowledge lies in meta-information about the training data. This could be document-annotated training information (Eidelman et al., 2012;"
Q16-1008,N03-1017,0,0.405782,"more powerful, would not be possible without a development set closely matching the target domain. We conduct our experiments on three language pairs and explore adaptation to 9 domain adaptation tasks in total. We observe significant and consistent performance improvements over the baseline domain-agnostic systems. This result confirms that our two features, and the latent subdomains they are computed from, are useful also for the very challenging domain adaptation setting considered in this work. 2 Domain-Invariance for Phrases At the core of a standard state-of-the-art phrasebased system (Koehn et al., 2003; Och and Ney, 2004) lies a phrase table {h˜ e, f˜i} extracted from a word-aligned training corpus together with estimates for phrase translation probabilities Pcount (˜ e |f˜) and Pcount (f˜ |e˜). Typically the phrases and their probabilities are obtained from large parallel corpora, which are usually broad enough to cover a mixture of several subdomains. In such mixtures, phrase distributions may be different across different subdomains. Some phrases (whether source or target) are more specific for certain subdomains than others, while some phrases are useful across many subdomains. Moreover"
Q16-1008,P07-2045,0,0.0198822,"Financials Test Professional Dev &Business Services Test Dev Legal Test Computer Software Computer Hardware Dev Test Dev Test Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words French guage models are interpolated 5-grams with KneserNey smoothing, estimated by KenLM (Heafield et al., 2013) from a large monolingual corpus of nearly 2.1B English words collected within the WMT 2015 MT Shared Task. Finally, we use MOSES as a decoder (Koehn et al., 2007). Our system is exactly the same as the baseline, plus three additional feature functions induced for the translation rules: two features for domainspecificity of phrases (both for the source side ~ (Dα (f˜)) and the target side (Dα (~e˜)), and one feature for source-target coherence across subdomains ~ (D(~e˜, f˜)). For the projection, we use K=12. We 2K 74.16K 83.85K 5K 92.84K 105.05K 2K 107.45K 117.16K 5K 101.82K 114.76K English Spanish 2K 31.70K 34.62K 5K 84.1K 93.4K 2K 35.06K 38.78K 5K 88.63K 102.71K 2K 37.23K 42.89K 5K 99.05K 109.81K English German 2K 80.49K 85.08K 5K 79.75K 85.28K 2K 50"
Q16-1008,W04-3250,0,0.264521,"Missing"
Q16-1008,2005.mtsummit-papers.11,0,0.0696099,"e LMs estimated in 103 Sents Words Training Data Sents Words Sents Words English French 5.01M 103.39M 125.81M English Spanish 4.00M 81.48M 89.08M English German 4.07M 93.19M 88.48M Table 1: Data Preparation. 4.1 Data We conduct experiments with large-scale SMT systems across a number of domains for three language pairs (English-Spanish, English-German and English-French). The datasets are summarized in Table 1. For English-Spanish, we run experiments with training data consisting of 4M sentence pairs collected from multiple resources within the WMT 2013 MT Shared Task. These include EuroParl (Koehn, 2005), Common Crawl Corpus, UN Corpus, and News Commentary. For English-German, our training data consists of 4.1M sentence pairs collected from the WMT 2015 MT Shared Task, including EuroParl, Common Crawl Corpus and News Commentary. Finally, for English-French, we train SMT systems on a corpus of 5M sentence pairs collected from the WMT 2015 MT Shared Task, including the 109 French-English corpus. We conducted experiments on 9 different domains (tasks) where the data was manually collected by a TAUS.4 Table 2 presents the translation tasks: each of the tasks deals with a specific domain, each of"
Q16-1008,N04-1022,0,0.0374551,"he goal and setting we are working on is markedly different (i.e., we do not have access to meta-information about the training and translation tasks at all). The domain-invariance induced is integrated into SMT systems as feature functions, redirecting the decoder to a better search space for the translation over adaptation tasks. This aims at biasing the decoder towards translations that are less domain-specific and more source-target domain coherent. There is an interesting relation between this work and extensive prior work on minimum Bayes risk (MBR) objectives (used either at test time (Kumar and Byrne, 2004) or during training (Smith and Eisner, 2006; Pauls et al., 2009)). As with our work, the goal of MBR minimization is to select translations that are less “risky&quot;. Their risk is due to the uncertainty in model predictions, and some of this uncertainty may indeed be associated with domainvariability of translations. Still, a system trained with an MBR objective will tend to output most frequent translation rather than the most domaininvariant one, and this, as we argued in the introduction, might not be the right decision when applying it across domains. We believe that the two classes of method"
Q16-1008,W14-3359,0,0.213077,"ima’an (2014b)). In some other cases, the prior knowledge lies in meta-information about the training data. This could be document-annotated training information (Eidelman et al., 2012; Hu et al., 2014; Hasler et al., 2014; Su et al., 2015; Zhang et al., 2014), and domainannotated sub-corpora (Chiang et al., 2011; Sennrich, 2012b; Chen et al., 2013b; Carpuat et al., 2014; Cuong and Sima’an, 2015). Some recent approaches perform adaptation by exploiting a target domain development, or even only the source side of the development set (Sennrich, 2012a; Carpuat et al., 2013; Carpuat et al., 2014; Mansour and Ney, 2014). Recently, there was some research on adapting simultaneously to multiple domains, the goal related to ours (Clark et al., 2012; Sennrich, 2012a). For instance, Clark et al. (2012) augment a phrase-based MT system with various domain indicator features to build a single system that performs well across a range of domains. Sennrich (2012a) proposed to cluster training data in an unsupervised fashion to build mixture models that yield good performance on multiple test domains. However, their approaches are very different from ours, that is minimizing risk associated with choosing domain-specifi"
Q16-1008,D09-1074,0,0.0279052,"f˜. In our experiments, we compare using our subdomain induction framework with relying on topic distributions provided by a standard topic model, Latent Dirichlet Allocation (Blei et al., 2003). Note that unlike LDA we rely on parallel data and word alignments when inducing domains. Our intuition is that latent variables capturing regularities in bilingual data may be more appropriate for the translation task. Inducing these probabilities directly is rather difficult as the task of designing a fully generative phrase-based model is known to be challenging.1 In order to avoid this, we follow Matsoukas et al. (2009) and Cuong and Sima’an (2014a) who “embed&quot; such a phrase-level model into a latent subdomain model that works at the sentence level. In other words, we associate latent domains with sentence pairs rather than with phrases, and use the posterior probabilities computed for the sentences with all the phrases appearing in the corresponding sentences. Given P (z |e, f ) - a latent subdomain model given sentence pairs he, f i - the estimation of P (z |e˜) and P (z |f˜), for phrases e˜ and f˜, can be simplified by computing expectations z for all z ∈ {1, . . . , K}: P e; e) e,f P (z = i|e, f )c(˜ P ("
Q16-1008,D08-1066,1,0.844454,"Missing"
Q16-1008,P11-1064,0,0.0239073,", in which P (e |z) ei−n , z) and P (f |z) = = i P (ei | Q j−1 Here, the notation ei−1 j P (fj |fj−n , z). i−n j−1 and fj−n is used to denote the history of length n for the source and target words ei and fj , respectively. Training. For training, we maximize the loglikelihood L of the data L= Formally, it is a uniform mixture of the generative processes for the two potential translation di1 Doing that requires incorporating into the model additional hidden variables encoding phrase segmentation (DeNero et al., 2006). This would significantly complicate inference (Mylonakis and Sima’an, 2008; Neubig et al., 2011; Cohn and Haffari, 2013). 102 X e,f log X P (z) z 1 2 P (e|z) X P (f , a|e, z) a  X 1 + P (f |z) P (e, a0 |f , z) . 2 0 3. if the e-to-f direction is chosen then generate the pair relying on P (e |z)P (f |e, z); 4. otherwise, use P (f |z)P (e |f , z). 2 As we aim for a simple approach, our TMs are computed through the introduction of hidden alignments a and a0 in f -to-e and e-to-f P directions respectively, in which P (f | e, z) = a P (f , a |e, z) P 0 and P (e |f , z) = a0 P (e, a |f , z). To make the marginalization of alignments tractable, we restrict P (f , a |e, z) and P (e, a0 |f ,"
Q16-1008,J03-1002,0,0.00693927,"ng data. In this way, we test the stability of our results across a wide range of target domains. X he,f i X c(i; zˆhe,f i )δ(˜ e; e), he,f i c(i; zˆhe,f i )δ(f˜; f ). Here, zˆhe, f i is the “winning&quot; latent subdomain for sentence pair he, f i: zˆhe, f i = argmax P (z = i |e, f ) i∈{1, ..., K} 4.2 Systems We use a standard state-of-the-art phrase-based system. The Baseline system includes MOSES (Koehn et al., 2007) baseline feature functions, plus eight hierarchical lexicalized reordering model feature functions (Galley and Manning, 2008). The training data is first word-aligned using GIZA++ (Och and Ney, 2003) and then symmetrized with grow(-diag)final-and (Koehn et al., 2003). We limit the phrase length to the maximum of seven words. The lan104 In practice, we found that using this hard version leads to better performance.5 4.3 Alternative tuning scenarios In order to tune all systems, we use the k-best batch MIRA (Cherry and Foster, 2012). We report the translation accuracy with three metrics - BLEU 5 A more principled alternative would be to use posterior regularization (Ganchev et al., 2009). Task System BLEU↑ /∆ English-French Baseline 21.4 Professional & Business Services Our System 21.5/+0.1"
Q16-1008,J04-4002,0,0.0827779,"d not be possible without a development set closely matching the target domain. We conduct our experiments on three language pairs and explore adaptation to 9 domain adaptation tasks in total. We observe significant and consistent performance improvements over the baseline domain-agnostic systems. This result confirms that our two features, and the latent subdomains they are computed from, are useful also for the very challenging domain adaptation setting considered in this work. 2 Domain-Invariance for Phrases At the core of a standard state-of-the-art phrasebased system (Koehn et al., 2003; Och and Ney, 2004) lies a phrase table {h˜ e, f˜i} extracted from a word-aligned training corpus together with estimates for phrase translation probabilities Pcount (˜ e |f˜) and Pcount (f˜ |e˜). Typically the phrases and their probabilities are obtained from large parallel corpora, which are usually broad enough to cover a mixture of several subdomains. In such mixtures, phrase distributions may be different across different subdomains. Some phrases (whether source or target) are more specific for certain subdomains than others, while some phrases are useful across many subdomains. Moreover, for a phrase pair,"
Q16-1008,P02-1040,0,0.0977283,"Missing"
Q16-1008,D09-1147,0,0.0243774,"we do not have access to meta-information about the training and translation tasks at all). The domain-invariance induced is integrated into SMT systems as feature functions, redirecting the decoder to a better search space for the translation over adaptation tasks. This aims at biasing the decoder towards translations that are less domain-specific and more source-target domain coherent. There is an interesting relation between this work and extensive prior work on minimum Bayes risk (MBR) objectives (used either at test time (Kumar and Byrne, 2004) or during training (Smith and Eisner, 2006; Pauls et al., 2009)). As with our work, the goal of MBR minimization is to select translations that are less “risky&quot;. Their risk is due to the uncertainty in model predictions, and some of this uncertainty may indeed be associated with domainvariability of translations. Still, a system trained with an MBR objective will tend to output most frequent translation rather than the most domaininvariant one, and this, as we argued in the introduction, might not be the right decision when applying it across domains. We believe that the two classes of methods are largely complementary, and leave further investigation for"
Q16-1008,P12-1099,0,0.106493,"induced sub-domains; (3) estimating feature weights on out-of-domain data (rather than on the target domain). We conduct experiments on three language pairs and a number of different domains. We observe consistent improvements over a baseline which does not explicitly reward domain invariance. 1 Introduction Mismatch in phrase translation distributions between test data (target domain) and train data is known to harm performance of statistical translation systems (Irvine et al., 2013; Carpuat et al., 2014). Domain-adaptation methods (Foster et al., 2010; Bisazza et al., 2011; Sennrich, 2012b; Razmara et al., 2012; Sennrich et al., 2013; Haddow, 2013; Joty et al., 2015) aim to specialize a system estimated on out-of-domain training data to a target domain represented by a small data sample. In practice, however, the target domain may not be known When the target domain is unknown at training time, the system could be trained to make safer choices, preferring translations which are likely to work across different domains. For example, when translating from English to Russian, the most natural translation for the word ‘code’ would be highly dependent on the domain (and the corresponding word sense). The"
Q16-1008,P13-1082,0,0.0784642,"3) estimating feature weights on out-of-domain data (rather than on the target domain). We conduct experiments on three language pairs and a number of different domains. We observe consistent improvements over a baseline which does not explicitly reward domain invariance. 1 Introduction Mismatch in phrase translation distributions between test data (target domain) and train data is known to harm performance of statistical translation systems (Irvine et al., 2013; Carpuat et al., 2014). Domain-adaptation methods (Foster et al., 2010; Bisazza et al., 2011; Sennrich, 2012b; Razmara et al., 2012; Sennrich et al., 2013; Haddow, 2013; Joty et al., 2015) aim to specialize a system estimated on out-of-domain training data to a target domain represented by a small data sample. In practice, however, the target domain may not be known When the target domain is unknown at training time, the system could be trained to make safer choices, preferring translations which are likely to work across different domains. For example, when translating from English to Russian, the most natural translation for the word ‘code’ would be highly dependent on the domain (and the corresponding word sense). The Russian words ‘xifr’, ‘"
Q16-1008,2012.eamt-1.43,0,0.33738,"re to individual induced sub-domains; (3) estimating feature weights on out-of-domain data (rather than on the target domain). We conduct experiments on three language pairs and a number of different domains. We observe consistent improvements over a baseline which does not explicitly reward domain invariance. 1 Introduction Mismatch in phrase translation distributions between test data (target domain) and train data is known to harm performance of statistical translation systems (Irvine et al., 2013; Carpuat et al., 2014). Domain-adaptation methods (Foster et al., 2010; Bisazza et al., 2011; Sennrich, 2012b; Razmara et al., 2012; Sennrich et al., 2013; Haddow, 2013; Joty et al., 2015) aim to specialize a system estimated on out-of-domain training data to a target domain represented by a small data sample. In practice, however, the target domain may not be known When the target domain is unknown at training time, the system could be trained to make safer choices, preferring translations which are likely to work across different domains. For example, when translating from English to Russian, the most natural translation for the word ‘code’ would be highly dependent on the domain (and the correspo"
Q16-1008,E12-1055,0,0.529958,"re to individual induced sub-domains; (3) estimating feature weights on out-of-domain data (rather than on the target domain). We conduct experiments on three language pairs and a number of different domains. We observe consistent improvements over a baseline which does not explicitly reward domain invariance. 1 Introduction Mismatch in phrase translation distributions between test data (target domain) and train data is known to harm performance of statistical translation systems (Irvine et al., 2013; Carpuat et al., 2014). Domain-adaptation methods (Foster et al., 2010; Bisazza et al., 2011; Sennrich, 2012b; Razmara et al., 2012; Sennrich et al., 2013; Haddow, 2013; Joty et al., 2015) aim to specialize a system estimated on out-of-domain training data to a target domain represented by a small data sample. In practice, however, the target domain may not be known When the target domain is unknown at training time, the system could be trained to make safer choices, preferring translations which are likely to work across different domains. For example, when translating from English to Russian, the most natural translation for the word ‘code’ would be highly dependent on the domain (and the correspo"
Q16-1008,P06-2101,0,0.041292,"rkedly different (i.e., we do not have access to meta-information about the training and translation tasks at all). The domain-invariance induced is integrated into SMT systems as feature functions, redirecting the decoder to a better search space for the translation over adaptation tasks. This aims at biasing the decoder towards translations that are less domain-specific and more source-target domain coherent. There is an interesting relation between this work and extensive prior work on minimum Bayes risk (MBR) objectives (used either at test time (Kumar and Byrne, 2004) or during training (Smith and Eisner, 2006; Pauls et al., 2009)). As with our work, the goal of MBR minimization is to select translations that are less “risky&quot;. Their risk is due to the uncertainty in model predictions, and some of this uncertainty may indeed be associated with domainvariability of translations. Still, a system trained with an MBR objective will tend to output most frequent translation rather than the most domaininvariant one, and this, as we argued in the introduction, might not be the right decision when applying it across domains. We believe that the two classes of methods are largely complementary, and leave furt"
Q16-1008,2006.amta-papers.25,0,0.231338,"Missing"
Q16-1008,P15-1023,0,0.0603535,"all other standard features, are tuned on a development set. Importantly, we show that there is no noteworthy benefit from tuning the weights on a sample from the target domain. It is enough to tune them on a mixed-domain dataset sufficiently different from the training data. We attribute this attractive property to the fact that our features, unlike the ones typically considered in standard domain-adaptation work, are generic and only affect the amount of risk our system takes. In contrast, for example, in Eidelman et al. (2012), Chiang et al. (2011), Hu et al. (2014), Hasler et al. (2014), Su et al. (2015), Sennrich (2012b), Chen et al. (2013b), and Carpuat et al. (2014), features capture similarities between a target domain and each of the training subdomains. Clearly, domain adaptation with such rich features, though potentially more powerful, would not be possible without a development set closely matching the target domain. We conduct our experiments on three language pairs and explore adaptation to 9 domain adaptation tasks in total. We observe significant and consistent performance improvements over the baseline domain-agnostic systems. This result confirms that our two features, and the"
Q16-1008,P11-1007,1,0.54996,"the uncertainty in model predictions, and some of this uncertainty may indeed be associated with domainvariability of translations. Still, a system trained with an MBR objective will tend to output most frequent translation rather than the most domaininvariant one, and this, as we argued in the introduction, might not be the right decision when applying it across domains. We believe that the two classes of methods are largely complementary, and leave further investigation for future work. At a conceptual level it is also related to regularizers used in learning domain-invariant neural models (Titov, 2011), specifically autoencoders. Though they also consider divergences between distributions of latent variable vectors, they use these divergences at learning time to bias models to induce representations maximally invariant across domains. Moreover, they assume access to meta-information about domains and consider only classification problems. 6 Conclusion This paper aims at adapting machine translation systems to all domains at once by favoring phrases that are domain-invariant, that are safe to use across a variety of domains. While typical domain adaptation systems expect a sample of the targ"
Q16-1008,P14-1072,0,0.221592,"s a  + P (f |z)P (e, a0 |f , z) . (6) In the M-step, we use the posteriors P (a, z |e, f ) and P (a0 , z |e, f ) to re-estimate parameters of both alignment models. This is done in a very similar way to estimation of the standard IBM Model 1. We use the posteriors to re-estimate LM parameters as follows X , z) ∝ P (z|e, f )c(ei1 ; e), (7) P (ei |ei−1 1 e,f P (fi |f1i−1 , z) ∝ X P (z|e, f )c(f1i ; f ). (8) e,f To obtain better parameter estimates for word predictions and avoid overfitting, we use smoothing in the M-step. In this work, we chose to apply expected Kneser-Ney smoothing technique (Zhang and Chiang, 2014) as it is simple and achieves state-of-theart performance on the language modeling problem. Finally, P (z) can be simply estimated as follows X P (z) ∝ P (z |e, f ) (9) e, f Hierarchical Training. In practice, we found that training the full joint model leads to brittle performance, as EM is very likely to get stuck in bad local maxima. To address this difficulty, in our implementation, we start out by first jointly training P (z), P (e |z) and P (f |z). In this way in the E-step, we fix our model parameters and compute P (z |e, f ) for every sentence pair: P (z |e, f ) ∝ P (e |z)P (f |z)P (z)"
Q16-1017,P08-1004,0,0.043906,"e a large number of relations need to be detected in a heterogeneous text collection (e.g., the entire Web). Though weakly-supervised approaches, such as distantly supervised methods and bootstrapping (Mintz et al., 2009; Agichtein and Gravano, 2000), reduce the amount of necessary supervision, they still require examples for every relation considered. These limitations led to the emergence of unsupervised approaches for RE. These methods extract surface or syntactic patterns between two entities and either directly use these patterns as substitutes for semantic relations (Banko et al., 2007; Banko and Etzioni, 2008) or cluster the patterns (sometimes in context-sensitive way) to form relations (Lin and Pantel, 2001; Yao et al., 2011; Nakashole et al., 2012; Yao et al., 2012). The existing methods, given their generative (or agglomerative clustering) nature, rely on simpler features than their supervised counterparts and also make strong modeling assumptions (e.g., assuming that arguments are conditionally independent of each other given the relation). These shortcomings are likely to harm their performance. In this work, we tackle the aforementioned challenges and introduce a new model for unsupervised r"
Q16-1017,D15-1056,0,0.0136537,"ase and Wikipedia infoboxes) with text (Mintz et al., 2009; Riedel et al., 2010; Surdeanu et al., 2012; Zeng et al., 2015). Similarly to our method they can use feature-rich models without the need for manually annotated data. However, a relation extractor trained in this way will only be able to predict relations already present in a knowledge base. These methods cannot be used to discover new relations. The framework we propose is completely unsupervised and does not have this shortcoming. Bootstrapping. Bootstrapping methods for relation extraction (Agichtein and Gravano, 2000; Brin, 1998; Batista et al., 2015) iteratively label new examples by finding the ones which are the most similar, according to some similarity function, to a seed set of labeled examples. The process continues until some convergence criteria is met. Even though this approach is not very labor-intensive (i.e., it requires only few manually labeled examples for the initial seed set), it requires some domain knowledge from the model designer. In contrast, unsupervised models are domain-agnostic and require only unlabeled text. Knowledge base factorization. Knowledge base completion via matrix or tensor factorization has received"
Q16-1017,N10-1083,0,0.0134514,"Missing"
Q16-1017,K16-1002,0,0.0350508,"Missing"
Q16-1017,D14-1165,0,0.0599682,"rguments in a flexible way (both in the reconstruction and encoding components). The use of a reconstruction-error objective, previously considered primarily in the context of training neural autoencoders (Hinton, 1989; Vincent et al., 2008), gives us an opportunity to borrow ideas from the well-established area of statistical relational learning (Getoor and Taskar, 2007), and, more specifically, relation factorization. In this area, tensor and matrix factorization methods have been shown to be effective for inferring missing facts in knowledge bases (Bordes et al., 2011; Riedel et al., 2013; Chang et al., 2014; Bordes et al., 2014; Sutskever et al., 2009). In our work, we also adopt a fairly standard RESCAL factorization (Nickel et al., 2011) and use it within our reconstruction component. Though there is a clear analogy between statistical relational learning and our setting, there is also a very significant difference. In contrast to relational learning, rather than factorizing existing relations (an existing ‘database’), our method simultaneously discovers the relational schema (i.e., an inventory of relations) and a mapping from text to the relations (i.e., a relation extractor), and it does it"
Q16-1017,W13-3809,0,0.101889,"Missing"
Q16-1017,P05-1045,0,0.0227143,"Missing"
Q16-1017,N15-1144,0,0.010219,"earning phase, the parameters of both the encoding and reconstruction part are chosen so as to minimize a reconstruction error (e.g., the Euclidean distance ||x − x ˜||2 ). Although popular within the neural network community (where y is defined as a real-valued vector), autoencoders have recently been applied to the discrete-state setting (where y is defined as a categorical random variable, a tuple of variables or a graph). For example, such models have been used in the context of dependency parsing (Daum´e III, 2009), or in the context of POS tagging and word alignment (Ammar et al., 2014; Lin et al., 2015a). The most related previous work (Titov and Khoddam, 2015) considers induction of semantic roles of verbal arguments (e.g., an agent, a performer of an action vs. a patient, an affected entity), though no grouping of predicates into relations was considered. We refer to such models as discrete-state autoencoders. We use different model families for the decoding and reconstruction components. The encoding part is a log-linear feature-rich model, while the reconstruction part is a tensor (or matrix) factorization 234 model which seeks to reconstruct entities, relying on the outcome of the enco"
Q16-1017,D15-1082,0,0.0255418,"Missing"
Q16-1017,P09-1113,0,0.860646,"asks, such as information retrieval (Liu et al., 2014), question answering (Ravichandran and Hovy, 2002) and textual entailment (Szpektor et al., 2004). Supervised methods for RE have been successful when small restricted sets of relations are considered. However, human annotation is expensive and time-consuming, and consequently these approaches do not scale well to the open-domain setting where a large number of relations need to be detected in a heterogeneous text collection (e.g., the entire Web). Though weakly-supervised approaches, such as distantly supervised methods and bootstrapping (Mintz et al., 2009; Agichtein and Gravano, 2000), reduce the amount of necessary supervision, they still require examples for every relation considered. These limitations led to the emergence of unsupervised approaches for RE. These methods extract surface or syntactic patterns between two entities and either directly use these patterns as substitutes for semantic relations (Banko et al., 2007; Banko and Etzioni, 2008) or cluster the patterns (sometimes in context-sensitive way) to form relations (Lin and Pantel, 2001; Yao et al., 2011; Nakashole et al., 2012; Yao et al., 2012). The existing methods, given thei"
Q16-1017,D12-1104,0,0.0241573,"Missing"
Q16-1017,P10-1045,0,0.00832834,"Missing"
Q16-1017,D09-1001,0,0.00711035,"supervised learning) or use data generated automatically by aligning knowledge bases (e.g., Freebase) with text (called distantlysupervised methods). Both classes of approaches assume a predefined inventory of relations and a manually constructed resource. In contrast, the focus of this paper is on opendomain unsupervised RE (also known as relation discovery) where no fixed inventory of relations is provided to the learner. The methods induce relations from the data itself. Previous work on this task (Banko et al., 2007), as well as on its generalization, called unsupervised semantic parsing (Poon and Domingos, 2009; Titov and Klementiev, 2011), groups patterns between entity pairs (e.g., wrote a review, wrote a critique and reviewed) and uses these clusters as relations. Other approaches (e.g., Shinyama and Sekine (2006); Yao et al. (2011); Yao et al. (2012); de Lacalle and Lapata (2013)), including the one introduced in this paper, perform context-sensitive clustering, that is, they treat relations as latent variables and induce them for each entity-pair occurrence individually. Rather than relying solely on a pattern between entity pairs, the latter class of methods can use additional context to decid"
Q16-1017,P02-1006,0,0.00882385,"he two components are estimated jointly so as to minimize errors in recovering arguments. We study factorization models inspired by previous work in relation factorization and selectional preference modeling. Our models substantially outperform the generative and agglomerative-clustering counterparts and achieve state-of-the-art performance. 1 Introduction The task of Relation Extraction (RE) consists of detecting and classifying the semantic relations present in text. RE has been shown to benefit a wide range of NLP tasks, such as information retrieval (Liu et al., 2014), question answering (Ravichandran and Hovy, 2002) and textual entailment (Szpektor et al., 2004). Supervised methods for RE have been successful when small restricted sets of relations are considered. However, human annotation is expensive and time-consuming, and consequently these approaches do not scale well to the open-domain setting where a large number of relations need to be detected in a heterogeneous text collection (e.g., the entire Web). Though weakly-supervised approaches, such as distantly supervised methods and bootstrapping (Mintz et al., 2009; Agichtein and Gravano, 2000), reduce the amount of necessary supervision, they still"
Q16-1017,W97-0209,0,0.403114,"Missing"
Q16-1017,N13-1008,0,0.289013,"ependencies between arguments in a flexible way (both in the reconstruction and encoding components). The use of a reconstruction-error objective, previously considered primarily in the context of training neural autoencoders (Hinton, 1989; Vincent et al., 2008), gives us an opportunity to borrow ideas from the well-established area of statistical relational learning (Getoor and Taskar, 2007), and, more specifically, relation factorization. In this area, tensor and matrix factorization methods have been shown to be effective for inferring missing facts in knowledge bases (Bordes et al., 2011; Riedel et al., 2013; Chang et al., 2014; Bordes et al., 2014; Sutskever et al., 2009). In our work, we also adopt a fairly standard RESCAL factorization (Nickel et al., 2011) and use it within our reconstruction component. Though there is a clear analogy between statistical relational learning and our setting, there is also a very significant difference. In contrast to relational learning, rather than factorizing existing relations (an existing ‘database’), our method simultaneously discovers the relational schema (i.e., an inventory of relations) and a mapping from text to the relations (i.e., a relation extrac"
Q16-1017,N06-1039,0,0.0577484,"of relations and a manually constructed resource. In contrast, the focus of this paper is on opendomain unsupervised RE (also known as relation discovery) where no fixed inventory of relations is provided to the learner. The methods induce relations from the data itself. Previous work on this task (Banko et al., 2007), as well as on its generalization, called unsupervised semantic parsing (Poon and Domingos, 2009; Titov and Klementiev, 2011), groups patterns between entity pairs (e.g., wrote a review, wrote a critique and reviewed) and uses these clusters as relations. Other approaches (e.g., Shinyama and Sekine (2006); Yao et al. (2011); Yao et al. (2012); de Lacalle and Lapata (2013)), including the one introduced in this paper, perform context-sensitive clustering, that is, they treat relations as latent variables and induce them for each entity-pair occurrence individually. Rather than relying solely on a pattern between entity pairs, the latter class of methods can use additional context to decide that Napoleon reviewed the Old Guard and the above sentence about Roger Ebert should not be labeled with the same relation. 1 In some of our examples we will use relation names, although our method, as virtua"
Q16-1017,D12-1042,0,0.0672651,"stic semi-supervised setting, though we leave any serious investigation of this set-up for future work. 241 6 Additional Related Work In this section, we mainly consider lines of related work not discussed in other sections of the paper, and we emphasize their relationship to our approach. Distant supervision. These methods can be regarded as a half-way point between unsupervised and supervised methods. Distantly supervised models are trained on data generated automatically by aligning knowledge bases (e.g., Freebase and Wikipedia infoboxes) with text (Mintz et al., 2009; Riedel et al., 2010; Surdeanu et al., 2012; Zeng et al., 2015). Similarly to our method they can use feature-rich models without the need for manually annotated data. However, a relation extractor trained in this way will only be able to predict relations already present in a knowledge base. These methods cannot be used to discover new relations. The framework we propose is completely unsupervised and does not have this shortcoming. Bootstrapping. Bootstrapping methods for relation extraction (Agichtein and Gravano, 2000; Brin, 1998; Batista et al., 2015) iteratively label new examples by finding the ones which are the most similar, a"
Q16-1017,W04-3206,0,0.034024,"mize errors in recovering arguments. We study factorization models inspired by previous work in relation factorization and selectional preference modeling. Our models substantially outperform the generative and agglomerative-clustering counterparts and achieve state-of-the-art performance. 1 Introduction The task of Relation Extraction (RE) consists of detecting and classifying the semantic relations present in text. RE has been shown to benefit a wide range of NLP tasks, such as information retrieval (Liu et al., 2014), question answering (Ravichandran and Hovy, 2002) and textual entailment (Szpektor et al., 2004). Supervised methods for RE have been successful when small restricted sets of relations are considered. However, human annotation is expensive and time-consuming, and consequently these approaches do not scale well to the open-domain setting where a large number of relations need to be detected in a heterogeneous text collection (e.g., the entire Web). Though weakly-supervised approaches, such as distantly supervised methods and bootstrapping (Mintz et al., 2009; Agichtein and Gravano, 2000), reduce the amount of necessary supervision, they still require examples for every relation considered"
Q16-1017,N15-1001,1,0.942203,"nd reconstruction part are chosen so as to minimize a reconstruction error (e.g., the Euclidean distance ||x − x ˜||2 ). Although popular within the neural network community (where y is defined as a real-valued vector), autoencoders have recently been applied to the discrete-state setting (where y is defined as a categorical random variable, a tuple of variables or a graph). For example, such models have been used in the context of dependency parsing (Daum´e III, 2009), or in the context of POS tagging and word alignment (Ammar et al., 2014; Lin et al., 2015a). The most related previous work (Titov and Khoddam, 2015) considers induction of semantic roles of verbal arguments (e.g., an agent, a performer of an action vs. a patient, an affected entity), though no grouping of predicates into relations was considered. We refer to such models as discrete-state autoencoders. We use different model families for the decoding and reconstruction components. The encoding part is a log-linear feature-rich model, while the reconstruction part is a tensor (or matrix) factorization 234 model which seeks to reconstruct entities, relying on the outcome of the encoding component. 4.1 Encoding component The encoding componen"
Q16-1017,P11-1145,1,0.839419,"se data generated automatically by aligning knowledge bases (e.g., Freebase) with text (called distantlysupervised methods). Both classes of approaches assume a predefined inventory of relations and a manually constructed resource. In contrast, the focus of this paper is on opendomain unsupervised RE (also known as relation discovery) where no fixed inventory of relations is provided to the learner. The methods induce relations from the data itself. Previous work on this task (Banko et al., 2007), as well as on its generalization, called unsupervised semantic parsing (Poon and Domingos, 2009; Titov and Klementiev, 2011), groups patterns between entity pairs (e.g., wrote a review, wrote a critique and reviewed) and uses these clusters as relations. Other approaches (e.g., Shinyama and Sekine (2006); Yao et al. (2011); Yao et al. (2012); de Lacalle and Lapata (2013)), including the one introduced in this paper, perform context-sensitive clustering, that is, they treat relations as latent variables and induce them for each entity-pair occurrence individually. Rather than relying solely on a pattern between entity pairs, the latter class of methods can use additional context to decide that Napoleon reviewed the"
Q16-1017,D13-1136,0,0.0106946,"ing to some similarity function, to a seed set of labeled examples. The process continues until some convergence criteria is met. Even though this approach is not very labor-intensive (i.e., it requires only few manually labeled examples for the initial seed set), it requires some domain knowledge from the model designer. In contrast, unsupervised models are domain-agnostic and require only unlabeled text. Knowledge base factorization. Knowledge base completion via matrix or tensor factorization has received a lot of attention in the past few years (Bordes et al., 2011; Jenatton et al., 2012; Weston et al., 2013; Bordes et al., 2013; Socher et al., 2013; Garc´ıa-Dur´an et al., 2014; Bordes et al., 2014; Lin et al., 2015b; Chang et al., 2014; Nickel et al., 2011). But in contrast to what we propose here, namely, induction of new relations, these models factorize relations already present in knowledge bases. Universal schema methods (Riedel et al., 2013) use factorization models to infer facts (e.g., predict missing entities), but they do not attempt to induce relations. In other words, they consider each given context as a relation and induce an embedding for each of them. They do not attempt to induc"
Q16-1017,D11-1135,0,0.40093,"rvised approaches, such as distantly supervised methods and bootstrapping (Mintz et al., 2009; Agichtein and Gravano, 2000), reduce the amount of necessary supervision, they still require examples for every relation considered. These limitations led to the emergence of unsupervised approaches for RE. These methods extract surface or syntactic patterns between two entities and either directly use these patterns as substitutes for semantic relations (Banko et al., 2007; Banko and Etzioni, 2008) or cluster the patterns (sometimes in context-sensitive way) to form relations (Lin and Pantel, 2001; Yao et al., 2011; Nakashole et al., 2012; Yao et al., 2012). The existing methods, given their generative (or agglomerative clustering) nature, rely on simpler features than their supervised counterparts and also make strong modeling assumptions (e.g., assuming that arguments are conditionally independent of each other given the relation). These shortcomings are likely to harm their performance. In this work, we tackle the aforementioned challenges and introduce a new model for unsupervised relation extraction. We also describe an efficient estimation algorithm which lets us experiment with large unannotated"
Q16-1017,P12-1075,0,0.77046,"vised methods and bootstrapping (Mintz et al., 2009; Agichtein and Gravano, 2000), reduce the amount of necessary supervision, they still require examples for every relation considered. These limitations led to the emergence of unsupervised approaches for RE. These methods extract surface or syntactic patterns between two entities and either directly use these patterns as substitutes for semantic relations (Banko et al., 2007; Banko and Etzioni, 2008) or cluster the patterns (sometimes in context-sensitive way) to form relations (Lin and Pantel, 2001; Yao et al., 2011; Nakashole et al., 2012; Yao et al., 2012). The existing methods, given their generative (or agglomerative clustering) nature, rely on simpler features than their supervised counterparts and also make strong modeling assumptions (e.g., assuming that arguments are conditionally independent of each other given the relation). These shortcomings are likely to harm their performance. In this work, we tackle the aforementioned challenges and introduce a new model for unsupervised relation extraction. We also describe an efficient estimation algorithm which lets us experiment with large unannotated collections. Our model is composed of two c"
Q16-1017,D15-1203,0,0.024536,"tting, though we leave any serious investigation of this set-up for future work. 241 6 Additional Related Work In this section, we mainly consider lines of related work not discussed in other sections of the paper, and we emphasize their relationship to our approach. Distant supervision. These methods can be regarded as a half-way point between unsupervised and supervised methods. Distantly supervised models are trained on data generated automatically by aligning knowledge bases (e.g., Freebase and Wikipedia infoboxes) with text (Mintz et al., 2009; Riedel et al., 2010; Surdeanu et al., 2012; Zeng et al., 2015). Similarly to our method they can use feature-rich models without the need for manually annotated data. However, a relation extractor trained in this way will only be able to predict relations already present in a knowledge base. These methods cannot be used to discover new relations. The framework we propose is completely unsupervised and does not have this shortcoming. Bootstrapping. Bootstrapping methods for relation extraction (Agichtein and Gravano, 2000; Brin, 1998; Batista et al., 2015) iteratively label new examples by finding the ones which are the most similar, according to some sim"
Q16-1017,D13-1040,0,\N,Missing
Q17-1003,N16-1067,1,0.720394,"depends on the predictability of the referent. 1.1 Scripts Scripts represent knowledge about typical event sequences (Schank and Abelson, 1977), for example the sequence of events happening when eating at a restaurant. Script knowledge thereby includes events like order, bring and eat as well as participants of those events, e.g., menu, waiter, food, guest. Existing methods for acquiring script knowledge are based on extracting narrative chains from text (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015; Modi, 2016; Ahrendt and Demberg, 2016) or by eliciting script knowledge via Crowdsourcing on Mechanical Turk (Regneri et al., 2010; Frermann et al., 2014; Modi and Titov, 2014). Modelling anticipated events and participants is motivated by evidence showing that event representations in humans contain information not only about the current event, but also about previous and future states, that is, humans generate anticipations about event sequences during normal language 33 comprehension (Sch¨utz-Bosbach and Prinz, 2007). Script knowledge representations have been shown to be useful in NLP applications for ambiguity resolution duri"
Q17-1003,J10-4006,0,0.0235405,"ree-valued feature indicates whether the previous mention of the candidate DR d is a pronoun, a non-pronominal noun phrase, or has never been observed before. 4.2.2 Selectional Preferences Feature The selectional preference feature captures how well the candidate DR d fits a given syntactic position r of a given verbal predicate v. It is computed as the cosine similarity simcos (xTd , xv,r ) of a vector-space representation of the DR xd and a structured vector-space representation of the predicate xv,r . The similarities are calculated using a Distributional Memory approach similar to that of Baroni and Lenci (2010). Their structured vector space representation has been shown to work well on tasks that evaluate correlation with human thematic fit estimates (Baroni and Lenci, 2010; Baroni et al., 2014; Sayeed et al., 2016) and is thus suited to our task. The representation xd is computed as an average of head word representations of all the previous mentions of DR d, where the word vectors are obtained from the TypeDM model of Baroni and Lenci (2010). This is a count-based, third-order cooccurrence tensor whose indices are a word w0 , a second word w1 , and a complex syntactic relation r, which is used as"
Q17-1003,P14-1023,0,0.0594907,"ture The selectional preference feature captures how well the candidate DR d fits a given syntactic position r of a given verbal predicate v. It is computed as the cosine similarity simcos (xTd , xv,r ) of a vector-space representation of the DR xd and a structured vector-space representation of the predicate xv,r . The similarities are calculated using a Distributional Memory approach similar to that of Baroni and Lenci (2010). Their structured vector space representation has been shown to work well on tasks that evaluate correlation with human thematic fit estimates (Baroni and Lenci, 2010; Baroni et al., 2014; Sayeed et al., 2016) and is thus suited to our task. The representation xd is computed as an average of head word representations of all the previous mentions of DR d, where the word vectors are obtained from the TypeDM model of Baroni and Lenci (2010). This is a count-based, third-order cooccurrence tensor whose indices are a word w0 , a second word w1 , and a complex syntactic relation r, which is used as a stand-in for a semantic link. The values for each (w0 , r, w1 ) cell of the tensor are the local mutual information (LMI) estimates obtained from a dependency-parsed combination of larg"
Q17-1003,P08-1090,0,0.722488,"uman expectations. • testing the hypothesis of Tily and Piantadosi that the choice of the type of referring expression (pronoun or full NP) depends on the predictability of the referent. 1.1 Scripts Scripts represent knowledge about typical event sequences (Schank and Abelson, 1977), for example the sequence of events happening when eating at a restaurant. Script knowledge thereby includes events like order, bring and eat as well as participants of those events, e.g., menu, waiter, food, guest. Existing methods for acquiring script knowledge are based on extracting narrative chains from text (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015; Modi, 2016; Ahrendt and Demberg, 2016) or by eliciting script knowledge via Crowdsourcing on Mechanical Turk (Regneri et al., 2010; Frermann et al., 2014; Modi and Titov, 2014). Modelling anticipated events and participants is motivated by evidence showing that event representations in humans contain information not only about the current event, but also about previous and future states, that is, humans generate anticipations about event sequences during normal language 33 comprehension (Sch¨utz"
Q17-1003,P09-1068,0,0.121207,"the hypothesis of Tily and Piantadosi that the choice of the type of referring expression (pronoun or full NP) depends on the predictability of the referent. 1.1 Scripts Scripts represent knowledge about typical event sequences (Schank and Abelson, 1977), for example the sequence of events happening when eating at a restaurant. Script knowledge thereby includes events like order, bring and eat as well as participants of those events, e.g., menu, waiter, food, guest. Existing methods for acquiring script knowledge are based on extracting narrative chains from text (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015; Modi, 2016; Ahrendt and Demberg, 2016) or by eliciting script knowledge via Crowdsourcing on Mechanical Turk (Regneri et al., 2010; Frermann et al., 2014; Modi and Titov, 2014). Modelling anticipated events and participants is motivated by evidence showing that event representations in humans contain information not only about the current event, but also about previous and future states, that is, humans generate anticipations about event sequences during normal language 33 comprehension (Sch¨utz-Bosbach and Prinz, 2007). Sc"
Q17-1003,E14-1006,1,0.888764,"chank and Abelson, 1977), for example the sequence of events happening when eating at a restaurant. Script knowledge thereby includes events like order, bring and eat as well as participants of those events, e.g., menu, waiter, food, guest. Existing methods for acquiring script knowledge are based on extracting narrative chains from text (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015; Modi, 2016; Ahrendt and Demberg, 2016) or by eliciting script knowledge via Crowdsourcing on Mechanical Turk (Regneri et al., 2010; Frermann et al., 2014; Modi and Titov, 2014). Modelling anticipated events and participants is motivated by evidence showing that event representations in humans contain information not only about the current event, but also about previous and future states, that is, humans generate anticipations about event sequences during normal language 33 comprehension (Sch¨utz-Bosbach and Prinz, 2007). Script knowledge representations have been shown to be useful in NLP applications for ambiguity resolution during reference resolution (Rahman and Ng, 2012). 2 Data: The InScript Corpus Ordinary texts, including narratives, en"
Q17-1003,E12-1034,0,0.242357,"Missing"
Q17-1003,W14-1606,1,0.882589,"), for example the sequence of events happening when eating at a restaurant. Script knowledge thereby includes events like order, bring and eat as well as participants of those events, e.g., menu, waiter, food, guest. Existing methods for acquiring script knowledge are based on extracting narrative chains from text (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015; Modi, 2016; Ahrendt and Demberg, 2016) or by eliciting script knowledge via Crowdsourcing on Mechanical Turk (Regneri et al., 2010; Frermann et al., 2014; Modi and Titov, 2014). Modelling anticipated events and participants is motivated by evidence showing that event representations in humans contain information not only about the current event, but also about previous and future states, that is, humans generate anticipations about event sequences during normal language 33 comprehension (Sch¨utz-Bosbach and Prinz, 2007). Script knowledge representations have been shown to be useful in NLP applications for ambiguity resolution during reference resolution (Rahman and Ng, 2012). 2 Data: The InScript Corpus Ordinary texts, including narratives, encode script structure i"
Q17-1003,L16-1555,1,0.929396,"have been shown to be useful in NLP applications for ambiguity resolution during reference resolution (Rahman and Ng, 2012). 2 Data: The InScript Corpus Ordinary texts, including narratives, encode script structure in a way that is too complex and too implicit at the same time to enable a systematic study of script-based expectation. They contain interleaved references to many different scripts, and they usually refer to single scripts in a point-wise fashion only, relying on the ability of the reader to infer the full event chain using their background knowledge. We use the InScript corpus (Modi et al., 2016) to study the predictive effect of script knowledge. InScript is a crowdsourced corpus of simple narrative texts. Participants were asked to write about a specific activity (e.g., a restaurant visit, a bus ride, or a grocery shopping event) which they personally experienced, and they were instructed to tell the story as if explaining the activity to a child. This resulted in stories that are centered around a specific scenario and that explicitly mention mundane details. Thus, they generally realize longer event chains associated with a single script, which makes them particularly appropriate"
Q17-1003,K16-1008,1,0.856447,"or full NP) depends on the predictability of the referent. 1.1 Scripts Scripts represent knowledge about typical event sequences (Schank and Abelson, 1977), for example the sequence of events happening when eating at a restaurant. Script knowledge thereby includes events like order, bring and eat as well as participants of those events, e.g., menu, waiter, food, guest. Existing methods for acquiring script knowledge are based on extracting narrative chains from text (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015; Modi, 2016; Ahrendt and Demberg, 2016) or by eliciting script knowledge via Crowdsourcing on Mechanical Turk (Regneri et al., 2010; Frermann et al., 2014; Modi and Titov, 2014). Modelling anticipated events and participants is motivated by evidence showing that event representations in humans contain information not only about the current event, but also about previous and future states, that is, humans generate anticipations about event sequences during normal language 33 comprehension (Sch¨utz-Bosbach and Prinz, 2007). Script knowledge representations have been shown to be useful in NLP applications f"
Q17-1003,N16-1098,0,0.0419051,"that were used; specifically, we would expect that larger predictability effects might be observable at script boundaries, rather than within a script, as is the case in our stories. A next step in moving our participant prediction model towards NLP applications would be to replicate our modelling results on automatic textto-script mapping instead of gold-standard data as done here (in order to approximate human level of processing). Furthermore, we aim to move to more complex text types that include reference to several scripts. We plan to consider the recently published ROC Stories corpus (Mostafazadeh et al., 2016), a large crowdsourced collection of topically unrestricted short and simple narratives, as a basis for these next steps in our research. Acknowledgments We thank the editors and the anonymous reviewers for their insightful suggestions. We would like to thank Florian Pusse for helping with the Amazon Mechanical Turk experiment. We would also like to thank Simon Ostermann and Tatjana Anikina for helping with the InScript corpus. This research was partially supported by the German Research Foundation (DFG) as part of SFB 1102 ‘Information Density and Linguistic Encoding’, European Research Counc"
Q17-1003,N15-1082,0,0.014094,"anical Turk experiment (Figure 2), our referent prediction model is asked to guess the upcoming DR. relation r, we collect all the predicates in the training set which have the participant type p in the position r. The embedding of the DR xp,r is given by the average embedding of these predicates. The feature is computed as the dot product of xp,r and the word embedding of the predicate v. Predicate schemas The following feature captures a specific aspect of knowledge about prototypical sequences of events. This knowledge is called predicate schemas in the recent co-reference modeling work of Peng et al. (2015). In predicate schemas, the goal is to model pairs of events such that if a DR d participated in the first event (in a specific role), it is likely to participate in the second event (again, in a specific role). For example, in the restaurant scenario, if one observes a phrase John ordered, one is likely to see John waited somewhere later in the document. Specific arguments are not that important (where it is John or some other DR), what is important is that the argument is reused across the predicates. This would correspond to the rule X-subject-of-order → X-subject-of-eat.4 Unlike the previo"
Q17-1003,E14-1024,0,0.0438992,"hoice of the type of referring expression (pronoun or full NP) depends on the predictability of the referent. 1.1 Scripts Scripts represent knowledge about typical event sequences (Schank and Abelson, 1977), for example the sequence of events happening when eating at a restaurant. Script knowledge thereby includes events like order, bring and eat as well as participants of those events, e.g., menu, waiter, food, guest. Existing methods for acquiring script knowledge are based on extracting narrative chains from text (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015; Modi, 2016; Ahrendt and Demberg, 2016) or by eliciting script knowledge via Crowdsourcing on Mechanical Turk (Regneri et al., 2010; Frermann et al., 2014; Modi and Titov, 2014). Modelling anticipated events and participants is motivated by evidence showing that event representations in humans contain information not only about the current event, but also about previous and future states, that is, humans generate anticipations about event sequences during normal language 33 comprehension (Sch¨utz-Bosbach and Prinz, 2007). Script knowledge representations have been shown"
Q17-1003,D12-1071,0,0.0196634,"nowledge via Crowdsourcing on Mechanical Turk (Regneri et al., 2010; Frermann et al., 2014; Modi and Titov, 2014). Modelling anticipated events and participants is motivated by evidence showing that event representations in humans contain information not only about the current event, but also about previous and future states, that is, humans generate anticipations about event sequences during normal language 33 comprehension (Sch¨utz-Bosbach and Prinz, 2007). Script knowledge representations have been shown to be useful in NLP applications for ambiguity resolution during reference resolution (Rahman and Ng, 2012). 2 Data: The InScript Corpus Ordinary texts, including narratives, encode script structure in a way that is too complex and too implicit at the same time to enable a systematic study of script-based expectation. They contain interleaved references to many different scripts, and they usually refer to single scripts in a point-wise fashion only, relying on the ability of the reader to infer the full event chain using their background knowledge. We use the InScript corpus (Modi et al., 2016) to study the predictive effect of script knowledge. InScript is a crowdsourced corpus of simple narrative"
Q17-1003,P10-1100,1,0.908959,"cal event sequences (Schank and Abelson, 1977), for example the sequence of events happening when eating at a restaurant. Script knowledge thereby includes events like order, bring and eat as well as participants of those events, e.g., menu, waiter, food, guest. Existing methods for acquiring script knowledge are based on extracting narrative chains from text (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015; Modi, 2016; Ahrendt and Demberg, 2016) or by eliciting script knowledge via Crowdsourcing on Mechanical Turk (Regneri et al., 2010; Frermann et al., 2014; Modi and Titov, 2014). Modelling anticipated events and participants is motivated by evidence showing that event representations in humans contain information not only about the current event, but also about previous and future states, that is, humans generate anticipations about event sequences during normal language 33 comprehension (Sch¨utz-Bosbach and Prinz, 2007). Script knowledge representations have been shown to be useful in NLP applications for ambiguity resolution during reference resolution (Rahman and Ng, 2012). 2 Data: The InScript Corpus Ordinary texts, i"
Q17-1003,S15-1024,1,0.925336,"Missing"
Q17-1003,W16-2518,1,0.85711,"preference feature captures how well the candidate DR d fits a given syntactic position r of a given verbal predicate v. It is computed as the cosine similarity simcos (xTd , xv,r ) of a vector-space representation of the DR xd and a structured vector-space representation of the predicate xv,r . The similarities are calculated using a Distributional Memory approach similar to that of Baroni and Lenci (2010). Their structured vector space representation has been shown to work well on tasks that evaluate correlation with human thematic fit estimates (Baroni and Lenci, 2010; Baroni et al., 2014; Sayeed et al., 2016) and is thus suited to our task. The representation xd is computed as an average of head word representations of all the previous mentions of DR d, where the word vectors are obtained from the TypeDM model of Baroni and Lenci (2010). This is a count-based, third-order cooccurrence tensor whose indices are a word w0 , a second word w1 , and a complex syntactic relation r, which is used as a stand-in for a semantic link. The values for each (w0 , r, w1 ) cell of the tensor are the local mutual information (LMI) estimates obtained from a dependency-parsed combination of large corpora (ukWaC, BNC,"
S13-1044,P98-1013,0,0.0410757,"e have observed no significant improvements from repeating the procedure, possibly owing to the noise introduced by the errors in preprocessing. In the evaluation we run only one iteration. In the notation introduced above, the self-training baseline model (S ELF) is trained on Pβ0 , the joint model (J OINT) – on Pβ1 and the combined model (C OMB) – on Tβ1 . 2.1 Modeling Role Correspondence It is necessary to distinguish between semantic roles and their interpretation in a particular context. The former can be defined in a variety of ways, depending on the formalism used. In case of FrameNet (Baker et al., 1998), for example, the interpretation of a semantic role (frame element) is explicitly provided for each separate frame, so a frame and a frame element label together describe the semantics of an argument. PropBank (Palmer et al., 2005a) follows a mixed strategy – the labels for a relatively small set of core roles are numbered and their interpretations are provided separately for each predicate (although those of the first two roles, A0 and A1, consistently denote what is known as ProtoAgent and Proto-Patient), while modifiers (Merlo and Leybold, 2001) bear labels that are interpreted consistentl"
S13-1044,W09-1206,0,0.0646302,"Missing"
S13-1044,C10-1011,0,0.032564,"amely English vs German, Spanish, Czech and Chinese, which we will denote en-de, en-es, en-cz and en-zh respectively. 3.1 Parallel Data The parallel data for the first three language pairs is drawn from Europarl v6 (Koehn, 2005) and from MultiUN (Eisele and Chen, 2010) for EnglishChinese. We applied Stanford Tokenizer for English, tokenizer scripts (Koehn, 2005) provided with the Europarl corpus to German, Spanish and Czech, and Stanford Chinese Segmenter (Chang et al., 2008) to Chinese, then performed POS-tagging, morphology tagging (where applicable) and dependency parsing using MATE-tools (Bohnet, 2010). Word alignments were acquired using GIZA++ (Och and Ney, 2003) with its standard settings. Predicate identification on the parallel data was done using the supervised classifiers of the monolingual SRL systems, except for German, where a simple heuristic had to be used instead, as only some of the predicates are marked in the training data, which makes it hard to train a supervised classifier. Following van der Plas et al. (2011), we then retain only those sentences where all identified predicates were aligned. In the experiments we used 50 thousand predicate pairs in each case, as increasin"
S13-1044,burchardt-etal-2006-salsa,0,0.0635526,"Missing"
S13-1044,P07-1036,0,0.0207405,"s nontrivial (Hwang et al., 2010), we are using a heuristic to address the most common version of this problem, i.e. a preposition or an auxiliary verb being an argument head. In such a case we also take into account any alignment links involving the head’s immediate descendants. 3.3 Implementation Our system is based on that of Bj¨orkelund et al. (2009). It is a pipeline system comprised of a set of binary or multiclass linear classifiers. Both here and in the projection model, the classifiers are trained using Liblinear (Fan et al., 2008). We employed a uniqueness constraint on role labels (Chang et al., 2007), preventing some of them from being assigned to more than one argument in the same predicate, which appears to be more reliable in a low-resource setting we consider than the reranker the original system employed. The constraint is enforced in the monolingual model inference using a beam-search approximation with the beam size of 10. The label uniqueness information was derived from the training sets. 3.4 The Projection Model Each projection model is realized by a single linear classifier applied to each argument pair independently. It relies on features derived from the source semantic role"
S13-1044,W08-0336,0,0.0582768,"Missing"
S13-1044,D12-1001,0,0.0188315,"t makes, compared to a regular one (table 4). 5 Related Work There is a number of approaches to semi-supervised semantic role labeling, and most suggest that some external supervision is required for such approaches to work (He and Gildea, 2006), such as measures of syntactic and semantic similarity (F¨urstenau and Lapata, 2009) or external confidence measures (Goldwasser et al., 2011). The alternative we propose is primarily motivated by the research on annotation projection (Pad´o and Lapata, 2009; van der Plas et al., 2011; Annesi and Basili, 2010; Naseem et al., 2012) and direct transfer (Durrett et al., 2012; Søgaard, 2011; Lopez et al., 2008; McDonald et al., 2011). The key difference of the present approach compared to annotation projection is that we assume en-cz* en-cz en-de* en-de en-es en-zh cz-en* cz-en de-en* de-en es-en* es-en zh-en* zh-en I NIT 61.11 62.45 66.81 70.39 64.20 75.80 66.82 74.93 66.82 74.93 66.82 74.93 66.82 74.93 S ELF 60.68 62.15 63.96 68.34 64.51 73.52 63.95 71.60 63.58 71.31 63.95 71.47 64.51 72.26 J OINT 72.49 70.19 76.78 79.22 75.43 76.75 70.75 79.70 69.46 77.34 69.92 79.55 67.19 76.51 ∆S ELF 11.81 8.04 12.82 10.88 10.92 3.22 6.80 8.10 5.88 6.03 5.97 8.08 2.68 4.26 ∆I"
S13-1044,eisele-chen-2010-multiun,0,0.0673384,"implementation as in the projection setup. Moreover, the inference procedure for projection setup is a special case of this one with γs (n) set to 0. The algorithm also demonstrates convergence similar to that of the projection version, although it lacks the optimality guarantees. 3 Experimental Setup We evaluate our approach on four language pairs, namely English vs German, Spanish, Czech and Chinese, which we will denote en-de, en-es, en-cz and en-zh respectively. 3.1 Parallel Data The parallel data for the first three language pairs is drawn from Europarl v6 (Koehn, 2005) and from MultiUN (Eisele and Chen, 2010) for EnglishChinese. We applied Stanford Tokenizer for English, tokenizer scripts (Koehn, 2005) provided with the Europarl corpus to German, Spanish and Czech, and Stanford Chinese Segmenter (Chang et al., 2008) to Chinese, then performed POS-tagging, morphology tagging (where applicable) and dependency parsing using MATE-tools (Bohnet, 2010). Word alignments were acquired using GIZA++ (Och and Ney, 2003) with its standard settings. Predicate identification on the parallel data was done using the supervised classifiers of the monolingual SRL systems, except for German, where a simple heuristic"
S13-1044,P12-2025,0,0.0169533,"oach can be seen as similar to cotraining (Blum and Mitchell, 1998), other applications of which to NLP are too numerous to list here. Most closely related is the joint inference in Zhuang and Zong (2010), the main difference being that it relies on a manually annotated parallel corpus, aligned on the argument level, and evaluates only the inference procedure and only on in-domain data. Other related approaches include Kim et al. (2010), where a cross-lingual transfer of relations is performed (which basically represent parts of the predicate-argument structure considered by SRL methods), and Frermann and Bond (2012), where semantic structure matching is used to rank HPSG parses for parallel sentences. Unsupervised semantic role labeling methods (Lang and Lapata, 2010; Lang and Lapata, 2011; Titov and Klementiev, 2012a; Lorenzo and Cerisara, 2012) present an alternative to the crosslingual information propagation approaches such as ours, and at least one the methods in this area also makes use of parallel data (Titov and Klementiev, 2012b). 325 We have presented an approach to information transfer between SRL systems for different language pairs using parallel data. The task proves challenging due to non-"
S13-1044,D09-1002,0,0.0753417,"Missing"
S13-1044,P11-1149,0,0.0222651,"more accurate monolingual models on the parallel data – those trained on the full training set, rather than the initial training set used in this particular experiment. We refer to the resulting RCM as oracle and assess the difference it makes, compared to a regular one (table 4). 5 Related Work There is a number of approaches to semi-supervised semantic role labeling, and most suggest that some external supervision is required for such approaches to work (He and Gildea, 2006), such as measures of syntactic and semantic similarity (F¨urstenau and Lapata, 2009) or external confidence measures (Goldwasser et al., 2011). The alternative we propose is primarily motivated by the research on annotation projection (Pad´o and Lapata, 2009; van der Plas et al., 2011; Annesi and Basili, 2010; Naseem et al., 2012) and direct transfer (Durrett et al., 2012; Søgaard, 2011; Lopez et al., 2008; McDonald et al., 2011). The key difference of the present approach compared to annotation projection is that we assume en-cz* en-cz en-de* en-de en-es en-zh cz-en* cz-en de-en* de-en es-en* es-en zh-en* zh-en I NIT 61.11 62.45 66.81 70.39 64.20 75.80 66.82 74.93 66.82 74.93 66.82 74.93 66.82 74.93 S ELF 60.68 62.15 63.96 68.34 64"
S13-1044,hajic-etal-2012-announcing,0,0.0484793,"Missing"
S13-1044,W10-0801,0,0.0187358,"and the test set. The secondary (“out-of-domain”) test sets are preserved as they are. In dependency-based SRL, only heads of syntactic constituents are marked with semantic roles. The heads of corresponding arguments may or may not align, however, even if the arguments are lexically very similar, because their syntactic structure may differ. In general, one would have to identify the whole phrase for each argument and take into account the links between constituents, rather than single words (Pad´o and Lapata, 2005). As reconstructing the constituents from the dependency tree is nontrivial (Hwang et al., 2010), we are using a heuristic to address the most common version of this problem, i.e. a preposition or an auxiliary verb being an argument head. In such a case we also take into account any alignment links involving the head’s immediate descendants. 3.3 Implementation Our system is based on that of Bj¨orkelund et al. (2009). It is a pipeline system comprised of a set of binary or multiclass linear classifiers. Both here and in the projection model, the classifiers are trained using Liblinear (Fan et al., 2008). We employed a uniqueness constraint on role labels (Chang et al., 2007), preventing s"
S13-1044,C10-1064,0,0.0191311,"some amount of training data for the target language, possibly using a different inventory of semantic roles. As mentioned previously, from the training point of view this approach can be seen as similar to cotraining (Blum and Mitchell, 1998), other applications of which to NLP are too numerous to list here. Most closely related is the joint inference in Zhuang and Zong (2010), the main difference being that it relies on a manually annotated parallel corpus, aligned on the argument level, and evaluates only the inference procedure and only on in-domain data. Other related approaches include Kim et al. (2010), where a cross-lingual transfer of relations is performed (which basically represent parts of the predicate-argument structure considered by SRL methods), and Frermann and Bond (2012), where semantic structure matching is used to rank HPSG parses for parallel sentences. Unsupervised semantic role labeling methods (Lang and Lapata, 2010; Lang and Lapata, 2011; Titov and Klementiev, 2012a; Lorenzo and Cerisara, 2012) present an alternative to the crosslingual information propagation approaches such as ours, and at least one the methods in this area also makes use of parallel data (Titov and Kle"
S13-1044,2005.mtsummit-papers.11,0,0.0709311,"allows us to use the same RCM implementation as in the projection setup. Moreover, the inference procedure for projection setup is a special case of this one with γs (n) set to 0. The algorithm also demonstrates convergence similar to that of the projection version, although it lacks the optimality guarantees. 3 Experimental Setup We evaluate our approach on four language pairs, namely English vs German, Spanish, Czech and Chinese, which we will denote en-de, en-es, en-cz and en-zh respectively. 3.1 Parallel Data The parallel data for the first three language pairs is drawn from Europarl v6 (Koehn, 2005) and from MultiUN (Eisele and Chen, 2010) for EnglishChinese. We applied Stanford Tokenizer for English, tokenizer scripts (Koehn, 2005) provided with the Europarl corpus to German, Spanish and Czech, and Stanford Chinese Segmenter (Chang et al., 2008) to Chinese, then performed POS-tagging, morphology tagging (where applicable) and dependency parsing using MATE-tools (Bohnet, 2010). Word alignments were acquired using GIZA++ (Och and Ney, 2003) with its standard settings. Predicate identification on the parallel data was done using the supervised classifiers of the monolingual SRL systems, ex"
S13-1044,N10-1137,0,0.0149296,"s the joint inference in Zhuang and Zong (2010), the main difference being that it relies on a manually annotated parallel corpus, aligned on the argument level, and evaluates only the inference procedure and only on in-domain data. Other related approaches include Kim et al. (2010), where a cross-lingual transfer of relations is performed (which basically represent parts of the predicate-argument structure considered by SRL methods), and Frermann and Bond (2012), where semantic structure matching is used to rank HPSG parses for parallel sentences. Unsupervised semantic role labeling methods (Lang and Lapata, 2010; Lang and Lapata, 2011; Titov and Klementiev, 2012a; Lorenzo and Cerisara, 2012) present an alternative to the crosslingual information propagation approaches such as ours, and at least one the methods in this area also makes use of parallel data (Titov and Klementiev, 2012b). 325 We have presented an approach to information transfer between SRL systems for different language pairs using parallel data. The task proves challenging due to non-trivial mapping between the role labels used in different SRL annotation schemes and the nature of parallel data – the difference in domains and the limit"
S13-1044,P11-1112,0,0.0171722,"n Zhuang and Zong (2010), the main difference being that it relies on a manually annotated parallel corpus, aligned on the argument level, and evaluates only the inference procedure and only on in-domain data. Other related approaches include Kim et al. (2010), where a cross-lingual transfer of relations is performed (which basically represent parts of the predicate-argument structure considered by SRL methods), and Frermann and Bond (2012), where semantic structure matching is used to rank HPSG parses for parallel sentences. Unsupervised semantic role labeling methods (Lang and Lapata, 2010; Lang and Lapata, 2011; Titov and Klementiev, 2012a; Lorenzo and Cerisara, 2012) present an alternative to the crosslingual information propagation approaches such as ours, and at least one the methods in this area also makes use of parallel data (Titov and Klementiev, 2012b). 325 We have presented an approach to information transfer between SRL systems for different language pairs using parallel data. The task proves challenging due to non-trivial mapping between the role labels used in different SRL annotation schemes and the nature of parallel data – the difference in domains and the limited accuracy of the prep"
S13-1044,I08-3008,0,0.0720663,"able 4). 5 Related Work There is a number of approaches to semi-supervised semantic role labeling, and most suggest that some external supervision is required for such approaches to work (He and Gildea, 2006), such as measures of syntactic and semantic similarity (F¨urstenau and Lapata, 2009) or external confidence measures (Goldwasser et al., 2011). The alternative we propose is primarily motivated by the research on annotation projection (Pad´o and Lapata, 2009; van der Plas et al., 2011; Annesi and Basili, 2010; Naseem et al., 2012) and direct transfer (Durrett et al., 2012; Søgaard, 2011; Lopez et al., 2008; McDonald et al., 2011). The key difference of the present approach compared to annotation projection is that we assume en-cz* en-cz en-de* en-de en-es en-zh cz-en* cz-en de-en* de-en es-en* es-en zh-en* zh-en I NIT 61.11 62.45 66.81 70.39 64.20 75.80 66.82 74.93 66.82 74.93 66.82 74.93 66.82 74.93 S ELF 60.68 62.15 63.96 68.34 64.51 73.52 63.95 71.60 63.58 71.31 63.95 71.47 64.51 72.26 J OINT 72.49 70.19 76.78 79.22 75.43 76.75 70.75 79.70 69.46 77.34 69.92 79.55 67.19 76.51 ∆S ELF 11.81 8.04 12.82 10.88 10.92 3.22 6.80 8.10 5.88 6.03 5.97 8.08 2.68 4.26 ∆I NIT 11.38 7.74 9.97 8.84 11.23 0.9"
S13-1044,W12-3404,0,0.0160013,"that it relies on a manually annotated parallel corpus, aligned on the argument level, and evaluates only the inference procedure and only on in-domain data. Other related approaches include Kim et al. (2010), where a cross-lingual transfer of relations is performed (which basically represent parts of the predicate-argument structure considered by SRL methods), and Frermann and Bond (2012), where semantic structure matching is used to rank HPSG parses for parallel sentences. Unsupervised semantic role labeling methods (Lang and Lapata, 2010; Lang and Lapata, 2011; Titov and Klementiev, 2012a; Lorenzo and Cerisara, 2012) present an alternative to the crosslingual information propagation approaches such as ours, and at least one the methods in this area also makes use of parallel data (Titov and Klementiev, 2012b). 325 We have presented an approach to information transfer between SRL systems for different language pairs using parallel data. The task proves challenging due to non-trivial mapping between the role labels used in different SRL annotation schemes and the nature of parallel data – the difference in domains and the limited accuracy of the preprocessing tools. We observe consistent improvements over s"
S13-1044,D11-1006,0,0.0440772,"ork There is a number of approaches to semi-supervised semantic role labeling, and most suggest that some external supervision is required for such approaches to work (He and Gildea, 2006), such as measures of syntactic and semantic similarity (F¨urstenau and Lapata, 2009) or external confidence measures (Goldwasser et al., 2011). The alternative we propose is primarily motivated by the research on annotation projection (Pad´o and Lapata, 2009; van der Plas et al., 2011; Annesi and Basili, 2010; Naseem et al., 2012) and direct transfer (Durrett et al., 2012; Søgaard, 2011; Lopez et al., 2008; McDonald et al., 2011). The key difference of the present approach compared to annotation projection is that we assume en-cz* en-cz en-de* en-de en-es en-zh cz-en* cz-en de-en* de-en es-en* es-en zh-en* zh-en I NIT 61.11 62.45 66.81 70.39 64.20 75.80 66.82 74.93 66.82 74.93 66.82 74.93 66.82 74.93 S ELF 60.68 62.15 63.96 68.34 64.51 73.52 63.95 71.60 63.58 71.31 63.95 71.47 64.51 72.26 J OINT 72.49 70.19 76.78 79.22 75.43 76.75 70.75 79.70 69.46 77.34 69.92 79.55 67.19 76.51 ∆S ELF 11.81 8.04 12.82 10.88 10.92 3.22 6.80 8.10 5.88 6.03 5.97 8.08 2.68 4.26 ∆I NIT 11.38 7.74 9.97 8.84 11.23 0.94 3.93 4.76 2.64 2.41 3."
S13-1044,W01-0715,0,0.0457384,"nding on the formalism used. In case of FrameNet (Baker et al., 1998), for example, the interpretation of a semantic role (frame element) is explicitly provided for each separate frame, so a frame and a frame element label together describe the semantics of an argument. PropBank (Palmer et al., 2005a) follows a mixed strategy – the labels for a relatively small set of core roles are numbered and their interpretations are provided separately for each predicate (although those of the first two roles, A0 and A1, consistently denote what is known as ProtoAgent and Proto-Patient), while modifiers (Merlo and Leybold, 2001) bear labels that are interpreted consistently across all predicates. Other resources, such as Prague Dependency Treebank (Hajiˇc et al., 2006), use a single set of semantic roles (functors), which are interpretable across different predicates. From the standpoint of defining the semantic similarity of parallel sentences, the important implication is that we cannot assume that the corresponding arguments should bear the same label, even if the annotation schemes used are compatible (Zhuang and Zong, 2010). Nor can we write down a single mapping between the roles that will be valid across diffe"
S13-1044,P12-1066,0,0.0218156,"RCM as oracle and assess the difference it makes, compared to a regular one (table 4). 5 Related Work There is a number of approaches to semi-supervised semantic role labeling, and most suggest that some external supervision is required for such approaches to work (He and Gildea, 2006), such as measures of syntactic and semantic similarity (F¨urstenau and Lapata, 2009) or external confidence measures (Goldwasser et al., 2011). The alternative we propose is primarily motivated by the research on annotation projection (Pad´o and Lapata, 2009; van der Plas et al., 2011; Annesi and Basili, 2010; Naseem et al., 2012) and direct transfer (Durrett et al., 2012; Søgaard, 2011; Lopez et al., 2008; McDonald et al., 2011). The key difference of the present approach compared to annotation projection is that we assume en-cz* en-cz en-de* en-de en-es en-zh cz-en* cz-en de-en* de-en es-en* es-en zh-en* zh-en I NIT 61.11 62.45 66.81 70.39 64.20 75.80 66.82 74.93 66.82 74.93 66.82 74.93 66.82 74.93 S ELF 60.68 62.15 63.96 68.34 64.51 73.52 63.95 71.60 63.58 71.31 63.95 71.47 64.51 72.26 J OINT 72.49 70.19 76.78 79.22 75.43 76.75 70.75 79.70 69.46 77.34 69.92 79.55 67.19 76.51 ∆S ELF 11.81 8.04 12.82 10.88 10.92 3.22"
S13-1044,J03-1002,0,0.0116056,"we will denote en-de, en-es, en-cz and en-zh respectively. 3.1 Parallel Data The parallel data for the first three language pairs is drawn from Europarl v6 (Koehn, 2005) and from MultiUN (Eisele and Chen, 2010) for EnglishChinese. We applied Stanford Tokenizer for English, tokenizer scripts (Koehn, 2005) provided with the Europarl corpus to German, Spanish and Czech, and Stanford Chinese Segmenter (Chang et al., 2008) to Chinese, then performed POS-tagging, morphology tagging (where applicable) and dependency parsing using MATE-tools (Bohnet, 2010). Word alignments were acquired using GIZA++ (Och and Ney, 2003) with its standard settings. Predicate identification on the parallel data was done using the supervised classifiers of the monolingual SRL systems, except for German, where a simple heuristic had to be used instead, as only some of the predicates are marked in the training data, which makes it hard to train a supervised classifier. Following van der Plas et al. (2011), we then retain only those sentences where all identified predicates were aligned. In the experiments we used 50 thousand predicate pairs in each case, as increasing the amount further did not yield noticeable benefits, while in"
S13-1044,H05-1108,0,0.259919,"Missing"
S13-1044,J05-1004,0,0.859249,"antly reduced by leveraging existing resources and the similarities between languages. This idea has lead to the development of crosslingual annotation projection approaches, which make use of parallel corpora (Pad´o and Lapata, 2009), as well as attempts to adapt models directly Ve školách se neučí rumunsky . LOC PAT Figure 1: Role correspondence in parallel sentences, an example. The notion of compatibility here is highly nontrivial, even for sentences translated as close to the original as possible. Zhuang and Zong (2010), for example, observe that in the English-Chinese parallel PropBank (Palmer et al., 2005b) corresponding arguments often bear different labels, even though the same inventory of semantic roles is used for both 317 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference c and the Shared Task, pages 317–327, Atlanta, Georgia, June 13-14, 2013. 2013 Association for Computational Linguistics languages and the annotation guidelines are similar. When different annotation schemes are considered, the problem is further complicated by the difference in the granularity of semantic roles used and varying notions of what is an argum"
S13-1044,W05-0309,0,0.310756,"antly reduced by leveraging existing resources and the similarities between languages. This idea has lead to the development of crosslingual annotation projection approaches, which make use of parallel corpora (Pad´o and Lapata, 2009), as well as attempts to adapt models directly Ve školách se neučí rumunsky . LOC PAT Figure 1: Role correspondence in parallel sentences, an example. The notion of compatibility here is highly nontrivial, even for sentences translated as close to the original as possible. Zhuang and Zong (2010), for example, observe that in the English-Chinese parallel PropBank (Palmer et al., 2005b) corresponding arguments often bear different labels, even though the same inventory of semantic roles is used for both 317 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference c and the Shared Task, pages 317–327, Atlanta, Georgia, June 13-14, 2013. 2013 Association for Computational Linguistics languages and the annotation guidelines are similar. When different annotation schemes are considered, the problem is further complicated by the difference in the granularity of semantic roles used and varying notions of what is an argum"
S13-1044,J08-2006,0,0.0450665,"ome sort of newswire text – Wall Street Journal in case of English, Xinhua newswire, Hong Kong news and Sinorama news magazine for Chinese, etc. Parallel data, on the other hand, comes from the proceedings of European Parliament and United Nations, which are quite different. For example, the sentences in the latter domain often start with someone being addressed, either by name or by title, which can hardly be expected to occur as often in a newspaper or a magazine article. As is well-known, the performance of many statistical tools drops significantly outside the domain they were trained on (Pradhan et al., 2008), and the preprocessing and SRL models used here are no exception, which results in relatively low quality of the initial predictions on the parallel text. The low argument identification performance, in particular, is presumably due to inaccurate dependency parses, on which it heavily relies. Several approached have been proposed to improve the accuracy of dependency parsers and other tools on out-of-domain data, but this is beyond the scope of this paper. In some cases (though seldom), sources of parallel data belonging to the same domain as the annotated training data can be obtained. Anoth"
S13-1044,P11-2120,0,0.0241252,"regular one (table 4). 5 Related Work There is a number of approaches to semi-supervised semantic role labeling, and most suggest that some external supervision is required for such approaches to work (He and Gildea, 2006), such as measures of syntactic and semantic similarity (F¨urstenau and Lapata, 2009) or external confidence measures (Goldwasser et al., 2011). The alternative we propose is primarily motivated by the research on annotation projection (Pad´o and Lapata, 2009; van der Plas et al., 2011; Annesi and Basili, 2010; Naseem et al., 2012) and direct transfer (Durrett et al., 2012; Søgaard, 2011; Lopez et al., 2008; McDonald et al., 2011). The key difference of the present approach compared to annotation projection is that we assume en-cz* en-cz en-de* en-de en-es en-zh cz-en* cz-en de-en* de-en es-en* es-en zh-en* zh-en I NIT 61.11 62.45 66.81 70.39 64.20 75.80 66.82 74.93 66.82 74.93 66.82 74.93 66.82 74.93 S ELF 60.68 62.15 63.96 68.34 64.51 73.52 63.95 71.60 63.58 71.31 63.95 71.47 64.51 72.26 J OINT 72.49 70.19 76.78 79.22 75.43 76.75 70.75 79.70 69.46 77.34 69.92 79.55 67.19 76.51 ∆S ELF 11.81 8.04 12.82 10.88 10.92 3.22 6.80 8.10 5.88 6.03 5.97 8.08 2.68 4.26 ∆I NIT 11.38 7.74"
S13-1044,E12-1003,1,0.847621,"), the main difference being that it relies on a manually annotated parallel corpus, aligned on the argument level, and evaluates only the inference procedure and only on in-domain data. Other related approaches include Kim et al. (2010), where a cross-lingual transfer of relations is performed (which basically represent parts of the predicate-argument structure considered by SRL methods), and Frermann and Bond (2012), where semantic structure matching is used to rank HPSG parses for parallel sentences. Unsupervised semantic role labeling methods (Lang and Lapata, 2010; Lang and Lapata, 2011; Titov and Klementiev, 2012a; Lorenzo and Cerisara, 2012) present an alternative to the crosslingual information propagation approaches such as ours, and at least one the methods in this area also makes use of parallel data (Titov and Klementiev, 2012b). 325 We have presented an approach to information transfer between SRL systems for different language pairs using parallel data. The task proves challenging due to non-trivial mapping between the role labels used in different SRL annotation schemes and the nature of parallel data – the difference in domains and the limited accuracy of the preprocessing tools. We observe"
S13-1044,P12-1068,1,0.853862,"), the main difference being that it relies on a manually annotated parallel corpus, aligned on the argument level, and evaluates only the inference procedure and only on in-domain data. Other related approaches include Kim et al. (2010), where a cross-lingual transfer of relations is performed (which basically represent parts of the predicate-argument structure considered by SRL methods), and Frermann and Bond (2012), where semantic structure matching is used to rank HPSG parses for parallel sentences. Unsupervised semantic role labeling methods (Lang and Lapata, 2010; Lang and Lapata, 2011; Titov and Klementiev, 2012a; Lorenzo and Cerisara, 2012) present an alternative to the crosslingual information propagation approaches such as ours, and at least one the methods in this area also makes use of parallel data (Titov and Klementiev, 2012b). 325 We have presented an approach to information transfer between SRL systems for different language pairs using parallel data. The task proves challenging due to non-trivial mapping between the role labels used in different SRL annotation schemes and the nature of parallel data – the difference in domains and the limited accuracy of the preprocessing tools. We observe"
S13-1044,P11-2052,0,0.1258,"Missing"
S13-1044,D10-1030,0,0.225759,"required to achieve reasonable performance on such tasks for other languages may be significantly reduced by leveraging existing resources and the similarities between languages. This idea has lead to the development of crosslingual annotation projection approaches, which make use of parallel corpora (Pad´o and Lapata, 2009), as well as attempts to adapt models directly Ve školách se neučí rumunsky . LOC PAT Figure 1: Role correspondence in parallel sentences, an example. The notion of compatibility here is highly nontrivial, even for sentences translated as close to the original as possible. Zhuang and Zong (2010), for example, observe that in the English-Chinese parallel PropBank (Palmer et al., 2005b) corresponding arguments often bear different labels, even though the same inventory of semantic roles is used for both 317 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference c and the Shared Task, pages 317–327, Atlanta, Georgia, June 13-14, 2013. 2013 Association for Computational Linguistics languages and the annotation guidelines are similar. When different annotation schemes are considered, the problem is further complicated by the dif"
S13-1044,N07-1070,0,\N,Missing
S13-1044,C98-1013,0,\N,Missing
S13-1044,W09-1201,0,\N,Missing
W06-1666,P05-1022,0,0.198683,"Missing"
W06-1666,P02-1034,0,0.0317966,"terms of a history-based generative probability model. These parameters are estimated using a neural network, the weights of which form the second level of parameterization. This approach allows the probability model to have an infinite number of parameters; the neural network only estimates the bounded number of parameters which are relevant to a given partial parse. We define data-defined kernels in terms of the second level of parameterization (the network weights). For the last set of experiments, we used the probabilistic model described in (Collins, 1999) (model 2), and the Tree Kernel (Collins and Duffy, 2002). However, in these experiments we only used the estimates from the discriminative classifier, so the details of the probabilistic model are not relevant. SSN TRK SSN-Estim LLK-Learn LK-Learn FK-Estim TRK-Estim R 80.9 81.1 81.4 81.2 81.5 81.4 81.5 P 81.7 82.4 82.3 82.4 82.2 82.6 82.8 F1 81.3 81.7 81.8 81.8 81.8 82.0 82.1 CM 18.3 18.2 18.3 17.6 17.8 18.3 18.6 Table 1: Percentage labeled constituent recall (R), precision (P), combination of both (F1 ) and percentage complete match (CM) on the testing set. lary if it occurred at least 20 time in the training set, which (with tag-unknown-word pair"
W06-1666,J93-2004,0,0.0279253,"s as a sample from the discriminative distribution. Kernel, like the TOP kernel for reranking, can be motivated by the minimization of the classification error of a linear classifier w T φLLK (x, y), where θˆ LLK φθˆ (x, y) is the feature extractor of the kernel given by: 4 ˆ = log( v(x, y, θ) φLLK (x, y) = θˆ ˆ ˆ ˆ ∂v(x, y, θ) ,..., ∂v(x, y, θ) ), (v(x, y, θ), ∂θ1 ∂θl where Expected Loss Learning log( X 0 ˆ P (y 0 |x, θ)∆(y , y)). 5 Experimental Evaluation To perform empirical evaluations of the proposed methods, we considered the task of parsing the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993). First, we perform experiments with SVM Struct (Tsochantaridis et al., 2004) as the learner. Since SVM Struct already uses the loss function during training to rescale the margin or slack variables, this learner allows us to test the hypothesis that loss functions are useful in parsing not only to define the optimization criteria but also to define the classifier and to define the feature space. However, SVM Struct training for large scale parsing experiments is computationally expensive2 , so here we use only a small portion of the available training data to perform evaluations of the differ"
W06-1666,J05-1003,0,0.127938,"ou are only interested in getting the label exactly correct (i.e. 0-1 loss), and you think the estimates are accurate. But if you are interested in a loss function where the loss is small when you choose a candidate which is similar to the correct candidate, then it is better to choose one of the candidates in the group. With this choice the loss will only be large if the outlier turns out to be correct, while if the outlier is chosen then the loss will be large if any of the group are correct. In other words, the expected loss of Introduction The reranking approach is widely used in parsing (Collins and Koo, 2005; Koo and Collins, 2005; Henderson and Titov, 2005; Shen and Joshi, 2003) as well as in other structured classification problems. For structured classification tasks, where labels are complex and have an internal structure of interdependency, the 0-1 loss considered in classical formulation of classification algorithms is not a natural choice and different loss functions are normally employed. To tackle this problem, several approaches have been proposed to accommodate loss functions in learning algorithms (Tsochantaridis et al., 2004; Taskar et al., 560 Proceedings of the 2006 Conference on E"
W06-1666,W96-0213,0,0.121512,"ed on loss, using the Loss Logit Kernel (equation (13)) and the Loss Kernel (equation (12)), respectively. FKEstim and TRK-Estim are the models which esti5.2 Experiments with SVM Struct Both the neural network probabilistic model and the kernel based classifiers were trained on section 0 (1,921 sentences, 40,930 words). Section 24 (1,346 sentences, 29,125 words) was used as the validation set during the neural network learning and for choosing parameters of the models. Section 23 (2,416 sentences, 54,268 words) was used for the final testing of the models. We used a publicly available tagger (Ratnaparkhi, 1996) to provide the part-of-speech tags for each word in the sentence. For each tag, there is an unknown-word vocabulary item which is used for all those words which are not sufficiently frequent with that tag to be included individually in the vocabulary. For these experiments, we only included a specific tag-word pair in the vocabu3 All our results are computed with the evalb program (Collins, 1999). 564 of the loss with data-defined kernels (12) and (13) was achieved when the parameter A is close to the inverse of the first component of the learned decision vector, which confirms the motivation"
W06-1666,W03-0402,0,0.0809515,"s), and you think the estimates are accurate. But if you are interested in a loss function where the loss is small when you choose a candidate which is similar to the correct candidate, then it is better to choose one of the candidates in the group. With this choice the loss will only be large if the outlier turns out to be correct, while if the outlier is chosen then the loss will be large if any of the group are correct. In other words, the expected loss of Introduction The reranking approach is widely used in parsing (Collins and Koo, 2005; Koo and Collins, 2005; Henderson and Titov, 2005; Shen and Joshi, 2003) as well as in other structured classification problems. For structured classification tasks, where labels are complex and have an internal structure of interdependency, the 0-1 loss considered in classical formulation of classification algorithms is not a natural choice and different loss functions are normally employed. To tackle this problem, several approaches have been proposed to accommodate loss functions in learning algorithms (Tsochantaridis et al., 2004; Taskar et al., 560 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 560–5"
W06-1666,P05-1023,1,0.213563,"actly correct (i.e. 0-1 loss), and you think the estimates are accurate. But if you are interested in a loss function where the loss is small when you choose a candidate which is similar to the correct candidate, then it is better to choose one of the candidates in the group. With this choice the loss will only be large if the outlier turns out to be correct, while if the outlier is chosen then the loss will be large if any of the group are correct. In other words, the expected loss of Introduction The reranking approach is widely used in parsing (Collins and Koo, 2005; Koo and Collins, 2005; Henderson and Titov, 2005; Shen and Joshi, 2003) as well as in other structured classification problems. For structured classification tasks, where labels are complex and have an internal structure of interdependency, the 0-1 loss considered in classical formulation of classification algorithms is not a natural choice and different loss functions are normally employed. To tackle this problem, several approaches have been proposed to accommodate loss functions in learning algorithms (Tsochantaridis et al., 2004; Taskar et al., 560 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (E"
W06-1666,W04-3201,0,0.0641018,"Missing"
W06-1666,N03-1014,1,0.854318,"(x) P (y|x)∆(y, y P 0) y∈G(x) P (y|x) . (4) For the reranking case, often the probabilistic model only estimates the joint probability P (x, y). However, neither this difference nor the denominator in (4) affects the classification. Thus, replacing the true probabilities with their estimates, we can define the classifier where G(x) denotes a candidate list provided by a baseline probabilistic model for the input x. In this paper we propose different approaches to loss approximation. We apply them to the parse reranking problem where the baseline probabilistic model is a neural network parser (Henderson, 2003), and to parse reranking of candidates provided by the (Collins, 1999) model. The resulting reranking method achieves very significant improvement in the considered loss function and improvement in most other standard measures of accuracy. In the following three sections we will discuss three approaches to learning such a classifier. The first two derive a classification criteria for use with a predefined probability model (the first generative, the second discriminative). The third defines a kernel for use with a classification method for minimizing loss. All use previously proposed learning"
W06-1666,P04-1013,1,0.885953,"Missing"
W06-1666,W05-1506,0,0.0549458,"Missing"
W06-1666,H05-1064,0,0.01165,"in getting the label exactly correct (i.e. 0-1 loss), and you think the estimates are accurate. But if you are interested in a loss function where the loss is small when you choose a candidate which is similar to the correct candidate, then it is better to choose one of the candidates in the group. With this choice the loss will only be large if the outlier turns out to be correct, while if the outlier is chosen then the loss will be large if any of the group are correct. In other words, the expected loss of Introduction The reranking approach is widely used in parsing (Collins and Koo, 2005; Koo and Collins, 2005; Henderson and Titov, 2005; Shen and Joshi, 2003) as well as in other structured classification problems. For structured classification tasks, where labels are complex and have an internal structure of interdependency, the 0-1 loss considered in classical formulation of classification algorithms is not a natural choice and different loss functions are normally employed. To tackle this problem, several approaches have been proposed to accommodate loss functions in learning algorithms (Tsochantaridis et al., 2004; Taskar et al., 560 Proceedings of the 2006 Conference on Empirical Methods in Nat"
W06-1666,C00-2137,0,0.0316772,"Missing"
W06-1666,N04-1022,0,0.12775,"Missing"
W06-1666,J03-4003,0,\N,Missing
W06-1666,P04-1043,0,\N,Missing
W06-1666,P96-1024,0,\N,Missing
W06-2902,P02-1034,0,0.33889,"used to train this parser to optimize performance on a small in-domain corpus. Large margin methods have demonstrated substantial success in applications to many machine learning problems, because they optimize a measure which is directly related to the expected testing performance. They achieve especially good performance compared to other classifiers when only a small amount of training data is available. Most of the large margin methods need the definition of a kernel. Work on kernels for natural language parsing has been mostly focused on the definition of kernels over parse trees (e.g. (Collins and Duffy, 2002)), which are chosen on the basis of domain knowledge. In (Henderson and Titov, 2005) it was proposed to apply a class of kernels derived from probabilistic models to the natural language parsing problem. In (Henderson and Titov, 2005), the kernel is constructed using the parameters of a trained probabilistic model. This type of kernel is called a datadefined kernel, because the kernel incorporates information from the data used to train the probabilistic model. We propose to exploit this property to transfer information from a large corpus to a statis6 Proceedings of the 10th Conference on Com"
W06-2902,P97-1003,0,0.208877,"Missing"
W06-2902,W01-0521,0,0.0294224,"k probabilistic model, this method achieves improved performance over the probabilistic model alone. 1 Introduction In recent years, significant progress has been made in the area of natural language parsing. This research has focused mostly on the development of statistical parsers trained on large annotated corpora, in particular the Penn Treebank WSJ corpus (Marcus et al., 1993). The best statistical parsers have shown good results on this benchmark, but these statistical parsers demonstrate far worse results when they are applied to data from a different domain (Roark and Bacchiani, 2003; Gildea, 2001; Ratnaparkhi, 1999). This is an important problem because we cannot expect to have large annotated corpora available for most domains. While identifying this problem, previous work has not proposed parsing methods which are specifically designed for porting parsers. Instead they propose methods for training a standard parser with a large amount of out-of-domain data and a small amount of in-domain data. In this paper, we propose using data-defined kernels and large margin methods to specifically address porting a parser to a new domain. Data-defined kernels are used to construct a new parser"
W06-2902,P05-1023,1,0.691912,"rge margin methods have demonstrated substantial success in applications to many machine learning problems, because they optimize a measure which is directly related to the expected testing performance. They achieve especially good performance compared to other classifiers when only a small amount of training data is available. Most of the large margin methods need the definition of a kernel. Work on kernels for natural language parsing has been mostly focused on the definition of kernels over parse trees (e.g. (Collins and Duffy, 2002)), which are chosen on the basis of domain knowledge. In (Henderson and Titov, 2005) it was proposed to apply a class of kernels derived from probabilistic models to the natural language parsing problem. In (Henderson and Titov, 2005), the kernel is constructed using the parameters of a trained probabilistic model. This type of kernel is called a datadefined kernel, because the kernel incorporates information from the data used to train the probabilistic model. We propose to exploit this property to transfer information from a large corpus to a statis6 Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X), c pages 6–13, New York City, June 20"
W06-2902,N03-1014,1,0.777253,"train a probabilistic model on both the large corpus and the target corpus. The kernel is derived from this trained model. In both scenarios, the kernel is used in a SVM classifier (Tsochantaridis et al., 2004) trained on a small amount of data from the target domain. This classifier is trained to rerank the candidate parses selected by the associated probabilistic model. We use the Penn Treebank Wall Street Journal corpus as the large corpus and individual sections of the Brown corpus as the target corpora (Marcus et al., 1993). The probabilistic model is a neural network statistical parser (Henderson, 2003), and the data-defined kernel is a TOP reranking kernel (Henderson and Titov, 2005). With both scenarios, the resulting parser demonstrates improved accuracy on the target domain over the probabilistic model alone. In additional experiments, we evaluate the hypothesis that the primary issue for porting parsers between domains is differences in the distributions of words in structures, and not in the distributions of the structures themselves. We partition the parameters of the probability model into those which define the distributions of words and those that only involve structural decisions,"
W06-2902,H05-1064,0,0.0119417,"ables (e.g. for PCFG models), the features of the data-defined kernel (except for the first feature) are a function of the counts used to estimate the model. For a PCFG, each such feature is a function of one rule’s counts, where the counts from different candidates are weighted using the probability estimates from the model. With latent variables, the meaning of the variable (not just its value) is learned from the data, and the associated features of the data-defined kernel capture this induced meaning. There has been much recent work on latent variable models (e.g. (Matsuzaki et al., 2005; Koo and Collins, 2005)). We choose to use an earlier neural network based probabilistic model of pars9 ing (Henderson, 2003), whose hidden units can be viewed as approximations to latent variables. This parsing model is also a good candidate for our experiments because it achieves state-of-the-art results on the standard Wall Street Journal (WSJ) parsing problem (Henderson, 2003), and data-defined kernels derived from this parsing model have recently been used with the Voted Perceptron algorithm on the WSJ parsing task, achieving a significant improvement in accuracy over the neural network parser alone (Henderson"
W06-2902,J93-2004,0,0.0315923,"babilistic model trained on the source domain (and possibly also the target domain) is used to define a kernel, which is then used in a large margin classifier trained only on the target domain. With a SVM classifier and a neural network probabilistic model, this method achieves improved performance over the probabilistic model alone. 1 Introduction In recent years, significant progress has been made in the area of natural language parsing. This research has focused mostly on the development of statistical parsers trained on large annotated corpora, in particular the Penn Treebank WSJ corpus (Marcus et al., 1993). The best statistical parsers have shown good results on this benchmark, but these statistical parsers demonstrate far worse results when they are applied to data from a different domain (Roark and Bacchiani, 2003; Gildea, 2001; Ratnaparkhi, 1999). This is an important problem because we cannot expect to have large annotated corpora available for most domains. While identifying this problem, previous work has not proposed parsing methods which are specifically designed for porting parsers. Instead they propose methods for training a standard parser with a large amount of out-of-domain data an"
W06-2902,P05-1010,0,0.054943,"Missing"
W06-2902,N03-1027,0,0.667449,"ssifier and a neural network probabilistic model, this method achieves improved performance over the probabilistic model alone. 1 Introduction In recent years, significant progress has been made in the area of natural language parsing. This research has focused mostly on the development of statistical parsers trained on large annotated corpora, in particular the Penn Treebank WSJ corpus (Marcus et al., 1993). The best statistical parsers have shown good results on this benchmark, but these statistical parsers demonstrate far worse results when they are applied to data from a different domain (Roark and Bacchiani, 2003; Gildea, 2001; Ratnaparkhi, 1999). This is an important problem because we cannot expect to have large annotated corpora available for most domains. While identifying this problem, previous work has not proposed parsing methods which are specifically designed for porting parsers. Instead they propose methods for training a standard parser with a large amount of out-of-domain data and a small amount of in-domain data. In this paper, we propose using data-defined kernels and large margin methods to specifically address porting a parser to a new domain. Data-defined kernels are used to construct"
W06-2902,W03-0402,0,0.0281828,"training of standard large margin methods, like SVMs, isn’t feasible on a large corpus, it is quite tractable to train them on a small target corpus.1 Also, the choice of the large margin classifier is motivated by their good generalization properties on small datasets, on which accurate probabilistic models are usually difficult to learn. We hypothesize that differences in vocabulary across domains is one of the main difficulties with parser portability. To address this problem, we propose constructing the kernel from a probabilistic model which has been reparameterized to better suit 1 In (Shen and Joshi, 2003) it was proposed to use an ensemble of SVMs trained the Wall Street Journal corpus, but we believe that the generalization performance of the resulting classifier is compromised in this approach. 8 the target domain vocabulary. As in other lexicalized statistical parsers, the probabilistic model we use treats words which are not frequent enough in the training set as ‘unknown’ words (Henderson, 2003). Thus there are no parameters in this model which are specifically for these words. When we consider a different target domain, a substantial proportion of the words in the target domain are treat"
W06-2902,C00-2137,0,0.0533585,"Missing"
W06-2902,J03-4003,0,\N,Missing
W07-2218,W06-2920,0,0.0386929,"itioning features. To define this model we use a recently proposed class of Bayesian Networks for structured prediction, Incremental Sigmoid Belief Networks. We demonstrate that the proposed model achieves state-of-the-art results on three different languages. We also demonstrate that the features induced by the ISBN’s latent variables are crucial to this success, and show that the proposed model is particularly good on long dependencies. 1 Introduction Dependency parsing has been a topic of active research in natural language processing during the last several years. The CoNLL-X shared task (Buchholz and Marsi, 2006) made a wide selection of standardized treebanks for different languages available for the research community and allowed for easy comparison between various statistical methods on a standardized benchmark. One of the surprising things discovered by this evaluation is that the best results are achieved by methods which are quite different from state-of-the-art models for constituent parsing, e.g. the deterministic parsing method of (Nivre et al., 2006) and the minimum spanning tree parser of (McDonald et al., 2006). All the most accurate dependency parsing models are fully discriminative, unli"
W07-2218,P05-1022,0,0.0279498,"Missing"
W07-2218,A00-2018,0,0.113921,"and Henderson, 2007), computing the conditional probabilities which we need for parsing is in general intractable with ISBNs, but they can be approximated efficiently in several ways. In particular, the neural network constituent parsers in (Henderson, 2003) and (Henderson, 2004) can be regarded as coarse approximations to their corresponding ISBN model. ISBNs use history-based probability models. The most common approach to handling the unbounded nature of the parse histories in these models is to choose a pre-defined set of features which can be unambiguously derived from the history (e.g. (Charniak, 2000; Collins, 1999; Nivre et al., 2004)). Decision probabilities are then assumed to be independent of all information not represented by this finite set of features. ISBNs instead use a vector of binary 144 Proceedings of the 10th Conference on Parsing Technologies, pages 144–155, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics latent variables to encode the information about the parser history. This history vector is similar to the hidden state of a Hidden Markov Model. But unlike the graphical model for an HMM, which specifies conditional dependency edges on"
W07-2218,dzeroski-etal-2006-towards,0,0.0119801,"feature set and disable the feature induction abilities of the model by removing all the edges between latent variables vectors. Comparison of this restricted model with the full ISBN model shows how important the feature induction is. Also, comparison of this restricted model with the MALT parser, which uses the same set of features, indicates whether our generative estimation method and use of beam search is beneficial. 6.1 Experimental Setup We used the CoNLL-X distributions of Danish DDT treebank (Kromann, 2003), Dutch Alpino treebank (van der Beek et al., 2002) and Slovene SDT treebank (Dzeroski et al., 2006). The choice of these treebanks was motivated by the fact that they all are freely distributed and have very different sizes of their training sets: 195,069 tokens for Dutch, 94,386 tokens for Danish and only 28,750 tokens for Slovene. As it is generally believed that discriminative models win over generative models with a large amount of training data, so we expected to see similar trend in our results. Test sets are about equal and contain about 5,000 scoring tokens. We followed the experimental setup of the shared task and used all the information provided for the languages: gold standard p"
W07-2218,P05-1023,1,0.935194,"dependencies. Additional experiments suggest that both feature induction abilities and use of the beam search contribute to this improvement. The fact that our model defines a probability model over parse trees, unlike the previous state-ofthe-art methods (Nivre et al., 2006; McDonald et al., 2006), makes it easier to use this model in applications which require probability estimates, e.g. in language processing pipelines. Also, as with any generative model, it may be easy to improve the parser’s accuracy by using discriminative retraining techniques (Henderson, 2004) or data-defined kernels (Henderson and Titov, 2005), with or even without introduction of any additional linguistic features. In addition, there are some applications, such as language modeling, which require generative models. Another advantage of generative models is that they do not suffer from the label bias problems (Bottou, 1991), which is a potential problem for conditional or deterministic history-based models, such as (Nivre et al., 2004). In the remainder of this paper, we will first review general ISBNs and how they can be approximated. Then we will define the generative parsing model, based on the algorithm of (Nivre et al., 2004),"
W07-2218,N03-1014,1,0.944228,"se a generative latent variable model for dependency parsing. It is based on Incremental Sigmoid Belief Networks (ISBNs), a class of directed graphical model for structure prediction problems recently proposed in (Titov and Henderson, 2007), where they were demonstrated to achieve competitive results on the constituent parsing task. As discussed in (Titov and Henderson, 2007), computing the conditional probabilities which we need for parsing is in general intractable with ISBNs, but they can be approximated efficiently in several ways. In particular, the neural network constituent parsers in (Henderson, 2003) and (Henderson, 2004) can be regarded as coarse approximations to their corresponding ISBN model. ISBNs use history-based probability models. The most common approach to handling the unbounded nature of the parse histories in these models is to choose a pre-defined set of features which can be unambiguously derived from the history (e.g. (Charniak, 2000; Collins, 1999; Nivre et al., 2004)). Decision probabilities are then assumed to be independent of all information not represented by this finite set of features. ISBNs instead use a vector of binary 144 Proceedings of the 10th Conference on P"
W07-2218,P04-1013,1,0.934251,"variable model for dependency parsing. It is based on Incremental Sigmoid Belief Networks (ISBNs), a class of directed graphical model for structure prediction problems recently proposed in (Titov and Henderson, 2007), where they were demonstrated to achieve competitive results on the constituent parsing task. As discussed in (Titov and Henderson, 2007), computing the conditional probabilities which we need for parsing is in general intractable with ISBNs, but they can be approximated efficiently in several ways. In particular, the neural network constituent parsers in (Henderson, 2003) and (Henderson, 2004) can be regarded as coarse approximations to their corresponding ISBN model. ISBNs use history-based probability models. The most common approach to handling the unbounded nature of the parse histories in these models is to choose a pre-defined set of features which can be unambiguously derived from the history (e.g. (Charniak, 2000; Collins, 1999; Nivre et al., 2004)). Decision probabilities are then assumed to be independent of all information not represented by this finite set of features. ISBNs instead use a vector of binary 144 Proceedings of the 10th Conference on Parsing Technologies, p"
W07-2218,H05-1064,0,0.010175,"it can not completely eliminate it. 153 7 Related Work There has not been much previous work on latent variable models for dependency parsing. Dependency parsing with Dynamic Bayesian Networks was considered in (Peshkin and Savova, 2005), with limited success. Roughly, the model considered the whole sentence at a time, with the DBN being used to decide which words correspond to leaves of the tree. The chosen words are then removed from the sentence and the model is recursively applied to the reduced sentence. Recently several latent variable models for constituent parsing have been proposed (Koo and Collins, 2005; Matsuzaki et al., 2005; Prescher, 2005; Riezler et al., 2002). In (Matsuzaki et al., 2005) non-terminals in a standard PCFG model are augmented with latent variables. A similar model of (Prescher, 2005) uses a head-driven PCFG with latent heads, thus restricting the flexibility of the latent-variable model by using explicit linguistic constraints. While the model of (Matsuzaki et al., 2005) significantly outperforms the constrained model of (Prescher, 2005), they both are well below the state-of-the-art in constituent parsing. In (Koo and Collins, 2005), an undirected graphical model for con"
W07-2218,P05-1010,0,0.0183894,"eliminate it. 153 7 Related Work There has not been much previous work on latent variable models for dependency parsing. Dependency parsing with Dynamic Bayesian Networks was considered in (Peshkin and Savova, 2005), with limited success. Roughly, the model considered the whole sentence at a time, with the DBN being used to decide which words correspond to leaves of the tree. The chosen words are then removed from the sentence and the model is recursively applied to the reduced sentence. Recently several latent variable models for constituent parsing have been proposed (Koo and Collins, 2005; Matsuzaki et al., 2005; Prescher, 2005; Riezler et al., 2002). In (Matsuzaki et al., 2005) non-terminals in a standard PCFG model are augmented with latent variables. A similar model of (Prescher, 2005) uses a head-driven PCFG with latent heads, thus restricting the flexibility of the latent-variable model by using explicit linguistic constraints. While the model of (Matsuzaki et al., 2005) significantly outperforms the constrained model of (Prescher, 2005), they both are well below the state-of-the-art in constituent parsing. In (Koo and Collins, 2005), an undirected graphical model for constituent parse reranking"
W07-2218,W06-2932,0,0.581035,"language processing during the last several years. The CoNLL-X shared task (Buchholz and Marsi, 2006) made a wide selection of standardized treebanks for different languages available for the research community and allowed for easy comparison between various statistical methods on a standardized benchmark. One of the surprising things discovered by this evaluation is that the best results are achieved by methods which are quite different from state-of-the-art models for constituent parsing, e.g. the deterministic parsing method of (Nivre et al., 2006) and the minimum spanning tree parser of (McDonald et al., 2006). All the most accurate dependency parsing models are fully discriminative, unlike constituent parsing where all the state of the art methods have a generaIn this paper we propose a generative latent variable model for dependency parsing. It is based on Incremental Sigmoid Belief Networks (ISBNs), a class of directed graphical model for structure prediction problems recently proposed in (Titov and Henderson, 2007), where they were demonstrated to achieve competitive results on the constituent parsing task. As discussed in (Titov and Henderson, 2007), computing the conditional probabilities whi"
W07-2218,W04-2407,0,0.496553,"the conditional probabilities which we need for parsing is in general intractable with ISBNs, but they can be approximated efficiently in several ways. In particular, the neural network constituent parsers in (Henderson, 2003) and (Henderson, 2004) can be regarded as coarse approximations to their corresponding ISBN model. ISBNs use history-based probability models. The most common approach to handling the unbounded nature of the parse histories in these models is to choose a pre-defined set of features which can be unambiguously derived from the history (e.g. (Charniak, 2000; Collins, 1999; Nivre et al., 2004)). Decision probabilities are then assumed to be independent of all information not represented by this finite set of features. ISBNs instead use a vector of binary 144 Proceedings of the 10th Conference on Parsing Technologies, pages 144–155, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics latent variables to encode the information about the parser history. This history vector is similar to the hidden state of a Hidden Markov Model. But unlike the graphical model for an HMM, which specifies conditional dependency edges only between adjacent states in the se"
W07-2218,W06-2933,0,0.0615868,"ndency parsing has been a topic of active research in natural language processing during the last several years. The CoNLL-X shared task (Buchholz and Marsi, 2006) made a wide selection of standardized treebanks for different languages available for the research community and allowed for easy comparison between various statistical methods on a standardized benchmark. One of the surprising things discovered by this evaluation is that the best results are achieved by methods which are quite different from state-of-the-art models for constituent parsing, e.g. the deterministic parsing method of (Nivre et al., 2006) and the minimum spanning tree parser of (McDonald et al., 2006). All the most accurate dependency parsing models are fully discriminative, unlike constituent parsing where all the state of the art methods have a generaIn this paper we propose a generative latent variable model for dependency parsing. It is based on Incremental Sigmoid Belief Networks (ISBNs), a class of directed graphical model for structure prediction problems recently proposed in (Titov and Henderson, 2007), where they were demonstrated to achieve competitive results on the constituent parsing task. As discussed in (Titov a"
W07-2218,W05-1512,0,0.0253468,"ted Work There has not been much previous work on latent variable models for dependency parsing. Dependency parsing with Dynamic Bayesian Networks was considered in (Peshkin and Savova, 2005), with limited success. Roughly, the model considered the whole sentence at a time, with the DBN being used to decide which words correspond to leaves of the tree. The chosen words are then removed from the sentence and the model is recursively applied to the reduced sentence. Recently several latent variable models for constituent parsing have been proposed (Koo and Collins, 2005; Matsuzaki et al., 2005; Prescher, 2005; Riezler et al., 2002). In (Matsuzaki et al., 2005) non-terminals in a standard PCFG model are augmented with latent variables. A similar model of (Prescher, 2005) uses a head-driven PCFG with latent heads, thus restricting the flexibility of the latent-variable model by using explicit linguistic constraints. While the model of (Matsuzaki et al., 2005) significantly outperforms the constrained model of (Prescher, 2005), they both are well below the state-of-the-art in constituent parsing. In (Koo and Collins, 2005), an undirected graphical model for constituent parse reranking uses dependency"
W07-2218,P02-1035,0,0.0109569,"as not been much previous work on latent variable models for dependency parsing. Dependency parsing with Dynamic Bayesian Networks was considered in (Peshkin and Savova, 2005), with limited success. Roughly, the model considered the whole sentence at a time, with the DBN being used to decide which words correspond to leaves of the tree. The chosen words are then removed from the sentence and the model is recursively applied to the reduced sentence. Recently several latent variable models for constituent parsing have been proposed (Koo and Collins, 2005; Matsuzaki et al., 2005; Prescher, 2005; Riezler et al., 2002). In (Matsuzaki et al., 2005) non-terminals in a standard PCFG model are augmented with latent variables. A similar model of (Prescher, 2005) uses a head-driven PCFG with latent heads, thus restricting the flexibility of the latent-variable model by using explicit linguistic constraints. While the model of (Matsuzaki et al., 2005) significantly outperforms the constrained model of (Prescher, 2005), they both are well below the state-of-the-art in constituent parsing. In (Koo and Collins, 2005), an undirected graphical model for constituent parse reranking uses dependency relations to define th"
W07-2218,N03-1028,0,0.0199101,"e flexibility of the latent-variable model by using explicit linguistic constraints. While the model of (Matsuzaki et al., 2005) significantly outperforms the constrained model of (Prescher, 2005), they both are well below the state-of-the-art in constituent parsing. In (Koo and Collins, 2005), an undirected graphical model for constituent parse reranking uses dependency relations to define the edges. Thus, it should be easy to apply a similar method to reranking dependency trees. Undirected graphical models, in particular Conditional Random Fields, are the standard tools for shallow parsing (Sha and Pereira, 2003). However, shallow parsing is effectively a sequence labeling problem and therefore differs significantly from full parsing. As discussed in (Titov and Henderson, 2007), undirected graphical models do not seem to be suitable for history-based parsing models. Sigmoid Belief Networks (SBNs) were used originally for character recognition tasks, but later a dynamic modification of this model was applied to the reinforcement learning task (Sallans, 2002). However, their graphical model, approximation method, and learning method differ significantly from those of this paper. The extension of dynamic"
W07-2218,P07-1080,1,0.088657,"ods which are quite different from state-of-the-art models for constituent parsing, e.g. the deterministic parsing method of (Nivre et al., 2006) and the minimum spanning tree parser of (McDonald et al., 2006). All the most accurate dependency parsing models are fully discriminative, unlike constituent parsing where all the state of the art methods have a generaIn this paper we propose a generative latent variable model for dependency parsing. It is based on Incremental Sigmoid Belief Networks (ISBNs), a class of directed graphical model for structure prediction problems recently proposed in (Titov and Henderson, 2007), where they were demonstrated to achieve competitive results on the constituent parsing task. As discussed in (Titov and Henderson, 2007), computing the conditional probabilities which we need for parsing is in general intractable with ISBNs, but they can be approximated efficiently in several ways. In particular, the neural network constituent parsers in (Henderson, 2003) and (Henderson, 2004) can be regarded as coarse approximations to their corresponding ISBN model. ISBNs use history-based probability models. The most common approach to handling the unbounded nature of the parse histories"
W07-2218,N07-1051,0,\N,Missing
W07-2218,D07-1097,0,\N,Missing
W07-2218,D07-1100,0,\N,Missing
W07-2218,J93-2004,0,\N,Missing
W07-2218,W07-2201,0,\N,Missing
W07-2218,W07-2207,0,\N,Missing
W07-2218,J03-4003,0,\N,Missing
W07-2218,P08-2054,0,\N,Missing
W07-2218,W07-2202,0,\N,Missing
W07-2218,P06-1055,0,\N,Missing
W07-2218,W08-2122,1,\N,Missing
W07-2218,W07-2416,0,\N,Missing
W07-2218,D07-1072,0,\N,Missing
W08-2122,J93-2004,0,0.0363444,"s statistical parsing and tagging, have recently paved the way to statistical learning techniques for levels of semantic representation, such as recovering the logical form of a sentence for information extraction and question-answering applications (e.g. (Wong and Mooney, 2007)) or jointly learning the syntactic structure of the sentence and the propositional argument-structure of its main predicates (Musillo and Merlo, 2006; Merlo and Musillo, 2008). In this vein, the CoNLL 2008 shared task sets the challenge of learning jointly both syntactic dependencies (extracted from the Penn Treebank (Marcus et al., 1993) ) and semantic dependencies (extracted both from PropBank (Palmer et al., 2005) ∗ c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 0 Authors in alphabetical order. and NomBank (Meyers et al., 2004) under a unified representation. We propose a solution that uses a generative history-based model to predict the most likely derivation of a synchronous dependency parser for both syntactic and semantic dependencies. Our probabilistic model is based on Incremental Sigmoid"
W08-2122,W08-2101,1,0.889757,"Missing"
W08-2122,W04-2705,0,0.277552,"Missing"
W08-2122,N06-2026,1,0.878058,"Missing"
W08-2122,W06-2933,0,0.320816,"e Probability Model Our probability model is a joint generative model of syntactic and semantic dependencies. The two dependency structures are specified as the sequence of actions for a synchronous parser, which requires each dependency structure to be projec178 CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 178–182 Manchester, August 2008 tivised separately. 2.1 Synchronous derivations The derivations for syntactic dependency trees are the same as specified in (Titov and Henderson, 2007b), which are based on the shift-reduce style parser of (Nivre et al., 2006). The derivations use a stack and an input queue. There are actions for creating a leftward or rightward arc between the top of the stack and the front of the queue, for popping a word from the stack, and for shifting a word from the queue to the stack. The derivations for semantic dependency graphs use virtually the same set of actions, but impose fewer constraints on when they can be applied, due to the fact that a word in a semantic dependency graph can have more than one parent. An additional action predicates was introduced to label a predicate with sense s. Let Td be a syntactic dependen"
W08-2122,J05-1004,0,0.0727012,"arning techniques for levels of semantic representation, such as recovering the logical form of a sentence for information extraction and question-answering applications (e.g. (Wong and Mooney, 2007)) or jointly learning the syntactic structure of the sentence and the propositional argument-structure of its main predicates (Musillo and Merlo, 2006; Merlo and Musillo, 2008). In this vein, the CoNLL 2008 shared task sets the challenge of learning jointly both syntactic dependencies (extracted from the Penn Treebank (Marcus et al., 1993) ) and semantic dependencies (extracted both from PropBank (Palmer et al., 2005) ∗ c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 0 Authors in alphabetical order. and NomBank (Meyers et al., 2004) under a unified representation. We propose a solution that uses a generative history-based model to predict the most likely derivation of a synchronous dependency parser for both syntactic and semantic dependencies. Our probabilistic model is based on Incremental Sigmoid Belief Networks (ISBNs), a recently proposed latent variable model for syntactic"
W08-2122,W08-2121,0,0.159555,"Missing"
W08-2122,P07-1080,1,0.599944,"ke 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 0 Authors in alphabetical order. and NomBank (Meyers et al., 2004) under a unified representation. We propose a solution that uses a generative history-based model to predict the most likely derivation of a synchronous dependency parser for both syntactic and semantic dependencies. Our probabilistic model is based on Incremental Sigmoid Belief Networks (ISBNs), a recently proposed latent variable model for syntactic structure prediction, which has shown very good behaviour for both constituency (Titov and Henderson, 2007a) and dependency parsing (Titov and Henderson, 2007b). The ability of ISBNs to induce their features automatically enables us to extend this architecture to learning a synchronous parse of syntax and semantics without modification of the main architecture. By solving the problem with synchronous parsing, a probabilistic model is learnt which maximises the joint probability of the syntactic and semantic dependencies and thereby guarantees that the output structure is globally coherent, while at the same time building the two structures separately. This extension of the ISBN architecture is the"
W08-2122,W07-2218,1,0.911479,"ke 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 0 Authors in alphabetical order. and NomBank (Meyers et al., 2004) under a unified representation. We propose a solution that uses a generative history-based model to predict the most likely derivation of a synchronous dependency parser for both syntactic and semantic dependencies. Our probabilistic model is based on Incremental Sigmoid Belief Networks (ISBNs), a recently proposed latent variable model for syntactic structure prediction, which has shown very good behaviour for both constituency (Titov and Henderson, 2007a) and dependency parsing (Titov and Henderson, 2007b). The ability of ISBNs to induce their features automatically enables us to extend this architecture to learning a synchronous parse of syntax and semantics without modification of the main architecture. By solving the problem with synchronous parsing, a probabilistic model is learnt which maximises the joint probability of the syntactic and semantic dependencies and thereby guarantees that the output structure is globally coherent, while at the same time building the two structures separately. This extension of the ISBN architecture is the"
W08-2122,P07-1121,0,0.0403536,"Missing"
W08-2122,W06-2303,1,\N,Missing
W09-1205,burchardt-etal-2006-salsa,0,0.0412684,"Missing"
W09-1205,W08-2122,1,0.777374,"g. Titov et al. (2009) found that only using the Swap action as a last resort is the best strategy for English (compared to using it preemptively to address future crossing arcs) and we use the same strategy here for all languages. Syntactic graphs do not use a Swap action. We adopt the HEAD method of Nivre and Nilsson (2005) to de-projectivise syntactic dependencies outside of parsing.1 3 Features and New Developments The synchronous derivations described above are modelled with a type of Bayesian Network called an Incremental Sigmoid Belief Network (ISBN) (Titov and Henderson, 2007a). As in Henderson et al. (2008), the ISBN model distinguishes two types of latent states: syntactic states, when syntactic decisions are considered, and semantic states, when semantic decision are considered. Latent states are vectors of binary latent variables, which are conditioned on variables from previous states via a pattern of connecting edges determined by the previous decisions. These latent-to-latent connections are used to engineer soft biases which reflect the relevant domains of locality in the structure being built. For these we used the set of connections proposed in Titov et al. (2009), which includes latent"
W09-1205,kawahara-etal-2002-construction,0,0.0146671,"ment score. All development effort took about two personmonths, mostly by someone who had no previous experience with the system. Most of this time was spent on the above differences in the task definition between the 2008 and 2009 shared tasks. 4 Results and Discussion We participated in the joint task of the closed challenge, as described in Hajiˇc et al. (2009). The datasets used in this challenge are described in Taul´e et al. (2008) (Catalan and Spanish), Palmer and Xue (2009) (Chinese), Hajiˇc et al. (2006) (Czech), Surdeanu et al. (2008) (English), Burchardt et al. (2006) (German), and Kawahara et al. (2002) (Japanese). Rank Average Catalan Chinese Czech 3 82.14 82.66 76.15 83.21 1 @85.77 @87.86 76.11 @80.38 3 78.42 77.44 76.05 86.02 macro F1 syntactic acc semantic F1 English German Japanese Spanish 86.03 79.59 84.91 82.43 88.79 87.29 92.34 @87.64 83.24 71.78 77.23 77.19 Table 2: The three main scores for our system. Rank is within task. Rank macro F1 syn Acc sem F1 3 2 3 Ave 75.93 78.01 73.63 Cze-ood Eng-ood Ger-ood @80.70 75.76 71.32 @76.41 80.84 76.77 84.99 70.65 65.25 Table 3: Results on out-of-domain for our system. Rank is within task. The official results on the testing set are shown in ta"
W09-1205,P08-1069,0,0.0241132,"Missing"
W09-1205,P05-1013,0,0.0314403,"Missing"
W09-1205,W06-2933,0,0.024019,"Missing"
W09-1205,W08-2121,0,0.154607,"Missing"
W09-1205,taule-etal-2008-ancora,0,0.083404,"Missing"
W09-1205,P07-1080,1,0.811623,"een previously imple1 The statistics in Table 1 suggest that, for some languages, swapping might be beneficial for syntax as well. 39 mented. For the former modifications, the system was adapted to allow the use of the PFEAT and FILLPRED fields in the data, which both resulted in improved accuracy for all the languages. The PFEAT data field (automatically predicted morphological features) was introduced in the system in two ways, as an atomic feature bundle that is predicted when predicting the word, and split into its elementary components when conditioning on a previous word, as was done in Titov and Henderson (2007b). Because the testing data included a specification of which words were annotated as predicates (the FILLPRED data field), we constrained the parser’s output so as to be consistent with this specification. For rare predicates, if the predicate was not in the parser’s lexicon (extracted from the training set), then a sense was taken from the list of senses reported in the Lexicon and Frame Set resources available for the closed challenge. If this information was not available, then a default sense was constructed based on the automatically predicted lemma (PLEMMA) of the predicate. We also ma"
W09-1205,D07-1099,1,0.816134,"een previously imple1 The statistics in Table 1 suggest that, for some languages, swapping might be beneficial for syntax as well. 39 mented. For the former modifications, the system was adapted to allow the use of the PFEAT and FILLPRED fields in the data, which both resulted in improved accuracy for all the languages. The PFEAT data field (automatically predicted morphological features) was introduced in the system in two ways, as an atomic feature bundle that is predicted when predicting the word, and split into its elementary components when conditioning on a previous word, as was done in Titov and Henderson (2007b). Because the testing data included a specification of which words were annotated as predicates (the FILLPRED data field), we constrained the parser’s output so as to be consistent with this specification. For rare predicates, if the predicate was not in the parser’s lexicon (extracted from the training set), then a sense was taken from the list of senses reported in the Lexicon and Frame Set resources available for the closed challenge. If this information was not available, then a default sense was constructed based on the automatically predicted lemma (PLEMMA) of the predicate. We also ma"
W09-1205,D07-1096,0,\N,Missing
W12-1901,bauer-etal-2012-dependency,0,0.0695217,"Missing"
W12-1901,burchardt-etal-2006-salsa,0,0.0326741,"wn to be especially hard for the FrameNetstyle representations for a number of reasons, including, the lack of cross-frame correspondence for most roles, fine-grain definitions of roles and frames in FrameNet, and relatively small amounts of statistically representative data (Erk and Pado, 2006; Das et al., 2010; Palmer and Sporleder, 2010; Das and Smith, 2011). Another reason for reduced interest in predicting FrameNet representations is the lack of annotated resources for most languages, with annotated corpora available or being developed only for English (Ruppenhofer et al., 2006), German (Burchardt et al., 2006), Spanish (Subirats, 2009) and Japanese (Ohara et al., 2004). Due to scarcity of labeled data, purely unsupervised set-ups recently started to receive considerable attention (Swier and Stevenson, 2004; Grenager and Manning, 2006; Lang and Lapata, 2010; Lang and 1 More accurately, FrameNet distinguishes core and noncore roles with non-core roles mostly corresponding to modifiers, e.g., M AN N ER in sentence (b). Non-core roles are expected to generalize across frames. 1 NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 1–7, c Montr´eal, Canada, June 3-8, 2012. 2012 Association"
W12-1901,W05-0620,0,0.181807,"Missing"
W12-1901,P11-1144,0,0.0418046,"dividual verb, or even individual senses of a verb. The only exceptions are modifiers and roles A0 and A1 which correspond to proto-agent (a doer, or initiator of the action) and proto-patient (an affected entity), respectively. However, the SRL task is known to be especially hard for the FrameNetstyle representations for a number of reasons, including, the lack of cross-frame correspondence for most roles, fine-grain definitions of roles and frames in FrameNet, and relatively small amounts of statistically representative data (Erk and Pado, 2006; Das et al., 2010; Palmer and Sporleder, 2010; Das and Smith, 2011). Another reason for reduced interest in predicting FrameNet representations is the lack of annotated resources for most languages, with annotated corpora available or being developed only for English (Ruppenhofer et al., 2006), German (Burchardt et al., 2006), Spanish (Subirats, 2009) and Japanese (Ohara et al., 2004). Due to scarcity of labeled data, purely unsupervised set-ups recently started to receive considerable attention (Swier and Stevenson, 2004; Grenager and Manning, 2006; Lang and Lapata, 2010; Lang and 1 More accurately, FrameNet distinguishes core and noncore roles with non-core"
W12-1901,erk-pado-2006-shalmaneser,0,0.147556,"ntations (Palmer et al., 2005) where roles are defined for each individual verb, or even individual senses of a verb. The only exceptions are modifiers and roles A0 and A1 which correspond to proto-agent (a doer, or initiator of the action) and proto-patient (an affected entity), respectively. However, the SRL task is known to be especially hard for the FrameNetstyle representations for a number of reasons, including, the lack of cross-frame correspondence for most roles, fine-grain definitions of roles and frames in FrameNet, and relatively small amounts of statistically representative data (Erk and Pado, 2006; Das et al., 2010; Palmer and Sporleder, 2010; Das and Smith, 2011). Another reason for reduced interest in predicting FrameNet representations is the lack of annotated resources for most languages, with annotated corpora available or being developed only for English (Ruppenhofer et al., 2006), German (Burchardt et al., 2006), Spanish (Subirats, 2009) and Japanese (Ohara et al., 2004). Due to scarcity of labeled data, purely unsupervised set-ups recently started to receive considerable attention (Swier and Stevenson, 2004; Grenager and Manning, 2006; Lang and Lapata, 2010; Lang and 1 More acc"
W12-1901,D09-1002,0,0.0674574,"Missing"
W12-1901,W06-1601,0,0.168802,"small amounts of statistically representative data (Erk and Pado, 2006; Das et al., 2010; Palmer and Sporleder, 2010; Das and Smith, 2011). Another reason for reduced interest in predicting FrameNet representations is the lack of annotated resources for most languages, with annotated corpora available or being developed only for English (Ruppenhofer et al., 2006), German (Burchardt et al., 2006), Spanish (Subirats, 2009) and Japanese (Ohara et al., 2004). Due to scarcity of labeled data, purely unsupervised set-ups recently started to receive considerable attention (Swier and Stevenson, 2004; Grenager and Manning, 2006; Lang and Lapata, 2010; Lang and 1 More accurately, FrameNet distinguishes core and noncore roles with non-core roles mostly corresponding to modifiers, e.g., M AN N ER in sentence (b). Non-core roles are expected to generalize across frames. 1 NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 1–7, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics CONTAINER COOK Mary cooks Apply_Heat FOOD the broccoli in a small pan Figure 1: An example of a semantic dependency graph. Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2012). However"
W12-1901,D08-1008,0,0.0196437,"ther words, our method jointly induces both frames and frame-specific semantic roles. We experiment only with verbal predicates and evaluate the performance of the model with respect to some natural baselines. Though the scores for frame induction are not high, we argue that this is primarily due to very high granularity of FrameNet frames which is hard to reproduce for unsupervised systems, as the implicit supervision signal is not capable of providing these distinctions. 2 Task Definition In this work, we use dependency representations of frame semantics. Dependency representations for SRL (Johansson and Nugues, 2008) were made popular by CoNLL-2008 and CoNLL-2009 shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009), but for English were limited to PropBank. Recently, English FrameNet was also released in the dependency format (Bauer et al., 2012). Instead of predicting argument spans, in dependency representation the goal is, roughly, to predict the syntactic head of the argument. The semantic dependency representation for sentence (a) is shown in Figure 1, labels on edges denote roles and labels on words denote frames. Note that in practice the structures can be more complex, as, for example, argume"
W12-1901,N10-1137,0,0.626335,"ly representative data (Erk and Pado, 2006; Das et al., 2010; Palmer and Sporleder, 2010; Das and Smith, 2011). Another reason for reduced interest in predicting FrameNet representations is the lack of annotated resources for most languages, with annotated corpora available or being developed only for English (Ruppenhofer et al., 2006), German (Burchardt et al., 2006), Spanish (Subirats, 2009) and Japanese (Ohara et al., 2004). Due to scarcity of labeled data, purely unsupervised set-ups recently started to receive considerable attention (Swier and Stevenson, 2004; Grenager and Manning, 2006; Lang and Lapata, 2010; Lang and 1 More accurately, FrameNet distinguishes core and noncore roles with non-core roles mostly corresponding to modifiers, e.g., M AN N ER in sentence (b). Non-core roles are expected to generalize across frames. 1 NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 1–7, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics CONTAINER COOK Mary cooks Apply_Heat FOOD the broccoli in a small pan Figure 1: An example of a semantic dependency graph. Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2012). However, all these approaches"
W12-1901,P11-1112,0,0.275083,"ntion (Swier and Stevenson, 2004; Grenager and Manning, 2006; Lang and Lapata, 2010; Lang and 1 More accurately, FrameNet distinguishes core and noncore roles with non-core roles mostly corresponding to modifiers, e.g., M AN N ER in sentence (b). Non-core roles are expected to generalize across frames. 1 NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 1–7, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics CONTAINER COOK Mary cooks Apply_Heat FOOD the broccoli in a small pan Figure 1: An example of a semantic dependency graph. Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2012). However, all these approaches have focused on PropBank-style representations. This may seem somewhat unnatural as FrameNet representations, though arguably more powerful, are harder to learn in the supervised setting, harder to annotate, and annotated data is available for a considerably fewer languages. This is the gap which we address in this preliminary study. More specifically, we extend an existing stateof-the-art Bayesian model for unsupervised semantic role labeling and apply it to support FrameNetstyle semantics. In other words, our method jointly induce"
W12-1901,D11-1122,0,0.227355,"ntion (Swier and Stevenson, 2004; Grenager and Manning, 2006; Lang and Lapata, 2010; Lang and 1 More accurately, FrameNet distinguishes core and noncore roles with non-core roles mostly corresponding to modifiers, e.g., M AN N ER in sentence (b). Non-core roles are expected to generalize across frames. 1 NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 1–7, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics CONTAINER COOK Mary cooks Apply_Heat FOOD the broccoli in a small pan Figure 1: An example of a semantic dependency graph. Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2012). However, all these approaches have focused on PropBank-style representations. This may seem somewhat unnatural as FrameNet representations, though arguably more powerful, are harder to learn in the supervised setting, harder to annotate, and annotated data is available for a considerably fewer languages. This is the gap which we address in this preliminary study. More specifically, we extend an existing stateof-the-art Bayesian model for unsupervised semantic role labeling and apply it to support FrameNetstyle semantics. In other words, our method jointly induce"
W12-1901,S10-1011,0,0.0319159,"verbal predicates only, the first stage would be trivial and the second stage could be handled with heuristics as in much of previous work on unsupervised SRL (Lang and Lapata, 2011a; Titov and Klementiev, 2012). Additionally to considering only verbal predicates, we also assume that every verb belongs to a single frame. This assumption, though restrictive, may be reasonable in practice as (a) the distributions across frames (i.e. senses) are generally highly skewed, (b) current state-of-the-art techniques for word-sense induction hardly beat mostfrequent-sense baselines in accuracy metrics (Manandhar et al., 2010). This assumption, or its minor relaxations, is relatively standard in work on unsupervised semantic parsing tasks (Poon and Domingos, 2009; Poon and Domingos, 2010; Titov and Klementiev, 2011). From the modeling prospective, there are no major obstacles to relaxing this assumption, but it would lead to a major explosion of the search space and, as a result, slow inference. 3 Model and Inference We follow previous work on unsupervised semantic role labeling (Lang and Lapata, 2011a; Titov and Klementiev, 2012) and associate arguments with their frame specific syntactic signatures which we refer"
W12-1901,C10-2107,0,0.06723,"oles are defined for each individual verb, or even individual senses of a verb. The only exceptions are modifiers and roles A0 and A1 which correspond to proto-agent (a doer, or initiator of the action) and proto-patient (an affected entity), respectively. However, the SRL task is known to be especially hard for the FrameNetstyle representations for a number of reasons, including, the lack of cross-frame correspondence for most roles, fine-grain definitions of roles and frames in FrameNet, and relatively small amounts of statistically representative data (Erk and Pado, 2006; Das et al., 2010; Palmer and Sporleder, 2010; Das and Smith, 2011). Another reason for reduced interest in predicting FrameNet representations is the lack of annotated resources for most languages, with annotated corpora available or being developed only for English (Ruppenhofer et al., 2006), German (Burchardt et al., 2006), Spanish (Subirats, 2009) and Japanese (Ohara et al., 2004). Due to scarcity of labeled data, purely unsupervised set-ups recently started to receive considerable attention (Swier and Stevenson, 2004; Grenager and Manning, 2006; Lang and Lapata, 2010; Lang and 1 More accurately, FrameNet distinguishes core and nonco"
W12-1901,J05-1004,0,0.306755,"heat]. the same semantic frame Apply Heat is evoked by verbs cook and sautee, and roles COOK and F OOD in the sentence (a) are filled by Mary and the broccoli, respectively. Note that roles are specific to the frame, not to the individual lexical units (verbs cook and sautee, in the example).1 Most approaches to predicting these representations, called semantic role labeling (SRL), have relied on large annotated datasets (Gildea and Jurafsky, 2002; Carreras and M`arquez, 2005; Surdeanu et al., 2008; Hajiˇc et al., 2009). By far, most of this work has focused on PropBank-style representations (Palmer et al., 2005) where roles are defined for each individual verb, or even individual senses of a verb. The only exceptions are modifiers and roles A0 and A1 which correspond to proto-agent (a doer, or initiator of the action) and proto-patient (an affected entity), respectively. However, the SRL task is known to be especially hard for the FrameNetstyle representations for a number of reasons, including, the lack of cross-frame correspondence for most roles, fine-grain definitions of roles and frames in FrameNet, and relatively small amounts of statistically representative data (Erk and Pado, 2006; Das et al."
W12-1901,D09-1001,0,0.328307,"on unsupervised SRL (Lang and Lapata, 2011a; Titov and Klementiev, 2012). Additionally to considering only verbal predicates, we also assume that every verb belongs to a single frame. This assumption, though restrictive, may be reasonable in practice as (a) the distributions across frames (i.e. senses) are generally highly skewed, (b) current state-of-the-art techniques for word-sense induction hardly beat mostfrequent-sense baselines in accuracy metrics (Manandhar et al., 2010). This assumption, or its minor relaxations, is relatively standard in work on unsupervised semantic parsing tasks (Poon and Domingos, 2009; Poon and Domingos, 2010; Titov and Klementiev, 2011). From the modeling prospective, there are no major obstacles to relaxing this assumption, but it would lead to a major explosion of the search space and, as a result, slow inference. 3 Model and Inference We follow previous work on unsupervised semantic role labeling (Lang and Lapata, 2011a; Titov and Klementiev, 2012) and associate arguments with their frame specific syntactic signatures which we refer to as argument keys: • Active or passive verb voice (ACT/PASS). • Argument position relative to predicate (LEFT/RIGHT). • Syntactic relati"
W12-1901,P10-1031,0,0.02664,"g and Lapata, 2011a; Titov and Klementiev, 2012). Additionally to considering only verbal predicates, we also assume that every verb belongs to a single frame. This assumption, though restrictive, may be reasonable in practice as (a) the distributions across frames (i.e. senses) are generally highly skewed, (b) current state-of-the-art techniques for word-sense induction hardly beat mostfrequent-sense baselines in accuracy metrics (Manandhar et al., 2010). This assumption, or its minor relaxations, is relatively standard in work on unsupervised semantic parsing tasks (Poon and Domingos, 2009; Poon and Domingos, 2010; Titov and Klementiev, 2011). From the modeling prospective, there are no major obstacles to relaxing this assumption, but it would lead to a major explosion of the search space and, as a result, slow inference. 3 Model and Inference We follow previous work on unsupervised semantic role labeling (Lang and Lapata, 2011a; Titov and Klementiev, 2012) and associate arguments with their frame specific syntactic signatures which we refer to as argument keys: • Active or passive verb voice (ACT/PASS). • Argument position relative to predicate (LEFT/RIGHT). • Syntactic relation to its governor. • Pre"
W12-1901,W08-2121,0,0.12004,"Missing"
W12-1901,P11-1145,1,0.926856,"v and Klementiev, 2012). Additionally to considering only verbal predicates, we also assume that every verb belongs to a single frame. This assumption, though restrictive, may be reasonable in practice as (a) the distributions across frames (i.e. senses) are generally highly skewed, (b) current state-of-the-art techniques for word-sense induction hardly beat mostfrequent-sense baselines in accuracy metrics (Manandhar et al., 2010). This assumption, or its minor relaxations, is relatively standard in work on unsupervised semantic parsing tasks (Poon and Domingos, 2009; Poon and Domingos, 2010; Titov and Klementiev, 2011). From the modeling prospective, there are no major obstacles to relaxing this assumption, but it would lead to a major explosion of the search space and, as a result, slow inference. 3 Model and Inference We follow previous work on unsupervised semantic role labeling (Lang and Lapata, 2011a; Titov and Klementiev, 2012) and associate arguments with their frame specific syntactic signatures which we refer to as argument keys: • Active or passive verb voice (ACT/PASS). • Argument position relative to predicate (LEFT/RIGHT). • Syntactic relation to its governor. • Preposition used for argument re"
W12-1901,E12-1003,1,0.623157,"on, 2004; Grenager and Manning, 2006; Lang and Lapata, 2010; Lang and 1 More accurately, FrameNet distinguishes core and noncore roles with non-core roles mostly corresponding to modifiers, e.g., M AN N ER in sentence (b). Non-core roles are expected to generalize across frames. 1 NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 1–7, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics CONTAINER COOK Mary cooks Apply_Heat FOOD the broccoli in a small pan Figure 1: An example of a semantic dependency graph. Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2012). However, all these approaches have focused on PropBank-style representations. This may seem somewhat unnatural as FrameNet representations, though arguably more powerful, are harder to learn in the supervised setting, harder to annotate, and annotated data is available for a considerably fewer languages. This is the gap which we address in this preliminary study. More specifically, we extend an existing stateof-the-art Bayesian model for unsupervised semantic role labeling and apply it to support FrameNetstyle semantics. In other words, our method jointly induces both frames and frame-specif"
W12-1901,W04-3213,0,\N,Missing
W12-1901,N10-1138,0,\N,Missing
W12-1901,W09-1201,0,\N,Missing
W12-1901,J02-3001,0,\N,Missing
W14-1606,W12-1901,1,0.797826,"ript information, induced from large unannotated corpora, should be highly beneficial. Our current model uses a fairly naive semantic composition component, we plan to extend it with more powerful recursive embedding methods which should be especially beneficial when considering very large text collections. Related Work Additionally to the work on script induction discussed above (Chambers and Jurafsky, 2008, 2009; Regneri et al., 2010), other methods for unsupervised learning of event semantics have been proposed. These methods include unsupervised frame induction techniques (O’Connor, 2012; Modi et al., 2012). Frames encode situations (or objects) along with their participants and properties (Fillmore, 1976). Events in these unsupervised approaches are represented with categorical latent variables, and they are induced relying primarily on the selectional preferences’ signal. The very recent work of Cheung et al. (2013) can be regarded as their extension but Cheung et al. also model transitions between events with Markov models. However, neither of these approaches considers (or directly optimizes) the discriminative objective of learning to order events, and neither of them uses distributed repre"
W14-1606,P11-1062,0,0.0398983,"Missing"
W14-1606,P09-1068,0,0.729939,"resentation Learning and exploiting distributed word representations (i.e. vectors of real values, also known as embeddings) have been shown to be beneficial in many NLP applications (Bengio et al., 2001; Turian et al., 2010; Collobert et al., 2011). These representations encode semantic and syntactic properties of a word, and are normally Our model is solely focusing on the ordering task, and admittedly does not represent all the information encoded by a script graph structure. For example, it cannot be directly used to predict a missing event given a set of events (the narrative cloze task (Chambers and Jurafsky, 2009)). Nev50 learned in the language modeling setting (i.e. learned to be predictive of local word context), though they can also be specialized by learning in the context of other NLP applications such as PoS tagging or semantic role labeling (Collobert et al., 2011). More recently, the area of distributional compositional semantics have started to emerge (Baroni and Zamparelli, 2011; Socher et al., 2012), they focus on inducing representations of phrases by learning a compositional model. Such a model would compute a representation of a phrase by starting with embeddings of individual words in t"
W14-1606,P08-1090,0,0.913237,"be used to answer, but doing this without explicitly representing the knowledge as a graph. In our method, the distributed representations (i.e. vectors of real numbers) of event realizations are computed based on distributed representations of predicates and their arguments, and then the event representations are used in a ranker to predict the prototypical ordering of events. Both the parameters of the compositional process for computing the event representation and the rankInduction of common sense knowledge about prototypical sequence of events has recently received much attention (e.g., Chambers and Jurafsky (2008); Regneri et al. (2010)). Instead of inducing this knowledge in the form of graphs, as in much of the previous work, in our method, distributed representations of event realizations are computed based on distributed representations of predicates and their arguments, and then these representations are used to predict prototypical event orderings. The parameters of the compositional process for computing the event representations and the ranking component of the model are jointly estimated. We show that this approach results in a substantial boost in performance on the event ordering task with r"
W14-1606,N13-1104,0,0.035405,"d Work Additionally to the work on script induction discussed above (Chambers and Jurafsky, 2008, 2009; Regneri et al., 2010), other methods for unsupervised learning of event semantics have been proposed. These methods include unsupervised frame induction techniques (O’Connor, 2012; Modi et al., 2012). Frames encode situations (or objects) along with their participants and properties (Fillmore, 1976). Events in these unsupervised approaches are represented with categorical latent variables, and they are induced relying primarily on the selectional preferences’ signal. The very recent work of Cheung et al. (2013) can be regarded as their extension but Cheung et al. also model transitions between events with Markov models. However, neither of these approaches considers (or directly optimizes) the discriminative objective of learning to order events, and neither of them uses distributed representations to encode semantic properties of events. As we pointed out before, our embedding approach is similar (or, in fact, a simplification of) the phrase embedding methods studied in the recent work on distributional compositional semantics (Baroni and Zamparelli, 2011; Socher et al., 2012). However, they have n"
W14-1606,W04-3205,0,0.0110602,"ks, we considered a few unseen predicate pairs where the EEverb model was correctly predicting their order. For many of these pairs there were no inferTable 3: Results on the Gigaword data for the verb-frequency baseline (BL), the verb-only embedding model (EEverb ), the full model (EE) and CJ08 rules. training. This selection strategy was chosen to create a negative bias for our model which is more expressive than the baseline methods and, consequently, better at memorizing examples. As a rule-based temporal classifier, we used high precision “happens-before” rules from the VerbOcean system (Chklovski and Pantel, 2004). Consider “to �verb-x� and then �verb-y�” as one example of such rule. We used predicted collapsed Stanford dependencies (de Marneffe et al., 2006) to extract arguments of the verbs, and used only a subset of dependents of a verb.7 This preprocessing ensured that (1) clues which form part of a pattern are not observable by our model both at train and test time; (2) there is no systematic difference between both events (e.g., for collapsed dependencies, the noun subject is attached to both verbs even if the verbs are conjoined); (3) no information about the order of events in text is available"
W14-1606,P10-1100,0,0.19924,"ugh representing the script knowledge as graphs is attractive from the human interpretability perspective, it may not be optimal from the application point of view. More specifically, these representations (1) require a model designer to choose an appropriate granularity of event mentions (e.g., whether nodes in the graph should be associated with verbs, or also their arguments); (2) do not provide a mechanism for deciding which scenario applies in a given discourse context and (3) often do not associate confidence levels with information encoded in the graph (e.g., the precedence relation in Regneri et al. (2010)). Instead of constructing a graph and using it to provide information (e.g., prototypical event ordering) to NLP applications, in this work we advocate for constructing a statistical model which is capable to “answer” at least some of the questions these graphs can be used to answer, but doing this without explicitly representing the knowledge as a graph. In our method, the distributed representations (i.e. vectors of real numbers) of event realizations are computed based on distributed representations of predicates and their arguments, and then the event representations are used in a ranker"
W14-1606,N13-1008,0,0.0162647,"Missing"
W14-1606,de-marneffe-etal-2006-generating,0,0.0409424,"Missing"
W14-1606,D12-1110,0,0.0333476,"Missing"
W14-1606,P10-1040,0,0.0151946,"Missing"
W14-1606,E14-1006,1,0.543514,"amount of training data, variability in the used vocabulary, optionality of events (e.g., going to the coffee machine may not be mentioned in a ESD), different granularity of events and variability in the ordering (e.g., coffee may be put in the filter before placing it in the coffee maker). Unlike our work, Regneri et al. (2010) relies on WordNet to provide extra signal when using the Multiple Se3.1.2 Results and discussion We evaluated our event embedding model (EE) against baseline systems (BL , MSA and BS). MSA is the system of Regneri et al. (2010). BS is a hierarchical Bayesian model by Frermann et al. (2014). BL chooses the order of events based on the preferred order of the corresponding verbs in the training set: (e1 , e2 ) is predicted to be in the 4 The event pairs are not coming from the same ESDs making the task harder as the events may not be in any temporal relation. 52 stereotypical order if the number of times the corresponding verbs v1 and v2 appear in this order in the training ESDs exceeds the number of times they appear in the opposite order (not necessary at adjacent positions); a coin is tossed to break ties (or if v1 and v2 are the same verb). This frequency counting method was p"
W14-1606,S13-2001,0,0.046004,"verb pair frequency counts are available at www.usna.edu/Users/cs/nchamber/data/schemas/acl09/verbpair-orders.gz 7 The list of dependencies not considered: aux, auxpass, attr, appos, cc, conj, complm, cop, dep, det, punct, mwe. 55 96.0 96.0 94.1   82.4  83.1 81.8 77.8 81.2 ity to the methods proposed in this work but they are mostly limited to binary relations and deal with predicting missing relations rather than with temporal reasoning of any kind. Identification of temporal relations within a text is a challenging problem and an active area of research (see, e.g., the TempEval task (UzZaman et al., 2013)). Many rule-based and supervised approaches have been proposed in the past. However, integration of common sense knowledge induced from large-scale unannotated resources still remains a challenge. We believe that our approach will provide a powerful signal complementary to information exploited by most existing methods. CJ08 BL EEverb EE 71.0   57.2 50.0 62.7      Figure 3: Results for different frequency bands: unseen, medium frequency (between 1 and 10) and high frequency (> 10) verb pairs. 5 ence chains of length 2 (e.g., chain of length 2 was found for the pair acc"
W14-1606,D10-1115,0,\N,Missing
W19-5211,W18-6412,1,0.90039,"Missing"
W19-5211,D18-1338,0,0.0449376,"Missing"
W19-5211,P17-1080,0,0.101257,"ine Translation with Lexical Shortcuts Denis Emelin1 , Ivan Titov1, 2 , and Rico Sennrich1, 3 1 University of Edinburgh, Scotland University of Amsterdam, Netherlands 3 University of Zurich, Switzerland D.Emelin@sms.ed.ac.uk ititov@inf.ed.ac.uk rico.sennrich@ed.ac.uk 2 Abstract networks (Chen et al., 2018), superior ability to perform lexical disambiguation, and capacity for capturing long-distance dependencies on par with existing alternatives (Tang et al., 2018). Recently, several studies have investigated the nature of features encoded within individual layers of neural translation models (Belinkov et al., 2017, 2018). One central finding reported in this body of work is that, in recurrent architectures, different layers prioritize different information types. As such, lower layers appear to predominantly perform morphological and syntactic processing, whereas semantic features reach their highest concentration towards the top of the layer stack. One necessary consequence of this distributed learning is that different types of information encoded within input representations received by the translation model have to be transported to the layers specialized in exploiting them. Within the transformer"
W19-5211,D18-1458,1,0.887688,"Missing"
W19-5211,W18-6319,0,0.0134713,"et al., 2017), the addition of skip connections between individual layers of a deep neural network results in an implicit ‘deep supervision’ effect (Lee et al., 2015), which aids the training process. In case of our modified transformer, this corresponds to the embedding layer receiving its learning signal from the model’s overall optimization objective as well as from each layer it is connected to, making the model easier to train. 3 3.1 3.3 The results of our translation experiments are summarized in Tables 1-2. To ensure their comparability, we evaluate translation quality using sacreBLEU (Post, 2018). As such, our baseline performance diverges from that reported in (Vaswani et al., 2017). We address this by evaluating our EN→DE models using the scoring script from the tensor2tensor toolkit3 (Vaswani et al., 2018) on the tokenized model output, and list the corresponding BLEU scores in the first column of Table 1. Our evaluation shows that the introduction of lexical shortcuts consistently improves translation quality of the transformer model across different test-sets and language pairs, outperforming transformer-BASE by 0.5 BLEU on average. With feature-fusion, we see even stronger impro"
W19-5211,D16-1079,0,0.0606861,"Missing"
W19-5211,W17-4702,1,0.799236,"Missing"
W19-5211,P18-2051,0,0.0440579,"Missing"
W19-5211,D16-1159,0,0.125732,"Missing"
W19-5211,W18-6401,0,\N,Missing
W19-5211,I17-1001,0,\N,Missing
W19-5211,P16-1162,1,\N,Missing
W19-5211,P18-1164,0,\N,Missing
