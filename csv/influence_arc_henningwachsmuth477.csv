2020.acl-main.287,D19-1290,1,0.813755,"almari and Virtanen (2005) state that four aspects affect persuasion in editorials: linguistic choices, prior beliefs of readers, prior beliefs and behaviors of authors, and the effect of the text. Persuasive effectiveness reflects the rhetorical quality of argumentation (Wachsmuth et al., 2017). To assess effectiveness, Zhang et al. (2016) modeled the flow of content in debates, and Wachsmuth et al. (2016) the argumentative structure of student essays. Others combined different features for these genres (Persing and Ng, 2015). The impact of content selection relates to the notion of framing (Ajjour et al., 2019) and is well-studied in theory (van Eemeren, 2015). As Wang et al. (2017), however, we hypothesize that content and style achieve persuasion jointly. We target argumentative style here primarily, and we analyze its impact on liberal and conservative readers. In related work, Lukin et al. (2017) found that emotional and rational arguments affect people with different personalities, and Durmus and Cardie (2018) take into account the religious and political ideology of debate portal participants. In followup work, Longpre et al. (2019) observed that style is more important for decided listeners."
2020.acl-main.287,D17-1141,1,0.932093,"n common stylistic choices in their leads, bodies, and endings through clustering. From these, we derive writing style patterns that challenge or reinforce the stance of (liberal) readers of (liberal) news editorials, giving insights into what makes argumentation effective. 2 Related Work Compared to other argumentative genres (Stede and Schneider, 2018), news editorials use many rhetorical means to achieve a persuasive effect on readers (van Dijk, 1995). Computational research has dealt with news editorials for retrieving opinions (Yu and Hatzivassiloglou, 2003; Bal, 2009), mining arguments (Al-Khatib et al., 2017), and 1 For reproducibility, the code of our experiments can be found here: https://github.com/webis-de/ acl20-editorials-style-persuasive-effect 3154 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3154–3160 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Feature Base Overview Reference Linguistic inquiry and word count NRC emotional and sentiment lexicon Webis Argumentative Discourse Units MPQA Arguing Lexicon MPQA Subjectivity Classifier Psychological meaningfulness in percentile Count of emotions (e,g. sad, etc.) and polari"
2020.acl-main.287,C16-1324,1,0.870048,"Missing"
2020.acl-main.287,W09-3723,0,0.726797,"s editorials, we finally obtain common stylistic choices in their leads, bodies, and endings through clustering. From these, we derive writing style patterns that challenge or reinforce the stance of (liberal) readers of (liberal) news editorials, giving insights into what makes argumentation effective. 2 Related Work Compared to other argumentative genres (Stede and Schneider, 2018), news editorials use many rhetorical means to achieve a persuasive effect on readers (van Dijk, 1995). Computational research has dealt with news editorials for retrieving opinions (Yu and Hatzivassiloglou, 2003; Bal, 2009), mining arguments (Al-Khatib et al., 2017), and 1 For reproducibility, the code of our experiments can be found here: https://github.com/webis-de/ acl20-editorials-style-persuasive-effect 3154 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3154–3160 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Feature Base Overview Reference Linguistic inquiry and word count NRC emotional and sentiment lexicon Webis Argumentative Discourse Units MPQA Arguing Lexicon MPQA Subjectivity Classifier Psychological meaningfulness in percentile Co"
2020.acl-main.287,bal-saint-dizier-2010-towards,0,0.142438,"con Webis Argumentative Discourse Units MPQA Arguing Lexicon MPQA Subjectivity Classifier Psychological meaningfulness in percentile Count of emotions (e,g. sad, etc.) and polarity words Count of each evidence type (e.g., statistics) Count of 17 types of arguing (e.g., assessments) Count of subjective and objective sentences Pennebaker et al. (2015) Mohammad and Turney (2013) Al-Khatib et al. (2017) Somasundaran et al. (2007) Riloff and Wiebe (2003) Table 1: Summary of the style feature types in our dataset. Each feature is quantified at the level of the editorial. analyzing their properties (Bal and Dizier, 2010; Scheffler and Stede, 2016). While Al-Khatib et al. (2016) modeled the structure underlying editorial argumentation, we use the corpus of El Baff et al. (2018) meant to study the persuasive effects of editorials depending on the readers’ political ideology. Halmari and Virtanen (2005) state that four aspects affect persuasion in editorials: linguistic choices, prior beliefs of readers, prior beliefs and behaviors of authors, and the effect of the text. Persuasive effectiveness reflects the rhetorical quality of argumentation (Wachsmuth et al., 2017). To assess effectiveness, Zhang et al. (201"
2020.acl-main.287,N18-1094,0,0.372668,"the stance of readers towards controversial political issues, depending on the readers’ ideology (El Baff et al., 2018). To affect readers, they often start with an enticing lead paragraph and end their argument with a “punch” (Rich, 2015). Existing research has studied the persuasive effect of argumentative content and structure (Zhang et al., 2016; Wachsmuth et al., 2016) or combinations of content and style (Wang et al., 2017; Persing and Ng, 2017). In addition, some works indicate that different types of content affect readers with different personalities (Lukin et al., 2017) and beliefs (Durmus and Cardie, 2018). However, it remains unexplored so far what stylistic choices in argumentation actually affect which readers. We expect such choices to be key to generating effective argumentation (Wachsmuth et al., 2018). This paper analyzes the persuasive effect of style in news editorial argumentation on readers with different political ideologies (conservative vs. liberal). We model style with widely-used features capturing argumentativeness (Somasundaran et al., 2007), psychological meaning (Tausczik and Pennebaker, 2010), and similar (Section 3). Based on the NYTimes editorial corpus of El Baff et al."
2020.acl-main.287,K18-1044,1,0.884192,"Missing"
2020.acl-main.287,W19-4519,0,0.0137528,"impact of content selection relates to the notion of framing (Ajjour et al., 2019) and is well-studied in theory (van Eemeren, 2015). As Wang et al. (2017), however, we hypothesize that content and style achieve persuasion jointly. We target argumentative style here primarily, and we analyze its impact on liberal and conservative readers. In related work, Lukin et al. (2017) found that emotional and rational arguments affect people with different personalities, and Durmus and Cardie (2018) take into account the religious and political ideology of debate portal participants. In followup work, Longpre et al. (2019) observed that style is more important for decided listeners. Unlike them, we focus on the stylistic choices made in well-planned argumentative texts. The lead paragraphs and the ending of an editorial have special importance (Rich, 2015). Hynds (1990) analyzes how leads and endings changed over time, whereas Moznette and Rarick (1968) examined the readability of an editorial based on them. To our knowledge, however, no one investigated their importance computationally so far. In this paper, we close this gap by analyzing what style of leads and endings is particularly effective compared to th"
2020.acl-main.287,E17-1070,0,0.419734,"aim to challenge or to reinforce the stance of readers towards controversial political issues, depending on the readers’ ideology (El Baff et al., 2018). To affect readers, they often start with an enticing lead paragraph and end their argument with a “punch” (Rich, 2015). Existing research has studied the persuasive effect of argumentative content and structure (Zhang et al., 2016; Wachsmuth et al., 2016) or combinations of content and style (Wang et al., 2017; Persing and Ng, 2017). In addition, some works indicate that different types of content affect readers with different personalities (Lukin et al., 2017) and beliefs (Durmus and Cardie, 2018). However, it remains unexplored so far what stylistic choices in argumentation actually affect which readers. We expect such choices to be key to generating effective argumentation (Wachsmuth et al., 2018). This paper analyzes the persuasive effect of style in news editorial argumentation on readers with different political ideologies (conservative vs. liberal). We model style with widely-used features capturing argumentativeness (Somasundaran et al., 2007), psychological meaning (Tausczik and Pennebaker, 2010), and similar (Section 3). Based on the NYTim"
2020.acl-main.287,W19-8607,1,0.887444,"Missing"
2020.acl-main.287,P15-1053,0,0.144705,"tudy the persuasive effects of editorials depending on the readers’ political ideology. Halmari and Virtanen (2005) state that four aspects affect persuasion in editorials: linguistic choices, prior beliefs of readers, prior beliefs and behaviors of authors, and the effect of the text. Persuasive effectiveness reflects the rhetorical quality of argumentation (Wachsmuth et al., 2017). To assess effectiveness, Zhang et al. (2016) modeled the flow of content in debates, and Wachsmuth et al. (2016) the argumentative structure of student essays. Others combined different features for these genres (Persing and Ng, 2015). The impact of content selection relates to the notion of framing (Ajjour et al., 2019) and is well-studied in theory (van Eemeren, 2015). As Wang et al. (2017), however, we hypothesize that content and style achieve persuasion jointly. We target argumentative style here primarily, and we analyze its impact on liberal and conservative readers. In related work, Lukin et al. (2017) found that emotional and rational arguments affect people with different personalities, and Durmus and Cardie (2018) take into account the religious and political ideology of debate portal participants. In followup w"
2020.acl-main.287,C18-1318,1,0.925384,"Missing"
2020.acl-main.287,I17-1060,0,0.0976805,"uistic choices of the author and their persuasive effect on the reader (Halmari and Virtanen, 2005). News editorials, in particular, aim to challenge or to reinforce the stance of readers towards controversial political issues, depending on the readers’ ideology (El Baff et al., 2018). To affect readers, they often start with an enticing lead paragraph and end their argument with a “punch” (Rich, 2015). Existing research has studied the persuasive effect of argumentative content and structure (Zhang et al., 2016; Wachsmuth et al., 2016) or combinations of content and style (Wang et al., 2017; Persing and Ng, 2017). In addition, some works indicate that different types of content affect readers with different personalities (Lukin et al., 2017) and beliefs (Durmus and Cardie, 2018). However, it remains unexplored so far what stylistic choices in argumentation actually affect which readers. We expect such choices to be key to generating effective argumentation (Wachsmuth et al., 2018). This paper analyzes the persuasive effect of style in news editorial argumentation on readers with different political ideologies (conservative vs. liberal). We model style with widely-used features capturing argumentativen"
2020.acl-main.287,Q17-1016,0,0.102694,"encoded in the linguistic choices of the author and their persuasive effect on the reader (Halmari and Virtanen, 2005). News editorials, in particular, aim to challenge or to reinforce the stance of readers towards controversial political issues, depending on the readers’ ideology (El Baff et al., 2018). To affect readers, they often start with an enticing lead paragraph and end their argument with a “punch” (Rich, 2015). Existing research has studied the persuasive effect of argumentative content and structure (Zhang et al., 2016; Wachsmuth et al., 2016) or combinations of content and style (Wang et al., 2017; Persing and Ng, 2017). In addition, some works indicate that different types of content affect readers with different personalities (Lukin et al., 2017) and beliefs (Durmus and Cardie, 2018). However, it remains unexplored so far what stylistic choices in argumentation actually affect which readers. We expect such choices to be key to generating effective argumentation (Wachsmuth et al., 2018). This paper analyzes the persuasive effect of style in news editorial argumentation on readers with different political ideologies (conservative vs. liberal). We model style with widely-used features c"
2020.acl-main.287,W03-1014,0,0.231148,"tative units in editorials that present evidence, we use the pre-trained evidence classifier of Al-Khatib et al. (2017). For each editorial, we identify the number of sentences that manifest anecdotal, statistical, and testimonial evidence respectively. MPQA Arguing Somasundaran et al. (2007) constructed a lexicon that includes various patterns of arguing such as assessments, doubt, authority, emphasis. For each lexicon, we have one feature that represents the count of the respective pattern in an editorial. MPQA Subjectivity We apply the subjectivity classifier provided in OpinionFinder 2.0 (Riloff and Wiebe, 2003; Wiebe and Riloff, 2005) on the editorials, in order to count the number of subjective and objective sentences there. 4 Data As the basis of our analysis, we use the WebisEditorial-Quality-18 corpus (El Baff et al., 2018). The corpus includes persuasive effect annotations of 1000 English news editorials from the liberal New York Times (NYTimes).2 The annotations capture whether a given editorial challenges the prior stance of readers (i.e., making them rethink it, but not necessarily change it), reinforces their stance (i.e., helping them argue better about the discussed topic), or is ineffec"
2020.acl-main.287,2007.sigdial-1.5,0,0.670249,"addition, some works indicate that different types of content affect readers with different personalities (Lukin et al., 2017) and beliefs (Durmus and Cardie, 2018). However, it remains unexplored so far what stylistic choices in argumentation actually affect which readers. We expect such choices to be key to generating effective argumentation (Wachsmuth et al., 2018). This paper analyzes the persuasive effect of style in news editorial argumentation on readers with different political ideologies (conservative vs. liberal). We model style with widely-used features capturing argumentativeness (Somasundaran et al., 2007), psychological meaning (Tausczik and Pennebaker, 2010), and similar (Section 3). Based on the NYTimes editorial corpus of El Baff et al. (2018) with ideology-specific effect annotations (Section 4), we compare style-oriented with content-oriented classifiers for persuasive effect (Section 5).1 While the general performance of effect prediction seems somewhat limited on the corpus, our experiments yield important results: Conservative readers seem largely unaffected by the style of the (liberal) NYTimes, matching the intuition that content is what dominates opposing ideologies. On the other ha"
2020.acl-main.287,W03-1017,0,0.19064,"t the specific structure of news editorials, we finally obtain common stylistic choices in their leads, bodies, and endings through clustering. From these, we derive writing style patterns that challenge or reinforce the stance of (liberal) readers of (liberal) news editorials, giving insights into what makes argumentation effective. 2 Related Work Compared to other argumentative genres (Stede and Schneider, 2018), news editorials use many rhetorical means to achieve a persuasive effect on readers (van Dijk, 1995). Computational research has dealt with news editorials for retrieving opinions (Yu and Hatzivassiloglou, 2003; Bal, 2009), mining arguments (Al-Khatib et al., 2017), and 1 For reproducibility, the code of our experiments can be found here: https://github.com/webis-de/ acl20-editorials-style-persuasive-effect 3154 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3154–3160 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Feature Base Overview Reference Linguistic inquiry and word count NRC emotional and sentiment lexicon Webis Argumentative Discourse Units MPQA Arguing Lexicon MPQA Subjectivity Classifier Psychological meaningfulness in p"
2020.acl-main.287,N16-1017,0,0.166831,"nteraction between the author and the intended reader of an argumentative text is encoded in the linguistic choices of the author and their persuasive effect on the reader (Halmari and Virtanen, 2005). News editorials, in particular, aim to challenge or to reinforce the stance of readers towards controversial political issues, depending on the readers’ ideology (El Baff et al., 2018). To affect readers, they often start with an enticing lead paragraph and end their argument with a “punch” (Rich, 2015). Existing research has studied the persuasive effect of argumentative content and structure (Zhang et al., 2016; Wachsmuth et al., 2016) or combinations of content and style (Wang et al., 2017; Persing and Ng, 2017). In addition, some works indicate that different types of content affect readers with different personalities (Lukin et al., 2017) and beliefs (Durmus and Cardie, 2018). However, it remains unexplored so far what stylistic choices in argumentation actually affect which readers. We expect such choices to be key to generating effective argumentation (Wachsmuth et al., 2018). This paper analyzes the persuasive effect of style in news editorial argumentation on readers with different political"
2020.acl-main.287,C16-1158,1,0.916003,"he author and the intended reader of an argumentative text is encoded in the linguistic choices of the author and their persuasive effect on the reader (Halmari and Virtanen, 2005). News editorials, in particular, aim to challenge or to reinforce the stance of readers towards controversial political issues, depending on the readers’ ideology (El Baff et al., 2018). To affect readers, they often start with an enticing lead paragraph and end their argument with a “punch” (Rich, 2015). Existing research has studied the persuasive effect of argumentative content and structure (Zhang et al., 2016; Wachsmuth et al., 2016) or combinations of content and style (Wang et al., 2017; Persing and Ng, 2017). In addition, some works indicate that different types of content affect readers with different personalities (Lukin et al., 2017) and beliefs (Durmus and Cardie, 2018). However, it remains unexplored so far what stylistic choices in argumentation actually affect which readers. We expect such choices to be key to generating effective argumentation (Wachsmuth et al., 2018). This paper analyzes the persuasive effect of style in news editorial argumentation on readers with different political ideologies (conservative"
2020.acl-main.287,E17-1017,1,0.85022,"of the editorial. analyzing their properties (Bal and Dizier, 2010; Scheffler and Stede, 2016). While Al-Khatib et al. (2016) modeled the structure underlying editorial argumentation, we use the corpus of El Baff et al. (2018) meant to study the persuasive effects of editorials depending on the readers’ political ideology. Halmari and Virtanen (2005) state that four aspects affect persuasion in editorials: linguistic choices, prior beliefs of readers, prior beliefs and behaviors of authors, and the effect of the text. Persuasive effectiveness reflects the rhetorical quality of argumentation (Wachsmuth et al., 2017). To assess effectiveness, Zhang et al. (2016) modeled the flow of content in debates, and Wachsmuth et al. (2016) the argumentative structure of student essays. Others combined different features for these genres (Persing and Ng, 2015). The impact of content selection relates to the notion of framing (Ajjour et al., 2019) and is well-studied in theory (van Eemeren, 2015). As Wang et al. (2017), however, we hypothesize that content and style achieve persuasion jointly. We target argumentative style here primarily, and we analyze its impact on liberal and conservative readers. In related work,"
2020.acl-main.399,C18-1139,0,0.0407377,"on’s stance, and (3) generating the conclusion’s text with the inferred stance 4334 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4334–4345 c July 5 - 10, 2020. 2020 Association for Computational Linguistics and the inferred target. In this paper, we focus on the first step by proposing two computational approaches for conclusion target inference. As sketched in Figure 1, we hypothesize that the conclusion target is related to the targets of the argument’s premises. To obtain premise targets, we train a state-of-the-art sequence labeling model (Akbik et al., 2018) on target-annotated claims (Bar-Haim et al., 2017). Since the exact relation of premise and conclusion targets is unknown, we develop two complementary inference approaches: One approach ranks premise targets based on their likelihood of being a conclusion target. The other one employs a triplet neural network (Hoffer and Ailon, 2015) that generates a conclusion target embedding from the premise targets in a learned target embedding space. A unique facet of the latter is the integration of the network with a knowledge base of targets (built from any training set), namely, the approach returns"
2020.acl-main.399,C16-1324,1,0.903957,"Missing"
2020.acl-main.399,D18-1403,0,0.0175808,"model of Wang and Ling (2016) as a baseline in our experiments. General research on summarization is manifold and beyond the scope of this work. For a survey, we refer the reader to Gambhir and Gupta (2017). In recent work, we summarize the core of an argument to be used as a snippet in the context of argument search by a two-sentence extract (Alshomary et al., 2020) and Egan et al. (2016) create abstractive summaries of the main points in a debate. We hypothesize a dependency between the target and stance of a conclusion and those of the premises. At a high level, this resembles the work of Angelidis and Lapata (2018) where aspects and sentiments are modeled for the extractive summarization of opinions. We focus on the inference of conclusion targets in this work. Our approach builds upon ideas of Bar-Haim et al. (2017), who classify the stance of premises to a conclusion. To do so, they identify and relate targets in these components, and model stance with sentiment. We do not explicitly tackle stance inference here, because our focus is a conclusion’s target. To identify premise targets, we first train a state-of-the art sequence tagger using contextualized word embeddings (Akbik et al., 2018) on the cor"
2020.acl-main.399,E17-1024,0,0.396839,"promotes equality. (3) Conclusion text generation Stance classification Parents who left school at a young age Premise target identification Premises Abstract con pro pro (2) Stance inference Raising the school leaving age promotes equal opportunities. pro Conclusion Figure 1: Illustration of our full model of generating an argument’s conclusion from its premises. This paper focuses on the identification and inference of targets. Introduction The conclusion (or claim) of a natural language argument conveys a pro or con stance towards some target, such as a controversial concept or statement (Bar-Haim et al., 2017). It is inferred from a set of premises. Conclusions are key to understanding arguments, and hence, critical for any downstream application that processes argumentation. The task of identifying conclusions has been studied intensively in the context of argument mining (Stab and Gurevych, 2014) and automatic essay assessment (Falakmasir et al., 2014). In genres other than essays, however, conclusions often remain implicit, since they are clear from the context of a discussion (Habernal and Gurevych, 2015) or hidden on purpose for rhetorical reasons, as is often the case in news editorials (Al K"
2020.acl-main.399,P16-2085,0,0.179685,"(Stab and Gurevych, 2014) and automatic essay assessment (Falakmasir et al., 2014). In genres other than essays, however, conclusions often remain implicit, since they are clear from the context of a discussion (Habernal and Gurevych, 2015) or hidden on purpose for rhetorical reasons, as is often the case in news editorials (Al Khatib et al., 2016). This alters the task entirely to become a synthesis task: Given an argument’s premises, generate its conclusion. As detailed in Section 2, research on argumentation synthesis is still limited. Existing approaches focus on generating single claims (Bilu and Slonim, 2016), new arguments (Reisert et al., 2015), counterarguments (Hua et al., 2019), or argumentative texts (Wachsmuth et al., 2018). Closer to conclusion generation, Egan et al. (2016) summarized the main points of online debates, and Wang and Ling (2016) worked on identifying the main claim of an argument through abstractive summarization. To our knowledge, however, no approach so far reconstructs an argument’s conclusion from its premises. In general, we consider the synthesis task outlined above. Conceptually, we decompose this task into three steps, as depicted in Figure 1: (1) inferring the conc"
2020.acl-main.399,Q17-1010,0,0.00569372,"ic Evaluation In this section, we report on empirical experiments, along with their results, performed to evaluate our approaches to target inference. were not explicitly phrased, our target identifier did not annotate any token. Hence, we eliminated those cases from the test set.4 Approaches For the premise target ranking approach, we trained LambdaMART (Burges, 2010) on each training set with 1000 estimators and a learning rate of 0.02. We refer to this approach below as Premise Targets (ranking). For target embedding learning, we used the pretrained FastText embeddings with 300 dimensions (Bojanowski et al., 2017) to initially represent each target. To obtain a knowledge base of candidate targets, we applied the target identifier to all conclusions of all training sets.5 The resulting lexicon contains 1,780 targets, each is represented by its FastText embedding. We implemented the triplet neural network as three feed-forward neural networks, each with two layers and shared weights. We call this approach Target Embedding (learning). The simple hybrid of both approaches introduced above is denoted Hybrid (ranking & embedding). To evaluate target inference, we use the iDebate Dataset and the two essay dat"
2020.acl-main.399,W16-2815,0,0.119572,"cific inference schemes for user-defined content (Green, 2017), modeled rhetorical aspects in synthesis (Wachsmuth et al., 4335 2018), and composed arguments that follow a strategy (El Baff et al., 2019). All these methods synthesize new argumentative content. In contrast, we aim for the missing components of given arguments. As such, our task resembles enthymeme reconstruction. An enthymeme is an implicit premise, usually the warrant (or major premise) that clarifies how a conclusion is inferred from the given premises (Walton et al., 2008). Motivated by the importance of finding the thesis, Boltuzic and Šnajder (2016) study how to identify such enthymemes given the other components. Similarly, Habernal et al. (2018) present the task of identifying the correct warrant from two options, and Rajendran et al. (2016) aim to generate the premise connecting an aspect-related opinion to an overall opinion. Instead of missing premises, we aim to synthesize (parts of) an argument’s conclusion. For any text generation task, a candidate technique is sequence-to-sequence models (Sutskever et al., 2014). Relevant in the given context, Hua and Wang (2018) used such models to generate counterarguments, and Hua et al. (201"
2020.acl-main.399,W18-6509,1,0.816119,"(2018) present the task of identifying the correct warrant from two options, and Rajendran et al. (2016) aim to generate the premise connecting an aspect-related opinion to an overall opinion. Instead of missing premises, we aim to synthesize (parts of) an argument’s conclusion. For any text generation task, a candidate technique is sequence-to-sequence models (Sutskever et al., 2014). Relevant in the given context, Hua and Wang (2018) used such models to generate counterarguments, and Hua et al. (2019) extended this approach by planning and retrieval mechanisms. With a comparable intention, Chen et al. (2018) modified the bias of news headlines from right-toleft or vice versa. Closest to our work is the approach of Wang and Ling (2016) whose sequenceto-sequence model generates summaries for opinionated and argumentative text. Like us, the authors face the problem of varying numbers of input components, and tackle this using an importancebased sampling method. For their evaluation, they crawled arguments from idebate.org. We use this dataset in our experiments. Unfortunately, their manual evaluation considers opinionated text only, leaving the semantic adequacy of the generated argument summaries u"
2020.acl-main.399,P19-1255,0,0.052996,"4). In genres other than essays, however, conclusions often remain implicit, since they are clear from the context of a discussion (Habernal and Gurevych, 2015) or hidden on purpose for rhetorical reasons, as is often the case in news editorials (Al Khatib et al., 2016). This alters the task entirely to become a synthesis task: Given an argument’s premises, generate its conclusion. As detailed in Section 2, research on argumentation synthesis is still limited. Existing approaches focus on generating single claims (Bilu and Slonim, 2016), new arguments (Reisert et al., 2015), counterarguments (Hua et al., 2019), or argumentative texts (Wachsmuth et al., 2018). Closer to conclusion generation, Egan et al. (2016) summarized the main points of online debates, and Wang and Ling (2016) worked on identifying the main claim of an argument through abstractive summarization. To our knowledge, however, no approach so far reconstructs an argument’s conclusion from its premises. In general, we consider the synthesis task outlined above. Conceptually, we decompose this task into three steps, as depicted in Figure 1: (1) inferring the conclusion’s target from the premises, (2) inferring the conclusion’s stance, a"
2020.acl-main.399,W16-2816,0,0.0652267,"m the context of a discussion (Habernal and Gurevych, 2015) or hidden on purpose for rhetorical reasons, as is often the case in news editorials (Al Khatib et al., 2016). This alters the task entirely to become a synthesis task: Given an argument’s premises, generate its conclusion. As detailed in Section 2, research on argumentation synthesis is still limited. Existing approaches focus on generating single claims (Bilu and Slonim, 2016), new arguments (Reisert et al., 2015), counterarguments (Hua et al., 2019), or argumentative texts (Wachsmuth et al., 2018). Closer to conclusion generation, Egan et al. (2016) summarized the main points of online debates, and Wang and Ling (2016) worked on identifying the main claim of an argument through abstractive summarization. To our knowledge, however, no approach so far reconstructs an argument’s conclusion from its premises. In general, we consider the synthesis task outlined above. Conceptually, we decompose this task into three steps, as depicted in Figure 1: (1) inferring the conclusion’s target from the premises, (2) inferring the conclusion’s stance, and (3) generating the conclusion’s text with the inferred stance 4334 Proceedings of the 58th Annual M"
2020.acl-main.399,P18-1021,0,0.0466629,"2008). Motivated by the importance of finding the thesis, Boltuzic and Šnajder (2016) study how to identify such enthymemes given the other components. Similarly, Habernal et al. (2018) present the task of identifying the correct warrant from two options, and Rajendran et al. (2016) aim to generate the premise connecting an aspect-related opinion to an overall opinion. Instead of missing premises, we aim to synthesize (parts of) an argument’s conclusion. For any text generation task, a candidate technique is sequence-to-sequence models (Sutskever et al., 2014). Relevant in the given context, Hua and Wang (2018) used such models to generate counterarguments, and Hua et al. (2019) extended this approach by planning and retrieval mechanisms. With a comparable intention, Chen et al. (2018) modified the bias of news headlines from right-toleft or vice versa. Closest to our work is the approach of Wang and Ling (2016) whose sequenceto-sequence model generates summaries for opinionated and argumentative text. Like us, the authors face the problem of varying numbers of input components, and tackle this using an importancebased sampling method. For their evaluation, they crawled arguments from idebate.org. W"
2020.acl-main.399,W19-8607,1,0.902062,"Missing"
2020.acl-main.399,W07-0734,0,0.0271692,"Missing"
2020.acl-main.399,P02-1040,0,0.107234,"Missing"
2020.acl-main.399,D15-1110,0,0.0217774,"earch on student essay assessment (Burstein and Marcu, 2003). Falakmasir et al. (2014) show the importance of essay conclusions in applications, whereas Jabbari et al. (2016) specifically target an essay’s overall conclusion, i.e., its thesis (also known as major, main, or central claim). Given the importance of theses, we dedicate one experiment particularly targeting them below. The classification of argument components (as theses, conclusions, premises, etc.) is a core task in argument mining (Stede and Schneider, 2018) and has been approached for different genres (Stab and Gurevych, 2014; Peldszus and Stede, 2015). As Habernal and Gurevych (2015) observe, though, real-world arguments often leave the conclusion implicit, particularly where it is clear in the context of a discussion. In genres such as news editorials, conclusions may even be left out on purpose, in order to persuade readers in a “hidden” manner (Al Khatib et al., 2016). If an implicit conclusion is needed, it hence needs to be synthesized. Argumentation synthesis research is on the rise. Early argument generation approaches relied on rule-based discourse planning techniques (Zukerman et al., 2000). Later, Reisert et al. (2015) generalize"
2020.acl-main.399,D15-1255,0,0.162101,"t conveys a pro or con stance towards some target, such as a controversial concept or statement (Bar-Haim et al., 2017). It is inferred from a set of premises. Conclusions are key to understanding arguments, and hence, critical for any downstream application that processes argumentation. The task of identifying conclusions has been studied intensively in the context of argument mining (Stab and Gurevych, 2014) and automatic essay assessment (Falakmasir et al., 2014). In genres other than essays, however, conclusions often remain implicit, since they are clear from the context of a discussion (Habernal and Gurevych, 2015) or hidden on purpose for rhetorical reasons, as is often the case in news editorials (Al Khatib et al., 2016). This alters the task entirely to become a synthesis task: Given an argument’s premises, generate its conclusion. As detailed in Section 2, research on argumentation synthesis is still limited. Existing approaches focus on generating single claims (Bilu and Slonim, 2016), new arguments (Reisert et al., 2015), counterarguments (Hua et al., 2019), or argumentative texts (Wachsmuth et al., 2018). Closer to conclusion generation, Egan et al. (2016) summarized the main points of online deb"
2020.acl-main.399,W16-2804,0,0.150749,"). All these methods synthesize new argumentative content. In contrast, we aim for the missing components of given arguments. As such, our task resembles enthymeme reconstruction. An enthymeme is an implicit premise, usually the warrant (or major premise) that clarifies how a conclusion is inferred from the given premises (Walton et al., 2008). Motivated by the importance of finding the thesis, Boltuzic and Šnajder (2016) study how to identify such enthymemes given the other components. Similarly, Habernal et al. (2018) present the task of identifying the correct warrant from two options, and Rajendran et al. (2016) aim to generate the premise connecting an aspect-related opinion to an overall opinion. Instead of missing premises, we aim to synthesize (parts of) an argument’s conclusion. For any text generation task, a candidate technique is sequence-to-sequence models (Sutskever et al., 2014). Relevant in the given context, Hua and Wang (2018) used such models to generate counterarguments, and Hua et al. (2019) extended this approach by planning and retrieval mechanisms. With a comparable intention, Chen et al. (2018) modified the bias of news headlines from right-toleft or vice versa. Closest to our wo"
2020.acl-main.399,N18-1175,1,0.737726,"Wachsmuth et al., 4335 2018), and composed arguments that follow a strategy (El Baff et al., 2019). All these methods synthesize new argumentative content. In contrast, we aim for the missing components of given arguments. As such, our task resembles enthymeme reconstruction. An enthymeme is an implicit premise, usually the warrant (or major premise) that clarifies how a conclusion is inferred from the given premises (Walton et al., 2008). Motivated by the importance of finding the thesis, Boltuzic and Šnajder (2016) study how to identify such enthymemes given the other components. Similarly, Habernal et al. (2018) present the task of identifying the correct warrant from two options, and Rajendran et al. (2016) aim to generate the premise connecting an aspect-related opinion to an overall opinion. Instead of missing premises, we aim to synthesize (parts of) an argument’s conclusion. For any text generation task, a candidate technique is sequence-to-sequence models (Sutskever et al., 2014). Relevant in the given context, Hua and Wang (2018) used such models to generate counterarguments, and Hua et al. (2019) extended this approach by planning and retrieval mechanisms. With a comparable intention, Chen et"
2020.acl-main.399,W15-0507,0,0.35911,"essay assessment (Falakmasir et al., 2014). In genres other than essays, however, conclusions often remain implicit, since they are clear from the context of a discussion (Habernal and Gurevych, 2015) or hidden on purpose for rhetorical reasons, as is often the case in news editorials (Al Khatib et al., 2016). This alters the task entirely to become a synthesis task: Given an argument’s premises, generate its conclusion. As detailed in Section 2, research on argumentation synthesis is still limited. Existing approaches focus on generating single claims (Bilu and Slonim, 2016), new arguments (Reisert et al., 2015), counterarguments (Hua et al., 2019), or argumentative texts (Wachsmuth et al., 2018). Closer to conclusion generation, Egan et al. (2016) summarized the main points of online debates, and Wang and Ling (2016) worked on identifying the main claim of an argument through abstractive summarization. To our knowledge, however, no approach so far reconstructs an argument’s conclusion from its premises. In general, we consider the synthesis task outlined above. Conceptually, we decompose this task into three steps, as depicted in Figure 1: (1) inferring the conclusion’s target from the premises, (2)"
2020.acl-main.399,P15-4019,0,0.0256367,"ersuade readers in a “hidden” manner (Al Khatib et al., 2016). If an implicit conclusion is needed, it hence needs to be synthesized. Argumentation synthesis research is on the rise. Early argument generation approaches relied on rule-based discourse planning techniques (Zukerman et al., 2000). Later, Reisert et al. (2015) generalized target-stance relations from claims and used them to automatically create new arguments. The relations were curated manually, though. An approach that finds the best conclusion for generation among a set of candidate claims was presented by Yanase et al. (2015). Sato et al. (2015) built upon this approach to phrase texts with multiple arguments. Others recycled targets and predicates of claims in new claims (Bilu and Slonim, 2016), generated arguments with specific inference schemes for user-defined content (Green, 2017), modeled rhetorical aspects in synthesis (Wachsmuth et al., 4335 2018), and composed arguments that follow a strategy (El Baff et al., 2019). All these methods synthesize new argumentative content. In contrast, we aim for the missing components of given arguments. As such, our task resembles enthymeme reconstruction. An enthymeme is an implicit premise"
2020.acl-main.399,W00-1408,0,0.278256,"rent genres (Stab and Gurevych, 2014; Peldszus and Stede, 2015). As Habernal and Gurevych (2015) observe, though, real-world arguments often leave the conclusion implicit, particularly where it is clear in the context of a discussion. In genres such as news editorials, conclusions may even be left out on purpose, in order to persuade readers in a “hidden” manner (Al Khatib et al., 2016). If an implicit conclusion is needed, it hence needs to be synthesized. Argumentation synthesis research is on the rise. Early argument generation approaches relied on rule-based discourse planning techniques (Zukerman et al., 2000). Later, Reisert et al. (2015) generalized target-stance relations from claims and used them to automatically create new arguments. The relations were curated manually, though. An approach that finds the best conclusion for generation among a set of candidate claims was presented by Yanase et al. (2015). Sato et al. (2015) built upon this approach to phrase texts with multiple arguments. Others recycled targets and predicates of claims in new claims (Bilu and Slonim, 2016), generated arguments with specific inference schemes for user-defined content (Green, 2017), modeled rhetorical aspects in"
2020.acl-main.399,P17-1099,0,0.0254025,"zation. For generation, we used three LSTM layers with hidden size 150 and a pretrained embedding of size 300. Extra features of the original approach were left out, as they did not help much in our case. We trained the model with batch size 48 and learning rate 0.1 using the Adagrad optimizer (Duchi et al., 2011). For translation, we followed Wang and Ling. To identify targets in the generated summaries, we employed our target identifier. We refer to this baseline as Seq2Seq. To test our hypothesis on the relation of premise and conclusion targets, we extended Seq2Seq by a pointer generator (See et al., 2017) and an extra binary feature that encodes whether a token belongs to a target or not, allowing the model to learn this relation. We call this Seq2Seq (w/ premise targets). On the other hand, we complemented our approaches with simpler variants, in order to check whether learning is needed. Instead of premise tar3 4 5.1 Premise Target Identification We implemented the target identifier as a BiLSTMCRF with hidden layer size 256, using the pretrained contextual string embedding model of Akbik et al. (2018). We trained the model on the training set of the Claim Stance Dataset with batch size 16 an"
2020.acl-main.399,D14-1006,0,0.209591,"of our full model of generating an argument’s conclusion from its premises. This paper focuses on the identification and inference of targets. Introduction The conclusion (or claim) of a natural language argument conveys a pro or con stance towards some target, such as a controversial concept or statement (Bar-Haim et al., 2017). It is inferred from a set of premises. Conclusions are key to understanding arguments, and hence, critical for any downstream application that processes argumentation. The task of identifying conclusions has been studied intensively in the context of argument mining (Stab and Gurevych, 2014) and automatic essay assessment (Falakmasir et al., 2014). In genres other than essays, however, conclusions often remain implicit, since they are clear from the context of a discussion (Habernal and Gurevych, 2015) or hidden on purpose for rhetorical reasons, as is often the case in news editorials (Al Khatib et al., 2016). This alters the task entirely to become a synthesis task: Given an argument’s premises, generate its conclusion. As detailed in Section 2, research on argumentation synthesis is still limited. Existing approaches focus on generating single claims (Bilu and Slonim, 2016), n"
2020.acl-main.399,C18-1318,1,0.924044,"Missing"
2020.acl-main.399,N16-1007,0,0.368999,"on purpose for rhetorical reasons, as is often the case in news editorials (Al Khatib et al., 2016). This alters the task entirely to become a synthesis task: Given an argument’s premises, generate its conclusion. As detailed in Section 2, research on argumentation synthesis is still limited. Existing approaches focus on generating single claims (Bilu and Slonim, 2016), new arguments (Reisert et al., 2015), counterarguments (Hua et al., 2019), or argumentative texts (Wachsmuth et al., 2018). Closer to conclusion generation, Egan et al. (2016) summarized the main points of online debates, and Wang and Ling (2016) worked on identifying the main claim of an argument through abstractive summarization. To our knowledge, however, no approach so far reconstructs an argument’s conclusion from its premises. In general, we consider the synthesis task outlined above. Conceptually, we decompose this task into three steps, as depicted in Figure 1: (1) inferring the conclusion’s target from the premises, (2) inferring the conclusion’s stance, and (3) generating the conclusion’s text with the inferred stance 4334 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4334–434"
2020.acl-main.399,W15-0512,0,0.0267417,"purpose, in order to persuade readers in a “hidden” manner (Al Khatib et al., 2016). If an implicit conclusion is needed, it hence needs to be synthesized. Argumentation synthesis research is on the rise. Early argument generation approaches relied on rule-based discourse planning techniques (Zukerman et al., 2000). Later, Reisert et al. (2015) generalized target-stance relations from claims and used them to automatically create new arguments. The relations were curated manually, though. An approach that finds the best conclusion for generation among a set of candidate claims was presented by Yanase et al. (2015). Sato et al. (2015) built upon this approach to phrase texts with multiple arguments. Others recycled targets and predicates of claims in new claims (Bilu and Slonim, 2016), generated arguments with specific inference schemes for user-defined content (Green, 2017), modeled rhetorical aspects in synthesis (Wachsmuth et al., 4335 2018), and composed arguments that follow a strategy (El Baff et al., 2019). All these methods synthesize new argumentative content. In contrast, we aim for the missing components of given arguments. As such, our task resembles enthymeme reconstruction. An enthymeme is"
2020.argmining-1.3,W14-2109,0,0.029218,"our approach to improve the quality of web argument corpora, beyond the one studied (Section 6). Altogether, the contribution of this paper is three-fold: • A semi-supervised approach to detect argumentatively irrelevant sentences in web arguments. • Several common lexical patterns of relevance and irrelevance in web arguments. • A cleaned version of the largest available argument corpus, with notably less irrelevant text. 2 Related Work Initially, research on tasks such as argument mining has largely been carried out on small, well-curated collections of texts, including Wikipedia articles (Aharoni et al., 2014), student essays (Stab and Gurevych, 2014), pure arguments (Peldszus and Stede, 2015), and presidential debates (Lawrence and Reed, 2017). Major real-world applications of computational argumentation, however, need to scale up to web contexts to fulfill their purpose. This includes search engines that oppose pro and con arguments on controversial issues (Wachsmuth et al., 2017b), technologies that debate humans (Toledo et al., 2019), and more. To obtain web arguments, many works have relied on crawled debate portals and similar web platforms, often in a distant-supervision manner where argumen"
2020.argmining-1.3,W17-5115,1,0.766277,"ervised learning approach for corpus cleansing. In general, we follow the bootstrapping idea of successful pattern mining methods, such as DIPRE (Brin, 1998), Snowball (Agichtein and Gravano, 2000), and Espresso (Pantel and Pennacchiotti, 2006). While these methods aim at semantically relevant information, we distinguish pragmatically relevant from irrelevant text within an author’s argumentative discourse. We are not aware of any other approach in this direction. It is noteworthy in this regard that the cleansing task at hand differs notably from the unit segmentation of argumentative texts (Ajjour et al., 2017). While all argumentative units match the notion of relevance considered here (defined in Section 3), also non-argumentative units may be seen as relevant, if they give facts, definitions, or other background information serving as context for the argumentative units. As such, our notion of relevance relates to the local relevance with respect to some conclusion rather than the global relevance of an argumentative statement in the discussion of an issue (Wachsmuth et al., 2017a). 3 Approach This section presents our semi-supervised learning approach to detecting irrelevant text in web argument"
2020.argmining-1.3,N16-1165,1,0.895625,"zkoreit et al., 2017), and debating technologies (Toledo et al., 2019). Such applications rely on large pools of up-to-date arguments, which can hardly be found anywere but on the web. One of the most used web argument sources are debate portals where people jointly collect arguments or debate each other on defined issues. Debate portals, and similar web platforms, are rich of argumentatively relevant content and structure, including arguments as well as facts, background information, and similar. This enables researchers to crawl large-scale argument corpora in a distantly-supervised manner (Al-Khatib et al., 2016). However, the texts found on debate portals also comprise debate-specific language and boilerplate text that is likely to be irrelevant, if not even detrimental, to the mentioned applications. In the text in Figure 1, for instance, the author defines the debated issue (sentence #2), states a thesis (#3–5), and presents two arguments (#6–8, #9–13) — all of which can be considered argumentatively relevant. In contrast, sentences #1, #14, and #15 add nothing of importance, merely making meta-comments and expressing gratitude. In other cases, irrelevant text includes salutations, insults, purely"
2020.argmining-1.3,P12-2041,0,0.030807,"fill their purpose. This includes search engines that oppose pro and con arguments on controversial issues (Wachsmuth et al., 2017b), technologies that debate humans (Toledo et al., 2019), and more. To obtain web arguments, many works have relied on crawled debate portals and similar web platforms, often in a distant-supervision manner where argumentative structure and similar annotations are directly derived from available meta-information (Al-Khatib et al., 2016). Corpora have been built in such a way based on several debate portals, including 4forums.com (Walker et al., 2012), idebate.org (Cabrio and Villata, 2012), createdebate.com (Habernal and Gurevych, 2016), debate.org (Durmus and Cardie, 2019), and reddit.com/r/changemyview (Egawa et al., 2020). Naturally, less curation of the acquired web texts comes at the cost of more noise, which in turn calls for a cleansing of the resulting corpus. 1 Both the original and the cleaned args.me corpus are found at: https://webis.de/data.html#args-me-corpus 20 Cleansing processes are described in several publications on argument corpora, mostly only referring to the acquired annotations though (Habernal and Gurevych, 2016; Toledo et al., 2019; Gretz et al., 2020"
2020.argmining-1.3,P19-1057,0,0.0229046,"ontroversial issues (Wachsmuth et al., 2017b), technologies that debate humans (Toledo et al., 2019), and more. To obtain web arguments, many works have relied on crawled debate portals and similar web platforms, often in a distant-supervision manner where argumentative structure and similar annotations are directly derived from available meta-information (Al-Khatib et al., 2016). Corpora have been built in such a way based on several debate portals, including 4forums.com (Walker et al., 2012), idebate.org (Cabrio and Villata, 2012), createdebate.com (Habernal and Gurevych, 2016), debate.org (Durmus and Cardie, 2019), and reddit.com/r/changemyview (Egawa et al., 2020). Naturally, less curation of the acquired web texts comes at the cost of more noise, which in turn calls for a cleansing of the resulting corpus. 1 Both the original and the cleaned args.me corpus are found at: https://webis.de/data.html#args-me-corpus 20 Cleansing processes are described in several publications on argument corpora, mostly only referring to the acquired annotations though (Habernal and Gurevych, 2016; Toledo et al., 2019; Gretz et al., 2020). In contrast, the paper at hand targets the cleansing of the corpus texts themselves"
2020.argmining-1.3,2020.lrec-1.143,0,0.0164283,"es that debate humans (Toledo et al., 2019), and more. To obtain web arguments, many works have relied on crawled debate portals and similar web platforms, often in a distant-supervision manner where argumentative structure and similar annotations are directly derived from available meta-information (Al-Khatib et al., 2016). Corpora have been built in such a way based on several debate portals, including 4forums.com (Walker et al., 2012), idebate.org (Cabrio and Villata, 2012), createdebate.com (Habernal and Gurevych, 2016), debate.org (Durmus and Cardie, 2019), and reddit.com/r/changemyview (Egawa et al., 2020). Naturally, less curation of the acquired web texts comes at the cost of more noise, which in turn calls for a cleansing of the resulting corpus. 1 Both the original and the cleaned args.me corpus are found at: https://webis.de/data.html#args-me-corpus 20 Cleansing processes are described in several publications on argument corpora, mostly only referring to the acquired annotations though (Habernal and Gurevych, 2016; Toledo et al., 2019; Gretz et al., 2020). In contrast, the paper at hand targets the cleansing of the corpus texts themselves. Only few works describe respective cleansing steps"
2020.argmining-1.3,P16-1150,0,0.0187073,"nes that oppose pro and con arguments on controversial issues (Wachsmuth et al., 2017b), technologies that debate humans (Toledo et al., 2019), and more. To obtain web arguments, many works have relied on crawled debate portals and similar web platforms, often in a distant-supervision manner where argumentative structure and similar annotations are directly derived from available meta-information (Al-Khatib et al., 2016). Corpora have been built in such a way based on several debate portals, including 4forums.com (Walker et al., 2012), idebate.org (Cabrio and Villata, 2012), createdebate.com (Habernal and Gurevych, 2016), debate.org (Durmus and Cardie, 2019), and reddit.com/r/changemyview (Egawa et al., 2020). Naturally, less curation of the acquired web texts comes at the cost of more noise, which in turn calls for a cleansing of the resulting corpus. 1 Both the original and the cleaned args.me corpus are found at: https://webis.de/data.html#args-me-corpus 20 Cleansing processes are described in several publications on argument corpora, mostly only referring to the acquired annotations though (Habernal and Gurevych, 2016; Toledo et al., 2019; Gretz et al., 2020). In contrast, the paper at hand targets the cl"
2020.argmining-1.3,J17-1004,0,0.0163681,"oth the original and the cleaned args.me corpus are found at: https://webis.de/data.html#args-me-corpus 20 Cleansing processes are described in several publications on argument corpora, mostly only referring to the acquired annotations though (Habernal and Gurevych, 2016; Toledo et al., 2019; Gretz et al., 2020). In contrast, the paper at hand targets the cleansing of the corpus texts themselves. Only few works describe respective cleansing steps in detail. Among these, Al-Khatib et al. (2016) deleted special symbols and debate-specific phrases such as “this house” from crawled arguments, and Habernal and Gurevych (2017) removed quotations of previous posts in debate posts. Wachsmuth et al. (2017b) discarded certain types of noisy instances completely for the argument search engine args.me, but the texts in the original associated corpus (Ajjour et al., 2019) still contain much irrelevant text, as our experiments will reveal. Applying our approach has led to an improved version of that corpus. In this paper, we introduce a semi-supervised learning approach for corpus cleansing. In general, we follow the bootstrapping idea of successful pattern mining methods, such as DIPRE (Brin, 1998), Snowball (Agichtein an"
2020.argmining-1.3,W17-5114,0,0.0158373,"paper is three-fold: • A semi-supervised approach to detect argumentatively irrelevant sentences in web arguments. • Several common lexical patterns of relevance and irrelevance in web arguments. • A cleaned version of the largest available argument corpus, with notably less irrelevant text. 2 Related Work Initially, research on tasks such as argument mining has largely been carried out on small, well-curated collections of texts, including Wikipedia articles (Aharoni et al., 2014), student essays (Stab and Gurevych, 2014), pure arguments (Peldszus and Stede, 2015), and presidential debates (Lawrence and Reed, 2017). Major real-world applications of computational argumentation, however, need to scale up to web contexts to fulfill their purpose. This includes search engines that oppose pro and con arguments on controversial issues (Wachsmuth et al., 2017b), technologies that debate humans (Toledo et al., 2019), and more. To obtain web arguments, many works have relied on crawled debate portals and similar web platforms, often in a distant-supervision manner where argumentative structure and similar annotations are directly derived from available meta-information (Al-Khatib et al., 2016). Corpora have been"
2020.argmining-1.3,P06-1015,0,0.154135,"posts in debate posts. Wachsmuth et al. (2017b) discarded certain types of noisy instances completely for the argument search engine args.me, but the texts in the original associated corpus (Ajjour et al., 2019) still contain much irrelevant text, as our experiments will reveal. Applying our approach has led to an improved version of that corpus. In this paper, we introduce a semi-supervised learning approach for corpus cleansing. In general, we follow the bootstrapping idea of successful pattern mining methods, such as DIPRE (Brin, 1998), Snowball (Agichtein and Gravano, 2000), and Espresso (Pantel and Pennacchiotti, 2006). While these methods aim at semantically relevant information, we distinguish pragmatically relevant from irrelevant text within an author’s argumentative discourse. We are not aware of any other approach in this direction. It is noteworthy in this regard that the cleansing task at hand differs notably from the unit segmentation of argumentative texts (Ajjour et al., 2017). While all argumentative units match the notion of relevance considered here (defined in Section 3), also non-argumentative units may be seen as relevant, if they give facts, definitions, or other background information ser"
2020.argmining-1.3,D15-1110,0,0.0220714,"ied (Section 6). Altogether, the contribution of this paper is three-fold: • A semi-supervised approach to detect argumentatively irrelevant sentences in web arguments. • Several common lexical patterns of relevance and irrelevance in web arguments. • A cleaned version of the largest available argument corpus, with notably less irrelevant text. 2 Related Work Initially, research on tasks such as argument mining has largely been carried out on small, well-curated collections of texts, including Wikipedia articles (Aharoni et al., 2014), student essays (Stab and Gurevych, 2014), pure arguments (Peldszus and Stede, 2015), and presidential debates (Lawrence and Reed, 2017). Major real-world applications of computational argumentation, however, need to scale up to web contexts to fulfill their purpose. This includes search engines that oppose pro and con arguments on controversial issues (Wachsmuth et al., 2017b), technologies that debate humans (Toledo et al., 2019), and more. To obtain web arguments, many works have relied on crawled debate portals and similar web platforms, often in a distant-supervision manner where argumentative structure and similar annotations are directly derived from available meta-inf"
2020.argmining-1.3,C14-1142,0,0.0315891,"web argument corpora, beyond the one studied (Section 6). Altogether, the contribution of this paper is three-fold: • A semi-supervised approach to detect argumentatively irrelevant sentences in web arguments. • Several common lexical patterns of relevance and irrelevance in web arguments. • A cleaned version of the largest available argument corpus, with notably less irrelevant text. 2 Related Work Initially, research on tasks such as argument mining has largely been carried out on small, well-curated collections of texts, including Wikipedia articles (Aharoni et al., 2014), student essays (Stab and Gurevych, 2014), pure arguments (Peldszus and Stede, 2015), and presidential debates (Lawrence and Reed, 2017). Major real-world applications of computational argumentation, however, need to scale up to web contexts to fulfill their purpose. This includes search engines that oppose pro and con arguments on controversial issues (Wachsmuth et al., 2017b), technologies that debate humans (Toledo et al., 2019), and more. To obtain web arguments, many works have relied on crawled debate portals and similar web platforms, often in a distant-supervision manner where argumentative structure and similar annotations a"
2020.argmining-1.3,D19-1564,0,0.0713464,"patterns from sentences matching the patterns. In the existing args.me corpus with 400k argumentative texts, our approach detects almost 87k irrelevant sentences, at a precision of 0.97 according to manual evaluation. With low effort, the approach can be adapted to other web argument corpora, providing a generic way to improve corpus quality. 1 Introduction Computational argumentation research lays the ground for applications that support opinion formation, including argument search engines (Wachsmuth et al., 2017b), collective deliberation (Uszkoreit et al., 2017), and debating technologies (Toledo et al., 2019). Such applications rely on large pools of up-to-date arguments, which can hardly be found anywere but on the web. One of the most used web argument sources are debate portals where people jointly collect arguments or debate each other on defined issues. Debate portals, and similar web platforms, are rich of argumentatively relevant content and structure, including arguments as well as facts, background information, and similar. This enables researchers to crawl large-scale argument corpora in a distantly-supervised manner (Al-Khatib et al., 2016). However, the texts found on debate portals al"
2020.argmining-1.3,E17-3002,0,0.0432646,"Missing"
2020.argmining-1.3,E17-1017,1,0.547477,"cally learns basic lexical patterns of relevance and irrelevance and then incrementally bootstraps new patterns from sentences matching the patterns. In the existing args.me corpus with 400k argumentative texts, our approach detects almost 87k irrelevant sentences, at a precision of 0.97 according to manual evaluation. With low effort, the approach can be adapted to other web argument corpora, providing a generic way to improve corpus quality. 1 Introduction Computational argumentation research lays the ground for applications that support opinion formation, including argument search engines (Wachsmuth et al., 2017b), collective deliberation (Uszkoreit et al., 2017), and debating technologies (Toledo et al., 2019). Such applications rely on large pools of up-to-date arguments, which can hardly be found anywere but on the web. One of the most used web argument sources are debate portals where people jointly collect arguments or debate each other on defined issues. Debate portals, and similar web platforms, are rich of argumentatively relevant content and structure, including arguments as well as facts, background information, and similar. This enables researchers to crawl large-scale argument corpora in"
2020.argmining-1.3,W17-5106,1,0.511043,"cally learns basic lexical patterns of relevance and irrelevance and then incrementally bootstraps new patterns from sentences matching the patterns. In the existing args.me corpus with 400k argumentative texts, our approach detects almost 87k irrelevant sentences, at a precision of 0.97 according to manual evaluation. With low effort, the approach can be adapted to other web argument corpora, providing a generic way to improve corpus quality. 1 Introduction Computational argumentation research lays the ground for applications that support opinion formation, including argument search engines (Wachsmuth et al., 2017b), collective deliberation (Uszkoreit et al., 2017), and debating technologies (Toledo et al., 2019). Such applications rely on large pools of up-to-date arguments, which can hardly be found anywere but on the web. One of the most used web argument sources are debate portals where people jointly collect arguments or debate each other on defined issues. Debate portals, and similar web platforms, are rich of argumentatively relevant content and structure, including arguments as well as facts, background information, and similar. This enables researchers to crawl large-scale argument corpora in"
2020.argmining-1.3,walker-etal-2012-corpus,0,0.0370863,"to scale up to web contexts to fulfill their purpose. This includes search engines that oppose pro and con arguments on controversial issues (Wachsmuth et al., 2017b), technologies that debate humans (Toledo et al., 2019), and more. To obtain web arguments, many works have relied on crawled debate portals and similar web platforms, often in a distant-supervision manner where argumentative structure and similar annotations are directly derived from available meta-information (Al-Khatib et al., 2016). Corpora have been built in such a way based on several debate portals, including 4forums.com (Walker et al., 2012), idebate.org (Cabrio and Villata, 2012), createdebate.com (Habernal and Gurevych, 2016), debate.org (Durmus and Cardie, 2019), and reddit.com/r/changemyview (Egawa et al., 2020). Naturally, less curation of the acquired web texts comes at the cost of more noise, which in turn calls for a cleansing of the resulting corpus. 1 Both the original and the cleaned args.me corpus are found at: https://webis.de/data.html#args-me-corpus 20 Cleansing processes are described in several publications on argument corpora, mostly only referring to the acquired annotations though (Habernal and Gurevych, 2016;"
2020.argmining-1.9,L16-1704,0,0.313714,"ng argument sources and how do different sources compare to each other in this regard? 2. How much do certain groups of users contribute to the overall social bias of a source? 3. What kinds of linguistic utterances contribute towards certain types of social bias? Applications such as those outlined above usually rely on web arguments for scaling reasons. To study the questions, we hence resort to five English debate portals that give access to arguments on versatile topics: 4forums.com, convinceme.net, createdebate.com, debate.org, and ChangeMyView. All five have been deployed in CA corpora (Abbott et al., 2016; Durmus and Cardie, 2019; Al Khatib et al., 2020). First, we analyze the general presence of social bias in each of the five debate portals. To this end, we train three custom word embedding models, one for each available corpus. Next, we evaluate the models for social bias using a widely used bias metric, called W EAT (Caliskan et al., 2017), and compare the results. We then inspect the debate.org portal more closely with regard to specific social groups. In particular, we group the texts based on the provided user information and apply the same evaluation. Lastly, to gain a better understan"
2020.argmining-1.9,2020.acl-main.632,0,0.467189,"compare to each other in this regard? 2. How much do certain groups of users contribute to the overall social bias of a source? 3. What kinds of linguistic utterances contribute towards certain types of social bias? Applications such as those outlined above usually rely on web arguments for scaling reasons. To study the questions, we hence resort to five English debate portals that give access to arguments on versatile topics: 4forums.com, convinceme.net, createdebate.com, debate.org, and ChangeMyView. All five have been deployed in CA corpora (Abbott et al., 2016; Durmus and Cardie, 2019; Al Khatib et al., 2020). First, we analyze the general presence of social bias in each of the five debate portals. To this end, we train three custom word embedding models, one for each available corpus. Next, we evaluate the models for social bias using a widely used bias metric, called W EAT (Caliskan et al., 2017), and compare the results. We then inspect the debate.org portal more closely with regard to specific social groups. In particular, we group the texts based on the provided user information and apply the same evaluation. Lastly, to gain a better understanding of what makes some texts more biased than oth"
2020.argmining-1.9,Q17-1010,0,0.0119801,"e African-American social group, but simply state that the association between the social group and the terms generally holds. 83 6.3 Limitations One limitation of our methodology is that it relies on the chosen embedding model to accurately model distances and associations between words. This makes it less applicable to smaller datasets, let alone single texts. Additionally, the influence of multiple factors on W EAT results is unclear. For example, choosing an alternative algorithm to generate an embedding model may yield different results, e.g., word2vec (Mikolov et al., 2013) or FastText (Bojanowski et al., 2017). Future work should explore more sophisticated methods to analyze social bias that do not depend on embedding models, as they are a point of uncertainty that might never be fully explainable due to the nature of generating the models. Further, the results presented in this work are limited to the accuracy of W EAT. This dependence is problematic for three main reasons: First, it assumes that the calculation done by the test accurately models the associations between the lexicons. While Caliskan et al. (2017) show that they are able to reproduce results from previous psychology studies with hu"
2020.argmining-1.9,D19-2004,0,0.14185,"social groups in society (Sweeney and Najafian, 2019; Papakyriakopoulos et al., 2020). A social group might be described by physical attributes of its members, such as sex and skin color, but also by more abstract categories, such as culture, heritage, gender identity, and religion. A typical, probably in itself biased, example of social bias is the old man’s belief in classic gender stereotypes. In most cases, social bias is deemed negative and undesirable. Recent research shows that bias towards social groups is also present in Machine Learning and Natural Language Processing (NLP) models (Chang et al., 2019), manifesting in the encoded states of a language model (Brown et al., 2020) or simply causing worse performance for underrepresented classes (Sun et al., 2019). Such bias has been studied for different NLP contexts, including coreference resolution (Rudinger et al., 2018), machine translation (Vanmassenhove et al., 2018), and the training of word embedding models (Bolukbasi et al., 2016). In contrast, Computational Argumentation (CA) has, to our knowledge, not seen any research in this direction so far. Given that major envisioned applications of CA include the enhancement of human debating ("
2020.argmining-1.9,W18-6509,1,0.666251,"ears, research on different types of bias in natural language has received a considerable amount of attention. Media bias is one prominent example (Fan et al., 2019), particular the political bias of news articles (Chen et al., 2020). In various sub-fields of NLP, studies on media bias are concerned with analyzing techniques utilized by media outlets when reporting news. These include the framing of an event by phrasing the report with positive or negative terms, and selective reporting by including or omitting facts depending on the tone and (political) stance a media outlet wants to convey (Chen et al., 2018; Hamborg, 2020; Lim et al., 2020). We do not target media bias here but social bias. Social bias can emerge from pre-existing stereotypes towards any social group (Sweeney and Najafian, 2019), often leading to prejudices and discrimination. Stereotypes are understood as “beliefs about the characteristics of group members” (Fiske, 2004). They may be so powerful that they do not only limit the freedom of individuals, but also cause hate, exclusion, and — in the worst case — extermination (Fiske, 1993). Even if stereotypes, and with that social biases, are individually controllable, they persist"
2020.argmining-1.9,2020.nlpcss-1.16,1,0.758372,"is not only reflected in the W EAT results, but also in unbalanced occurrences of identity words for certain social groups. More generally, we observe that the use of names as identity terms for social groups has unpredictable effects. With those insights, we contribute an initial understanding of social bias in sources of dialectical argumentative texts. 2 Related Work In recent years, research on different types of bias in natural language has received a considerable amount of attention. Media bias is one prominent example (Fan et al., 2019), particular the political bias of news articles (Chen et al., 2020). In various sub-fields of NLP, studies on media bias are concerned with analyzing techniques utilized by media outlets when reporting news. These include the framing of an event by phrasing the report with positive or negative terms, and selective reporting by including or omitting facts depending on the tone and (political) stance a media outlet wants to convey (Chen et al., 2018; Hamborg, 2020; Lim et al., 2020). We do not target media bias here but social bias. Social bias can emerge from pre-existing stereotypes towards any social group (Sweeney and Najafian, 2019), often leading to preju"
2020.argmining-1.9,P19-1057,0,0.334668,"nd how do different sources compare to each other in this regard? 2. How much do certain groups of users contribute to the overall social bias of a source? 3. What kinds of linguistic utterances contribute towards certain types of social bias? Applications such as those outlined above usually rely on web arguments for scaling reasons. To study the questions, we hence resort to five English debate portals that give access to arguments on versatile topics: 4forums.com, convinceme.net, createdebate.com, debate.org, and ChangeMyView. All five have been deployed in CA corpora (Abbott et al., 2016; Durmus and Cardie, 2019; Al Khatib et al., 2020). First, we analyze the general presence of social bias in each of the five debate portals. To this end, we train three custom word embedding models, one for each available corpus. Next, we evaluate the models for social bias using a widely used bias metric, called W EAT (Caliskan et al., 2017), and compare the results. We then inspect the debate.org portal more closely with regard to specific social groups. In particular, we group the texts based on the provided user information and apply the same evaluation. Lastly, to gain a better understanding of what makes some t"
2020.argmining-1.9,P19-1166,0,0.0226275,"when generating analogies for a given set of words. This suggests that the distance between word vectors can act as a proxy to identify biases held by a model. Building on that notion, methods to automatically quantify the bias in a pre-trained word embedding model were presented. Caliskan et al. (2017) introduce a metric named the Word Embedding Association Test (W EAT) that adapts the idea of the Implicit Association Test (Greenwald et al., 1998). Other methods include the Mean Average Cosine Similarity (MAC) test (Manzini et al., 2019), the Relational Inner Product Association (RIPA) test (Ethayarajh et al., 2019), the Embedding Coherence Test (ECT), and the Embedding Quality Test (EQT) (Dev and Phillips, 2019). For a more detailed literature review on detecting and mitigating biases in word embedding models, see Sun et al. (2019). Our approach builds on the W EAT metric to evaluate social biases in embedding models generated from debate portal texts. Probably closest to our work is the study of Rios et al. (2020). Building on a method developed by Garg et al. (2018), the authors analyzed scientific abstracts of biomedical studies published in a time span of 60 years to quantify gender bias in the fiel"
2020.argmining-1.9,D19-1664,0,0.0614615,"Missing"
2020.argmining-1.9,2020.acl-srw.12,0,0.0350163,"ifferent types of bias in natural language has received a considerable amount of attention. Media bias is one prominent example (Fan et al., 2019), particular the political bias of news articles (Chen et al., 2020). In various sub-fields of NLP, studies on media bias are concerned with analyzing techniques utilized by media outlets when reporting news. These include the framing of an event by phrasing the report with positive or negative terms, and selective reporting by including or omitting facts depending on the tone and (political) stance a media outlet wants to convey (Chen et al., 2018; Hamborg, 2020; Lim et al., 2020). We do not target media bias here but social bias. Social bias can emerge from pre-existing stereotypes towards any social group (Sweeney and Najafian, 2019), often leading to prejudices and discrimination. Stereotypes are understood as “beliefs about the characteristics of group members” (Fiske, 2004). They may be so powerful that they do not only limit the freedom of individuals, but also cause hate, exclusion, and — in the worst case — extermination (Fiske, 1993). Even if stereotypes, and with that social biases, are individually controllable, they persist to this day; p"
2020.argmining-1.9,2020.lrec-1.184,0,0.0413633,"of bias in natural language has received a considerable amount of attention. Media bias is one prominent example (Fan et al., 2019), particular the political bias of news articles (Chen et al., 2020). In various sub-fields of NLP, studies on media bias are concerned with analyzing techniques utilized by media outlets when reporting news. These include the framing of an event by phrasing the report with positive or negative terms, and selective reporting by including or omitting facts depending on the tone and (political) stance a media outlet wants to convey (Chen et al., 2018; Hamborg, 2020; Lim et al., 2020). We do not target media bias here but social bias. Social bias can emerge from pre-existing stereotypes towards any social group (Sweeney and Najafian, 2019), often leading to prejudices and discrimination. Stereotypes are understood as “beliefs about the characteristics of group members” (Fiske, 2004). They may be so powerful that they do not only limit the freedom of individuals, but also cause hate, exclusion, and — in the worst case — extermination (Fiske, 1993). Even if stereotypes, and with that social biases, are individually controllable, they persist to this day; prominent examples o"
2020.argmining-1.9,N19-1062,0,0.0284231,"that pre-trained models contain gender bias. They found that it is revealed when generating analogies for a given set of words. This suggests that the distance between word vectors can act as a proxy to identify biases held by a model. Building on that notion, methods to automatically quantify the bias in a pre-trained word embedding model were presented. Caliskan et al. (2017) introduce a metric named the Word Embedding Association Test (W EAT) that adapts the idea of the Implicit Association Test (Greenwald et al., 1998). Other methods include the Mean Average Cosine Similarity (MAC) test (Manzini et al., 2019), the Relational Inner Product Association (RIPA) test (Ethayarajh et al., 2019), the Embedding Coherence Test (ECT), and the Embedding Quality Test (EQT) (Dev and Phillips, 2019). For a more detailed literature review on detecting and mitigating biases in word embedding models, see Sun et al. (2019). Our approach builds on the W EAT metric to evaluate social biases in embedding models generated from debate portal texts. Probably closest to our work is the study of Rios et al. (2020). Building on a method developed by Garg et al. (2018), the authors analyzed scientific abstracts of biomedical"
2020.argmining-1.9,D14-1162,0,0.0833907,"cosine distance between the four lists in a given model. The score represents the effect size of the difference in distances, which is formulated as follows: meanx∈X s(x, A, B) − meany∈Y s(y, A, B) std_devw∈X∪Y s(w, A, B) where s(x, A, B) is the difference in cosine distances for each word in A to x and each word in B to x. The effect size then acts as a proxy to quantify bias. Portal-specific Models To apply the W EAT metric to the texts of debate portals, we first extract all posts from the corpora and train one separate custom word embedding model on each corpus using the GloVe algorithm (Pennington et al., 2014). Given the custom models, we then evaluate social bias in them, focusing on three of the most common bias types: towards ethnicity, gender, and age (Fiske, 2004). These types are roughly represented by seven of the originally proposed W EAT tests, found in Table 1. As a notion of stability of the results, we additionally create five embedding models that are trained on random splits of the evaluated corpora and calculate the standard deviation of their W EAT scores. Baseline Models To be able to assess the W EAT results obtained for the trained custom embedding models, we also evaluate two pr"
2020.argmining-1.9,2020.bionlp-1.1,0,0.0270042,"ociation Test (Greenwald et al., 1998). Other methods include the Mean Average Cosine Similarity (MAC) test (Manzini et al., 2019), the Relational Inner Product Association (RIPA) test (Ethayarajh et al., 2019), the Embedding Coherence Test (ECT), and the Embedding Quality Test (EQT) (Dev and Phillips, 2019). For a more detailed literature review on detecting and mitigating biases in word embedding models, see Sun et al. (2019). Our approach builds on the W EAT metric to evaluate social biases in embedding models generated from debate portal texts. Probably closest to our work is the study of Rios et al. (2020). Building on a method developed by Garg et al. (2018), the authors analyzed scientific abstracts of biomedical studies published in a time span of 60 years to quantify gender bias in the field and to track changes over time. For this purpose, they generated separate word embedding models for each decade in their data and evaluated them using the W EAT metric. Using the RIPA test in addition, the authors identified the “most biased words” (Rios et al., 2020) of each model. While we will apply a similar method to detect social bias in textual data, our study differs in two main regards: First,"
2020.argmining-1.9,N18-2002,0,0.0295194,"Missing"
2020.argmining-1.9,2020.acl-main.486,0,0.0411299,"es and discrimination. Stereotypes are understood as “beliefs about the characteristics of group members” (Fiske, 2004). They may be so powerful that they do not only limit the freedom of individuals, but also cause hate, exclusion, and — in the worst case — extermination (Fiske, 1993). Even if stereotypes, and with that social biases, are individually controllable, they persist to this day; prominent examples of social groups that have historically been subject to bias are ethnicities, genders, and age groups (Fiske, 1998). A major factor in carrying and reinforcing those biases is language (Sap et al., 2020). In spoken and written argumentation and debates, biased language can be present if, for example, one sides argues in self-interest (Zenker, 2011) to favor a certain social group or uses unbalanced arguments (Kienpointner and Kindt, 1997). As CA methods are receiving more and more attention (Stede and Schneider, 2018), it is important to understand how existing social biases influence them. One possible source is the human-generated data on which automated systems are trained and evaluated (Chang et al., 2019). In CA, one of the main sources of data are online debate portals, both for researc"
2020.argmining-1.9,W16-2813,0,0.0238355,"the portal, and linguistic features of their arguments. The authors find that information on a user is more informative in predicting the success compared to linguistic features. Al Khatib et al. (2020) retrieved all debates and posts from Reddit’s discussion forum ChangeMyView to analyze the characteristics of debaters there. With this information, they were able to enhance existing approaches to predict the persuasiveness of an argument and a debater’s resistance to be persuaded. 77 While the evaluation of debates notably misses work on social biases, other forms of bias have been studied. Stab and Gurevych (2016), for example, attempted to build a classifier that predicts the presence or absence of myside bias in monological texts. In contrast, this work offers an initial evaluation of social bias in dialogical argumentation by analyzing the posts of debate portals. More generally, many approaches have been proposed to identify different types of social bias in word embedding models. In one of the first studies, Bolukbasi et al. (2016) showed that pre-trained models contain gender bias. They found that it is revealed when generating analogies for a given set of words. This suggests that the distance b"
2020.argmining-1.9,P19-1159,0,0.0977266,"uch as sex and skin color, but also by more abstract categories, such as culture, heritage, gender identity, and religion. A typical, probably in itself biased, example of social bias is the old man’s belief in classic gender stereotypes. In most cases, social bias is deemed negative and undesirable. Recent research shows that bias towards social groups is also present in Machine Learning and Natural Language Processing (NLP) models (Chang et al., 2019), manifesting in the encoded states of a language model (Brown et al., 2020) or simply causing worse performance for underrepresented classes (Sun et al., 2019). Such bias has been studied for different NLP contexts, including coreference resolution (Rudinger et al., 2018), machine translation (Vanmassenhove et al., 2018), and the training of word embedding models (Bolukbasi et al., 2016). In contrast, Computational Argumentation (CA) has, to our knowledge, not seen any research in this direction so far. Given that major envisioned applications of CA include the enhancement of human debating (Lawrence et al., 2017) and the support of self-determined opinion formation (Wachsmuth et al., 2017), we argue that studying social bias is particularly critica"
2020.argmining-1.9,P19-1162,0,0.27944,"ra and systematically evaluate their bias using W EAT, an existing metric to measure bias in word embeddings. In a word co-occurrence analysis, we then investigate causes of bias. The results suggest that all tested debate corpora contain unbalanced and biased data, mostly in favor of male people with European-American names. Our empirical insights contribute towards an understanding of bias in argumentative data sources. 1 Introduction Social bias can be understood as implicit or explicit prejudices against, as well as unequal treatment or discrimination of, certain social groups in society (Sweeney and Najafian, 2019; Papakyriakopoulos et al., 2020). A social group might be described by physical attributes of its members, such as sex and skin color, but also by more abstract categories, such as culture, heritage, gender identity, and religion. A typical, probably in itself biased, example of social bias is the old man’s belief in classic gender stereotypes. In most cases, social bias is deemed negative and undesirable. Recent research shows that bias towards social groups is also present in Machine Learning and Natural Language Processing (NLP) models (Chang et al., 2019), manifesting in the encoded state"
2020.argmining-1.9,D18-1334,0,0.0159543,"biased, example of social bias is the old man’s belief in classic gender stereotypes. In most cases, social bias is deemed negative and undesirable. Recent research shows that bias towards social groups is also present in Machine Learning and Natural Language Processing (NLP) models (Chang et al., 2019), manifesting in the encoded states of a language model (Brown et al., 2020) or simply causing worse performance for underrepresented classes (Sun et al., 2019). Such bias has been studied for different NLP contexts, including coreference resolution (Rudinger et al., 2018), machine translation (Vanmassenhove et al., 2018), and the training of word embedding models (Bolukbasi et al., 2016). In contrast, Computational Argumentation (CA) has, to our knowledge, not seen any research in this direction so far. Given that major envisioned applications of CA include the enhancement of human debating (Lawrence et al., 2017) and the support of self-determined opinion formation (Wachsmuth et al., 2017), we argue that studying social bias is particularly critical for CA. In general, social bias may affect diverse stages of CA: In argument acquisition, for example, researchers may introduce social bias unintentionally, for"
2020.argmining-1.9,W17-5106,1,0.836049,"or simply causing worse performance for underrepresented classes (Sun et al., 2019). Such bias has been studied for different NLP contexts, including coreference resolution (Rudinger et al., 2018), machine translation (Vanmassenhove et al., 2018), and the training of word embedding models (Bolukbasi et al., 2016). In contrast, Computational Argumentation (CA) has, to our knowledge, not seen any research in this direction so far. Given that major envisioned applications of CA include the enhancement of human debating (Lawrence et al., 2017) and the support of self-determined opinion formation (Wachsmuth et al., 2017), we argue that studying social bias is particularly critical for CA. In general, social bias may affect diverse stages of CA: In argument acquisition, for example, researchers may introduce social bias unintentionally, for instance, by collecting arguments from web sources that are only popular in a certain part of the world. This is known as sample bias (Chang et al., 2019). In argument quality assessment, a machine learning model may develop a prejudicial bias and judge arguments made by a certain social group better, for instance, because it considers features inadequate for the task, such"
2020.coling-main.551,D11-1024,0,0.0610311,"ation metrics. In particular, various metrics for k exist, including harmonic mean (Griffiths and Steyvers, 2004), pairwise cosine distance (Cao et al., 2009), and KL divergence (Arun et al., 2010). Due to the inaccuracy of these metrics, Wallach et al. (2009b) proposed the chib-style estimator, which maximizes the probability of held-out documents. However, this estimator does not assess the interpretability of topics. As a solution, Chang et al. (2009) measured the topic coherence of models based on human judgments, Newman et al. (2010) developed a technique to estimate human judgments, and Mimno et al. (2011), Stevens et al. (2012), and R¨oder et al. (2015) studied the accuracy of several coherence metrics. Since we aim for topics that can be interpreted well in terms of the workers’ problems, topic coherence metrics make most sense in the given setting. Thus, we evaluated our models based on the following metrics: 1. The Mimno metric, which uses top-word co-occurrence statistics and is computed in three main steps: (a) identification of distinct classes of low-quality topics, /b) identification of specific semantic problems in topic models, and (c) optimization of the topic coherence (Mimno et al"
2020.coling-main.551,N10-1012,0,0.064413,"14 generated models, we compared the suitability of several candidate evaluation metrics. In particular, various metrics for k exist, including harmonic mean (Griffiths and Steyvers, 2004), pairwise cosine distance (Cao et al., 2009), and KL divergence (Arun et al., 2010). Due to the inaccuracy of these metrics, Wallach et al. (2009b) proposed the chib-style estimator, which maximizes the probability of held-out documents. However, this estimator does not assess the interpretability of topics. As a solution, Chang et al. (2009) measured the topic coherence of models based on human judgments, Newman et al. (2010) developed a technique to estimate human judgments, and Mimno et al. (2011), Stevens et al. (2012), and R¨oder et al. (2015) studied the accuracy of several coherence metrics. Since we aim for topics that can be interpreted well in terms of the workers’ problems, topic coherence metrics make most sense in the given setting. Thus, we evaluated our models based on the following metrics: 1. The Mimno metric, which uses top-word co-occurrence statistics and is computed in three main steps: (a) identification of distinct classes of low-quality topics, /b) identification of specific semantic problem"
2020.coling-main.551,D12-1087,0,0.0177695,"ticular, various metrics for k exist, including harmonic mean (Griffiths and Steyvers, 2004), pairwise cosine distance (Cao et al., 2009), and KL divergence (Arun et al., 2010). Due to the inaccuracy of these metrics, Wallach et al. (2009b) proposed the chib-style estimator, which maximizes the probability of held-out documents. However, this estimator does not assess the interpretability of topics. As a solution, Chang et al. (2009) measured the topic coherence of models based on human judgments, Newman et al. (2010) developed a technique to estimate human judgments, and Mimno et al. (2011), Stevens et al. (2012), and R¨oder et al. (2015) studied the accuracy of several coherence metrics. Since we aim for topics that can be interpreted well in terms of the workers’ problems, topic coherence metrics make most sense in the given setting. Thus, we evaluated our models based on the following metrics: 1. The Mimno metric, which uses top-word co-occurrence statistics and is computed in three main steps: (a) identification of distinct classes of low-quality topics, /b) identification of specific semantic problems in topic models, and (c) optimization of the topic coherence (Mimno et al., 2011). 2. Normalized"
2020.coling-main.592,K18-1044,1,0.868326,"Missing"
2020.coling-main.592,2020.acl-main.287,1,0.744993,"Missing"
2020.coling-main.592,P19-1093,0,0.078427,"Altogether, an intrinsic computational assessment of argument quality seems useful but not enough alone. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 6739 Proceedings of the 28th International Conference on Computational Linguistics, pages 6739–6745 Barcelona, Spain (Online), December 8-13, 2020 2 Related Work Soon after the rise of argument mining, argument quality assessment has come up as a task (Stede and Schneider, 2018), due to its importance for applications such as Project Debater (Gleize et al., 2019). It rests on extensive theoretical discussions about what good arguments (Johnson and Blair, 2006) and bad arguments are (Walton, 2006), and how to argue reasonably (van Eemeren and Grootendorst, 2004). Several corpora and approaches were proposed for specific argument quality dimensions, first related to essay scoring (Persing and Ng, 2015), some of which modeling arguments explicitly (Wachsmuth et al., 2016). Later approaches targeted arguments from debate portals (Wei et al., 2016), student essays (Stab and Gurevych, 2017), mixed web texts (Wachsmuth et al., 2017c), and news editorials (Ya"
2020.coling-main.592,D16-1129,0,0.0872849,"o make advancements for the common good you can change the world forever. Allot of people have succeded in doing so. Our founding fathers, Thomas Edison, George Washington, Martin Luther King jr, and many more. These people made huge advances for the common good and they are honored for it.” The argument is well-organized (Persing et al., 2010), its premises are certainly largely acceptable (Yang et al., 2019) and relevant to the topic (Wachsmuth et al., 2017c). Whether they suffice to draw the conclusion (Stab and Gurevych, 2017) is another question, let alone how convincing the argument is (Habernal and Gurevych, 2016b). Some dimensions may be reflected in linguistic features of an argument’s text. Others depend on context, require topic or background knowledge, or are inherently subjective. In this paper, we benchmark what quality dimensions of an argument can be assessed intrinsically, i.e., when analyzing the text of an argument only. Given the corpus of Wachsmuth et al. (2017b) with 304 English debate portal arguments on 16 topics scored for 15 logical, rhetorical, and dialectical quality dimensions by three experts, we carry out systematic leave-one-topic-out cross-validation experiments. In particula"
2020.coling-main.592,P16-1150,0,0.279812,"o make advancements for the common good you can change the world forever. Allot of people have succeded in doing so. Our founding fathers, Thomas Edison, George Washington, Martin Luther King jr, and many more. These people made huge advances for the common good and they are honored for it.” The argument is well-organized (Persing et al., 2010), its premises are certainly largely acceptable (Yang et al., 2019) and relevant to the topic (Wachsmuth et al., 2017c). Whether they suffice to draw the conclusion (Stab and Gurevych, 2017) is another question, let alone how convincing the argument is (Habernal and Gurevych, 2016b). Some dimensions may be reflected in linguistic features of an argument’s text. Others depend on context, require topic or background knowledge, or are inherently subjective. In this paper, we benchmark what quality dimensions of an argument can be assessed intrinsically, i.e., when analyzing the text of an argument only. Given the corpus of Wachsmuth et al. (2017b) with 304 English debate portal arguments on 16 topics scored for 15 logical, rhetorical, and dialectical quality dimensions by three experts, we carry out systematic leave-one-topic-out cross-validation experiments. In particula"
2020.coling-main.592,E17-1070,0,0.143601,"us. However, we do not know any previous assessment approach developed on the corpus, possibly due to its limited size (see Section 3). We focus on features intrinsic to an argument’s text. This complements the study of Potash et al. (2017) who employed external knowledge to assess convincingness. Like them, we find that longer arguments tend to be judged better. Toledo et al. (2019) limit arguments to at most 36 words, avoiding length bias but also preventing deeper reasoning. Quality in their corpus reflects which argument is in doubt preferred. Ultimately, such judgments remain subjective (Lukin et al., 2017). To alleviate this, El Baff et al. (2018) encode the reader’s ideology and personality, but such information is often not given in practice. 3 Data The corpus of Wachsmuth et al. (2017b) is a subset of 320 debate portal posts from the dataset of Habernal and Gurevych (2016a), 20 each for 16 controversial topics. Three human experts (all authors of the paper) scored all posts that they saw as arguments for the following 15 logical, rhetorical, and dialectical quality dimensions on a scale from 1 (low) to 3 (high). In line with the experiments of Wachsmuth et al. (2017a), we use only those 304"
2020.coling-main.592,L18-1008,0,0.0352421,"Missing"
2020.coling-main.592,P15-1053,0,0.101977,"s 6739–6745 Barcelona, Spain (Online), December 8-13, 2020 2 Related Work Soon after the rise of argument mining, argument quality assessment has come up as a task (Stede and Schneider, 2018), due to its importance for applications such as Project Debater (Gleize et al., 2019). It rests on extensive theoretical discussions about what good arguments (Johnson and Blair, 2006) and bad arguments are (Walton, 2006), and how to argue reasonably (van Eemeren and Grootendorst, 2004). Several corpora and approaches were proposed for specific argument quality dimensions, first related to essay scoring (Persing and Ng, 2015), some of which modeling arguments explicitly (Wachsmuth et al., 2016). Later approaches targeted arguments from debate portals (Wei et al., 2016), student essays (Stab and Gurevych, 2017), mixed web texts (Wachsmuth et al., 2017c), and news editorials (Yang et al., 2019; El Baff et al., 2020). We use the corpus of Wachsmuth et al. (2017b), as it is the only one annotated for diverse dimensions and is claimed to reflect argument quality comprehensively. In follow-up work, Wachsmuth et al. (2017a) found correlations with the convincingness reasons of Habernal and Gurevych (2016a), and Potthast"
2020.coling-main.592,D10-1023,0,0.141753,"ood depends on the setting, though (van Eemeren and Grootendorst, 2004; Johnson and Blair, 2006). Several dimensions may be assessed computationally, as we exemplify for an argument in favor of advancing the common good, taken from Wachsmuth et al. (2017a): “While striving to make advancements for the common good you can change the world forever. Allot of people have succeded in doing so. Our founding fathers, Thomas Edison, George Washington, Martin Luther King jr, and many more. These people made huge advances for the common good and they are honored for it.” The argument is well-organized (Persing et al., 2010), its premises are certainly largely acceptable (Yang et al., 2019) and relevant to the topic (Wachsmuth et al., 2017c). Whether they suffice to draw the conclusion (Stab and Gurevych, 2017) is another question, let alone how convincing the argument is (Habernal and Gurevych, 2016b). Some dimensions may be reflected in linguistic features of an argument’s text. Others depend on context, require topic or background knowledge, or are inherently subjective. In this paper, we benchmark what quality dimensions of an argument can be assessed intrinsically, i.e., when analyzing the text of an argumen"
2020.coling-main.592,I17-1035,0,0.725075,"ore regression on various text features; from content and distributional semantics, to style, structure, and length, to text quality, evidence, and subjectivity. For 11 dimensions, we observe moderate but significant prediction gains of the features over a mean baseline. Following intuition, rhetorical quality related to credibility and emotions seem hardest to assess. Features capturing subjectivity (e.g., sentiment and pronoun usage) turn out particularly effective. Even better performs the length feature, though, revealing bias in the corpus and matching previous findings on other corpora (Potash et al., 2017). Follow-up experiments indicate that the experts strongly differ in their assessability, and that some beat our features only slightly in assessing quality. Altogether, an intrinsic computational assessment of argument quality seems useful but not enough alone. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 6739 Proceedings of the 28th International Conference on Computational Linguistics, pages 6739–6745 Barcelona, Spain (Online), December 8-13, 2020 2 Related Work Soon after the rise of arg"
2020.coling-main.592,E17-1092,0,0.449466,"vor of advancing the common good, taken from Wachsmuth et al. (2017a): “While striving to make advancements for the common good you can change the world forever. Allot of people have succeded in doing so. Our founding fathers, Thomas Edison, George Washington, Martin Luther King jr, and many more. These people made huge advances for the common good and they are honored for it.” The argument is well-organized (Persing et al., 2010), its premises are certainly largely acceptable (Yang et al., 2019) and relevant to the topic (Wachsmuth et al., 2017c). Whether they suffice to draw the conclusion (Stab and Gurevych, 2017) is another question, let alone how convincing the argument is (Habernal and Gurevych, 2016b). Some dimensions may be reflected in linguistic features of an argument’s text. Others depend on context, require topic or background knowledge, or are inherently subjective. In this paper, we benchmark what quality dimensions of an argument can be assessed intrinsically, i.e., when analyzing the text of an argument only. Given the corpus of Wachsmuth et al. (2017b) with 304 English debate portal arguments on 16 topics scored for 15 logical, rhetorical, and dialectical quality dimensions by three expe"
2020.coling-main.592,D19-1564,0,0.156341,"(2017a) found correlations with the convincingness reasons of Habernal and Gurevych (2016a), and Potthast et al. (2019) as well as Gretz et al. (2020) have evaluated their annotations against the quality annotation scheme of the corpus. However, we do not know any previous assessment approach developed on the corpus, possibly due to its limited size (see Section 3). We focus on features intrinsic to an argument’s text. This complements the study of Potash et al. (2017) who employed external knowledge to assess convincingness. Like them, we find that longer arguments tend to be judged better. Toledo et al. (2019) limit arguments to at most 36 words, avoiding length bias but also preventing deeper reasoning. Quality in their corpus reflects which argument is in doubt preferred. Ultimately, such judgments remain subjective (Lukin et al., 2017). To alleviate this, El Baff et al. (2018) encode the reader’s ideology and personality, but such information is often not given in practice. 3 Data The corpus of Wachsmuth et al. (2017b) is a subset of 320 debate portal posts from the dataset of Habernal and Gurevych (2016a), 20 each for 16 controversial topics. Three human experts (all authors of the paper) score"
2020.coling-main.592,C16-1158,1,0.94273,"Work Soon after the rise of argument mining, argument quality assessment has come up as a task (Stede and Schneider, 2018), due to its importance for applications such as Project Debater (Gleize et al., 2019). It rests on extensive theoretical discussions about what good arguments (Johnson and Blair, 2006) and bad arguments are (Walton, 2006), and how to argue reasonably (van Eemeren and Grootendorst, 2004). Several corpora and approaches were proposed for specific argument quality dimensions, first related to essay scoring (Persing and Ng, 2015), some of which modeling arguments explicitly (Wachsmuth et al., 2016). Later approaches targeted arguments from debate portals (Wei et al., 2016), student essays (Stab and Gurevych, 2017), mixed web texts (Wachsmuth et al., 2017c), and news editorials (Yang et al., 2019; El Baff et al., 2020). We use the corpus of Wachsmuth et al. (2017b), as it is the only one annotated for diverse dimensions and is claimed to reflect argument quality comprehensively. In follow-up work, Wachsmuth et al. (2017a) found correlations with the convincingness reasons of Habernal and Gurevych (2016a), and Potthast et al. (2019) as well as Gretz et al. (2020) have evaluated their anno"
2020.coling-main.592,P17-2039,1,0.825778,"seems hardest to assess, and subjectivity features turn out strong, although length bias in the corpus impedes full validity. We also find that human assessors differ more clearly to each other than to our approach. 1 Introduction Good arguments help to persuade people, to compromise, or to at least understand each other better. What quality dimension is meant by good depends on the setting, though (van Eemeren and Grootendorst, 2004; Johnson and Blair, 2006). Several dimensions may be assessed computationally, as we exemplify for an argument in favor of advancing the common good, taken from Wachsmuth et al. (2017a): “While striving to make advancements for the common good you can change the world forever. Allot of people have succeded in doing so. Our founding fathers, Thomas Edison, George Washington, Martin Luther King jr, and many more. These people made huge advances for the common good and they are honored for it.” The argument is well-organized (Persing et al., 2010), its premises are certainly largely acceptable (Yang et al., 2019) and relevant to the topic (Wachsmuth et al., 2017c). Whether they suffice to draw the conclusion (Stab and Gurevych, 2017) is another question, let alone how convinc"
2020.coling-main.592,E17-1017,1,0.886251,"seems hardest to assess, and subjectivity features turn out strong, although length bias in the corpus impedes full validity. We also find that human assessors differ more clearly to each other than to our approach. 1 Introduction Good arguments help to persuade people, to compromise, or to at least understand each other better. What quality dimension is meant by good depends on the setting, though (van Eemeren and Grootendorst, 2004; Johnson and Blair, 2006). Several dimensions may be assessed computationally, as we exemplify for an argument in favor of advancing the common good, taken from Wachsmuth et al. (2017a): “While striving to make advancements for the common good you can change the world forever. Allot of people have succeded in doing so. Our founding fathers, Thomas Edison, George Washington, Martin Luther King jr, and many more. These people made huge advances for the common good and they are honored for it.” The argument is well-organized (Persing et al., 2010), its premises are certainly largely acceptable (Yang et al., 2019) and relevant to the topic (Wachsmuth et al., 2017c). Whether they suffice to draw the conclusion (Stab and Gurevych, 2017) is another question, let alone how convinc"
2020.coling-main.592,E17-1105,1,0.766702,"seems hardest to assess, and subjectivity features turn out strong, although length bias in the corpus impedes full validity. We also find that human assessors differ more clearly to each other than to our approach. 1 Introduction Good arguments help to persuade people, to compromise, or to at least understand each other better. What quality dimension is meant by good depends on the setting, though (van Eemeren and Grootendorst, 2004; Johnson and Blair, 2006). Several dimensions may be assessed computationally, as we exemplify for an argument in favor of advancing the common good, taken from Wachsmuth et al. (2017a): “While striving to make advancements for the common good you can change the world forever. Allot of people have succeded in doing so. Our founding fathers, Thomas Edison, George Washington, Martin Luther King jr, and many more. These people made huge advances for the common good and they are honored for it.” The argument is well-organized (Persing et al., 2010), its premises are certainly largely acceptable (Yang et al., 2019) and relevant to the topic (Wachsmuth et al., 2017c). Whether they suffice to draw the conclusion (Stab and Gurevych, 2017) is another question, let alone how convinc"
2020.coling-main.592,P16-2032,0,0.0490492,"up as a task (Stede and Schneider, 2018), due to its importance for applications such as Project Debater (Gleize et al., 2019). It rests on extensive theoretical discussions about what good arguments (Johnson and Blair, 2006) and bad arguments are (Walton, 2006), and how to argue reasonably (van Eemeren and Grootendorst, 2004). Several corpora and approaches were proposed for specific argument quality dimensions, first related to essay scoring (Persing and Ng, 2015), some of which modeling arguments explicitly (Wachsmuth et al., 2016). Later approaches targeted arguments from debate portals (Wei et al., 2016), student essays (Stab and Gurevych, 2017), mixed web texts (Wachsmuth et al., 2017c), and news editorials (Yang et al., 2019; El Baff et al., 2020). We use the corpus of Wachsmuth et al. (2017b), as it is the only one annotated for diverse dimensions and is claimed to reflect argument quality comprehensively. In follow-up work, Wachsmuth et al. (2017a) found correlations with the convincingness reasons of Habernal and Gurevych (2016a), and Potthast et al. (2019) as well as Gretz et al. (2020) have evaluated their annotations against the quality annotation scheme of the corpus. However, we do"
2020.coling-main.592,D19-1293,0,0.303546,"; Johnson and Blair, 2006). Several dimensions may be assessed computationally, as we exemplify for an argument in favor of advancing the common good, taken from Wachsmuth et al. (2017a): “While striving to make advancements for the common good you can change the world forever. Allot of people have succeded in doing so. Our founding fathers, Thomas Edison, George Washington, Martin Luther King jr, and many more. These people made huge advances for the common good and they are honored for it.” The argument is well-organized (Persing et al., 2010), its premises are certainly largely acceptable (Yang et al., 2019) and relevant to the topic (Wachsmuth et al., 2017c). Whether they suffice to draw the conclusion (Stab and Gurevych, 2017) is another question, let alone how convincing the argument is (Habernal and Gurevych, 2016b). Some dimensions may be reflected in linguistic features of an argument’s text. Others depend on context, require topic or background knowledge, or are inherently subjective. In this paper, we benchmark what quality dimensions of an argument can be assessed intrinsically, i.e., when analyzing the text of an argument only. Given the corpus of Wachsmuth et al. (2017b) with 304 Engli"
2020.findings-emnlp.383,N19-1216,0,0.0715649,"sentence-level bias based on parse trees. Gangula et al. (2019) made use of headline attention to classify article bias. Li and Goldwasser (2019) encoded social information in their Graph-CNN. While deep learning is believed to capture deeper relations among its inputs, we show that extending a neural network from sentence-level to article-level bias detection does not “just work”. One point of variation in media bias detection is the level of text being analyzed, which varies from tokens (Fan et al., 2019) and sentences (Bhatia and Deepak, 2018) to articles (Kulkarni et al., 2018), sources (Baly et al., 2019), and users (Preo¸tiucPietro et al., 2017). While the effectiveness of machine learning models on different levels helps understanding how media bias becomes manifest at different levels, Lin et al. (2006) are to our knowledge the only to discuss the difference between sentence-level and article-level bias detection. Source-level and user-level bias can be seen as directly emerging from summing up bias in the associated texts. For example, Baly et al. (2019) averaged the feature vectors of articles as the feature vectors of a source. The relation between sentencelevel and article-level bias re"
2020.findings-emnlp.383,W18-6212,0,0.221679,"al. (2014) used RNNs to aggregate the polarity of each word to predict sentence-level bias based on parse trees. Gangula et al. (2019) made use of headline attention to classify article bias. Li and Goldwasser (2019) encoded social information in their Graph-CNN. While deep learning is believed to capture deeper relations among its inputs, we show that extending a neural network from sentence-level to article-level bias detection does not “just work”. One point of variation in media bias detection is the level of text being analyzed, which varies from tokens (Fan et al., 2019) and sentences (Bhatia and Deepak, 2018) to articles (Kulkarni et al., 2018), sources (Baly et al., 2019), and users (Preo¸tiucPietro et al., 2017). While the effectiveness of machine learning models on different levels helps understanding how media bias becomes manifest at different levels, Lin et al. (2006) are to our knowledge the only to discuss the difference between sentence-level and article-level bias detection. Source-level and user-level bias can be seen as directly emerging from summing up bias in the associated texts. For example, Baly et al. (2019) averaged the feature vectors of articles as the feature vectors of a sou"
2020.findings-emnlp.383,W18-6509,1,0.479296,"utperform those without. 1 Henning Wachsmuth Paderborn University Department of Computer Science henningw@upb.de Lysol maker issues warning against injections of disinfectant after Trump comments — The Hill, center-oriented “This notion of injecting or ingesting any type of cleansing product into the body is irresponsible and it’s dangerous,” said Gupta. — NBC News, left-oriented Introduction Media bias is discussed and analyzed in journalism research (Groseclose and Milyo, 2005; DellaVigna and Kaplan, 2007; Iyengar and Hahn, 2009) and NLP research (Gerrish and Blei, 2011; Iyyer et al., 2014; Chen et al., 2018). According to the study of Groseclose and Milyo (2005), bias “has nothing to do with the honesty or accuracy”, but it means “taste or preference”. In fact, journalists may (1) report facts only in favor of one particular political side and thus (2) conclude with their own opinion. As an example, the following sentences from allsides.com reporting on the event “Trump asks if disinfectant, sunlight can treat coronavirus” From an NLP perspective, bias in the example sentences could be detected by capturing sentiment words, such as “falsely” or “irresponsible”. Without the background knowledge of"
2020.findings-emnlp.383,D19-1664,0,0.399279,"c) their sequential order. For each type, we model the bias distribution in a new way through a Gaussian Mixture Model (GMM), in order to then exploit it as features of an SVM (for frequency), Naïve Bayes (for positions), and a firstorder Markov model (for sequential order). The results show strong correlations between the two levels for frequency and position information, whereas sequential order seems less correlated. To study Q1–Q3, we employ the BASIL dataset, which includes manually annotated bias labels at article level as well as lexical and informational bias labels at sentence level (Fan et al., 2019). While the dataset contains only 300 articles, it provides the best basis for understanding the interaction of bias at both levels available so far. For Q3, finally, we propose a new approach applicable in realistic settings. In particular, we retrain the bias detectors from the Q1 experiments on the sentence level and then exploit the GMM as above to predict to article level bias. In our evaluation, the approach significantly outperforms the article-level approaches analyzed for Q1. CounterQ1. How effective are standard classification approaches in article-level bias detection, with and with"
2020.findings-emnlp.383,W19-4809,0,0.0845579,"from the used dataset. It becomes clear that the actual words in the biased sentences are not always indicative to distinguish biased from neutral articles, nor is the count of the biased sentences: Bias assessments on sentence level do not “add up”. In this regard, the position of biased sentences appears to be a better feature. The existing approaches to bias detection are transferred from other, less intricate text classification tasks. They largely model low-level lexical information, either explicitly, e.g. by using bag-ofwords (Gerrish and Blei, 2011), or implicitly via neural networks (Gangula et al., 2019). Such approaches tend to fail at the article level, particularly for articles on events not covered in the training data. The reason is that bias clues are subtle and rare in articles, especially event-independent clues. Altogether, modeling low-level information at the 4290 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4290–4300 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Facing Congress, Clinton Defends Her Actions Before and After Libya Attack Republicans challenge Clinton claims on budget cuts, Benghazi cable 5 Middle Beginning 1"
2020.findings-emnlp.383,C18-1232,0,0.0166763,"remains unstudied so far. The goal of this paper is not to discuss the difference between these levels. Rather, we examine how to aggregate the sentence-level bias to generate second-order features, and then use these features to predict article-level bias. The use of low-level information to generate second-order features was studied in the context of product reviews by modeling patterns in the reviews’ sentiment flow (Wachsmuth et al., 2015), by tuning neural network to capture important sentences (Xu et al., 2016), and by routing in aggregating sentence embeddings into document embedding (Gong et al., 2018). In particular, our usage of low-level information is inspired by Wachsmuth et al. (2015), where we hypothesize that such flows exist in media bias as well. However, we do not limit our approach to entire sequences of sentencelevel information, but we also consider frequency, position, or only two to three continuous sentences. 3 Standard Bias Detection Approaches Standard approaches for bias detection, on both article and sentence level, mainly exploit the lowlevel lexical features to classify the texts as biased or not, neglecting bias-specific features. The two main low-level lexical featu"
2020.findings-emnlp.383,N09-1057,0,0.0140725,"r bias information, i.e., sentence-level bias. (3) We show that second-order bias information is an effective means to build better article-level bias classifiers. 2 Related Work Media bias detection has been studied with computers since the work of Lin et al. (2006). As of then, media bias has been investigated in slight variations under different names, including perspective (Lin et al., 2006), ideology (Iyyer et al., 2014), truthfulness (Rashkin et al., 2017), and hyperpartisanship (Kiesel et al., 2019). To detect bias, early approaches relied on low-level lexical information. For example, Greene and Resnik (2009) used kill verbs and domain-relevant verbs to detect articles being pro Israeli or Palestinian perspectives. Recasens et al. (2013) relied on linguistic cues, such as factoid verbs and implicatives, in order to assess whether a Wikipedia sentence conveys a neutral point of view or not. Besides the NLP community, also researchers in journalism have approached the measurement of media bias. E.g., Gentzkow and Shapiro (2010) used the preferences of phrases at each side (such as “war on terror” for Republican but “war in Iraq” for Democratic). Groseclose and Milyo (2005) used the counts of think-t"
2020.findings-emnlp.383,P14-1105,0,0.490258,"nformation clearly outperform those without. 1 Henning Wachsmuth Paderborn University Department of Computer Science henningw@upb.de Lysol maker issues warning against injections of disinfectant after Trump comments — The Hill, center-oriented “This notion of injecting or ingesting any type of cleansing product into the body is irresponsible and it’s dangerous,” said Gupta. — NBC News, left-oriented Introduction Media bias is discussed and analyzed in journalism research (Groseclose and Milyo, 2005; DellaVigna and Kaplan, 2007; Iyengar and Hahn, 2009) and NLP research (Gerrish and Blei, 2011; Iyyer et al., 2014; Chen et al., 2018). According to the study of Groseclose and Milyo (2005), bias “has nothing to do with the honesty or accuracy”, but it means “taste or preference”. In fact, journalists may (1) report facts only in favor of one particular political side and thus (2) conclude with their own opinion. As an example, the following sentences from allsides.com reporting on the event “Trump asks if disinfectant, sunlight can treat coronavirus” From an NLP perspective, bias in the example sentences could be detected by capturing sentiment words, such as “falsely” or “irresponsible”. Without the bac"
2020.findings-emnlp.383,S19-2145,1,0.885092,"Missing"
2020.findings-emnlp.383,D18-1388,0,0.101855,"polarity of each word to predict sentence-level bias based on parse trees. Gangula et al. (2019) made use of headline attention to classify article bias. Li and Goldwasser (2019) encoded social information in their Graph-CNN. While deep learning is believed to capture deeper relations among its inputs, we show that extending a neural network from sentence-level to article-level bias detection does not “just work”. One point of variation in media bias detection is the level of text being analyzed, which varies from tokens (Fan et al., 2019) and sentences (Bhatia and Deepak, 2018) to articles (Kulkarni et al., 2018), sources (Baly et al., 2019), and users (Preo¸tiucPietro et al., 2017). While the effectiveness of machine learning models on different levels helps understanding how media bias becomes manifest at different levels, Lin et al. (2006) are to our knowledge the only to discuss the difference between sentence-level and article-level bias detection. Source-level and user-level bias can be seen as directly emerging from summing up bias in the associated texts. For example, Baly et al. (2019) averaged the feature vectors of articles as the feature vectors of a source. The relation between sentencele"
2020.findings-emnlp.383,P19-1247,0,0.014373,"have approached the measurement of media bias. E.g., Gentzkow and Shapiro (2010) used the preferences of phrases at each side (such as “war on terror” for Republican but “war in Iraq” for Democratic). Groseclose and Milyo (2005) used the counts of think-tank citations to estimate the bias. With the rise of deep learning, NLP researchers have also used neural-based approaches for bias detection. Iyyer et al. (2014) used RNNs to aggregate the polarity of each word to predict sentence-level bias based on parse trees. Gangula et al. (2019) made use of headline attention to classify article bias. Li and Goldwasser (2019) encoded social information in their Graph-CNN. While deep learning is believed to capture deeper relations among its inputs, we show that extending a neural network from sentence-level to article-level bias detection does not “just work”. One point of variation in media bias detection is the level of text being analyzed, which varies from tokens (Fan et al., 2019) and sentences (Bhatia and Deepak, 2018) to articles (Kulkarni et al., 2018), sources (Baly et al., 2019), and users (Preo¸tiucPietro et al., 2017). While the effectiveness of machine learning models on different levels helps underst"
2020.findings-emnlp.383,W06-2915,0,0.352236,"the fact that the sentencelevel detector creates more deterministic sentence bias features, allowing our approach to learn from them in a more robust way. Altogether, the contribution of this paper is threefold: (1) We provide evidence that standard approaches fail in detecting article-level bias. (2) We develop a new approach utilizing second-order bias information, i.e., sentence-level bias. (3) We show that second-order bias information is an effective means to build better article-level bias classifiers. 2 Related Work Media bias detection has been studied with computers since the work of Lin et al. (2006). As of then, media bias has been investigated in slight variations under different names, including perspective (Lin et al., 2006), ideology (Iyyer et al., 2014), truthfulness (Rashkin et al., 2017), and hyperpartisanship (Kiesel et al., 2019). To detect bias, early approaches relied on low-level lexical information. For example, Greene and Resnik (2009) used kill verbs and domain-relevant verbs to detect articles being pro Israeli or Palestinian perspectives. Recasens et al. (2013) relied on linguistic cues, such as factoid verbs and implicatives, in order to assess whether a Wikipedia sente"
2020.findings-emnlp.383,P17-1068,0,0.0601338,"Missing"
2020.findings-emnlp.383,D17-1317,0,0.0564728,"er is threefold: (1) We provide evidence that standard approaches fail in detecting article-level bias. (2) We develop a new approach utilizing second-order bias information, i.e., sentence-level bias. (3) We show that second-order bias information is an effective means to build better article-level bias classifiers. 2 Related Work Media bias detection has been studied with computers since the work of Lin et al. (2006). As of then, media bias has been investigated in slight variations under different names, including perspective (Lin et al., 2006), ideology (Iyyer et al., 2014), truthfulness (Rashkin et al., 2017), and hyperpartisanship (Kiesel et al., 2019). To detect bias, early approaches relied on low-level lexical information. For example, Greene and Resnik (2009) used kill verbs and domain-relevant verbs to detect articles being pro Israeli or Palestinian perspectives. Recasens et al. (2013) relied on linguistic cues, such as factoid verbs and implicatives, in order to assess whether a Wikipedia sentence conveys a neutral point of view or not. Besides the NLP community, also researchers in journalism have approached the measurement of media bias. E.g., Gentzkow and Shapiro (2010) used the prefere"
2020.findings-emnlp.383,P13-1162,0,0.0248535,"rticle-level bias classifiers. 2 Related Work Media bias detection has been studied with computers since the work of Lin et al. (2006). As of then, media bias has been investigated in slight variations under different names, including perspective (Lin et al., 2006), ideology (Iyyer et al., 2014), truthfulness (Rashkin et al., 2017), and hyperpartisanship (Kiesel et al., 2019). To detect bias, early approaches relied on low-level lexical information. For example, Greene and Resnik (2009) used kill verbs and domain-relevant verbs to detect articles being pro Israeli or Palestinian perspectives. Recasens et al. (2013) relied on linguistic cues, such as factoid verbs and implicatives, in order to assess whether a Wikipedia sentence conveys a neutral point of view or not. Besides the NLP community, also researchers in journalism have approached the measurement of media bias. E.g., Gentzkow and Shapiro (2010) used the preferences of phrases at each side (such as “war on terror” for Republican but “war in Iraq” for Democratic). Groseclose and Milyo (2005) used the counts of think-tank citations to estimate the bias. With the rise of deep learning, NLP researchers have also used neural-based approaches for bias"
2020.findings-emnlp.383,D15-1072,1,0.848763,"All sentences labeled as having lexical or informational bias are highlighted; their position can be read from the numbers next to them. article level is insufficient to detect article-level bias, as we will later stress in experiments. We study article-level bias detection both with and without allowing to learn event-specific information. The latter scenario is more challenging, but it is closer to the real world, because we cannot expect that the information in future articles always relates to past events. Inspired by ideas from modeling local and global polarities in sentiment analysis (Wachsmuth et al., 2015), we hypothesize that using second-order bias information in terms of lexical and informational bias at the sentence level is key to detecting article-level bias. To the best of our knowledge, no bias detection approach so far uses such information. We investigate this hypothesis in light of three research questions: For Q1, we evaluate an n-gram-based SVM and a BERT-based neural network in article-level bias detection. To assess the impact of event-related information, we split the dataset in two ways, once with event overlap in the training set and test set, and once without. As expected, we"
2020.findings-emnlp.383,D16-1172,0,0.0247725,"as the feature vectors of a source. The relation between sentencelevel and article-level bias remains unstudied so far. The goal of this paper is not to discuss the difference between these levels. Rather, we examine how to aggregate the sentence-level bias to generate second-order features, and then use these features to predict article-level bias. The use of low-level information to generate second-order features was studied in the context of product reviews by modeling patterns in the reviews’ sentiment flow (Wachsmuth et al., 2015), by tuning neural network to capture important sentences (Xu et al., 2016), and by routing in aggregating sentence embeddings into document embedding (Gong et al., 2018). In particular, our usage of low-level information is inspired by Wachsmuth et al. (2015), where we hypothesize that such flows exist in media bias as well. However, we do not limit our approach to entire sequences of sentencelevel information, but we also consider frequency, position, or only two to three continuous sentences. 3 Standard Bias Detection Approaches Standard approaches for bias detection, on both article and sentence level, mainly exploit the lowlevel lexical features to classify the"
2020.nlpcss-1.16,D18-1389,0,0.0214508,"ntial media bias patterns. The results show that our model can utilize highlevel semantic features of bias, and it confirms that they are manifested in larger granularity levels, i.e., on the paragraph level and the discourse level. At the word level, we find some LIWC categories to be particularly correlated with political bias and unfairness, such as negative emotion, focus present, and percept. At levels of larger granularity, we observe that the last part of an article usually tends to be most biased and unfairest. 2 Related Work Computational approaches to media bias focus on factuality (Baly et al., 2018), political ideology (Iyyer et al., 2014), and information quality (Rashkin et al., 2017). Bias detection has been done at different granularity levels: single sentences (Bhatia and Deepak, 2018), articles (Kulkarni et al., 2018), and media sources (Baly et al., 2019). Recently, the authors of this paper studied how the two granularity levels “sentence” and “discourse” affect each other. An outcome of this research are insights and means of how sentence-level bias classifiers can be leveraged to better predict article-level bias (Chen et al., 2020). Given these results, the paper in hand digs"
2020.nlpcss-1.16,N19-1216,0,0.0171293,"C categories to be particularly correlated with political bias and unfairness, such as negative emotion, focus present, and percept. At levels of larger granularity, we observe that the last part of an article usually tends to be most biased and unfairest. 2 Related Work Computational approaches to media bias focus on factuality (Baly et al., 2018), political ideology (Iyyer et al., 2014), and information quality (Rashkin et al., 2017). Bias detection has been done at different granularity levels: single sentences (Bhatia and Deepak, 2018), articles (Kulkarni et al., 2018), and media sources (Baly et al., 2019). Recently, the authors of this paper studied how the two granularity levels “sentence” and “discourse” affect each other. An outcome of this research are insights and means of how sentence-level bias classifiers can be leveraged to better predict article-level bias (Chen et al., 2020). Given these results, the paper in hand digs deeper by investigating how bias is actually manifested in a text at different granularity levels. Our approach resembles techniques where the attention mechanism in a model is used to output weights (which indicate feature importance) for each text segment (Bahdanau"
2020.nlpcss-1.16,W18-6212,0,0.0129239,"he paragraph level and the discourse level. At the word level, we find some LIWC categories to be particularly correlated with political bias and unfairness, such as negative emotion, focus present, and percept. At levels of larger granularity, we observe that the last part of an article usually tends to be most biased and unfairest. 2 Related Work Computational approaches to media bias focus on factuality (Baly et al., 2018), political ideology (Iyyer et al., 2014), and information quality (Rashkin et al., 2017). Bias detection has been done at different granularity levels: single sentences (Bhatia and Deepak, 2018), articles (Kulkarni et al., 2018), and media sources (Baly et al., 2019). Recently, the authors of this paper studied how the two granularity levels “sentence” and “discourse” affect each other. An outcome of this research are insights and means of how sentence-level bias classifiers can be leveraged to better predict article-level bias (Chen et al., 2020). Given these results, the paper in hand digs deeper by investigating how bias is actually manifested in a text at different granularity levels. Our approach resembles techniques where the attention mechanism in a model is used to output wei"
2020.nlpcss-1.16,D18-1388,0,0.0172569,"level. At the word level, we find some LIWC categories to be particularly correlated with political bias and unfairness, such as negative emotion, focus present, and percept. At levels of larger granularity, we observe that the last part of an article usually tends to be most biased and unfairest. 2 Related Work Computational approaches to media bias focus on factuality (Baly et al., 2018), political ideology (Iyyer et al., 2014), and information quality (Rashkin et al., 2017). Bias detection has been done at different granularity levels: single sentences (Bhatia and Deepak, 2018), articles (Kulkarni et al., 2018), and media sources (Baly et al., 2019). Recently, the authors of this paper studied how the two granularity levels “sentence” and “discourse” affect each other. An outcome of this research are insights and means of how sentence-level bias classifiers can be leveraged to better predict article-level bias (Chen et al., 2020). Given these results, the paper in hand digs deeper by investigating how bias is actually manifested in a text at different granularity levels. Our approach resembles techniques where the attention mechanism in a model is used to output weights (which indicate feature impor"
2020.nlpcss-1.16,D14-1162,0,0.0864531,"Missing"
2020.nlpcss-1.16,D17-1317,0,0.0210276,"tic features of bias, and it confirms that they are manifested in larger granularity levels, i.e., on the paragraph level and the discourse level. At the word level, we find some LIWC categories to be particularly correlated with political bias and unfairness, such as negative emotion, focus present, and percept. At levels of larger granularity, we observe that the last part of an article usually tends to be most biased and unfairest. 2 Related Work Computational approaches to media bias focus on factuality (Baly et al., 2018), political ideology (Iyyer et al., 2014), and information quality (Rashkin et al., 2017). Bias detection has been done at different granularity levels: single sentences (Bhatia and Deepak, 2018), articles (Kulkarni et al., 2018), and media sources (Baly et al., 2019). Recently, the authors of this paper studied how the two granularity levels “sentence” and “discourse” affect each other. An outcome of this research are insights and means of how sentence-level bias classifiers can be leveraged to better predict article-level bias (Chen et al., 2020). Given these results, the paper in hand digs deeper by investigating how bias is actually manifested in a text at different granularit"
2020.nlpcss-1.16,N16-1174,0,0.0222836,"t granularity levels. Our approach resembles techniques where the attention mechanism in a model is used to output weights (which indicate feature importance) for each text segment (Bahdanau et al., 2014). Zhou et al. (2016), for instance, use word-level attention to focus on sentiment words, while Ji et al. (2017) use sentencelevel attention to select valid sentences for entity relation extraction. Related to media bias, Kulkarni et al. (2018) show that the learned attention can focus on the biased sentences when detecting political ideology. Regarding “attention” at multi-level granularity, Yang et al. (2016) propose a hierarchical attention network for document classification that is used at both word and sentence level. Yet, problems when using attention for such analyses are that the analysis unit (be it a word or a sentence) has to be defined before the training process, and that the set of classifiers which can output attention is limited. By contrast, our unsupervised reverse feature analysis can be used with any classifier and at an arbitrary semantic level, even after the training process. 3 Media Bias Corpus Although bias detection is viewed as an important task, we could not find any rea"
2020.nlpcss-1.16,D16-1024,0,0.0292956,"Missing"
2020.nlpcss-1.16,2020.findings-emnlp.383,1,0.87009,"Paderborn University Bauhaus-Universität Weimar Department of Computer Science Faculty of Media, Webis Group cwf@mail.upb.de khalid.alkhatib@uni-weimar.de Henning Wachsmuth Paderborn University Department of Computer Science henningw@upb.de Benno Stein Bauhaus-Universität Weimar Faculty of Media, Webis Group benno.stein@uni-weimar.de Abstract them) can allow media organizations to maintain credibility and can enable readers to choose what to consume and what not. In pursuit of this goal, we recently studied how sentence-level bias in an article affects the political bias of the whole article (Chen et al., 2020). Also other researchers have proposed approaches to automatic bias detection (see Section 2 for details). However, the existing approaches lack an analysis of what makes up bias, and how it exposes in different granularity levels, from single words, to sentences and paragraphs, to the entire discourse. To close this gap, we analyze political bias and unfairness in this paper within three steps: Media organizations bear great reponsibility because of their considerable influence on shaping beliefs and positions of our society. Any form of media can contain overly biased content, e.g., by repor"
2020.nlpcss-1.16,W18-6509,1,0.630631,"lems when using attention for such analyses are that the analysis unit (be it a word or a sentence) has to be defined before the training process, and that the set of classifiers which can output attention is limited. By contrast, our unsupervised reverse feature analysis can be used with any classifier and at an arbitrary semantic level, even after the training process. 3 Media Bias Corpus Although bias detection is viewed as an important task, we could not find any reasonably sized labeled corpus that suits our study, which is why we decided to built a new one. We started with the corpus of Chen et al. (2018) in order to obtain news articles with topics and political bias labels. We extended the corpus by crawling the latest articles from allsides.com and by adding the fairness labels provided by adfontesmedia.com. The new corpus is available at https://github.com/ webis-de/NLPCSS-20. Allsides.com This website is a news aggregator that collects news articles about American politics, starting from June 1st, 2012. Each event comes with articles representing one of the three political camps: left, center, and right. Chen et al. (2018) crawled the website to extract 6447 articles. For our study, we ex"
2020.nlpcss-1.16,P14-1105,0,0.204757,"how that our model can utilize highlevel semantic features of bias, and it confirms that they are manifested in larger granularity levels, i.e., on the paragraph level and the discourse level. At the word level, we find some LIWC categories to be particularly correlated with political bias and unfairness, such as negative emotion, focus present, and percept. At levels of larger granularity, we observe that the last part of an article usually tends to be most biased and unfairest. 2 Related Work Computational approaches to media bias focus on factuality (Baly et al., 2018), political ideology (Iyyer et al., 2014), and information quality (Rashkin et al., 2017). Bias detection has been done at different granularity levels: single sentences (Bhatia and Deepak, 2018), articles (Kulkarni et al., 2018), and media sources (Baly et al., 2019). Recently, the authors of this paper studied how the two granularity levels “sentence” and “discourse” affect each other. An outcome of this research are insights and means of how sentence-level bias classifiers can be leveraged to better predict article-level bias (Chen et al., 2020). Given these results, the paper in hand digs deeper by investigating how bias is actua"
2020.peoples-1.4,C16-1324,1,0.931256,"lity is relatively high in neuroticism and low in extraversion. We further investigate the role of editorials’ geographical scope; whether it tackles a global, national, or state topics. 2 Related Work News editorials reflect argumentation related to political issues and, therefore, comprise hidden rhetorical means (van Dijk, 1995), which makes them a challenging genre to study. Some works dealt with news editorials for information retrieval purposes (Yu and Hatzivassiloglou, 2003; Bal, 2009) or for analyzing arguments (Bal and Dizier, 2010; Kiesel et al., 2015; Scheffler and Stede, 2016). Al Khatib et al. (2016) represent editorial argumentation explicitly by annotating 300 news editorials with argumentative discourse units on the sub-sentence level. We employ their model to extract features from news editorials for predicting the persuasive effectiveness of news editorials, as detailed in Section 4. Aristotle (2007) argued that a persuasive effect is best achieved by providing showing a good character (ethos), evoking the right emotions (pathos), and providing logically reasoned arguments (logos) in a wellarranged and well-phrased way. This view was modeled by Wachsmuth et al. (2018) for argumentati"
2020.peoples-1.4,D17-1141,1,0.910616,"s, the average range is ≥ 33 and &lt; 67. 32 Feature Base Overview Linguistic inquiry and word count Psychological meaningfulness in percentile NRC emotional and sentiment lexicon Count of emotions (e,g. fear, etc.) and polarity words Webis Argumentative Discourse Units Count of each evidence type (anecdote and testimony) Count of 17 types of arguing (asMPQA Arguing Lexicon sessments, doubt, etc.) Count of subjective and objecMPQA Subjectivity Classifier tive sentences Lemma 1–3-grams TfIdf of lemma for 1–3-grams Reference Label Pennebaker et al. (2015) liwc Mohammad and Turney (2013) emotion Al Khatib et al. (2017) evidence Somasundaran et al. (2007) arguing Riloff and Wiebe (2003) subjectivity Miller (1998) lemma Table 3: Summary of the six feature types used. Each feature is quantified at both the level of the editorial. The labels (rightmost column) are used to refer to the respective feature. Gerlach et al. (2018) developed an approach to identify personality types, which they applied to more than 1.5 million participants. They found robust evidence for at least four distinct personality types and one of them is labeled as the “role model”, who is low in neuroticism and high in all the other traits."
2020.peoples-1.4,2020.acl-main.632,1,0.926615,"aims at changing or affecting the behavior of others or at strengthening the existing beliefs of those who already agree. And (5) the channel represents the mean used to read the editorial, e.g., an online news portal. We leave an analysis of the impact of the medium used on the editorial’s effectiveness to future research. Previous research tackled how people are affected by arguments depending on their personality traits, interests, and beliefs. However, most studies conducted their analysis on dialogical text from debate portals and similar (Lukin et al., 2017; Durmus and Cardie, 2018; Al Khatib et al., 2020). For news editorials, we recently revealed that liberal readers, unlike conservatives, are affected by the linguistic style (El Baff et al., 2020). Still, it remains unexplored to what extent also the intensity of a political ideology plays a role, let alone a reader’s personality traits. In our work here, we fill this gap, and we consider both the content and the style of an editorial. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 A German newspaper: https://www.spiegel.de/international/"
2020.peoples-1.4,bal-saint-dizier-2010-towards,0,0.0279657,"e somewhat impacted by style, and the same holds for readers whose personality is relatively high in neuroticism and low in extraversion. We further investigate the role of editorials’ geographical scope; whether it tackles a global, national, or state topics. 2 Related Work News editorials reflect argumentation related to political issues and, therefore, comprise hidden rhetorical means (van Dijk, 1995), which makes them a challenging genre to study. Some works dealt with news editorials for information retrieval purposes (Yu and Hatzivassiloglou, 2003; Bal, 2009) or for analyzing arguments (Bal and Dizier, 2010; Kiesel et al., 2015; Scheffler and Stede, 2016). Al Khatib et al. (2016) represent editorial argumentation explicitly by annotating 300 news editorials with argumentative discourse units on the sub-sentence level. We employ their model to extract features from news editorials for predicting the persuasive effectiveness of news editorials, as detailed in Section 4. Aristotle (2007) argued that a persuasive effect is best achieved by providing showing a good character (ethos), evoking the right emotions (pathos), and providing logically reasoned arguments (logos) in a wellarranged and well-phr"
2020.peoples-1.4,W09-3723,0,0.0606116,"ow that people with extreme ideology are somewhat impacted by style, and the same holds for readers whose personality is relatively high in neuroticism and low in extraversion. We further investigate the role of editorials’ geographical scope; whether it tackles a global, national, or state topics. 2 Related Work News editorials reflect argumentation related to political issues and, therefore, comprise hidden rhetorical means (van Dijk, 1995), which makes them a challenging genre to study. Some works dealt with news editorials for information retrieval purposes (Yu and Hatzivassiloglou, 2003; Bal, 2009) or for analyzing arguments (Bal and Dizier, 2010; Kiesel et al., 2015; Scheffler and Stede, 2016). Al Khatib et al. (2016) represent editorial argumentation explicitly by annotating 300 news editorials with argumentative discourse units on the sub-sentence level. We employ their model to extract features from news editorials for predicting the persuasive effectiveness of news editorials, as detailed in Section 4. Aristotle (2007) argued that a persuasive effect is best achieved by providing showing a good character (ethos), evoking the right emotions (pathos), and providing logically reasoned"
2020.peoples-1.4,N18-1094,0,0.204693,"states that persuasive text aims at changing or affecting the behavior of others or at strengthening the existing beliefs of those who already agree. And (5) the channel represents the mean used to read the editorial, e.g., an online news portal. We leave an analysis of the impact of the medium used on the editorial’s effectiveness to future research. Previous research tackled how people are affected by arguments depending on their personality traits, interests, and beliefs. However, most studies conducted their analysis on dialogical text from debate portals and similar (Lukin et al., 2017; Durmus and Cardie, 2018; Al Khatib et al., 2020). For news editorials, we recently revealed that liberal readers, unlike conservatives, are affected by the linguistic style (El Baff et al., 2020). Still, it remains unexplored to what extent also the intensity of a political ideology plays a role, let alone a reader’s personality traits. In our work here, we fill this gap, and we consider both the content and the style of an editorial. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 A German newspaper: https://www.s"
2020.peoples-1.4,K18-1044,1,0.708994,"Missing"
2020.peoples-1.4,2020.acl-main.287,1,0.375206,"Missing"
2020.peoples-1.4,P16-1150,0,0.01192,"liefs of readers. Lukin et al. (2017) find that emotional and rational arguments are effective depending on the Big Five personality traits (John et al., 1991). Also, Durmus and Cardie (2018) provide a debate portal dataset with a controlled task setting that takes into consideration the reader’s religious and political ideology, and Al Khatib et al. (2020) exploit the personal characteristics of debaters to improve persuasiveness prediction. (4) Impact reflects the effect of a text, which has been assessed for essays (Persing and Ng, 2015; Wachsmuth et al., 2016) and debate portal arguments (Habernal and Gurevych, 2016; Persing and Ng, 2017). (5) Channel, finally, means the communication medium. Joiner and Jones (2003) study the effect of the medium on argumentation. They found that the quality of argumentation in face-to-face discussions is higher than in online discussions. In previous work, we annotated news editorials at the document level, covering an editorial’s persuasive effectiveness by reflecting to what extent a writer persuades a reader (El Baff et al., 2018); the effectiveness concept is based on the argumentation quality taxonomy defined by Wachsmuth et al. (2017). In our annotation setup, we"
2020.peoples-1.4,W17-5102,0,0.0116636,"a wellarranged and well-phrased way. This view was modeled by Wachsmuth et al. (2018) for argumentation synthesis. Instead, we here follow the communication-persuasion paradigm of O’Keefe (2015), stating that an argumentative text, and hence a news editorial, should comply with five factors to be persuasive, as already indicated in Section 1. Each of them is tackled in some way in related work: (1) Source refers to the prior beliefs and behaviors of the writer. Each news portal reflects its beliefs (van Dijk, 1995). (2) Message deals with the linguistic choices in the content. In this regard, Hidey et al. (2017) study the semantic types of argument components in an online forum, and El Baff et al. (2020) analyze the persuasive effect of linguistic style on readers. Also, Hidey and McKeown (2018) and Durmus et al. (2020), respectively, exploit the role of argument sequencing in detecting persuasive influence, and the role of pragmatics and discourse context in determining argument impact. (3) Target includes the prior beliefs of readers. Lukin et al. (2017) find that emotional and rational arguments are effective depending on the Big Five personality traits (John et al., 1991). Also, Durmus and Cardie"
2020.peoples-1.4,W15-0505,1,0.824942,"style, and the same holds for readers whose personality is relatively high in neuroticism and low in extraversion. We further investigate the role of editorials’ geographical scope; whether it tackles a global, national, or state topics. 2 Related Work News editorials reflect argumentation related to political issues and, therefore, comprise hidden rhetorical means (van Dijk, 1995), which makes them a challenging genre to study. Some works dealt with news editorials for information retrieval purposes (Yu and Hatzivassiloglou, 2003; Bal, 2009) or for analyzing arguments (Bal and Dizier, 2010; Kiesel et al., 2015; Scheffler and Stede, 2016). Al Khatib et al. (2016) represent editorial argumentation explicitly by annotating 300 news editorials with argumentative discourse units on the sub-sentence level. We employ their model to extract features from news editorials for predicting the persuasive effectiveness of news editorials, as detailed in Section 4. Aristotle (2007) argued that a persuasive effect is best achieved by providing showing a good character (ethos), evoking the right emotions (pathos), and providing logically reasoned arguments (logos) in a wellarranged and well-phrased way. This view w"
2020.peoples-1.4,E17-1070,0,0.318433,"and Virtanen (2005) states that persuasive text aims at changing or affecting the behavior of others or at strengthening the existing beliefs of those who already agree. And (5) the channel represents the mean used to read the editorial, e.g., an online news portal. We leave an analysis of the impact of the medium used on the editorial’s effectiveness to future research. Previous research tackled how people are affected by arguments depending on their personality traits, interests, and beliefs. However, most studies conducted their analysis on dialogical text from debate portals and similar (Lukin et al., 2017; Durmus and Cardie, 2018; Al Khatib et al., 2020). For news editorials, we recently revealed that liberal readers, unlike conservatives, are affected by the linguistic style (El Baff et al., 2020). Still, it remains unexplored to what extent also the intensity of a political ideology plays a role, let alone a reader’s personality traits. In our work here, we fill this gap, and we consider both the content and the style of an editorial. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 A German"
2020.peoples-1.4,P15-1053,0,0.0188343,"se context in determining argument impact. (3) Target includes the prior beliefs of readers. Lukin et al. (2017) find that emotional and rational arguments are effective depending on the Big Five personality traits (John et al., 1991). Also, Durmus and Cardie (2018) provide a debate portal dataset with a controlled task setting that takes into consideration the reader’s religious and political ideology, and Al Khatib et al. (2020) exploit the personal characteristics of debaters to improve persuasiveness prediction. (4) Impact reflects the effect of a text, which has been assessed for essays (Persing and Ng, 2015; Wachsmuth et al., 2016) and debate portal arguments (Habernal and Gurevych, 2016; Persing and Ng, 2017). (5) Channel, finally, means the communication medium. Joiner and Jones (2003) study the effect of the medium on argumentation. They found that the quality of argumentation in face-to-face discussions is higher than in online discussions. In previous work, we annotated news editorials at the document level, covering an editorial’s persuasive effectiveness by reflecting to what extent a writer persuades a reader (El Baff et al., 2018); the effectiveness concept is based on the argumentation"
2020.peoples-1.4,I17-1060,0,0.0117986,". (2017) find that emotional and rational arguments are effective depending on the Big Five personality traits (John et al., 1991). Also, Durmus and Cardie (2018) provide a debate portal dataset with a controlled task setting that takes into consideration the reader’s religious and political ideology, and Al Khatib et al. (2020) exploit the personal characteristics of debaters to improve persuasiveness prediction. (4) Impact reflects the effect of a text, which has been assessed for essays (Persing and Ng, 2015; Wachsmuth et al., 2016) and debate portal arguments (Habernal and Gurevych, 2016; Persing and Ng, 2017). (5) Channel, finally, means the communication medium. Joiner and Jones (2003) study the effect of the medium on argumentation. They found that the quality of argumentation in face-to-face discussions is higher than in online discussions. In previous work, we annotated news editorials at the document level, covering an editorial’s persuasive effectiveness by reflecting to what extent a writer persuades a reader (El Baff et al., 2018); the effectiveness concept is based on the argumentation quality taxonomy defined by Wachsmuth et al. (2017). In our annotation setup, we considered the beliefs"
2020.peoples-1.4,W03-1014,0,0.0556693,"Missing"
2020.peoples-1.4,2007.sigdial-1.5,0,0.282657,"nd &lt; 67. 32 Feature Base Overview Linguistic inquiry and word count Psychological meaningfulness in percentile NRC emotional and sentiment lexicon Count of emotions (e,g. fear, etc.) and polarity words Webis Argumentative Discourse Units Count of each evidence type (anecdote and testimony) Count of 17 types of arguing (asMPQA Arguing Lexicon sessments, doubt, etc.) Count of subjective and objecMPQA Subjectivity Classifier tive sentences Lemma 1–3-grams TfIdf of lemma for 1–3-grams Reference Label Pennebaker et al. (2015) liwc Mohammad and Turney (2013) emotion Al Khatib et al. (2017) evidence Somasundaran et al. (2007) arguing Riloff and Wiebe (2003) subjectivity Miller (1998) lemma Table 3: Summary of the six feature types used. Each feature is quantified at both the level of the editorial. The labels (rightmost column) are used to refer to the respective feature. Gerlach et al. (2018) developed an approach to identify personality types, which they applied to more than 1.5 million participants. They found robust evidence for at least four distinct personality types and one of them is labeled as the “role model”, who is low in neuroticism and high in all the other traits. Figure 2 shows that the upper clust"
2020.peoples-1.4,C16-1158,1,0.849608,"ing argument impact. (3) Target includes the prior beliefs of readers. Lukin et al. (2017) find that emotional and rational arguments are effective depending on the Big Five personality traits (John et al., 1991). Also, Durmus and Cardie (2018) provide a debate portal dataset with a controlled task setting that takes into consideration the reader’s religious and political ideology, and Al Khatib et al. (2020) exploit the personal characteristics of debaters to improve persuasiveness prediction. (4) Impact reflects the effect of a text, which has been assessed for essays (Persing and Ng, 2015; Wachsmuth et al., 2016) and debate portal arguments (Habernal and Gurevych, 2016; Persing and Ng, 2017). (5) Channel, finally, means the communication medium. Joiner and Jones (2003) study the effect of the medium on argumentation. They found that the quality of argumentation in face-to-face discussions is higher than in online discussions. In previous work, we annotated news editorials at the document level, covering an editorial’s persuasive effectiveness by reflecting to what extent a writer persuades a reader (El Baff et al., 2018); the effectiveness concept is based on the argumentation quality taxonomy defined"
2020.peoples-1.4,E17-1017,1,0.835541,"debate portal arguments (Habernal and Gurevych, 2016; Persing and Ng, 2017). (5) Channel, finally, means the communication medium. Joiner and Jones (2003) study the effect of the medium on argumentation. They found that the quality of argumentation in face-to-face discussions is higher than in online discussions. In previous work, we annotated news editorials at the document level, covering an editorial’s persuasive effectiveness by reflecting to what extent a writer persuades a reader (El Baff et al., 2018); the effectiveness concept is based on the argumentation quality taxonomy defined by Wachsmuth et al. (2017). In our annotation setup, we considered the beliefs of readers by profiling annotators based on political ideology (liberals or conservatives). We also provides additional information about the annotators’ ideology 30 (a) Intensity Effect Lean Extreme (b) Personality Role Models Other Train Test Train Test Train Test Train Test Effect Challenging Ineffective Reinforcing 100 274 409 21 70 105 156 133 494 43 30 123 Challenging Ineffective Reinforcing 74 121 588 15 31 150 106 412 265 26 108 62 Overall 783 196 783 196 Overall 783 196 783 196 Table 1: Distribution of the majority persuasive effect"
2020.peoples-1.4,C18-1318,1,0.890881,"Missing"
2020.peoples-1.4,W03-1017,0,0.425214,"style features. Our results show that people with extreme ideology are somewhat impacted by style, and the same holds for readers whose personality is relatively high in neuroticism and low in extraversion. We further investigate the role of editorials’ geographical scope; whether it tackles a global, national, or state topics. 2 Related Work News editorials reflect argumentation related to political issues and, therefore, comprise hidden rhetorical means (van Dijk, 1995), which makes them a challenging genre to study. Some works dealt with news editorials for information retrieval purposes (Yu and Hatzivassiloglou, 2003; Bal, 2009) or for analyzing arguments (Bal and Dizier, 2010; Kiesel et al., 2015; Scheffler and Stede, 2016). Al Khatib et al. (2016) represent editorial argumentation explicitly by annotating 300 news editorials with argumentative discourse units on the sub-sentence level. We employ their model to extract features from news editorials for predicting the persuasive effectiveness of news editorials, as detailed in Section 4. Aristotle (2007) argued that a persuasive effect is best achieved by providing showing a good character (ethos), evoking the right emotions (pathos), and providing logica"
2020.semeval-1.186,2020.semeval-1.241,0,0.183778,"ËË  Ë 11. 13. 14. 16. 17. 20. 21. 22. Ë       Ë  Ë Ë (Tao and Zhou, 2020) (Singh et al., 2020) (Chauhan and Diddee, 2020) (Dao et al., 2020) (Martinkovic et al., 2020) (Kim and Bethard, 2020) (Patil et al., 2020) (Kranzlein et al., 2020)   Ë   (Morio et al., 2020) (Jurkiewicz et al., 2020) (Chernyavskiy et al., 2020) (Khosla et al., 2020) (Paraschiv and Cercel, 2020) (Dimov et al., 2020) (Blaschke et al., 2020) (Verma et al., 2020)         Ë 23. 25. 26. 27. 28. 31. 33. – ËËË (Mikhalkova et al., 2020) (Li and Xiao, 2020) (Dementieva et al., 2020) (Arsenos and Siolas, 2020) (Ermurachi and Gifu, 2020) (Dewantara et al., 2020) (Daval-Frerot and Yannick, 2020) (Krishnamurthy et al., 2020) Table 3: Overview of the approaches to the span identification subtask. =part of the official submission; Ë=considered in internal experiments. The references to the description papers appear at the bottom. A large number of the participating teams built systems that rely heavily on engineered features. For instance, Team CyberWallE(SI:8) (Blaschke et al., 2020) used features modeling sentiment, rhetorical structure, and POS tags, while team UTMN(SI:23) injected the sentiment intensity from VADER and it wa"
2020.semeval-1.186,I17-1078,0,0.0125125,"pective: a fine-grained analysis of the text that complements existing approaches and can, in principle, be combined with them. Propaganda in text (and in other channels) is conveyed through the use of diverse propaganda techniques (Miller, 1939), which range from leveraging on the emotions of the audience —such as using loaded language or appeals to fear— to using logical fallacies —such as straw men (misrepresenting someone’s opinion), hidden ad-hominem fallacies, and red herring (presenting irrelevant data). Some of these techniques have been studied in tasks such as hate speech detection (Gao et al., 2017) and computational argumentation (Habernal et al., 2018). Figure 1 shows the fine-grained propaganda identification pipeline, including the two targeted subtasks. Our goal is to facilitate the development of models capable of spotting text fragments where propaganda techniques are used. The task featured the following subtasks: Subtask SI (Span Identification): Given a plain-text document, identify those specific fragments that contain at least one propaganda technique. (This is a binary sequence tagging task.) Subtask TC (Technique Classification): Given a propagandistic text snippet and its"
2020.semeval-1.186,2020.semeval-1.187,0,0.0431519,"Missing"
2020.semeval-1.186,2020.semeval-1.238,0,0.192615,"8. CyberWallE     10. Duth   11. DiSaster  Ë    13. SocCogCom  Ë Ë  Ë 14. TTUI     15. JUST   16. NLFIIT Ë  Ë Ë Ë 17. UMSIForeseer    18. BPGC  Ë Ë  Ë   19. UPB   20. syrapropa      21. WMD  Ë Ë Ë Ë   22. YNUHPCC   Ë 24. DoNotDistribute     25. NTUAAILS   26. UAIC1860 ËË Ë    27. UNTLing     1. (Jurkiewicz et al., 2020) 2. (Chernyavskiy et al., 2020) 3. (Morio et al., 2020) 4. (Raj et al., 2020) 5. (Singh et al., 2020) 6. (Dimov et al., 2020) 7. (Grigorev and Ivanov, 2020) 8. (Blaschke et al., 2020) 10. (Bairaktaris et al., 2020) 11. (Kaas et al., 2020) 13. (Krishnamurthy et al., 2020) 14. (Kim and Bethard, 2020) 15. (Altiti et al., 2020) 16. (Martinkovic et al., 2020) 17. (Jiang et al., 2020) 18. (Patil et al., 2020) 19. (Paraschiv and Cercel, 2020) 20. (Li and Xiao, 2020) 21. (Daval-Frerot and Yannick, 2020) 22. (Dao et al., 2020) 24. (Kranzlein et al., 2020) 25. (Arsenos and Siolas, 2020) 26. (Ermurachi and Gifu, 2020) 27. (Petee and Palmer, 2020) 29. (Verma et al., 2020) Table 4: Overview of the approaches to the technique classification subtask. =part of the official submission; Ë=considered in internal experiments. The references to t"
2020.semeval-1.186,2020.semeval-1.230,0,0.187088,"n distributional semantics. Finally, team WMD(SI:33) (Daval-Frerot and Yannick, 2020) applied multiple strategies to augment the data such as back translation, synonym replacement and TF.IDF replacement (replace unimportant words, based on TF.IDF score, by other unimportant words). Closing the top-three submissions, Team aschern(SI:3) (Chernyavskiy et al., 2020) fine-tuned an ensemble of two differently intialized RoBERTa models, each with an attached CRF for sequence labeling and span character boundary post-processing. There have been several other promising strategies. Team LTIatCMU(SI:4) (Khosla et al., 2020) used a multi-granular BERT BiLSTM model with additional syntactic and semantic features at the word, sentence and document level, including PoS, named entities, sentiment, and subjectivity. It was trained jointly for token and sentence propaganda classification, with class balancing. They further fine-tuned BERT on persuasive language using 10,000 articles from propaganda websites, which turned out to be important. Team PsuedoProp(SI:14) (Chauhan and Diddee, 2020) built a preliminary sentence-level classifier using an ensemble of XLNet and RoBERTa, before it fine-tuned a BERT-based CRF sequen"
2020.semeval-1.186,2020.semeval-1.240,0,0.194543,"ApplicaAI aschern LTIatCMU UPB NoPropaganda CyberWallE Transformers YNUtaoxin newsSweeper PsuedoProp YNUHPCC NLFIIT TTUI BPGC DoNotDistribute UTMN syrapropa SkoltechNLP NTUAAILS UAIC1860 3218IR WMD UNTLing 1. 2. 3. 4. 5. 7. 8. 9.            Ë  Ë   Ë Ë   Ë                            Ë  Ë          Ë  Ë  Ë   Ë ËË  Ë Ë     Ë Ë Ë   Ë Ë Ë        ËË  Ë ËË  Ë 11. 13. 14. 16. 17. 20. 21. 22. Ë       Ë  Ë Ë (Tao and Zhou, 2020) (Singh et al., 2020) (Chauhan and Diddee, 2020) (Dao et al., 2020) (Martinkovic et al., 2020) (Kim and Bethard, 2020) (Patil et al., 2020) (Kranzlein et al., 2020)   Ë   (Morio et al., 2020) (Jurkiewicz et al., 2020) (Chernyavskiy et al., 2020) (Khosla et al., 2020) (Paraschiv and Cercel, 2020) (Dimov et al., 2020) (Blaschke et al., 2020) (Verma et al., 2020)         Ë 23. 25. 26. 27. 28. 31. 33. – ËËË (Mikhalkova et al., 2020) (Li and Xiao, 2020) (Dementieva et al., 2020) (Arsenos and Siolas, 2020) (Ermurachi and Gifu, 2020) (Dewantara et al., 2020) (Daval-Frerot and Yannick, 2020) (Krishnamurthy et al., 2020) Table 3: Overview of the approaches to the span identification subtask. =part of the"
2020.semeval-1.186,2020.semeval-1.196,0,0.206471,"CyberWallE Transformers YNUtaoxin newsSweeper PsuedoProp YNUHPCC NLFIIT TTUI BPGC DoNotDistribute UTMN syrapropa SkoltechNLP NTUAAILS UAIC1860 3218IR WMD UNTLing 1. 2. 3. 4. 5. 7. 8. 9.            Ë  Ë   Ë Ë   Ë                            Ë  Ë          Ë  Ë  Ë   Ë ËË  Ë Ë     Ë Ë Ë   Ë Ë Ë        ËË  Ë ËË  Ë 11. 13. 14. 16. 17. 20. 21. 22. Ë       Ë  Ë Ë (Tao and Zhou, 2020) (Singh et al., 2020) (Chauhan and Diddee, 2020) (Dao et al., 2020) (Martinkovic et al., 2020) (Kim and Bethard, 2020) (Patil et al., 2020) (Kranzlein et al., 2020)   Ë   (Morio et al., 2020) (Jurkiewicz et al., 2020) (Chernyavskiy et al., 2020) (Khosla et al., 2020) (Paraschiv and Cercel, 2020) (Dimov et al., 2020) (Blaschke et al., 2020) (Verma et al., 2020)         Ë 23. 25. 26. 27. 28. 31. 33. – ËËË (Mikhalkova et al., 2020) (Li and Xiao, 2020) (Dementieva et al., 2020) (Arsenos and Siolas, 2020) (Ermurachi and Gifu, 2020) (Dewantara et al., 2020) (Daval-Frerot and Yannick, 2020) (Krishnamurthy et al., 2020) Table 3: Overview of the approaches to the span identification subtask. =part of the official submission; Ë=considered in internal"
2020.semeval-1.186,2020.semeval-1.232,0,0.203255,". 27. 28. 31. 33. – Hitachi ApplicaAI aschern LTIatCMU UPB NoPropaganda CyberWallE Transformers YNUtaoxin newsSweeper PsuedoProp YNUHPCC NLFIIT TTUI BPGC DoNotDistribute UTMN syrapropa SkoltechNLP NTUAAILS UAIC1860 3218IR WMD UNTLing 1. 2. 3. 4. 5. 7. 8. 9.            Ë  Ë   Ë Ë   Ë                            Ë  Ë          Ë  Ë  Ë   Ë ËË  Ë Ë     Ë Ë Ë   Ë Ë Ë        ËË  Ë ËË  Ë 11. 13. 14. 16. 17. 20. 21. 22. Ë       Ë  Ë Ë (Tao and Zhou, 2020) (Singh et al., 2020) (Chauhan and Diddee, 2020) (Dao et al., 2020) (Martinkovic et al., 2020) (Kim and Bethard, 2020) (Patil et al., 2020) (Kranzlein et al., 2020)   Ë   (Morio et al., 2020) (Jurkiewicz et al., 2020) (Chernyavskiy et al., 2020) (Khosla et al., 2020) (Paraschiv and Cercel, 2020) (Dimov et al., 2020) (Blaschke et al., 2020) (Verma et al., 2020)         Ë 23. 25. 26. 27. 28. 31. 33. – ËËË (Mikhalkova et al., 2020) (Li and Xiao, 2020) (Dementieva et al., 2020) (Arsenos and Siolas, 2020) (Ermurachi and Gifu, 2020) (Dewantara et al., 2020) (Daval-Frerot and Yannick, 2020) (Krishnamurthy et al., 2020) Table 3: Overview of the approaches to the span identification"
2020.semeval-1.186,J15-3003,0,0.0243002,"Missing"
2020.semeval-1.186,S19-2149,1,0.79131,"of 24.88 only (and only 10.43 in the datathon). This is why, here we decided to split the task into subtasks in order to allow researchers to focus on one subtask at a time. Moreover, we merged some of the original 18 propaganda techniques to reduce data sparseness issues. Other related shared tasks include the FEVER 2018 and 2019 tasks on Fact Extraction and VERification (Thorne et al., 2018), the SemEval 2017 and 2019 tasks on determining the veracity of rumors (Derczynski et al., 2017; Gorrell et al., 2019) and the SemEval 2019 task on Fact-Checking in Community Question Answering Forums (Mihaylova et al., 2019). Also, the CLEF 2018–2020 CheckThat! labs’ shared tasks (Nakov et al., 2018; Elsayed et al., 2019a; Elsayed et al., 2019b; Barr´on-Cede˜no et al., 2020a; Barr´on-Cede˜no et al., 2020b), which featured tasks on automatic identification (Atanasova et al., 2018; Atanasova et al., 2019) and verification (Barr´on-Cede˜no et al., 2018; Hasanain et al., 2019; Hasanain et al., 2020; Shaar et al., 2020) of claims in political debates and in social media. 8 You can also try the Prta system (Da San Martino et al., 2020b) online at: http://www.tanbih.org/prta http://www.datasciencesociety.net/hack-news-d"
2020.semeval-1.186,2020.semeval-1.245,0,0.0349937,"Ë  Ë          Ë  Ë  Ë   Ë ËË  Ë Ë     Ë Ë Ë   Ë Ë Ë        ËË  Ë ËË  Ë 11. 13. 14. 16. 17. 20. 21. 22. Ë       Ë  Ë Ë (Tao and Zhou, 2020) (Singh et al., 2020) (Chauhan and Diddee, 2020) (Dao et al., 2020) (Martinkovic et al., 2020) (Kim and Bethard, 2020) (Patil et al., 2020) (Kranzlein et al., 2020)   Ë   (Morio et al., 2020) (Jurkiewicz et al., 2020) (Chernyavskiy et al., 2020) (Khosla et al., 2020) (Paraschiv and Cercel, 2020) (Dimov et al., 2020) (Blaschke et al., 2020) (Verma et al., 2020)         Ë 23. 25. 26. 27. 28. 31. 33. – ËËË (Mikhalkova et al., 2020) (Li and Xiao, 2020) (Dementieva et al., 2020) (Arsenos and Siolas, 2020) (Ermurachi and Gifu, 2020) (Dewantara et al., 2020) (Daval-Frerot and Yannick, 2020) (Krishnamurthy et al., 2020) Table 3: Overview of the approaches to the span identification subtask. =part of the official submission; Ë=considered in internal experiments. The references to the description papers appear at the bottom. A large number of the participating teams built systems that rely heavily on engineered features. For instance, Team CyberWallE(SI:8) (Blaschke et al., 2020) used features modeling sentiment, rhetorical s"
2020.semeval-1.186,2020.semeval-1.228,0,0.577546,"bindex on the right of each team represents their official rank in the subtasks. Appendix A includes brief descriptions of all systems. 4.1 Span Identification Subtask Table 3 shows a quick overview of the systems that took part in the SI subtask.7 All systems in the top-10 positions relied on some kind of Transformer, in combination with an LSTM or a CRF. In most cases, the Transformer-generated representations were complemented by engineered features, such as named entities and the presence of sentiment and subjectivity clues. Team Hitachi(SI:1) achieved the top performance in this subtask (Morio et al., 2020). They used a BIO encoding, which is typical for related segmentation and labeling tasks (e.g., named entity recognition). They relied upon a complex heterogeneous multi-layer neural network, trained end-to-end. The network uses pre-trained language models, which generate a representation for each input token. They further added part-of-speech (PoS) and named entity (NE) embeddings. As a result, there are three representations for each token, which are concatenated and used as an input to bi-LSTMs. At this moment, the network branches, as it is trained with three objectives: (i) the main BIO t"
2020.semeval-1.186,2020.semeval-1.244,0,0.326496,"S UAIC1860 3218IR WMD UNTLing 1. 2. 3. 4. 5. 7. 8. 9.            Ë  Ë   Ë Ë   Ë                            Ë  Ë          Ë  Ë  Ë   Ë ËË  Ë Ë     Ë Ë Ë   Ë Ë Ë        ËË  Ë ËË  Ë 11. 13. 14. 16. 17. 20. 21. 22. Ë       Ë  Ë Ë (Tao and Zhou, 2020) (Singh et al., 2020) (Chauhan and Diddee, 2020) (Dao et al., 2020) (Martinkovic et al., 2020) (Kim and Bethard, 2020) (Patil et al., 2020) (Kranzlein et al., 2020)   Ë   (Morio et al., 2020) (Jurkiewicz et al., 2020) (Chernyavskiy et al., 2020) (Khosla et al., 2020) (Paraschiv and Cercel, 2020) (Dimov et al., 2020) (Blaschke et al., 2020) (Verma et al., 2020)         Ë 23. 25. 26. 27. 28. 31. 33. – ËËË (Mikhalkova et al., 2020) (Li and Xiao, 2020) (Dementieva et al., 2020) (Arsenos and Siolas, 2020) (Ermurachi and Gifu, 2020) (Dewantara et al., 2020) (Daval-Frerot and Yannick, 2020) (Krishnamurthy et al., 2020) Table 3: Overview of the approaches to the span identification subtask. =part of the official submission; Ë=considered in internal experiments. The references to the description papers appear at the bottom. A large number of the participating teams built systems that"
2020.semeval-1.186,2020.semeval-1.226,0,0.607064,"syntactic and semantic features at the word, sentence and document level, including PoS, named entities, sentiment, and subjectivity. It was trained jointly for token and sentence propaganda classification, with class balancing. They further fine-tuned BERT on persuasive language using 10,000 articles from propaganda websites, which turned out to be important. Team PsuedoProp(SI:14) (Chauhan and Diddee, 2020) built a preliminary sentence-level classifier using an ensemble of XLNet and RoBERTa, before it fine-tuned a BERT-based CRF sequence tagger to identify the exact spans. Team BPGC(SI:21) (Patil et al., 2020) went beyond these multigranularity approaches. Information both at the article and at the sentence level was considered when classifying each word as propaganda or not, by computing and concatenating vectorial representations for the three inputs. 7 Tables 3 and 4 only include the systems for which a description paper was submitted. 1384 Transformers Learning Models Representations Misc BERT RoBERTa XLNet XLM XLM RoBERTa ALBERT GPT-2 SpanBERT LaserTagger LSTM CNN SVM Na¨ıve Bayes Boosting Log regressor Random forest CRF Embeddings ELMo NEs Words/n-grams Chars/n-grams PoS Trees Sentiment Subje"
2020.semeval-1.186,2020.semeval-1.243,0,0.229367,"al., 2020) 3. (Morio et al., 2020) 4. (Raj et al., 2020) 5. (Singh et al., 2020) 6. (Dimov et al., 2020) 7. (Grigorev and Ivanov, 2020) 8. (Blaschke et al., 2020) 10. (Bairaktaris et al., 2020) 11. (Kaas et al., 2020) 13. (Krishnamurthy et al., 2020) 14. (Kim and Bethard, 2020) 15. (Altiti et al., 2020) 16. (Martinkovic et al., 2020) 17. (Jiang et al., 2020) 18. (Patil et al., 2020) 19. (Paraschiv and Cercel, 2020) 20. (Li and Xiao, 2020) 21. (Daval-Frerot and Yannick, 2020) 22. (Dao et al., 2020) 24. (Kranzlein et al., 2020) 25. (Arsenos and Siolas, 2020) 26. (Ermurachi and Gifu, 2020) 27. (Petee and Palmer, 2020) 29. (Verma et al., 2020) Table 4: Overview of the approaches to the technique classification subtask. =part of the official submission; Ë=considered in internal experiments. The references to the description papers appear at the bottom. Team aschern(TC:2) (Chernyavskiy et al., 2020) was the second best, and it based its success on a RoBERTa ensemble with several interesting techniques. They treated the task as one of sequence classification, using an average embedding of the surrounding tokens and the length of the span as contextual features. They further incorporated knowledge from the spa"
2020.semeval-1.186,C10-2115,1,0.593446,"Missing"
2020.semeval-1.186,2020.semeval-1.236,0,0.0349132,"Team 1. ApplicaAI    2. aschern     3. Hitachi           4. Solomon  Ë    5. newsSweeper Ë  ËË Ë 6. NoPropaganda   7. Inno Ë Ë ËË Ë Ë 8. CyberWallE     10. Duth   11. DiSaster  Ë    13. SocCogCom  Ë Ë  Ë 14. TTUI     15. JUST   16. NLFIIT Ë  Ë Ë Ë 17. UMSIForeseer    18. BPGC  Ë Ë  Ë   19. UPB   20. syrapropa      21. WMD  Ë Ë Ë Ë   22. YNUHPCC   Ë 24. DoNotDistribute     25. NTUAAILS   26. UAIC1860 ËË Ë    27. UNTLing     1. (Jurkiewicz et al., 2020) 2. (Chernyavskiy et al., 2020) 3. (Morio et al., 2020) 4. (Raj et al., 2020) 5. (Singh et al., 2020) 6. (Dimov et al., 2020) 7. (Grigorev and Ivanov, 2020) 8. (Blaschke et al., 2020) 10. (Bairaktaris et al., 2020) 11. (Kaas et al., 2020) 13. (Krishnamurthy et al., 2020) 14. (Kim and Bethard, 2020) 15. (Altiti et al., 2020) 16. (Martinkovic et al., 2020) 17. (Jiang et al., 2020) 18. (Patil et al., 2020) 19. (Paraschiv and Cercel, 2020) 20. (Li and Xiao, 2020) 21. (Daval-Frerot and Yannick, 2020) 22. (Dao et al., 2020) 24. (Kranzlein et al., 2020) 25. (Arsenos and Siolas, 2020) 26. (Ermurachi and Gifu, 2020) 27. (Petee and Palmer, 2020) 29. (Verma et al., 2020) Table 4:"
2020.semeval-1.186,D17-1317,0,0.400865,"ching very large audiences (Muller, 2018; Tard´aguila et al., 2018; Glowacki et al., 2018). Propaganda is most successful when it goes unnoticed by the reader, and it often takes some training for people to be able to spot it. The task is way more difficult for inexperienced users, and the volume of text produced on a daily basis makes it difficult for experts to cope with it manually. With the recent interest in “fake news”, the detection of propaganda or highly biased texts has emerged as an active research area. However, most previous work has performed analysis at the document level only (Rashkin et al., 2017; Barr´on-Cede˜no et al., 2019a) or has analyzed the general patterns of online propaganda (Garimella et al., 2015; Chatfield et al., 2015). SemEval-2020 Task 11 offers a different perspective: a fine-grained analysis of the text that complements existing approaches and can, in principle, be combined with them. Propaganda in text (and in other channels) is conveyed through the use of diverse propaganda techniques (Miller, 1939), which range from leveraging on the emotions of the audience —such as using loaded language or appeals to fear— to using logical fallacies —such as straw men (misrepres"
2020.semeval-1.186,2020.semeval-1.231,0,0.293475,"1. 2. 3. 4. 5. 7. 8. 9. 11. 13. 14. 16. 17. 20. 21. 22. 23. 25. 26. 27. 28. 31. 33. – Hitachi ApplicaAI aschern LTIatCMU UPB NoPropaganda CyberWallE Transformers YNUtaoxin newsSweeper PsuedoProp YNUHPCC NLFIIT TTUI BPGC DoNotDistribute UTMN syrapropa SkoltechNLP NTUAAILS UAIC1860 3218IR WMD UNTLing 1. 2. 3. 4. 5. 7. 8. 9.            Ë  Ë   Ë Ë   Ë                            Ë  Ë          Ë  Ë  Ë   Ë ËË  Ë Ë     Ë Ë Ë   Ë Ë Ë        ËË  Ë ËË  Ë 11. 13. 14. 16. 17. 20. 21. 22. Ë       Ë  Ë Ë (Tao and Zhou, 2020) (Singh et al., 2020) (Chauhan and Diddee, 2020) (Dao et al., 2020) (Martinkovic et al., 2020) (Kim and Bethard, 2020) (Patil et al., 2020) (Kranzlein et al., 2020)   Ë   (Morio et al., 2020) (Jurkiewicz et al., 2020) (Chernyavskiy et al., 2020) (Khosla et al., 2020) (Paraschiv and Cercel, 2020) (Dimov et al., 2020) (Blaschke et al., 2020) (Verma et al., 2020)         Ë 23. 25. 26. 27. 28. 31. 33. – ËËË (Mikhalkova et al., 2020) (Li and Xiao, 2020) (Dementieva et al., 2020) (Arsenos and Siolas, 2020) (Ermurachi and Gifu, 2020) (Dewantara et al., 2020) (Daval-Frerot and Yannick, 2020) (Krishnamurthy et"
2020.semeval-1.186,2020.semeval-1.247,0,0.0380213,"processing Rank. Team 1. 2. 3. 4. 5. 7. 8. 9. 11. 13. 14. 16. 17. 20. 21. 22. 23. 25. 26. 27. 28. 31. 33. – Hitachi ApplicaAI aschern LTIatCMU UPB NoPropaganda CyberWallE Transformers YNUtaoxin newsSweeper PsuedoProp YNUHPCC NLFIIT TTUI BPGC DoNotDistribute UTMN syrapropa SkoltechNLP NTUAAILS UAIC1860 3218IR WMD UNTLing 1. 2. 3. 4. 5. 7. 8. 9.            Ë  Ë   Ë Ë   Ë                            Ë  Ë          Ë  Ë  Ë   Ë ËË  Ë Ë     Ë Ë Ë   Ë Ë Ë        ËË  Ë ËË  Ë 11. 13. 14. 16. 17. 20. 21. 22. Ë       Ë  Ë Ë (Tao and Zhou, 2020) (Singh et al., 2020) (Chauhan and Diddee, 2020) (Dao et al., 2020) (Martinkovic et al., 2020) (Kim and Bethard, 2020) (Patil et al., 2020) (Kranzlein et al., 2020)   Ë   (Morio et al., 2020) (Jurkiewicz et al., 2020) (Chernyavskiy et al., 2020) (Khosla et al., 2020) (Paraschiv and Cercel, 2020) (Dimov et al., 2020) (Blaschke et al., 2020) (Verma et al., 2020)         Ë 23. 25. 26. 27. 28. 31. 33. – ËËË (Mikhalkova et al., 2020) (Li and Xiao, 2020) (Dementieva et al., 2020) (Arsenos and Siolas, 2020) (Ermurachi and Gifu, 2020) (Dewantara et al., 2020) (Daval-Frerot and Yannick, 202"
2020.semeval-1.186,C18-1283,0,0.020638,"classification F1 performance on the development set. The systems are ordered based on the final ranking on the test set (cf. Table 6), whereas the ranking is the one on the development set. Columns 1 to 14 show the performance on each class (cf. Section 2). The best score for each class is bold. Rnk 1 2 8 16 5 10 11 13 7 6 23 9 26 28 17 24 20 19 18 21 3 33 29 25 30 32 34 37 36 41 43 27 4 12 14 15 22 31 35 38 39 40 42 44 45 46 47 6 Related Work Propaganda is particularly visible in the context of “fake news” on social media, which have attracted a lot of research recently (Shu et al., 2017). Thorne and Vlachos (2018) surveyed fact-checking approaches to fake news and related problems, and Li et al. (2016) focused on truth discovery in general. Two recent articles in Science offered a general discussion on the science of “fake news” (Lazer et al., 2018) and the process of proliferation of true and false news online (Vosoughi et al., 2018). We are particularly interested here in how different forms of propaganda are manifested in text. So far, the computational identification of propaganda has been tackled mostly at the article level. Rashkin et al. (2017) created a corpus, where news articles are labeled a"
2020.semeval-1.186,N18-1074,0,0.0272213,"d ELMo, or context-independent representations based on lexical, sentiment, readability, and TF-IDF features. As in the task at hand, ensembles were also popular. Still, the most successful submissions achieved an F1 -score of 24.88 only (and only 10.43 in the datathon). This is why, here we decided to split the task into subtasks in order to allow researchers to focus on one subtask at a time. Moreover, we merged some of the original 18 propaganda techniques to reduce data sparseness issues. Other related shared tasks include the FEVER 2018 and 2019 tasks on Fact Extraction and VERification (Thorne et al., 2018), the SemEval 2017 and 2019 tasks on determining the veracity of rumors (Derczynski et al., 2017; Gorrell et al., 2019) and the SemEval 2019 task on Fact-Checking in Community Question Answering Forums (Mihaylova et al., 2019). Also, the CLEF 2018–2020 CheckThat! labs’ shared tasks (Nakov et al., 2018; Elsayed et al., 2019a; Elsayed et al., 2019b; Barr´on-Cede˜no et al., 2020a; Barr´on-Cede˜no et al., 2020b), which featured tasks on automatic identification (Atanasova et al., 2018; Atanasova et al., 2019) and verification (Barr´on-Cede˜no et al., 2018; Hasanain et al., 2019; Hasanain et al., 2"
2020.semeval-1.186,2020.semeval-1.239,0,0.211588,"Ë  Ë   Ë Ë   Ë                            Ë  Ë          Ë  Ë  Ë   Ë ËË  Ë Ë     Ë Ë Ë   Ë Ë Ë        ËË  Ë ËË  Ë 11. 13. 14. 16. 17. 20. 21. 22. Ë       Ë  Ë Ë (Tao and Zhou, 2020) (Singh et al., 2020) (Chauhan and Diddee, 2020) (Dao et al., 2020) (Martinkovic et al., 2020) (Kim and Bethard, 2020) (Patil et al., 2020) (Kranzlein et al., 2020)   Ë   (Morio et al., 2020) (Jurkiewicz et al., 2020) (Chernyavskiy et al., 2020) (Khosla et al., 2020) (Paraschiv and Cercel, 2020) (Dimov et al., 2020) (Blaschke et al., 2020) (Verma et al., 2020)         Ë 23. 25. 26. 27. 28. 31. 33. – ËËË (Mikhalkova et al., 2020) (Li and Xiao, 2020) (Dementieva et al., 2020) (Arsenos and Siolas, 2020) (Ermurachi and Gifu, 2020) (Dewantara et al., 2020) (Daval-Frerot and Yannick, 2020) (Krishnamurthy et al., 2020) Table 3: Overview of the approaches to the span identification subtask. =part of the official submission; Ë=considered in internal experiments. The references to the description papers appear at the bottom. A large number of the participating teams built systems that rely heavily on engineered features. For instance, Team CyberWall"
2020.semeval-1.186,E17-1017,1,0.83519,"tained using distant supervision, assuming that all articles from a given news source share the label of that source, which introduces noise (Horne et al., 2018). Barr´on-Cede˜no et al. (2019b) experimented with a binary version of the problem: propaganda vs. no propaganda. See (Da San Martino et al., 2020a) for a recent survey on computational propaganda detection. In general, propaganda techniques serve as a means to persuade people, often in argumentative settings. While they may increase the rhetorical effectiveness of arguments, they naturally harm other aspects of argumentation quality (Wachsmuth et al., 2017). In particular, many of the span propaganda techniques considered in this shared task relate to the notion of fallacies, i.e. arguments whose reasoning is flawed in some way, often hidden and often on purpose (Tindale, 2007). Some recent work in computational argumentation has dealt with such fallacies. Among these, Habernal et al. (2018) presented and analyzed a corpus of web forum discussions with Ad hominem fallacies, and Habernal et al. (2017) introduced Argotario, a game that educates people to recognize fallacies. Argotario also had a corpus as a by-product, with 1.3k arguments annotate"
2021.acl-long.126,D19-1290,1,0.78767,", or whether a claim paraphrases another. We build syntopical graphs by transferring pretrained pairwise models, requiring no additional training data to be annotated. We decompose the problem of viewpoint reconstruction into the subtasks of stance detection and aspect detection, and evaluate the benefits of syntopical graphs — which are a collection-level approach — on both tasks. For stance detection, we use the sentential argumentation mining collection (Stab et al., 2018) and the IBM claim stance dataset (Bar-Haim et al., 2017a). For aspect detection we use the argument frames collection (Ajjour et al., 2019). We treat the graph as an input to: (a) a graph neural network architecture for stance detection, and (b) graph algorithms for unsupervised tasks such as aspect clustering. In both settings, our results show that the syntopical graph approach improves significantly over content-only baselines. The contributions of the work are two-fold: 1. A well-motivated data structure for capturing the latent structure of an argumentative corpus, the syntopical graph. 2. An instantiation of syntopical graphs that yields state-of-the-art results on stance detection and aspect detection. 2 Related Work First"
2021.acl-long.126,E17-1024,0,0.105542,"simultaneously represents relationships such as relative stance, relative specificity, or whether a claim paraphrases another. We build syntopical graphs by transferring pretrained pairwise models, requiring no additional training data to be annotated. We decompose the problem of viewpoint reconstruction into the subtasks of stance detection and aspect detection, and evaluate the benefits of syntopical graphs — which are a collection-level approach — on both tasks. For stance detection, we use the sentential argumentation mining collection (Stab et al., 2018) and the IBM claim stance dataset (Bar-Haim et al., 2017a). For aspect detection we use the argument frames collection (Ajjour et al., 2019). We treat the graph as an input to: (a) a graph neural network architecture for stance detection, and (b) graph algorithms for unsupervised tasks such as aspect clustering. In both settings, our results show that the syntopical graph approach improves significantly over content-only baselines. The contributions of the work are two-fold: 1. A well-motivated data structure for capturing the latent structure of an argumentative corpus, the syntopical graph. 2. An instantiation of syntopical graphs that yields sta"
2021.acl-long.126,W17-5104,0,0.114145,"simultaneously represents relationships such as relative stance, relative specificity, or whether a claim paraphrases another. We build syntopical graphs by transferring pretrained pairwise models, requiring no additional training data to be annotated. We decompose the problem of viewpoint reconstruction into the subtasks of stance detection and aspect detection, and evaluate the benefits of syntopical graphs — which are a collection-level approach — on both tasks. For stance detection, we use the sentential argumentation mining collection (Stab et al., 2018) and the IBM claim stance dataset (Bar-Haim et al., 2017a). For aspect detection we use the argument frames collection (Ajjour et al., 2019). We treat the graph as an input to: (a) a graph neural network architecture for stance detection, and (b) graph algorithms for unsupervised tasks such as aspect clustering. In both settings, our results show that the syntopical graph approach improves significantly over content-only baselines. The contributions of the work are two-fold: 1. A well-motivated data structure for capturing the latent structure of an argumentative corpus, the syntopical graph. 2. An instantiation of syntopical graphs that yields sta"
2021.acl-long.126,2020.emnlp-main.3,0,0.031921,"approach with a BertGCN baseline, and we ablate all considered edge types, in order to show the importance of capturing these different textual relationships. Ultimately, we seek to facilitate understanding of the main viewpoints in a text collection. Qiu and Jiang (2013) used clustering-based viewpoint discovery to study the impact of the interaction of topics and users in forum discussions. Egan et al. (2016) used multi-document summarization techniques to mine and organize the main points in a debate, and Vilares and He (2017) mined the main topics and their aspects using a Bayesian model. Bar-Haim et al. (2020) introduced the idea of keypoint analysis, grouping arguments found in a collection by the viewpoint they reflect and summarizing each group to a salient keypoint. While our graph-based analysis is likely to be suitable for finding keypoints, we instead focus on reconstructing latent viewpoints by grouping claims, leaving open the option to identify the key claims in future work as it would require manual evaluation. 3 Syntopical Graphs We now introduce the concept of a syntopical graph. The goal of our syntopical graph is to systematically model the salient interactions of all claims in a col"
2021.acl-long.126,P19-3022,0,0.0191178,"order to help individuals make sense of a collection of documents for a given topic. Viewed through the lens of computational argumentation, these documents state claims or conclusions that can be grouped by the aspects of the topic they discuss as well as by the stance they convey towards the topic (Stede and Schneider, 2018). An individual aiming to form a thorough understanding of the topic needs to get an overview of these viewpoints and their interactions. This may be hard even if adequate tool support for browsing the collection is available (Wachsmuth et al., 2017a; Stab et al., 2018; Chen et al., 2019). We seek to enable systems that are capable of reconstructing viewpoints within a collection, where a viewpoint is expressed as a triple V = (topic, aspect, stance). We consider the argumentative unit of a claim to be the minimal expression of a viewpoint in natural language, such that a single viewpoint can have many claims expressing it. As an example, consider the following two claims: “Nuclear energy emits zero CO2 .” “Nuclear can provide a clean baseload, eliminating the need for fracking and coal mining.” Within a collection these claims express: V = (Nuclear Energy, env. impact, PRO) T"
2021.acl-long.126,C04-1051,0,0.324634,"nodes can have multiple edges of different types between them; a claim can both contradict and refute another claim, for instance. Edge Weights An edge can have a real-valued weight associated with it on the range (−1, 1), representing the strength of the connection. The relative stance edge between a claim which strongly refutes another would receive a weight close to −1. 3.2 Graph Construction For graph edges, we combine four pretrained models and two similarity measures. The pretrained edge types are: relative stance and relative specificity from Durmus et al. (2019), paraphrase edges from Dolan et al. (2004); Morris et al. (2020), and natural language inference edges from Williams et al. (2018); Liu et al. (2019). The edge weights are the confidence scores defined by weight(u, v, r) = ppos(u,v) − pneg(u,v) , where u and v are claims, r is the relation type, and ppos(u,v) is the probability of a positive association between the claims (e.g., “is a paraphrase” or “does entail”), pneg(u,v) for a negative one. For similarity-based edges, we use standard TF-IDF for term-based similarity and LDA for topic-based similarity (Blei et al., 2003), using cosine similarity as the edge weight. The document-cla"
2021.acl-long.126,P19-1456,0,0.0971144,"lated claims and the sentiment towards these aspects. From this information, they derived stance based on the contrastiveness of the aspects. Later, Bar-Haim et al. (2017b) mod1584 eled the context of a claim to account for cases without sentiment. Our work follows up on and generalizes this idea, systematically incorporating implicit and explicit structure induced by the topics, aspects, claims, and participants in a debate. In a similar vein, Li et al. (2018) embedded debate posts and authors jointly based on their interactions, in order to classify a post’s stance towards the debate topic. Durmus et al. (2019) encoded related pairs of claims using BERT to predict the stance and specificity of any claim in a complex structure of online debates. However, neither of these exploited the full graph structure resulting from all the relations and interactions in a debate, which is the gap we fill in this paper. Sridhar et al. (2015) model collective information about debate posts, authors, and their agreement and disagreement using probabilistic soft logic. Whereas they are restricted to the structure available in a forum, our approach can in principle be applied to arbitrary collections of text. We also"
2021.acl-long.126,W16-2816,0,0.0149183,"ce- or documentlevel classification. Our work generalizes this approach, focusing on incorporating many edge types with different meanings, such as relative stance or relative specificity. We compare our approach with a BertGCN baseline, and we ablate all considered edge types, in order to show the importance of capturing these different textual relationships. Ultimately, we seek to facilitate understanding of the main viewpoints in a text collection. Qiu and Jiang (2013) used clustering-based viewpoint discovery to study the impact of the interaction of topics and users in forum discussions. Egan et al. (2016) used multi-document summarization techniques to mine and organize the main points in a debate, and Vilares and He (2017) mined the main topics and their aspects using a Bayesian model. Bar-Haim et al. (2020) introduced the idea of keypoint analysis, grouping arguments found in a collection by the viewpoint they reflect and summarizing each group to a salient keypoint. While our graph-based analysis is likely to be suitable for finding keypoints, we instead focus on reconstructing latent viewpoints by grouping claims, leaving open the option to identify the key claims in future work as it woul"
2021.acl-long.126,P19-1049,0,0.0191126,"ose argument frames dataset we use, instead clustered aspects with Latent Semantic Analysis (LSA) and topic modeling. But, in general, aspects might not be mentioned in a text explicitly. Therefore, we follow these other approaches, treating the task as a clustering problem. Unlike them, however, we do not model only the content and linguistic structure of texts, but we combine them with the debate structure. Different types of argumentation graphs have been proposed, covering expert-stance information (Toledo-Ronen et al., 2016), basic argument and debate structure (Peldszus and Stede, 2015; Gemechu and Reed, 2019), specific effect relations (Al-Khatib et al., 2020; Kobbe et al., 2020), social media graphs (Aldayel and Magdy, 2019), and knowledge graphs (Zhang et al., 2020). Our main focus is not learning to construct ground-truth graphs, but how to use an approximated graph to derive properties such as stance and aspect. Our work resembles approaches that derive the relevance of arguments (Wachsmuth et al., 2017b) or their centrality and divisiveness in a discussion (Lawrence and Reed, 2017) from respective graphs. Sawhney et al. (2020) used a neural graph attention network to classify speech stance ba"
2021.acl-long.126,I13-1191,0,0.0251487,"as aspect clustering. In both settings, our results show that the syntopical graph approach improves significantly over content-only baselines. The contributions of the work are two-fold: 1. A well-motivated data structure for capturing the latent structure of an argumentative corpus, the syntopical graph. 2. An instantiation of syntopical graphs that yields state-of-the-art results on stance detection and aspect detection. 2 Related Work First attempts at stance detection used contentoriented features (Somasundaran and Wiebe, 2009). Later approaches, such as those by Ranade et al. (2013) and Hasan and Ng (2013), exploited common patterns in dialogic structure to improve stance detection. More tailored to argumentation, BarHaim et al. (2017a) first identified the aspects of a discussed topic in two related claims and the sentiment towards these aspects. From this information, they derived stance based on the contrastiveness of the aspects. Later, Bar-Haim et al. (2017b) mod1584 eled the context of a claim to account for cases without sentiment. Our work follows up on and generalizes this idea, systematically incorporating implicit and explicit structure induced by the topics, aspects, claims, and par"
2021.acl-long.126,2020.emnlp-main.4,0,0.0759605,"Missing"
2021.acl-long.126,W17-5114,0,0.0222256,"rt-stance information (Toledo-Ronen et al., 2016), basic argument and debate structure (Peldszus and Stede, 2015; Gemechu and Reed, 2019), specific effect relations (Al-Khatib et al., 2020; Kobbe et al., 2020), social media graphs (Aldayel and Magdy, 2019), and knowledge graphs (Zhang et al., 2020). Our main focus is not learning to construct ground-truth graphs, but how to use an approximated graph to derive properties such as stance and aspect. Our work resembles approaches that derive the relevance of arguments (Wachsmuth et al., 2017b) or their centrality and divisiveness in a discussion (Lawrence and Reed, 2017) from respective graphs. Sawhney et al. (2020) used a neural graph attention network to classify speech stance based on a graph with texts, speakers, and topics as nodes. While we also use a relational graph convolutional network for learning, the graph we propose captures implicit claim relations as well as explicit structure. In addition, text-based graph neural models have been proposed to facilitate classification, such as TextGCN (Yao et al., 2019) as well as the followup work BertGCN (Lin et al., 2021). These approaches build a graph over terms (using normalized mutual information for ed"
2021.acl-long.126,C18-1316,0,0.0233389,"ructure to improve stance detection. More tailored to argumentation, BarHaim et al. (2017a) first identified the aspects of a discussed topic in two related claims and the sentiment towards these aspects. From this information, they derived stance based on the contrastiveness of the aspects. Later, Bar-Haim et al. (2017b) mod1584 eled the context of a claim to account for cases without sentiment. Our work follows up on and generalizes this idea, systematically incorporating implicit and explicit structure induced by the topics, aspects, claims, and participants in a debate. In a similar vein, Li et al. (2018) embedded debate posts and authors jointly based on their interactions, in order to classify a post’s stance towards the debate topic. Durmus et al. (2019) encoded related pairs of claims using BERT to predict the stance and specificity of any claim in a complex structure of online debates. However, neither of these exploited the full graph structure resulting from all the relations and interactions in a debate, which is the gap we fill in this paper. Sridhar et al. (2015) model collective information about debate posts, authors, and their agreement and disagreement using probabilistic soft lo"
2021.acl-long.126,2021.findings-acl.126,0,0.137173,"(Wachsmuth et al., 2017b) or their centrality and divisiveness in a discussion (Lawrence and Reed, 2017) from respective graphs. Sawhney et al. (2020) used a neural graph attention network to classify speech stance based on a graph with texts, speakers, and topics as nodes. While we also use a relational graph convolutional network for learning, the graph we propose captures implicit claim relations as well as explicit structure. In addition, text-based graph neural models have been proposed to facilitate classification, such as TextGCN (Yao et al., 2019) as well as the followup work BertGCN (Lin et al., 2021). These approaches build a graph over terms (using normalized mutual information for edge weights) as well as sentences and documents (using TF-IDF for edge weights) to improve sentence- or documentlevel classification. Our work generalizes this approach, focusing on incorporating many edge types with different meanings, such as relative stance or relative specificity. We compare our approach with a BertGCN baseline, and we ablate all considered edge types, in order to show the importance of capturing these different textual relationships. Ultimately, we seek to facilitate understanding of the"
2021.acl-long.126,2021.ccl-1.108,0,0.0468629,"Missing"
2021.acl-long.126,N15-1046,0,0.0300103,"s. However, neither of these exploited the full graph structure resulting from all the relations and interactions in a debate, which is the gap we fill in this paper. Sridhar et al. (2015) model collective information about debate posts, authors, and their agreement and disagreement using probabilistic soft logic. Whereas they are restricted to the structure available in a forum, our approach can in principle be applied to arbitrary collections of text. We also tackle aspect detection, which may at first seem more content-oriented in nature. Accordingly, previous research such as the works of Misra et al. (2015) and Reimers et al. (2019b) employed word-based features or contextualized word embeddings for topic-specific aspect clustering. Ajjour et al. (2019), whose argument frames dataset we use, instead clustered aspects with Latent Semantic Analysis (LSA) and topic modeling. But, in general, aspects might not be mentioned in a text explicitly. Therefore, we follow these other approaches, treating the task as a clustering problem. Unlike them, however, we do not model only the content and linguistic structure of texts, but we combine them with the debate structure. Different types of argumentation g"
2021.acl-long.126,2020.emnlp-demos.16,0,0.0599366,"Missing"
2021.acl-long.126,D15-1110,0,0.0280648,". Ajjour et al. (2019), whose argument frames dataset we use, instead clustered aspects with Latent Semantic Analysis (LSA) and topic modeling. But, in general, aspects might not be mentioned in a text explicitly. Therefore, we follow these other approaches, treating the task as a clustering problem. Unlike them, however, we do not model only the content and linguistic structure of texts, but we combine them with the debate structure. Different types of argumentation graphs have been proposed, covering expert-stance information (Toledo-Ronen et al., 2016), basic argument and debate structure (Peldszus and Stede, 2015; Gemechu and Reed, 2019), specific effect relations (Al-Khatib et al., 2020; Kobbe et al., 2020), social media graphs (Aldayel and Magdy, 2019), and knowledge graphs (Zhang et al., 2020). Our main focus is not learning to construct ground-truth graphs, but how to use an approximated graph to derive properties such as stance and aspect. Our work resembles approaches that derive the relevance of arguments (Wachsmuth et al., 2017b) or their centrality and divisiveness in a discussion (Lawrence and Reed, 2017) from respective graphs. Sawhney et al. (2020) used a neural graph attention network to"
2021.acl-long.126,N13-1123,0,0.0269051,"erms (using normalized mutual information for edge weights) as well as sentences and documents (using TF-IDF for edge weights) to improve sentence- or documentlevel classification. Our work generalizes this approach, focusing on incorporating many edge types with different meanings, such as relative stance or relative specificity. We compare our approach with a BertGCN baseline, and we ablate all considered edge types, in order to show the importance of capturing these different textual relationships. Ultimately, we seek to facilitate understanding of the main viewpoints in a text collection. Qiu and Jiang (2013) used clustering-based viewpoint discovery to study the impact of the interaction of topics and users in forum discussions. Egan et al. (2016) used multi-document summarization techniques to mine and organize the main points in a debate, and Vilares and He (2017) mined the main topics and their aspects using a Bayesian model. Bar-Haim et al. (2020) introduced the idea of keypoint analysis, grouping arguments found in a collection by the viewpoint they reflect and summarizing each group to a salient keypoint. While our graph-based analysis is likely to be suitable for finding keypoints, we inst"
2021.acl-long.126,W13-4008,0,0.0304941,"unsupervised tasks such as aspect clustering. In both settings, our results show that the syntopical graph approach improves significantly over content-only baselines. The contributions of the work are two-fold: 1. A well-motivated data structure for capturing the latent structure of an argumentative corpus, the syntopical graph. 2. An instantiation of syntopical graphs that yields state-of-the-art results on stance detection and aspect detection. 2 Related Work First attempts at stance detection used contentoriented features (Somasundaran and Wiebe, 2009). Later approaches, such as those by Ranade et al. (2013) and Hasan and Ng (2013), exploited common patterns in dialogic structure to improve stance detection. More tailored to argumentation, BarHaim et al. (2017a) first identified the aspects of a discussed topic in two related claims and the sentiment towards these aspects. From this information, they derived stance based on the contrastiveness of the aspects. Later, Bar-Haim et al. (2017b) mod1584 eled the context of a claim to account for cases without sentiment. Our work follows up on and generalizes this idea, systematically incorporating implicit and explicit structure induced by the topics,"
2021.acl-long.126,D19-1410,0,0.0877671,"hese exploited the full graph structure resulting from all the relations and interactions in a debate, which is the gap we fill in this paper. Sridhar et al. (2015) model collective information about debate posts, authors, and their agreement and disagreement using probabilistic soft logic. Whereas they are restricted to the structure available in a forum, our approach can in principle be applied to arbitrary collections of text. We also tackle aspect detection, which may at first seem more content-oriented in nature. Accordingly, previous research such as the works of Misra et al. (2015) and Reimers et al. (2019b) employed word-based features or contextualized word embeddings for topic-specific aspect clustering. Ajjour et al. (2019), whose argument frames dataset we use, instead clustered aspects with Latent Semantic Analysis (LSA) and topic modeling. But, in general, aspects might not be mentioned in a text explicitly. Therefore, we follow these other approaches, treating the task as a clustering problem. Unlike them, however, we do not model only the content and linguistic structure of texts, but we combine them with the debate structure. Different types of argumentation graphs have been proposed,"
2021.acl-long.126,P19-1054,0,0.105891,"hese exploited the full graph structure resulting from all the relations and interactions in a debate, which is the gap we fill in this paper. Sridhar et al. (2015) model collective information about debate posts, authors, and their agreement and disagreement using probabilistic soft logic. Whereas they are restricted to the structure available in a forum, our approach can in principle be applied to arbitrary collections of text. We also tackle aspect detection, which may at first seem more content-oriented in nature. Accordingly, previous research such as the works of Misra et al. (2015) and Reimers et al. (2019b) employed word-based features or contextualized word embeddings for topic-specific aspect clustering. Ajjour et al. (2019), whose argument frames dataset we use, instead clustered aspects with Latent Semantic Analysis (LSA) and topic modeling. But, in general, aspects might not be mentioned in a text explicitly. Therefore, we follow these other approaches, treating the task as a clustering problem. Unlike them, however, we do not model only the content and linguistic structure of texts, but we combine them with the debate structure. Different types of argumentation graphs have been proposed,"
2021.acl-long.126,2020.coling-main.426,0,0.0250927,", basic argument and debate structure (Peldszus and Stede, 2015; Gemechu and Reed, 2019), specific effect relations (Al-Khatib et al., 2020; Kobbe et al., 2020), social media graphs (Aldayel and Magdy, 2019), and knowledge graphs (Zhang et al., 2020). Our main focus is not learning to construct ground-truth graphs, but how to use an approximated graph to derive properties such as stance and aspect. Our work resembles approaches that derive the relevance of arguments (Wachsmuth et al., 2017b) or their centrality and divisiveness in a discussion (Lawrence and Reed, 2017) from respective graphs. Sawhney et al. (2020) used a neural graph attention network to classify speech stance based on a graph with texts, speakers, and topics as nodes. While we also use a relational graph convolutional network for learning, the graph we propose captures implicit claim relations as well as explicit structure. In addition, text-based graph neural models have been proposed to facilitate classification, such as TextGCN (Yao et al., 2019) as well as the followup work BertGCN (Lin et al., 2021). These approaches build a graph over terms (using normalized mutual information for edge weights) as well as sentences and documents"
2021.acl-long.126,P09-1026,0,0.0669409,"rk architecture for stance detection, and (b) graph algorithms for unsupervised tasks such as aspect clustering. In both settings, our results show that the syntopical graph approach improves significantly over content-only baselines. The contributions of the work are two-fold: 1. A well-motivated data structure for capturing the latent structure of an argumentative corpus, the syntopical graph. 2. An instantiation of syntopical graphs that yields state-of-the-art results on stance detection and aspect detection. 2 Related Work First attempts at stance detection used contentoriented features (Somasundaran and Wiebe, 2009). Later approaches, such as those by Ranade et al. (2013) and Hasan and Ng (2013), exploited common patterns in dialogic structure to improve stance detection. More tailored to argumentation, BarHaim et al. (2017a) first identified the aspects of a discussed topic in two related claims and the sentiment towards these aspects. From this information, they derived stance based on the contrastiveness of the aspects. Later, Bar-Haim et al. (2017b) mod1584 eled the context of a claim to account for cases without sentiment. Our work follows up on and generalizes this idea, systematically incorporatin"
2021.acl-long.126,W16-2814,0,0.0237338,"ntextualized word embeddings for topic-specific aspect clustering. Ajjour et al. (2019), whose argument frames dataset we use, instead clustered aspects with Latent Semantic Analysis (LSA) and topic modeling. But, in general, aspects might not be mentioned in a text explicitly. Therefore, we follow these other approaches, treating the task as a clustering problem. Unlike them, however, we do not model only the content and linguistic structure of texts, but we combine them with the debate structure. Different types of argumentation graphs have been proposed, covering expert-stance information (Toledo-Ronen et al., 2016), basic argument and debate structure (Peldszus and Stede, 2015; Gemechu and Reed, 2019), specific effect relations (Al-Khatib et al., 2020; Kobbe et al., 2020), social media graphs (Aldayel and Magdy, 2019), and knowledge graphs (Zhang et al., 2020). Our main focus is not learning to construct ground-truth graphs, but how to use an approximated graph to derive properties such as stance and aspect. Our work resembles approaches that derive the relevance of arguments (Wachsmuth et al., 2017b) or their centrality and divisiveness in a discussion (Lawrence and Reed, 2017) from respective graphs."
2021.acl-long.126,2020.argmining-1.5,0,0.0354702,"uage inference model that predicts whether the claim entails the topic. We initialize the document representations with a sentence vectorizer over the text of the document. 4 Viewpoint Reconstruction A viewpoint can be understood as a judgment of some aspect of a topic that conveys a stance towards the topic. The goal of viewpoint reconstruction is to identify the set of viewpoints in a collection given a topic, starting with the claims. An example of this process is shown on the right in Figure 1. To denote viewpoints, we borrow notation in line with the idea of aspect-based argument mining (Trautmann, 2020), which in turn was inspired by aspectbased sentiment analysis. In particular, we express a viewpoint as a triple V : V = (topic, aspect, stance) A claim is an expression of a viewpoint in natural language, and a single viewpoint can be expressed in several ways throughout a collection in many claims. Aspects are facets of the broader argument around the topic. While some actual claims may encode multiple viewpoints simultaneously, henceforth we consider each claim to encode one viewpoint for simplicity. To tackle viewpoint reconstruction computationally, we decompose it into two sub-tasks, st"
2021.acl-long.126,D17-1165,0,0.0242068,"h different meanings, such as relative stance or relative specificity. We compare our approach with a BertGCN baseline, and we ablate all considered edge types, in order to show the importance of capturing these different textual relationships. Ultimately, we seek to facilitate understanding of the main viewpoints in a text collection. Qiu and Jiang (2013) used clustering-based viewpoint discovery to study the impact of the interaction of topics and users in forum discussions. Egan et al. (2016) used multi-document summarization techniques to mine and organize the main points in a debate, and Vilares and He (2017) mined the main topics and their aspects using a Bayesian model. Bar-Haim et al. (2020) introduced the idea of keypoint analysis, grouping arguments found in a collection by the viewpoint they reflect and summarizing each group to a salient keypoint. While our graph-based analysis is likely to be suitable for finding keypoints, we instead focus on reconstructing latent viewpoints by grouping claims, leaving open the option to identify the key claims in future work as it would require manual evaluation. 3 Syntopical Graphs We now introduce the concept of a syntopical graph. The goal of our synt"
2021.acl-long.126,W17-5106,1,0.932597,"yntopical reading process computationally in order to help individuals make sense of a collection of documents for a given topic. Viewed through the lens of computational argumentation, these documents state claims or conclusions that can be grouped by the aspects of the topic they discuss as well as by the stance they convey towards the topic (Stede and Schneider, 2018). An individual aiming to form a thorough understanding of the topic needs to get an overview of these viewpoints and their interactions. This may be hard even if adequate tool support for browsing the collection is available (Wachsmuth et al., 2017a; Stab et al., 2018; Chen et al., 2019). We seek to enable systems that are capable of reconstructing viewpoints within a collection, where a viewpoint is expressed as a triple V = (topic, aspect, stance). We consider the argumentative unit of a claim to be the minimal expression of a viewpoint in natural language, such that a single viewpoint can have many claims expressing it. As an example, consider the following two claims: “Nuclear energy emits zero CO2 .” “Nuclear can provide a clean baseload, eliminating the need for fracking and coal mining.” Within a collection these claims express:"
2021.acl-long.126,E17-1105,1,0.929104,"yntopical reading process computationally in order to help individuals make sense of a collection of documents for a given topic. Viewed through the lens of computational argumentation, these documents state claims or conclusions that can be grouped by the aspects of the topic they discuss as well as by the stance they convey towards the topic (Stede and Schneider, 2018). An individual aiming to form a thorough understanding of the topic needs to get an overview of these viewpoints and their interactions. This may be hard even if adequate tool support for browsing the collection is available (Wachsmuth et al., 2017a; Stab et al., 2018; Chen et al., 2019). We seek to enable systems that are capable of reconstructing viewpoints within a collection, where a viewpoint is expressed as a triple V = (topic, aspect, stance). We consider the argumentative unit of a claim to be the minimal expression of a viewpoint in natural language, such that a single viewpoint can have many claims expressing it. As an example, consider the following two claims: “Nuclear energy emits zero CO2 .” “Nuclear can provide a clean baseload, eliminating the need for fracking and coal mining.” Within a collection these claims express:"
2021.acl-long.126,N18-1101,0,0.0323565,"adict and refute another claim, for instance. Edge Weights An edge can have a real-valued weight associated with it on the range (−1, 1), representing the strength of the connection. The relative stance edge between a claim which strongly refutes another would receive a weight close to −1. 3.2 Graph Construction For graph edges, we combine four pretrained models and two similarity measures. The pretrained edge types are: relative stance and relative specificity from Durmus et al. (2019), paraphrase edges from Dolan et al. (2004); Morris et al. (2020), and natural language inference edges from Williams et al. (2018); Liu et al. (2019). The edge weights are the confidence scores defined by weight(u, v, r) = ppos(u,v) − pneg(u,v) , where u and v are claims, r is the relation type, and ppos(u,v) is the probability of a positive association between the claims (e.g., “is a paraphrase” or “does entail”), pneg(u,v) for a negative one. For similarity-based edges, we use standard TF-IDF for term-based similarity and LDA for topic-based similarity (Blei et al., 2003), using cosine similarity as the edge weight. The document-claim edges have a single type, contains, with an edge weight of 1. We compute each of the"
2021.acl-long.126,2020.acl-main.291,0,0.0143002,"in a text explicitly. Therefore, we follow these other approaches, treating the task as a clustering problem. Unlike them, however, we do not model only the content and linguistic structure of texts, but we combine them with the debate structure. Different types of argumentation graphs have been proposed, covering expert-stance information (Toledo-Ronen et al., 2016), basic argument and debate structure (Peldszus and Stede, 2015; Gemechu and Reed, 2019), specific effect relations (Al-Khatib et al., 2020; Kobbe et al., 2020), social media graphs (Aldayel and Magdy, 2019), and knowledge graphs (Zhang et al., 2020). Our main focus is not learning to construct ground-truth graphs, but how to use an approximated graph to derive properties such as stance and aspect. Our work resembles approaches that derive the relevance of arguments (Wachsmuth et al., 2017b) or their centrality and divisiveness in a discussion (Lawrence and Reed, 2017) from respective graphs. Sawhney et al. (2020) used a neural graph attention network to classify speech stance based on a graph with texts, speakers, and topics as nodes. While we also use a relational graph convolutional network for learning, the graph we propose captures i"
2021.acl-long.366,2020.findings-emnlp.47,0,0.171872,"ng the graphs’ knowledge into debate portal texts for generating arguments with superior quality than those generated without knowledge. 1 Introduction Arguments are our means to build stances on controversial topics, to persuade others, or to negotiate. Automatic argument generation has the potential to effectively support such tasks: it may not only regenerate known arguments but also uncover new facets of a topic. Existing argument generation approaches work either in an end-to-end fashion (Hua and Wang, 2018) or they are controlled with respect to the argument’s topic, aspects, or stance (Gretz et al., 2020; Schiller et al., 2021). In contrast, no approach integrates external knowledge into the generation process so far, even though knowledge graphs have been shown to be useful for supporting text generation models in other areas (KoncelKedziorski et al., 2019a; Ribeiro et al., 2020). Previous research has proposed argumentation knowledge graphs (AKGs) that model supporting and attacking interactions between concepts (AlKhatib et al., 2020). Such an AKG may assist argument generation models in different ways. For example, meaningful prompts on controversial topics can be constructed from an AKG"
2021.acl-long.366,2021.eacl-main.17,1,0.712023,"neural language models. Hidey and McKeown (2019) built a sequenceto-sequence model to rewrite claims into opposing claims, and Hua et al. (2019) presented a sophisticated approach that, given a stance on a controversial topic, combines retrieval with neural generation techniques to create full arguments with the opposite stance. Gretz et al. (2020) developed a transformer-based pipeline to generate coherent 4751 and plausible claims, whereas Schiller et al. (2021) proposed a language model that controls argument generation on a fine-grained level for a given topic, stance, and aspect. Lastly, Alshomary et al. (2021) generated belief-based claims, encoding the beliefs via conditional language models. Most similar to our work are the studies of Gretz et al. (2020) and Schiller et al. (2021). Like us, the former also exploits the power of GPT-2, adding context to the model’s training data. The latter is comparable in that it attempts to steer the generation towards aspect-specific arguments. To the best of our knowledge, however, our approach is the first to employ external knowledge from knowledge graphs for the task of argument generation. Argumentation Knowledge Graphs Besides the argumentation knowledge"
2021.acl-long.366,N19-1174,0,0.0187107,"his section, we outline related studies on argument generation, argumentation knowledge graphs, and graph-to-text generation. Argument Generation Different approaches to the generation of arguments, or of components thereof, have been proposed in the last years. To create new claims, Bilu and Slonim (2016) recomposed predicates from existing claims with new topics. El Baff et al. (2019) composed complete arguments from given claims following specific rhetorical strategies based on the theoretical model of Wachsmuth et al. (2018). Unlike these approaches, we make use of neural language models. Hidey and McKeown (2019) built a sequenceto-sequence model to rewrite claims into opposing claims, and Hua et al. (2019) presented a sophisticated approach that, given a stance on a controversial topic, combines retrieval with neural generation techniques to create full arguments with the opposite stance. Gretz et al. (2020) developed a transformer-based pipeline to generate coherent 4751 and plausible claims, whereas Schiller et al. (2021) proposed a language model that controls argument generation on a fine-grained level for a given topic, stance, and aspect. Lastly, Alshomary et al. (2021) generated belief-based c"
2021.acl-long.366,P16-2085,0,0.0204118,"astly, since our approach is meant as a proof of concept, we used the small GPT-2 model with the parameters adopted from Gretz et al. (2020). Using a larger model and exploring different sampling methods and parameter settings will probably result in a higher quality of the arguments generated. 5 Related Work In this section, we outline related studies on argument generation, argumentation knowledge graphs, and graph-to-text generation. Argument Generation Different approaches to the generation of arguments, or of components thereof, have been proposed in the last years. To create new claims, Bilu and Slonim (2016) recomposed predicates from existing claims with new topics. El Baff et al. (2019) composed complete arguments from given claims following specific rhetorical strategies based on the theoretical model of Wachsmuth et al. (2018). Unlike these approaches, we make use of neural language models. Hidey and McKeown (2019) built a sequenceto-sequence model to rewrite claims into opposing claims, and Hua et al. (2019) presented a sophisticated approach that, given a stance on a controversial topic, combines retrieval with neural generation techniques to create full arguments with the opposite stance."
2021.acl-long.366,P19-1255,0,0.0193515,"ph-to-text generation. Argument Generation Different approaches to the generation of arguments, or of components thereof, have been proposed in the last years. To create new claims, Bilu and Slonim (2016) recomposed predicates from existing claims with new topics. El Baff et al. (2019) composed complete arguments from given claims following specific rhetorical strategies based on the theoretical model of Wachsmuth et al. (2018). Unlike these approaches, we make use of neural language models. Hidey and McKeown (2019) built a sequenceto-sequence model to rewrite claims into opposing claims, and Hua et al. (2019) presented a sophisticated approach that, given a stance on a controversial topic, combines retrieval with neural generation techniques to create full arguments with the opposite stance. Gretz et al. (2020) developed a transformer-based pipeline to generate coherent 4751 and plausible claims, whereas Schiller et al. (2021) proposed a language model that controls argument generation on a fine-grained level for a given topic, stance, and aspect. Lastly, Alshomary et al. (2021) generated belief-based claims, encoding the beliefs via conditional language models. Most similar to our work are the st"
2021.acl-long.366,2020.findings-emnlp.428,0,0.0211573,"ith the type of effect relation between them as keyphrases separated by special tokens. We use ‘positive’ and ‘negative’ to represent the effect relations. For example, the paragraph “Animal studies suggests marijuana causes physical dependence, and serious problems” will be transformed into: 4747 “&lt;|startoftext|>’[’marijuana»positive»physicaldependence’, ’mariguana»positive»problems’] @ Animal studies suggests ...’&lt;|endoftext|>” While this way of matching and encoding has limitations, it has shown good results in practice when used with pre-trained neural models (Witteveen and Andrews, 2019; Cachola et al., 2020). 3.3 Neural Language Model Fine-tuning We use our text-knowledge encoding dataset to fine-tune the GPT-2 neural language model (Radford et al., 2019) for argument generation. Since GPT-2 cannot deal with graph structure as input directly, we fine-tune it on all paragraphs, including those with encoded relations as textual representations (i.e., keyphrases). We expect to thereby leverage the powerful generation capabilities of GPT-2 while biasing it to generate texts related to the encoded relations. It is worth noting that, in training, we encode multiple relations at once and the generated a"
2021.acl-long.366,P18-1021,0,0.0125581,", including argumentativeness and plausibility. The results demonstrate the positive impact of encoding the graphs’ knowledge into debate portal texts for generating arguments with superior quality than those generated without knowledge. 1 Introduction Arguments are our means to build stances on controversial topics, to persuade others, or to negotiate. Automatic argument generation has the potential to effectively support such tasks: it may not only regenerate known arguments but also uncover new facets of a topic. Existing argument generation approaches work either in an end-to-end fashion (Hua and Wang, 2018) or they are controlled with respect to the argument’s topic, aspects, or stance (Gretz et al., 2020; Schiller et al., 2021). In contrast, no approach integrates external knowledge into the generation process so far, even though knowledge graphs have been shown to be useful for supporting text generation models in other areas (KoncelKedziorski et al., 2019a; Ribeiro et al., 2020). Previous research has proposed argumentation knowledge graphs (AKGs) that model supporting and attacking interactions between concepts (AlKhatib et al., 2020). Such an AKG may assist argument generation models in dif"
2021.acl-long.366,D14-1125,0,0.0475823,"Missing"
2021.acl-long.366,W19-8607,1,0.902254,"Missing"
2021.acl-long.366,P11-1099,0,0.0235406,"ch controlled argument generation, investigating for the first time the ability to generate high-quality and content-rich arguments by integrating knowledge from AKGs into standard neural-based generation models. To this end, we exploit multiple manually and automatically created knowledge graphs, devoting particular attention to causal knowledge (Al-Khatib et al., 2020; Heindorf et al., 2020). Causality plays a major role in argumentation due to its frequent usage in real-life discussions; argument from cause to effect and argument from consequences are frequently used argumentation schemes (Feng and Hirst, 2011; Reisert et al., 2018). To utilize AKGs for argument generation, we collect argumentative texts from diverse sources such as online debate portals. In these texts, we find arguments that contain instances of the knowledge covered in the graphs. We encode this knowledge as keyphrases in the arguments. Unlike Gretz et al. (2020) and Schiller et al. (2021), our keyphrases cover multiple aspects and stances related to the same topic. The resulting texts are used to finetune a transformer-based generation model, GPT-2 (Radford et al., 2019). The underlying hypothesis is 4744 Proceedings of the 59t"
2021.acl-long.366,P19-1049,0,0.0282992,"of Gretz et al. (2020) and Schiller et al. (2021). Like us, the former also exploits the power of GPT-2, adding context to the model’s training data. The latter is comparable in that it attempts to steer the generation towards aspect-specific arguments. To the best of our knowledge, however, our approach is the first to employ external knowledge from knowledge graphs for the task of argument generation. Argumentation Knowledge Graphs Besides the argumentation knowledge graph of Al-Khatib et al. (2020), Toledo-Ronen et al. (2016) created an expert stance graph to support stance classification. Gemechu and Reed (2019) encoded the relations between segments of an argument into a graph and demonstrated the graph’s effectiveness for argument mining. In our work, we utilize one of the available graphs, among others, using its knowledge to control the argument generation process. Closely related to argumentation knowledge, causality graphs gained some attention recently. While general knowledge bases such as ConceptNet (Speer et al., 2017) contain causal knowledge, the causality graph of Heindorf et al. (2020) that we utilized is the largest source of causal knowledge, exceeding others by orders of magnitude. G"
2021.acl-long.366,N19-1238,0,0.0570514,"Missing"
2021.acl-long.366,N19-1236,0,0.0645107,"Missing"
2021.acl-long.366,W18-5210,0,0.0233569,"generation, investigating for the first time the ability to generate high-quality and content-rich arguments by integrating knowledge from AKGs into standard neural-based generation models. To this end, we exploit multiple manually and automatically created knowledge graphs, devoting particular attention to causal knowledge (Al-Khatib et al., 2020; Heindorf et al., 2020). Causality plays a major role in argumentation due to its frequent usage in real-life discussions; argument from cause to effect and argument from consequences are frequently used argumentation schemes (Feng and Hirst, 2011; Reisert et al., 2018). To utilize AKGs for argument generation, we collect argumentative texts from diverse sources such as online debate portals. In these texts, we find arguments that contain instances of the knowledge covered in the graphs. We encode this knowledge as keyphrases in the arguments. Unlike Gretz et al. (2020) and Schiller et al. (2021), our keyphrases cover multiple aspects and stances related to the same topic. The resulting texts are used to finetune a transformer-based generation model, GPT-2 (Radford et al., 2019). The underlying hypothesis is 4744 Proceedings of the 59th Annual Meeting of the"
2021.acl-long.366,2021.naacl-main.34,0,0.454671,"edge into debate portal texts for generating arguments with superior quality than those generated without knowledge. 1 Introduction Arguments are our means to build stances on controversial topics, to persuade others, or to negotiate. Automatic argument generation has the potential to effectively support such tasks: it may not only regenerate known arguments but also uncover new facets of a topic. Existing argument generation approaches work either in an end-to-end fashion (Hua and Wang, 2018) or they are controlled with respect to the argument’s topic, aspects, or stance (Gretz et al., 2020; Schiller et al., 2021). In contrast, no approach integrates external knowledge into the generation process so far, even though knowledge graphs have been shown to be useful for supporting text generation models in other areas (KoncelKedziorski et al., 2019a; Ribeiro et al., 2020). Previous research has proposed argumentation knowledge graphs (AKGs) that model supporting and attacking interactions between concepts (AlKhatib et al., 2020). Such an AKG may assist argument generation models in different ways. For example, meaningful prompts on controversial topics can be constructed from an AKG with simple hand-defined"
2021.acl-long.366,2020.argmining-1.9,1,0.730117,"l use the keyphrases to constrain the generation of arguments. During application, we provide the model with knowledge (as keyphrases) to obtain new arguments that further elaborate the knowledge. Figure 1 gives an overview of the main steps of our approach. We evaluate the ability of our approach to generating new arguments for a variety of claim-like prompts: 400 generated arguments are manually assessed for their relevance to the prompt, argumentativeness, content richness, and plausibility. As a recent study indicates the adoption of bias from argumentative source data in word embeddings (Spliethöver and Wachsmuth, 2020), we also inspect potential social bias and abusive language in the generated arguments. Moreover, we evaluate the generated arguments automatically using recently developed argument mining techniques, in order to then examine correlations between manual and automatic evaluations. The results reveal an evident benefit of using the graphs’ knowledge in generating controlled arguments that are rich in content and plausible. However, we also observe the presence of social bias in the outputs of GPT-2, suggesting the need for careful postproceeing step in argument generation. Both the resources an"
2021.acl-long.366,N18-5005,0,0.0430293,"Missing"
2021.acl-long.366,W16-2814,0,0.0288407,"oding the beliefs via conditional language models. Most similar to our work are the studies of Gretz et al. (2020) and Schiller et al. (2021). Like us, the former also exploits the power of GPT-2, adding context to the model’s training data. The latter is comparable in that it attempts to steer the generation towards aspect-specific arguments. To the best of our knowledge, however, our approach is the first to employ external knowledge from knowledge graphs for the task of argument generation. Argumentation Knowledge Graphs Besides the argumentation knowledge graph of Al-Khatib et al. (2020), Toledo-Ronen et al. (2016) created an expert stance graph to support stance classification. Gemechu and Reed (2019) encoded the relations between segments of an argument into a graph and demonstrated the graph’s effectiveness for argument mining. In our work, we utilize one of the available graphs, among others, using its knowledge to control the argument generation process. Closely related to argumentation knowledge, causality graphs gained some attention recently. While general knowledge bases such as ConceptNet (Speer et al., 2017) contain causal knowledge, the causality graph of Heindorf et al. (2020) that we utili"
2021.acl-long.366,C18-1318,1,0.901555,"Missing"
2021.acl-long.366,D19-5623,0,0.0177936,"he paragraph, encoding them with the type of effect relation between them as keyphrases separated by special tokens. We use ‘positive’ and ‘negative’ to represent the effect relations. For example, the paragraph “Animal studies suggests marijuana causes physical dependence, and serious problems” will be transformed into: 4747 “&lt;|startoftext|>’[’marijuana»positive»physicaldependence’, ’mariguana»positive»problems’] @ Animal studies suggests ...’&lt;|endoftext|>” While this way of matching and encoding has limitations, it has shown good results in practice when used with pre-trained neural models (Witteveen and Andrews, 2019; Cachola et al., 2020). 3.3 Neural Language Model Fine-tuning We use our text-knowledge encoding dataset to fine-tune the GPT-2 neural language model (Radford et al., 2019) for argument generation. Since GPT-2 cannot deal with graph structure as input directly, we fine-tune it on all paragraphs, including those with encoded relations as textual representations (i.e., keyphrases). We expect to thereby leverage the powerful generation capabilities of GPT-2 while biasing it to generate texts related to the encoded relations. It is worth noting that, in training, we encode multiple relations at o"
2021.acl-long.366,2020.emnlp-main.577,0,0.015936,"ble graphs, among others, using its knowledge to control the argument generation process. Closely related to argumentation knowledge, causality graphs gained some attention recently. While general knowledge bases such as ConceptNet (Speer et al., 2017) contain causal knowledge, the causality graph of Heindorf et al. (2020) that we utilized is the largest source of causal knowledge, exceeding others by orders of magnitude. Graph-to-Text Generation In the related area of neural graph-to-text generation, researchers have used various techniques (Song et al., 2018; KoncelKedziorski et al., 2019b; Schmitt et al., 2020). Within this area, the approaches most related to ours are those that exploit the usage of knowledge in graphs as input to sequence-to-sequence models (Moryossef et al., 2019) as well as those that make use of large pre-trained language models such as Liu et al. (2021), where the pretrained model BART is augmented by knowledge from a graph for generative commonsense reasoning. Overall, our work concentrates on the context of argumentation, with an approach to encoding different types of argumentation knowledge into the pretrained model GPT-2 in order to allow for more controlled argument gene"
2021.acl-long.366,P18-1150,0,0.0254763,"t mining. In our work, we utilize one of the available graphs, among others, using its knowledge to control the argument generation process. Closely related to argumentation knowledge, causality graphs gained some attention recently. While general knowledge bases such as ConceptNet (Speer et al., 2017) contain causal knowledge, the causality graph of Heindorf et al. (2020) that we utilized is the largest source of causal knowledge, exceeding others by orders of magnitude. Graph-to-Text Generation In the related area of neural graph-to-text generation, researchers have used various techniques (Song et al., 2018; KoncelKedziorski et al., 2019b; Schmitt et al., 2020). Within this area, the approaches most related to ours are those that exploit the usage of knowledge in graphs as input to sequence-to-sequence models (Moryossef et al., 2019) as well as those that make use of large pre-trained language models such as Liu et al. (2021), where the pretrained model BART is augmented by knowledge from a graph for generative commonsense reasoning. Overall, our work concentrates on the context of argumentation, with an approach to encoding different types of argumentation knowledge into the pretrained model GP"
2021.argmining-1.19,D14-1226,0,0.0748302,"Missing"
2021.argmining-1.19,D17-1218,0,0.0271879,"as the duced by Bar-Haim et al. (2020a) is key point anal- summary of an argument (Petasis and Karkaletsis, ysis where the goal is to map a collection of argu1 https://2021.argmining.org/shared_task_ibm, last accessed: ments to a set of salient key points (say, high-level 2021-08-08 2 arguments) to provide a quantitative summary of The code is available under https://github.com/webis-de/ these arguments. ArgMining-21 184 Proceedings of The 8th Workshop on Argument Mining, pages 184–189 Punta Cana, Dominican Republic, November 10–11, 2021. ©2021 Association for Computational Linguistics 2016; Daxenberger et al., 2017). Wang and Ling (2016) used a sequence-to-sequence model for the abstractive summarization of arguments from online debate portals. A complementary task of generating conclusions as informative argument summaries was introduced by Syed et al. (2021). Similar to Alshomary et al. (2020b) who inferred a conclusion’s target with a triplet neural network, we rely on contrastive learning here, using a siamese network though. Also, we build upon ideas of Alshomary et al. (2020a) who proposed a graph-based model using PageRank (Page et al., 1999) that extracts the argument’s conclusion and the main su"
2021.argmining-1.19,W16-2816,0,0.0243861,"ontrastive learning here, using a siamese network though. Also, we build upon ideas of Alshomary et al. (2020a) who proposed a graph-based model using PageRank (Page et al., 1999) that extracts the argument’s conclusion and the main supporting reason as an extractive summary. All these works represent the single-document summarization paradigm where only one argument is summarized at a time, whereas the given shared task is a multi-document summarization setting. The first approaches to multi-document argument summarization aimed to identify the main points of online discussions. Among these, Egan et al. (2016) grouped verb frames into pattern clusters that serve as input to a structured summarization pipeline, whereas Misra et al. (2016) proposed a more condensed approach by directly extracting argumentative sentences, summarized by similarity clustering. Bar-Haim et al. (2020a) continued this line of research by introducing the notion of key points and contributing the ArgsKP corpus, a collection of arguments mapped to manually-created key points. These key points are concise and selfcontained sentences that capture the gist of the arguments. Later, Bar-Haim et al. (2020b) proposed a quantitative"
2021.argmining-1.19,W04-1013,0,0.0128467,"ph-based Summarization Aspect Clustering 19.8 18.9 3.5 4.7 18.0 17.1 For the graph-based summarization model, we employed Spacy (Honnibal et al., 2020) to split the arguments into sentences. Similar to (Bar-Haim et al., 2020b), only sentences with a minimum of 5 and a maximum of 20 tokens, and not starting with a pronoun, were used for building the graph. Argument quality scores for each sentence were obtained from Project Debater’s API (Toledo et al., 2019)3 . We selected the thresholds for the parameters d, qual and match in Equation 1 as 0.2, 0.8 and 0.4 respectively, optimizing for ROUGE (Lin, 2004). In particular, we computed ROUGE-L between the ground-truth key points and the top 10 ranked sentences as our predictions, averaged over all the topic and stance combinations in the training split. We excluded sentences with a matching score higher than 0.8 with the selected candidates to minimize redundancy. For aspect clustering, we created 15 clusters per topic and stance combination. After greedy approximation of the candidate sentences, we removed redundant ones using a threshold of 0.65 for the normalized BERTScore (Zhang et al., 2020) with the previously selected candidates. Table 1:"
2021.argmining-1.19,D19-1387,0,0.0191228,"ss, matching findings of Syed et al. (2021). For the aspect clustering, we observe that the key points are more focused on specific aspects such as “disease” (for Pro) and “effectiveness” (for Con). In a real-world application, this may provide the flexibility to choose key points by aspects of interest to the end-user, especially with further improvement of aspect tagger by avoiding non-essential extracted phrases as “mandatory”. Hence, given the task of generating a quantitative summary of a collection of arguments, we believe that the graph-based summary provides We employed RoBERTa-large (Liu and Lapata, 2019) for encoding the tokens of the two inputs of key point matching to the siamese neural network, which acts as a mean-pooling layer and projects the encoder outputs (matrix of token embeddings) into a sentence embedding of size 768. We used Sentence-BERT (Reimers and Gurevych, 2019) to train our model for 10 epochs, with batch size 32, and maximum input length of 70, leaving all other parameters to their defaults. For automatic evaluation, we computed both strict and relaxed mean Average Precision (mAP) following Friedman et al. (2021). In cases where there is no majority label for matching, th"
2021.argmining-1.19,W16-3636,0,0.0248737,"aph-based model using PageRank (Page et al., 1999) that extracts the argument’s conclusion and the main supporting reason as an extractive summary. All these works represent the single-document summarization paradigm where only one argument is summarized at a time, whereas the given shared task is a multi-document summarization setting. The first approaches to multi-document argument summarization aimed to identify the main points of online discussions. Among these, Egan et al. (2016) grouped verb frames into pattern clusters that serve as input to a structured summarization pipeline, whereas Misra et al. (2016) proposed a more condensed approach by directly extracting argumentative sentences, summarized by similarity clustering. Bar-Haim et al. (2020a) continued this line of research by introducing the notion of key points and contributing the ArgsKP corpus, a collection of arguments mapped to manually-created key points. These key points are concise and selfcontained sentences that capture the gist of the arguments. Later, Bar-Haim et al. (2020b) proposed a quantitative argument summarization framework that automatically extracts key points from a set of arguments. Building upon this research, our"
2021.argmining-1.19,W16-2811,0,0.045117,"Missing"
2021.argmining-1.19,D19-1410,0,0.0185596,"oints by aspects of interest to the end-user, especially with further improvement of aspect tagger by avoiding non-essential extracted phrases as “mandatory”. Hence, given the task of generating a quantitative summary of a collection of arguments, we believe that the graph-based summary provides We employed RoBERTa-large (Liu and Lapata, 2019) for encoding the tokens of the two inputs of key point matching to the siamese neural network, which acts as a mean-pooling layer and projects the encoder outputs (matrix of token embeddings) into a sentence embedding of size 768. We used Sentence-BERT (Reimers and Gurevych, 2019) to train our model for 10 epochs, with batch size 32, and maximum input length of 70, leaving all other parameters to their defaults. For automatic evaluation, we computed both strict and relaxed mean Average Precision (mAP) following Friedman et al. (2021). In cases where there is no majority label for matching, the relaxed mAP considers them to be a match while the strict mAP considers them as not matching. In the development phase, we trained our model on the training split and evaluated on the validation split provided by the organizers. The strict and relaxed mAP on the validation set we"
2021.argmining-1.19,2021.naacl-main.34,0,0.0187054,"sentence to the final set of key points if its maximum matching score with the already selected candidates is below a certain threshold. Aspect Clustering Extracting key points is conceptually similar to identifying aspects (Bar-Haim Graph-based Summarization Following the et al., 2020a), which inspired our clustering apwork of Alshomary et al. (2020a), we first construct proach that selects representative sentences from an undirected graph with the arguments’ sentences multiple aspect clusters as the final key points. We as nodes. As a filtering step, we compute argument employ the tagger of Schiller et al. (2021) to exquality scores for each sentence as Toledo et al. tract the arguments’ aspects (on average, 2.1 as(2019) and exclude low-quality arguments from pects per argument). To tackle the lack of diversity, the graph. Next, we employ our key point match- we follow Heinisch and Cimiano (2021) and creing model (Section 4.1) to compute the edge weight ate k diverse aspect clusters by projecting the exbetween two nodes as the pairwise matching score tracted aspect phrases to an embedding space. Next, of the corresponding sentences. Only nodes with a we model the candidate selection of argument sensco"
2021.argmining-1.19,2020.coling-main.470,1,0.82388,"Missing"
2021.argmining-1.19,2021.findings-acl.306,1,0.802537,"Missing"
2021.argmining-1.19,D19-1564,0,0.0281845,"selection of argument senscore above a defined threshold are connected via tences as the set cover problem. Specifically, the 186 Approach R-1 R-2 R-L 5.2 Graph-based Summarization Aspect Clustering 19.8 18.9 3.5 4.7 18.0 17.1 For the graph-based summarization model, we employed Spacy (Honnibal et al., 2020) to split the arguments into sentences. Similar to (Bar-Haim et al., 2020b), only sentences with a minimum of 5 and a maximum of 20 tokens, and not starting with a pronoun, were used for building the graph. Argument quality scores for each sentence were obtained from Project Debater’s API (Toledo et al., 2019)3 . We selected the thresholds for the parameters d, qual and match in Equation 1 as 0.2, 0.8 and 0.4 respectively, optimizing for ROUGE (Lin, 2004). In particular, we computed ROUGE-L between the ground-truth key points and the top 10 ranked sentences as our predictions, averaged over all the topic and stance combinations in the training split. We excluded sentences with a matching score higher than 0.8 with the selected candidates to minimize redundancy. For aspect clustering, we created 15 clusters per topic and stance combination. After greedy approximation of the candidate sentences, we r"
2021.argmining-1.19,N16-1007,0,0.0282445,"t al. (2020a) is key point anal- summary of an argument (Petasis and Karkaletsis, ysis where the goal is to map a collection of argu1 https://2021.argmining.org/shared_task_ibm, last accessed: ments to a set of salient key points (say, high-level 2021-08-08 2 arguments) to provide a quantitative summary of The code is available under https://github.com/webis-de/ these arguments. ArgMining-21 184 Proceedings of The 8th Workshop on Argument Mining, pages 184–189 Punta Cana, Dominican Republic, November 10–11, 2021. ©2021 Association for Computational Linguistics 2016; Daxenberger et al., 2017). Wang and Ling (2016) used a sequence-to-sequence model for the abstractive summarization of arguments from online debate portals. A complementary task of generating conclusions as informative argument summaries was introduced by Syed et al. (2021). Similar to Alshomary et al. (2020b) who inferred a conclusion’s target with a triplet neural network, we rely on contrastive learning here, using a siamese network though. Also, we build upon ideas of Alshomary et al. (2020a) who proposed a graph-based model using PageRank (Page et al., 1999) that extracts the argument’s conclusion and the main supporting reason as an"
2021.argmining-1.7,2021.ccl-1.108,0,0.0216869,"Missing"
2021.argmining-1.7,2020.acl-main.399,1,0.908178,"the standard use of transformers for text classification, we leverage the structure of arguments for their assessment. Wachsmuth et al. (2016) provided evidence that mining the argumentative structure of persuasive essays helps to better assess four essay-level quality dimensions of argumentation. Similarly, we use annotations of the premises and conclusions of arguments for the sufficiency assessment, but we target the arguments. Moreover, we explore to benefit of conclusion generation for the assessment. The idea of reconstructing an argument’s conclusion from its premises was introduced by Alshomary et al. (2020), but their approach focused on the inference of a conclusion’s target. The actual generation of entire conclusions has so far only been studied by Syed et al. (2021). The authors presented the first corpus for this task along with experiments where they adapted BART (Lewis et al., 2020) from summarization to conclusion generation. While they trained BART to directly generate a conclusion based on premises, we generate conclusions that fit the context of an entire argument. To this end, we leverage and finetune BART’s inherent denoising capabilities obtained during pretraining to replace a mas"
2021.argmining-1.7,2021.eacl-main.147,1,0.885857,"While the upper one was deemed sufficient by human experts (Stab and Gurevych, 2017b), the lower one was not, likely because the second premise tries to reason from a single example. A reliable computational assessment of argument sufficiency would allow systems to determine those arguments that are well-reasoned. The quality assessment of natural language argumentation is nowadays studied extensively for various genres and text granularities, from entire news editorials (El Baff et al., 2020) to arguments in online forums (Lauscher et al., 2020) to single claims in social media discussions (Skitalinskaya et al., 2021). The reason lies in its importance for driving downstream applications such as writing support (Stab, 2017), argument search (Wachsmuth et al., 2017b), and debating technologies (Slonim et al., 2021). Wachsmuth et al. (2017a) organized quality dimensions of arguments into three complementary aspects: logic, rhetoric, and dialectic. Logical quality refers to the actual argument structure, that is, how strong an argument is in terms of the support of a claim (the argument’s conclusion) by evidence and other reasons (the premises). As detailed in Section 2, previous approaches to argument suffic"
2021.argmining-1.7,N19-1423,0,0.00740629,"ment approach through generation: (1) BART is used to generate the masked conclusion in an argument. (2) The generated conclusion is combined with the ground truth annotation of the argument. (3) RoBERTa classifies the enriched argument as sufficient/insufficient. Several ablations of the annotations are tested in our experiments. 4.2 Given a modified argument, the second task is to predict whether the premises in the argument are rationally worth drawing the conclusion. We use RoBERTa (Liu et al., 2019) for this task by adding a linear layer on top of the pooled output of the original model (Devlin et al., 2019). Second, the generated conclusion is used to assess the argument’s sufficiency by experimenting with eight modified versions of the original input argument (Section 6). An overview of the approach is shown in Figure 2. In the following, we detail how we train the models for the two steps. 4.1 Sufficiency Assessment using Structure Successfully optimizing RoBERTa using mean squared error (MSE) or cross-entropy loss functions would be difficult, as both of them do not align well with the target metric of sufficiency assessment (macro F1 -score). We therefore follow ideas of Puthiya Parambath et"
2021.argmining-1.7,J17-3005,0,0.0933941,"t than health in a human’s life and the happiness and welfare come with health. Therefore, a government’s role should be providing means that lead its citizens to learn how to prevent from potential illness that can cause misery in people’s lives. For example, the marketing campaign of Ministry of Health in Turkey which aimed smoking problem among the youth increased the well-being of those who quit smoking and adapted a better lifestyle after the campaign. Figure 1: Two example arguments from a persuasive student essay, one classified as sufficient, the other as insufficient in the corpus of Stab and Gurevych (2017b). Introduction A key dimension of logical quality is sufficiency, capturing whether an argument’s premises together make it rationally worthy of drawing its conclusion (Johnson and Blair, 2006). Consider, for example, the two arguments on health education in Figure 1, taken from the argument-annotated essay corpus of Stab and Gurevych (2017a). While the upper one was deemed sufficient by human experts (Stab and Gurevych, 2017b), the lower one was not, likely because the second premise tries to reason from a single example. A reliable computational assessment of argument sufficiency would all"
2021.argmining-1.7,2020.acl-main.287,1,0.842272,"Missing"
2021.argmining-1.7,E17-1092,0,0.213511,"t than health in a human’s life and the happiness and welfare come with health. Therefore, a government’s role should be providing means that lead its citizens to learn how to prevent from potential illness that can cause misery in people’s lives. For example, the marketing campaign of Ministry of Health in Turkey which aimed smoking problem among the youth increased the well-being of those who quit smoking and adapted a better lifestyle after the campaign. Figure 1: Two example arguments from a persuasive student essay, one classified as sufficient, the other as insufficient in the corpus of Stab and Gurevych (2017b). Introduction A key dimension of logical quality is sufficiency, capturing whether an argument’s premises together make it rationally worthy of drawing its conclusion (Johnson and Blair, 2006). Consider, for example, the two arguments on health education in Figure 1, taken from the argument-annotated essay corpus of Stab and Gurevych (2017a). While the upper one was deemed sufficient by human experts (Stab and Gurevych, 2017b), the lower one was not, likely because the second premise tries to reason from a single example. A reliable computational assessment of argument sufficiency would all"
2021.argmining-1.7,2021.findings-acl.306,1,0.814644,"Missing"
2021.argmining-1.7,2020.coling-main.402,0,0.138105,"rom the argument-annotated essay corpus of Stab and Gurevych (2017a). While the upper one was deemed sufficient by human experts (Stab and Gurevych, 2017b), the lower one was not, likely because the second premise tries to reason from a single example. A reliable computational assessment of argument sufficiency would allow systems to determine those arguments that are well-reasoned. The quality assessment of natural language argumentation is nowadays studied extensively for various genres and text granularities, from entire news editorials (El Baff et al., 2020) to arguments in online forums (Lauscher et al., 2020) to single claims in social media discussions (Skitalinskaya et al., 2021). The reason lies in its importance for driving downstream applications such as writing support (Stab, 2017), argument search (Wachsmuth et al., 2017b), and debating technologies (Slonim et al., 2021). Wachsmuth et al. (2017a) organized quality dimensions of arguments into three complementary aspects: logic, rhetoric, and dialectic. Logical quality refers to the actual argument structure, that is, how strong an argument is in terms of the support of a claim (the argument’s conclusion) by evidence and other reasons (the p"
2021.argmining-1.7,D19-1564,0,0.0231132,"an-written conclusions in terms of sufficiency, likeliness, and novelty (Section 5). To quantify the impact on sufficiency assessment, we explore various combinations of premises, original conclusion, and generated conclusion in systematic ablation tests, and we compare them to the state of the art and a human upper bound (Section 6). Our sufficiency experiments reveal that, even on the plain input text of an argument, RoBERTa already improves significantly 1 The experiment code can be found under: https://gi thub.com/webis-de/ArgMining-21 68 on a holistic view of quality (Gretz et al., 2020; Toledo et al., 2019), although a few approaches used transformers for some of the dimensions of Wachsmuth et al. (2017a), such as Lauscher et al. (2020), or somewhat related dimensions in light of quality improvement (Skitalinskaya et al., 2021). In contrast to the standard use of transformers for text classification, we leverage the structure of arguments for their assessment. Wachsmuth et al. (2016) provided evidence that mining the argumentative structure of persuasive essays helps to better assess four essay-level quality dimensions of argumentation. Similarly, we use annotations of the premises and conclusio"
2021.argmining-1.7,2020.acl-main.703,0,0.502912,"ased architecture, namely we adapt RoBERTa (Liu et al., 2019) to assess sufficiency. Approaches to argument quality assessment using such architectures are still limited, mostly focusing In this paper, we study whether generating a conclusion from an argument’s premises benefits the computational assessment of the argument’s sufficiency. In particular, we first enrich the argument with structural annotations, highlighting which parts are the premises and which part is the conclusion. We propose in Section 4 to then mask the conclusion in order to learn to re-generate it using fine-tuned BART (Lewis et al., 2020). Combining the generated conclusion with the original argument and its annotations, our approach learns to distinguish sufficient from insufficient arguments using a modified RoBERTa model (Liu et al., 2019). Starting from ground-truth argument structure, we subsequently evaluate conclusion generation and sufficiency assessment on the merged annotations of the corpora of Stab and Gurevych (2017a) and Stab and Gurevych (2017b), as described in Section 3. Our generation experiments indicate that fine-tuning BART leads to better conclusions, which are on par with human-written conclusions in ter"
2021.argmining-1.7,C16-1158,1,0.858216,"n on the plain input text of an argument, RoBERTa already improves significantly 1 The experiment code can be found under: https://gi thub.com/webis-de/ArgMining-21 68 on a holistic view of quality (Gretz et al., 2020; Toledo et al., 2019), although a few approaches used transformers for some of the dimensions of Wachsmuth et al. (2017a), such as Lauscher et al. (2020), or somewhat related dimensions in light of quality improvement (Skitalinskaya et al., 2021). In contrast to the standard use of transformers for text classification, we leverage the structure of arguments for their assessment. Wachsmuth et al. (2016) provided evidence that mining the argumentative structure of persuasive essays helps to better assess four essay-level quality dimensions of argumentation. Similarly, we use annotations of the premises and conclusions of arguments for the sufficiency assessment, but we target the arguments. Moreover, we explore to benefit of conclusion generation for the assessment. The idea of reconstructing an argument’s conclusion from its premises was introduced by Alshomary et al. (2020), but their approach focused on the inference of a conclusion’s target. The actual generation of entire conclusions has"
2021.argmining-1.7,W17-5106,1,0.936872,"reason from a single example. A reliable computational assessment of argument sufficiency would allow systems to determine those arguments that are well-reasoned. The quality assessment of natural language argumentation is nowadays studied extensively for various genres and text granularities, from entire news editorials (El Baff et al., 2020) to arguments in online forums (Lauscher et al., 2020) to single claims in social media discussions (Skitalinskaya et al., 2021). The reason lies in its importance for driving downstream applications such as writing support (Stab, 2017), argument search (Wachsmuth et al., 2017b), and debating technologies (Slonim et al., 2021). Wachsmuth et al. (2017a) organized quality dimensions of arguments into three complementary aspects: logic, rhetoric, and dialectic. Logical quality refers to the actual argument structure, that is, how strong an argument is in terms of the support of a claim (the argument’s conclusion) by evidence and other reasons (the premises). As detailed in Section 2, previous approaches to argument sufficiency assessment model the task as a standard text classification problem and tackle it with convolutional neural networks (Stab and Gurevych, 2017b)"
2021.argmining-1.7,2020.coling-main.592,1,0.90558,"rs to the actual argument structure, that is, how strong an argument is in terms of the support of a claim (the argument’s conclusion) by evidence and other reasons (the premises). As detailed in Section 2, previous approaches to argument sufficiency assessment model the task as a standard text classification problem and tackle it with convolutional neural networks (Stab and Gurevych, 2017b) or traditional feature engineer67 Proceedings of The 8th Workshop on Argument Mining, pages 67–77 Punta Cana, Dominican Republic, November 10–11, 2021. ©2021 Association for Computational Linguistics ing (Wachsmuth and Werner, 2020). In the focused domain of persuasive student essays, Stab and Gurevych (2017b) obtained a macro F1 -score of .827, not far away from human performance in their setting (.887). However, to further improve the state of the art, we expect the integration of knowledge beyond what is directly available in the text at hand is needed. In particular, we observe that existing work neither explicitly considers an argument’s premises and conclusions, nor a property of their relationship. We hypothesize that only a sufficient argument makes it possible to infer the conclusion from the premises. Consequen"
2021.eacl-main.147,W18-0528,0,0.020813,"n posters, and Wei et al. (2016) used user upvotes and downvotes for the same purpose. Here, we resort to kialo.com, where users cannot only state argumentative claims and vote on the impact of claims submitted by others, but they can also help improve claims by suggesting revisions, which are approved or disapproved by moderators. While Durmus et al. (2019) assessed quality based on the impact value of claims from kialo.com, we derive information on quality from the revision history of claims. The only work we are aware of that analyzes revision quality of argumentative texts is the study of Afrin and Litman (2018). From the corpus of Zhang et al. (2017) containing 60 student essays with three draft versions each, 940 sentence writing revision pairs were annotated for whether the revision improves essay quality or not. The authors then trained a random forest classifier for automatic revision quality classification. In contrast, instead of sentences, we shift our focus to claims. Moreover, our dataset is orders of magnitude larger and includes notably longer revision chains, which enables deeper analyses and more reliable prediction of revision quality using data-intensive methods. 3 Data Here, we prese"
2021.eacl-main.147,N16-1165,1,0.795933,"In this work, we assess quality based on different revisions of the same text. In this setting, the quality is primarily focused on how a text is formulated, which will help to better understand what influences argument quality in general, irrespective of the topic. To be able to do so, we refer to online debate portals. Debate portals give users the opportunity to discuss their views on a wide range of topics. Existing research has used the rich argumentative content and structure of different portals for argument mining, including createdebate.com (Habernal and Gurevych, 2015), idebate.org (Al-Khatib et al., 2016), and others. Also, large-scale debate portal datasets form the basis of applications such as argument search engines (Ajjour et al., 2019). Unlike these works, we exploit debate portals for studying quality. Tan et al. (2016) predicted argument persuasiveness in the discussion forum ChangeMyView from ground-truth labels given by opinion posters, and Wei et al. (2016) used user upvotes and downvotes for the same purpose. Here, we resort to kialo.com, where users cannot only state argumentative claims and vote on the impact of claims submitted by others, but they can also help improve claims by"
2021.eacl-main.147,N19-1423,0,0.119645,"essment based on claim revisions. In a manual annotation study, we provide support for our underlying hypothesis that a revision improves a claim in most cases, and we test how much the revision types correlate with known argument quality dimensions. Given the corpus, we study two tasks: (a) how to compare revisions of a claim by quality and (b) how to rank a set of claim revisions. As initial approaches to the first task, we select in Section 4 a “traditional” logistic regression model based on word embeddings as well as transformer-based neural networks (Vaswani et al., 2017), such as BERT (Devlin et al., 2019) and SBERT (Reimers and Gurevych, 2019). For the ranking task, we consider the Bradley-Terry-Luce model (Bradley and Terry, 1952; Luce, 2012) and SVMRank (Joachims, 2006). They achieve promising results, indicating that the compiled corpus allows learning topic-independent characteristics associated with the quality of claims (Section 5). To understand what claim quality improvements can be assessed reliably, we then carry out a detailed error analysis for different revision types and numbers of revisions. The main contributions of our work are: (1) A new corpus for topic-independent claim qua"
2021.eacl-main.147,D19-1568,0,0.043542,"Missing"
2021.eacl-main.147,2020.acl-main.287,1,0.808285,"Missing"
2021.eacl-main.147,P19-1093,0,0.0134925,"16) created a dataset with argument convincingness pairs 1719 on 32 topics. To mitigate annotator bias, the arguments in a pair always have the same stance on the same topic. The more convincing argument is then predicted using a feature-rich SVM and a simple bidirectional LSTM. Other approaches to the same task map passage representations to real-valued scores using Gaussian Process Preference Learning (Simpson and Gurevych, 2018) or represent arguments by the sum of their token embeddings (Potash et al., 2017), later extended by a Feed Forward Neural Network (Potash et al., 2019). Recently, Gleize et al. (2019) employed a Siamese neural network to rank arguments by the convincingness of evidence. In our experiments below, we take on some of these ideas, but also explore the impact of transformer-based methods such as BERT (Devlin et al., 2019), which have been shown to predict argument quality well (Gretz et al., 2019). Potash et al. (2017) observed that longer arguments tend to be judged better in existing corpora, a phenomenon we will also check for below. Toledo et al. (2019b) prevent such bias in their corpora for both pointwise and pairwise quality, by restricting the length of arguments to 8–3"
2021.eacl-main.147,D15-1255,0,0.026801,"of the aspects of topics being discussed. In this work, we assess quality based on different revisions of the same text. In this setting, the quality is primarily focused on how a text is formulated, which will help to better understand what influences argument quality in general, irrespective of the topic. To be able to do so, we refer to online debate portals. Debate portals give users the opportunity to discuss their views on a wide range of topics. Existing research has used the rich argumentative content and structure of different portals for argument mining, including createdebate.com (Habernal and Gurevych, 2015), idebate.org (Al-Khatib et al., 2016), and others. Also, large-scale debate portal datasets form the basis of applications such as argument search engines (Ajjour et al., 2019). Unlike these works, we exploit debate portals for studying quality. Tan et al. (2016) predicted argument persuasiveness in the discussion forum ChangeMyView from ground-truth labels given by opinion posters, and Wei et al. (2016) used user upvotes and downvotes for the same purpose. Here, we resort to kialo.com, where users cannot only state argumentative claims and vote on the impact of claims submitted by others, bu"
2021.eacl-main.147,P16-1150,0,0.103407,"s and from diverse perspectives, many of which are inherently subjective (Wachsmuth et al., 2017a); they depend on the prior beliefs and stance 1 Data and code:https://github.com/GabriellaSky/claimrev Henning Wachsmuth Department of Computer Science Paderborn University Paderborn, Germany henningw@upb.de on a topic as well as on the personal weighting of different aspects of the topic (Kock, 2007). Existing research largely ignores this limitation, by focusing on learning to predict argument quality based on subjective assessments of human annotators (see Section 2 for examples). In contrast, Habernal and Gurevych (2016) control for topic and stance to compare the convincingness of arguments. Wachsmuth et al. (2017b) abstract from an argument’s text, assessing its relevance only structurally. Lukin et al. (2017) and El Baff et al. (2020) focus on personality-specific and ideology-specific quality perception, respectively, whereas Toledo et al. (2019a) asked annotators to disregard their own stance in judging length-restricted arguments. However, none of these approaches controls for the concrete aspects of a topic that the arguments claim and reason about. This renders it difficult to learn what makes an argu"
2021.eacl-main.147,2020.coling-main.402,0,0.496154,"Ng (2015) do the same for argument strength, and Stab and Gurevych (2017) classify whether an argument’s premises sufficiently support its conclusion. All these are trained on pointwise quality annotations in the form of scores or binary judgments. Gretz et al. (2019) provide a corpus with crowdsourced quality annotations for 30,497 arguments, the largest to date for pointwise argument quality. The authors studied how their annotations correlate with the 15 dimensions from the framework of Wachsmuth et al. (2017a), finding that only global relevance and effectiveness are captured. Similarly, Lauscher et al. (2020) built a new corpus based on the framework to then exploit interactions between the dimensions in a neural approach. We present a small related annotation study for our dataset below. However, we follow Habernal and Gurevych (2016) in that we cast argument quality assessment as a relation classification problem, where the goal is to identify the better among a pair of instances. In particular, Habernal and Gurevych (2016) created a dataset with argument convincingness pairs 1719 on 32 topics. To mitigate annotator bias, the arguments in a pair always have the same stance on the same topic. The"
2021.eacl-main.147,E17-1070,0,0.145552,"ning Wachsmuth Department of Computer Science Paderborn University Paderborn, Germany henningw@upb.de on a topic as well as on the personal weighting of different aspects of the topic (Kock, 2007). Existing research largely ignores this limitation, by focusing on learning to predict argument quality based on subjective assessments of human annotators (see Section 2 for examples). In contrast, Habernal and Gurevych (2016) control for topic and stance to compare the convincingness of arguments. Wachsmuth et al. (2017b) abstract from an argument’s text, assessing its relevance only structurally. Lukin et al. (2017) and El Baff et al. (2020) focus on personality-specific and ideology-specific quality perception, respectively, whereas Toledo et al. (2019a) asked annotators to disregard their own stance in judging length-restricted arguments. However, none of these approaches controls for the concrete aspects of a topic that the arguments claim and reason about. This renders it difficult to learn what makes an argument and its building blocks good or bad in general. In this paper, we study quality in argumentation irrespective of the discussed topics, aspects, and stances by assessing different revisions o"
2021.eacl-main.147,N19-1063,0,0.0266595,"e change the input data, provided as a ranked list, into a set of ordered pairs, where the (binary) class label for every pair is the order in which the elements of the pair should be ranked. Then, SVMRank learns by minimizing the error of the order relation when comparing all possible combinations of candidate pairs. Given the nature of the algorithm we cannot work with token embeddings obtained from BERT directly. Thus, we utilize one of most commonly used approaches to transform token embeddings to a sentence embedding: extracting the special [CLS] token vector (Reimers and Gurevych, 2019; May et al., 2019). In our experiments we select a linear kernel for the SVM and use PySVMRank,5 a python API to the SVMrank library written in C.6 5 Experiments and Discussion We now present empirical experiments with the approaches from Section 4. The goal is to evaluate how hard it is to compare and rank the claim revisions in our corpus from Section 3 by quality. 5.1 Experimental Setup We carry out experiments in two settings. The first considers creating random splits over revision histories, ensuring that all versions of the same 5 PySVMRank, https://github.com/ds4dm/PySVMRank SVMrank , www.cs.cornell.edu"
2021.eacl-main.147,P13-1026,0,0.0333081,"can be considered from a logical, rhetorical, and dialectical perspectives. To achieve a common understanding, the authors suggest a unified framework with 15 quality dimensions, which together give a holistic quality evaluation at a certain abstraction level. They point out, that several dimensions may be perceived differently depending on the target audience. In recent follow-up work, Wachsmuth and Werner (2020) examined how well each dimension can be assessed only based on plain text only. Most existing quality assessment approaches target a single dimension. On mixed-topic student essays, Persing and Ng (2013) learn to score the clarity of an argument’s thesis, Persing and Ng (2015) do the same for argument strength, and Stab and Gurevych (2017) classify whether an argument’s premises sufficiently support its conclusion. All these are trained on pointwise quality annotations in the form of scores or binary judgments. Gretz et al. (2019) provide a corpus with crowdsourced quality annotations for 30,497 arguments, the largest to date for pointwise argument quality. The authors studied how their annotations correlate with the 15 dimensions from the framework of Wachsmuth et al. (2017a), finding that o"
2021.eacl-main.147,P15-1053,0,0.0251349,". To achieve a common understanding, the authors suggest a unified framework with 15 quality dimensions, which together give a holistic quality evaluation at a certain abstraction level. They point out, that several dimensions may be perceived differently depending on the target audience. In recent follow-up work, Wachsmuth and Werner (2020) examined how well each dimension can be assessed only based on plain text only. Most existing quality assessment approaches target a single dimension. On mixed-topic student essays, Persing and Ng (2013) learn to score the clarity of an argument’s thesis, Persing and Ng (2015) do the same for argument strength, and Stab and Gurevych (2017) classify whether an argument’s premises sufficiently support its conclusion. All these are trained on pointwise quality annotations in the form of scores or binary judgments. Gretz et al. (2019) provide a corpus with crowdsourced quality annotations for 30,497 arguments, the largest to date for pointwise argument quality. The authors studied how their annotations correlate with the 15 dimensions from the framework of Wachsmuth et al. (2017a), finding that only global relevance and effectiveness are captured. Similarly, Lauscher e"
2021.eacl-main.147,S18-2023,0,0.0642547,"Missing"
2021.eacl-main.147,I17-1035,0,0.258946,"the goal is to identify the better among a pair of instances. In particular, Habernal and Gurevych (2016) created a dataset with argument convincingness pairs 1719 on 32 topics. To mitigate annotator bias, the arguments in a pair always have the same stance on the same topic. The more convincing argument is then predicted using a feature-rich SVM and a simple bidirectional LSTM. Other approaches to the same task map passage representations to real-valued scores using Gaussian Process Preference Learning (Simpson and Gurevych, 2018) or represent arguments by the sum of their token embeddings (Potash et al., 2017), later extended by a Feed Forward Neural Network (Potash et al., 2019). Recently, Gleize et al. (2019) employed a Siamese neural network to rank arguments by the convincingness of evidence. In our experiments below, we take on some of these ideas, but also explore the impact of transformer-based methods such as BERT (Devlin et al., 2019), which have been shown to predict argument quality well (Gretz et al., 2019). Potash et al. (2017) observed that longer arguments tend to be judged better in existing corpora, a phenomenon we will also check for below. Toledo et al. (2019b) prevent such bias"
2021.eacl-main.147,W19-4517,0,0.0180816,"cular, Habernal and Gurevych (2016) created a dataset with argument convincingness pairs 1719 on 32 topics. To mitigate annotator bias, the arguments in a pair always have the same stance on the same topic. The more convincing argument is then predicted using a feature-rich SVM and a simple bidirectional LSTM. Other approaches to the same task map passage representations to real-valued scores using Gaussian Process Preference Learning (Simpson and Gurevych, 2018) or represent arguments by the sum of their token embeddings (Potash et al., 2017), later extended by a Feed Forward Neural Network (Potash et al., 2019). Recently, Gleize et al. (2019) employed a Siamese neural network to rank arguments by the convincingness of evidence. In our experiments below, we take on some of these ideas, but also explore the impact of transformer-based methods such as BERT (Devlin et al., 2019), which have been shown to predict argument quality well (Gretz et al., 2019). Potash et al. (2017) observed that longer arguments tend to be judged better in existing corpora, a phenomenon we will also check for below. Toledo et al. (2019b) prevent such bias in their corpora for both pointwise and pairwise quality, by restrictin"
2021.eacl-main.147,D19-1410,0,0.0970425,". In a manual annotation study, we provide support for our underlying hypothesis that a revision improves a claim in most cases, and we test how much the revision types correlate with known argument quality dimensions. Given the corpus, we study two tasks: (a) how to compare revisions of a claim by quality and (b) how to rank a set of claim revisions. As initial approaches to the first task, we select in Section 4 a “traditional” logistic regression model based on word embeddings as well as transformer-based neural networks (Vaswani et al., 2017), such as BERT (Devlin et al., 2019) and SBERT (Reimers and Gurevych, 2019). For the ranking task, we consider the Bradley-Terry-Luce model (Bradley and Terry, 1952; Luce, 2012) and SVMRank (Joachims, 2006). They achieve promising results, indicating that the compiled corpus allows learning topic-independent characteristics associated with the quality of claims (Section 5). To understand what claim quality improvements can be assessed reliably, we then carry out a detailed error analysis for different revision types and numbers of revisions. The main contributions of our work are: (1) A new corpus for topic-independent claim quality assessment, with distantly supervi"
2021.eacl-main.147,D15-1050,0,0.0308519,"ng-based logistic regression and transformer-based neural networks show promising results, suggesting that learned indicators generalize well across topics. In a detailed error analysis, we give insights into what quality dimensions of claims can be assessed reliably. We provide the data and scripts needed to reproduce all results.1 1 Introduction Assessing argument quality is as important as it is questionable in nature. On the one hand, identifying the good and the bad claims and reasons for arguing on a given topic is key to convincingly support or attack a stance in debating technologies (Rinott et al., 2015), argument search (Ajjour et al., 2019), and similar. On the other hand, argument quality can be considered on different granularity levels and from diverse perspectives, many of which are inherently subjective (Wachsmuth et al., 2017a); they depend on the prior beliefs and stance 1 Data and code:https://github.com/GabriellaSky/claimrev Henning Wachsmuth Department of Computer Science Paderborn University Paderborn, Germany henningw@upb.de on a topic as well as on the personal weighting of different aspects of the topic (Kock, 2007). Existing research largely ignores this limitation, by focusi"
2021.eacl-main.147,Q18-1026,0,0.0176035,") in that we cast argument quality assessment as a relation classification problem, where the goal is to identify the better among a pair of instances. In particular, Habernal and Gurevych (2016) created a dataset with argument convincingness pairs 1719 on 32 topics. To mitigate annotator bias, the arguments in a pair always have the same stance on the same topic. The more convincing argument is then predicted using a feature-rich SVM and a simple bidirectional LSTM. Other approaches to the same task map passage representations to real-valued scores using Gaussian Process Preference Learning (Simpson and Gurevych, 2018) or represent arguments by the sum of their token embeddings (Potash et al., 2017), later extended by a Feed Forward Neural Network (Potash et al., 2019). Recently, Gleize et al. (2019) employed a Siamese neural network to rank arguments by the convincingness of evidence. In our experiments below, we take on some of these ideas, but also explore the impact of transformer-based methods such as BERT (Devlin et al., 2019), which have been shown to predict argument quality well (Gretz et al., 2019). Potash et al. (2017) observed that longer arguments tend to be judged better in existing corpora, a"
2021.eacl-main.147,E17-1092,0,0.369132,"ified framework with 15 quality dimensions, which together give a holistic quality evaluation at a certain abstraction level. They point out, that several dimensions may be perceived differently depending on the target audience. In recent follow-up work, Wachsmuth and Werner (2020) examined how well each dimension can be assessed only based on plain text only. Most existing quality assessment approaches target a single dimension. On mixed-topic student essays, Persing and Ng (2013) learn to score the clarity of an argument’s thesis, Persing and Ng (2015) do the same for argument strength, and Stab and Gurevych (2017) classify whether an argument’s premises sufficiently support its conclusion. All these are trained on pointwise quality annotations in the form of scores or binary judgments. Gretz et al. (2019) provide a corpus with crowdsourced quality annotations for 30,497 arguments, the largest to date for pointwise argument quality. The authors studied how their annotations correlate with the 15 dimensions from the framework of Wachsmuth et al. (2017a), finding that only global relevance and effectiveness are captured. Similarly, Lauscher et al. (2020) built a new corpus based on the framework to then e"
2021.eacl-main.147,D19-1564,0,0.348012,"Missing"
2021.eacl-main.147,E17-1017,1,0.791563,"of claims can be assessed reliably. We provide the data and scripts needed to reproduce all results.1 1 Introduction Assessing argument quality is as important as it is questionable in nature. On the one hand, identifying the good and the bad claims and reasons for arguing on a given topic is key to convincingly support or attack a stance in debating technologies (Rinott et al., 2015), argument search (Ajjour et al., 2019), and similar. On the other hand, argument quality can be considered on different granularity levels and from diverse perspectives, many of which are inherently subjective (Wachsmuth et al., 2017a); they depend on the prior beliefs and stance 1 Data and code:https://github.com/GabriellaSky/claimrev Henning Wachsmuth Department of Computer Science Paderborn University Paderborn, Germany henningw@upb.de on a topic as well as on the personal weighting of different aspects of the topic (Kock, 2007). Existing research largely ignores this limitation, by focusing on learning to predict argument quality based on subjective assessments of human annotators (see Section 2 for examples). In contrast, Habernal and Gurevych (2016) control for topic and stance to compare the convincingness of argum"
2021.eacl-main.147,E17-1105,1,0.601639,"of claims can be assessed reliably. We provide the data and scripts needed to reproduce all results.1 1 Introduction Assessing argument quality is as important as it is questionable in nature. On the one hand, identifying the good and the bad claims and reasons for arguing on a given topic is key to convincingly support or attack a stance in debating technologies (Rinott et al., 2015), argument search (Ajjour et al., 2019), and similar. On the other hand, argument quality can be considered on different granularity levels and from diverse perspectives, many of which are inherently subjective (Wachsmuth et al., 2017a); they depend on the prior beliefs and stance 1 Data and code:https://github.com/GabriellaSky/claimrev Henning Wachsmuth Department of Computer Science Paderborn University Paderborn, Germany henningw@upb.de on a topic as well as on the personal weighting of different aspects of the topic (Kock, 2007). Existing research largely ignores this limitation, by focusing on learning to predict argument quality based on subjective assessments of human annotators (see Section 2 for examples). In contrast, Habernal and Gurevych (2016) control for topic and stance to compare the convincingness of argum"
2021.eacl-main.147,2020.coling-main.592,1,0.78139,"there has been an increase of research on the quality of arguments and the claims and reasoning they are composed of. Wachsmuth et al. (2017a) describe argumentation quality as a multidimensional concept that can be considered from a logical, rhetorical, and dialectical perspectives. To achieve a common understanding, the authors suggest a unified framework with 15 quality dimensions, which together give a holistic quality evaluation at a certain abstraction level. They point out, that several dimensions may be perceived differently depending on the target audience. In recent follow-up work, Wachsmuth and Werner (2020) examined how well each dimension can be assessed only based on plain text only. Most existing quality assessment approaches target a single dimension. On mixed-topic student essays, Persing and Ng (2013) learn to score the clarity of an argument’s thesis, Persing and Ng (2015) do the same for argument strength, and Stab and Gurevych (2017) classify whether an argument’s premises sufficiently support its conclusion. All these are trained on pointwise quality annotations in the form of scores or binary judgments. Gretz et al. (2019) provide a corpus with crowdsourced quality annotations for 30,"
2021.eacl-main.147,P16-2032,0,0.0298092,"their views on a wide range of topics. Existing research has used the rich argumentative content and structure of different portals for argument mining, including createdebate.com (Habernal and Gurevych, 2015), idebate.org (Al-Khatib et al., 2016), and others. Also, large-scale debate portal datasets form the basis of applications such as argument search engines (Ajjour et al., 2019). Unlike these works, we exploit debate portals for studying quality. Tan et al. (2016) predicted argument persuasiveness in the discussion forum ChangeMyView from ground-truth labels given by opinion posters, and Wei et al. (2016) used user upvotes and downvotes for the same purpose. Here, we resort to kialo.com, where users cannot only state argumentative claims and vote on the impact of claims submitted by others, but they can also help improve claims by suggesting revisions, which are approved or disapproved by moderators. While Durmus et al. (2019) assessed quality based on the impact value of claims from kialo.com, we derive information on quality from the revision history of claims. The only work we are aware of that analyzes revision quality of argumentative texts is the study of Afrin and Litman (2018). From th"
2021.eacl-main.147,P17-1144,0,0.0224176,"pvotes and downvotes for the same purpose. Here, we resort to kialo.com, where users cannot only state argumentative claims and vote on the impact of claims submitted by others, but they can also help improve claims by suggesting revisions, which are approved or disapproved by moderators. While Durmus et al. (2019) assessed quality based on the impact value of claims from kialo.com, we derive information on quality from the revision history of claims. The only work we are aware of that analyzes revision quality of argumentative texts is the study of Afrin and Litman (2018). From the corpus of Zhang et al. (2017) containing 60 student essays with three draft versions each, 940 sentence writing revision pairs were annotated for whether the revision improves essay quality or not. The authors then trained a random forest classifier for automatic revision quality classification. In contrast, instead of sentences, we shift our focus to claims. Moreover, our dataset is orders of magnitude larger and includes notably longer revision chains, which enables deeper analyses and more reliable prediction of revision quality using data-intensive methods. 3 Data Here, we present our corpus created based on claim rev"
2021.findings-acl.159,2021.eacl-main.17,1,0.722278,"th for single claims and complete arguments. Bilu et al. (2015) composed opposing claims combining rules with classifiers, whereas Hidey and McKeown (2019) tackled an analog task with neural methods. Alshomary et al. (2020) reconstructed implicit claims from argument premises using triplet neural networks, and Gretz et al. (2020) explored ability of 2 Our assumption is that each sentence represents a premise supporting the main claim mentioned in the title of the post 3 Code and resources can be found under https:// github.com/webis-de/ACL-21 1817 GPT-2 to generate claims on topics. Recently, Alshomary et al. (2021) studied how to encode specific beliefs into generated claims, whereas Chen et al. (2018) flipped the bias of claim-like news headlines using style transfer. Sato et al. (2015) generated full arguments in a largely rule-based way. Building on the model of rhetorical argumentation strategies by Wachsmuth et al. (2018a), El Baff et al. (2019) modeled argument synthesis as a language modeling task, and Schiller et al. (2020) studied the neural generation of arguments on a topic with controlled aspects and stance. Unlike all these, we deal with counter-arguments. Research exists for mining attack"
2021.findings-acl.159,2020.acl-main.399,1,0.844959,"or detecting premise attackability, achieving state-of-the-art effectiveness. • A new approach to counter-argument generation that identifies and attacks weak premises. • Empirical evidence of the impact of considering specific attackable premises in the argument when generating a counter-argument. 2 Related Work Recently, text generation has gained much interest in computational argumentation, both for single claims and complete arguments. Bilu et al. (2015) composed opposing claims combining rules with classifiers, whereas Hidey and McKeown (2019) tackled an analog task with neural methods. Alshomary et al. (2020) reconstructed implicit claims from argument premises using triplet neural networks, and Gretz et al. (2020) explored ability of 2 Our assumption is that each sentence represents a premise supporting the main claim mentioned in the title of the post 3 Code and resources can be found under https:// github.com/webis-de/ACL-21 1817 GPT-2 to generate claims on topics. Recently, Alshomary et al. (2021) studied how to encode specific beliefs into generated claims, whereas Chen et al. (2018) flipped the bias of claim-like news headlines using style transfer. Sato et al. (2015) generated full argument"
2021.findings-acl.159,W15-0511,0,0.0814234,"from Reddit changemyview. The italicized premise part was quoted by the user who stated the counter-argument. Introduction Following Walton (2009), a counter-argument can be defined as an attack on a specific argument by arguing against either its claim (called rebuttal), the validity of reasoning of its premises toward its claim (undercut), or the validity of one of its premises (undermining). Not only the mining and retrieval of counter-arguments have been studied (Peldszus and Stede, 2015; Wachsmuth et al., 2018b), recent works also tackled the generation of counter-arguments. Among these, Bilu et al. (2015) and Hidey and McKeown (2019) studied the task of contrastive claim generation, the former in a partly rule-based manner, the latter data-driven. Moreover, Hua and Wang (2019) proposed a neural counter-argument generation approach. So far, however, research focused only on rebutting a given argument, ignoring the other aforementioned types. We expand this research by studying to what extent argument undermining can be utilized in counterargument generation. In argument undermining, the validity of some premises is questioned. Such a phenomenon can be observed often in online discussions on soc"
2021.findings-acl.159,D19-1291,0,0.0264544,"fs into generated claims, whereas Chen et al. (2018) flipped the bias of claim-like news headlines using style transfer. Sato et al. (2015) generated full arguments in a largely rule-based way. Building on the model of rhetorical argumentation strategies by Wachsmuth et al. (2018a), El Baff et al. (2019) modeled argument synthesis as a language modeling task, and Schiller et al. (2020) studied the neural generation of arguments on a topic with controlled aspects and stance. Unlike all these, we deal with counter-arguments. Research exists for mining attack relations (Cocarascu and Toni, 2017; Chakrabarty et al., 2019; Orbach et al., 2020), mining counter-considerations from text (Peldszus and Stede, 2015), and retrieving counter-arguments (Wachsmuth et al., 2018b; Orbach et al., 2019). However, only the two works of Hua and Wang (2018, 2019) realy generated such arguments. Their latest neural approach takes an argument or claim as input and generates a counter-argument rebutting it. Differently, we consider countering an argument by attacking one of its premises, known as undermining (Walton, 2009). Part of our approach is to identify attackable premises, which can be studied from an argument quality pers"
2021.findings-acl.159,W18-6509,1,0.838201,"ning rules with classifiers, whereas Hidey and McKeown (2019) tackled an analog task with neural methods. Alshomary et al. (2020) reconstructed implicit claims from argument premises using triplet neural networks, and Gretz et al. (2020) explored ability of 2 Our assumption is that each sentence represents a premise supporting the main claim mentioned in the title of the post 3 Code and resources can be found under https:// github.com/webis-de/ACL-21 1817 GPT-2 to generate claims on topics. Recently, Alshomary et al. (2021) studied how to encode specific beliefs into generated claims, whereas Chen et al. (2018) flipped the bias of claim-like news headlines using style transfer. Sato et al. (2015) generated full arguments in a largely rule-based way. Building on the model of rhetorical argumentation strategies by Wachsmuth et al. (2018a), El Baff et al. (2019) modeled argument synthesis as a language modeling task, and Schiller et al. (2020) studied the neural generation of arguments on a topic with controlled aspects and stance. Unlike all these, we deal with counter-arguments. Research exists for mining attack relations (Cocarascu and Toni, 2017; Chakrabarty et al., 2019; Orbach et al., 2020), mini"
2021.findings-acl.159,D17-1144,0,0.0285889,"w to encode specific beliefs into generated claims, whereas Chen et al. (2018) flipped the bias of claim-like news headlines using style transfer. Sato et al. (2015) generated full arguments in a largely rule-based way. Building on the model of rhetorical argumentation strategies by Wachsmuth et al. (2018a), El Baff et al. (2019) modeled argument synthesis as a language modeling task, and Schiller et al. (2020) studied the neural generation of arguments on a topic with controlled aspects and stance. Unlike all these, we deal with counter-arguments. Research exists for mining attack relations (Cocarascu and Toni, 2017; Chakrabarty et al., 2019; Orbach et al., 2020), mining counter-considerations from text (Peldszus and Stede, 2015), and retrieving counter-arguments (Wachsmuth et al., 2018b; Orbach et al., 2019). However, only the two works of Hua and Wang (2018, 2019) realy generated such arguments. Their latest neural approach takes an argument or claim as input and generates a counter-argument rebutting it. Differently, we consider countering an argument by attacking one of its premises, known as undermining (Walton, 2009). Part of our approach is to identify attackable premises, which can be studied fro"
2021.findings-acl.159,W19-8607,1,0.896693,"Missing"
2021.findings-acl.159,2020.findings-emnlp.47,0,0.0247972,"nt generation that identifies and attacks weak premises. • Empirical evidence of the impact of considering specific attackable premises in the argument when generating a counter-argument. 2 Related Work Recently, text generation has gained much interest in computational argumentation, both for single claims and complete arguments. Bilu et al. (2015) composed opposing claims combining rules with classifiers, whereas Hidey and McKeown (2019) tackled an analog task with neural methods. Alshomary et al. (2020) reconstructed implicit claims from argument premises using triplet neural networks, and Gretz et al. (2020) explored ability of 2 Our assumption is that each sentence represents a premise supporting the main claim mentioned in the title of the post 3 Code and resources can be found under https:// github.com/webis-de/ACL-21 1817 GPT-2 to generate claims on topics. Recently, Alshomary et al. (2021) studied how to encode specific beliefs into generated claims, whereas Chen et al. (2018) flipped the bias of claim-like news headlines using style transfer. Sato et al. (2015) generated full arguments in a largely rule-based way. Building on the model of rhetorical argumentation strategies by Wachsmuth et"
2021.findings-acl.159,N19-1174,0,0.0473113,"Missing"
2021.findings-acl.159,P18-1021,0,0.0237511,"l argumentation strategies by Wachsmuth et al. (2018a), El Baff et al. (2019) modeled argument synthesis as a language modeling task, and Schiller et al. (2020) studied the neural generation of arguments on a topic with controlled aspects and stance. Unlike all these, we deal with counter-arguments. Research exists for mining attack relations (Cocarascu and Toni, 2017; Chakrabarty et al., 2019; Orbach et al., 2020), mining counter-considerations from text (Peldszus and Stede, 2015), and retrieving counter-arguments (Wachsmuth et al., 2018b; Orbach et al., 2019). However, only the two works of Hua and Wang (2018, 2019) realy generated such arguments. Their latest neural approach takes an argument or claim as input and generates a counter-argument rebutting it. Differently, we consider countering an argument by attacking one of its premises, known as undermining (Walton, 2009). Part of our approach is to identify attackable premises, which can be studied from an argument quality perspective. That is, a premise is attackable when it lacks specific quality criteria. A significant body of research has studied argument quality assessment, with a comprehensive survey of quality criteria presented in Wachsm"
2021.findings-acl.159,D19-1055,0,0.209842,"defined as an attack on a specific argument by arguing against either its claim (called rebuttal), the validity of reasoning of its premises toward its claim (undercut), or the validity of one of its premises (undermining). Not only the mining and retrieval of counter-arguments have been studied (Peldszus and Stede, 2015; Wachsmuth et al., 2018b), recent works also tackled the generation of counter-arguments. Among these, Bilu et al. (2015) and Hidey and McKeown (2019) studied the task of contrastive claim generation, the former in a partly rule-based manner, the latter data-driven. Moreover, Hua and Wang (2019) proposed a neural counter-argument generation approach. So far, however, research focused only on rebutting a given argument, ignoring the other aforementioned types. We expand this research by studying to what extent argument undermining can be utilized in counterargument generation. In argument undermining, the validity of some premises is questioned. Such a phenomenon can be observed often in online discussions on social media. For example, in the discussion excerpt in Table 1, taken from the Reddit forum changemyview,1 a user contests the whole stated argument (claim and premises) by refe"
2021.findings-acl.159,2020.emnlp-main.1,0,0.246547,"o tackle the task of counter-argument generation by attacking one of the weak premises of an argument. We hypothesize that identifying a weak premise is key to effective counter-argument generation—especially when the argument is of high complexity, comprising multiple interlinked claims and premises, which makes it hard to comprehend the argument as a single unit. Figure 1 illustrates our two-step pipeline approach: it first detects premises that may be attackable and then generates a counter-argument targeting one or more of these premises. To identify weak premises, we build on the work of Jo et al. (2020), who classify attackable sentences using BERT. Unlike the authors, we rank premises based on their attackability concerning the argument’s main claim, utilizing the learning-to-rank approach of Han et al. (2020). For the second step, similar to Wolf et al. (2019), we fine-tune a pre-trained transformer-based language model (Radford et al., 2018), in a multitask learning setting: next-token classification and counter-argument classification. In our experiments, we make use of the changemyview (CMV) dataset of Jo et al. (2020), where each instance is a post consisting of a title (say, an argume"
2021.findings-acl.159,D19-1561,0,0.0281432,"Missing"
2021.findings-acl.159,2020.acl-main.633,0,0.0107266,"whereas Chen et al. (2018) flipped the bias of claim-like news headlines using style transfer. Sato et al. (2015) generated full arguments in a largely rule-based way. Building on the model of rhetorical argumentation strategies by Wachsmuth et al. (2018a), El Baff et al. (2019) modeled argument synthesis as a language modeling task, and Schiller et al. (2020) studied the neural generation of arguments on a topic with controlled aspects and stance. Unlike all these, we deal with counter-arguments. Research exists for mining attack relations (Cocarascu and Toni, 2017; Chakrabarty et al., 2019; Orbach et al., 2020), mining counter-considerations from text (Peldszus and Stede, 2015), and retrieving counter-arguments (Wachsmuth et al., 2018b; Orbach et al., 2019). However, only the two works of Hua and Wang (2018, 2019) realy generated such arguments. Their latest neural approach takes an argument or claim as input and generates a counter-argument rebutting it. Differently, we consider countering an argument by attacking one of its premises, known as undermining (Walton, 2009). Part of our approach is to identify attackable premises, which can be studied from an argument quality perspective. That is, a pr"
2021.findings-acl.159,W15-0513,0,0.184859,"n’t an example of homophobia... Table 1: An example argument (claim + premises) and a counter-argument in response to it, taken from Reddit changemyview. The italicized premise part was quoted by the user who stated the counter-argument. Introduction Following Walton (2009), a counter-argument can be defined as an attack on a specific argument by arguing against either its claim (called rebuttal), the validity of reasoning of its premises toward its claim (undercut), or the validity of one of its premises (undermining). Not only the mining and retrieval of counter-arguments have been studied (Peldszus and Stede, 2015; Wachsmuth et al., 2018b), recent works also tackled the generation of counter-arguments. Among these, Bilu et al. (2015) and Hidey and McKeown (2019) studied the task of contrastive claim generation, the former in a partly rule-based manner, the latter data-driven. Moreover, Hua and Wang (2019) proposed a neural counter-argument generation approach. So far, however, research focused only on rebutting a given argument, ignoring the other aforementioned types. We expand this research by studying to what extent argument undermining can be utilized in counterargument generation. In argument unde"
2021.findings-acl.159,E17-1017,1,0.833237,"(2018, 2019) realy generated such arguments. Their latest neural approach takes an argument or claim as input and generates a counter-argument rebutting it. Differently, we consider countering an argument by attacking one of its premises, known as undermining (Walton, 2009). Part of our approach is to identify attackable premises, which can be studied from an argument quality perspective. That is, a premise is attackable when it lacks specific quality criteria. A significant body of research has studied argument quality assessment, with a comprehensive survey of quality criteria presented in Wachsmuth et al. (2017). Implicitly, we target criteria such as a premise’s acceptability or relevance. Still, we follow Jo et al. (2020) in deriving attackability from the sentences of posts that users attack in the Reddit forum CMV. These sentences represent premises supporting the claim encoded in a post’s title. The authors experimented with different features that potentially reflect weaknesses in the premises. Their best model for identifying attackable premises is a BERT-based classifier. We use their data to learn weak premise identification, but we address it as a learning-to-rank task. As for text generati"
2021.findings-acl.159,C18-1318,1,0.889685,"Missing"
2021.findings-acl.159,P18-1023,1,0.928982,"ia... Table 1: An example argument (claim + premises) and a counter-argument in response to it, taken from Reddit changemyview. The italicized premise part was quoted by the user who stated the counter-argument. Introduction Following Walton (2009), a counter-argument can be defined as an attack on a specific argument by arguing against either its claim (called rebuttal), the validity of reasoning of its premises toward its claim (undercut), or the validity of one of its premises (undermining). Not only the mining and retrieval of counter-arguments have been studied (Peldszus and Stede, 2015; Wachsmuth et al., 2018b), recent works also tackled the generation of counter-arguments. Among these, Bilu et al. (2015) and Hidey and McKeown (2019) studied the task of contrastive claim generation, the former in a partly rule-based manner, the latter data-driven. Moreover, Hua and Wang (2019) proposed a neural counter-argument generation approach. So far, however, research focused only on rebutting a given argument, ignoring the other aforementioned types. We expand this research by studying to what extent argument undermining can be utilized in counterargument generation. In argument undermining, the validity of"
2021.findings-acl.159,2020.emnlp-demos.6,0,0.0738354,"Missing"
2021.findings-emnlp.228,D19-1290,1,0.930048,"eform, though, the presence of what may be six million illegal aliens in this country exacts an economic and social toll. Table 1: (a) Sample text from the media frames corpus (Card et al., 2015). The bold sentence is labeled with the economic frame. Having reframed the sentence with the approach of this paper, the text remains largely coherent and topic-consistent while showing the legality frame (b) and crime frame (c), respectively. Framing is a rhetorical means to emphasize a perspective of an issue (de Vreese, 2005; Chong and Druckman, 2007). It is basically driven by argument selection (Ajjour et al., 2019) and, hence, it belongs to the inventio canon in particular (Aristo- replacing specific terms can be enough to reach a reframing effect. Consider in this regard a reporter tle and Roberts, 2004). The media employ framing who may prefer to use “undocumented worker” into reorient how audiences think (Chong and Druckman, 2007), or to promote a decided interpretation. stead of “illegal aliens” in left-leaning news (Webson et al., 2020). While still referring to the same For example, when talking about a certain law one may emphasize its economic impact or its conse- people, the former can provoke"
2021.findings-emnlp.228,P15-2072,0,0.134377,"sence of what may be six million illegal aliens in this country exacts an economic and social toll. (c) Crime Frame (reframed text) Key Congressional backers of the measure, sponsored by Senator Alan K. Simpson, Republican of Arizona, and Romano L. Mazzoli, Democrat of Kentucky, wanted a flexible spending limit. “Illegal aliens’ is a growing problem in the country,” says a spokesman for the measure’s sponsors. Without reform, though, the presence of what may be six million illegal aliens in this country exacts an economic and social toll. Table 1: (a) Sample text from the media frames corpus (Card et al., 2015). The bold sentence is labeled with the economic frame. Having reframed the sentence with the approach of this paper, the text remains largely coherent and topic-consistent while showing the legality frame (b) and crime frame (c), respectively. Framing is a rhetorical means to emphasize a perspective of an issue (de Vreese, 2005; Chong and Druckman, 2007). It is basically driven by argument selection (Ajjour et al., 2019) and, hence, it belongs to the inventio canon in particular (Aristo- replacing specific terms can be enough to reach a reframing effect. Consider in this regard a reporter tle"
2021.findings-emnlp.228,2021.naacl-main.394,0,0.120217,"Chong and Druckman, 2007), or to promote a decided interpretation. stead of “illegal aliens” in left-leaning news (Webson et al., 2020). While still referring to the same For example, when talking about a certain law one may emphasize its economic impact or its conse- people, the former can provoke a discussion of the economic impact of hiring them; the latter may quences regarding crime. raise issues of crime and possible deportation. Such Reframing means to change the perspective of low-level style reframing has been studied in recent an issue. It can be a strategy to communicate with work (Chakrabarty et al., 2021). opposing camps of audiences, and, sometimes, just 2683 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2683–2693 November 7–11, 2021. ©2021 Association for Computational Linguistics Usually, reframing requires rewriting entire sentences rather than single words or phrases. Table 1 illustrates the change of a sentence from the economic frame (a) to the legality frame (b) and the crime frame (c). While the original text emphasizes the cost of immigration reform, the legality-framed text quotes that “It’s time for Congress to take action,” and the crime-framed text"
2021.findings-emnlp.228,2020.findings-emnlp.383,1,0.753833,"that share an aspect (Ajjour et al., 2019). As for frame classification, most of the proposed approaches (Naderi and Hirst, 2017; Hartmann et al., 2019; Khanehzar et al., 2021) employ the media frames corpus (Card et al., 2015), which is built upon the 1 framing scheme of Boydstun et al. (2013). FollowEthical concerns regarding the correctness of reframed texts will be discussed in Section 8. ing these approaches, we utilize the media frames 2684 corpus to build the dataset for our task. The study of media frames is closely related to the analysis of bias and unfairness conveyed by the media (Chen et al., 2020a,b). For example, Chen et al. (2018) observed that the (potentially frame-specific) word choice may directly make a news article appear to have politically left or right bias. The only existing reframing approach that we are aware of is the one of Chakrabarty et al. (2021). In that work, a new model for reframing is developed by identifying phrases indicative for specific frames, and then replacing phrases that belong to the source frame with some that belong to the target one. As such, most of the content of the reframed text is kept, and only a few words are replaced. In contrast, we deal w"
2021.findings-emnlp.228,2020.nlpcss-1.16,1,0.783591,"that share an aspect (Ajjour et al., 2019). As for frame classification, most of the proposed approaches (Naderi and Hirst, 2017; Hartmann et al., 2019; Khanehzar et al., 2021) employ the media frames corpus (Card et al., 2015), which is built upon the 1 framing scheme of Boydstun et al. (2013). FollowEthical concerns regarding the correctness of reframed texts will be discussed in Section 8. ing these approaches, we utilize the media frames 2684 corpus to build the dataset for our task. The study of media frames is closely related to the analysis of bias and unfairness conveyed by the media (Chen et al., 2020a,b). For example, Chen et al. (2018) observed that the (potentially frame-specific) word choice may directly make a news article appear to have politically left or right bias. The only existing reframing approach that we are aware of is the one of Chakrabarty et al. (2021). In that work, a new model for reframing is developed by identifying phrases indicative for specific frames, and then replacing phrases that belong to the source frame with some that belong to the target one. As such, most of the content of the reframed text is kept, and only a few words are replaced. In contrast, we deal w"
2021.findings-emnlp.228,W18-6509,1,0.812941,"2019). As for frame classification, most of the proposed approaches (Naderi and Hirst, 2017; Hartmann et al., 2019; Khanehzar et al., 2021) employ the media frames corpus (Card et al., 2015), which is built upon the 1 framing scheme of Boydstun et al. (2013). FollowEthical concerns regarding the correctness of reframed texts will be discussed in Section 8. ing these approaches, we utilize the media frames 2684 corpus to build the dataset for our task. The study of media frames is closely related to the analysis of bias and unfairness conveyed by the media (Chen et al., 2020a,b). For example, Chen et al. (2018) observed that the (potentially frame-specific) word choice may directly make a news article appear to have politically left or right bias. The only existing reframing approach that we are aware of is the one of Chakrabarty et al. (2021). In that work, a new model for reframing is developed by identifying phrases indicative for specific frames, and then replacing phrases that belong to the source frame with some that belong to the target one. As such, most of the content of the reframed text is kept, and only a few words are replaced. In contrast, we deal with reframing at the sentence level,"
2021.findings-emnlp.228,N19-1142,0,0.0162792,"eneric. For example, the possible issue-specific frames for the topic of Internet may include online communication and online services, whereas the generic ones include economically optimistic and political criticism (Rössler, 2001). In this paper, we adopt the following narrow definition of frames: “a frame is an emphasis in the salience of different aspects of a topic” (de Vreese, 2005). In the area of natural language processing, media frame analysis is a relatively new topic. Most existing works adopt the frame definition in social science, where framing refers to a choice of perspective (Hartmann et al., 2019). A more specific definition, which targets the argumentation contexts, defines a frame as a set of arguments that share an aspect (Ajjour et al., 2019). As for frame classification, most of the proposed approaches (Naderi and Hirst, 2017; Hartmann et al., 2019; Khanehzar et al., 2021) employ the media frames corpus (Card et al., 2015), which is built upon the 1 framing scheme of Boydstun et al. (2013). FollowEthical concerns regarding the correctness of reframed texts will be discussed in Section 8. ing these approaches, we utilize the media frames 2684 corpus to build the dataset for our tas"
2021.findings-emnlp.228,2021.naacl-main.174,0,0.0326384,"tion of frames: “a frame is an emphasis in the salience of different aspects of a topic” (de Vreese, 2005). In the area of natural language processing, media frame analysis is a relatively new topic. Most existing works adopt the frame definition in social science, where framing refers to a choice of perspective (Hartmann et al., 2019). A more specific definition, which targets the argumentation contexts, defines a frame as a set of arguments that share an aspect (Ajjour et al., 2019). As for frame classification, most of the proposed approaches (Naderi and Hirst, 2017; Hartmann et al., 2019; Khanehzar et al., 2021) employ the media frames corpus (Card et al., 2015), which is built upon the 1 framing scheme of Boydstun et al. (2013). FollowEthical concerns regarding the correctness of reframed texts will be discussed in Section 8. ing these approaches, we utilize the media frames 2684 corpus to build the dataset for our task. The study of media frames is closely related to the analysis of bias and unfairness conveyed by the media (Chen et al., 2020a,b). For example, Chen et al. (2018) observed that the (potentially frame-specific) word choice may directly make a news article appear to have politically le"
2021.findings-emnlp.228,2020.emnlp-main.491,0,0.0383988,"t of the reframed text is kept, and only a few words are replaced. In contrast, we deal with reframing at the sentence level, and we do not require parallel training pairs or a dictionary to correlate words and frames. In principle, reframing can be seen as a style transfer task (Shardlow, 2014; Shen et al., 2017; Chen et al., 2018). Research on text style transfer focus on the areas of sentiment transfer (e.g., replacing ‘gross’ by ‘awesome’) (Shen et al., 2017) and text simplification (e.g., replacing ‘perched’ by ‘sat’) (Shardlow, 2014). We applied recent style transfer models to our task (Mai et al., 2020; Shen et al., 2020), observing that these models perform very poorly (e.g., generating unreadable text). 3 Approach We now present our approach to sentence-level reframing. We discuss how we tackle the reframing problem as a fill-in-the-blank task, and we propose three training strategies to generate a sentence that is framed as desired and that fits to the surrounding text. Figure 1 illustrates our approach. 3.1 Reframing as a Fill-in-the-Blank Task Input s1 Key Congressional backers ... wanted a flexible spending limit. s2 MASK s3 ... illegal aliens in this country exacts an economic and so"
2021.findings-emnlp.228,naderi-hirst-2017-classifying,0,0.0278332,"his paper, we adopt the following narrow definition of frames: “a frame is an emphasis in the salience of different aspects of a topic” (de Vreese, 2005). In the area of natural language processing, media frame analysis is a relatively new topic. Most existing works adopt the frame definition in social science, where framing refers to a choice of perspective (Hartmann et al., 2019). A more specific definition, which targets the argumentation contexts, defines a frame as a set of arguments that share an aspect (Ajjour et al., 2019). As for frame classification, most of the proposed approaches (Naderi and Hirst, 2017; Hartmann et al., 2019; Khanehzar et al., 2021) employ the media frames corpus (Card et al., 2015), which is built upon the 1 framing scheme of Boydstun et al. (2013). FollowEthical concerns regarding the correctness of reframed texts will be discussed in Section 8. ing these approaches, we utilize the media frames 2684 corpus to build the dataset for our task. The study of media frames is closely related to the analysis of bias and unfairness conveyed by the media (Chen et al., 2020a,b). For example, Chen et al. (2018) observed that the (potentially frame-specific) word choice may directly m"
2021.findings-emnlp.228,2020.emnlp-main.335,0,0.0377094,"ely. Framing is a rhetorical means to emphasize a perspective of an issue (de Vreese, 2005; Chong and Druckman, 2007). It is basically driven by argument selection (Ajjour et al., 2019) and, hence, it belongs to the inventio canon in particular (Aristo- replacing specific terms can be enough to reach a reframing effect. Consider in this regard a reporter tle and Roberts, 2004). The media employ framing who may prefer to use “undocumented worker” into reorient how audiences think (Chong and Druckman, 2007), or to promote a decided interpretation. stead of “illegal aliens” in left-leaning news (Webson et al., 2020). While still referring to the same For example, when talking about a certain law one may emphasize its economic impact or its conse- people, the former can provoke a discussion of the economic impact of hiring them; the latter may quences regarding crime. raise issues of crime and possible deportation. Such Reframing means to change the perspective of low-level style reframing has been studied in recent an issue. It can be a strategy to communicate with work (Chakrabarty et al., 2021). opposing camps of audiences, and, sometimes, just 2683 Findings of the Association for Computational Linguis"
2021.findings-emnlp.228,2020.emnlp-demos.6,0,0.0253146,"Missing"
C10-1127,J96-2004,0,0.0184661,"ime, a trend indicator and the author of a statement. Altogether, 2,075 statements are tagged in this way. As in Figure 3, only information that refers to a statement on revenue (typed in bold face) is annotated. These annotations may be spread across the text. The source documents were manually selected and prepared by our industrial partners, and two of their employees annotated the plain document text. With respect to the statement annotations, a preceding pilot study yielded substantial interannotator agreement, as indicated by the value κ = 0.79 of the conservative measure Cohen’s Kappa (Carletta, 1996). Additionally, we performed a manual correction process for each annotated document to improve consistency. Experimental Setup To find candidate sentences, we implemented a sentence splitter that can handle article elements such as subheadings, URLs, or bracketed sentences. We then constructed sophisticated, but efficient regular expressions for time and money. They do not represent correct language, in general, but model the structure of temporal and monetary entities, and use word lists provided by domain experts on the lowest level.4 For feature computation, we assumed that the closest pai"
C10-1127,C08-1060,0,0.031082,"like us, the authors put much emphasis on retrieval aspects and applied dependency grammar parsing to identify market statements. As a consequence their approach suffers from the limitation to a small number of predefined sentence structures. While we obtain market forecasts by extracting expert statements from the Web, related approaches derive them from past market behavior and quantitative news data. Koppel and Shtrimberg (2004) studied the effect of news on financial markets. Lavrenko et al. (2000) used timeseries analysis and language models to predict stock market prices and, similarly, Lerman et al. (2008) proposed a system for forecasting public opinion based on concurrent modeling of news articles and market history. Another related field is opinion mining in the sense that it relies on the aggregation of individual statements. Glance et al. (2005) inferred marketing intelligence from opinions in online discussions. Liu et al. (2007) examined the effect of Weblogs on box office revenues and combined time-series with sentiment analysis to predict the sales performance of movies. The mentioned approaches are intended to reflect or to predict present developments and, therefore, primarily help f"
C10-1127,P00-1010,0,0.081855,"Missing"
C10-1127,C02-1053,0,0.0847143,"Missing"
C10-1127,W09-1119,0,0.0204774,"ressions, which represent the complex but finite structures of such phrases, we can achieve nearly perfect recall in recognition (see Section 5). We apply named entity recognition (NER) of organizations and markets in this stage, too, so we can relate statements to the appropriate subjects, later on. Note that market names do not follow a unique naming scheme, but we observed that they often involve similar phrase patterns that can be exploited as features. NER is usually done by sequence labeling, and we use heuristic beam search due to our effort to design a highly efficient overall system. Ratinov and Roth (2009) have shown for the CoNLL-2003 shared task that Greedy decoding (i.e., beam search of width 1) is competitive to the widely used Viterbi algorithm while being over 100 times faster at the same time. 3.4 Identify Statements Based on time and money information, sentences that represent a statement Sχ can be identified. Such a sentence gives us valuable hints on which temporal and monetary entity stick together and how to interpret them in relation. Additionally, it serves as evidence for the statement’s correctness (or incorrectness). Every sentence with at least one temporal and one monetary en"
C12-2125,P10-1014,0,0.0341808,"ngly, we do not deal with joint extraction, which often suffers from its computational cost (Poon and Domingos, 2007). In (Wachsmuth et al., 2011), we introduced a generic method to construct efficient pipelines that achieves run-time improvements of one order of magnitude without harming a pipeline’s effectiveness. Similarly, Shen et al. (2007) and Doan et al. (2009) optimize schedules in a declarative extraction framework. These works give only heuristic hints on the reasons behind empirical results. While some algebraic foundations of scheduling are established for rule-based approaches by Chiticariu et al. (2010), we explain the determinants of efficiency for any set of extraction algorithms. To the best of our knowledge, we are the first to address scheduling in information extraction with dynamic programming, which relies on dividing a problem into smaller subproblems and solving recurring subproblems only once (Cormen et al., 2009). In our research we analyze the impact of text types on the efficiency of information extraction pipelines. Existing work on text types in information extraction mainly deals with the filtering of promising documents, such as (Agichtein and Gravano, 2003). Instead, we id"
C12-2125,P05-1045,0,0.0948357,"ormer contains 752 online business news articles with 21,586 sentences, whereas 553 mixed classic newspaper articles with 12,713 sentences refer to the latter. Task. The conjunctive filtering task that we consider emanates from the purpose of the Revenue corpus, namely, we define a text unit to be classified as relevant as a sentence that represents a forecast and that contains a money entity, a time entity, and an organization name. Algorithms. We employed four algorithms that filter text units: regex-based money and time entity recognizers A M and A T , the CRF-based STANFORD NER system AN (Finkel et al., 2005; Faruqui and Padó, 2010) for organization names, and the SVM-based forecast event detector A F from (Wachsmuth et al., 2011) that needs time entities as input. Further algorithms were used only as preprocessors. In all experiments we executed each preprocessing algorithm right before its output was needed. Hence, we simply speak of the algorithm set A1 = {A M , A T , AN , A F } in the following without loss of generality. All algorithms in A1 operate on sentence-level. Application of the Pipeline Viterbi Algorithm. On both corpora, we executed all applicable ( j) ( j) ( j) pipelines Πi for A1"
C12-2125,W06-1673,0,0.0285444,"ploys special index structures and fast extraction algorithms, but it is restricted to simple relations. In contrast, we target at template filling tasks that relate several entities to events (Cunningham, 2006). We approach such tasks with classic pipelines where each algorithm takes on one analysis, e.g. a certain type of entity recognition (Grishman, 1997). The decisions within a pipeline can be viewed as irreversible, which allows to perform filtering. Hence, an algorithm can never make up for false classifications of its predecessors, as in iterative or probabilistic pipeline approaches (Finkel et al., 2006; Hollingshead and Roark, 2007). Accordingly, we do not deal with joint extraction, which often suffers from its computational cost (Poon and Domingos, 2007). In (Wachsmuth et al., 2011), we introduced a generic method to construct efficient pipelines that achieves run-time improvements of one order of magnitude without harming a pipeline’s effectiveness. Similarly, Shen et al. (2007) and Doan et al. (2009) optimize schedules in a declarative extraction framework. These works give only heuristic hints on the reasons behind empirical results. While some algebraic foundations of scheduling are e"
C12-2125,P07-1120,0,0.0176629,"tructures and fast extraction algorithms, but it is restricted to simple relations. In contrast, we target at template filling tasks that relate several entities to events (Cunningham, 2006). We approach such tasks with classic pipelines where each algorithm takes on one analysis, e.g. a certain type of entity recognition (Grishman, 1997). The decisions within a pipeline can be viewed as irreversible, which allows to perform filtering. Hence, an algorithm can never make up for false classifications of its predecessors, as in iterative or probabilistic pipeline approaches (Finkel et al., 2006; Hollingshead and Roark, 2007). Accordingly, we do not deal with joint extraction, which often suffers from its computational cost (Poon and Domingos, 2007). In (Wachsmuth et al., 2011), we introduced a generic method to construct efficient pipelines that achieves run-time improvements of one order of magnitude without harming a pipeline’s effectiveness. Similarly, Shen et al. (2007) and Doan et al. (2009) optimize schedules in a declarative extraction framework. These works give only heuristic hints on the reasons behind empirical results. While some algebraic foundations of scheduling are established for rule-based appro"
C12-2125,C08-5001,0,0.0440676,"Missing"
C12-2125,W11-1802,0,0.0709323,"Missing"
C12-2125,C10-1127,1,0.322235,"ed to U, which leads to an overall upper bound of O(m3 · t ma x (U)).3 4 The Impact of Text Types on the Efficiency of Pipelines In this section, we first analyze the influence of text types on the optimality of schedules. Then, we reveal that the efficiency of an information extraction pipeline is governed by the distribution of relevant entities, relations, and events in the input texts. In all experiments, we evaluated the following set-up on a 2 GHz Intel Core 2 Duo MacBook with 4 GB memory: Data. We processed the training sets of two German text corpora: the Revenue corpus introduced in (Wachsmuth et al., 2010) and the corpus of the CoNLL’03 shared task (Tjong Kim Sang and De Meulder, 2003).4 The former contains 752 online business news articles with 21,586 sentences, whereas 553 mixed classic newspaper articles with 12,713 sentences refer to the latter. Task. The conjunctive filtering task that we consider emanates from the purpose of the Revenue corpus, namely, we define a text unit to be classified as relevant as a sentence that represents a forecast and that contains a money entity, a time entity, and an organization name. Algorithms. We employed four algorithms that filter text units: regex-bas"
C14-1053,C10-1103,0,\N,Missing
C14-1053,baccianella-etal-2010-sentiwordnet,0,\N,Missing
C14-1053,D08-1020,0,\N,Missing
C14-1053,P12-2041,0,\N,Missing
C14-1053,P04-1035,0,\N,Missing
C14-1053,D09-1155,0,\N,Missing
C14-1053,I11-1071,1,\N,Missing
C14-1053,P05-1015,0,\N,Missing
C14-1053,W02-1011,0,\N,Missing
C14-1053,P13-1160,0,\N,Missing
C14-1053,P10-1114,1,\N,Missing
C14-1053,P13-1119,1,\N,Missing
C14-1053,D13-1170,0,\N,Missing
C14-1053,C12-1113,0,\N,Missing
C16-1158,C16-1324,1,0.460322,"Missing"
C16-1158,W15-0514,0,0.0600879,"Missing"
C16-1158,W98-0303,0,0.625226,"dress a respective essay scoring task. The argument strength approach adopts ideas from the approach of Stab and Gurevych (2014b), but it finds structure heuristically only and, thus, does not perform argument mining. In the paper at hand, we fill this gap, i.e., we exploit the output of an argument mining approach trained on ground-truth data to assess the four quality dimensions. In general, numerous approaches exist that assess essay quality. Classical essay scoring often focuses on grammar, vocabulary, and similar (Dikli, 2006), partly employing structural features like discourse markers (Burstein et al., 1998). In contrast, Song et al. (2014) study whether essays comply with critical 1681 essay level sentence level Argument1 ... Argument2 ... ... ... ADU type21 ... ADU type2m ... Argumentk ... ... Argumentative structure paragraph level Figure 2: Application-oriented model of the argumentative structure of essays. Each paragraph is seen as an argument, defined as a sequence of sentence-level ADU types ∈ {Thesis, Conclusion, Premise, None}. questions of an applied argumentation scheme. On manual annotations, they find correlations between an essay’s score and the number of answered questions. Closer"
C16-1158,P12-2041,0,0.0159023,"ustering. In (Wachsmuth et al., 2015), we further abstract the flows to optimize their domain generality in global sentiment analysis. Discourse structure, discourse functions, and sentiment flows serve as baselines in our experiments in Section 5. Unlike all mentioned approaches, however, we analyze the output of argument mining. In particular, we use the mined structure to assess argumentation quality. While there is no common definition of such quality, Blair (2012) specifies the goals of relevance, acceptability, and sufficiency for arguments. To find accepted arguments in debate portals, Cabrio and Villata (2012) analyze attack relations between arguments based on the framework of Dung (1995). Rinott et al. (2015) detect three types of evidence in Wikipedia articles, and Boltuži´c and Šnajder (2015) seek for the prominent arguments in online debates. Here, we are not interested in the quality of single arguments but rather in the quality of a complete argumentation, namely, the argumentation found in a persuasive essay. We target quality dimensions of persuasive essays that are directly related to argumentation: organization (Persing et al., 2010), thesis clarity (Persing and Ng, 2013), prompt adheren"
C16-1158,C12-1041,0,0.0473775,"texts. Habernal and Gurevych (2015) adapt the fine-grained argument model of Toulmin (1958) for web texts. As detailed in Section 3, we rely on the essay-oriented model of Stab and Gurevych (2014a). For us, mining is a preprocessing step only, though. For statistical reliability, we restrict our view to the units of arguments. Like Moens et al. (2007), we classify units on the sentence level, but we consider four different unit types. This results in a sequential structure comparable to argumentative zones (Teufel et al., 2009). The latter have also been exploited for downstream applications (Contractor et al., 2012). Our focus is the analysis of argumentative structure. Related structures have been analyzed before: To measure text coherence, Feng et al. (2014) build on discourse structure (Mann and Thompson, 1988), which is connected but not equivalent to argumentative structure (Peldszus and Stede, 2013). Faulkner (2014) classifies the stance of essays using argument representations derived from dependency parse trees. For essay scoring, Persing et al. (2010) detect the discourse function of each paragraph in an essay in order to align the resulting function sequence with known function sequences. Simil"
C16-1158,C14-1089,0,0.0102423,"-oriented model of Stab and Gurevych (2014a). For us, mining is a preprocessing step only, though. For statistical reliability, we restrict our view to the units of arguments. Like Moens et al. (2007), we classify units on the sentence level, but we consider four different unit types. This results in a sequential structure comparable to argumentative zones (Teufel et al., 2009). The latter have also been exploited for downstream applications (Contractor et al., 2012). Our focus is the analysis of argumentative structure. Related structures have been analyzed before: To measure text coherence, Feng et al. (2014) build on discourse structure (Mann and Thompson, 1988), which is connected but not equivalent to argumentative structure (Peldszus and Stede, 2013). Faulkner (2014) classifies the stance of essays using argument representations derived from dependency parse trees. For essay scoring, Persing et al. (2010) detect the discourse function of each paragraph in an essay in order to align the resulting function sequence with known function sequences. Similarly, we capture a review’s overall structure in (Wachsmuth et al., 2014a) by comparing the local sentiment flow in the review to a set of common f"
C16-1158,P16-2089,0,0.198839,"t1 ... Argument2 ... ... ... ADU type21 ... ADU type2m ... Argumentk ... ... Argumentative structure paragraph level Figure 2: Application-oriented model of the argumentative structure of essays. Each paragraph is seen as an argument, defined as a sequence of sentence-level ADU types ∈ {Thesis, Conclusion, Premise, None}. questions of an applied argumentation scheme. On manual annotations, they find correlations between an essay’s score and the number of answered questions. Closer to our work, Ong et al. (2014) analyze argumentative discourse units found with a simple heuristic algorithm. And Ghosh et al. (2016) even derive features from argument mining, although they hardly exploit structure. Either way, all these approaches assign overall essay scores only, leaving unclear to what extent argumentation quality is captured. 3 Mining Argumentative Structure This paper does not aim at new approaches to argument mining. Still, the effectiveness of mining as well as the underlying argumentation model directly affect the analysis of argumentative structure. Therefore, we summarize our mining approach in the following.1 3.1 An Application-Oriented Model of Argumentative Structure We focus on the argumentat"
C16-1158,D15-1255,0,0.0555108,"common patterns in the argumentative structure of persuasive essays statistically. 3. We provide the new state of the art approach to two argumentation-related essay scoring tasks. 2 Related Work Several approaches to argument mining have been introduced, often grounded in argumentation theory: Matching the argumentation schemes of Walton et al. (2008), Mochales and Moens (2011) model each argument in legal cases as a conclusion with a set of premises. Based on (Freeman, 2011), Peldszus and Stede (2015) capture support and attack relations between argumentative discourse units of microtexts. Habernal and Gurevych (2015) adapt the fine-grained argument model of Toulmin (1958) for web texts. As detailed in Section 3, we rely on the essay-oriented model of Stab and Gurevych (2014a). For us, mining is a preprocessing step only, though. For statistical reliability, we restrict our view to the units of arguments. Like Moens et al. (2007), we classify units on the sentence level, but we consider four different unit types. This results in a sequential structure comparable to argumentative zones (Teufel et al., 2009). The latter have also been exploited for downstream applications (Contractor et al., 2012). Our focus"
C16-1158,W14-2104,0,0.0755636,"Song et al. (2014) study whether essays comply with critical 1681 essay level sentence level Argument1 ... Argument2 ... ... ... ADU type21 ... ADU type2m ... Argumentk ... ... Argumentative structure paragraph level Figure 2: Application-oriented model of the argumentative structure of essays. Each paragraph is seen as an argument, defined as a sequence of sentence-level ADU types ∈ {Thesis, Conclusion, Premise, None}. questions of an applied argumentation scheme. On manual annotations, they find correlations between an essay’s score and the number of answered questions. Closer to our work, Ong et al. (2014) analyze argumentative discourse units found with a simple heuristic algorithm. And Ghosh et al. (2016) even derive features from argument mining, although they hardly exploit structure. Either way, all these approaches assign overall essay scores only, leaving unclear to what extent argumentation quality is captured. 3 Mining Argumentative Structure This paper does not aim at new approaches to argument mining. Still, the effectiveness of mining as well as the underlying argumentation model directly affect the analysis of argumentative structure. Therefore, we summarize our mining approach in"
C16-1158,D15-1110,0,0.15759,"earch: 1. We examine the use of argument mining for assessing argumentation quality for the first time. 2. We reveal common patterns in the argumentative structure of persuasive essays statistically. 3. We provide the new state of the art approach to two argumentation-related essay scoring tasks. 2 Related Work Several approaches to argument mining have been introduced, often grounded in argumentation theory: Matching the argumentation schemes of Walton et al. (2008), Mochales and Moens (2011) model each argument in legal cases as a conclusion with a set of premises. Based on (Freeman, 2011), Peldszus and Stede (2015) capture support and attack relations between argumentative discourse units of microtexts. Habernal and Gurevych (2015) adapt the fine-grained argument model of Toulmin (1958) for web texts. As detailed in Section 3, we rely on the essay-oriented model of Stab and Gurevych (2014a). For us, mining is a preprocessing step only, though. For statistical reliability, we restrict our view to the units of arguments. Like Moens et al. (2007), we classify units on the sentence level, but we consider four different unit types. This results in a sequential structure comparable to argumentative zones (Teu"
C16-1158,P13-1026,0,0.484215,"d structure. So far, however, the benefit of this structure remains largely unexplored (see Section 2 for details). This paper puts the assessment step into the focus. We ask if, to what extent, and how the output of argument mining can be leveraged to assess the argumentation quality of a text. In particular, we consider these questions for persuasive student essays. Such an essay seeks to justify a thesis on a given topic via a composition of arguments. Different quality dimensions related to argumentation have been studied for persuasive essays, such as the clarity of the justified thesis (Persing and Ng, 2013). Also, argument mining has already been performed effectively on persuasive essays (Stab and Gurevych, 2014b). We build on the outlined research in that we use argument mining to assess an essay’s argumentation quality. First, we adapt a state-of-the-art approach for mining argumentative discourse units (Section 3). Then, we apply the approach to all essays from the International Corpus of Learner English (Granger et al., 2009) in order to analyze their argumentative structure. We find statistically reliable patterns that yield insights into how students argue in essays. From these, we derive"
C16-1158,P14-1144,0,0.516928,"lyze attack relations between arguments based on the framework of Dung (1995). Rinott et al. (2015) detect three types of evidence in Wikipedia articles, and Boltuži´c and Šnajder (2015) seek for the prominent arguments in online debates. Here, we are not interested in the quality of single arguments but rather in the quality of a complete argumentation, namely, the argumentation found in a persuasive essay. We target quality dimensions of persuasive essays that are directly related to argumentation: organization (Persing et al., 2010), thesis clarity (Persing and Ng, 2013), prompt adherence (Persing and Ng, 2014), and argument strength (Persing and Ng, 2015). In all four publications, sophisticated features are engineered to address a respective essay scoring task. The argument strength approach adopts ideas from the approach of Stab and Gurevych (2014b), but it finds structure heuristically only and, thus, does not perform argument mining. In the paper at hand, we fill this gap, i.e., we exploit the output of an argument mining approach trained on ground-truth data to assess the four quality dimensions. In general, numerous approaches exist that assess essay quality. Classical essay scoring often foc"
C16-1158,P15-1053,0,0.49707,"on the framework of Dung (1995). Rinott et al. (2015) detect three types of evidence in Wikipedia articles, and Boltuži´c and Šnajder (2015) seek for the prominent arguments in online debates. Here, we are not interested in the quality of single arguments but rather in the quality of a complete argumentation, namely, the argumentation found in a persuasive essay. We target quality dimensions of persuasive essays that are directly related to argumentation: organization (Persing et al., 2010), thesis clarity (Persing and Ng, 2013), prompt adherence (Persing and Ng, 2014), and argument strength (Persing and Ng, 2015). In all four publications, sophisticated features are engineered to address a respective essay scoring task. The argument strength approach adopts ideas from the approach of Stab and Gurevych (2014b), but it finds structure heuristically only and, thus, does not perform argument mining. In the paper at hand, we fill this gap, i.e., we exploit the output of an argument mining approach trained on ground-truth data to assess the four quality dimensions. In general, numerous approaches exist that assess essay quality. Classical essay scoring often focuses on grammar, vocabulary, and similar (Dikl"
C16-1158,D10-1023,0,0.392736,"sequential structure comparable to argumentative zones (Teufel et al., 2009). The latter have also been exploited for downstream applications (Contractor et al., 2012). Our focus is the analysis of argumentative structure. Related structures have been analyzed before: To measure text coherence, Feng et al. (2014) build on discourse structure (Mann and Thompson, 1988), which is connected but not equivalent to argumentative structure (Peldszus and Stede, 2013). Faulkner (2014) classifies the stance of essays using argument representations derived from dependency parse trees. For essay scoring, Persing et al. (2010) detect the discourse function of each paragraph in an essay in order to align the resulting function sequence with known function sequences. Similarly, we capture a review’s overall structure in (Wachsmuth et al., 2014a) by comparing the local sentiment flow in the review to a set of common flow patterns that are learned through clustering. In (Wachsmuth et al., 2015), we further abstract the flows to optimize their domain generality in global sentiment analysis. Discourse structure, discourse functions, and sentiment flows serve as baselines in our experiments in Section 5. Unlike all mentio"
C16-1158,D15-1050,0,0.0919594,"global sentiment analysis. Discourse structure, discourse functions, and sentiment flows serve as baselines in our experiments in Section 5. Unlike all mentioned approaches, however, we analyze the output of argument mining. In particular, we use the mined structure to assess argumentation quality. While there is no common definition of such quality, Blair (2012) specifies the goals of relevance, acceptability, and sufficiency for arguments. To find accepted arguments in debate portals, Cabrio and Villata (2012) analyze attack relations between arguments based on the framework of Dung (1995). Rinott et al. (2015) detect three types of evidence in Wikipedia articles, and Boltuži´c and Šnajder (2015) seek for the prominent arguments in online debates. Here, we are not interested in the quality of single arguments but rather in the quality of a complete argumentation, namely, the argumentation found in a persuasive essay. We target quality dimensions of persuasive essays that are directly related to argumentation: organization (Persing et al., 2010), thesis clarity (Persing and Ng, 2013), prompt adherence (Persing and Ng, 2014), and argument strength (Persing and Ng, 2015). In all four publications, soph"
C16-1158,D13-1170,0,0.00217968,"Missing"
C16-1158,W14-2110,0,0.069122,". The argument strength approach adopts ideas from the approach of Stab and Gurevych (2014b), but it finds structure heuristically only and, thus, does not perform argument mining. In the paper at hand, we fill this gap, i.e., we exploit the output of an argument mining approach trained on ground-truth data to assess the four quality dimensions. In general, numerous approaches exist that assess essay quality. Classical essay scoring often focuses on grammar, vocabulary, and similar (Dikli, 2006), partly employing structural features like discourse markers (Burstein et al., 1998). In contrast, Song et al. (2014) study whether essays comply with critical 1681 essay level sentence level Argument1 ... Argument2 ... ... ... ADU type21 ... ADU type2m ... Argumentk ... ... Argumentative structure paragraph level Figure 2: Application-oriented model of the argumentative structure of essays. Each paragraph is seen as an argument, defined as a sequence of sentence-level ADU types ∈ {Thesis, Conclusion, Premise, None}. questions of an applied argumentation scheme. On manual annotations, they find correlations between an essay’s score and the number of answered questions. Closer to our work, Ong et al. (2014) a"
C16-1158,C14-1142,0,0.299109,"details). This paper puts the assessment step into the focus. We ask if, to what extent, and how the output of argument mining can be leveraged to assess the argumentation quality of a text. In particular, we consider these questions for persuasive student essays. Such an essay seeks to justify a thesis on a given topic via a composition of arguments. Different quality dimensions related to argumentation have been studied for persuasive essays, such as the clarity of the justified thesis (Persing and Ng, 2013). Also, argument mining has already been performed effectively on persuasive essays (Stab and Gurevych, 2014b). We build on the outlined research in that we use argument mining to assess an essay’s argumentation quality. First, we adapt a state-of-the-art approach for mining argumentative discourse units (Section 3). Then, we apply the approach to all essays from the International Corpus of Learner English (Granger et al., 2009) in order to analyze their argumentative structure. We find statistically reliable patterns that yield insights into how students argue in essays. From these, we derive novel solely structure-oriented features for machine learning (Section 4). Finally, we tackle essay scoring"
C16-1158,D14-1006,0,0.492205,"details). This paper puts the assessment step into the focus. We ask if, to what extent, and how the output of argument mining can be leveraged to assess the argumentation quality of a text. In particular, we consider these questions for persuasive student essays. Such an essay seeks to justify a thesis on a given topic via a composition of arguments. Different quality dimensions related to argumentation have been studied for persuasive essays, such as the clarity of the justified thesis (Persing and Ng, 2013). Also, argument mining has already been performed effectively on persuasive essays (Stab and Gurevych, 2014b). We build on the outlined research in that we use argument mining to assess an essay’s argumentation quality. First, we adapt a state-of-the-art approach for mining argumentative discourse units (Section 3). Then, we apply the approach to all essays from the International Corpus of Learner English (Granger et al., 2009) in order to analyze their argumentative structure. We find statistically reliable patterns that yield insights into how students argue in essays. From these, we derive novel solely structure-oriented features for machine learning (Section 4). Finally, we tackle essay scoring"
C16-1158,D09-1155,0,0.00791671,"15) capture support and attack relations between argumentative discourse units of microtexts. Habernal and Gurevych (2015) adapt the fine-grained argument model of Toulmin (1958) for web texts. As detailed in Section 3, we rely on the essay-oriented model of Stab and Gurevych (2014a). For us, mining is a preprocessing step only, though. For statistical reliability, we restrict our view to the units of arguments. Like Moens et al. (2007), we classify units on the sentence level, but we consider four different unit types. This results in a sequential structure comparable to argumentative zones (Teufel et al., 2009). The latter have also been exploited for downstream applications (Contractor et al., 2012). Our focus is the analysis of argumentative structure. Related structures have been analyzed before: To measure text coherence, Feng et al. (2014) build on discourse structure (Mann and Thompson, 1988), which is connected but not equivalent to argumentative structure (Peldszus and Stede, 2013). Faulkner (2014) classifies the stance of essays using argument representations derived from dependency parse trees. For essay scoring, Persing et al. (2010) detect the discourse function of each paragraph in an e"
C16-1158,C14-1053,1,0.954964,"ture. Related structures have been analyzed before: To measure text coherence, Feng et al. (2014) build on discourse structure (Mann and Thompson, 1988), which is connected but not equivalent to argumentative structure (Peldszus and Stede, 2013). Faulkner (2014) classifies the stance of essays using argument representations derived from dependency parse trees. For essay scoring, Persing et al. (2010) detect the discourse function of each paragraph in an essay in order to align the resulting function sequence with known function sequences. Similarly, we capture a review’s overall structure in (Wachsmuth et al., 2014a) by comparing the local sentiment flow in the review to a set of common flow patterns that are learned through clustering. In (Wachsmuth et al., 2015), we further abstract the flows to optimize their domain generality in global sentiment analysis. Discourse structure, discourse functions, and sentiment flows serve as baselines in our experiments in Section 5. Unlike all mentioned approaches, however, we analyze the output of argument mining. In particular, we use the mined structure to assess argumentation quality. While there is no common definition of such quality, Blair (2012) specifies t"
C16-1158,D15-1072,1,0.814067,"), which is connected but not equivalent to argumentative structure (Peldszus and Stede, 2013). Faulkner (2014) classifies the stance of essays using argument representations derived from dependency parse trees. For essay scoring, Persing et al. (2010) detect the discourse function of each paragraph in an essay in order to align the resulting function sequence with known function sequences. Similarly, we capture a review’s overall structure in (Wachsmuth et al., 2014a) by comparing the local sentiment flow in the review to a set of common flow patterns that are learned through clustering. In (Wachsmuth et al., 2015), we further abstract the flows to optimize their domain generality in global sentiment analysis. Discourse structure, discourse functions, and sentiment flows serve as baselines in our experiments in Section 5. Unlike all mentioned approaches, however, we analyze the output of argument mining. In particular, we use the mined structure to assess argumentation quality. While there is no common definition of such quality, Blair (2012) specifies the goals of relevance, acceptability, and sufficiency for arguments. To find accepted arguments in debate portals, Cabrio and Villata (2012) analyze att"
C16-1324,D10-1023,0,\N,Missing
C16-1324,W03-1017,0,\N,Missing
C16-1324,P14-5010,0,\N,Missing
C16-1324,W15-0505,1,\N,Missing
C16-1324,W14-2105,0,\N,Missing
C16-1324,D15-1255,0,\N,Missing
C16-1324,W14-2109,0,\N,Missing
C16-1324,W14-2106,0,\N,Missing
C16-1324,D15-1110,0,\N,Missing
C16-1324,D15-1072,1,\N,Missing
C16-1324,N16-1165,1,\N,Missing
C16-1324,C16-1158,1,\N,Missing
C16-1324,W03-2102,0,\N,Missing
C16-1324,E12-1085,0,\N,Missing
C16-1324,D15-1050,0,\N,Missing
C18-1318,W17-5115,1,0.938912,"acter, and (3) pathos, the evoking of the right emotions in the target audience. As Aristotle, we target argumentation that aims for persuasion — as opposed to critical discussions where strategic maneuvering is needed to achieve reasonableness (van Eemeren et al., 2014). Joviˇci´c (2006) proposed a procedure to evaluate the effectiveness of persuasive argumentation, though not in a computational way. In natural language processing, most computational argumentation research focuses on the mining of argumentative units and relations from text (Stab and Gurevych, 2014; Peldszus and Stede, 2015; Ajjour et al., 2017). Argument mining infers the logical structure of arguments, but it does not analyze the strategy used to compose arguments. Feng and Hirst (2011) classify the five most common argumentation schemes of Walton et al. (2008). Such a scheme defines a pattern capturing the logical inference from an 3754 argument’s premises to its claim, i.e., it primarily aims at logos only. Song et al. (2014) combine the idea of schemes with strategy-related considerations, and Habernal and Gurevych (2015) annotate pathos in forum comments and blog posts. Likewise, Duthie et al. (2016) develop a corpus and an app"
C18-1318,C16-1324,1,0.861364,"rsuasion tactics of four types: those that postulate outcomes of an uptake, those that generalize in some way, those that appeal to external authorities, and those that rely on interpersonal factors. These tactics are found in small text spans and could be seen as the local counterpart of the global strategies we consider. To our knowledge, only we have explicitly worked towards a computational analysis of such strategies so far. In particular, we presented a corpus with 300 news editorials whose units are labeled with their roles in the argumentation, such as “testimony” and “common ground” (Al-Khatib et al., 2016). In (Al-Khatib et al., 2017), we then trained a classifier on this corpus to find sequential role patterns in 30k New York Times editorials. While we observed insightful variances in the use of evidence across editorial topics, it still remains unclear to what extent such patterns really reflect rhetorical strategies. Clearly diverging from previous research, we consider rhetorical strategies in argumentation synthesis, for which related work is generally still scarce. Bilu and Slonim (2016) generate new claims by recycling topics and predicates from existing claims, whereas Reisert et al. (2"
C18-1318,D17-1141,1,0.860545,"s: those that postulate outcomes of an uptake, those that generalize in some way, those that appeal to external authorities, and those that rely on interpersonal factors. These tactics are found in small text spans and could be seen as the local counterpart of the global strategies we consider. To our knowledge, only we have explicitly worked towards a computational analysis of such strategies so far. In particular, we presented a corpus with 300 news editorials whose units are labeled with their roles in the argumentation, such as “testimony” and “common ground” (Al-Khatib et al., 2016). In (Al-Khatib et al., 2017), we then trained a classifier on this corpus to find sequential role patterns in 30k New York Times editorials. While we observed insightful variances in the use of evidence across editorial topics, it still remains unclear to what extent such patterns really reflect rhetorical strategies. Clearly diverging from previous research, we consider rhetorical strategies in argumentation synthesis, for which related work is generally still scarce. Bilu and Slonim (2016) generate new claims by recycling topics and predicates from existing claims, whereas Reisert et al. (2015) synthesize complete argu"
C18-1318,P16-2085,0,0.187524,"ose units are labeled with their roles in the argumentation, such as “testimony” and “common ground” (Al-Khatib et al., 2016). In (Al-Khatib et al., 2017), we then trained a classifier on this corpus to find sequential role patterns in 30k New York Times editorials. While we observed insightful variances in the use of evidence across editorial topics, it still remains unclear to what extent such patterns really reflect rhetorical strategies. Clearly diverging from previous research, we consider rhetorical strategies in argumentation synthesis, for which related work is generally still scarce. Bilu and Slonim (2016) generate new claims by recycling topics and predicates from existing claims, whereas Reisert et al. (2015) synthesize complete arguments based on the model of Toulmin (1958) where a claim is supported by data that is reasoned by a warrant. Like us, those authors rely on a pool of argument units from which arguments are built. However, they restrict their view to logical argument structure. The same holds for Green (2017) who generates arguments with particular schemes (e.g., “cause to effect”) that are used to teach learners how to argue. Yanase et al. (2015) present a method that arranges th"
C18-1318,N16-1166,0,0.0279473,"essed dimensions relate to rhetoric, such as argument strength, which is defined based on how many readers are persuaded. Habernal and Gurevych (2016) examine the reasons that make arguments convincing, and Lukin et al. (2017) study how effective logos-oriented and pathosoriented arguments are depending on the target audience. However, none of these works considers the application of rhetorical strategies. Tan et al. (2016) find that the chance to persuade someone in good-faith discussions on Reddit Change My View is increased through multiple interactions and an appropriate linguistic style. Cano-Basave and He (2016) analyze the impact of the semantic framing of arguments (e.g., “taking sides” and “manipulation”) in political debates. Similarly, Wang et al. (2017) reveal the importance of selecting the right framing of a discussion topic for winning classical debates. In such dialogical situations, the arguments of opposing parties are usually fragmented into several alternating parts. Our work, in contrast, analyzes the rhetorical strategies of complete monological texts where an author presents his or her entire argumentation. Studying persuasive blog posts, Anand et al. (2011) develop a scheme with 16"
C18-1318,P11-1099,0,0.0515776,"as opposed to critical discussions where strategic maneuvering is needed to achieve reasonableness (van Eemeren et al., 2014). Joviˇci´c (2006) proposed a procedure to evaluate the effectiveness of persuasive argumentation, though not in a computational way. In natural language processing, most computational argumentation research focuses on the mining of argumentative units and relations from text (Stab and Gurevych, 2014; Peldszus and Stede, 2015; Ajjour et al., 2017). Argument mining infers the logical structure of arguments, but it does not analyze the strategy used to compose arguments. Feng and Hirst (2011) classify the five most common argumentation schemes of Walton et al. (2008). Such a scheme defines a pattern capturing the logical inference from an 3754 argument’s premises to its claim, i.e., it primarily aims at logos only. Song et al. (2014) combine the idea of schemes with strategy-related considerations, and Habernal and Gurevych (2015) annotate pathos in forum comments and blog posts. Likewise, Duthie et al. (2016) develop a corpus and an approach to mine explicit expressions of ethos from political debates, whereas Hidey et al. (2017) even annotate all three means of persuasion for th"
C18-1318,D17-1249,0,0.0183513,"synthesize arguments such as The Debater (Rinott et al., 2015), namely, how to deliver arguments effectively. So far, strategical systems exist only for formal argumentation (Rosenfeld and Kraus, 2016). Our synthesis-oriented model covers content, structure, and style properties. For computational purposes, such properties need to be mined before. To capture content, some works identify the different frames under which a topic can be viewed (Naderi and Hirst, 2017), model key aspects of a topic (Menini et al., 2017), or analyze potentially strategy-related topic patterns in campaign speeches (Gautrais et al., 2017). In terms of structure, Persing et al. (2010) learn sequences of discourse functions in essays (e.g., “rebuttal” or “conclusion”) that correlate with a good organization, and we have modeled flows of arbitrary types of information to classify text properties, such as stance or quality (Wachsmuth and Stein, 2017). 3755 In (Wachsmuth et al., 2017), we extend this idea to model sequential and hierarchical argumentation structure simultaneously. Regarding style, Lawrence et al. (2017) analyze rhetorical figures in arguments, Song et al. (2017) classify discourse modes in essays, and Zhang et al."
C18-1318,D15-1255,0,0.0399843,"ses on the mining of argumentative units and relations from text (Stab and Gurevych, 2014; Peldszus and Stede, 2015; Ajjour et al., 2017). Argument mining infers the logical structure of arguments, but it does not analyze the strategy used to compose arguments. Feng and Hirst (2011) classify the five most common argumentation schemes of Walton et al. (2008). Such a scheme defines a pattern capturing the logical inference from an 3754 argument’s premises to its claim, i.e., it primarily aims at logos only. Song et al. (2014) combine the idea of schemes with strategy-related considerations, and Habernal and Gurevych (2015) annotate pathos in forum comments and blog posts. Likewise, Duthie et al. (2016) develop a corpus and an approach to mine explicit expressions of ethos from political debates, whereas Hidey et al. (2017) even annotate all three means of persuasion for the premises of arguments. The provided data and methods may be helpful for studying persuasive effectiveness, but they have not been employed so far for that purpose. In (Wachsmuth et al., 2016), we use the output of argument mining to assess four quality dimensions of persuasive essays. Some assessed dimensions relate to rhetoric, such as argu"
C18-1318,D16-1129,0,0.0156505,". (2016) develop a corpus and an approach to mine explicit expressions of ethos from political debates, whereas Hidey et al. (2017) even annotate all three means of persuasion for the premises of arguments. The provided data and methods may be helpful for studying persuasive effectiveness, but they have not been employed so far for that purpose. In (Wachsmuth et al., 2016), we use the output of argument mining to assess four quality dimensions of persuasive essays. Some assessed dimensions relate to rhetoric, such as argument strength, which is defined based on how many readers are persuaded. Habernal and Gurevych (2016) examine the reasons that make arguments convincing, and Lukin et al. (2017) study how effective logos-oriented and pathosoriented arguments are depending on the target audience. However, none of these works considers the application of rhetorical strategies. Tan et al. (2016) find that the chance to persuade someone in good-faith discussions on Reddit Change My View is increased through multiple interactions and an appropriate linguistic style. Cano-Basave and He (2016) analyze the impact of the semantic framing of arguments (e.g., “taking sides” and “manipulation”) in political debates. Simi"
C18-1318,W17-5102,0,0.0434853,"analyze the strategy used to compose arguments. Feng and Hirst (2011) classify the five most common argumentation schemes of Walton et al. (2008). Such a scheme defines a pattern capturing the logical inference from an 3754 argument’s premises to its claim, i.e., it primarily aims at logos only. Song et al. (2014) combine the idea of schemes with strategy-related considerations, and Habernal and Gurevych (2015) annotate pathos in forum comments and blog posts. Likewise, Duthie et al. (2016) develop a corpus and an approach to mine explicit expressions of ethos from political debates, whereas Hidey et al. (2017) even annotate all three means of persuasion for the premises of arguments. The provided data and methods may be helpful for studying persuasive effectiveness, but they have not been employed so far for that purpose. In (Wachsmuth et al., 2016), we use the output of argument mining to assess four quality dimensions of persuasive essays. Some assessed dimensions relate to rhetoric, such as argument strength, which is defined based on how many readers are persuaded. Habernal and Gurevych (2016) examine the reasons that make arguments convincing, and Lukin et al. (2017) study how effective logos-"
C18-1318,E17-1070,0,0.239536,"s logos, ethos, and pathos. In the realm of the area of computational argumentation, rhetorical strategies are particularly relevant for technologies that synthesize argumentative text and that aim to deliver arguments effectively. Existing argument mining research largely focuses on the logical structure of arguments, identifying their units (premises vs. conclusions) and relations (support vs. attack). Recently, a few studies have tackled strategy-related aspects, such as explicit expressions of ethos (Duthie et al., 2016) and the effects of logical and emotional arguments across audiences (Lukin et al., 2017). So far, however, strategies have not been considered in argumentation synthesis, which altogether has not received much attention (see Section 2). In this paper, we study the role of rhetorical strategies when synthesizing argumentation. In particular, we consider monological argumentative texts where an author seeks to persuade target readers of his or her stance towards a given topic, such as news editorials and persuasive essays. Conceptually, we argue that an author synthesizes a text of such genres in three subsequent steps: 1. Selecting content in terms of argumentative discourse units"
C18-1318,D17-1318,0,0.153505,"tative texts in particular. With that, we seek to contribute a missing aspect to technologies that synthesize arguments such as The Debater (Rinott et al., 2015), namely, how to deliver arguments effectively. So far, strategical systems exist only for formal argumentation (Rosenfeld and Kraus, 2016). Our synthesis-oriented model covers content, structure, and style properties. For computational purposes, such properties need to be mined before. To capture content, some works identify the different frames under which a topic can be viewed (Naderi and Hirst, 2017), model key aspects of a topic (Menini et al., 2017), or analyze potentially strategy-related topic patterns in campaign speeches (Gautrais et al., 2017). In terms of structure, Persing et al. (2010) learn sequences of discourse functions in essays (e.g., “rebuttal” or “conclusion”) that correlate with a good organization, and we have modeled flows of arbitrary types of information to classify text properties, such as stance or quality (Wachsmuth and Stein, 2017). 3755 In (Wachsmuth et al., 2017), we extend this idea to model sequential and hierarchical argumentation structure simultaneously. Regarding style, Lawrence et al. (2017) analyze rhet"
C18-1318,naderi-hirst-2017-classifying,0,0.0521996,"systems (Reiter and Dale, 1997), but it targets argumentative texts in particular. With that, we seek to contribute a missing aspect to technologies that synthesize arguments such as The Debater (Rinott et al., 2015), namely, how to deliver arguments effectively. So far, strategical systems exist only for formal argumentation (Rosenfeld and Kraus, 2016). Our synthesis-oriented model covers content, structure, and style properties. For computational purposes, such properties need to be mined before. To capture content, some works identify the different frames under which a topic can be viewed (Naderi and Hirst, 2017), model key aspects of a topic (Menini et al., 2017), or analyze potentially strategy-related topic patterns in campaign speeches (Gautrais et al., 2017). In terms of structure, Persing et al. (2010) learn sequences of discourse functions in essays (e.g., “rebuttal” or “conclusion”) that correlate with a good organization, and we have modeled flows of arbitrary types of information to classify text properties, such as stance or quality (Wachsmuth and Stein, 2017). 3755 In (Wachsmuth et al., 2017), we extend this idea to model sequential and hierarchical argumentation structure simultaneously."
C18-1318,D15-1110,1,0.865613,"credibility and good character, and (3) pathos, the evoking of the right emotions in the target audience. As Aristotle, we target argumentation that aims for persuasion — as opposed to critical discussions where strategic maneuvering is needed to achieve reasonableness (van Eemeren et al., 2014). Joviˇci´c (2006) proposed a procedure to evaluate the effectiveness of persuasive argumentation, though not in a computational way. In natural language processing, most computational argumentation research focuses on the mining of argumentative units and relations from text (Stab and Gurevych, 2014; Peldszus and Stede, 2015; Ajjour et al., 2017). Argument mining infers the logical structure of arguments, but it does not analyze the strategy used to compose arguments. Feng and Hirst (2011) classify the five most common argumentation schemes of Walton et al. (2008). Such a scheme defines a pattern capturing the logical inference from an 3754 argument’s premises to its claim, i.e., it primarily aims at logos only. Song et al. (2014) combine the idea of schemes with strategy-related considerations, and Habernal and Gurevych (2015) annotate pathos in forum comments and blog posts. Likewise, Duthie et al. (2016) devel"
C18-1318,D10-1023,0,0.0229266,"t et al., 2015), namely, how to deliver arguments effectively. So far, strategical systems exist only for formal argumentation (Rosenfeld and Kraus, 2016). Our synthesis-oriented model covers content, structure, and style properties. For computational purposes, such properties need to be mined before. To capture content, some works identify the different frames under which a topic can be viewed (Naderi and Hirst, 2017), model key aspects of a topic (Menini et al., 2017), or analyze potentially strategy-related topic patterns in campaign speeches (Gautrais et al., 2017). In terms of structure, Persing et al. (2010) learn sequences of discourse functions in essays (e.g., “rebuttal” or “conclusion”) that correlate with a good organization, and we have modeled flows of arbitrary types of information to classify text properties, such as stance or quality (Wachsmuth and Stein, 2017). 3755 In (Wachsmuth et al., 2017), we extend this idea to model sequential and hierarchical argumentation structure simultaneously. Regarding style, Lawrence et al. (2017) analyze rhetorical figures in arguments, Song et al. (2017) classify discourse modes in essays, and Zhang et al. (2017) study rhetorical questions in political"
C18-1318,W15-0507,0,0.220234,"tib et al., 2016). In (Al-Khatib et al., 2017), we then trained a classifier on this corpus to find sequential role patterns in 30k New York Times editorials. While we observed insightful variances in the use of evidence across editorial topics, it still remains unclear to what extent such patterns really reflect rhetorical strategies. Clearly diverging from previous research, we consider rhetorical strategies in argumentation synthesis, for which related work is generally still scarce. Bilu and Slonim (2016) generate new claims by recycling topics and predicates from existing claims, whereas Reisert et al. (2015) synthesize complete arguments based on the model of Toulmin (1958) where a claim is supported by data that is reasoned by a warrant. Like us, those authors rely on a pool of argument units from which arguments are built. However, they restrict their view to logical argument structure. The same holds for Green (2017) who generates arguments with particular schemes (e.g., “cause to effect”) that are used to teach learners how to argue. Yanase et al. (2015) present a method that arranges the sentences of an argumentative text in a natural way. Sato et al. (2015) build upon this method. Their sys"
C18-1318,D15-1050,0,0.134517,"Missing"
C18-1318,P15-4019,0,0.205315,"from existing claims, whereas Reisert et al. (2015) synthesize complete arguments based on the model of Toulmin (1958) where a claim is supported by data that is reasoned by a warrant. Like us, those authors rely on a pool of argument units from which arguments are built. However, they restrict their view to logical argument structure. The same holds for Green (2017) who generates arguments with particular schemes (e.g., “cause to effect”) that are used to teach learners how to argue. Yanase et al. (2015) present a method that arranges the sentences of an argumentative text in a natural way. Sato et al. (2015) build upon this method. Their system pursues similar goals as we do, phrasing an ordered text with multiple arguments. We extend their idea by rhetorical considerations, and we propose a general argumentation synthesis model. It is in line with classical concepts of building natural language generation systems (Reiter and Dale, 1997), but it targets argumentative texts in particular. With that, we seek to contribute a missing aspect to technologies that synthesize arguments such as The Debater (Rinott et al., 2015), namely, how to deliver arguments effectively. So far, strategical systems exi"
C18-1318,W14-2110,0,0.024727,"ional way. In natural language processing, most computational argumentation research focuses on the mining of argumentative units and relations from text (Stab and Gurevych, 2014; Peldszus and Stede, 2015; Ajjour et al., 2017). Argument mining infers the logical structure of arguments, but it does not analyze the strategy used to compose arguments. Feng and Hirst (2011) classify the five most common argumentation schemes of Walton et al. (2008). Such a scheme defines a pattern capturing the logical inference from an 3754 argument’s premises to its claim, i.e., it primarily aims at logos only. Song et al. (2014) combine the idea of schemes with strategy-related considerations, and Habernal and Gurevych (2015) annotate pathos in forum comments and blog posts. Likewise, Duthie et al. (2016) develop a corpus and an approach to mine explicit expressions of ethos from political debates, whereas Hidey et al. (2017) even annotate all three means of persuasion for the premises of arguments. The provided data and methods may be helpful for studying persuasive effectiveness, but they have not been employed so far for that purpose. In (Wachsmuth et al., 2016), we use the output of argument mining to assess four"
C18-1318,P17-1011,0,0.136561,"tegy-related topic patterns in campaign speeches (Gautrais et al., 2017). In terms of structure, Persing et al. (2010) learn sequences of discourse functions in essays (e.g., “rebuttal” or “conclusion”) that correlate with a good organization, and we have modeled flows of arbitrary types of information to classify text properties, such as stance or quality (Wachsmuth and Stein, 2017). 3755 In (Wachsmuth et al., 2017), we extend this idea to model sequential and hierarchical argumentation structure simultaneously. Regarding style, Lawrence et al. (2017) analyze rhetorical figures in arguments, Song et al. (2017) classify discourse modes in essays, and Zhang et al. (2017) study rhetorical questions in political discourse. All such methods may be relevant when we operationalize our model. Finally, the use of strategies in synthesis scenarios follows up on early work on discourse planning (Young et al., 1994; Zukerman et al., 2000). The computational approaches relied on rule-based techniques to create effective arguments (Carenini and Moore, 2006) for a few selected tactics. More recent research on general text generation uses probabilistic models to employ text structure (Barzilay, 2010), or synthesiz"
C18-1318,D14-1006,0,0.455036,"stration of the speaker’s credibility and good character, and (3) pathos, the evoking of the right emotions in the target audience. As Aristotle, we target argumentation that aims for persuasion — as opposed to critical discussions where strategic maneuvering is needed to achieve reasonableness (van Eemeren et al., 2014). Joviˇci´c (2006) proposed a procedure to evaluate the effectiveness of persuasive argumentation, though not in a computational way. In natural language processing, most computational argumentation research focuses on the mining of argumentative units and relations from text (Stab and Gurevych, 2014; Peldszus and Stede, 2015; Ajjour et al., 2017). Argument mining infers the logical structure of arguments, but it does not analyze the strategy used to compose arguments. Feng and Hirst (2011) classify the five most common argumentation schemes of Walton et al. (2008). Such a scheme defines a pattern capturing the logical inference from an 3754 argument’s premises to its claim, i.e., it primarily aims at logos only. Song et al. (2014) combine the idea of schemes with strategy-related considerations, and Habernal and Gurevych (2015) annotate pathos in forum comments and blog posts. Likewise,"
C18-1318,C16-1158,1,0.888935,"ises to its claim, i.e., it primarily aims at logos only. Song et al. (2014) combine the idea of schemes with strategy-related considerations, and Habernal and Gurevych (2015) annotate pathos in forum comments and blog posts. Likewise, Duthie et al. (2016) develop a corpus and an approach to mine explicit expressions of ethos from political debates, whereas Hidey et al. (2017) even annotate all three means of persuasion for the premises of arguments. The provided data and methods may be helpful for studying persuasive effectiveness, but they have not been employed so far for that purpose. In (Wachsmuth et al., 2016), we use the output of argument mining to assess four quality dimensions of persuasive essays. Some assessed dimensions relate to rhetoric, such as argument strength, which is defined based on how many readers are persuaded. Habernal and Gurevych (2016) examine the reasons that make arguments convincing, and Lukin et al. (2017) study how effective logos-oriented and pathosoriented arguments are depending on the target audience. However, none of these works considers the application of rhetorical strategies. Tan et al. (2016) find that the chance to persuade someone in good-faith discussions on"
C18-1318,D17-1253,1,0.845016,"e. To capture content, some works identify the different frames under which a topic can be viewed (Naderi and Hirst, 2017), model key aspects of a topic (Menini et al., 2017), or analyze potentially strategy-related topic patterns in campaign speeches (Gautrais et al., 2017). In terms of structure, Persing et al. (2010) learn sequences of discourse functions in essays (e.g., “rebuttal” or “conclusion”) that correlate with a good organization, and we have modeled flows of arbitrary types of information to classify text properties, such as stance or quality (Wachsmuth and Stein, 2017). 3755 In (Wachsmuth et al., 2017), we extend this idea to model sequential and hierarchical argumentation structure simultaneously. Regarding style, Lawrence et al. (2017) analyze rhetorical figures in arguments, Song et al. (2017) classify discourse modes in essays, and Zhang et al. (2017) study rhetorical questions in political discourse. All such methods may be relevant when we operationalize our model. Finally, the use of strategies in synthesis scenarios follows up on early work on discourse planning (Young et al., 1994; Zukerman et al., 2000). The computational approaches relied on rule-based techniques to create effect"
C18-1318,Q17-1016,0,0.067982,"e the reasons that make arguments convincing, and Lukin et al. (2017) study how effective logos-oriented and pathosoriented arguments are depending on the target audience. However, none of these works considers the application of rhetorical strategies. Tan et al. (2016) find that the chance to persuade someone in good-faith discussions on Reddit Change My View is increased through multiple interactions and an appropriate linguistic style. Cano-Basave and He (2016) analyze the impact of the semantic framing of arguments (e.g., “taking sides” and “manipulation”) in political debates. Similarly, Wang et al. (2017) reveal the importance of selecting the right framing of a discussion topic for winning classical debates. In such dialogical situations, the arguments of opposing parties are usually fragmented into several alternating parts. Our work, in contrast, analyzes the rhetorical strategies of complete monological texts where an author presents his or her entire argumentation. Studying persuasive blog posts, Anand et al. (2011) develop a scheme with 16 persuasion tactics of four types: those that postulate outcomes of an uptake, those that generalize in some way, those that appeal to external authori"
C18-1318,W15-0512,0,0.182313,"work is generally still scarce. Bilu and Slonim (2016) generate new claims by recycling topics and predicates from existing claims, whereas Reisert et al. (2015) synthesize complete arguments based on the model of Toulmin (1958) where a claim is supported by data that is reasoned by a warrant. Like us, those authors rely on a pool of argument units from which arguments are built. However, they restrict their view to logical argument structure. The same holds for Green (2017) who generates arguments with particular schemes (e.g., “cause to effect”) that are used to teach learners how to argue. Yanase et al. (2015) present a method that arranges the sentences of an argumentative text in a natural way. Sato et al. (2015) build upon this method. Their system pursues similar goals as we do, phrasing an ordered text with multiple arguments. We extend their idea by rhetorical considerations, and we propose a general argumentation synthesis model. It is in line with classical concepts of building natural language generation systems (Reiter and Dale, 1997), but it targets argumentative texts in particular. With that, we seek to contribute a missing aspect to technologies that synthesize arguments such as The D"
C18-1318,D17-1164,0,0.0256897,"et al., 2017). In terms of structure, Persing et al. (2010) learn sequences of discourse functions in essays (e.g., “rebuttal” or “conclusion”) that correlate with a good organization, and we have modeled flows of arbitrary types of information to classify text properties, such as stance or quality (Wachsmuth and Stein, 2017). 3755 In (Wachsmuth et al., 2017), we extend this idea to model sequential and hierarchical argumentation structure simultaneously. Regarding style, Lawrence et al. (2017) analyze rhetorical figures in arguments, Song et al. (2017) classify discourse modes in essays, and Zhang et al. (2017) study rhetorical questions in political discourse. All such methods may be relevant when we operationalize our model. Finally, the use of strategies in synthesis scenarios follows up on early work on discourse planning (Young et al., 1994; Zukerman et al., 2000). The computational approaches relied on rule-based techniques to create effective arguments (Carenini and Moore, 2006) for a few selected tactics. More recent research on general text generation uses probabilistic models to employ text structure (Barzilay, 2010), or synthesizes texts such that they have a certain style in terms of sen"
C18-1318,W00-1408,0,0.740009,"t properties, such as stance or quality (Wachsmuth and Stein, 2017). 3755 In (Wachsmuth et al., 2017), we extend this idea to model sequential and hierarchical argumentation structure simultaneously. Regarding style, Lawrence et al. (2017) analyze rhetorical figures in arguments, Song et al. (2017) classify discourse modes in essays, and Zhang et al. (2017) study rhetorical questions in political discourse. All such methods may be relevant when we operationalize our model. Finally, the use of strategies in synthesis scenarios follows up on early work on discourse planning (Young et al., 1994; Zukerman et al., 2000). The computational approaches relied on rule-based techniques to create effective arguments (Carenini and Moore, 2006) for a few selected tactics. More recent research on general text generation uses probabilistic models to employ text structure (Barzilay, 2010), or synthesizes texts such that they have a certain style in terms of sentiment or similar (Hu et al., 2017; Shen et al., 2017). Our model is meant to provide an abstract framework to be exploited in such approaches. 3 Model We now delineate our model of rhetorical strategies for synthesizing a monological argumentative text following"
D15-1072,C14-1002,0,0.0189951,"bal sentiment: neutral (3 out of 5) Global sentiment: neutral (2 out of 3) Figure 1. Example web reviews with neutral global sentiment from three domains, taken from the corpora described in Section 5. Corpus annotations of positive and negative local sentiment are marked in light green and medium red, respectively. to align domain-specific features (Prettenhofer and Stein, 2010). Our model complements these techniques and could be leveraged for pivot features. In tasks like authorship attribution and argumentative zoning, non-topical words benefit domain independence (Menon and Choi, 2011; Ó Séaghdha and Teufel, 2014). Instead, we focus on the local sentiment on different aspects in a review here. Aspect-based sentiment analysis extracts finegrained opinions from a review (Popescu and Etzioni, 2005). These aspects in turn impact the review’s global sentiment (Wang et al., 2010). However, relevant aspects naturally tend to be domainspecific, like the picture quality of HDD players or the beds of hotels (cf. Figure 1). While weaklysupervised approaches to extract aspects and local sentiment exist (Brody and Elhadad, 2010; Lazaridou et al., 2013), it is not clear how to align aspects from different domains. W"
D15-1072,P05-1015,0,0.0904038,"e locations belong to a predefined training set and two to a validation and a test set each. For all locations, the reviews are evenly distributed over the five TripAdvisor overall scores. In accordance with the product corpus, we see score 4–5 as positive global sentiment, 3 as neutral, and 1–2 as negative. In each review, all main clauses together with their subordinate clauses have been classified as being positive, negative, or neutral. Movie Domain Finally, the third corpus (Mao and Lebanon, 2007) compiles 450 Rotten Tomatoes reviews from the Cornell Movie Review Data scale dataset v1.0 (Pang and Lee, 2005) that refer to two authors. We use the 201 reviews of Scott Renshaw for training and the 249 of Dennis Schwartz for testing. The reviews lack punctuation, capitalization, and their overall ratings. We recovered the overall ratings from the original dataset based on the rating scale 0–2, resulting in 178 positive, 139 neutral, and 133 negative reviews. In each review, Mao and Lebanon (2007) classified all sentences to be very positive, positive, neutral, negative, or very negative, which we reduce to three classes. Analysis of the Generality of the Model We now report on experiments on corpora"
D15-1072,D10-1023,0,0.212093,"Missing"
D15-1072,H05-1043,0,0.0561257,"ed in Section 5. Corpus annotations of positive and negative local sentiment are marked in light green and medium red, respectively. to align domain-specific features (Prettenhofer and Stein, 2010). Our model complements these techniques and could be leveraged for pivot features. In tasks like authorship attribution and argumentative zoning, non-topical words benefit domain independence (Menon and Choi, 2011; Ó Séaghdha and Teufel, 2014). Instead, we focus on the local sentiment on different aspects in a review here. Aspect-based sentiment analysis extracts finegrained opinions from a review (Popescu and Etzioni, 2005). These aspects in turn impact the review’s global sentiment (Wang et al., 2010). However, relevant aspects naturally tend to be domainspecific, like the picture quality of HDD players or the beds of hotels (cf. Figure 1). While weaklysupervised approaches to extract aspects and local sentiment exist (Brody and Elhadad, 2010; Lazaridou et al., 2013), it is not clear how to align aspects from different domains. We ignore aspects here, only preserving the local sentiment itself. State-of-the-art approaches for classifying local sentiment within a domain model the composition of words, e.g., rely"
D15-1072,P10-1114,1,0.897834,"f Web Review Argumentation Henning Wachsmuth and Johannes Kiesel and Benno Stein Faculty of Media, Bauhaus-Universität Weimar, Germany {henning.wachsmuth,johannes.kiesel,benno.stein}@uni-weimar.de Abstract across domains, as illustrated in Figure 1 for a product, a hotel, and a movie review. As a consequence, sentiment analysis suffers from domain dependence (Wu et al., 2010), i.e., high effectiveness is often achieved only in the domain an approach has been specifically modeled for. To adapt to other domains, prior knowledge about these domains or about domain-independent features is needed (Prettenhofer and Stein, 2010). This paper considers the question as to whether the overall argumentation of web reviews can be modeled in a general way in order to increase domain independence in sentiment analysis. We observe that people structure web reviews largely sequentially—in contrast to the complex structures of many other argumentative texts. While the reviewed aspects differ between domains, our assumption is that the overall argumentation of a web review is generally represented by a sequence of local sentiments, called the review’s sentiment flow (Mao and Lebanon, 2007). In particular, we hypothesize that, un"
D15-1072,P13-1160,0,0.0358059,"l words benefit domain independence (Menon and Choi, 2011; Ó Séaghdha and Teufel, 2014). Instead, we focus on the local sentiment on different aspects in a review here. Aspect-based sentiment analysis extracts finegrained opinions from a review (Popescu and Etzioni, 2005). These aspects in turn impact the review’s global sentiment (Wang et al., 2010). However, relevant aspects naturally tend to be domainspecific, like the picture quality of HDD players or the beds of hotels (cf. Figure 1). While weaklysupervised approaches to extract aspects and local sentiment exist (Brody and Elhadad, 2010; Lazaridou et al., 2013), it is not clear how to align aspects from different domains. We ignore aspects here, only preserving the local sentiment itself. State-of-the-art approaches for classifying local sentiment within a domain model the composition of words, e.g., relying on deep learning (Socher et al., 2013). We do not compete with such an approach, but we use it to then predict global sentiment. Täckström and McDonald (2011) observe that local and global sentiment correlate, aiming for the opposite direction, though. In (Wachsmuth et al., 2014b), we already compute frequent flows of local sentiment, but we nei"
D15-1072,D13-1170,0,0.00780468,"n impact the review’s global sentiment (Wang et al., 2010). However, relevant aspects naturally tend to be domainspecific, like the picture quality of HDD players or the beds of hotels (cf. Figure 1). While weaklysupervised approaches to extract aspects and local sentiment exist (Brody and Elhadad, 2010; Lazaridou et al., 2013), it is not clear how to align aspects from different domains. We ignore aspects here, only preserving the local sentiment itself. State-of-the-art approaches for classifying local sentiment within a domain model the composition of words, e.g., relying on deep learning (Socher et al., 2013). We do not compete with such an approach, but we use it to then predict global sentiment. Täckström and McDonald (2011) observe that local and global sentiment correlate, aiming for the opposite direction, though. In (Wachsmuth et al., 2014b), we already compute frequent flows of local sentiment, but we neither analyze their generality, nor do we use them for prediction. The idea of modeling sentiment flow was introduced by Mao and Lebanon (2007) who classify local sentiment based on neighboring local sentiment in a review. When inferring global sentiment from a flow, however, the authors mod"
D15-1072,E14-1051,0,0.0568313,"Missing"
D15-1072,R11-1043,0,0.0289288,"neutral (3 out of 5) Global sentiment: neutral (3 out of 5) Global sentiment: neutral (2 out of 3) Figure 1. Example web reviews with neutral global sentiment from three domains, taken from the corpora described in Section 5. Corpus annotations of positive and negative local sentiment are marked in light green and medium red, respectively. to align domain-specific features (Prettenhofer and Stein, 2010). Our model complements these techniques and could be leveraged for pivot features. In tasks like authorship attribution and argumentative zoning, non-topical words benefit domain independence (Menon and Choi, 2011; Ó Séaghdha and Teufel, 2014). Instead, we focus on the local sentiment on different aspects in a review here. Aspect-based sentiment analysis extracts finegrained opinions from a review (Popescu and Etzioni, 2005). These aspects in turn impact the review’s global sentiment (Wang et al., 2010). However, relevant aspects naturally tend to be domainspecific, like the picture quality of HDD players or the beds of hotels (cf. Figure 1). While weaklysupervised approaches to extract aspects and local sentiment exist (Brody and Elhadad, 2010; Lazaridou et al., 2013), it is not clear how to align asp"
D15-1072,C14-1053,1,0.711501,"ther argumentative texts. While the reviewed aspects differ between domains, our assumption is that the overall argumentation of a web review is generally represented by a sequence of local sentiments, called the review’s sentiment flow (Mao and Lebanon, 2007). In particular, we hypothesize that, under an adequate model, similar sentiment flows express similar global sentiments, also across domains. All reviews in Figure 1, for instance, express neutral global sentiment starting with positive, continuing with negative, and ending with positive local sentiment. Unlike in our previous approach (Wachsmuth et al., 2014a), we analyze the major abstraction steps when modeling sentiment flow to represent global sentiment. A general model should abstract from both content and other domain differences, such as a review’s length or the density of local sentiment in it. Based on web review corpora with known sentiment flows, we empirically analyze several model variants across three domains. Our results offer clear evidence for the truth of our hypothesis, indicating the generality of sentiment flow as a model of web review argumentation. The abstract nature of sentiment flow, however, does not directly achieve do"
D15-1072,H05-2017,0,\N,Missing
D15-1072,N10-1122,0,\N,Missing
D17-1141,baccianella-etal-2010-sentiwordnet,0,\N,Missing
D17-1141,I11-1071,1,\N,Missing
D17-1141,W14-2105,0,\N,Missing
D17-1141,D15-1072,1,\N,Missing
D17-1141,D15-1091,0,\N,Missing
D17-1141,C16-1324,1,\N,Missing
D17-1253,C16-1324,1,0.912549,"Missing"
D17-1253,W16-2808,0,0.013364,"tial structure (Wachsmuth et al., 2014b; Al Khatib et al., 2016) are out of scope. We do not mine the structure of argumentative texts, but we exploit the previously mined structure to tackle downstream tasks of computational argumentation, namely, to classify the myside bias and stance of texts. For myside bias, Stab and Gurevych (2016) use features derived from discourse structure, whereas Faulkner (2014) and Sobhani et al. (2015) model arguments to classify stance. Ong et al. (2014) and we ourselves (Wachsmuth et al., 2016) do similar to assess the quality of persuasive essays, and Beigman Klebanov et al. (2016) examine how an essay’s content and structure influence quality. Other works predict the outcome of legal cases based on the applied types of reasoning (Brüninghaus and Ashley, 2003) or analyze inference schemes for given arguments (Feng and Hirst, 2011). In contrast to the local structure of single arguments employed by all these approaches, we study the impact of the global overall structure of complete monological argumentative texts. 2380 In (Wachsmuth et al., 2017), we point out that the argumentation quality of a text is affected by interactions of its content at different levels of gran"
D17-1253,P11-1099,0,0.492762,"ssify the myside bias and stance of texts. For myside bias, Stab and Gurevych (2016) use features derived from discourse structure, whereas Faulkner (2014) and Sobhani et al. (2015) model arguments to classify stance. Ong et al. (2014) and we ourselves (Wachsmuth et al., 2016) do similar to assess the quality of persuasive essays, and Beigman Klebanov et al. (2016) examine how an essay’s content and structure influence quality. Other works predict the outcome of legal cases based on the applied types of reasoning (Brüninghaus and Ashley, 2003) or analyze inference schemes for given arguments (Feng and Hirst, 2011). In contrast to the local structure of single arguments employed by all these approaches, we study the impact of the global overall structure of complete monological argumentative texts. 2380 In (Wachsmuth et al., 2017), we point out that the argumentation quality of a text is affected by interactions of its content at different levels of granularity, from single argument units over arguments to overall argumentation. Stede (2016) explores how different depths of overall argumentation can be identified, observing differences across genres. Unlike in our experiments, however, the genres consid"
D17-1253,C14-1089,0,0.0202512,"approaches, we study the impact of the global overall structure of complete monological argumentative texts. 2380 In (Wachsmuth et al., 2017), we point out that the argumentation quality of a text is affected by interactions of its content at different levels of granularity, from single argument units over arguments to overall argumentation. Stede (2016) explores how different depths of overall argumentation can be identified, observing differences across genres. Unlike in our experiments, however, the genres considered there reflect diverging types of argumentation. Related to argumentation, Feng et al. (2014) build upon rhetorical structure theory (Mann and Thompson, 1988) to assess the coherence of texts, while Persing et al. (2010) score the organization of persuasive essays based on sequences of sentence and paragraph functions. We introduced the first explicit computational model of overall argumentation in (Wachsmuth et al., 2014a). There, we compared the flow of local sentiment in a review to a set of learned flow patterns in order to classify global sentiment. Recently, we generalized the model in order to make flows applicable to any type of information relevant for argumentation-related a"
D17-1253,P15-4004,0,0.0721504,"Missing"
D17-1253,D15-1255,0,0.414783,"the overall argumentation of a monological argumentative text such as the one in Figure 1(a), this results in an implicit hierarchical structure with the text’s main claim at the lowest depth. In addition, the text has an explicit linguistic structure that can be seen as a regulated sequence of speech acts (van Eemeren and Grootendorst, 2004). Figure 1(b) illustrates the interplay of the two types of overall structure in form of a tree-like graph. Natural language processing research has largely adopted the outlined hierarchical models for mining arguments from text (Stab and Gurevych, 2014; Habernal and Gurevych, 2015; Peldszus and Stede, 2016). However, the adequacy of the resulting overall structure for downstream analysis tasks of computational argumentation has rarely been evaluated (see Section 2 for details). In fact, a computational approach that can capture patterns in hierarchical overall argumentation is missing so far. Even more, our previous work indicates that a sequential model of overall structure is preferable for analysis tasks such as stance classification or quality assessment (Wachsmuth and Stein, 2017). In this paper, we ask and investigate what model of (monological) overall argumenta"
D17-1253,E06-1015,0,0.0575943,"compared the flow of local sentiment in a review to a set of learned flow patterns in order to classify global sentiment. Recently, we generalized the model in order to make flows applicable to any type of information relevant for argumentation-related analysis tasks (Wachsmuth and Stein, 2017). However, flows capture only sequential structure, whereas here we also model the hierarchical structure of overall argumentation. To this end, we make use of kernel methods. Kernel methods are a popular approach for learning on structured data, with several applications in natural language processing (Moschitti, 2006b) including argument mining (Rooney et al., 2012). They employ a similarity function defined between any two input objects that are represented in a taskspecific implicit feature space. The evaluation of such a kernel function relies on the common features of the input objects (Cristianini and ShaweTaylor, 2000). The kernel function encodes knowledge of the task in the form of these features. Several kernel functions have been defined for structured data. To assess the impact of sequential argumentation, we refer to the function of Mooney and Bunescu (2006), which computes common subsequences"
D17-1253,W14-2104,0,0.0162687,"3. Corpora focusing on dialogical argumentation (Walker et al., 2012), topic-related arguments (Rinott et al., 2015), or sequential structure (Wachsmuth et al., 2014b; Al Khatib et al., 2016) are out of scope. We do not mine the structure of argumentative texts, but we exploit the previously mined structure to tackle downstream tasks of computational argumentation, namely, to classify the myside bias and stance of texts. For myside bias, Stab and Gurevych (2016) use features derived from discourse structure, whereas Faulkner (2014) and Sobhani et al. (2015) model arguments to classify stance. Ong et al. (2014) and we ourselves (Wachsmuth et al., 2016) do similar to assess the quality of persuasive essays, and Beigman Klebanov et al. (2016) examine how an essay’s content and structure influence quality. Other works predict the outcome of legal cases based on the applied types of reasoning (Brüninghaus and Ashley, 2003) or analyze inference schemes for given arguments (Feng and Hirst, 2011). In contrast to the local structure of single arguments employed by all these approaches, we study the impact of the global overall structure of complete monological argumentative texts. 2380 In (Wachsmuth et al.,"
D17-1253,W16-2813,0,0.0836566,"tab and Gurevych, 2014). Since we target monological overall argumentation, we use those that capture the complete structure of texts, as detailed in Section 3. Corpora focusing on dialogical argumentation (Walker et al., 2012), topic-related arguments (Rinott et al., 2015), or sequential structure (Wachsmuth et al., 2014b; Al Khatib et al., 2016) are out of scope. We do not mine the structure of argumentative texts, but we exploit the previously mined structure to tackle downstream tasks of computational argumentation, namely, to classify the myside bias and stance of texts. For myside bias, Stab and Gurevych (2016) use features derived from discourse structure, whereas Faulkner (2014) and Sobhani et al. (2015) model arguments to classify stance. Ong et al. (2014) and we ourselves (Wachsmuth et al., 2016) do similar to assess the quality of persuasive essays, and Beigman Klebanov et al. (2016) examine how an essay’s content and structure influence quality. Other works predict the outcome of legal cases based on the applied types of reasoning (Brüninghaus and Ashley, 2003) or analyze inference schemes for given arguments (Feng and Hirst, 2011). In contrast to the local structure of single arguments employ"
D17-1253,W02-1011,0,0.0181603,"Missing"
D17-1253,C16-1312,0,0.369186,"l argumentative text such as the one in Figure 1(a), this results in an implicit hierarchical structure with the text’s main claim at the lowest depth. In addition, the text has an explicit linguistic structure that can be seen as a regulated sequence of speech acts (van Eemeren and Grootendorst, 2004). Figure 1(b) illustrates the interplay of the two types of overall structure in form of a tree-like graph. Natural language processing research has largely adopted the outlined hierarchical models for mining arguments from text (Stab and Gurevych, 2014; Habernal and Gurevych, 2015; Peldszus and Stede, 2016). However, the adequacy of the resulting overall structure for downstream analysis tasks of computational argumentation has rarely been evaluated (see Section 2 for details). In fact, a computational approach that can capture patterns in hierarchical overall argumentation is missing so far. Even more, our previous work indicates that a sequential model of overall structure is preferable for analysis tasks such as stance classification or quality assessment (Wachsmuth and Stein, 2017). In this paper, we ask and investigate what model of (monological) overall argumentation is important to tackle"
D17-1253,D10-1023,0,0.0892123,"uth et al., 2017), we point out that the argumentation quality of a text is affected by interactions of its content at different levels of granularity, from single argument units over arguments to overall argumentation. Stede (2016) explores how different depths of overall argumentation can be identified, observing differences across genres. Unlike in our experiments, however, the genres considered there reflect diverging types of argumentation. Related to argumentation, Feng et al. (2014) build upon rhetorical structure theory (Mann and Thompson, 1988) to assess the coherence of texts, while Persing et al. (2010) score the organization of persuasive essays based on sequences of sentence and paragraph functions. We introduced the first explicit computational model of overall argumentation in (Wachsmuth et al., 2014a). There, we compared the flow of local sentiment in a review to a set of learned flow patterns in order to classify global sentiment. Recently, we generalized the model in order to make flows applicable to any type of information relevant for argumentation-related analysis tasks (Wachsmuth and Stein, 2017). However, flows capture only sequential structure, whereas here we also model the hie"
D17-1253,P15-1053,0,0.0391931,"Missing"
D17-1253,D15-1050,0,0.042587,"the mining of argument units and their relations from text (Mochales and Moens, 2011). Several corpora with annotated argument structure have been published in the last years. Many of the corpora adapt the hierarchical models from theory (Reed and Rowe, 2004; Habernal and Gurevych, 2015; Peldszus and Stede, 2016) or propose comparable models (Stab and Gurevych, 2014). Since we target monological overall argumentation, we use those that capture the complete structure of texts, as detailed in Section 3. Corpora focusing on dialogical argumentation (Walker et al., 2012), topic-related arguments (Rinott et al., 2015), or sequential structure (Wachsmuth et al., 2014b; Al Khatib et al., 2016) are out of scope. We do not mine the structure of argumentative texts, but we exploit the previously mined structure to tackle downstream tasks of computational argumentation, namely, to classify the myside bias and stance of texts. For myside bias, Stab and Gurevych (2016) use features derived from discourse structure, whereas Faulkner (2014) and Sobhani et al. (2015) model arguments to classify stance. Ong et al. (2014) and we ourselves (Wachsmuth et al., 2016) do similar to assess the quality of persuasive essays, a"
D17-1253,W15-0509,0,0.0225758,"e the complete structure of texts, as detailed in Section 3. Corpora focusing on dialogical argumentation (Walker et al., 2012), topic-related arguments (Rinott et al., 2015), or sequential structure (Wachsmuth et al., 2014b; Al Khatib et al., 2016) are out of scope. We do not mine the structure of argumentative texts, but we exploit the previously mined structure to tackle downstream tasks of computational argumentation, namely, to classify the myside bias and stance of texts. For myside bias, Stab and Gurevych (2016) use features derived from discourse structure, whereas Faulkner (2014) and Sobhani et al. (2015) model arguments to classify stance. Ong et al. (2014) and we ourselves (Wachsmuth et al., 2016) do similar to assess the quality of persuasive essays, and Beigman Klebanov et al. (2016) examine how an essay’s content and structure influence quality. Other works predict the outcome of legal cases based on the applied types of reasoning (Brüninghaus and Ashley, 2003) or analyze inference schemes for given arguments (Feng and Hirst, 2011). In contrast to the local structure of single arguments employed by all these approaches, we study the impact of the global overall structure of complete monol"
D17-1253,C14-1142,0,0.389456,"s of other arguments. For the overall argumentation of a monological argumentative text such as the one in Figure 1(a), this results in an implicit hierarchical structure with the text’s main claim at the lowest depth. In addition, the text has an explicit linguistic structure that can be seen as a regulated sequence of speech acts (van Eemeren and Grootendorst, 2004). Figure 1(b) illustrates the interplay of the two types of overall structure in form of a tree-like graph. Natural language processing research has largely adopted the outlined hierarchical models for mining arguments from text (Stab and Gurevych, 2014; Habernal and Gurevych, 2015; Peldszus and Stede, 2016). However, the adequacy of the resulting overall structure for downstream analysis tasks of computational argumentation has rarely been evaluated (see Section 2 for details). In fact, a computational approach that can capture patterns in hierarchical overall argumentation is missing so far. Even more, our previous work indicates that a sequential model of overall structure is preferable for analysis tasks such as stance classification or quality assessment (Wachsmuth and Stein, 2017). In this paper, we ask and investigate what model of (m"
D17-1253,C16-1158,1,0.858935,"gumentation (Walker et al., 2012), topic-related arguments (Rinott et al., 2015), or sequential structure (Wachsmuth et al., 2014b; Al Khatib et al., 2016) are out of scope. We do not mine the structure of argumentative texts, but we exploit the previously mined structure to tackle downstream tasks of computational argumentation, namely, to classify the myside bias and stance of texts. For myside bias, Stab and Gurevych (2016) use features derived from discourse structure, whereas Faulkner (2014) and Sobhani et al. (2015) model arguments to classify stance. Ong et al. (2014) and we ourselves (Wachsmuth et al., 2016) do similar to assess the quality of persuasive essays, and Beigman Klebanov et al. (2016) examine how an essay’s content and structure influence quality. Other works predict the outcome of legal cases based on the applied types of reasoning (Brüninghaus and Ashley, 2003) or analyze inference schemes for given arguments (Feng and Hirst, 2011). In contrast to the local structure of single arguments employed by all these approaches, we study the impact of the global overall structure of complete monological argumentative texts. 2380 In (Wachsmuth et al., 2017), we point out that the argumentatio"
D17-1253,E17-1017,1,0.800826,"Ong et al. (2014) and we ourselves (Wachsmuth et al., 2016) do similar to assess the quality of persuasive essays, and Beigman Klebanov et al. (2016) examine how an essay’s content and structure influence quality. Other works predict the outcome of legal cases based on the applied types of reasoning (Brüninghaus and Ashley, 2003) or analyze inference schemes for given arguments (Feng and Hirst, 2011). In contrast to the local structure of single arguments employed by all these approaches, we study the impact of the global overall structure of complete monological argumentative texts. 2380 In (Wachsmuth et al., 2017), we point out that the argumentation quality of a text is affected by interactions of its content at different levels of granularity, from single argument units over arguments to overall argumentation. Stede (2016) explores how different depths of overall argumentation can be identified, observing differences across genres. Unlike in our experiments, however, the genres considered there reflect diverging types of argumentation. Related to argumentation, Feng et al. (2014) build upon rhetorical structure theory (Mann and Thompson, 1988) to assess the coherence of texts, while Persing et al. (2"
D17-1253,C14-1053,1,0.929586,"s from text (Mochales and Moens, 2011). Several corpora with annotated argument structure have been published in the last years. Many of the corpora adapt the hierarchical models from theory (Reed and Rowe, 2004; Habernal and Gurevych, 2015; Peldszus and Stede, 2016) or propose comparable models (Stab and Gurevych, 2014). Since we target monological overall argumentation, we use those that capture the complete structure of texts, as detailed in Section 3. Corpora focusing on dialogical argumentation (Walker et al., 2012), topic-related arguments (Rinott et al., 2015), or sequential structure (Wachsmuth et al., 2014b; Al Khatib et al., 2016) are out of scope. We do not mine the structure of argumentative texts, but we exploit the previously mined structure to tackle downstream tasks of computational argumentation, namely, to classify the myside bias and stance of texts. For myside bias, Stab and Gurevych (2016) use features derived from discourse structure, whereas Faulkner (2014) and Sobhani et al. (2015) model arguments to classify stance. Ong et al. (2014) and we ourselves (Wachsmuth et al., 2016) do similar to assess the quality of persuasive essays, and Beigman Klebanov et al. (2016) examine how an"
D17-1253,walker-etal-2012-corpus,0,0.034321,"processing, argumentation research deals with the mining of argument units and their relations from text (Mochales and Moens, 2011). Several corpora with annotated argument structure have been published in the last years. Many of the corpora adapt the hierarchical models from theory (Reed and Rowe, 2004; Habernal and Gurevych, 2015; Peldszus and Stede, 2016) or propose comparable models (Stab and Gurevych, 2014). Since we target monological overall argumentation, we use those that capture the complete structure of texts, as detailed in Section 3. Corpora focusing on dialogical argumentation (Walker et al., 2012), topic-related arguments (Rinott et al., 2015), or sequential structure (Wachsmuth et al., 2014b; Al Khatib et al., 2016) are out of scope. We do not mine the structure of argumentative texts, but we exploit the previously mined structure to tackle downstream tasks of computational argumentation, namely, to classify the myside bias and stance of texts. For myside bias, Stab and Gurevych (2016) use features derived from discourse structure, whereas Faulkner (2014) and Sobhani et al. (2015) model arguments to classify stance. Ong et al. (2014) and we ourselves (Wachsmuth et al., 2016) do simila"
D18-2011,D17-1253,1,0.926296,"discussion, SEAS (Lowrance et al., 2000), VUE (Baroni et al., 2015), and Dialectic Map (Niu, 2016) provide a combination of automatic argument analysis and visual argument summaries. With similar intentions, Lexical Episode Plots (Gold et al., 2015), ConToVi (El-Assady et al., 2016), NEREx (El-Assady et al., 2017), and Jentner et al. (2017) visualize specific aspects of transcribed discussions. All these works focus on single arguments or the set of arguments within a single debate or text. In contrast, we present a visualization that summarizes arguments from many different texts. Unlike in (Wachsmuth et al., 2017a), where we illustrated structural argumentation patterns in the texts of a corpus, here we target the content of arguments. As the above-mentioned system ConToVi, we visualize the topic space covered by a set of arguments. While ConToVi provides insights into the flow of aspects during the discussion of a controversial topic, our visualization aims to make arguments on specific aspects easily findable. Moreover, we allow arguments to cover a weighted distribution of multiple aspects rather than only a single aspect. Argument Search with args.me As presented in (Wachsmuth et al., 2017b), the"
D18-2011,W17-5106,1,0.829301,"discussion, SEAS (Lowrance et al., 2000), VUE (Baroni et al., 2015), and Dialectic Map (Niu, 2016) provide a combination of automatic argument analysis and visual argument summaries. With similar intentions, Lexical Episode Plots (Gold et al., 2015), ConToVi (El-Assady et al., 2016), NEREx (El-Assady et al., 2017), and Jentner et al. (2017) visualize specific aspects of transcribed discussions. All these works focus on single arguments or the set of arguments within a single debate or text. In contrast, we present a visualization that summarizes arguments from many different texts. Unlike in (Wachsmuth et al., 2017a), where we illustrated structural argumentation patterns in the texts of a corpus, here we target the content of arguments. As the above-mentioned system ConToVi, we visualize the topic space covered by a set of arguments. While ConToVi provides insights into the flow of aspects during the discussion of a controversial topic, our visualization aims to make arguments on specific aspects easily findable. Moreover, we allow arguments to cover a weighted distribution of multiple aspects rather than only a single aspect. Argument Search with args.me As presented in (Wachsmuth et al., 2017b), the"
D19-1290,N16-1165,1,0.8595,"e people debate or collect arguments for or against controversial topics. Some debate portals are dialogical, such as debate.org, allowing two opponents to debate one topic in rounds. Other debate portals such as debatepedia.org are wiki-like where arguments are listed according to their stance on the topic. Debate portals keep a canonical structure of the arguments considered for each topic (usually a conclusion and a premise). The structure and the wide topic coverage offered by debate portals has made them a suitable resource for research on computational argumentation (Cabrio and Villata; Al-Khatib et al., 2016; Wachsmuth et al., 2017a). 3.1 Argument Frames from Debatepedia.org For the given work, we crawled all arguments from debatepedia.org in order to construct a dataset for the evaluation of frame identification. Debatepe# Topics # Frames # Merged Frames # Arguments 465 1 645 1 623 12 326 Table 1: Counts of topics, frames, merged frames, and arguments in the webis-argument-framing-19 dataset. Frames Economics 119 Public Opinion Environment Feasibility Rights Democracy Crime Politics Security Safety 20 40 60 80 100 #Topics Figure 1: The number of topics in which each of the 10 most frequent frame"
D19-1290,P16-1150,0,0.0251517,"smuth et al., 2923 2017a; Levy et al., 2018; Stab et al., 2018) with the goal of retrieving relevant arguments for an input claim. Use cases for argument search include writing and debating support. In comparison to user queries in conventional search that can often be satisfied by one or a few retrieved documents, these use cases require a broader consideration of the retrieved arguments. Hence, the user of an argument search engine will often investigate both stances and multiple frames on a given topic. While several studies tackled the task of ranking arguments according to their quality (Habernal and Gurevych, 2016; Wachsmuth et al., 2017b), how to aggregate arguments into frames is largely unstudied. The relation between arguments and frames was introduced briefly in some works (Boydstun et al., 2013; Gabrielsen et al., 2011). Still, recent research on computational argumentation largely ignores frames, and a model for aggregating arguments into frames is still missing. Naderi (2016) considered a frame to be an argument and classified sentences in parliamentary speeches into one of seven frames. (Reimers et al., 2019) created a dataset of argument pairs that are labeled according to their similarity. B"
D19-1290,E17-1024,0,0.124406,"Missing"
D19-1290,W17-5105,0,0.0274675,"Missing"
D19-1290,P15-2072,0,0.046065,"the research community.1 2 Related Work Research on framing is scattered across different fields such as media, social, and cognitive studies. Entman (1993) was the first to introduce a formal definition of framing as a way to select and make specific aspects of a topic salient. Subsequent research on framing is concentrated on the effect of using frames in news on a specific audience. One of the open questions is whether frames are topic-specific or generic concepts, or both. Vreese (2005) studied framing in news articles and considered frames to be both of the two. Johnson et al. (2017) and Card et al. (2015), on the other hand, defined frames to be independent of the topic and investigated their usage across different topics. 1 Argument framing dataset: https://webis.de/ data/webis-argument-framing-19.html or https://doi.org/10.5281/zenodo.3373355 Recently, framing caught some attention in the NLP community. Different computational models have been developed for modeling frames in natural language text. Tsur et al. (2015) used topic models on statements released by congress members of the two major parties in the US, Republicans and Democrats. The learned topics were then aggregated into clusters"
D19-1290,C14-1141,0,0.05413,"ss their audience. The constellation of pro and con arguments for a topic is an urgent need for authors of argumentative texts. Argument search is a new research area that aims at assessing users in forming an opinion and debating. Current approaches use clas2922 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2922–2932, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics sifiers to mine arguments for a given topic from a relevant document (Levy et al., 2014; Stab et al., 2018; Wachsmuth et al., 2017a). The mentioned approaches ignore identifying the frames of arguments during mining and retrieval, this way omitting extremely valuable information. The paper in hand starts by reviewing related work to framing (Section 2). In Section 3, we introduce the first argument dataset that has been annotated with frames and topics, and we provide statistical insights into the dataset. Section 4 presents a new unsupervised approach to identify frames in a set of arguments. Our approach first removes topical features from the arguments and then clusters the a"
D19-1290,C18-1176,0,0.0426699,"ments from natural language text (Al-Khatib et al., 2016). Most approaches uses supervised classifiers to extract the structure of arguments (conclusion and premise) (Stab and Gurevych, 2014). (Lawerence and Reed, 2017) showed that topic models helps identifying the relevance of a premise to a conclusion when they are trained on topically relevant documents. The stance of the mined arguments is classified as pro or con towards a given topic (Somasundaran and Wiebe, 2010; Bar-Haim et al., 2017). The arguments are then used for applications such as argument search (Wachsmuth et al., 2923 2017a; Levy et al., 2018; Stab et al., 2018) with the goal of retrieving relevant arguments for an input claim. Use cases for argument search include writing and debating support. In comparison to user queries in conventional search that can often be satisfied by one or a few retrieved documents, these use cases require a broader consideration of the retrieved arguments. Hence, the user of an argument search engine will often investigate both stances and multiple frames on a given topic. While several studies tackled the task of ranking arguments according to their quality (Habernal and Gurevych, 2016; Wachsmuth et a"
D19-1290,D17-1318,0,0.0504683,"Missing"
D19-1290,naderi-hirst-2017-classifying,0,0.147114,"Missing"
D19-1290,P19-1054,0,0.105236,"veral studies tackled the task of ranking arguments according to their quality (Habernal and Gurevych, 2016; Wachsmuth et al., 2017b), how to aggregate arguments into frames is largely unstudied. The relation between arguments and frames was introduced briefly in some works (Boydstun et al., 2013; Gabrielsen et al., 2011). Still, recent research on computational argumentation largely ignores frames, and a model for aggregating arguments into frames is still missing. Naderi (2016) considered a frame to be an argument and classified sentences in parliamentary speeches into one of seven frames. (Reimers et al., 2019) created a dataset of argument pairs that are labeled according to their similarity. Based on the dataset, they introduced the task of argument clustering which aims at classifying an argument pair with the same topic into similar or dissimilar. The main difference to this work is that no explicit aspect are assigned to the arguments during annotation. 3 Data Debate portals are websites where people debate or collect arguments for or against controversial topics. Some debate portals are dialogical, such as debate.org, allowing two opponents to debate one topic in rounds. Other debate portals s"
D19-1290,W10-0214,0,0.102823,"Missing"
D19-1290,N18-5005,0,0.0752923,"Missing"
D19-1290,D14-1006,0,0.127534,"Missing"
D19-1290,P15-1157,0,0.0725048,"Missing"
D19-1290,W17-5106,1,0.936702,"f pro and con arguments for a topic is an urgent need for authors of argumentative texts. Argument search is a new research area that aims at assessing users in forming an opinion and debating. Current approaches use clas2922 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2922–2932, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics sifiers to mine arguments for a given topic from a relevant document (Levy et al., 2014; Stab et al., 2018; Wachsmuth et al., 2017a). The mentioned approaches ignore identifying the frames of arguments during mining and retrieval, this way omitting extremely valuable information. The paper in hand starts by reviewing related work to framing (Section 2). In Section 3, we introduce the first argument dataset that has been annotated with frames and topics, and we provide statistical insights into the dataset. Section 4 presents a new unsupervised approach to identify frames in a set of arguments. Our approach first removes topical features from the arguments and then clusters the arguments into frames. In Section 5, we desc"
D19-1290,E17-1105,1,0.941953,"f pro and con arguments for a topic is an urgent need for authors of argumentative texts. Argument search is a new research area that aims at assessing users in forming an opinion and debating. Current approaches use clas2922 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2922–2932, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics sifiers to mine arguments for a given topic from a relevant document (Levy et al., 2014; Stab et al., 2018; Wachsmuth et al., 2017a). The mentioned approaches ignore identifying the frames of arguments during mining and retrieval, this way omitting extremely valuable information. The paper in hand starts by reviewing related work to framing (Section 2). In Section 3, we introduce the first argument dataset that has been annotated with frames and topics, and we provide statistical insights into the dataset. Section 4 presents a new unsupervised approach to identify frames in a set of arguments. Our approach first removes topical features from the arguments and then clusters the arguments into frames. In Section 5, we desc"
D19-5009,W16-3638,0,0.0209312,"Waseem and Hovy, 2016), Yahoo (Nobata et al., 2016), and Wikipedia (Wulczyn et al., 2017). In terms of the number of labeled texts, the latter is the biggest, consisting of more than 100,000 Wikipedia talk page comments. We use this dataset for the evaluation of our approach. Abusive (or offensive) language detection usually follows a supervised learning paradigm with either binary or multi-class classifiers. While existing abusiveness classifiers exploit a variety of lexical, syntactic, semantic, and knowledge-based features, one study showed character n-grams alone to be very good features (Mehdad and Tetreault, 2016). Until recently, the most effective overall approaches rely on neural network architectures such as CNN and RNN (Badjatiya et al., 2017; Pavlopoulos et al., 2017). On the personal attacks corpus, Pavlopoulos et al. (2017) have developed several very effective deep learning models with word embedding features. We employ the best-performing neural model, but we analyze the effect of adding our new approach (i.e., to unravel the abusiveness search space) that simultaneously helps to improve lexicon-based explainability. • We investigate how to unravel the search space of abusive language based o"
D19-5009,I13-1066,0,0.0312922,"he target of attack, i.e., being the direct recipient or a third party. The experimental results show that our search space unraveling slightly improves over state-ofthe-art single-space classifiers with the additional bonus of a dynamic abusiveness lexicon that can help to explain the classifier’s decisions. The contribution of this paper is three-fold: Related Work The automatic detection of abusive language has been studied extensively in the last years. Proposed approaches target different types of abusive language, ranging from hate speech (Warner and Hirschberg, 2012) and cyberbullying (Nitta et al., 2013) to profanity (Sood et al., 2012) and personal attacks (Wulczyn et al., 2017). Despite the importance of labeled data for abusive language detection, only few datasets are available so far for this task. Most of them come from large online platforms, such as Twitter (Waseem and Hovy, 2016), Yahoo (Nobata et al., 2016), and Wikipedia (Wulczyn et al., 2017). In terms of the number of labeled texts, the latter is the biggest, consisting of more than 100,000 Wikipedia talk page comments. We use this dataset for the evaluation of our approach. Abusive (or offensive) language detection usually follo"
D19-5009,D17-1117,0,0.138631,"ting of more than 100,000 Wikipedia talk page comments. We use this dataset for the evaluation of our approach. Abusive (or offensive) language detection usually follows a supervised learning paradigm with either binary or multi-class classifiers. While existing abusiveness classifiers exploit a variety of lexical, syntactic, semantic, and knowledge-based features, one study showed character n-grams alone to be very good features (Mehdad and Tetreault, 2016). Until recently, the most effective overall approaches rely on neural network architectures such as CNN and RNN (Badjatiya et al., 2017; Pavlopoulos et al., 2017). On the personal attacks corpus, Pavlopoulos et al. (2017) have developed several very effective deep learning models with word embedding features. We employ the best-performing neural model, but we analyze the effect of adding our new approach (i.e., to unravel the abusiveness search space) that simultaneously helps to improve lexicon-based explainability. • We investigate how to unravel the search space of abusive language based on the underlying offending way. • We develop computational approach that performs the unraveling in practice, and we evaluate it for the classification of Wikipedi"
D19-5009,D14-1162,0,0.0821523,"dard approaches training abusiveness classifiers on all examples at once, we propose to apply a three-stage approach. 5.1 To represent the state of the art, we employ the best-performing model on the personal attack corpus proposed by Pavlopoulos et al. (2017): an RNN model where the basic cell is a GRU. An embedding layer transforms an input word sequence into a word embedding sequence. Then, the model learns a hidden state from the word embeddings. The hidden state is employed to predict the probability of ‘not-attack’ using a linear regression layer. We use 300-dimensional word embeddings (Pennington et al., 2014) pre-trained on the Common Crawl with 840 billion tokens and a vocabulary size of 2.2 million. Out-of-vocabulary words are mapped to one random vector. We use Glorot (Glorot and Bengio, 2010) to initialize the model, with mean-square error as loss function, Adam for optimization (Kingma and Ba, 2014), a learning rate of 0.001, and a batch size of 128. The initial abusive lexicon used for splitting the search space is the complete set of words in the base lexicon of Wiegand et al. (2018) containing 1650 negative polar expressions. This lexicon performed better in our pilot experiments compared"
D19-5009,W17-1101,0,0.0140578,"ossibly in obfuscated form (e.g., “a$$h0le”), or abusiveness can also happen implicitly via sarcasm (e.g., “go back to school, whatever you learned didn’t stick”) or via new racist or abusive codes (e.g., on the platform 4chan, “Google” is used as a slur for black people, “skittle” for Arabs, and “butterfly” for gays).1 Some recent studies have pointed to different types and to the importance of separating them, especially (Waseem et al., 2017). However, the distinction between the different offending dimensions has hardly been investigated for the development of abusive language classifiers (Schmidt and Wiegand, 2017). Accordingly, existing approaches consider the language of all abusive texts irrespective of their offending dimensions as one single search space. They simply train one machine learning model with different linguistic features on this space in order to classify unseen text as being abusive or not. Due to the diversity of language in offending dimensions, we expect such models to often result in limited effectiveness in practice. The reason is that, when learning to detect abusive texts following one way, for instance, the inclusion of training texts following other ways induces noise that di"
D19-5009,W12-2103,0,0.05317,"ddition, the corpus includes manual labels for the target of attack, i.e., being the direct recipient or a third party. The experimental results show that our search space unraveling slightly improves over state-ofthe-art single-space classifiers with the additional bonus of a dynamic abusiveness lexicon that can help to explain the classifier’s decisions. The contribution of this paper is three-fold: Related Work The automatic detection of abusive language has been studied extensively in the last years. Proposed approaches target different types of abusive language, ranging from hate speech (Warner and Hirschberg, 2012) and cyberbullying (Nitta et al., 2013) to profanity (Sood et al., 2012) and personal attacks (Wulczyn et al., 2017). Despite the importance of labeled data for abusive language detection, only few datasets are available so far for this task. Most of them come from large online platforms, such as Twitter (Waseem and Hovy, 2016), Yahoo (Nobata et al., 2016), and Wikipedia (Wulczyn et al., 2017). In terms of the number of labeled texts, the latter is the biggest, consisting of more than 100,000 Wikipedia talk page comments. We use this dataset for the evaluation of our approach. Abusive (or offe"
D19-5009,N16-2013,0,0.0158751,"classifier’s decisions. The contribution of this paper is three-fold: Related Work The automatic detection of abusive language has been studied extensively in the last years. Proposed approaches target different types of abusive language, ranging from hate speech (Warner and Hirschberg, 2012) and cyberbullying (Nitta et al., 2013) to profanity (Sood et al., 2012) and personal attacks (Wulczyn et al., 2017). Despite the importance of labeled data for abusive language detection, only few datasets are available so far for this task. Most of them come from large online platforms, such as Twitter (Waseem and Hovy, 2016), Yahoo (Nobata et al., 2016), and Wikipedia (Wulczyn et al., 2017). In terms of the number of labeled texts, the latter is the biggest, consisting of more than 100,000 Wikipedia talk page comments. We use this dataset for the evaluation of our approach. Abusive (or offensive) language detection usually follows a supervised learning paradigm with either binary or multi-class classifiers. While existing abusiveness classifiers exploit a variety of lexical, syntactic, semantic, and knowledge-based features, one study showed character n-grams alone to be very good features (Mehdad and Tetreault,"
D19-5009,W17-3012,0,0.0215798,"nformatik .uni-halle.de Abstract other person, entity, or group (other recipient). On the other hand, abusive words and phrases may be used explicitly (e.g., “asshole!”), possibly in obfuscated form (e.g., “a$$h0le”), or abusiveness can also happen implicitly via sarcasm (e.g., “go back to school, whatever you learned didn’t stick”) or via new racist or abusive codes (e.g., on the platform 4chan, “Google” is used as a slur for black people, “skittle” for Arabs, and “butterfly” for gays).1 Some recent studies have pointed to different types and to the importance of separating them, especially (Waseem et al., 2017). However, the distinction between the different offending dimensions has hardly been investigated for the development of abusive language classifiers (Schmidt and Wiegand, 2017). Accordingly, existing approaches consider the language of all abusive texts irrespective of their offending dimensions as one single search space. They simply train one machine learning model with different linguistic features on this space in order to classify unseen text as being abusive or not. Due to the diversity of language in offending dimensions, we expect such models to often result in limited effectiveness"
D19-5009,N18-1095,0,0.0509307,"th the predictions of the developed classifier. The first lexicon contains 1650 words and expressions in which 551 of them are abusive, while the second contains 8478 words and expressions with 2989 abusive ones. The results of using the lexicon for detecting the abusive language in micro-posts demonstrate high effectiveness, particularly in cross-domain settings. Data In this section, we detail the data that we employ for the implementation and evaluation of our approach. Specifically, we describe the Wikipedia personal attack corpus (Wulczyn et al., 2017) and the abusive language lexicon of Wiegand et al. (2018). 3.1 Train Wikipedia Personal Attack Corpus Wikipedia is one of the online platforms suffering from abusive language, especially from personal attacks (Shachaf and Hara, 2010). In particular, each Wikipedia article is associated to a so called talk page, where users are solicited to write comments in order to discuss and improve the quality of the article’s content. While the large majority of comments is valuable, some users attack others with texts comprising hate speech and harassment, among others. Our analysis and evaluation are based on the personal attack corpus (Wulczyn et al., 2017)"
D19-5009,H05-1044,0,0.0621967,"Missing"
E17-1017,N16-1165,1,0.380564,"sive? When is it reasonable? We subsume such questions under the term argumentation quality; they have driven logicians, rhetoricians, linguists, and argumentation theorists since the Ancient Greeks (Aristotle, 2007). Now that the area of computational argumentation is seeing an influx of research activity, the automatic assessment of argumentation quality is coming into the focus, due to its importance for envisioned applications such as writing support (Stab and Gurevych, 2014) and argument search (Wachsmuth et al., 2017), among others. Existing research covers the mining of argument units (Al-Khatib et al., 2016), specific types of evidence (Rinott et al., 2015), and argumentative relations (Peldszus and Stede, 2015). Other works clasEveryone has an inalienable human right to life, even those who commit murder; sentencing a person to death and executing them violates that right. Although implicit, the conclusion about the death penalty seems sound in terms of (informal) logic, and the argument is clear from a linguistic viewpoint. Some people might not accept the first stated premise, though, especially if emotionally affected by some legal case at hand. Or, they might not be persuaded that the stated"
E17-1017,W16-2808,0,0.057931,") Winning side Zhang et al. (2016) Convincingness Habernal et al. (2016) Figure 1: The proposed taxonomy of argumentation quality as well as the mapping of existing assessment approaches to the covered quality dimensions. Arrows show main dependencies between the dimensions. even if said to aim for argument quality (Swanson et al., 2015). Much work exists for general text quality, most notably in the context of readability (Pitler and Nenkova, 2008) and classical essay scoring. Some scoring approaches derive features from discourse (Burstein et al., 1998), arguments (Ong et al., 2014; Beigman Klebanov et al., 2016; Ghosh et al., 2016), or schemes (Song et al., 2014)—all this may be indicative of quality. However, our focus is approaches that target argumentation quality at heart. Similarly, review helpfulness (Liu et al., 2008) and deception (Ott et al., 2011) are not treated, as arguments only partly play a role there. Also, only few Wikipedia quality flaws relate to arguments, e.g., verifiability (Anderka et al., 2012). 3 A Taxonomy of Argumentation Quality Given all surveyed quality dimensions, we now propose a unifying taxonomy of argumentation quality. The taxonomy decomposes quality assessment sy"
E17-1017,P16-2085,1,0.841124,"Naderi Yufang Hou University of Toronto IBM Research Toronto, Canada Dublin, Ireland nona@cs.toronto.edu yhou@ie.ibm.com Yonatan Bilu IBM Research Haifa, Israel yonatanb@il.ibm.com Vinodkumar Prabhakaran Tim Alberdingk Thijm, Graeme Hirst Benno Stein Stanford University University of Toronto Bauhaus-Universität Weimar Stanford, CA, USA Toronto, Canada Weimar, Germany vinod@cs.stanford.edu {thijm, gh}@cs.toronto.edu Abstract sify argumentation schemes (Feng et al., 2014) and frames (Naderi and Hirst, 2015), analyze overall argumentation structures (Wachsmuth et al., 2015), or generate claims (Bilu and Slonim, 2016). Also, theories of argumentation quality exist, and some quality dimensions have been assessed computationally (see Section 2 for details). Until now, however, the assertion of O’Keefe and Jackson (1995) that there is neither a general idea of what constitutes argumentation quality in natural language nor a clear definition of its dimensions still holds. The reasons for this deficit originate in the varying goals of argumentation: persuading audiences, resolving disputes, achieving agreement, completing inquiries, and recommending actions (Tindale, 2007). As a result, diverse quality dimensio"
E17-1017,W15-0514,0,0.0600702,"Missing"
E17-1017,W98-0303,0,0.311573,"nal appeal Persuasiveness Tan et al. (2016), Wei et al. (2016) Winning side Zhang et al. (2016) Convincingness Habernal et al. (2016) Figure 1: The proposed taxonomy of argumentation quality as well as the mapping of existing assessment approaches to the covered quality dimensions. Arrows show main dependencies between the dimensions. even if said to aim for argument quality (Swanson et al., 2015). Much work exists for general text quality, most notably in the context of readability (Pitler and Nenkova, 2008) and classical essay scoring. Some scoring approaches derive features from discourse (Burstein et al., 1998), arguments (Ong et al., 2014; Beigman Klebanov et al., 2016; Ghosh et al., 2016), or schemes (Song et al., 2014)—all this may be indicative of quality. However, our focus is approaches that target argumentation quality at heart. Similarly, review helpfulness (Liu et al., 2008) and deception (Ott et al., 2011) are not treated, as arguments only partly play a role there. Also, only few Wikipedia quality flaws relate to arguments, e.g., verifiability (Anderka et al., 2012). 3 A Taxonomy of Argumentation Quality Given all surveyed quality dimensions, we now propose a unifying taxonomy of argument"
E17-1017,P12-2041,0,0.119946,"Missing"
E17-1017,P11-1099,1,0.393064,"Missing"
E17-1017,C14-1089,1,0.513151,"tation Quality Assessment in Natural Language Henning Wachsmuth Bauhaus-Universität Weimar Weimar, Germany henning.wachsmuth@uni-weimar.de Nona Naderi Yufang Hou University of Toronto IBM Research Toronto, Canada Dublin, Ireland nona@cs.toronto.edu yhou@ie.ibm.com Yonatan Bilu IBM Research Haifa, Israel yonatanb@il.ibm.com Vinodkumar Prabhakaran Tim Alberdingk Thijm, Graeme Hirst Benno Stein Stanford University University of Toronto Bauhaus-Universität Weimar Stanford, CA, USA Toronto, Canada Weimar, Germany vinod@cs.stanford.edu {thijm, gh}@cs.toronto.edu Abstract sify argumentation schemes (Feng et al., 2014) and frames (Naderi and Hirst, 2015), analyze overall argumentation structures (Wachsmuth et al., 2015), or generate claims (Bilu and Slonim, 2016). Also, theories of argumentation quality exist, and some quality dimensions have been assessed computationally (see Section 2 for details). Until now, however, the assertion of O’Keefe and Jackson (1995) that there is neither a general idea of what constitutes argumentation quality in natural language nor a clear definition of its dimensions still holds. The reasons for this deficit originate in the varying goals of argumentation: persuading audien"
E17-1017,P16-2089,0,0.0173404,"al. (2016) Convincingness Habernal et al. (2016) Figure 1: The proposed taxonomy of argumentation quality as well as the mapping of existing assessment approaches to the covered quality dimensions. Arrows show main dependencies between the dimensions. even if said to aim for argument quality (Swanson et al., 2015). Much work exists for general text quality, most notably in the context of readability (Pitler and Nenkova, 2008) and classical essay scoring. Some scoring approaches derive features from discourse (Burstein et al., 1998), arguments (Ong et al., 2014; Beigman Klebanov et al., 2016; Ghosh et al., 2016), or schemes (Song et al., 2014)—all this may be indicative of quality. However, our focus is approaches that target argumentation quality at heart. Similarly, review helpfulness (Liu et al., 2008) and deception (Ott et al., 2011) are not treated, as arguments only partly play a role there. Also, only few Wikipedia quality flaws relate to arguments, e.g., verifiability (Anderka et al., 2012). 3 A Taxonomy of Argumentation Quality Given all surveyed quality dimensions, we now propose a unifying taxonomy of argumentation quality. The taxonomy decomposes quality assessment systematically, thus or"
E17-1017,P16-1150,0,0.41004,"right side of Figure 1 show where the approaches surveyed in Section 2.2 are positioned in the taxonomy. Some dimensions have been tackled multiple times (e.g., clarity), others not at all (e.g., credibility). The taxonomy indicates what sub-dimensions will affect the same high-level dimension. 4 The Dagstuhl-15512 ArgQuality Corpus Finally, we present our new annotated Dagstuhl15512 ArgQuality Corpus for studying argumentation quality based on the developed taxonomy, and we report on a first corpus analysis.3 4.1 Data and Annotation Process Our corpus is based on the UKPConvArgRank dataset (Habernal and Gurevych, 2016), which contains rankings of 25 to 35 textual debate portal arguments for two stances on 16 issues, such as evolution vs. creation and ban plastic water bottles. All ranks were derived from crowdsourced convincingness labels. For every issue/stance pair, we took the five top-ranked texts and chose five further via stratified sampling. Thereby, we covered both high-quality arguments and different levels of lower quality. Two example texts follow below in Figure 2. Before annotating the 320 chosen texts, we carried out a full annotation study with seven authors of this paper on 20 argumentative"
E17-1017,W14-2104,0,0.0646839,". (2016), Wei et al. (2016) Winning side Zhang et al. (2016) Convincingness Habernal et al. (2016) Figure 1: The proposed taxonomy of argumentation quality as well as the mapping of existing assessment approaches to the covered quality dimensions. Arrows show main dependencies between the dimensions. even if said to aim for argument quality (Swanson et al., 2015). Much work exists for general text quality, most notably in the context of readability (Pitler and Nenkova, 2008) and classical essay scoring. Some scoring approaches derive features from discourse (Burstein et al., 1998), arguments (Ong et al., 2014; Beigman Klebanov et al., 2016; Ghosh et al., 2016), or schemes (Song et al., 2014)—all this may be indicative of quality. However, our focus is approaches that target argumentation quality at heart. Similarly, review helpfulness (Liu et al., 2008) and deception (Ott et al., 2011) are not treated, as arguments only partly play a role there. Also, only few Wikipedia quality flaws relate to arguments, e.g., verifiability (Anderka et al., 2012). 3 A Taxonomy of Argumentation Quality Given all surveyed quality dimensions, we now propose a unifying taxonomy of argumentation quality. The taxonomy d"
E17-1017,P11-1032,0,0.00976501,"ween the dimensions. even if said to aim for argument quality (Swanson et al., 2015). Much work exists for general text quality, most notably in the context of readability (Pitler and Nenkova, 2008) and classical essay scoring. Some scoring approaches derive features from discourse (Burstein et al., 1998), arguments (Ong et al., 2014; Beigman Klebanov et al., 2016; Ghosh et al., 2016), or schemes (Song et al., 2014)—all this may be indicative of quality. However, our focus is approaches that target argumentation quality at heart. Similarly, review helpfulness (Liu et al., 2008) and deception (Ott et al., 2011) are not treated, as arguments only partly play a role there. Also, only few Wikipedia quality flaws relate to arguments, e.g., verifiability (Anderka et al., 2012). 3 A Taxonomy of Argumentation Quality Given all surveyed quality dimensions, we now propose a unifying taxonomy of argumentation quality. The taxonomy decomposes quality assessment systematically, thus organizing and clarifying the roles of practical approaches. It does not require a particular argumentation model, but it rests on the notion of the granularity levels from Section 1. 3.1 Overview of the Theory-based Taxonomy Our ob"
E17-1017,D15-1110,0,0.0252984,"riven logicians, rhetoricians, linguists, and argumentation theorists since the Ancient Greeks (Aristotle, 2007). Now that the area of computational argumentation is seeing an influx of research activity, the automatic assessment of argumentation quality is coming into the focus, due to its importance for envisioned applications such as writing support (Stab and Gurevych, 2014) and argument search (Wachsmuth et al., 2017), among others. Existing research covers the mining of argument units (Al-Khatib et al., 2016), specific types of evidence (Rinott et al., 2015), and argumentative relations (Peldszus and Stede, 2015). Other works clasEveryone has an inalienable human right to life, even those who commit murder; sentencing a person to death and executing them violates that right. Although implicit, the conclusion about the death penalty seems sound in terms of (informal) logic, and the argument is clear from a linguistic viewpoint. Some people might not accept the first stated premise, though, especially if emotionally affected by some legal case at hand. Or, they might not be persuaded that the stated argument is the most relevant in the debate on death penalty. This example reveals three central challeng"
E17-1017,P13-1026,0,0.362263,"Missing"
E17-1017,P14-1144,0,0.0803542,"Missing"
E17-1017,P15-1053,0,0.125852,"Missing"
E17-1017,D10-1023,0,0.426521,"Missing"
E17-1017,D08-1020,0,0.0224306,"brio and Villata (2012) Local relevance Cogency Rahimi et al. (2014) Acceptability Persing et al. (2015) Emotional appeal Persuasiveness Tan et al. (2016), Wei et al. (2016) Winning side Zhang et al. (2016) Convincingness Habernal et al. (2016) Figure 1: The proposed taxonomy of argumentation quality as well as the mapping of existing assessment approaches to the covered quality dimensions. Arrows show main dependencies between the dimensions. even if said to aim for argument quality (Swanson et al., 2015). Much work exists for general text quality, most notably in the context of readability (Pitler and Nenkova, 2008) and classical essay scoring. Some scoring approaches derive features from discourse (Burstein et al., 1998), arguments (Ong et al., 2014; Beigman Klebanov et al., 2016; Ghosh et al., 2016), or schemes (Song et al., 2014)—all this may be indicative of quality. However, our focus is approaches that target argumentation quality at heart. Similarly, review helpfulness (Liu et al., 2008) and deception (Ott et al., 2011) are not treated, as arguments only partly play a role there. Also, only few Wikipedia quality flaws relate to arguments, e.g., verifiability (Anderka et al., 2012). 3 A Taxonomy of"
E17-1017,W15-0603,0,0.0749477,"ncy, following the definition of Johnson and Blair (2006). Their experiments suggest that convolutional neural networks outperform feature-based sufficiency classification. Rhetoric Persing et al. (2010) tackle the proper arrangement of an essay, namely, its organization in terms of the logical development of an argument. The authors rely on manual 7-point score annotations for 1003 essays from the ICLE corpus (Granger et al., 2009). In their experiments, sequences of paragraph discourse functions (e.g., introduction or rebuttal) turn out to be most effective. Organization is also analyzed by Rahimi et al. (2015) on the same dataset used for the evidence 179 Aspect Quality Dimension Granularity Text Genres Sources Evidence Level of support Sufficiency Argumentation Argument unit Argument Student essays Wikipedia articles Student essays Rahimi et al. (2014) Braunstain et al. (2016) Stab and Gurevych (2017) Rhetoric Argument strength Evaluability Global coherence Organization Persuasiveness Prompt adherence Thesis clarity Winning side Argumentation Argumentation Argumentation Argumentation Argument Argumentation Argumentation Debate Student essays Law comments Student essays Student essays Forum discuss"
E17-1017,D15-1050,0,0.330444,"s under the term argumentation quality; they have driven logicians, rhetoricians, linguists, and argumentation theorists since the Ancient Greeks (Aristotle, 2007). Now that the area of computational argumentation is seeing an influx of research activity, the automatic assessment of argumentation quality is coming into the focus, due to its importance for envisioned applications such as writing support (Stab and Gurevych, 2014) and argument search (Wachsmuth et al., 2017), among others. Existing research covers the mining of argument units (Al-Khatib et al., 2016), specific types of evidence (Rinott et al., 2015), and argumentative relations (Peldszus and Stede, 2015). Other works clasEveryone has an inalienable human right to life, even those who commit murder; sentencing a person to death and executing them violates that right. Although implicit, the conclusion about the death penalty seems sound in terms of (informal) logic, and the argument is clear from a linguistic viewpoint. Some people might not accept the first stated premise, though, especially if emotionally affected by some legal case at hand. Or, they might not be persuaded that the stated argument is the most relevant in the debate on de"
E17-1017,W14-2110,0,0.0505174,"al et al. (2016) Figure 1: The proposed taxonomy of argumentation quality as well as the mapping of existing assessment approaches to the covered quality dimensions. Arrows show main dependencies between the dimensions. even if said to aim for argument quality (Swanson et al., 2015). Much work exists for general text quality, most notably in the context of readability (Pitler and Nenkova, 2008) and classical essay scoring. Some scoring approaches derive features from discourse (Burstein et al., 1998), arguments (Ong et al., 2014; Beigman Klebanov et al., 2016; Ghosh et al., 2016), or schemes (Song et al., 2014)—all this may be indicative of quality. However, our focus is approaches that target argumentation quality at heart. Similarly, review helpfulness (Liu et al., 2008) and deception (Ott et al., 2011) are not treated, as arguments only partly play a role there. Also, only few Wikipedia quality flaws relate to arguments, e.g., verifiability (Anderka et al., 2012). 3 A Taxonomy of Argumentation Quality Given all surveyed quality dimensions, we now propose a unifying taxonomy of argumentation quality. The taxonomy decomposes quality assessment systematically, thus organizing and clarifying the role"
E17-1017,D14-1006,0,0.0178285,"essment. 1 benno.stein@uni-weimar.de Introduction What is a good argument? What premises should it be based on? When is argumentation persuasive? When is it reasonable? We subsume such questions under the term argumentation quality; they have driven logicians, rhetoricians, linguists, and argumentation theorists since the Ancient Greeks (Aristotle, 2007). Now that the area of computational argumentation is seeing an influx of research activity, the automatic assessment of argumentation quality is coming into the focus, due to its importance for envisioned applications such as writing support (Stab and Gurevych, 2014) and argument search (Wachsmuth et al., 2017), among others. Existing research covers the mining of argument units (Al-Khatib et al., 2016), specific types of evidence (Rinott et al., 2015), and argumentative relations (Peldszus and Stede, 2015). Other works clasEveryone has an inalienable human right to life, even those who commit murder; sentencing a person to death and executing them violates that right. Although implicit, the conclusion about the death penalty seems sound in terms of (informal) logic, and the argument is clear from a linguistic viewpoint. Some people might not accept the f"
E17-1017,W16-2813,0,0.364991,"Missing"
E17-1017,E17-1092,0,0.242722,"al models and argument-oriented features, they rank sentencelevel argument units according to the level of support they provide for an answer. Unlike classical essay scoring, Rahimi et al. (2014) score an essay’s evidence, a quality dimension of argumentation: it captures how sufficiently the given details support the essay’s thesis. On the dataset of Correnti et al. (2013) with 1569 student essays and scores from 1 to 4, they find that the concentration and specificity of words related to the essay prompt (i.e., the statement defining the discussed issue) impacts scoring accuracy. Similarly, Stab and Gurevych (2017) introduce an essay corpus with 1029 argument-level annotations of sufficiency, following the definition of Johnson and Blair (2006). Their experiments suggest that convolutional neural networks outperform feature-based sufficiency classification. Rhetoric Persing et al. (2010) tackle the proper arrangement of an essay, namely, its organization in terms of the logical development of an argument. The authors rely on manual 7-point score annotations for 1003 essays from the ICLE corpus (Granger et al., 2009). In their experiments, sequences of paragraph discourse functions (e.g., introduction or"
E17-1017,W15-4631,0,0.0408966,"ltužic´ and Šnajder (2015) Relevance Reasonableness le Sufficiency Local sufficiency Prominence ia Evidence Cabrio and Villata (2012) Local relevance Cogency Rahimi et al. (2014) Acceptability Persing et al. (2015) Emotional appeal Persuasiveness Tan et al. (2016), Wei et al. (2016) Winning side Zhang et al. (2016) Convincingness Habernal et al. (2016) Figure 1: The proposed taxonomy of argumentation quality as well as the mapping of existing assessment approaches to the covered quality dimensions. Arrows show main dependencies between the dimensions. even if said to aim for argument quality (Swanson et al., 2015). Much work exists for general text quality, most notably in the context of readability (Pitler and Nenkova, 2008) and classical essay scoring. Some scoring approaches derive features from discourse (Burstein et al., 1998), arguments (Ong et al., 2014; Beigman Klebanov et al., 2016; Ghosh et al., 2016), or schemes (Song et al., 2014)—all this may be indicative of quality. However, our focus is approaches that target argumentation quality at heart. Similarly, review helpfulness (Liu et al., 2008) and deception (Ott et al., 2011) are not treated, as arguments only partly play a role there. Also,"
E17-1017,D15-1072,1,0.846424,"Germany henning.wachsmuth@uni-weimar.de Nona Naderi Yufang Hou University of Toronto IBM Research Toronto, Canada Dublin, Ireland nona@cs.toronto.edu yhou@ie.ibm.com Yonatan Bilu IBM Research Haifa, Israel yonatanb@il.ibm.com Vinodkumar Prabhakaran Tim Alberdingk Thijm, Graeme Hirst Benno Stein Stanford University University of Toronto Bauhaus-Universität Weimar Stanford, CA, USA Toronto, Canada Weimar, Germany vinod@cs.stanford.edu {thijm, gh}@cs.toronto.edu Abstract sify argumentation schemes (Feng et al., 2014) and frames (Naderi and Hirst, 2015), analyze overall argumentation structures (Wachsmuth et al., 2015), or generate claims (Bilu and Slonim, 2016). Also, theories of argumentation quality exist, and some quality dimensions have been assessed computationally (see Section 2 for details). Until now, however, the assertion of O’Keefe and Jackson (1995) that there is neither a general idea of what constitutes argumentation quality in natural language nor a clear definition of its dimensions still holds. The reasons for this deficit originate in the varying goals of argumentation: persuading audiences, resolving disputes, achieving agreement, completing inquiries, and recommending actions (Tindale,"
E17-1017,C16-1158,1,0.644819,"Missing"
E17-1017,E17-1105,1,0.83682,"ion What is a good argument? What premises should it be based on? When is argumentation persuasive? When is it reasonable? We subsume such questions under the term argumentation quality; they have driven logicians, rhetoricians, linguists, and argumentation theorists since the Ancient Greeks (Aristotle, 2007). Now that the area of computational argumentation is seeing an influx of research activity, the automatic assessment of argumentation quality is coming into the focus, due to its importance for envisioned applications such as writing support (Stab and Gurevych, 2014) and argument search (Wachsmuth et al., 2017), among others. Existing research covers the mining of argument units (Al-Khatib et al., 2016), specific types of evidence (Rinott et al., 2015), and argumentative relations (Peldszus and Stede, 2015). Other works clasEveryone has an inalienable human right to life, even those who commit murder; sentencing a person to death and executing them violates that right. Although implicit, the conclusion about the death penalty seems sound in terms of (informal) logic, and the argument is clear from a linguistic viewpoint. Some people might not accept the first stated premise, though, especially if em"
E17-1017,P16-2032,0,0.105377,"Missing"
E17-1017,N16-1017,0,0.0362025,"Missing"
E17-1105,W14-2109,0,0.0390945,"., 2009). To assess argument relevance objectively, we adapt 1118 a core retrieval technique, recursive link analysis. Due to its wide use, we build upon Google’s original PageRank algorithm (Page et al., 1999), but alternatives such as (Kleinberg, 1999) would also apply. PageRank is sensitive to certain manipulations, such as link farms (Croft et al., 2009). Some of them will affect argument graphs, too. Improvements of the original algorithm should therefore be taken into account in future work. In this paper, we omit them on purpose for simplicity and clarity. flat relations between units (Aharoni et al., 2014). To maximize size and heterogeneity, we here refer to the Argument Web (Bex et al., 2013), which is to our knowledge the largest ground-truth argument database available so far. It includes relationrich corpora, e.g., AraucariaDB (Reed and Rowe, 2004), as well as much annotated web text, e.g., from (Walker et al., 2012) and (Wacholder et al., 2014). Thus, it serves as a suitable basis for constructing an argument graph. We already introduced our PageRank approach in (Al-Khatib et al., 2016a), but we only roughly sketched its general idea there. Recursive analyses have also been proposed for f"
E17-1105,N16-1165,1,0.769121,"people from committing serious violent crimes. The thing that deters is the likelihood of being caught and punished.” Introduction What stance should I take? What are the best arguments to back up my stance? Information needs of people aim more and more at arguments in favor of or against a given controversial topic (Cabrio and Villata, 2012b). As a result, future information systems, above all search engines, are expected to deliver pros and cons in response to respective queries (Rinott et al., 2015). Recently, argument mining has become emerging in research, also being studied for the web (Al-Khatib et al., 2016a). Such mining finds and relates units of arguments (i.e., premises and conclusions) in natural language text, but it does not assess what arguments are relevant for a topic. Consider the following arguments (with implicit conclusions) for a query “reasons against capital punishment”: In this paper, we study from a retrieval perspective how to assess argument relevance objectively, i.e., without relying on explicit human judgments. Following argumentation theory, we see relevance as a dialectical quality that depends on how beneficial all participants of a discussion deem the use of an argume"
E17-1105,C16-1324,1,0.896205,"Missing"
E17-1105,baccianella-etal-2010-sentiwordnet,0,0.00667877,"e. 2. Frequency. An argument’s relevance corresponds to the frequency of its premises in the graph. This baseline captures popularity, as proposed in related work (see Section 2). 3. Similarity. An argument’s relevance corresponds to the similarity of its premises to its conclusion. We use the Jaccard similarity between all words in the premises and the conclusion. This basic content-oriented baseline quantifies the support of premises. 4. Sentiment. An argument’s relevance corresponds to the positivity of its premises. Here, we sum up the positive values of all premise words in SentiWordNet (Baccianella et al., 2010) and substract all negatives. Also this baseline quantifies the support of premises. 5. Most premises. An argument’s relevance corresponds to its number of premises. This simple baseline captures the amount of support. 6. Random. The relevance is decided randomly. This baseline helps interpreting the results. Experiment For all 32 conclusions of our benchmark rankings, we assessed the relevance of every associated argument with all six approaches—in case of 1.–4. once for each premise aggregation method. For all approaches, we then compared the resulting ranks with the respective benchmark ran"
E17-1105,W15-0511,0,0.0542093,"raph with three potentially relevant arguments for a queried stance. Figure 2: Example for the reuse of an argument’s conclusion as a premise in two other arguments. Figure 1 sketches an argument graph. Given a user query with a stance on a controversial topic, as shown, each argument whose (maybe implicit) conclusion c matches the stance is potentially relevant. Stance classification is outside the scope of this paper. We assess the relevance of arguments with conclusion c. The reuse of such conclusions in other arguments is exemplified in Figure 2. sion, e.g., its opposite can be generated (Bilu et al., 2015) to balance support and attack somehow. In general, the usage of conclusions as premises favors a monotonous assessment (the more the better), which we implement in Section 4. Note that we allow circles in the graph. This might look unwanted as it enables circular reasoning. However, not all arguments use the same inference rule (say, modus ponens). Hence, it is reasonable that they, directly or indirectly, refer to each other. Altogether, our model defines a framework for assessing argument relevance. It is instantiated by concrete mining and graph processing algorithms. An analysis of argume"
E17-1105,W15-0514,0,0.0721614,"Missing"
E17-1105,P12-2041,0,0.401631,"exist (see Section 2). However, they hardly account for the problem that argument quality (and relevance in particular) is often perceived subjectively. Whether a3 , e.g., is more relevant than a1 or less depends on personal judgment: Example a3 . “The death penalty doesn’t deter people from committing serious violent crimes. The thing that deters is the likelihood of being caught and punished.” Introduction What stance should I take? What are the best arguments to back up my stance? Information needs of people aim more and more at arguments in favor of or against a given controversial topic (Cabrio and Villata, 2012b). As a result, future information systems, above all search engines, are expected to deliver pros and cons in response to respective queries (Rinott et al., 2015). Recently, argument mining has become emerging in research, also being studied for the web (Al-Khatib et al., 2016a). Such mining finds and relates units of arguments (i.e., premises and conclusions) in natural language text, but it does not assess what arguments are relevant for a topic. Consider the following arguments (with implicit conclusions) for a query “reasons against capital punishment”: In this paper, we study from a ret"
E17-1105,P11-1099,0,0.119233,"ts found in web pages. Both complex argument models from theory (Toulmin, 1958; Reisert et al., 2015) and simple proprietary models (Levy et al., 2014) have been studied for web text. Some include quality-related concepts, such as evidence types (Al-Khatib et al., 2016b). Others represent the overall structure of argumentation (Wachsmuth et al., 2015). Like Mochales and Moens (2011), we consider only premises and conclusions as units of arguments here. This is the common ground of nearly all argument-level models, and it will allow an integration with approaches to analyze argument inference (Feng and Hirst, 2011) based on the argumentation schemes of Walton et al. (2008). Edges in our graph emerge from the usage of units in different arguments. Alternatively, it would be possible to mine support and attack relations between arguments (Park and Cardie, 2014; Peldszus and Stede, 2015). Not all mining steps work robustly on web text yet (Al-Khatib et al., 2016a). To focus on the impact of PageRank, we thus rely on ground-truth data in our experiments. In isolation, existing argument corpora do not adequately mimic web context, as they are small and dedicated to a specific genre (Stab and Gurevych, 2014),"
E17-1105,P16-1150,0,0.21223,"(Walton, 2006): An argument is more globally relevant the more it contributes to resolving an issue (van Eemeren, 2015). While Blair (2012) deems both types as vague and resisting analysis so far, we assess global relevance using objective statistics. In (Wachsmuth et al., 2017), we comprehensively survey theories on argumentation quality as well as computational approaches to specific quality dimensions. Among the latter, Persing and Ng (2015) rely on manual annotations of essays to predict how strong an essay’s argument is—a naturally subjective and non-scalable assessment. For scalability, Habernal and Gurevych (2016) learn on crowdsourced labels, which of two arguments is more convincing. Similar to us, they construct a graph to rank arguments, but since their graph is based on the labels, the subjectivity remains. This also holds for (Braunstain et al., 2016) where classical retrieval and argument-related features serve to rank argument units by the level of support they provide in community question answering. More objectively, Boltuži´c and Šnajder (2015) find popular arguments in online debates. However, popularity alone is often not correlated with merit (Govier, 2010). We additionally analyze depend"
E17-1105,C14-1141,0,0.0986088,"ue facts will be found on many trustworthy web pages (Yin et al., 2007; Galland et al., 2010). Pasternack and Roth (2010) model a user’s prior knowledge in addition. Close to argumentation, Samadi et al. (2016) evaluate claims using a credibility graph derived from evidence found in web pages. All these works target truth. In order to capture relevance, we base PageRank on the reuse of argument units instead. 3 In particular, we construct a graph from all arguments found in web pages. Both complex argument models from theory (Toulmin, 1958; Reisert et al., 2015) and simple proprietary models (Levy et al., 2014) have been studied for web text. Some include quality-related concepts, such as evidence types (Al-Khatib et al., 2016b). Others represent the overall structure of argumentation (Wachsmuth et al., 2015). Like Mochales and Moens (2011), we consider only premises and conclusions as units of arguments here. This is the common ground of nearly all argument-level models, and it will allow an integration with approaches to analyze argument inference (Feng and Hirst, 2011) based on the argumentation schemes of Walton et al. (2008). Edges in our graph emerge from the usage of units in different argume"
E17-1105,W14-2105,0,0.0497307,"l-Khatib et al., 2016b). Others represent the overall structure of argumentation (Wachsmuth et al., 2015). Like Mochales and Moens (2011), we consider only premises and conclusions as units of arguments here. This is the common ground of nearly all argument-level models, and it will allow an integration with approaches to analyze argument inference (Feng and Hirst, 2011) based on the argumentation schemes of Walton et al. (2008). Edges in our graph emerge from the usage of units in different arguments. Alternatively, it would be possible to mine support and attack relations between arguments (Park and Cardie, 2014; Peldszus and Stede, 2015). Not all mining steps work robustly on web text yet (Al-Khatib et al., 2016a). To focus on the impact of PageRank, we thus rely on ground-truth data in our experiments. In isolation, existing argument corpora do not adequately mimic web context, as they are small and dedicated to a specific genre (Stab and Gurevych, 2014), or restricted to The Web as an Argument Graph We now present the model that we envision as the basis for argument relevance in future web search, targeting information needs of the following kind: “What are the most relevant arguments to support o"
E17-1105,C10-1099,0,0.0306348,"t includes relationrich corpora, e.g., AraucariaDB (Reed and Rowe, 2004), as well as much annotated web text, e.g., from (Walker et al., 2012) and (Wacholder et al., 2014). Thus, it serves as a suitable basis for constructing an argument graph. We already introduced our PageRank approach in (Al-Khatib et al., 2016a), but we only roughly sketched its general idea there. Recursive analyses have also been proposed for fact finding, assuming that trustworthy web pages contain many true facts, and that true facts will be found on many trustworthy web pages (Yin et al., 2007; Galland et al., 2010). Pasternack and Roth (2010) model a user’s prior knowledge in addition. Close to argumentation, Samadi et al. (2016) evaluate claims using a credibility graph derived from evidence found in web pages. All these works target truth. In order to capture relevance, we base PageRank on the reuse of argument units instead. 3 In particular, we construct a graph from all arguments found in web pages. Both complex argument models from theory (Toulmin, 1958; Reisert et al., 2015) and simple proprietary models (Levy et al., 2014) have been studied for web text. Some include quality-related concepts, such as evidence types (Al-Khat"
E17-1105,W15-0513,0,0.0377796,". Others represent the overall structure of argumentation (Wachsmuth et al., 2015). Like Mochales and Moens (2011), we consider only premises and conclusions as units of arguments here. This is the common ground of nearly all argument-level models, and it will allow an integration with approaches to analyze argument inference (Feng and Hirst, 2011) based on the argumentation schemes of Walton et al. (2008). Edges in our graph emerge from the usage of units in different arguments. Alternatively, it would be possible to mine support and attack relations between arguments (Park and Cardie, 2014; Peldszus and Stede, 2015). Not all mining steps work robustly on web text yet (Al-Khatib et al., 2016a). To focus on the impact of PageRank, we thus rely on ground-truth data in our experiments. In isolation, existing argument corpora do not adequately mimic web context, as they are small and dedicated to a specific genre (Stab and Gurevych, 2014), or restricted to The Web as an Argument Graph We now present the model that we envision as the basis for argument relevance in future web search, targeting information needs of the following kind: “What are the most relevant arguments to support or attack my stance?” The mo"
E17-1105,P15-1053,0,0.200648,"for drawing the conclusion (Johnson and Blair, 2006). Here, we are interested in an argument’s global relevance, which refers to the benefit of the argument in a discussion (Walton, 2006): An argument is more globally relevant the more it contributes to resolving an issue (van Eemeren, 2015). While Blair (2012) deems both types as vague and resisting analysis so far, we assess global relevance using objective statistics. In (Wachsmuth et al., 2017), we comprehensively survey theories on argumentation quality as well as computational approaches to specific quality dimensions. Among the latter, Persing and Ng (2015) rely on manual annotations of essays to predict how strong an essay’s argument is—a naturally subjective and non-scalable assessment. For scalability, Habernal and Gurevych (2016) learn on crowdsourced labels, which of two arguments is more convincing. Similar to us, they construct a graph to rank arguments, but since their graph is based on the labels, the subjectivity remains. This also holds for (Braunstain et al., 2016) where classical retrieval and argument-related features serve to rank argument units by the level of support they provide in community question answering. More objectively"
E17-1105,W15-0507,0,0.0177859,"worthy web pages contain many true facts, and that true facts will be found on many trustworthy web pages (Yin et al., 2007; Galland et al., 2010). Pasternack and Roth (2010) model a user’s prior knowledge in addition. Close to argumentation, Samadi et al. (2016) evaluate claims using a credibility graph derived from evidence found in web pages. All these works target truth. In order to capture relevance, we base PageRank on the reuse of argument units instead. 3 In particular, we construct a graph from all arguments found in web pages. Both complex argument models from theory (Toulmin, 1958; Reisert et al., 2015) and simple proprietary models (Levy et al., 2014) have been studied for web text. Some include quality-related concepts, such as evidence types (Al-Khatib et al., 2016b). Others represent the overall structure of argumentation (Wachsmuth et al., 2015). Like Mochales and Moens (2011), we consider only premises and conclusions as units of arguments here. This is the common ground of nearly all argument-level models, and it will allow an integration with approaches to analyze argument inference (Feng and Hirst, 2011) based on the argumentation schemes of Walton et al. (2008). Edges in our graph"
E17-1105,D15-1050,0,0.30256,"g., is more relevant than a1 or less depends on personal judgment: Example a3 . “The death penalty doesn’t deter people from committing serious violent crimes. The thing that deters is the likelihood of being caught and punished.” Introduction What stance should I take? What are the best arguments to back up my stance? Information needs of people aim more and more at arguments in favor of or against a given controversial topic (Cabrio and Villata, 2012b). As a result, future information systems, above all search engines, are expected to deliver pros and cons in response to respective queries (Rinott et al., 2015). Recently, argument mining has become emerging in research, also being studied for the web (Al-Khatib et al., 2016a). Such mining finds and relates units of arguments (i.e., premises and conclusions) in natural language text, but it does not assess what arguments are relevant for a topic. Consider the following arguments (with implicit conclusions) for a query “reasons against capital punishment”: In this paper, we study from a retrieval perspective how to assess argument relevance objectively, i.e., without relying on explicit human judgments. Following argumentation theory, we see relevance"
E17-1105,C14-1142,0,0.0620953,"ce (Feng and Hirst, 2011) based on the argumentation schemes of Walton et al. (2008). Edges in our graph emerge from the usage of units in different arguments. Alternatively, it would be possible to mine support and attack relations between arguments (Park and Cardie, 2014; Peldszus and Stede, 2015). Not all mining steps work robustly on web text yet (Al-Khatib et al., 2016a). To focus on the impact of PageRank, we thus rely on ground-truth data in our experiments. In isolation, existing argument corpora do not adequately mimic web context, as they are small and dedicated to a specific genre (Stab and Gurevych, 2014), or restricted to The Web as an Argument Graph We now present the model that we envision as the basis for argument relevance in future web search, targeting information needs of the following kind: “What are the most relevant arguments to support or attack my stance?” The model relies on three principles that aim at the separation of concerns: I. Freedom of Inference. No inference from argument premises to conclusions is challenged. II. Freedom of Mining. No restrictions are made for how to mine and relate argument units. III. Freedom of Assessment. No graph processing method is presupposed t"
E17-1105,W14-4918,0,0.0184974,"al., 2009). Some of them will affect argument graphs, too. Improvements of the original algorithm should therefore be taken into account in future work. In this paper, we omit them on purpose for simplicity and clarity. flat relations between units (Aharoni et al., 2014). To maximize size and heterogeneity, we here refer to the Argument Web (Bex et al., 2013), which is to our knowledge the largest ground-truth argument database available so far. It includes relationrich corpora, e.g., AraucariaDB (Reed and Rowe, 2004), as well as much annotated web text, e.g., from (Walker et al., 2012) and (Wacholder et al., 2014). Thus, it serves as a suitable basis for constructing an argument graph. We already introduced our PageRank approach in (Al-Khatib et al., 2016a), but we only roughly sketched its general idea there. Recursive analyses have also been proposed for fact finding, assuming that trustworthy web pages contain many true facts, and that true facts will be found on many trustworthy web pages (Yin et al., 2007; Galland et al., 2010). Pasternack and Roth (2010) model a user’s prior knowledge in addition. Close to argumentation, Samadi et al. (2016) evaluate claims using a credibility graph derived from"
E17-1105,D15-1072,1,0.754152,"et al. (2016) evaluate claims using a credibility graph derived from evidence found in web pages. All these works target truth. In order to capture relevance, we base PageRank on the reuse of argument units instead. 3 In particular, we construct a graph from all arguments found in web pages. Both complex argument models from theory (Toulmin, 1958; Reisert et al., 2015) and simple proprietary models (Levy et al., 2014) have been studied for web text. Some include quality-related concepts, such as evidence types (Al-Khatib et al., 2016b). Others represent the overall structure of argumentation (Wachsmuth et al., 2015). Like Mochales and Moens (2011), we consider only premises and conclusions as units of arguments here. This is the common ground of nearly all argument-level models, and it will allow an integration with approaches to analyze argument inference (Feng and Hirst, 2011) based on the argumentation schemes of Walton et al. (2008). Edges in our graph emerge from the usage of units in different arguments. Alternatively, it would be possible to mine support and attack relations between arguments (Park and Cardie, 2014; Peldszus and Stede, 2015). Not all mining steps work robustly on web text yet (Al-"
E17-1105,E17-1017,1,0.917789,"lly help accepting or rejecting its conclusion. Such relevance is one prerequisite of a cogent argument, along with the accepability of the premises and their sufficiency for drawing the conclusion (Johnson and Blair, 2006). Here, we are interested in an argument’s global relevance, which refers to the benefit of the argument in a discussion (Walton, 2006): An argument is more globally relevant the more it contributes to resolving an issue (van Eemeren, 2015). While Blair (2012) deems both types as vague and resisting analysis so far, we assess global relevance using objective statistics. In (Wachsmuth et al., 2017), we comprehensively survey theories on argumentation quality as well as computational approaches to specific quality dimensions. Among the latter, Persing and Ng (2015) rely on manual annotations of essays to predict how strong an essay’s argument is—a naturally subjective and non-scalable assessment. For scalability, Habernal and Gurevych (2016) learn on crowdsourced labels, which of two arguments is more convincing. Similar to us, they construct a graph to rank arguments, but since their graph is based on the labels, the subjectivity remains. This also holds for (Braunstain et al., 2016) wh"
E17-1105,walker-etal-2012-corpus,0,0.0490101,"ch as link farms (Croft et al., 2009). Some of them will affect argument graphs, too. Improvements of the original algorithm should therefore be taken into account in future work. In this paper, we omit them on purpose for simplicity and clarity. flat relations between units (Aharoni et al., 2014). To maximize size and heterogeneity, we here refer to the Argument Web (Bex et al., 2013), which is to our knowledge the largest ground-truth argument database available so far. It includes relationrich corpora, e.g., AraucariaDB (Reed and Rowe, 2004), as well as much annotated web text, e.g., from (Walker et al., 2012) and (Wacholder et al., 2014). Thus, it serves as a suitable basis for constructing an argument graph. We already introduced our PageRank approach in (Al-Khatib et al., 2016a), but we only roughly sketched its general idea there. Recursive analyses have also been proposed for fact finding, assuming that trustworthy web pages contain many true facts, and that true facts will be found on many trustworthy web pages (Yin et al., 2007; Galland et al., 2010). Pasternack and Roth (2010) model a user’s prior knowledge in addition. Close to argumentation, Samadi et al. (2016) evaluate claims using a cr"
E17-3004,P14-5016,0,\N,Missing
E17-3004,C16-1324,1,\N,Missing
I11-1071,J96-2004,0,0.132185,"Missing"
I11-1071,P09-1080,0,0.0218346,"rete classes of text is meant to be an interpretation for a concrete learning situation rather than a redefinition of the task. In the remaining sections, we 633 use the classes personal, commercial, and informational for a first evaluation of LFA. Our intuition is that, statistically, language functions imply shallow linguistic features, such as certain partsof-speech or writing style characteristics. 3 (1994) try to automatically separate informative from imaginative texts. Stamatatos et. al. (2000) rely on more concrete press-related genres (e.g. “Letter to the editor” or “Spot news”), and Garera and Yarowsky (2009) investigate modern conversational genres, such as “Email”. Related Work The two latter show that genres do not only describe the function and purpose of a text, but also its form and target audience and, thus, also represent concepts orthogonal to language functions. Correspondingly, a great deal of genre research in the last decade focused on web genres as surveyed in (Stein et al., 2010). Two standard corpora for web genre identification, KI-04 (Meyer zu Eissen and Stein, 2004) and SANTINIS (Santini, 2010), illustrate a common situation in genre research: Their classification schemes partly"
I11-1071,P10-1114,0,0.0434532,"follow Webber (2009) who emphasizes the importance of text-internal and linguistic features, such as particular parts-of-speech. Also, we investigate both character-based and wordbased n-grams, which were most successful in the above-mentioned evaluation of genre collections of Sharoff et al. (2010). Further promising features relate to sentiment analysis and authorship attribution. Like LFA, sentiment analysis covers the issue of subjectivity (Pang and Lee, 2008), but it addresses what is said. Correspondingly, research in sentiment analysis often focuses only on characteristic terms as in (Prettenhofer and Stein, 2010). In contrast, approaches to authorship attribution aim at measuring the writing style of a text; sometimes based on 1 While literature theory also addresses the intention of the reader and the intention of the text itself (Eco, 1990), only authorial intention is relevant for the purpose of this paper. 634 lexical and shallow linguistic information (Luyckx and Daelemans, 2008), sometimes using deeper analyses like parsing (Raghavan et al., 2010). We adopt some of these features in Section 5 and 6. 4 personal. ... How did Alex recently ask when he saw Kravitz’ latest best-of collection: Is it h"
I11-1071,P10-2008,0,0.0176605,"nd Lee, 2008), but it addresses what is said. Correspondingly, research in sentiment analysis often focuses only on characteristic terms as in (Prettenhofer and Stein, 2010). In contrast, approaches to authorship attribution aim at measuring the writing style of a text; sometimes based on 1 While literature theory also addresses the intention of the reader and the intention of the text itself (Eco, 1990), only authorial intention is relevant for the purpose of this paper. 634 lexical and shallow linguistic information (Luyckx and Daelemans, 2008), sometimes using deeper analyses like parsing (Raghavan et al., 2010). We adopt some of these features in Section 5 and 6. 4 personal. ... How did Alex recently ask when he saw Kravitz’ latest best-of collection: Is it his own liking, the voting on his website, or the chart position what counts? Good question. However, in our case, there is nothing to argue about: 27 songs, all were number one. The Beatles. The LFA-11 Corpus Biggest band on the globe. ... commercial. ... The sitars sound authentically Indian. In To evaluate LFA, we built the LFA-11 corpus with manually annotated German texts from two separated domains: music and smartphones. The purpose of the"
I11-1071,C94-2174,0,0.221046,"Missing"
I11-1071,sharoff-etal-2010-web,0,0.0249372,"Missing"
I11-1071,P97-1005,0,0.233191,"Missing"
I11-1071,C00-2117,0,0.0828604,"Missing"
I11-1071,C08-1065,0,0.0270856,"tion. Like LFA, sentiment analysis covers the issue of subjectivity (Pang and Lee, 2008), but it addresses what is said. Correspondingly, research in sentiment analysis often focuses only on characteristic terms as in (Prettenhofer and Stein, 2010). In contrast, approaches to authorship attribution aim at measuring the writing style of a text; sometimes based on 1 While literature theory also addresses the intention of the reader and the intention of the text itself (Eco, 1990), only authorial intention is relevant for the purpose of this paper. 634 lexical and shallow linguistic information (Luyckx and Daelemans, 2008), sometimes using deeper analyses like parsing (Raghavan et al., 2010). We adopt some of these features in Section 5 and 6. 4 personal. ... How did Alex recently ask when he saw Kravitz’ latest best-of collection: Is it his own liking, the voting on his website, or the chart position what counts? Good question. However, in our case, there is nothing to argue about: 27 songs, all were number one. The Beatles. The LFA-11 Corpus Biggest band on the globe. ... commercial. ... The sitars sound authentically Indian. In To evaluate LFA, we built the LFA-11 corpus with manually annotated German texts"
I11-1071,P09-1076,0,0.0151176,"onal trait as well as to some formal properties, several abstract and concrete classification schemes have been proposed for genre identification. In a pioneer study on genres, Biber (1986) analyzes basic textual dimensions, such as “informative vs. involved”, while Karlgren and Cutting However, language functions still represent one important aspect of genres. Accordingly, genre identification and LFA have similarities with respect to both practical applications (e.g. document filtering) and potentially helpful features. Since our focus is on a text itself as opposed to a document, we follow Webber (2009) who emphasizes the importance of text-internal and linguistic features, such as particular parts-of-speech. Also, we investigate both character-based and wordbased n-grams, which were most successful in the above-mentioned evaluation of genre collections of Sharoff et al. (2010). Further promising features relate to sentiment analysis and authorship attribution. Like LFA, sentiment analysis covers the issue of subjectivity (Pang and Lee, 2008), but it addresses what is said. Correspondingly, research in sentiment analysis often focuses only on characteristic terms as in (Prettenhofer and Stei"
I13-1061,C12-1004,0,0.035075,"Missing"
I13-1061,N10-1004,0,0.0229215,"ffective, but they are not suitable when efficiency is important. van Noord (2009) trades parsing efficiency for parsing effectiveness by learning a heuristic filtering of useful parses. In contrast, we develop a selfsupervised online learning algorithm to achieve efficient extraction without reducing effectiveness. While our approach works with every predefined relation and event type, arbitrary binary relations are found in self-supervised open information extraction (Fader et al., 2011). Self-supervised learning aims to fully overcome manual text labeling, mostly for learning language like McClosky et al. (2010). To our knowledge, we are the first to apply it for predicting extraction efficiency. bution of relevant information. We argue that such kinds of uncertainty and lack of a-priori knowledge cannot be tackled offline, but they require to learn and to adapt to the characteristics of input texts to avoid a noticeable efficiency loss. 1.1 Contributions and Outline In this paper, we analyze to what extent the heterogeneity of natural language texts in the distribution of relevant information affects the efficiency of an information extraction pipeline. For a high heterogeneity, we propose an adapta"
I13-1061,W06-1651,0,0.0169375,"013. sists in relating a number of entities to events of predefined types (Cunningham, 2006). Recent research, e.g. (Jean-Louis et al., 2011), and major evaluation tracks, e.g. (Kim et al., 2011), show the ongoing importance of template filling. We consider extraction pipelines that perform filtering, which have a long tradition (Cowie and Lehnert, 1996). Sarawagi (2008) sees the efficient filtering of relevant portions of input texts as a main challenge. In the pipelines we focus on, each algorithm takes on one analysis (Grishman, 1997). Other approaches such as joint information extraction (Choi et al., 2006) can be effective, but they are not suitable when efficiency is important. van Noord (2009) trades parsing efficiency for parsing effectiveness by learning a heuristic filtering of useful parses. In contrast, we develop a selfsupervised online learning algorithm to achieve efficient extraction without reducing effectiveness. While our approach works with every predefined relation and event type, arbitrary binary relations are found in self-supervised open information extraction (Fader et al., 2011). Self-supervised learning aims to fully overcome manual text labeling, mostly for learning langu"
I13-1061,E09-1093,0,0.235869,"Missing"
I13-1061,D11-1142,0,0.012657,"hm takes on one analysis (Grishman, 1997). Other approaches such as joint information extraction (Choi et al., 2006) can be effective, but they are not suitable when efficiency is important. van Noord (2009) trades parsing efficiency for parsing effectiveness by learning a heuristic filtering of useful parses. In contrast, we develop a selfsupervised online learning algorithm to achieve efficient extraction without reducing effectiveness. While our approach works with every predefined relation and event type, arbitrary binary relations are found in self-supervised open information extraction (Fader et al., 2011). Self-supervised learning aims to fully overcome manual text labeling, mostly for learning language like McClosky et al. (2010). To our knowledge, we are the first to apply it for predicting extraction efficiency. bution of relevant information. We argue that such kinds of uncertainty and lack of a-priori knowledge cannot be tackled offline, but they require to learn and to adapt to the characteristics of input texts to avoid a noticeable efficiency loss. 1.1 Contributions and Outline In this paper, we analyze to what extent the heterogeneity of natural language texts in the distribution of r"
I13-1061,P05-1045,0,0.0634701,"Missing"
I13-1061,C10-1127,1,0.835833,"cally valid. Thereby, we limit the online adaptation algorithm to a certain degree in learning linguistic features from the texts, but we gain that we can measure the benefit of online adaptation as a function of the averaged deviation. i=1 We compute exact values σ(C|D) in Section 5 to measure the impact of heterogeneity. In general, the averaged deviation can also be estimated on a sample of texts. For illustration, Table 1 lists the deviations for the three most common named entity types in the German part of the CoNLL’03 corpus (Tjong Kim Sang and De Meulder, 2003), in the Revenue corpus (Wachsmuth et al., 2010), in a sample of the German Wikipedia (the first 10,000 articles according to internal page ID), and in the LFA-11 smartphone corpus, which is a web crawl of blog posts (Wachsmuth and Bujna, 2011). Here, we recognized entities using Stanford NER (Finkel et al., 2005; Faruqui and Pad´o, 2010). Different from other sampling-based efficiency estimations, cf. (Wang et al., 2011), the averaged deviation does not measure the typical characteristics of input texts, but it quantifies how much these characteristics vary. By that, it helps pipeline designers to decide whether an online adaptation of pip"
I13-1061,I11-1071,1,0.845828,"on as a function of the averaged deviation. i=1 We compute exact values σ(C|D) in Section 5 to measure the impact of heterogeneity. In general, the averaged deviation can also be estimated on a sample of texts. For illustration, Table 1 lists the deviations for the three most common named entity types in the German part of the CoNLL’03 corpus (Tjong Kim Sang and De Meulder, 2003), in the Revenue corpus (Wachsmuth et al., 2010), in a sample of the German Wikipedia (the first 10,000 articles according to internal page ID), and in the LFA-11 smartphone corpus, which is a web crawl of blog posts (Wachsmuth and Bujna, 2011). Here, we recognized entities using Stanford NER (Finkel et al., 2005; Faruqui and Pad´o, 2010). Different from other sampling-based efficiency estimations, cf. (Wang et al., 2011), the averaged deviation does not measure the typical characteristics of input texts, but it quantifies how much these characteristics vary. By that, it helps pipeline designers to decide whether an online adaptation of pipeline schedules is needed to ensure efficient ex5 Evaluation We now present controlled experiments with the online adaptation algorithm on text corpora of different heterogeneity. The goal is to s"
I13-1061,I11-1081,0,0.0641743,"Missing"
I13-1061,C12-2125,1,0.925541,"traction can always be approached as a filtering task as discussed in detail in (Wachsmuth et al., 2013b): By filling a template slot, each algorithm in A implicitly classifies certain units of an input text as relevant. Only these units need to be filtered for the next algorithm in π. As a result, a smart schedule π will often significantly improve the overall extraction efficiency. If the input requirements of all algorithms in A are met within π, the effectiveness of Π (in terms of both precision and recall) will be maintained, since the output of Π exactly lies in the filtered text units (Wachsmuth and Stein, 2012).1 When given a big data filtering task, the designer of a pipeline faces two challenges: (1) How to determine the most efficient schedule for a set of extraction algorithms and a collection or a stream of texts? (2) How to maintain efficiency under heterogeneous text characteristics? With regard to the former challenge we resort to existing research (cf. Section 2). The latter becomes an issue where input texts are not fully known or come from different sources as in the web. Moreover, streams of texts can undergo substantial changes in the distriFrom an efficiency viewpoint, information extr"
I13-1061,W11-1801,0,0.0347816,"between two entity types, such as &lt;ORG> was founded in &lt;TIME>. E.g., the sentence “Google was established by two Stanford students.” needs not to be filtered for relation extraction, as it contains no time entity. The schedule of the two implied entity recognition steps will affect the extraction efficiency. 534 International Joint Conference on Natural Language Processing, pages 534–542, Nagoya, Japan, 14-18 October 2013. sists in relating a number of entities to events of predefined types (Cunningham, 2006). Recent research, e.g. (Jean-Louis et al., 2011), and major evaluation tracks, e.g. (Kim et al., 2011), show the ongoing importance of template filling. We consider extraction pipelines that perform filtering, which have a long tradition (Cowie and Lehnert, 1996). Sarawagi (2008) sees the efficient filtering of relevant portions of input texts as a main challenge. In the pipelines we focus on, each algorithm takes on one analysis (Grishman, 1997). Other approaches such as joint information extraction (Choi et al., 2006) can be effective, but they are not suitable when efficiency is important. van Noord (2009) trades parsing efficiency for parsing effectiveness by learning a heuristic filtering"
I13-1061,W03-0419,0,\N,Missing
K18-1044,D17-1141,1,0.855103,"shape the opinion of the masses. Similarly, they can increase or decrease the gap between readers with opposing beliefs (van Dijk, 1995). As such, news editorials represent an important resource for research on argument mining (Mochales and Moens, 2011) and debating technologies (Rinott et al., 2015). On the other hand, a single news editorial rarely changes the stance of a reader completely. Moreover, many editorials do not put an explicit focus on arguments. Rather, they follow a subtle rhetorical strategy combining emotional anecdotes with hidden claims and ethotic evidence, among others (Al-Khatib et al., 2017). So, if not persuasive arguments, what makes a news editorial effective or ineffective then? In other words: How can we measure its argumentation quality? In this paper, we introduce a new corpus with 1000 news editorials from the New York Times where we consider argumentation quality from a dialectical perspective (van Eemeren and Grootendorst, 2004). While several quality dimensions are known in theory (Wachsmuth et al., 2017b), existing approaches rely on subjective assessments of absolute (Persing and Ng, 2015) or relative (Habernal and Gurevych, 2016) persuasiveness. In contrast, our cor"
K18-1044,C16-1324,1,0.876787,"Missing"
K18-1044,W09-3723,0,0.519708,"tation related to politics often comprises more sophisticated mechanisms, bringing together logical arguments (Johnson and Blair, 2006) with rhetorical means (Aristotle, translated 2007) and dialectic (van Eemeren and Grootendorst, 2004). A typical genre of such kind is news editorials. As outlined in Section 1, news editorials are opinionated articles that aim to persuade their readers of a stance towards some controversial issue, usually with implicit, hidden strategies (van Dijk, 1995). Editorials have been used for opinion mining and retrieval in some works (Yu and Hatzivassiloglou, 2003; Bal, 2009), partly towards analyzing argumentation (Bal and Dizier, 2010; Kiesel et al., 2015). To our knowledge, the only corpus of noteworthy size that exists for studying editorial argumentation explicitly is the one of Al-Khatib et al. (2016) who segmented 300 editorials into argumentative discourse units of different claim and evidence types. Al-Khatib et al. (2017) trained classifiers on their corpus and applied them to 28,986 editorials from the New York Times Annotated Corpus (Sandhaus, 2008). They found topic-specific evidence type patterns, which appear to be related to persuasive strategies."
K18-1044,bal-saint-dizier-2010-towards,0,0.463693,"isticated mechanisms, bringing together logical arguments (Johnson and Blair, 2006) with rhetorical means (Aristotle, translated 2007) and dialectic (van Eemeren and Grootendorst, 2004). A typical genre of such kind is news editorials. As outlined in Section 1, news editorials are opinionated articles that aim to persuade their readers of a stance towards some controversial issue, usually with implicit, hidden strategies (van Dijk, 1995). Editorials have been used for opinion mining and retrieval in some works (Yu and Hatzivassiloglou, 2003; Bal, 2009), partly towards analyzing argumentation (Bal and Dizier, 2010; Kiesel et al., 2015). To our knowledge, the only corpus of noteworthy size that exists for studying editorial argumentation explicitly is the one of Al-Khatib et al. (2016) who segmented 300 editorials into argumentative discourse units of different claim and evidence types. Al-Khatib et al. (2017) trained classifiers on their corpus and applied them to 28,986 editorials from the New York Times Annotated Corpus (Sandhaus, 2008). They found topic-specific evidence type patterns, which appear to be related to persuasive strategies. However, editorial-level annotations are missing that actually"
K18-1044,P12-2041,0,0.0331381,"hsmuth et al. (2017b). The authors developed a taxonomy with one main aspect each for logical (cogency), rhetorical (effectiveness), and dialectical quality (reasonableness), as well as several concrete quality dimensions. Effectiveness reflects to what extent an author persuades a reader, and reasonableness reflects an argument’s contribution to agreement. As detailed in Section 3, the dimension we propose is meant to measure persuasive effectiveness, yet, from a dialectical perspective, which is more suitable for editorials. We hypothesize it to be related to the acceptability of arguments (Cabrio and Villata, 2012) and the helpfulness of argumentation (Liu et al., 2017). While Louis and Nenkova (2013) study the general quality of news articles, our goal is to provide a basis for studying their argumentation quality more objectively. Some existing computational approaches to assessing argumentation quality rely on human persuasiveness ratings of essays (Persing and Ng, 2015; Wachsmuth et al., 2016) or debate portal arguments (Persing and Ng, 2017). The problem here is that persuasiveness is subjective by heart, underlined by the low inter-annotator agreement for effectiveness in the corpus of Wachsmuth e"
K18-1044,N16-1166,0,0.0615486,"Missing"
K18-1044,P16-1150,0,0.429909,"laims and ethotic evidence, among others (Al-Khatib et al., 2017). So, if not persuasive arguments, what makes a news editorial effective or ineffective then? In other words: How can we measure its argumentation quality? In this paper, we introduce a new corpus with 1000 news editorials from the New York Times where we consider argumentation quality from a dialectical perspective (van Eemeren and Grootendorst, 2004). While several quality dimensions are known in theory (Wachsmuth et al., 2017b), existing approaches rely on subjective assessments of absolute (Persing and Ng, 2015) or relative (Habernal and Gurevych, 2016) persuasiveness. In contrast, our corpus captures quality in terms of whether an editorial brings readers of opposing belief closer together or rather increases the gap between them. We argue that, thereby, we better account for the practically achieved persuasive effect, resulting in a qualitative media measurement analysis of editorials that intrigue our thoughts. Persuasion, according to Halmari and Virtanen (2005), is an umbrella term for linguistic choices that aim at changing or affecting the behavior of others or at strengthening the existing beliefs of those who already agree, includin"
K18-1044,W17-5102,0,0.108097,"s into argumentative discourse units of different claim and evidence types. Al-Khatib et al. (2017) trained classifiers on their corpus and applied them to 28,986 editorials from the New York Times Annotated Corpus (Sandhaus, 2008). They found topic-specific evidence type patterns, which appear to be related to persuasive strategies. However, editorial-level annotations are missing that actually connect the patterns to persuasiveness. For blog posts and forum discussions respectively, previous work has annotated persuasive acts (Anand et al., 2011) and the use of Aristotle’s rhetorical means (Hidey et al., 2017). Still, this would not allow distinguishing effective from 455 ineffective strategies. We fill this gap by presenting the first editorial corpus with persuasion-related annotations of argumentation quality. To obtain a larger corpus size, we rely on the editorials from Sandhaus (2008) rather than those from Al-Khatib et al. (2016). Potash et al. (2017) observe bias in existing corpora towards higher quality for longer arguments. To prevent such bias, we consider only editorials from a narrow length range. Research on argumentation quality has recently been surveyed by Wachsmuth et al. (2017b)"
K18-1044,W15-0505,1,0.875024,"bringing together logical arguments (Johnson and Blair, 2006) with rhetorical means (Aristotle, translated 2007) and dialectic (van Eemeren and Grootendorst, 2004). A typical genre of such kind is news editorials. As outlined in Section 1, news editorials are opinionated articles that aim to persuade their readers of a stance towards some controversial issue, usually with implicit, hidden strategies (van Dijk, 1995). Editorials have been used for opinion mining and retrieval in some works (Yu and Hatzivassiloglou, 2003; Bal, 2009), partly towards analyzing argumentation (Bal and Dizier, 2010; Kiesel et al., 2015). To our knowledge, the only corpus of noteworthy size that exists for studying editorial argumentation explicitly is the one of Al-Khatib et al. (2016) who segmented 300 editorials into argumentative discourse units of different claim and evidence types. Al-Khatib et al. (2017) trained classifiers on their corpus and applied them to 28,986 editorials from the New York Times Annotated Corpus (Sandhaus, 2008). They found topic-specific evidence type patterns, which appear to be related to persuasive strategies. However, editorial-level annotations are missing that actually connect the patterns"
K18-1044,C14-1141,0,0.0188643,"three liberals and three conservatives. The annotators also reported free-text reasons for the effects they observed. 2 Webis-Editorial-Quality-18 corpus, available at http: //www.webis.de/data 2 Related Work Computational argumentation has lately become popular in the natural language processing community. So far, most computational argumentation research deals with the mining of arguments from text (Mochales and Moens, 2011). Accordingly, many studied corpora capture argument structure, often for a specific text genre, such as persuasive essays (Stab and Gurevych, 2014), Wikipedia articles (Levy et al., 2014), or even pure arguments (Peldszus and Stede, 2015). These genres share that they make claims and reasons explicit, i.e., they argue rationally. In contrast, real-world argumentation related to politics often comprises more sophisticated mechanisms, bringing together logical arguments (Johnson and Blair, 2006) with rhetorical means (Aristotle, translated 2007) and dialectic (van Eemeren and Grootendorst, 2004). A typical genre of such kind is news editorials. As outlined in Section 1, news editorials are opinionated articles that aim to persuade their readers of a stance towards some controver"
K18-1044,D17-1142,0,0.052097,"Missing"
K18-1044,Q13-1028,0,0.0300506,"ical (cogency), rhetorical (effectiveness), and dialectical quality (reasonableness), as well as several concrete quality dimensions. Effectiveness reflects to what extent an author persuades a reader, and reasonableness reflects an argument’s contribution to agreement. As detailed in Section 3, the dimension we propose is meant to measure persuasive effectiveness, yet, from a dialectical perspective, which is more suitable for editorials. We hypothesize it to be related to the acceptability of arguments (Cabrio and Villata, 2012) and the helpfulness of argumentation (Liu et al., 2017). While Louis and Nenkova (2013) study the general quality of news articles, our goal is to provide a basis for studying their argumentation quality more objectively. Some existing computational approaches to assessing argumentation quality rely on human persuasiveness ratings of essays (Persing and Ng, 2015; Wachsmuth et al., 2016) or debate portal arguments (Persing and Ng, 2017). The problem here is that persuasiveness is subjective by heart, underlined by the low inter-annotator agreement for effectiveness in the corpus of Wachsmuth et al. (2017b): effectiveness depends on the prior stance of the annotator. Habernal and"
K18-1044,E17-1070,0,0.211526,"not common for political argumentation. Instead, we tackle subjectiveness by letting people with both stances on a discussed issue annotate quality. Cano-Basave and He (2016) point out that persuasive argumentation is about both changing and reinforcing stance — a view that we follow. The authors study the impact of persuasive language of political debates based on poll changes. Such a direct effect on different audiences is not accessible for most argumentative texts, including editorials. Persuasiveness does not only depend on a text itself, but also on the reader’s beliefs and personality. Lukin et al. (2017) find that different types of arguments (rational vs. emotional) are effective depending on the “Big Five” personality traits (Goldberg, 1990). We captured our annotators’ personality traits, too. However, we primarily focus on nine political profiles from left to right (Doherty et al., 2017) in order to represent prior stance. We are not aware of any previous work in computational argumentation considering such profiles so far. 3 A New Model of Argumentation Quality for News Editorials We propose a model that quantifies the argumentation quality of an editorial at the discourse level. Two dim"
K18-1044,D15-1110,0,0.0236464,"annotators also reported free-text reasons for the effects they observed. 2 Webis-Editorial-Quality-18 corpus, available at http: //www.webis.de/data 2 Related Work Computational argumentation has lately become popular in the natural language processing community. So far, most computational argumentation research deals with the mining of arguments from text (Mochales and Moens, 2011). Accordingly, many studied corpora capture argument structure, often for a specific text genre, such as persuasive essays (Stab and Gurevych, 2014), Wikipedia articles (Levy et al., 2014), or even pure arguments (Peldszus and Stede, 2015). These genres share that they make claims and reasons explicit, i.e., they argue rationally. In contrast, real-world argumentation related to politics often comprises more sophisticated mechanisms, bringing together logical arguments (Johnson and Blair, 2006) with rhetorical means (Aristotle, translated 2007) and dialectic (van Eemeren and Grootendorst, 2004). A typical genre of such kind is news editorials. As outlined in Section 1, news editorials are opinionated articles that aim to persuade their readers of a stance towards some controversial issue, usually with implicit, hidden strategie"
K18-1044,P15-1053,0,0.519424,"g emotional anecdotes with hidden claims and ethotic evidence, among others (Al-Khatib et al., 2017). So, if not persuasive arguments, what makes a news editorial effective or ineffective then? In other words: How can we measure its argumentation quality? In this paper, we introduce a new corpus with 1000 news editorials from the New York Times where we consider argumentation quality from a dialectical perspective (van Eemeren and Grootendorst, 2004). While several quality dimensions are known in theory (Wachsmuth et al., 2017b), existing approaches rely on subjective assessments of absolute (Persing and Ng, 2015) or relative (Habernal and Gurevych, 2016) persuasiveness. In contrast, our corpus captures quality in terms of whether an editorial brings readers of opposing belief closer together or rather increases the gap between them. We argue that, thereby, we better account for the practically achieved persuasive effect, resulting in a qualitative media measurement analysis of editorials that intrigue our thoughts. Persuasion, according to Halmari and Virtanen (2005), is an umbrella term for linguistic choices that aim at changing or affecting the behavior of others or at strengthening the existing be"
K18-1044,I17-1060,0,0.189454,"fectiveness, yet, from a dialectical perspective, which is more suitable for editorials. We hypothesize it to be related to the acceptability of arguments (Cabrio and Villata, 2012) and the helpfulness of argumentation (Liu et al., 2017). While Louis and Nenkova (2013) study the general quality of news articles, our goal is to provide a basis for studying their argumentation quality more objectively. Some existing computational approaches to assessing argumentation quality rely on human persuasiveness ratings of essays (Persing and Ng, 2015; Wachsmuth et al., 2016) or debate portal arguments (Persing and Ng, 2017). The problem here is that persuasiveness is subjective by heart, underlined by the low inter-annotator agreement for effectiveness in the corpus of Wachsmuth et al. (2017b): effectiveness depends on the prior stance of the annotator. Habernal and Gurevych (2016) compare the convincingness of arguments with only one stance on a given issue, which circumvents the problem, but does not help for actual persuasion. While Tan et al. (2016) analyze how people are persuaded by others with opposing stance, they restrict their view to good-faith discussions (where people are open to be persuaded) — a s"
K18-1044,I17-1035,0,0.165057,"-level annotations are missing that actually connect the patterns to persuasiveness. For blog posts and forum discussions respectively, previous work has annotated persuasive acts (Anand et al., 2011) and the use of Aristotle’s rhetorical means (Hidey et al., 2017). Still, this would not allow distinguishing effective from 455 ineffective strategies. We fill this gap by presenting the first editorial corpus with persuasion-related annotations of argumentation quality. To obtain a larger corpus size, we rely on the editorials from Sandhaus (2008) rather than those from Al-Khatib et al. (2016). Potash et al. (2017) observe bias in existing corpora towards higher quality for longer arguments. To prevent such bias, we consider only editorials from a narrow length range. Research on argumentation quality has recently been surveyed by Wachsmuth et al. (2017b). The authors developed a taxonomy with one main aspect each for logical (cogency), rhetorical (effectiveness), and dialectical quality (reasonableness), as well as several concrete quality dimensions. Effectiveness reflects to what extent an author persuades a reader, and reasonableness reflects an argument’s contribution to agreement. As detailed in S"
K18-1044,D15-1050,0,0.0195949,"article that argues in favor of a particular stance on a usually timely controversial issue, such as the relocation of the US embassy in Israel to Jerusalem. Usually, it reflects the political ideology of the newspaper, aiming to persuade readers of the respective stance. Such editorials are said to have the power to shape the opinion of the masses. Similarly, they can increase or decrease the gap between readers with opposing beliefs (van Dijk, 1995). As such, news editorials represent an important resource for research on argument mining (Mochales and Moens, 2011) and debating technologies (Rinott et al., 2015). On the other hand, a single news editorial rarely changes the stance of a reader completely. Moreover, many editorials do not put an explicit focus on arguments. Rather, they follow a subtle rhetorical strategy combining emotional anecdotes with hidden claims and ethotic evidence, among others (Al-Khatib et al., 2017). So, if not persuasive arguments, what makes a news editorial effective or ineffective then? In other words: How can we measure its argumentation quality? In this paper, we introduce a new corpus with 1000 news editorials from the New York Times where we consider argumentation"
K18-1044,C14-1142,0,0.0228244,"ts of 1000 news editorials, each annotated by three liberals and three conservatives. The annotators also reported free-text reasons for the effects they observed. 2 Webis-Editorial-Quality-18 corpus, available at http: //www.webis.de/data 2 Related Work Computational argumentation has lately become popular in the natural language processing community. So far, most computational argumentation research deals with the mining of arguments from text (Mochales and Moens, 2011). Accordingly, many studied corpora capture argument structure, often for a specific text genre, such as persuasive essays (Stab and Gurevych, 2014), Wikipedia articles (Levy et al., 2014), or even pure arguments (Peldszus and Stede, 2015). These genres share that they make claims and reasons explicit, i.e., they argue rationally. In contrast, real-world argumentation related to politics often comprises more sophisticated mechanisms, bringing together logical arguments (Johnson and Blair, 2006) with rhetorical means (Aristotle, translated 2007) and dialectic (van Eemeren and Grootendorst, 2004). A typical genre of such kind is news editorials. As outlined in Section 1, news editorials are opinionated articles that aim to persuade their re"
K18-1044,C16-1158,1,0.807989,"mension we propose is meant to measure persuasive effectiveness, yet, from a dialectical perspective, which is more suitable for editorials. We hypothesize it to be related to the acceptability of arguments (Cabrio and Villata, 2012) and the helpfulness of argumentation (Liu et al., 2017). While Louis and Nenkova (2013) study the general quality of news articles, our goal is to provide a basis for studying their argumentation quality more objectively. Some existing computational approaches to assessing argumentation quality rely on human persuasiveness ratings of essays (Persing and Ng, 2015; Wachsmuth et al., 2016) or debate portal arguments (Persing and Ng, 2017). The problem here is that persuasiveness is subjective by heart, underlined by the low inter-annotator agreement for effectiveness in the corpus of Wachsmuth et al. (2017b): effectiveness depends on the prior stance of the annotator. Habernal and Gurevych (2016) compare the convincingness of arguments with only one stance on a given issue, which circumvents the problem, but does not help for actual persuasion. While Tan et al. (2016) analyze how people are persuaded by others with opposing stance, they restrict their view to good-faith discuss"
K18-1044,P17-2039,1,0.933183,"an explicit focus on arguments. Rather, they follow a subtle rhetorical strategy combining emotional anecdotes with hidden claims and ethotic evidence, among others (Al-Khatib et al., 2017). So, if not persuasive arguments, what makes a news editorial effective or ineffective then? In other words: How can we measure its argumentation quality? In this paper, we introduce a new corpus with 1000 news editorials from the New York Times where we consider argumentation quality from a dialectical perspective (van Eemeren and Grootendorst, 2004). While several quality dimensions are known in theory (Wachsmuth et al., 2017b), existing approaches rely on subjective assessments of absolute (Persing and Ng, 2015) or relative (Habernal and Gurevych, 2016) persuasiveness. In contrast, our corpus captures quality in terms of whether an editorial brings readers of opposing belief closer together or rather increases the gap between them. We argue that, thereby, we better account for the practically achieved persuasive effect, resulting in a qualitative media measurement analysis of editorials that intrigue our thoughts. Persuasion, according to Halmari and Virtanen (2005), is an umbrella term for linguistic choices tha"
K18-1044,E17-1017,1,0.885902,"an explicit focus on arguments. Rather, they follow a subtle rhetorical strategy combining emotional anecdotes with hidden claims and ethotic evidence, among others (Al-Khatib et al., 2017). So, if not persuasive arguments, what makes a news editorial effective or ineffective then? In other words: How can we measure its argumentation quality? In this paper, we introduce a new corpus with 1000 news editorials from the New York Times where we consider argumentation quality from a dialectical perspective (van Eemeren and Grootendorst, 2004). While several quality dimensions are known in theory (Wachsmuth et al., 2017b), existing approaches rely on subjective assessments of absolute (Persing and Ng, 2015) or relative (Habernal and Gurevych, 2016) persuasiveness. In contrast, our corpus captures quality in terms of whether an editorial brings readers of opposing belief closer together or rather increases the gap between them. We argue that, thereby, we better account for the practically achieved persuasive effect, resulting in a qualitative media measurement analysis of editorials that intrigue our thoughts. Persuasion, according to Halmari and Virtanen (2005), is an umbrella term for linguistic choices tha"
K18-1044,W03-1017,0,0.614233,"In contrast, real-world argumentation related to politics often comprises more sophisticated mechanisms, bringing together logical arguments (Johnson and Blair, 2006) with rhetorical means (Aristotle, translated 2007) and dialectic (van Eemeren and Grootendorst, 2004). A typical genre of such kind is news editorials. As outlined in Section 1, news editorials are opinionated articles that aim to persuade their readers of a stance towards some controversial issue, usually with implicit, hidden strategies (van Dijk, 1995). Editorials have been used for opinion mining and retrieval in some works (Yu and Hatzivassiloglou, 2003; Bal, 2009), partly towards analyzing argumentation (Bal and Dizier, 2010; Kiesel et al., 2015). To our knowledge, the only corpus of noteworthy size that exists for studying editorial argumentation explicitly is the one of Al-Khatib et al. (2016) who segmented 300 editorials into argumentative discourse units of different claim and evidence types. Al-Khatib et al. (2017) trained classifiers on their corpus and applied them to 28,986 editorials from the New York Times Annotated Corpus (Sandhaus, 2008). They found topic-specific evidence type patterns, which appear to be related to persuasive"
N16-1165,E12-1049,0,\N,Missing
N16-1165,D13-1191,0,\N,Missing
N16-1165,E12-1062,0,\N,Missing
N16-1165,P12-2041,0,\N,Missing
N16-1165,P11-1055,0,\N,Missing
N16-1165,D15-1255,0,\N,Missing
N16-1165,W14-2109,0,\N,Missing
N16-1165,C14-1141,0,\N,Missing
N16-1165,reschke-etal-2014-event,0,\N,Missing
N16-1165,C14-1142,0,\N,Missing
N16-1165,D15-1072,1,\N,Missing
N16-1165,W14-2112,0,\N,Missing
N18-1036,jain-etal-2014-corpus,0,\N,Missing
N18-1036,P16-1150,1,\N,Missing
N18-1036,L18-1526,1,\N,Missing
P17-2039,W16-2808,1,0.847172,"cy, but arguments are judged specifically by their reasonableness for achieving agreement (van Eemeren and Grootendorst, 2004). Wachsmuth et al. (2017a) point out that dialectical builds on rhetorical, and rhetorical builds on logical quality. They derive a unifying taxonomy from the major theories, decomposing quality hierarchically into cogency, effectiveness, reasonableness, and subdimensions. Table 1 lists all 15 dimensions 2.2 Practical Views of Quality Assessment There is an application area where absolute quality ratings of argumentative text are common practice: essay scoring (Beigman Klebanov et al., 2016). Persing and Ng (2015) annotated the argumentative strength of essays composing multiple arguments with notable agreement. For single arguments, however, all existing approaches that we are aware of assess quality in relative terms, e.g., Cabrio and Villata (2012) find accepted arguments based on attack relations, Wei et al. (2016) rank arguments by their persuasiveness, and Wachsmuth et al. (2017b) rank them by their relevance. Boudry et al. (2015) argue that normative concepts such as fallacies rarely apply to real-life arguments and that they are too sophisticated for operationalization. B"
P17-2039,P12-2041,0,0.0279224,"ying taxonomy from the major theories, decomposing quality hierarchically into cogency, effectiveness, reasonableness, and subdimensions. Table 1 lists all 15 dimensions 2.2 Practical Views of Quality Assessment There is an application area where absolute quality ratings of argumentative text are common practice: essay scoring (Beigman Klebanov et al., 2016). Persing and Ng (2015) annotated the argumentative strength of essays composing multiple arguments with notable agreement. For single arguments, however, all existing approaches that we are aware of assess quality in relative terms, e.g., Cabrio and Villata (2012) find accepted arguments based on attack relations, Wei et al. (2016) rank arguments by their persuasiveness, and Wachsmuth et al. (2017b) rank them by their relevance. Boudry et al. (2015) argue that normative concepts such as fallacies rarely apply to real-life arguments and that they are too sophisticated for operationalization. Based on the idea that relative assessment is easier, Habernal and Gurevych (2016b) crowdsourced the UKPConvArg1 corpus. Argument pairs (A, B) from a debate portal were classified as to which argument is more convincing. Without giving any guidelines, the authors al"
P17-2039,D16-1129,1,0.101498,"better to give then to receive. It’s better to give other people you’re hand out in help then you holding your own hand.” Argumentation quality is viewed differently in argumentation theory and in practical assessment approaches. This paper studies to what extent the views match empirically. We find that most observations on quality phrased spontaneously are in fact adequately represented by theory. Even more, relative comparisons of arguments in practice correlate with absolute quality ratings based on theory. Our results clarify how the two views can learn from each other. 1 In the study of Habernal and Gurevych (2016b), annotators assessed Argument A as more convincing than B. When giving reasons for their assessment, though, they saw A as more credible and well thought through; that does not seem to be too far from the theoretical notion of cogency. This paper gives empirical answers to the question of how different the theoretical and practical views of argumentation quality actually are. Section 2 briefly reviews existing theories and practical approaches. Section 3 then empirically analyzes correlations in two recent argument corpora, one annotated for 15 well-defined quality dimensions taken from the"
P17-2039,P16-1150,1,0.106561,"better to give then to receive. It’s better to give other people you’re hand out in help then you holding your own hand.” Argumentation quality is viewed differently in argumentation theory and in practical assessment approaches. This paper studies to what extent the views match empirically. We find that most observations on quality phrased spontaneously are in fact adequately represented by theory. Even more, relative comparisons of arguments in practice correlate with absolute quality ratings based on theory. Our results clarify how the two views can learn from each other. 1 In the study of Habernal and Gurevych (2016b), annotators assessed Argument A as more convincing than B. When giving reasons for their assessment, though, they saw A as more credible and well thought through; that does not seem to be too far from the theoretical notion of cogency. This paper gives empirical answers to the question of how different the theoretical and practical views of argumentation quality actually are. Section 2 briefly reviews existing theories and practical approaches. Section 3 then empirically analyzes correlations in two recent argument corpora, one annotated for 15 well-defined quality dimensions taken from the"
P17-2039,N13-1132,0,0.0340534,"y dimensions with scores from 1 to 3 (or choose “cannot judge”). Each argument was rated 10 times at an offered price of $0.10 for each rating (102 annotators in total). Given the crowd ratings, we then performed two comparisons as detailed in the following. 4.2 Agreement of the Crowd with Experts First, we checked to what extent lay annotators and experts agree in terms of Krippendorff’s α. On one hand, we compared the mean of all 10 crowd ratings to the mean of the three ratings of Wachsmuth et al. (2017a). On the other hand, we estimated a reliable rating from the crowd ratings using MACE (Hovy et al., 2013) and compared it to the experts. 253 (a) Crowd / Expert Quality Dimension Cog LA LR LS Eff Cre Emo Cla App Arr Rea GA GR GS OQ Cogency Local acceptability Local relevance Local sufficiency Effectiveness Credibility Emotional appeal Clarity Appropriateness Arrangement Reasonableness Global acceptability Global relevance Global sufficiency Overall quality (b) Crowd 1 / 2 / Expert (c) Crowd 1 / Expert (d) Crowd 2 / Expert Mean MACE Mean MACE Mean MACE Mean MACE .27 .49 .42 .18 .13 .41 .45 .42 .54 .53 .33 .54 .44 –.17 .43 .38 .35 .39 .31 .31 .27 .23 .28 .26 .30 .40 .40 .31 .19 .43 .24 .37 .33 .21"
P17-2039,P15-1053,0,0.080264,"dged specifically by their reasonableness for achieving agreement (van Eemeren and Grootendorst, 2004). Wachsmuth et al. (2017a) point out that dialectical builds on rhetorical, and rhetorical builds on logical quality. They derive a unifying taxonomy from the major theories, decomposing quality hierarchically into cogency, effectiveness, reasonableness, and subdimensions. Table 1 lists all 15 dimensions 2.2 Practical Views of Quality Assessment There is an application area where absolute quality ratings of argumentative text are common practice: essay scoring (Beigman Klebanov et al., 2016). Persing and Ng (2015) annotated the argumentative strength of essays composing multiple arguments with notable agreement. For single arguments, however, all existing approaches that we are aware of assess quality in relative terms, e.g., Cabrio and Villata (2012) find accepted arguments based on attack relations, Wei et al. (2016) rank arguments by their persuasiveness, and Wachsmuth et al. (2017b) rank them by their relevance. Boudry et al. (2015) argue that normative concepts such as fallacies rarely apply to real-life arguments and that they are too sophisticated for operationalization. Based on the idea that r"
P17-2039,D15-1050,0,0.125974,"ng theories and practical approaches. Section 3 then empirically analyzes correlations in two recent argument corpora, one annotated for 15 well-defined quality dimensions taken from theory (Wachsmuth et al., 2017a) and one with 17 reasons for quality differences phrased spontaneously in practice (Habernal and Gurevych, 2016a). In a crowdsourcing study, we test whether lay annotators achieve agreement on the theoretical quality dimensions (Section 4). Introduction The assessment of argumentation quality is critical for any application built upon argument mining, such as debating technologies (Rinott et al., 2015). However, research still disagrees on whether quality should be assessed from a theoretical or from a practical viewpoint (Allwood, 2016). Theory states, among other things, that a cogent argument has acceptable premises that are relevant to its conclusion and sufficient to draw the conclusion (Johnson and Blair, 2006). Practitioners object that such quality dimensions are hard to assess for real-life arguments (Habernal and Gurevych, 2016b). Moreover, the normative nature of theory suggests absolute quality ratings, but in practice it seems much easier to state which argument is more convinc"
P17-2039,E17-1017,1,0.313544,"nnotators assessed Argument A as more convincing than B. When giving reasons for their assessment, though, they saw A as more credible and well thought through; that does not seem to be too far from the theoretical notion of cogency. This paper gives empirical answers to the question of how different the theoretical and practical views of argumentation quality actually are. Section 2 briefly reviews existing theories and practical approaches. Section 3 then empirically analyzes correlations in two recent argument corpora, one annotated for 15 well-defined quality dimensions taken from theory (Wachsmuth et al., 2017a) and one with 17 reasons for quality differences phrased spontaneously in practice (Habernal and Gurevych, 2016a). In a crowdsourcing study, we test whether lay annotators achieve agreement on the theoretical quality dimensions (Section 4). Introduction The assessment of argumentation quality is critical for any application built upon argument mining, such as debating technologies (Rinott et al., 2015). However, research still disagrees on whether quality should be assessed from a theoretical or from a practical viewpoint (Allwood, 2016). Theory states, among other things, that a cogent argu"
P17-2039,E17-1105,1,0.205942,"nnotators assessed Argument A as more convincing than B. When giving reasons for their assessment, though, they saw A as more credible and well thought through; that does not seem to be too far from the theoretical notion of cogency. This paper gives empirical answers to the question of how different the theoretical and practical views of argumentation quality actually are. Section 2 briefly reviews existing theories and practical approaches. Section 3 then empirically analyzes correlations in two recent argument corpora, one annotated for 15 well-defined quality dimensions taken from theory (Wachsmuth et al., 2017a) and one with 17 reasons for quality differences phrased spontaneously in practice (Habernal and Gurevych, 2016a). In a crowdsourcing study, we test whether lay annotators achieve agreement on the theoretical quality dimensions (Section 4). Introduction The assessment of argumentation quality is critical for any application built upon argument mining, such as debating technologies (Rinott et al., 2015). However, research still disagrees on whether quality should be assessed from a theoretical or from a practical viewpoint (Allwood, 2016). Theory states, among other things, that a cogent argu"
P17-2039,P16-2032,0,0.0789411,"o cogency, effectiveness, reasonableness, and subdimensions. Table 1 lists all 15 dimensions 2.2 Practical Views of Quality Assessment There is an application area where absolute quality ratings of argumentative text are common practice: essay scoring (Beigman Klebanov et al., 2016). Persing and Ng (2015) annotated the argumentative strength of essays composing multiple arguments with notable agreement. For single arguments, however, all existing approaches that we are aware of assess quality in relative terms, e.g., Cabrio and Villata (2012) find accepted arguments based on attack relations, Wei et al. (2016) rank arguments by their persuasiveness, and Wachsmuth et al. (2017b) rank them by their relevance. Boudry et al. (2015) argue that normative concepts such as fallacies rarely apply to real-life arguments and that they are too sophisticated for operationalization. Based on the idea that relative assessment is easier, Habernal and Gurevych (2016b) crowdsourced the UKPConvArg1 corpus. Argument pairs (A, B) from a debate portal were classified as to which argument is more convincing. Without giving any guidelines, the authors also asked for reasons as to why A is more convincing than B. In a foll"
W17-5106,walker-etal-2012-corpus,0,\N,Missing
W17-5106,P12-2041,0,\N,Missing
W17-5106,W14-2105,0,\N,Missing
W17-5106,W14-2107,0,\N,Missing
W17-5106,D15-1255,0,\N,Missing
W17-5106,W15-0514,0,\N,Missing
W17-5106,D15-1110,0,\N,Missing
W17-5106,N16-1007,0,\N,Missing
W17-5106,P16-1150,0,\N,Missing
W17-5106,W17-5115,1,\N,Missing
W17-5115,W02-1001,0,\N,Missing
W17-5115,P03-1054,0,\N,Missing
W17-5115,N12-1003,0,\N,Missing
W17-5115,prasad-etal-2008-penn,0,\N,Missing
W17-5115,D15-1255,0,\N,Missing
W17-5115,W15-0508,0,\N,Missing
W17-5115,C14-1141,0,\N,Missing
W17-5115,D15-1110,0,\N,Missing
W17-5115,C14-1142,0,\N,Missing
W17-5115,W14-2111,0,\N,Missing
W17-5115,W14-2112,0,\N,Missing
W17-5115,L16-1167,0,\N,Missing
W17-5115,C16-1324,1,\N,Missing
W17-5115,P17-1002,0,\N,Missing
W17-5115,E17-1105,1,\N,Missing
W19-8607,W18-5215,0,0.0251713,"nalysis tasks are widely studied including identifying the claims along with their supporting premises (Stab and Gurevych, 2014), finding the relation between argumentative units (Cocarascu and Toni, 2017), and assessing the persuasiveness of arguments (Habernal and Gurevych, 2016). Diverse downstream applications, however, necessitate the development of argumentation synthesis technologies. For example, synthesis is needed to produce a summary of arguments for a given topic (Wang and Ling, 2016) or to build a debating system where new arguments are exchanged between the users and the system (Le et al., 2018). As a result, a number of recent studies addresses the argumentation synthesis task. These studies 1 We consider a single argument to be a sequence of ADUs where each ADU has a specific role: thesis, con, or pro. 54 Proceedings of The 12th International Conference on Natural Language Generation, pages 54–64, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics composed of meaningful units, a property that neural generation methods barely achieve so far. In our evaluation, we utilize the dataset of Wachsmuth et al. (2018). This dataset contains 260 argumentative"
W19-8607,P16-2085,0,0.137945,", independent of their finances what is the good of a wonderfully outfitted university if it doesn’t actually allow the majority of clever people to broaden their horizons with all that great equipment p5 p6 p7 p8 p9 p10 p11 p12 Topic Should all universities in Germany charge tuition fees? Stance Con Table 1: The candidate thesis, con, and pro units for one topic-stance pair in the dataset of Wachsmuth et al. (2018). 2. Dispositio ∼ Arranging the selected ADUs in a sequential order. Some other approaches have been proposed that recompose existing text segments in new arguments. In particular, Bilu and Slonim (2016) generated new claims by “recycling” topics and predicates that were found in a database of claims. Claim selection involves preferring predicates that are generally amenable to claim units and that are relevant for the target topic. Egan et al. (2016) created summaries of the main points in a debate, and Reisert et al. (2015) synthesized complete arguments from a set of manually curated topic-stance relations based on the fine-grained argument model of Toulmin (1958). However, we are not aware of any approach that synthesizes arguments fully automatically, let alone that follows rhetorical co"
W19-8607,D17-1144,0,0.0659392,"arranges the structure of the types based on the units’ argumentative roles, and finally “phrases” an argument by instantiating the structure with semantically coherent units from the pool. Our evaluation suggests that the model can, to some extent, mimic the human synthesis of strategy-specific arguments. 1 Introduction Existing research on computational argumentation largely focuses on the analysis side. Various analysis tasks are widely studied including identifying the claims along with their supporting premises (Stab and Gurevych, 2014), finding the relation between argumentative units (Cocarascu and Toni, 2017), and assessing the persuasiveness of arguments (Habernal and Gurevych, 2016). Diverse downstream applications, however, necessitate the development of argumentation synthesis technologies. For example, synthesis is needed to produce a summary of arguments for a given topic (Wang and Ling, 2016) or to build a debating system where new arguments are exchanged between the users and the system (Le et al., 2018). As a result, a number of recent studies addresses the argumentation synthesis task. These studies 1 We consider a single argument to be a sequence of ADUs where each ADU has a specific ro"
W19-8607,W16-2816,0,0.0845706,"es in Germany charge tuition fees? Stance Con Table 1: The candidate thesis, con, and pro units for one topic-stance pair in the dataset of Wachsmuth et al. (2018). 2. Dispositio ∼ Arranging the selected ADUs in a sequential order. Some other approaches have been proposed that recompose existing text segments in new arguments. In particular, Bilu and Slonim (2016) generated new claims by “recycling” topics and predicates that were found in a database of claims. Claim selection involves preferring predicates that are generally amenable to claim units and that are relevant for the target topic. Egan et al. (2016) created summaries of the main points in a debate, and Reisert et al. (2015) synthesized complete arguments from a set of manually curated topic-stance relations based on the fine-grained argument model of Toulmin (1958). However, we are not aware of any approach that synthesizes arguments fully automatically, let alone that follows rhetorical considerations in the synthesis process. 3 3. Elocutio ∼ Phrasing the arranged ADUs by adding connectives at unit-initial or unit-final positions. Specifically, Wachsmuth et al. (2018) selected a pool of 200 ADUs for 10 pairs of controversial topic and s"
W19-8607,D14-1162,0,0.0830054,"Missing"
W19-8607,P16-1150,0,0.0216463,"es, and finally “phrases” an argument by instantiating the structure with semantically coherent units from the pool. Our evaluation suggests that the model can, to some extent, mimic the human synthesis of strategy-specific arguments. 1 Introduction Existing research on computational argumentation largely focuses on the analysis side. Various analysis tasks are widely studied including identifying the claims along with their supporting premises (Stab and Gurevych, 2014), finding the relation between argumentative units (Cocarascu and Toni, 2017), and assessing the persuasiveness of arguments (Habernal and Gurevych, 2016). Diverse downstream applications, however, necessitate the development of argumentation synthesis technologies. For example, synthesis is needed to produce a summary of arguments for a given topic (Wang and Ling, 2016) or to build a debating system where new arguments are exchanged between the users and the system (Le et al., 2018). As a result, a number of recent studies addresses the argumentation synthesis task. These studies 1 We consider a single argument to be a sequence of ADUs where each ADU has a specific role: thesis, con, or pro. 54 Proceedings of The 12th International Conference"
W19-8607,W15-0507,0,0.0710582,"is, con, and pro units for one topic-stance pair in the dataset of Wachsmuth et al. (2018). 2. Dispositio ∼ Arranging the selected ADUs in a sequential order. Some other approaches have been proposed that recompose existing text segments in new arguments. In particular, Bilu and Slonim (2016) generated new claims by “recycling” topics and predicates that were found in a database of claims. Claim selection involves preferring predicates that are generally amenable to claim units and that are relevant for the target topic. Egan et al. (2016) created summaries of the main points in a debate, and Reisert et al. (2015) synthesized complete arguments from a set of manually curated topic-stance relations based on the fine-grained argument model of Toulmin (1958). However, we are not aware of any approach that synthesizes arguments fully automatically, let alone that follows rhetorical considerations in the synthesis process. 3 3. Elocutio ∼ Phrasing the arranged ADUs by adding connectives at unit-initial or unit-final positions. Specifically, Wachsmuth et al. (2018) selected a pool of 200 ADUs for 10 pairs of controversial topic and stance from the English version of the arg-microtexts corpus (Peldszus and St"
W19-8607,P18-1152,0,0.0201934,"2 2 Related Work Recently, some researchers have tackled argumentation synthesis statistically with neural networks. For instance, Wang and Ling (2016) employed a sequence-to-sequence model to generate summaries of argumentative texts, and Hua and Wang (2018) did similar to generate counterarguments. Using neural methods in text generation, it is possible to achieve output that is on topic and grammatically (more or less) correct. However, when the desired text is to span multiple sentences, the generated text regularly suffers from incoherence and repetitiveness, as for instance discussed by Holtzman et al. (2018) who examine texts that were produced by RNNs in various domains. While these problems may be tolerable to some extent in some applications, such as chatbots, bad text cannot be accepted in an argumentative or debating scenario, where the goal is to convince or persuade a reader (rather than to merely inform or entertain). Holtzman et al. (2018) propose to alleviate incoherence and repetitiveness by training a set of discriminators, which aim to ensure that a text respects the Gricean maxims of quantity, quality, relation, and manner (Grice, 1975). To this end, they Most related to our approac"
W19-8607,P18-1021,0,0.100511,"view of argumentation synthesis that represents argumentative and rhetorical considerations with language modeling. 2. A novel approach that selects, arranges, and phrases ADUs to synthesize strategy-specific arguments for any topic and stance. 3. First experimental evidence that arguments with basic rhetorical strategies can be synthesized computationally.2 2 Related Work Recently, some researchers have tackled argumentation synthesis statistically with neural networks. For instance, Wang and Ling (2016) employed a sequence-to-sequence model to generate summaries of argumentative texts, and Hua and Wang (2018) did similar to generate counterarguments. Using neural methods in text generation, it is possible to achieve output that is on topic and grammatically (more or less) correct. However, when the desired text is to span multiple sentences, the generated text regularly suffers from incoherence and repetitiveness, as for instance discussed by Holtzman et al. (2018) who examine texts that were produced by RNNs in various domains. While these problems may be tolerable to some extent in some applications, such as chatbots, bad text cannot be accepted in an argumentative or debating scenario, where th"
W19-8607,2007.sigdial-1.5,0,0.256035,"each ADU is represented by a cluster label (A–F in Figure 2), where each label represents one ADU type. Now, for each of the strategies, we map each manually-generated sequence of ADUs to a sequence of cluster labels. Using these sequences of labels, we train one separated selection language model for each strategy. For clustering, we rely on topic-independent features that we expect to implicitly encode logical and emotional strategies: (1) psychological meaningfulness (Pennebaker et al., 2015), (2) eight basic emotions (Plutchik, 1980; Mohammad and Turney, 2013), and (3) argumentativeness (Somasundaran et al., 2007). In the following, we elaborate on the concrete features that we extract: Selection Language Model This model handles the selection of a set of n ADUs for a topic-stance pair x and a rhetorical strategy. We approach the selection as a language modeling task where each ADU is a “word” of our language model and each argument a “sentence”. To abstract from topic, the model actually selects ADU types, as explained in the following. 57 Argument corpus Topic+stance 1 … Argument1,1 ... 1-grams P( ) = 0.20 ... P( ) = 0.60 E F 3. Phrasing Regression Model → 2. Arrangement Language Model D A Argumentm,"
W19-8607,D14-1006,0,0.0583418,"t of unit types according to a basic rhetorical strategy (logos vs. pathos), arranges the structure of the types based on the units’ argumentative roles, and finally “phrases” an argument by instantiating the structure with semantically coherent units from the pool. Our evaluation suggests that the model can, to some extent, mimic the human synthesis of strategy-specific arguments. 1 Introduction Existing research on computational argumentation largely focuses on the analysis side. Various analysis tasks are widely studied including identifying the claims along with their supporting premises (Stab and Gurevych, 2014), finding the relation between argumentative units (Cocarascu and Toni, 2017), and assessing the persuasiveness of arguments (Habernal and Gurevych, 2016). Diverse downstream applications, however, necessitate the development of argumentation synthesis technologies. For example, synthesis is needed to produce a summary of arguments for a given topic (Wang and Ling, 2016) or to build a debating system where new arguments are exchanged between the users and the system (Le et al., 2018). As a result, a number of recent studies addresses the argumentation synthesis task. These studies 1 We conside"
W19-8607,C18-1318,1,0.571429,"Missing"
W19-8607,N16-1007,0,0.250645,"uments. 1 Introduction Existing research on computational argumentation largely focuses on the analysis side. Various analysis tasks are widely studied including identifying the claims along with their supporting premises (Stab and Gurevych, 2014), finding the relation between argumentative units (Cocarascu and Toni, 2017), and assessing the persuasiveness of arguments (Habernal and Gurevych, 2016). Diverse downstream applications, however, necessitate the development of argumentation synthesis technologies. For example, synthesis is needed to produce a summary of arguments for a given topic (Wang and Ling, 2016) or to build a debating system where new arguments are exchanged between the users and the system (Le et al., 2018). As a result, a number of recent studies addresses the argumentation synthesis task. These studies 1 We consider a single argument to be a sequence of ADUs where each ADU has a specific role: thesis, con, or pro. 54 Proceedings of The 12th International Conference on Natural Language Generation, pages 54–64, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics composed of meaningful units, a property that neural generation methods barely achieve so"
W19-8607,W15-0512,0,0.148233,"ricean maxims of quantity, quality, relation, and manner (Grice, 1975). To this end, they Most related to our approach is the system of Sato et al. (2015), where a user can enter a claimlike topic along with a stance. The system then generates argumentative paragraphs on specific aspects of the topic by selecting sentences from 10 million news texts of the Gigaword corpus. Potentially relevant aspects are those that trigger evaluative judgment in the reader. The sentences are arranged so that the text starts with a claim sentence and is followed by support sentences, employing the approach of Yanase et al. (2015). The support sentences are ordered by maximizing the semantic connectivity between sentences. Finally, some rephrasing is done in terms of certain aspects of surface realization. In a manual evaluation, however, no text was seen as sounding natural, underlining the difficulty of the task. In contrast to Sato et al. (2015), we learn directly from input data what argumentative discourse units to combine and how to arrange them. We leave surface realization aside to keep the focus on the argument composition. 2 The code for running the experiments is available here: https://github.com/webis-de/"
W19-8607,W00-1408,0,0.695119,"Missing"
