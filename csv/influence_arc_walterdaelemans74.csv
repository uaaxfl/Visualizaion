1993.eamt-1.6,J94-3007,1,\N,Missing
1993.eamt-1.6,J92-4001,0,\N,Missing
1993.eamt-1.6,E93-1007,1,\N,Missing
2020.coling-main.159,D14-1142,0,0.0192633,"essay. The essays were written in response to eight different writing prompts, all of which appear in all 11 L1 groups, by authors with low, medium, or high English proficiency. The dataset is considered a benchmark dataset for NLI and was used in two shared tasks on the NLI task (Tetreault et al., 2013; Malmasi et al., 2017). ICLE (Granger et al., 2009): the ICLEv2 dataset consists of essays written by highly-proficient nonnative college-level students of English. For comparability, we used a 7-language subset of the corpus normalized for topic and character encoding (Tetreault et al., 2012; Ionescu et al., 2014) (the so-called ICLE-NLI subset (Tetreault et al., 2012) to which we refer as ICLE). This subset contains 110 essays for each of the 7 languages: Bulgarian (BUL), Chinese (CHI), Czech (CZE), French (FRE), Japanese (JPN), Russian (RUS), and Spanish (SPA) (in total 770 essays with on average 747 tokens per essay). 3 Methodology Instead of training a classifier, we fine-tune a pretrained generative model (GPT-2) on the subset of the training samples written by the authors with the same L1, which provides us with n GPT-2 models (n equals the number of L1s (classes)). Each of these models has learn"
2020.coling-main.159,W13-1714,0,0.0498177,"Missing"
2020.coling-main.159,W17-5043,0,0.0264748,"Missing"
2020.coling-main.159,W15-0620,0,0.037845,"Missing"
2020.coling-main.159,W17-5007,0,0.156379,"rams (Jarvis et al., 2013), cognates (Markov et al., 2019), etymologically-related words (Nastase and Strapparava, 2017); syntactic features, e.g., context-free grammar features (Wong and Dras, 2011), Stanford parser dependency features (Tetreault et al., 2012); stylometric features, e.g., punctuation (Markov et al., 2018a), character n-gram features (Kulmizev et al., 2017); emotion-based features (Markov et al., 2018b), etc. The combination of such features provides the best results for NLI, as shown by the two shared tasks on the NLI task organized in 2013 (Tetreault et al., 2013) and 2017 (Malmasi et al., 2017), where the two top-ranked systems (Cimino and Dell’Orletta, 2017; Markov et al., 2017) used Support Vector Machines (SVM) with a variety of engineered features. NLI has also been approached using deep neural networks: Franco-Salvador et al. (2017) used word embeddings, reporting a slight increase in performance when they are combined with string kernels in a meta-learning set-up; Chen (2016) and Bjerva et al. (2017) used convolutional neural networks (CNN) and long short-term memory networks (LSTM), concluding that traditional methods, i.e., SVM with engineered features, appear to work better"
2020.coling-main.159,W17-5042,1,0.87778,"Missing"
2020.coling-main.159,C18-1293,1,0.837445,"Missing"
2020.coling-main.159,W18-6218,1,0.813315,"Missing"
2020.coling-main.159,W19-4429,1,0.788265,"Missing"
2020.coling-main.159,2020.sigdial-1.28,0,0.0140376,"Each of these models has learned the characteristics of a certain L1 group, which we will use to discriminate between new samples: at inference time, we feed an unseen text to all these models and calculate their LM loss. Based on the fine-tuning, we expect to get the least LM loss from the model that is trained on the same class of texts, thus the assigned label is the argmin of all models’ losses. An example of assigning a label to an unseen text (Turkish L1 in this case) is shown in Figure 11 . Although the LM loss (as a measure of likelihood) has been used before as an evaluation metric (Mehri and Eskenazi, 2020), to the best of our knowledge, it has not been exploited in this way for text classification tasks in general and for NLI in particular. 4 Results and Discussion To compare the performance of our approach with the previously published state-of-the-art results (Markov, 2018), we report the results in terms of classification accuracy on the TOEFL11 test set, as well as on the TOEFL11 dataset under 10-fold cross-validation (10FCV) and on the ICLE dataset under 5-fold cross-validation (5FCV). To better situate the method, several baseline models have also 1 The LM loss scores are provided for a r"
2020.coling-main.159,D17-1286,0,0.0250855,"Missing"
2020.coling-main.159,C12-1158,0,0.176808,"n features that capture the systematic fingerprints of the first language in the second language writing (native language interference (Odlin, 1989)). Numerous feature types that capture various aspects of the interference phenomenon have been explored for NLI: spelling errors (Koppel et al., 2005; Chen et al., 2017); lexical features, e.g., word and lemma n-grams (Jarvis et al., 2013), cognates (Markov et al., 2019), etymologically-related words (Nastase and Strapparava, 2017); syntactic features, e.g., context-free grammar features (Wong and Dras, 2011), Stanford parser dependency features (Tetreault et al., 2012); stylometric features, e.g., punctuation (Markov et al., 2018a), character n-gram features (Kulmizev et al., 2017); emotion-based features (Markov et al., 2018b), etc. The combination of such features provides the best results for NLI, as shown by the two shared tasks on the NLI task organized in 2013 (Tetreault et al., 2013) and 2017 (Malmasi et al., 2017), where the two top-ranked systems (Cimino and Dell’Orletta, 2017; Markov et al., 2017) used Support Vector Machines (SVM) with a variety of engineered features. NLI has also been approached using deep neural networks: Franco-Salvador et al"
2020.coling-main.159,W13-1706,0,0.137906,"features, e.g., word and lemma n-grams (Jarvis et al., 2013), cognates (Markov et al., 2019), etymologically-related words (Nastase and Strapparava, 2017); syntactic features, e.g., context-free grammar features (Wong and Dras, 2011), Stanford parser dependency features (Tetreault et al., 2012); stylometric features, e.g., punctuation (Markov et al., 2018a), character n-gram features (Kulmizev et al., 2017); emotion-based features (Markov et al., 2018b), etc. The combination of such features provides the best results for NLI, as shown by the two shared tasks on the NLI task organized in 2013 (Tetreault et al., 2013) and 2017 (Malmasi et al., 2017), where the two top-ranked systems (Cimino and Dell’Orletta, 2017; Markov et al., 2017) used Support Vector Machines (SVM) with a variety of engineered features. NLI has also been approached using deep neural networks: Franco-Salvador et al. (2017) used word embeddings, reporting a slight increase in performance when they are combined with string kernels in a meta-learning set-up; Chen (2016) and Bjerva et al. (2017) used convolutional neural networks (CNN) and long short-term memory networks (LSTM), concluding that traditional methods, i.e., SVM with engineered"
2020.coling-main.159,D11-1148,0,0.0510913,"Missing"
2020.figlang-1.36,K16-1017,0,0.30397,"Missing"
2020.figlang-1.36,D17-1070,0,0.0622587,"rkov et al., 2020). 2 https://universaldependencies.org/u/pos/ The features were weighted using the term frequency (tf) weighting scheme and fed to liblinear SVM with optimized parameters (the optimal liblinear classifier parameters were selected: penalty parameter (C), loss function (loss), and tolerance for stopping criteria (tol) (based on grid search). The liblinear scikit-learn (Pedregosa et al., 2011) implementation of SVM was used. 4.1.4 MLP This model consists of simple multi-layer perceptron (MLP) classifier based on sentence embeddings from the Infersent model developed by Facebook (Conneau et al., 2017). Infersent is trained on natural language inference data, which is a motivation to use this model in our ensemble approach, since it might spot the logical discrepancies that often play a role in creating and detecting sarcasm. Infersent works with GloVe or Fasttext word embeddings as input and gives a 4092-dimensional sentence embedding. For this task we concatenated the response and context embeddings (with GloVe) and fed the resulting 8184-dimensional vector to an MLP with Relu non-linearity and a sigmoid at the end for classification. This was attempted with different architectures among"
2020.figlang-1.36,W16-6208,0,0.0658506,"Missing"
2020.figlang-1.36,W16-0425,0,0.310124,"Missing"
2020.figlang-1.36,J18-4009,0,0.268301,"sarcasm in social media data is described. The ensemble was designed in the context of The Second Workshop on Figurative Language Processing held in conjunction with ACL 20201 . It was the goal of the shared task to create a robust sarcasm detection model for tweets and Reddit comments and investigate the role of conversational context in automatic sarcasm detection models. Detecting sarcasm can be a challenging task, not only for machines, but also for humans, because sarcasm is subjective and culturally dependent, and because an utterance on its own can be both sarcastic and non-sarcastic (Ghosh et al., 2018; Joshi 1 https://sites.google.com/view/figlang2020/ Related research In this section, recent advances and papers related to sarcasm detection are described. The first advance is related to automatic annotation methods where the annotators use computational methods to obtain the labels (Joshi et al., 2017), for instance by searching for “#sarcasm” in tweets (e.g. Gonz´alezIb´an˜ ez et al. (2011)). Automatic labelling is often preferred to manual labelling, because it is faster, cheaper, allows for the creation of larger data sets, and because the author of an utterance knows best whether it wa"
2020.figlang-1.36,P11-2102,0,0.51893,"Missing"
2020.figlang-1.36,W03-0504,0,0.0501982,"example, hashtags and emojis were used as signifiers to modify the rest of a sentence. A bidirectional LSTM model was used to recognize these modifications in relation to the main embedded vocabulary and predict binary sarcasm (Zhou et al., 2016). Context and response words were vectorized using pretrained GloVe embeddings (Pennington et al., 2014). Emojis were embedded using Emoji2Vec (Eisner et al., 2016). All words were then further embedded using an RNN model, trained on tweets from the Chirps corpus to predict hashtags (Shwartz et al., 2017); comparable to a sentence summarization task (Jing et al., 2003), which contributed to the Reddit task as well as Twitter, by using base text alone. 265 These 3 embedding layers were combined for the bidirectional LSTM to iterate over. To mitigate overfitting, dropout was applied two times and optimized: (i) to the embedding layers, and (ii) within the LSTM layers. Finally the concatenated output was passed to a sigmoid layer for prediction. 4.1.2 CNN-LSTM This model uses word embeddings of the response and context pretrained with GloVe embeddings (Pennington et al., 2014)), and punctuation, casing, sentiment and stop word features. The punctuation feature"
2020.figlang-1.36,W16-2111,0,0.0509621,"Missing"
2020.figlang-1.36,L18-1102,0,0.0392131,"Missing"
2020.figlang-1.36,K18-2016,0,0.0242754,"word list. The response and context word embeddings were twice fed to a sequence of a convolutional layer with max pooling, and to a bidirectional LSTM layer. The other feature vectors were each passed to a dense layer. To avoid overfitting, dropout was applied and optimized after each embedding, convolutional, LSTM, and dense layer. Finally, the outputs of all of the above were concatenated and passed to a sigmoid layer for prediction. 4.1.3 SVM In this approach, the response messages were represented through a combination of part-of-speech (POS) tags (obtained using the StanfordNLP library (Qi et al., 2018)), function words (i.e., words belonging to the closed syntactic classes2 ), and emotion-based features from the NRC emotion lexicon (Mohammad and Turney, 2013). From this representation, n-grams (with n from 1 to 3) were built. Character n-gram features (with n from 1 to 3) were added as a separate feature vector. This approach captures the stylometric and emotion characteristics of a textual content and is described in detail in (Markov et al., 2020). 2 https://universaldependencies.org/u/pos/ The features were weighted using the term frequency (tf) weighting scheme and fed to liblinear SVM"
2020.figlang-1.36,D13-1066,0,0.156752,"Missing"
2020.figlang-1.36,S17-1019,0,0.0195681,"-word features have a noticeable effect on sarcasm transparency. For example, hashtags and emojis were used as signifiers to modify the rest of a sentence. A bidirectional LSTM model was used to recognize these modifications in relation to the main embedded vocabulary and predict binary sarcasm (Zhou et al., 2016). Context and response words were vectorized using pretrained GloVe embeddings (Pennington et al., 2014). Emojis were embedded using Emoji2Vec (Eisner et al., 2016). All words were then further embedded using an RNN model, trained on tweets from the Chirps corpus to predict hashtags (Shwartz et al., 2017); comparable to a sentence summarization task (Jing et al., 2003), which contributed to the Reddit task as well as Twitter, by using base text alone. 265 These 3 embedding layers were combined for the bidirectional LSTM to iterate over. To mitigate overfitting, dropout was applied two times and optimized: (i) to the embedding layers, and (ii) within the LSTM layers. Finally the concatenated output was passed to a sigmoid layer for prediction. 4.1.2 CNN-LSTM This model uses word embeddings of the response and context pretrained with GloVe embeddings (Pennington et al., 2014)), and punctuation,"
2020.figlang-1.36,J18-4010,0,0.0287265,"Missing"
2020.figlang-1.36,P14-2084,0,0.484661,"Missing"
2020.figlang-1.36,C16-1329,0,0.030764,"es only on stylometric and emotion-based properties of the response. All other models use the two conversational turns preceding the response as context, since this was the minimum amount of context that was provided for each response. 4.1 Component models 4.1.1 LSTM Preliminary studies showed that non-word features have a noticeable effect on sarcasm transparency. For example, hashtags and emojis were used as signifiers to modify the rest of a sentence. A bidirectional LSTM model was used to recognize these modifications in relation to the main embedded vocabulary and predict binary sarcasm (Zhou et al., 2016). Context and response words were vectorized using pretrained GloVe embeddings (Pennington et al., 2014). Emojis were embedded using Emoji2Vec (Eisner et al., 2016). All words were then further embedded using an RNN model, trained on tweets from the Chirps corpus to predict hashtags (Shwartz et al., 2017); comparable to a sentence summarization task (Jing et al., 2003), which contributed to the Reddit task as well as Twitter, by using base text alone. 265 These 3 embedding layers were combined for the bidirectional LSTM to iterate over. To mitigate overfitting, dropout was applied two times an"
2020.latechclfl-1.5,N18-1118,0,0.0127285,"ched with information in the target language. Tiedemann and Scherrer (2017) investigated the benefits of the extended context approach in attentionbased NMT for DE→EN subtitles (see Table 1). The source sentence was concatenated with the previous source sentence and, then, the same technique was additionally applied to the target sentence. They used a special prefix to mark tokens belonging to the extended context. Although the improvement over the baseline was moderate, the NMT models were able to utilize the additional context and to distinguish it from the main sentence. In follow-up work, Bawden et al. (2018) designed EN→FR test sets to investigate the usefulness of the previous source and target sentences in the context of NMT. They demonstrated that the concatenation strategy leads to improved performance. Agrawal et al. (2018) applied the concatenation technique with a Transformer-based architecture to EN→IT TED talks and experimentally varied the number of concatenated sentences included. There too, the extended context was demonstrated to be beneficial for Transformers. Junczys-Dowmunt (2019), finally, developed one of the best-performing systems based on the same idea in the context of the W"
2020.latechclfl-1.5,P19-1175,0,0.0437211,"Missing"
2020.latechclfl-1.5,W14-4012,0,0.169643,"Missing"
2020.latechclfl-1.5,2020.acl-main.144,0,0.0256498,"data augmentation method for NMT that yielded substantial performance improvements for EN→NL and EN→HU. The source sentence was concatenated with fuzzy matches, or sentences in the target language retrieved from a translation memory, that covered the entire training set. The fuzzy matches were selected on the basis of a simple similarity measurement between each source sentence and all other source sentences from the translation memory. Then, the fuzzy source sentences with a similarity score above a given threshold were stored with their corresponding target sentences. In a subsequent study, Jitao et al. (2020) improved the previously proposed method by explicitly informing models about any relevant tokens in the fuzzy matches and incorporating distributed sentence representations (see Table 1). Inspired by this previous work, we investigate the use of Iconclass in the context of artwork title translations. We use the definitions and keywords associated with Iconclass codes to extend and augment the artwork titles. Our main contribution is that we demonstrate that Iconclass definitions, when provided within a data augmentation strategy, improve translation performance. extended context data augmenta"
2020.latechclfl-1.5,W19-5321,0,0.0200556,"able to utilize the additional context and to distinguish it from the main sentence. In follow-up work, Bawden et al. (2018) designed EN→FR test sets to investigate the usefulness of the previous source and target sentences in the context of NMT. They demonstrated that the concatenation strategy leads to improved performance. Agrawal et al. (2018) applied the concatenation technique with a Transformer-based architecture to EN→IT TED talks and experimentally varied the number of concatenated sentences included. There too, the extended context was demonstrated to be beneficial for Transformers. Junczys-Dowmunt (2019), finally, developed one of the best-performing systems based on the same idea in the context of the WMT19 news translation shared task for EN→DE. Bult´e and Tezcan (2019) proposed a simple and efficient data augmentation method for NMT that yielded substantial performance improvements for EN→NL and EN→HU. The source sentence was concatenated with fuzzy matches, or sentences in the target language retrieved from a translation memory, that covered the entire training set. The fuzzy matches were selected on the basis of a simple similarity measurement between each source sentence and all other s"
2020.latechclfl-1.5,P17-4012,0,0.0326785,"ncludes self-attention mechanisms that can access all positions in a previous layer. Therefore, the receptive field is not as myopic as with convolutional models. Additionally, the absence of recurrent connections allows one to make the training process fully parallellizable. Therefore, such models nowadays are more appealing for our problem. 3.3 Training and Inference Details As character-level translation works better for the translation of artwork titles (Banar et al., 2020), we applied a four-layer character-level Transformer (Vaswani et al., 2017), implemented in the OpenNMTpy framework (Klein et al., 2017). The vocabulary size was set to 300 characters and the length of sentences was limited to 450 characters. The models were trained by minimizing the negative conditional 46 log-likelihood using the Adam optimizer (Kingma and Ba, 2014) with a batch size of 6,144 tokens and an accumulation count of 4. First, we pre-train the models on the general corpus and, then, fine-tune them on our domain-specific corpus. Each model was trained on a single GeForce GTX 1080 Ti with 11 GB RAM. In the pre-training phase, the models were initialized using the method proposed by Glorot and Bengio (2010) and train"
2020.latechclfl-1.5,Q17-1026,0,0.016566,"1,777,653 sentence pairs extracted from the Europarl corpus (Tiedemann, 2012) for the NL↔EN language pair, in order to pre-train the models on a generic background corpus. We randomly selected 3,000 sentence pairs as a development set and 3,000 sentence pairs in the test set respectively. 3.2 Model Details Banar et al. (2020) demonstrated the advantages of character-level translation over the subword-level approach for artwork titles. Hence, we exclusively resorted to character-level models in the present work. However, Banar et al. (2020) used a fusion of recurrent and convolutional models (Lee et al., 2017) that has become outdated. The recent emergence of NMT models started with recurrent neural 45 source target English definition English keywords Dutch keywords concatenation 1 concatenation 2 concatenation 3 concatenation 4 Ceres bespot door Stellio (Metamorphosen 5: 446-461) Mocking of Ceres by Stellio (Metamorphoses 5: 446-461) a little boy (Abas, Ascalabus, or Stellio) laughs at Ceres, because she drinks too avidly while she is resting at an old woman’s house Ascalabus, boy, drinking, laughing, old woman, thirst Ascalabus, dorst, drinken, jongen, lachend, old woman Ceres bespot door Stellio"
2020.latechclfl-1.5,D15-1166,0,0.0200345,"p, honeycomb, lamb, staff Johannes de Doper en de H. Hieronymus (txs) the monk and hermit Jerome (Hieronymus); possible attributes: book, cardinal’s hat, crucifix, hour-glass, lion, skull, stone (txe) Johannes de Doper en de H. Hieronymus (txs) John the Baptist; possible attributes: book, reed cross, baptismal cup, honeycomb, lamb, staff (txe) Table 5: Example of the concatenation of a title having multiple Iconclass codes with corresponding English definitions from 11H(JEROME) and 11H(JOHN THE BAPTIST). networks such as GRU or LSTM memory cells (Bahdanau et al., 2014; Sutskever et al., 2014; Luong et al., 2015; Cho et al., 2014), but since then it has been established that Transformer-based architectures (Vaswani et al., 2017) persuasively outperform recurrent and convolutional models across various tasks. The Transformer model mitigates some of the limitations of recurrent and convolutional models; the Transformer, for example, includes self-attention mechanisms that can access all positions in a previous layer. Therefore, the receptive field is not as myopic as with convolutional models. Additionally, the absence of recurrent connections allows one to make the training process fully parallellizab"
2020.latechclfl-1.5,P02-1040,0,0.109661,"g phase, the models were initialized using the method proposed by Glorot and Bengio (2010) and trained for 100,000 updates using the Noam decay schedule (Popel and Bojar, 2018) with an initial learning rate of 2. In the fine-tuning phase, the initial learning rate was set to 0.0001. The fine-tuning was interrupted as soon as the validation loss did not decrease for 600 updates. In the decoding part of the architecture, we applied a beam search with a beam size of 25. The evaluation was conducted using three standard metrics: C HARAC TER3 (Wang et al., 2016), CHRF4 (Popovi´c, 2015) and BLEU-4 (Papineni et al., 2002) . 4 Results and Discussion We present our quantitative results in Section 4.1. We divide our experimental results into two different sections. First, we assess the use of the extended context in Section 4.1.1. Second, we compare the various data augmentation strategies to the baseline in Section 4.1.2. In Section 4.2 we manually inspect a selection of outputs for the best performing model. 4.1 Quantitative Analysis Pair NL-EN EN-NL Additional Information (a) baseline (b) keywordsnl (c) keywordsen (d) definitionen (e) (d) + (c) (f) baseline (g) keywordsnl (h) keywordsen (i) definitionen (j) (i"
2020.latechclfl-1.5,W15-3049,0,0.0255025,"Missing"
2020.latechclfl-1.5,N19-5004,0,0.0977053,"t al., 2012; Lin et al., 2014). These datasets contain millions of training items, which allowed researchers to achieve impressive results in many tasks. However, the construction of such materials in the domain of cultural heritage material is an even more expensive process, as it requires the intervention of highly-trained subject experts. Hence, many institutions can only offer smaller datasets, that contain just a fraction of the number of training examples that are needed to train a deep learning algorithms. Transfer learning is a common solution to overcome such a lack of training data (Ruder et al., 2019). In neural machine translation (NMT), networks are nowadays commonly pre-trained on large generic datasets of parallel sentences, before they get fine-tuned on a more specific “downstream” corpus. Such networks, however, are conventionally only exposed to the actual sentence pairs in the target domain and are ignorant of additional knowledge that might be available such as, for example, iconographic metadata about objects and their relations. In the case of artworks, computational methods that can exploit such additional knowledge are highly appealing. This work aims to apply NMT in the conte"
2020.latechclfl-1.5,W17-4811,0,0.146208,"with proposals for future work in Section 5. 2 Related Work Modern NMT systems nowadays often work at the level of an individual sentence pair and aim to translate a source sentence into a target sentence, without making use of additional information other than the source sentence itself. The idea to concatenate a source sentence with additional information, however, is not new. This preprocessing step is appealing due to its simplicity and model-agnostic applicability. Previous approaches in this respect are generally divided into 2 categories (see examples in Table 1): (i) extended context (Tiedemann and Scherrer, 2017), where additional information in the source language is added to the source sentence (and sometimes to the target sentence); (ii) data augmentation (Bult´e and Tezcan, 2019), where the source sentence is enriched with information in the target language. Tiedemann and Scherrer (2017) investigated the benefits of the extended context approach in attentionbased NMT for DE→EN subtitles (see Table 1). The source sentence was concatenated with the previous source sentence and, then, the same technique was additionally applied to the target sentence. They used a special prefix to mark tokens belongi"
2020.latechclfl-1.5,tiedemann-2012-parallel,0,0.146601,"ble to developers and researchers. Artificial intelligence, and machine learning in particular, increasingly plays an important role in this process (Fiorucci et al., 2020). Recent case studies have demonstrated successful applications of machine learning methods to cultural heritage collections. Most of this work relies on advances in computational methods and utilizes a modelling framework known as deep neural networks (LeCun et al., 2015; Schmidhuber, 2015). However, such algorithms are dataintensive and require large annotated datasets, which recently have become available in some fields (Tiedemann, 2012; Krizhevsky et al., 2012; Lin et al., 2014). These datasets contain millions of training items, which allowed researchers to achieve impressive results in many tasks. However, the construction of such materials in the domain of cultural heritage material is an even more expensive process, as it requires the intervention of highly-trained subject experts. Hence, many institutions can only offer smaller datasets, that contain just a fraction of the number of training examples that are needed to train a deep learning algorithms. Transfer learning is a common solution to overcome such a lack of t"
2020.latechclfl-1.5,W16-2342,0,0.0167578,"GeForce GTX 1080 Ti with 11 GB RAM. In the pre-training phase, the models were initialized using the method proposed by Glorot and Bengio (2010) and trained for 100,000 updates using the Noam decay schedule (Popel and Bojar, 2018) with an initial learning rate of 2. In the fine-tuning phase, the initial learning rate was set to 0.0001. The fine-tuning was interrupted as soon as the validation loss did not decrease for 600 updates. In the decoding part of the architecture, we applied a beam search with a beam size of 25. The evaluation was conducted using three standard metrics: C HARAC TER3 (Wang et al., 2016), CHRF4 (Popovi´c, 2015) and BLEU-4 (Papineni et al., 2002) . 4 Results and Discussion We present our quantitative results in Section 4.1. We divide our experimental results into two different sections. First, we assess the use of the extended context in Section 4.1.1. Second, we compare the various data augmentation strategies to the baseline in Section 4.1.2. In Section 4.2 we manually inspect a selection of outputs for the best performing model. 4.1 Quantitative Analysis Pair NL-EN EN-NL Additional Information (a) baseline (b) keywordsnl (c) keywordsen (d) definitionen (e) (d) + (c) (f) bas"
2020.lrec-1.22,K18-1008,1,0.796464,". 1.2. Main Contributions In this paper, we analyze three different feature sets and their various parameterizations, and show that the way these feature sets are currently used is suboptimal, for two reasons: first, the parameterizations of the feature sets used in other papers are not ideal. Second, the number of nearest neighbors considered in calculating the neighborhood effect is not ideal. Furthermore, we show that the optimal parameters for these metrics differ from the parameters used in psycholinguistic research on word reading. We directly compare our results to previous work, i.e., Tulkens et al. (2018a), and show that the conclusions drawn from this work are incomplete because the research was carried out with a suboptimal number of nearest neighbors and parameters. We thus conclude that many different orthographic codes are equally feasible as far as the neighborhood effect is concerned, but only when properly optimized, which was not the case before. In a second experiment, we show that the inverse of mutual information weighting improves the explained variance of the neighborhood effect for all feature sets, and that weighing with regular mutual information almost removes the neighborho"
2020.lrec-1.22,L18-1427,1,0.885426,"Missing"
2020.lrec-1.22,W18-6303,0,0.0382977,"Missing"
2020.lrec-1.407,gavrilidou-etal-2012-meta,1,0.919419,"Missing"
2020.lrec-1.407,2020.lrec-1.420,1,0.860379,"Missing"
2020.lrec-1.407,L18-1213,1,0.894888,"Missing"
2020.lrec-1.407,piperidis-etal-2014-meta,1,0.824391,"ween 2010 and 2017, supported through the EU projects T4ME, CESAR, METANET4U, META-NORD and CRACKER. One of its main goals is technology support for all European languages as well as fostering innovative research by providing strategic recommendations with regard to key research topics (Rehm and Uszkoreit, 2013). META-SHARE3 is an infrastructure that brings together providers and consumers of language data, tools and services. It is a network of repositories that store resources, documented with high-quality metadata aggregated in central inventories (Piperidis, 2012; Gavrilidou et al., 2012; Piperidis et al., 2014). CLARIN ERIC The CLARIN European Research Infrastructure for Language Resources and Technology is a legal entity set up in 2012, with 20 member countries at present.4 CLARIN makes language resources available to scholars, researchers, and students from all disciplines with a focus on the humanities and social sciences. CLARIN offers solutions and services for deploying, connecting, analyzing and sustaining digital language data and tools. Call ICT-17-2014 – “Cracking the Language Barrier” The EU call ICT-17-2014, which was informed by key META-NET results (Rehm and Uszkoreit, 2012), funded a"
2020.lrec-1.407,piperidis-2012-meta,1,0.92358,"n 34 European countries. META-NET was, between 2010 and 2017, supported through the EU projects T4ME, CESAR, METANET4U, META-NORD and CRACKER. One of its main goals is technology support for all European languages as well as fostering innovative research by providing strategic recommendations with regard to key research topics (Rehm and Uszkoreit, 2013). META-SHARE3 is an infrastructure that brings together providers and consumers of language data, tools and services. It is a network of repositories that store resources, documented with high-quality metadata aggregated in central inventories (Piperidis, 2012; Gavrilidou et al., 2012; Piperidis et al., 2014). CLARIN ERIC The CLARIN European Research Infrastructure for Language Resources and Technology is a legal entity set up in 2012, with 20 member countries at present.4 CLARIN makes language resources available to scholars, researchers, and students from all disciplines with a focus on the humanities and social sciences. CLARIN offers solutions and services for deploying, connecting, analyzing and sustaining digital language data and tools. Call ICT-17-2014 – “Cracking the Language Barrier” The EU call ICT-17-2014, which was informed by key META"
2020.lrec-1.407,L16-1251,1,0.865781,"Missing"
2020.lrec-1.407,2020.lrec-1.413,1,0.785764,"Missing"
2020.peoples-1.15,2020.acl-main.112,0,0.0407795,"Missing"
2020.peoples-1.15,W18-6218,1,0.860007,"Missing"
2020.peoples-1.15,S18-1001,0,0.0303931,"ovene) and two topics (migrants and LGBT). We show significant and consistent improvements in automatic classification across all languages and topics, as well as consistent (and expected) emotion distributions across all languages and topics, proving for the manually corrected lexicons to be a useful addition to the severely lacking area of emotion lexicons, the crucial resource for emotive analysis of text. 1 Introduction Emotion lexicons are rather scarce resources for most languages (Buechel et al., 2020), although they are a very important ingredient for robust emotion detection in text (Mohammad et al., 2018). They are mostly differentiated between depending on the way they encode emotions – either via continuous or discrete representations (Calvo and Mac Kim, 2013). In this work, we present emotion lexicons for three languages – Croatian, Dutch and Slovene, developed by manually correcting automatic translations that are part of the NRC Emotion Lexicon (Mohammad and Turney, 2013). In that lexicon, sentiment (positive or negative) and a discrete model of emotion covering anger, anticipation, disgust, fear, joy, sadness, surprise and trust, are encoded via a binary variable for each emotion. The si"
2021.bionlp-1.22,W19-4813,0,0.0698711,"qualitatively interpretable rules. 1 Introduction Understanding and explaining decisions of complex models such as neural networks has recently gained a lot of attention for engendering trust in these models, improving them, and understanding them better (Montavon et al., 2018; Alishahi et al., 2019; Belinkov and Glass, 2019). Several previous studies developing interpretability techniques provide a set of input features or segments that are the most salient for the model output. Approaches such as input perturbation and gradient computation are popular for this purpose (Ancona et al., 2018; Arras et al., 2019). A drawback of these approaches is the lack of information about interaction between different features. While heatmaps (Li et al., 2016b,a; Arras et al., 2017) and partial dependence plots (Lundberg and Lee, 2017) are popularly used, they only provide a qualitative view which quickly ∗ gets complex as the number of features increases. To overcome this limitation, rule induction for model interpretability has become popular, which accounts for interactions between multiple features and output classes (Lakkaraju et al., 2017; Puri et al., 2017; Ming et al., 2018; Ribeiro et al., 2018; Sushil e"
2021.bionlp-1.22,Q19-1004,0,0.024705,"sion lists (also called rules) over skipgrams. For evaluation of explanations, we create a synthetic sepsis-identification dataset, as well as apply our technique on additional clinical and sentiment analysis datasets. We find that our technique persistently achieves high explanation fidelity and qualitatively interpretable rules. 1 Introduction Understanding and explaining decisions of complex models such as neural networks has recently gained a lot of attention for engendering trust in these models, improving them, and understanding them better (Montavon et al., 2018; Alishahi et al., 2019; Belinkov and Glass, 2019). Several previous studies developing interpretability techniques provide a set of input features or segments that are the most salient for the model output. Approaches such as input perturbation and gradient computation are popular for this purpose (Ancona et al., 2018; Arras et al., 2019). A drawback of these approaches is the lack of information about interaction between different features. While heatmaps (Li et al., 2016b,a; Arras et al., 2017) and partial dependence plots (Lundberg and Lee, 2017) are popularly used, they only provide a qualitative view which quickly ∗ gets complex as the"
2021.bionlp-1.22,P19-1283,0,0.0517069,"Missing"
2021.bionlp-1.22,W18-5408,0,0.0211718,"erns as the L2 norm, albeit with higher magnitudes. In Section 4.1, we analyze the importance scores obtained with these techniques qualitatively and quantitatively to identify the preferred one. 2.2 Skipgrams to incorporate context One of the contributions of this work is to find explanation rules for sequential models such as RNNs. Conjunctive clauses of if-then-else rules are order independent although this order is critical for RNNs. To account for word order in input documents, some previous approaches find the most important ngrams instead of the top words only (Murdoch and Szlam, 2017; Jacovi et al., 2018). To incorporate word order also in explanation rules, we compute the importance of subsequences in the documents before combining different subsequences into conjunctive rules. We define importance of a subsequence as the mean saliency of all the tokens in that subsequence. We represent subsequences as skipgrams with length in the range [1,4] and with maximum two skip tokens1 . After computing the scores, we retain the 50 most important skipgrams for every document (based on absolute importance scores). The number of unique skipgrams obtained in this manner is very high. To limit the complexi"
2021.bionlp-1.22,D19-1523,0,0.0342615,"Missing"
2021.bionlp-1.22,N16-1082,0,0.0272427,"tly gained a lot of attention for engendering trust in these models, improving them, and understanding them better (Montavon et al., 2018; Alishahi et al., 2019; Belinkov and Glass, 2019). Several previous studies developing interpretability techniques provide a set of input features or segments that are the most salient for the model output. Approaches such as input perturbation and gradient computation are popular for this purpose (Ancona et al., 2018; Arras et al., 2019). A drawback of these approaches is the lack of information about interaction between different features. While heatmaps (Li et al., 2016b,a; Arras et al., 2017) and partial dependence plots (Lundberg and Lee, 2017) are popularly used, they only provide a qualitative view which quickly ∗ gets complex as the number of features increases. To overcome this limitation, rule induction for model interpretability has become popular, which accounts for interactions between multiple features and output classes (Lakkaraju et al., 2017; Puri et al., 2017; Ming et al., 2018; Ribeiro et al., 2018; Sushil et al., 2018; Evans et al., 2019; Pastor and Baralis, 2019). Most of these work treat the explained models as black boxes, and fit a separ"
2021.bionlp-1.22,D14-1162,0,0.0839774,"Missing"
2021.bionlp-1.22,P18-1032,0,0.246121,", while ensuring that these rules generalize to unseen data. To this end, we quantify skipgram importance in LSTMs by first pooling gradients across embedding dimensions to compute word importance, and thereby aggregating them into skipgram importance. Skipgrams incorporate word order in explanations and increase interpretability. 2. To overcome existing limitations with auResearch conducted while at CLiPS. 202 Proceedings of the BioNLP 2021 workshop, pages 202–212 June 11, 2021. ©2021 Association for Computational Linguistics tomated explanation evaluation (Lertvittayakumjorn and Toni, 2019; Poerner et al., 2018), we provide a synthetic clinical text classification dataset for evaluating interpretability techniques. We construct this dataset according to existing medical knowledge and clinical corpus. We validate our explanation pipeline on this synthetic dataset by recovering the labeling rules of the dataset. We then apply our pipeline to two clinical datasets for sepsis classification, and one dataset for sentiment analysis. We confirm that the explanation results obtained on synthetic data are scalable to real corpora. 2 Explanation pipeline We propose a method to find decision lists as explanatio"
2021.bionlp-1.22,D13-1170,0,0.00587475,"Missing"
2021.bionlp-1.22,W18-5411,1,0.892307,"., 2019). A drawback of these approaches is the lack of information about interaction between different features. While heatmaps (Li et al., 2016b,a; Arras et al., 2017) and partial dependence plots (Lundberg and Lee, 2017) are popularly used, they only provide a qualitative view which quickly ∗ gets complex as the number of features increases. To overcome this limitation, rule induction for model interpretability has become popular, which accounts for interactions between multiple features and output classes (Lakkaraju et al., 2017; Puri et al., 2017; Ming et al., 2018; Ribeiro et al., 2018; Sushil et al., 2018; Evans et al., 2019; Pastor and Baralis, 2019). Most of these work treat the explained models as black boxes, and fit a separate interpretable model on the original input to find rules that mimic the output of the explained model. However, because the interpretable model does not have information about the parameters of the complex model, global explanation is expensive, and the explaining and explained models could fit different curves to arrive to the same output. Sushil et al. (2018) incorporates model gradients in the explanation process to overcome these challenges, but this technique ca"
2021.bionlp-1.3,Q17-1010,0,0.0511043,"embedding representation un from the set of names Cn belonging to the concept. This constraint implies that the dimensionality of the encoder output should be the same as that of the input. However, if the input dimensionality is smaller than the desired output dimensionality, this could be solved using e.g. random projections, which work well for increasing the dimensionality of neural encoder inputs (Wieting and Kiela, 2019). 3 Experiments and discussion 3.1 Pretrained representations We experiment with 3 pretrained name representations. As a first baseline, we use 300-dimensional fastText (Bojanowski et al., 2017) word embeddings which we train on 76M sentences of preprocessed MEDLINE articles released by Hakala et al. (2016). We use average pooling (Shen et al., 2018) to extract a 300-dimensional name representation. As a second baseline, we average the 728-dimensional context-specific token activations of a name extracted from the publicly released BioBERT model (Lee et al., 2019). As state-of-the-art reference, we extract 200dimensional name representations using the publicly released pretrained BNE model with skipgram word embeddings, BNE + SGw ,3 which was trained on approximately 16K synonym sets"
2021.bionlp-1.3,N15-1184,0,0.0749098,"Missing"
2021.bionlp-1.3,2021.eacl-main.208,1,0.840514,"enc(un ) + un (1) 2 where f (n) is the output representation for a biomedical name, un is its pretrained input representation, and enc is the feedforward neural network which transforms the input representation. The averaging step ensures that the encoder architecture learns to update the pretrained input representation rather than create an entirely new representation. This makes our model more robust against overfitting in few-shot learning settings. f (n) = 2.2 Training objectives Our training objectives are based on the state-ofthe-art BNE model by Phan et al. (2019) and the DAN model by Fivez et al. (2021b), which generalizes the BNE model to any hierarchical level of biomedical concepts. Our framework requires a set of concepts C, where each concept c ∈ C contains a set of concept names Cn . The set of biomedical names N contains the union of all those sets of concept names. We propose a simple multi-task training regime which applies two training objectives to each biomedical name n ∈ N . We use cosine distance as distance function d for both objectives. Semantic similarity We enforce embedding similarity between names that are from the same concept by using a siamese triplet loss (Chechik e"
2021.bionlp-1.3,P19-1317,0,0.0599951,"g the impact of conceptual distinctions on robust biomedical name representations. Our code is opensource and available at www.github.com/ clips/fewshot-biomedical-names. 1 F34 Persistent mood disorders F63 Habit and impulse disorders F34.0 Cyclothymia F34.1 Dysthymia F63.0 Pathological gambling F63.1 Pyromania Table 1: Example of how reference names are grouped together within the ICD-10 hierarchy of disorders. compensation for this complexity, such models can be heavily regularized during training, e.g. by tying the output of a nested LSTM to a pooled embedding of its input representations (Phan et al., 2019), or by integrating a finetuned BERT model with sparse lexical representations (Sung et al., 2020). Secondly, encoders are typically trained on finegrained concepts from biomedical ontologies such as the UMLS, i.e., concepts with no child nodes in the ontological directed graph. Small synonym sets of such fine-grained concepts are readily available as training data, and often serve as evaluation data for normalization tasks to which trained encoders can be applied. Lastly, as a result of using fine-grained concepts, vast amounts of biomedical names are needed to model the large collection of f"
2021.bionlp-1.3,2021.louhi-1.6,1,0.837556,"enc(un ) + un (1) 2 where f (n) is the output representation for a biomedical name, un is its pretrained input representation, and enc is the feedforward neural network which transforms the input representation. The averaging step ensures that the encoder architecture learns to update the pretrained input representation rather than create an entirely new representation. This makes our model more robust against overfitting in few-shot learning settings. f (n) = 2.2 Training objectives Our training objectives are based on the state-ofthe-art BNE model by Phan et al. (2019) and the DAN model by Fivez et al. (2021b), which generalizes the BNE model to any hierarchical level of biomedical concepts. Our framework requires a set of concepts C, where each concept c ∈ C contains a set of concept names Cn . The set of biomedical names N contains the union of all those sets of concept names. We propose a simple multi-task training regime which applies two training objectives to each biomedical name n ∈ N . We use cosine distance as distance function d for both objectives. Semantic similarity We enforce embedding similarity between names that are from the same concept by using a siamese triplet loss (Chechik e"
2021.bionlp-1.3,2020.coling-main.577,0,0.079383,"Missing"
2021.bionlp-1.3,W16-2913,0,0.0144203,"ionality of the encoder output should be the same as that of the input. However, if the input dimensionality is smaller than the desired output dimensionality, this could be solved using e.g. random projections, which work well for increasing the dimensionality of neural encoder inputs (Wieting and Kiela, 2019). 3 Experiments and discussion 3.1 Pretrained representations We experiment with 3 pretrained name representations. As a first baseline, we use 300-dimensional fastText (Bojanowski et al., 2017) word embeddings which we train on 76M sentences of preprocessed MEDLINE articles released by Hakala et al. (2016). We use average pooling (Shen et al., 2018) to extract a 300-dimensional name representation. As a second baseline, we average the 728-dimensional context-specific token activations of a name extracted from the publicly released BioBERT model (Lee et al., 2019). As state-of-the-art reference, we extract 200dimensional name representations using the publicly released pretrained BNE model with skipgram word embeddings, BNE + SGw ,3 which was trained on approximately 16K synonym sets of disease Multi-task loss Our multi-task loss sums the losses of the 2 training objectives: L = αLsem + βLground"
2021.bionlp-1.3,P18-1041,0,0.0294722,"ame as that of the input. However, if the input dimensionality is smaller than the desired output dimensionality, this could be solved using e.g. random projections, which work well for increasing the dimensionality of neural encoder inputs (Wieting and Kiela, 2019). 3 Experiments and discussion 3.1 Pretrained representations We experiment with 3 pretrained name representations. As a first baseline, we use 300-dimensional fastText (Bojanowski et al., 2017) word embeddings which we train on 76M sentences of preprocessed MEDLINE articles released by Hakala et al. (2016). We use average pooling (Shen et al., 2018) to extract a 300-dimensional name representation. As a second baseline, we average the 728-dimensional context-specific token activations of a name extracted from the publicly released BioBERT model (Lee et al., 2019). As state-of-the-art reference, we extract 200dimensional name representations using the publicly released pretrained BNE model with skipgram word embeddings, BNE + SGw ,3 which was trained on approximately 16K synonym sets of disease Multi-task loss Our multi-task loss sums the losses of the 2 training objectives: L = αLsem + βLground Training data (4) where α and β are possibl"
2021.bionlp-1.3,N16-1018,0,0.0607893,"Missing"
2021.bionlp-1.5,W19-1909,0,0.0373096,"Missing"
2021.bionlp-1.5,2020.findings-emnlp.292,0,0.0198959,"ing skills of the state-of-the-art models, which we investigate further. Related work Since the BERT models were found to be effective for a wide range of NLP tasks (Devlin et al., 2019), several efforts have been extended towards improving them by more efficient training strategies (Liu et al., 2019; Yang et al., 2019b; Sanh et al., 2019; Lan et al., 2019), training them for different domains (Beltagy et al., 2019; Lee et al., 2019a; Lee and Hsiang, 2019; Chalkidis et al., 2020; Gururangan et al., 2020) and languages (Devlin, 2018; de Vries et al., 2019; Le et al., 2020; Martin et al., 2020; Delobelle et al., 2020; Cañete et al., 2020). Within the clinical domain, different models include the BioBERT models pretrained on PubMed abstracts and PMC full-text articles (Lee et al., 2019a), SciBERT trained on scientific text (Beltagy et al., 2019), clinicalBERT models trained on patient notes from the MIMIC-III corpus (Johnson et al., 2016) (sometimes as a continuation of the BioBERT models) (Alsentzer et al., 2019), and BlueBERT models that also use Pubmed abstracts and MIMIC-III patient notes for training (Peng Use of TF-IDF (Ullman, 2011) and BM25 scores has been frequently explored for evidence retrieval"
2021.bionlp-1.5,D19-1371,0,0.0188165,"ground knowledge integration are therefore more easily transferable to other domains. Talmor et al. (2020) have shown earlier that having explicit access to external information can often improve reasoning skills of the state-of-the-art models, which we investigate further. Related work Since the BERT models were found to be effective for a wide range of NLP tasks (Devlin et al., 2019), several efforts have been extended towards improving them by more efficient training strategies (Liu et al., 2019; Yang et al., 2019b; Sanh et al., 2019; Lan et al., 2019), training them for different domains (Beltagy et al., 2019; Lee et al., 2019a; Lee and Hsiang, 2019; Chalkidis et al., 2020; Gururangan et al., 2020) and languages (Devlin, 2018; de Vries et al., 2019; Le et al., 2020; Martin et al., 2020; Delobelle et al., 2020; Cañete et al., 2020). Within the clinical domain, different models include the BioBERT models pretrained on PubMed abstracts and PMC full-text articles (Lee et al., 2019a), SciBERT trained on scientific text (Beltagy et al., 2019), clinicalBERT models trained on patient notes from the MIMIC-III corpus (Johnson et al., 2016) (sometimes as a continuation of the BioBERT models) (Alsentzer et al"
2021.bionlp-1.5,N19-1423,0,0.189651,"we show that the methods fail due to unreliable knowledge retrieval for complex domain-specific reasoning. We conclude that the task of unsupervised text retrieval to bridge the gap in existing information to facilitate inference is more complex than what the state-of-the-art methods can solve, and warrants extensive research in the future. 1 Introduction Transformers-based neural architectures (Vaswani et al., 2017) currently hold the state-of-the-art performance on several NLP tasks and domains. In the biomedical domain itself, there exist several versions of transformers-based BERT models (Devlin et al., 2019) that have been shown to be successful. However, an analysis of the availability of medical knowledge to these models is incomplete. To facilitate better understanding, in our research, we analyze a sample of errors made by BioBERT (v1.1) 41 Proceedings of the BioNLP 2021 workshop, pages 41–53 June 11, 2021. ©2021 Association for Computational Linguistics relation-based methods, we find that these methods successfully shortlist sentences related to the topic, but it is difficult to then automatically rank the best candidate among the shortlisted options. This best candidate should fill the inf"
2021.bionlp-1.5,2020.emnlp-main.99,0,0.0183987,"dapting entity embeddings learned by models such as BERT by providing additional knowledge from different ontologies that define relations between entities. This can be done either by using templates to convert the relations to text before finetuning embeddings (Weissenborn et al., 2017; Lauscher et al., 2020; Chen et al., 2020), by combining relational information from knowledge graphs with text embeddings (Mihaylov and Frank, 2018; Chen et al., 2018; Zhang et al., 2019; Yang et al., 2019a; Liu et al., 2020), or by jointly learning knowledge graph and textual embeddings (Peters et al., 2019; Feng et al., 2020). These ontologies are either generic like WordNet (Miller, 1995), ConceptNet (Liu and Singh, 2004), and Wikidata (Vrandeˇci´c and Krötzsch, 2014), or more specific to a particular domain like the UMLS (Bodenreider, 2004). An advantage of using ontologies is that the semantics of entities gets encoded in the learned representations, thereby enhancing their effectiveness. However, they are expensive to construct and either are incomplete, or do not exist for specialized domains. Methods that make use of textual corpora for background knowledge integration are therefore more easily transferable"
2021.bionlp-1.5,P17-1171,0,0.0279959,"linical domain, different models include the BioBERT models pretrained on PubMed abstracts and PMC full-text articles (Lee et al., 2019a), SciBERT trained on scientific text (Beltagy et al., 2019), clinicalBERT models trained on patient notes from the MIMIC-III corpus (Johnson et al., 2016) (sometimes as a continuation of the BioBERT models) (Alsentzer et al., 2019), and BlueBERT models that also use Pubmed abstracts and MIMIC-III patient notes for training (Peng Use of TF-IDF (Ullman, 2011) and BM25 scores has been frequently explored for evidence retrieval from Wikipedia for open domain QA (Chen et al., 2017; Wang et al., 2018; Glass et al., 2020). An42 other popular approach includes representation similarity-based evidence retrieval (Lee et al., 2018; Das et al., 2019). Recently, joint training of retriever for span identification and pretraining language models have also been analyzed by Hu et al. (2019); Lee et al. (2019b); Guu et al. (2020). Although the methods extensively explore QA, this line of work has not been explored much for language inference, especially in specialized domains. Existing studies for augmenting medical knowledge for clinical language inference are limited to the use"
2021.bionlp-1.5,2020.acl-main.247,0,0.0374492,"de the BioBERT models pretrained on PubMed abstracts and PMC full-text articles (Lee et al., 2019a), SciBERT trained on scientific text (Beltagy et al., 2019), clinicalBERT models trained on patient notes from the MIMIC-III corpus (Johnson et al., 2016) (sometimes as a continuation of the BioBERT models) (Alsentzer et al., 2019), and BlueBERT models that also use Pubmed abstracts and MIMIC-III patient notes for training (Peng Use of TF-IDF (Ullman, 2011) and BM25 scores has been frequently explored for evidence retrieval from Wikipedia for open domain QA (Chen et al., 2017; Wang et al., 2018; Glass et al., 2020). An42 other popular approach includes representation similarity-based evidence retrieval (Lee et al., 2018; Das et al., 2019). Recently, joint training of retriever for span identification and pretraining language models have also been analyzed by Hu et al. (2019); Lee et al. (2019b); Guu et al. (2020). Although the methods extensively explore QA, this line of work has not been explored much for language inference, especially in specialized domains. Existing studies for augmenting medical knowledge for clinical language inference are limited to the use of UMLS knowledge graph embeddings (Shar"
2021.bionlp-1.5,2020.findings-emnlp.313,0,0.0231916,"owledge of these models and their limitations when in-depth domain knowledge is required for correctly solving a task. Much prior research has explored augmentation of background knowledge in neural models to make them more effective for downstream tasks. Most common approaches include adapting entity embeddings learned by models such as BERT by providing additional knowledge from different ontologies that define relations between entities. This can be done either by using templates to convert the relations to text before finetuning embeddings (Weissenborn et al., 2017; Lauscher et al., 2020; Chen et al., 2020), by combining relational information from knowledge graphs with text embeddings (Mihaylov and Frank, 2018; Chen et al., 2018; Zhang et al., 2019; Yang et al., 2019a; Liu et al., 2020), or by jointly learning knowledge graph and textual embeddings (Peters et al., 2019; Feng et al., 2020). These ontologies are either generic like WordNet (Miller, 1995), ConceptNet (Liu and Singh, 2004), and Wikidata (Vrandeˇci´c and Krötzsch, 2014), or more specific to a particular domain like the UMLS (Bodenreider, 2004). An advantage of using ontologies is that the semantics of entities gets encoded in the le"
2021.bionlp-1.5,2020.acl-main.740,0,0.038122,"Missing"
2021.bionlp-1.5,P18-1224,0,0.02539,"ior research has explored augmentation of background knowledge in neural models to make them more effective for downstream tasks. Most common approaches include adapting entity embeddings learned by models such as BERT by providing additional knowledge from different ontologies that define relations between entities. This can be done either by using templates to convert the relations to text before finetuning embeddings (Weissenborn et al., 2017; Lauscher et al., 2020; Chen et al., 2020), by combining relational information from knowledge graphs with text embeddings (Mihaylov and Frank, 2018; Chen et al., 2018; Zhang et al., 2019; Yang et al., 2019a; Liu et al., 2020), or by jointly learning knowledge graph and textual embeddings (Peters et al., 2019; Feng et al., 2020). These ontologies are either generic like WordNet (Miller, 1995), ConceptNet (Liu and Singh, 2004), and Wikidata (Vrandeˇci´c and Krötzsch, 2014), or more specific to a particular domain like the UMLS (Bodenreider, 2004). An advantage of using ontologies is that the semantics of entities gets encoded in the learned representations, thereby enhancing their effectiveness. However, they are expensive to construct and either are incompl"
2021.bionlp-1.5,W19-5052,0,0.0151256,"2018; Das et al., 2019). Recently, joint training of retriever for span identification and pretraining language models have also been analyzed by Hu et al. (2019); Lee et al. (2019b); Guu et al. (2020). Although the methods extensively explore QA, this line of work has not been explored much for language inference, especially in specialized domains. Existing studies for augmenting medical knowledge for clinical language inference are limited to the use of UMLS knowledge graph embeddings (Sharma et al., 2019), interaction weighting between premise and hypothesis based on distance in the UMLS (Chopra et al., 2019), augmenting clinical concept definitions during representation learning (Lu et al., 2019) and adding domain knowledge by means of pretraining existing models further on different in-domain corpora and closely related tasks (Romanov and Shivade, 2018; Lee et al., 2019a; Alsentzer et al., 2019; Chopra et al., 2019). The closest work to ours is the contemporary work by He et al. (2020) that shows improvements when knowledge from Wikipedia is implicitly integrated by training BERT masked language models to predict disease names and their aspects (such as symptoms, treatments) given the correspond"
2021.bionlp-1.5,2020.emnlp-main.372,0,0.121672,"al knowledge for clinical language inference are limited to the use of UMLS knowledge graph embeddings (Sharma et al., 2019), interaction weighting between premise and hypothesis based on distance in the UMLS (Chopra et al., 2019), augmenting clinical concept definitions during representation learning (Lu et al., 2019) and adding domain knowledge by means of pretraining existing models further on different in-domain corpora and closely related tasks (Romanov and Shivade, 2018; Lee et al., 2019a; Alsentzer et al., 2019; Chopra et al., 2019). The closest work to ours is the contemporary work by He et al. (2020) that shows improvements when knowledge from Wikipedia is implicitly integrated by training BERT masked language models to predict disease names and their aspects (such as symptoms, treatments) given the corresponding context. In our work, we instead explore whether we can augment domain knowledge by dynamically fetching relevant context in an unsupervised manner to improve medical language inference. 3 hypothesis are processed through a dense neural layer to classify the correct class. We then perform manual analysis on a subset of 50 incorrectly classified instances in the development set to"
2021.bionlp-1.5,P19-1221,0,0.025361,"a continuation of the BioBERT models) (Alsentzer et al., 2019), and BlueBERT models that also use Pubmed abstracts and MIMIC-III patient notes for training (Peng Use of TF-IDF (Ullman, 2011) and BM25 scores has been frequently explored for evidence retrieval from Wikipedia for open domain QA (Chen et al., 2017; Wang et al., 2018; Glass et al., 2020). An42 other popular approach includes representation similarity-based evidence retrieval (Lee et al., 2018; Das et al., 2019). Recently, joint training of retriever for span identification and pretraining language models have also been analyzed by Hu et al. (2019); Lee et al. (2019b); Guu et al. (2020). Although the methods extensively explore QA, this line of work has not been explored much for language inference, especially in specialized domains. Existing studies for augmenting medical knowledge for clinical language inference are limited to the use of UMLS knowledge graph embeddings (Sharma et al., 2019), interaction weighting between premise and hypothesis based on distance in the UMLS (Chopra et al., 2019), augmenting clinical concept definitions during representation learning (Lu et al., 2019) and adding domain knowledge by means of pretraining"
2021.bionlp-1.5,D19-1259,0,0.0258305,"ervision simplifies the problem statement in the first case, hence resulting in the documented success. However, the more realistic setup of retrieving the specific context that can fill the information gap between pairs of sentences without supervision is not yet solved. 2 et al., 2019). These models hold promising performance for clinical language processing (Si et al., 2019; Lin et al., 2019) and have become a popular choice for several classification tasks that involve the medical data, spanning tasks such as literature search and question answering for assisting healthcare professionals (Jin et al., 2019; Wang et al., 2020; Möller et al., 2020), as well as patient outcome prediction such as diagnosis prediction (Franz et al., 2020; Rasmy et al., 2020). Despite being a popular choice, little is known about the medical knowledge of these models and their limitations when in-depth domain knowledge is required for correctly solving a task. Much prior research has explored augmentation of background knowledge in neural models to make them more effective for downstream tasks. Most common approaches include adapting entity embeddings learned by models such as BERT by providing additional knowledge f"
2021.bionlp-1.5,W19-1908,0,0.0397366,"passage that mentions the correct entities, they are insufficient for the more complex task of augmenting missing information for pairwise domain knowledge-based reasoning in an unsupervised setup. Entity span-based supervision simplifies the problem statement in the first case, hence resulting in the documented success. However, the more realistic setup of retrieving the specific context that can fill the information gap between pairs of sentences without supervision is not yet solved. 2 et al., 2019). These models hold promising performance for clinical language processing (Si et al., 2019; Lin et al., 2019) and have become a popular choice for several classification tasks that involve the medical data, spanning tasks such as literature search and question answering for assisting healthcare professionals (Jin et al., 2019; Wang et al., 2020; Möller et al., 2020), as well as patient outcome prediction such as diagnosis prediction (Franz et al., 2020; Rasmy et al., 2020). Despite being a popular choice, little is known about the medical knowledge of these models and their limitations when in-depth domain knowledge is required for correctly solving a task. Much prior research has explored augmentati"
2021.bionlp-1.5,2021.ccl-1.108,0,0.0840722,"Missing"
2021.bionlp-1.5,2020.deelio-1.5,0,0.0188386,"wn about the medical knowledge of these models and their limitations when in-depth domain knowledge is required for correctly solving a task. Much prior research has explored augmentation of background knowledge in neural models to make them more effective for downstream tasks. Most common approaches include adapting entity embeddings learned by models such as BERT by providing additional knowledge from different ontologies that define relations between entities. This can be done either by using templates to convert the relations to text before finetuning embeddings (Weissenborn et al., 2017; Lauscher et al., 2020; Chen et al., 2020), by combining relational information from knowledge graphs with text embeddings (Mihaylov and Frank, 2018; Chen et al., 2018; Zhang et al., 2019; Yang et al., 2019a; Liu et al., 2020), or by jointly learning knowledge graph and textual embeddings (Peters et al., 2019; Feng et al., 2020). These ontologies are either generic like WordNet (Miller, 1995), ConceptNet (Liu and Singh, 2004), and Wikidata (Vrandeˇci´c and Krötzsch, 2014), or more specific to a particular domain like the UMLS (Bodenreider, 2004). An advantage of using ontologies is that the semantics of entities ge"
2021.bionlp-1.5,2020.lrec-1.302,0,0.0273664,"l information can often improve reasoning skills of the state-of-the-art models, which we investigate further. Related work Since the BERT models were found to be effective for a wide range of NLP tasks (Devlin et al., 2019), several efforts have been extended towards improving them by more efficient training strategies (Liu et al., 2019; Yang et al., 2019b; Sanh et al., 2019; Lan et al., 2019), training them for different domains (Beltagy et al., 2019; Lee et al., 2019a; Lee and Hsiang, 2019; Chalkidis et al., 2020; Gururangan et al., 2020) and languages (Devlin, 2018; de Vries et al., 2019; Le et al., 2020; Martin et al., 2020; Delobelle et al., 2020; Cañete et al., 2020). Within the clinical domain, different models include the BioBERT models pretrained on PubMed abstracts and PMC full-text articles (Lee et al., 2019a), SciBERT trained on scientific text (Beltagy et al., 2019), clinicalBERT models trained on patient notes from the MIMIC-III corpus (Johnson et al., 2016) (sometimes as a continuation of the BioBERT models) (Alsentzer et al., 2019), and BlueBERT models that also use Pubmed abstracts and MIMIC-III patient notes for training (Peng Use of TF-IDF (Ullman, 2011) and BM25 scores has be"
2021.bionlp-1.5,P18-1076,0,0.0205638,"ly solving a task. Much prior research has explored augmentation of background knowledge in neural models to make them more effective for downstream tasks. Most common approaches include adapting entity embeddings learned by models such as BERT by providing additional knowledge from different ontologies that define relations between entities. This can be done either by using templates to convert the relations to text before finetuning embeddings (Weissenborn et al., 2017; Lauscher et al., 2020; Chen et al., 2020), by combining relational information from knowledge graphs with text embeddings (Mihaylov and Frank, 2018; Chen et al., 2018; Zhang et al., 2019; Yang et al., 2019a; Liu et al., 2020), or by jointly learning knowledge graph and textual embeddings (Peters et al., 2019; Feng et al., 2020). These ontologies are either generic like WordNet (Miller, 1995), ConceptNet (Liu and Singh, 2004), and Wikidata (Vrandeˇci´c and Krötzsch, 2014), or more specific to a particular domain like the UMLS (Bodenreider, 2004). An advantage of using ontologies is that the semantics of entities gets encoded in the learned representations, thereby enhancing their effectiveness. However, they are expensive to construct and"
2021.bionlp-1.5,D18-1053,0,0.023829,"rained on scientific text (Beltagy et al., 2019), clinicalBERT models trained on patient notes from the MIMIC-III corpus (Johnson et al., 2016) (sometimes as a continuation of the BioBERT models) (Alsentzer et al., 2019), and BlueBERT models that also use Pubmed abstracts and MIMIC-III patient notes for training (Peng Use of TF-IDF (Ullman, 2011) and BM25 scores has been frequently explored for evidence retrieval from Wikipedia for open domain QA (Chen et al., 2017; Wang et al., 2018; Glass et al., 2020). An42 other popular approach includes representation similarity-based evidence retrieval (Lee et al., 2018; Das et al., 2019). Recently, joint training of retriever for span identification and pretraining language models have also been analyzed by Hu et al. (2019); Lee et al. (2019b); Guu et al. (2020). Although the methods extensively explore QA, this line of work has not been explored much for language inference, especially in specialized domains. Existing studies for augmenting medical knowledge for clinical language inference are limited to the use of UMLS knowledge graph embeddings (Sharma et al., 2019), interaction weighting between premise and hypothesis based on distance in the UMLS (Chopr"
2021.bionlp-1.5,2020.nlpcovid19-acl.18,0,0.0340232,"ment in the first case, hence resulting in the documented success. However, the more realistic setup of retrieving the specific context that can fill the information gap between pairs of sentences without supervision is not yet solved. 2 et al., 2019). These models hold promising performance for clinical language processing (Si et al., 2019; Lin et al., 2019) and have become a popular choice for several classification tasks that involve the medical data, spanning tasks such as literature search and question answering for assisting healthcare professionals (Jin et al., 2019; Wang et al., 2020; Möller et al., 2020), as well as patient outcome prediction such as diagnosis prediction (Franz et al., 2020; Rasmy et al., 2020). Despite being a popular choice, little is known about the medical knowledge of these models and their limitations when in-depth domain knowledge is required for correctly solving a task. Much prior research has explored augmentation of background knowledge in neural models to make them more effective for downstream tasks. Most common approaches include adapting entity embeddings learned by models such as BERT by providing additional knowledge from different ontologies that define rela"
2021.bionlp-1.5,W19-5034,0,0.0282767,"gl=US 43 are fairly simple, they have been shown to be effective for several open domain QA tasks (Lee et al., 2019b). Our goal is to investigate whether these simple methods are also effective at more complex information retrieval in our setup. To this end, we construct a query from premise and hypothesis by retaining only the lemmas that are a part of infrequent medical entities, and then use the best match 25 (BM25) algorithm (Robertson and Zaragoza, 2009) to find the most relevant sentences. As the first step, we recognize premise and hypothesis medical entities with the help of Scispacy (Neumann et al., 2019). We lemmatize these entities and retain only those lemmas that occur less than a thousand times in the external corpus2 . These lemmas jointly form the query. We first rank the documents in the external corpora according to their BM25 scores to retain the top 10 documents. The query is then used again to find the best matching sentences from these documents. Due to the manner in which the MedNLI data has been annotated, premise is longer and more varied than the hypothesis. Hence, premise entities often dominate the BM25 retrieval at the cost of hypothesis entities. To overcome this, we prune"
2021.bionlp-1.5,W19-5006,0,0.0925006,"tion of the BioBERT models) (Alsentzer et al., 2019), and BlueBERT models that also use Pubmed abstracts and MIMIC-III patient notes for training (Peng Use of TF-IDF (Ullman, 2011) and BM25 scores has been frequently explored for evidence retrieval from Wikipedia for open domain QA (Chen et al., 2017; Wang et al., 2018; Glass et al., 2020). An42 other popular approach includes representation similarity-based evidence retrieval (Lee et al., 2018; Das et al., 2019). Recently, joint training of retriever for span identification and pretraining language models have also been analyzed by Hu et al. (2019); Lee et al. (2019b); Guu et al. (2020). Although the methods extensively explore QA, this line of work has not been explored much for language inference, especially in specialized domains. Existing studies for augmenting medical knowledge for clinical language inference are limited to the use of UMLS knowledge graph embeddings (Sharma et al., 2019), interaction weighting between premise and hypothesis based on distance in the UMLS (Chopra et al., 2019), augmenting clinical concept definitions during representation learning (Lu et al., 2019) and adding domain knowledge by means of pretraining"
2021.bionlp-1.5,D19-1005,0,0.0405224,"Missing"
2021.bionlp-1.5,D18-1187,0,0.0414502,"Missing"
2021.bionlp-1.5,P19-1226,0,0.0216814,"of background knowledge in neural models to make them more effective for downstream tasks. Most common approaches include adapting entity embeddings learned by models such as BERT by providing additional knowledge from different ontologies that define relations between entities. This can be done either by using templates to convert the relations to text before finetuning embeddings (Weissenborn et al., 2017; Lauscher et al., 2020; Chen et al., 2020), by combining relational information from knowledge graphs with text embeddings (Mihaylov and Frank, 2018; Chen et al., 2018; Zhang et al., 2019; Yang et al., 2019a; Liu et al., 2020), or by jointly learning knowledge graph and textual embeddings (Peters et al., 2019; Feng et al., 2020). These ontologies are either generic like WordNet (Miller, 1995), ConceptNet (Liu and Singh, 2004), and Wikidata (Vrandeˇci´c and Krötzsch, 2014), or more specific to a particular domain like the UMLS (Bodenreider, 2004). An advantage of using ontologies is that the semantics of entities gets encoded in the learned representations, thereby enhancing their effectiveness. However, they are expensive to construct and either are incomplete, or do not exist for specialized do"
2021.bionlp-1.5,D19-1631,0,0.0204879,"020). An42 other popular approach includes representation similarity-based evidence retrieval (Lee et al., 2018; Das et al., 2019). Recently, joint training of retriever for span identification and pretraining language models have also been analyzed by Hu et al. (2019); Lee et al. (2019b); Guu et al. (2020). Although the methods extensively explore QA, this line of work has not been explored much for language inference, especially in specialized domains. Existing studies for augmenting medical knowledge for clinical language inference are limited to the use of UMLS knowledge graph embeddings (Sharma et al., 2019), interaction weighting between premise and hypothesis based on distance in the UMLS (Chopra et al., 2019), augmenting clinical concept definitions during representation learning (Lu et al., 2019) and adding domain knowledge by means of pretraining existing models further on different in-domain corpora and closely related tasks (Romanov and Shivade, 2018; Lee et al., 2019a; Alsentzer et al., 2019; Chopra et al., 2019). The closest work to ours is the contemporary work by He et al. (2020) that shows improvements when knowledge from Wikipedia is implicitly integrated by training BERT masked lang"
2021.bionlp-1.5,P19-1139,0,0.0209283,"plored augmentation of background knowledge in neural models to make them more effective for downstream tasks. Most common approaches include adapting entity embeddings learned by models such as BERT by providing additional knowledge from different ontologies that define relations between entities. This can be done either by using templates to convert the relations to text before finetuning embeddings (Weissenborn et al., 2017; Lauscher et al., 2020; Chen et al., 2020), by combining relational information from knowledge graphs with text embeddings (Mihaylov and Frank, 2018; Chen et al., 2018; Zhang et al., 2019; Yang et al., 2019a; Liu et al., 2020), or by jointly learning knowledge graph and textual embeddings (Peters et al., 2019; Feng et al., 2020). These ontologies are either generic like WordNet (Miller, 1995), ConceptNet (Liu and Singh, 2004), and Wikidata (Vrandeˇci´c and Krötzsch, 2014), or more specific to a particular domain like the UMLS (Bodenreider, 2004). An advantage of using ontologies is that the semantics of entities gets encoded in the learned representations, thereby enhancing their effectiveness. However, they are expensive to construct and either are incomplete, or do not exist"
2021.eacl-main.208,Q17-1010,0,0.0932159,"e a siamese triplet loss (Chechik et al., 2010). This loss forces the encoding of a biomedical name to be closer to the encoding of a true synonym than that of a negative sample name, within a specified (possibly tuned) margin: f (n) = enc(un ) where Nt is the bag of tokens from a name, ut is a pretrained word embedding of a token, un is a name embedding created by averaging all the pretrained word embeddings of all tokens, and enc is a feedforward neural network with Rectified Linear Unit (ReLU) as non-linear activation function. As pretrained word embeddings we use 300-dimensional fastText (Bojanowski et al., 2017) representations which we train on 76M sentences of preprocessed MEDLINE articles released by Hakala et al. (2016). This fastText model also allows for constructing word embeddings for out-ofvocabulary tokens by composing character n-gram embeddings. 3.2 Training objectives Our training objectives optimize the mapping between an encoded name f (n) and the pretrained embedding of its concept up . While in principle any type of pretrained concept embeddings could be used, our experiments use concept embeddings pos = d(f (CCA(n)), f (CCA(npos ))) neg = d(f (CCA(n)), f (CCA(nneg ))) (3) Lsyn = max"
2021.eacl-main.208,W16-2913,0,0.263176,"e encoding of a true synonym than that of a negative sample name, within a specified (possibly tuned) margin: f (n) = enc(un ) where Nt is the bag of tokens from a name, ut is a pretrained word embedding of a token, un is a name embedding created by averaging all the pretrained word embeddings of all tokens, and enc is a feedforward neural network with Rectified Linear Unit (ReLU) as non-linear activation function. As pretrained word embeddings we use 300-dimensional fastText (Bojanowski et al., 2017) representations which we train on 76M sentences of preprocessed MEDLINE articles released by Hakala et al. (2016). This fastText model also allows for constructing word embeddings for out-ofvocabulary tokens by composing character n-gram embeddings. 3.2 Training objectives Our training objectives optimize the mapping between an encoded name f (n) and the pretrained embedding of its concept up . While in principle any type of pretrained concept embeddings could be used, our experiments use concept embeddings pos = d(f (CCA(n)), f (CCA(npos ))) neg = d(f (CCA(n)), f (CCA(nneg ))) (3) Lsyn = max(pos − neg + margin, 0) where CCA denotes that the pretrained name embedding used as input for the DAN has first b"
2021.eacl-main.208,P15-1162,0,0.0381162,"Missing"
2021.eacl-main.208,D18-1221,0,0.254369,"eralizes across different biomedical subdomains and corpora. To date, the most effective approaches have applied some form of conceptual grounding: minimizing the distance between on the one hand representations of names, and on the other hand pretrained embeddings of their concept identifiers. These concept embeddings are supposed to reflect domain-specific semantics, and are constructed using a variety of different techniques, including distributional similarity of graph relations and distributional similarity of textual occurrences in large-scale free-text, as well as combinations thereof (Kartsaklis et al., 2018; Phan et al., 2019). While knowledge graph embeddings of biomedical concepts can encode a variety of semantic relations, Kartsaklis et al. (2018) show that such graph embeddings need to incorporate textual features to make them effective targets for conceptual grounding. Such features help to translate textual representations of names to the topology of the concept embedding space, which otherwise reflects only ontological information. In other words, concept embeddings are mostly useful targets for grounding to the extent that name representations can be efficiently mapped to them by the enc"
2021.eacl-main.208,P19-1317,0,0.102608,"ing for downstream applications. Effective dense representation of these biomedical names has been mainly investigated through the normalization task of disorder linking, which consists of matching disease mentions to reference terms of concept identifiers in ontologies (e.g. matching the mention myocardial depression to the reference term Myocardial Dysfunction) (Leaman et al., 2015). While past research has gradually shifted its focus from lexical representations (Leaman et al., 2013; D’Souza and Ng, 2015) to dense distributed representations (Limsopatham and Collier, 2016; Li et al., 2017; Phan et al., 2019; Sung et al., 2020), encoders are still typically optimized towards normalization tasks, which are focused on resolving word-level analogies between synonymous biomedical names. Recent research has focused more explicitly on encoding domain-specific biomedical semantics by training biomedical name representations that are robust, i.e., reflecting the synonymy and semantic relatedness of names by their closeness in the embedding space, preferably in a consistent way 2440 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2440–2450"
2021.eacl-main.208,P16-1096,0,0.0283521,"itic pain vs. pain breathing), which is challenging for downstream applications. Effective dense representation of these biomedical names has been mainly investigated through the normalization task of disorder linking, which consists of matching disease mentions to reference terms of concept identifiers in ontologies (e.g. matching the mention myocardial depression to the reference term Myocardial Dysfunction) (Leaman et al., 2015). While past research has gradually shifted its focus from lexical representations (Leaman et al., 2013; D’Souza and Ng, 2015) to dense distributed representations (Limsopatham and Collier, 2016; Li et al., 2017; Phan et al., 2019; Sung et al., 2020), encoders are still typically optimized towards normalization tasks, which are focused on resolving word-level analogies between synonymous biomedical names. Recent research has focused more explicitly on encoding domain-specific biomedical semantics by training biomedical name representations that are robust, i.e., reflecting the synonymy and semantic relatedness of names by their closeness in the embedding space, preferably in a consistent way 2440 Proceedings of the 16th Conference of the European Chapter of the Association for Comput"
2021.eacl-main.208,P18-1041,0,0.43262,"ee et al., 2019) representations and uses them in tandem with lexical TF-IDF representations. While past research has explicitly investigated the role of various training objectives, even jointly in multi-task training regimes, the specific impact of encoder architectures has not received much attention or comparison. Averaging networks Research on sentence embeddings and paraphrasing has consistently found that simple encoding procedures such as averaging of word embeddings can rival or even outperform complex neural architectures on tasks for which those are finetuned (Wieting et al., 2016; Shen et al., 2018; Wieting and Kiela, 2019). Moreover, research on Deep Averaging Networks (Iyyer et al., 2015) has found that feedforward neural networks that use averaged word embeddings as input can be tuned to textual classification tasks such as sentiment analysis if the network is sufficiently large 2441 and/or deep. This way, small differences in the input can be magnified by the network where relevant. Prototypical networks While successful approaches to few-shot learning such as Matching Networks (Vinyals et al., 2016) optimize representation models on the level of single instances, follow-up work has"
2021.eacl-main.208,N18-1049,0,0.0178797,"r the disorder data, since it was trained on at least part of that data, and we want to avoid that data leakage affects the fairness of the model comparisons. Baselines As baseline encoder we use the 300dimensional fastText name embeddings which are used as input for the DAN (defined in Equation 1 in Section 3.1). This encoder is an example of a Simple Word-Embedding Model (SWEM) with average pooling, which has been proven to be a strong baseline for various NLP tasks (Shen et al., 2018). We also include two other pretrained baselines among our comparison of encoders: 600dimensional Sent2Vec (Pagliardini et al., 2018) 4 https://github.com/minhcp/BNE embeddings with word unigram and bigram representations, trained on the same MEDLINE data as our fastText embeddings; and averaged 728dimensional context-specific token activations extracted from the publicly released BioBERT model (Lee et al., 2019). 5.3 Training details We fit the CCA for the linear constraint using all training names and their corresponding concept prototypes constructed from the same training names. The encoder architectures of our own DAN model and the BNE reference model are implemented in PyTorch (Paszke et al., 2019). Both the input and"
2021.eacl-main.208,2020.acl-main.335,0,0.0740647,"applications. Effective dense representation of these biomedical names has been mainly investigated through the normalization task of disorder linking, which consists of matching disease mentions to reference terms of concept identifiers in ontologies (e.g. matching the mention myocardial depression to the reference term Myocardial Dysfunction) (Leaman et al., 2015). While past research has gradually shifted its focus from lexical representations (Leaman et al., 2013; D’Souza and Ng, 2015) to dense distributed representations (Limsopatham and Collier, 2016; Li et al., 2017; Phan et al., 2019; Sung et al., 2020), encoders are still typically optimized towards normalization tasks, which are focused on resolving word-level analogies between synonymous biomedical names. Recent research has focused more explicitly on encoding domain-specific biomedical semantics by training biomedical name representations that are robust, i.e., reflecting the synonymy and semantic relatedness of names by their closeness in the embedding space, preferably in a consistent way 2440 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2440–2450 April 19 - 23, 2021"
2021.emnlp-main.294,2020.acl-main.740,0,0.0615965,"Missing"
2021.emnlp-main.294,D14-1058,0,0.196088,"erty value red. Solving math word problems automatically is an active area of research in natural language processing. It poses interesting challenges in extracting relevant entities and quantities from a concise textual narrative, and reasoning about the relationships between them to answer the question posed in the text. While several approaches for different areas of mathematics have been proposed in the past, the majority have focused on arithmetic and algebraic problems where the mathematical representation is specified as a simple equation system that can be used with a standard solver (Hosseini et al., 2014; Roy and Roth, 2015; Kushman et al., 2014; Koncel-Kedziorski et al., 2015; Upadhyay and Chang, 2015; Huang et al., 2016; Wang et al., 2017; Ling et al., 2017; Amini et al., 2019; Miao et al., 2020). This paper focuses on a class of word problems that have not received much attenAutomatically constructing such a representation tion: those about probability that may be found, from the natural language text is challenging for for example, in introductory textbooks for discrete several reasons. The text may not explicitly state mathematics. whether sampling is with or without replacement, 3627 Pr"
2021.emnlp-main.294,P16-1084,0,0.0157116,"oses interesting challenges in extracting relevant entities and quantities from a concise textual narrative, and reasoning about the relationships between them to answer the question posed in the text. While several approaches for different areas of mathematics have been proposed in the past, the majority have focused on arithmetic and algebraic problems where the mathematical representation is specified as a simple equation system that can be used with a standard solver (Hosseini et al., 2014; Roy and Roth, 2015; Kushman et al., 2014; Koncel-Kedziorski et al., 2015; Upadhyay and Chang, 2015; Huang et al., 2016; Wang et al., 2017; Ling et al., 2017; Amini et al., 2019; Miao et al., 2020). This paper focuses on a class of word problems that have not received much attenAutomatically constructing such a representation tion: those about probability that may be found, from the natural language text is challenging for for example, in introductory textbooks for discrete several reasons. The text may not explicitly state mathematics. whether sampling is with or without replacement, 3627 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3627–3640 c November 7–11, 2"
2021.emnlp-main.294,P17-1089,0,0.020016,"chieves a solution by constructing a combinatorial formula in a stepwise manner. 2.3 Mapping to other types of executable representations There are similarities between our work and the existing NLP work on mapping text to other kinds of executable representations. In text-to-SQL mapping, the goal is to encode the database relations in an accessible way for the semantic parser, and to model the alignment between database columns and their mentions in a given query, which is analogous to our task. Seq2seq models with attention have been widely studied for this task as well (Zhong et al., 2017; Iyer et al., 2017). In addition to SQL representations, more general knowledge base representations (Zettlemoyer and Collins, 2005; Yih et al., 2015) and general-purpose source code generation for programming languages like Python and Java (Ling et al., 2016; Yin and Neubig, 2017) are conceptually similar as well. 3 Approaches to Solving Probability Word Problems Our ultimate objective is to address the following: Given: A natural language description of a word problem involving computing a probability. Do: Produce the correct answer to the problem. We explored multiple ways to instantiate each approach, which"
2021.emnlp-main.294,Q15-1042,0,0.0730224,"tive area of research in natural language processing. It poses interesting challenges in extracting relevant entities and quantities from a concise textual narrative, and reasoning about the relationships between them to answer the question posed in the text. While several approaches for different areas of mathematics have been proposed in the past, the majority have focused on arithmetic and algebraic problems where the mathematical representation is specified as a simple equation system that can be used with a standard solver (Hosseini et al., 2014; Roy and Roth, 2015; Kushman et al., 2014; Koncel-Kedziorski et al., 2015; Upadhyay and Chang, 2015; Huang et al., 2016; Wang et al., 2017; Ling et al., 2017; Amini et al., 2019; Miao et al., 2020). This paper focuses on a class of word problems that have not received much attenAutomatically constructing such a representation tion: those about probability that may be found, from the natural language text is challenging for for example, in introductory textbooks for discrete several reasons. The text may not explicitly state mathematics. whether sampling is with or without replacement, 3627 Proceedings of the 2021 Conference on Empirical Methods in Natural Language"
2021.emnlp-main.294,P14-1026,0,0.185324,"automatically is an active area of research in natural language processing. It poses interesting challenges in extracting relevant entities and quantities from a concise textual narrative, and reasoning about the relationships between them to answer the question posed in the text. While several approaches for different areas of mathematics have been proposed in the past, the majority have focused on arithmetic and algebraic problems where the mathematical representation is specified as a simple equation system that can be used with a standard solver (Hosseini et al., 2014; Roy and Roth, 2015; Kushman et al., 2014; Koncel-Kedziorski et al., 2015; Upadhyay and Chang, 2015; Huang et al., 2016; Wang et al., 2017; Ling et al., 2017; Amini et al., 2019; Miao et al., 2020). This paper focuses on a class of word problems that have not received much attenAutomatically constructing such a representation tion: those about probability that may be found, from the natural language text is challenging for for example, in introductory textbooks for discrete several reasons. The text may not explicitly state mathematics. whether sampling is with or without replacement, 3627 Proceedings of the 2021 Conference on Empiri"
2021.emnlp-main.294,N16-3014,0,0.013805,"respectively, with or without replacement n elements from a group defines a constraint holding on a set created by a take/take wr action specifies a question in the problem; probability of an observable property of a set constraints on the properties of the objects in a set and, or, not take, take wr observe probability at least, exactly, all, some, . . . Table 2: Summary of predicates used in the N LP 4 PLP language. Among the works that adhere to the conceptual split between the text-to-representation mapping and solver application, like in our case, we can point out the following examples. Liang et al. (2016) present a hybrid rule-based and ML approach where the problem texts are first mapped to a linguistic representation that highlights the syntactic relationships between words, then a logic representation is built that is passed to the inference engine, which carries out math operations to obtain the answer. A similar decomposition of the problem has been previously studied in Matsuzaki et al. (2013), where the problems are translated to logical forms using Combinatory Categorial Grammar and Discourse Representation Structure formalisms, then rewritten into the input language of the solver. 2.2"
2021.emnlp-main.294,P16-1057,0,0.0583274,"Missing"
2021.emnlp-main.294,P17-1015,0,0.0730581,"ng relevant entities and quantities from a concise textual narrative, and reasoning about the relationships between them to answer the question posed in the text. While several approaches for different areas of mathematics have been proposed in the past, the majority have focused on arithmetic and algebraic problems where the mathematical representation is specified as a simple equation system that can be used with a standard solver (Hosseini et al., 2014; Roy and Roth, 2015; Kushman et al., 2014; Koncel-Kedziorski et al., 2015; Upadhyay and Chang, 2015; Huang et al., 2016; Wang et al., 2017; Ling et al., 2017; Amini et al., 2019; Miao et al., 2020). This paper focuses on a class of word problems that have not received much attenAutomatically constructing such a representation tion: those about probability that may be found, from the natural language text is challenging for for example, in introductory textbooks for discrete several reasons. The text may not explicitly state mathematics. whether sampling is with or without replacement, 3627 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3627–3640 c November 7–11, 2021. 2021 Association for Computationa"
2021.emnlp-main.294,P14-5010,0,0.0026661,"ing contextualised representations from language models. BiLSTM The baseline approach is a 400dimensional, single-layer bidirectional LSTM encoder-decoder. The encoding is provided by processing the problem text as a sequence of tokens. The decoder then must generate the question’s representation in Dries et al.’s language as a linear sequence. The solver will use the output as is. Hence, the model must generate the structural parts of the representation such as punctuation and parentheses. We augment the textual input with three types of additional features obtained with the CoreNLP toolkit (Manning et al., 2014): part-of-speech tags of the entire problem sequence; numerical entities recognised by the NER component; and dependency label of the relation connecting the word to its parent. We consider including three types of contextualised representations to the encoder: FrozenEncoder uses the contextualised representations of the input text, but randomly initialises the BiLSTM encoder of our model and freezes it during training. Prior research indicates that using such random encoders can lead to performance that is robust and sometimes even competitive with finetuned encoders, since this approach maxi"
2021.emnlp-main.294,I13-1009,0,0.0332354,"he N LP 4 PLP language. Among the works that adhere to the conceptual split between the text-to-representation mapping and solver application, like in our case, we can point out the following examples. Liang et al. (2016) present a hybrid rule-based and ML approach where the problem texts are first mapped to a linguistic representation that highlights the syntactic relationships between words, then a logic representation is built that is passed to the inference engine, which carries out math operations to obtain the answer. A similar decomposition of the problem has been previously studied in Matsuzaki et al. (2013), where the problems are translated to logical forms using Combinatory Categorial Grammar and Discourse Representation Structure formalisms, then rewritten into the input language of the solver. 2.2 Probability word problems problem. There are two fundamental differences with the N LP 4 PLP dataset. First, M ATH QA considers multiple choice questions, whereas N LP 4 PLP questions ask for a number. Second, M ATH QA annotations describe how to solve the problem, whereas N LP 4 PLP describe what the problem is. The set of predicates in M ATH QA defines the basic arithmetic operations as well as p"
2021.emnlp-main.294,2020.acl-main.92,0,0.0220213,"m a concise textual narrative, and reasoning about the relationships between them to answer the question posed in the text. While several approaches for different areas of mathematics have been proposed in the past, the majority have focused on arithmetic and algebraic problems where the mathematical representation is specified as a simple equation system that can be used with a standard solver (Hosseini et al., 2014; Roy and Roth, 2015; Kushman et al., 2014; Koncel-Kedziorski et al., 2015; Upadhyay and Chang, 2015; Huang et al., 2016; Wang et al., 2017; Ling et al., 2017; Amini et al., 2019; Miao et al., 2020). This paper focuses on a class of word problems that have not received much attenAutomatically constructing such a representation tion: those about probability that may be found, from the natural language text is challenging for for example, in introductory textbooks for discrete several reasons. The text may not explicitly state mathematics. whether sampling is with or without replacement, 3627 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3627–3640 c November 7–11, 2021. 2021 Association for Computational Linguistics and this information may n"
2021.emnlp-main.294,D15-1202,0,0.113282,"math word problems automatically is an active area of research in natural language processing. It poses interesting challenges in extracting relevant entities and quantities from a concise textual narrative, and reasoning about the relationships between them to answer the question posed in the text. While several approaches for different areas of mathematics have been proposed in the past, the majority have focused on arithmetic and algebraic problems where the mathematical representation is specified as a simple equation system that can be used with a standard solver (Hosseini et al., 2014; Roy and Roth, 2015; Kushman et al., 2014; Koncel-Kedziorski et al., 2015; Upadhyay and Chang, 2015; Huang et al., 2016; Wang et al., 2017; Ling et al., 2017; Amini et al., 2019; Miao et al., 2020). This paper focuses on a class of word problems that have not received much attenAutomatically constructing such a representation tion: those about probability that may be found, from the natural language text is challenging for for example, in introductory textbooks for discrete several reasons. The text may not explicitly state mathematics. whether sampling is with or without replacement, 3627 Proceedings of the 202"
2021.emnlp-main.294,D17-1088,0,0.0401966,"Missing"
2021.emnlp-main.294,P15-1128,0,0.0389743,"tions There are similarities between our work and the existing NLP work on mapping text to other kinds of executable representations. In text-to-SQL mapping, the goal is to encode the database relations in an accessible way for the semantic parser, and to model the alignment between database columns and their mentions in a given query, which is analogous to our task. Seq2seq models with attention have been widely studied for this task as well (Zhong et al., 2017; Iyer et al., 2017). In addition to SQL representations, more general knowledge base representations (Zettlemoyer and Collins, 2005; Yih et al., 2015) and general-purpose source code generation for programming languages like Python and Java (Ling et al., 2016; Yin and Neubig, 2017) are conceptually similar as well. 3 Approaches to Solving Probability Word Problems Our ultimate objective is to address the following: Given: A natural language description of a word problem involving computing a probability. Do: Produce the correct answer to the problem. We explored multiple ways to instantiate each approach, which we will now describe in more detail. 3.1 Two-step approach We consider a baseline seq2seq approach and then explore how to augment"
2021.emnlp-main.294,P17-1041,0,0.0232858,"ions. In text-to-SQL mapping, the goal is to encode the database relations in an accessible way for the semantic parser, and to model the alignment between database columns and their mentions in a given query, which is analogous to our task. Seq2seq models with attention have been widely studied for this task as well (Zhong et al., 2017; Iyer et al., 2017). In addition to SQL representations, more general knowledge base representations (Zettlemoyer and Collins, 2005; Yih et al., 2015) and general-purpose source code generation for programming languages like Python and Java (Ling et al., 2016; Yin and Neubig, 2017) are conceptually similar as well. 3 Approaches to Solving Probability Word Problems Our ultimate objective is to address the following: Given: A natural language description of a word problem involving computing a probability. Do: Produce the correct answer to the problem. We explored multiple ways to instantiate each approach, which we will now describe in more detail. 3.1 Two-step approach We consider a baseline seq2seq approach and then explore how to augment it using contextualised representations from language models. BiLSTM The baseline approach is a 400dimensional, single-layer bidirec"
2021.louhi-1.6,W16-2913,0,0.186933,"yyer et al., 2015) which extracts a fixed-size representation for an input name n: un = 1 X ut |Nt | t∈Nt (1) f (n) = enc(un ) where Nt is the bag of tokens from a name, ut is a pretrained word embedding of a token, un is a name embedding created by averaging all the pretrained word embeddings of all tokens, and enc is a feedforward neural network with Rectified Linear Unit (ReLU) as non-linear activation function. As pretrained word embeddings we use 300-dimensional fastText (Bojanowski et al., 2017) representations which we train on 76M sentences of preprocessed MEDLINE articles released by Hakala et al. (2016). This fastText model also allows for constructing word embeddings for out-ofvocabulary tokens by composing character n-gram embeddings. Related work While context-dependent self-supervised representations usually outperform other text representations on a variety of BioNLP problems, such as semantic similarity and question answering, there is no single embedding model for biomedical and clinical texts that is consistently superior and thus can serve as a generally suitable bio-encoder (Tawfik and Spruit, 2020). To this date, the BNE model by Phan et al. (2019) is the most prominent attempt at"
2021.louhi-1.6,P15-1162,0,0.0555543,"Missing"
2021.louhi-1.6,2020.louhi-1.3,0,0.0822733,"Missing"
2021.louhi-1.6,D18-1221,0,0.0181077,"P problems, such as semantic similarity and question answering, there is no single embedding model for biomedical and clinical texts that is consistently superior and thus can serve as a generally suitable bio-encoder (Tawfik and Spruit, 2020). To this date, the BNE model by Phan et al. (2019) is the most prominent attempt at developing a supervised resource for encoding biomedical names. It uses a multi-task training regime in which it combines objectives from different aspects of deep representation learning, such as a contrastive loss (Le-Khac et al., 2020), conceptual grounding (see e.g. (Kartsaklis et al., 2018)), and explicit regularization of the learned representations (e.g. used by Vuli´c and Mrkˇsi´c (2018)). Our modifications to the original BNE model are informed by such literature. Our application of a Deep Averaging Network (DAN) (Iyyer et al., 2015) is inspired by a recent subfield of NLP research which has emphasized the effectiveness of random encoders (Wieting and 3.2 Training objectives Our proposed approach is a simple modification of the multi-task training regime of the BNE model. We use cosine distance as distance function d for all three training objectives. Semantic similarity The"
2021.louhi-1.6,Q17-1010,0,0.0642337,"ooling (Shen et al., 2018). 3 3.1 Encoding model Encoder architecture Our encoder is a Deep Averaging Network (DAN) (Iyyer et al., 2015) which extracts a fixed-size representation for an input name n: un = 1 X ut |Nt | t∈Nt (1) f (n) = enc(un ) where Nt is the bag of tokens from a name, ut is a pretrained word embedding of a token, un is a name embedding created by averaging all the pretrained word embeddings of all tokens, and enc is a feedforward neural network with Rectified Linear Unit (ReLU) as non-linear activation function. As pretrained word embeddings we use 300-dimensional fastText (Bojanowski et al., 2017) representations which we train on 76M sentences of preprocessed MEDLINE articles released by Hakala et al. (2016). This fastText model also allows for constructing word embeddings for out-ofvocabulary tokens by composing character n-gram embeddings. Related work While context-dependent self-supervised representations usually outperform other text representations on a variety of BioNLP problems, such as semantic similarity and question answering, there is no single embedding model for biomedical and clinical texts that is consistently superior and thus can serve as a generally suitable bio-enc"
2021.louhi-1.6,W19-5032,0,0.0278202,"s that are semantically related, we use a siamese triplet loss (Chechik et al., 2010). This loss forces the encoding of a biomedical name f (n) to be closer to the encoding of a semantically similar name f (npos ) than that of an encoded negative sample name f (nneg ), within a 51 specified (possibly tuned) margin: trained name representation and the pretrained concept representation: pos = d(f (n), f (npos )) neg = d(f (n), f (nneg )) (2) Lground Lsem = max(pos − neg + margin, 0) L = αLsem + βLcont + γLground 4 Data and task setup 4.1 Extracting hierarchical data Following previous research (Kotitsas et al., 2019; Camacho-Collados et al., 2018), we use IS-A relations between concepts from the SNOMEDCT1 ontology as biomedical hypo-hypernymy relations. For direct comparison with the publicly released BNE embeddings, which were trained on all disorder concepts of SNOMED-CT, we use the 2018AB release of the UMLS2 to extract only those SNOMED-CT concepts which are included in the semantic group of disorders3 , and extract their reference terms as disorder names. While the resulting directed graph should be acyclic, there are many inconsistencies, which we resolve by removing all cyclic edges, similar to th"
2021.louhi-1.6,W18-1708,0,0.0123152,"eaded lizard bite. By clustering similar names together with other bite wounds during training, the DAN model has learned to recognize the test mention as a bite wound. The BNE has failed to do so. The effectiveness of our unsupervised method using only cosine similarity contrasts with earlier approaches which explicitly require more than cosine similarity to properly work. For example, Vuli´c and Mrkˇsi´c (2018) use vector norms to encode hierarchical hypernymic relations, while other research into hypernymy even requires other geometric spaces than Euclidean space, such as hyperbolic space (Dhingra et al., 2018). Our results can indicate that cosine similarity in Euclidean space still shows potential for encoding these hierarchical 55 C0560169 3 poisoning caused by mexican beaded lizard bite bite wound / bite wound (disorder) Subgraph Level Test mention Matching hypernyms Top 5 ranking DAN Level 1 bite wound (disorder) bite wound open traumatic dislocation of hip, unspecified open traumatic dislocation of hip, unspecified (disorder) open dislocation of phalanx of foot (disorder) BNE infestation caused by fly larvae (disorder) fly larva infestation infestation caused by fly larvae infestation by fly l"
2021.louhi-1.6,N18-1103,0,0.0601972,"Missing"
2021.louhi-1.6,2020.emnlp-main.502,0,0.019557,".73 0.74 0.65 0.62 0.71 1 0.48 0.45 0.41 0.39 0.34 0.28 0.42 C0560169 2 3 0.54 0.52 0.58 0.56 0.54 0.58 0.53 0.58 0.47 0.49 0.41 0.46 0.48 0.57 4 0.63 0.67 0.70 0.74 0.63 0.58 0.70 1 0.52 0.50 0.48 0.46 0.38 0.39 0.49 C0263661 2 3 0.57 0.55 0.60 0.57 0.57 0.62 0.54 0.59 0.45 0.50 0.47 0.48 0.49 0.54 4 0.62 0.63 0.67 0.71 0.59 0.59 0.68 Table 5: Test performance for unsupervised hypernym detection per level, as measured by MRR. The highest score per level of each subgraph is denoted in bold; the second highest score is underlined. here compared to fastText. This is in line with the findings by Yu et al. (2020), who report that BERT does not yield considerable improvement for hypernymy detection in their experiments. It also puts into perspective to what extent we can expect higher-level semantics to be encoded solely through self-supervised methods. relations given the right training objectives. 5.4 Semantic relatedness benchmarks We also evaluate our name encoders on two biomedical benchmarks of semantic similarity, which allow to compare cosine similarity between name embeddings with human judgments of relatedness. MayoSRS (Pakhomov et al., 2011) contains multi-word name pairs of related but diff"
2021.louhi-1.6,P19-1317,0,0.384958,"er Daelemans CLiPS Research Centre University of Antwerp walter.daelemans@uantwerpen.be Abstract ral encoder architectures such as LSTMs (Kartsaklis et al., 2018) or Transformers (Sung et al., 2020; Kalyan and Sangeetha, 2020) to finetune name representations for biomedical normalization tasks. Such representations are often tailored towards normalization tasks (e.g. linking names to corresponding concept identifiers), without providing explicit guarantees about their transferability to other use contexts and applications. As a solution for this issue, the Biomedical Name Encoder (BNE) model (Phan et al., 2019) has been proposed as a comprehensive framework for robust and transferable representations. According to this framework, the robustness of biomedical name representations is characterized along three dimensions. Firstly, semantic similarity between names should be reflected by their closeness in the embedding space. Secondly, the variety of textual contexts in which a name appears should be somehow represented in the encoding. Lastly, a name embedding should be sufficiently close to a pretrained prototypical representation of its conceptual meaning, e.g. a representation of its corresponding"
2021.louhi-1.6,P18-1041,0,0.113853,"ur DAN model scores substantially better than the publicly released pretrained BNE model, which was trained on a large amount of fine-grained disorder concepts from SNOMED-CT using an elaborate BiLSTM architecture. These results provide tangible evidence that training name representations on large coarse-grained categories can help to encode exploitable higher-level semantics. 2 Kiela, 2019) and simple pooling mechanisms of word embeddings. The fastText encoder which we use as a baseline and as input for the DAN is an example of a Simple Word-Embedding-based Model (SWEM) with average pooling (Shen et al., 2018). 3 3.1 Encoding model Encoder architecture Our encoder is a Deep Averaging Network (DAN) (Iyyer et al., 2015) which extracts a fixed-size representation for an input name n: un = 1 X ut |Nt | t∈Nt (1) f (n) = enc(un ) where Nt is the bag of tokens from a name, ut is a pretrained word embedding of a token, un is a name embedding created by averaging all the pretrained word embeddings of all tokens, and enc is a feedforward neural network with Rectified Linear Unit (ReLU) as non-linear activation function. As pretrained word embeddings we use 300-dimensional fastText (Bojanowski et al., 2017) r"
2021.louhi-1.6,2020.acl-main.335,0,0.0438869,"Missing"
2021.mrqa-1.1,2020.lrec-1.113,0,0.0388953,"t the time of writing, it is not yet publicly available. 3 3.2 A common issue with datasets collected from the web is the redundancy of data (Lee et al., 2021). For example, hotel pages on TripAdvisor typically have an FAQ pair referring to shuttle services from the airport to the hotel.9 The only changing term is the name of the hotel. Algorithms such as SimHash (Charikar, 2002) and MinHash (Broder, 1997) can detect such duplicates. MinHash is an approximate matching algorithm widely used in large-scale deduplication tasks (Lee et al., 2021; Versley and Panchenko, 2012; Gabriel et al., 2018; Gyawali et al., 2020). The main idea of MinHash is to efficiently estimate the Jaccard similarity between two documents, represented by their set of n-grams. Because of the sparse nature of n-grams, computing the full Jaccard similarity between all documents is prohibitive. MinHash alleviates this issue by reducing each document to a fixed-length hash which can be used to efficiently approximate the Jaccard similarity between two documents. MinHash has the additional property that similar documents will have similar hashes, we can then use Locality Sensitive Hashing (LSH) (Leskovec et al., 2014) to efficiently ret"
2021.mrqa-1.1,P19-4007,0,0.0914607,"know how long protection lasts for those who are vaccinated. [...] Abstract In this paper, we present the first multilingual FAQ dataset publicly available. We collected around 6M FAQ pairs from the web, in 21 different languages. Although this is significantly larger than existing FAQ retrieval datasets, it comes with its own challenges: duplication of content and uneven distribution of topics. We adopt a similar setup as Dense Passage Retrieval (DPR) (Karpukhin et al., 2020) and test various bi-encoders on this dataset. Our experiments reveal that a multilingual model based on XLM-RoBERTa (Conneau et al., 2019) achieves the best results, except for English. Lower resources languages seem to learn from one another as a multilingual model achieves a higher MRR than language-specific ones. Our qualitative analysis reveals the brittleness of the model on simple word changes. We publicly release our dataset1 , model2 and training script3 . 1 Table 1: Example FAQs about the COVID-19 vaccine from the CDC website. queries are matched against pairs of questions and answers, as opposed to passages for non-factoid QA. Since FAQ-Finder (Hammond et al., 1995), researchers applied different methods to the task of"
2021.mrqa-1.1,2020.emnlp-main.550,0,0.397564,"gredients in COVID-19 vaccines? Vaccine ingredients can vary by manufacturer. How long does protection from a COVID-19 vaccine last? We don’t know how long protection lasts for those who are vaccinated. [...] Abstract In this paper, we present the first multilingual FAQ dataset publicly available. We collected around 6M FAQ pairs from the web, in 21 different languages. Although this is significantly larger than existing FAQ retrieval datasets, it comes with its own challenges: duplication of content and uneven distribution of topics. We adopt a similar setup as Dense Passage Retrieval (DPR) (Karpukhin et al., 2020) and test various bi-encoders on this dataset. Our experiments reveal that a multilingual model based on XLM-RoBERTa (Conneau et al., 2019) achieves the best results, except for English. Lower resources languages seem to learn from one another as a multilingual model achieves a higher MRR than language-specific ones. Our qualitative analysis reveals the brittleness of the model on simple word changes. We publicly release our dataset1 , model2 and training script3 . 1 Table 1: Example FAQs about the COVID-19 vaccine from the CDC website. queries are matched against pairs of questions and answer"
2021.nlp4convai-1.24,2020.emnlp-main.275,0,0.067188,"Missing"
2021.nlp4convai-1.24,2021.eacl-main.74,0,0.190991,"ucing a pretrained generative models like GPT2 (Radford response to the user’s utterance conditioned on both et al., 2019), BART (Lewis et al., 2019) and T5 254 Proceedings of the Third Workshop on Natural Language Processing for Conversational AI, pages 254–262 November 10, 2021. ©2021 Association for Computational Linguistics Figure 1: General overview of our model based on a pretrained encoder-decoder like BART (Raffel et al., 2020) which allow leveraging their ability in conditional text generation (Zhao et al., 2020; De Bruyn et al., 2020). More recently RAG (Lewis et al., 2021) and FiD (Izacard and Grave, 2021) models have been proposed (primarily in the QA context) to ease the computational costs and limitations of these big models, especially if the supporting passage(s) needs to be retrieved from a huge unstructured corpus. Since these models integrate the KS and RG tasks, they do not need labeled knowledge for training, although a knowledge pool of manageable size is provided often via a parametric or non-parametric retrieval module. In this study we propose K-Mine (Knowledge Mixing in encoder); a model that bridges between the two aforementioned paradigms by doing unsupervised knowledge selecti"
2021.nlp4convai-1.24,P18-1138,0,0.0293443,"ieval module and a generation modway this knowledge is being incorporated in the ule, trained separately or simultaneously, with generation process. or without having access to a ‘gold’ knowledge option. With the introduction of large Early examples of knowledge grounded converpre-trained generative models, the selection sation models mainly tried to diffuse the external and generation part have become more and knowledge as an extra hidden state into the demore entangled, shifting the focus towards encoder part of a recurrent seq-to-seq architecture hancing knowledge incorporation (from multi(Liu et al., 2018; Ghazvininejad et al., 2018). With ple sources) instead of trying to pick the best the release of large knowledge grounded converknowledge option. These approaches however sational datasets like Wizard of Wikipedia (Dinan depend on knowledge labels and/or a sepaet al., 2019), Topical-chat (Gopalakrishnan et al., rate dense retriever for their best performance. In this work we study the unsupervised selec2019) and Holl-E (Moghe et al., 2018), the field wittion abilities of pre-trained generative models nessed numerous studies aimed to best coordinate (e.g. BART) and show that by adding a score"
2021.nlp4convai-1.24,D18-1255,0,0.168214,"n state into the demore entangled, shifting the focus towards encoder part of a recurrent seq-to-seq architecture hancing knowledge incorporation (from multi(Liu et al., 2018; Ghazvininejad et al., 2018). With ple sources) instead of trying to pick the best the release of large knowledge grounded converknowledge option. These approaches however sational datasets like Wizard of Wikipedia (Dinan depend on knowledge labels and/or a sepaet al., 2019), Topical-chat (Gopalakrishnan et al., rate dense retriever for their best performance. In this work we study the unsupervised selec2019) and Holl-E (Moghe et al., 2018), the field wittion abilities of pre-trained generative models nessed numerous studies aimed to best coordinate (e.g. BART) and show that by adding a scorethe KS and RG sub-tasks to improve the overall perand-aggregate module between encoder and formance of models. As an early standard baseline decoder, they are capable of learning to pick Dinan et al. (2019) proposed variations of Transthe proper knowledge through minimising the former MemNet, a generative model trained to do language modelling loss (i.e. without having KS and RG using a memory network for selecting access to knowledge labels"
2021.nlp4convai-1.24,2021.findings-emnlp.320,0,0.228091,"nother (more general) framework to incorporate knowledge in text generation which allows the (pretrained) decoder to choose content from top-k retrieved knowledge pieces via token-wise or sequence-wise marginalization. In principle FiD and RAG replace the KS step with a pool retrieval task that provides the pretrained model with multiple (top-k) relevant passages to attend to (FiD) or marginalize over (RAG) during generation. In particular RAG benefits from a DPR retriever which is updated (only the query encoder part) during training through the back-propagation of generation error. Recently Shuster et al. (2021) adopted FiD and RAG (originally introduced mainly for QA tasks) for knowledge grounded conversation and tried to improve the performance with a variety of general and taskinspired modifications, e.g. using poly encoders (Humeau et al., 2020) or extending the marginalized decoding idea to dialog turns. Like most NLP tasks, knowledge grounded conversation has been significantly influenced by the introduction of large pretrained language models, which have helped generative models beat retrieval models in both automatic and human evaluations (Roller et al., 2020). Thanks to their language modOur"
2021.nlp4convai-1.24,2021.naacl-main.446,0,0.032729,"ivided into two categories, based on separate dense retriever. their point of focus. Selection oriented methods focus on enhancing the KS task, usually by in1 Introduction troducing additional learning signals like the priorThe ability to properly ground conversations in posterior discrepancy (Lian et al., 2019; Chen et al., structured and unstructured data, has become an 2020) or long-term structural traits of conversations increasingly important feature in designing conver- like flow and initiative changes (Kim et al., 2020; sational agents. By generating more informative Meng et al., 2020; Zhan et al., 2021; Meng et al., and specific responses, such models can establish 2021; Zheng et al., 2020). Generation oriented human-machine interactions that are more engag- methods on the other hand, try to mitigate the seing and less prone to producing bland and com- lection bottleneck by employing more powerful mon responses. The task of modelling knowledge- methods to incorporate knowledge in the generagrounded conversations is traditionally decom- tion process, thus reformulating the KS problem as posed into two sub-tasks: 1) knowledge selection an adaptive fine-grained selection to be dealt with (KS),"
2021.nlp4convai-1.24,2020.emnlp-main.272,0,0.127862,"cially encouraged with the introduction of large and 2) response generation (RG), i.e. producing a pretrained generative models like GPT2 (Radford response to the user’s utterance conditioned on both et al., 2019), BART (Lewis et al., 2019) and T5 254 Proceedings of the Third Workshop on Natural Language Processing for Conversational AI, pages 254–262 November 10, 2021. ©2021 Association for Computational Linguistics Figure 1: General overview of our model based on a pretrained encoder-decoder like BART (Raffel et al., 2020) which allow leveraging their ability in conditional text generation (Zhao et al., 2020; De Bruyn et al., 2020). More recently RAG (Lewis et al., 2021) and FiD (Izacard and Grave, 2021) models have been proposed (primarily in the QA context) to ease the computational costs and limitations of these big models, especially if the supporting passage(s) needs to be retrieved from a huge unstructured corpus. Since these models integrate the KS and RG tasks, they do not need labeled knowledge for training, although a knowledge pool of manageable size is provided often via a parametric or non-parametric retrieval module. In this study we propose K-Mine (Knowledge Mixing in encoder); a m"
2021.nlp4convai-1.24,2020.findings-emnlp.11,0,0.0327666,"ection oriented methods focus on enhancing the KS task, usually by in1 Introduction troducing additional learning signals like the priorThe ability to properly ground conversations in posterior discrepancy (Lian et al., 2019; Chen et al., structured and unstructured data, has become an 2020) or long-term structural traits of conversations increasingly important feature in designing conver- like flow and initiative changes (Kim et al., 2020; sational agents. By generating more informative Meng et al., 2020; Zhan et al., 2021; Meng et al., and specific responses, such models can establish 2021; Zheng et al., 2020). Generation oriented human-machine interactions that are more engag- methods on the other hand, try to mitigate the seing and less prone to producing bland and com- lection bottleneck by employing more powerful mon responses. The task of modelling knowledge- methods to incorporate knowledge in the generagrounded conversations is traditionally decom- tion process, thus reformulating the KS problem as posed into two sub-tasks: 1) knowledge selection an adaptive fine-grained selection to be dealt with (KS), i.e. picking the proper knowledge piece(s) in decoding (Zheng and Zhou, 2019). This was e"
2021.nlp4if-1.2,2020.lrec-1.760,0,0.0987616,"Missing"
2021.nlp4if-1.2,W17-4716,0,0.014354,"je and RobBERT (de Vries et al., 2019; Delobelle et al., 2020; the Dutch versions of BERT and RobBERTa) using HuggingFace 4.0.0 (Wolf et al., 2020). In an attempt to improve these models, the &quot;tags&quot; method described above was used, but with the “<met>” (onset) and “</met>” (offset) placeholders for generic features and the same more fine-grained placeholders as described above when using source domain features. This tagging method is frequently used to highlight textual features or external knowledge in sequence-to-sequence tasks such as machine translation and named entity recognition (e.g., Chatterjee et al., 2017; Li et al., 2018). Four epochs were used for training and all other parameters were set to default. The experiments were conducted five times with different seeds and we report the median of these runs. 4 In order to ensure that the placeholders were not confused with actual text, all text was lowercased and all placeholders were uppercased before training. 12 5 5.1 Results Approach SVM +n tokens +n expressions +suffix +tokens +tags +all BERTje +tags RobBERT +tags Quantitative results The 10-fold cross-validation and test results of the SVM model5 , BERTje and RobBERT without additional featu"
2021.nlp4if-1.2,W18-5102,0,0.0439316,"Missing"
2021.nlp4if-1.2,2020.findings-emnlp.292,0,0.0361964,"Missing"
2021.nlp4if-1.2,N19-1423,0,0.014601,"were annotated for the type of hate speech and the target of hate speech, and for “hateful metaphors”, 1 2 Related research Hate speech detection Hate speech – frequently defined as a form of communication that disparages a person or a group on the basis of some characteristic such as race, color, ethnicity, gender, sexual orientation, nationality, religion, or other (Nockleby, 2000) – has been extensively researched in the field of NLP. Pretrained language models such as Bidirectional Encoder Representations from Transformers (BERT) and Robustly Optimized BERT Pretraining Approach (RoBERTa) (Devlin et al., 2019; Liu et al., 2019) provide the best results for hate speech detection, including type and target classification (Basile et al., 2019; Zampieri et al., 2019b, 2020), while shallow machine learning models (e.g., Support Vector Mahttps://lilah.eu/ 7 Proceedings of the 4th NLP4IF Workshop on NLP for Internet Freedom, pages 7–16 June 6, 2021. ©2021 Association for Computational Linguistics chines (SVM)) can achieve a near state-of-the-art performance (MacAvaney et al., 2019). Examples of successful machine learning models include the winning teams of both subtasks A (binary hate speech detection)"
2021.nlp4if-1.2,W18-4401,0,0.0541693,"Missing"
2021.nlp4if-1.2,2020.figlang-1.3,0,0.0661419,"Missing"
2021.nlp4if-1.2,2020.figlang-1.4,0,0.0618154,"Missing"
2021.nlp4if-1.2,W18-0907,0,0.0188121,"detection accuracy. The results of the conducted experiments show that hateful metaphor features improve model performance for the both tasks. To our knowledge, it is the first time that the effectiveness of hateful metaphors as an information source for hate speech classification is investigated. 1 Introduction In this paper, the usefulness of hateful metaphors used as features for detecting the type and target of Dutch online hate speech comments is investigated. Although both hate speech and metaphor detection have been researched widely (e.g., MacAvaney et al., 2019; Basile et al., 2019; Leong et al., 2018, 2020), and figurative language used in hateful content has been identified as one of the main challenges in (implicit) hate speech detection (MacAvaney et al., 2019; van Aken et al., 2018), the question whether detecting (hateful) metaphors and using them as features improves hate speech detection models has remained unstudied in previous research. Therefore, it is the goal of the present paper to address this question. In order to achieve this goal, we used the Dutch LiLaH1 corpus which consists Facebook comments on online newspaper articles related to either migrants or the LGBT community."
2021.nlp4if-1.2,W18-5105,0,0.0509558,"Missing"
2021.nlp4if-1.2,2020.emnlp-demos.6,0,0.0390421,"Missing"
2021.nlp4if-1.2,2021.ccl-1.108,0,0.0410292,"Missing"
2021.nlp4if-1.2,N19-1144,0,0.0180675,"ech – frequently defined as a form of communication that disparages a person or a group on the basis of some characteristic such as race, color, ethnicity, gender, sexual orientation, nationality, religion, or other (Nockleby, 2000) – has been extensively researched in the field of NLP. Pretrained language models such as Bidirectional Encoder Representations from Transformers (BERT) and Robustly Optimized BERT Pretraining Approach (RoBERTa) (Devlin et al., 2019; Liu et al., 2019) provide the best results for hate speech detection, including type and target classification (Basile et al., 2019; Zampieri et al., 2019b, 2020), while shallow machine learning models (e.g., Support Vector Mahttps://lilah.eu/ 7 Proceedings of the 4th NLP4IF Workshop on NLP for Internet Freedom, pages 7–16 June 6, 2021. ©2021 Association for Computational Linguistics chines (SVM)) can achieve a near state-of-the-art performance (MacAvaney et al., 2019). Examples of successful machine learning models include the winning teams of both subtasks A (binary hate speech detection) and B (binary target classification) of task 5 of SemEval 2019: multilingual detection of hate speech against women and immigrants on Twitter (Basile et al."
2021.nlp4if-1.2,S19-2010,0,0.0219063,"ech – frequently defined as a form of communication that disparages a person or a group on the basis of some characteristic such as race, color, ethnicity, gender, sexual orientation, nationality, religion, or other (Nockleby, 2000) – has been extensively researched in the field of NLP. Pretrained language models such as Bidirectional Encoder Representations from Transformers (BERT) and Robustly Optimized BERT Pretraining Approach (RoBERTa) (Devlin et al., 2019; Liu et al., 2019) provide the best results for hate speech detection, including type and target classification (Basile et al., 2019; Zampieri et al., 2019b, 2020), while shallow machine learning models (e.g., Support Vector Mahttps://lilah.eu/ 7 Proceedings of the 4th NLP4IF Workshop on NLP for Internet Freedom, pages 7–16 June 6, 2021. ©2021 Association for Computational Linguistics chines (SVM)) can achieve a near state-of-the-art performance (MacAvaney et al., 2019). Examples of successful machine learning models include the winning teams of both subtasks A (binary hate speech detection) and B (binary target classification) of task 5 of SemEval 2019: multilingual detection of hate speech against women and immigrants on Twitter (Basile et al."
2021.nlp4if-1.2,2020.peoples-1.15,1,0.826588,"Missing"
2021.nlp4if-1.2,2021.wassa-1.16,1,0.840554,"Missing"
2021.nlp4if-1.3,N19-1423,0,0.0311214,"ivation to make the prediction for the binary classification. Long short-term memory networks (LSTM) We use an LSTM model (Hochreiter and Schmidhuber, 1997), which takes a sequence of words as input and aims at capturing long-term dependencies. We process the sequence of word embeddings (trained with GloVe (Pennington et al., 2014)) with a unidirectional LSTM layer with 300 units, followed by a dropout of 0.2, and a dense layer with a sigmoid activation for predictions. 2.2 Models BERT and RoBERTa Pretrained language models, i.e., Bidirectional Encoder Representations from Transformers, BERT (Devlin et al., 2019) and Robustly Optimized BERT Pretraining Approach, RoBERTa (Liu et al., 2019b), currently provide the best results for hate speech detection, as shown by several shared tasks in the field (Zampieri et al., 2019b; Mandl et al., 2019; Zampieri et al., 2020). We use the BERT-base-cased (12-layer, 768-hidden, 12-heads, 110 million parameters) and RoBERTa-base (12-layer, 768-hidden, 12-heads, 125 million parameters) models from the huggingEnsemble We use a simple ensembling strategy, which consists in combining the predictions produced by the deep learning and machine learning approaches: BERT, RoB"
2021.nlp4if-1.3,D14-1181,0,0.00457388,"(Pedregosa et al., 2011) implementation of the SVM algorithm with optimized parameters (penalty parameter (C), loss function (loss), and tolerance for stopping criteria (tol)) selected based on grid search. Baselines Bag of words (BoW) We use a tf-weighted lowercased bag-of-words (BoW) approach with the liblinear Support Vector Machines (SVM) classifier. The optimal SVM parameters (penalty parameter (C), loss function (loss), and tolerance for stopping criteria (tol)) were selected based on grid search. Convolutional neural networks (CNN) We use a convolutional neural networks (CNN) approach (Kim, 2014) to learn discriminative wordlevel hate speech features with the following architecture: to process the word embeddings (trained with fastText (Joulin et al., 2017)), we use a convolutional layer followed by a global average pooling layer and a dropout of 0.6. Then, a dense layer with a ReLU activation is applied, followed by a dropout of 0.6, and finally, a dense layer with a sigmoid activation to make the prediction for the binary classification. Long short-term memory networks (LSTM) We use an LSTM model (Hochreiter and Schmidhuber, 1997), which takes a sequence of words as input and aims a"
2021.nlp4if-1.3,E17-2068,0,0.033871,"pping criteria (tol)) selected based on grid search. Baselines Bag of words (BoW) We use a tf-weighted lowercased bag-of-words (BoW) approach with the liblinear Support Vector Machines (SVM) classifier. The optimal SVM parameters (penalty parameter (C), loss function (loss), and tolerance for stopping criteria (tol)) were selected based on grid search. Convolutional neural networks (CNN) We use a convolutional neural networks (CNN) approach (Kim, 2014) to learn discriminative wordlevel hate speech features with the following architecture: to process the word embeddings (trained with fastText (Joulin et al., 2017)), we use a convolutional layer followed by a global average pooling layer and a dropout of 0.6. Then, a dense layer with a ReLU activation is applied, followed by a dropout of 0.6, and finally, a dense layer with a sigmoid activation to make the prediction for the binary classification. Long short-term memory networks (LSTM) We use an LSTM model (Hochreiter and Schmidhuber, 1997), which takes a sequence of words as input and aims at capturing long-term dependencies. We process the sequence of word embeddings (trained with GloVe (Pennington et al., 2014)) with a unidirectional LSTM layer with"
2021.nlp4if-1.3,S19-2011,0,0.0167324,"mory networks (LSTM) We use an LSTM model (Hochreiter and Schmidhuber, 1997), which takes a sequence of words as input and aims at capturing long-term dependencies. We process the sequence of word embeddings (trained with GloVe (Pennington et al., 2014)) with a unidirectional LSTM layer with 300 units, followed by a dropout of 0.2, and a dense layer with a sigmoid activation for predictions. 2.2 Models BERT and RoBERTa Pretrained language models, i.e., Bidirectional Encoder Representations from Transformers, BERT (Devlin et al., 2019) and Robustly Optimized BERT Pretraining Approach, RoBERTa (Liu et al., 2019b), currently provide the best results for hate speech detection, as shown by several shared tasks in the field (Zampieri et al., 2019b; Mandl et al., 2019; Zampieri et al., 2020). We use the BERT-base-cased (12-layer, 768-hidden, 12-heads, 110 million parameters) and RoBERTa-base (12-layer, 768-hidden, 12-heads, 125 million parameters) models from the huggingEnsemble We use a simple ensembling strategy, which consists in combining the predictions produced by the deep learning and machine learning approaches: BERT, RoBERTa, and SVM, through a hard majority-voting ensemble, i.e., selecting the"
2021.nlp4if-1.3,2021.ccl-1.108,0,0.0715427,"Missing"
2021.nlp4if-1.3,W18-5102,0,0.0521064,"Missing"
2021.nlp4if-1.3,2020.peoples-1.15,1,0.739061,"Missing"
2021.nlp4if-1.3,P19-1163,0,0.0289449,"ls for an offensive word stronger than other signals from the context, leading to false positive predictions, and further removal of harmless content online (van Aken et al., 2018; Zhang and Luo, 2018). Labelling non-hateful utterances as hate speech (false positives or type II errors) is a common error even for human annotators due to personal bias. Several studies showed that providing context, detailed annotation guidelines, or the background of the author of a message improves annotation quality by reducing the number of utterances erroneously annotated as hateful (de Gibert et al., 2018; Sap et al., 2019; Vidgen and Derczynski, 2020). We assess the performance of deep learning models that currently provide state-of-the-art results for the hate speech detection task (Zampieri et al., 2019b, 2020) both under in-domain and crossdomain hate speech detection conditions, and introduce an SVM approach with a variety of engineered features (e.g., stylometric, emotion, hate speech lexicon features, described further in the paper) that significantly improves the results when combined with the deep learning models in an ensemble, mainly by reducing the false positive rate. We target the use cases where"
2021.nlp4if-1.3,N03-1033,0,0.128789,"h taking the advantage of explicit feature engineering based on task and domain knowledge. In more detail, we focus on the approaches described below. 2.1 Support Vector Machines (SVM) The Support Vector Machines (SVM) algorithm (Cortes and Vapnik, 1995) is commonly used for the hate speech detection task (Davidson et al., 2017; Salminen et al., 2018; MacAvaney et al., 2019; Del Vigna et al., 2017; Ljubeši´c et al., 2020). Following Markov et al. (2021), we lemmatize the messages in our data and represent them through universal part-of-speech (POS) tags (obtained with the Stanford POS Tagger (Toutanova et al., 2003)), function words (words belonging to the closed syntactic classes)3 , and emotionconveying words (from the NRC word-emotion association lexicon (Mohammad and Turney, 2013)) to capture stylometric and emotion-based peculiarities of hateful content. For example, the phrase @USER all conservatives are bad people [OLID id: 22902] is represented through POS, function words, and emotion-conveying words as ‘PROPN’, ‘all’, ‘NOUN’, ‘be’, ‘bad’, ‘NOUN’. From this representation n-grams (with n = 1–3) are built. We use the NRC lexicon emotion associations (e.g., bad = ‘anger’, ‘disgust’, ‘fear’, ‘negati"
2021.nlp4if-1.3,2021.wassa-1.16,1,0.846321,"Missing"
2021.nlp4if-1.3,W18-5105,0,0.0469797,"Missing"
2021.nlp4if-1.3,N19-1144,0,0.234952,"hang and Luo, 2018). Labelling non-hateful utterances as hate speech (false positives or type II errors) is a common error even for human annotators due to personal bias. Several studies showed that providing context, detailed annotation guidelines, or the background of the author of a message improves annotation quality by reducing the number of utterances erroneously annotated as hateful (de Gibert et al., 2018; Sap et al., 2019; Vidgen and Derczynski, 2020). We assess the performance of deep learning models that currently provide state-of-the-art results for the hate speech detection task (Zampieri et al., 2019b, 2020) both under in-domain and crossdomain hate speech detection conditions, and introduce an SVM approach with a variety of engineered features (e.g., stylometric, emotion, hate speech lexicon features, described further in the paper) that significantly improves the results when combined with the deep learning models in an ensemble, mainly by reducing the false positive rate. We target the use cases where messages are flagged automatically and can be mistakenly removed, without or with little moderator intervention. While existing optimization strategies (e.g., threshold variation) allow t"
2021.nlp4if-1.3,S19-2010,0,0.179815,"hang and Luo, 2018). Labelling non-hateful utterances as hate speech (false positives or type II errors) is a common error even for human annotators due to personal bias. Several studies showed that providing context, detailed annotation guidelines, or the background of the author of a message improves annotation quality by reducing the number of utterances erroneously annotated as hateful (de Gibert et al., 2018; Sap et al., 2019; Vidgen and Derczynski, 2020). We assess the performance of deep learning models that currently provide state-of-the-art results for the hate speech detection task (Zampieri et al., 2019b, 2020) both under in-domain and crossdomain hate speech detection conditions, and introduce an SVM approach with a variety of engineered features (e.g., stylometric, emotion, hate speech lexicon features, described further in the paper) that significantly improves the results when combined with the deep learning models in an ensemble, mainly by reducing the false positive rate. We target the use cases where messages are flagged automatically and can be mistakenly removed, without or with little moderator intervention. While existing optimization strategies (e.g., threshold variation) allow t"
2021.nlp4if-1.3,D14-1162,0,0.0852247,"the word embeddings (trained with fastText (Joulin et al., 2017)), we use a convolutional layer followed by a global average pooling layer and a dropout of 0.6. Then, a dense layer with a ReLU activation is applied, followed by a dropout of 0.6, and finally, a dense layer with a sigmoid activation to make the prediction for the binary classification. Long short-term memory networks (LSTM) We use an LSTM model (Hochreiter and Schmidhuber, 1997), which takes a sequence of words as input and aims at capturing long-term dependencies. We process the sequence of word embeddings (trained with GloVe (Pennington et al., 2014)) with a unidirectional LSTM layer with 300 units, followed by a dropout of 0.2, and a dense layer with a sigmoid activation for predictions. 2.2 Models BERT and RoBERTa Pretrained language models, i.e., Bidirectional Encoder Representations from Transformers, BERT (Devlin et al., 2019) and Robustly Optimized BERT Pretraining Approach, RoBERTa (Liu et al., 2019b), currently provide the best results for hate speech detection, as shown by several shared tasks in the field (Zampieri et al., 2019b; Mandl et al., 2019; Zampieri et al., 2020). We use the BERT-base-cased (12-layer, 768-hidden, 12-hea"
2021.wassa-1.16,W19-3501,0,0.0194155,"ied. We propose the hypothesis that stylometric characteristics of hateful writing are distinctive enough to contribute to the hate speech detection task. In other words, hate speech acts as a specific text type with an associated writing style. On the other hand, we are motivated by psychological and sociological studies, which correlate toxic behaviour online with the emotional profile of the user (Kokkinos and Kipritsi, 2012). However, unlike previous research that used sentiment information for detecting unacceptable content (Davidson et al., 2017; Dani et al., 2017; Van Hee et al., 2018; Brassard-Gourdeau and Khoury, 2019), we test whether we are able to capture some of these phenomena by going beyond the sentiment level (positive / negative / neutral) to a more fine-grained emotion level. We compare the performance of stylometric and emotion-based features with commonly used features for hate speech detection: words, character n-grams, and their combination, and with more recent deep learning models that currently provide the state-of-the-art results for the hate speech detection task (Mandl et al., 2019; Basile et al., 2019): convolutional neural networks (CNN), long shortterm memory networks (LSTM), and bidi"
2021.wassa-1.16,N19-1423,0,0.0584378,"Missing"
2021.wassa-1.16,P82-1020,0,0.702682,"Missing"
2021.wassa-1.16,W14-0908,0,0.349442,"that the style and emotional dimension of hateful textual content may provide useful cues for its detection. We investigate this through a binary hate speech classification task using features that model such information, i.e., function words and emotion-based features. The latter are operationalized in terms of the types of emotions expressed and the frequency of emotionconveying words in the data. Function word usage is one of the most important and revealing aspects of style in written language, as shown by numerous studies in stylometric analysis for authorship attribution (Grieve, 2007; Kestemont, 2014; Markov et al., 2018). While stylometric characteristics have been implicitly included in some hate speech detection studies (e.g., in bag-of-words or character-level models), 149 Proceedings of the 11th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 149–159 April 19, 2021. ©2021 Association for Computational Linguistics their impact on the task has not been studied. We propose the hypothesis that stylometric characteristics of hateful writing are distinctive enough to contribute to the hate speech detection task. In other words, hate speech a"
2021.wassa-1.16,D14-1181,0,0.00897115,"Missing"
2021.wassa-1.16,2020.peoples-1.15,1,0.377744,"Missing"
2021.wassa-1.16,K18-2016,0,0.0269953,"Missing"
2021.wassa-1.16,P19-1163,0,0.0452067,"onality, religion, or other characteristics (Nockleby, 2000). The exact definition of hate speech, however, remains a disputed topic, as it is a subjective and multi-interpretable concept (Waseem et al., 2017; Poletto et al., 2020). The lack of a consensus on its definition poses a challenge to hate speech annotation. Annotating hateful content remains prone to personal bias and is culture-dependent, which often results in low inter-annotator agreement and therefore scarcity of high quality training data for developing supervised hate speech detection systems (Ross et al., 2016; Waseem, 2016; Sap et al., 2019). Hate speech online presents additional challenges for natural language processing (NLP): offensive vocabulary and keywords evolve fast due to their relatedness with the hate speech triggering events (Florio et al., 2020), moreover, users may adapt their lexical choices as a countermeasure against identification or introduce minor misspellings to bypass filtering systems (Berger and Perez, 2006; Vidgen et al., 2019). Therefore, we intend to investigate more abstract features, less susceptible to specific vocabulary, topic or corpus bias, which we examine in in-domain and crossdomain settings:"
2021.wassa-1.16,N03-1033,0,0.184881,"llness on parade. Table 3 shows an example of the representation of this message through the features described above. From the POS & FW & emotion word representations, n-grams (n = 1– 3) are built.4 The count of emotionally-charged words and the emotion associations were added as additional feature vectors. Part-of-speech (POS) POS features capture the morpho-syntactic patterns in a text, and are indicative of hate speech, especially when used in combination with other types of features (Warner and Hirschberg, 2012; Robinson et al., 2018). POS tags were obtained with the Stanford POS Tagger (Toutanova et al., 2003). We used the same 17 universal POS tags for the three languages and built n-grams from this representation with n = 1–3. Stylometric features Function words (FW) are considered one of the most important stylometric feature types (Kestemont, 2014). They clarify the relationships between the content-carrying elements of a sentence, and introduce syntactic structures like verbal complements, relative clauses, and questions (Smith and Witten, 1993). With respect to emotion features, FW can appear as quantifiers, intensifiers (e.g., very good) or modify the emotion phrase Mental illness on parade"
2021.wassa-1.16,R15-1086,1,0.828623,"Missing"
2021.wassa-1.16,W19-3509,0,0.0395717,"esults in low inter-annotator agreement and therefore scarcity of high quality training data for developing supervised hate speech detection systems (Ross et al., 2016; Waseem, 2016; Sap et al., 2019). Hate speech online presents additional challenges for natural language processing (NLP): offensive vocabulary and keywords evolve fast due to their relatedness with the hate speech triggering events (Florio et al., 2020), moreover, users may adapt their lexical choices as a countermeasure against identification or introduce minor misspellings to bypass filtering systems (Berger and Perez, 2006; Vidgen et al., 2019). Therefore, we intend to investigate more abstract features, less susceptible to specific vocabulary, topic or corpus bias, which we examine in in-domain and crossdomain settings: training and testing on social media datasets belonging to same/different domains, for three languages: English, Slovene, and Dutch. Our hypothesis is that the style and emotional dimension of hateful textual content may provide useful cues for its detection. We investigate this through a binary hate speech classification task using features that model such information, i.e., function words and emotion-based feature"
2021.wassa-1.16,W12-2103,0,0.0286242,"sage is. Consider the following English comment from our data belonging to the hate speech class: Mental illness on parade. Table 3 shows an example of the representation of this message through the features described above. From the POS & FW & emotion word representations, n-grams (n = 1– 3) are built.4 The count of emotionally-charged words and the emotion associations were added as additional feature vectors. Part-of-speech (POS) POS features capture the morpho-syntactic patterns in a text, and are indicative of hate speech, especially when used in combination with other types of features (Warner and Hirschberg, 2012; Robinson et al., 2018). POS tags were obtained with the Stanford POS Tagger (Toutanova et al., 2003). We used the same 17 universal POS tags for the three languages and built n-grams from this representation with n = 1–3. Stylometric features Function words (FW) are considered one of the most important stylometric feature types (Kestemont, 2014). They clarify the relationships between the content-carrying elements of a sentence, and introduce syntactic structures like verbal complements, relative clauses, and questions (Smith and Witten, 1993). With respect to emotion features, FW can appear"
2021.wassa-1.16,W16-5618,0,0.00931299,"entation, nationality, religion, or other characteristics (Nockleby, 2000). The exact definition of hate speech, however, remains a disputed topic, as it is a subjective and multi-interpretable concept (Waseem et al., 2017; Poletto et al., 2020). The lack of a consensus on its definition poses a challenge to hate speech annotation. Annotating hateful content remains prone to personal bias and is culture-dependent, which often results in low inter-annotator agreement and therefore scarcity of high quality training data for developing supervised hate speech detection systems (Ross et al., 2016; Waseem, 2016; Sap et al., 2019). Hate speech online presents additional challenges for natural language processing (NLP): offensive vocabulary and keywords evolve fast due to their relatedness with the hate speech triggering events (Florio et al., 2020), moreover, users may adapt their lexical choices as a countermeasure against identification or introduce minor misspellings to bypass filtering systems (Berger and Perez, 2006; Vidgen et al., 2019). Therefore, we intend to investigate more abstract features, less susceptible to specific vocabulary, topic or corpus bias, which we examine in in-domain and cr"
2021.wassa-1.16,W17-3012,0,0.0138072,"orms words and character n-gram features under cross-domain conditions, and provides a significant boost to deep learning models, which currently obtain the best results, when combined with them in an ensemble. 1 Introduction Hate speech is commonly defined as communication that disparages a person or a group on the basis of some characteristic such as race, color, ethnicity, gender, sexual orientation, nationality, religion, or other characteristics (Nockleby, 2000). The exact definition of hate speech, however, remains a disputed topic, as it is a subjective and multi-interpretable concept (Waseem et al., 2017; Poletto et al., 2020). The lack of a consensus on its definition poses a challenge to hate speech annotation. Annotating hateful content remains prone to personal bias and is culture-dependent, which often results in low inter-annotator agreement and therefore scarcity of high quality training data for developing supervised hate speech detection systems (Ross et al., 2016; Waseem, 2016; Sap et al., 2019). Hate speech online presents additional challenges for natural language processing (NLP): offensive vocabulary and keywords evolve fast due to their relatedness with the hate speech triggeri"
C00-1048,J95-4004,0,0.884311,"This paper describes the use of rule induction techniques for the automatic extraction of phonemic knowledge and rules from pairs of pronunciation lexica. This extracted knowledge allows the adaptation of speech processing systems to regional variants of a language. As a case study, we apply the approach to Northern Dutch and Flemish (the variant of Dutch spoken in Flanders, a part of Belgium), based on Celex and Fonilex, pronunciation lexica for Northern Dutch and Flemish, respectively. In our study, we compare two rule induction techniques, TransformationBased Error-Driven Learning (TBEDL) (Brill, 1995) and C5.0 (Quinlan, 1993), and evaluate the extracted knowledge quantitatively (accuracy) and qualitatively (linguistic relevance of the rules). We conclude that, whereas classi cation-based rule induction with C5.0 is more accurate, the transformation rules learned with TBEDL can be more easily interpreted. 1 Introduction A central component of speech processing systems is a pronunciation lexicon de ning the relationship between the spelling and pronunciation of words. Regional variants of a language may di er considerably in their pronunciation. Once a speaker from a particular region is det"
C00-1048,J95-2004,0,0.132364,"techniques, viz. Transformation-Based Error Driven Learning (TBEDL) (Brill, 1995) and C5.0 (Quinlan, 1993). In the process of Transformation-Based Error-Driven Learning, transformation rules are learned by comparing a corpus that is annotated by an initial state annotator to a correctly annotated corpus, which is called the 	ruth&quot;. During that comparison, an ordered list of transformation rules is learned. This ordering implies that the application of an earlier rule sometimes makes it possible for a later rule to apply (so-called feeding&quot;). In other cases, as also described in the work of Roche and Schabes (1995), a given structure fails to undergo a rule as a consequence of some earlier rule (leeding&quot;). These rules are applied to the output of the initial state annotator in order to bring that output closer to the 	ruth&quot;. A rule consists of two parts: a transformation and a 	riggering environment&quot;. For each iteration in the learning process, it is investigated for each possible rule how many mistakes can be corrected through application of that rule. The rule which causes the greatest error reduction is retained. Figure 1 shows the TBEDL learning process applied to the comparison of the Celex rep"
C00-2124,J96-1002,0,0.00566433,"tion process. Attention is constrained to models with these properties. The MaxEnt principle now demands that among all the probability distributions that obey these constraints, the most uniform is chosen. During training, features are assigned weights in such a way that, given the MaxEnt principle, the training data is matched as well as possible. During evaluation it is tested which features are active (i.e. a feature is active when the context meets the requirements given by the feature). For every class the weights of the active features are combined and the best scoring class is chosen (Berger et al., 1996). For the classi er built here the surrounding words, their POS tags and baseNP tags predicted for the previous words are used as evidence. A mixture of simple features (consisting of one of the mentioned information sources) and complex features (combinations thereof) were used. The left context never exceeded 3 words, the right context was maximally 2 words. The model was calculated using existing software (Dehaspe, 1997). MBSL (Argamon et al., 1999) uses POS data in order to identify baseNPs. Inference relies on a memory which contains all the occurrences of POS sequences which appear in th"
C00-2124,P98-1029,0,0.184833,"Missing"
C00-2124,J93-2004,0,0.0295021,"Task description Base noun phrases (baseNPs) are noun phrases which do not contain another noun phrase. For example, the sentence In [ early trading ] in [ Hong Kong ] [ Monday ] , [ gold ] was quoted at [ $ 366.50 ] [ an ounce ] . contains six baseNPs (marked as phrases between square brackets). The phrase $ 366.50 an ounce is a noun phrase as well. However, it is not a baseNP since it contains two other noun phrases. Two baseNP data sets have been put forward by Ramshaw and Marcus (1995). The main data set consist of four sections of the Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al., 1993) as training material (sections 15-18, 211727 tokens) and one section as test material (section 20, 47377 tokens)1 . The data contains words, their part-of-speech 1 This Ramshaw and Marcus (1995) baseNP data set is available via ftp://ftp.cis.upenn.edu/pub/chunker/ (POS) tags as computed by the Brill tagger and their baseNP segmentation as derived from the Treebank (with some modi cations). In the baseNP identi cation task, performance is measured with three rates. First, with the percentage of detected noun phrases that are correct (precision). Second, with the percentage of noun phrases in t"
C00-2124,W99-0621,1,0.89852,"Missing"
C00-2124,W95-0107,0,0.246942,"ing algorithms that we will apply to the task. We will conclude with an overview of the combination methods that we will test. 2.1 Task description Base noun phrases (baseNPs) are noun phrases which do not contain another noun phrase. For example, the sentence In [ early trading ] in [ Hong Kong ] [ Monday ] , [ gold ] was quoted at [ $ 366.50 ] [ an ounce ] . contains six baseNPs (marked as phrases between square brackets). The phrase $ 366.50 an ounce is a noun phrase as well. However, it is not a baseNP since it contains two other noun phrases. Two baseNP data sets have been put forward by Ramshaw and Marcus (1995). The main data set consist of four sections of the Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al., 1993) as training material (sections 15-18, 211727 tokens) and one section as test material (section 20, 47377 tokens)1 . The data contains words, their part-of-speech 1 This Ramshaw and Marcus (1995) baseNP data set is available via ftp://ftp.cis.upenn.edu/pub/chunker/ (POS) tags as computed by the Brill tagger and their baseNP segmentation as derived from the Treebank (with some modi cations). In the baseNP identi cation task, performance is measured with three rates. First"
C00-2124,A00-2007,1,0.844897,"ection 2.1, noun phrases are represented by bracket structures. It has been shown by Mu~noz et al. (1999) that for baseNP recognition, the representation with brackets outperforms other data representations. One classi er can be trained to recognize open brackets (O) and another can handle close brackets (C). Their results can be combined by making pairs of open and close brackets with large probability scores. We have used this bracket representation (O+C) as well. However, we have not used the combination strategy from Mu~noz et al. (1999) but instead used the strategy outlined in Tjong Kim Sang (2000): regard only the shortest possible phrases between candidate open and close brackets as base noun phrases. An alternative representation for baseNPs has been put forward by Ramshaw and Marcus (1995). They have de ned baseNP recognition as a tagging task: words can be inside a baseNP (I) or outside a baseNP (O). In the case that one baseNP immediately follows another baseNP, the rst word in the second baseNP receives tag B. Example: InO earlyI tradingI inO HongI KongI MondayB ,O goldI wasO quotedO atO $I 366.50I anB ounceI .O This set of three tags is sucient for encoding baseNP structures si"
C00-2124,P98-1081,1,0.877991,"Missing"
C00-2124,C00-1034,1,\N,Missing
C00-2124,C98-1078,1,\N,Missing
C00-2124,C98-1029,0,\N,Missing
C08-1065,C04-1088,0,0.178872,"Missing"
C08-1065,luyckx-daelemans-2008-personae,1,0.776647,"iggs Type Indicator (MBTI) (Briggs Myers and Myers, 1980) test and submitted their profile, the text and some user information via a website. All students released the copyright of their text and explicitly allowed the use of their text and associated personality profile for research, which makes it possible to distribute the corpus. The corpus cannot only be used for authorship attribution and verification experiments, but also for personality prediction. More information about the motivation behind the corpus and results from exploratory experiments in personality prediction can be found in Luyckx & Daelemans (2008). 3 Methodology We approach authorship attribution and verification as automatic text categorization tasks that label documents according to a set of predefined categories (Sebastiani, 2002, 3). Like in most text cat1 The Personae corpus can be downloaded from http://www.cnts.ua.ac.be/∼kim/Personae.html 514 egorization systems, we take a two-step approach in which our system (i) achieves automatic selection of features that have high predictive value for the categories to be learned (see Section 3.1), and (ii) uses machine learning algorithms to learn to categorize new documents by using the f"
C08-1065,P04-1026,0,\N,Missing
C12-1085,W07-1713,0,0.396004,"Missing"
C12-1085,W03-1106,0,0.178735,"of stop words, grammar tools or dictionaries– resources that resource-scarce languages may not always have. It is however also possible to use character level n-grams. Character n-grams can often capture the languagedependent elements without having to resort to external stemmers, lemmatizers or word segmentation tools. Character n-grams will highlight highly relevant word segments, and since each string is decomposed into small parts, any error can affect only a limited number of these parts, which makes character n-grams ideal to use with informal texts containing many typographical errors. Peng et al. (2003a, 2003b) skipped language dependent pre-processing and feature selection stages and demonstrated that a language modelling approach on character n-grams gives superior classification results for English and competitive results for Chinese and Japanese. For English, the most accurate performance was achieved with n-grams of 3 characters or more, peaking at 6grams with Witten-Bells smoothing. Bigrams (or higher order) with different smoothing techniques worked best for Chinese, whereas 6-grams (or more) with absolute or Written-Bell smoothing worked best for Japanese. Wei et al. (2008)’s experi"
C12-1085,N03-1025,0,0.216948,"of stop words, grammar tools or dictionaries– resources that resource-scarce languages may not always have. It is however also possible to use character level n-grams. Character n-grams can often capture the languagedependent elements without having to resort to external stemmers, lemmatizers or word segmentation tools. Character n-grams will highlight highly relevant word segments, and since each string is decomposed into small parts, any error can affect only a limited number of these parts, which makes character n-grams ideal to use with informal texts containing many typographical errors. Peng et al. (2003a, 2003b) skipped language dependent pre-processing and feature selection stages and demonstrated that a language modelling approach on character n-grams gives superior classification results for English and competitive results for Chinese and Japanese. For English, the most accurate performance was achieved with n-grams of 3 characters or more, peaking at 6grams with Witten-Bells smoothing. Bigrams (or higher order) with different smoothing techniques worked best for Chinese, whereas 6-grams (or more) with absolute or Written-Bell smoothing worked best for Japanese. Wei et al. (2008)’s experi"
C12-1085,C00-2137,0,0.0200707,"0.398 0.071 0.137 Supermamos macro-F micro-F 0.277 0.316 0.320 0.353 0.323 0.358 0.356 0.392 TABLE 4 – Classification results for “Supermamos”. Tokens-1 Stems-1 Lemmas-1 Characters-4 Random baseline Majority baseline Rinkimų programos’04 acc. macro-F micro-F 0.298 0.238 0.282 0.326 0.262 0.306 0.363 0.291 0.346 0.376 0.290 0.351 0.126 0.231 TABLE 5 – Classification results for “Rinkimų programos’04”. To determine whether the performances of the classifiers trained with each feature type were significantly different from each other, we performed approximate randomization testing (Noreen, 1989; Yeh, 2000). For “Lietuvos rytas”, all differences are significant to a very high degree (p &lt;= 0.0001), with the following exceptions: the difference between the “Stems-1” and the “Lemmas-1” results are not statistically significant; the difference between “Characters-4” and “Stems-1” is significant to a high degree (accuracy: p = 0.001, macro-F: p = 0.006, micro-F: p = 0.003); the difference between “Characters-4” and “Lemmas-1” is only significant in terms of accuracy (p = 0.0005) and micro-F (p = 0.001). For “Supermamos”, all differences are significant to a very high degree (p &lt;= 0.0001), with the ex"
C18-1090,D09-1062,0,0.0161837,"lness of a web-derived polarity lexicon. One drawback that remains however, is the neglect of domain-specific information in these generalpurpose lexicons. The next subsection will discuss work that has been done in generating lexicons for domain-specific purposes. 2.3 Domain adaptation There is a large variety of recent work in obtaining domain-specific polarity lexicons. Most are concerned with generating a new lexicon for a target domain (Bross and Ehrig, 2013; Tai and Kao, 2013), while others aim to augment existing lexicons for domain-specific use with a variation of polarity clues. 1057 Choi and Cardie (2009) use linear programming to find the most likely polarity label for words based on their occurrence in opinion expressions. Two types of clues; word-to-word relations within each expression and word-to-expression relations, are exploited to adapt a general-purpose lexicon. The adapted lexicon significantly improves expression classification with regards to the original lexicon. Du et. al (2010) adapt the information bottleneck (IB) method to take domain-specific knowledge into account. This slightly changes the problem of adapting a general-purpose lexicon to fit a specific domain, because IB i"
C18-1090,D16-1057,0,0.536872,"ed techniques rely on high quality polarity lexicons. Document-level polarity can be predicted by looking at occurrences of lexical units in the document and doing some form of averaging on their respective polarity values. Despite not yielding the state of the art results of corpus based methods, lexicon based methods still see frequent use since they do not rely on big (labelled) data and allow for more interpretability. In recent years, polarity lexicons have been improved by adding new phrases from web data (Velikovich et al., 2010) or by automatically learning domain-specific polarities (Hamilton et al., 2016). Sentprop is a framework for inducing domain-specific polarities (Hamilton et al., 2016). SentProp has been shown to accurately reproduce domain-specific lexicons from in-domain word embeddings. Additionally, it has been used to examine interesting historical and community-specific polarity shifts. We elaborate on SentProp by evaluating its use for domain adaptation of a general-purpose lexicon. Starting from a Dutch general polarity lexicon and a corpus of political forum posts, we automatically expand and enhance the general lexicon entries. We show that the enhanced lexicon outperforms the"
C18-1090,P97-1023,0,0.667165,"Missing"
C18-1090,E09-1046,0,0.0326709,"d by Levy et. al. (2015) who also provide a useful overview of hyper parameters in each of the discussed methods. In particular this led us to run 500-dimensional SVD-embeddings with a context-window of 1058 Source Politics.be 9lives.be Total Documents 1,778,101 434,261 2,212,362 Types 824,978 315,266 935,292 Tokens 68,596,562 28,006,269 96,602,831 Table 1: Corpus specifications. size six (Velikovich et al., 2010) with context-distribution smoothing and eigenvalue weighting (Levy et al., 2015). 3.2 General-purpose lexicon We used the DuOMAn subjectivity lexicon as our general-purpose lexicon (Jijkoun and Hofmann, 2009). With 8,782 entries, it is the largest polarity lexicon for Dutch. Polarity scores range from -2 (very negative) to 2 (very positive) and contains 3,631 zero (neutral) values. 3.3 Seed sets SentProp’s default pipeline requires the word embeddings and a positive and negative seed word set. We extracted 10 positive and 10 negative seed words by sorting candidates from DuOMan by polarity first and by occurrences in our corpus second. This provided us with a selection of the most positive and negative words that are also used relatively frequently. We then manually filtered words that have any so"
C18-1090,Q15-1016,0,0.125318,"Missing"
C18-1090,E09-1077,0,0.102549,"Missing"
C18-1090,S17-2088,0,0.0634219,"performs worse than the original lexicon in an out-domain task, showing that the words we added and the polarity shifts we applied are domain-specific and do not translate well to an out-domain setting. 1 Introduction Work in sentiment analysis can roughly be divided into two approaches: corpus based techniques and lexicon based techniques (Taboada et al., 2011). Corpus based techniques use supervised learning on large target domain corpora to learn classification models. Currently, implementations using deep learning make up the state of the art in binary polarity classification on Twitter (Rosenthal et al., 2017). Lexicon based techniques rely on high quality polarity lexicons. Document-level polarity can be predicted by looking at occurrences of lexical units in the document and doing some form of averaging on their respective polarity values. Despite not yielding the state of the art results of corpus based methods, lexicon based methods still see frequent use since they do not rely on big (labelled) data and allow for more interpretability. In recent years, polarity lexicons have been improved by adding new phrases from web data (Velikovich et al., 2010) or by automatically learning domain-specific"
C18-1090,J11-2001,0,0.327489,"rpose lexicon, for use in the political domain. By adding only top sentiment bearing words from the vocabulary and applying small polarity shifts in the general-purpose lexicon, we increase accuracy in an in-domain classification task. The enhanced lexicon performs worse than the original lexicon in an out-domain task, showing that the words we added and the polarity shifts we applied are domain-specific and do not translate well to an out-domain setting. 1 Introduction Work in sentiment analysis can roughly be divided into two approaches: corpus based techniques and lexicon based techniques (Taboada et al., 2011). Corpus based techniques use supervised learning on large target domain corpora to learn classification models. Currently, implementations using deep learning make up the state of the art in binary polarity classification on Twitter (Rosenthal et al., 2017). Lexicon based techniques rely on high quality polarity lexicons. Document-level polarity can be predicted by looking at occurrences of lexical units in the document and doing some form of averaging on their respective polarity values. Despite not yielding the state of the art results of corpus based methods, lexicon based methods still se"
C18-1090,N10-1119,0,0.367399,"in binary polarity classification on Twitter (Rosenthal et al., 2017). Lexicon based techniques rely on high quality polarity lexicons. Document-level polarity can be predicted by looking at occurrences of lexical units in the document and doing some form of averaging on their respective polarity values. Despite not yielding the state of the art results of corpus based methods, lexicon based methods still see frequent use since they do not rely on big (labelled) data and allow for more interpretability. In recent years, polarity lexicons have been improved by adding new phrases from web data (Velikovich et al., 2010) or by automatically learning domain-specific polarities (Hamilton et al., 2016). Sentprop is a framework for inducing domain-specific polarities (Hamilton et al., 2016). SentProp has been shown to accurately reproduce domain-specific lexicons from in-domain word embeddings. Additionally, it has been used to examine interesting historical and community-specific polarity shifts. We elaborate on SentProp by evaluating its use for domain adaptation of a general-purpose lexicon. Starting from a Dutch general polarity lexicon and a corpus of political forum posts, we automatically expand and enhanc"
C88-1028,C86-1144,0,0.04272,"Missing"
C88-1028,A88-1011,0,0.0613608,"Missing"
C88-1028,E87-1012,1,0.81102,"level in which word forms are represented as records with fields pointing to other records and fields containing various kinds of information, and a dynamic knowledge level in which word forms are instances of linguistic objects grouped in inheritance hierarchies, and have available to them (through inheritance) various kinds of linguistic knowledge and processes. This way new entries and new information associated with existing entries can be dynamically created, and (after checking by the user) stored in the lexical database. This lexical database architecture is described in more detail in Daelemans (1987a). 4. MORPHOLOGICAL ANALYSIS Morphological analysis consists of two stages: segmentation and parsing. The segmentation routine finds possible ways in which the input string can be partitioned into dictionary entries (working from right to left). In the present application, segmentation stops with the &apos;longest&apos; solution. Continuing to look for analyses with smaller dictionary entries leads to a considerable loss in processing efficiency and an increased risk at nonsense-analyses. The loss in accuracy is minimal (recall that the internal structure of word forms listed in the lexical database ca"
C88-1028,C86-1063,0,0.204067,"Missing"
C98-1078,H92-1022,0,0.0121164,"Missing"
C98-1078,W96-0102,1,\N,Missing
C98-1078,W96-0213,0,\N,Missing
canales-etal-2017-towards,J93-2004,0,\N,Missing
canales-etal-2017-towards,H05-1073,0,\N,Missing
canales-etal-2017-towards,D14-1162,0,\N,Missing
canales-etal-2017-towards,E14-1058,0,\N,Missing
canales-etal-2017-towards,N12-1071,0,\N,Missing
D08-1075,W06-2920,0,0.0134564,"chez-Graillet and Poesio, 2007). We processed the texts with the GENIA tagger (Tsuruoka and Tsujii, 2005; Tsuruoka et al., 2005), a bidirectional inference based tagger that analyzes English sentences and outputs the base forms, part-of-speech tags, chunk tags, and named entity tags in a tab-separated format2 . Additionally, we converted the annotation about scope of negation into a token-per-token representation. Table 1 shows an example sentence of the corpus that results from converting and processing the BioScope representation. Following the standard format of the CoNLL Shared Task 2006 (Buchholz and Marsi, 2006), sentences are separated by a blank line and fields are separated by a single tab character. A sentence consists of tokens, each one starting on a new line. A token consists of the following 10 fields: 1. ABSTRACT ID: number of the GENIA abstract. 2. SENTENCE ID: sentence counter starting at 1 for each new abstract. 3. TOKEN ID: token counter, starting at 1 for each new sentence. 4. FORM: word form or punctuation symbol. 5. LEMMA: lemma of word form. 6. POS TAG: Penn Treebank part-of-speech tags described in (Santorini, 1990). 7. CHUNK TAG: IOB (Inside, Outside, Begin) tags produced by the GE"
D08-1075,E99-1043,0,0.0342535,"the non-existence or uncertainty of something are annotated, in contrast to other corpora where only sentences of interest in the domain are annotated. A second characteristic is that the annotation is extended to the biggest syntactic unit possible so that scopes have the maximal length. In (2) below, negation signal no scopes over primary impairment of glucocorticoid metabolism instead of scoping only over primary. (2) There is [no] primary impairment of glucocorticoid metabolism in the asthmatics. 1 The part used in our experiments are the biological paper abstracts from the GENIA corpus (Collier et al., 1999). This part consists of 11,872 sentences in 1,273 abstracts. We automatically discarded five sentences due to annotation errors. The total number of words used is 313,222, 1,739 of which are negation signals that belong to the different types described in (Sanchez-Graillet and Poesio, 2007). We processed the texts with the GENIA tagger (Tsuruoka and Tsujii, 2005; Tsuruoka et al., 2005), a bidirectional inference based tagger that analyzes English sentences and outputs the base forms, part-of-speech tags, chunk tags, and named entity tags in a tab-separated format2 . Additionally, we converted"
D08-1075,W08-2128,1,0.867313,"Missing"
D08-1075,D07-1096,0,0.0131332,"Missing"
D08-1075,W95-0107,0,0.0128028,"have been obtained by converting the xml files of BioScope. Each token can have one or more NEG SCOPE tags, depending on the number of negation signals in the sentence. 4 Task description We approach the scope finding task as a classification task that consists of classifying the tokens of a sentence as being a negation signal or not, and as being inside or outside the scope of the negation signal(s). This happens as many times as there are 718 negation signals in the sentence. Our conception of the task is inspired by Ramshaw and Marcus’ representation of text chunking as a tagging problem (Ramshaw and Marcus, 1995) . The information that can be used to train the system appears in columns 1 to 8 of Table 1. The information to be predicted by the system is contained in columns 9 and 10. As far as we know, approaching the negation scope finding task as a token per token classification task is novel, whereas at the same time it conforms to the well established standards of the recent CoNLL Shared Tasks3 on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007) and semantic role labeling (Surdeanu et al., 2008). By setting up the task in this way we show that the negation scope finding task can be"
D08-1075,W08-2121,0,0.0271576,"Missing"
D08-1075,W08-0606,0,0.263472,"Missing"
D08-1075,H05-1059,0,0.0188566,"ucocorticoid metabolism instead of scoping only over primary. (2) There is [no] primary impairment of glucocorticoid metabolism in the asthmatics. 1 The part used in our experiments are the biological paper abstracts from the GENIA corpus (Collier et al., 1999). This part consists of 11,872 sentences in 1,273 abstracts. We automatically discarded five sentences due to annotation errors. The total number of words used is 313,222, 1,739 of which are negation signals that belong to the different types described in (Sanchez-Graillet and Poesio, 2007). We processed the texts with the GENIA tagger (Tsuruoka and Tsujii, 2005; Tsuruoka et al., 2005), a bidirectional inference based tagger that analyzes English sentences and outputs the base forms, part-of-speech tags, chunk tags, and named entity tags in a tab-separated format2 . Additionally, we converted the annotation about scope of negation into a token-per-token representation. Table 1 shows an example sentence of the corpus that results from converting and processing the BioScope representation. Following the standard format of the CoNLL Shared Task 2006 (Buchholz and Marsi, 2006), sentences are separated by a blank line and fields are separated by a single"
D12-1053,U10-1012,0,0.0347457,"Missing"
D12-1053,W03-1310,0,0.0193458,"Missing"
D12-1053,D08-1068,0,0.0172336,"n terms of the rhetorical categories Introduction, Methods, Results and Discussion (IMRAD) (Agarwal and Yu, 2009) or richer categories, such as problem-setting or insight (Mizuta et al., 2006). There exists a wide range of statistical relational learning systems (Getoor and Taskar, 2007; De Raedt et al., 2008), and many of these systems are in principle useful for natural language processing. The most popular formalism today is Markov Logic, which has already been used for natural language processing tasks such as semantic role labeling (Riedel and Meza-Ruiz, 2008) and coreference resolution (Poon and Domingos, 2008). With respect to Markov Logic, two distinguishing features of kLog are that 1) it employs kernel based methods grounded in statistical learning theory, and 2) it employs a Prolog like language for defining and using background knowledge. As Prolog is a programming language, this is more flexible that the formalism used by Markov Logic. 581 3 Methodology In learning from examples, or interpretations (De Raedt et al., 2008), the instances are sampled identically and independently from some unknown but fixed distribution. They can be represented as pairs z = (x, y), in which x represents the inp"
D12-1053,W08-2125,0,0.0105785,"al articles. In this case areas of text are classified in terms of the rhetorical categories Introduction, Methods, Results and Discussion (IMRAD) (Agarwal and Yu, 2009) or richer categories, such as problem-setting or insight (Mizuta et al., 2006). There exists a wide range of statistical relational learning systems (Getoor and Taskar, 2007; De Raedt et al., 2008), and many of these systems are in principle useful for natural language processing. The most popular formalism today is Markov Logic, which has already been used for natural language processing tasks such as semantic role labeling (Riedel and Meza-Ruiz, 2008) and coreference resolution (Poon and Domingos, 2008). With respect to Markov Logic, two distinguishing features of kLog are that 1) it employs kernel based methods grounded in statistical learning theory, and 2) it employs a Prolog like language for defining and using background knowledge. As Prolog is a programming language, this is more flexible that the formalism used by Markov Logic. 581 3 Methodology In learning from examples, or interpretations (De Raedt et al., 2008), the instances are sampled identically and independently from some unknown but fixed distribution. They can be represent"
D12-1053,D07-1111,0,0.0298776,"Missing"
D12-1053,J07-1005,0,\N,Missing
daelemans-etal-2004-automatic,W99-0707,1,\N,Missing
daelemans-etal-2004-automatic,W99-0629,1,\N,Missing
daelemans-etal-2004-automatic,C96-2183,0,\N,Missing
daelemans-etal-2004-automatic,E99-1011,0,\N,Missing
daelemans-etal-2004-automatic,P02-1040,0,\N,Missing
daelemans-etal-2004-automatic,P99-1037,1,\N,Missing
daelemans-etal-2004-automatic,W01-0100,0,\N,Missing
daelemans-etal-2004-automatic,vandeghinste-tjong-kim-sang-2004-using,1,\N,Missing
daelemans-hoste-2002-evaluation,W96-0208,0,\N,Missing
daelemans-hoste-2002-evaluation,W00-0720,1,\N,Missing
daelemans-hoste-2002-evaluation,P01-1005,0,\N,Missing
de-smedt-daelemans-2012-vreselijk,E09-1046,0,\N,Missing
de-smedt-daelemans-2012-vreselijk,J11-2001,0,\N,Missing
de-smedt-daelemans-2012-vreselijk,P04-1035,0,\N,Missing
de-smedt-daelemans-2012-vreselijk,W02-1011,0,\N,Missing
de-smedt-daelemans-2012-vreselijk,esuli-sebastiani-2006-sentiwordnet,0,\N,Missing
E09-1094,D07-1042,0,0.0845803,"Missing"
E09-1094,J05-1004,0,0.0155803,"tic role J in the nearest neighbor set (CJk ): X P S(RJ |Si ) = f (j) × ηj (5) j∈CJk The weight of an exemplar j (ηj ) is given by an exponential decay function, taken from Shepard (1987), over the distance between that exemplar and the target exemplar i (dij ): ηj = e−α×dij 829 (6) of 133566 verb-role-noun triples extracted from the Wall Street Journal and Brown parts of the Penn Treebank (Marcus et al., 1993). These were first annotated with semantic roles using a stateof-the-art semantic role labeling system (Koomen et al., 2005). Semantic roles are approximated by PropBank argument roles (Palmer et al., 2005). These consist of a limited set of numbered roles that are used for all verbs but are defined on a verb-by-verb basis. This contrasts with FrameNet roles, which are sense-specific. Hence PropBank roles provide a shallower level of semantic role annotation. They also do not refer consistently to the same semantic roles over different verbs, although the A0 and A1 roles in the majority of cases do correspond to the agent and patient roles, respectively. The A2 role refers to a third participant involved in the event, but the label can stand for several types of semantic roles, such as beneficia"
E09-1094,W05-0625,0,0.0338726,") is found by summing the distance-weighted frequency of all exemplars with semantic role J in the nearest neighbor set (CJk ): X P S(RJ |Si ) = f (j) × ηj (5) j∈CJk The weight of an exemplar j (ηj ) is given by an exponential decay function, taken from Shepard (1987), over the distance between that exemplar and the target exemplar i (dij ): ηj = e−α×dij 829 (6) of 133566 verb-role-noun triples extracted from the Wall Street Journal and Brown parts of the Penn Treebank (Marcus et al., 1993). These were first annotated with semantic roles using a stateof-the-art semantic role labeling system (Koomen et al., 2005). Semantic roles are approximated by PropBank argument roles (Palmer et al., 2005). These consist of a limited set of numbered roles that are used for all verbs but are defined on a verb-by-verb basis. This contrasts with FrameNet roles, which are sense-specific. Hence PropBank roles provide a shallower level of semantic role annotation. They also do not refer consistently to the same semantic roles over different verbs, although the A0 and A1 roles in the majority of cases do correspond to the agent and patient roles, respectively. The A2 role refers to a third participant involved in the eve"
E09-1094,P98-2127,0,0.0284707,"he model presented in Section 3.2: P j∈C k f (j) × ηj P S(RJ |Ti ) = P J (7) l∈C k f (l) × ηl Someone familiar with the literature on human categorization behavior might recognize Equation 7; this model is actually simply a Generalized Context Model (GCM) (Nosofsky, 1986), with the ‘context’ being restricted to the k nearest neighbors of the target item. Therefore, we will refer to this model using the shorthand kGCM. 4 4.1 4.3 To obtain the semantic distances between nouns and verbs for the calculation of the distance between exemplars (see Equation 3), we make use of a thesaurus compiled by Lin (1998), which lists the 200 nearest neighbors for a large number of English noun and verb lemmas, together with their similarity values. This resource was created by computing the similarity between word dependency vectors that are composed of frequency counts of (head, relation, dependent) triples (dependency triples) in a 64-million word parsed corpus. To compute these similarities, an information-theoretic similarity metric was used. The basic idea of this metric is that the similarity between two words is the amount of information contained in the commonality between the two words, i.e. the freq"
E09-1094,P97-1056,1,0.684945,"ould be its potential robustness for data sparsity, since similarity-based smoothing is an intrinsic part of the model. Even if neither the verb nor the argument of a verb-argument pair occur in the exemplar memory, role plausibilities can be predicted, as long as the similarity of the target exemplar’s semantic representation with the semantic representations in the exemplar memory can be calculated. An additional advantage of similaritybased smoothing is that it does not involve the estimation of an exponential number of smoothing parameters, as is the case for backed-off smoothing methods (Zavrel and Daelemans, 1997). For this study, we will implement three different kinds of exemplar-based models. The first model dij = wv × δ(vi , vj ) + wn × δ(ni , nj ) (3) We are not theoretically committed to any specific semantic representation or similarity metric for the computation of δ(vi , vj ) and δ(ni , nj ). The only requirement is that they should be able to distinguish nouns that typically occur in the same contexts, but in different roles (like writer and book), which probably excludes all vector-based approaches that do not take into account syntactic information (see also Pad´o et al. (2007)). In the nex"
E09-1094,J93-2004,0,0.0350123,"model (henceforth DD). Formally, the preference strength (P S) for a semantic role J (RJ ) given a verb-argument tuple i (Si ) is found by summing the distance-weighted frequency of all exemplars with semantic role J in the nearest neighbor set (CJk ): X P S(RJ |Si ) = f (j) × ηj (5) j∈CJk The weight of an exemplar j (ηj ) is given by an exponential decay function, taken from Shepard (1987), over the distance between that exemplar and the target exemplar i (dij ): ηj = e−α×dij 829 (6) of 133566 verb-role-noun triples extracted from the Wall Street Journal and Brown parts of the Penn Treebank (Marcus et al., 1993). These were first annotated with semantic roles using a stateof-the-art semantic role labeling system (Koomen et al., 2005). Semantic roles are approximated by PropBank argument roles (Palmer et al., 2005). These consist of a limited set of numbered roles that are used for all verbs but are defined on a verb-by-verb basis. This contrasts with FrameNet roles, which are sense-specific. Hence PropBank roles provide a shallower level of semantic role annotation. They also do not refer consistently to the same semantic roles over different verbs, although the A0 and A1 roles in the majority of cas"
E09-1094,J07-2002,0,0.107039,"Missing"
E09-1094,C98-2122,0,\N,Missing
E87-1012,C86-1020,0,0.0653497,"Missing"
E87-1012,C86-1107,0,\N,Missing
E93-1007,C88-1028,1,0.789302,"Missing"
E93-1007,E93-1023,0,0.0195413,"Missing"
hendrickx-etal-2008-coreference,J98-2001,0,\N,Missing
hendrickx-etal-2008-coreference,W96-0102,1,\N,Missing
hendrickx-etal-2008-coreference,M95-1005,0,\N,Missing
hendrickx-etal-2008-coreference,N01-1008,0,\N,Missing
hendrickx-etal-2008-coreference,A88-1003,0,\N,Missing
hendrickx-etal-2008-coreference,W02-1008,0,\N,Missing
hendrickx-etal-2008-coreference,W05-0303,0,\N,Missing
hendrickx-etal-2008-coreference,C02-1139,0,\N,Missing
hendrickx-etal-2008-coreference,N06-1025,0,\N,Missing
hendrickx-etal-2008-coreference,J01-4004,0,\N,Missing
hendrickx-etal-2008-coreference,P98-2143,0,\N,Missing
hendrickx-etal-2008-coreference,C98-2138,0,\N,Missing
hendrickx-etal-2008-coreference,hoste-de-pauw-2006-knack,1,\N,Missing
hendrickx-etal-2008-coreference,M95-1006,0,\N,Missing
hendrickx-etal-2008-coreference,M95-1011,0,\N,Missing
hendrickx-etal-2008-coreference,P03-1023,0,\N,Missing
J01-2002,P98-1003,0,0.0324594,"Missing"
J01-2002,W99-0628,0,0.0237749,"Missing"
J01-2002,J96-1002,0,0.0134234,"Missing"
J01-2002,A00-1031,0,0.0633477,"hods for unsupervised training of HMM&apos;s do exist, training is usually done in a supervised way by estimation of the above probabilities from relative frequencies in the training data. The HMM approach to tagging is by far the most studied and applied (Church 1988; DeRose 1988; Charniak 1993). In van Halteren, Zavrel, and Daelemans (1998) we used a straightforward implementation of HMM&apos;s, which turned out to have the worst accuracy of the four competing methods. In the present work, we have replaced this by the TnT system (we will refer to this tagger as HMM below). 2° TnT is a trigram tagger (Brants 2000), which means that it considers the previous two tags as features for deciding on the current tag. Moreover, it considers the capitalization of the previous word as well in its state representation. The lexical probabilities depend on the identity of the current word for known words and on a suffix tree smoothed with successive abstraction (Samuelsson 1996) for guessing the tags of unknown words. As we will see below, it shows a surprisingly higher accuracy than our previous HMM implementation. When we compare it with the other taggers used in this paper, we see that a trigram HMM tagger uses"
J01-2002,H92-1022,0,0.0481784,"-2 1 p o p u l a r type of learning method, each uses slightly different features of the text (see Table 1), and each has a completely different representation for its language model. All publicly available systems are used with the default settings that are suggested in their documentation. 3.2.1 Error-driven Transformation-based Learning. This learning m e t h o d finds a set of rules that transforms the corpus from a baseline annotation so as to minimize the n u m b e r of errors (we will refer to this system as TBL below). A tagger generator using this learning m e t h o d is described in Brill (1992, 1994). The implementation that we use is Eric Brill&apos;s publicly available set of C p r o g r a m s and Perl scripts. 16 W h e n training, this system starts with a baseline corpus annotation A0. In A0, each k n o w n w o r d is tagged with its most likely tag in the training set, and each u n k n o w n w o r d is tagged as a n o u n (or p r o p e r n o u n if capitalized). The system then searches t h r o u g h a space of transformation rules (defined b y rule templates) in order to reduce the discrepancy between its current annotation and the p r o v i d e d correct one. There are separate t"
J01-2002,P98-1029,0,0.26839,"Missing"
J01-2002,E99-1025,0,0.0332837,"Missing"
J01-2002,A88-1019,0,0.0790155,"it the words of a sentence with a probability P(w [ St), the states St themselves model tags or sequences of tags. The transitions are controlled by Markovian state transition probabilities P(Stl ] Sti_l ). Because a sentence could have been generated by a number of different state sequences, the states are considered to be &quot;Hidden.&quot; Although methods for unsupervised training of HMM&apos;s do exist, training is usually done in a supervised way by estimation of the above probabilities from relative frequencies in the training data. The HMM approach to tagging is by far the most studied and applied (Church 1988; DeRose 1988; Charniak 1993). In van Halteren, Zavrel, and Daelemans (1998) we used a straightforward implementation of HMM&apos;s, which turned out to have the worst accuracy of the four competing methods. In the present work, we have replaced this by the TnT system (we will refer to this tagger as HMM below). 2° TnT is a trigram tagger (Brants 2000), which means that it considers the previous two tags as features for deciding on the current tag. Moreover, it considers the capitalization of the previous word as well in its state representation. The lexical probabilities depend on the identity of"
J01-2002,W96-0102,1,0.605679,"istic. Corpus statistics have been at the basis of selecting the rule sequence, but the resulting tagger does not explicitly use a probabilistic model. 3.2.2 Memory-Based Learning. Another learning method that does not explicitly manipulate probabilities is machine-based learning. However, rather than extracting a concise set of rules, memory-based learning focuses on storing all examples of a task in memory in an efficient way (see Section 2.3). New examples are then classified by similarity-based reasoning from these stored examples. A tagger using this learning method, MBT, was proposed by Daelemans et al. (1996). TM During the training phase, the training corpus is transformed into two case bases, one which is to be used for known words and one for unknown words. The cases are stored in an IGTree (a heuristically indexed version of a case memory [Daelemans, Van den Bosch, and Weijters 1997]), and during tagging, new cases are classified by matching cases with those in memory going from the most important feature to the least important. The order of feature relevance is determined by Information Gain. For known words, the system used here has access to information about the focus word and its potentia"
J01-2002,J88-1003,0,0.107252,"of a sentence with a probability P(w [ St), the states St themselves model tags or sequences of tags. The transitions are controlled by Markovian state transition probabilities P(Stl ] Sti_l ). Because a sentence could have been generated by a number of different state sequences, the states are considered to be &quot;Hidden.&quot; Although methods for unsupervised training of HMM&apos;s do exist, training is usually done in a supervised way by estimation of the above probabilities from relative frequencies in the training data. The HMM approach to tagging is by far the most studied and applied (Church 1988; DeRose 1988; Charniak 1993). In van Halteren, Zavrel, and Daelemans (1998) we used a straightforward implementation of HMM&apos;s, which turned out to have the worst accuracy of the four competing methods. In the present work, we have replaced this by the TnT system (we will refer to this tagger as HMM below). 2° TnT is a trigram tagger (Brants 2000), which means that it considers the previous two tags as features for deciding on the current tag. Moreover, it considers the capitalization of the previous word as well in its state representation. The lexical probabilities depend on the identity of the current w"
J01-2002,W99-0623,0,0.0691286,"Missing"
J01-2002,J93-2004,0,0.0509357,"Missing"
J01-2002,W99-0608,0,0.0643732,"Missing"
J01-2002,W96-0213,0,0.735561,"test case, and returning the most frequent class at the last node. The case representation uses exactly the same features as the memory-based learner. 3. Experimental Setup In order to test the potential of system combination, we obviously need systems to combine, i.e., a n u m b e r of different taggers. As we are primarily interested in the 5 This is also sometimes referred to as mutual information in the computational linguistics literature. 6 Maccent is available from http://www.cs.kuleuven.ac.be/~ldh. 7 For a more detailed discussion, see Berger, Della Pietra, and Della Pietra (1996) and Ratnaparkhi (1996). 8 c5.0 is commerciallyavailable from http://www.rulequest.com/. Its predecessor, c4.5, can be downloaded from http://www.cse.unsw.edu.au/~quinlan/. 206 van Halteren, Zavrel, and Daelemans Combination of Machine Learning Systems combination of classifiers trained on the same data sets, we are in fact looking for data sets (in this case, tagged corpora) and systems that can automatically generate a tagger on the basis of those data sets. For the current experiments, we have selected three tagged corpora and four tagger generators. Before giving a detailed description of each of these, we first"
J01-2002,P97-1007,0,0.0282115,"Missing"
J01-2002,C96-2151,0,0.00962173,"ghtforward implementation of HMM&apos;s, which turned out to have the worst accuracy of the four competing methods. In the present work, we have replaced this by the TnT system (we will refer to this tagger as HMM below). 2° TnT is a trigram tagger (Brants 2000), which means that it considers the previous two tags as features for deciding on the current tag. Moreover, it considers the capitalization of the previous word as well in its state representation. The lexical probabilities depend on the identity of the current word for known words and on a suffix tree smoothed with successive abstraction (Samuelsson 1996) for guessing the tags of unknown words. As we will see below, it shows a surprisingly higher accuracy than our previous HMM implementation. When we compare it with the other taggers used in this paper, we see that a trigram HMM tagger uses a very limited set of features (Table 1). o n the other hand, it is able to access some information about the rest of the sentence indirectly, through its use of the Viterbi algorithm. 4. Overall Results The first set of results from our experiments is the measurement of overall accuracy for the base taggers. In addition, we can observe the agreement betwee"
J01-2002,W00-0724,1,0.777233,"Missing"
J01-2002,W00-0734,1,0.796618,"Missing"
J01-2002,P98-1081,1,0.620957,"Missing"
J01-2002,zavrel-daelemans-2000-bootstrapping,1,0.888638,"Missing"
J01-2002,W99-0606,0,\N,Missing
J01-2002,C98-1003,0,\N,Missing
J01-2002,C98-1078,1,\N,Missing
J01-2002,C98-1029,0,\N,Missing
J92-2004,P88-1021,0,0.0202737,"uistic work on pragmatics has tended to turn to general nonstandard logics as tools for the job, rather than their less expressive network relations. Thus Joshi et al. (1984) and Lascarides and Asher (1991) have made the case for nonmonotonic logics in formalizing Gricean maxims, while Schubert and Hwang (1989) show how a probabilistic logic might be Used in story understanding. Mercer and Reiter (1982) and Mercer (1988) have employed Reiter's default logic to capture the behavior of natural language presuppositions. Perrault (1990) uses default logic to express a theory of speech acts, while Appelt and Konolige (1988) have deployed an extension to Moore's (1985) autoepistemic logic for the same purpose. 5. Concluding Remarks Within computational linguistics, it is possible to see three distinct trends emerging. The first is the increasing employment of monotonic type lattices in unification~based grammars to constrain the space of permissible descriptions. The second is the use of a variety of general nonmonotonic logics for formalizing pragmatic components of NLP systems. And the third is the development of a variety of restricted default inheritance languages designed for the representation of phonologic"
J92-2004,J92-2003,0,0.189579,"Missing"
J92-2004,E89-1008,0,0.194702,"e made extensive use of DATR networks to describe lexical morphophonological phenomena such as German umlaut, Kikuyu tone, and Arabic vowel intercalation (Gibbon 1990b, in press; Reinhard 1990; Reinhard and Gibbon 1991). And Daelemans (1987a,b, 1988) uses the object-oriented knowledge representation language KRS to implement default orthogonal inheritance networks for the lexical representation of phonological, orthographic, and morphological knowledge of Dutch and shows how such a lexicon architecture can be used for both language generation and automatic dictionary construction. The work of Calder (1989) and his associates at Edinburgh and Stuttgart on ""paradigmatic morphology"" also fits within this tradition in that it invokes a restricted kind of default orthogonal inheritance for morphophonological description. However, the emphasis in this work is on the use of string unification to define morphological operations rather than on the default structure of the lexicon per se. In subsequent work, Calder and Bird (1991) use a general 212 Walter Daelernans et al. Inheritance in Natural Language Processing nonmonotonic logic to give a formal reconstruction of ""underspecification phonology"" (Arch"
J92-2004,P91-1002,0,0.0204032,"nne (in press) extend the analysis further still so as to encompass some of the trickiest and most-debated data in the syntax of English. Pollard and Sag (1987), in the first book-length presentation of HPSG, treat the lexicon as a monotonic multiple-inheritance type hierarchy. They implicitly reject the use of an ""overriding mechanism"" (p. 194, n4) in favor of a variety of restrictions designed to prevent overgeneration, together with a nonmonotonic formulation of lexical rules (pp. 212-213). A concern to preserve monotonic inheritance in HPSG is likewise evident in more recent work, such as Carpenter and Pollard (1991) and Zajac (this issue). Monotonic multiple inheritance type hierarchies figure in a good deal of recent work in unification-based grammars. Examples include papers by Porter (1987), Emele and Zajac (1990), and Emele et al. (1990), who all use a semantics based on Ait-Kaci (1984); the use of sorts in Unification Categorial Grammar (Moens et al. 1989); the CLE project (Alshawi et al. 1989) and theoretical work by Smolka (1988). Default multiple inheritance also figures centrally in a couple of grammatical frameworks. One is Hudson's (1984, 1990) Word Grammar, and a detailed exposition is provid"
J92-2004,W91-0209,0,0.0586268,"KL-ONE) to express selectional restrictions, while Andry et al. (in press) use DATR for the same purpose. Cahill and Evans (1990) use DATR to build up complex lambda calculus representations in the lexicon of a message understanding system. Briscoe et al. (1990) use a version of PATR augmented with defeasible templates to implement a default orthogonal inheritance network for a Pustejovskian analysis of metonymic sense extension in lexical semantics (e.g. interpreting the film in Enjoy the film! as watching the film). 5 Their approach is further elaborated in Briscoe and Copestake (1991) and Copestake and Briscoe (1991). A semantic analog of the monotonic type hierarchies discussed above in connection with syntax is manifested in the situation theoretic ""infon lattices"" introduced by Kameyama et al. (1991) to deal with meaning mismatches in machine translation. The use of inheritance networks for specifically linguistic pragmatic purposes (as opposed to general reasoning) is notable largely for its absence. The only example we know of is Etherington et al.'s (1989) proposal to represent the consequences of Gricean 4 Compare Gibbon's (1990a) use of DATR to the same end. 5 See Pustejovsky (1989, 1991). 213 Com"
J92-2004,C90-3052,0,0.0150508,"eat the lexicon as a monotonic multiple-inheritance type hierarchy. They implicitly reject the use of an ""overriding mechanism"" (p. 194, n4) in favor of a variety of restrictions designed to prevent overgeneration, together with a nonmonotonic formulation of lexical rules (pp. 212-213). A concern to preserve monotonic inheritance in HPSG is likewise evident in more recent work, such as Carpenter and Pollard (1991) and Zajac (this issue). Monotonic multiple inheritance type hierarchies figure in a good deal of recent work in unification-based grammars. Examples include papers by Porter (1987), Emele and Zajac (1990), and Emele et al. (1990), who all use a semantics based on Ait-Kaci (1984); the use of sorts in Unification Categorial Grammar (Moens et al. 1989); the CLE project (Alshawi et al. 1989) and theoretical work by Smolka (1988). Default multiple inheritance also figures centrally in a couple of grammatical frameworks. One is Hudson's (1984, 1990) Word Grammar, and a detailed exposition is provided by Fraser and Hudson in this issue. Word Grammar is a feature-based variant of dependency grammar, one that makes pervasive use of a (multiple) inheritance relation. The latter is unusual in that stipul"
J92-2004,C90-3019,0,0.010976,"ic multiple-inheritance type hierarchy. They implicitly reject the use of an ""overriding mechanism"" (p. 194, n4) in favor of a variety of restrictions designed to prevent overgeneration, together with a nonmonotonic formulation of lexical rules (pp. 212-213). A concern to preserve monotonic inheritance in HPSG is likewise evident in more recent work, such as Carpenter and Pollard (1991) and Zajac (this issue). Monotonic multiple inheritance type hierarchies figure in a good deal of recent work in unification-based grammars. Examples include papers by Porter (1987), Emele and Zajac (1990), and Emele et al. (1990), who all use a semantics based on Ait-Kaci (1984); the use of sorts in Unification Categorial Grammar (Moens et al. 1989); the CLE project (Alshawi et al. 1989) and theoretical work by Smolka (1988). Default multiple inheritance also figures centrally in a couple of grammatical frameworks. One is Hudson's (1984, 1990) Word Grammar, and a detailed exposition is provided by Fraser and Hudson in this issue. Word Grammar is a feature-based variant of dependency grammar, one that makes pervasive use of a (multiple) inheritance relation. The latter is unusual in that stipulated exceptions do not au"
J92-2004,E89-1009,1,0.871662,"lymeans that the corresponding regular form is not a permissible option. This is known as ""blocking."" 211 Computational Linguistics Volume 18, Number 2 The other is ELU (Russell et al. in press), which extends a PATR-like grammar formalism with a language for defining default multiple inheritance networks for the lexicon. Inspired by CLOS, an object-oriented extension of Common LISP, they adopt prioritized inheritance to escape the problem caused by conflicting inherited information. Russell et al. (in press) illustrate their approach with ELU analyses of English and German verbal morphology. Evans and Gazdar (1989a) outline the syntax and theory of inference for DATR, a language for lexical knowledge representation, and (1989b) they provide a semantics for the language that is loosely based on the approach taken by Moore (1985) in his semantics for autoepistemic logic. DATR allows multiple default inheritance but enforces orthogonality. Evans et al. (in press) show how DATR can also be used to encode certain kinds of prioritized inheritance. Unlike ELU and the Word Grammar notation, DATR is not intended to be a full grammar formalism. Rather, it is intended to be a lexical formalism that can be used wi"
J92-2004,P85-1032,0,0.0139003,"Missing"
J92-2004,J92-2001,0,0.0620105,"Missing"
J92-2004,C86-1130,0,0.0163084,"ral language understanding that goes back at least as far as Simmons (1973). Much of the work in this tradition has concerned itself with domain and world knowledge relevant to disambiguation and to drawing inferences from what is said, but not to the semantic representations of words, phrases and utterances per se. Exceptions to this generalization are not hard to find, however. For example, Barnett et al. (1990) use the same language (CycL) for linguistic semantic representation as is used in the encyclopedic inheritance network for which they are providing a natural language interface. And Jacobs (1986, 1987) proposes a uniform hierarchical encoding of both linguistic and conceptual knowledge in a framebased formalism called ACE. Jacobs then uses the resulting inheritance network to give an account of metaphor, inter alia. By contrast, Allgayer et al. (1989) employ two separate inheritance networks, one for linguistic semantic knowledge and the other for conceptual knowledge, both being implemented in a KL-ONE derivative called SB-ONE. Several of the inheritance-based linguistic knowledge representation formalisms that we have introduced in earlier sections are being used for semantic purpo"
J92-2004,P88-1024,0,0.102656,"ed that ""the cost of such a move is great, however, because the use 210 Walter Daelemans et al. Inheritance in Natural Language Processing of overwriting eliminates the order independence that is so advantageous a property in a formalism"" (1986, p. 60). In a subsequent implementation of PATR, Karttunen (1986) makes all D-PATR templates subject to overwriting. The very similar notion of ""priority union"" is introduced in the context of LFG by Kaplan (1987, p. 180). These ideas are developed by Bouma (this issue) who gives a definition of default unification on the basis of a logic for features. Kameyama (1988) uses PATR-style templates to build a multiple inheritance multilingual lexicon to support Categorial Unification Grammar descriptions of Arabic, English, French, German, and Japanese nominals. Although the system described is monotonic, there is a footnote suggesting a move toward a default inheritance system to deal with marked constituent orders (p. 202, nl0). Of all the unification-based grammar formalisms, it is HPSG which has thus far led to the greatest use of inheritance networks, both default and monotonic. Flickinger, Pollard, and Wasow (1985) proposed a treatment of lexical organiza"
J92-2004,P91-1025,0,0.0115682,"he lexicon of a message understanding system. Briscoe et al. (1990) use a version of PATR augmented with defeasible templates to implement a default orthogonal inheritance network for a Pustejovskian analysis of metonymic sense extension in lexical semantics (e.g. interpreting the film in Enjoy the film! as watching the film). 5 Their approach is further elaborated in Briscoe and Copestake (1991) and Copestake and Briscoe (1991). A semantic analog of the monotonic type hierarchies discussed above in connection with syntax is manifested in the situation theoretic ""infon lattices"" introduced by Kameyama et al. (1991) to deal with meaning mismatches in machine translation. The use of inheritance networks for specifically linguistic pragmatic purposes (as opposed to general reasoning) is notable largely for its absence. The only example we know of is Etherington et al.'s (1989) proposal to represent the consequences of Gricean 4 Compare Gibbon's (1990a) use of DATR to the same end. 5 See Pustejovsky (1989, 1991). 213 Computational Linguistics Volume 18, Number 2 maxims in a default inheritance network designed for fast (though not necessarily correct) reasoning. Most recent computational linguistic work on"
J92-2004,C86-1016,0,0.00930286,"nification grammars began in the mid1980s. Shieber (1986b, p. 57ff) noted that the provision of lexical ""templates"" in PATR amounted to a language for defining monotonic multiple inheritance networks. He drew attention to the possibility of adding a nonmonotonic ""overwriting"" operation to PATR and commented that ""the cost of such a move is great, however, because the use 210 Walter Daelemans et al. Inheritance in Natural Language Processing of overwriting eliminates the order independence that is so advantageous a property in a formalism"" (1986, p. 60). In a subsequent implementation of PATR, Karttunen (1986) makes all D-PATR templates subject to overwriting. The very similar notion of ""priority union"" is introduced in the context of LFG by Kaplan (1987, p. 180). These ideas are developed by Bouma (this issue) who gives a definition of default unification on the basis of a logic for features. Kameyama (1988) uses PATR-style templates to build a multiple inheritance multilingual lexicon to support Categorial Unification Grammar descriptions of Arabic, English, French, German, and Japanese nominals. Although the system described is monotonic, there is a footnote suggesting a move toward a default in"
J92-2004,E91-1024,0,0.0609207,"lexical knowledge representation, and (1989b) they provide a semantics for the language that is loosely based on the approach taken by Moore (1985) in his semantics for autoepistemic logic. DATR allows multiple default inheritance but enforces orthogonality. Evans et al. (in press) show how DATR can also be used to encode certain kinds of prioritized inheritance. Unlike ELU and the Word Grammar notation, DATR is not intended to be a full grammar formalism. Rather, it is intended to be a lexical formalism that can be used with any grammar that can be encoded in terms of attributes and values. Kilbury et al. (1991) show how a DATR lexicon can be linked to a PATR syntax, while Andry et al. (in press) employ a DATR lexicon in the context of a Unification Categorial Grammar. The use of DATR to describe morphology is illustrated, for Latin, by Gazdar (in press) and in the fragments of Arabic, English, German, and Japanese included in Evans and Gazdar (1990). All of our discussion thus far has presupposed the use of inheritance networks to store essentially static information. But, following the precedent set by Brachman and Schmolze (1985), a number of researchers have begun to explore their utility in lang"
J92-2004,P91-1008,0,0.0325499,"ning) is notable largely for its absence. The only example we know of is Etherington et al.'s (1989) proposal to represent the consequences of Gricean 4 Compare Gibbon's (1990a) use of DATR to the same end. 5 See Pustejovsky (1989, 1991). 213 Computational Linguistics Volume 18, Number 2 maxims in a default inheritance network designed for fast (though not necessarily correct) reasoning. Most recent computational linguistic work on pragmatics has tended to turn to general nonstandard logics as tools for the job, rather than their less expressive network relations. Thus Joshi et al. (1984) and Lascarides and Asher (1991) have made the case for nonmonotonic logics in formalizing Gricean maxims, while Schubert and Hwang (1989) show how a probabilistic logic might be Used in story understanding. Mercer and Reiter (1982) and Mercer (1988) have employed Reiter's default logic to capture the behavior of natural language presuppositions. Perrault (1990) uses default logic to express a theory of speech acts, while Appelt and Konolige (1988) have deployed an extension to Moore's (1985) autoepistemic logic for the same purpose. 5. Concluding Remarks Within computational linguistics, it is possible to see three distinct"
J92-2004,E89-1024,0,0.0259624,"f a variety of restrictions designed to prevent overgeneration, together with a nonmonotonic formulation of lexical rules (pp. 212-213). A concern to preserve monotonic inheritance in HPSG is likewise evident in more recent work, such as Carpenter and Pollard (1991) and Zajac (this issue). Monotonic multiple inheritance type hierarchies figure in a good deal of recent work in unification-based grammars. Examples include papers by Porter (1987), Emele and Zajac (1990), and Emele et al. (1990), who all use a semantics based on Ait-Kaci (1984); the use of sorts in Unification Categorial Grammar (Moens et al. 1989); the CLE project (Alshawi et al. 1989) and theoretical work by Smolka (1988). Default multiple inheritance also figures centrally in a couple of grammatical frameworks. One is Hudson's (1984, 1990) Word Grammar, and a detailed exposition is provided by Fraser and Hudson in this issue. Word Grammar is a feature-based variant of dependency grammar, one that makes pervasive use of a (multiple) inheritance relation. The latter is unusual in that stipulated exceptions do not automatically override an inherited default: the latter has to be explicitly negated if the grammar requires its suppression"
J92-2004,P87-1032,0,0.0253058,"ion of HPSG, treat the lexicon as a monotonic multiple-inheritance type hierarchy. They implicitly reject the use of an ""overriding mechanism"" (p. 194, n4) in favor of a variety of restrictions designed to prevent overgeneration, together with a nonmonotonic formulation of lexical rules (pp. 212-213). A concern to preserve monotonic inheritance in HPSG is likewise evident in more recent work, such as Carpenter and Pollard (1991) and Zajac (this issue). Monotonic multiple inheritance type hierarchies figure in a good deal of recent work in unification-based grammars. Examples include papers by Porter (1987), Emele and Zajac (1990), and Emele et al. (1990), who all use a semantics based on Ait-Kaci (1984); the use of sorts in Unification Categorial Grammar (Moens et al. 1989); the CLE project (Alshawi et al. 1989) and theoretical work by Smolka (1988). Default multiple inheritance also figures centrally in a couple of grammatical frameworks. One is Hudson's (1984, 1990) Word Grammar, and a detailed exposition is provided by Fraser and Hudson in this issue. Word Grammar is a feature-based variant of dependency grammar, one that makes pervasive use of a (multiple) inheritance relation. The latter i"
J92-2004,J91-4003,0,0.0356659,"Missing"
J92-2004,E91-1023,0,0.041713,"nference over an inheritance network. 3. Phonology, Orthography, and Morphophonology Computational phonology is perhaps the youngest and least studied branch of NLP. But notions of default have played such a prominent role in linguistic discussion of the area that it is not surprising that default inheritance networks have found a place in this subfield right from the start. Thus Gibbon and Reinhard have made extensive use of DATR networks to describe lexical morphophonological phenomena such as German umlaut, Kikuyu tone, and Arabic vowel intercalation (Gibbon 1990b, in press; Reinhard 1990; Reinhard and Gibbon 1991). And Daelemans (1987a,b, 1988) uses the object-oriented knowledge representation language KRS to implement default orthogonal inheritance networks for the lexical representation of phonological, orthographic, and morphological knowledge of Dutch and shows how such a lexicon architecture can be used for both language generation and automatic dictionary construction. The work of Calder (1989) and his associates at Edinburgh and Stuttgart on ""paradigmatic morphology"" also fits within this tradition in that it invokes a restricted kind of default orthogonal inheritance for morphophonological desc"
J92-2004,C86-1050,0,0.515374,"ecedence over others has been a continuing concern in the literature (see, e.g., Pullum 1979, especially Section 1.4.1, and references therein). The consensus view, though largely unspoken, among computational linguists currently working with default inheritance networks appears to be that nodes that are close (or identical) to the root(s) of the network should be used to encode that which is regular, ""unmarked,"" and productive, and that distance from the root(s) should correlate with increasing irregularity, ""markedness,"" and lack of productivity. At the 2 See Evans (1987), Gazdar (1987),and Shieber (1986a) on the various defaulty characteristics of GPSG. 209 Computational Linguistics Volume 18, Number 2 very least, this is what emerges from their practice. The differences between the current strands of NLP work in this area are partly philosophical (e.g., as to whether psycholinguistic data could or should be relevant to the structure of the network), partly methodological (e.g., as to whether networks should be built in a formal language designed for the purpose or implemented in an existing computer language), partly technical (e.g., whether a negation operator is useful, or whether orthogo"
J92-2004,J92-2005,0,0.0286319,"Missing"
J92-2004,P89-1024,0,0.0127135,"r et al. (1989) employ two separate inheritance networks, one for linguistic semantic knowledge and the other for conceptual knowledge, both being implemented in a KL-ONE derivative called SB-ONE. Several of the inheritance-based linguistic knowledge representation formalisms that we have introduced in earlier sections are being used for semantic purposes. Thus Fraser and Hudson (this issue) make crucial use of the Word Grammar inheritance network to reconstruct the meanings of various types of constituent (e.g. verb phrases) that cannot be syntactically reconstructed in a dependency grammar. Weischedel (1989) uses the taxonomic language NKL (based on KL-ONE) to express selectional restrictions, while Andry et al. (in press) use DATR for the same purpose. Cahill and Evans (1990) use DATR to build up complex lambda calculus representations in the lexicon of a message understanding system. Briscoe et al. (1990) use a version of PATR augmented with defeasible templates to implement a default orthogonal inheritance network for a Pustejovskian analysis of metonymic sense extension in lexical semantics (e.g. interpreting the film in Enjoy the film! as watching the film). 5 Their approach is further elabo"
J92-2004,J92-2002,0,0.0494391,"Missing"
J92-2004,J92-3001,0,\N,Missing
J92-2004,J92-3002,0,\N,Missing
J92-2004,E87-1012,1,\N,Missing
J92-2004,J92-3003,0,\N,Missing
J94-3007,C90-3044,0,0.0155046,"Missing"
J94-4007,J92-3003,0,0.0191763,"rticles were concerned with the use of inheritance in the lexicon as well. Some of the articles in both collections even overlap. Readers interested in developments in the field after this CL special issue may be disappointed, however. Most of the papers in the book do not differ very much from the version in the Acquilex proceedings. This leads in some cases to the paradoxical situation that the work described in this book (available only in early 1994) was already out of date when the 1992 CL special issue appeared. The article by Russell et al., for example, lacks the scope and examples of Russell et al. 1992, and the TFS system described in the article by Zajac lacks the completeness of the description in Zajac 1992. Which lexical representation formalism is preferable is still an open research question as some of the complexities, limitations, and subtleties of the different proposals become apparent. Current research, as evidenced in this book, focuses on special-purpose default inheritance formalisms or limited extensions of unificationbased feature-structure formalisms. This is acceptable because until recently lexical semantics and pragmatics have remained relatively undeveloped. More sophis"
J94-4007,J92-2002,0,0.0719962,"n overlap. Readers interested in developments in the field after this CL special issue may be disappointed, however. Most of the papers in the book do not differ very much from the version in the Acquilex proceedings. This leads in some cases to the paradoxical situation that the work described in this book (available only in early 1994) was already out of date when the 1992 CL special issue appeared. The article by Russell et al., for example, lacks the scope and examples of Russell et al. 1992, and the TFS system described in the article by Zajac lacks the completeness of the description in Zajac 1992. Which lexical representation formalism is preferable is still an open research question as some of the complexities, limitations, and subtleties of the different proposals become apparent. Current research, as evidenced in this book, focuses on special-purpose default inheritance formalisms or limited extensions of unificationbased feature-structure formalisms. This is acceptable because until recently lexical semantics and pragmatics have remained relatively undeveloped. More sophisticated theories in this area (e.g., Pustejovsky 1991) and descriptions of less-studied languages may soon req"
K18-1008,L18-1427,1,0.295374,"are named after, and based on the work of, Wickelgren (1969). As we saw above, slot-based encodings predict that words which are not aligned are completely dissimilar. Wickelgraphs attempt to overcome this downside by representing words as sets of contiguous ngrams, where n is usually set to 3, and n − 1 padding characters are added to the start and end of each word. For example, the word “SALT” has the following wickelgraph representation: {##S, #SA, SAL, ALT, LT#, T##}. 3.2 Features We use four different orthographic feature sets. All the feature sets were previously implemented in wordkit (Tulkens et al., 2018). 3.2.1 Wickelgraphs Slots The two slot-based feature encodings are created by left-justifying strings, padding them with spaces to the length of the longest word in our corpus, and then replacing each letter in each resulting slot by a feature vector. These feature vectors are then concatenated to create a final feature vector. As noted in the introduction, these types of encodings are thought to be unrealistic (Grainger and Van Heuven, 2004; Davis and Bowers, 2006), as they predict that words which are not aligned have low similarity. The words “STAR” and “TAR”, for example, have a similarit"
L16-1258,I13-1041,0,0.0608582,"Missing"
L16-1258,D11-1120,0,0.0508224,"one, while this is much less the case for the other two dimensions. However, for languages for which we have fewer than 500 authors, namely Italian and German, the model usually does not outperform the majority baseline. Discussion and related work Because different dataset types and sizes, collection methods, evaluation metrics, and preprocessing methods make direct comparisons impossible, we conclude from our gender identification results that they are comparable to or better than the best published results on gender identification from Twitter for the different languages in our corpus. See Burger et al. (2011) for another comparative multilingual study on gender identification from twitter data, but using an approach that is difficult to compare to ours (learning all languages with one classifier). Predicting Myers-Briggs type indicators from linguistic input has been studied in the seminal paper of Luyckx and Daelemans (2008). They created a corpus for Dutch, consisting of 145 student essays about a documentary on artificial life. Recently, the CSI (CLiPS Stylometry Investigation) corpus was introduced, which includes Dutch reviews as well as essays and annotations for both Big Five and MBTI annot"
L16-1258,D13-1114,0,0.0879019,"Missing"
L16-1258,P11-1137,0,0.0173024,"Missing"
L16-1258,N13-1037,0,0.0238928,"Missing"
L16-1258,P12-3005,0,0.0437849,"Missing"
L16-1258,W14-1303,0,0.0339602,"Missing"
L16-1258,luyckx-daelemans-2008-personae,1,0.803938,"ted random baseline, which should be regarded as the main point of comparison. For four languages (Dutch, French, Portuguese, Spanish) our model even outperforms the higher majority baseline consistently for two dimensions, namely I NTROVERT–E XTRAVERT and T HINKING – F EELING. The other two dimensions are more difficult to predict and our model does not reach majority baseline (with only one exception, S–N for Dutch). This has been observed earlier on English by Plank and Hovy (2015). They found the exact same dimensions where no improvement was achieved, and a similar trend was described by Luyckx and Daelemans (2008) for the last dimension (J–P). This suggests that I NTROVERT–E XTRAVERT as well as T HINKING –F EELING are predictable from linguistic input alone, while this is much less the case for the other two dimensions. However, for languages for which we have fewer than 500 authors, namely Italian and German, the model usually does not outperform the majority baseline. Discussion and related work Because different dataset types and sizes, collection methods, evaluation metrics, and preprocessing methods make direct comparisons impossible, we conclude from our gender identification results that they ar"
L16-1258,D15-1130,0,0.0823792,"Missing"
L16-1258,W11-1515,0,0.105661,"Missing"
L16-1258,W15-2913,1,0.777475,"hich are written in highly canonical language. Such controlled settings inhibit the expression of individual traits much more than spontaneous language. As such data is hard to obtain, only limited amounts were available. With the availability of social media text, recent efforts shifted toward using such data (Schwartz et al., 2013a; Schwartz et al., 2013b; Park et al., 2015; Kosinski et al., 2015). For example, Kosinski et al. (2015) collected a large amount of social media data with Big Five (Kosinski et al., 2015) annotations through a tailored Facebook app. Another approach, suggested by Plank and Hovy (2015), is to use the large amounts of textual data voluntarily produced on social media (i.e., Twitter) together with self-assessed Myers-Briggs Type Indicators (Briggs Myers and Myers, 2010), abbreviated MBTI, to collect large amounts of labeled data. Myers-Briggs classifies users along four dimensions (I NTROVERT–E XTRAVERT, I N TUITIVE –S ENSING , T HINKING –F EELING , J UDGING – P ERCEIVING), amounting to 16 different types, e.g., INTJ, ESFP, etc. As such, Myers-Briggs personality types have the distinct advantage of being readily available in large quantities on social media, in particular Twi"
L16-1258,W15-1203,0,0.226107,"al and McKeown, 2011; Nguyen et al., 2011; Eisenstein et al., 2011; Volkova et al., 2013; Alowibdi et al., 2013; Ciot et al., 2013; Volkova et al., 2015). Apart from demographic features, such as age or gender, there is also a growing interest in predicting psychological properties such as personality, attested by a growing literature and recent shared tasks on this topic (Celli et al., 2013; Celli et al., 2014; Rangel et al., 2015). Predicting personality types is not only of interest for psychology, but also for commercial purposes to even health care. Recent work by Preot¸iuc-Pietro et al. (2015) investigated the link between personality types, social media behavior, and psychological disorders, such as depression and post-traumatic stress disorder. They found that certain personality traits are predictive of mental illness. However, computational personality recognition is hampered by the availability of limited amounts of labeled data (Nowson and Gill, 2014). Many early existing data sets contain written essays of a certain topic, which are written in highly canonical language. Such controlled settings inhibit the expression of individual traits much more than spontaneous language."
L16-1258,P11-1077,0,0.193224,"Missing"
L16-1258,verhoeven-daelemans-2014-clips,1,0.206658,"ther comparative multilingual study on gender identification from twitter data, but using an approach that is difficult to compare to ours (learning all languages with one classifier). Predicting Myers-Briggs type indicators from linguistic input has been studied in the seminal paper of Luyckx and Daelemans (2008). They created a corpus for Dutch, consisting of 145 student essays about a documentary on artificial life. Recently, the CSI (CLiPS Stylometry Investigation) corpus was introduced, which includes Dutch reviews as well as essays and annotations for both Big Five and MBTI annotations (Verhoeven and Daelemans, 2014). In contrast, we here focus on social media data, in particular Twitter, and self-assessed (and self-reported) MBTI personality types. In many prior studies, participants were asked to participate in a personality test and produce essay(s). Collecting personality data from social media has been done before (Schwartz et al., 2013a; Schwartz et al., 2013b; Park et al., 2015). For instance, the myPersonality dataset (Kosinski et al., 2015) contains personality types and messages from 75,000 users collected through a Facebook app. Earlier work using social media data is mostly smaller scale, e.g."
L16-1258,D13-1187,0,0.0568628,"Missing"
L16-1652,W13-3520,0,0.128371,"as an abundance of deviations from grammar and spelling conventions in the latter, drastically complicate computationally interpreting the meaning of, and relations between words. This task of understanding lies at the heart of natural language processing (NLP). Neural-network-based language models such as the models in word2vec have recently gained strong interest in NLP due to the fact that they improved state-of-the-art performance on a variety of tasks in the field. Given these developments, we found it surprising that only one set of word embeddings has been publicly released for Dutch (Al-Rfou et al., 2013), which does not offer sufficiently large dimensionality for state-of-the-art performance. The primary goal of this research is thus evaluating word embeddings derived from several popular Dutch corpora and the impact of these sources on their quality, specifically focusing on problems characteristic for Dutch. Word embeddings—being an unsupervised technique—cannot be easily evaluated without comparing performance in some downstream task. Therefore, we present two novel benchmarking tasks of our own making: a relation identification task analogous to previous evaluations on English, in which t"
L16-1652,P14-1023,0,0.26709,"nique—cannot be easily evaluated without comparing performance in some downstream task. Therefore, we present two novel benchmarking tasks of our own making: a relation identification task analogous to previous evaluations on English, in which the quality of different kinds of word embeddings is measured, and a dialect identification task which measures the usefulness of word embeddings as a linguistic resource for Dutch in particular. In the literature, there has been some debate on the effectiveness of prediction-based embeddings when compared to more classical count-based embedding models (Baroni et al., 2014). As such, we train both count- (SPPMI) and prediction-based (SGNS) models, and compare them to previous efforts in both Dutch and English. Additionally, we make the trained embeddings, the means to construct these models on new corpora, as well as the materials to evaluate their quality available to the research community1 . 2. Related Work An idea mostly brought forward by the earlier distributional semantic models (DSMs), is that the context in which words occur (the distribution of the words surrounding them) can serve as a representation of their meaning, also known as the distributional"
L16-1652,W11-0704,0,0.0415694,"task benchmark for use in further research, and demonstrate how the benchmarked embeddings prove a useful unsupervised linguistic resource, effectively used in a downstream task. Keywords: word embeddings, benchmarking, word2vec, SPPMI, language variation, dialect identification 1. Introduction The strong variability of language use within, and across textual media (Collins et al., 1977; Linell, 1982) has on many occasions been marked as an important challenge for research in the area of computational linguistics (Resnik, 1999; Rosenfeld, 2000), in particular in applications to social media (Gouws et al., 2011). Formal and informal varieties, as well as an abundance of deviations from grammar and spelling conventions in the latter, drastically complicate computationally interpreting the meaning of, and relations between words. This task of understanding lies at the heart of natural language processing (NLP). Neural-network-based language models such as the models in word2vec have recently gained strong interest in NLP due to the fact that they improved state-of-the-art performance on a variety of tasks in the field. Given these developments, we found it surprising that only one set of word embedding"
L16-1652,W14-1618,0,0.0172447,"s are projected into n-dimensional vector spaces in which more similar words are closer together, and are therefore referred to as word embeddings. Recently, several models which create prediction-based word 1 Code and data are accessible via https://github.com/ clips/dutchembeddings. 4130 embeddings (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2013b; Pennington et al., 2014) have proved successful (Turian et al., 2010; Collobert et al., 2011; Baroni et al., 2014) and consequently have quickly found their way into many applications of NLP. Following Levy et al. (2014), we call the embeddings represented by dense vectors implicit, as it is not immediately clear what each dimension represents. Matrixbased sparse embeddings are then called explicit as each dimension represents a separate context, which is more easily interpretable. One of the more successful and most popular methods for creating word embeddings is word2vec (Mikolov et al., 2013a; Mikolov et al., 2013b). While word2vec often referred to as a single model, it is actually a collection of two different architectures, SkipGram (SG) and Continuous Bag of Words (CBoW), and two different training met"
L16-1652,Q15-1016,0,0.0331624,"t, as it is not immediately clear what each dimension represents. Matrixbased sparse embeddings are then called explicit as each dimension represents a separate context, which is more easily interpretable. One of the more successful and most popular methods for creating word embeddings is word2vec (Mikolov et al., 2013a; Mikolov et al., 2013b). While word2vec often referred to as a single model, it is actually a collection of two different architectures, SkipGram (SG) and Continuous Bag of Words (CBoW), and two different training methods, hierarchical skipgram (HS) and negative sampling (NS). Levy et al. (2015) show that one of the architectures in the word2vec toolkit, SkipGram with Negative Sampling (SGNS) implicitly factorizes a cooccurrence matrix which has been shifted by a factor of log(k), where k is the number of negative samples. Negative samples, in this case, are noise words which do not belong to the context currently being modelled. Subsequently, the authors propose SPPMI, which is the explicit, count-based version of SGNS, i.e. it explicitly creates a cooccurrence matrix, and then shifts all cells in the matrix by log(k). SPPMI is therefore a count-based model which is theoretically eq"
L16-1652,W14-3909,0,0.059824,"ted into n-dimensional vector spaces in which more similar words are closer together, and are therefore referred to as word embeddings. Recently, several models which create prediction-based word 1 Code and data are accessible via https://github.com/ clips/dutchembeddings. 4130 embeddings (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2013b; Pennington et al., 2014) have proved successful (Turian et al., 2010; Collobert et al., 2011; Baroni et al., 2014) and consequently have quickly found their way into many applications of NLP. Following Levy et al. (2014), we call the embeddings represented by dense vectors implicit, as it is not immediately clear what each dimension represents. Matrixbased sparse embeddings are then called explicit as each dimension represents a separate context, which is more easily interpretable. One of the more successful and most popular methods for creating word embeddings is word2vec (Mikolov et al., 2013a; Mikolov et al., 2013b). While word2vec often referred to as a single model, it is actually a collection of two different architectures, SkipGram (SG) and Continuous Bag of Words (CBoW), and two different training met"
L16-1652,D14-1162,0,0.0844149,"Missing"
L16-1652,schafer-bildhauer-2012-building,0,0.0686557,"Missing"
L16-1652,P10-1040,0,0.0614831,"current word through its context, the model will learn that words which occur in similar sentence contexts are semantically related. These representations are projected into n-dimensional vector spaces in which more similar words are closer together, and are therefore referred to as word embeddings. Recently, several models which create prediction-based word 1 Code and data are accessible via https://github.com/ clips/dutchembeddings. 4130 embeddings (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2013b; Pennington et al., 2014) have proved successful (Turian et al., 2010; Collobert et al., 2011; Baroni et al., 2014) and consequently have quickly found their way into many applications of NLP. Following Levy et al. (2014), we call the embeddings represented by dense vectors implicit, as it is not immediately clear what each dimension represents. Matrixbased sparse embeddings are then called explicit as each dimension represents a separate context, which is more easily interpretable. One of the more successful and most popular methods for creating word embeddings is word2vec (Mikolov et al., 2013a; Mikolov et al., 2013b). While word2vec often referred to as a si"
L16-1652,D15-1036,0,\N,Missing
L18-1427,P16-1038,0,0.0275163,"ut and sample from this set according to their frequency. 3.1. Readers Readers are modules which take as input strings representing the orthographic form of words, e.g. ‘wood’ or ‘dog’, and extract structured information corresponding to this string from a corpus. The information extracted from a corpus depends on the information present in that corpus: every corpus reader defines a set of so-called descriptors, which denote which data fields are contained within that corpus. We currently offer readers for three corpora: Celex (Baayen et al., 1993), a large multilingual collection of corpora (Deri and Knight, 2016), and CMUdict. The current corpus readers and their fields are listed in Table 1. If the phonological information in the corpus does not correspond to the IPA (International Phonetic Alphabet), as in the case of Celex and CMUdict corpora, we convert the representation to IPA, which serves to make these corpora comparable. As an example of use, consider the word ‘wind’, which is a phonologically ambiguous homograph. Table 2 shows the structured records we get as output when we present ‘wind’ as input to the Celex corpus reader. Corpus readers can also be combined in parallel, which allows one t"
laureys-etal-2004-evaluation,P99-1037,1,\N,Missing
luyckx-daelemans-2008-personae,W99-0707,1,\N,Missing
luyckx-daelemans-2008-personae,C04-1088,0,\N,Missing
luyckx-daelemans-2008-personae,P04-1026,0,\N,Missing
morante-daelemans-2012-conandoyle,burchardt-etal-2006-salto,0,\N,Missing
morante-daelemans-2012-conandoyle,W08-0606,0,\N,Missing
morante-daelemans-2012-conandoyle,S10-1008,1,\N,Missing
morante-daelemans-2012-conandoyle,W10-3110,0,\N,Missing
morante-daelemans-2012-conandoyle,P11-1059,0,\N,Missing
morante-daelemans-2012-conandoyle,erk-pado-2004-powerful,0,\N,Missing
N18-1140,W15-2614,0,0.025494,"ng rare entities only, with a focus on improving a reading comprehension system with external knowledge sources (Long et al., 2017). Another popular way of creating datasets for reading comprehension is crowdsourcing (Rajpurkar et al., 2016; Richardson et al., 2013; Nguyen et al., 2016; Trischler et al., 2017). These datasets exist primarily for the general domain; for specialized domains where background knowledge is crucial, crowdsourcing is intuitively less suitable (Welbl et al., 2017b), although some positive precedent exists for example in crowdsourcing annotations of radiology reports (Cocos et al., 2015). Compared to automated dataset construction, crowdsourcing is more likely to provide highquality queries and answers. On the other hand, human question generation may also lead to less varied datasets as questions would tend to be of wh- type; for cloze datasets, the questions may be more varied and might require readers to possess a different set of skills.1 1 Support for this is given in Sugawara et al. (2017), who show that Who-did-what dataset, for example, requires on average a larger number of reading skills than SQuAD (Rajpurkar et al., 2016) and MCTest (Richardson et al., 2013). 3 Dat"
N18-1140,D14-1159,0,0.0605935,"Missing"
N18-1140,J07-1005,0,0.0445333,"ng (Sugawara et al., 2017; Lai et al., 2017). Machine comprehension for healthcare and medicine has received little attention so far, although it offers great potential for practical use. A typical application would be clinical decision support, where given a massive amount of text, a clinician asks questions about either external, medical knowledge (reading literature) or about particular patients (reading electronic health records). Currently, patient-specific questions are tackled by manually browsing or searching those records. This task can be facilitated by summarization and QA systems (Demner-Fushman and Lin, 2007; Demner-Fushman et al., 2009), and we believe, by fine-grained machine reading. Reading comprehension systems that perform on a finer level could play an important role especially when combined with 1551 Proceedings of NAACL-HLT 2018, pages 1551–1563 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics document retrieval to perform machine reading at scale, such as in the models of Chen et al. (2017) and Watanabe et al. (2017) for the general domain. For our dataset, we construct queries, answers and supporting passages from BMJ Case Reports, the largest"
N18-1140,P17-1168,0,0.207522,"mple from the dataset in Figure 1. We examine the performance on the dataset in two ways. First, we report machine performance for several baselines and neural readers. To enable a more flexible answer evaluation, we expand the answers with their respective synonyms from a medical knowledge base, and additionally supplement the standard evaluation metrics with BLEU and embedding-based methods. We investigate different ways of representing medical entities in the text and how this affects the neural readers. We obtain the best results with a recurrent neural network (RNN) with gated attention (Dhingra et al., 2017a), but a simple approach based on embedding similarity proves to be a strong baseline as well. Second, we look at how well humans perform on this task, by asking both a medical expert and a novice to answer a portion of the validation set. When categorizing the skills necessary to find the right answer, we observe that a large number of comprehension skills get activated and that prior knowledge in the form of the ability to perform lexico-grammatical inferences matters the most. This suggests that for our dataset and possibly for domain-specific datasets more generally, more background knowl"
N18-1140,W16-2913,0,0.0167921,"ws us to investigate also the NoEnt set-up.5 We train each reader with the best hyperparameters found on the validation set using random search (Bergstra and Bengio, 2012), and evaluate it on the test part of the dataset. We provide more details about parameter optimization in Appendix A. The models use word embeddings pretrained on biomedical texts. 5.3 Embedding data and pre-training We induce the word embeddings on a combination of the CliCR training corpus and PubMed abstracts with open-access PMC articles available until 2015 (segmented and tokenized), amounting to over 9 billion tokens (Hakala et al., 2016). Considering the large effect of hyper-parameter selection on the quality of word embeddings (Levy et al., 2015), we optimize the embedding hyper-parameters also using random search. 6 Evaluation A model f takes as input a passage–query pair and outputs an answer a ˆ.6 We carry out the evaluation 5 We assume the candidate entities are known in advance. In our case, the answer is a word or a word phrase representing a medical entity. Alternatively, one could also take R the UMLS CUI identifier as the answering unit. However, in that case, it would mean that sometimes the original word phrase i"
N18-1140,P16-1145,0,0.344425,"tributions of our paper are: • We propose a large dataset for reading comprehension in the medical domain, using clinical case descriptions. • We carry out an empirical analysis of a) system and human performance on reading comprehension, and b) comprehension skills that are required for answering the queries correctly and that allow us to position the dataset according to its difficulty on each of the skills. 2 Related datasets Numerous general-domain datasets have been recently created to allow machine comprehension using data-intensive methods. These datasets were collected from Wikipedia (Hewlett et al., 2016; Joshi et al., 2017; Rajpurkar et al., 2016), web search queries (Nguyen et al., 2016), news articles (Hermann et al., 2015; Onishi et al., 2016; Trischler et al., 2017), books (Bajgar et al., 2016; Hill et al., 2016; Paperno et al., 2016) and English exams (Lai et al., 2017). In Table 1, we compare our dataset to several domain-specific datasets for machine comprehension. In Quasar-S, the queries are constructed from definitions of software entity tags in a community QA website, while in our case the queries are more varied and explicitly relate to the supporting passages. SciQ is a dataset"
N18-1140,P17-1147,0,0.0541826,"r are: • We propose a large dataset for reading comprehension in the medical domain, using clinical case descriptions. • We carry out an empirical analysis of a) system and human performance on reading comprehension, and b) comprehension skills that are required for answering the queries correctly and that allow us to position the dataset according to its difficulty on each of the skills. 2 Related datasets Numerous general-domain datasets have been recently created to allow machine comprehension using data-intensive methods. These datasets were collected from Wikipedia (Hewlett et al., 2016; Joshi et al., 2017; Rajpurkar et al., 2016), web search queries (Nguyen et al., 2016), news articles (Hermann et al., 2015; Onishi et al., 2016; Trischler et al., 2017), books (Bajgar et al., 2016; Hill et al., 2016; Paperno et al., 2016) and English exams (Lai et al., 2017). In Table 1, we compare our dataset to several domain-specific datasets for machine comprehension. In Quasar-S, the queries are constructed from definitions of software entity tags in a community QA website, while in our case the queries are more varied and explicitly relate to the supporting passages. SciQ is a dataset of science exam ques"
N18-1140,P16-1086,0,0.0304143,"rd answers should be allowed. Gated-Attention (GA) Reader Dhingra et al. (2017a) investigate neural readers with a finegrained attention mechanism that learns token representations for the passage that are also conditional on the query, but are in addition refined through multiple hops of the network. The model predicts the answer using attention weights with explicit reference to answer positions in the passage: a ˆ = argmax i∈E X αt , (3) t∈R(i,p) where R is the set of indices in passage p at which a token from the candidate i occurs. This operation is also called the pointer sum attention (Kadlec et al., 2016). Since the model marks the references for each token in the answer separately, it allows us to investigate also the NoEnt set-up.5 We train each reader with the best hyperparameters found on the validation set using random search (Bergstra and Bengio, 2012), and evaluate it on the test part of the dataset. We provide more details about parameter optimization in Appendix A. The models use word embeddings pretrained on biomedical texts. 5.3 Embedding data and pre-training We induce the word embeddings on a combination of the CliCR training corpus and PubMed abstracts with open-access PMC articl"
N18-1140,P14-1026,0,0.0835522,"Missing"
N18-1140,D17-1082,0,0.285249,"datasets built from news, fiction and Wikipedia texts. For specialized domains, however, large machine comprehension datasets are extremely scarce (Welbl et al., 2017a), and ∗ We provide the information about accessing the dataset, as well as the code for the experiments, at http://github. com/clips/clicr. the required comprehension skills poorly understood. With our work we hope to narrow this gap by proposing a new resource for reading comprehension in the clinical domain, and by analyzing the different types of comprehension skills that are triggered while answering (Sugawara et al., 2017; Lai et al., 2017). Machine comprehension for healthcare and medicine has received little attention so far, although it offers great potential for practical use. A typical application would be clinical decision support, where given a massive amount of text, a clinician asks questions about either external, medical knowledge (reading literature) or about particular patients (reading electronic health records). Currently, patient-specific questions are tackled by manually browsing or searching those records. This task can be facilitated by summarization and QA systems (Demner-Fushman and Lin, 2007; Demner-Fushman"
N18-1140,Q15-1016,0,0.0121751,"tion set using random search (Bergstra and Bengio, 2012), and evaluate it on the test part of the dataset. We provide more details about parameter optimization in Appendix A. The models use word embeddings pretrained on biomedical texts. 5.3 Embedding data and pre-training We induce the word embeddings on a combination of the CliCR training corpus and PubMed abstracts with open-access PMC articles available until 2015 (segmented and tokenized), amounting to over 9 billion tokens (Hakala et al., 2016). Considering the large effect of hyper-parameter selection on the quality of word embeddings (Levy et al., 2015), we optimize the embedding hyper-parameters also using random search. 6 Evaluation A model f takes as input a passage–query pair and outputs an answer a ˆ.6 We carry out the evaluation 5 We assume the candidate entities are known in advance. In our case, the answer is a word or a word phrase representing a medical entity. Alternatively, one could also take R the UMLS CUI identifier as the answering unit. However, in that case, it would mean that sometimes the original word phrase is lost. This is because entity linking with CUIs can be noisy, and only a part of a word phrase may be linked to"
N18-1140,D16-1230,0,0.0223503,"Missing"
N18-1140,D17-1086,0,0.0238437,"list that best answers the query about a news event. They do not use summaries for query formation but remove a named entity from the initial sentence in a news article, and then perform information retrieval to find independent passages relevant to the query. Another cloze dataset for language understanding is ROCStories (Mostafazadeh et al., 2016), but it is targeted more towards script knowledge evaluation, and only contains five-sentence stories. Another related task is predicting rare entities only, with a focus on improving a reading comprehension system with external knowledge sources (Long et al., 2017). Another popular way of creating datasets for reading comprehension is crowdsourcing (Rajpurkar et al., 2016; Richardson et al., 2013; Nguyen et al., 2016; Trischler et al., 2017). These datasets exist primarily for the general domain; for specialized domains where background knowledge is crucial, crowdsourcing is intuitively less suitable (Welbl et al., 2017b), although some positive precedent exists for example in crowdsourcing annotations of radiology reports (Cocos et al., 2015). Compared to automated dataset construction, crowdsourcing is more likely to provide highquality queries and an"
N18-1140,N16-1098,0,0.0346602,"et al. (2015), who similarly to us blank out entities in abstractive CNN and Daily Mail summaries, but who are only concerned with short proper nouns and short passages. Who-did-what (Onishi et al., 2016) requires the reader to select the person name from a short candidate list that best answers the query about a news event. They do not use summaries for query formation but remove a named entity from the initial sentence in a news article, and then perform information retrieval to find independent passages relevant to the query. Another cloze dataset for language understanding is ROCStories (Mostafazadeh et al., 2016), but it is targeted more towards script knowledge evaluation, and only contains five-sentence stories. Another related task is predicting rare entities only, with a focus on improving a reading comprehension system with external knowledge sources (Long et al., 2017). Another popular way of creating datasets for reading comprehension is crowdsourcing (Rajpurkar et al., 2016; Richardson et al., 2013; Nguyen et al., 2016; Trischler et al., 2017). These datasets exist primarily for the general domain; for specialized domains where background knowledge is crucial, crowdsourcing is intuitively less"
N18-1140,W03-1310,0,0.0273085,"passage, the system can learn to exploit this cue to find the answers more easily (Wang et al., 2017). Although this simplifies the task, it also makes it less realistic as the entities may not be recognized at test time. Realizing that the presence of entities makes the task easier for the machines, Hermann et al. (2015) anonymize the entities, also with a goal of discouraging language model solutions to the queries. In our case, it is not clear how relevant the anonymization is since we deal with medical entities, which have different properties than proper name entities (Kim et al., 2003; Niu et al., 2003). We explore different entity-annotation choices in the empirical part, where we refer to them as Ent (entities marked) and Anonym (entities marked but anonymized). We further examine a more challenging setup in which the reader can not rely on entity markers as they are not present in the passage (NoEnt). In all cases, the reader chooses an answer among the candidates E collected from all entities in the passage.4 Multi-word entities, which are common in our dataset, are treated as a single token by Ent and Anonym. 4 The candidate answers could in principle be obtained also in some other way,"
N18-1140,D16-1241,0,0.162826,"carry out an empirical analysis of a) system and human performance on reading comprehension, and b) comprehension skills that are required for answering the queries correctly and that allow us to position the dataset according to its difficulty on each of the skills. 2 Related datasets Numerous general-domain datasets have been recently created to allow machine comprehension using data-intensive methods. These datasets were collected from Wikipedia (Hewlett et al., 2016; Joshi et al., 2017; Rajpurkar et al., 2016), web search queries (Nguyen et al., 2016), news articles (Hermann et al., 2015; Onishi et al., 2016; Trischler et al., 2017), books (Bajgar et al., 2016; Hill et al., 2016; Paperno et al., 2016) and English exams (Lai et al., 2017). In Table 1, we compare our dataset to several domain-specific datasets for machine comprehension. In Quasar-S, the queries are constructed from definitions of software entity tags in a community QA website, while in our case the queries are more varied and explicitly relate to the supporting passages. SciQ is a dataset of science exam questions, in which question-answer pairs are used to retrieve the text passages. For each question, four candidate answers are a"
N18-1140,P16-1144,0,0.23145,"and b) comprehension skills that are required for answering the queries correctly and that allow us to position the dataset according to its difficulty on each of the skills. 2 Related datasets Numerous general-domain datasets have been recently created to allow machine comprehension using data-intensive methods. These datasets were collected from Wikipedia (Hewlett et al., 2016; Joshi et al., 2017; Rajpurkar et al., 2016), web search queries (Nguyen et al., 2016), news articles (Hermann et al., 2015; Onishi et al., 2016; Trischler et al., 2017), books (Bajgar et al., 2016; Hill et al., 2016; Paperno et al., 2016) and English exams (Lai et al., 2017). In Table 1, we compare our dataset to several domain-specific datasets for machine comprehension. In Quasar-S, the queries are constructed from definitions of software entity tags in a community QA website, while in our case the queries are more varied and explicitly relate to the supporting passages. SciQ is a dataset of science exam questions, in which question-answer pairs are used to retrieve the text passages. For each question, four candidate answers are available. In our dataset, the number of candidate answer is much 1552 higher as the candidate a"
N18-1140,P02-1040,0,0.101284,"ctuation and case distinction (same for other metrics). F1 metric is applied per instance and measures the overlap between the prediction a ˆ and the ground truth a, which are treated as bags of words.7 While these two metrics are arguably sufficient in news-style machine comprehension where the entities are proper nouns which allow for little variation and synonymy, in our case the medical entities are often mostly common nouns modified by specifiers and qualifiers. To take into account potentially large lexical and word-order variation, we use two additional metrics. First, we measure BLEU (Papineni et al., 2002) for n-grams of length 2 (shortly, B2) and 4 (B4) using the package by Chen et al. (2015), with which we aim to capture contiguity of tokens in longer answers. Second, it may occur that answers contain no word overlap yet still be good candidates because of their semantical relatedness, as in “renal failure”–“kidney breakdown”. We take this into account by using an embedding metric (Emb), in which we construct mean vectors for both ground-truth and system answer sequences, and then compare them with the cosine similarity. This and other embedding metrics for evaluation were previously studied"
N18-1140,D16-1264,0,0.771618,"a large dataset for reading comprehension in the medical domain, using clinical case descriptions. • We carry out an empirical analysis of a) system and human performance on reading comprehension, and b) comprehension skills that are required for answering the queries correctly and that allow us to position the dataset according to its difficulty on each of the skills. 2 Related datasets Numerous general-domain datasets have been recently created to allow machine comprehension using data-intensive methods. These datasets were collected from Wikipedia (Hewlett et al., 2016; Joshi et al., 2017; Rajpurkar et al., 2016), web search queries (Nguyen et al., 2016), news articles (Hermann et al., 2015; Onishi et al., 2016; Trischler et al., 2017), books (Bajgar et al., 2016; Hill et al., 2016; Paperno et al., 2016) and English exams (Lai et al., 2017). In Table 1, we compare our dataset to several domain-specific datasets for machine comprehension. In Quasar-S, the queries are constructed from definitions of software entity tags in a community QA website, while in our case the queries are more varied and explicitly relate to the supporting passages. SciQ is a dataset of science exam questions, in which question-"
N18-1140,D13-1020,0,0.0611259,"the initial sentence in a news article, and then perform information retrieval to find independent passages relevant to the query. Another cloze dataset for language understanding is ROCStories (Mostafazadeh et al., 2016), but it is targeted more towards script knowledge evaluation, and only contains five-sentence stories. Another related task is predicting rare entities only, with a focus on improving a reading comprehension system with external knowledge sources (Long et al., 2017). Another popular way of creating datasets for reading comprehension is crowdsourcing (Rajpurkar et al., 2016; Richardson et al., 2013; Nguyen et al., 2016; Trischler et al., 2017). These datasets exist primarily for the general domain; for specialized domains where background knowledge is crucial, crowdsourcing is intuitively less suitable (Welbl et al., 2017b), although some positive precedent exists for example in crowdsourcing annotations of radiology reports (Cocos et al., 2015). Compared to automated dataset construction, crowdsourcing is more likely to provide highquality queries and answers. On the other hand, human question generation may also lead to less varied datasets as questions would tend to be of wh- type; f"
N18-1140,P17-1075,0,0.499598,". Most are open-domain datasets built from news, fiction and Wikipedia texts. For specialized domains, however, large machine comprehension datasets are extremely scarce (Welbl et al., 2017a), and ∗ We provide the information about accessing the dataset, as well as the code for the experiments, at http://github. com/clips/clicr. the required comprehension skills poorly understood. With our work we hope to narrow this gap by proposing a new resource for reading comprehension in the clinical domain, and by analyzing the different types of comprehension skills that are triggered while answering (Sugawara et al., 2017; Lai et al., 2017). Machine comprehension for healthcare and medicine has received little attention so far, although it offers great potential for practical use. A typical application would be clinical decision support, where given a massive amount of text, a clinician asks questions about either external, medical knowledge (reading literature) or about particular patients (reading electronic health records). Currently, patient-specific questions are tackled by manually browsing or searching those records. This task can be facilitated by summarization and QA systems (Demner-Fushman and Lin, 2"
N18-1140,W17-2623,0,0.518548,"l analysis of a) system and human performance on reading comprehension, and b) comprehension skills that are required for answering the queries correctly and that allow us to position the dataset according to its difficulty on each of the skills. 2 Related datasets Numerous general-domain datasets have been recently created to allow machine comprehension using data-intensive methods. These datasets were collected from Wikipedia (Hewlett et al., 2016; Joshi et al., 2017; Rajpurkar et al., 2016), web search queries (Nguyen et al., 2016), news articles (Hermann et al., 2015; Onishi et al., 2016; Trischler et al., 2017), books (Bajgar et al., 2016; Hill et al., 2016; Paperno et al., 2016) and English exams (Lai et al., 2017). In Table 1, we compare our dataset to several domain-specific datasets for machine comprehension. In Quasar-S, the queries are constructed from definitions of software entity tags in a community QA website, while in our case the queries are more varied and explicitly relate to the supporting passages. SciQ is a dataset of science exam questions, in which question-answer pairs are used to retrieve the text passages. For each question, four candidate answers are available. In our dataset,"
N18-1140,W17-2604,0,0.0495332,"sk formulation The reading comprehension problem in our case can be represented as a tuple (q, p, A), where q is the query, built from a learning point; the passage p is the entire report excluding the Learning points section; and A is the set of ground-truth entities answering q. In defining the task, it is important to consider how to take into account entity annotation and how to define the answer output space. We look at these more closely in the rest of this section. Whenever the entities are marked in the passage, the system can learn to exploit this cue to find the answers more easily (Wang et al., 2017). Although this simplifies the task, it also makes it less realistic as the entities may not be recognized at test time. Realizing that the presence of entities makes the task easier for the machines, Hermann et al. (2015) anonymize the entities, also with a goal of discouraging language model solutions to the queries. In our case, it is not clear how relevant the anonymization is since we deal with medical entities, which have different properties than proper name entities (Kim et al., 2003; Niu et al., 2003). We explore different entity-annotation choices in the empirical part, where we refe"
N18-1140,W17-4413,0,0.22867,"ily depends on the introduction of new datasets (Burges, 2013), which encourages the development of new algorithms and deepens our understanding of the (linguistic) challenges that can or can not be tackled well by these algorithms. Recently, a number of reading comprehension datasets have been proposed (§ 2), differing in various aspects such as mode of construction, answer-query formulation and required understanding skills. Most are open-domain datasets built from news, fiction and Wikipedia texts. For specialized domains, however, large machine comprehension datasets are extremely scarce (Welbl et al., 2017a), and ∗ We provide the information about accessing the dataset, as well as the code for the experiments, at http://github. com/clips/clicr. the required comprehension skills poorly understood. With our work we hope to narrow this gap by proposing a new resource for reading comprehension in the clinical domain, and by analyzing the different types of comprehension skills that are triggered while answering (Sugawara et al., 2017; Lai et al., 2017). Machine comprehension for healthcare and medicine has received little attention so far, although it offers great potential for practical use. A typ"
P03-1062,daelemans-hoste-2002-evaluation,1,0.825919,"onstrated by (Daelemans et al., 1999), exceptions do typically reoccur in language data. Hence, machine learning algorithms that retain a memory trace of individual instances, like memory-based learning algorithms based on the k-nearest neighbour classifier, outperform decision tree or rule inducers precisely for this reason. Comparing the performance of machine learning algorithms is not straightforward, and deserves careful methodological consideration. For a fair comparison, both algorithms should be objectively and automatically optimized for the task to be learned. This point is made by (Daelemans and Hoste, 2002), who show that, for tasks such as word-sense disambiguation and part-of-speech tagging, tuning algorithms in terms of feature selection and classifier parameters gives rise to significant improvements in performance. In this paper, therefore, we optimize both CART and MBL individually and per task, using a heuristic optimization method called iterative deepening. The second issue, that of task combination, stems from the intuition that the two tasks have a lot in common. For instance, (Hirschberg, 1993) reports that knowledge of the location of breaks facilitates accent placement. Although pi"
P03-1062,W96-0102,1,0.89273,". An excerpt of the annotated data with all generated symbolic and numeric1 features is presented in Table 1. Word forms (Wrd) – The word form tokens form the central unit to which other features are added. Pre- and post-punctuation – All punctuation marks in the data are transferred to two separate features: a pre-punctuation feature (PreP) for punctuation marks such as quotation marks appearing before the token, and a post-punctuation feature (PostP) for punctuation marks such as periods, commas, and question marks following the token. Part-of-speech (POS) tagging – We used MBT version 1.0 (Daelemans et al., 1996) to develop a memory-based POS tagger trained on the Eindhoven corpus of written Dutch, which does not overlap with our base data. We split up the full POS tags into two features, the first (PosC) containing the main POS category, the second (PosF) the POS subfeatures. Diacritical accent – Some tokens bear an orthographical diacritical accent put there by the author to particularly emphasize the token in question. These accents were stripped off the accented letter, and transferred to a binary feature (DiA). NP and VP chunking (NpC & VpC) – An approximation of the syntactic structure is provid"
P03-1062,P00-1030,0,0.0146539,"1997). Predicting prosody is known to be a hard problem that is thought to require information on syntactic boundaries, syntactic and semantic relations between constituents, discourse-level knowledge, and phonological well-formedness constraints (Hirschberg, 1993). However, producing all this information – using full parsing, including establishing semanto-syntactic relations, and full discourse analysis – is currently infeasible for a realtime system. Resolving this dilemma has been the topic of several studies in pitch accent placement (Hirschberg, 1993; Black, 1995; Pan and McKeown, 1999; Pan and Hirschberg, 2000; Marsi et al., 2002) and in prosodic boundary placement (Wang and Hirschberg, 1997; Taylor and Black, 1998). The commonly adopted solution is to use shallow information sources that approximate full syntactic, semantic and discourse information, such as the words of the text themselves, their part-of-speech tags, or their information content (in general, or in the text at hand), since words with a high (semantic) information content or load tend to receive pitch accents (Ladd, 1996). Within this research paradigm, we investigate pitch accent and prosodic boundary placement for Dutch, using an"
P03-1062,W99-0619,0,0.0486747,"erance (Cutler et al., 1997). Predicting prosody is known to be a hard problem that is thought to require information on syntactic boundaries, syntactic and semantic relations between constituents, discourse-level knowledge, and phonological well-formedness constraints (Hirschberg, 1993). However, producing all this information – using full parsing, including establishing semanto-syntactic relations, and full discourse analysis – is currently infeasible for a realtime system. Resolving this dilemma has been the topic of several studies in pitch accent placement (Hirschberg, 1993; Black, 1995; Pan and McKeown, 1999; Pan and Hirschberg, 2000; Marsi et al., 2002) and in prosodic boundary placement (Wang and Hirschberg, 1997; Taylor and Black, 1998). The commonly adopted solution is to use shallow information sources that approximate full syntactic, semantic and discourse information, such as the words of the text themselves, their part-of-speech tags, or their information content (in general, or in the text at hand), since words with a high (semantic) information content or load tend to receive pitch accents (Ladd, 1996). Within this research paradigm, we investigate pitch accent and prosodic boundary pla"
P97-1056,W96-0211,0,0.0522246,"tally. Finally, it should be mentioned that MBclassifiers, despite their description as table-lookup algorithms here, can be implemented to work fast, using e.g. tree-based indexing into the casebase (Daelemans et al., 1997). where: 3 tf(xi,yi) = 0 i f xi = yi, else 1 (2) This metric simply counts the number of (mis)matching feature values in both patterns. If we do not have information about the importance of features, this is a reasonable choice. But if we do have some information about feature relevance one possibility would be to add linguistic bias to weight or select different features (Cardie, 1996). An alternative--more empiricist--approach, is to look at the behavior of features in the set of examples used for training. We can compute statistics about the relevance of features by looking at which features are good predictors of the class labels. Information Theory gives us a useful tool for measuring feature relevance in this way (Quinlan, 1986; Quinlan, 1993). I n f o r m a t i o n G a i n (IG) weighting looks at each feature in isolation, and measures how much information it contributes to our knowledge of the correct class label. The Information Gain of feature f is measured by comp"
P97-1056,P96-1041,0,0.0373733,"asses and pattern X. If pattern X is described by a number of feature-values X l , . . . , xn, we can write the conditional probability as P ( c l a s s l x l , . . . , xn). If a particular pattern x ~ , . . . , x"" is not literally present among the examples, all classes have zero ML probability estimates. Smoothing methods are needed to avoid zeroes on events that could occur in the test material. There are two main approaches to smoothing: count re-estimation smoothing such as the Add-One or Good-Turing methods (Church & Gale, 1991), and Back-off type methods (Bahl et al., 1983; Katz, 1987; Chen & Goodman, 1996; Samuelsson, 1996). We will focus here on a comparison with Back-off type methods, because an experimental comparison in Chen & Goodman (1996) shows the superiority of Back-off based methods over count re-estimation smoothing methods. With the Back-off method the probabilities of complex conditioning events are approximated by (a linear interpolation of) the probabilities of more general events: (3) (4) Where C is the set of class labels, Vf is the set of values for feature f , and H(C) = - ~ c e c P(c) log 2 P(e) is the entropy of the class labels. The probabilities are estimated from relati"
P97-1056,P96-1025,0,0.014613,"n i. In this case, if F is the number of features, there are 2 F - 1 more general terms, and we need to estimate A~&apos;s for all of these. In most applications the interpolation method is used for tasks with clear orderings of feature-sets (e.g. n-gram language modeling) so that many of the 2 F - - 1 terms can be omitted beforehand. More recently, the integration of information sources, and the modeling of more complex language processing tasks in the statistical framework has increased the interest in smoothing methods (Collins ~z Brooks, 1995; Ratnaparkhi, 1996; Magerman, 1994; Ng & Lee, 1996; Collins, 1996). For such applications with a diverse set of features it is not necessarily the case that terms can be excluded beforehand. If we let the Axe depend on the value of X ~, the number of parameters explodes even faster. A practical solution for this is to make a smaller number of buckets for the X i, e.g. by clustering (see e.g. Magerman (1994)): Note that linear interpolation (equation 5) actually performs two functions. In the first place, if the most specific terms have non-zero frequency, it still interpolates them with the more general terms. Because the more general terms should never over"
P97-1056,W95-0103,0,0.253945,"erms. Because the more general terms should never overrule the more specific ones, the Ax e for the more general terms should be quite small. Therefore the interpolation effect is usually small or negligible. The second function is the pure back-off function: if the more specific terms have zero frequency, the probabilities of the more general terms are used instead. Only if terms are of a similar specificity, the A&apos;s truly serve to weight relevance of the interpolation terms. If we isolate the pure back-off function of the interpolation equation we get an algorithm similar to the one used in Collins & Brooks (1995). It is given in a schematic form in Table 1. Each step consists of a back-off to a lower level of specificity. There are as many steps as features, and there are a total o f 2 F terms, divided over all the steps. Because all features are considered of equal importance, we call this the Naive Back-off algorithm. Usually, not all features x are equally important, so that not all back-off terms are equally relevant for the re-estimation. Hence, the problem of fitting the Axe parameters is replaced by a term selection task. To optimize the term selection, an evaluation of the up to 2 F terms on h"
P97-1056,W96-0102,1,0.842863,"pecially for the guessing of the POS-tag of words not present in the lexicon. Relevant information for guessing the tag of an unknown word includes contextual information (the words and tags in the context of the word), and word form information (prefixes and suffixes, first and last letters of the word as an approximation of affix information, presence or absence of capitalization, numbers, special characters etc.). There is a large number of potentially informative features that could play a role in correctly predicting the tag of an unknown word (Ratnaparkhi, 1996; Weischedel et al., 1993; Daelemans et al., 1996). A priori, it is not clear what the relative importance is of these features. We compared Naive Back-off estimation and MBL with two sets of features: • P D A S S : the first letter of the unknown word (p), the tag of the word to the left of the unknown word (d), a tag representing the set of possible lexical categories of the word to the right of the unknown word (a), and the two last letters (s). The first letter provides information about capitalisation and the prefix, the two last letters 440 about suffixes. • PDDDAAASSS: more left and right context features, and more suffix information."
P97-1056,P94-1038,0,0.113137,"ntegration of information sources, and the modeling of more complex language processing tasks in the statistical framework has increased the interest in smoothing methods (Collins ~z Brooks, 1995; Ratnaparkhi, 1996; Magerman, 1994; Ng & Lee, 1996; Collins, 1996). For such applications with a diverse set of features it is not necessarily the case that terms can be excluded beforehand. If we let the Axe depend on the value of X ~, the number of parameters explodes even faster. A practical solution for this is to make a smaller number of buckets for the X i, e.g. by clustering (see e.g. Magerman (1994)): Note that linear interpolation (equation 5) actually performs two functions. In the first place, if the most specific terms have non-zero frequency, it still interpolates them with the more general terms. Because the more general terms should never overrule the more specific ones, the Ax e for the more general terms should be quite small. Therefore the interpolation effect is usually small or negligible. The second function is the pure back-off function: if the more specific terms have zero frequency, the probabilities of the more general terms are used instead. Only if terms are of a simil"
P97-1056,H92-1021,0,0.054228,"Only open-class words were used during construction of the training set. For both IBI-IG and Naive Back-off, a 10-fold cross-validation experiment was run using both PDASS and PDDDAAASSS patterns. The results are in Table 3. The IG values for the features are given in Figure 2. 6 0.30 - Conclusion We have analysed the relationship between Backoff smoothing and Memory-Based Learning and established a close correspondence between these two frameworks which were hitherto mostly seen as unrelated. An exception is the use of similarity for alleviating the sparse data problem in language modeling (Essen & Steinbiss, 1992; Brown et al., 1992; Dagan et al., 1994). However, these works differ in their focus from our analysis in that the emphasis is put on similarity between values of a feature (e.g. words), instead of similarity between patterns that are a (possibly complex) combination of many features. 0.25 - 0.20 The results show that for Naive Back-off (and m l ) the addition of more, possibly irrelevant, features quickly becomes detrimental (decrease from 88.5 to 85.9), even if these added features do make a generalisation performance increase possible (witness the increase with IBI-IG from 88.3 to 89.8). N"
P97-1056,P96-1006,0,0.0260256,"X i, but only on i. In this case, if F is the number of features, there are 2 F - 1 more general terms, and we need to estimate A~&apos;s for all of these. In most applications the interpolation method is used for tasks with clear orderings of feature-sets (e.g. n-gram language modeling) so that many of the 2 F - - 1 terms can be omitted beforehand. More recently, the integration of information sources, and the modeling of more complex language processing tasks in the statistical framework has increased the interest in smoothing methods (Collins ~z Brooks, 1995; Ratnaparkhi, 1996; Magerman, 1994; Ng & Lee, 1996; Collins, 1996). For such applications with a diverse set of features it is not necessarily the case that terms can be excluded beforehand. If we let the Axe depend on the value of X ~, the number of parameters explodes even faster. A practical solution for this is to make a smaller number of buckets for the X i, e.g. by clustering (see e.g. Magerman (1994)): Note that linear interpolation (equation 5) actually performs two functions. In the first place, if the most specific terms have non-zero frequency, it still interpolates them with the more general terms. Because the more general terms s"
P97-1056,H94-1048,0,0.340962,"Missing"
P97-1056,C96-2151,0,0.0716346,"f pattern X is described by a number of feature-values X l , . . . , xn, we can write the conditional probability as P ( c l a s s l x l , . . . , xn). If a particular pattern x ~ , . . . , x"" is not literally present among the examples, all classes have zero ML probability estimates. Smoothing methods are needed to avoid zeroes on events that could occur in the test material. There are two main approaches to smoothing: count re-estimation smoothing such as the Add-One or Good-Turing methods (Church & Gale, 1991), and Back-off type methods (Bahl et al., 1983; Katz, 1987; Chen & Goodman, 1996; Samuelsson, 1996). We will focus here on a comparison with Back-off type methods, because an experimental comparison in Chen & Goodman (1996) shows the superiority of Back-off based methods over count re-estimation smoothing methods. With the Back-off method the probabilities of complex conditioning events are approximated by (a linear interpolation of) the probabilities of more general events: (3) (4) Where C is the set of class labels, Vf is the set of values for feature f , and H(C) = - ~ c e c P(c) log 2 P(e) is the entropy of the class labels. The probabilities are estimated from relative frequencies in t"
P97-1056,J93-2006,0,\N,Missing
P97-1056,W96-0213,0,\N,Missing
P97-1056,J92-4003,0,\N,Missing
P97-1056,E95-1020,0,\N,Missing
P98-1081,H92-1022,0,0.0124735,"Missing"
P98-1081,W96-0102,1,0.567118,"earned). During tagging these rules are applied in sequence to new text. Of all the four systems, this one has access to the most information: contextual information (the words and tags in a window spanning three positions before and after the focus word) as well as lexical information (the existence of words formed by suffix/prefix addition/deletion). However, the actual use of this information is severely limited in that the individual information items can only be combined according to the patterns laid down in the rule templates. The third system uses Memory-Based Learning as described by Daelemans et al. (1996; henceforth tagger M, for Memory). During the training phase, cases containing information about the word, the context and the correct tag are stored in memory. During tagging, the case most similar to that of the focus word is retrieved from the memory, which is indexed on the basis of the Information Gain of each feature, and the accompanying tag is selected. The system used here has access to information about the focus word and the two positions before and after, at least for known words. For unknown words, the single position before and after, three suffix letters, and information about"
P98-1081,W96-0213,0,\N,Missing
P99-1037,J94-3007,1,0.632688,"by storing examples of a task in memory. Computational effort is invested on a &quot;call-by-need&quot; basis for solving new examples (henceforth called instances) of the same task. When new instances are presented to a memory-based learner, it searches for the bestmatching instances in memory, according to a task-dependent similarity metric. When it has found the best matches (the nearest neighbors), it transfers their solution (classification, label) to the new instance. Memory-based learning has been shown to be quite adequate for various natural-language processing tasks such as stress assignment (Daelemans et al., 1994), grapheme-phoneme conversion (Daelemans and Van den Bosch, 1996; Van den Bosch, 1997), and part-of-speech tagging (Daelemans et al., 1996b). The paper is structured as follows. First, we give a brief overview of Dutch morphology in Section 2. We then turn to a description of MBMA in Section 3. In Section 4 we present 285 the experimental outcomes of our study with MBMA. Section 5 summarizes our findings, reports briefly on a partial study of English showing that the approach is applicable to other languages, and lists our conclusions. 2 Dutch Morphology The processes of Dutch morphology inclu"
P99-1037,W96-0102,1,0.869778,"lled instances) of the same task. When new instances are presented to a memory-based learner, it searches for the bestmatching instances in memory, according to a task-dependent similarity metric. When it has found the best matches (the nearest neighbors), it transfers their solution (classification, label) to the new instance. Memory-based learning has been shown to be quite adequate for various natural-language processing tasks such as stress assignment (Daelemans et al., 1994), grapheme-phoneme conversion (Daelemans and Van den Bosch, 1996; Van den Bosch, 1997), and part-of-speech tagging (Daelemans et al., 1996b). The paper is structured as follows. First, we give a brief overview of Dutch morphology in Section 2. We then turn to a description of MBMA in Section 3. In Section 4 we present 285 the experimental outcomes of our study with MBMA. Section 5 summarizes our findings, reports briefly on a partial study of English showing that the approach is applicable to other languages, and lists our conclusions. 2 Dutch Morphology The processes of Dutch morphology include inflection, derivation, and compounding. Inflection of verbs, adjectives, and nouns is mostly achieved by suffixation, but a circumfix"
P99-1037,E93-1023,0,0.0748031,"+en, we stop); there is variation between s and z and f and v (e.g. huis, house, versus huizen, houses). Finally, between the parts of a compound, a linking morpheme may appear (e.g. staat+s+loterij, state lottery). For a detailed discussion of morphological phenomena in Dutch, see De Haas and Trommelen (1993). Previous approaches to Dutch morphological analysis have been based on finite-state transducers (e.g., XEROX&apos;es morphological analyzer), or on parsing with context-free word grammars interleaved with exploration of possible spelling changes (e.g. Heemskerk and van Heuven (1993); or see Heemskerk (1993) for a probabilistic variant). 286 3 Applying memory-based learning to morphological a n a l y s i s Most linguistic problems can be seen as,contextsensitive mappings from one representation to another (e.g., from text to speech; from a sequence of spelling words to a parse tree; from a parse tree to logical form, from source language to target language, etc.) (Daelemans, 1995). This is also the case for morphological analysis. Memory-based learning algorithms can learn mappings (classifications) if a sufficient number of instances of these mappings is presented to them. We drew our instances"
R09-1003,J07-4002,0,0.414631,"Missing"
R09-1003,J04-4004,0,0.0534769,"Missing"
R09-1003,J03-4003,0,0.0470852,"mostly addressed in the context of artificial situations like the quadruple classification task [18] in which only two possible attachment sites, each time a noun or a verb, are possible. In this paper we provide a method to evaluate the task in a more natural situation, making it possible to compare the approach to full statistical parsing approaches. First, we show how to extract anchor-pp pairs from parse trees in the GENIA and WSJ treebanks. Next, we discuss the extension of the shallow parser with a PP-attacher. We compare the PP attachment module with a statistical full parsing approach [4] and analyze the results. More specifically, we investigate the domain adaptation properties of both approaches (in this case domain shifts between journalistic and medical language). Keywords prepositional phrase attachment, shallow parsing, machine learning of language 1 Introduction Shallow parsing (also called partial parsing) is an approach to language processing that computes a basic analysis of sentence structure rather than attempting full syntactic analysis. Originally defined by Abney [1] as a task to be solved with handcrafted regular expressions (finite state methods) and limited t"
R09-1003,W99-0707,1,0.767786,"nguistics University of Antwerp Prinsstraat 13 B-2000 Antwerpen, Belgium Walter.Daelemans@ua.ac.be See http://ifarm.nl/signll/conll/ However, a shallow approach also has its shortcomings, an important one being that prepositional phrases, which contain important semantic information for interpreting events, are left unattached. Furthermore, while statistical full parsing used to be more noise-sensitive and less efficient than shallow parsing, that is no longer necessarily the case with recent developments in parse ranking. In this paper, we extend an existing memory based shallow parser, MBSP [5, 6], with a machine learning based prepositional phrase attachment module, and compare it to PP attachment in a state of the art statistical parser. The machine learning method chosen is memory-based learning. We also investigate the ability of this Memory-based PP attachment (MBPA) to cope with the problem of domain adaptation, i.e. the often dramatic decrease in accuracy when testing a trained system on data from a domain different from the domain of the data on which it was trained. The remainder of this paper starts with an explanation of how the corpus is prepared in order to use it for PP a"
R09-1003,P06-2029,0,0.0371504,"Missing"
R09-1003,J93-1005,0,0.625091,"Missing"
R09-1003,H94-1020,0,0.0438343,"Missing"
R09-1003,J06-3002,0,0.0364645,"Missing"
R09-1003,W95-0107,0,0.0153165,"computes a basic analysis of sentence structure rather than attempting full syntactic analysis. Originally defined by Abney [1] as a task to be solved with handcrafted regular expressions (finite state methods) and limited to finding basic (nonrecursive) phrases in text, the label shallow parsing has meanwhile broadened its scope to machine learning methods and to a set of related tasks including part of speech tagging, finding phrases (chunking), clause identification, grammatical role labeling, etc. Especially the machine learning approach to shallow parsing, pioneered by Ramshaw and Marcus [17] has been investigated intensively, in part because of the availability of benchmark datasets and competitions (CoNLL shared tasks 1999 to 2001)1 . 1 Walter Daelemans CLiPS - Computational Linguistics University of Antwerp Prinsstraat 13 B-2000 Antwerpen, Belgium Walter.Daelemans@ua.ac.be See http://ifarm.nl/signll/conll/ However, a shallow approach also has its shortcomings, an important one being that prepositional phrases, which contain important semantic information for interpreting events, are left unattached. Furthermore, while statistical full parsing used to be more noise-sensitive and"
R09-1003,H94-1048,0,0.236768,"Missing"
R09-1003,I05-2038,0,0.0602525,"Missing"
R09-1003,W06-2112,0,0.0321472,"Missing"
R09-1003,W97-1016,1,0.698438,"Missing"
R09-1013,P07-1007,0,0.0537392,"Missing"
R09-1013,W00-1306,0,0.0330813,"Missing"
R09-1013,P96-1042,0,0.0963054,"Missing"
R09-1013,W04-3202,0,0.0289154,"Missing"
R15-1086,E09-1046,0,0.0109468,"s. # Posts 3085 181 1671 129 546 23 49 1 • Character n-gram bag-of-words: binary features indicating the presence of character trigrams (without crossing word boundaries), to provide some abstraction from the word level. • Sentiment lexicon features: four numeric features representing the number of positive, negative, and neutral lexicon words (averaged over text length) and the overall post polarity (i.e. the sum of the values of identified sentiment words averaged over text length)4 . The features were calculated based on existing sentiment lexicons for Dutch (De Smedt and Daelemans, 2012b; Jijkoun and Hofmann, 2009). Table 3: Data distribution for the different author roles in cyberbullying events. 4 Experiments This section describes the experiments that were conducted to gain insight into the detection and fine-grained classification of cyberbullying events. 4.1 Features Experimental Setup 5 Two sets of experiments were conducted. Firstly, we explored the detection of cyberbullying posts regardless of the harmfulness score (i.e. we considered posts that were given a score of 1 or 2) and the author’s role. The second set of experiments focuses on a more complex task, the identification of fine-grained t"
R15-1086,de-smedt-daelemans-2012-vreselijk,1,0.415387,"Missing"
R15-1086,desmet-hoste-2014-recognising,1,0.851197,"f cyberbullying posts regardless of the harmfulness score (i.e. we considered posts that were given a score of 1 or 2) and the author’s role. The second set of experiments focuses on a more complex task, the identification of fine-grained text categories related to cyberbullying (see Section 3.2). To this end, a binary classifier was built for each category. Evaluation was done using 10-fold cross-validation. We used Support Vector Machines (SVM) as the classification algorithm since they have proven to work well for high-skew text classification tasks similar to the ones under investigation (Desmet and Hoste, 2014). We used linear kernels and experimentally determined the optimal cost value c to be 1. All experiments were carried out using Pattern (De Smedt and Daelemans, 2012a), a Python Results We implemented different experimental set-ups with various feature groups and hence determined the informativeness of each feature group for the current classification tasks. We explored the contributiveness of the following feature groups in isolation: word unigram bag-of-words (which can be considered as the baseline approach), word bigram bag-of-words, character trigram bag-ofwords, and sentiment lexicon fea"
R15-1086,E12-2021,0,0.0383805,"Missing"
rehm-etal-2014-strategic,P07-2045,0,\N,Missing
rehm-etal-2014-strategic,piperidis-etal-2014-meta,1,\N,Missing
rehm-etal-2014-strategic,piperidis-2012-meta,1,\N,Missing
reinberger-daelemans-2004-unsupervised,W99-0707,1,\N,Missing
reinberger-daelemans-2004-unsupervised,W99-0629,1,\N,Missing
reinberger-daelemans-2004-unsupervised,J93-1005,0,\N,Missing
reinberger-daelemans-2004-unsupervised,W02-0905,0,\N,Missing
reinberger-daelemans-2004-unsupervised,P99-1008,0,\N,Missing
reinberger-daelemans-2004-unsupervised,P98-2127,0,\N,Missing
reinberger-daelemans-2004-unsupervised,C98-2122,0,\N,Missing
reinberger-daelemans-2004-unsupervised,W99-0609,0,\N,Missing
S01-1020,W96-0102,1,0.895448,"Missing"
S01-1020,kilgarriff-rosenzweig-2000-english,0,0.0438141,"even with limited training data. 1 Introdu tion We report on the use of ma hine learning, espe ially memory-based learning and lassi er ombination, for word sense disambiguation (WSD) in the English all words task of SENSEVAL2. WSD an be des ribed as the problem of assigning the appropriate sense to a given word in a given ontext. Ma hine learning te hniques show state-of-the-art a ura y on WSD, e.g. memory-based learning (Ng and Lee, 1996; Veenstra et al., 2000), de ision lists (Yarowsky, 2000), and ombination methods (Es udero et al., 2000). Results of the rst SENSEVAL exer ise for English (Killgarri and Rosenzweig, 2000), in whi h only a restri ted set of words had to be disambiguated, showed that supervised learning systems outperform unsupervised ones, even when little orpus training material was available. In our submission to SENSEVAL2, we investigated whether the supervised learning approa h an be s aled to the all-words task. As a ba k-o for word-tag pairs for whi h no or not enough training data was available, we used the most frequent sense in the WordNet1.7 sense lexi on (Fellbaum, 1998) as default lassi er in the disambiguation pro ess. Sense disambiguation was mainly performed by a memory-based lea"
S01-1020,P96-1006,0,0.0972931,"le word expert was determined. Results show that espe ially memory-based learning in a word-expert approa h is a feasible method for unrestri ted word-sense disambiguation, even with limited training data. 1 Introdu tion We report on the use of ma hine learning, espe ially memory-based learning and lassi er ombination, for word sense disambiguation (WSD) in the English all words task of SENSEVAL2. WSD an be des ribed as the problem of assigning the appropriate sense to a given word in a given ontext. Ma hine learning te hniques show state-of-the-art a ura y on WSD, e.g. memory-based learning (Ng and Lee, 1996; Veenstra et al., 2000), de ision lists (Yarowsky, 2000), and ombination methods (Es udero et al., 2000). Results of the rst SENSEVAL exer ise for English (Killgarri and Rosenzweig, 2000), in whi h only a restri ted set of words had to be disambiguated, showed that supervised learning systems outperform unsupervised ones, even when little orpus training material was available. In our submission to SENSEVAL2, we investigated whether the supervised learning approa h an be s aled to the all-words task. As a ba k-o for word-tag pairs for whi h no or not enough training data was available, we used"
van-eynde-etal-2000-part,W96-0102,1,\N,Missing
van-eynde-etal-2000-part,zavrel-daelemans-2000-bootstrapping,1,\N,Missing
van-eynde-etal-2000-part,P99-1037,1,\N,Missing
van-eynde-etal-2000-part,P98-1081,1,\N,Missing
van-eynde-etal-2000-part,C98-1078,1,\N,Missing
van-eynde-etal-2000-part,A00-1031,0,\N,Missing
van-eynde-etal-2000-part,oostdijk-2000-spoken,0,\N,Missing
vaneyghen-etal-2006-mixed,W04-0108,1,\N,Missing
vaneyghen-etal-2006-mixed,vandeghinste-2002-lexicon,0,\N,Missing
verhoeven-daelemans-2014-clips,luyckx-daelemans-2008-personae,1,\N,Missing
verhoeven-daelemans-2014-clips,P09-2078,0,\N,Missing
verhoeven-daelemans-2014-clips,P11-1032,0,\N,Missing
W00-0704,J96-1002,0,0.00395153,"For more references and information about these algorithms we refer to Daelemans et al. (1999a). 2.2 In this classification-based approach, diverse sources of information are combined in an exponential statistical model that computes weights (parameters) for all features by iteratively maximizing the likelihood of the training data. The binary features act as constraints for the model. The general idea of maximum entropy modeling is to construct a model that meets these constraints but is otherwise as uniform as possible. A good introduction to the paradigm of maximum entropy can be found in Berger et al. (1996). MXPOST (Ratnaparkhi, 1996) applied maximum Entropy learning to the tagging problem. The binary features of the statistical model are defined on the linguistic context of the word to be disambiguated (two positions to the left, two positions to the right) given the tag of the word. Information sources used include the words themselves, the tag of the previous words, and for unknown words: prefix letters, suffix letters, and information about whether a word contains a number, an upcase character, or a hyphen. These are the primitive information sources which are combined during feature generat"
W00-0704,W96-0102,1,0.851147,"Missing"
W00-0704,P98-1081,1,0.879879,"Missing"
W00-0704,C98-1078,1,\N,Missing
W02-0809,S01-1003,1,\N,Missing
W02-0809,J01-3001,0,\N,Missing
W02-0809,P97-1056,1,\N,Missing
W02-0809,W02-0814,1,\N,Missing
W02-0809,P96-1006,0,\N,Missing
W02-0814,P99-1037,1,0.884605,"Missing"
W02-0814,P01-1005,0,0.0641157,"Missing"
W02-0814,W96-0102,1,0.832954,"Missing"
W02-0814,S01-1020,1,0.844583,"Missing"
W02-0814,J98-1001,0,0.0345256,"Missing"
W02-0814,P96-1006,0,0.0730626,"polysemy and sense distributions. Iris Hendrickx and Antal van den Bosch ILK Computational Linguistics Tilburg University, The Netherlands I.H.E.Hendrickx,antalb @kub.nl  1 Introduction The task of word sense disambiguation (WSD) is to assign a sense label to a word in context. Both knowledge-based and statistical methods have been applied to the problem. See (Ide and V´eronis, 1998) for an introduction to the area. Recently (both S ENSEVAL competitions), various machine learning (ML) approaches have been demonstrated to produce relatively successful WSD systems, e.g. memory-based learning (Ng and Lee, 1996; Veenstra et al., 2000), decision lists (Yarowsky, 2000), boosting (Escudero et al., 2000). In this paper, we evaluate the results of a memorybased learning approach to WSD. We ask ourselves whether we can learn lessons from the errors made in the S ENSEVAL -2 competition. More particularly, we are interested whether there are words or categories of words which are more difficult to predict than other words. If so, do these words have certain characteristic features? We furthermore investigate the interaction between the use of different information sources and the part-of-speech categories o"
W02-0814,J01-3001,0,0.0788235,"Missing"
W02-0814,1995.iwpt-1.8,0,\N,Missing
W02-0814,J95-4002,0,\N,Missing
W02-0814,C00-2098,0,\N,Missing
W02-0814,H94-1020,0,\N,Missing
W02-0814,P00-1058,0,\N,Missing
W02-0814,P90-1035,0,\N,Missing
W02-0814,P83-1017,0,\N,Missing
W02-0814,P98-2156,0,\N,Missing
W02-0814,C98-2151,0,\N,Missing
W03-0435,daelemans-hoste-2002-evaluation,1,\N,Missing
W03-0435,P02-1062,0,\N,Missing
W04-0108,E93-1023,0,0.0475338,"ogical processes in Dutch account for a wide range of spelling alternations. For instance: a syllable containing a long vowel is written with two vowels in a closed syllable (e.g. poot (paw)) or with one vowel in an open syllable (e.g. poten (paws)). Consonants in the coda of a syllable can become voiced (e.g. huis huizen (house(s)) or doubled (e.g. kip - kippen (chicken(s))). These and other types of spelling alternations make morphological segmentation of Dutch word forms a challenging task. It is therefore not surprising to find that only a handful of research efforts have been attempted. (Heemskerk, 1993; Dehaspe et al., 1995; Van den Bosch et al., 1996; Van den Bosch and Daelemans, 1999; Laureys et al., 2002). This limited number may to some extent also be due to the limited amount of Dutch morphological resources available. The Morphological Database of CELEX Currently, CELEX is the only extensive and publicly available morphological database for Dutch (Baayen et al., 1995). Unfortunately, this database is not readily applicable as an information source in a practical system due to both a considerable amount of annotation errors and a number of practical considerations. Since both of the sy"
W04-0108,laureys-etal-2004-evaluation,1,0.868783,"Missing"
W04-0108,P99-1037,1,0.920762,"Missing"
W04-0827,C02-1039,0,0.0184143,"and value weighting methods, different neighborhood size and weighting parameters, etc., that should be optimized for each word expert independently. See (Daelemans et al., 2003b) for more information. It has been claimed, e.g. in (Daelemans et al., 1999), that lazy learning has the right bias for learning natural language processing tasks as it makes possible learning from atypical and low-frequency events that are usually discarded by eager learning methods. Architecture. Previous work on memory-based WSD includes work from Ng and Lee (1996), Veenstra et al. (2000), Hoste et al. (2002) and Mihalcea (2002). The current design of our WSD system is largely based on Hoste et al. (2002). Figure 1 gives an overview of the design of our WSD system: the training text is first linguistically analyzed. For each word-lemma–POS-tag combination, we check if it (i) is in our sense lexicon, (ii) has more than one sense and (iii) has a frequency in the training text above a certain threshold. For all combinations matching these three conditions, we train a word expert module. To all combinations with only one sense, or with more senses and a frequency below the threshold, we assign the default sense, which is"
W04-0827,P96-1006,0,0.452921,"ice between different statistical and information-theoretic feature and value weighting methods, different neighborhood size and weighting parameters, etc., that should be optimized for each word expert independently. See (Daelemans et al., 2003b) for more information. It has been claimed, e.g. in (Daelemans et al., 1999), that lazy learning has the right bias for learning natural language processing tasks as it makes possible learning from atypical and low-frequency events that are usually discarded by eager learning methods. Architecture. Previous work on memory-based WSD includes work from Ng and Lee (1996), Veenstra et al. (2000), Hoste et al. (2002) and Mihalcea (2002). The current design of our WSD system is largely based on Hoste et al. (2002). Figure 1 gives an overview of the design of our WSD system: the training text is first linguistically analyzed. For each word-lemma–POS-tag combination, we check if it (i) is in our sense lexicon, (ii) has more than one sense and (iii) has a frequency in the training text above a certain threshold. For all combinations matching these three conditions, we train a word expert module. To all combinations with only one sense, or with more senses and a fre"
W04-2414,W99-0629,1,0.820692,"role labeling: Optimizing features, algorithm, and output Antal van den Bosch, Sander Canisius, Walter Daelemans, Iris Hendrickx Erik Tjong Kim Sang ILK / Computational Linguistics CNTS / Department of Linguistics Tilburg University, P.O. Box 90153, University of Antwerp, Universiteitsplein 1, NL-5000 LE Tilburg, The Netherlands B-2610 Antwerpen, Belgium {Antal.vdnBosch,S.V.M.Canisius, {Walter.Daelemans, I.H.E.Hendrickx}@uvt.nl 1 Introduction In this paper we interpret the semantic role labeling problem as a classification task, and apply memory-based learning to it in an approach similar to Buchholz et al. (1999) and Buchholz (2002) for grammatical relation labeling. We apply feature selection and algorithm parameter optimization strategies to our learner. In addition, we investigate the effect of two innovations: (i) the use of sequences of classes as classification output, combined with a simple voting mechanism, and (ii) the use of iterative classifier stacking which takes as input the original features and a pattern of outputs of a first-stage classifier. Our claim is that both methods avoid errors in sequences of predictions typically made by simple classifiers that are unaware of their previous"
W04-2414,W04-2412,0,0.0763194,"Missing"
W05-0611,W02-2004,0,0.0774683,"Missing"
W05-0611,W96-0102,1,0.827984,"lier. This well-known problem has triggered at least the following three main types of solutions. Feedback loop Each training or test example may represent not only the regular windowed input, but also a copy of previously made classifications, to allow the classifier to be more consistent with its previous decisions. Direct feedback loops that copy a predicted output label to the input representation of the next example have been used in symbolic machine-learning architectures such as the the maximum-entropy tagger described by Ratnaparkhi (1996) and the memory-based tagger (MBT) proposed by Daelemans et al. (1996). This solution assumes that processing is directed, e.g. from left to right. A noted problem of this approach is the label bias problem (Lafferty et al., 2001), which is that a feedback-loop classifier may be driven to be consistent with its previous decision also in the case this decision was wrong; sequences of errors may result. Stacking, boosting, and voting The partly incorrect concatenated output sequence of a single classifier may serve as input to a second-stage classifier in a stacking architecture, a common machine-learning optimization technique (Wolpert, 1992). Although less elega"
W05-0611,W03-0425,0,0.0607795,"Missing"
W05-0611,W95-0107,0,0.073809,"semantic nature: English base phrase chunking (henceforth CHUNK), English named-entity recognition (NER), and disfluency chunking in transcribed spoken Dutch utterances (DISFL). C HUNK is the task of splitting sentences into non-overlapping syntactic phrases or constituents. The used data set, extracted from the WSJ Penn Treebank, contains 211,727 training examples and 47,377 test instances. The examples represent seven-word windows of words and their respective (predicted) part-of-speech tags, and each example is labeled with a class using the IOB type of segmentation coding as introduced by Ramshaw and Marcus (1995), marking whether the middle word is inside (I), outside (O), or at the beginning (B) of a chunk. Words occuring less than ten times in the training material are attenuated (converted into a more general string that retains some of the word’s surface form). Generalization performance is measured by the F-score on correctly identified and labeled constituents in test data, using the evaluation method originally used in the “shared task” subevent of the CoNLL-2000 conference (Tjong Kim Sang and Buchholz, 2000) in which this particular training and test set were used. An example sentence with bas"
W05-0611,N03-1028,0,0.0984263,"Missing"
W05-0611,W00-0726,0,0.0475029,"le is labeled with a class using the IOB type of segmentation coding as introduced by Ramshaw and Marcus (1995), marking whether the middle word is inside (I), outside (O), or at the beginning (B) of a chunk. Words occuring less than ten times in the training material are attenuated (converted into a more general string that retains some of the word’s surface form). Generalization performance is measured by the F-score on correctly identified and labeled constituents in test data, using the evaluation method originally used in the “shared task” subevent of the CoNLL-2000 conference (Tjong Kim Sang and Buchholz, 2000) in which this particular training and test set were used. An example sentence with base phrases marked and labeled is the following: [He]N P [reckons]V P [the current account deficit]N P [will narrow]V P [to]P P [only $ 1.8 billion]N P [in]P P [September]N P . 82 N ER, named-entity recognition, is to recognize and type named entities in text. We employ the English NER shared task data set used in the CoNLL2003 conference, again using the same evaluation method as originally used in the shared task (Tjong Kim Sang and De Meulder, 2003). This data set discriminates four name types: persons, org"
W05-0611,W03-0419,0,0.0324592,"Missing"
W05-0611,A00-2007,0,0.0555511,"Missing"
W06-2602,W95-0107,0,0.0334661,"method, and the trigram method combined both with majority voting, and with constraint satisfaction inference. The last column shows the performance of the (hypothetical) oracle inference procedure. spelling changes due to compounding, derivation, or inflection that would enable the reconstruction of the appropriate root forms of the involved morphemes. For CHUNK, and the three information extraction tasks, instances represent a seven-token window of words and their (predicted) part-of-speech tags. Each token is labelled with a class using the IOB type of segmentation coding as introduced by Ramshaw and Marcus (1995), marking whether the middle word is inside (I), outside (O), or at the beginning (B) of a chunk, or named entity. Performance is measured by the F-score on correctly identified and labelled chunks, or named entities. Instances for PHONEME, and MORPHO consist of a seven-letter window of letters only. The labels assigned to an instance are task-specific and have been introduced above, together with the tasks themselves. Generalisation performance is measured on the word accuracy level: if the entire phonological transcription of the word is predicted correctly, or if all three aspects of the mo"
W06-2602,W00-0726,0,0.0385326,"ing tasks, we composed a benchmark set consisting of six different tasks, covering four areas in natural language processing: syntax (syntactic chunking), morphology (morphological analysis), phonology (grapheme-to-phoneme conversion), and information extraction (general, medical, and biomedical named-entity recognition). Below, the six data sets used for these tasks are introduced briefly. C HUNK is the task of splitting sentences into non-overlapping syntactic phrases or constituents. The data set, extracted from the WSJ Penn Treebank, and first used in the CoNLL-2000 shared task (Tjong Kim Sang and Buchholz, 2000), contains 211,727 training examples and 47,377 test instances. N ER, named-entity recognition, involves identifying and labelling named entities in text. We employ the English NER shared task data set used in the CoNLL-2003 conference (Tjong Kim Sang and De Meulder, 2003). This data set discriminates four name types: persons, organisations, locations, and a rest category of “miscellany names”. The data set is a collection of newswire 12 articles from the Reuters Corpus, RCV11 . The given training set contains 203,621 examples; as test set we use the “testb” evaluation set which contains 46,43"
W06-2602,W03-0419,0,0.0388559,"Missing"
W06-2602,W05-0611,1,0.795576,"Missing"
W06-2901,H01-1052,0,0.024865,"t this is exceptional rather than common practice in comparative experiments. Yet everyone knows that many factors potentially play a role in the outcome of a (comparative) machine learning experiment: the data used (the sample selection and the sample size), the information sources used (the features selected) and their representation (e.g. as nominal or binary features), the class representation (error coding, binarization of classes), and the algorithm parameter settings (most ML algorithms have various parameters that can be tuned). Moreover,all these factors are known to interact. E.g., (Banko and Brill, 2001) demonstrated that for confusion set disambiguation, a prototypical disambiguation in context problem, the amount of data used dominates the effect of the bias of the learning method employed. The effect of training data size on relevance of POS-tag information on top of lexical information in relation finding was studied in (van den Bosch and Buchholz, 2001). The positive effect of POS-tags disappears with sufficient data. In (Daelemans et al., 2003) it is shown that the joined optimization of feature selection and algorithm parameter optimization significantly improves accuracy compared to s"
W06-2901,W00-0701,0,0.0316642,"uestions all relate to learning algorithm bias issues. Learning is a search process in a hypothesis space. Heuristic limitations on the search process and restrictions on the representations allowed for input and hypothesis representations together define this bias. There is not a lot of work on matching properties of learning algorithms with properties of language processing tasks, or more specifically on how the bias of particular (families of) learning algorithms relates to the hypothesis spaces of particular (types of) language processing tasks. As an example of such a unifying approach, (Roth, 2000) shows that several different algorithms (memory-based learning, tbl, snow, decision lists, various statistical learners, ...) use the same type of knowledge representation, a linear representation over a feature space based on a transformation of the original instance space. However, the only relation to language here is rather negative with the claim that this bias is not sufficient for learning higher level language processing tasks. As another example of this type of work, Memory-Based Learning (MBL) (Daelemans and van den Bosch, 2005), with its implicit similaritybased smoothing, storage"
W06-2901,P02-1055,0,\N,Missing
W06-2907,piperidis-etal-2004-multimodal,1,\N,Missing
W06-2907,H93-1061,0,\N,Missing
W06-2907,P04-1036,0,\N,Missing
W06-2907,W07-1401,1,\N,Missing
W06-2907,P05-1014,1,\N,Missing
W06-2907,daelemans-etal-2004-automatic,1,\N,Missing
W06-2907,P98-2127,0,\N,Missing
W06-2907,C98-2122,0,\N,Missing
W08-1129,W07-2302,0,0.0688957,"irstname.lastname@ua.ac.be Abstract In this paper we describe our machine learning approach to the generation of referring expressions. As our algorithm we use memory-based learning. Our results show that in case of predicting the TYPE of the expression, having one general classifier gives the best results. On the contrary, when predicting the full set of properties of an expression, a combined set of specialized classifiers for each subdomain gives the best performance. 1 Introduction In this paper we describe the systems with which we participated in the GREC task of the REG 2008 challenge (Belz and Varges, 2007). The GREC task concerns predicting which expression is appropriate to refer to a particular discourse referent in a certain position in a text, given a set of alternative referring expressions for selection. The organizers provided the GREC corpus that consists of 2000 texts collected from Wikipedia, from 5 different subdomains (people, cities, countries, mountains and rivers) . One of the main goals of the task is to discover what kind of information is useful in the input to make the decision between candidate referring expressions. We experimented with a pool of features and several machin"
W08-2128,C04-1186,0,0.0628602,"Missing"
W08-2128,S07-1038,1,0.801742,"n memory has been argued to provide a key advantage over abstracting methods in NLP that ignore exceptions and subregularities (Daelemans et al., 1999). Memory-based algorithms have been previously applied to semantic role labeling. Van den Bosch et al. (2004) participated in the CoNLL2004 shared task with a system that extended the basic memory-based learning method with class n-grams, iterative classifier stacking, and automatic output post-processing. Tjong Kim Sang et al. (2005) participated in the CoNLL2005 shared task with a system that incorporates spelling error correction techniques. Morante and Busser (2007) participated in the SemEval-2007 competition with a semantic role labeler for Spanish based on gold standard constituent syntax. These systems use different types of constituent syntax (shallow parsing, full parsing). We are aware of two systems that perform semantic role labeling based on dependency syntax previous to the CoNLL-2008 shared task. Hacioglu (2004) converts the data from the CoNLL-2004 shared task into dependency trees and uses support vector machines. Morante (2008) describes a memorybased semantic role labeling system for Spanish based on gold standard dependency syntax. We de"
W08-2128,W08-2121,0,0.101009,"Missing"
W08-2128,W06-2933,0,0.0284678,"account for relative differences in discriminative power of the features. 2.1 Syntactic dependencies The MaltParser 0.41 (Nivre, 2006; Nivre et al., 2007) is an inductive dependency parser that uses four essential components: a deterministic algorithm for building labeled projective dependency graphs; history-based feature models for predicting the next parser action; support vector machines for mapping histories to parser actions; and graph transformations for recovering nonprojective structures. The learner type used was support vector machines, with the same parameter options reported by (Nivre et al., 2006). The parser algorithm used was Nivre, with the options and model (eng.par) for English as specified on http://w3.msi.vxu.se/users/jha/conll07/. The tagset.pos, tagset.cpos and tagset.dep were extracted from the training corpus. 2.2 Semantic dependencies The semantics task consists of finding the predicates, assigning a PropBank or a NomBank frame to them and extracting their semantic role dependencies. Because of lack of resources, we did not have time to develop a word sense disambiguation system. So, predicates were assigned the frame ‘.01’ by default. The system handles the semantic role l"
W08-2128,W05-0637,0,0.208478,"Missing"
W08-2128,W04-2414,1,0.883024,"Missing"
W08-2128,morante-2008-semantic,1,\N,Missing
W09-0604,C04-1051,0,0.105585,"Missing"
W09-0604,W03-1608,0,0.0699754,"Missing"
W09-0604,A00-2024,0,0.110693,"Missing"
W09-0604,N03-1003,0,0.280604,"Missing"
W09-0604,W03-1101,0,0.0331291,"the observed data. We analyse the remaining problems and conclude that in those cases word order changes and paraphrasing are crucial, and argue for more elaborate sentence compression models which build on NLG work. 1 Introduction The task of sentence compression (or sentence reduction) can be defined as summarizing a single sentence by removing information from it (Jing and McKeown, 2000). The compressed sentence should retain the most important information and remain grammatical. One of the applications is in automatic summarization in order to compress sentences extracted for the summary (Lin, 2003; Jing and McKeown, 2000). Other applications include automatic subtitling (Vandeghinste and Tsjong Kim Sang, 2004; Vandeghinste and Pan, 2004; Daelemans et al., 2004) and displaying text on devices with very small screens (CorstonOliver, 2001). A more restricted version defines sentence compression as dropping any subset of words from the input sentence while retaining important information and grammaticality (Knight and Proceedings of the 12th European Workshop on Natural Language Generation, pages 25–32, c Athens, Greece, 30 – 31 March 2009. 2009 Association for Computational Linguistics 25"
W09-0604,E06-1040,0,0.0771484,"Missing"
W09-0604,P06-1048,0,0.0449688,"Missing"
W09-0604,P05-1036,0,0.305391,"Missing"
W09-0604,W04-1015,0,0.450146,"crucial, and argue for more elaborate sentence compression models which build on NLG work. 1 Introduction The task of sentence compression (or sentence reduction) can be defined as summarizing a single sentence by removing information from it (Jing and McKeown, 2000). The compressed sentence should retain the most important information and remain grammatical. One of the applications is in automatic summarization in order to compress sentences extracted for the summary (Lin, 2003; Jing and McKeown, 2000). Other applications include automatic subtitling (Vandeghinste and Tsjong Kim Sang, 2004; Vandeghinste and Pan, 2004; Daelemans et al., 2004) and displaying text on devices with very small screens (CorstonOliver, 2001). A more restricted version defines sentence compression as dropping any subset of words from the input sentence while retaining important information and grammaticality (Knight and Proceedings of the 12th European Workshop on Natural Language Generation, pages 25–32, c Athens, Greece, 30 – 31 March 2009. 2009 Association for Computational Linguistics 25 Atranos and Musa – on automatic subtitling (Vandeghinste and Tsjong Kim Sang, 2004; Vandeghinste and Pan, 2004; Daelemans et al., 2004). All"
W09-0604,vandeghinste-tjong-kim-sang-2004-using,0,0.0675821,"Missing"
W09-0604,daelemans-etal-2004-automatic,1,\N,Missing
W09-0604,Y03-1033,0,\N,Missing
W09-1105,W06-2920,0,0.00604904,"0.01 13.55 877 4.98 4.84 Papers 9 2670 60935 5566 26.24 11.27 27.67 29.55 17.00 0.03 12.70 389 8.81 7.61 Abstracts 1273 11871 282243 14506 26.43 3.17 30.49 35.93 19.76 10.63 13.45 1848 9.43 8.06 6.33 5.69 8.55 97.64 2.35 81.77 18.22 85.70 14.29 2005), a bidirectional inference based tagger that analyzes English sentences and outputs the base forms, part-of-speech tags, chunk tags, and named entity tags in a tab-separated format. Additionally, we converted the annotation about scope of negation into a token-per-token representation, following the standard format of the 2006 CoNLL Shared Task (Buchholz and Marsi, 2006), where sentences are separated by a blank line and fields are separated by a single tab character. A sentence consists of a sequence of tokens, each one starting on a new line. 4 Table 1: Statistics about the subcorpora in the BioScope corpus and the negation scopes (“Av”. stands for average). The BioScope corpus consists of three parts: clinical free-texts (radiology reports), biological full papers and biological paper abstracts from the GENIA corpus (Collier et al., 1999). Table 1 shows statistics about the corpora. Negation signals are represented by one or more tokens. Only one negation"
W09-1105,E99-1043,0,0.589616,"Missing"
W09-1105,W08-2128,1,0.881647,"Missing"
W09-1105,W08-0606,0,0.667574,"Missing"
W09-1105,W05-0637,0,0.0461433,"Missing"
W09-1105,H05-1059,0,0.0194498,"Missing"
W09-1304,W06-2920,0,0.0108606,"73.28 26.71 76.55 23.44 82.45 17.54 Table 1: Statistics about the subcorpora in the BioScope corpus and the hedge scopes (“Av”. stands for average). The texts have been processed with the GENIA tagger (Tsuruoka and Tsujii, 2005; Tsuruoka et al., 2005), a bidirectional inference based tagger that analyzes English sentences and outputs the base forms, part-of-speech tags, chunk tags, and named entity tags in a tab-separated format. Additionally, we converted the annotation about scope of negation into a token-per-token representation, following the standard format of the 2006 CoNLL Shared Task (Buchholz and Marsi, 2006), where sentences are separated by a blank line and fields are separated by a single tab character. A sentence consists of a se2 Web page: www.inf.u-szeged.hu/rgai/bioscope. quence of tokens, each one starting on a new line. 30 4 Finding the scope of hedge cues We model this task in the same way that we modelled the task for finding the scope of negation (Morante and Daelemans, 2009), i.e., as two consecutive classification tasks: a first one that consists of classifying the tokens of a sentence as being at the beginning of a hedge signal, inside or outside. This allows the system to find mult"
W09-1304,E99-1043,0,0.694642,"Missing"
W09-1304,W08-0607,0,0.618012,"Available at http://www.benmedlock.co.uk/hedgeclassif.html. Medlock and Briscoe (2007), whereas with a lemma representation the system achieves a peak performance of 0.8 BEP, and with bigrams of 0.82 BEP. Szarvas (2008) follows Medlock and Briscoe (2007) in classifying sentences as being speculative or nonspeculative. Szarvas develops a MaxEnt system that incorporates bigrams and trigrams in the feature representation and performs a complex feature selection procedure in order to reduce the number of keyword candidates. It achieves up to 0.85 BEP and 85.08 F1 by using an external dictionary. Kilicoglu and Bergler (2008) apply a linguistically motivated approach to the same clasification task by using knowledge from existing lexical resources and incorporating syntactic patterns. Additionally, hedge cues are weighted by automatically assigning an information gain measure and by assigning weights semi– automatically depending on their types and centrality to hedging. The system achieves results of 0.85 BEP. As mentioned earlier, we are not aware of research that has focused on learning the scope of hedge signals inside or outside of the biomedical domain, which makes a direct comparison with the approaches des"
W09-1304,W04-3103,0,0.669823,"Missing"
W09-1304,P07-1125,0,0.61545,"in patient documents into controlled vocabulary terms”. The system uses a semantic grammar that consists of rules that specify well-formed semantic patterns. The extracted findings are assigned one of five types of modality information: no, low certainty, moderate certainty, high certainty and cannot evaluate. Di Marco and Mercer (2005) use hedging information to classify citations. They observe that citations appear to occur in sentences marked with hedging cues. Work on hedging in the machine learning field has as a goal to classify sentences into speculative or definite (non speculative). Medlock and Briscoe (2007) provide a definition of what they consider to be hedge instances and define hedge classification as a weakly supervised machine learning task. The method they use to derive a learning model from a seed corpus is based on iteratively predicting labels for unlabeled training samples. They report experiments with SVMs on a dataset that they make publicly available1 . The experiments achieve a recall/precision break even point (BEP) of 0.76. They apply a bag-of-words (BOG) approach to sample representation. Medlock (2008) presents an extension of this work by experimenting with more features (par"
W09-1304,W09-1105,1,0.771533,"= “X7.5.2”&gt; might &lt;/cue&gt; be involved &lt;xcope id=“X7.5.1”&gt;in terminal granulocyte differentiation &lt;cue type= “speculation” ref=“X7.5.1” &gt;or&lt;/cue&gt; in regulating granulocyte functionality &lt;/xcope&gt;&lt;/xcope&gt;&lt;/xcope&gt;. Proceedings of the Workshop on BioNLP, pages 28–36, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics Contrary to current practice to only detect modality, our system also determines the part of the sentence that is hedged. We are not aware of other systems that perform this task. The system is based on a similar system that finds the scope of negation cues (Morante and Daelemans, 2009). We show that the system performs well for this task and that the same scope finding approach can be applied to both negation and hedging. To investigate the robustness of the approach, the system is tested on three subcorpora of the BioScope corpus that represent different text types. Although the system was developed and tested on biomedical text, the same approach can also be applied to text from other domains. The paper is organised as follows. In Section 2, we summarise related work. In Section 3, we describe the corpus on which the system has been developed. In Section 4, we introduce t"
W09-1304,W08-0606,0,0.74984,"adjectives, adverbs, and nouns. Additionally, it includes also a variety of non–lexical cues. Light et 29 al. (2004) analyse the use of speculative language in MEDLINE abstacts. They studied the expression of levels of belief (hypothesis, tentative conclusions, hedges, and speculations) and annotated a corpus of abstracts in order to check if the distinction between high speculative, low speculative and definite sentences could be made reliably. They found that the speculative vs. definite distinction was reliable, but the distinction between low and high speculative was not. Thompson et al. (2008) report on a list of words and phrases that express modality in biomedical texts and put forward a categorisation scheme. The list and the scheme are validated by annotating 202 MEDLINE abstracts. Some NLP applications incorporate modality information. Friedman et al. (1994) develop a medical text processor “that translates clinical information in patient documents into controlled vocabulary terms”. The system uses a semantic grammar that consists of rules that specify well-formed semantic patterns. The extracted findings are assigned one of five types of modality information: no, low certaint"
W09-1304,P08-1033,0,0.696896,"ly available1 . The experiments achieve a recall/precision break even point (BEP) of 0.76. They apply a bag-of-words (BOG) approach to sample representation. Medlock (2008) presents an extension of this work by experimenting with more features (part-of-speech (PoS), lemmas, and bigrams). Experiments show that the PoS representation does not yield significant improvement over the results in 1 Available at http://www.benmedlock.co.uk/hedgeclassif.html. Medlock and Briscoe (2007), whereas with a lemma representation the system achieves a peak performance of 0.8 BEP, and with bigrams of 0.82 BEP. Szarvas (2008) follows Medlock and Briscoe (2007) in classifying sentences as being speculative or nonspeculative. Szarvas develops a MaxEnt system that incorporates bigrams and trigrams in the feature representation and performs a complex feature selection procedure in order to reduce the number of keyword candidates. It achieves up to 0.85 BEP and 85.08 F1 by using an external dictionary. Kilicoglu and Bergler (2008) apply a linguistically motivated approach to the same clasification task by using knowledge from existing lexical resources and incorporating syntactic patterns. Additionally, hedge cues are"
W09-1304,H05-1059,0,0.01795,"Missing"
W09-1408,W06-2920,0,0.00903165,"syntactic information as features for the machine learner. GDep is a a dependency parser for biomedical text trained on the Tsujii Lab’s GENIA treebank. The dependency parser predicts for every word the partof-speech tag, the lemma, the syntactic head, and the dependency relation. In addition to these regular dependency tags it also provides information about the IOB-style chunks and named entities. The classifiers use the output of GDep in addition to some frequency measures as features. We represent the data into a columns format, following the standard format of the CoNLL Shared Task 2006 (Buchholz and Marsi, 2006), in which sentences are separated by a blank line and fields are separated by a single tab character. A sentence consists of tokens, each one starting on a new line. 4.1 Phase 1: Entity Detection In the first phase, a memory based classifier predicts for every word in the corpus whether it is an entity or not and the type of entity. In this setting, entity refers to what in the shared task definition are events and entities other than proteins. Classes are defined in the IOB-style3 in order to find entities that span over multiple words. Figure 1 shows a simplified version of a sentence in wh"
W09-1408,W06-0602,0,0.0655271,"Missing"
W09-1408,E99-1043,0,0.0900074,"Missing"
W09-1408,W09-1105,1,0.814887,"of the shared task definition: rich semantics, a text-bound approach, and decomposition of linguistic phenomena. Memory-based algorithms have been successfully applied in language processing to a wide range of linguistic tasks, from phonology to semantic analysis. Our goal was to investigate the performance of a memory–based approach to the event extraction task, using only the information available in the training corpus and modelling the task applying an approach similar to the one that has been applied to tasks like semantic role labeling (Morante et al., 2008) or negation scope detection (Morante and Daelemans, 2009). In Section 2 we briefly describe the task. Section 3 reviews some related work. Section 4 presents the system, and Section 5 the results. Finally, some conclusions are put forward in Section 6. 2 Task description The BioNLP Shared Task 2009 on event extraction consists of recognising bio-molecular events in biomedical texts, focusing on molecular events involving proteins and genes. An event is defined as a relation that holds between multiple entities that fulfil different roles. Events can participate in one type Proceedings of the Workshop on BioNLP: Shared Task, pages 59–67, c Boulder, C"
W09-1408,W08-2128,1,0.887388,"Missing"
W09-1408,D07-1111,0,0.0154539,"e detected. In the second phase, event participants and arguments are identified. In the third phase, postprocessing heuristics select the best frame for each event. Parameterisation of the classifiers used in Phases 1 and 2 was performed by experimenting with sets of parameters on the development set. We experimented with manually selected parameters and with parameters selected by a genetic algorithm, but the parameters found by the genetic algorithm did not yield better results than the manually selected parameters As a first step, we preprocess the corpora with the GDep dependency parser (Sagae and Tsujii, 2007) so that we can use part-of-speech tags and syntactic information as features for the machine learner. GDep is a a dependency parser for biomedical text trained on the Tsujii Lab’s GENIA treebank. The dependency parser predicts for every word the partof-speech tag, the lemma, the syntactic head, and the dependency relation. In addition to these regular dependency tags it also provides information about the IOB-style chunks and named entities. The classifiers use the output of GDep in addition to some frequency measures as features. We represent the data into a columns format, following the sta"
W09-1408,C08-1096,0,0.0129636,"Official results of Task 2. Approximate Span Matching/Approximate Recursive Matching. Results obtained on the development set are a little bit higher. For Task1 an overall F1 of 34.78 and for Task 2 33.54. For most event types precision and recall are unbalanced, the system scores higher in recall. Further research should focus on increasing precision because the system is predicting false positives. It would be possible to add a step in order to filter out the false positives by comparing word sequences with event patterns derived from the corpus, which is an approach taken in the system by Sasaki et al. (2008) . In the case of Binding events, both precision and recall are low. There are two explanations for this. In the first place, the first classifier misses almost half of the binding events. As an example, for the sentence in (8.1), the gold standard identifies as binding event the multiwords binds as a homodimer and form heterodimers, whereas the system identifies two binding events for the same sentence, binds and homodimer, none of which is correct because the correct one is the multiword unit. For the sentence in (8.2), the gold standard identifies as binding events bind, form homo-, and het"
W09-2812,J05-3002,0,0.0716088,"Missing"
W09-2812,N03-1020,0,0.181494,"the maximal margin relevance criterion (Carbonell and Goldstein, 1998). MMR models the trade-off between a focused summary and a summary with a wide scope. The novelty-reranker is an extension of the cosine-reranker and boosts sentences occurring after an important sentence by multiplying with 1.2. The reranker tries to mimic human behavior as people tend to pick clusters of sentences when summarizing. 4 For the experiments on the development set, we compare each of the automatically produced extracts with five manually written summaries and report macro-average Rouge-2 and Rouge-SU4 scores (Lin and Hovy, 2003). For the experiments on the test set, we also perform a manual evaluation. We follow the DUC 2006 guidelines for manual evaluation of responsiveness and the linguistic quality of the produced summaries. The responsiveness scores express the information content of the summary with respect to the query. The linguistic quality is evaluated on five different objectives: grammaticality, non-redundancy, coherence, referential clarity and focus. The annotators can choose a value on a five point scale where 1 means ‘very poor’ and 5 means ‘very good’. We use two independent annotators to evaluate the"
W09-2812,radev-etal-2004-mead,0,0.100917,"Missing"
W09-2812,vossen-etal-2008-integrating,0,0.0604442,"Missing"
W10-2605,P08-1034,0,0.00891607,"ich no annotated data is available. As a first step, the metric resulting in the best linear fit between the metric and the accuracy should be searched. If a linear relation can be established, one can take annotated training data from the domain that is closest to the unannotated corpus and assume that this will give the best accuracy score. Related Work In articles dealing with the influence of domain shifts on the performance of an NLP tool, the in-domain data and out-of-domain data are taken from different corpora, e.g., sentences from movie snippets, newspaper texts and personal weblogs (Andreevskaia and Bergler, 2008). It can be expected that these corpora are indeed dissimilar enough to consider them as separate domains, but no objective measure has been used to define them as such. The fact that the NLP tool produces lower results for cross-domain experiments can be taken as an indication of the presence of separate domains. A nice overview paper on statistical domain adaptation can be found in Bellegarda (2004). A way to express the degree of relatedness, apart from this well-known accuracy drop, can be found in Daum´e and Marcu (2006). They propose a domain adaptation framework containing a parameter π"
W10-2605,C94-1103,0,0.0568781,"tators provided 9 domain codes (i.e. wridom), making it possible to divide the text from books and periodicals into 9 subcorpora. These annotated semantic domains are: imaginative (wridom1), natural & pure science (wridom2), applied science (wridom3), social science (wridom4), world affairs (wridom5), commerce & finance (wridom6), arts (wridom7), belief & thought (wridom8), and leisure (wridom9). The extracted corpus contains sentences in which every token is tagged with a part-of-speech tag as defined by the BNC. Since the BNC has been tagged automatically, using the CLAWS4 automatic tagger (Leech et al., 1994) and the Template Tagger (Pacey et al., 1997), the experiments in this article are artificial in the sense that they do not learn real part-of-speech tags but rather partof-speech tags as they are assigned by the automatic taggers. 3.2 Rényi 0.99 MORE SIMILAR social-world social-art social-belief Euclidean MORE SIMILAR social-belief social-world social-art art-social social-imaginative social-imaginative LESS SIMILAR LESS SIMILAR Figure 1: A visual comparison of two similarity metrics: R´enyi with α = 0.99 and Euclidean. Figure 1 gives an impression of the difference between two similarity met"
W10-2605,P07-1056,0,0.143942,"he NLP tool produces lower results for cross-domain experiments can be taken as an indication of the presence of separate domains. A nice overview paper on statistical domain adaptation can be found in Bellegarda (2004). A way to express the degree of relatedness, apart from this well-known accuracy drop, can be found in Daum´e and Marcu (2006). They propose a domain adaptation framework containing a parameter π. Low values of π mean that in-domain and out-of-domain data differ significantly. They also used Kullback-Leibler divergence to compute the similarity between unigram language models. Blitzer et al. (2007) propose a supervised way of measuring the similarity between the two domains. They compute the Huber loss, as a proxy of the A-distance (Kifer et al., 2004), for every instance that they labeled with their tool. The resulting measure correlates with the adaptation loss they observe when applying a sentiment classification tool on different domains. 5 In this article we implemented a way to measure the similarity between two corpora. One may decide to use such a metric to categorize the available corpora for a given task into groups, depending on their similarity. It should be noted that in or"
W10-2605,P06-1035,0,0.027186,"Missing"
W10-2605,gimenez-marquez-2004-svmtool,0,0.0107939,"Missing"
W10-3006,W08-0607,0,0.634068,"ension of this work by experimenting with more features (part-of-speech, lemmas, and bigrams). With a lemma representation the system achieves a peak performance of 0.80 BEP, and with bigrams of 0.82 BEP. Szarvas (2008) follows Medlock and Briscoe (2007) in classifying sentences as being speculative or non-speculative. Szarvas develops a MaxEnt system that incorporates bigrams and trigrams in the feature representation and performs a complex feature selection procedure in order to reduce the number of keyword candidates. It achieves up to 0.85 BEP and 85.08 F1 by using an external dictionary. Kilicoglu and Bergler (2008) apply a linguistically motivated approach to the same classification task by using knowledge from existing lexical resources and incorporating syntactic patterns. Additionally, hedge cues are weighted by automatically assigning an information gain measure and by assigning weights semi–automatically depending on their types and centrality to hedging. The system achieves results of 0.85 BEP. As for Task 2, previous work (Morante and ¨ ur and Radev, 2009) has Daelemans, 2009; Ozg¨ focused on finding the scope of hedge cues in the BioScope corpus (Vincze et al., 2008). Both systems approach the t"
W10-3006,W04-3103,0,0.181568,"Hedging has been broadly treated from a theoretical perspective. The term hedging is originally due to Lakoff (1972). Palmer (1986) defines a term related to hedging, epistemic modality, which expresses the speaker’s degree of commitment to the truth of a proposition. Hyland (1998) focuses specifically on scientific texts. He proposes a pragmatic classification of hedge expressions based on an exhaustive analysis of a corpus. The catalogue of hedging cues includes modal auxiliaries, epistemic lexical verbs, epistemic adjectives, adverbs, nouns, and a variety of non–lexical cues. Light et al. (2004) analyse the use of speculative language in MEDLINE abstracts. Some NLP applications incorporate modality information (Friedman et al., 1994; Di Marco and Mercer, 2005). As for annotated corpora, Thompson et al. (2008) report on a list of words and phrases that express modality in biomedical texts and put forward a categorisation scheme. Additionally, the BioScope corpus (Vincze et al., 2008) consists of a collection of clinical free-texts, biological full papers, and biological abstracts annotated with negation and speculation cues and their scope. Although only a few pieces of research have"
W10-3006,P07-1125,0,0.51731,"putational Linguistics model from a seed corpus is based on iteratively predicting labels for unlabeled training samples. They report experiments with SVMs on a dataset that they make publicly available3 . The experiments achieve a recall/precision break even point (BEP) of 0.76. They apply a bag-of-words approach to sample representation. Medlock (2008) presents an extension of this work by experimenting with more features (part-of-speech, lemmas, and bigrams). With a lemma representation the system achieves a peak performance of 0.80 BEP, and with bigrams of 0.82 BEP. Szarvas (2008) follows Medlock and Briscoe (2007) in classifying sentences as being speculative or non-speculative. Szarvas develops a MaxEnt system that incorporates bigrams and trigrams in the feature representation and performs a complex feature selection procedure in order to reduce the number of keyword candidates. It achieves up to 0.85 BEP and 85.08 F1 by using an external dictionary. Kilicoglu and Bergler (2008) apply a linguistically motivated approach to the same classification task by using knowledge from existing lexical resources and incorporating syntactic patterns. Additionally, hedge cues are weighted by automatically assigni"
W10-3006,W06-2920,0,0.00400713,"m the training data. 3 the xml format to the CoNLL format is a source of error propagation, we convert the gold CoNLL files into xml format and we run the scorer provided by the task organisers. The results obtained are listed in Table 2. Task 1 Task 2 WIKI BIO-ART BIO-ABS BIO-ART BIO-ABS F1 100.00 100.00 100.00 99.10 99.66 Preprocessing Table 2: Evaluation of the conversion from xml to CoNLL format. As a first step, we preprocess the data in order to extract features for the machine learners. We convert the xml files into a token-per-token representation, following the standard CoNLL format (Buchholz and Marsi, 2006), where sentences are separated by a blank line and fields are separated by a single tab character. A sentence consists of a sequence of tokens, each one starting on a new line. The WIKI data are processed with the Memory Based Shallow Parser (MBSP) (Daelemans and van den Bosch, 2005) in order to obtain lemmas, part-of-speech (PoS) tags, and syntactic chunks, and with the MaltParser (Nivre, 2006) in order to obtain dependency trees. The BIO data are processed with the GDep parser (Sagae and Tsujii, 2007) in order to get the same information. # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20"
W10-3006,W09-1304,1,0.908589,"ces and incorporating syntactic patterns. Additionally, hedge cues are weighted by automatically assigning an information gain measure and by assigning weights semi–automatically depending on their types and centrality to hedging. The system achieves results of 0.85 BEP. As for Task 2, previous work (Morante and ¨ ur and Radev, 2009) has Daelemans, 2009; Ozg¨ focused on finding the scope of hedge cues in the BioScope corpus (Vincze et al., 2008). Both systems approach the task in two steps, identifying the hedge cues and finding their scope. The main difference between the two systems is that Morante and Daelemans (2009) perform the sec¨ ond phase with a machine learner, whereas Ozgur and Radev (2009) perform the second phase with a rule-based system that exploits syntactic information. The approach to resolving the scopes of hedge cues that we present in this paper is similar to the approach followed in Morante and Daelemans (2009) in that the task is modelled in the same way. A difference between the two systems is that this system uses only one classifier to solve Task 2, whereas the system described in Morante and Daelemans (2009) used three classifiers and a metof classifying the tokens of a sentence as"
W10-3006,D09-1145,0,0.874751,"ocedure in order to reduce the number of keyword candidates. It achieves up to 0.85 BEP and 85.08 F1 by using an external dictionary. Kilicoglu and Bergler (2008) apply a linguistically motivated approach to the same classification task by using knowledge from existing lexical resources and incorporating syntactic patterns. Additionally, hedge cues are weighted by automatically assigning an information gain measure and by assigning weights semi–automatically depending on their types and centrality to hedging. The system achieves results of 0.85 BEP. As for Task 2, previous work (Morante and ¨ ur and Radev, 2009) has Daelemans, 2009; Ozg¨ focused on finding the scope of hedge cues in the BioScope corpus (Vincze et al., 2008). Both systems approach the task in two steps, identifying the hedge cues and finding their scope. The main difference between the two systems is that Morante and Daelemans (2009) perform the sec¨ ond phase with a machine learner, whereas Ozgur and Radev (2009) perform the second phase with a rule-based system that exploits syntactic information. The approach to resolving the scopes of hedge cues that we present in this paper is similar to the approach followed in Morante and Daele"
W10-3006,D07-1111,0,0.0074119,"he xml files into a token-per-token representation, following the standard CoNLL format (Buchholz and Marsi, 2006), where sentences are separated by a blank line and fields are separated by a single tab character. A sentence consists of a sequence of tokens, each one starting on a new line. The WIKI data are processed with the Memory Based Shallow Parser (MBSP) (Daelemans and van den Bosch, 2005) in order to obtain lemmas, part-of-speech (PoS) tags, and syntactic chunks, and with the MaltParser (Nivre, 2006) in order to obtain dependency trees. The BIO data are processed with the GDep parser (Sagae and Tsujii, 2007) in order to get the same information. # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 WORD The structural evidence lends strong support to the inferred domain pair , resulting in a high confidence set of domain pairs . LEMMA The structural evidence lend strong support to the inferred domain pair , result in a high confidence set of domain pair . PoS CHUNK NE D DT B-NP O 3 JJ I-NP O 3 NN I-NP O 4 VBZ B-VP O 0 JJ B-NP O 6 NN I-NP O 4 TO B-PP O 6 DT B-NP O 11 JJ I-NP O 11 NN I-NP O 11 NN I-NP O 7 , O O 4 VBG B-VP O 4 IN B-PP O 13 DT B-NP O 18 JJ I-NP O 18 NN I-NP O 18 NN I-NP O 14 IN"
W10-3006,W10-3001,0,0.359779,"Missing"
W10-3006,P08-1033,0,0.163583,"010 Association for Computational Linguistics model from a seed corpus is based on iteratively predicting labels for unlabeled training samples. They report experiments with SVMs on a dataset that they make publicly available3 . The experiments achieve a recall/precision break even point (BEP) of 0.76. They apply a bag-of-words approach to sample representation. Medlock (2008) presents an extension of this work by experimenting with more features (part-of-speech, lemmas, and bigrams). With a lemma representation the system achieves a peak performance of 0.80 BEP, and with bigrams of 0.82 BEP. Szarvas (2008) follows Medlock and Briscoe (2007) in classifying sentences as being speculative or non-speculative. Szarvas develops a MaxEnt system that incorporates bigrams and trigrams in the feature representation and performs a complex feature selection procedure in order to reduce the number of keyword candidates. It achieves up to 0.85 BEP and 85.08 F1 by using an external dictionary. Kilicoglu and Bergler (2008) apply a linguistically motivated approach to the same classification task by using knowledge from existing lexical resources and incorporating syntactic patterns. Additionally, hedge cues ar"
W10-3006,W08-0606,0,0.608347,"Missing"
W11-0141,baker-etal-2010-modality,0,0.0143565,"e scope finding task that would be different based on observations from a corpus from a different domain (Section 4). 1 2 Example to be found in http://www.biomedcentral.com/content/pdf/1475-2875-1-1.pdf [last consulted 8-10-2010] A cue is the lexical marker that expresses negation or modality. 350 2 Achievements in scope processing In the last years, several corpora have been annotated with information related to modality and polarity, which have made it possible to develop machine learning systems. Annotation has been performed at different levels: word (Hassan and Radev, 2010), expression (Baker et al., 2010; Toprak et al., 2010), sentence (Medlock and Briscoe, 2007), event (Saur´ı and Pustejovsky, 2009), discourse relation (Prasad et al., 2006), text (Amancio et al., 2010), and scope of negation and modality cues (Vincze et al., 2008). Thanks to the existence of the BioScope corpus, the scope processing task was recently introduced. BioScope is a freely available resource, that consists of three parts of medical and biological texts annotated with negation and hedge cues and their scope. The scope processing task is concerned with determining at a sentence level which tokens are affected by moda"
W11-0141,W10-3110,0,0.0702159,"f the BioScope corpus, the scope processing task was recently introduced. BioScope is a freely available resource, that consists of three parts of medical and biological texts annotated with negation and hedge cues and their scope. The scope processing task is concerned with determining at a sentence level which tokens are affected by modality and negation cues. It was first modelled as a classification problem by Morante et al. (2008). ¨ ur Later on several systems have been trained on the same corpus (Morante and Daelemans, 2009; Ozg¨ and Radev, 2009; Agarwal and Yu, 2010; Li et al., 2010). Councill et al. (2010) process scopes of negation cues in a different corpus of product reviews, but this corpus is not publicly available. The CoNLL Shared Task 2010 on Learning to detect hedges and their scope in natural language text (Farkas et al., 2010) boosted research on the topic. It consisted of identifying sentences containing uncertainty and recognizing speculative text spans inside sentences. Participating systems would, for example, produce the tagged sentence in (2)3 , in which propose, suggest and possible are identified as hedge cues and their scope is marked in agreement with the gold standard. We"
W11-0141,P09-2044,0,0.0140353,"hold in the world. Negation is a linguistic resource used to express negative polarity. Although the treatment of these topics in computational linguistics is relatively new compared to other areas like machine translation, parsing or semantic role labeling, incorporating information about modality and polarity has been shown to be useful for a number of applications, such as biomedical text processing (Di Marco and Mercer, 2005; Chapman et al., 2001), opinion mining and sentiment analysis (Wilson et al., 2005), recognizing textual entailment (Snow et al., 2006), and automatic style checking (Ganter and Strube, 2009). In general, the treatment of modality and negation is very relevant for computational applications that process factuality (Saur´ı, 2008). For example, information extraction systems may be confronted with fragments of texts like the one presented in (1)1 , which contains two negation cues2 (not, un-) and one speculation cue (likely) that affect the factuality of the events being expressed: The atovaquone/proguanil combination has not been widely used yet in West Africa so it is unlikely that the patient was initially infected with an atovaquone-resistant strain. (1) So far two main tasks ha"
W11-0141,P10-1041,0,0.0329917,"on 3), and we point out aspects of the scope finding task that would be different based on observations from a corpus from a different domain (Section 4). 1 2 Example to be found in http://www.biomedcentral.com/content/pdf/1475-2875-1-1.pdf [last consulted 8-10-2010] A cue is the lexical marker that expresses negation or modality. 350 2 Achievements in scope processing In the last years, several corpora have been annotated with information related to modality and polarity, which have made it possible to develop machine learning systems. Annotation has been performed at different levels: word (Hassan and Radev, 2010), expression (Baker et al., 2010; Toprak et al., 2010), sentence (Medlock and Briscoe, 2007), event (Saur´ı and Pustejovsky, 2009), discourse relation (Prasad et al., 2006), text (Amancio et al., 2010), and scope of negation and modality cues (Vincze et al., 2008). Thanks to the existence of the BioScope corpus, the scope processing task was recently introduced. BioScope is a freely available resource, that consists of three parts of medical and biological texts annotated with negation and hedge cues and their scope. The scope processing task is concerned with determining at a sentence level w"
W11-0141,D10-1070,0,0.0114771,"to the existence of the BioScope corpus, the scope processing task was recently introduced. BioScope is a freely available resource, that consists of three parts of medical and biological texts annotated with negation and hedge cues and their scope. The scope processing task is concerned with determining at a sentence level which tokens are affected by modality and negation cues. It was first modelled as a classification problem by Morante et al. (2008). ¨ ur Later on several systems have been trained on the same corpus (Morante and Daelemans, 2009; Ozg¨ and Radev, 2009; Agarwal and Yu, 2010; Li et al., 2010). Councill et al. (2010) process scopes of negation cues in a different corpus of product reviews, but this corpus is not publicly available. The CoNLL Shared Task 2010 on Learning to detect hedges and their scope in natural language text (Farkas et al., 2010) boosted research on the topic. It consisted of identifying sentences containing uncertainty and recognizing speculative text spans inside sentences. Participating systems would, for example, produce the tagged sentence in (2)3 , in which propose, suggest and possible are identified as hedge cues and their scope is marked in agreement wit"
W11-0141,P07-1125,0,0.110652,"n observations from a corpus from a different domain (Section 4). 1 2 Example to be found in http://www.biomedcentral.com/content/pdf/1475-2875-1-1.pdf [last consulted 8-10-2010] A cue is the lexical marker that expresses negation or modality. 350 2 Achievements in scope processing In the last years, several corpora have been annotated with information related to modality and polarity, which have made it possible to develop machine learning systems. Annotation has been performed at different levels: word (Hassan and Radev, 2010), expression (Baker et al., 2010; Toprak et al., 2010), sentence (Medlock and Briscoe, 2007), event (Saur´ı and Pustejovsky, 2009), discourse relation (Prasad et al., 2006), text (Amancio et al., 2010), and scope of negation and modality cues (Vincze et al., 2008). Thanks to the existence of the BioScope corpus, the scope processing task was recently introduced. BioScope is a freely available resource, that consists of three parts of medical and biological texts annotated with negation and hedge cues and their scope. The scope processing task is concerned with determining at a sentence level which tokens are affected by modality and negation cues. It was first modelled as a classific"
W11-0141,W09-1304,1,0.845103,"), and scope of negation and modality cues (Vincze et al., 2008). Thanks to the existence of the BioScope corpus, the scope processing task was recently introduced. BioScope is a freely available resource, that consists of three parts of medical and biological texts annotated with negation and hedge cues and their scope. The scope processing task is concerned with determining at a sentence level which tokens are affected by modality and negation cues. It was first modelled as a classification problem by Morante et al. (2008). ¨ ur Later on several systems have been trained on the same corpus (Morante and Daelemans, 2009; Ozg¨ and Radev, 2009; Agarwal and Yu, 2010; Li et al., 2010). Councill et al. (2010) process scopes of negation cues in a different corpus of product reviews, but this corpus is not publicly available. The CoNLL Shared Task 2010 on Learning to detect hedges and their scope in natural language text (Farkas et al., 2010) boosted research on the topic. It consisted of identifying sentences containing uncertainty and recognizing speculative text spans inside sentences. Participating systems would, for example, produce the tagged sentence in (2)3 , in which propose, suggest and possible are ident"
W11-0141,D08-1075,1,0.870073,"nd Pustejovsky, 2009), discourse relation (Prasad et al., 2006), text (Amancio et al., 2010), and scope of negation and modality cues (Vincze et al., 2008). Thanks to the existence of the BioScope corpus, the scope processing task was recently introduced. BioScope is a freely available resource, that consists of three parts of medical and biological texts annotated with negation and hedge cues and their scope. The scope processing task is concerned with determining at a sentence level which tokens are affected by modality and negation cues. It was first modelled as a classification problem by Morante et al. (2008). ¨ ur Later on several systems have been trained on the same corpus (Morante and Daelemans, 2009; Ozg¨ and Radev, 2009; Agarwal and Yu, 2010; Li et al., 2010). Councill et al. (2010) process scopes of negation cues in a different corpus of product reviews, but this corpus is not publicly available. The CoNLL Shared Task 2010 on Learning to detect hedges and their scope in natural language text (Farkas et al., 2010) boosted research on the topic. It consisted of identifying sentences containing uncertainty and recognizing speculative text spans inside sentences. Participating systems would, fo"
W11-0141,W10-3006,1,0.87553,"Missing"
W11-0141,D09-1145,0,0.0282011,"Missing"
W11-0141,W06-0305,0,0.0308548,"und in http://www.biomedcentral.com/content/pdf/1475-2875-1-1.pdf [last consulted 8-10-2010] A cue is the lexical marker that expresses negation or modality. 350 2 Achievements in scope processing In the last years, several corpora have been annotated with information related to modality and polarity, which have made it possible to develop machine learning systems. Annotation has been performed at different levels: word (Hassan and Radev, 2010), expression (Baker et al., 2010; Toprak et al., 2010), sentence (Medlock and Briscoe, 2007), event (Saur´ı and Pustejovsky, 2009), discourse relation (Prasad et al., 2006), text (Amancio et al., 2010), and scope of negation and modality cues (Vincze et al., 2008). Thanks to the existence of the BioScope corpus, the scope processing task was recently introduced. BioScope is a freely available resource, that consists of three parts of medical and biological texts annotated with negation and hedge cues and their scope. The scope processing task is concerned with determining at a sentence level which tokens are affected by modality and negation cues. It was first modelled as a classification problem by Morante et al. (2008). ¨ ur Later on several systems have been"
W11-0141,S10-1008,1,0.784109,"is not the case that tRNASec and tRNAPyl have usual secondary structures 515 4 Annotating scopes in a different domain The existing scope labelers have been trained on biomedical texts. However, it is reasonable to expect that texts from other domains contain different phenomena that would affect the systems performance. We are currently analysing negations and their scopes in a complete different corpus, The Hound of the Baskervilles (HB) by Conan Doyle. This corpus has been annotated with coreference and semantic roles for the SemEval Task Linking Events and Their Participants in Discourse (Ruppenhofer et al., 2010), and will be further annotated with negation and modality cues. Phenomena in this corpus show that whereas the scope of cues can be determined in a similar way as it is determined in biomedical texts, identifying 352 negation cues in certain contexts, which is the first part of the scope finding task, is not only a matter of lexical lookup: − Not all negative affixes are negation cues. For example the affix un- in unspoken does not negate its root morpheme. Unspoken does not mean ‘not spoken’, but ‘understood without the need for words’. Consequently, in (16) unspoken is not a negation cue. ("
W11-0141,N06-1005,0,0.0149348,"nformation as a counterfact, a fact that does not hold in the world. Negation is a linguistic resource used to express negative polarity. Although the treatment of these topics in computational linguistics is relatively new compared to other areas like machine translation, parsing or semantic role labeling, incorporating information about modality and polarity has been shown to be useful for a number of applications, such as biomedical text processing (Di Marco and Mercer, 2005; Chapman et al., 2001), opinion mining and sentiment analysis (Wilson et al., 2005), recognizing textual entailment (Snow et al., 2006), and automatic style checking (Ganter and Strube, 2009). In general, the treatment of modality and negation is very relevant for computational applications that process factuality (Saur´ı, 2008). For example, information extraction systems may be confronted with fragments of texts like the one presented in (1)1 , which contains two negation cues2 (not, un-) and one speculation cue (likely) that affect the factuality of the events being expressed: The atovaquone/proguanil combination has not been widely used yet in West Africa so it is unlikely that the patient was initially infected with an a"
W11-0141,P10-1059,0,0.0318531,"that would be different based on observations from a corpus from a different domain (Section 4). 1 2 Example to be found in http://www.biomedcentral.com/content/pdf/1475-2875-1-1.pdf [last consulted 8-10-2010] A cue is the lexical marker that expresses negation or modality. 350 2 Achievements in scope processing In the last years, several corpora have been annotated with information related to modality and polarity, which have made it possible to develop machine learning systems. Annotation has been performed at different levels: word (Hassan and Radev, 2010), expression (Baker et al., 2010; Toprak et al., 2010), sentence (Medlock and Briscoe, 2007), event (Saur´ı and Pustejovsky, 2009), discourse relation (Prasad et al., 2006), text (Amancio et al., 2010), and scope of negation and modality cues (Vincze et al., 2008). Thanks to the existence of the BioScope corpus, the scope processing task was recently introduced. BioScope is a freely available resource, that consists of three parts of medical and biological texts annotated with negation and hedge cues and their scope. The scope processing task is concerned with determining at a sentence level which tokens are affected by modality and negation cues"
W11-0141,W10-3105,0,0.165055,"Missing"
W11-0141,W08-0606,0,0.123564,"Missing"
W11-0141,H05-2018,0,0.0316481,"the world, whereas negative polarity is used to put information as a counterfact, a fact that does not hold in the world. Negation is a linguistic resource used to express negative polarity. Although the treatment of these topics in computational linguistics is relatively new compared to other areas like machine translation, parsing or semantic role labeling, incorporating information about modality and polarity has been shown to be useful for a number of applications, such as biomedical text processing (Di Marco and Mercer, 2005; Chapman et al., 2001), opinion mining and sentiment analysis (Wilson et al., 2005), recognizing textual entailment (Snow et al., 2006), and automatic style checking (Ganter and Strube, 2009). In general, the treatment of modality and negation is very relevant for computational applications that process factuality (Saur´ı, 2008). For example, information extraction systems may be confronted with fragments of texts like the one presented in (1)1 , which contains two negation cues2 (not, un-) and one speculation cue (likely) that affect the factuality of the events being expressed: The atovaquone/proguanil combination has not been widely used yet in West Africa so it is unlike"
W11-0141,W10-2313,0,\N,Missing
W11-1713,W10-0203,0,0.025115,"section 8 gives some pointers for future research. 2 Related Work The techniques that have been used for emotion classification can roughly be divided into pattern-based methods and machine-learning methods. An often104 Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 104–110, c 24 June, 2011, Portland, Oregon, USA 2011 Association for Computational Linguistics used technique in pattern-based approaches is to use pre-defined lists of keywords which help determine an instance’s overall emotion contents. The AESOP system by Goyal et al. (2010), for instance, attempts to analyze the affective state of characters in fables by identifying affective verbs and by using a set of projection rules to calculate the verbs’ influence on their patients. Another possible approach –which we subscribe to– is to let a machine learner determine the appropriate emotion class. Mishne (2005) and Keshtkar and Inkpen (2009), for instance, attempt to classify LiveJournal posts according to their mood using Support Vector Machines trained with frequency features, length-related features, semantic orientation features and features representing special symb"
W11-1713,rentoumi-etal-2010-united,0,0.0140788,", attempts to analyze the affective state of characters in fables by identifying affective verbs and by using a set of projection rules to calculate the verbs’ influence on their patients. Another possible approach –which we subscribe to– is to let a machine learner determine the appropriate emotion class. Mishne (2005) and Keshtkar and Inkpen (2009), for instance, attempt to classify LiveJournal posts according to their mood using Support Vector Machines trained with frequency features, length-related features, semantic orientation features and features representing special symbols. Finally, Rentoumi et al. (2010) posit that combining the rule-based and machine learning approaches can have a positive effect on classification performance. By classifying strongly figurative examples using Hidden Markov Models while relying on a rule-based system to classify the mildly figurative ones, the overall performance of the classification system is improved. Whereas emotion classification in general is a relatively active domain in the field of computational linguistics, little research has been done regarding the automatic classification of text according to frameworks for interpersonal communication. We have pr"
W11-1713,L10-1000,0,\N,Missing
W12-3506,W95-0107,0,0.0228756,"n, 2005), particularly on the basis of limited training data. Encouraging results have been reported in the unsupervised induction of sequence tags (Collobert et al., 2011). In the context of the ALADIN project, we therefore decided to adopt a concept tagging approach as a shallow grammar interface between utterance and meaning. In this vein, each command is segmented into chunks of words, which are tagged with the semantic concepts (i.e. frame slots) to which they refer. We use a tagging framework which is based on so-called IOB tagging, commonly used in the context of phrase chunking tasks (Ramshaw and Marcus, 1995). Words inside a chunk are labeled with a tag starting with I and words outside the chunks are labeled with an O tag, which means that they do not refer to any concept in the action frame. Fig. 3 illustrates the concept tagging approach for an example command. 3 Experimental setup The experiments described in this paper pertain to a vocal interface for the card game Patience. This presents an appropriate case study, since a C&C interface for this game needs to learn a non-trivial, but fairly restrictive vocabulary and grammar. Commands such as “put the four of clubs on the five of hearts” or “"
W13-3913,W10-1310,0,0.0186777,"ability for whom operating and controlling devices would require exhaustive physical effort [1]. Unfortunately, even state-of-the-art speech recognition systems offer little, if any, robustness to dialectic or dysarthric speech (often encountered with disabled users), and are often restricted in their vocabulary and grammar. In practice, it is not feasible to design speech interfaces featuring custom acoustic and language models that cater to the dialectic and/or pathological speech of individual users, and adaptation of existing acoustic models is limited to only very mild speech pathologies [2, 3, 4, 5, 6]. Moreover, the user’s voice may change over time due to progressive speech impairments. Our aim is to build a VUI that is trained by the end-user himself, which means that it is maximally adapted to the — possibly dysarthric — speech of the user, and can be used with any vocabulary and grammar. The challenge is to learn both acoustics and grammar from a small number of examples, with as only supervisory information coarse annotation in the form of associated actions. For example, the annotation of the command “Turn on the television please”, accompanied by a button press, would only be annota"
W13-3913,W11-2302,0,0.0205236,"ability for whom operating and controlling devices would require exhaustive physical effort [1]. Unfortunately, even state-of-the-art speech recognition systems offer little, if any, robustness to dialectic or dysarthric speech (often encountered with disabled users), and are often restricted in their vocabulary and grammar. In practice, it is not feasible to design speech interfaces featuring custom acoustic and language models that cater to the dialectic and/or pathological speech of individual users, and adaptation of existing acoustic models is limited to only very mild speech pathologies [2, 3, 4, 5, 6]. Moreover, the user’s voice may change over time due to progressive speech impairments. Our aim is to build a VUI that is trained by the end-user himself, which means that it is maximally adapted to the — possibly dysarthric — speech of the user, and can be used with any vocabulary and grammar. The challenge is to learn both acoustics and grammar from a small number of examples, with as only supervisory information coarse annotation in the form of associated actions. For example, the annotation of the command “Turn on the television please”, accompanied by a button press, would only be annota"
W13-3913,W12-3506,1,0.34012,"position. By employing the NMF factorization Eq. 3, which is called the local NMF, the corresponding slot value activations are calculated. Hs = arg min DKL (Vs ||Wa Hs ) Hs NMF uses non-negativity constraints for decomposing a matrix into its components [20, 21, 22, 23], i.e given a non-negative matrix V of size [M xN ], NMF approximately decomposes it into its non-negative components W of size [M xR] and H of size [RxN ]. Under the right conditions, NMF is able to find parts in data. In ASR, NMF is used to discover recurring acoustic patterns (word units) through some grounding information [24, 25, 26]. (3) This is followed by the calculation of the activation matrix As . Each column of the activation matrix contains labeling information of all slot values for a particular window position. As = Wg Hs (4) In the simplest form of decoding, called NMF decoding, the slot values are inferred directly from the local (sliding window) NMF. The activations for all slot values are accumulated over all window positions, i.e. over the complete utterance. Since each slot can have at most one value assigned, only the value hypothesis with the largest accumulated activation is kept per slot. The slot valu"
W14-5703,P98-1015,0,0.243818,"Missing"
W14-5703,W09-3707,0,0.0241596,"ikaans and Dutch respectively. We assume that further improvements are possible with alternative systems and parameter optimization. 23 4 Compound Semantics The automatic processing of the semantics of compounds (or other complex nominals) is a topic in computational linguistics that, although it has been studied regularly in the past, cannot be considered a solved problem. Although previous research was often promising, it also had an almost exclusive focus on English noun-noun (NN) compounds. In recent years, more languages have been studied (e.g. German (Hinrichs et al., 2013) and Italian (Celli and Nissim, 2009)), and this project added Dutch and Afrikaans to the list. It is worth noting that a number of different operationalizations of compound interpretation have been studied. The most notable are semantic classification of the constituent relation according to a limited ´ S´eaghdha (2008)), and the generation of possible paraphrases for set of semantic categories (e.g. O the compound that express its meaning more explicitly (Hendrickx et al., 2013). Our study adopts the classification model, in which the set of semantic relations to be predicted (the classification scheme) is crucial. 4.1 Related"
W14-5703,W04-0108,1,0.828694,"Missing"
W14-5703,S13-2025,0,0.0296942,"Missing"
W14-5703,I05-1082,0,0.310952,"Missing"
W14-5703,N04-1016,0,0.0562201,"Missing"
W14-5703,N13-1090,0,0.0115995,"Missing"
W14-5703,P07-3013,0,0.0495382,"Missing"
W14-5703,N09-2060,0,0.201624,"Missing"
W14-5703,P10-1070,0,0.0336104,"Missing"
W14-5703,W14-5704,1,0.82588,"Missing"
W14-5703,W13-0506,1,0.857481,"Missing"
W14-5703,W04-2609,0,\N,Missing
W14-5703,W07-1108,0,\N,Missing
W14-5703,C98-1015,0,\N,Missing
W15-2405,D14-1162,0,0.0822961,"coming elements. Perhaps the most explicit account of such a process can be found in the work of Chang et al. (2006), who use a recurrent neural network in combination with an event semantics, in order to generate sentences with unseen bindings between words and semantic roles – i.e., the types of novel sentential constructions that could be afforded by abstract syntactic structure. It is noteworthy, given this line of work, that prediction is central to recently popular methods from Natural Language Processing (NLP) for obtaining distributional representations of words (Mikolov et al., 2013; Pennington et al., 2014). Vector representations (often called word embeddings) obtained using these methods cluster closely in terms of semantic and syntactic types – an achievement due to engineering efforts, without emphasis on psychological constraints. Thus, if these methods are to be used for modelling aspects of human language processing, they should be modified to reflect such constraints. Here, we attempt to take a first step into this direction: we modify the skipgram model from the word2vec family of models (Mikolov et al., 2013) – which predicts both the left and right context of a word – to predict only"
W15-2405,C00-2137,0,0.00911847,"8 0.799 context + phonology mi. F1 prec. rec. ma. F1 mi. F1 0.821 72 67 58 88 94 77 4 52 0.566 0.734 0.840 81 72 70 89 94 86 30 66 0.707 0.804 0.858 79 82 75 90 97 83 32 66 0.725 0.812 Table 1: Precision and recall (in percent), together with micro and macro F1 scores, based on a 10-NN classifier trained on the word embeddings at different stages during the training process. ter than performance at the previous stage. Significance of differences is computed via approximate randomization testing (Noreen, 1989),4 a statistical test suitable for comparing evaluation metrics such as F-scores (cf. Yeh, 2000). Results are based on a dAE with 400 hidden units, trained with a learning rate of 0.01, and a corruption level of 0.1. The softmax model was trained with a learning rate of 0.008, with context words sampled from a sentence-internal window of t = 3 words to the right. Both models were optimized via true stochastic gradient descent. 5 conditions, although there is no such significant difference in the context + phonology condition (but p ≈ 0.07 for the difference between micro F1 scores). There is no significant difference between the context and context + stress conditions at any stage, where"
W15-2405,Q15-1016,0,\N,Missing
W15-2406,J11-2002,0,0.0709681,"Missing"
W15-2406,W10-2922,0,0.0219856,"the most? Unsupervised contexts selection for lexical category acquisition Giovanni Cassani Robert Grimm Walter Daelemans University of Antwerp, CLiPS {name.surname}@uantwerpen.be Abstract Data (PLD), only using general-purpose learning mechanisms. Thus, we look at language acquisition from an emergentist perspective, exploring the fruitfulness of the distributional bootstrapping hypothesis. Starting with Cartwright and Brent (1997), a variety of models for Parts-of-Speech (PoS) induction has been proposed (Clark, 2000; Mintz et al., 2002; Mintz, 2003; Parisien et al., 2008; Leibbrandt, 2009; Chrupała and Alishahi, 2010; St. Clair et al., 2010), showing that PLD are rich enough in distributional cues to provide the child with enough information to group words according to their syntactic category. Among such models, two major approaches can be identified: i) a frame-based one which starts by selecting the relevant cues and then evaluate how these help categorization, and ii) a probabilistic approach that considers all possible contexts in a left and right window whose size is set in advance, and determines the best category for each word based on a probabilistic match between the context of each new word and"
W15-2406,W00-0717,0,0.058968,"Missing"
W15-2406,W08-2112,0,0.045344,"Missing"
W16-2910,D14-1110,0,0.0776231,"Missing"
W16-2910,P08-3009,0,0.00808467,"g a BoW representation of all words that denote that semantic type. For each ambiguous term, a target word vector is created by taking a window of words from the right and left of the term. The concept which is associated with the ST with the lowest cosine distance is then taken to be the correct sense of the term. Similarly, Alexopoulou et al. (2009) create a method which finds the closest concept based on a combination of co-occurrence with other semantic types and ontological similarity through is-a relationships. 3 Closest to our approach is the machine readable dictionary (MRD) approach (McInnes, 2008; Jimeno-Yepes et al., 2011), which uses definitions from the UMLS to create concept vectors by creating BoW representations of concepts using all definitions of the concept and those of related concepts. This BoW representation contains TF-IDF values where D is the number of concepts in which a word appears, thereby reducing the influence of general words which occur in many concepts. These representations are then compared to the vectorized contexts of the ambiguous terms using cosine distance. A refinement of MRD, called second-order co-occurrence MRD (2-MRD) (McInnes, 2008), replaces each"
W16-2910,P08-1028,0,0.033803,". For each concept, all definition vectors D are then composed into a concept vector C using a second composition function f (x). placed is also comparable. Hence, for each ambiguous word we encounter, we can use the cosine distance between the abstract vector of the ambiguous utterance and each possible sense of that word to determine the correct sense. This makes our approach very similar to the Lesk family of approaches (Lesk, 1986). In terms of composition function we experimented with elementwise multiplication, averaging and summation, all of which are unordered compositional functions (Mitchell and Lapata, 2008). In addition, it is worth noting that there’s still a lively debate whether ordered composition actually leads to better results for estimating document-, or sentence-level meaning, when compared to unordered composition (Iyyer et al., 2015; Socher et al., 2013). 5 Results The accuracy scores obtained by our models using the different word vectors are displayed in Table 2. med, mim and bio denote the vectors created on the small Medline corpus, the Mimic-III corpus and the BioASQ vectors, respectively. We consider both a constrained and an unconstrained version of the task. For each word, the"
W16-2910,J98-1001,0,0.0857422,"Missing"
W16-2910,D13-1170,0,0.00366744,"mbiguous utterance and each possible sense of that word to determine the correct sense. This makes our approach very similar to the Lesk family of approaches (Lesk, 1986). In terms of composition function we experimented with elementwise multiplication, averaging and summation, all of which are unordered compositional functions (Mitchell and Lapata, 2008). In addition, it is worth noting that there’s still a lively debate whether ordered composition actually leads to better results for estimating document-, or sentence-level meaning, when compared to unordered composition (Iyyer et al., 2015; Socher et al., 2013). 5 Results The accuracy scores obtained by our models using the different word vectors are displayed in Table 2. med, mim and bio denote the vectors created on the small Medline corpus, the Mimic-III corpus and the BioASQ vectors, respectively. We consider both a constrained and an unconstrained version of the task. For each word, the constrained version of the task only considers the senses present 2 Available on the BioASQ website. While we concede that the BioASQ corpora might contain abstracts from the MSH dataset, it does not contain any explicit labeled information that might be used in"
W16-2910,N04-3012,0,\N,Missing
W16-2910,P15-1162,0,\N,Missing
W17-1610,Q14-1012,0,0.0740981,"Missing"
W17-1610,P16-2096,0,\N,Missing
W17-2317,E17-2027,0,0.0299832,"to different corrections in different contexts, e.g. iron deficiency due to enemia → anemia vs. fluid injected with enemia → enema. A noisy channel model like the one by Lai et al. will choose the same item for both corrections. Our proposed method exploits contextual clues by using neural embeddings to rank misspelling replacement candidates according to their semantic fit in the misspelling context. Neural embeddings have recently proven useful for a variety of related tasks, such as unsupervised normalization (Sridhar, 2015) and reducing the candidate search space for spelling correction (Pande, 2017). We hypothesize that, by using neural embeddings, our method can counter the frequency bias of a noisy channel model. We test our system on manually annotated misspellings from the MIMIC-III (Johnson et al., 2016) clinical notes. In this paper, we focus on already detected non-word misspellings, i.e. where the misspellings are not real words, following Lai et al. We present an unsupervised contextsensitive spelling correction method for clinical free-text that uses word and character n-gram embeddings. Our method generates misspelling replacement candidates and ranks them according to their s"
W17-2317,W15-1502,0,0.0236566,"at ignoring contextual clues harms performance where a specific misspelling maps to different corrections in different contexts, e.g. iron deficiency due to enemia → anemia vs. fluid injected with enemia → enema. A noisy channel model like the one by Lai et al. will choose the same item for both corrections. Our proposed method exploits contextual clues by using neural embeddings to rank misspelling replacement candidates according to their semantic fit in the misspelling context. Neural embeddings have recently proven useful for a variety of related tasks, such as unsupervised normalization (Sridhar, 2015) and reducing the candidate search space for spelling correction (Pande, 2017). We hypothesize that, by using neural embeddings, our method can counter the frequency bias of a noisy channel model. We test our system on manually annotated misspellings from the MIMIC-III (Johnson et al., 2016) clinical notes. In this paper, we focus on already detected non-word misspellings, i.e. where the misspellings are not real words, following Lai et al. We present an unsupervised contextsensitive spelling correction method for clinical free-text that uses word and character n-gram embeddings. Our method ge"
W17-2317,Q14-1012,0,0.0746518,"Missing"
W17-2317,W15-3822,0,\N,Missing
W17-4407,P14-1016,0,0.0219217,"vely influence several text classification tasks (Hovy, 2015). Gender profiling research on Twitter generally takes a data-driven, open-vocabulary approach using bag of words, or bag of n-gram features (Alowibdi et al., 2013; Ciot et al., 2013; Verhoeven et al., 2016), applying supervised classification using manually annotated profiles. However, distant supervision has as of yet only looked at non-textual cues for this task, unlike for example age, personality, and mental health (e.g. Al Zamal et al., 2012; Plank and Hovy, 2015; Coppersmith et al., 2015). For gender, Burger et al. (2011) and Li et al. (2014) collect links to external profiles, whereas Al Zamal et al. (2012) and Li et al. (2015) use a list with gender-associated names. Both of these approaches rely on continuous monitoring of streaming data, and utilize indicators that are typically easy cues for annotators, thereby omitting profiles that would be costly to annotate. In contrast, our method only has to be repeated once a week, and includes a different set of users where sampling is not influenced by external resources. 3 N hand 1,456 1,109 1,059 1,091 1,045 F .806 .873 .882 .887 .885 F+R .806 .887 .896 .891 .900 Table 1: Several f"
W17-4407,D11-1120,0,0.296945,"iling has shown to positively influence several text classification tasks (Hovy, 2015). Gender profiling research on Twitter generally takes a data-driven, open-vocabulary approach using bag of words, or bag of n-gram features (Alowibdi et al., 2013; Ciot et al., 2013; Verhoeven et al., 2016), applying supervised classification using manually annotated profiles. However, distant supervision has as of yet only looked at non-textual cues for this task, unlike for example age, personality, and mental health (e.g. Al Zamal et al., 2012; Plank and Hovy, 2015; Coppersmith et al., 2015). For gender, Burger et al. (2011) and Li et al. (2014) collect links to external profiles, whereas Al Zamal et al. (2012) and Li et al. (2015) use a list with gender-associated names. Both of these approaches rely on continuous monitoring of streaming data, and utilize indicators that are typically easy cues for annotators, thereby omitting profiles that would be costly to annotate. In contrast, our method only has to be repeated once a week, and includes a different set of users where sampling is not influenced by external resources. 3 N hand 1,456 1,109 1,059 1,091 1,045 F .806 .873 .882 .887 .885 F+R .806 .887 .896 .891 .9"
W17-4407,W11-1515,0,0.0413131,"Missing"
W17-4407,D13-1114,0,0.0215634,"to social media — encompassing a wide variety of attributes such as gender, age, personality, location, education, income, religion, and political polarity (Eisenstein et al., 2011; Alowibdi et al., 2013; Volkova et al., 2014; Plank and Hovy, 2015; Volkova and Bachrach, 2016). Apart from relevancy in marketing, security and forensics, author profiling has shown to positively influence several text classification tasks (Hovy, 2015). Gender profiling research on Twitter generally takes a data-driven, open-vocabulary approach using bag of words, or bag of n-gram features (Alowibdi et al., 2013; Ciot et al., 2013; Verhoeven et al., 2016), applying supervised classification using manually annotated profiles. However, distant supervision has as of yet only looked at non-textual cues for this task, unlike for example age, personality, and mental health (e.g. Al Zamal et al., 2012; Plank and Hovy, 2015; Coppersmith et al., 2015). For gender, Burger et al. (2011) and Li et al. (2014) collect links to external profiles, whereas Al Zamal et al. (2012) and Li et al. (2015) use a list with gender-associated names. Both of these approaches rely on continuous monitoring of streaming data, and utilize indicators"
W17-4407,W15-1201,0,0.0177231,"ng, security and forensics, author profiling has shown to positively influence several text classification tasks (Hovy, 2015). Gender profiling research on Twitter generally takes a data-driven, open-vocabulary approach using bag of words, or bag of n-gram features (Alowibdi et al., 2013; Ciot et al., 2013; Verhoeven et al., 2016), applying supervised classification using manually annotated profiles. However, distant supervision has as of yet only looked at non-textual cues for this task, unlike for example age, personality, and mental health (e.g. Al Zamal et al., 2012; Plank and Hovy, 2015; Coppersmith et al., 2015). For gender, Burger et al. (2011) and Li et al. (2014) collect links to external profiles, whereas Al Zamal et al. (2012) and Li et al. (2015) use a list with gender-associated names. Both of these approaches rely on continuous monitoring of streaming data, and utilize indicators that are typically easy cues for annotators, thereby omitting profiles that would be costly to annotate. In contrast, our method only has to be repeated once a week, and includes a different set of users where sampling is not influenced by external resources. 3 N hand 1,456 1,109 1,059 1,091 1,045 F .806 .873 .882 .8"
W17-4407,W15-2913,0,0.64479,""" + : Related Work Author profiling applies machine learning to linguistic features within a piece of writing to make inferences regarding its author. The ability to make such inferences was first discussed for gender by Koppel et al. (2002), and initially applied to blogs (Argamon et al., 2007; Rosenthal and McKeown, 2011; Nguyen et al., 2011). Later, the work extended to social media — encompassing a wide variety of attributes such as gender, age, personality, location, education, income, religion, and political polarity (Eisenstein et al., 2011; Alowibdi et al., 2013; Volkova et al., 2014; Plank and Hovy, 2015; Volkova and Bachrach, 2016). Apart from relevancy in marketing, security and forensics, author profiling has shown to positively influence several text classification tasks (Hovy, 2015). Gender profiling research on Twitter generally takes a data-driven, open-vocabulary approach using bag of words, or bag of n-gram features (Alowibdi et al., 2013; Ciot et al., 2013; Verhoeven et al., 2016), applying supervised classification using manually annotated profiles. However, distant supervision has as of yet only looked at non-textual cues for this task, unlike for example age, personality, and men"
W17-4407,P11-1137,0,0.160197,"ion for Computational Linguistics 2 filter none rt rt + "" rt + : rt + "" + : Related Work Author profiling applies machine learning to linguistic features within a piece of writing to make inferences regarding its author. The ability to make such inferences was first discussed for gender by Koppel et al. (2002), and initially applied to blogs (Argamon et al., 2007; Rosenthal and McKeown, 2011; Nguyen et al., 2011). Later, the work extended to social media — encompassing a wide variety of attributes such as gender, age, personality, location, education, income, religion, and political polarity (Eisenstein et al., 2011; Alowibdi et al., 2013; Volkova et al., 2014; Plank and Hovy, 2015; Volkova and Bachrach, 2016). Apart from relevancy in marketing, security and forensics, author profiling has shown to positively influence several text classification tasks (Hovy, 2015). Gender profiling research on Twitter generally takes a data-driven, open-vocabulary approach using bag of words, or bag of n-gram features (Alowibdi et al., 2013; Ciot et al., 2013; Verhoeven et al., 2016), applying supervised classification using manually annotated profiles. However, distant supervision has as of yet only looked at non-textu"
W17-4407,P16-1080,0,0.207354,"ffective at this task using predictive models trained on manual annotations, the process of hand-labelling profiles is costly. Even for the ostensibly straight-forward task of annotating gender, a large portion of Twitter users purposefully avoids providing simple indicators such as real names or profile photos including a face. Consequently, this forces annotators to either dive deep into the user’s timeline in search for linguistic cues, or to make decisions based on some personal interpretation, for which they have shown to often incorrectly apply stereotypical biases (Nguyen et al., 2014; Flekova et al., 2016). We show that running a small collection of adhoc queries for self-reports of gender once (“I’m a male, female, man, woman” etc.) — provides distant labels for 6,610 profiles with high confidence in one week worth of data. Employing these for distant supervision, we demonstrate them to be an accurate signal for gender classification, and form a reliable, cheap method that has competitive performance with models trained on costly humanlabelled profiles. Our contributions are as follows: The majority of research on extracting missing user attributes from social media profiles use costly hand-an"
W17-4407,P11-1077,0,0.0467139,"reproduce the experiments is made available open-source at https://github.com/ cmry/simple-queries. 50 Proceedings of the 3rd Workshop on Noisy User-generated Text, pages 50–55 c Copenhagen, Denmark, September 7, 2017. 2017 Association for Computational Linguistics 2 filter none rt rt + "" rt + : rt + "" + : Related Work Author profiling applies machine learning to linguistic features within a piece of writing to make inferences regarding its author. The ability to make such inferences was first discussed for gender by Koppel et al. (2002), and initially applied to blogs (Argamon et al., 2007; Rosenthal and McKeown, 2011; Nguyen et al., 2011). Later, the work extended to social media — encompassing a wide variety of attributes such as gender, age, personality, location, education, income, religion, and political polarity (Eisenstein et al., 2011; Alowibdi et al., 2013; Volkova et al., 2014; Plank and Hovy, 2015; Volkova and Bachrach, 2016). Apart from relevancy in marketing, security and forensics, author profiling has shown to positively influence several text classification tasks (Hovy, 2015). Gender profiling research on Twitter generally takes a data-driven, open-vocabulary approach using bag of words, or"
W17-4407,D15-1162,0,0.0355146,"Missing"
W17-4407,D14-1121,0,0.422442,"ess than 200 tweets were excluded, as well as any consecutive tweets that would not exactly fit into a batch of 200. The corpora were divided between a (gender 4 https://github.com/Mimino666/ langdetect 5 https://github.com/facebookresearch/ fastText 52 Test Volkova Plank Query Average Majority .556 .659 .674 .630 Lexicon .796 .740 .668 .735 Volkova .822 (0.001) .741 (0.005) .730 (0.007) .764 Train Plank .701 (0.007) .723 (0.003) .689 (0.005) .704 Query .771 (0.007) .724 (0.009) .756 (0.002) .750 Table 4: Individual accuracy scores and averages for majority baseline (Majority), the lexicon of Sap et al. (2014), and the three models (trained on Volkova, Plank, and our dataset respectively) evaluated on the test set for each corpus. Standard deviation is reported after repeating the same experiment 20 times. for parallelising Stochastic Gradient Descent, randomness in the vector representations cannot be controlled using a seed. To estimate the standard deviation in the results, we ran each experiment 20 times. To evaluate how our distantly supervised model compares to using manual annotations, we trained all models in this same configuration for all three corpora. Each model was then evaluated on th"
W17-4407,P15-1073,0,0.325516,"ted Work Author profiling applies machine learning to linguistic features within a piece of writing to make inferences regarding its author. The ability to make such inferences was first discussed for gender by Koppel et al. (2002), and initially applied to blogs (Argamon et al., 2007; Rosenthal and McKeown, 2011; Nguyen et al., 2011). Later, the work extended to social media — encompassing a wide variety of attributes such as gender, age, personality, location, education, income, religion, and political polarity (Eisenstein et al., 2011; Alowibdi et al., 2013; Volkova et al., 2014; Plank and Hovy, 2015; Volkova and Bachrach, 2016). Apart from relevancy in marketing, security and forensics, author profiling has shown to positively influence several text classification tasks (Hovy, 2015). Gender profiling research on Twitter generally takes a data-driven, open-vocabulary approach using bag of words, or bag of n-gram features (Alowibdi et al., 2013; Ciot et al., 2013; Verhoeven et al., 2016), applying supervised classification using manually annotated profiles. However, distant supervision has as of yet only looked at non-textual cues for this task, unlike for example age, personality, and men"
W17-4407,L16-1258,1,0.842556,"encompassing a wide variety of attributes such as gender, age, personality, location, education, income, religion, and political polarity (Eisenstein et al., 2011; Alowibdi et al., 2013; Volkova et al., 2014; Plank and Hovy, 2015; Volkova and Bachrach, 2016). Apart from relevancy in marketing, security and forensics, author profiling has shown to positively influence several text classification tasks (Hovy, 2015). Gender profiling research on Twitter generally takes a data-driven, open-vocabulary approach using bag of words, or bag of n-gram features (Alowibdi et al., 2013; Ciot et al., 2013; Verhoeven et al., 2016), applying supervised classification using manually annotated profiles. However, distant supervision has as of yet only looked at non-textual cues for this task, unlike for example age, personality, and mental health (e.g. Al Zamal et al., 2012; Plank and Hovy, 2015; Coppersmith et al., 2015). For gender, Burger et al. (2011) and Li et al. (2014) collect links to external profiles, whereas Al Zamal et al. (2012) and Li et al. (2015) use a list with gender-associated names. Both of these approaches rely on continuous monitoring of streaming data, and utilize indicators that are typically easy c"
W17-4407,P14-1018,0,0.258116,"Missing"
W17-4914,W14-0908,1,0.700231,"cal variety over mere word quantity. A list of the 18 most prolific authors, their number of documents and the respective average length of these documents is given in Fig. 1. spans—and that it is therefore able to learn syntactic dependencies and structures from the training material. This is in stark contrast with a NGLM, which only reasons on the basis of a very local history and have little abstractive power. Importantly, however, it should be emphasized that most approaches to AA operate on very local features, such as lower-order character ngrams (Stamatatos, 2013; Sapkota et al., 2015; Kestemont, 2014). Most state-of-the-art models for AA indeed depend on document vectors containing normalized character ngram frequencies (typically in the range of 2-4), which are fed to a standard classifier, such as a support-vector machine with a linear kernel. The fact that the RNNLM might generate more realistic sentences than the NGLM does not necessarily entail that it would have an advantage in AA with respect to a conventional NGLM, which will stay closer to the original source documents. An important, if only secondary, question is therefore whether the use of an RNNLM in the context of AA would ou"
W17-4914,Q16-1037,0,0.0724215,"Missing"
W17-4914,W04-2405,0,0.0266058,"after every sentence. This asymmetry is motivated by the fact that NGLM the output distribution of an NGLM at each step is much more skewed and therefore sentences generated from the same seed tend to be be much less varied. For model fitting we set the NGLM order at 6, which, on a subjective evaluation, seemed a sufficiently large value for the comparatively small size of the datasets. For the RNNLM models the following parameter settings were selected. Embedding dimenIn addition, we conduct a final experiment which can be characterized from the point of view of self-learning or co-learning (Mihalcea, 2004)— a semi-supervised learning technique where a core of training data is expanded with examples from a related but unlabeled dataset that can be classified with high confidence by a classifier trained in the original labeled dataset. In this experiment we 120 sionality M was set to 24, the hidden layer dimension was 200 and we stacked up 2 LSTM layers to encourage the model to learn more abstract representations. Parameters were chosen based on common practice and reasonable defaults without further hyperparameter search. Each model was trained during 50 epochs using the adaptive variant of Sto"
W17-4914,D15-1221,0,0.0317205,"n Shakespeare’s oeuvre, was able to generate new, artificial text which, certainly in the eyes of non-experts, undeniably displayed some Shakespearean qualities. This blog has inspired a wide array of other applications – ranging from cooking recipes (Brewe, 2015) to Bach’s sonatas (Feynman et al., 2016). Much of this work has so far been published in the online blogosphere and the assessment of the quality of neurally generated text has often remained fairly informal and anecdotal, apart from a number of more empirically oriented studies, for instance in the field of hiphop lyric generation (Potash et al., 2015; Malmi et al., 2015). In this paper, we report an attempt at a systematic assessment of the properties of neurally generated text in the context of style-based authorship attribution in stylometry (Stamatatos, 2009). We address the following research questions: (1) To which extent is the text, neurally generated on the basis of a single author’s oeuvre, still attributable to the original input author? and (2) To which extent is the neural generation of text useful for training data augmentation in stylometry, e.g. for authors for whom little reference data is available? Below, we first presen"
W17-4914,N15-1010,0,0.0744676,"token counts, and lexical variety over mere word quantity. A list of the 18 most prolific authors, their number of documents and the respective average length of these documents is given in Fig. 1. spans—and that it is therefore able to learn syntactic dependencies and structures from the training material. This is in stark contrast with a NGLM, which only reasons on the basis of a very local history and have little abstractive power. Importantly, however, it should be emphasized that most approaches to AA operate on very local features, such as lower-order character ngrams (Stamatatos, 2013; Sapkota et al., 2015; Kestemont, 2014). Most state-of-the-art models for AA indeed depend on document vectors containing normalized character ngram frequencies (typically in the range of 2-4), which are fed to a standard classifier, such as a support-vector machine with a linear kernel. The fact that the RNNLM might generate more realistic sentences than the NGLM does not necessarily entail that it would have an advantage in AA with respect to a conventional NGLM, which will stay closer to the original source documents. An important, if only secondary, question is therefore whether the use of an RNNLM in the cont"
W18-3922,W17-1214,0,0.0685062,"ar Languages, Varieties and Dialects, pages 191–198 Santa Fe, New Mexico, USA, August 20, 2018. 2 Related work Eleven teams participated in the previous edition of the DSL shared task (Zampieri et al., 2017). The task involved determining the language or language variety for written news excerpts. It featured ten different languages with some having two or three varieties, making for a set of fourteen possible classes. For a multiclass classification problem with a random baseline at 10 percent, the performance of most systems was very good. The best system yielded a weighted F-score of .927 (Bestgen, 2017). There are only slight differences with regards to performance and overall approach in the topperforming teams. One trend is the use of word and character n-grams as the most discriminating features, which is also commonly the case for the task of Native Language Identification (NLI) (Tetreault et al., 2017). In choosing a classification algorithm, four out of the five top teams opted to use linear support vector machines in their setup, which also echoes techniques used in NLI. The system paper describing the best submission (Bestgen, 2017) again underlines this methodological similarity, ha"
W18-3922,L16-1284,0,0.0659663,"Missing"
W18-3922,W17-1222,0,0.0421665,"ies (Tetreault et al., 2017). The mentioned tasks on dialect identification are most closely related to the newly proposed DFS task, in that they deal with language variety, but the provided data in ADI and GDI is based on spoken data and provides some phonetic insight into these transcripts (either through separate acoustic Vectors or a rich phonetic transcript) whereas the DFS data does not. We can, however, mention some promising approaches in the previous tasks using ensembles or metaclassifiers with a large variety of word and character n-gram features that yield top performances in ADI (Malmasi and Zampieri, 2017a) (second place) and GDI (Malmasi and Zampieri, 2017b) (first place). Malmasi and Zampieri use combinations of base classifiers to approach both tasks. In the first ensemble method, each base classifier votes for its most probable label output and the label with the most votes serves as the system’s final output. The second ensemble method averages the probability outputs of all base classifiers and the class label with the highest averaged probability serves as the system’s final output. The third variant, and most accurate approach, uses a Random Forest algorithm to meta-classify the probab"
W18-3922,W17-1220,0,0.0419705,"ies (Tetreault et al., 2017). The mentioned tasks on dialect identification are most closely related to the newly proposed DFS task, in that they deal with language variety, but the provided data in ADI and GDI is based on spoken data and provides some phonetic insight into these transcripts (either through separate acoustic Vectors or a rich phonetic transcript) whereas the DFS data does not. We can, however, mention some promising approaches in the previous tasks using ensembles or metaclassifiers with a large variety of word and character n-gram features that yield top performances in ADI (Malmasi and Zampieri, 2017a) (second place) and GDI (Malmasi and Zampieri, 2017b) (first place). Malmasi and Zampieri use combinations of base classifiers to approach both tasks. In the first ensemble method, each base classifier votes for its most probable label output and the label with the most votes serves as the system’s final output. The second ensemble method averages the probability outputs of all base classifiers and the class label with the highest averaged probability serves as the system’s final output. The third variant, and most accurate approach, uses a Random Forest algorithm to meta-classify the probab"
W18-3922,W17-5000,0,0.538322,"featured ten different languages with some having two or three varieties, making for a set of fourteen possible classes. For a multiclass classification problem with a random baseline at 10 percent, the performance of most systems was very good. The best system yielded a weighted F-score of .927 (Bestgen, 2017). There are only slight differences with regards to performance and overall approach in the topperforming teams. One trend is the use of word and character n-grams as the most discriminating features, which is also commonly the case for the task of Native Language Identification (NLI) (Tetreault et al., 2017). In choosing a classification algorithm, four out of the five top teams opted to use linear support vector machines in their setup, which also echoes techniques used in NLI. The system paper describing the best submission (Bestgen, 2017) again underlines this methodological similarity, having drawn inspiration from entries in NLI shared tasks. Alongside the main DSL task there were two specific tasks of language variety identification in 2017, namely Arabic Dialect Identification (ADI) and German Dialect Identification (GDI). The goal in both tasks is to identify a native language dialect in"
W18-3922,W17-1224,0,0.0505948,"Missing"
W18-3922,W17-1201,0,0.129599,"Missing"
W18-3922,W18-3901,0,0.0609866,"Missing"
W18-5411,P18-1032,0,0.314574,"Missing"
W18-5411,N16-3020,0,0.11866,"have a macro-averaged F-score 0.80 when explaining the predictions of a feedforward neural network trained to classify a subset of documents from the 20 newsgroups dataset3 into those about either ‘Medicine’, ‘Space’, ‘Cryptography’, or ‘Electronics’. 2 uct of the gradient value and the input (Kindermans et al., 2016). Sometimes the importance of a feature is analyzed by setting its value to a reference value, and then backpropagating the difference (DeepLIFT) (Shrikumar et al., 2017). In another approach, a separate ‘explanation model’ is trained to fit the predictions of the original model (Ribeiro et al., 2016; Lundberg and Lee, 2017; Lakkaraju et al., 2017). In an information theoretic approach, the mutual information between feature subsets and the model output is approximated to identify the most important features, similar to feature selection techniques (Chen et al., 2018). For recurrent neural networks with an attention mechanism, attention weights are often used as feature importance scores (Hermann et al., 2015; Yang et al., 2016; Choi et al., 2016). Poerner et al. (2018) have investigated several of the previously discussed techniques and have found LRP and DeepLIFT to be the most effectiv"
W18-5411,N16-1174,0,0.305765,"the difference (DeepLIFT) (Shrikumar et al., 2017). In another approach, a separate ‘explanation model’ is trained to fit the predictions of the original model (Ribeiro et al., 2016; Lundberg and Lee, 2017; Lakkaraju et al., 2017). In an information theoretic approach, the mutual information between feature subsets and the model output is approximated to identify the most important features, similar to feature selection techniques (Chen et al., 2018). For recurrent neural networks with an attention mechanism, attention weights are often used as feature importance scores (Hermann et al., 2015; Yang et al., 2016; Choi et al., 2016). Poerner et al. (2018) have investigated several of the previously discussed techniques and have found LRP and DeepLIFT to be the most effective approaches for explaining deep neural networks in NLP. Related Work There has been a lot of recent interest in making machine learning models interpretable. Different approaches can be broadly grouped under two headings—1) the use of interpretable models, and 2) model-agnostic interpretability techniques. In the first case, the choice of machine learning methods is limited to the more interpretable models such as linear models and"
W18-5411,D16-1216,0,\N,Missing
W18-5411,N16-1082,0,\N,Missing
W18-5603,P17-1132,0,0.0316745,"ntence into segments, so the encoding and the pooling operations apply to one segment at a time. Each sentence consists of five segments: tokens preceding the first concept c1 ; c1 itself; tokens between c1 and c2 ; concept c2 ; and the tokens following it. This idea is related to dynamic pooling, known from previous event extraction work on the ACE 2005 dataset (Chen et al., 2015). More generally, the extension of neural networks with background information have been studied, inter alia, for text categorization, natural language inference, and entity and event extraction (K. M. et al., 2018; Yang and Mitchell, 2017). Recently, segment convolutional neural networks have been proposed for end-to-end relation extraction in the clinical domain, achieving results comparable to or outperforming the approaches with heavy manual feature engineering. In this paper, we analyze the errors made by the neural classifier based on confusion matrices, and then investigate three simple extensions to overcome its limitations. We find that including ontological association between drugs and problems, and data-induced association between medical concepts does not reliably improve the performance, but that large gains are ob"
W18-6248,P18-2061,0,0.0318931,"Missing"
W96-0102,H92-1022,0,0.0628734,"tical methods has used n-gram models or Hidden Markov Model-based taggers (e.g. Church, 1988; DeRose, 1988; Cutting et al. 1992; Merialdo, 1994, etc.). In 14 these approaches, a tag sequence is chosen for a sentence that maximizes the product of lexical and contextual probabilities as estimated from a tagged corpus. In rule-based approaches, words are assigned a tag based on a set of rules and a lexicon. These rules can either be hand-crafted (Garside et al., 1987; Klein & Simmons, 1963; Green & Rubin, 1971), or learned, as in Hindle (1989) or the transformation-based error-driven approach of Brill (1992). In a memory-based approach, a set of cases is kept in memory. Each case consists of a word (or a lexical representation for the word) with preceding and following context, and the corresponding category for that word in that context. A new sentence is tagged by selecting for each word in the sentence and its context the most similar case(s) in memory, and extrapolating the category of the word from these 'nearest neighbors'. A memorybased approach has features of both learning rule-based taggers (each case can be regarded as a very specific rule, the similarity based reasoning as a form of c"
W96-0102,A88-1019,0,0.132105,"useful for a number of applications: as a preprocessing stage to parsing, in information retrieval, in text to speech systems, in corpus linguistics, etc. The two factors determining the syntactic category of a word are its lexical probability (e.g. without context, man is more probably a noun than a verb), and its contextual probability (e.g. after a pronoun, man is more probably a verb than a noun, as in they man the boats). Several approaches have been proposed to construct automatic taggers. Most work on statistical methods has used n-gram models or Hidden Markov Model-based taggers (e.g. Church, 1988; DeRose, 1988; Cutting et al. 1992; Merialdo, 1994, etc.). In 14 these approaches, a tag sequence is chosen for a sentence that maximizes the product of lexical and contextual probabilities as estimated from a tagged corpus. In rule-based approaches, words are assigned a tag based on a set of rules and a lexicon. These rules can either be hand-crafted (Garside et al., 1987; Klein & Simmons, 1963; Green & Rubin, 1971), or learned, as in Hindle (1989) or the transformation-based error-driven approach of Brill (1992). In a memory-based approach, a set of cases is kept in memory. Each case consis"
W96-0102,A92-1018,0,0.50143,"cations: as a preprocessing stage to parsing, in information retrieval, in text to speech systems, in corpus linguistics, etc. The two factors determining the syntactic category of a word are its lexical probability (e.g. without context, man is more probably a noun than a verb), and its contextual probability (e.g. after a pronoun, man is more probably a verb than a noun, as in they man the boats). Several approaches have been proposed to construct automatic taggers. Most work on statistical methods has used n-gram models or Hidden Markov Model-based taggers (e.g. Church, 1988; DeRose, 1988; Cutting et al. 1992; Merialdo, 1994, etc.). In 14 these approaches, a tag sequence is chosen for a sentence that maximizes the product of lexical and contextual probabilities as estimated from a tagged corpus. In rule-based approaches, words are assigned a tag based on a set of rules and a lexicon. These rules can either be hand-crafted (Garside et al., 1987; Klein & Simmons, 1963; Green & Rubin, 1971), or learned, as in Hindle (1989) or the transformation-based error-driven approach of Brill (1992). In a memory-based approach, a set of cases is kept in memory. Each case consists of a word (or a lexical represen"
W96-0102,J88-1003,0,0.0138965,"umber of applications: as a preprocessing stage to parsing, in information retrieval, in text to speech systems, in corpus linguistics, etc. The two factors determining the syntactic category of a word are its lexical probability (e.g. without context, man is more probably a noun than a verb), and its contextual probability (e.g. after a pronoun, man is more probably a verb than a noun, as in they man the boats). Several approaches have been proposed to construct automatic taggers. Most work on statistical methods has used n-gram models or Hidden Markov Model-based taggers (e.g. Church, 1988; DeRose, 1988; Cutting et al. 1992; Merialdo, 1994, etc.). In 14 these approaches, a tag sequence is chosen for a sentence that maximizes the product of lexical and contextual probabilities as estimated from a tagged corpus. In rule-based approaches, words are assigned a tag based on a set of rules and a lexicon. These rules can either be hand-crafted (Garside et al., 1987; Klein & Simmons, 1963; Green & Rubin, 1971), or learned, as in Hindle (1989) or the transformation-based error-driven approach of Brill (1992). In a memory-based approach, a set of cases is kept in memory. Each case consists of a word ("
W96-0102,P89-1015,0,0.0076427,"e been proposed to construct automatic taggers. Most work on statistical methods has used n-gram models or Hidden Markov Model-based taggers (e.g. Church, 1988; DeRose, 1988; Cutting et al. 1992; Merialdo, 1994, etc.). In 14 these approaches, a tag sequence is chosen for a sentence that maximizes the product of lexical and contextual probabilities as estimated from a tagged corpus. In rule-based approaches, words are assigned a tag based on a set of rules and a lexicon. These rules can either be hand-crafted (Garside et al., 1987; Klein & Simmons, 1963; Green & Rubin, 1971), or learned, as in Hindle (1989) or the transformation-based error-driven approach of Brill (1992). In a memory-based approach, a set of cases is kept in memory. Each case consists of a word (or a lexical representation for the word) with preceding and following context, and the corresponding category for that word in that context. A new sentence is tagged by selecting for each word in the sentence and its context the most similar case(s) in memory, and extrapolating the category of the word from these 'nearest neighbors'. A memorybased approach has features of both learning rule-based taggers (each case can be regarded as a"
W96-0102,J94-2001,0,0.0722672,"essing stage to parsing, in information retrieval, in text to speech systems, in corpus linguistics, etc. The two factors determining the syntactic category of a word are its lexical probability (e.g. without context, man is more probably a noun than a verb), and its contextual probability (e.g. after a pronoun, man is more probably a verb than a noun, as in they man the boats). Several approaches have been proposed to construct automatic taggers. Most work on statistical methods has used n-gram models or Hidden Markov Model-based taggers (e.g. Church, 1988; DeRose, 1988; Cutting et al. 1992; Merialdo, 1994, etc.). In 14 these approaches, a tag sequence is chosen for a sentence that maximizes the product of lexical and contextual probabilities as estimated from a tagged corpus. In rule-based approaches, words are assigned a tag based on a set of rules and a lexicon. These rules can either be hand-crafted (Garside et al., 1987; Klein & Simmons, 1963; Green & Rubin, 1971), or learned, as in Hindle (1989) or the transformation-based error-driven approach of Brill (1992). In a memory-based approach, a set of cases is kept in memory. Each case consists of a word (or a lexical representation for the w"
W96-0102,W95-0108,0,0.0304644,"Missing"
W96-0102,W93-0420,0,0.015075,".22 .82 .23 ---np np np np , np , cd , cd nns cd nns jj-np nns jj-np , np np cd nns JJ for similar approaches). 4.3 Unknown Words If a word is not present in the lexicon, its ambiguous category cannot be retrieved. In that case, a category can be guessed only on the basis of the form or the context of the word. Again, we take advantage of the d a t a fusion capabilities of a memory-based approach by combining these two sources of information in the case representation, and having the information gain feature relevance weighting technique figure out their relative relevance (see Schmid, 1994; Samuelsson, 1994 for similar solutions). In most taggers, some form of morphological analysis is performed on unknown words, in an a t t e m p t to relate the unknown word to a known combination of known morphemes, thereby allowing its association with one or more possible categories. After determining this ambiguous category, the word is disambiguated using context knowledge, the same way as known words. Morphological analysis presupposes the availability of highly language-specific resources such as a morpheme lexicon, spelling rules, morphological rules, and heuristics to prioritise possible analyses of a"
W96-0102,C94-1027,0,0.0684304,"entation d f a .22 .82 .23 ---np np np np , np , cd , cd nns cd nns jj-np nns jj-np , np np cd nns JJ for similar approaches). 4.3 Unknown Words If a word is not present in the lexicon, its ambiguous category cannot be retrieved. In that case, a category can be guessed only on the basis of the form or the context of the word. Again, we take advantage of the d a t a fusion capabilities of a memory-based approach by combining these two sources of information in the case representation, and having the information gain feature relevance weighting technique figure out their relative relevance (see Schmid, 1994; Samuelsson, 1994 for similar solutions). In most taggers, some form of morphological analysis is performed on unknown words, in an a t t e m p t to relate the unknown word to a known combination of known morphemes, thereby allowing its association with one or more possible categories. After determining this ambiguous category, the word is disambiguated using context knowledge, the same way as known words. Morphological analysis presupposes the availability of highly language-specific resources such as a morpheme lexicon, spelling rules, morphological rules, and heuristics to prioritise possi"
W96-0102,P94-1025,0,0.0470726,"Missing"
W96-0102,J96-4008,0,\N,Missing
W97-1016,W96-0211,0,0.0232317,"hms. It is the latter approach which we investigate in this paper. It is our experience that lazy learning (such as the Memory-Based Learning approach adopted here) is more effective for several languageZavrel, Daelemans ~4 Veenstra w~ 5(x~, Yi) (1) i=l where: 5(x,, y~) = 0 i f x, = Yi, else 1 (2) This metric simply counts the r/umber of (mis)matching feature values in both patterns. If no information about the importance of features is available, this is a reasonable choice. But if we have information about feature relevance, we can add linguistic bias to weight or select different features (Cardie, 1996). An alternative, more empiricist, approach is to look at the behavior of features in the set of examples used for training. We can compute Statistics about the relevance of features by looking at which features are good predictors of the class labels. Information Theory provides a useful tool for measuring feature relevance in this way, see Quinlan (1993). 137 Memory-Based PP Attachment I n f o r m a t i o n G a i n (IG) weighting looks at each feature in isolation, and measures how much information it contributes to our knowledge of the correct class label. The Information Gain of feature f"
W97-1016,J93-2004,0,0.0494185,"eatures either task dependent (MVDM) or task independent (LexSpace) syntactic vector representations for words. The introduction of vector represenations leads to a number of modifications to the distance metrics and extrapolation rules in the MBL framework. A final experiment examines a number of weighted voting rules. The experiments in this section are conducted on a simplified version of the ""full"" P P - a t t a c h m e n t problem, i.e. the a t t a c h m e n t of a P P in the sequence: VP NP PP. The d a t a consist of four-tuples of words, extracted from the Wall Street Journal Treebank (Marcus, Santorini, and Marcinkiewicz, 1993) by a group at IBM (Ratnaparkhi, Reynar, and Roukos, 1994). 2 They took all sentences t h a t contained the pattern VP NP PP and extracted the head words from the constituents, yielding a V N1 P N2 pattern. For each pattern they recorded whether the PP was attached to the verb or to the noun in the treebank parse. Example sentences 1 and 2 would then become: 3 eats, pizza, with, fork, Y. 4 eats, pizza, with, anchovies, N. The data set contains 20801 training patterns, 3097 test patterns, and an independent validation set of 4039 patterns for p a r a m e t e r optimization. It has been used in"
W97-1016,H94-1048,0,0.428004,"ace) syntactic vector representations for words. The introduction of vector represenations leads to a number of modifications to the distance metrics and extrapolation rules in the MBL framework. A final experiment examines a number of weighted voting rules. The experiments in this section are conducted on a simplified version of the ""full"" P P - a t t a c h m e n t problem, i.e. the a t t a c h m e n t of a P P in the sequence: VP NP PP. The d a t a consist of four-tuples of words, extracted from the Wall Street Journal Treebank (Marcus, Santorini, and Marcinkiewicz, 1993) by a group at IBM (Ratnaparkhi, Reynar, and Roukos, 1994). 2 They took all sentences t h a t contained the pattern VP NP PP and extracted the head words from the constituents, yielding a V N1 P N2 pattern. For each pattern they recorded whether the PP was attached to the verb or to the noun in the treebank parse. Example sentences 1 and 2 would then become: 3 eats, pizza, with, fork, Y. 4 eats, pizza, with, anchovies, N. The data set contains 20801 training patterns, 3097 test patterns, and an independent validation set of 4039 patterns for p a r a m e t e r optimization. It has been used in statistical disambiguation methods by Ratnaparkhi, Reynar"
W97-1016,C96-1018,1,0.848064,"Missing"
W97-1016,P97-1056,1,0.87954,"Missing"
W97-1016,J93-1005,0,0.0675084,"borious and expensive, and is only feasible in a very restricted domain. The modeling of pragmatic inference seems to be even more difficult in a computational system. Due to the difficulties with the modeling of semantic strategies for ambiguity resolution, an attractive alternative is to look at the statistics of word patterns in annotated corpora. In such a corpus, different kinds of information used to resolve attachment ambiguity are, implicitly, represented in co-occurrence regularities. Several statistical techniques can use this information in learning attachment ambiguity resolution. Hindle and Rooth (1993) were the first to show that a corpus-based approach to PP attachment ambiguity resolution can lead to good results. For sentences with a verb~noun attachment ambiguity, they measured the lexical association between the noun and the preposition, and the verb and the preposition in unambiguous sentences. Their method bases attachment decisions on the ratio and 136 Memory-Based PP Attachment Jakub Zavrel, Walter Daelemans and Jorn Veenstra (1997) Resolving P P attachment Ambiguities w i t h Memory-Based Learning. In T.M. EUison (ed.) CoNLL97: Computational Natural Language Learning, ACL ~ 136-14"
W97-1016,E95-1020,0,\N,Missing
W98-1201,P97-1008,0,\N,Missing
W98-1201,W95-0103,0,\N,Missing
W98-1223,C88-1028,1,0.825637,"Missing"
W98-1223,J94-3007,1,0.894369,"Missing"
W98-1223,J76-4008,0,0.257262,"se. Mean results can be employed further in significance tests. In our experiments, n = 10, and one-tailed ttests axe performed. 3 Three word-pronunciation architectures Out experiments axe grouped in three series, each involving the application of IGTR~.B to a paxticula~ word-pronunciation system. The a~chitectures of these systems axe displayed in Figure 1. In the following subsections, each system is introduced, an outline is given of the experiments performed on the system, and the results a~e briefly discussed. 3.1 M-A-G-Y-S The axchitectu~e of the M-A-G-Y-S system is inspixed by SGUND1 (Hunnicutt, 1976; Hunnicutt, 1980), the word-pronunciation subsystem of the MIT~kLK text-to-speech system (Allen, Hunnicutt, and Klatt, 1987). When the MITALK system is faced with an unknown word, sounD1 produces on the basis of that word a phonemic transcription with stress markers (Allen, Hunnieutt, and Klatt, 1987). This wordpronunciation process is divided into the following five processing components: 1. morphological segmentalion, which we implement as the module referred to as M; 2. graphemic parsing, module A; 3. grapheme-phoneme conversion, module G; 4. sfllabifica~ion, module y; 5. stress assignment"
W98-1223,P84-1038,0,0.0599276,"ssential levels of abstraction in language processing tasks. Appl;ed to morpho-phonology, the argument states that generic learning methods are not able to discover morphology, graphematies, and stress patterns autonomonsly when learning word pronunciation, although this knowledge appears essential. Phonological and morphological theories, influenced by Chomskyan theory across the board since the publication of spy. (Chomsky and Halle, 1968), have generally adopted the idea of abstraction levels in various guises (e.g., levels, tapes, tiers, grids) (Goldsmith, 1976; Liberman and Prince, 1977; Koskenniemi, 1984; Mohanan, 1986). Although there is no general consensus on which levels of abstraction can be discerned in phonology and morphology, there is a rough, global agreement on the fact that words can be represented on different abstraction levels as Modul,~rity in Word Pronunciation systems Antal van den Bosch, Ton Weijters and Walter Daelemans (1998) Modularity in Inductively-Learned Word Pronunciation Systems. In D.M.W. Powers (ed.) NeMLaP3/CoNLL98: New Methods in Language Processing and Computational Natural Language Learning, ACL, pp 185-194. I strings ofletters, graphemes, morphemes, phonemes"
W98-1223,E93-1007,1,0.88377,"Missing"
W98-1223,W83-0114,0,\N,Missing
W98-1224,J94-3007,1,0.877197,"Missing"
W99-0629,P98-1010,0,0.141408,"assignment. Abney (1991) is one of the first who proposed to split up parsing into several cascades. He suggests to first find the chunks and then the dependecies between these chunks. Grefenstette (1996) describes a cascade of finite-state transducers, which first finds noun and verb groups, then their heads, and finally syntactic functions. Brants and Skut (1998) describe a partially automated annotation tool which constructs a complete parse of a sentence by recursively adding levels to the tree. (Collins, 1997; Ratnaparkhi, 1997) use cascaded processing for full parsing with good results. Argamon et al. (1998) applied Memory-Based Sequence Learning (MBSL) to NP chunking and subject/object identification. However, their subject and object finders are independent of their chunker (i.e. not cascaded). Drawing from this previous work we will explicitly study the effect of adding steps to the grammatical relations assignment cascade. Through experiments with cascading several classifiers, we will show that even using imperfect classifiers can improve overall performance of the cascaded classifier. We illustrate this claim on the task of finding grammatical relations (e.g. subject, object, locative) to v"
W99-0629,C92-3126,0,0.00939194,"g comparison. I G T r e e : In.this variant, a decision tree is created with features as tests, and ordered according to the information gain of the features, as a heuristic approximation of the computationally more expensive IB1 variants. For more references and information about these algorithms we refer to (Daelemans et al., 1998; Daelemans et al., 1999b). For other 1For the experiments described in this paper we have used TiMBL, an MBL software package developed in the ILK-group (Daelemans et al., 1998), TiMBL is available from: http://ilk.kub.nl/. memory-based approaches to parsing, see (Bod, 1992) and (Sekine, 1998). 3 Methods and Results In this section we describe the stages of the cascade. The very first stage consists of a MemoryBased Part-of-Speech Tagger (MBT) for which we refer to (Daelemans et al., 1996). The next three stages involve determining boundaries and labels of chunks. Chunks are nonrecursive, non-overlapping constituent parts of sentences (see (Abney, 1991)). First, we simultaneously chunk sentences into: NP-, VP, Prep-, A D J P - and APVP-chunks. As these chunks are non-overlapping, no words can belong to more than one chunk, and thus no conflicts can arise. Prep-ch"
W99-0629,W99-0707,1,0.554196,"his paper we will first briefly describe Memory-Based Learning in Section 2. In 239 Section 3.1, we discuss the chunking classifiers that we later use as steps in the cascade. Section 3.2 describes the basic G R classifier. Section 3.3 presents the architecture and results of the cascaded G R assignment experiments. We discuss the results in Section 4 and conclude with Section 5. 2 Memory-Based Learning Memory-Based Learning (MBL) keeps all training d a t a in m e m o r y and only abstracts at classification time by extrapolating a class from the most similar item(s) in memory. In recent work Daelemans et al. (1999b) have shown that for typical natural language processing tasks, this approach is at an advantage because it also &quot;remembers&quot; exceptional, low-frequency cases which are useful to extrapolate from. Moreover, automatic feature weighting in the similarity metric of an MB learner makes the approach well-suited for domains with large numbers of features from heterogeneous sources, as it embodies a smoothing-by-similarity method when d a t a is sparse (Zavrel and Daelemans, 1997). We have used the following MBL algorithms1: I B 1 : A variant of the k-nearest neighbor (kNN) algorithm. The distance b"
W99-0629,W98-1207,0,0.102339,"Missing"
W99-0629,P97-1003,0,0.0518161,"o higher modules? Recently, many people have looked at cascaded and/or shallow parsing and GR assignment. Abney (1991) is one of the first who proposed to split up parsing into several cascades. He suggests to first find the chunks and then the dependecies between these chunks. Grefenstette (1996) describes a cascade of finite-state transducers, which first finds noun and verb groups, then their heads, and finally syntactic functions. Brants and Skut (1998) describe a partially automated annotation tool which constructs a complete parse of a sentence by recursively adding levels to the tree. (Collins, 1997; Ratnaparkhi, 1997) use cascaded processing for full parsing with good results. Argamon et al. (1998) applied Memory-Based Sequence Learning (MBSL) to NP chunking and subject/object identification. However, their subject and object finders are independent of their chunker (i.e. not cascaded). Drawing from this previous work we will explicitly study the effect of adding steps to the grammatical relations assignment cascade. Through experiments with cascading several classifiers, we will show that even using imperfect classifiers can improve overall performance of the cascaded classifier. We il"
W99-0629,W96-0102,1,0.63484,"on then we need. For many tasks detecting only shallow structures in a sentence in a fast and reliable way is to be preferred over full parsing. For example, in information retrieval it can be enough to find only simple NPs and VPs in a sentence, for information extraction we might also want to find relations between constituents as for example the subject and object of a verb. In this paper we discuss some Memory-Based (MB) shallow parsing techniques to find labeled chunks and grammatical relations in a sentence. Several MB modules have been developed in previous work, such as: a POS tagger (Daelemans et al., 1996), a chunker (Veenstra, 1998; Tjong Kim Sang and Veenstra, 1999) and a grammatical relation (GR) assigner (Buchholz, 1998). The questions we will answer in this paper are: Can we reuse these modules in a cascade of classifiers? What is the effect of cascading? Will errors at a lower level percolate to higher modules? Recently, many people have looked at cascaded and/or shallow parsing and GR assignment. Abney (1991) is one of the first who proposed to split up parsing into several cascades. He suggests to first find the chunks and then the dependecies between these chunks. Grefenstette (1996) d"
W99-0629,J93-2004,0,0.0515171,"Missing"
W99-0629,W95-0107,0,0.0210108,"recision and recall measures. Precision is the percentage of predicted chunks/relations that are actually correct, recall is the percentage of correct chunks/relations that are actually found. For convenient comparisons of only one value, we also list the FZ=i value (C.J.van Rijsbergen, 1979): (Z2+l)&apos;prec&apos;rec /~2.prec+rec , with/~ = 1 3.1 Chunking In the first experiment described in this section, the task is to segment the sentence into chunks and to assign labels to these chunks. This process of chunking and labeling is carried out by assigning a tag to each word in a sentence leftto-right. Ramshaw and Marcus (1995) first assigned a chunk tag to each word in the sentence: I for inside a chunk, O for outside a chunk, and 240 B tbr inside a chunk, but tile preceding word is in another chunk. As we want to find more than one kind of chunk, we have to [hrther differentiate tile IOB tags as to which kind of chunk (NP, VP, Prep, A D J P or ADVP) the word is ill. W i t h the extended IOB tag set at hand we can tag the sentence: But/CO [NP the/DT dollar/NN NP] [ADVP later/KB ADVP] [VP rebounded/VBD VP] ,/, [VP finishing/VBG VP] [ADJP slightly/KB higher/RBK ADJP] [Prep against/IN Prep] [NP the/DT yen/NNS NP] [ADJ"
W99-0629,W97-0301,0,0.0148043,"s? Recently, many people have looked at cascaded and/or shallow parsing and GR assignment. Abney (1991) is one of the first who proposed to split up parsing into several cascades. He suggests to first find the chunks and then the dependecies between these chunks. Grefenstette (1996) describes a cascade of finite-state transducers, which first finds noun and verb groups, then their heads, and finally syntactic functions. Brants and Skut (1998) describe a partially automated annotation tool which constructs a complete parse of a sentence by recursively adding levels to the tree. (Collins, 1997; Ratnaparkhi, 1997) use cascaded processing for full parsing with good results. Argamon et al. (1998) applied Memory-Based Sequence Learning (MBSL) to NP chunking and subject/object identification. However, their subject and object finders are independent of their chunker (i.e. not cascaded). Drawing from this previous work we will explicitly study the effect of adding steps to the grammatical relations assignment cascade. Through experiments with cascading several classifiers, we will show that even using imperfect classifiers can improve overall performance of the cascaded classifier. We illustrate this claim"
W99-0629,E99-1023,1,0.480445,"res in a sentence in a fast and reliable way is to be preferred over full parsing. For example, in information retrieval it can be enough to find only simple NPs and VPs in a sentence, for information extraction we might also want to find relations between constituents as for example the subject and object of a verb. In this paper we discuss some Memory-Based (MB) shallow parsing techniques to find labeled chunks and grammatical relations in a sentence. Several MB modules have been developed in previous work, such as: a POS tagger (Daelemans et al., 1996), a chunker (Veenstra, 1998; Tjong Kim Sang and Veenstra, 1999) and a grammatical relation (GR) assigner (Buchholz, 1998). The questions we will answer in this paper are: Can we reuse these modules in a cascade of classifiers? What is the effect of cascading? Will errors at a lower level percolate to higher modules? Recently, many people have looked at cascaded and/or shallow parsing and GR assignment. Abney (1991) is one of the first who proposed to split up parsing into several cascades. He suggests to first find the chunks and then the dependecies between these chunks. Grefenstette (1996) describes a cascade of finite-state transducers, which first fin"
W99-0629,P97-1056,1,0.824686,"y and only abstracts at classification time by extrapolating a class from the most similar item(s) in memory. In recent work Daelemans et al. (1999b) have shown that for typical natural language processing tasks, this approach is at an advantage because it also &quot;remembers&quot; exceptional, low-frequency cases which are useful to extrapolate from. Moreover, automatic feature weighting in the similarity metric of an MB learner makes the approach well-suited for domains with large numbers of features from heterogeneous sources, as it embodies a smoothing-by-similarity method when d a t a is sparse (Zavrel and Daelemans, 1997). We have used the following MBL algorithms1: I B 1 : A variant of the k-nearest neighbor (kNN) algorithm. The distance between a test item and each memory item is defined as the number of features for which they have a different value (overlap metric). IBi-IG : IB1 with information gain (an information-theoretic notion measuring the reduction of uncertainty a b o u t the class to be predicted when knowing the value of a feature) to weight the cost of a feature value mismatch during comparison. I G T r e e : In.this variant, a decision tree is created with features as tests, and ordered accord"
W99-0629,C98-1010,0,\N,Missing
W99-0707,P98-1010,0,0.317093,"Missing"
W99-0707,H92-1022,0,0.0583452,"Missing"
W99-0707,P98-1034,0,0.0338288,"Missing"
W99-0707,W96-0102,1,0.757819,"Missing"
W99-0707,W97-1016,1,0.411611,"Missing"
W99-0707,J93-2004,0,0.0537042,"Missing"
W99-0707,W95-0107,0,0.0432778,"Missing"
W99-0707,W97-0301,0,0.0146128,"Missing"
W99-0707,C96-1047,0,0.0107971,"Missing"
W99-0707,P97-1056,1,0.820917,"Missing"
W99-0707,C98-1034,0,\N,Missing
zavrel-daelemans-2000-bootstrapping,W96-0102,1,\N,Missing
zavrel-daelemans-2000-bootstrapping,van-eynde-etal-2000-part,1,\N,Missing
zavrel-daelemans-2000-bootstrapping,J95-4004,0,\N,Missing
zavrel-daelemans-2000-bootstrapping,P98-1081,1,\N,Missing
zavrel-daelemans-2000-bootstrapping,C98-1078,1,\N,Missing
zavrel-daelemans-2000-bootstrapping,A00-1031,0,\N,Missing
zavrel-daelemans-2000-bootstrapping,J01-2002,1,\N,Missing
zavrel-daelemans-2000-bootstrapping,P98-1029,0,\N,Missing
zavrel-daelemans-2000-bootstrapping,C98-1029,0,\N,Missing
zavrel-daelemans-2000-bootstrapping,W94-0103,0,\N,Missing
