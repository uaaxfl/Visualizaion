2020.acl-main.305,N12-1074,0,0.0278822,"t we consistently perform better both in conversation cold start and with varying degrees of sparsity of user history and conversation contexts. Lastly, to provide more insights into user interest dynamics, we inspect our model outputs and find that users indeed tend to engage in different types of conversations at different times, confirming the usefulness of tracking user preferences in real-time for conversation recommendation. 2 Related Work User Response Prediction. This work is in line with user response prediction, such as message popularity forecast with handcrafted response features (Artzi et al., 2012; Backstrom et al., 2013) and conversation trajectory with user interaction structures (Cheng et al., 2017b; Jiao et al., 2018; Zeng et al., 2019a). These works predict responses from general public, while we work on personalized recommendation and focus on user interest modeling. For recommendation, there are extensive efforts on post-level recommendation (Chen et al., 2012; Yan et al., 2012) and conversation-level (Chen et al., 2011; Zeng et al., 2018, 2019b). In contrast with them which assume static user interests, we capture how user interests change over time and take advantage of the re"
2020.acl-main.305,D17-1243,0,0.0974531,"ser history and conversation contexts. Lastly, to provide more insights into user interest dynamics, we inspect our model outputs and find that users indeed tend to engage in different types of conversations at different times, confirming the usefulness of tracking user preferences in real-time for conversation recommendation. 2 Related Work User Response Prediction. This work is in line with user response prediction, such as message popularity forecast with handcrafted response features (Artzi et al., 2012; Backstrom et al., 2013) and conversation trajectory with user interaction structures (Cheng et al., 2017b; Jiao et al., 2018; Zeng et al., 2019a). These works predict responses from general public, while we work on personalized recommendation and focus on user interest modeling. For recommendation, there are extensive efforts on post-level recommendation (Chen et al., 2012; Yan et al., 2012) and conversation-level (Chen et al., 2011; Zeng et al., 2018, 2019b). In contrast with them which assume static user interests, we capture how user interests change over time and take advantage of the recent advancement of dynamic product recommendation (Wu et al., 2017; Beutel et al., 2018). To recommend co"
2020.acl-main.305,D14-1179,0,0.0100025,"Missing"
2020.acl-main.305,D14-1181,0,0.00373426,"duce how we model the user interest dynamics with their chatting history in Section 3.1, followed by the description of conversation modeling in Section 3.2. Afterwards, Section 3.3 will present how we produce final recommendation outputs. Objective function and learning procedures will be finally presented in Section 3.4. 3.1 Message-level Modeling. We model messagelevel representation from its word sequence. Specifically, given u’s historical message m, we first use a pre-trained word embedding layer to map each word into a vector space, and then employ a Convolutional Neural Network (CNN) (Kim, 2014) encoder to model word occurrence with their neighbors. Afterwards, we output representation zm to reflect m’s content. User Interest Dynamic Modeling Given a sequence of chronologically ordered historical messages hm1 , m2 , · · · , m|u |i of a user u (|u |is the message number of u), a message therein corresponds to a word sequence wm . Our goal is to capture the temporal patterns from the sequence of user chatting messages and then produce the user interest representation. We employ two-level modeling — message level and user level. User-level Modeling. As shown in Wu et al. (2017), some us"
2020.acl-main.305,W02-0109,0,0.0915391,"th More-path Tech Learn Dataset Fun Figure 4: Distributions of conversation structure. “Onepath”, “Two-path”, and “More-path” indicate the conversation has 1, 2, and more root-to-leaf paths. versations contain two or more paths, illustrating complex who-replies-to-whom interactions in the tree structure (with the original post as the root node and in-reply-to relations as edges). Therefore, graph-structured encoder may be a suitable alternative for capturing rich turn interactions in Reddit conversations. Preprocessing. For all datasets, we applied open source natural language toolkit (NLTK) (Loper and Bird, 2002) for tokenization. Further, links were replaced by a generic tag “hURLi” and all number tokens were removed. In the experiments, we maintained a vocabulary with all the remaining tokens (including punctuation and emoticons). Model Settings. In training, we adopt negative sampling with sampling ratio of 5 (see Section 3.4). We also randomly sample 100 negative instances for each positive one during validation and test, to avoid unbalanced labels. For parameters, we initialize the word embedding layer with 300-dim Common Crawl version of Glove embedding (Pennington et al., 2014), and the dimensi"
2020.acl-main.305,D15-1178,0,0.0278975,"how user interests change over time and take advantage of the recent advancement of dynamic product recommendation (Wu et al., 2017; Beutel et al., 2018). To recommend conversations, we aim to learn user interest dynamics from chatting content and interaction behavior, which have never been explored in previous research. Conversation Structure Modeling. Our work is also related to previous work to understand how participants interact with each other in conversation structure. Earlier efforts focus on discovering word statistic patterns via probabilistic graphical models (Ritter et al., 2010; Louis and Cohen, 2015), which are unable to capture deep semantics embedded in complex interactions. Recent research points out the effectiveness to understand conversation structure from temporal dynamics (Cheng et al., 2017a; Jiao et al., 2018) and replying struc3332 - ?,? (Predicted Score) ? MLP Mechanism Attention Msg Encoder GCN GRU ?|?| … … … Bi-GRU ?? ?? ??? Msg Encoder Text Initialize User User Factor Embedding ?? ?? ?? ?? Figure 2: Overall structure of our model. The left module is to model user interest dynamics, whose results together with conversation representations derived from the right part are used"
2020.acl-main.305,D17-1159,0,0.05026,"4 . We design the model to capture user interests from both what they said in the past, and how they interacted with each other in the conversation structure. We first capture time-variant representations from user chatting history, where we assume user interests may change over time and therefore apply a gated recurrent unit (GRU) (Cho et al., 2014) to model time dependency. User interactions in the conversation context are then explored with both bidirectional gated recurrent unit (Bi-GRU) (Cho et al., 2014) for conversation turns’ chronological order and graph convolutional networks (GCN) (Marcheggiani and Titov, 2017) for in-reply-to relations. Both representations are learned to encode how participants formed the conversation structure, including what they said and whom they replied to. Next, we propose a user-aware attention to convey the user interest dynamics, which is further put over an interactionencoded conversation to measure whether its ongoing contexts fit a user’s current interests. Finally, we predict how likely a user will engage in a conversation, as a result of recommendation. To the best of our knowledge, we are the first to study dynamic online conversation recommendation and to explore t"
2020.acl-main.305,C18-1322,0,0.0175857,"g et al., 2017a; Jiao et al., 2018) and replying struc3332 - ?,? (Predicted Score) ? MLP Mechanism Attention Msg Encoder GCN GRU ?|?| … … … Bi-GRU ?? ?? ??? Msg Encoder Text Initialize User User Factor Embedding ?? ?? ?? ?? Figure 2: Overall structure of our model. The left module is to model user interest dynamics, whose results together with conversation representations derived from the right part are used for producing final prediction. Predicted score yˆu,c indicates how likely u will engage in c. “Msg Encoder” mainly contains two layers: word embedding layer and CNN modeling layer. ture (Miura et al., 2018; Zayats and Ostendorf, 2018; Zeng et al., 2019b). The two factors are coupled in our interaction modeling and their joint effects for dynamic conversation recommendation, ignored by prior work, will be extensively studied here. 3 Our Dynamic Conversation Recommendation Model This section describes our dynamic conversation recommendation model, whose overall structure is shown in Figure 2. In the following, we will first introduce how we model the user interest dynamics with their chatting history in Section 3.1, followed by the description of conversation modeling in Section 3.2. Afterwards,"
2020.acl-main.305,D14-1162,0,0.082686,"Missing"
2020.acl-main.305,N10-1020,0,0.0591909,"interests, we capture how user interests change over time and take advantage of the recent advancement of dynamic product recommendation (Wu et al., 2017; Beutel et al., 2018). To recommend conversations, we aim to learn user interest dynamics from chatting content and interaction behavior, which have never been explored in previous research. Conversation Structure Modeling. Our work is also related to previous work to understand how participants interact with each other in conversation structure. Earlier efforts focus on discovering word statistic patterns via probabilistic graphical models (Ritter et al., 2010; Louis and Cohen, 2015), which are unable to capture deep semantics embedded in complex interactions. Recent research points out the effectiveness to understand conversation structure from temporal dynamics (Cheng et al., 2017a; Jiao et al., 2018) and replying struc3332 - ?,? (Predicted Score) ? MLP Mechanism Attention Msg Encoder GCN GRU ?|?| … … … Bi-GRU ?? ?? ??? Msg Encoder Text Initialize User User Factor Embedding ?? ?? ?? ?? Figure 2: Overall structure of our model. The left module is to model user interest dynamics, whose results together with conversation representations derived from"
2020.acl-main.305,P12-1054,0,0.0286043,"time for conversation recommendation. 2 Related Work User Response Prediction. This work is in line with user response prediction, such as message popularity forecast with handcrafted response features (Artzi et al., 2012; Backstrom et al., 2013) and conversation trajectory with user interaction structures (Cheng et al., 2017b; Jiao et al., 2018; Zeng et al., 2019a). These works predict responses from general public, while we work on personalized recommendation and focus on user interest modeling. For recommendation, there are extensive efforts on post-level recommendation (Chen et al., 2012; Yan et al., 2012) and conversation-level (Chen et al., 2011; Zeng et al., 2018, 2019b). In contrast with them which assume static user interests, we capture how user interests change over time and take advantage of the recent advancement of dynamic product recommendation (Wu et al., 2017; Beutel et al., 2018). To recommend conversations, we aim to learn user interest dynamics from chatting content and interaction behavior, which have never been explored in previous research. Conversation Structure Modeling. Our work is also related to previous work to understand how participants interact with each other in con"
2020.acl-main.305,Q18-1009,0,0.0285754,"o et al., 2018) and replying struc3332 - ?,? (Predicted Score) ? MLP Mechanism Attention Msg Encoder GCN GRU ?|?| … … … Bi-GRU ?? ?? ??? Msg Encoder Text Initialize User User Factor Embedding ?? ?? ?? ?? Figure 2: Overall structure of our model. The left module is to model user interest dynamics, whose results together with conversation representations derived from the right part are used for producing final prediction. Predicted score yˆu,c indicates how likely u will engage in c. “Msg Encoder” mainly contains two layers: word embedding layer and CNN modeling layer. ture (Miura et al., 2018; Zayats and Ostendorf, 2018; Zeng et al., 2019b). The two factors are coupled in our interaction modeling and their joint effects for dynamic conversation recommendation, ignored by prior work, will be extensively studied here. 3 Our Dynamic Conversation Recommendation Model This section describes our dynamic conversation recommendation model, whose overall structure is shown in Figure 2. In the following, we will first introduce how we model the user interest dynamics with their chatting history in Section 3.1, followed by the description of conversation modeling in Section 3.2. Afterwards, Section 3.3 will present how"
2020.acl-main.305,N18-1035,1,0.873366,"s and discuss topics they are interested in. However, the huge volume of online conversations produced daily hinders people’s capability of finding the information they are interested in. As a result, there is pressing demand for developing a conversation recommendation engine that tracks ongoing conversations and recommends suitable ones to users. Viewing the deluge of information streaming through social media, it is not hard to envision that users’ tastes, stances, and behaviors evolve over time (Wu et al., 2017). Nonetheless, existing work on recommending conversations (Chen et al., 2011; Zeng et al., 2018, 2019b) assume users’ discussion preferences do not change over time. Moreover, the common practice of recommendation is via collaborative filtering (CF), which relies on rich user interaction history for model training (Zeng et al., 2018, 2019b). When a conversation is entirely absent from training data, the model performance is inevitably compromised. This phenomenon is referred to as conversation cold start. As a result, existing methods which ignore the time-evolving user interests is insurmountable to tackle a common problem in practice, i.e., to predict future conversations created afte"
2020.acl-main.305,P19-1270,1,0.829133,"previous studies (Zeng et al., 2018, 2019b), which let training data contain partial context for any conversations to allow the possibility of predicting users’ future engagement 1 The datasets and codes are available at: https:// github.com/zxshamson/dy-conv-rec for recommendation. Experimental results in main comparisons show that our model significantly outperforms all previous methods that ignore the change of user interests or interactions within contexts. For example, we achieve 0.375 MAP in discussions of “technology”, compared with 0.222 yielded by our previous stateof-the-art model (Zeng et al., 2019b). Further study shows that we consistently perform better both in conversation cold start and with varying degrees of sparsity of user history and conversation contexts. Lastly, to provide more insights into user interest dynamics, we inspect our model outputs and find that users indeed tend to engage in different types of conversations at different times, confirming the usefulness of tracking user preferences in real-time for conversation recommendation. 2 Related Work User Response Prediction. This work is in line with user response prediction, such as message popularity forecast with hand"
2020.acl-main.305,D19-1470,1,0.79988,"previous studies (Zeng et al., 2018, 2019b), which let training data contain partial context for any conversations to allow the possibility of predicting users’ future engagement 1 The datasets and codes are available at: https:// github.com/zxshamson/dy-conv-rec for recommendation. Experimental results in main comparisons show that our model significantly outperforms all previous methods that ignore the change of user interests or interactions within contexts. For example, we achieve 0.375 MAP in discussions of “technology”, compared with 0.222 yielded by our previous stateof-the-art model (Zeng et al., 2019b). Further study shows that we consistently perform better both in conversation cold start and with varying degrees of sparsity of user history and conversation contexts. Lastly, to provide more insights into user interest dynamics, we inspect our model outputs and find that users indeed tend to engage in different types of conversations at different times, confirming the usefulness of tracking user preferences in real-time for conversation recommendation. 2 Related Work User Response Prediction. This work is in line with user response prediction, such as message popularity forecast with hand"
2020.acl-main.566,E17-2032,0,0.451826,"g system automatically (Young et al., 2013; Su et al., 2016; Williams et al., 2017; Peng et al., 2017, 2018a,b; Lipton et al., 2018; Li et al., 2020; Lee et al., 2019). However, RL-based approaches are rarely used in realworld applications, for these algorithms often require (too) many experiences for learning due to the sparse and uninformative rewards. A lot of progress is being made towards mitigating this sample complexity problem by incorporating prior knowledge. (Su et al., 2017) utilizes a corpus of demonstration to pre-train the RL-based models for accelerating learning from scratch. (Chen et al., 2017b) attempts to accelerate RL-based agents by introducing extra rewards from a virtual rule-based teacher. However, the method requires extra efforts to design a rule-based dialogue manager. (Hester et al., 2018) improve RL learning by utilizing a combination of demonstration, temporal difference (TD), supervised, and regularization losses. (Chen et al., 2017a) introduced a similar approach called companion teaching to incorporate human teacher feedback into policy learning. Nevertheless, companion teaching assumes that there is a human teacher to directly give a correct action during policy le"
2020.acl-main.566,D17-1260,0,0.431334,"g system automatically (Young et al., 2013; Su et al., 2016; Williams et al., 2017; Peng et al., 2017, 2018a,b; Lipton et al., 2018; Li et al., 2020; Lee et al., 2019). However, RL-based approaches are rarely used in realworld applications, for these algorithms often require (too) many experiences for learning due to the sparse and uninformative rewards. A lot of progress is being made towards mitigating this sample complexity problem by incorporating prior knowledge. (Su et al., 2017) utilizes a corpus of demonstration to pre-train the RL-based models for accelerating learning from scratch. (Chen et al., 2017b) attempts to accelerate RL-based agents by introducing extra rewards from a virtual rule-based teacher. However, the method requires extra efforts to design a rule-based dialogue manager. (Hester et al., 2018) improve RL learning by utilizing a combination of demonstration, temporal difference (TD), supervised, and regularization losses. (Chen et al., 2017a) introduced a similar approach called companion teaching to incorporate human teacher feedback into policy learning. Nevertheless, companion teaching assumes that there is a human teacher to directly give a correct action during policy le"
2020.acl-main.566,P17-1045,0,0.104076,"Missing"
2020.acl-main.566,I17-1074,0,0.117724,"Missing"
2020.acl-main.566,D18-1266,0,0.023463,"ion teaching assumes that there is a human teacher to directly give a correct action during policy learning process and meanwhile train an action prediction model for reward shaping based on human feedback. Policy shaping Policy Shaping is an algorithm that enables introducing prior knowledge into policy learning. (Griffith et al., 2013) formulates human feedback on the actions from an agent policy as policy feedback and proposes Advise algorithm to estimate humans Bayes feedback policy and combine it with the policy from the agent. It shows significant improvement in two gaming environment. (Misra et al., 2018) uses policy shaping to bias the search procedure towards semantic parses that are more compatible with the text and achieve excellent performance. Reward shaping Reward shaping leverages prior knowledge to provides a learning agent with an extra intermediate reward F in addition to environmental reward r, making the system learn from a composite signal R + F (Ng et al., 1999). However, it is not guaranteed that with reward shaping, an MDP can still have an optimal policy that is 6356 Human Demonstrations (?, ?, ?, ? ′ ) Reward Shaping User Policy Model ?? ?|? g(??, ??) Supervised Learning ? ="
2020.acl-main.566,P18-1203,1,0.732704,"operate in. As illustrated in Figure 1, RL-based dialogue agents need to interact with human users and update its policy in an online fashion requiring that the agents have a good online performance from the start of training. In addition, one of the biggest challenges of RL approaches is reward sparsity issue, which leads to exploration in large action space inefficient. As a consequence, training RL-based agents expects a prohibitively large number of interactions to achieve acceptable performance, which may incur a significant amount of expense (Pietquin et al., 2011; Lipton et al., 2016; Peng et al., 2018b). Several attempts are made to improve learning efficiency and tackle reward sparsity issues. Different types of heuristics has been proposed in the form of intrinsic rewards to guide exploration more efficiently (Lipton et al., 2016; Mohamed and Rezende, 2015; Peng et al., 2017, 2018a; Takanobu et al., 2019). When building a dialogue system, it is typically affordable to recruit experts to gather some demonstrations about the expected agent behaviors. We therefore aim to address the aforementioned challenges from a different perspective and assume having access to human-provided demonstrati"
2020.acl-main.566,N07-2038,0,0.0602695,"es in the beginning. For a fair comparison, we pre-fill the experience replay buffer Da with human demonstrations for all the variants of agents (Lipton et al., 2016). Confidence factor C used in policy shaping is set 0.7. As for the reward shaping, γ in equ.7 is set as 1. 4.3 User Simulator Training RL-based dialogue agents require an environment to interact with, and it usually needs a large volume of interactions to achieve good performance, which is not affordable in reality. It is commonly acceptable to employ a user simulator to train RL-based agents (Jain et al., 2018; Li et al., 2016; Schatzmann et al., 2007). We adopt a public available agenda-based user simulator (Li et al., 2016) for our experiment setup. During training, the simulator provides the agent with responses and rewards. The reward is defined as -1 for each turn to encourage short turns and a 6360 (a) Movie (b) Restaurant (c) Taxi Figure 3: The effect of number of human demonstration on the performance. The moving averaged success rate is calculated within 120 epochs for Movie, 200 epochs for Restaurant, and 200 epochs for Taxi. large positive reward (2L) for successful dialogue or a negative reward of L for failed one, where L (set"
2020.acl-main.566,W17-5518,0,0.0414357,"d Work Dialogue policy learning Deep reinforcement learning (RL) methods have shown great potential in building a robust dialog system automatically (Young et al., 2013; Su et al., 2016; Williams et al., 2017; Peng et al., 2017, 2018a,b; Lipton et al., 2018; Li et al., 2020; Lee et al., 2019). However, RL-based approaches are rarely used in realworld applications, for these algorithms often require (too) many experiences for learning due to the sparse and uninformative rewards. A lot of progress is being made towards mitigating this sample complexity problem by incorporating prior knowledge. (Su et al., 2017) utilizes a corpus of demonstration to pre-train the RL-based models for accelerating learning from scratch. (Chen et al., 2017b) attempts to accelerate RL-based agents by introducing extra rewards from a virtual rule-based teacher. However, the method requires extra efforts to design a rule-based dialogue manager. (Hester et al., 2018) improve RL learning by utilizing a combination of demonstration, temporal difference (TD), supervised, and regularization losses. (Chen et al., 2017a) introduced a similar approach called companion teaching to incorporate human teacher feedback into policy lear"
2020.acl-main.566,W15-4655,0,0.0265221,"h an extra intermediate reward F in addition to environmental reward r, making the system learn from a composite signal R + F (Ng et al., 1999). However, it is not guaranteed that with reward shaping, an MDP can still have an optimal policy that is 6356 Human Demonstrations (?, ?, ?, ? ′ ) Reward Shaping User Policy Model ?? ?|? g(??, ??) Supervised Learning ? = ? + ?? Imitation Model ?? ?|? Policy Shaping Figure 1: Illustration of the S 2 Agent for dialogue policy learning. identical to the original problem unless the shaping is potential-based reward shaping(Ng et al., 1999; Marthi, 2007). (Su et al., 2015) proposes to use RNNs to predict turn-level rewards and use the predicted reward as informative reward shaping potentials. (Peng et al., 2018a; Takanobu et al., 2019) use inverse reinforcement learning to recover reward functions from demonstrations for reward shaping. However, the estimated reward using these methods inevitably contains noise and failed to conform to potential-based reward function to guarantee the optimal policy. Inspired by (Brys et al., 2015), we directly estimate potential-based reward function from demonstrations. 3 Approach Our S 2 Agent is illustrated in Figure 1, cons"
2020.acl-main.566,D19-1010,0,0.153517,"issue, which leads to exploration in large action space inefficient. As a consequence, training RL-based agents expects a prohibitively large number of interactions to achieve acceptable performance, which may incur a significant amount of expense (Pietquin et al., 2011; Lipton et al., 2016; Peng et al., 2018b). Several attempts are made to improve learning efficiency and tackle reward sparsity issues. Different types of heuristics has been proposed in the form of intrinsic rewards to guide exploration more efficiently (Lipton et al., 2016; Mohamed and Rezende, 2015; Peng et al., 2017, 2018a; Takanobu et al., 2019). When building a dialogue system, it is typically affordable to recruit experts to gather some demonstrations about the expected agent behaviors. We therefore aim to address the aforementioned challenges from a different perspective and assume having access to human-provided demonstrations. In this paper, we investigate how to efficiently leverage these demonstrations to alleviate reward sparsity and improve policy learning quality. Previous work (Lipton et al., 2016) used a simple technique termed as Replay Buffer Spiking (RBS) to pre-fill experience replay buffer with human demonstrations,"
2020.acl-main.566,P17-1062,0,0.094393,"can effectively leverage human demonstrations to improve learning efficiency and quality through policy shaping and reward shaping. • We experimentally show that S 2 Agent can efficiently learn good policy with limited demonstrations on three single domain dialogue tasks and a challenging domain adaptation task using both simulator and human evaluations. 1 Agent with policy Shaping and reward Shaping 2 Related Work Dialogue policy learning Deep reinforcement learning (RL) methods have shown great potential in building a robust dialog system automatically (Young et al., 2013; Su et al., 2016; Williams et al., 2017; Peng et al., 2017, 2018a,b; Lipton et al., 2018; Li et al., 2020; Lee et al., 2019). However, RL-based approaches are rarely used in realworld applications, for these algorithms often require (too) many experiences for learning due to the sparse and uninformative rewards. A lot of progress is being made towards mitigating this sample complexity problem by incorporating prior knowledge. (Su et al., 2017) utilizes a corpus of demonstration to pre-train the RL-based models for accelerating learning from scratch. (Chen et al., 2017b) attempts to accelerate RL-based agents by introducing extra re"
2020.acl-main.566,D17-1237,1,0.955444,"d research efforts. Dialogue policy optimization is one of the most critical tasks of dialogue modeling. One of the most straightforward approaches is the rule-based method, which contains a set of expert-defined rules for dialogue modeling. Though rule-based dialogue systems have a reasonable performance in some scenarios, handcrafting such kinds of rules is time-consuming and not scalable. Recently, dialogue policy learning is formulated as a reinforcement learning (RL) problem and tackled with deep RL models (Li et al., 2017; Lipton ∗ Corresponding author † Equal Contribution et al., 2018; Peng et al., 2017). It has shown great potentials of using the RL-based method for building robust dialogue systems automatically. However, due to its interactive nature, RL-based agents demand of an environment to operate in. As illustrated in Figure 1, RL-based dialogue agents need to interact with human users and update its policy in an online fashion requiring that the agents have a good online performance from the start of training. In addition, one of the biggest challenges of RL approaches is reward sparsity issue, which leads to exploration in large action space inefficient. As a consequence, training R"
2020.emnlp-main.538,W12-2510,0,0.0520984,"Missing"
2020.emnlp-main.538,N16-1134,0,0.0225935,"anguage generation. Experiment results on two large-scale datasets in English and Chinese demonstrate that our quotation generation model outperforms the state-of-the-art models. Further analysis shows that topic, interaction, and query consistency are all helpful to learn how to quote in online conversations. 1 Figure 1: A Reddit conversation snippet about buying a Scuf controller. The quotation is in blue and italic. [T 1] to [T 4] are history turns while [Q] is for query turn. Introduction Quotations, or quotes, are memorable phrases or sentences widely echoed to spread patterns of wisdom (Booten and Hearst, 2016). They are derived from the ancient art of rhetoric and now appearing in various daily activities, ranging from formal writings (Tan et al., 2015) to everyday conversations (Lee et al., 2016), all help us present clear, beautiful, and persuasive language. However, for many individuals, writing a suitable quotation that fits the ongoing contexts is a daunting task. The issue becomes more pressing for quoting in online conversations where quick responses are usually needed on mobile devices (Lee et al., 2016). To help online users find what to quote in the discussions they are involved in, our w"
2020.emnlp-main.538,D14-1179,0,0.00821638,"Missing"
2020.emnlp-main.538,D15-1259,1,0.900933,"Missing"
2020.emnlp-main.538,P19-1210,0,0.021157,"er quotations as discrete attributes (for learning to rank) and hence largely ignore the rich information reflected by a quotation’s internal word patterns. Compared with them, our model learns to quote with language generation, which can usefully exploit how words appear in both contexts and quotations. For methodology, we are inspired by the encoder-decoder neural language generation models (Sutskever et al., 2014; Bahdanau et al., 2014). In dialogue domains, such models have achieved huge success in digesting contexts and generate microblog hashtags (Wang et al., 2019b), meeting summaries (Li et al., 2019), dialogue responses (Hu et al., 2019), etc. Here we explore how the encoder-decoder architecture works to generate quotations in conversations, which has never been studied in existing work. Our study is also related to previous research to understand conversation contexts (Ma et al., 2018; Liu and Chen, 2019; Sun et al., 2019), where it is shown to be useful to capture interaction structures (Liu and Chen, 2019) and latent topics (Zeng et al., 2019). For latent topics, we are benefited from the recent advance of neural topic models (Miao et al., 2017; Wang et al., 2019a)), which allows end-t"
2020.emnlp-main.538,W04-1013,0,0.0150983,"t consistency. Finally, Section 5.3 presents more analysis to characterize quotations in online conversations. 5.1 Evaluation Metrics. We first adopt recommendation metrics with popular information retrieval metrics Precision at K (P@K) and mean average precision (MAP) scores (Sch¨utze et al., 2008) used. For P@K, K=1 to measure the top prediction, while for MAP we consider the top 5 outputs. Here we measure the generation models with their predictions after quotation matching (Section 3.3). Then, generation metrics are employed to evaluate word-level predictions. Here we consider both ROUGE (Lin, 2004) from summarization (F1 scores of ROUGE-1 and ROUGE-L are adopted) and BLEU (Papineni et al., 2002) from translation. To allow comparable results, generation models are measured with their original outputs (without quotation matching) while for ranking competitors, we take their top-1 ranked quotes. Comparisons. We first adopt two weak baselines that select quotations unaware of the target conversation: 1) R ANDOM: selecting quotations randomly; 2) F REQUENCY: ranking quotations with frequency. Then, we compared two ranking baselines: 3) non-neural learning to rank model (henceforth LTR) with"
2020.emnlp-main.538,P19-1552,0,0.128341,"turns in history to which the query responds (e.g., T 1 and T 4 in Figure 1) and guide the 6640 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6640–6650, c November 16–20, 2020. 2020 Association for Computational Linguistics quote to follow such interaction. Query consistency measures the language coherence of quote in continuing the story started by the query. For example, the quote in Figure 1 is to support the query’s argument. In previous work of quotation recommendation, there are many methods designed for formal writings (Tan et al., 2015; Liu et al., 2019); whereas much fewer efforts are made for online conversations with informal language and complex interactions in their contexts. Lee et al. (2016) use a ranking model to recommend quotes for Twitter conversations. Different from them, we attempt to generate quotations in a word-by-word manner, which allows the semantic consistency of quotes and contexts to be explored. Concretely, we propose a neural encoderdecoder framework to predict a quotation that continues the given conversation contexts. We capture topic consistency with latent topics (i.e., word distributions), which are learned by a"
2020.emnlp-main.538,P19-1543,0,0.0184425,"methodology, we are inspired by the encoder-decoder neural language generation models (Sutskever et al., 2014; Bahdanau et al., 2014). In dialogue domains, such models have achieved huge success in digesting contexts and generate microblog hashtags (Wang et al., 2019b), meeting summaries (Li et al., 2019), dialogue responses (Hu et al., 2019), etc. Here we explore how the encoder-decoder architecture works to generate quotations in conversations, which has never been studied in existing work. Our study is also related to previous research to understand conversation contexts (Ma et al., 2018; Liu and Chen, 2019; Sun et al., 2019), where it is shown to be useful to capture interaction structures (Liu and Chen, 2019) and latent topics (Zeng et al., 2019). For latent topics, we are benefited from the recent advance of neural topic models (Miao et al., 2017; Wang et al., 2019a)), which allows end-toend topic inference in neural architectures. Nevertheless, none of the above work attempts to study the semantic consistency of quotes in conversation contexts, which is a gap our work fills in. 3 Our Quotation Generation Model This section describes our neural encoder-decoder framework that generates quotati"
2020.emnlp-main.538,N18-1185,0,0.0212548,"d quotations. For methodology, we are inspired by the encoder-decoder neural language generation models (Sutskever et al., 2014; Bahdanau et al., 2014). In dialogue domains, such models have achieved huge success in digesting contexts and generate microblog hashtags (Wang et al., 2019b), meeting summaries (Li et al., 2019), dialogue responses (Hu et al., 2019), etc. Here we explore how the encoder-decoder architecture works to generate quotations in conversations, which has never been studied in existing work. Our study is also related to previous research to understand conversation contexts (Ma et al., 2018; Liu and Chen, 2019; Sun et al., 2019), where it is shown to be useful to capture interaction structures (Liu and Chen, 2019) and latent topics (Zeng et al., 2019). For latent topics, we are benefited from the recent advance of neural topic models (Miao et al., 2017; Wang et al., 2019a)), which allows end-toend topic inference in neural architectures. Nevertheless, none of the above work attempts to study the semantic consistency of quotes in conversation contexts, which is a gap our work fills in. 3 Our Quotation Generation Model This section describes our neural encoder-decoder framework th"
2020.emnlp-main.538,P02-1040,0,0.110668,"n online conversations. 5.1 Evaluation Metrics. We first adopt recommendation metrics with popular information retrieval metrics Precision at K (P@K) and mean average precision (MAP) scores (Sch¨utze et al., 2008) used. For P@K, K=1 to measure the top prediction, while for MAP we consider the top 5 outputs. Here we measure the generation models with their predictions after quotation matching (Section 3.3). Then, generation metrics are employed to evaluate word-level predictions. Here we consider both ROUGE (Lin, 2004) from summarization (F1 scores of ROUGE-1 and ROUGE-L are adopted) and BLEU (Papineni et al., 2002) from translation. To allow comparable results, generation models are measured with their original outputs (without quotation matching) while for ranking competitors, we take their top-1 ranked quotes. Comparisons. We first adopt two weak baselines that select quotations unaware of the target conversation: 1) R ANDOM: selecting quotations randomly; 2) F REQUENCY: ranking quotations with frequency. Then, we compared two ranking baselines: 3) non-neural learning to rank model (henceforth LTR) with handcrafted features proposed in Tan et al. (2015). 4) CNN-LSTM (Lee et al., 2016): previous quotat"
2020.emnlp-main.538,Q19-1014,0,0.0423485,"inspired by the encoder-decoder neural language generation models (Sutskever et al., 2014; Bahdanau et al., 2014). In dialogue domains, such models have achieved huge success in digesting contexts and generate microblog hashtags (Wang et al., 2019b), meeting summaries (Li et al., 2019), dialogue responses (Hu et al., 2019), etc. Here we explore how the encoder-decoder architecture works to generate quotations in conversations, which has never been studied in existing work. Our study is also related to previous research to understand conversation contexts (Ma et al., 2018; Liu and Chen, 2019; Sun et al., 2019), where it is shown to be useful to capture interaction structures (Liu and Chen, 2019) and latent topics (Zeng et al., 2019). For latent topics, we are benefited from the recent advance of neural topic models (Miao et al., 2017; Wang et al., 2019a)), which allows end-toend topic inference in neural architectures. Nevertheless, none of the above work attempts to study the semantic consistency of quotes in conversation contexts, which is a gap our work fills in. 3 Our Quotation Generation Model This section describes our neural encoder-decoder framework that generates quotations in conversation"
2020.emnlp-main.538,P19-1240,1,0.934786,"tency. To the best of our knowledge, we are the first to explore quotation generation in conversations and extensively study the effects of topic, interaction, and query consistency on this task. Our empirical study is conducted on two largescale datasets, one in Chinese from Weibo and the other in English from Reddit, both of which are constructed as part of this work. Experiment results show that our model significantly outperforms both the state-of-the-art model based on quote rankings (Lee et al., 2016) and the recent topic-aware encoder-decoder model for social media language generation (Wang et al., 2019a). For example, we achieve 27.2 precision@1 on Weibo compared with 24.0 by Wang et al. (2019a). Further discussions show that topic, interaction, and query consistency can all usefully indicate what to quote in online conversations. We also study how length of history and quotation affects the quoting results and find that we perform consistently better than comparison models in varying scenarios. 2 Related Work Our work is in the line with content-based recommendation (Liu et al., 2019) or cloze-style reading comprehension (Zheng et al., 2019), which learns to put suitable text fragments (e."
2020.emnlp-main.538,N19-1164,1,0.936524,"tency. To the best of our knowledge, we are the first to explore quotation generation in conversations and extensively study the effects of topic, interaction, and query consistency on this task. Our empirical study is conducted on two largescale datasets, one in Chinese from Weibo and the other in English from Reddit, both of which are constructed as part of this work. Experiment results show that our model significantly outperforms both the state-of-the-art model based on quote rankings (Lee et al., 2016) and the recent topic-aware encoder-decoder model for social media language generation (Wang et al., 2019a). For example, we achieve 27.2 precision@1 on Weibo compared with 24.0 by Wang et al. (2019a). Further discussions show that topic, interaction, and query consistency can all usefully indicate what to quote in online conversations. We also study how length of history and quotation affects the quoting results and find that we perform consistently better than comparison models in varying scenarios. 2 Related Work Our work is in the line with content-based recommendation (Liu et al., 2019) or cloze-style reading comprehension (Zheng et al., 2019), which learns to put suitable text fragments (e."
2020.emnlp-main.538,Q19-1017,1,0.813422,"gue domains, such models have achieved huge success in digesting contexts and generate microblog hashtags (Wang et al., 2019b), meeting summaries (Li et al., 2019), dialogue responses (Hu et al., 2019), etc. Here we explore how the encoder-decoder architecture works to generate quotations in conversations, which has never been studied in existing work. Our study is also related to previous research to understand conversation contexts (Ma et al., 2018; Liu and Chen, 2019; Sun et al., 2019), where it is shown to be useful to capture interaction structures (Liu and Chen, 2019) and latent topics (Zeng et al., 2019). For latent topics, we are benefited from the recent advance of neural topic models (Miao et al., 2017; Wang et al., 2019a)), which allows end-toend topic inference in neural architectures. Nevertheless, none of the above work attempts to study the semantic consistency of quotes in conversation contexts, which is a gap our work fills in. 3 Our Quotation Generation Model This section describes our neural encoder-decoder framework that generates quotations in conversations, whose architecture is shown in Figure 2. The encoding process works for context modeling of turn interactions (described i"
2020.emnlp-main.538,D18-1351,1,0.908405,"efforts are made for online conversations with informal language and complex interactions in their contexts. Lee et al. (2016) use a ranking model to recommend quotes for Twitter conversations. Different from them, we attempt to generate quotations in a word-by-word manner, which allows the semantic consistency of quotes and contexts to be explored. Concretely, we propose a neural encoderdecoder framework to predict a quotation that continues the given conversation contexts. We capture topic consistency with latent topics (i.e., word distributions), which are learned by a neural topic model (Zeng et al., 2018a) and inferred jointly with the other components. Interaction consistency is modeled with a turn-based attention over the history turns, and the query is additionally encoded to initialize the decoder’s states for query consistency. To the best of our knowledge, we are the first to explore quotation generation in conversations and extensively study the effects of topic, interaction, and query consistency on this task. Our empirical study is conducted on two largescale datasets, one in Chinese from Weibo and the other in English from Reddit, both of which are constructed as part of this work."
2020.emnlp-main.538,N18-1035,1,0.928004,"efforts are made for online conversations with informal language and complex interactions in their contexts. Lee et al. (2016) use a ranking model to recommend quotes for Twitter conversations. Different from them, we attempt to generate quotations in a word-by-word manner, which allows the semantic consistency of quotes and contexts to be explored. Concretely, we propose a neural encoderdecoder framework to predict a quotation that continues the given conversation contexts. We capture topic consistency with latent topics (i.e., word distributions), which are learned by a neural topic model (Zeng et al., 2018a) and inferred jointly with the other components. Interaction consistency is modeled with a turn-based attention over the history turns, and the query is additionally encoded to initialize the decoder’s states for query consistency. To the best of our knowledge, we are the first to explore quotation generation in conversations and extensively study the effects of topic, interaction, and query consistency on this task. Our empirical study is conducted on two largescale datasets, one in Chinese from Weibo and the other in English from Reddit, both of which are constructed as part of this work."
2020.emnlp-main.538,P19-1075,0,0.0237897,"Missing"
2020.semeval-1.47,N19-1423,0,0.153617,"19). But question-answering is hard to directly 1 All results before 29 April, 2020 *Corresponding author. This work is licensed under a Creative Commons Attribution 4.0 International License. http://creativecommons.org/licenses/by/4.0/ 391 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 391–400 Barcelona, Spain (Online), December 12, 2020. License details: evaluate the commonsense in contextualized representations. And there has been few work investigating commonsense in pre-trained language models (Zhou et al., 2019), such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). Introduced by (Wang et al., 2019a), sense-making is a task to tests whether a model can differentiate sense-making and non-sensemaking statements. Specifically, the statements typically differ only in one keyword which covers nouns, verbs, adjectives, and adverbs. There are two existing approaches that can address this problem, one simple way is to use more commonsense knowledge can be learned from larger training sets (Wang et al., 2019b). On the other hands, some works (Lin et al., 2019) focus on effectively utilizing external, structured commonsense knowledge graphs, such as ConceptNet (S"
2020.semeval-1.47,P17-1025,0,0.125874,"y, the statements typically differ only in one keyword which covers nouns, verbs, adjectives, and adverbs. There are two existing approaches that can address this problem, one simple way is to use more commonsense knowledge can be learned from larger training sets (Wang et al., 2019b). On the other hands, some works (Lin et al., 2019) focus on effectively utilizing external, structured commonsense knowledge graphs, such as ConceptNet (Speer et al., 2016) and COMET (Bosselut et al., 2019). Insipred by previous works, more researchers are trying to fuse commonsense knowledge and language model (Forbes and Choi, 2017), and apply them to downstream tasks (Zhong et al., 2019). Recently, a new hybrid approach has been proposed for common sense reasoning (He et al., 2019). The core idea behind it is multi-task learning (Liu et al., 2019b), which has been widely applied in natural language tasks (Liu et al., 2019a). But existing work in this area has been frustratingly slow, and much of the work is completely theoretical. The field might well benefit if commonsense argumentation were systematically described and evaluated. To tackle it, this system focuses on a benchmark to directly test whether a system can di"
2020.semeval-1.47,D19-6002,0,0.0210724,"his problem, one simple way is to use more commonsense knowledge can be learned from larger training sets (Wang et al., 2019b). On the other hands, some works (Lin et al., 2019) focus on effectively utilizing external, structured commonsense knowledge graphs, such as ConceptNet (Speer et al., 2016) and COMET (Bosselut et al., 2019). Insipred by previous works, more researchers are trying to fuse commonsense knowledge and language model (Forbes and Choi, 2017), and apply them to downstream tasks (Zhong et al., 2019). Recently, a new hybrid approach has been proposed for common sense reasoning (He et al., 2019). The core idea behind it is multi-task learning (Liu et al., 2019b), which has been widely applied in natural language tasks (Liu et al., 2019a). But existing work in this area has been frustratingly slow, and much of the work is completely theoretical. The field might well benefit if commonsense argumentation were systematically described and evaluated. To tackle it, this system focuses on a benchmark to directly test whether a system can differentiate sentences that make sense from those that do not make sense. Our results indicate that pre-trained models are not able to demonstrate well on"
2020.semeval-1.47,D19-1282,0,0.0127333,"nse in pre-trained language models (Zhou et al., 2019), such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). Introduced by (Wang et al., 2019a), sense-making is a task to tests whether a model can differentiate sense-making and non-sensemaking statements. Specifically, the statements typically differ only in one keyword which covers nouns, verbs, adjectives, and adverbs. There are two existing approaches that can address this problem, one simple way is to use more commonsense knowledge can be learned from larger training sets (Wang et al., 2019b). On the other hands, some works (Lin et al., 2019) focus on effectively utilizing external, structured commonsense knowledge graphs, such as ConceptNet (Speer et al., 2016) and COMET (Bosselut et al., 2019). Insipred by previous works, more researchers are trying to fuse commonsense knowledge and language model (Forbes and Choi, 2017), and apply them to downstream tasks (Zhong et al., 2019). Recently, a new hybrid approach has been proposed for common sense reasoning (He et al., 2019). The core idea behind it is multi-task learning (Liu et al., 2019b), which has been widely applied in natural language tasks (Liu et al., 2019a). But existing w"
2020.semeval-1.47,P19-1441,0,0.181377,"an be learned from larger training sets (Wang et al., 2019b). On the other hands, some works (Lin et al., 2019) focus on effectively utilizing external, structured commonsense knowledge graphs, such as ConceptNet (Speer et al., 2016) and COMET (Bosselut et al., 2019). Insipred by previous works, more researchers are trying to fuse commonsense knowledge and language model (Forbes and Choi, 2017), and apply them to downstream tasks (Zhong et al., 2019). Recently, a new hybrid approach has been proposed for common sense reasoning (He et al., 2019). The core idea behind it is multi-task learning (Liu et al., 2019b), which has been widely applied in natural language tasks (Liu et al., 2019a). But existing work in this area has been frustratingly slow, and much of the work is completely theoretical. The field might well benefit if commonsense argumentation were systematically described and evaluated. To tackle it, this system focuses on a benchmark to directly test whether a system can differentiate sentences that make sense from those that do not make sense. Our results indicate that pre-trained models are not able to demonstrate well on the benchmark, and some remaining cases demonstrating that human"
2020.semeval-1.47,N18-1202,0,0.0236459,"swering (QA) (Talmor et al., 2019). But question-answering is hard to directly 1 All results before 29 April, 2020 *Corresponding author. This work is licensed under a Creative Commons Attribution 4.0 International License. http://creativecommons.org/licenses/by/4.0/ 391 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 391–400 Barcelona, Spain (Online), December 12, 2020. License details: evaluate the commonsense in contextualized representations. And there has been few work investigating commonsense in pre-trained language models (Zhou et al., 2019), such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). Introduced by (Wang et al., 2019a), sense-making is a task to tests whether a model can differentiate sense-making and non-sensemaking statements. Specifically, the statements typically differ only in one keyword which covers nouns, verbs, adjectives, and adverbs. There are two existing approaches that can address this problem, one simple way is to use more commonsense knowledge can be learned from larger training sets (Wang et al., 2019b). On the other hands, some works (Lin et al., 2019) focus on effectively utilizing external, structured commonsense knowledg"
2020.semeval-1.47,P19-1487,0,0.0568371,". According to (Liu et al., 2019a), we set dropout rate ranged in {0.1, 0.2, 0.3}. To avoid the exploding gradient problem, we clipped the gradient norm within 1. We set the mixture ratio as 0.4 to re-weighting different tasks(Xu et al., 2018), more details can be found in the implementation 2 . 4.5 Subtask C Since this is a text generation problem, we choose to use the GPT model as our baseline. Since some of the samples use knowledge from subtask A, we conducted contrast experiments by using data from 2 The codes are available at: https://github.com/ruleGreen/myMTDNN 396 subtask A and CoS-E(Rajani et al., 2019). We observed that adding explanations led to a very small decrease in the performance compared to the baseline at test datasets, but adding data from subtask A improve about 0.3. Some generated explanations are showed in Table 5 below. Model GPT Corpus Task C + Task A + Aug Test 12.65 12.94 12.31 Dev 5.96 5.99 6.54 Table 4: Task C: CommonSense Explanation False Sent True Sent The inverter was able to power the house The inverter was able to power the house There are beautiful planes here and there in the garden There are beautiful flowers here and there in the garden The chef put extra lemons"
2020.semeval-1.47,2020.tacl-1.54,0,0.0160316,"ent downstream tasks(Devlin et al., 2019). It is an encoder based on multi-head attention with the self-attention mechanism in a fully connected layer. The input representation of BERT is constructed by summing the corresponding token, segment, and position embeddings. As an autoencoding (AE) model, It can capture the global context in both forward and backward directions. The pre-train of BERT uses two unsupervised strategies. 1) Masked LM; 2) Next Sentence Prediction (NSP). By optimizing for both of two tasks, BERT not only can learn semantic and synthetic knowledge but also world knowledge(Rogers et al., 2020). These explain why BERT has astonishing performance. RoBERTa is a extended study of BERT which showed that carefully tuning hyper-parameters and increase training data size lead to significantly improved results on language understanding. More specifically, (Liu et al., 2019c) proposed three methods to improve BERT 1) training the model longer, with bigger batches, over more data; 2) removing the next sentence prediction task; 3) training on longer sequences, and 4) dynamically changing the masking pattern applied to the training data. As same as other NLP tasks, RoBERTa gets more higher accu"
2020.semeval-1.47,N19-1421,0,0.0228901,"restrictions on such benchmarks. First, they do not provide a straight quantitatively standard to measure sense masking capability. Second, they do not explicitly identify the key factors required in a sense-making process. And also, they do not need the model to explain why it makes that prediction. Common sense reasoning require the agent or the model to utilize a world knowledge to take inferences or deep semantic understanding, not only the pattern recognition. Some empirical analysis has been done previously for common sense reasoning, mainly focus on the form of question answering (QA) (Talmor et al., 2019). But question-answering is hard to directly 1 All results before 29 April, 2020 *Corresponding author. This work is licensed under a Creative Commons Attribution 4.0 International License. http://creativecommons.org/licenses/by/4.0/ 391 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 391–400 Barcelona, Spain (Online), December 12, 2020. License details: evaluate the commonsense in contextualized representations. And there has been few work investigating commonsense in pre-trained language models (Zhou et al., 2019), such as ELMo (Peters et al., 2018) and BERT (Dev"
2020.semeval-1.47,P19-1393,0,0.156061,"o directly 1 All results before 29 April, 2020 *Corresponding author. This work is licensed under a Creative Commons Attribution 4.0 International License. http://creativecommons.org/licenses/by/4.0/ 391 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 391–400 Barcelona, Spain (Online), December 12, 2020. License details: evaluate the commonsense in contextualized representations. And there has been few work investigating commonsense in pre-trained language models (Zhou et al., 2019), such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). Introduced by (Wang et al., 2019a), sense-making is a task to tests whether a model can differentiate sense-making and non-sensemaking statements. Specifically, the statements typically differ only in one keyword which covers nouns, verbs, adjectives, and adverbs. There are two existing approaches that can address this problem, one simple way is to use more commonsense knowledge can be learned from larger training sets (Wang et al., 2019b). On the other hands, some works (Lin et al., 2019) focus on effectively utilizing external, structured commonsense knowledge graphs, such as ConceptNet (Speer et al., 2016) and COMET (Boss"
2020.semeval-1.47,2020.semeval-1.39,0,0.0290667,"are two similar sentences but only one of them makes sense are used in subtask A called Validation, For the against-common-sense statement s1 or s2 , we have three optional sentences o1 , o2 and o3 to explain why the statement contradicts common sense. The subtask B, named Explanation (Multi-Choice), requires that the model can identify the only correct reason from distractors. Finally, subtask C naming Explanation (Generation), asks the model to generate the reason why it does not make sense. Each statement is paired with three possible explanations r1 , r2 and r3 from different perspective(Wang et al., 2020). Subtask A: Unlike other classification problem, subtask A gives us two statements s1 , s2 which have similar wordings. Their dependency tree or semantic structure is extremely similar and that requires us to build a model which can recognize these subtle differences and reasoning to judge the sentence whether or not it makes sense. Subtask B: Subtask B gives us one false sentence sf (either s1 or s2 ) which means this sentence does not make sense and three options o1 , o2 , o3 . We need to choose one right option which can explain why the give sentence does not make sense. Subtask C: Subtask"
2020.semeval-1.47,D19-1016,0,0.0615949,"Missing"
2021.acl-short.95,P19-1552,0,0.0940079,"wever, it is a daunting task for many individuals to write down a suitable quotation in a short time. This results in a pressing need to develop a quotation recommendation tool to meet such a demand. To that end, extensive efforts have been made to quotation recommendation, which aims to recommend an ongoing conversation with a quotation whose sense continues with the existing context (Wang et al., 2020). As quotations are concise phrases or sentences to spread wisdom, which are always in figurative language and difficult to understand, they are assumed written in a different pseudo-language (Liu et al., 2019a). Intuitively, we The code is available at https://github.com/ Lingzhi-WANG/Quotation-Recommendation can infer the meanings of quotations by their neighborhood contexts, especially by the query turn (the last turn of conversation that needs recommendation). To illustrate our motivation, Figure 1 shows a Reddit conversation with some history queries associated with quotation Q, “A fool and his money are soon parted”. From the queries (t5 and h1 to h3 ), we can infer the meaning of quotation Q is “A foolish person spends money carelessly and won’t have a lot of money.” based on the contexts. F"
2021.acl-short.95,D14-1162,0,0.0860608,"tation: X Lmap = ||M rnc − rqqc ||22 (7) c∈C To train our model, the final objective is to minimize L, the combination of the two losses: L = Lrec + λ · Lmap (8) where λ are the coefficient determining the contribution of the latter loss. 4 Experimental Setup Datasets. We conduct experiments based on datasets from two different platforms, Weibo and Reddit, released by Wang et al. (2020). To make our experimental results comparable to Wang et al. (2020), we utilize their preprocessed data directly. Parameter Setting. We first initialize the embedding layer with 200-dimensional Glove embedding (Pennington et al., 2014) for Reddit and Chinese words embedding (Song et al., 2018) for Weibo. For transformer layers, we choose the number of layers and heads as (2, 3) for Reddit and (4, 4) for Weibo. For the hidden dimension of transformer layers and BiGRU layers (each direction), we set it to 200. We employ Adam optimizer (Kingma and Ba, 2015) with initial learning rate with 1e-4 and early stop adoption (Caruana et al., 2001) in training. The batch size is set to 32. Dropout strategy (Srivastava et al., 2014) and L2 regularization are used to alleviate overfitting. And the tradeoff parameter λ is chosen from {1e-"
2021.acl-short.95,N18-2028,0,0.014756,"the final objective is to minimize L, the combination of the two losses: L = Lrec + λ · Lmap (8) where λ are the coefficient determining the contribution of the latter loss. 4 Experimental Setup Datasets. We conduct experiments based on datasets from two different platforms, Weibo and Reddit, released by Wang et al. (2020). To make our experimental results comparable to Wang et al. (2020), we utilize their preprocessed data directly. Parameter Setting. We first initialize the embedding layer with 200-dimensional Glove embedding (Pennington et al., 2014) for Reddit and Chinese words embedding (Song et al., 2018) for Weibo. For transformer layers, we choose the number of layers and heads as (2, 3) for Reddit and (4, 4) for Weibo. For the hidden dimension of transformer layers and BiGRU layers (each direction), we set it to 200. We employ Adam optimizer (Kingma and Ba, 2015) with initial learning rate with 1e-4 and early stop adoption (Caruana et al., 2001) in training. The batch size is set to 32. Dropout strategy (Srivastava et al., 2014) and L2 regularization are used to alleviate overfitting. And the tradeoff parameter λ is chosen from {1e-4, 1e-3}. All the hyper-parameters above are tuned on the v"
2021.acl-short.95,2020.emnlp-main.538,1,0.925583,"t). Quotations to be recommended are in square brackets. Indicative words are on wavy-underline. Introduction Quotations are essential for successful persuasion and explanation in interpersonal communication. However, it is a daunting task for many individuals to write down a suitable quotation in a short time. This results in a pressing need to develop a quotation recommendation tool to meet such a demand. To that end, extensive efforts have been made to quotation recommendation, which aims to recommend an ongoing conversation with a quotation whose sense continues with the existing context (Wang et al., 2020). As quotations are concise phrases or sentences to spread wisdom, which are always in figurative language and difficult to understand, they are assumed written in a different pseudo-language (Liu et al., 2019a). Intuitively, we The code is available at https://github.com/ Lingzhi-WANG/Quotation-Recommendation can infer the meanings of quotations by their neighborhood contexts, especially by the query turn (the last turn of conversation that needs recommendation). To illustrate our motivation, Figure 1 shows a Reddit conversation with some history queries associated with quotation Q, “A fool a"
2021.emnlp-main.621,I17-1074,0,0.0605475,"Missing"
2021.emnlp-main.621,2020.findings-emnlp.316,0,0.0619331,"Missing"
2021.emnlp-main.621,W19-5912,0,0.0183421,"al., 2017; Rashid et al., 2018; Jhun- mulative rewards. Specifically, Agent1 aims jhunwala et al., 2020). Towards multi-agent task- to learn the domain policy πd that maximizes oriented dialog policy, a lot of progress is being the expected sum of rewardsP condition  on s d t made in modeling the interaction as a stochastic col- and a that Eπd ,st =s,ad =πd (st ) t γ rt , where t laborative game, where dialog agent and the user rt denotes the reward from the user at turn t, simulator are jointly optimized with their objec- and γ ∈ [0, 1] is a discount factor. Simitives (Liu and Lane, 2017; Papangelis et al., 2019; larly, the intent policy πi is trained P t to  maxiTakanobu et al., 2020). Building a user simulator in mize Eπi ,st =s,ad =ad ,ai =πi (st ||ad ) γ r t , while t t t t this way is more flexible. However, different from Agent3 tries to optimize πs that Pmaximizes  t existing frameworks, our multi-agent framework is Eπs ,st =s,ad =ad ,ai =ai ,as =πs (st ||ad ||ai ) t γ rt . t t t t t devoted to decompose concatenated actions in orAll policies can be learned with DQN (Mnih der to reduce the large action space size to improve et al., 2015). Concretely, the domain policy esthe performance of"
2021.emnlp-main.621,P18-1203,1,0.82228,"Missing"
2021.emnlp-main.621,D17-1237,1,0.929828,"concatenated actions may achieve acceptable performance in simple cases, however, continuously suffer from being laborious to engineers and struggled with edge cases in multi-domain or complex scenes. Another drawback of the centralized agent is its exponential growth in the observation and actions spaces with the growing number of domains (Lee et al., 2019b). To alleviate the problem of large user-agent interaction requirements caused by the large action space, a hierarchical reinforcement learning framework was proposed to learn the dialog policy that operates at different temporal scales (Peng et al., 2017). It has achieved promising results, however, is still up against some challenges. Firstly, their setting requires a rule-based critic to provide the intrinsic reward for the low-level agent. However, creating such a critic is not easy, especially in intricate scenarios. The man-made critic, somewhat inadvertently, may bias the convergent optimal. Moreover, the action space composed of intent and slot for the low-level agent can be still large, especially when there are a lot of intent types and slot names. Drawing the structural features of dialog actions, we address the above problems with a"
2021.emnlp-main.621,2020.acl-main.59,0,0.0371356,"Missing"
2021.emnlp-main.621,2020.acl-main.566,1,0.756239,"e agents is passed to the user. Many studies have been dedicated to optimizing 3.1 Multi-agent Dialog Policy dialog policy with reinforcement learning, most of which learn a centralized agent that maps the Specifically, Agent1 perceives the state s and learns observation to a joint action (Young et al., 2013; the domain policy πd that selects a domain category Su et al., 2016; Williams et al., 2017; Peng et al., ad ∈ Ad . Meanwhile, Agent2 equipped with the 2018a,b; Lipton et al., 2018; Li et al., 2020a; Zhu intent policy πi , takes as input the state s and the et al., 2020; Li et al., 2020b; Wang et al., 2020). selected domain ad , and decides the intent type For more efficient exploration, (Peng et al., 2017) ai ∈ Ai . Then, Agent3 receives s, ad and ai , and factor the centralized spaces into hierarchical rein- determines the slot names as ∈ As based on the forcement learning paradigms. slot policy πs . Where Ad , Ai , and As are the sets Meanwhile, cooperative multi-agent reinforce- of all possible domain names, intent types, and slot ment learning methods have started moving from names, respectively. tabular methods to deep learning methods and Naturally, we aim to simultaneously optimize are w"
2021.emnlp-main.621,P17-1062,0,0.0785334,"Missing"
2021.emnlp-main.621,2020.acl-demos.19,0,0.0158345,"mization, the agents do not experience unexpected changes in the environment because different agents can exchange policy information through the shared hidden layers φ. X L(θd,i,s ; φ) = L(θk ; φ) (7) i 0 = r + γ max Qθi0 (s ||(a ) , (a ) ) A detailed summary of the learning algorithm of the collaborative multi-agent reinforcement learning for dialog policy based on joint optimization and independent experience replay buffer (JOIE) is provided in Algorithm 1 in Appendix D. 4 Experiments Comparison is on MultiWoz (Budzianowski et al., 2018) with a public available agenda-based user simulator (Zhu et al., 2020). The detail of the user simulator and implementation is in Appendix B, C. We first evaluate 2-agent based models that factor the centralized spaces into two subspaces of the domain and joint intent-slot on 3 different domains sizes of 2, 4, and 7 on MultiWoz. Then we compare 3-agent based models that decompose the action spaces into three subspaces of the domain, intent, and slot. The dataset contains 7 domains, 13 intents, and 28 slots totally. Details of the dataset are provided in Appendix A. 4.1 Baseline Agents We compare JOIE with DQN, Hierarchical DQN (H-DQN), and two multi-agent RL age"
2021.findings-emnlp.183,P14-1005,0,0.0693568,"Missing"
2021.findings-emnlp.183,D14-1179,0,0.0245997,"Missing"
2021.findings-emnlp.183,2020.acl-main.700,0,0.0108969,"tion (Zeng sionary, it has repeated target user “A”, which leads to a higher re-entry rate than the other two ex- et al., 2019; Backstrom et al., 2013; Budak and Agrawal, 2013) aims to forecast whether the users pansionary threads. Therefore, we can conclude will return to a discussion they once entered that both Spread Pattern and Repeated Target User signals help predict re-entry behavior. Further- and Zeng et al. (2019) achieves state-of-the-art more, since more challenging tasks get better per- performance by exploiting user’s history context formance (Mao, 2020), we propose Turn Author- (Flek, 2020). Re-entry prediction focuses on conversation-level response prediction (Zeng et al., ship Prediction, where we predict whether each 2018; Chen et al., 2011). Most of them adopt a turn’s author is a target user or not. complex framework (Zeng et al., 2018) and masBefore the introduction of pretraining technique sive parameters (see Figure 4(b)) while our model (Peters et al., 2018; Devlin et al., 2018; Radford is simple and effectively combines the current conet al., 2019), researchers focused on developing versation and chatting history. complex models (Lu and Ng, 2020), such as key phrase ge"
2021.findings-emnlp.183,D15-1259,1,0.817089,"imental Setup (10) Datasets. For experiments, we construct two new datasets from Twitter and Reddit. The raw Twitter and Reddit data is released by Zeng et al. (2018, By combining the intention of the SP and RT 2019) and both in English. For both Twitter and tasks, we further design Turn Authorship PredicReddit, we form the conversations with postings tion (henceforth TA) task. The TA task aims to and replies (all the comments and replies also predict whether the turn’s author is the target user viewed as a single turn) following the practice in and we label &quot;yes&quot; with 1 and &quot;no&quot; with 0. This Li et al. (2015) and Zeng et al. (2018). task benefits the main task by signaling both the In our main experiment, different from Zeng conversation spread pattern and repeated user patet al. (2019), we do not focus on predicting first retern. Specifically, this is a turn-level authorship entries (i.e. only giving the context until the target prediction and can help learn meaningful turn repuser’s first participation), we generalize the setting resentations, which are essential for conversation into re-entry prediction regardless of the number modeling. of user’s past participation. In this way, our model Form"
2021.findings-emnlp.183,D19-1682,0,0.0276346,"China 4 Microsoft Corporation, Beijing, China 1,2 {lzwang,kfwong}@se.cuhk.edu.hk 3 zeng.xingshan@huawei.com 4 {huahu,djiang}@microsoft.com Abstract target user) will come back to a conversation they once participated in. Nevertheless, the state-of-theIn recent years, world business in online disart work (Zeng et al., 2019) mostly focuses on rich cussions and opinion sharing on social media information in users’ previous chatting history and is booming. Re-entry prediction task is thus ignores the thread pattern information (Backstrom proposed to help people keep track of the diset al., 2013; Tan et al., 2019). To this end, we study cussions which they wish to continue. Neverin re-entry prediction by exploiting the conversatheless, existing works only focus on exploiting chatting history and context information, tion thread pattern to signal whether a user would and ignore the potential useful learning sigcome back since the degree of repeated engagenals underlying conversation data, such as conment of users can indicate their temporary interests versation thread patterns and repeated engagein the ongoing conversation. ment of target users, which help better unSelf-supervised learning aims to train"
2021.findings-emnlp.183,W02-0109,0,0.179999,"that “AB”, “ABA” and “ABC” are the most frequent patterns. And re-entry rate for focused conversations (i.e. only two users participate, such as “AB” and “ABAB”) is generally higher than expansionary conversations, since prior contributions in one conversation may result in continued participation. Such a phenomenon verifies our motivation to design self-supervised tasks. Preprocessing. We applied the Glove tweet preprocessing toolkit (Pennington et al., 2014) to the Twitter dataset. As for the Reddit dataset, we first tokenized the words with the open-source natural language toolkit (NLTK) (Loper and Bird, 2002). We then removed all the non-alphabetic tokens and replaced links with the generic tag “URL”. For both datasets, a vocabulary was maintained with all the remaining tokens, including emoticons and punctuation marks. Parameter Setting. For the parameters in the main model, we first initialize the embedding layer with 200-dimensional Glove embedding (Pennington et al., 2014), whose Twitter version is used for the Twitter dataset and Common Crawl version is applied to Reddit2 . For our BiGRU layers, we set the size of hidden states for each direction to 200. We employ Adam optimizer (Kingma and B"
2021.findings-emnlp.183,P19-1214,0,0.0148786,"(Chen model by initializing the beginning hidden state of and Wang, 2019) and dialogue learning (Wu et al., the target turn. The main model is jointly trained 2019). These auxiliary tasks can be categorized with the three self-supervised tasks in the manner into word-level tasks and sentence-level tasks. 2128 In word-level tasks, nearby word prediction (Mikolov et al., 2013) and next word prediction (Bengio et al., 2003; Wang and Gupta, 2015) are widely explored in language modeling. Masked language model (Devlin et al., 2018) is also in the line of word-level tasks. In sentence-level tasks, Wang et al. (2019a) exploits Mask, Replace and Switch for extractive summarization. Wu et al. (2019) propose Inconsistent Order Detection for dialogue learning. Xie et al. (2020) exploit Drop, Replace, and TOV (Temporal Order Verification) for story cloze test. Xu et al. (2020) also design several self-supervised tasks to improve the performance of response selection. Most of the previous self-supervised tasks (both in word-level and sentence-level) focus on the general domain while our work is based on taskorientated supervised methods and achieves better performance. 3 Re-entry Prediction Framework This sect"
2021.findings-emnlp.183,Q15-1029,0,0.0113179,"s a target user or not. complex framework (Zeng et al., 2018) and masBefore the introduction of pretraining technique sive parameters (see Figure 4(b)) while our model (Peters et al., 2018; Devlin et al., 2018; Radford is simple and effectively combines the current conet al., 2019), researchers focused on developing versation and chatting history. complex models (Lu and Ng, 2020), such as key phrase generation with neural topic model (Wang Self-supervised Learning. Self-supervised et al., 2019b) and structured models for coreference learning aims to train a network on an auxiliary resolution (Martschat and Strube, 2015; Björkelund task where the ground-truth label is automatically and Kuhn, 2014). Thus models are time-consuming derived from the data itself (Wu et al., 2019; Lan in training and testing. For this reason, we pro- et al., 2019; Erhan et al., 2010; Hinton et al., 2006). pose our compact main model, which consists of It has been applied to many tasks, such as text three parts, turn encoder, conversation encoder and classification (Yu and Jiang, 2016), neural machine prediction layer. In addition, the chatting history translation (Ruiter et al., 2019), multi-turn response information of the target"
2021.findings-emnlp.183,D14-1162,0,0.0851786,"-axis: re-entry rate for each user pattern. We also present the distribution of thread patterns with their re-entry rate for Reddit in Figure 3. It can be seen that “AB”, “ABA” and “ABC” are the most frequent patterns. And re-entry rate for focused conversations (i.e. only two users participate, such as “AB” and “ABAB”) is generally higher than expansionary conversations, since prior contributions in one conversation may result in continued participation. Such a phenomenon verifies our motivation to design self-supervised tasks. Preprocessing. We applied the Glove tweet preprocessing toolkit (Pennington et al., 2014) to the Twitter dataset. As for the Reddit dataset, we first tokenized the words with the open-source natural language toolkit (NLTK) (Loper and Bird, 2002). We then removed all the non-alphabetic tokens and replaced links with the generic tag “URL”. For both datasets, a vocabulary was maintained with all the remaining tokens, including emoticons and punctuation marks. Parameter Setting. For the parameters in the main model, we first initialize the embedding layer with 200-dimensional Glove embedding (Pennington et al., 2014), whose Twitter version is used for the Twitter dataset and Common Cr"
2021.findings-emnlp.183,N18-1202,0,0.0233286,"re-entry behavior. Further- and Zeng et al. (2019) achieves state-of-the-art more, since more challenging tasks get better per- performance by exploiting user’s history context formance (Mao, 2020), we propose Turn Author- (Flek, 2020). Re-entry prediction focuses on conversation-level response prediction (Zeng et al., ship Prediction, where we predict whether each 2018; Chen et al., 2011). Most of them adopt a turn’s author is a target user or not. complex framework (Zeng et al., 2018) and masBefore the introduction of pretraining technique sive parameters (see Figure 4(b)) while our model (Peters et al., 2018; Devlin et al., 2018; Radford is simple and effectively combines the current conet al., 2019), researchers focused on developing versation and chatting history. complex models (Lu and Ng, 2020), such as key phrase generation with neural topic model (Wang Self-supervised Learning. Self-supervised et al., 2019b) and structured models for coreference learning aims to train a network on an auxiliary resolution (Martschat and Strube, 2015; Björkelund task where the ground-truth label is automatically and Kuhn, 2014). Thus models are time-consuming derived from the data itself (Wu et al., 2019; Lan"
2021.findings-emnlp.183,P19-1178,0,0.0526187,"Missing"
2021.findings-emnlp.183,P19-1240,0,0.0160236,"(Chen model by initializing the beginning hidden state of and Wang, 2019) and dialogue learning (Wu et al., the target turn. The main model is jointly trained 2019). These auxiliary tasks can be categorized with the three self-supervised tasks in the manner into word-level tasks and sentence-level tasks. 2128 In word-level tasks, nearby word prediction (Mikolov et al., 2013) and next word prediction (Bengio et al., 2003; Wang and Gupta, 2015) are widely explored in language modeling. Masked language model (Devlin et al., 2018) is also in the line of word-level tasks. In sentence-level tasks, Wang et al. (2019a) exploits Mask, Replace and Switch for extractive summarization. Wu et al. (2019) propose Inconsistent Order Detection for dialogue learning. Xie et al. (2020) exploit Drop, Replace, and TOV (Temporal Order Verification) for story cloze test. Xu et al. (2020) also design several self-supervised tasks to improve the performance of response selection. Most of the previous self-supervised tasks (both in word-level and sentence-level) focus on the general domain while our work is based on taskorientated supervised methods and achieves better performance. 3 Re-entry Prediction Framework This sect"
2021.findings-emnlp.183,P19-1375,0,0.044043,"Missing"
2021.findings-emnlp.183,D16-1023,0,0.0287707,"d Learning. Self-supervised et al., 2019b) and structured models for coreference learning aims to train a network on an auxiliary resolution (Martschat and Strube, 2015; Björkelund task where the ground-truth label is automatically and Kuhn, 2014). Thus models are time-consuming derived from the data itself (Wu et al., 2019; Lan in training and testing. For this reason, we pro- et al., 2019; Erhan et al., 2010; Hinton et al., 2006). pose our compact main model, which consists of It has been applied to many tasks, such as text three parts, turn encoder, conversation encoder and classification (Yu and Jiang, 2016), neural machine prediction layer. In addition, the chatting history translation (Ruiter et al., 2019), multi-turn response information of the target user is also applied to our selection (Xu et al., 2020), summarization (Chen model by initializing the beginning hidden state of and Wang, 2019) and dialogue learning (Wu et al., the target turn. The main model is jointly trained 2019). These auxiliary tasks can be categorized with the three self-supervised tasks in the manner into word-level tasks and sentence-level tasks. 2128 In word-level tasks, nearby word prediction (Mikolov et al., 2013) a"
2021.findings-emnlp.183,N18-1035,1,0.900347,"conclude will return to a discussion they once entered that both Spread Pattern and Repeated Target User signals help predict re-entry behavior. Further- and Zeng et al. (2019) achieves state-of-the-art more, since more challenging tasks get better per- performance by exploiting user’s history context formance (Mao, 2020), we propose Turn Author- (Flek, 2020). Re-entry prediction focuses on conversation-level response prediction (Zeng et al., ship Prediction, where we predict whether each 2018; Chen et al., 2011). Most of them adopt a turn’s author is a target user or not. complex framework (Zeng et al., 2018) and masBefore the introduction of pretraining technique sive parameters (see Figure 4(b)) while our model (Peters et al., 2018; Devlin et al., 2018; Radford is simple and effectively combines the current conet al., 2019), researchers focused on developing versation and chatting history. complex models (Lu and Ng, 2020), such as key phrase generation with neural topic model (Wang Self-supervised Learning. Self-supervised et al., 2019b) and structured models for coreference learning aims to train a network on an auxiliary resolution (Martschat and Strube, 2015; Björkelund task where the ground-"
2021.findings-emnlp.183,P19-1270,1,0.374219,"for Online Conversations via Self-Supervised Learning Lingzhi Wang1,2 , Xingshan Zeng3 , Huang Hu4 , Kam-Fai Wong1,2 , Daxin Jiang4 1 The Chinese University of Hong Kong, Hong Kong, China 2 MoE Key Laboratory of High Confidence Software Technologies, China 3 Huawei Noah’s Ark Lab, Hong Kong, China 4 Microsoft Corporation, Beijing, China 1,2 {lzwang,kfwong}@se.cuhk.edu.hk 3 zeng.xingshan@huawei.com 4 {huahu,djiang}@microsoft.com Abstract target user) will come back to a conversation they once participated in. Nevertheless, the state-of-theIn recent years, world business in online disart work (Zeng et al., 2019) mostly focuses on rich cussions and opinion sharing on social media information in users’ previous chatting history and is booming. Re-entry prediction task is thus ignores the thread pattern information (Backstrom proposed to help people keep track of the diset al., 2013; Tan et al., 2019). To this end, we study cussions which they wish to continue. Neverin re-entry prediction by exploiting the conversatheless, existing works only focus on exploiting chatting history and context information, tion thread pattern to signal whether a user would and ignore the potential useful learning sigcome b"
2021.findings-emnlp.5,2020.acl-main.269,0,0.0135177,") to capture deep interaction of user browsed news. These works formulated user history as an ordered linear sequence of news, to which recurrent or attention models were applied without modeling the structural correlation of user browsed news. Hu et al. (2020) formulated news and user jointly with a bipartite graph and disentangled user preferences with routing mechanism, which however implicitly relied on the manually-set latent preference factor. 3 −c t ; ← −t mt = [→ N c 1] To facilitate semantic interaction between title and content, we design a gated cross-selective network, inspired by Geng et al. (2020). Concretely, we utilize the semantic memory vector mc(t) to perform feature recalibration (Hu et al., 2018) on the sequential features {ht(c) } by a sigmoid gate function. The motivation behind this gate function is to utilize the memory vector of content (title) mc(t) to cross-select important semantic information from t(c) the i-th title (content) sequential feature hi . Methodology Our model is composed of the Collaborative News Encoding (CNE) module presented in Section 3.1 and Structural User Encoding (SUE) module presented in Section 3.2. CNE and SUE extract representations of candidate"
2021.findings-emnlp.5,2020.acl-main.392,0,0.041786,"imestamp. An et al. (2019) utilized RNN to learn short-term user representations from the browsing history, combined with long-term user embeddings. Various attention networks are also widely used to attend to important news in user history (Wu et al., 2019a,b; Zhu et al., 2019). Wu et al. (2019c) employed multi-head self-attention (Vaswani et al., 2017) to capture deep interaction of user browsed news. These works formulated user history as an ordered linear sequence of news, to which recurrent or attention models were applied without modeling the structural correlation of user browsed news. Hu et al. (2020) formulated news and user jointly with a bipartite graph and disentangled user preferences with routing mechanism, which however implicitly relied on the manually-set latent preference factor. 3 −c t ; ← −t mt = [→ N c 1] To facilitate semantic interaction between title and content, we design a gated cross-selective network, inspired by Geng et al. (2020). Concretely, we utilize the semantic memory vector mc(t) to perform feature recalibration (Hu et al., 2018) on the sequential features {ht(c) } by a sigmoid gate function. The motivation behind this gate function is to utilize the memory vect"
2021.findings-emnlp.5,P19-1033,0,0.330826,"s for a short period, CF-based methods suffer from severe coldstart problem. To tackle this, content-based methods used handcrafted features to encode news and users (Li et al., 2010; Son et al., 2013; Bansal et al., 2015). In recent years, deep neural models have achieved superior performance in news recommendation. Many studies pinpointed that this improvement came from the fine-grained news and user representations, which were extracted by deep neural networks (Wu et al., 2019a,c; Wang et al., 2020). For news representation learning, existing works used convolutional neural networks (CNN) (An et al., 2019), knowledge-aware CNN (Wang et al., 2018), personalized attention networks (Wu et al., 2019b), and multi-head self-attention networks (Wu et al., 2019c) to extract features from news title text as news representations. Zhu et al. (2019) employed parallel CNNs to encode news title and content respectively and then concatenated them into a unified representation. Wu et al. (2019a) encoded news title and content separately and incorporated them with multi-view attention. For user representation learning, Okura et al. (2) Encoding the user-interest-news correlation with hierarchical cluster-struct"
2021.findings-emnlp.5,D14-1162,0,0.087522,"Missing"
2021.findings-emnlp.5,W19-2403,0,0.02392,"nsilverbullet/NNR 46 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 46–55 November 7–11, 2021. ©2021 Association for Computational Linguistics empirically better than content-encoding (Wu et al., 2020). This can be attributed to the crucial information that the human-summarized title naturally represents; (ii) News titles are always subjective and rhetorical to attract potential readers. This leads to a severe textual data sparsity problem. News titles with unseen terminology, metaphor and ambiguity make it difficult to comprehend news with limited title wording (Shree et al., 2019). For example in Figure 1(b), the word “curse” is a metaphor, which cannot be resolved by the training corpus or the title itself, due to its unique semantic occurrence. News encoders must turn to the content to interpret the semantics of the word “curse” (i.e., “has never made the playoffs” and “mean Alabama is toast”). However, news encoders proposed by previous works either extract features solely from the title, or encode title and content separately, then perform concatenation or attention fusion on them (Zhu et al., 2019; Wu et al., 2019a). Such separate encodings of title and content wi"
2021.findings-emnlp.5,D19-1671,0,0.363164,"to comprehend news with limited title wording (Shree et al., 2019). For example in Figure 1(b), the word “curse” is a metaphor, which cannot be resolved by the training corpus or the title itself, due to its unique semantic occurrence. News encoders must turn to the content to interpret the semantics of the word “curse” (i.e., “has never made the playoffs” and “mean Alabama is toast”). However, news encoders proposed by previous works either extract features solely from the title, or encode title and content separately, then perform concatenation or attention fusion on them (Zhu et al., 2019; Wu et al., 2019a). Such separate encodings of title and content without leveraging their semantic interaction are inadequate for news text comprehension. hierarchical cluster-structure is more precise to represent the correlation of news and user interests. To address the above issues, in this work, we propose collaborative news encoding (CNE) and structural user encoding (SUE) to learn semanticinteractive news representations and hierarchical user representations. We conduct experiments on the MIND dataset (Wu et al., 2020), showing the encoding effectiveness of our proposed model. Experiments and further a"
2021.findings-emnlp.5,2020.acl-main.331,0,0.209669,"news recommendation system should push relevant news to users to satisfy their diverse personalized interests (IJntema et al., 2010). From the perspective of representation learning (Bengio et al., 2013), existing works mainly study how to effectively encode news and users into discriminative representations (Okura et al., 2017; Wu 1 Our code is released at https://github.com/Veasonsilverbullet/NNR 46 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 46–55 November 7–11, 2021. ©2021 Association for Computational Linguistics empirically better than content-encoding (Wu et al., 2020). This can be attributed to the crucial information that the human-summarized title naturally represents; (ii) News titles are always subjective and rhetorical to attract potential readers. This leads to a severe textual data sparsity problem. News titles with unseen terminology, metaphor and ambiguity make it difficult to comprehend news with limited title wording (Shree et al., 2019). For example in Figure 1(b), the word “curse” is a metaphor, which cannot be resolved by the training corpus or the title itself, due to its unique semantic occurrence. News encoders must turn to the content to"
2021.findings-emnlp.5,2020.acl-main.77,0,0.490956,"ork? (a) A user’s browsing history (sorted by click timestamp) An example news of MIND dataset Title Distillation Curse of the No. 3 Seed in the Initial CFP Rankings: How Would That Work? Interpretation ff Content The team ranked third in the initial College Football Playo rankings has never made the playo s. Does that mean Alabama is toast this year? We take a look at the trend and ask How Would That Work? … N6 (b) Semantic interaction between news title and content ff Figure 1: (a) An example of user browsing history. (b) An example of news title-content semantic interaction. et al., 2019a; Wang et al., 2020). News encoders typically extract semantic representations of news from the textual spans (e.g., news title and content). User encoders are employed to learn the representation of a user from her browsing history. News recommendation models predict the matching probabilities between candidate news and users by measuring the similarity of their representations. Existing news recommendation models typically encode news title and content separately and encode users’ browsing histories without explicit structural modeling. We argue that these encodings restrict the power of the news and user repre"
2021.naacl-main.27,D18-1547,0,0.0437512,"Missing"
2021.naacl-main.27,P19-1546,0,0.0301697,"Missing"
2021.naacl-main.27,K19-1036,1,0.826738,"nd, since show an illustrated example of a multi-domain dithe model is conditioned on domain-slot pairs, the alogue in Figure 1, which involves two domains, computational complexity is not constant and will i.e., TRAIN and HOTEL. grow as the number of domains and slots involved Previous approaches for DST usually fall into in dialogues increases. To be more specific, if there the following four categories: (1) adopt encoderare 1000 domain-slot pairs, the model needs to run decoder models to generates states (Kim et al., 1000 times to obtain the expected dialogue states 2020; Ren et al., 2019; Li et al., 2019; Lee et al., for the current turn at each time, which is a huge 2019; Wu et al., 2019) ; (2) cast DST as a multicomputational overhead. On the other hand, previlabel classification task when a full candidate-value ous works usually directly concatenate the history list is available (Shan et al., 2020; Ramadan et al., content and the current utterance as input, which 2018; Zhong et al., 2018; Ren et al., 2018); (3) is difficult to scale in the multi-turn scenarios, esemploy span-based methods to directly extract the pecially when the number of turns of a dialogue states (Chao and Lane, 2019; G"
2021.naacl-main.27,P19-1441,0,0.137424,"ter scalability, especially in tackling dialogues with multiple turns. Additionally, we employ a correction module to handle the changes of the states as the dialogue proceeds. 2 Our Proposed Model Formally, a multi-turn dialogue is represented as T = {(s1 , u1 , d1 ), (s2 , u2 , d2 ), · · · , (sn , un , dn )}, di ∈ D, where si , ui and di refer to the system utterance, the user utterance, and the domain at turn i, respectively2 , and D represents the set of all domains in the training dataset. The overall architecture of our model is shown in Figure 2. In our proposed model, we choose MTDNN (Liu et al., 2019), pretrained model which has the same architecture as BERT but trained on multiple GLUE tasks (Wang et al., 2019). MT-DNN has been shown to be a better contextual feature extractor for downstream NLP tasks. Given dialogue utterances as input, we represent the output of MTDNN as {H[CLS] , H1 , H2 , · · · , Hn }, where n is the length of the concatenation of the system and user utterances. As a sentence-level representation, H[CLS] is expected to encode the information of the whole input sequence (Devlin et al., 2019; Liu et al., 2019). Based on these contextual representations, we predict the d"
2021.naacl-main.27,N19-1423,0,0.00640656,"ecture of our model is shown in Figure 2. In our proposed model, we choose MTDNN (Liu et al., 2019), pretrained model which has the same architecture as BERT but trained on multiple GLUE tasks (Wang et al., 2019). MT-DNN has been shown to be a better contextual feature extractor for downstream NLP tasks. Given dialogue utterances as input, we represent the output of MTDNN as {H[CLS] , H1 , H2 , · · · , Hn }, where n is the length of the concatenation of the system and user utterances. As a sentence-level representation, H[CLS] is expected to encode the information of the whole input sequence (Devlin et al., 2019; Liu et al., 2019). Based on these contextual representations, we predict the domain (see §2.1) and belief 2 We assume that the turn-level utterances only contain one domain, and the Multiwoz 2.0 dataset we use in this paper also conforms to this assumption. states (see §2.2 and §2.3). Figure 1 shows a typical multi-domain dialogue example, from which we can observe that some slot values can be directly found from utterances (e.g. cambridge and london), while other slot values are implicit which are more challenging to discover, e.g., requiring classification to infer the values (e.g. interne"
2021.naacl-main.27,W19-5932,0,0.0121042,"9; Lee et al., for the current turn at each time, which is a huge 2019; Wu et al., 2019) ; (2) cast DST as a multicomputational overhead. On the other hand, previlabel classification task when a full candidate-value ous works usually directly concatenate the history list is available (Shan et al., 2020; Ramadan et al., content and the current utterance as input, which 2018; Zhong et al., 2018; Ren et al., 2018); (3) is difficult to scale in the multi-turn scenarios, esemploy span-based methods to directly extract the pecially when the number of turns of a dialogue states (Chao and Lane, 2019; Gao et al., 2019); is large. Furthermore, we observe that generative and (4) combine both classification-based and spanapproaches may generate some domain outlier1 based methods to jointly complete the dialogue triplets due to lack of domain constraints. state extraction (Zhang et al., 2019). To tackle these issues, we propose a fast and The most related work to ours is DS-DST (Zhang 1 et al., 2019), a joint model which highlights the We refer a predicted result as “domain outlier” when slot problem that using classification-based or span- types are out of the domain pertaining to current utterances. 289 Proce"
2021.naacl-main.27,W14-4337,0,0.0401363,"ifier is formulated as y p = sigmoid(W p [ˆ p; E(Dl ); H[CLS] ]) (7) Here each item in p is embedded using E(·) and pˆ is the embedding sum of the three items in p. During training, we use cross entropy loss for y d , c y , y s and y p , which are represented as Lyd , Lyc , Lys and Lyp , respectively. The loss for R (denoted as LR ) is defined as Kullback-Leibler (KL) divergence between Rreal and R (i.e, KL(Rreal ||R)). All parameters are jointly trained by minimizing the weighted-sum of five losses (α, β, γ, θ,  are hyper-parameters), Evaluation metrics We utilize joint goal accuracy (JGA) (Henderson et al., 2014) to evaluate the model performance. Joint goal accuracy is the accuracy of the dialogue state of each turn and a dialogue state is regarded as correct only if all the values of slots are correctly predicted. Implementation details The hyper-parameters of our model go as follows: both the embedding and the hidden size is 1024; we used a learning rate of 0.0001 with a gradient clip of 2.0, minibatch SGD with a batch size of 32, and Adam optimizer (Kingma and Ba, 2014) for 50 epoch training. We set a value of 1 to the five weighted hyper-parameters: α, β, γ, θ, . Metric # of multi-domain dialogs"
2021.naacl-main.27,2020.acl-main.53,0,0.0404908,"Missing"
2021.naacl-main.27,P18-2069,0,0.0637763,"Missing"
2021.naacl-main.27,D19-1196,0,0.0269783,"Missing"
2021.naacl-main.27,D18-1299,0,0.0373342,"Missing"
2021.naacl-main.27,2020.acl-main.563,0,0.0207149,"or DST usually fall into in dialogues increases. To be more specific, if there the following four categories: (1) adopt encoderare 1000 domain-slot pairs, the model needs to run decoder models to generates states (Kim et al., 1000 times to obtain the expected dialogue states 2020; Ren et al., 2019; Li et al., 2019; Lee et al., for the current turn at each time, which is a huge 2019; Wu et al., 2019) ; (2) cast DST as a multicomputational overhead. On the other hand, previlabel classification task when a full candidate-value ous works usually directly concatenate the history list is available (Shan et al., 2020; Ramadan et al., content and the current utterance as input, which 2018; Zhong et al., 2018; Ren et al., 2018); (3) is difficult to scale in the multi-turn scenarios, esemploy span-based methods to directly extract the pecially when the number of turns of a dialogue states (Chao and Lane, 2019; Gao et al., 2019); is large. Furthermore, we observe that generative and (4) combine both classification-based and spanapproaches may generate some domain outlier1 based methods to jointly complete the dialogue triplets due to lack of domain constraints. state extraction (Zhang et al., 2019). To tackle"
2021.naacl-main.27,P19-1078,0,0.0671877,"domain-slot pairs, the alogue in Figure 1, which involves two domains, computational complexity is not constant and will i.e., TRAIN and HOTEL. grow as the number of domains and slots involved Previous approaches for DST usually fall into in dialogues increases. To be more specific, if there the following four categories: (1) adopt encoderare 1000 domain-slot pairs, the model needs to run decoder models to generates states (Kim et al., 1000 times to obtain the expected dialogue states 2020; Ren et al., 2019; Li et al., 2019; Lee et al., for the current turn at each time, which is a huge 2019; Wu et al., 2019) ; (2) cast DST as a multicomputational overhead. On the other hand, previlabel classification task when a full candidate-value ous works usually directly concatenate the history list is available (Shan et al., 2020; Ramadan et al., content and the current utterance as input, which 2018; Zhong et al., 2018; Ren et al., 2018); (3) is difficult to scale in the multi-turn scenarios, esemploy span-based methods to directly extract the pecially when the number of turns of a dialogue states (Chao and Lane, 2019; Gao et al., 2019); is large. Furthermore, we observe that generative and (4) combine bot"
2021.naacl-main.27,P18-1135,0,0.0205964,"g four categories: (1) adopt encoderare 1000 domain-slot pairs, the model needs to run decoder models to generates states (Kim et al., 1000 times to obtain the expected dialogue states 2020; Ren et al., 2019; Li et al., 2019; Lee et al., for the current turn at each time, which is a huge 2019; Wu et al., 2019) ; (2) cast DST as a multicomputational overhead. On the other hand, previlabel classification task when a full candidate-value ous works usually directly concatenate the history list is available (Shan et al., 2020; Ramadan et al., content and the current utterance as input, which 2018; Zhong et al., 2018; Ren et al., 2018); (3) is difficult to scale in the multi-turn scenarios, esemploy span-based methods to directly extract the pecially when the number of turns of a dialogue states (Chao and Lane, 2019; Gao et al., 2019); is large. Furthermore, we observe that generative and (4) combine both classification-based and spanapproaches may generate some domain outlier1 based methods to jointly complete the dialogue triplets due to lack of domain constraints. state extraction (Zhang et al., 2019). To tackle these issues, we propose a fast and The most related work to ours is DS-DST (Zhang 1 et al."
C04-1101,P90-1016,0,0.178631,"le. 1 Introduction Temporal information describes changes and time of the changes. In a language, the time of an event may be specified explicitly, for example “ 他 们 在 1997 年解决了该市的交通问题 (They solved the traffic problem of the city in 1997)”; or it may be related to the time of another event, for example “修成立交 桥以后, 他们解决了该市的交通问题 (They solved the traffic problem of the city after the street bridge had been built”. Temporal reference describes how events relate to one another, which is essential to natural language processing (NLP). Its major applications cover syntactic structural disambiguation (Brent, 1990), information extraction and question answering (Li, 2002), language generation and machine translation (Dorr, 2002). Many researchers have attempted to characterize the nature of temporal reference in a discourse. Identifying temporal relations 1 between two events de1 The relations under examined include both intra-sentence and interWenjie Li Department of Computing The Hong Kong Polytechnic University, Hong Kong cswjli@comp.polyu.edu.hk Chunfa Yuan Department of Computer Science and Technology Tsinghua University, Beijing, China. cfyuan@tsinghua.edu.cn pends on a combination of information"
C04-1101,P91-1008,0,0.0849163,"Missing"
C04-1101,J88-2005,0,0.132459,"se structure regardless of the lexical words included in the sentence. Generally, constraints were used to support syntactic disambiguation (Brent, 1990) or to generate acceptable sentences (Dorr, 2002). In a given CTS, a past perfect clause should precede the event described by a simple past clause. However, the order of two events in CTS does not necessarily correspond to the order imposed by the interpretation of the connective (Dorr, 2002). Temporal/casual connective, such as “after”, “before” or “because”, can supply explicit information about the temporal ordering of events. Passonneau (Passonneau, 1988), Brent (Brent, 1990 and Sing (Sing, 1997) determined intra-sentential relations by accounting for temporal or causal connectives. Dorr and Gaasterland (Dorr, 2002), on the other hand, studied how to generate the sentences which reflect event temporal relations by selecting proper connecting words. However, temporal connectives can be ambiguous. For instance, a “when” clause permits many possible temporal relations. Several researchers have developed the models that incorporated aspectual types (such as those distinct from states, processes and events) to interpret temporal relations between c"
C04-1101,J00-4004,0,0.0410779,"Missing"
C04-1101,P94-1013,0,0.0219908,"be jointly affected by auxiliary words (e.g. 过 , were/was), trend verbs (起来, begin to), and so on. Obviously, it is not a simple task to map the combined effects of the thirteen linguistic features to the corresponding relations. Therefore, a machine learning approach is proposed, which investigates how these features contribute to the task and how they should be combined. 4 Combining Linguistic Features with Machine Learning Approach Previous efforts in corpus-based NLP have incorporated machine learning methods to coordinate multiple linguistic features, for example, in accent restoration (Yarowsky, 1994) and event classification (Siegel, 1998). Temporal relation determination can be modeled as a relation classification task. We formulate the thirteen temporal relations (see Figure 1) as the classes to be decided by a classifier. The classification process is to assign an event pair to one class according to their linguistic features. There existed numerous classification algorithms based upon supervised learning principle. One of the most effective classifiers is Bayesian Classifier, introduced by Duda Effect Not Applicable Tense Aspect Discourse Structure/Aspect Discourse Structure Tense/Asp"
C04-1101,J88-2003,0,\N,Missing
C04-1101,J88-2006,0,\N,Missing
C04-1101,E95-1035,0,\N,Missing
C08-1124,W97-0703,0,0.0174732,"ng centroid (Radev et al., 2004), signature terms (Lin and Hovy, 2000) and high frequency words (Nenkova e t al., 2006). Radev et al. (2004) defined centroid words as those whose average tf*idf score were higher than a threshold. Lin and Hovy (2000) identified signature terms that were strongly associated with documents based on statistics measures. Nenkova et al. (2006) later reported that high frequency words were crucial in reflecting the focus of the document. Bag of words is somewhat loose and omits structural information. Document structure is another possible feature for summarization. Barzilay and Elhadad (1997) constructed lexical chains and extracted strong chains in summaries. Marcu (1997) parsed documents as rhetorical trees and identified important sentences based on the trees. However, only moderate results were reported. On the other hand, Dejong (1978) represented documents using predefined templates. The procedure to create and fill the templates was time consuming and it was hard to adapt the method to different domains. Recently, semi-structure events (Filatovia and Hatzivassiloglou, 2004; Li et al., 2006; Wu, 2006) have been investigated by many researchers as they balanced document repre"
C08-1124,W04-3247,0,0.0136217,"uctures. They defined events as verbs (or action nouns) plus the associated named entities. For instance, given the sentence “Yasser Arafat on Tuesday accused the United States of threatening to kill PLO officials”, they first identified “accused”, “threatening” and “kill” as event terms; and “Yasser Arafat”, “United States”, “PLO” and “Tuesday” as event elements. Encouraging results based on events were reported for news stories. From another point of view, sentences in a document are somehow connected. Sentence relevance has been used as an alternative means to identify important sentences. Erkan and Radev (2004) and Yoshioka (2004) evaluate the relevance (similarity) between any two sentences first. Then a web analysis approach, PageRank, was used to select important sentences from a sentence map built on relevance. Promising results were reported. However, the combination of these features is not well studied. Wu et al. (2007) conducted preliminary research on this problem, but event features were not considered. Normally labeling procedure in supervised learning is very time consuming. Blum and Mitchell (1998) proposed co-training approach to exploit labeled and unlabeled data. Promising results we"
C08-1124,W04-1017,0,0.0250281,"Missing"
C08-1124,P04-1074,1,0.897441,"research on this problem, but event features were not considered. Normally labeling procedure in supervised learning is very time consuming. Blum and Mitchell (1998) proposed co-training approach to exploit labeled and unlabeled data. Promising results were reported from their experiments on web page classification. A number of successful studies emerged thereafter for other natural language processing tasks, such as text classification (Denis and Gilleron, 2003), noun phrase chunking (Pierce and Cardie, 2001), parsing (Sarkar, 2001) and reference or relation resolution (Muller et al., 2001; Li et al., 2004). To our knowledge, there is little research in the application of co-training techniques to extractive summarization. 3 The Framework for Extractive Summarization Extractive summarization can be regarded as a classification problem. Given the features of a sentence, a machine-learning based classification model will judge how likely the sentence is important. The classification model can be supervised or semi-supervised learning. Supervised approaches normally perform better, but require more labeled training data. SVMs perform well in many classification problems. Thus we employ it for super"
C08-1124,P06-1047,1,0.809104,"ly. Although the evaluation results are encouraging, supervised learning approach requires much labeled data. Therefore we investigate co-training by combining labeled and unlabeled data. Experiments show that this semisupervised learning approach achieves comparable performance to its supervised counterpart and saves about half of the labeling time cost. 1 Introduction 1 Automatic text summarization involves condensing a document or a document set to produce a human comprehensible summary. Two kinds of summarization approaches were suggested in the past, i.e., extractive (Radev et al., 2004; Li et al., 2006) and abstractive summarization (Dejong, 1978). The abstractive approaches typically need © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/). Some rights reserved. to “understand” and then paraphrase the salient concepts across documents. Due to the limitations in natural language processing technology, abstractive approaches are restricted to specific domains. In contrast, extractive approaches commonly select sentences that contain the most significant concepts in the documents. These approa"
C08-1124,C00-1072,0,0.00947415,"zation framework. Section 4 outlines the various sentence features and Section 5 describes supervised/semi-supervised learning approaches. Section 6 presents experiments and results. Finally, Section 7 concludes the paper. 2 Related Work Traditionally, features for summarization were studied separately. Radev et al. (2004) reported that position and length are useful surface features. They observed that sentences located at the document head most likely contained important information. Recently, content features were also well studied, including centroid (Radev et al., 2004), signature terms (Lin and Hovy, 2000) and high frequency words (Nenkova e t al., 2006). Radev et al. (2004) defined centroid words as those whose average tf*idf score were higher than a threshold. Lin and Hovy (2000) identified signature terms that were strongly associated with documents based on statistics measures. Nenkova et al. (2006) later reported that high frequency words were crucial in reflecting the focus of the document. Bag of words is somewhat loose and omits structural information. Document structure is another possible feature for summarization. Barzilay and Elhadad (1997) constructed lexical chains and extracted s"
C08-1124,N03-1020,0,0.043526,"ters of relevant documents and 308 documents in total. Each cluster deals with a specific topic (e.g. a hurricane) and comes with model summaries created by NIST assessors. 50, 100, 200 and 400 word summaries are provided. Twenty-five of the thirty document clusters are used as training data and the remaining five are used as testing. The training/testing configuration is same in experiments of supervised learning and semi-supervised learning, while the difference is that some sentences in training data are not tagged for semi-supervised learning. An automatic evaluation package, i.e., ROUGE (Lin and Hovy, 2003) is employed to evaluate the summarization performance. It compares machine-generated summaries with model summaries based on the overlap. Precision and recall measures are used to evaluate the classification performance. For comparison, we evaluate our approaches on DUC 2004 data set also. It contains 50 clusters of documents. Only 665-character summaries are given by assessors for each cluster. 6.1 Experiments on Supervised Learning Approach We use LibSVM3 as our classification model for SVM classifiers normally perform better. Types of features presented in previous section are evaluated in"
C08-1124,W01-0501,0,0.00800588,"rted. However, the combination of these features is not well studied. Wu et al. (2007) conducted preliminary research on this problem, but event features were not considered. Normally labeling procedure in supervised learning is very time consuming. Blum and Mitchell (1998) proposed co-training approach to exploit labeled and unlabeled data. Promising results were reported from their experiments on web page classification. A number of successful studies emerged thereafter for other natural language processing tasks, such as text classification (Denis and Gilleron, 2003), noun phrase chunking (Pierce and Cardie, 2001), parsing (Sarkar, 2001) and reference or relation resolution (Muller et al., 2001; Li et al., 2004). To our knowledge, there is little research in the application of co-training techniques to extractive summarization. 3 The Framework for Extractive Summarization Extractive summarization can be regarded as a classification problem. Given the features of a sentence, a machine-learning based classification model will judge how likely the sentence is important. The classification model can be supervised or semi-supervised learning. Supervised approaches normally perform better, but require more l"
C08-1124,radev-etal-2004-mead,0,0.0471075,"Missing"
C08-1124,N01-1023,0,0.00741574,"hese features is not well studied. Wu et al. (2007) conducted preliminary research on this problem, but event features were not considered. Normally labeling procedure in supervised learning is very time consuming. Blum and Mitchell (1998) proposed co-training approach to exploit labeled and unlabeled data. Promising results were reported from their experiments on web page classification. A number of successful studies emerged thereafter for other natural language processing tasks, such as text classification (Denis and Gilleron, 2003), noun phrase chunking (Pierce and Cardie, 2001), parsing (Sarkar, 2001) and reference or relation resolution (Muller et al., 2001; Li et al., 2004). To our knowledge, there is little research in the application of co-training techniques to extractive summarization. 3 The Framework for Extractive Summarization Extractive summarization can be regarded as a classification problem. Given the features of a sentence, a machine-learning based classification model will judge how likely the sentence is important. The classification model can be supervised or semi-supervised learning. Supervised approaches normally perform better, but require more labeled training data. SV"
C08-1124,P06-3007,1,0.728649,"structure is another possible feature for summarization. Barzilay and Elhadad (1997) constructed lexical chains and extracted strong chains in summaries. Marcu (1997) parsed documents as rhetorical trees and identified important sentences based on the trees. However, only moderate results were reported. On the other hand, Dejong (1978) represented documents using predefined templates. The procedure to create and fill the templates was time consuming and it was hard to adapt the method to different domains. Recently, semi-structure events (Filatovia and Hatzivassiloglou, 2004; Li et al., 2006; Wu, 2006) have been investigated by many researchers as they balanced document representation with words and structures. They defined events as verbs (or action nouns) plus the associated named entities. For instance, given the sentence “Yasser Arafat on Tuesday accused the United States of threatening to kill PLO officials”, they first identified “accused”, “threatening” and “kill” as event terms; and “Yasser Arafat”, “United States”, “PLO” and “Tuesday” as event elements. Encouraging results based on events were reported for news stories. From another point of view, sentences in a document are someho"
C08-1124,P97-1013,0,\N,Missing
C08-1124,P02-1045,0,\N,Missing
C12-2138,al-saif-markert-2010-leeds,0,0.186853,"Missing"
C12-2138,D11-1068,0,0.0230127,"Missing"
C12-2138,W01-1605,0,0.519818,"Missing"
C12-2138,C10-3004,0,0.0258654,"Missing"
C12-2138,W11-0401,0,0.0653223,"Missing"
C12-2138,miltsakaki-etal-2004-penn,0,0.159515,"Missing"
C12-2138,J03-1002,0,0.00537512,"Missing"
C12-2138,P09-2004,0,0.133057,"Missing"
C12-2138,C08-2022,0,0.0926301,"Missing"
C12-2138,prasad-etal-2008-penn,0,0.615867,"Missing"
C12-2138,I08-7010,0,0.1145,"Missing"
C12-2138,W05-0312,0,0.515718,"Missing"
C12-2138,D11-1015,1,0.868,"Missing"
C12-2138,P12-1008,0,0.376064,"Missing"
C16-2041,W15-3109,0,0.0307185,"se words and usage; (2) Identify the potential spelling errors; (3) Provide correction suggestions. To the best of our knowledge, there is no work related to automatically identify colloquial Cantonese. We do not aware any work on colloquialism in other language as well. For the work related to Chinese spelling error, (Lee et al., 2014) applied N-gram model and rule-based system to judge a sentence based on large number of data and experts knowledge. (Xie et al., 2015) builds a system using both N-gram model and Language model, and implements a dynamic programming to increase the efficiency. (Chang et al., 2015) implements a rule-base model and a linear regression model to tackle the task with the help of Chinese Orthographic Database. We observed that large training corpus is one of the key element for a reliable model (Tseng et al., 2015). Unfortunately, such setup is difficult to apply in our scenario because of the lack of Cantonese corpus. 2 System Description ACE has two main modules: Cantonese detector and spelling error detector. Here is an outline of ACE: (Step 1) Identify over-segment parts in a sentence; (Step 2) Apply the Cantonese detector to check if there is any colloquial Cantonese (b"
C16-2041,C14-2015,0,0.0199416,"he same but the position of the character “先” is different). In this paper, we proposed a system called ACE (Automatic Colloquialism and Spelling Error Detector) to deal with all the errors stated previously. In ACE, there are three functions: (1) Identify the colloquial Cantonese words and usage; (2) Identify the potential spelling errors; (3) Provide correction suggestions. To the best of our knowledge, there is no work related to automatically identify colloquial Cantonese. We do not aware any work on colloquialism in other language as well. For the work related to Chinese spelling error, (Lee et al., 2014) applied N-gram model and rule-based system to judge a sentence based on large number of data and experts knowledge. (Xie et al., 2015) builds a system using both N-gram model and Language model, and implements a dynamic programming to increase the efficiency. (Chang et al., 2015) implements a rule-base model and a linear regression model to tackle the task with the help of Chinese Orthographic Database. We observed that large training corpus is one of the key element for a reliable model (Tseng et al., 2015). Unfortunately, such setup is difficult to apply in our scenario because of the lack"
C16-2041,min-etal-2000-typographical,0,0.017651,"s on detecting colloquial Cantonese (a dialect of Chinese) at the current stage, it can be extended to detect other dialects. We chose Cantonese becauase it has many interesting properties, such as unique grammar system and huge colloquial terms, that turn the detection task extremely challenging. We conducted experiments using real data and synthetic data. The results indicated that ACE is highly reliable and effective. 1 Introduction In general, there are two kinds of writing errors, typographical error (a.k.a. spelling errors) and orthographic error (a.k.a. cognitive error) (Damerau, 1964; Min et al., 2000). Typographical error means incorrectly substituting a right character with a wrong one, whereas orthographic error happens during the process of cognition. For colloquialism, there are two kinds as well: colloquial word and colloquial usage. For example, the word “返⼯” (means “back to work”) is a colloquial Cantonese word. Its formal counterpart is “上班” (note: the characters of both words are completely different). On the other hand, the phrase “吃飯先” (go to dinner first), is a colloquial Cantonese usage and its formal counterpart is “先吃飯” (note: all characters in both words are the same but th"
C16-2041,W15-3106,0,0.0195495,"lloquialism in other language as well. For the work related to Chinese spelling error, (Lee et al., 2014) applied N-gram model and rule-based system to judge a sentence based on large number of data and experts knowledge. (Xie et al., 2015) builds a system using both N-gram model and Language model, and implements a dynamic programming to increase the efficiency. (Chang et al., 2015) implements a rule-base model and a linear regression model to tackle the task with the help of Chinese Orthographic Database. We observed that large training corpus is one of the key element for a reliable model (Tseng et al., 2015). Unfortunately, such setup is difficult to apply in our scenario because of the lack of Cantonese corpus. 2 System Description ACE has two main modules: Cantonese detector and spelling error detector. Here is an outline of ACE: (Step 1) Identify over-segment parts in a sentence; (Step 2) Apply the Cantonese detector to check if there is any colloquial Cantonese (both usages and words); (Step 3) Apply the This work is licenced under a Creative Commons Attribution 4.0 International Licence. Licence details: http: //creativecommons.org/licenses/by/4.0/ 194 Proceedings of COLING 2016, the 26th In"
C16-2041,W10-4107,0,0.2734,"ails: http: //creativecommons.org/licenses/by/4.0/ 194 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: System Demonstrations, pages 194–197, Osaka, Japan, December 11-17 2016. spelling error detector to check if there is any spelling error; (Step 4) Give correction suggestions for the errors detected in Step 2 and Step 3. In the followings we briefly describe the major elements within ACE. 2.1 Over-segment Parts It is well proven that after sentence segmentation, the over-segment parts is an effective indicator to indicate potential spelling errors (Wu et al., 2010). Consider: “現在簡介有關⾹港 電台數碼地⾯電視廣播法展概況”, and its segmentation result: “現在/簡介/有關/⾹港電台 /數 碼/地⾯/電視廣播/法/展/概況”. The spelling error is “法” (the 4th last character). The correct character is“發”. Note that the last four characters are segmented into three parts: “法/展/概 況”. If this sentence is written correctly as “... 發展概況”, then the segmentation result will become “.../發展/概況”. Hence two parts are resulted. By identifying the over-segment parts, we may have some cues if there is any potential spelling error. There are many different kinds of segmentation algorithms, such as HMM and Maximum Probability."
C16-2041,W15-3120,0,0.0224128,"d Spelling Error Detector) to deal with all the errors stated previously. In ACE, there are three functions: (1) Identify the colloquial Cantonese words and usage; (2) Identify the potential spelling errors; (3) Provide correction suggestions. To the best of our knowledge, there is no work related to automatically identify colloquial Cantonese. We do not aware any work on colloquialism in other language as well. For the work related to Chinese spelling error, (Lee et al., 2014) applied N-gram model and rule-based system to judge a sentence based on large number of data and experts knowledge. (Xie et al., 2015) builds a system using both N-gram model and Language model, and implements a dynamic programming to increase the efficiency. (Chang et al., 2015) implements a rule-base model and a linear regression model to tackle the task with the help of Chinese Orthographic Database. We observed that large training corpus is one of the key element for a reliable model (Tseng et al., 2015). Unfortunately, such setup is difficult to apply in our scenario because of the lack of Cantonese corpus. 2 System Description ACE has two main modules: Cantonese detector and spelling error detector. Here is an outline"
D11-1015,N07-1054,0,0.0288585,"Missing"
D11-1015,W01-1605,0,0.129796,"Missing"
D11-1015,C10-3004,0,0.0119562,"nding named entity tags and part-of-speech tags, respectively. (4) Added sentiment tag (P : Positive; N : Negative) to all sentiment words. By applying above rules, the SSRs for D1 and D2 would be: d1 : [PER V|Ja01 RB|Ka01 JJ|Ee14|P IN NN|Dk03]s , [PRP V|Ja01 DT JJ|Ga16|N NN|Ae13 ]n d2 : [PER V|Ja01 JJ|Ee14|P IN NN|Bp12]s , [PRP V|He15|N NN|Di10 NN|Dd08 ]n Refer to d1 and d2 , ""Boris"" could match ""John"" in SSRs because they were converted to ""PER"" and they all appeared at the beginning of discourse instances. ""Ja01"", ""Ee14"" etc. were semantic labels from Chinese synonym list extended version (Che et al., 2010). There were similar resources in other languages such as Wordnet(Fellbaum, 1998) in English. The next problem became how to start from current SSRs and generate new SSRs for recognizing discourse relations without cue phrases. 4.2 Mining common SSRs Recall assumption (2), in order to incorporate lexical, structural and semantic information for the similarity calculation of two SSRs holding the same discourse relation, three types of matches were defined for {(u, v)|u ∈ di[k] , v ∈ dj[k] , k = 1, 2}: (1)Full match: (i) u = v or (ii) u.pos = v.pos and u.semlabel=v.semlabel or (iii) u.pos=v.pos"
D11-1015,P09-1075,0,0.0161866,"odeled the scheme to improve opinion polarity classification. However, opinion frames was difficult to be implemented because the recognition of opinion target was very challenging in general text. Our work differs from their approaches in two key aspects: (1) we distinguished nucleus and satellite in discourse but opinion frames did not; (2) our method for discourse discovery was unsupervised while their method needed annotated data. Most research works about discourse classification were not related to sentiment analysis. Supervised discourse classification methods (Soricut and Marcu, 2003; Duverle and Prendinger, 2009) needed manually annotated data. Marcu and Echihabi (2002) presented an unsupervised method to recognize discourse relations held between arbitrary spans of text. They showed that lexical pairs extracted from massive amount of data can have a major impact on discourse classification. BlairGoldensohn et al. (2007) extended Marcu&apos;s work by using parameter opitimization, topic segmentation and syntactic parsing. However, syntactic parsers were usually costly and impractical when dealing with large scale of text. Thus, in additional to lexical features, we incorporated sequential and semantic info"
D11-1015,P02-1047,0,0.611789,"However, opinion frames was difficult to be implemented because the recognition of opinion target was very challenging in general text. Our work differs from their approaches in two key aspects: (1) we distinguished nucleus and satellite in discourse but opinion frames did not; (2) our method for discourse discovery was unsupervised while their method needed annotated data. Most research works about discourse classification were not related to sentiment analysis. Supervised discourse classification methods (Soricut and Marcu, 2003; Duverle and Prendinger, 2009) needed manually annotated data. Marcu and Echihabi (2002) presented an unsupervised method to recognize discourse relations held between arbitrary spans of text. They showed that lexical pairs extracted from massive amount of data can have a major impact on discourse classification. BlairGoldensohn et al. (2007) extended Marcu&apos;s work by using parameter opitimization, topic segmentation and syntactic parsing. However, syntactic parsers were usually costly and impractical when dealing with large scale of text. Thus, in additional to lexical features, we incorporated sequential and semantic information in proposed method for discourse relation classifi"
D11-1015,miltsakaki-etal-2004-penn,0,0.0931139,"2 means CUE2 Relation Table 1: Examples of cue phrases 4 Methods The proposed methods were based on two assumptions: (1) Cue-phrase-based patterns could be used to find limited number of high quality discourse instances; (2) discourse relations were determined by lexical, structural and semantic information between two segments. Cue-phrase-based patterns could find only limited number of discourse instances with high precision (Marcu and Echihabi, 2002). Therefore, we could not rely on cue-phrase-based patterns alone. Moreover, there was no annotated corpus similar to Penn Discourse TreeBank (Miltsakaki et al., 2004) in other languages such as Chinese. Thus, we proposed a language independent unsupervised method to identify discourse relations without cue phrases while maintaining relatively high precision. For each discourse relation, we started with several cuephrase-based patterns and collected a large number of discourse instances from raw corpus. Then, discourse instances were converted to semantic sequential representations (SSRs). Finally, an unsupervised method was adopted to generate, weigh and filter common SSRs without cue phrases. The mined common SSRs could be directly used in our SSR-based c"
D11-1015,W02-1011,0,0.0193578,"t the proposed methods could effectively recognize the defined discourse relations and achieve significant improvement in sentence-level polarity classification comparing to BPC. The remainder of this paper is organized as follows. Section 2 introduces the related work. Section 3 presents the discourse scheme with discourse constraints on polarity. Section 4 gives the detail of proposed method. Experimental results are reported and discussed in Section 5 and Section 6 concludes this paper. 2 Related Work Research on polarity classification were generally conducted on 4 levels: document-level (Pang et al., 2002), sentence-level (Riloff et al., 2003), phraselevel (Wilson et al., 2009) and feature-level (Hu and Liu, 2004; Xia et al., 2007). There was little research focusing on the automatic recognition of intra-sentence level discourse 163 relations for sentiment analysis in the literature. Polanyi and Zaenen (2006) argued that valence calculation is critically affected by discourse structure. Asher et al. (2008) proposed a shallow semantic representation using a feature structure and use five types of rhetorical relations to build a finegrained corpus for deep contextual sentiment analysis. Neverthel"
D11-1015,W03-0404,0,0.0376784,"vely recognize the defined discourse relations and achieve significant improvement in sentence-level polarity classification comparing to BPC. The remainder of this paper is organized as follows. Section 2 introduces the related work. Section 3 presents the discourse scheme with discourse constraints on polarity. Section 4 gives the detail of proposed method. Experimental results are reported and discussed in Section 5 and Section 6 concludes this paper. 2 Related Work Research on polarity classification were generally conducted on 4 levels: document-level (Pang et al., 2002), sentence-level (Riloff et al., 2003), phraselevel (Wilson et al., 2009) and feature-level (Hu and Liu, 2004; Xia et al., 2007). There was little research focusing on the automatic recognition of intra-sentence level discourse 163 relations for sentiment analysis in the literature. Polanyi and Zaenen (2006) argued that valence calculation is critically affected by discourse structure. Asher et al. (2008) proposed a shallow semantic representation using a feature structure and use five types of rhetorical relations to build a finegrained corpus for deep contextual sentiment analysis. Nevertheless, they did not propose a computatio"
D11-1015,sadamitsu-etal-2008-sentiment,0,0.0605118,"ructure. Asher et al. (2008) proposed a shallow semantic representation using a feature structure and use five types of rhetorical relations to build a finegrained corpus for deep contextual sentiment analysis. Nevertheless, they did not propose a computational model for their discourse scheme. Snyder and Barzilay (2007) combined an agreement model based on contrastive RST relations with a local aspect model to make a more informed overall decision for sentiment classification. Nonetheless, contrastive relations were only one type of discourse relations which may help polarity classification. Sadamitsu et al. (2008) modeled polarity reversal using HCRFs integrated with inter-sentence discourse structures. However, our work is on intrasentence level and our purpose is not to find polarity reversals but trying to adapt general discourse schemes (e.g., RST) to help determine the overall polarity of ambiguous sentences. The most closely related works were (Somasundaran et al., 2008) and (Somasundaran et al., 2009), which proposed opinion frames as a representation of discourse-level associations on dialogue and modeled the scheme to improve opinion polarity classification. However, opinion frames was difficu"
D11-1015,N07-1038,0,0.0149574,"d feature-level (Hu and Liu, 2004; Xia et al., 2007). There was little research focusing on the automatic recognition of intra-sentence level discourse 163 relations for sentiment analysis in the literature. Polanyi and Zaenen (2006) argued that valence calculation is critically affected by discourse structure. Asher et al. (2008) proposed a shallow semantic representation using a feature structure and use five types of rhetorical relations to build a finegrained corpus for deep contextual sentiment analysis. Nevertheless, they did not propose a computational model for their discourse scheme. Snyder and Barzilay (2007) combined an agreement model based on contrastive RST relations with a local aspect model to make a more informed overall decision for sentiment classification. Nonetheless, contrastive relations were only one type of discourse relations which may help polarity classification. Sadamitsu et al. (2008) modeled polarity reversal using HCRFs integrated with inter-sentence discourse structures. However, our work is on intrasentence level and our purpose is not to find polarity reversals but trying to adapt general discourse schemes (e.g., RST) to help determine the overall polarity of ambiguous sen"
D11-1015,C08-1101,0,0.328817,"gnized the defined discourse relations but also achieved significant improvement by integrating discourse information in sentence-level polarity classification. (a) [Although Fujimori was criticized by the international community]，[he was loved by the domestic population]， [because people hated the corrupted ruling class]. (儘管 國際間對藤森口誅筆伐，他在國內一直深受百姓愛 戴，原因是百姓對腐化的統治階級早就深惡痛絕。) 1 Introduction As an important task of sentiment analysis, polarity classification is critically affected by discourse structure (Polanyi and Zaenen, 2006). Previous research developed discourse schema (Asher et al., 2008) (Somasundaran et al., 2008) and proved that the utilization of discourse relations could improve the performance of polarity classification on dialogues (Somasundaran et al., 2009). However, cur1 Defined as ambiguous sentences in this paper Example (a) is a positive sentence holding a Contrast relation between first two segments and a Cause relation between last two segments. The polarity of ""criticized"", ""hated"" and ""corrupted"" are recognized as negative expressions while ""loved"" is recognized as a positive expression. Example (a) is difficult for existing polarity classification methods for two reasons: (1) the number"
D11-1015,D09-1018,0,0.124863,"cation. (a) [Although Fujimori was criticized by the international community]，[he was loved by the domestic population]， [because people hated the corrupted ruling class]. (儘管 國際間對藤森口誅筆伐，他在國內一直深受百姓愛 戴，原因是百姓對腐化的統治階級早就深惡痛絕。) 1 Introduction As an important task of sentiment analysis, polarity classification is critically affected by discourse structure (Polanyi and Zaenen, 2006). Previous research developed discourse schema (Asher et al., 2008) (Somasundaran et al., 2008) and proved that the utilization of discourse relations could improve the performance of polarity classification on dialogues (Somasundaran et al., 2009). However, cur1 Defined as ambiguous sentences in this paper Example (a) is a positive sentence holding a Contrast relation between first two segments and a Cause relation between last two segments. The polarity of ""criticized"", ""hated"" and ""corrupted"" are recognized as negative expressions while ""loved"" is recognized as a positive expression. Example (a) is difficult for existing polarity classification methods for two reasons: (1) the number of positive expressions is less than negative expressions; (2) the importance of each sentiment expression is unknown. However, consider Figure 1, if we"
D11-1015,N03-1030,0,0.0814805,"iations on dialogue and modeled the scheme to improve opinion polarity classification. However, opinion frames was difficult to be implemented because the recognition of opinion target was very challenging in general text. Our work differs from their approaches in two key aspects: (1) we distinguished nucleus and satellite in discourse but opinion frames did not; (2) our method for discourse discovery was unsupervised while their method needed annotated data. Most research works about discourse classification were not related to sentiment analysis. Supervised discourse classification methods (Soricut and Marcu, 2003; Duverle and Prendinger, 2009) needed manually annotated data. Marcu and Echihabi (2002) presented an unsupervised method to recognize discourse relations held between arbitrary spans of text. They showed that lexical pairs extracted from massive amount of data can have a major impact on discourse classification. BlairGoldensohn et al. (2007) extended Marcu&apos;s work by using parameter opitimization, topic segmentation and syntactic parsing. However, syntactic parsers were usually costly and impractical when dealing with large scale of text. Thus, in additional to lexical features, we incorporat"
D11-1015,J09-3003,0,0.0288966,"e relations and achieve significant improvement in sentence-level polarity classification comparing to BPC. The remainder of this paper is organized as follows. Section 2 introduces the related work. Section 3 presents the discourse scheme with discourse constraints on polarity. Section 4 gives the detail of proposed method. Experimental results are reported and discussed in Section 5 and Section 6 concludes this paper. 2 Related Work Research on polarity classification were generally conducted on 4 levels: document-level (Pang et al., 2002), sentence-level (Riloff et al., 2003), phraselevel (Wilson et al., 2009) and feature-level (Hu and Liu, 2004; Xia et al., 2007). There was little research focusing on the automatic recognition of intra-sentence level discourse 163 relations for sentiment analysis in the literature. Polanyi and Zaenen (2006) argued that valence calculation is critically affected by discourse structure. Asher et al. (2008) proposed a shallow semantic representation using a feature structure and use five types of rhetorical relations to build a finegrained corpus for deep contextual sentiment analysis. Nevertheless, they did not propose a computational model for their discourse schem"
D11-1015,C08-2002,0,\N,Missing
D13-1091,D11-1052,0,0.0185275,"m the traditional semantic similarity, and more focused on revealing the underlying sentiment relations between words. Mohtarami et al. (2013b) proposed a hidden emotional model to calculating the sentiment similarity of word pairs. However, the impact of the different corpora is not considered for this task. Mohammad et al. (2013) generated wordsentiment association lexicons from Tweets with the help of hashtags and emoticons. Pak and Paroubek (2010) collected tweets with happy and sad emoticons as training corpus, and built sentiment classifier based on traditional machine learning methods. Brody and Diakopoulos (2011) showed that lengthening was strongly associated with subjectivity and sentiment in tweets. Davidov et al. (2010) treated 50 Twitter tags and 15 smileys as sentiment labels and a supervised sentiment classification framework was proposed to classify the tweets. The previous literatures have showed that the emoticons can be treated as natural sentiment labels of the tweets. 6 Conclusion and Future Work The quality of corpus may affect the performance of sentiment similarity measurement. In this paper, we compare the Twitter data with the Google, Web1T and Wikipedia data in polarity classificati"
D13-1091,C10-2028,0,0.0320834,"ohtarami et al. (2013b) proposed a hidden emotional model to calculating the sentiment similarity of word pairs. However, the impact of the different corpora is not considered for this task. Mohammad et al. (2013) generated wordsentiment association lexicons from Tweets with the help of hashtags and emoticons. Pak and Paroubek (2010) collected tweets with happy and sad emoticons as training corpus, and built sentiment classifier based on traditional machine learning methods. Brody and Diakopoulos (2011) showed that lengthening was strongly associated with subjectivity and sentiment in tweets. Davidov et al. (2010) treated 50 Twitter tags and 15 smileys as sentiment labels and a supervised sentiment classification framework was proposed to classify the tweets. The previous literatures have showed that the emoticons can be treated as natural sentiment labels of the tweets. 6 Conclusion and Future Work The quality of corpus may affect the performance of sentiment similarity measurement. In this paper, we compare the Twitter data with the Google, Web1T and Wikipedia data in polarity classification task. The experiment results validate that when using the seed word groups the Twitter can achieve a much bett"
D13-1091,P97-1023,0,0.143292,"groups We further add the emoticons ‘:)’ and ‘:(’ into the seed word groups, denoted by Twitter+ in Table 3. The emoticons are natural sentiment labels. We can see that the performances are further improved by considering emoticons as seed words. The above experiment results have validated the effectiveness of Twitter data as a better corpus for measuring the sentiment similarity. The results also reveal the potential usefulness of Twitter corpus in semantic similarity measurement. 5 Related Work Detecting the polarity of words is the fundamental problem for most of sentiment analysis tasks (Hatzivassiloglou and McKeown, 1997; Pang and Lee, 2007; Feldman, 2013). Many methods have been proposed to measure the words’ or short texts similarity based on large corpus (Sahami and Heilman, 2006; Yih and Meek, 2007; Gabrilovich and Markovitch, 2007). Bollegala et al. (2011) submitted the word to the search engine, and the related result pages were employed to represent the meaning of the original word. Mihalcea et al. (2006) proposed a method to measure the semantic similarity of words or short texts, considering both corpus-based and knowledge-based information. Although the previous algorithms have achieved promising re"
D13-1091,D07-1115,0,0.0498347,"Missing"
D13-1091,W06-1642,0,0.0711845,"Missing"
D13-1091,P98-2127,0,0.288074,"Missing"
D13-1091,P13-1097,0,0.021623,"employed to represent the meaning of the original word. Mihalcea et al. (2006) proposed a method to measure the semantic similarity of words or short texts, considering both corpus-based and knowledge-based information. Although the previous algorithms have achieved promising results, there are no work done on evaluating the quality of different corpora. Mohtarami et al. (2012; 2013a; 2013b) introduced the concept of sentiment similarity, which was considered as different from the traditional semantic similarity, and more focused on revealing the underlying sentiment relations between words. Mohtarami et al. (2013b) proposed a hidden emotional model to calculating the sentiment similarity of word pairs. However, the impact of the different corpora is not considered for this task. Mohammad et al. (2013) generated wordsentiment association lexicons from Tweets with the help of hashtags and emoticons. Pak and Paroubek (2010) collected tweets with happy and sad emoticons as training corpus, and built sentiment classifier based on traditional machine learning methods. Brody and Diakopoulos (2011) showed that lengthening was strongly associated with subjectivity and sentiment in tweets. Davidov et al. (2010)"
D13-1091,S13-2053,0,0.0249338,"knowledge-based information. Although the previous algorithms have achieved promising results, there are no work done on evaluating the quality of different corpora. Mohtarami et al. (2012; 2013a; 2013b) introduced the concept of sentiment similarity, which was considered as different from the traditional semantic similarity, and more focused on revealing the underlying sentiment relations between words. Mohtarami et al. (2013b) proposed a hidden emotional model to calculating the sentiment similarity of word pairs. However, the impact of the different corpora is not considered for this task. Mohammad et al. (2013) generated wordsentiment association lexicons from Tweets with the help of hashtags and emoticons. Pak and Paroubek (2010) collected tweets with happy and sad emoticons as training corpus, and built sentiment classifier based on traditional machine learning methods. Brody and Diakopoulos (2011) showed that lengthening was strongly associated with subjectivity and sentiment in tweets. Davidov et al. (2010) treated 50 Twitter tags and 15 smileys as sentiment labels and a supervised sentiment classification framework was proposed to classify the tweets. The previous literatures have showed that t"
D13-1091,pak-paroubek-2010-twitter,0,0.0565938,"evaluating the quality of different corpora. Mohtarami et al. (2012; 2013a; 2013b) introduced the concept of sentiment similarity, which was considered as different from the traditional semantic similarity, and more focused on revealing the underlying sentiment relations between words. Mohtarami et al. (2013b) proposed a hidden emotional model to calculating the sentiment similarity of word pairs. However, the impact of the different corpora is not considered for this task. Mohammad et al. (2013) generated wordsentiment association lexicons from Tweets with the help of hashtags and emoticons. Pak and Paroubek (2010) collected tweets with happy and sad emoticons as training corpus, and built sentiment classifier based on traditional machine learning methods. Brody and Diakopoulos (2011) showed that lengthening was strongly associated with subjectivity and sentiment in tweets. Davidov et al. (2010) treated 50 Twitter tags and 15 smileys as sentiment labels and a supervised sentiment classification framework was proposed to classify the tweets. The previous literatures have showed that the emoticons can be treated as natural sentiment labels of the tweets. 6 Conclusion and Future Work The quality of corpus"
D13-1091,P02-1053,0,0.01025,"ndexed by Google. Cilibrasi and Vitanyi have validated the effectiveness of Google distance in measuring the semantic similarity between concept words. Based on the above formulas, we compare the Twitter data with the Web and Wikipedia data as the similarity measurement corpus. Given a candidate word w, we firstly measure its sentiment similarity with a positive seed word and a negative seed word respectively in Formula (1), and the difference of sim is used to further detect the polarity of w. The above four similarity measurements serve as sim with Web, Wikipedia and Twitter data as corpus. Turney (2002) chose excellent and poor as seed words. However, using isolated seed words may cause the bias problem. Therefore, we further select two groups of seed words that are lack of sensitivity to context and form a positive seed set P S and a negative seed set N S (Turney, 2003). The Formula (1) can be rewritten as: SO(w) = X sim(w, sep ) − sep ∈P S X sim(w, sen ) (6) 4.1 Liu MPQA sen ∈N S Based on the Formula(6) and the sentiment seed words, we can measure the sentiment polarity of the given candidate words. 4 are also removed. Finally, we construct the Twitter corpus that consists of 266.8 million"
D13-1091,N10-1119,0,0.0326073,"Missing"
D13-1091,H05-1044,0,0.0683748,"ation to the word co-occurrence statistics in Web pages in a predefined window size (1 ≤ n ≤ 5). For example, the 5 gram Web1T data means the cooccurrence window size is 5. The English Wikipedia dump 1 we used was extracted at the end of March 2013, which contained more than 13 million articles. We extracted the plain texts of the Wikipedia data as the training corpus for the Formula (6). Evaluation Method. Two well-know sentiment lexicons are utilized as gold standard for polarity classification task. The statistics of Liu’s sentiment lexicon (Liu et al., 2005) and MPQA subjectivity lexicon (Wilson et al., 2005) are shown in Table 1. For each word w in the lexicons, we employ the Formula (6) to calculate the word’s polarity using different corpora. If SO(w) > 0, the word w is classified into the positive category. Otherwise if SO(w) &lt; 0, it is classified into the negative category. The accuracy of the classification result is used to measure the quality of the corpus. Experiment Experiment Setup Corpus Preparing. The Twitter corpus corresponds to the 476 million Twitter tweets (Yang and Leskovec, 2011), which includes over 476 million Twitter posts from 20 million users, covering a 7 month period fro"
D13-1091,C98-2122,0,\N,Missing
D14-1123,P11-1016,0,0.0366737,"ent analysis and event detection in microblog, respectively. Sentiment Analysis in Microblog: Sentiment analysis (Pang and Lee, 2008; Liu, 2012) is mainly about analyzing people’s opinions, sentiments and emotions towards a given event, topic, product, etc. Microblog platforms like Twitter and Weibo, provide people a convenient way to post their emotional reactions towards social events in almost real time. This leads to increasing number of interests on sentiment analysis in microblog data (Davidov et al., 2010; Liu et al., 2012; Go et al., 2009; Agarwal et al., 2011; Pak and Paroubek, 2010; Jiang et al., 2011; Speriosu et al., 2011; Bermingham and Smeaton, 2010). The training data for microblog sentiment analysis are usually obtained in an automatic manner by utilizing emoticons, hashtags and smileys. Davidov et al. (2010) propose an approach to automatically obtain labeled training examples by exploiting hashtags and smileys. Liu et al. (2012) proposed an emoticon smoothed method to integrate both manually labeled data and noisy labeled data for Twitter sentiment classification. Different from existing microblog sentiment analysis work, which aims at discovering sentiments and emotions for an eve"
D14-1123,W11-0705,0,0.0440545,"section, we review the related work on sentiment analysis and event detection in microblog, respectively. Sentiment Analysis in Microblog: Sentiment analysis (Pang and Lee, 2008; Liu, 2012) is mainly about analyzing people’s opinions, sentiments and emotions towards a given event, topic, product, etc. Microblog platforms like Twitter and Weibo, provide people a convenient way to post their emotional reactions towards social events in almost real time. This leads to increasing number of interests on sentiment analysis in microblog data (Davidov et al., 2010; Liu et al., 2012; Go et al., 2009; Agarwal et al., 2011; Pak and Paroubek, 2010; Jiang et al., 2011; Speriosu et al., 2011; Bermingham and Smeaton, 2010). The training data for microblog sentiment analysis are usually obtained in an automatic manner by utilizing emoticons, hashtags and smileys. Davidov et al. (2010) propose an approach to automatically obtain labeled training examples by exploiting hashtags and smileys. Liu et al. (2012) proposed an emoticon smoothed method to integrate both manually labeled data and noisy labeled data for Twitter sentiment classification. Different from existing microblog sentiment analysis work, which aims at di"
D14-1123,C10-2028,0,0.267155,"f precision, recall and F-measure. 2 Related Work In this section, we review the related work on sentiment analysis and event detection in microblog, respectively. Sentiment Analysis in Microblog: Sentiment analysis (Pang and Lee, 2008; Liu, 2012) is mainly about analyzing people’s opinions, sentiments and emotions towards a given event, topic, product, etc. Microblog platforms like Twitter and Weibo, provide people a convenient way to post their emotional reactions towards social events in almost real time. This leads to increasing number of interests on sentiment analysis in microblog data (Davidov et al., 2010; Liu et al., 2012; Go et al., 2009; Agarwal et al., 2011; Pak and Paroubek, 2010; Jiang et al., 2011; Speriosu et al., 2011; Bermingham and Smeaton, 2010). The training data for microblog sentiment analysis are usually obtained in an automatic manner by utilizing emoticons, hashtags and smileys. Davidov et al. (2010) propose an approach to automatically obtain labeled training examples by exploiting hashtags and smileys. Liu et al. (2012) proposed an emoticon smoothed method to integrate both manually labeled data and noisy labeled data for Twitter sentiment classification. Different from exi"
D14-1123,pak-paroubek-2010-twitter,0,0.0435762,"e related work on sentiment analysis and event detection in microblog, respectively. Sentiment Analysis in Microblog: Sentiment analysis (Pang and Lee, 2008; Liu, 2012) is mainly about analyzing people’s opinions, sentiments and emotions towards a given event, topic, product, etc. Microblog platforms like Twitter and Weibo, provide people a convenient way to post their emotional reactions towards social events in almost real time. This leads to increasing number of interests on sentiment analysis in microblog data (Davidov et al., 2010; Liu et al., 2012; Go et al., 2009; Agarwal et al., 2011; Pak and Paroubek, 2010; Jiang et al., 2011; Speriosu et al., 2011; Bermingham and Smeaton, 2010). The training data for microblog sentiment analysis are usually obtained in an automatic manner by utilizing emoticons, hashtags and smileys. Davidov et al. (2010) propose an approach to automatically obtain labeled training examples by exploiting hashtags and smileys. Liu et al. (2012) proposed an emoticon smoothed method to integrate both manually labeled data and noisy labeled data for Twitter sentiment classification. Different from existing microblog sentiment analysis work, which aims at discovering sentiments and"
D14-1123,W11-2207,0,0.0183136,"nt detection in microblog, respectively. Sentiment Analysis in Microblog: Sentiment analysis (Pang and Lee, 2008; Liu, 2012) is mainly about analyzing people’s opinions, sentiments and emotions towards a given event, topic, product, etc. Microblog platforms like Twitter and Weibo, provide people a convenient way to post their emotional reactions towards social events in almost real time. This leads to increasing number of interests on sentiment analysis in microblog data (Davidov et al., 2010; Liu et al., 2012; Go et al., 2009; Agarwal et al., 2011; Pak and Paroubek, 2010; Jiang et al., 2011; Speriosu et al., 2011; Bermingham and Smeaton, 2010). The training data for microblog sentiment analysis are usually obtained in an automatic manner by utilizing emoticons, hashtags and smileys. Davidov et al. (2010) propose an approach to automatically obtain labeled training examples by exploiting hashtags and smileys. Liu et al. (2012) proposed an emoticon smoothed method to integrate both manually labeled data and noisy labeled data for Twitter sentiment classification. Different from existing microblog sentiment analysis work, which aims at discovering sentiments and emotions for an event or topic given in ad"
D14-1123,D12-1134,0,0.340478,"UDRQDOGR     tion on March 12 indicates a public event:3.11 EDFHOROD   Earthquake in Japan. We can see that emotional changes immediately reflect the occurring of realworld events, thus it is reasonable to use them to perform event detection. Most existing research on microblog event detection (Weng and Lee, 2011; Sakaki et al., 2010; Becker et al., 2010) only account for the factual information (e.g., burstness of topic keywords). They usually ignore the importance of emotion information for event detection. Although there have recently been a few papers (Zhao et al., 2012a; Nguyen et al., 2013; Akcora et al., 2010) in this direction, they have a number of disadvantages. Firstly, they can not detect communityrelated events. Since they all aggregate emotion at global level, they can only discover national attention events, such as public holidays ( “Christmas” and “Spring Festival”) or natural disasters. In many applications, discovering events related to a certain group of users or a certain topic is more meaningful. Consider the following questions: “what happened in the football community last week?” and “what are the most significant events in the lawyer com"
D15-1259,W04-1013,0,0.00881063,"maries for each repost tree. To ensure the quality of reference summaries, we first extracted a list of frequent nouns from each repost tree and generalized 7 to 10 topics based on the nouns list, which provided a high-level overview of a repost tree to editors. Then, our guideline required editors to read all repost microblogs ordered sequentially on a repost tree. For every message, its entire repost tree path was also provided as supplementary context information. When finished reading, editors wrote down one or two sentences to summarize each topic in the list. We utilized ROUGE-N metric (Lin, 2004) for benchmark, which is a standard for evaluating automatic summaries based on N-gram overlapping between a generated summary and a reference. Specifically, ROUGE-1 and ROUGE-2 F1-measure were used as our evaluation metrics. Lin et al. (2004) has demonstrated that ROUGE-2 correlates well with humans in summarizing formal texts. And ROUGE-1 is a better alternative in evaluating summaries for short and informal 4 All descriptions are English translations of the root microblogs originally in Chinese. microblog messages (Inouye and Kalita, 2011; Chang et al., 2013). In our human-generated summari"
D15-1259,W11-0709,0,0.0876751,"f social media has made microblog summarization a hot topic. Most prior works are on event-level or topic-level summarization. Typically, the first step is to cluster posts into sub-events (Chakrabarti and Punera, 2011; Duan et al., 2012; Shen et al., 2013) or subtopics (Long et al., 2011; Rosa et al., 2011; Meng et al., 2012), and then the second step generates summary for each cluster. Some works tried to apply conventional extractive summarization models directly, e.g., LexRank (Erkan and Radev, 2004), MEAD (Radev et al., 2004), TF-IDF (Inouye and Kalita, 2011), Integer Linear Programming (Liu et al., 2011; Takamura et al., 2011), etc. Sharif et al. (2010) modeled the problem as optimal path finding on a phrase reinforcement graph. However, these general summarizers were found not suitable for microblog posts, which are informal and noisy (Chang et al., 2013). Researchers 2169 then considered social signals like user following relations and retweet count (Duan et al., 2012; Liu et al., 2012), and reported such features useful to help summarize microblog posts. Our work studies repost tree summarization by leveraging content-level structure to enrich context of messages, which is a different kin"
D15-1259,C12-1104,0,0.167824,"ome works tried to apply conventional extractive summarization models directly, e.g., LexRank (Erkan and Radev, 2004), MEAD (Radev et al., 2004), TF-IDF (Inouye and Kalita, 2011), Integer Linear Programming (Liu et al., 2011; Takamura et al., 2011), etc. Sharif et al. (2010) modeled the problem as optimal path finding on a phrase reinforcement graph. However, these general summarizers were found not suitable for microblog posts, which are informal and noisy (Chang et al., 2013). Researchers 2169 then considered social signals like user following relations and retweet count (Duan et al., 2012; Liu et al., 2012), and reported such features useful to help summarize microblog posts. Our work studies repost tree summarization by leveraging content-level structure to enrich context of messages, which is a different kind of signal. Chang et al. (2013) proposed a task to summarize Twitter context trees consisting of an original tweet and all its reposts (i.e., replies and retweets). They combined user influence signals into a supervised summarization framework. Our work is different from theirs: 1) They simply treat a context tree as a tweets stream while we consider repost tree structures in summarization"
D15-1259,C12-1047,0,0.0623679,"portant information (Radev et al., 2002). Generally, text summarization techniques can be categorized into extractive and abstractive methods (Das and Martins, 2007). Extractive approaches focus on how to identify and distill salient contents from original texts whereas abstractive approaches aim at producing grammatical summaries by text generation. Recently, the development of social media has made microblog summarization a hot topic. Most prior works are on event-level or topic-level summarization. Typically, the first step is to cluster posts into sub-events (Chakrabarti and Punera, 2011; Duan et al., 2012; Shen et al., 2013) or subtopics (Long et al., 2011; Rosa et al., 2011; Meng et al., 2012), and then the second step generates summary for each cluster. Some works tried to apply conventional extractive summarization models directly, e.g., LexRank (Erkan and Radev, 2004), MEAD (Radev et al., 2004), TF-IDF (Inouye and Kalita, 2011), Integer Linear Programming (Liu et al., 2011; Takamura et al., 2011), etc. Sharif et al. (2010) modeled the problem as optimal path finding on a phrase reinforcement graph. However, these general summarizers were found not suitable for microblog posts, which are in"
D15-1259,P13-4009,0,0.0757462,"Sort all v ∈ V by RN (v) in descending order 19: Pick the top-n messages as summary WALK-2 sampling from leader probabilities, it also reduces the risk of including real followers into summary. 5 Experiments and Results To evaluate the two modules in our repost tree summarization system, i.e., CRF-based model for leader detection and LeadSum model for summarization, we conducted two sets of experiments based on microblog posts data collected from Sina Weibo, which has a similar market penetration as Twitter (Rapoza, 2011)2 . Microblog messages on Sina Weibo are in Chinese and we use FudanNLP (Qiu et al., 2013) for text preprocessing including word segmentation and POS tagging. 5.1 Experiment for Leader Detection In this experiment, we evaluated the performance of CRF model for leader detection task. 5.1.1 Data Collection and Setup We first crawled 1,300 different repost trees using the public PKUVIS toolkit (Ren et al., 2014). Given an original microblog post, the toolkit can automatically crawl its complete repost tree. For each tree, we randomly selected one path and further formed a set with 1,300 repost tree paths, 2 The datasets are available at http://www1.se. cuhk.edu.hk/˜lijing/data/repost_"
D15-1259,J02-4001,0,0.176959,". • We identify a novel problem of leader detection for summarization, which aims to reduce noise on repost trees, and present a CRF-based method for effectively detecting leaders by utilizing the tree structure and message contents. • We incorporate the leader detection result into an unsupervised summarization model based on random walk and substantially enhance the model to reduce the impact of leader detection errors on summarization. 2 Related Work The goal of text summarization is to automatically produce a succinct summary for one or more documents that preserves important information (Radev et al., 2002). Generally, text summarization techniques can be categorized into extractive and abstractive methods (Das and Martins, 2007). Extractive approaches focus on how to identify and distill salient contents from original texts whereas abstractive approaches aim at producing grammatical summaries by text generation. Recently, the development of social media has made microblog summarization a hot topic. Most prior works are on event-level or topic-level summarization. Typically, the first step is to cluster posts into sub-events (Chakrabarti and Punera, 2011; Duan et al., 2012; Shen et al., 2013) or"
D15-1259,radev-etal-2004-mead,0,0.286211,"Missing"
D15-1259,N13-1135,0,0.106426,"Missing"
D15-1259,D11-1040,0,0.0316202,"ount based on user following relations. • LeadProSum: LeadProSum ranks and selects reposting messages by their marginal probabilities as leaders determined by our CRF-based leader detection model. • SVDSum: SVDSum adopts the Singular Value Decomposition (SVD) to discover hidden sub-topics for summarization (Gong and Liu, 2001). Reposting messages are ranked according to latent semantic analysis with SVD on termmessage matrix. • DivRankSum: DivRankSum directly applies DivRank (Mei et al., 2010) algorithm to rank all messages unaware of leaders and followers. A similar model is also reported in Yan et al. (2011). Following their work, we set damping weight as 0.85. • UserInfSum: Chang et al. (2013) ranks messages utilizing Gradient Boosted Decision Tree (GBDT) algorithm with text, popularity, temporal and user influence signals to summarize Twitter context tree. In particular, without the interaction data with external users, we utilize users’ fol2174 Name Tree (I) Tree (II) Tree (III) Tree (IV) Tree (V) Tree (VI) Tree (VII) Tree (VIII) Tree (IX) Tree (X) # of nodes 21,353 9,616 13,087 12,865 10,666 21,127 18,974 2,021 9,230 10,052 # of nodes with comments 15,409 6,073 9,583 7,083 7,129 15,057 12,399"
D17-1237,P17-1045,1,0.83128,"we need to book air tickets, reserve a hotel, rent a car, etc. in a collective way so as to satisfy a set of cross-subtask constraints, which we call slot constraints. Examples of slot constraints for travel planning are: hotel check-in time should be later than the flight’s arrival time, hotel check-out time may be earlier than the return flight depart time, the number of flight tickets equals to that of hotel check-in people, and so on. It is common to learn a task-completion dialogue agent using reinforcement learning (RL); see Su et al. (2016); Cuay´ahuitl (2017); Williams et al. (2017); Dhingra et al. (2017) and Li et al. (2017a) for a few recent examples. Compared to these dialogue agents developed for individual domains, the composite task presents additional challenges to commonly used, flat RL approaches such as DQN (Mnih et al., 2015). The first challenge is reward sparsity. Dialogue policy learning for composite tasks requires exploration in a much larger state-action space, and it often takes many more conversation turns between user and agent to fulfill a task, leading to a much longer trajectory. Thus, the reward signals (usually provided by users at the end of a conversation) are delaye"
D17-1237,W17-5526,0,0.00786429,"Missing"
D17-1237,I17-1074,1,0.409205,"ts, reserve a hotel, rent a car, etc. in a collective way so as to satisfy a set of cross-subtask constraints, which we call slot constraints. Examples of slot constraints for travel planning are: hotel check-in time should be later than the flight’s arrival time, hotel check-out time may be earlier than the return flight depart time, the number of flight tickets equals to that of hotel check-in people, and so on. It is common to learn a task-completion dialogue agent using reinforcement learning (RL); see Su et al. (2016); Cuay´ahuitl (2017); Williams et al. (2017); Dhingra et al. (2017) and Li et al. (2017a) for a few recent examples. Compared to these dialogue agents developed for individual domains, the composite task presents additional challenges to commonly used, flat RL approaches such as DQN (Mnih et al., 2015). The first challenge is reward sparsity. Dialogue policy learning for composite tasks requires exploration in a much larger state-action space, and it often takes many more conversation turns between user and agent to fulfill a task, leading to a much longer trajectory. Thus, the reward signals (usually provided by users at the end of a conversation) are delayed and sparse. As we"
D17-1237,W11-2033,0,0.125959,"ition (Dietterich, 2000). In this paper, we choose the options framework for its conceptual simplicity and generality (Sutton et al., 1998); more details are found in the next section. Our work is also motivated by hierarchicalDQN (Kulkarni et al., 2016) which integrates hierarchical value functions to operate at different temporal scales. The model achieved superior performance on a complicated ATARI game “Montezuma’s Revenge” with a hierarchical structure. A related but different extension to singledomain dialogues is multi-domain dialogues, where each domain is handled by a separate agent (Lison, 2011; Gasic et al., 2015a,b; Cuay´ahuitl et al., 2016). In contrast to compositedomain dialogues studied in this paper, a conversation in a multi-domain dialogue normally involves one domain, so completion of a task does not require solving sub-tasks in different domains. Consequently, work on multi-domain dialogues focuses on different technical challenges such as transfer learning across different domains (Gasic 2232 et al., 2015a) and domain selection (Cuay´ahuitl et al., 2016). 3 Dialogue Policy Learning Our composite task-completion dialogue agent consists of four components: (1) an LSTMbased"
D17-1237,D15-1199,0,0.0314686,"Missing"
D17-1237,N07-2038,0,0.83697,"A Rule+ Agent requests and informs all the slots in a pre-defined order exhaustedly, and then confirms with the user about the reserved tickets. The average turn of this agent is longer than that of the Rule agent. • A flat RL Agent is trained with a standard flat deep reinforcement learning method (DQN) which learns a flat dialogue policy using extrinsic rewards only. 4.3 User Simulator Training reinforcement learners is challenging because they need an environment to interact with. In the dialogue research community, it is common to use simulated users as shown in Figure 3 for this purpose (Schatzmann et al., 2007; Asri et al., 2016). In this work, we adapted the publiclyavailable user simulator, developed by Li et al. (2016), to the composite task-completion dialogue setting using the human-human conversation data described in Section 4.1.2 During training, the simulator provides the agent with an (extrinsic) reward signal at the end of the dialogue. A dialogue is considered to be successful only when a travel plan is made successfully, and the information provided by the agent satisfies user’s constraints. At the end of each dialogue, the agent receives a positive reward of 2⇤max turn (max turn = 60"
D17-1237,P17-1062,0,0.331325,"r to make a travel plan, we need to book air tickets, reserve a hotel, rent a car, etc. in a collective way so as to satisfy a set of cross-subtask constraints, which we call slot constraints. Examples of slot constraints for travel planning are: hotel check-in time should be later than the flight’s arrival time, hotel check-out time may be earlier than the return flight depart time, the number of flight tickets equals to that of hotel check-in people, and so on. It is common to learn a task-completion dialogue agent using reinforcement learning (RL); see Su et al. (2016); Cuay´ahuitl (2017); Williams et al. (2017); Dhingra et al. (2017) and Li et al. (2017a) for a few recent examples. Compared to these dialogue agents developed for individual domains, the composite task presents additional challenges to commonly used, flat RL approaches such as DQN (Mnih et al., 2015). The first challenge is reward sparsity. Dialogue policy learning for composite tasks requires exploration in a much larger state-action space, and it often takes many more conversation turns between user and agent to fulfill a task, leading to a much longer trajectory. Thus, the reward signals (usually provided by users at the end of a c"
D17-1237,E06-1032,0,\N,Missing
D17-1237,J09-4008,0,\N,Missing
D17-1237,P02-1040,0,\N,Missing
D17-1237,P09-2025,0,\N,Missing
D17-1237,P14-2074,0,\N,Missing
D17-1237,P15-1044,0,\N,Missing
D17-1237,D16-1230,0,\N,Missing
D17-1237,P16-2043,0,\N,Missing
D17-1237,P16-2008,0,\N,Missing
D17-1237,W16-3622,0,\N,Missing
D17-1237,C16-1105,0,\N,Missing
D17-1237,W17-5525,0,\N,Missing
D17-1237,W16-6644,0,\N,Missing
D17-1237,N16-1086,0,\N,Missing
D19-1465,N10-1122,0,0.525707,", 2016). As can be seen from R1, aspect words “price” and “laptop” tend to appear together in R1-like sentences concerning “laptop price”. As for R2, its aspect words “i7”, though not co-occurring with other aspects, have similar neighbors “for the” in local context with “price”, which reveals its high likelihood of being aspect words, the same as “price”. Inspired by the phenomenon above, we propose a novel unsupervised model capable of coupling global and local context to discover aspect word clusters. Our model is built on the success of topic models in aspect extraction (Lin and He, 2009; Brody and Elhadad, 2010; Zhao et al., 2010). It is attributed to their ability to form latent topics with words likely to co-occur in a subset of sentences instead of widely appearing in the entire corpus (Blei et al., 2003). These words happen to exhibit similar patterns of how aspect words occur on sentence level (Lin and He, 2009). However, the above methods, only exploiting global context, are arguably suboptimal for largely ignoring the rich information delivered by local context. Some recent work (He et al., 2017), on the other way around, focus on using local context, yet ignore its coupled effects with globa"
D19-1465,D17-1245,0,0.0207696,"lue. Wavy underlines indicate local context words indicating aspect word “i7”. Introduction Opinion, one of the main factors shaping human behavior, is crucial to our daily activities (Liu, 2012). Every choice we make in our life, ranging from where to go for a Friday dinner to which job offer to pick up, is largely influenced by what other people think. To help individuals navigate decision-making processes, there exists growing attentions on opinion mining algorithms that distill massive opinion-rich texts — such as digital product reviews (Poddar et al., 2017) and social media discussions (Dusmanu et al., 2017) — into the opinionated information we need. ∗ This work was partially done when Ming Liao was an intern at Tencent AI Lab, Shenzhen, China. † Jing Li is the corresponding author and conducted most of the work at Tencent AI Lab, Shenzhen, China. Towards human opinion understanding, it is essential to figure out what target the opinion centers around. After all, previous studies have long pointed out that human language mostly conveys opinion with aspect and sentiment words (Liu, 2012). In this work, we focus on aspect extraction, targeting at the recognition of words indicating opinion aspects"
D19-1465,P16-2011,0,0.0312241,"standing, it is essential to figure out what target the opinion centers around. After all, previous studies have long pointed out that human language mostly conveys opinion with aspect and sentiment words (Liu, 2012). In this work, we focus on aspect extraction, targeting at the recognition of words indicating opinion aspects (henceforth aspect words). We believe developing effective aspect extraction models will benefit a broad range of compelling applications, such as aspect-based sentiment classification (Tang et al., 2016), opinion summarization (Wu et al., 2016), trending event tracking (Feng et al., 2016), and so forth. To date, most progress made in aspect extraction has focused on training sequence tagging models on human-annotated data (Li and Lam, 2017; Xu et al., 2018; Wang and Pan, 2018). However, acquiring manual labels will inevitably undergo an expensive data annotation process and is hence difficult to scale for datasets from new domain or language. In this work, we explore how aspect words can be discovered in a fully unsupervised manner. We are inspired by the linguistic phenomenon that aspect words generally distinguish themselves from other words in their occurrence patterns with"
D19-1465,D16-1057,0,0.0261463,". To ensure comparable performance, for clustering-based approaches, we select the top 30 nouns from 40 aspect clusters, same as our set up. For the rest, the top 1, 200 nouns are extracted. Here we adopt two sets of evaluation metrics. First, we follow Qiu et al. (2011) to test sentence-level aspect extraction, where the intersection of our selection and the words appear in a review sentence are considered as the extracted aspects. In this evaluation, we report precision, recall, and F1 scores. Second, we evaluate our ability to build aspect lexicon (a.k.a. corpus-level extraction) following Hamilton et al. (2016). We consider all the annotated aspects as gold standard lexicon and adopt accuracy for evaluation. Comparisons. We first consider a simple baseline that randomly selects nouns as aspect words (henceforth R ANDOM). We also compare with extracting- and clustering-based baselines — T F I DF (Bahdanau et al., 2015), K- MEANS (Lloyd, 1982) (implemented with sklearn toolkit3 and taking Glove embedding for similarity measure), and BTM4 (Yan et al., 2013), state-of-the-art in short text topic modeling and well-performed in aspect extraction (He et al., 2017). In addition, we consider the following re"
D19-1465,P17-1036,0,0.592533,"Our model is built on the success of topic models in aspect extraction (Lin and He, 2009; Brody and Elhadad, 2010; Zhao et al., 2010). It is attributed to their ability to form latent topics with words likely to co-occur in a subset of sentences instead of widely appearing in the entire corpus (Blei et al., 2003). These words happen to exhibit similar patterns of how aspect words occur on sentence level (Lin and He, 2009). However, the above methods, only exploiting global context, are arguably suboptimal for largely ignoring the rich information delivered by local context. Some recent work (He et al., 2017), on the other way around, focus on using local context, yet ignore its coupled effects with global context. Our work, to the best of our knowledge, is the first to explore how global and local context jointly indicate aspect words. Moreover, taken advantage of the recent advances in neural topic models (Miao et al., 2017; Srivastava and Sutton, 2017), we enable end-to-end learning of global and local representation, where the interaction between them contributed to aspect recognition can be automatically captured. In experiments, we first compare our model with existing unsupervised models on"
D19-1465,C10-1074,0,0.0955358,"epresentations from global and local context, which interprets our superiority in aspect extraction. In addition, we empirically analyze global and local word context on our datasets. The results confirm that aspect words indeed vary in their global and local context compared with non-aspect ones, hence providing useful clues for aspect identification. 2 Related Work Our work is mainly in the line with aspect extraction research. On this task, early studies mostly focus on the design of hand crafted rules (Hu and Liu, 2004; Zhuang et al., 2006; Qiu et al., 2011) or features (Jin et al., 2009; Li et al., 2010). Recently, the propose of neural models enables automatic representation learning without laborintensive feature engineering (Wang et al., 2016, 2017; Li and Lam, 2017; Xu et al., 2018; Wang and Pan, 2018). These supervised models, rely on manually annotated data, thus restricted in their scaling ability for new domain or language. Instead, our work, focusing on unsupervised aspect extraction, can discover aspect words via exploiting how words occur in global and local context. Our work is inspired by the unsupervised methods capturing latent aspect factors with LDAstyle topic models (Lin and"
D19-1465,D17-1310,0,0.238935,"conveys opinion with aspect and sentiment words (Liu, 2012). In this work, we focus on aspect extraction, targeting at the recognition of words indicating opinion aspects (henceforth aspect words). We believe developing effective aspect extraction models will benefit a broad range of compelling applications, such as aspect-based sentiment classification (Tang et al., 2016), opinion summarization (Wu et al., 2016), trending event tracking (Feng et al., 2016), and so forth. To date, most progress made in aspect extraction has focused on training sequence tagging models on human-annotated data (Li and Lam, 2017; Xu et al., 2018; Wang and Pan, 2018). However, acquiring manual labels will inevitably undergo an expensive data annotation process and is hence difficult to scale for datasets from new domain or language. In this work, we explore how aspect words can be discovered in a fully unsupervised manner. We are inspired by the linguistic phenomenon that aspect words generally distinguish themselves from other words in their occurrence patterns within global and local context. Here global context refers to how pairs of words co-occur with each other at sentence level (without considering word order a"
D19-1465,Q15-1022,0,0.184578,"se supervised models, rely on manually annotated data, thus restricted in their scaling ability for new domain or language. Instead, our work, focusing on unsupervised aspect extraction, can discover aspect words via exploiting how words occur in global and local context. Our work is inspired by the unsupervised methods capturing latent aspect factors with LDAstyle topic models (Lin and He, 2009; Brody and Elhadad, 2010; Zhao et al., 2010). We are also related with non-neural models incorporating word embeddings (encoding local context) to learn latent topics (discovered from global context) (Nguyen et al., 2015; Li et al., 2016; Shi et al., 2017). Compared with them — relying on expertise to customize inference algorithms, our model — in a neural architecture — does not require model-specific derivation, and enables interactions between global and local representations to be automatically learned. Though some neural models were recently proposed for our task (Wang et al., 2015; He et al., 2017), they focus on local context, unable to leverage global information. Distinguishing from them, we examine how the coupled effects of global and local context can signal aspect words, which have never been stu"
D19-1465,D14-1162,0,0.081031,"Missing"
D19-1465,D17-1049,0,0.0694589,"Missing"
D19-1465,S15-2082,0,0.231718,"Missing"
D19-1465,S14-2004,0,0.178116,"Missing"
D19-1465,J11-1002,0,0.705447,"onstrate our capability of capturing meaningful representations from global and local context, which interprets our superiority in aspect extraction. In addition, we empirically analyze global and local word context on our datasets. The results confirm that aspect words indeed vary in their global and local context compared with non-aspect ones, hence providing useful clues for aspect identification. 2 Related Work Our work is mainly in the line with aspect extraction research. On this task, early studies mostly focus on the design of hand crafted rules (Hu and Liu, 2004; Zhuang et al., 2006; Qiu et al., 2011) or features (Jin et al., 2009; Li et al., 2010). Recently, the propose of neural models enables automatic representation learning without laborintensive feature engineering (Wang et al., 2016, 2017; Li and Lam, 2017; Xu et al., 2018; Wang and Pan, 2018). These supervised models, rely on manually annotated data, thus restricted in their scaling ability for new domain or language. Instead, our work, focusing on unsupervised aspect extraction, can discover aspect words via exploiting how words occur in global and local context. Our work is inspired by the unsupervised methods capturing latent as"
D19-1465,C16-1311,0,0.0301698,"ucted most of the work at Tencent AI Lab, Shenzhen, China. Towards human opinion understanding, it is essential to figure out what target the opinion centers around. After all, previous studies have long pointed out that human language mostly conveys opinion with aspect and sentiment words (Liu, 2012). In this work, we focus on aspect extraction, targeting at the recognition of words indicating opinion aspects (henceforth aspect words). We believe developing effective aspect extraction models will benefit a broad range of compelling applications, such as aspect-based sentiment classification (Tang et al., 2016), opinion summarization (Wu et al., 2016), trending event tracking (Feng et al., 2016), and so forth. To date, most progress made in aspect extraction has focused on training sequence tagging models on human-annotated data (Li and Lam, 2017; Xu et al., 2018; Wang and Pan, 2018). However, acquiring manual labels will inevitably undergo an expensive data annotation process and is hence difficult to scale for datasets from new domain or language. In this work, we explore how aspect words can be discovered in a fully unsupervised manner. We are inspired by the linguistic phenomenon that aspect wor"
D19-1465,S14-2036,0,0.0705254,"Missing"
D19-1465,P15-1060,0,0.186678,"Missing"
D19-1465,P18-1202,0,0.0645823,"ntiment words (Liu, 2012). In this work, we focus on aspect extraction, targeting at the recognition of words indicating opinion aspects (henceforth aspect words). We believe developing effective aspect extraction models will benefit a broad range of compelling applications, such as aspect-based sentiment classification (Tang et al., 2016), opinion summarization (Wu et al., 2016), trending event tracking (Feng et al., 2016), and so forth. To date, most progress made in aspect extraction has focused on training sequence tagging models on human-annotated data (Li and Lam, 2017; Xu et al., 2018; Wang and Pan, 2018). However, acquiring manual labels will inevitably undergo an expensive data annotation process and is hence difficult to scale for datasets from new domain or language. In this work, we explore how aspect words can be discovered in a fully unsupervised manner. We are inspired by the linguistic phenomenon that aspect words generally distinguish themselves from other words in their occurrence patterns within global and local context. Here global context refers to how pairs of words co-occur with each other at sentence level (without considering word order and can be extended to capture document"
D19-1465,D16-1059,0,0.0689889,"and local word context on our datasets. The results confirm that aspect words indeed vary in their global and local context compared with non-aspect ones, hence providing useful clues for aspect identification. 2 Related Work Our work is mainly in the line with aspect extraction research. On this task, early studies mostly focus on the design of hand crafted rules (Hu and Liu, 2004; Zhuang et al., 2006; Qiu et al., 2011) or features (Jin et al., 2009; Li et al., 2010). Recently, the propose of neural models enables automatic representation learning without laborintensive feature engineering (Wang et al., 2016, 2017; Li and Lam, 2017; Xu et al., 2018; Wang and Pan, 2018). These supervised models, rely on manually annotated data, thus restricted in their scaling ability for new domain or language. Instead, our work, focusing on unsupervised aspect extraction, can discover aspect words via exploiting how words occur in global and local context. Our work is inspired by the unsupervised methods capturing latent aspect factors with LDAstyle topic models (Lin and He, 2009; Brody and Elhadad, 2010; Zhao et al., 2010). We are also related with non-neural models incorporating word embeddings (encoding local"
D19-1465,P18-2094,0,0.419044,"ith aspect and sentiment words (Liu, 2012). In this work, we focus on aspect extraction, targeting at the recognition of words indicating opinion aspects (henceforth aspect words). We believe developing effective aspect extraction models will benefit a broad range of compelling applications, such as aspect-based sentiment classification (Tang et al., 2016), opinion summarization (Wu et al., 2016), trending event tracking (Feng et al., 2016), and so forth. To date, most progress made in aspect extraction has focused on training sequence tagging models on human-annotated data (Li and Lam, 2017; Xu et al., 2018; Wang and Pan, 2018). However, acquiring manual labels will inevitably undergo an expensive data annotation process and is hence difficult to scale for datasets from new domain or language. In this work, we explore how aspect words can be discovered in a fully unsupervised manner. We are inspired by the linguistic phenomenon that aspect words generally distinguish themselves from other words in their occurrence patterns within global and local context. Here global context refers to how pairs of words co-occur with each other at sentence level (without considering word order and can be extende"
D19-1465,D10-1006,0,0.453044,"rom R1, aspect words “price” and “laptop” tend to appear together in R1-like sentences concerning “laptop price”. As for R2, its aspect words “i7”, though not co-occurring with other aspects, have similar neighbors “for the” in local context with “price”, which reveals its high likelihood of being aspect words, the same as “price”. Inspired by the phenomenon above, we propose a novel unsupervised model capable of coupling global and local context to discover aspect word clusters. Our model is built on the success of topic models in aspect extraction (Lin and He, 2009; Brody and Elhadad, 2010; Zhao et al., 2010). It is attributed to their ability to form latent topics with words likely to co-occur in a subset of sentences instead of widely appearing in the entire corpus (Blei et al., 2003). These words happen to exhibit similar patterns of how aspect words occur on sentence level (Lin and He, 2009). However, the above methods, only exploiting global context, are arguably suboptimal for largely ignoring the rich information delivered by local context. Some recent work (He et al., 2017), on the other way around, focus on using local context, yet ignore its coupled effects with global context. Our work,"
D19-1470,N12-1074,0,0.548573,"k Our work is in line with conversation behavior analysis, where studies explore user interactions in ongoing conversations (Ritter et al., 2010) and how they signal the conversations’ future trajectory, such as continued activity (Backstrom et al., 2013; Jiao et al., 2018; Zeng et al., 2019) and the risk of going awry (Zhang et al., 2018). Different from these proposals which do not model personal interests, we study conversation recommendation for a specific user, where we measure how a user’s preferences match a conversation’s context. This work is also related to user response prediction (Artzi et al., 2012; Zhang et al., 2015) and post recommendation (Duan et al., 2010; Chen et al., 2012; Yan et al., 2012; Hong et al., 2013). While most of these studies focus on post modeling, we examine conversation context to predict user engagements, which goes beyond the postlevel prediction task. Other prior work examining conversation-level recommendation relies on either manual features (Chen et al., 2011) or shallow word occurrence patterns (Zeng et al., 2018), largely ignoring the useful features from historical user interactions. On the contrary, we utilize online user interactions in the conversation"
D19-1470,P18-1026,0,0.236442,"ded in their conversation interactions. To the best of our knowledge, this is the first work to explore neural conversation recommendation with online interactions explicitly encoded for user preference modeling. To evaluate our model, we conduct extensive experiments on two large-scale datasets with online conversations from Twitter and Reddit1 . Experimental results show that our method significantly outperforms state-of-the-art models that do not capture user interactions. For example, our model obtains an MAP (Mean Average Precision) of 0.625 on Twitter, compared with 0.591 by Zeng et al. (2018). We further find that our model still exhibits superior performance when the sparsity levels of user history and conversation context are varied, demonstrating our model’s potential ability to handle sparse conversation records. Additional experiments on an ablation study confirms the effectiveness of different components in our framework. A case study further reveals important interaction features captured by our model, which indicate their conversation entries and hence explain our model’s advanced performance. Finally, we 1 The datasets and codes are available at: https:// github.com/zxsha"
D19-1470,D14-1181,0,0.00330313,"/0 9:; 4/72 4/76 4/78, &lt;=&gt; 6 Conversation Interaction Modeling time (a) Graph-State LSTM &1&apos; /0 &1&apos; /2 LSTM Turn Interaction Modeling. To encode conversation interaction structure, we first organize the turns in a conversation c as a reply tree to formulate who replies to whom. Each node therein 4/2 ReLU &1&apos; /5 LSTM Wsuc LSTM (BiLSTM layer) Turn-level Modeling. Here we describe how we model turn-level representations, which combine what content it conveys and who its author is. Content representation is to reflect how words appear therein, where we employ a Convolutional Neural Network (CNN) (Kim, 2014) encoder to model a turn’s word sequence. Specifically, given a turn t in conversation c, we first map each word in t into a word embedding layer (initialized with pre-trained word vectors) to explore deep word semantics. And then, to capture how a word appears in local context with its neighbors, a CNN encoder is exploited to generate the turn-level content representation zt . Next, we concatenate zt , conveying content features, and ruCI , embedded with the interaction patt terns of t’s author ut , to produce a turn representation rtT R . It couples turn t’s word occurrence patterns and its"
D19-1470,C10-1034,0,0.209152,"studies explore user interactions in ongoing conversations (Ritter et al., 2010) and how they signal the conversations’ future trajectory, such as continued activity (Backstrom et al., 2013; Jiao et al., 2018; Zeng et al., 2019) and the risk of going awry (Zhang et al., 2018). Different from these proposals which do not model personal interests, we study conversation recommendation for a specific user, where we measure how a user’s preferences match a conversation’s context. This work is also related to user response prediction (Artzi et al., 2012; Zhang et al., 2015) and post recommendation (Duan et al., 2010; Chen et al., 2012; Yan et al., 2012; Hong et al., 2013). While most of these studies focus on post modeling, we examine conversation context to predict user engagements, which goes beyond the postlevel prediction task. Other prior work examining conversation-level recommendation relies on either manual features (Chen et al., 2011) or shallow word occurrence patterns (Zeng et al., 2018), largely ignoring the useful features from historical user interactions. On the contrary, we utilize online user interactions in the conversation history, to allow the inclusion of richer information of modeli"
D19-1470,D15-1259,1,0.899584,"Missing"
D19-1470,P16-1199,1,0.909654,"Missing"
D19-1470,J18-4008,1,0.830471,"Missing"
D19-1470,W02-0109,0,0.181076,"ter dataset were mainly posted from Jan 23 to Feb 8, 2011, and discussion threads in Reddit dataset were posted from Jan to Dec, 2008. To discover the whole conversations, we retrieved all messages with replying relations (indicated by “parent id” property in Reddit corpus, for example), and recorded their authors and parent messages. Finally, conversations with only one message were removed. We applied the Glove tweet preprocessing toolkit (Pennington et al., 2014)3 on the Twitter dataset. As for the Reddit dataset, we performed tokenization using open source natural language toolkit (NLTK) (Loper and Bird, 2002), with links replaced to a generic tag “URL” and all number tokens removed. We maintained a vocabulary with all the rest characters appearing in the corpus for both datasets, including punctuation and emoticons. Data Statistics and Analysis. The statistics of two datasets are shown in Table 1, with more information in Figure 4. We can observe that Reddit dataset contains more conversations, with a higher average number of conversations per user. On the other hand, Twitter conversations are longer, with fewer participants. Figure 4(a) shows that most users participate in very few conversations"
D19-1470,D17-1159,0,0.360881,"o, a reply tree is extended to a directed graph (such as the one in Figure 1), with both replying and temporal interactions encoded and therefore named as an interaction graph. For each turn t on the graph, we distinguish its neighbors into predecessors, denoted by E p (t), and successors, E s (t). Then, we employ graph-structured networks to model the interaction structure. There are two modeling methods discussed here: Graph-State LSTM (Long Short-Term Memory) (henceforth GLSTM) (Beck et al., 2018; Song et al., 2018) and Graph Convolutional Networks (henceforth GCN) (Kipf and Welling, 2017; Marcheggiani and Titov, 2017), whose empirical effectiveness will be compared in Section 5.1. Here we present their architecture in Figure 3 and describe how they model conversation interactions below. Graph-State LSTM. We start with GLSTM and show its architecture in Figure 3(a). It is an extension of LSTM from sequence to graph structure, where a turn’s hidden states are updated conditioned on both the turn-level representation rtT R 4636 and the states of all its neighbors on the graph. The update strategy is the same as standard LSTM (Hochreiter and Schmidhuber, 1997), except for the following formula, which can be us"
D19-1470,C18-1322,0,0.0823794,"Missing"
D19-1470,P18-1125,0,0.0219937,"we 1 The datasets and codes are available at: https:// github.com/zxshamson/neural-conv-rec investigate the challenging task of first time replies prediction, where our model again produces significantly better results than existing popular recommendation models. 2 Related Work Our work is in line with conversation behavior analysis, where studies explore user interactions in ongoing conversations (Ritter et al., 2010) and how they signal the conversations’ future trajectory, such as continued activity (Backstrom et al., 2013; Jiao et al., 2018; Zeng et al., 2019) and the risk of going awry (Zhang et al., 2018). Different from these proposals which do not model personal interests, we study conversation recommendation for a specific user, where we measure how a user’s preferences match a conversation’s context. This work is also related to user response prediction (Artzi et al., 2012; Zhang et al., 2015) and post recommendation (Duan et al., 2010; Chen et al., 2012; Yan et al., 2012; Hong et al., 2013). While most of these studies focus on post modeling, we examine conversation context to predict user engagements, which goes beyond the postlevel prediction task. Other prior work examining conversatio"
D19-1470,D14-1162,0,0.0834365,"er is from Zeng et al. (2019), which is comprised of discussion threads about political issues on Reddit, a popular discussion website. The tweets in Twitter dataset were mainly posted from Jan 23 to Feb 8, 2011, and discussion threads in Reddit dataset were posted from Jan to Dec, 2008. To discover the whole conversations, we retrieved all messages with replying relations (indicated by “parent id” property in Reddit corpus, for example), and recorded their authors and parent messages. Finally, conversations with only one message were removed. We applied the Glove tweet preprocessing toolkit (Pennington et al., 2014)3 on the Twitter dataset. As for the Reddit dataset, we performed tokenization using open source natural language toolkit (NLTK) (Loper and Bird, 2002), with links replaced to a generic tag “URL” and all number tokens removed. We maintained a vocabulary with all the rest characters appearing in the corpus for both datasets, including punctuation and emoticons. Data Statistics and Analysis. The statistics of two datasets are shown in Table 1, with more information in Figure 4. We can observe that Reddit dataset contains more conversations, with a higher average number of conversations per user."
D19-1470,N10-1020,0,0.152077,"framework. A case study further reveals important interaction features captured by our model, which indicate their conversation entries and hence explain our model’s advanced performance. Finally, we 1 The datasets and codes are available at: https:// github.com/zxshamson/neural-conv-rec investigate the challenging task of first time replies prediction, where our model again produces significantly better results than existing popular recommendation models. 2 Related Work Our work is in line with conversation behavior analysis, where studies explore user interactions in ongoing conversations (Ritter et al., 2010) and how they signal the conversations’ future trajectory, such as continued activity (Backstrom et al., 2013; Jiao et al., 2018; Zeng et al., 2019) and the risk of going awry (Zhang et al., 2018). Different from these proposals which do not model personal interests, we study conversation recommendation for a specific user, where we measure how a user’s preferences match a conversation’s context. This work is also related to user response prediction (Artzi et al., 2012; Zhang et al., 2015) and post recommendation (Duan et al., 2010; Chen et al., 2012; Yan et al., 2012; Hong et al., 2013). Whil"
D19-1470,P18-1150,0,0.118242,"ind of edges to indicate chronological order (such as the blue arrows in Figure 1). In doing so, a reply tree is extended to a directed graph (such as the one in Figure 1), with both replying and temporal interactions encoded and therefore named as an interaction graph. For each turn t on the graph, we distinguish its neighbors into predecessors, denoted by E p (t), and successors, E s (t). Then, we employ graph-structured networks to model the interaction structure. There are two modeling methods discussed here: Graph-State LSTM (Long Short-Term Memory) (henceforth GLSTM) (Beck et al., 2018; Song et al., 2018) and Graph Convolutional Networks (henceforth GCN) (Kipf and Welling, 2017; Marcheggiani and Titov, 2017), whose empirical effectiveness will be compared in Section 5.1. Here we present their architecture in Figure 3 and describe how they model conversation interactions below. Graph-State LSTM. We start with GLSTM and show its architecture in Figure 3(a). It is an extension of LSTM from sequence to graph structure, where a turn’s hidden states are updated conditioned on both the turn-level representation rtT R 4636 and the states of all its neighbors on the graph. The update strategy is the sa"
D19-1470,P12-1054,0,0.783176,"interest. To address this issue, we study the problem of online conversation recommendation, with the goal of identifying conversations that fit a user’s preferences, hence likely to result in the user’s future engagement. ∗ This work was mainly conducted when Jing Li was affiliated with Tencent AI Lab, Shenzhen, China. In previous studies, it has been shown that effective online conversation recommendation has the potential to produce more positive online social interaction experience (Chen et al., 2011; Zeng et al., 2018). Prior work on this subject has focused on post-level recommendation (Yan et al., 2012; Chen et al., 2012), or conversation-level suggestion with handcrafted features (Chen et al., 2011) and word co-occurrence patterns (Zeng et al., 2018). Nevertheless, they ignore the useful information embedded in replying relations, where the conversation structure is formed via messages sent among users. In this work, we examine conversation context, and model the participants’ interactions therein. This approach enables deep representation learning that reflects personal interests and conversation preferences, together signaling what conversations a user is likely to be involved in. To ill"
D19-1470,Q18-1009,0,0.0993896,"Missing"
D19-1470,N18-1035,1,0.666232,"em. It is hence difficult for one to discover online discussions that are potentially of interest. To address this issue, we study the problem of online conversation recommendation, with the goal of identifying conversations that fit a user’s preferences, hence likely to result in the user’s future engagement. ∗ This work was mainly conducted when Jing Li was affiliated with Tencent AI Lab, Shenzhen, China. In previous studies, it has been shown that effective online conversation recommendation has the potential to produce more positive online social interaction experience (Chen et al., 2011; Zeng et al., 2018). Prior work on this subject has focused on post-level recommendation (Yan et al., 2012; Chen et al., 2012), or conversation-level suggestion with handcrafted features (Chen et al., 2011) and word co-occurrence patterns (Zeng et al., 2018). Nevertheless, they ignore the useful information embedded in replying relations, where the conversation structure is formed via messages sent among users. In this work, we examine conversation context, and model the participants’ interactions therein. This approach enables deep representation learning that reflects personal interests and conversation prefer"
D19-1470,P19-1270,1,0.88667,"lain our model’s advanced performance. Finally, we 1 The datasets and codes are available at: https:// github.com/zxshamson/neural-conv-rec investigate the challenging task of first time replies prediction, where our model again produces significantly better results than existing popular recommendation models. 2 Related Work Our work is in line with conversation behavior analysis, where studies explore user interactions in ongoing conversations (Ritter et al., 2010) and how they signal the conversations’ future trajectory, such as continued activity (Backstrom et al., 2013; Jiao et al., 2018; Zeng et al., 2019) and the risk of going awry (Zhang et al., 2018). Different from these proposals which do not model personal interests, we study conversation recommendation for a specific user, where we measure how a user’s preferences match a conversation’s context. This work is also related to user response prediction (Artzi et al., 2012; Zhang et al., 2015) and post recommendation (Duan et al., 2010; Chen et al., 2012; Yan et al., 2012; Hong et al., 2013). While most of these studies focus on post modeling, we examine conversation context to predict user engagements, which goes beyond the postlevel predict"
E17-1043,P16-1004,0,0.0168475,"nding techniques, (Mesnil et al., 2015) tag each word in a sentence with a predefined slot. A dialog modeling approach (Young et al., 2013) is also relevant to our task. However, this approach requires the definition of semantic slot names and human labeling of dialog acts in each utterance. There are a number of relevant applications of neural attention models. Nallapati et al. (2016) proposed using sequence to sequence model to summarize source code into natural language; they used a LSTM as encoder and another attentional LSTM and decoder to jointly learn content selection and realization. Dong and Lapata (2016) presented a sequence to sequence model with a tree structure decoder to map natural language to its logical form. The tree structure decoder shows superior performance on data that has nested output structure. It has also been used in other domains including machine translation (Sutskever et al., 2014; Bahdanau et al., 2014), and image caption generation (Fang et al., 2015). From this perspective, the most related work is (Mei et al., 2016) in which they proposed using a sequence-tosequence model to map navigational instructions in natural language to actions, which is conceptually similar to"
E17-1043,D10-1049,0,0.0175282,"ous context-free grammar in (Wu, 1997; Chiang et al., 2006). Quirk et al. (2015) created models to map natural language descriptions to executable code using productions from the formal language. Beltagy and Quirk (2016) improved the performance of semantic parsing on If-Then statements by using neural networks to model derivation trees and leveraged several techniques like synthetic training data from paraphrases and grammar combinations to improve generalization and reduce overfitting. In addition, there are some other research works focusing on text generation from structured data records. Angeli et al. (2010) proposed of a domain independent probabilistic approach to performing content selection and surface realization, making text generation as a local decision process. Konstas and Lapata (2013) created a global model to generate text from structured records, which jointly modeled content selection and surface realization with a probabilistic context-free grammar. In contrast, in this paper we focus on generating structured data records from text descriptions. 6 Conclusion In this paper we have presented an end to end method for extracting structured information from unstructured conversations us"
E17-1043,P16-1069,0,0.0127526,"mory Gate + Role: Mushroom Pizza, Qty=1, Combo=True Sprite, Size=Lrg, Qty=1, Modifier=( no ice ); Caesar Salad, Size=Med, Qty=1 Figure 6: Examples of outputs generated by each model for the conversation in first row. Mooney (2006) used syntax-based statistical machine translation method to do semantic parsing. Translation of natural language to a formal meaning representation is captured by a synchronous context-free grammar in (Wu, 1997; Chiang et al., 2006). Quirk et al. (2015) created models to map natural language descriptions to executable code using productions from the formal language. Beltagy and Quirk (2016) improved the performance of semantic parsing on If-Then statements by using neural networks to model derivation trees and leveraged several techniques like synthetic training data from paraphrases and grammar combinations to improve generalization and reduce overfitting. In addition, there are some other research works focusing on text generation from structured data records. Angeli et al. (2010) proposed of a domain independent probabilistic approach to performing content selection and surface realization, making text generation as a local decision process. Konstas and Lapata (2013) created"
E17-1043,P82-1020,0,0.751966,"Missing"
E17-1043,E06-1047,0,0.012009,"o ice ); Caesar Salad, Size=Med, Qty=1 Mushroom Pizza, Qty=1, Combo=True Sprite, Size=Lrg, Qty=1, Modifier=( no ice ); Caesar Salad, Size=Med, Qty=1 NAM + Memory Gate + Role: Mushroom Pizza, Qty=1, Combo=True Sprite, Size=Lrg, Qty=1, Modifier=( no ice ); Caesar Salad, Size=Med, Qty=1 Figure 6: Examples of outputs generated by each model for the conversation in first row. Mooney (2006) used syntax-based statistical machine translation method to do semantic parsing. Translation of natural language to a formal meaning representation is captured by a synchronous context-free grammar in (Wu, 1997; Chiang et al., 2006). Quirk et al. (2015) created models to map natural language descriptions to executable code using productions from the formal language. Beltagy and Quirk (2016) improved the performance of semantic parsing on If-Then statements by using neural networks to model derivation trees and leveraged several techniques like synthetic training data from paraphrases and grammar combinations to improve generalization and reduce overfitting. In addition, there are some other research works focusing on text generation from structured data records. Angeli et al. (2010) proposed of a domain independent proba"
E17-1043,P14-1062,0,0.0096304,"how much new information is added. The reset gate, rt , controls to what extent the history state contributes to the hypothesis state. If rt is zero, then GRU ignores all the history information. The conversation encoding is obtained by concatenating the GRU hidden state vectors from the forward and backward directions. Thus the encoder operation can be summarized as follows Encoder Network The encoder network is designed to generate a semantically meaningful representation of unstructured conversations. Several neural network architectures have been proposed for this purpose, including CNNs (Kalchbrenner et al., 2014; Hu et al., 2014), RNNs (Sutskever et al., 2014) and LSTMs (Hochreiter and Schmidhuber, 1997). In this work, we use an encoder constructed from a recurrent neural network with gated RNN units (GRU) (Cho et al., 2014). The GRU has been shown to alleviate the gradient vanishing problem of RNNs, enabling the model to learn long term dependencies in the input sequence. GRUs have been shown to perform comparably to LSTMs (Chung et al., 2014). At time t, the new state of a GRU is computed as follows: xt = We wt , t ∈ [1, T ] → − −−−→ ht = GRU (xt ), t ∈ [1, T ] ← − ←−−− ht = GRU (xt ), t ∈ [T, 1] →"
E17-1043,D14-1179,0,0.038577,"Missing"
E17-1043,P06-1115,0,0.0168239,"chanism, are sparser than those in 5(a) and better able to ignore uninformative terms in the input. Qualitative analysis Figure 6 shows a sample input and the output from each model. We see that the NAM augmented with memory gates and role information successfully captures the interaction and generates the correct record. To better understand the proposed model, we visualize the attention weight at each time step in Figure 5. The figure compares the attention weights produced by a conventional context mem5 Related Work There has been much work on information extraction from single utterances. Kate and Mooney (2006) proposed the use of SVM classifiers based on string kenels to parse natural language to a formal meaning representation. Wong and 456 Waiter: Customer: Waiter: Customer: My name’s Alexis how can I help you. Yeah can I get a cheese pizza combo? Something to drink? Change that to a mushroom pizza combo with a large sprite no ice. Waiter: Okay. Customer: Thank you. Waiter: Thank you. Using spoken language understanding techniques, (Mesnil et al., 2015) tag each word in a sentence with a predefined slot. A dialog modeling approach (Young et al., 2013) is also relevant to our task. However, this a"
E17-1043,N03-1017,0,0.0340152,"the conditional probability of the structured data record Y given the observed conversation X can be written as • In contrast to machine translation, we do not wish to create a verbatim “translation” of the input, but instead a logical distillation of it To attack this problem, we implemented two baselines and several sequence-to-sequence models. The first baseline is an information-retrieval approach based on a TF-IDF match (Salton et al., 1975) which finds the most similar conversation in the training data, and returns the associated order. The second uses phrase-based machine translation (Koehn et al., 2003) to “translate” from the conversational input to the tokens in the structured order. We compare these to a sequence-to-sequence (s2s) model with attention (Chan et al., 2016; Bahdanau et al., 2014; Devlin et al., 2015; Yao and Zweig, 2015; Sutskever et al., 2014; Mei et al., 2016), and then extend the s2s model with the addition of a gating mechanism on the attention memory and with an auxiliary input that indicates the conversational role of the speaker (customer or ordertaker). We show that it is in fact possible to extract the orders from conversations recorded at a real restaurant 1 , and"
E17-1043,P15-2017,1,0.847694,"t instead a logical distillation of it To attack this problem, we implemented two baselines and several sequence-to-sequence models. The first baseline is an information-retrieval approach based on a TF-IDF match (Salton et al., 1975) which finds the most similar conversation in the training data, and returns the associated order. The second uses phrase-based machine translation (Koehn et al., 2003) to “translate” from the conversational input to the tokens in the structured order. We compare these to a sequence-to-sequence (s2s) model with attention (Chan et al., 2016; Bahdanau et al., 2014; Devlin et al., 2015; Yao and Zweig, 2015; Sutskever et al., 2014; Mei et al., 2016), and then extend the s2s model with the addition of a gating mechanism on the attention memory and with an auxiliary input that indicates the conversational role of the speaker (customer or ordertaker). We show that it is in fact possible to extract the orders from conversations recorded at a real restaurant 1 , and achieve an F measure of over 70 from raw text and 65 from ASR transcriptions. 2 Mem. • The items in the structured order may appear in a different sequence than they are mentioned GRU Cu. : We’d like a large pizza Wa."
E17-1043,P16-1094,0,0.0307128,"tor, We is the embedding matrix, and xt is the word embedding for −−−→ ←−−− wt . The functions GRU (xt ) and GRU (xt ) represent the GRU operating in the forward and backward directions, respectively, with processing defined by equations 6–9. This produces a sequence of context vectors, h+ t which are subsequently consumed by an attention mechanism in the decoder. We use the final attention vector h+ T to initialize the hidden state of the decoder. 3.2 In many sequence-to-sequence models, there is no notion of different speakers with different roles. Inspired by the work in dialog generation (Li et al., 2016) and spoken language understanding (Hori et al., 2016), we propose the addition of speaker information into the encoder network to explicitly model the interaction patterns of the customer and order-taker. Specifically we learn separate word and role embeddings, and concatenate them to form the input. The input to the encoder network becomes: Memory Gate In most sequence-to-sequence tasks such as machine translation, every word in the input is important. However, in our scenario, where the input to the system is conversational speech, not all the words in the conversation contribute to the pre"
E17-1043,D15-1166,0,0.0177579,"to dynamically select important information (Yao et al., 2015; Hochreiter and Schmidhuber, 1997; Tu et al., 2016). In light of this, we propose the use of an additional memory gate to select important information from the memory vector. The memory gate we use consists of a single-layer feed-forward neural network gt = σ(Wg h+ t + bg ) xw = We wt , t ∈ [1, T ] t (16) = Wr rt , t ∈ [1, T ] (17) xrt xt = 3.3 xw t ⊕ xrt , t ∈ [1, T ] (18) Decoder Network The decoder network is used to predict the next word given all the previously predicted words and the context vectors from the encoder network (Luong et al., 2015; Bahdanau et al., 2014). We use an RNN with GRU units to predict each word yt sequentially based on the previously predicted word yt−1 and the output of the attention process at that computes a weighted combination of the context vectors in memory. If we define st as the hidden layer of the decoder at time t, the decoder’s operation can be expressed as (14) where σ is a sigmoid activation function and Wg and bg are weight matrix and bias, respectively, and h+ t is the context vector at time t defined in equation 10. The gate is then applied to the context vector h+ t using an element-wise mul"
E17-1043,P16-5005,0,0.0155655,"nput. The input to the encoder network becomes: Memory Gate In most sequence-to-sequence tasks such as machine translation, every word in the input is important. However, in our scenario, where the input to the system is conversational speech, not all the words in the conversation contribute to the prediction of structured data record. For example, it is reasonable to ignore the chit-chat that is present in many conversations. Further, in other tasks, gating mechanisms have been shown to be useful to dynamically select important information (Yao et al., 2015; Hochreiter and Schmidhuber, 1997; Tu et al., 2016). In light of this, we propose the use of an additional memory gate to select important information from the memory vector. The memory gate we use consists of a single-layer feed-forward neural network gt = σ(Wg h+ t + bg ) xw = We wt , t ∈ [1, T ] t (16) = Wr rt , t ∈ [1, T ] (17) xrt xt = 3.3 xw t ⊕ xrt , t ∈ [1, T ] (18) Decoder Network The decoder network is used to predict the next word given all the previously predicted words and the context vectors from the encoder network (Luong et al., 2015; Bahdanau et al., 2014). We use an RNN with GRU units to predict each word yt sequentially base"
E17-1043,N06-1056,0,0.122968,"Missing"
E17-1043,J97-3002,0,0.0465021,"difier=( no ice ); Caesar Salad, Size=Med, Qty=1 Mushroom Pizza, Qty=1, Combo=True Sprite, Size=Lrg, Qty=1, Modifier=( no ice ); Caesar Salad, Size=Med, Qty=1 NAM + Memory Gate + Role: Mushroom Pizza, Qty=1, Combo=True Sprite, Size=Lrg, Qty=1, Modifier=( no ice ); Caesar Salad, Size=Med, Qty=1 Figure 6: Examples of outputs generated by each model for the conversation in first row. Mooney (2006) used syntax-based statistical machine translation method to do semantic parsing. Translation of natural language to a formal meaning representation is captured by a synchronous context-free grammar in (Wu, 1997; Chiang et al., 2006). Quirk et al. (2015) created models to map natural language descriptions to executable code using productions from the formal language. Beltagy and Quirk (2016) improved the performance of semantic parsing on If-Then statements by using neural networks to model derivation trees and leveraged several techniques like synthetic training data from paraphrases and grammar combinations to improve generalization and reduce overfitting. In addition, there are some other research works focusing on text generation from structured data records. Angeli et al. (2010) proposed of a do"
E17-1043,K16-1028,0,0.0601406,"Alexis how can I help you. Yeah can I get a cheese pizza combo? Something to drink? Change that to a mushroom pizza combo with a large sprite no ice. Waiter: Okay. Customer: Thank you. Waiter: Thank you. Using spoken language understanding techniques, (Mesnil et al., 2015) tag each word in a sentence with a predefined slot. A dialog modeling approach (Young et al., 2013) is also relevant to our task. However, this approach requires the definition of semantic slot names and human labeling of dialog acts in each utterance. There are a number of relevant applications of neural attention models. Nallapati et al. (2016) proposed using sequence to sequence model to summarize source code into natural language; they used a LSTM as encoder and another attentional LSTM and decoder to jointly learn content selection and realization. Dong and Lapata (2016) presented a sequence to sequence model with a tree structure decoder to map natural language to its logical form. The tree structure decoder shows superior performance on data that has nested output structure. It has also been used in other domains including machine translation (Sutskever et al., 2014; Bahdanau et al., 2014), and image caption generation (Fang et"
E17-1043,J03-1002,0,0.00835795,"d the training set of transcriptions as a collection of documents, each mapped to a corresponding order. The test conversation was used as a query to find the most similar training set conversation. The corresponding order was returned as the estimated order. In our experiment, we use TFIDF to compute the similarity score. PBMT: The goal of a phrase-based translation model is to map a conversation into its structured record with alignment and language models. In our experiments, we use the Moses decoder, a state-of-the-art phrase-based MT system available for research purposes. We use GIZA++ (Och and Ney, 2003) to learn word alignment and irstlm to learn the language model. The models are trained on the conversation/order pairs in the training set and used to predict the structured data record given a conversation. 455 hi how can i help you yeah i'd like a mushroom peperoni pizza no cheese a large ceaser a large coke okay anything else for you nah okay </s> Figure 4: Example of memory gate weights at each time stamp. (a) Attention weight of NAM (b) Attention weight of NAM with memory gate Figure 5: Examples of attention weights of models (a) without memory gate and (b) with memory gate. (b) shows s"
E17-1043,P02-1040,0,0.101224,"second is generated by a speech recognition decoder that was trained on the conversations in the training set. We denote the second set as ASR-dev and ASR-test. Table 2 lists the statistics of the data sets. Note that the audio of a conversation was collected as a single file and then automatically segmented into turns for ASR decoding. This process was not perfect and likely introduced some errors. Thus, the average length and number of turns of differ between the ASR transcriptions and the manual transcriptions. 4.3 Evaluation A typical metric to evaluate a generation system is BLEU score (Papineni et al., 2002) which uses ngram overlap to quantify the degree to which a hypothesis matches the reference. However, our scenario is more demanding: order items are either correct or incorrect. Therefore, we adopt precision and recall at the item level as our evaluation metric. Note that an item is defined as a row in the structured data record and typically includes multiple fields. Using Table 1 as an example, there are three items to be scored. Only when the model produces an item that is exactly the same as the reference item do we count it as correct. As an additional measure, we report accuracy of the"
E17-1043,P15-1085,0,0.0149153,"Size=Med, Qty=1 Mushroom Pizza, Qty=1, Combo=True Sprite, Size=Lrg, Qty=1, Modifier=( no ice ); Caesar Salad, Size=Med, Qty=1 NAM + Memory Gate + Role: Mushroom Pizza, Qty=1, Combo=True Sprite, Size=Lrg, Qty=1, Modifier=( no ice ); Caesar Salad, Size=Med, Qty=1 Figure 6: Examples of outputs generated by each model for the conversation in first row. Mooney (2006) used syntax-based statistical machine translation method to do semantic parsing. Translation of natural language to a formal meaning representation is captured by a synchronous context-free grammar in (Wu, 1997; Chiang et al., 2006). Quirk et al. (2015) created models to map natural language descriptions to executable code using productions from the formal language. Beltagy and Quirk (2016) improved the performance of semantic parsing on If-Then statements by using neural networks to model derivation trees and leveraged several techniques like synthetic training data from paraphrases and grammar combinations to improve generalization and reduce overfitting. In addition, there are some other research works focusing on text generation from structured data records. Angeli et al. (2010) proposed of a domain independent probabilistic approach to"
he-etal-2012-quantising,W11-0705,0,\N,Missing
he-etal-2012-quantising,pak-paroubek-2010-twitter,0,\N,Missing
I05-1037,C02-1150,0,0.0766252,"Missing"
I05-1037,A00-1023,1,\N,Missing
I05-1037,W01-1203,0,\N,Missing
I05-1037,N01-1005,0,\N,Missing
I05-1037,W03-1208,0,\N,Missing
I05-1037,P04-1072,0,\N,Missing
I05-1037,H01-1069,0,\N,Missing
I05-1037,A00-1041,0,\N,Missing
I05-3013,W03-0420,0,0.0164323,"by forming words in capital with the first letters of a series of either English words or Chinese Pinyin. Chinese Pinyin is a popular approach to Chinese character input. Some Pinyin input methods incorporate lexical intelligence to support word or 96 rency. The objective is achieved by employing either handcrafted knowledge or supervised learning techniques. The latter is currently dominating in NER amongst which the most popular methods are decision tree (Sekine et al., 1998; Pailouras et al., 2000), Hidden Markov Model (Zhang et al., 2003; Zhao, 2004), maximum entropy (Chieu and Ng, 2002; Bender et al., 2003), and support vector machines (Isozaki and Kazawa, 2002; Takeuchi and Collier, 2002; Mayfield, 2003). From the linguistic perspective, NIL expressions are rather different from named entities in nature. Firstly, named entity is typically noun or noun phrase (NP), but NIL expression can be any kind, e.g. number “94” in NIL represents “ህᰃ” which is a verb meaning “exactly be”. Secondly, named entities often have well-defined meanings in text and are tractable from a standard dictionary; but NIL expressions are either unknown to the dictionary or ambiguous. For example, “⿔佁” appears in convention"
I05-3013,C02-1025,0,0.0156351,"yms are then created by forming words in capital with the first letters of a series of either English words or Chinese Pinyin. Chinese Pinyin is a popular approach to Chinese character input. Some Pinyin input methods incorporate lexical intelligence to support word or 96 rency. The objective is achieved by employing either handcrafted knowledge or supervised learning techniques. The latter is currently dominating in NER amongst which the most popular methods are decision tree (Sekine et al., 1998; Pailouras et al., 2000), Hidden Markov Model (Zhang et al., 2003; Zhao, 2004), maximum entropy (Chieu and Ng, 2002; Bender et al., 2003), and support vector machines (Isozaki and Kazawa, 2002; Takeuchi and Collier, 2002; Mayfield, 2003). From the linguistic perspective, NIL expressions are rather different from named entities in nature. Firstly, named entity is typically noun or noun phrase (NP), but NIL expression can be any kind, e.g. number “94” in NIL represents “ህᰃ” which is a verb meaning “exactly be”. Secondly, named entities often have well-defined meanings in text and are tractable from a standard dictionary; but NIL expressions are either unknown to the dictionary or ambiguous. For example, “⿔佁”"
I05-3013,C02-1054,0,0.0181297,"of a series of either English words or Chinese Pinyin. Chinese Pinyin is a popular approach to Chinese character input. Some Pinyin input methods incorporate lexical intelligence to support word or 96 rency. The objective is achieved by employing either handcrafted knowledge or supervised learning techniques. The latter is currently dominating in NER amongst which the most popular methods are decision tree (Sekine et al., 1998; Pailouras et al., 2000), Hidden Markov Model (Zhang et al., 2003; Zhao, 2004), maximum entropy (Chieu and Ng, 2002; Bender et al., 2003), and support vector machines (Isozaki and Kazawa, 2002; Takeuchi and Collier, 2002; Mayfield, 2003). From the linguistic perspective, NIL expressions are rather different from named entities in nature. Firstly, named entity is typically noun or noun phrase (NP), but NIL expression can be any kind, e.g. number “94” in NIL represents “ህᰃ” which is a verb meaning “exactly be”. Secondly, named entities often have well-defined meanings in text and are tractable from a standard dictionary; but NIL expressions are either unknown to the dictionary or ambiguous. For example, “⿔佁” appears in conventional dictionary with the meaning of Chinese porridge, but"
I05-3013,N01-1025,0,0.0103328,"o be a NIL expression if it is succeeded by a unit word. With this pattern, “8” within sentence “ҪᎹњ  ϾᇣᯊǄ (He has been working for eight hours.)” is not recognized as a NIL expression. NILER System 5.1 Architecture We develop NILER system to recognize NIL expressions in NIL text and convert them to normal language text. The latter functionality is discussed in other literatures. Architecture of NILER system is presented in Figure 2. Chat Text 5.2.2 Method II: Support Vector Machines Support vector machines (SVM) method produces high performance in many classification tasks (Joachims, 1998; Kudo and Matsumoto, 2001). As SVM can handle large numbers of features efficiently, we employ SVM classification method to NIL expression recognition. Suppose we have a set of training data for a two-class classification problem {(x1,y1), (x2, y2),…,(xN, yN)}, where xi  R D (i 1,2,...N ) is a feature vector of the i-th order sample in the training set and yi  {1,1} is the label for the sample. The goal of SVM is to find a decision function that accurately predicts y for unseen x. A non-linear SVM classifier gives a decision function f ( x) sign( g ( x)) for an input vector x, where Word Segmentation Word POS Taggi"
I05-3013,W03-0429,0,0.013032,". Chinese Pinyin is a popular approach to Chinese character input. Some Pinyin input methods incorporate lexical intelligence to support word or 96 rency. The objective is achieved by employing either handcrafted knowledge or supervised learning techniques. The latter is currently dominating in NER amongst which the most popular methods are decision tree (Sekine et al., 1998; Pailouras et al., 2000), Hidden Markov Model (Zhang et al., 2003; Zhao, 2004), maximum entropy (Chieu and Ng, 2002; Bender et al., 2003), and support vector machines (Isozaki and Kazawa, 2002; Takeuchi and Collier, 2002; Mayfield, 2003). From the linguistic perspective, NIL expressions are rather different from named entities in nature. Firstly, named entity is typically noun or noun phrase (NP), but NIL expression can be any kind, e.g. number “94” in NIL represents “ህᰃ” which is a verb meaning “exactly be”. Secondly, named entities often have well-defined meanings in text and are tractable from a standard dictionary; but NIL expressions are either unknown to the dictionary or ambiguous. For example, “⿔佁” appears in conventional dictionary with the meaning of Chinese porridge, but in NIL text it represents “ ୰” which surpri"
I05-3013,W98-1120,0,0.0327056,"ut full Chinese sentences in textbased chatting environment, e.g. over the mobile phone. Thus abbreviations and acronyms are then created by forming words in capital with the first letters of a series of either English words or Chinese Pinyin. Chinese Pinyin is a popular approach to Chinese character input. Some Pinyin input methods incorporate lexical intelligence to support word or 96 rency. The objective is achieved by employing either handcrafted knowledge or supervised learning techniques. The latter is currently dominating in NER amongst which the most popular methods are decision tree (Sekine et al., 1998; Pailouras et al., 2000), Hidden Markov Model (Zhang et al., 2003; Zhao, 2004), maximum entropy (Chieu and Ng, 2002; Bender et al., 2003), and support vector machines (Isozaki and Kazawa, 2002; Takeuchi and Collier, 2002; Mayfield, 2003). From the linguistic perspective, NIL expressions are rather different from named entities in nature. Firstly, named entity is typically noun or noun phrase (NP), but NIL expression can be any kind, e.g. number “94” in NIL represents “ህᰃ” which is a verb meaning “exactly be”. Secondly, named entities often have well-defined meanings in text and are tractable"
I05-3013,W02-2029,0,0.0141971,"lish words or Chinese Pinyin. Chinese Pinyin is a popular approach to Chinese character input. Some Pinyin input methods incorporate lexical intelligence to support word or 96 rency. The objective is achieved by employing either handcrafted knowledge or supervised learning techniques. The latter is currently dominating in NER amongst which the most popular methods are decision tree (Sekine et al., 1998; Pailouras et al., 2000), Hidden Markov Model (Zhang et al., 2003; Zhao, 2004), maximum entropy (Chieu and Ng, 2002; Bender et al., 2003), and support vector machines (Isozaki and Kazawa, 2002; Takeuchi and Collier, 2002; Mayfield, 2003). From the linguistic perspective, NIL expressions are rather different from named entities in nature. Firstly, named entity is typically noun or noun phrase (NP), but NIL expression can be any kind, e.g. number “94” in NIL represents “ህᰃ” which is a verb meaning “exactly be”. Secondly, named entities often have well-defined meanings in text and are tractable from a standard dictionary; but NIL expressions are either unknown to the dictionary or ambiguous. For example, “⿔佁” appears in conventional dictionary with the meaning of Chinese porridge, but in NIL text it represents “"
I05-3013,W03-1730,0,0.0570468,"evolution from psychological and cognitive perspectives (Danet, 2002; McElhearn, 2000; Nishimura, 2003). Researchers claim that languages have never been changing as fast as today since inception of the Internet; and the language for Internet communication, i.e. NIL, gets more concise and effective than formal language. Processing NIL text requires unconventional linguistic knowledge and techniques. Unfortunately, developed to handle formal language text, the existing natural language processing (NLP) approaches exhibit less effectiveness in dealing with NIL text. For example, we use ICTCLAS (Zhang et al., 2003) tool to process sentence “Ҫ㒚 ܿ㒚㽕ᓔӮଞ˛(Is he going to attend a meeting?)”. The word segmentation result is “Ҫ|㒚|ܿ|㒚|㽕|ᓔӮ|ଞ|˛”. In this sentence , “㒚 ܿ 㒚 (xi4 ba1 xi4)” is a NIL expression which means ‘is he ….?’ in this case. It can be concluded that without identifying the expression, further Chinese text processing techniques are not able to produce reasonable result. This problem leads to our recent research in “NIL is Not Nothing” project, which aims to produce techniques for NIL processing, thus avails understanding of change patterns and behaviors in language (particularly in Internet lan"
I05-3013,W04-1216,0,0.0121975,". Thus abbreviations and acronyms are then created by forming words in capital with the first letters of a series of either English words or Chinese Pinyin. Chinese Pinyin is a popular approach to Chinese character input. Some Pinyin input methods incorporate lexical intelligence to support word or 96 rency. The objective is achieved by employing either handcrafted knowledge or supervised learning techniques. The latter is currently dominating in NER amongst which the most popular methods are decision tree (Sekine et al., 1998; Pailouras et al., 2000), Hidden Markov Model (Zhang et al., 2003; Zhao, 2004), maximum entropy (Chieu and Ng, 2002; Bender et al., 2003), and support vector machines (Isozaki and Kazawa, 2002; Takeuchi and Collier, 2002; Mayfield, 2003). From the linguistic perspective, NIL expressions are rather different from named entities in nature. Firstly, named entity is typically noun or noun phrase (NP), but NIL expression can be any kind, e.g. number “94” in NIL represents “ህᰃ” which is a verb meaning “exactly be”. Secondly, named entities often have well-defined meanings in text and are tractable from a standard dictionary; but NIL expressions are either unknown to the dicti"
I17-4002,D14-1162,0,0.0889845,"Missing"
I17-4002,W10-0208,0,0.0330003,", lhlee@ntnu.edu.tw, wangjin@ynu.edu.cn, kfwong@se.cuhk.edu.hk 1 numerical values on multiple dimensions, such as valence-arousal (VA) space (Russell, 1980), as shown in Fig. 1. The valence represents the degree of pleasant and unpleasant (or positive and negative) feelings, and the arousal represents the degree of excitement and calm. Based on this twodimensional representation, any affective state can be represented as a point in the VA coordinate plane by determining the degrees of valence and arousal of given words (Wei et al., 2011; Malandrakis et al., 2011; Wang et al., 2016a) or texts (Kim et al., 2010; Paltoglou et al, 2013; Wang et al., 2016b). Dimensional sentiment analysis has emerged as a compelling topic for research with applications including antisocial behavior detection (Munezero et al., 2011), mood analysis (De Choudhury et al., 2012) and product review ranking (Ren and Nickerson, 2014) The IJCNLP 2017 features a shared task for dimensional sentiment analysis for Chinese words, providing an evaluation platform for the development and implementation of advanced techniques for affective computing. Sentiment lexicons with valence-arousal ratings are useful resources for the developm"
I17-4002,N16-1066,1,0.779642,"e output format is “term_id, valence_rating, arousal_rating”. Below are the input/output formats of the example words “好” (good), “非常好” (very good), “滿意” (satisfy) and “不滿意” (not satisfy). Example 1: Input: 1, 好 Output: 1, 6.8, 5.2 Example 2: Input: 2, 非常好 Output: 2, 8.500, 6.625 Example 3: Input: 3, 滿意 Output: 3, 7.2, 5.6 Example 4: Input: 4, 不滿意 Output: 4, 2.813, 5.688 Test set: For single words, we selected 750 words that were not included in the CVAW 2.0 from NTUSD (Ku and Chen, 2007) using the same method presented in our previous task on Dimensional Sentiment Analysis for Chinese Words (Yu et al, 2016b). Each single word in both training and test sets was annotated with valence-arousal ratings by five annotators and the average ratings were taken as ground truth. Each multi-word phrase was rated by at least 10 different annotators. Once the rating process was finished, a corpus clean up procedure was performed to remove outlier ratings that did not fall within the mean plus/minus 1.5 standard deviations. They were then excluded from the calculation of the average ratings for each phrase. The policy of this shared task was implemented as is an open test. That is, in addition to the above of"
I17-4002,P16-2037,1,0.861068,"ontact: lcyu@saturn.yzu.edu.tw, lhlee@ntnu.edu.tw, wangjin@ynu.edu.cn, kfwong@se.cuhk.edu.hk 1 numerical values on multiple dimensions, such as valence-arousal (VA) space (Russell, 1980), as shown in Fig. 1. The valence represents the degree of pleasant and unpleasant (or positive and negative) feelings, and the arousal represents the degree of excitement and calm. Based on this twodimensional representation, any affective state can be represented as a point in the VA coordinate plane by determining the degrees of valence and arousal of given words (Wei et al., 2011; Malandrakis et al., 2011; Wang et al., 2016a) or texts (Kim et al., 2010; Paltoglou et al, 2013; Wang et al., 2016b). Dimensional sentiment analysis has emerged as a compelling topic for research with applications including antisocial behavior detection (Munezero et al., 2011), mood analysis (De Choudhury et al., 2012) and product review ranking (Ren and Nickerson, 2014) The IJCNLP 2017 features a shared task for dimensional sentiment analysis for Chinese words, providing an evaluation platform for the development and implementation of advanced techniques for affective computing. Sentiment lexicons with valence-arousal ratings are usef"
I17-4002,W11-3704,0,0.0240642,"Missing"
J18-4008,W05-0613,0,0.0178369,"auses, namely, elementary discourse units. Adjacent units are linked by rhetorical relations—condition, comparison, elaboration, and so forth. Based on RST, early work uses hand-coded rules for automatic discourse analysis (Marcu 2000; Thanh, Abeysinghe, and Huyck 2004). Later, thanks to the development of large-scale discourse corpora—RST corpus (Carlson, Marcu, and Okurovsky 2001), Graph Bank corpus (Wolf and Gibson 2005), and Penn Discourse Treebank (Prasad et al. 2008)—data-driven and learning-based discourse parsers that exploit various manually designed features (Soricut and Marcu 2003; Baldridge and Lascarides 2005; Fisher and Roark 2007; Lin, Kan, and Ng 2009; Subba and Eugenio 2009; Joty, Carenini, and Ng 2012; Feng and Hirst 2014) and representative learning (Ji and Eisenstein 2014; Li, Li, and Hovy 2014) became popular. 2.2.2 Discourse Analysis on Conversations. Stolcke et al. (2000) provide one of the first studies focusing on this problem, and it provides a general schema of understanding conversations with discourse analysis. Because of the complex structure and informal language style, discourse parsing on conversations is still a challenging problem (Perret et al. 2016). Most research focuses o"
J18-4008,P06-1026,0,0.123548,"Missing"
J18-4008,D14-1226,0,0.0636155,"Missing"
J18-4008,W04-3240,0,0.130138,"Missing"
J18-4008,W09-3951,0,0.0825383,"Missing"
J18-4008,P15-1077,0,0.0674667,"Missing"
J18-4008,P06-1039,0,0.142338,"Missing"
J18-4008,P16-2044,0,0.0311334,"Another potential line is to combine our work with representation learning on social media. Although some previous studies have provided intriguing approaches to learning representations at the level of words (Mikolov et al. 2013; Mikolov, Yih, and Zweig 2013), sentences (Le and Mikolov 2014), and paragraphs (Kiros et al. 2015), they are limited in modeling social media content with colloquial relations. Following similar ideas in this work, where discourse and topics are jointly explored, we can conduct other types of representation learning, embeddings for words (Li et al. 2017b), messages (Dhingra et al. 2016), or users (Ding, Bickel, and Pan 2017), in the context of conversations, which should complement social media representation learning and vice versa. Appendix A In this section, we present the key steps for inferring our joint model of conversational discourse and latent topics. Its generation process has been described in Section 3. As described in Section 3, we use collapsed Gibbs sampling (Griffiths et al. 2004) for model inference. Before providing the formula of sampling steps, we first define the notations of all variables used in the formulations of Gibbs sampling, described in Table A"
J18-4008,D17-1241,0,0.0428534,"Missing"
J18-4008,C12-1047,0,0.0996001,"hy content and noncontent background (general information) (Daum´e and Marcu 2006; Haghighi and ¨ 2010), and (2) to cluster sentences Vanderwende 2009; C ¸ elikyilmaz and Hakkani-Tur 724 Li et al. A Joint Model of Discourse and Topics on Microblogs or documents into topics, with summaries then generated from each topic cluster for minimizing content redundancy (Salton et al. 1997; McKeown et al. 1999; Siddharthan, Nenkova, and McKeown 2004). Similar techniques have also been applied to summarize events or opinions on microblogs (Chakrabarti and Punera 2011; Long et al. 2011; Rosa et al. 2011; Duan et al. 2012; Shen et al. 2013; Meng et al. 2012). Our downstream application on microblog summarization lies in the research line of point (1), whereas we integrate the effects of discourse on key content identification, which has not been studied in any prior work. Also it is worth noting that, following point (2) to cluster messages before summarization is beyond the scope of this work because we are focusing on summarizing a single conversation tree, on which there are limited topics. We leave the potential of using our model to segment topics for multiconversation summarization to future work. 2.2 Di"
J18-4008,C10-1034,0,0.371052,"microblog conversations. 1. Introduction Over the past two decades, the Internet has been revolutionizing the way we communicate. Microblogging, a social networking channel over the Internet, further accelerates communication and information exchange. Popular microblog platforms, such as Twitter1 and Sina Weibo,2 have become important outlets for individuals to share information and voice opinions, which further benefit downstream applications such as instant detection of breaking events (Lin et al. 2010; Weng and Lee 2011; Peng et al. 2015), real-time and ad hoc search of microblog messages (Duan et al. 2010; Li et al. 2015b), public opinions and user behavior understanding on societal issues (Pak and Paroubek 2010; Popescu and Pennacchiotti 2010; Kouloumpis, Wilson, and Moore 2011), and so forth. However, the explosive growth of microblog data far outpaces human beings’ speed of reading and understanding. As a consequence, there is a pressing need for effective natural language processing (NLP) systems that can automatically identify gist information, and make sense of the unmanageable amount of user-generated social media content (Farzindar and Inkpen 2015). As one of the important and fundamen"
J18-4008,P14-1048,0,0.0169086,"nd so forth. Based on RST, early work uses hand-coded rules for automatic discourse analysis (Marcu 2000; Thanh, Abeysinghe, and Huyck 2004). Later, thanks to the development of large-scale discourse corpora—RST corpus (Carlson, Marcu, and Okurovsky 2001), Graph Bank corpus (Wolf and Gibson 2005), and Penn Discourse Treebank (Prasad et al. 2008)—data-driven and learning-based discourse parsers that exploit various manually designed features (Soricut and Marcu 2003; Baldridge and Lascarides 2005; Fisher and Roark 2007; Lin, Kan, and Ng 2009; Subba and Eugenio 2009; Joty, Carenini, and Ng 2012; Feng and Hirst 2014) and representative learning (Ji and Eisenstein 2014; Li, Li, and Hovy 2014) became popular. 2.2.2 Discourse Analysis on Conversations. Stolcke et al. (2000) provide one of the first studies focusing on this problem, and it provides a general schema of understanding conversations with discourse analysis. Because of the complex structure and informal language style, discourse parsing on conversations is still a challenging problem (Perret et al. 2016). Most research focuses on the detection of dialogue acts (DAs),4 which are 4 Dialogue act can be used interchangeably with speech act (Stolcke et"
J18-4008,P07-1062,0,0.0392749,"urse units. Adjacent units are linked by rhetorical relations—condition, comparison, elaboration, and so forth. Based on RST, early work uses hand-coded rules for automatic discourse analysis (Marcu 2000; Thanh, Abeysinghe, and Huyck 2004). Later, thanks to the development of large-scale discourse corpora—RST corpus (Carlson, Marcu, and Okurovsky 2001), Graph Bank corpus (Wolf and Gibson 2005), and Penn Discourse Treebank (Prasad et al. 2008)—data-driven and learning-based discourse parsers that exploit various manually designed features (Soricut and Marcu 2003; Baldridge and Lascarides 2005; Fisher and Roark 2007; Lin, Kan, and Ng 2009; Subba and Eugenio 2009; Joty, Carenini, and Ng 2012; Feng and Hirst 2014) and representative learning (Ji and Eisenstein 2014; Li, Li, and Hovy 2014) became popular. 2.2.2 Discourse Analysis on Conversations. Stolcke et al. (2000) provide one of the first studies focusing on this problem, and it provides a general schema of understanding conversations with discourse analysis. Because of the complex structure and informal language style, discourse parsing on conversations is still a challenging problem (Perret et al. 2016). Most research focuses on the detection of dial"
J18-4008,P11-2008,0,0.133607,"Missing"
J18-4008,N09-1041,0,0.248132,"as conversation starter. Topic Assignments. Messages on one conversation tree focus on related topics. To exploit such intuition in topic assignments, the topic of each message m on conversation tree c (i.e., zc,m ) is sampled from the topic mixture θc of conversation tree c. 3.2 Word-Level Modeling To distinguish varying types of word distributions to separately capture discourse, topic, and background representations, we follow the solutions from previous work to assign each word as a discrete and exact source that reflects one particular type of word representation (Daum´e and Marcu 2006; Haghighi and Vanderwende 2009; Ritter, Cherry, and Dolan 2010). To this end, for each word n in message m and tree c, a ternary variable xc,m,n ∈ {DISC, TOPIC, BACK} controls word n to fall into one of the three types: discourse, topic, and background word. In doing so, words in the given collection are explicitly separated into three types, based on which the word distributions representing discourse, topic, and background components are separated accordingly. Discourse words. (DISC) indicate the discourse role of a message; for example, in Figure 1, “How” and the question mark “?” reflect that [R1] should be assigned th"
J18-4008,P14-1002,0,0.0234826,"ded rules for automatic discourse analysis (Marcu 2000; Thanh, Abeysinghe, and Huyck 2004). Later, thanks to the development of large-scale discourse corpora—RST corpus (Carlson, Marcu, and Okurovsky 2001), Graph Bank corpus (Wolf and Gibson 2005), and Penn Discourse Treebank (Prasad et al. 2008)—data-driven and learning-based discourse parsers that exploit various manually designed features (Soricut and Marcu 2003; Baldridge and Lascarides 2005; Fisher and Roark 2007; Lin, Kan, and Ng 2009; Subba and Eugenio 2009; Joty, Carenini, and Ng 2012; Feng and Hirst 2014) and representative learning (Ji and Eisenstein 2014; Li, Li, and Hovy 2014) became popular. 2.2.2 Discourse Analysis on Conversations. Stolcke et al. (2000) provide one of the first studies focusing on this problem, and it provides a general schema of understanding conversations with discourse analysis. Because of the complex structure and informal language style, discourse parsing on conversations is still a challenging problem (Perret et al. 2016). Most research focuses on the detection of dialogue acts (DAs),4 which are 4 Dialogue act can be used interchangeably with speech act (Stolcke et al. 2000). 725 Computational Linguistics Volume 44,"
J18-4008,D12-1083,0,0.0384174,"Missing"
J18-4008,P13-1160,0,0.0181528,"Missing"
J18-4008,D15-1259,1,0.704119,"tions. 1. Introduction Over the past two decades, the Internet has been revolutionizing the way we communicate. Microblogging, a social networking channel over the Internet, further accelerates communication and information exchange. Popular microblog platforms, such as Twitter1 and Sina Weibo,2 have become important outlets for individuals to share information and voice opinions, which further benefit downstream applications such as instant detection of breaking events (Lin et al. 2010; Weng and Lee 2011; Peng et al. 2015), real-time and ad hoc search of microblog messages (Duan et al. 2010; Li et al. 2015b), public opinions and user behavior understanding on societal issues (Pak and Paroubek 2010; Popescu and Pennacchiotti 2010; Kouloumpis, Wilson, and Moore 2011), and so forth. However, the explosive growth of microblog data far outpaces human beings’ speed of reading and understanding. As a consequence, there is a pressing need for effective natural language processing (NLP) systems that can automatically identify gist information, and make sense of the unmanageable amount of user-generated social media content (Farzindar and Inkpen 2015). As one of the important and fundamental text analyti"
J18-4008,P16-1199,1,0.313837,"t al. 2013). Quan et al. (2015) propose self-aggregation-based topic modeling (SATM) that aggregates texts jointly with topic inference. Another popular solution is to take into account word relations to alleviate document-level word sparseness. Biterm topic model (BTM) directly models the generation of word-pair co-occurrence patterns in each individual message (Yan et al. 2013; Cheng et al. 2014). More recently, word embeddings trained by large-scale external data are leveraged to capture word relations and improve topic models on short texts (Das, Zaheer, and Dyer 2015; Nguyen et al. 2015; Li et al. 2016a, 2017a; Shi et al. 2017; Xun et al. 2017). To date, most efforts focus on content in messages, but ignore the rich discourse structure embedded in ubiquitous user interactions on microblog platforms. On microblogs, which were originally built for user communication and interaction, conversations are freely formed on issues of interests by reposting messages and replying to others. When joining a conversation, users generally post topically related content, which naturally provide effective contextual information for topic discovery. AlvarezMelis and Saveski (2016) have shown that simply aggr"
J18-4008,D14-1220,0,0.0410723,"Missing"
J18-4008,W04-1013,0,0.0251576,"scourse and latent topics is fully unsupervised, therefore does not require any manual annotation. For evaluation, we conduct quantitative and qualitative analysis on large-scale Twitter and Sina Weibo corpora. Experimental results show that topics induced by our model are more coherent than existing models. Qualitative analysis on discourse further shows that our model can yield meaningful clusters of words related to manually crafted discourse categories. In addition, we present an empirical study on downstream application of microblog conversation summarization. Empirical results on ROUGE (Lin 2004) show that summaries produced based on our joint model contain more salient information than state-of-the-art summarization systems. Human evaluation also indicates that our output summaries are competitive with existing unsupervised summarization systems in the aspects of informativeness, conciseness, and readability. In summary, our contributions in this article are 3-fold: • 722 Microblog posts organized as conversation trees for topic modeling. We propose a novel concept of representing microblog posts as conversation trees by connecting microblog posts based on reposting and replying Li e"
J18-4008,D09-1036,0,0.0944317,"Missing"
J18-4008,W11-0709,0,0.0644897,"Missing"
J18-4008,C12-1104,0,0.0289953,"ev et al. 2004), TF-IDF (Inouye and Kalita 2011), integer linear programming (Liu, Liu, and Weng 2011; Takamura, Yokono, and Okumura 2011), graph learning (Sharifi, Hutton, and Kalita 2010), and so on. Later, researchers found that standard summarization models are not suitable for microblog posts because of the severe redundancy, noise, and sparsity problems exhibited in short and colloquial messages (Chang et al. 2013; Li et al. 2015a). To solve these problems, one common solution is to use social signals such as the user influence and retweet counts to help summarization (Duan et al. 2012; Liu et al. 2012; Chang et al. 2013). Different from the aforementioned studies, we do not include external features such 726 Li et al. A Joint Model of Discourse and Topics on Microblogs as the social network structure, which ensures the general applicability of our approach when applied to domains without such information. Discourse has been reported useful to microblog summarization. Zhang et al. (2013) and Li et al. (2015a) leverage dialogue acts to indicate summary-worthy messages. In the field of conversation summarization from other domains (e.g., meetings, forums, and e-mails), it is also popular to l"
J18-4008,J00-3005,0,0.250894,"re theory (RST) (Mann and Thompson 1988) is one of the most influential discourse theories. According to its assumption, a coherent document can be represented by text units at different levels (e.g., clauses, sentences, paragraphs) in a hierarchical tree structure. In particular, the minimal units in RST (i.e., leaves of the tree structure) are defined as sub-sentential clauses, namely, elementary discourse units. Adjacent units are linked by rhetorical relations—condition, comparison, elaboration, and so forth. Based on RST, early work uses hand-coded rules for automatic discourse analysis (Marcu 2000; Thanh, Abeysinghe, and Huyck 2004). Later, thanks to the development of large-scale discourse corpora—RST corpus (Carlson, Marcu, and Okurovsky 2001), Graph Bank corpus (Wolf and Gibson 2005), and Penn Discourse Treebank (Prasad et al. 2008)—data-driven and learning-based discourse parsers that exploit various manually designed features (Soricut and Marcu 2003; Baldridge and Lascarides 2005; Fisher and Roark 2007; Lin, Kan, and Ng 2009; Subba and Eugenio 2009; Joty, Carenini, and Ng 2012; Feng and Hirst 2014) and representative learning (Ji and Eisenstein 2014; Li, Li, and Hovy 2014) became"
J18-4008,N13-1090,0,0.156114,"s regardless of the different discourse roles of messages. The work by Li et al. (2016b) serves as another prior effort to leverage conversation structure, captured by a supervised discourse tagger, on topic induction. Different from them, our model learns discourse structure for conversations in a fully unsupervised manner, which does not require annotated data. Another line of research tackles data sparseness by modeling word relations rather than word occurrences in documents. For example, recent research work has shown that distributional similarities of words captured by word embeddings (Mikolov et al. 2013; Mikolov, Yih, and Zweig 2013) are useful in recognizing interpretable topic word clusters from short texts (Das, Zaheer, and Dyer 2015; Nguyen et al. 2015; Li et al. 2016a, 2017a; Shi et al. 2017; Xun et al. 2017). These topic models heavily rely on meaningful word embeddings needed to be trained on a large-scale, high-quality external corpus, which should be both in the same domain and the same language as the data for topic modeling (Bollegala, Maehara, and Kawarabayashi 2015). However, such external resource is not always available. For example, to the best of our knowledge, there current"
J18-4008,D11-1024,0,0.0306012,"ocessing, which retains the same common settings to ensure comparable performance.18 Evaluation Metrics. Topic model evaluation is inherently difficult. Although in many previous studies perplexity is a popular metric to evaluate the predictive abilities of topic models given held-out data set with unseen words (Blei, Ng, and Jordan 2003), we do not consider perplexity here because high perplexity does not necessarily indicate semantically coherent topics in human perception (Chang et al. 2009). The quality of topics is commonly measured by UCI (Newman et al. 2010) and UMass coherence scores (Mimno et al. 2011), assuming that words representing a coherent topic are likely to co-occur within the same document. We only consider UMass coherence here as UMass and UCI generally agree with each other, according to Stevens et al. (2012). We also consider a newer evaluation metric, the CV coherence ¨ measure (Roder, Both, and Hinneburg 2015), as it has been proven to provide the scores closest to human evaluation compared with other widely used topic coherence metrics, including UCI and UMass scores.19 For the CV coherence measure, in brief, given a word list for topic representations (i.e., the top N words"
J18-4008,N06-1047,0,0.0509304,"we do not include external features such 726 Li et al. A Joint Model of Discourse and Topics on Microblogs as the social network structure, which ensures the general applicability of our approach when applied to domains without such information. Discourse has been reported useful to microblog summarization. Zhang et al. (2013) and Li et al. (2015a) leverage dialogue acts to indicate summary-worthy messages. In the field of conversation summarization from other domains (e.g., meetings, forums, and e-mails), it is also popular to leverage the pre-detected discourse structure for summarization (Murray et al. 2006; McKeown, Shrestha, and Rambow 2007; Wang and Cardie 2013; Bhatia, Biyani, and Mitra 2014; Bokaei, Sameti, and Liu 2016). Oya and Carenini (2014) and Qin, Wang, and Kim (2017) address discourse tagging together with salient content discovery on e-mails and meetings, and show the usefulness of their relations in summarization. For all the systems mentioned here, manually crafted tags and annotated data are required for discourse modeling. Instead, the discourse structure is discovered in a fully unsupervised manner in our model, which is represented by word distributions and can be different f"
J18-4008,N10-1012,0,0.234042,"(Phan, Nguyen, and Horiguchi 2008; Zeng et al. 2018a), and recommendation on microblogs (Zeng et al. 2018b). Conventionally, probabilistic topic models (e.g., probabilistic latent semantic analysis [Hofmann 1999] and latent Dirichlet allocation [Blei et al. 2003]) have achieved huge success over the past decade, owing to their fully unsupervised manner and ease of extension. The semantic structure discovered by these topic models have facilitated the progress of many research fields, for example, information retrieval (Boyd-Graber, Hu, and Mimno 2017), data mining (Lin et al. 2015), and NLP (Newman et al. 2010). Nevertheless, ascribing to their reliance on document-level word co-occurrence patterns, the progress is still limited to formal conventional documents such as news reports (Blei, Ng, and Jordan 2003) and scientific articles (Rosen-Zvi et al. 2004). The aforementioned models work poorly when directly applied to short and colloquial texts (e.g., microblog posts) owing to severe sparsity exhibited in such text genre (Wang and McCallum 2006; Hong and Davison 2010). Previous research has proposed several methods to deal with the sparsity issue in short texts. One common approach is to aggregate"
J18-4008,Q15-1022,0,0.575772,"ing 2010; Mehrotra et al. 2013). Quan et al. (2015) propose self-aggregation-based topic modeling (SATM) that aggregates texts jointly with topic inference. Another popular solution is to take into account word relations to alleviate document-level word sparseness. Biterm topic model (BTM) directly models the generation of word-pair co-occurrence patterns in each individual message (Yan et al. 2013; Cheng et al. 2014). More recently, word embeddings trained by large-scale external data are leveraged to capture word relations and improve topic models on short texts (Das, Zaheer, and Dyer 2015; Nguyen et al. 2015; Li et al. 2016a, 2017a; Shi et al. 2017; Xun et al. 2017). To date, most efforts focus on content in messages, but ignore the rich discourse structure embedded in ubiquitous user interactions on microblog platforms. On microblogs, which were originally built for user communication and interaction, conversations are freely formed on issues of interests by reposting messages and replying to others. When joining a conversation, users generally post topically related content, which naturally provide effective contextual information for topic discovery. AlvarezMelis and Saveski (2016) have shown"
J18-4008,N13-1039,0,0.0942168,"Missing"
J18-4008,W14-4318,0,0.0172156,"which ensures the general applicability of our approach when applied to domains without such information. Discourse has been reported useful to microblog summarization. Zhang et al. (2013) and Li et al. (2015a) leverage dialogue acts to indicate summary-worthy messages. In the field of conversation summarization from other domains (e.g., meetings, forums, and e-mails), it is also popular to leverage the pre-detected discourse structure for summarization (Murray et al. 2006; McKeown, Shrestha, and Rambow 2007; Wang and Cardie 2013; Bhatia, Biyani, and Mitra 2014; Bokaei, Sameti, and Liu 2016). Oya and Carenini (2014) and Qin, Wang, and Kim (2017) address discourse tagging together with salient content discovery on e-mails and meetings, and show the usefulness of their relations in summarization. For all the systems mentioned here, manually crafted tags and annotated data are required for discourse modeling. Instead, the discourse structure is discovered in a fully unsupervised manner in our model, which is represented by word distributions and can be different from any human designed discourse inventory. The effects of such discourse representations on salient content identification have not been explored"
J18-4008,pak-paroubek-2010-twitter,0,0.131623,"Missing"
J18-4008,N16-1013,0,0.0483436,"Missing"
J18-4008,prasad-etal-2008-penn,0,0.0237269,"n a hierarchical tree structure. In particular, the minimal units in RST (i.e., leaves of the tree structure) are defined as sub-sentential clauses, namely, elementary discourse units. Adjacent units are linked by rhetorical relations—condition, comparison, elaboration, and so forth. Based on RST, early work uses hand-coded rules for automatic discourse analysis (Marcu 2000; Thanh, Abeysinghe, and Huyck 2004). Later, thanks to the development of large-scale discourse corpora—RST corpus (Carlson, Marcu, and Okurovsky 2001), Graph Bank corpus (Wolf and Gibson 2005), and Penn Discourse Treebank (Prasad et al. 2008)—data-driven and learning-based discourse parsers that exploit various manually designed features (Soricut and Marcu 2003; Baldridge and Lascarides 2005; Fisher and Roark 2007; Lin, Kan, and Ng 2009; Subba and Eugenio 2009; Joty, Carenini, and Ng 2012; Feng and Hirst 2014) and representative learning (Ji and Eisenstein 2014; Li, Li, and Hovy 2014) became popular. 2.2.2 Discourse Analysis on Conversations. Stolcke et al. (2000) provide one of the first studies focusing on this problem, and it provides a general schema of understanding conversations with discourse analysis. Because of the comple"
J18-4008,P17-1090,0,0.0965772,"Missing"
J18-4008,P13-4009,0,0.0372628,"Missing"
J18-4008,radev-etal-2004-mead,0,0.135384,"Missing"
J18-4008,J02-4001,0,0.172797,"Missing"
J18-4008,N10-1020,0,0.38114,"Missing"
J18-4008,N13-1135,0,0.0207025,"content background (general information) (Daum´e and Marcu 2006; Haghighi and ¨ 2010), and (2) to cluster sentences Vanderwende 2009; C ¸ elikyilmaz and Hakkani-Tur 724 Li et al. A Joint Model of Discourse and Topics on Microblogs or documents into topics, with summaries then generated from each topic cluster for minimizing content redundancy (Salton et al. 1997; McKeown et al. 1999; Siddharthan, Nenkova, and McKeown 2004). Similar techniques have also been applied to summarize events or opinions on microblogs (Chakrabarti and Punera 2011; Long et al. 2011; Rosa et al. 2011; Duan et al. 2012; Shen et al. 2013; Meng et al. 2012). Our downstream application on microblog summarization lies in the research line of point (1), whereas we integrate the effects of discourse on key content identification, which has not been studied in any prior work. Also it is worth noting that, following point (2) to cluster messages before summarization is beyond the scope of this work because we are focusing on summarizing a single conversation tree, on which there are limited topics. We leave the potential of using our model to segment topics for multiconversation summarization to future work. 2.2 Discourse Analysis D"
J18-4008,C04-1129,0,0.0301286,"Missing"
J18-4008,N03-1030,0,0.291035,"Missing"
J18-4008,D12-1087,0,0.0833286,"Missing"
J18-4008,J00-3003,0,0.760268,"Missing"
J18-4008,N09-1064,0,0.0691813,"Missing"
J18-4008,C04-1048,0,0.10585,"Missing"
J18-4008,P13-1137,0,0.0670602,"Missing"
J18-4008,J05-2005,0,0.082294,"rent levels (e.g., clauses, sentences, paragraphs) in a hierarchical tree structure. In particular, the minimal units in RST (i.e., leaves of the tree structure) are defined as sub-sentential clauses, namely, elementary discourse units. Adjacent units are linked by rhetorical relations—condition, comparison, elaboration, and so forth. Based on RST, early work uses hand-coded rules for automatic discourse analysis (Marcu 2000; Thanh, Abeysinghe, and Huyck 2004). Later, thanks to the development of large-scale discourse corpora—RST corpus (Carlson, Marcu, and Okurovsky 2001), Graph Bank corpus (Wolf and Gibson 2005), and Penn Discourse Treebank (Prasad et al. 2008)—data-driven and learning-based discourse parsers that exploit various manually designed features (Soricut and Marcu 2003; Baldridge and Lascarides 2005; Fisher and Roark 2007; Lin, Kan, and Ng 2009; Subba and Eugenio 2009; Joty, Carenini, and Ng 2012; Feng and Hirst 2014) and representative learning (Ji and Eisenstein 2014; Li, Li, and Hovy 2014) became popular. 2.2.2 Discourse Analysis on Conversations. Stolcke et al. (2000) provide one of the first studies focusing on this problem, and it provides a general schema of understanding conversati"
J18-4008,D18-1351,1,0.865754,") systems that can automatically identify gist information, and make sense of the unmanageable amount of user-generated social media content (Farzindar and Inkpen 2015). As one of the important and fundamental text analytic approaches, topic models extract key components embedded in microblog content by clustering words that describe similar semantic meanings to form latent “topics.” The derived intermediate topic representations have proven beneficial to many NLP applications for social media, such as summarization (Harabagiu and Hickl 2011), classification (Phan, Nguyen, and Horiguchi 2008; Zeng et al. 2018a), and recommendation on microblogs (Zeng et al. 2018b). Conventionally, probabilistic topic models (e.g., probabilistic latent semantic analysis [Hofmann 1999] and latent Dirichlet allocation [Blei et al. 2003]) have achieved huge success over the past decade, owing to their fully unsupervised manner and ease of extension. The semantic structure discovered by these topic models have facilitated the progress of many research fields, for example, information retrieval (Boyd-Graber, Hu, and Mimno 2017), data mining (Lin et al. 2015), and NLP (Newman et al. 2010). Nevertheless, ascribing to thei"
J18-4008,N18-1035,1,0.810545,") systems that can automatically identify gist information, and make sense of the unmanageable amount of user-generated social media content (Farzindar and Inkpen 2015). As one of the important and fundamental text analytic approaches, topic models extract key components embedded in microblog content by clustering words that describe similar semantic meanings to form latent “topics.” The derived intermediate topic representations have proven beneficial to many NLP applications for social media, such as summarization (Harabagiu and Hickl 2011), classification (Phan, Nguyen, and Horiguchi 2008; Zeng et al. 2018a), and recommendation on microblogs (Zeng et al. 2018b). Conventionally, probabilistic topic models (e.g., probabilistic latent semantic analysis [Hofmann 1999] and latent Dirichlet allocation [Blei et al. 2003]) have achieved huge success over the past decade, owing to their fully unsupervised manner and ease of extension. The semantic structure discovered by these topic models have facilitated the progress of many research fields, for example, information retrieval (Boyd-Graber, Hu, and Mimno 2017), data mining (Lin et al. 2015), and NLP (Newman et al. 2010). Nevertheless, ascribing to thei"
J18-4008,P15-1071,0,\N,Missing
J18-4008,P10-1084,0,\N,Missing
L18-1078,strotgen-gertz-2012-temporal,0,0.0760372,"Missing"
L18-1078,W16-5002,0,0.0709891,"Missing"
L18-1078,D11-1147,0,0.0885703,"Missing"
L18-1078,W08-0606,0,0.0330734,"first priority is to figure out the types of uncertainty in microblogs. Kiefer (2005) pointed out that uncertainty can be divided into Epistemic and Hypothetical. Epistemic contains Possible and Probable. In fact, the two subclasses are fairly similar in Chinese. Wei (2013) observed Question and External frequently appeared on these posts or comments which reveals uncertainty. Considering Dynamic, one sub-class of Hypothetical, hardly existing in Chinese microblogs, we removed this label in our annotation scheme. At present, there are several corpora in different domains: (1) BioScope corpus (Vincze et al., 2008) annotated uncertainty, negation sentences and their scope in biomedical texts containing 20,879 sentences from 3,236 documents. (2) the dataset for CoNLL’2010 shared task (Vincze, 2010) consisted of biological part of BioScope corpus and the selection of Wikipedia articles, which annotated uncertain sentences and cues. (3) Uncertainty Corpus in complex spoken dialogue systems derived from 120 digital dialogues recording from 60 students, totaling 2,171 turns for students and 2,531 turns for tutor. (4) The Scientific Literature Corpus for Chinese (Chen et al., 2013 ) including 19 full papers a"
L18-1078,W10-3001,0,0.0887126,"Missing"
L18-1078,J96-2004,0,0.756853,"Missing"
N18-1035,P11-2008,0,0.20005,"Missing"
N18-1035,N12-1074,0,0.67722,"Missing"
N18-1035,W09-3951,0,0.100687,", 2016). Distinguishing from prior work that focuses on post-level recommendation, we tackle the challenges of predicting user reply behaviors at the conversation-level. In addition, our model not only captures latent factors such as the topical interests of users, but also leverages the automatically learned discourse structure. Much of the previous work on discourse structure and dialogue acts has relied on labeled data (Jurafsky et al., 1997; Stolcke et al., 2000), while unsupervised approaches have not been applied to the problem of conversation recommendation (Woszczyna and Waibel, 1994; Crook et al., 2009; Ritter et al., 2010; Joty et al., 2011). Our work is also in line with conversation modeling for social media discussions (Ritter et al., 2010; Budak and Agrawal, 2013; Louis and Cohen, 2015; Cheng et al., 2017). Topic modeling 2 To ensure the general applicability of our approach to domains lacking such information, we do not utilize external features such as network structure, but it may certainly be added in future, more narrowly targeted applications. 376 which weights positive instances higher during training and is thus suited to our data. Formally, for user u and conversation c, we me"
N18-1035,C10-1034,0,0.333494,"sing attention in digital communication research (Agichtein et al., 2008; Kwak et al., 2010; Wu et al., 2011). The problem studied here is closely related to work on recommendation and response prediction in microblogs (Artzi et al., 2012; Hong et al., 2013), where the goal is to predict whether a user will share or reply to a given post. Existing methods focus on measuring features that reflect personalized user interests, including topics (Hong et al., 2013) and network structures (Pan et al., 2013; He and Tan, 2015). These features have been investigated under a learning to rank framework (Duan et al., 2010; Artzi et al., 2012), graph ranking models (Yan et al., 2012; Feng and Wang, 2013; Alawad et al., 2016), and neural network-based representation learning methods (Yu et al., 2016). Distinguishing from prior work that focuses on post-level recommendation, we tackle the challenges of predicting user reply behaviors at the conversation-level. In addition, our model not only captures latent factors such as the topical interests of users, but also leverages the automatically learned discourse structure. Much of the previous work on discourse structure and dialogue acts has relied on labeled data ("
N18-1035,N10-1020,0,0.59776,"large quantities of superfluous material select which conversations to engage in, and how might we better algorithmically recommend conversations suited to individual users? We approach this problem from a microblog conversation recommendation framework. Where prior work has focused on the content of individual posts for recommendation (Chen et al., 2012; Yan et al., 2012; Vosecky et al., 2014; He and Tan, 2015), we examine the entire history and context of a conversation, including both topical content and discourse modes such as agreement, question-asking, argument and other dialogue acts (Ritter et al., 2010).1 And where Backstrom et al. (2013) leveraged conversation reply structure (such as previous user engagement), their model is unable to predict first entry into new conversations, while ours is able to predict both new 1 In this paper, discourse mode refers to a certain type of dialogue act, e.g., agreement or argument. The discourse structure of a conversation means some combination (or a probability distribution) of discourse modes. 375 Proceedings of NAACL-HLT 2018, pages 375–385 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics and repeated entry i"
N18-1035,D15-1178,0,0.0588574,"del not only captures latent factors such as the topical interests of users, but also leverages the automatically learned discourse structure. Much of the previous work on discourse structure and dialogue acts has relied on labeled data (Jurafsky et al., 1997; Stolcke et al., 2000), while unsupervised approaches have not been applied to the problem of conversation recommendation (Woszczyna and Waibel, 1994; Crook et al., 2009; Ritter et al., 2010; Joty et al., 2011). Our work is also in line with conversation modeling for social media discussions (Ritter et al., 2010; Budak and Agrawal, 2013; Louis and Cohen, 2015; Cheng et al., 2017). Topic modeling 2 To ensure the general applicability of our approach to domains lacking such information, we do not utilize external features such as network structure, but it may certainly be added in future, more narrowly targeted applications. 376 which weights positive instances higher during training and is thus suited to our data. Formally, for user u and conversation c, we measure reply preference based on the MSE between predicted preference score pu,c and reply history ru,c . ru,c equals 1 if u is in the conversation history; otherwise, it is 0. The first term o"
N18-1035,J00-3003,0,0.738079,"Missing"
N18-1035,N13-1039,0,0.146625,"Missing"
N18-1035,P12-1054,0,0.839358,"itical discussion, for instance, might prefer content concerning a specific candidate or issue, and only then if discussed in a positive light without controversy (Adamic and Glance, 2005; Bakshy et al., 2015). How do individuals facing such large quantities of superfluous material select which conversations to engage in, and how might we better algorithmically recommend conversations suited to individual users? We approach this problem from a microblog conversation recommendation framework. Where prior work has focused on the content of individual posts for recommendation (Chen et al., 2012; Yan et al., 2012; Vosecky et al., 2014; He and Tan, 2015), we examine the entire history and context of a conversation, including both topical content and discourse modes such as agreement, question-asking, argument and other dialogue acts (Ritter et al., 2010).1 And where Backstrom et al. (2013) leveraged conversation reply structure (such as previous user engagement), their model is unable to predict first entry into new conversations, while ours is able to predict both new 1 In this paper, discourse mode refers to a certain type of dialogue act, e.g., agreement or argument. The discourse structure of a con"
N18-1035,P16-2073,0,0.0554006,"and response prediction in microblogs (Artzi et al., 2012; Hong et al., 2013), where the goal is to predict whether a user will share or reply to a given post. Existing methods focus on measuring features that reflect personalized user interests, including topics (Hong et al., 2013) and network structures (Pan et al., 2013; He and Tan, 2015). These features have been investigated under a learning to rank framework (Duan et al., 2010; Artzi et al., 2012), graph ranking models (Yan et al., 2012; Feng and Wang, 2013; Alawad et al., 2016), and neural network-based representation learning methods (Yu et al., 2016). Distinguishing from prior work that focuses on post-level recommendation, we tackle the challenges of predicting user reply behaviors at the conversation-level. In addition, our model not only captures latent factors such as the topical interests of users, but also leverages the automatically learned discourse structure. Much of the previous work on discourse structure and dialogue acts has relied on labeled data (Jurafsky et al., 1997; Stolcke et al., 2000), while unsupervised approaches have not been applied to the problem of conversation recommendation (Woszczyna and Waibel, 1994; Crook e"
P04-1074,W99-0613,0,0.0160745,"Missing"
P04-1074,P92-1001,0,0.296954,"Missing"
P04-1074,J00-4004,0,0.0513276,"Missing"
P04-1074,P94-1013,0,0.0601715,"ts. Notice that the mapping is not one-to-one. For example, adverbs affect tense/aspect as well as discourse structure. For another example, tense/aspect can be affected by auxiliary words, trend verbs, etc. This shows that classification of temporal indicators based on partof-speech (POS) information alone cannot determine relative temporal relations. 3 Machine Learning Approaches for Relative Relation Resolution Previous efforts in corpus-based natural language processing have incorporated machine learning methods to coordinate multiple linguistic features for example in accent restoration (Yarowsky, 1994) and event classification (Siegel and McKeown, 1998), etc. Relative relation resolution can be modeled as a relation classification task. We model the thirteen relative temporal relations (see Figure 1) as the classes to be decided by a classifier. The resolution process is to assign an event pair (i.e. the two events under concern)2 to one class according to their linguistic features. For this purpose, we train two classifiers, a Probabilistic Decision Tree Classifier (PDT) and a Naïve Bayesian Classifier (NBC). We then combine the results by the Collaborative Bootstrapping (CB) technique whi"
P04-1074,E95-1035,0,\N,Missing
P06-1125,J90-2002,0,0.342873,"Missing"
P06-1125,W06-2808,1,0.654156,"Missing"
P06-1125,xia-etal-2006-constructing,1,0.754567,"estimation method with Chinese character trigram model on NIL corpus. In our implementation, Katz Backoff smoothing technique (Katz, 1987) is used to handle the sparse data problem, and Viterbi algorithm is employed to find the optimal solution in XSCM. 6 6.1 Evaluation Data Description Training Sets Two types of training data are used in our experiments. We use news from Xinhua News Agency in LDC Chinese Gigaword v.2 (CNGIGA) (Graf et al., 2005) as standard Chinese corpus to construct phonetic mapping models because of its excellent coverage of standard Simplified Chinese. We use NIL corpus (Xia et al., 2006b) as chat language corpus. To evaluate our methods on size-varying training data, six chat language corpora are created based on NIL corpus. We select 6056 sentences from NIL corpus randomly to make the first chat language corpus, i.e. C#1. In every next corpus, we add extra 1,211 random sentences. So 7,267 sentences are contained in C#2, 8,478 in C#3, 9,689 in C#4, 10,200 in C#5, and 12,113 in C#6. Test Sets Test sets are used to prove that chat language is dynamic and XSCM is effective and robust in normalizing dynamic chat language terms. Six time-varying test sets, i.e. T#1 ~ T#6, are cre"
P06-1125,I05-3013,1,\N,Missing
P08-2034,C04-1200,0,0.0693601,"tected with intensity features and the stress level is determined in the second step with timbre and rhythm features. It is proved difficult to detect stress level using audio as classification proof. Song sentiment classification using lyric as proof is recently investigated by Chen et al. (2006). They adopt the hierarchical framework and make use of song lyric to detect stress level in the second step. In fact, many literatures have been produced to address the sentiment analysis problem in natural language processing research. Three approaches are dominating, i.e. knowledge-based approach (Kim and Hovy, 2004), information retrieval-based approach (Turney and Littman, 2003) and machine learning approach (Pang et al., 2002), in which the last approach is found very popular. Pang et al. (2002) adopt the VSM model to represent product reviews and apply text classification algorithms such as Naïve Bayes, maximum entropy and support vector machines to predict sentiment polarity of given product review. Chen et al. (2006) also apply the VSM model in lyric-based song sentiment classification. However, our experiments show that song sentiment classification with the VSM model delivers disappointing quality"
P08-2034,W02-1011,0,0.0217393,". It is proved difficult to detect stress level using audio as classification proof. Song sentiment classification using lyric as proof is recently investigated by Chen et al. (2006). They adopt the hierarchical framework and make use of song lyric to detect stress level in the second step. In fact, many literatures have been produced to address the sentiment analysis problem in natural language processing research. Three approaches are dominating, i.e. knowledge-based approach (Kim and Hovy, 2004), information retrieval-based approach (Turney and Littman, 2003) and machine learning approach (Pang et al., 2002), in which the last approach is found very popular. Pang et al. (2002) adopt the VSM model to represent product reviews and apply text classification algorithms such as Naïve Bayes, maximum entropy and support vector machines to predict sentiment polarity of given product review. Chen et al. (2006) also apply the VSM model in lyric-based song sentiment classification. However, our experiments show that song sentiment classification with the VSM model delivers disappointing quality (see Section 5). Error analysis reveals that the VSM model is problematic in representing song lyric. It is necess"
P09-1121,D08-1092,0,0.113058,"t the same time, ranking search results for the query Han Feizi (d dd) is likely to be harder in English than in Chinese. A large portion of web queries have such properties that they are originated in a language different from the one they are searched. This variance in problem difficulty across languages is not unique to web search; it appears in a wide range of natural language processing problems. Much recent work on bilingual data has focused on exploiting these variations in difficulty to improve a variety of monolingual tasks, including parsing (Hwa et al., 2005; Smith and Smith, 2004; Burkett and Klein, 2008; Snyder and Barzilay, 2008), named entity recognition (Chang et al., 2009), and topic clustering (Wu and Oard, 2008). In this work, we exploit a similar intuition to improve monolingual web search. Our problem setting differs from cross-lingual web search, where the goal is to return machinetranslated results from one language in response to a query from another (Lavrenko et al., 2002). We operate under the assumption that for many monolingual English queries (e.g., Han Feizi), there exist good documents in English. If we have Chinese information as well, we can exploit it to help find these"
P09-1121,N09-1034,0,0.0268404,"y to be harder in English than in Chinese. A large portion of web queries have such properties that they are originated in a language different from the one they are searched. This variance in problem difficulty across languages is not unique to web search; it appears in a wide range of natural language processing problems. Much recent work on bilingual data has focused on exploiting these variations in difficulty to improve a variety of monolingual tasks, including parsing (Hwa et al., 2005; Smith and Smith, 2004; Burkett and Klein, 2008; Snyder and Barzilay, 2008), named entity recognition (Chang et al., 2009), and topic clustering (Wu and Oard, 2008). In this work, we exploit a similar intuition to improve monolingual web search. Our problem setting differs from cross-lingual web search, where the goal is to return machinetranslated results from one language in response to a query from another (Lavrenko et al., 2002). We operate under the assumption that for many monolingual English queries (e.g., Han Feizi), there exist good documents in English. If we have Chinese information as well, we can exploit it to help find these documents. As we will see, machine translation can provide important predic"
P09-1121,W04-3207,0,0.0322795,"ng function constant. At the same time, ranking search results for the query Han Feizi (d dd) is likely to be harder in English than in Chinese. A large portion of web queries have such properties that they are originated in a language different from the one they are searched. This variance in problem difficulty across languages is not unique to web search; it appears in a wide range of natural language processing problems. Much recent work on bilingual data has focused on exploiting these variations in difficulty to improve a variety of monolingual tasks, including parsing (Hwa et al., 2005; Smith and Smith, 2004; Burkett and Klein, 2008; Snyder and Barzilay, 2008), named entity recognition (Chang et al., 2009), and topic clustering (Wu and Oard, 2008). In this work, we exploit a similar intuition to improve monolingual web search. Our problem setting differs from cross-lingual web search, where the goal is to return machinetranslated results from one language in response to a query from another (Lavrenko et al., 2002). We operate under the assumption that for many monolingual English queries (e.g., Han Feizi), there exist good documents in English. If we have Chinese information as well, we can explo"
P09-1121,P08-1084,0,0.0325253,"search results for the query Han Feizi (d dd) is likely to be harder in English than in Chinese. A large portion of web queries have such properties that they are originated in a language different from the one they are searched. This variance in problem difficulty across languages is not unique to web search; it appears in a wide range of natural language processing problems. Much recent work on bilingual data has focused on exploiting these variations in difficulty to improve a variety of monolingual tasks, including parsing (Hwa et al., 2005; Smith and Smith, 2004; Burkett and Klein, 2008; Snyder and Barzilay, 2008), named entity recognition (Chang et al., 2009), and topic clustering (Wu and Oard, 2008). In this work, we exploit a similar intuition to improve monolingual web search. Our problem setting differs from cross-lingual web search, where the goal is to return machinetranslated results from one language in response to a query from another (Lavrenko et al., 2002). We operate under the assumption that for many monolingual English queries (e.g., Han Feizi), there exist good documents in English. If we have Chinese information as well, we can exploit it to help find these documents. As we will see, m"
P10-1139,W06-1641,0,0.0213367,"act-based documents to calculate an opinion weight. These weights were used to compute opinion scores for each retrieved document. A weighted dictionary was generated from previous TREC relevance data (Amati et al., 2007). This dictionary was submitted as a query to a search engine to get an initial query-independent opinion score of all retrieved Unified Opinion Retrieval Model In addition to conventional 2-stage approach, there has been some research on unified opinion retrieval models. Eguchi and Lavrenko proposed an opinion retrieval model in the framework of generative language modeling (Eguchi and Lavrenko, 2006). They modeled a collection of natural language documents or statements, each of which consisted of some topic-bearing and some sentiment-bearing words. The sentiment was either represented by a group of predefined seed words, or extracted from a training sentiment corpus. This model was shown to be effective on the MPQA corpus. Mei et al. tried to build a fine-grained opinion retrieval system for consumer products (Mei et al., 2007). The opinion score for a product was a mixture of several facets. Due to the difficulty in 1373 associating sentiment with products and facets, the system was onl"
P10-1139,W04-3247,0,0.036271,"|| vector of hub scores for the word pairs at iteration. In order to ensure convergence of the iterative form, and are normalized in each iteration cycle. 1370 For computation of the final scores, the initial scores of all documents are set to √ , and top. The ic-sentiment word pairs are set to √ above iterative steps are then used to compute the new scores until convergence. Usually the convergence of the iteration algorithm is achieved when the difference between the scores computed at two successive iterations for any nodes falls below a given threshold (Wan et al., 2008; Li et al., 2009; Erkan and Radev, 2004). In our model, we use the hub scores to denote the associative degree of each word pair and the authority scores as the total scores. The documents are then ranked based on the total scores. (2) The opinion word lexicon provided by National Taiwan University which consists of 2,812 positive words and 8,276 negative words; (3) Sentiment word lexicon and comment word lexicon from Hownet. It contains 1836 positive sentiment words, 3,730 positive comments, 1,254 negative sentiment words and 3,116 negative comment words. The different graphemes corresponding to Traditional Chinese and Simplified C"
P10-1139,P09-1083,0,0.0158946,"ration and is the || vector of hub scores for the word pairs at iteration. In order to ensure convergence of the iterative form, and are normalized in each iteration cycle. 1370 For computation of the final scores, the initial scores of all documents are set to √ , and top. The ic-sentiment word pairs are set to √ above iterative steps are then used to compute the new scores until convergence. Usually the convergence of the iteration algorithm is achieved when the difference between the scores computed at two successive iterations for any nodes falls below a given threshold (Wan et al., 2008; Li et al., 2009; Erkan and Radev, 2004). In our model, we use the hub scores to denote the associative degree of each word pair and the authority scores as the total scores. The documents are then ranked based on the total scores. (2) The opinion word lexicon provided by National Taiwan University which consists of 2,812 positive words and 8,276 negative words; (3) Sentiment word lexicon and comment word lexicon from Hownet. It contains 1836 positive sentiment words, 3,730 positive comments, 1,254 negative sentiment words and 3,116 negative comment words. The different graphemes corresponding to Traditional"
P10-1139,H05-1115,0,0.029896,"Missing"
P10-1139,H05-1043,0,0.119533,"nion Analysis Evaluation (COAE) workshop, consisting of blogs and reviews. 20 queries are provided in COAE08. In our experiment, we created relevance judgments through pooling method, where documents are ranked at different levels: irrelevant, relevant but without opinion, and relevant with opinion. Since polarity is not considered, all relevant documents with opinion are classified into the same level. In order to acquire the collection of topic terms, we adopt two expansion methods, dictionary-based method and pseudo relevance feedback method. The dictionary-based method utilizes Wikipedia (Popescu and Etzioni, 2005) to find an entry page for a phrase or a single term in a query. If such an entry exists, all titles of the entry page are extracted as synonyms of the query concept. For example, if we search “绿坝” (Green Tsunami, a firewall) in Wikipedia, it is re-directed to an entry page titled “花季护航” (Youth Escort). This term is then added as a synonym of “绿坝” (Green Tsunami) in the query. Synonyms are treated the same as the original query terms in a retrieval process. The content words in the entry page are ranked by their frequencies in the page. The top-k terms are returned as potential expanded topic"
P10-1139,H05-1044,0,0.0345216,"th existing approaches, experimental results on the COAE08 dataset showed that our graph-based model achieved significant improvement. 1 Introduction In recent years, there is a growing interest in sharing personal opinions on the Web, such as product reviews, economic analysis, political polls, etc. These opinions cannot only help independent users make decisions, but also obtain valuable feedbacks (Pang et al., 2008). Opinion oriented research, including sentiment classification, opinion extraction, opinion question answering, and opinion summarization, etc. are receiving growing attention (Wilson, et al., 2005; Liu et al., 2005; Oard et al., 2006). However, most existing works concentrate on analyzing opinions expressed in the documents, and none on how to represent the information needs required to retrieve opinionated documents. In this paper, we focus on opinion retrieval, whose goal is to find a set of documents containing not only the query keyword(s) but also the relevant opinions. This requirement brings about the challenge on how to represent information needs for effective opinion retrieval. In order to solve the above problem, previous work adopts a 2-stage approach. In the first stage, r"
P10-1139,H05-2017,0,\N,Missing
P11-1012,W06-1615,0,0.0605286,"D03 0.0247 0.0711 -0.0127 NP04 to TD04 0.0525 0.1737 0.0163 Table 4: The Kendall’s τ of Rweight and Rmap in HP/NP to TD adaptation. Weighting method query-aggr query-comp query-rand TD03 to HP03 0.1172 0.1304 −0.0291 TD04 to HP04 0.0121 0.1393 0.0022 TD03 to NP03 0.0574 0.1586 0.0161 TD04 to NP04 0.0464 0.0545 -0.0262 Table 5: The Kendall’s τ of Rweight and Rmap in TD to HP/NP adaptation. fine-grained similarity values. 6 Related Work Cross-domain knowledge transfer has became an important topic in machine learning and natural language processing (Ben-David et al., 2010; Jiang and Zhai, 2007; Blitzer et al., 2006; Daum´e III and Marcu, 2006). (Blitzer et al., 2006) proposed model adaptation using pivot features to build structural feature correspondence in two domains. (Pan et al., 2009) proposed to seek a common features space to reduce the distribution difference between the source and target domain. (Daum´e III and Marcu, 2006) assumed training instances were generated from source domain, target domain and crossdomain distributions, and estimated the parameter for the mixture distribution. Recently, domain adaptation in learning to rank received more and more attentions due to the lack of training"
P11-1012,D09-1053,0,0.149155,"tware Technologies, Ministry of Education, China Abstract by domain experts for achieving better ranking performance. In real applications, however, it is time consuming and expensive to annotate training data for each search domain. To alleviate the lack of training data in the target domain, many researchers have proposed to transfer ranking knowledge from the source domain with plenty of labeled data to the target domain where only a few or no labeled data is available, which is known as ranking model adaptation (Chen et al., 2008a; Chen et al., 2010; Chen et al., 2008b; Geng et al., 2009; Gao et al., 2009). We propose to directly measure the importance of queries in the source domain to the target domain where no rank labels of documents are available, which is referred to as query weighting. Query weighting is a key step in ranking model adaptation. As the learning object of ranking algorithms is divided by query instances, we argue that it’s more reasonable to conduct importance weighting at query level than document level. We present two query weighting schemes. The first compresses the query into a query feature vector, which aggregates all document instances in the same query, and then con"
P11-1012,P07-1034,0,0.480542,"es these fine-grained similarity values for its importance estimation. Adaptation experiments on LETOR3.0 data set demonstrate that query weighting significantly outperforms document instance weighting methods. 1 Intuitively, the more similar an source instance is to the target instances, it is expected to be more useful for cross-domain knowledge transfer. This motivated the popular domain adaptation solution based on instance weighting, which assigns larger weights to those transferable instances so that the model trained on the source domain can adapt more effectively to the target domain (Jiang and Zhai, 2007). Existing instance weighting schemes mainly focus on the adaptation problem for classification (Zadrozny, 2004; Huang et al., 2007; Jiang and Zhai, 2007; Sugiyama et al., 2008). Introduction Learning to rank, which aims at ranking documents in terms of their relevance to user’s query, has been widely studied in machine learning and information retrieval communities (Herbrich et al., 2000; Freund et al., 2004; Burges et al., 2005; Yue et al., 2007; Cao et al., 2007; Liu, 2009). In general, large amount of training data need to be annotated Although instance weighting scheme may be applied to d"
P12-2053,P07-1056,0,0.049187,"ains. We present an Information-theoretic Multi-view Adaptation Model (IMAM) based on a multi-way clustering scheme, where word and link clusters can draw together seemingly unrelated domain-specific features from both sides and iteratively boost the consistency between document clusterings based on word and link views. Experiments show that IMAM significantly outperforms state-of-the-art baselines. 1 Introduction Domain adaptation has been shown useful to many natural language processing applications including document classification (Sarinnapakorn and Kubat, 2007), sentiment classification (Blitzer et al., 2007), part-of-speech tagging (Jiang and Zhai, 2007) and entity mention detection (Daum´e III and Marcu, 2006). Documents can be represented by multiple independent sets of features such as words and link structures of the documents. Multi-view learning aims to improve classifiers by leveraging the redundancy and consistency among these multiple views (Blum and Mitchell, 1998; R¨uping and Scheffer, 2005; Abney, 2002). Existing methods were designed for data from single domain, assuming that either view alone is sufficient to predict the target class accurately. However, this view-consistency assump"
P12-2053,P07-1034,0,0.167391,"-view Adaptation Model (IMAM) based on a multi-way clustering scheme, where word and link clusters can draw together seemingly unrelated domain-specific features from both sides and iteratively boost the consistency between document clusterings based on word and link views. Experiments show that IMAM significantly outperforms state-of-the-art baselines. 1 Introduction Domain adaptation has been shown useful to many natural language processing applications including document classification (Sarinnapakorn and Kubat, 2007), sentiment classification (Blitzer et al., 2007), part-of-speech tagging (Jiang and Zhai, 2007) and entity mention detection (Daum´e III and Marcu, 2006). Documents can be represented by multiple independent sets of features such as words and link structures of the documents. Multi-view learning aims to improve classifiers by leveraging the redundancy and consistency among these multiple views (Blum and Mitchell, 1998; R¨uping and Scheffer, 2005; Abney, 2002). Existing methods were designed for data from single domain, assuming that either view alone is sufficient to predict the target class accurately. However, this view-consistency assumption is largely violated in the setting of doma"
P12-2053,P02-1046,0,\N,Missing
P13-2011,J96-2004,0,0.0239954,"Missing"
P13-2011,W10-3001,0,0.664687,"Missing"
P13-2011,P09-2044,0,0.554532,"Missing"
P13-2011,W10-3004,0,0.0563479,"ork on uncertainty identification focused on classifying sentences into uncertain or definite categories. Existing approaches are mainly based on supervised methods (Light et al., 2004; Medlock and Briscoe, 2007; Medlock, 2008; Szarvas, 2008) using the annotated corpus with different types of features including Part-OfSpeech (POS) tags, stems, n-grams, etc.. Classification of uncertain sentences was consolidated as a task in the 2010 edition of CoNLL shared task on learning to detect hedge cues and their scope in natural language text (Farkas et al., 2010). The best system for Wikipedia data (Georgescul, 2010) employed Support Vector Machine (SVM), and the best system for biological data (Tang et al., 2010) adopted Conditional 2 http://www.timeml.org/site/timebank/ timebank.html Random Fields (CRF). In our work, we conduct an empirical study of uncertainty identification on tweets dataset and explore the effectiveness of different types of features (i.e., content-based, user-based and Twitterspecific) from social media context. 3 Uncertainty corpus for microblogs 3.1 Types of uncertainty in microblogs Traditionally, uncertainty can be divided into two categories, namely Epistemic and Hypothetical ("
P13-2011,W08-0607,0,0.0429344,"al media. Although uncertainty has been studied theoretically for a long time as a grammatical phenomena (Seifert and Welte, 1987), the computational treatment of uncertainty is a newly emerging area of research. Szarvas et al. (2012) pointed out that “Uncertainty - in its most general sense - can be interpreted as lack of information: the receiver of the information (i.e., the hearer or the reader) cannot be certain about some pieces of information”. In recent years, the identification of uncertainty in formal text, e.g., biomedical text, reviews or newswire, has attracted lots of attention (Kilicoglu and Bergler, 2008; Medlock and Briscoe, 2007; Szarvas, 2008; Light et al., 2004). However, uncertainty identification in social media context is rarely explored. Previous research shows that uncertainty identification is domain dependent as the usage of hedge cues varies widely in different domains (Morante and Sporleder, 2012). Therefore, the employment of existing out-of-domain corpus to social media context is ineffective. Furthermore, compared to the existing uncertainty corpus, the expression of uncertainty in social media is fairly different from that in formal text in a sense that people usually raise q"
P13-2011,D11-1147,0,0.0933427,"Missing"
P13-2011,J12-2004,0,0.617265,"Missing"
P13-2011,P08-1033,0,0.113025,"cally for a long time as a grammatical phenomena (Seifert and Welte, 1987), the computational treatment of uncertainty is a newly emerging area of research. Szarvas et al. (2012) pointed out that “Uncertainty - in its most general sense - can be interpreted as lack of information: the receiver of the information (i.e., the hearer or the reader) cannot be certain about some pieces of information”. In recent years, the identification of uncertainty in formal text, e.g., biomedical text, reviews or newswire, has attracted lots of attention (Kilicoglu and Bergler, 2008; Medlock and Briscoe, 2007; Szarvas, 2008; Light et al., 2004). However, uncertainty identification in social media context is rarely explored. Previous research shows that uncertainty identification is domain dependent as the usage of hedge cues varies widely in different domains (Morante and Sporleder, 2012). Therefore, the employment of existing out-of-domain corpus to social media context is ineffective. Furthermore, compared to the existing uncertainty corpus, the expression of uncertainty in social media is fairly different from that in formal text in a sense that people usually raise questions or refer to external information"
P13-2011,W10-3002,0,0.449553,"gories. Existing approaches are mainly based on supervised methods (Light et al., 2004; Medlock and Briscoe, 2007; Medlock, 2008; Szarvas, 2008) using the annotated corpus with different types of features including Part-OfSpeech (POS) tags, stems, n-grams, etc.. Classification of uncertain sentences was consolidated as a task in the 2010 edition of CoNLL shared task on learning to detect hedge cues and their scope in natural language text (Farkas et al., 2010). The best system for Wikipedia data (Georgescul, 2010) employed Support Vector Machine (SVM), and the best system for biological data (Tang et al., 2010) adopted Conditional 2 http://www.timeml.org/site/timebank/ timebank.html Random Fields (CRF). In our work, we conduct an empirical study of uncertainty identification on tweets dataset and explore the effectiveness of different types of features (i.e., content-based, user-based and Twitterspecific) from social media context. 3 Uncertainty corpus for microblogs 3.1 Types of uncertainty in microblogs Traditionally, uncertainty can be divided into two categories, namely Epistemic and Hypothetical (Kiefer, 2005). For Epistemic, there are two sub-classes Possible and Probable. For Hypothetical, th"
P13-2011,W04-3103,0,0.203939,"g time as a grammatical phenomena (Seifert and Welte, 1987), the computational treatment of uncertainty is a newly emerging area of research. Szarvas et al. (2012) pointed out that “Uncertainty - in its most general sense - can be interpreted as lack of information: the receiver of the information (i.e., the hearer or the reader) cannot be certain about some pieces of information”. In recent years, the identification of uncertainty in formal text, e.g., biomedical text, reviews or newswire, has attracted lots of attention (Kilicoglu and Bergler, 2008; Medlock and Briscoe, 2007; Szarvas, 2008; Light et al., 2004). However, uncertainty identification in social media context is rarely explored. Previous research shows that uncertainty identification is domain dependent as the usage of hedge cues varies widely in different domains (Morante and Sporleder, 2012). Therefore, the employment of existing out-of-domain corpus to social media context is ineffective. Furthermore, compared to the existing uncertainty corpus, the expression of uncertainty in social media is fairly different from that in formal text in a sense that people usually raise questions or refer to external information when making uncertain"
P13-2011,P07-1125,0,0.270428,"y has been studied theoretically for a long time as a grammatical phenomena (Seifert and Welte, 1987), the computational treatment of uncertainty is a newly emerging area of research. Szarvas et al. (2012) pointed out that “Uncertainty - in its most general sense - can be interpreted as lack of information: the receiver of the information (i.e., the hearer or the reader) cannot be certain about some pieces of information”. In recent years, the identification of uncertainty in formal text, e.g., biomedical text, reviews or newswire, has attracted lots of attention (Kilicoglu and Bergler, 2008; Medlock and Briscoe, 2007; Szarvas, 2008; Light et al., 2004). However, uncertainty identification in social media context is rarely explored. Previous research shows that uncertainty identification is domain dependent as the usage of hedge cues varies widely in different domains (Morante and Sporleder, 2012). Therefore, the employment of existing out-of-domain corpus to social media context is ineffective. Furthermore, compared to the existing uncertainty corpus, the expression of uncertainty in social media is fairly different from that in formal text in a sense that people usually raise questions or refer to extern"
P13-2011,J12-2001,0,0.0854876,"ed as lack of information: the receiver of the information (i.e., the hearer or the reader) cannot be certain about some pieces of information”. In recent years, the identification of uncertainty in formal text, e.g., biomedical text, reviews or newswire, has attracted lots of attention (Kilicoglu and Bergler, 2008; Medlock and Briscoe, 2007; Szarvas, 2008; Light et al., 2004). However, uncertainty identification in social media context is rarely explored. Previous research shows that uncertainty identification is domain dependent as the usage of hedge cues varies widely in different domains (Morante and Sporleder, 2012). Therefore, the employment of existing out-of-domain corpus to social media context is ineffective. Furthermore, compared to the existing uncertainty corpus, the expression of uncertainty in social media is fairly different from that in formal text in a sense that people usually raise questions or refer to external information when making uncertain statements. But, neither of the uncertainty expressions can be represented based on the existing types of uncertainty defined in the literature. Therefore, a different uncertainty classification scheme is needed in social media context. In this pap"
P13-2011,W08-0606,0,\N,Missing
P16-1199,D15-1259,1,0.51264,"topic such as [O] and [R7] or raise a new aspect (subtopic) of the previously discussed topics such as [R2] and [R10]. These messages are named as leaders, which contain salient content in topic description, e.g., the italic and underlined words in Figure 1. The remaining messages, named as followers, do not raise new issues but simply respond to their reposted or replied messages following what has been raised by the leaders and often contain non-topic words, e.g., OMG, OK, agree, etc. Conversation tree structures from microblogs have been previously shown helpful to microblog summarization (Li et al., 2015), but have never been explored for topic modeling. We follows Li et al. (2015) to detect leaders and followers across paths of conversation trees using Conditional Random Fields (CRF) trained on annotated data. The detected leader/follower information is then incorporated as prior knowledge into our proposed topic model. Our experimental results show that our model, which captures parent-child topic correlations in conversation trees and generates topics by considering messages being leaders or followers separately, is able to induce high-quality topics and outperforms a number of competitive"
P16-1199,D11-1024,0,0.170074,"Missing"
P16-1199,P13-4009,0,0.150509,"Missing"
P16-1199,N10-1020,0,0.204366,"Missing"
P16-1199,N15-1119,0,0.111395,"oorly when directly applied to short and colloquial microblog content due to severe sparsity in microblog messages (Wang and McCallum, 2006; Hong and Davison, 2010). A common way to deal with short text sparsity is to aggregate short messages into long pseudodocuments. Most of the studies heuristically aggregate messages based on authorship (Zhao et al., 2011; Hong and Davison, 2010), shared words (Weng et al., 2010), or hashtags (Ramage et al., 2010; Mehrotra et al., 2013). Some works directly take into account the word relations to alleviate document-level word sparseness (Yan et al., 2013; Sridhar, 2015). More recently, a self-aggregation-based topic model called SATM (Quan et al., 2015) was proposed to aggregate texts jointly with topic inference. However, we argue that the existing aggregation strategies are suboptimal for modeling topics in short texts. Microblogs allow users to share and comment on messages with friends through reposting or replying, similar to our everyday conversations. Intuitively, the conversation structures can not only enrich context, but also provide useful clues for identifying relevant topics. This is nonetheless ignored in previous approaches. Moreover, the occu"
P16-1199,P11-1153,0,0.167809,"e topics. Since leader messages subsume the content of their followers, the topic of a leader can be generated from the topic distribution of the entire tree. Consequently, the topic mixture of a conversation tree is determined by the topic assignments to the leader messages on it. The topics of followers, however, exhibit strong and explicit dependencies on the topics of their ancestors. So, their topics need to be generated in consideration of local constraints. Here, we mainly address how to model the topic dependencies of followers. Enlighten by the general Structural Topic Model (strTM) (Wang et al., 2011), which incorporates document structures into topic model by explicitly modeling topic dependencies between adjacent sentences, we exploit the topical transitions between parents and children in the trees for guiding topic assignments. Intuitively, the emergence of a leader results in potential topic shift. It tends to weaken the topic similarities between the emerging leaders and their predecessors. For example, [R7] in Figure 1 transfers the topic to a new focus, thus weakens the tie with its parent. We can simplify our case by assuming that followers are topically responsive just up to (hen"
P17-1066,P04-1054,0,0.0327022,"considering ancestors on propagation paths) and contextsensitive cases. 4.4 Table 1: Statistics of the datasets Statistic # of users # of source tweets # of threads # of non-rumors # of false rumors # of true rumors # of unverified rumors Avg. time length / tree Avg. # of posts / tree Max # of posts / tree Min # of posts / tree Rumor Detection via Kernel Learning The advantage of kernel-based method is that we can avoid painstakingly engineering the features. This is possible because the kernel function can explore an implicit feature space when calculating the similarity between two objects (Culotta and Sorensen, 2004). We incorporate the proposed tree kernel functions, i.e., PTK (equation 2) or cPTK (equation 3), into a supervised learning framework, for which we utilize a kernel-based SVM classifier. We treat each tree as an instance, and its similarity values with all training instances as feature space. Therefore, the kernel matrix of training set is m × m and that of test set is n × m where m and n are the sizes of training and test sets, respectively. For our multi-class task, we perform a one-vsall classification for each label and then assign the one with the highest likelihood among the four, i.e.,"
P17-1066,D15-1311,0,0.0170465,"as an instance, and its similarity values with all training instances as feature space. Therefore, the kernel matrix of training set is m × m and that of test set is n × m where m and n are the sizes of training and test sets, respectively. For our multi-class task, we perform a one-vsall classification for each label and then assign the one with the highest likelihood among the four, i.e., non-rumor, false rumor, true rumor or unverified rumor. We choose this method due to interpretability of results, similar to recent work on occupational class classification (Preotiuc-Pietro et al., 2015; Lukasik et al., 2015). 5 Twitter16 173,487 818 204,820 205 205 205 203 848 Hours 251 2,765 81 Twrench3 and crawled the replies through Twitter’s web interface. Finally, we annotated the source tweets by referring to the labels of the events they are from. We first turned the label of each event in Twitter15 and Twitter16 from binary to quaternary according to the veracity tag of the article in rumor debunking websites (e.g., snopes.com, Emergent.info, etc). Then we labeled the source tweets by following these rules: 1) Source tweets from unverified rumor events or non-rumor events are labeled the same as the corre"
P17-1066,P04-1043,0,0.0528519,"source tweets, each being associated with a group of retweets and replies. Here we focus on classifying a given source tweet regarding a claim which is a finer-grained task. Similar setting was also considered in (Wu et al., 2015; Qazvinian et al., 2011). Kernel methods are designed to evaluate the similarity between two objects, and tree kernel specifically addresses structured data which has been successfully applied for modeling syntactic information in many natural language tasks such as syntactic parsing (Collins and Duffy, 2001), question-answering (Moschitti, 2006), semantic analysis (Moschitti, 2004), relation extraction (Zhang et al., 2008) and machine translation (Sun et al., 2010). These kernels are not suitable for modeling the social media propagation structures because the nodes are not given as discrete values like part-of-speech tags, but are represented as high dimensional real-valued vectors. Our proposed method is a substantial extension of tree kernel for modeling such structures. posts, each represented as a continuous vector, and edges representing the direction of propagation and providing the context to individual posts. The basic idea is to find and capture the salient su"
P17-1066,P15-1169,0,0.00977902,"classifier. We treat each tree as an instance, and its similarity values with all training instances as feature space. Therefore, the kernel matrix of training set is m × m and that of test set is n × m where m and n are the sizes of training and test sets, respectively. For our multi-class task, we perform a one-vsall classification for each label and then assign the one with the highest likelihood among the four, i.e., non-rumor, false rumor, true rumor or unverified rumor. We choose this method due to interpretability of results, similar to recent work on occupational class classification (Preotiuc-Pietro et al., 2015; Lukasik et al., 2015). 5 Twitter16 173,487 818 204,820 205 205 205 203 848 Hours 251 2,765 81 Twrench3 and crawled the replies through Twitter’s web interface. Finally, we annotated the source tweets by referring to the labels of the events they are from. We first turned the label of each event in Twitter15 and Twitter16 from binary to quaternary according to the veracity tag of the article in rumor debunking websites (e.g., snopes.com, Emergent.info, etc). Then we labeled the source tweets by following these rules: 1) Source tweets from unverified rumor events or non-rumor events are labele"
P17-1066,D11-1147,0,0.272341,"Ma et al. (2016) used recurrent neural networks to learn the representations of rumor signals from tweet text at different times. Our work will consider temporal, structural and linguistic signals in a unified framework based on propagation tree kernel. Most previous work formulated the task as classification at event level where an event is comprised of a number of source tweets, each being associated with a group of retweets and replies. Here we focus on classifying a given source tweet regarding a claim which is a finer-grained task. Similar setting was also considered in (Wu et al., 2015; Qazvinian et al., 2011). Kernel methods are designed to evaluate the similarity between two objects, and tree kernel specifically addresses structured data which has been successfully applied for modeling syntactic information in many natural language tasks such as syntactic parsing (Collins and Duffy, 2001), question-answering (Moschitti, 2006), semantic analysis (Moschitti, 2004), relation extraction (Zhang et al., 2008) and machine translation (Sun et al., 2010). These kernels are not suitable for modeling the social media propagation structures because the nodes are not given as discrete values like part-of-spee"
P17-1066,P10-1032,0,0.015345,"focus on classifying a given source tweet regarding a claim which is a finer-grained task. Similar setting was also considered in (Wu et al., 2015; Qazvinian et al., 2011). Kernel methods are designed to evaluate the similarity between two objects, and tree kernel specifically addresses structured data which has been successfully applied for modeling syntactic information in many natural language tasks such as syntactic parsing (Collins and Duffy, 2001), question-answering (Moschitti, 2006), semantic analysis (Moschitti, 2004), relation extraction (Zhang et al., 2008) and machine translation (Sun et al., 2010). These kernels are not suitable for modeling the social media propagation structures because the nodes are not given as discrete values like part-of-speech tags, but are represented as high dimensional real-valued vectors. Our proposed method is a substantial extension of tree kernel for modeling such structures. posts, each represented as a continuous vector, and edges representing the direction of propagation and providing the context to individual posts. The basic idea is to find and capture the salient substructures in the propagation trees indicative of rumors. We also extend PTK into a"
P17-1066,D07-1076,0,0.0219241,"r longitudinal diffusion (Zubiaga et al., 2016; Kwon et al., 2017). 4.3 One defect of PTK is that it ignores the clues outside the subtrees, e.g., how the information propagates from source post to the current subtree. Intuitively, propagation paths provide further clue for determining the truthfulness of information since they embed the route and context of how the propagation happens. Therefore, we propose contextsensitive PTK (cPTK) by considering the propagation paths from the root of the tree to the roots of subtrees, which shares similar intuition with the context-sensitive tree kernel (Zhou et al., 2007). For a propagation tree node v ∈ T (r), let Lrv be the length (i.e., # of nodes) of the propagation path from root r to v, and v[x] be the x-th ancestor of v on the path starting from v (0 ≤ x &lt; Lrv , v[0] = v, v[Lrv − 1] = r). cPTK evaluates the similarity between two trees T1 (r1 ) and T2 (r2 ) as follows: |N gram(ci ) ∩ N gram(cj )| |N gram(ci ) ∪ N gram(cj )| where ci and cj are the sets of content words in two nodes. For n-grams here, we adopt both uni-grams and bi-grams. It can capture cue terms e.g., ‘false’, ‘debunk’, ‘not true’, etc. commonly occurring in rumors but not in non-rumors"
P18-1184,W14-4012,0,0.209233,"Missing"
P18-1184,P16-2064,0,0.218475,"er, it can be seen that when a post denies the false rumor, it tends to spark supportive or affirmative replies confirming the denial; in contrast, denial to a true rumor tends to trigger question or denial in its replies. This observation may suggest a more general hypothesis that the repliers tend to disagree with (or question) who support a false rumor or deny a true rumor, and also they tend to agree with who deny a false rumor or support a true rumor. Meanwhile, a reply, rather than directly responding to the source tweet (i.e., the root), is usually responsive to its immediate ancestor (Lukasik et al., 2016; Zubiaga et al., 2016a), suggesting obvious local characteristic of the interaction. The recursive network naturally models such structures for learning to capture the rumor indicative signals and enhance the representation by recursively aggregating the signals from different branches. To this end, we extend the standard RvNN into two variants, i.e., a bottom-up (BU) model and a top-down (TD) model, which represent the propagation tree structure from different angles, in order to visit the nodes and combine their representations following distinct directions. The important merit of such arch"
P18-1184,P17-1066,1,0.352397,"ng and debunking. For automating rumor detection, most of the previous studies focused on text mining from sequential microblog streams using supervised models based on feature engineering (Castillo et al., 2011; Kwon et al., 2013; Liu et al., 2015; Ma et al., 2015), and more recently deep neural models (Ma et al., 2016; Chen et al., 2017; Ruchansky et al., 2017). These methods largely ignore or oversimplify the structural information associated with message propagation which however has been shown conducive to provide useful clues for identifying rumors. Kernel-based method (Wu et al., 2015; Ma et al., 2017) was thus proposed to model the structure as propagation trees in order to differentiate rumorous and non-rumorous claims by comparing their tree-based similarities. But such kind of approach cannot directly classify a tree without pairwise comparison with all other trees imposing unnecessary overhead, and it also cannot automatically learn any high-level feature representations out of the noisy surface features. In this paper, we present a neural rumor detection approach based on recursive neural networks (RvNN) to bridge the content semantics and propagation clues. RvNN and its variants 1980"
P18-1184,D15-1279,0,0.0159973,"a Weibo. Ma et al. (2017) used tree kernel to capture the similarity of propagation trees by counting their similar substructures in order to identify different types of rumors on Twitter. Compared to their studies, our model can learn the useful features via a more natural and general approach, i.e., the tree-structured neural network, to jointly generate representations from both structure and content. and Cardie, 2014). In order to avoid gradient vanishing, some studies integrated Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) to RvNN (Zhu et al., 2015; Tai et al., 2015). Mou et al. (2015) used a convolutional network over tree structures for syntactic tree parsing of natural language sentences. 3 Problem Statement We define a Twitter rumor detection dataset as a set of claims C = {C1 , C2 , · · · , C|C |}, where each claim Ci corresponds to a source tweet ri which consists of ideally all its relevant responsive tweets in chronological order, i.e., Ci = {ri , xi1 , xi2 , · · · , xim } where each xi∗ is a responsive tweet of the root ri . Note that although the tweets are notated sequentially, there are connections among them based on their reply or repost relationships, which c"
P18-1184,D11-1147,0,0.278999,"umors in massive scale with low cost thanks to the popularity of social media outlets on Twitter, Facebook, etc. The worst effect of false rumors could be devastating to individual and/or society. Research pertaining rumors spans multiple disciplines, such as philosophy and humanities (DiFonzo and Bordia, 2007; Donovan, 2007), social psychology (Allport and Postman, 1965; Jaeger et al., 1980; Rosnow and Foster, 2005), political studies (Allport and Postman, 1946; Berinsky, 2017), management science (DiFonzo et al., 1994; Kimmel, 2004) and recently computer science and artificial intelligence (Qazvinian et al., 2011; Ratkiewicz et al., 2011; Castillo et al., 2011; Hannak et al., 2014; Zhao et al., 2015; Ma et al., 2015). Rumor is commonly defined as information that emerge and spread among people whose truth value is unverified or intentionally false (DiFonzo and Bordia, 2007; Qazvinian et al., 2011). Analysis shows that people tend to stop spreading a rumor if it is known as false (Zubiaga et al., 2016b). However, identifying such misinformation is non-trivial and needs investigative journalism to fact check the suspected claim, which is labor-intensive and time-consuming. The proliferation of social me"
P18-1184,D12-1110,0,0.427275,"ims, that is f : Ci → Yi , where Yi takes one of the four finer-grained classes: non-rumor, false rumor, true rumor, and unverified rumor that are introduced in the literature (Ma et al., 2017; Zubiaga et al., 2016b). An important issue of the tree structure is concerned about the direction of edges, which can result in two different architectures of the model: 1) a bottom-up tree; 2) a top-down tree, which are defined as follows: RvNN has demonstrated state-of-the-art performances in a variety of tasks, e.g., images segmentation (Socher et al., 2011), phrase representation from word vectors (Socher et al., 2012), and sentiment classification in sentences (Socher et al., 2013). More recently, a deep RvNN was proposed to model the compositionality in natural language for fine-grained sentiment classification by stacking multiple recursive layers (Irsoy 1982 • Bottom-up tree takes the similar shape as shown in Figure 1, where responsive nodes always point to their responded nodes and leaf nodes not having any response are laid out at the furthest level. We represent a tree as Ti = hVi , Ei i, where Vi = Ci which consists of all relevant posts as nodes, and Ei denotes a set of all directed links, where f"
P18-1184,D13-1170,0,0.0466271,"rained classes: non-rumor, false rumor, true rumor, and unverified rumor that are introduced in the literature (Ma et al., 2017; Zubiaga et al., 2016b). An important issue of the tree structure is concerned about the direction of edges, which can result in two different architectures of the model: 1) a bottom-up tree; 2) a top-down tree, which are defined as follows: RvNN has demonstrated state-of-the-art performances in a variety of tasks, e.g., images segmentation (Socher et al., 2011), phrase representation from word vectors (Socher et al., 2012), and sentiment classification in sentences (Socher et al., 2013). More recently, a deep RvNN was proposed to model the compositionality in natural language for fine-grained sentiment classification by stacking multiple recursive layers (Irsoy 1982 • Bottom-up tree takes the similar shape as shown in Figure 1, where responsive nodes always point to their responded nodes and leaf nodes not having any response are laid out at the furthest level. We represent a tree as Ti = hVi , Ei i, where Vi = Ci which consists of all relevant posts as nodes, and Ei denotes a set of all directed links, where for any u, v ∈ Vi , u ← v exists if v responses to u. This structu"
P18-1184,P15-1150,0,0.0608266,"cting rumors on Sina Weibo. Ma et al. (2017) used tree kernel to capture the similarity of propagation trees by counting their similar substructures in order to identify different types of rumors on Twitter. Compared to their studies, our model can learn the useful features via a more natural and general approach, i.e., the tree-structured neural network, to jointly generate representations from both structure and content. and Cardie, 2014). In order to avoid gradient vanishing, some studies integrated Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) to RvNN (Zhu et al., 2015; Tai et al., 2015). Mou et al. (2015) used a convolutional network over tree structures for syntactic tree parsing of natural language sentences. 3 Problem Statement We define a Twitter rumor detection dataset as a set of claims C = {C1 , C2 , · · · , C|C |}, where each claim Ci corresponds to a source tweet ri which consists of ideally all its relevant responsive tweets in chronological order, i.e., Ci = {ri , xi1 , xi2 , · · · , xim } where each xi∗ is a responsive tweet of the root ri . Note that although the tweets are notated sequentially, there are connections among them based on their reply or repost rel"
P18-1184,C16-1230,0,0.524785,"osnow and Foster, 2005), political studies (Allport and Postman, 1946; Berinsky, 2017), management science (DiFonzo et al., 1994; Kimmel, 2004) and recently computer science and artificial intelligence (Qazvinian et al., 2011; Ratkiewicz et al., 2011; Castillo et al., 2011; Hannak et al., 2014; Zhao et al., 2015; Ma et al., 2015). Rumor is commonly defined as information that emerge and spread among people whose truth value is unverified or intentionally false (DiFonzo and Bordia, 2007; Qazvinian et al., 2011). Analysis shows that people tend to stop spreading a rumor if it is known as false (Zubiaga et al., 2016b). However, identifying such misinformation is non-trivial and needs investigative journalism to fact check the suspected claim, which is labor-intensive and time-consuming. The proliferation of social media makes it worse due to the ever-increasing information load and dynamics. Therefore, it is necessary to develop automatic and assistant approaches to facilitate real-time rumor tracking and debunking. For automating rumor detection, most of the previous studies focused on text mining from sequential microblog streams using supervised models based on feature engineering (Castillo et al., 20"
P18-2033,D17-1237,1,0.889592,"Missing"
P18-2033,N07-2038,0,0.0554368,"the turn information. In terms of the representation vector of symptoms, it’s dimension is equal to the number of all symptoms, whose elements for positive symptoms are 1, negative symptoms are -1, notsure symptoms are −2 and not-mentioned sympUser Simulator At the beginning of each dialogue session, a user simulator samples a user goal from the experiment dataset. At each turn t, the user takes an action au,t according to the current user state su,t and the previous agent action at−1 , and transits into the next user state su,t+1 . In practice, the user state su is factored into an agenda A (Schatzmann et al., 2007) and a goal G, noted as su = (A, G). During the course of the dialogue, the goal G ensures that the user behaves in a consistent, goal-oriented manner. And the agenda contains a list of symptoms and their status (whether or not they are requested) to track the progress of the conversation. Every dialogue session is initiated by the user 203 4 toms are 0. Each state s ∈ S is the concatenation of these four vectors. Actions A. An action a ∈ A is composed of a dialogue act (e.g., inform, request, deny and confirm) and a slot (i.e., normalized symptoms or a special slot disease). In addition, than"
P18-2033,I17-1074,0,0.222406,"agnose tests, vital signs and medical image. And it is collected accumulatively following a diagnostic procedure in clinic, which involves interactions between patients and doctors and some complicated medical tests. Therefore, it is very expensive to collect EHRs for different diseases. How to collect the information from patient automatically remains the challenge for automatic diagnosis. Recently, due to its promising potentials and alluring commercial values, research about taskoriented dialogue system (DS) has attracted increasing attention in different domains, including ticket booking (Li et al., 2017; Peng et al., 2017a), online shopping (Yan et al., 2017) and restaurant searching (Wen et al., 2017). We believe that applying DS in the medical domain has great potential to reduce the cost of collecting data from patients. However, there is a gap to fill for applying DS in disease identification. There are basically two major challenges. First, the lack of annotated medical dialogue dataset. Second, no available DS framework for disease identification. By addressing these two problems, we make the first move to build a dialogue system facilitating automatic information collection and diagno"
P18-2033,E17-1042,0,0.0993595,"Missing"
P19-1244,W18-5513,0,0.0348731,"al., 2015), which is a task of classifying the relationship between a pair of sentences, composed by a premise and a hypothesis, as Entails, Contradicts or Neutral, Thorne et al. (2018a) formulated claim verification as a task that aims to classify claims into Supported, Refuted or Not Enough Info (NEI). They released a large dataset containing mutated claims based on relevant Wikipedia articles and developed a basic pipeline with document retrieval, sentence selection, and NLI modules. Similar pipelines were developed by most of the participating teams (Nie et al., 2019; Padia et al., 2018; Alhindi et al., 2018; Hanselowski et al., 2018) in FEVER shared task (Thorne et al., 2018b). Apart from the document retrieval function, our model is end-to-end and aims to learn sentence-level evidence with a hierarchical attention framework. Attention is in general used to attend on the most important part of texts, and has been successfully applied in machine translation (Luong et al.), question answering (Xiong et al., 2016) and parsing (Dozat and Manning, 2016), and is adopted in our model for attending on important sentences as evidence. Our work is also related to coherence modeling. Different from traditi"
P19-1244,D15-1075,0,0.592546,"rning models such as recurrent neural networks (RNN) (Ma et al., 2016), convolutional neural networks (CNN) (Wang, 2017) and recursive neural 2562 networks (Ma et al., 2018) were also exploited to learn the feature representations. More recently, semantic matching methods were proposed to retrieve evidence from relatively trustworthy sources such as checked news and Wikipedia articles. Popat et al. (2018) attempted to debunk false claims by learning claim representations from relevant articles using an attention mechanism to focus on words that are closely related to the claim. Following NLI (Bowman et al., 2015), which is a task of classifying the relationship between a pair of sentences, composed by a premise and a hypothesis, as Entails, Contradicts or Neutral, Thorne et al. (2018a) formulated claim verification as a task that aims to classify claims into Supported, Refuted or Not Enough Info (NEI). They released a large dataset containing mutated claims based on relevant Wikipedia articles and developed a basic pipeline with document retrieval, sentence selection, and NLI modules. Similar pipelines were developed by most of the participating teams (Nie et al., 2019; Padia et al., 2018; Alhindi et"
P19-1244,W14-4012,0,0.0551999,"Missing"
P19-1244,D17-1070,0,0.0225496,"s: yˆ = softmax(Vo · hcS + bo ) Entailment-based Evidence Attention We further enhance the sentence representation by capturing the entailment relations between the sentences and the claim based on the NLI method (Bowman et al., 2015) for strengthening the semantic inference capacity of our model. Given c and si , we represent each such pair by integrating three matching functions between hc ˜ s : 1) concatenation [hc , h ˜ s ]; 2) elementand h i i ˜ wise product hc hsi ; and 3) absolute element˜ s |. The similar matching wise difference |hc − h i scheme was commonly used to train NLI models (Conneau et al., 2017; Mou et al., 2016; Liu et al., 2016; Chen et al., 2016). We then perform a transformation to obtain the joint representation hcsi as follow:  h i ˜ s , hc h ˜ s , |hc − h ˜s | hcsi = tanh We · hc , h i i i (6) where We are trainable weights for transforming the long concatenation into an l-dimensional vector. We omit the bias to avoid notational clutter. To capture entailment-based evidence, we again apply attention over the original sentences guided by the joint representation hcsi which is obtained on top of the coherence attention. This yields: bi = tanh(Ve · hcsi + be ) exp(bi ) βi = P"
P19-1244,P81-1022,0,0.360561,"Missing"
P19-1244,C18-1284,0,0.147537,"Missing"
P19-1244,N16-1138,0,0.0220374,"Wikipedia. 2 Related Work The literature on fact-checking and credibility assessment has been reviewed by several comprehensive surveys (Shu et al., 2017; Zubiaga et al., 2018; Kumar and Shah, 2018; Sharma et al., 2019). We only briefly review prior works closely related to ours. Many studies on claim verification extracted veracity-indicative features that can reflect stances and writing styles from relevant texts such as news articles, microblog posts, etc. and used the traditional supervised models to learn the parameters (Castillo et al., 2011; Qazvinian et al., 2011; Rubin et al., 2016; Ferreira and Vlachos, 2016; Rashkin et al., 2017). Deep learning models such as recurrent neural networks (RNN) (Ma et al., 2016), convolutional neural networks (CNN) (Wang, 2017) and recursive neural 2562 networks (Ma et al., 2018) were also exploited to learn the feature representations. More recently, semantic matching methods were proposed to retrieve evidence from relatively trustworthy sources such as checked news and Wikipedia articles. Popat et al. (2018) attempted to debunk false claims by learning claim representations from relevant articles using an attention mechanism to focus on words that are closely rela"
P19-1244,W18-5516,0,0.122718,"a task of classifying the relationship between a pair of sentences, composed by a premise and a hypothesis, as Entails, Contradicts or Neutral, Thorne et al. (2018a) formulated claim verification as a task that aims to classify claims into Supported, Refuted or Not Enough Info (NEI). They released a large dataset containing mutated claims based on relevant Wikipedia articles and developed a basic pipeline with document retrieval, sentence selection, and NLI modules. Similar pipelines were developed by most of the participating teams (Nie et al., 2019; Padia et al., 2018; Alhindi et al., 2018; Hanselowski et al., 2018) in FEVER shared task (Thorne et al., 2018b). Apart from the document retrieval function, our model is end-to-end and aims to learn sentence-level evidence with a hierarchical attention framework. Attention is in general used to attend on the most important part of texts, and has been successfully applied in machine translation (Luong et al.), question answering (Xiong et al., 2016) and parsing (Dozat and Manning, 2016), and is adopted in our model for attending on important sentences as evidence. Our work is also related to coherence modeling. Different from traditional coherence studies focu"
P19-1244,D16-1032,0,0.0158176,"o-end and aims to learn sentence-level evidence with a hierarchical attention framework. Attention is in general used to attend on the most important part of texts, and has been successfully applied in machine translation (Luong et al.), question answering (Xiong et al., 2016) and parsing (Dozat and Manning, 2016), and is adopted in our model for attending on important sentences as evidence. Our work is also related to coherence modeling. Different from traditional coherence studies focusing on discourse coherence among sentences that are widely applied in text generation (Park and Kim, 2015; Kiddon et al., 2016) and summarization (Logeswaran et al., 2018), we try to capture evidential sentences topically coherent not only among themselves but also with respect to the target claim. trieved from text collections containing variable number of sentences, and we disregard the order of sentences and which documents they are from. Our task is to classify an instance into a class defined by the specific dataset, such as veracity class labels, e.g., True/False, or NLI-style class labels, e.g., Supported/Refuted/NEI. Our approach exploits and integrates two core semantic relations: 1) coherence of the sentence"
P19-1244,P18-1184,1,0.846231,"019). We only briefly review prior works closely related to ours. Many studies on claim verification extracted veracity-indicative features that can reflect stances and writing styles from relevant texts such as news articles, microblog posts, etc. and used the traditional supervised models to learn the parameters (Castillo et al., 2011; Qazvinian et al., 2011; Rubin et al., 2016; Ferreira and Vlachos, 2016; Rashkin et al., 2017). Deep learning models such as recurrent neural networks (RNN) (Ma et al., 2016), convolutional neural networks (CNN) (Wang, 2017) and recursive neural 2562 networks (Ma et al., 2018) were also exploited to learn the feature representations. More recently, semantic matching methods were proposed to retrieve evidence from relatively trustworthy sources such as checked news and Wikipedia articles. Popat et al. (2018) attempted to debunk false claims by learning claim representations from relevant articles using an attention mechanism to focus on words that are closely related to the claim. Following NLI (Bowman et al., 2015), which is a task of classifying the relationship between a pair of sentences, composed by a premise and a hypothesis, as Entails, Contradicts or Neutral"
P19-1244,P16-2022,0,0.0365285,"cS + bo ) Entailment-based Evidence Attention We further enhance the sentence representation by capturing the entailment relations between the sentences and the claim based on the NLI method (Bowman et al., 2015) for strengthening the semantic inference capacity of our model. Given c and si , we represent each such pair by integrating three matching functions between hc ˜ s : 1) concatenation [hc , h ˜ s ]; 2) elementand h i i ˜ wise product hc hsi ; and 3) absolute element˜ s |. The similar matching wise difference |hc − h i scheme was commonly used to train NLI models (Conneau et al., 2017; Mou et al., 2016; Liu et al., 2016; Chen et al., 2016). We then perform a transformation to obtain the joint representation hcsi as follow:  h i ˜ s , hc h ˜ s , |hc − h ˜s | hcsi = tanh We · hc , h i i i (6) where We are trainable weights for transforming the long concatenation into an l-dimensional vector. We omit the bias to avoid notational clutter. To capture entailment-based evidence, we again apply attention over the original sentences guided by the joint representation hcsi which is obtained on top of the coherence attention. This yields: bi = tanh(Ve · hcsi + be ) exp(bi ) βi = P i exp(bi ) X βi ·"
P19-1244,W18-5527,0,0.0208916,"owing NLI (Bowman et al., 2015), which is a task of classifying the relationship between a pair of sentences, composed by a premise and a hypothesis, as Entails, Contradicts or Neutral, Thorne et al. (2018a) formulated claim verification as a task that aims to classify claims into Supported, Refuted or Not Enough Info (NEI). They released a large dataset containing mutated claims based on relevant Wikipedia articles and developed a basic pipeline with document retrieval, sentence selection, and NLI modules. Similar pipelines were developed by most of the participating teams (Nie et al., 2019; Padia et al., 2018; Alhindi et al., 2018; Hanselowski et al., 2018) in FEVER shared task (Thorne et al., 2018b). Apart from the document retrieval function, our model is end-to-end and aims to learn sentence-level evidence with a hierarchical attention framework. Attention is in general used to attend on the most important part of texts, and has been successfully applied in machine translation (Luong et al.), question answering (Xiong et al., 2016) and parsing (Dozat and Manning, 2016), and is adopted in our model for attending on important sentences as evidence. Our work is also related to coherence modeling."
P19-1244,D14-1162,0,0.091726,"he distributions of the predicted and the ground-truth entailment classes. Overall Training After pre-training, all the model parameters are trained end-to-end by minimizing the squared error between the class probability distribution of the prediction and that of the ground truth over the claims. Parameters are updated through backpropagation (Collobert et al., 2011) with AdaGrad (Duchi et al., 2011) for speeding up convergence. The training process ends when the model converges or the maximum epoch number is met. We represent input words using pre-trained GloVe Wikipedia 6B word embeddings (Pennington et al., 2014). We set d to 300 for word vectors and l to 100 for hidden units, and no parameter depends on n which varies with different claims. 5 5.1 Experiments and Results Datasets and Evaluation Metrics We use three public fact-checking datasets for evaluation: 1) Snopes and 2) PolitiFact, released by Popat et al. (2018), containing 4,341 and 3,568 news claims, respectively, along with relevant articles collected from various web sources; 3) FEVER, released by Thorne et al. (2018a), which consists of 185,445 claims accompanied by human-annotated relevant Wikipedia articles and evidence-bearing sentence"
P19-1244,D18-1003,0,0.207437,"iable sources, e.g., encyclopedia articles, verified news, etc., as an important distinguishing factor (Thorne and Vlachos, 2018). Ferreira and Vlachos (2016) use news headlines as evidence to predict whether it is for, against or observing a claim. In the Fake News Challenge2 , the body text of an article is used as evidence to detect the stances relative to the claim made in the headline. Thorne et al. (2018a) formulate the Fact Extraction and VERification (FEVER) task which requires extracting evidence from Wikipedia and synthesizing information from multiple documents to verify the claim. Popat et al. (2018) propose DeClarE, an evidence-aware neural attention model to aggregate salient words from source news articles as the 2 http://www.fakenewschallenge.org/ 2561 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2561–2571 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics c: s1 : s2 : s3 : s4 : s5 : s6 : s7 : The test of a 5G cellular network is the cause of unexplained bird deaths occurring in a park in The Hague, Netherlands. Verdict: False [Contradict]: Lots of tests going on with it in the Netherlands, but"
P19-1244,D11-1147,0,0.0497903,"ollected from snopes.com, politifact.com and Wikipedia. 2 Related Work The literature on fact-checking and credibility assessment has been reviewed by several comprehensive surveys (Shu et al., 2017; Zubiaga et al., 2018; Kumar and Shah, 2018; Sharma et al., 2019). We only briefly review prior works closely related to ours. Many studies on claim verification extracted veracity-indicative features that can reflect stances and writing styles from relevant texts such as news articles, microblog posts, etc. and used the traditional supervised models to learn the parameters (Castillo et al., 2011; Qazvinian et al., 2011; Rubin et al., 2016; Ferreira and Vlachos, 2016; Rashkin et al., 2017). Deep learning models such as recurrent neural networks (RNN) (Ma et al., 2016), convolutional neural networks (CNN) (Wang, 2017) and recursive neural 2562 networks (Ma et al., 2018) were also exploited to learn the feature representations. More recently, semantic matching methods were proposed to retrieve evidence from relatively trustworthy sources such as checked news and Wikipedia articles. Popat et al. (2018) attempted to debunk false claims by learning claim representations from relevant articles using an attention m"
P19-1244,D17-1317,0,0.44431,"winternet.org/2018/03/ 01/social-media-use-in-2018/ claims being produced on daily basis. Therefore, it is an urgent need to automate the process and ease the human burden in assessing the veracity of claims (Thorne and Vlachos, 2018). Not surprisingly, various methods for automatic claim verification have been proposed using machine learning. Typically, given the claims, models are learned from auxiliary relevant sources such as news articles or social media responses for capturing words and linguistic units that might indicate viewpoint or language style towards the claim (Jin et al., 2016; Rashkin et al., 2017; Popat et al., 2017; Volkova et al., 2017; Dungs et al., 2018). However, the factuality of a claim is independent of people’s belief and subjective language use, and human perception is unconsciously prone to misinformation due to the common cognitive biases such as naive realism (Reed et al., 2013) and confirmation bias (Nickerson, 1998). A recent trend is that researchers are trying to establish more objective tasks and evidence-based verification solutions, which focus on the use of evidence obtained from more reliable sources, e.g., encyclopedia articles, verified news, etc., as an import"
P19-1244,W16-0802,0,0.0765855,", politifact.com and Wikipedia. 2 Related Work The literature on fact-checking and credibility assessment has been reviewed by several comprehensive surveys (Shu et al., 2017; Zubiaga et al., 2018; Kumar and Shah, 2018; Sharma et al., 2019). We only briefly review prior works closely related to ours. Many studies on claim verification extracted veracity-indicative features that can reflect stances and writing styles from relevant texts such as news articles, microblog posts, etc. and used the traditional supervised models to learn the parameters (Castillo et al., 2011; Qazvinian et al., 2011; Rubin et al., 2016; Ferreira and Vlachos, 2016; Rashkin et al., 2017). Deep learning models such as recurrent neural networks (RNN) (Ma et al., 2016), convolutional neural networks (CNN) (Wang, 2017) and recursive neural 2562 networks (Ma et al., 2018) were also exploited to learn the feature representations. More recently, semantic matching methods were proposed to retrieve evidence from relatively trustworthy sources such as checked news and Wikipedia articles. Popat et al. (2018) attempted to debunk false claims by learning claim representations from relevant articles using an attention mechanism to focus on"
P19-1244,C18-1283,0,0.025314,"pes micF1 macF1 0.750 0.674 0.776 0.727 0.788 0.741 0.779 0.728 0.771 0.738 0.807 0.759 PolitiFact micF1 macF1 0.470 0.431 0.495 0.455 0.516 0.473 0.508 0.463 0.520 0.471 0.523 0.487 Results of Comparison Table 3: Results of ablation test across different attentions on Snopes (left) and PoliFact (right) datasets. 5.2 Experiments on Veracity-based Datasets We compare our model and several state-of-the-art baseline methods described below. 1) SVM: A linear SVM model for fake news detection using a set of linguistic features (e.g., bag-of-words, ngrams, etc.) handcrafted from relevant sentences (Thorne and Vlachos, 2018); 2) CNN and LSTM: The CNN-based detection model (Wang, 2017) and LSTM-based RNN model for representation learning from word sequences (Rashkin et al., 2017), respectively, both using only claim content without considering external resources; 3) DeClarE: The word-level neural attention model for Debunking Claims with Interpretable Evidence (Popat et al., 2018) capturing world-level evidence from relevant articles; 4) HAN: Our full model based on Hierarchical Attention Networks, where coherence component uses Eq. 3; 5) HAN-ba: A variant of HAN with biaffine attention in Eq. 2; 6) HANna: Our red"
P19-1244,N18-1074,0,0.146413,"Missing"
P19-1244,W18-5501,0,0.0882274,"Missing"
P19-1244,P17-2102,0,0.116006,"Missing"
P19-1244,P17-2067,0,0.218944,"l., 2018; Kumar and Shah, 2018; Sharma et al., 2019). We only briefly review prior works closely related to ours. Many studies on claim verification extracted veracity-indicative features that can reflect stances and writing styles from relevant texts such as news articles, microblog posts, etc. and used the traditional supervised models to learn the parameters (Castillo et al., 2011; Qazvinian et al., 2011; Rubin et al., 2016; Ferreira and Vlachos, 2016; Rashkin et al., 2017). Deep learning models such as recurrent neural networks (RNN) (Ma et al., 2016), convolutional neural networks (CNN) (Wang, 2017) and recursive neural 2562 networks (Ma et al., 2018) were also exploited to learn the feature representations. More recently, semantic matching methods were proposed to retrieve evidence from relatively trustworthy sources such as checked news and Wikipedia articles. Popat et al. (2018) attempted to debunk false claims by learning claim representations from relevant articles using an attention mechanism to focus on words that are closely related to the claim. Following NLI (Bowman et al., 2015), which is a task of classifying the relationship between a pair of sentences, composed by a premise"
P19-1244,D18-1010,0,0.100673,"t the essential objective of our model is not for evidence retrieval and ranking. Instead of ranking sentences into the top-k positions, we pay more attention on claim verification accuracy by embedding and aggregating the useful sentences as evidence like we have explained above. However, such discrepancy inspires us to investigate in the future an end-to-end approach to jointly model evidence retrieval and claim verification in a unified framework based on our sentence-level attention mechanism. Finally, thanks to one of our reviewers, we learn about another two-stage model named TwoWingOS (Yin and Roth, 2018), which achieves a comparable FEVER score but a little bit higher accuracy than ours on FEVER task. The TwoWingOS applies a two-wing optimization approach to jointly optimizing sentence selection and veracity classification. The reasons regarding their higher performance might lie in that: 1) their input word embeddings are fine-tuned based on the context of the evidence and claim while ours are fixed during training; and 2) the document retrieval module of the TwoWingOS has demonstrated higher effectiveness than that of the NSMN (see rate (recall) and acc ceiling (OFEVER) in Tables 2 in (Yin"
P19-1270,N12-1074,0,0.771335,"Missing"
P19-1270,D17-1243,0,0.0884173,"g Embedding t1 LSTM a1 . . . . . . t|c| Context Modeling Layer ...... Turn Encoder Embedding a|c| m1 H& |&apos;| Embedding ...... m|u| User History Modeling Layer Figure 2: The generic framework for re-entry prediction. We implement it with three encoders (Average Embedding, CNN, and BiLSTM) for turn modeling and four mechanisms (Simple Concatenation, Attention, Memory Networks, and Bi-attention) for modeling interactions between context and user history. better understand the structure of conversations, Recurrent Neural Network (RNN)-based methods have been exploited to capture temporal dynamics (Cheng et al., 2017; Zayats and Ostendorf, 2018; Jiao et al., 2018). Different from the above work, our model not only utilizes the conversations themselves, but also leverages users’ prior posts in other discussions. 3 Neural Re-entry Prediction Combining Context and User History This section describes our neural network-based conversation re-entry prediction framework exploring the joint effects of context and user history. Figure 2 shows the overall architecture of our framework, consisting of three main layers: context modeling layer, user history modeling layer, and interaction modeling layer to learn how i"
P19-1270,N10-1020,0,0.204523,"Missing"
P19-1270,P12-1054,0,0.419325,"bi-attention outperforms both humans, suggesting the difficulty of the task as well as the effectiveness of our proposed framework. 2 Related Work Response Prediction. Previous work on response prediction mainly focuses on predicting whether users will respond to a given social media post or thread. Efforts have been made to measure the popularity of a social media post via modeling the response patterns in replies or retweets (Artzi et al., 2012; Zhang et al., 2015). Some studies investigate post recommendation by predicting whether a response will be made by a given user (Chen et al., 2012; Yan et al., 2012; Hong et al., 2013; Alawad et al., 2016). In addition to post-level prediction, other studies focus on response prediction at the conversation-level. Zeng et al. (2018) investigate microblog conversation recommendation by exploiting latent factors of topics and discourse with a Bayesian model, which often requires domain expertise for customized learning algorithms. Our neural framework can automatically acquire the interactions among important components that contribute to the re-entry prediction problem, and can be easily adapted to new domains. For the prediction of re-entry behavior in on"
P19-1270,Q18-1009,0,0.0829916,"a1 . . . . . . t|c| Context Modeling Layer ...... Turn Encoder Embedding a|c| m1 H& |&apos;| Embedding ...... m|u| User History Modeling Layer Figure 2: The generic framework for re-entry prediction. We implement it with three encoders (Average Embedding, CNN, and BiLSTM) for turn modeling and four mechanisms (Simple Concatenation, Attention, Memory Networks, and Bi-attention) for modeling interactions between context and user history. better understand the structure of conversations, Recurrent Neural Network (RNN)-based methods have been exploited to capture temporal dynamics (Cheng et al., 2017; Zayats and Ostendorf, 2018; Jiao et al., 2018). Different from the above work, our model not only utilizes the conversations themselves, but also leverages users’ prior posts in other discussions. 3 Neural Re-entry Prediction Combining Context and User History This section describes our neural network-based conversation re-entry prediction framework exploring the joint effects of context and user history. Figure 2 shows the overall architecture of our framework, consisting of three main layers: context modeling layer, user history modeling layer, and interaction modeling layer to learn how information captured by the p"
P19-1270,N18-1035,1,0.907686,"t). More importantly, our framework enables the re-entry prediction and corresponding representations to be learned in an end-to-end manner. On the contrary, previous methods for the same task rely on handcrafted features (Backstrom et al., 2013; Budak and Agrawal, 2013), which often require laborintensive and time-consuming feature engineering processes. To the best of our knowledge, we are the first to explore the joint effect of conversation context and user history on predicting re-entry behavior in a neural network framework. We experiment with two large-scale datasets, one from Twitter (Zeng et al., 2018), the other from Reddit which is newly collected1 . Our framework with bi-attention significantly outperforms all the comparing methods including the previous state of the art (Backstrom et al., 2013). For instance, our model achieves an F1 score of 61.1 on Twitter conversations, compared to an F1 score of 57.0 produced by Backstrom et al. (2013), which is based on a rich set of handcrafted features. Further experiments also show that the model with bi-attention can consistently outperform comparisons given varying lengths of conversation context. It shows that bi-attention mechanism can well"
P19-1270,P18-1125,0,0.0787958,"Missing"
P19-1270,W02-0109,0,0.0960075,"itter and Reddit users exhibit different conversation behaviors. Reddit users tend to engage in more conversations, resulting in more messages in user history (as shown in Figure 3(a)). Twitter users are more likely to stay within each conversation, leading to lengthy discussions and larger re-entry frequencies on average, as shown in Figure 3(b) and Table 1. 2813 Data Preprocessing and Model Setting. For preprocessing Twitter data, we applied Glove tweet preprocessing toolkit (Pennington et al., 2014).5 For the Reddit dataset, we first applied the open source natural language toolkit (NLTK) (Loper and Bird, 2002) for word tokenization. Then, we replaced links with the generic tag “URL” and removed all the nonalphabetic tokens. For both datasets, a vocabulary was built and maintained in experiments with all the tokens (including emoticons and punctuation) from training data. For model setups, we initialize the embedding layer with 200-dimensional Glove embedding (Pennington et al., 2014), where Twitter version is used for our Twitter dataset and the Common Crawl version applied on Reddit dataset.6 All the hyper-parameters are tuned on the development set by grid search. The batch size is set to 32. Ada"
P19-1270,D15-1166,0,0.0601222,"sigmoid-activated neural perceptron (Glorot et al., 2011), for predicting final output p(u, c). It indicates how likely the target user u will re-engage in the target conversation c. We then describe the four mechanisms to learn rO in turn below. Simple Concatenation. Here we simply put context representation (last state) and user representations (with average pooling) side by side, P|u |U C; yielding rO = [H|c| j Hj /|u|] as the interaction representation for re-entry prediction. Attention. To capture the context information useful for re-entry prediction, we exploit an attention mechanism (Luong et al., 2015) over H C . Attentions are employed to “soft-address” important context turns according to their similarity with user representation (with average pooling). Here we adopt dot attention weights and define the attended interaction representation as: rO = |c| X i αi · HiC , αi = sof tmax(HiC · |u| X HjU /|u|) (1) j Memory Networks. To further recognize indicative chatting messages in user history, we also apply end-to-end memory networks (MemN2N) (Sukhbaatar et al., 2015) for interaction modeling. It can be seen as a recurrent attention mechanism over chatting messages (stored in memory). Hence f"
P19-1270,D14-1162,0,0.0820763,"context. Thus combining them both might help alleviate the sparsity in one information source. We also notice that Twitter and Reddit users exhibit different conversation behaviors. Reddit users tend to engage in more conversations, resulting in more messages in user history (as shown in Figure 3(a)). Twitter users are more likely to stay within each conversation, leading to lengthy discussions and larger re-entry frequencies on average, as shown in Figure 3(b) and Table 1. 2813 Data Preprocessing and Model Setting. For preprocessing Twitter data, we applied Glove tweet preprocessing toolkit (Pennington et al., 2014).5 For the Reddit dataset, we first applied the open source natural language toolkit (NLTK) (Loper and Bird, 2002) for word tokenization. Then, we replaced links with the generic tag “URL” and removed all the nonalphabetic tokens. For both datasets, a vocabulary was built and maintained in experiments with all the tokens (including emoticons and punctuation) from training data. For model setups, we initialize the embedding layer with 200-dimensional Glove embedding (Pennington et al., 2014), where Twitter version is used for our Twitter dataset and the Common Crawl version applied on Reddit da"
P19-1270,P06-4018,0,\N,Missing
S15-2111,S13-2053,0,0.0815975,"ment analysis module: creates a SVM classifier that incorporates the above features classify the polarity of each tweet. Finally, Twitter-OpinMiner outputs the polarity of each tweet. 2.2 Development Data and Lexicon The development data are necessary in our system. We fully utilize the training tweets provided by SemEval 2013. The dataset consists of 9,912 annotated tweets. Besides, for sentiment analysis, we also utilize several sentiment lexicons, including Liu’s sentiment lexicon (Liu, 2012), MPQA subjectivity lexicon (Wilson et al., 2005), and the sentiment lexicon generated from tweets (Mohammad et al., 2013). 665 Feature Extraction The objective of this task is to determine whether a given message is positive, negative, or neutral. We train sentiment classifiers with LibLinear (Fan et al., 2008) on the training set and dev set, and tune parameter −c, −wi of SVM on the test set of SemEval 2013. SVM is a popular machine learning algorithm, the effectiveness of which has been proved in sentiment analysis on formal texts in related work (Pang and Lee, 2002; Liu, 2012). Since the performance of SVM classifier will be greatly influenced by the features selection, we explore a variety of features in the"
S15-2111,S13-2052,0,0.043413,"eliminate polarity ambiguities in compound sentences where both positive and negative sentiments are appearing. Based on the SemEval 2014 and SemEval 2015 Twitter sentiment analysis task datasets, the experimental results show that the performance of Twitter-OpinMiner could effectively recognize opinionated messages and identify the polarities. 1 Introduction This year comes the third edition of SemEval Twitter sentiment analysis task consisting of new genres, including topic-based polarity classification, trends detection towards a topic, and the sentimental strength of association of terms (Nakov et al., 2013). Corresponding We only participated in the subtask of message sentiment analysis and built up a system, named Twitter-OpinMiner for the task. TwitterOpinMiner stems from two different angles: LDAbased topic detection for discovering the opinionated features of trending tweets’ topics and sentiment analysis based on a variety of features.  Topic detection Recent studies show that people often search Twitter to find temporally relevant information (Teevan et al., 2011), such as emergent events, trending topics. In fact, similar opinions were likely to express on the same topic/event in Twitte"
S15-2111,W02-1011,0,0.0164539,"Missing"
S15-2111,P14-1146,0,0.0195006,"ether the last token contains an exclamation or question mark; • Emoticons: the presence of positive and negative emoticons at any position in the tweet; whether the last token is an emoticon; • OOV: the ratio of words out of vocabulary; • Elongated words: the presence of sentiment words with one character repeated more than two times, for example, ‘cooool’; • URL: whether the tweet contains a URL. • Reply or Retweet: Is the current tweet a reply/retweet tweet 3.4 Word embedding We also utilize word embedding technique for feature extraction. We adopt sentiment-specific word embedding method (Tang et al., 2014) that could encode sentiment information in the continuous representation of words. In our approach, each term is extended into a 150 dimensional vector. 3.5 Discourse specific feature Since tweets are usually expressed informally, there are many compound sentences in a tweet, which always contain positive sentiment and negative sentiment with ambiguity. For example, It may not be the biggest squad in the last 10yrs, but Ancelotti is working for quality over quantity. Everyone... http://t.co/oCdPXQWggT. 666 In this case, there are two segments in the tweet that holds a Contrast discourse relat"
S15-2111,H05-1044,0,0.0478955,"res, sentiment distribution among topics, and word embedding. (3) Sentiment analysis module: creates a SVM classifier that incorporates the above features classify the polarity of each tweet. Finally, Twitter-OpinMiner outputs the polarity of each tweet. 2.2 Development Data and Lexicon The development data are necessary in our system. We fully utilize the training tweets provided by SemEval 2013. The dataset consists of 9,912 annotated tweets. Besides, for sentiment analysis, we also utilize several sentiment lexicons, including Liu’s sentiment lexicon (Liu, 2012), MPQA subjectivity lexicon (Wilson et al., 2005), and the sentiment lexicon generated from tweets (Mohammad et al., 2013). 665 Feature Extraction The objective of this task is to determine whether a given message is positive, negative, or neutral. We train sentiment classifiers with LibLinear (Fan et al., 2008) on the training set and dev set, and tune parameter −c, −wi of SVM on the test set of SemEval 2013. SVM is a popular machine learning algorithm, the effectiveness of which has been proved in sentiment analysis on formal texts in related work (Pang and Lee, 2002; Liu, 2012). Since the performance of SVM classifier will be greatly infl"
S15-2111,D11-1015,1,0.775819,"rking for quality over quantity. Everyone... http://t.co/oCdPXQWggT. 666 In this case, there are two segments in the tweet that holds a Contrast discourse relation, and the polarity is determined by “but” segment. In our system, we also take into consideration of intrasentence discourse relation features for processing compound sentences. Mann and Thompson (1988) defined a complete discourse scheme Rhetorical Structure Theory (RST). Since not all of the discourse relations in RST would help eliminate polarity ambiguities, the discourse relations were implemented in our system was on a subset (Zhou et al., 2011). In our system, we use cue-phrase based method for discourse relation identification. We maintain a cue phrase lexicon and the examples of the cue phrases were shown in Table 2. 4 Experiment We trained a SVM classifier on 9,912 annotated tweets (8,258 in the training set and 1,654 in the development set). We used the same evaluation metrics with SemEval 2013, including the macroaveraged F-score of the positive and negative classes. The experimental results obtained by our system on the training set (ten-fold cross validation), development set, and test sets on Twitter 2013 were shown in Table"
W01-1305,P92-1030,0,0.0332149,"Missing"
W01-1305,M93-1023,0,\N,Missing
W06-2808,W03-1730,0,0.0932568,"Missing"
W07-1511,W03-2405,0,0.024716,"lysis. However, the performances of automatic collocation extraction systems are not satisfactory (Pecina 2005). A problem is that collocations are word combinations that co-occur within a short context, but not all such co-occurrences are true collocations. Further examinations is needed to filter out pseudo-collocations once co-occurred word pairs are identified. A collocation bank with true collocations annotated is naturally an indispensable resource for collocation research. (Kosho et al. 2000) presented their works of collocation annotation on Japanese text. Also, the Turkish treebank, (Bedin 2003) included collocation annotation as one step in its annotation. These two collocation banks provided collocation identification and co-occurrence verification information. (Tutin 2005) used shallow analysis based on finite state transducers and lexicon-grammar to identify and annotate collocations in a French corpus. This collocation bank further provided the lexical functions of the collocations. However to this day, there is no reported Chinese collocation bank available. 61 Proceedings of the Linguistic Annotation Workshop, pages 61–68, c Prague, June 2007. 2007 Association for Computationa"
W07-1511,shudo-etal-2000-collocations,0,0.0790794,"Missing"
W07-1511,J93-1007,0,\N,Missing
W07-1511,P05-2003,0,\N,Missing
W17-5905,W15-3106,0,0.503626,"/or similar pronunciations. 2. There are no delimiters between words. 29 Proceedings of the 4th Workshop on Natural Language Processing Techniques for Educational Applications, pages 29–34, c Taipei, Taiwan, December 1, 2017 2017 AFNLP Chinese and should be replaced by its traditional counterpart 詞. For the evaluation, it should be noted that we do not have any widely recognised or standard evaluation schemas specifically designed for evaluating the performance of Chinese spelling checkers. Nonetheless, different evaluation schemas have been proposed for different purposes (Duan et al., 2012; Tseng et al., 2015; Wu et al., 2013; Yu et al., 2008, 2014; Zhao and Liu, 2010). Since we could not find any existing evaluation schema that fulfils all our evaluation criteria, we proposed a new evaluation schema in this task. We understand that this proposed evaluation schema may not be perfect, however it does capture most essential elements for considering whether a spelling checker is effective. The rest of this paper is organised as follows: Section 2 describes the benchmark dataset, Section 3 presents the tasks, Section 4 outlines the evaluation schema, and Section 5 reports the findings and concludes th"
W17-5905,W13-4406,0,0.0210571,"Missing"
W17-5905,I08-4016,0,0.208055,"are no delimiters between words. 29 Proceedings of the 4th Workshop on Natural Language Processing Techniques for Educational Applications, pages 29–34, c Taipei, Taiwan, December 1, 2017 2017 AFNLP Chinese and should be replaced by its traditional counterpart 詞. For the evaluation, it should be noted that we do not have any widely recognised or standard evaluation schemas specifically designed for evaluating the performance of Chinese spelling checkers. Nonetheless, different evaluation schemas have been proposed for different purposes (Duan et al., 2012; Tseng et al., 2015; Wu et al., 2013; Yu et al., 2008, 2014; Zhao and Liu, 2010). Since we could not find any existing evaluation schema that fulfils all our evaluation criteria, we proposed a new evaluation schema in this task. We understand that this proposed evaluation schema may not be perfect, however it does capture most essential elements for considering whether a spelling checker is effective. The rest of this paper is organised as follows: Section 2 describes the benchmark dataset, Section 3 presents the tasks, Section 4 outlines the evaluation schema, and Section 5 reports the findings and concludes this paper. 2 4. Colloquialism – Inc"
W17-5905,W14-6820,0,\N,Missing
xia-etal-2006-constructing,brants-plaehn-2000-interactive,0,\N,Missing
xia-etal-2006-constructing,W03-1730,0,\N,Missing
xu-etal-2008-opinion,W03-1017,0,\N,Missing
xu-etal-2008-opinion,W03-0404,0,\N,Missing
xu-etal-2008-opinion,P03-1027,0,\N,Missing
xu-etal-2008-opinion,J04-3002,0,\N,Missing
xu-etal-2008-opinion,W02-1011,0,\N,Missing
xu-etal-2008-opinion,P97-1023,0,\N,Missing
zhou-etal-2014-cuhk,miltsakaki-etal-2004-penn,0,\N,Missing
zhou-etal-2014-cuhk,D07-1075,0,\N,Missing
zhou-etal-2014-cuhk,W11-0401,0,\N,Missing
zhou-etal-2014-cuhk,J93-2004,0,\N,Missing
zhou-etal-2014-cuhk,D11-1015,1,\N,Missing
zhou-etal-2014-cuhk,D09-1018,0,\N,Missing
zhou-etal-2014-cuhk,mladova-etal-2008-sentence,0,\N,Missing
zhou-etal-2014-cuhk,W05-0312,0,\N,Missing
zhou-etal-2014-cuhk,H05-1033,0,\N,Missing
zhou-etal-2014-cuhk,al-saif-markert-2010-leeds,0,\N,Missing
zhou-etal-2014-cuhk,W01-1605,0,\N,Missing
zhou-etal-2014-cuhk,W09-3006,0,\N,Missing
zhou-etal-2014-cuhk,N03-1030,0,\N,Missing
zhou-etal-2014-cuhk,P09-1075,0,\N,Missing
zhou-etal-2014-cuhk,prasad-etal-2008-penn,0,\N,Missing
zhou-etal-2014-cuhk,I08-7009,0,\N,Missing
zhou-etal-2014-cuhk,P12-1008,0,\N,Missing
zhou-etal-2014-cuhk,I11-1170,0,\N,Missing
zhou-etal-2014-cuhk,I08-7010,0,\N,Missing
zhou-etal-2014-cuhk,W10-1844,0,\N,Missing
zhou-etal-2014-cuhk,C12-2138,1,\N,Missing
zhou-etal-2014-cuhk,D12-1084,0,\N,Missing
