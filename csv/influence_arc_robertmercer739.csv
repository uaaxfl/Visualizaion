1988.tmi-1.19,C86-1033,0,\N,Missing
1992.tmi-1.8,J90-2002,1,0.565638,"Missing"
1992.tmi-1.8,P91-1022,1,0.846501,"Missing"
1992.tmi-1.8,P91-1034,1,0.862146,"Missing"
2020.argmining-1.10,D17-1218,0,0.0188335,"; Liakata et al., 2012; S´andor and de Waard, 2012; Longo et al., 2012; Longo and Hederman, 2013; Graves et al., 2014; Green, 2014a; Green, 2015; Kirschner et al., 2015; Mayer et al., 2018; Mayer et al., 2020). Lippi and Torroni (2015) provide an excellent survey of research done until 2015 which is further updated to 2018 by Stede and Schneider (2018). Corpus creation and analysis has also been another aspect of argumentation mining studies (Stab and Gurevych, 2017). More recently, neural net machine learning has provided a new machine learning paradigm for doing cross-domain claim analysis (Daxenberger et al., 2017). Eger et al. (2017) and Mayer et al. (2020) provide neural end-to-end models for computational argumentation mining. They label student essays and randomized control trials, respectively, with a BIO encoding to indicate argumentative and non-argumentative text spans, component type, and the stance between the components. The rich history of work in argumentation mining has tended to focus on non-scientific text, however work in scientific text argumentation mining does have a following. The research done on various aspects of argumentation in scientific text begins with Argumentation Zoning ("
2020.argmining-1.10,P17-1002,0,0.202777,"mises for other claims, ending finally with the main claim of the paper. This research follows a similar path to Lawrence and Reed (2017), who manually constructed large scale graphs, and adds to the argumentation structure research by examining complete biochemistry articles and linking the argument schemes. Two models of analysis will demonstrate the interactions of claims. The first model is a highlevel argument diagram of the data and claims structure capturing the essence of Green (2014b) and in the spirit of other high-level diagramming models (Stab et al., 2014; Kirschner et al., 2015; Eger et al., 2017; Stab and Gurevych, 2017) and diagramming-assisting software (Janier et al., 2014). The network of data and claims allows one to investigate its properties. In particular, the data is found in figures, tables, and other comments by the paper’s authors. This data initiates the argumentation flow which ends in the main claim of the paper. This investigation leads to the second model, the Model of Informational Hierarchy (MIH). It makes more precise the units of the argument structure and their position in that structure. This hierarchical description of data and claims differs from other argume"
2020.argmining-1.10,P11-1099,0,0.235662,"g the larger scale argumentation structure has a similar motivation to the works of Wachsmuth et al. (2017) and Lawrence and Reed (2017), who are interested in investigating various properties of large scale argument networks. This new dimension adds to the previous works they point to as examples that consider particular aspects of argument structure: distinguishing argumentative and non-argumentative sentences (Moens et al., 2007), classifying text spans as premises or conclusions (Mochales Palau and Moens, 2009), classifying relations between specific sets of premises and their conclusion (Feng and Hirst, 2011), or classifying the different types of premise that can support a given conclusion (Park and Cardie, 2014). In addition to these examples, various manual and computational studies have been done to analyze different argumentation aspects, including: the structure of valid arguments in legal documents using feature-based machine learning (Mochales Palau and Moens, 2011), opinion and didatic texts using Rhetorical Structure Theory (Saint-Dizier, 2012), debates using textual entailment (Cabrio and Villata, 2012), and deconstructing the argumentation into the premises (referred 89 to as evidence"
2020.argmining-1.10,W14-2113,1,0.891116,"hales Palau and Moens, 2011), opinion and didatic texts using Rhetorical Structure Theory (Saint-Dizier, 2012), debates using textual entailment (Cabrio and Villata, 2012), and deconstructing the argumentation into the premises (referred 89 to as evidence in the evidence-based medical text genre) and conclusions (also referred to as claims) in scientific articles manually, with feature-based machine learning, or with neural end-to-end machine learning (Blake, 2010; Teufel, 2010; Green et al., 2011; Liakata et al., 2012; S´andor and de Waard, 2012; Longo et al., 2012; Longo and Hederman, 2013; Graves et al., 2014; Green, 2014a; Green, 2015; Kirschner et al., 2015; Mayer et al., 2018; Mayer et al., 2020). Lippi and Torroni (2015) provide an excellent survey of research done until 2015 which is further updated to 2018 by Stede and Schneider (2018). Corpus creation and analysis has also been another aspect of argumentation mining studies (Stab and Gurevych, 2017). More recently, neural net machine learning has provided a new machine learning paradigm for doing cross-domain claim analysis (Daxenberger et al., 2017). Eger et al. (2017) and Mayer et al. (2020) provide neural end-to-end models for computatio"
2020.argmining-1.10,W14-2102,0,0.115501,"ents. Such models would allow the mechanization of argumentation analysis which could enable scientific claim validation, a task made difficult because of the huge number of claims being made. A claim is any statement made within a paper which presents a novel finding based on the conducted experiment (Leonelli, 2015). A claim requires evidence to verify it. The structure of individual claims and their evidence is well illustrated by the Toulmin model (Toulmin, 2003), and the logic underlying these relationships can be categorized with argumentation schemes and premise classes (Karbach, 1987; Green, 2014a; Al Qassas et al., 2015; Green, 2015; Mayer et al., 2018). The Toulmin model of argumentation is adept at illustrating the components of an individual argument. When composing argumentation text, every claim that is made must be supported by evidence and a warrant connecting the claim and evidence (Karbach, 1987). In the experimental sciences, the warrant is very often implicit, given that the intended audience can easily fill that slot in the argument structure. These arguments with implicit premises (and sometimes conclusions) are called enthymemes. Although each individual argument can be"
2020.argmining-1.10,W15-0502,0,0.283775,"anization of argumentation analysis which could enable scientific claim validation, a task made difficult because of the huge number of claims being made. A claim is any statement made within a paper which presents a novel finding based on the conducted experiment (Leonelli, 2015). A claim requires evidence to verify it. The structure of individual claims and their evidence is well illustrated by the Toulmin model (Toulmin, 2003), and the logic underlying these relationships can be categorized with argumentation schemes and premise classes (Karbach, 1987; Green, 2014a; Al Qassas et al., 2015; Green, 2015; Mayer et al., 2018). The Toulmin model of argumentation is adept at illustrating the components of an individual argument. When composing argumentation text, every claim that is made must be supported by evidence and a warrant connecting the claim and evidence (Karbach, 1987). In the experimental sciences, the warrant is very often implicit, given that the intended audience can easily fill that slot in the argument structure. These arguments with implicit premises (and sometimes conclusions) are called enthymemes. Although each individual argument can be modelled in this way, it fails to rec"
2020.argmining-1.10,W15-0501,0,0.321189,"of which are used as premises for other claims, ending finally with the main claim of the paper. This research follows a similar path to Lawrence and Reed (2017), who manually constructed large scale graphs, and adds to the argumentation structure research by examining complete biochemistry articles and linking the argument schemes. Two models of analysis will demonstrate the interactions of claims. The first model is a highlevel argument diagram of the data and claims structure capturing the essence of Green (2014b) and in the spirit of other high-level diagramming models (Stab et al., 2014; Kirschner et al., 2015; Eger et al., 2017; Stab and Gurevych, 2017) and diagramming-assisting software (Janier et al., 2014). The network of data and claims allows one to investigate its properties. In particular, the data is found in figures, tables, and other comments by the paper’s authors. This data initiates the argumentation flow which ends in the main claim of the paper. This investigation leads to the second model, the Model of Informational Hierarchy (MIH). It makes more precise the units of the argument structure and their position in that structure. This hierarchical description of data and claims differ"
2020.argmining-1.10,W17-5114,0,0.168608,"is facilitated by argumentation schemes and the Toulmin model structure. The next step to understanding the argumentation set forth in a scientific paper is analyzing the complete argumentation structure of the full paper. We present here our preliminary work on the analysis of this larger scale argumentation structure of biochemistry papers. This study is done by examining the flow of the paper from its data used as premises to support certain claims, some of which are used as premises for other claims, ending finally with the main claim of the paper. This research follows a similar path to Lawrence and Reed (2017), who manually constructed large scale graphs, and adds to the argumentation structure research by examining complete biochemistry articles and linking the argument schemes. Two models of analysis will demonstrate the interactions of claims. The first model is a highlevel argument diagram of the data and claims structure capturing the essence of Green (2014b) and in the spirit of other high-level diagramming models (Stab et al., 2014; Kirschner et al., 2015; Eger et al., 2017; Stab and Gurevych, 2017) and diagramming-assisting software (Janier et al., 2014). The network of data and claims allo"
2020.argmining-1.10,W18-5204,0,0.386129,"argumentation analysis which could enable scientific claim validation, a task made difficult because of the huge number of claims being made. A claim is any statement made within a paper which presents a novel finding based on the conducted experiment (Leonelli, 2015). A claim requires evidence to verify it. The structure of individual claims and their evidence is well illustrated by the Toulmin model (Toulmin, 2003), and the logic underlying these relationships can be categorized with argumentation schemes and premise classes (Karbach, 1987; Green, 2014a; Al Qassas et al., 2015; Green, 2015; Mayer et al., 2018). The Toulmin model of argumentation is adept at illustrating the components of an individual argument. When composing argumentation text, every claim that is made must be supported by evidence and a warrant connecting the claim and evidence (Karbach, 1987). In the experimental sciences, the warrant is very often implicit, given that the intended audience can easily fill that slot in the argument structure. These arguments with implicit premises (and sometimes conclusions) are called enthymemes. Although each individual argument can be modelled in this way, it fails to recognize the variations"
2020.argmining-1.10,W14-2105,0,0.0215106,") and Lawrence and Reed (2017), who are interested in investigating various properties of large scale argument networks. This new dimension adds to the previous works they point to as examples that consider particular aspects of argument structure: distinguishing argumentative and non-argumentative sentences (Moens et al., 2007), classifying text spans as premises or conclusions (Mochales Palau and Moens, 2009), classifying relations between specific sets of premises and their conclusion (Feng and Hirst, 2011), or classifying the different types of premise that can support a given conclusion (Park and Cardie, 2014). In addition to these examples, various manual and computational studies have been done to analyze different argumentation aspects, including: the structure of valid arguments in legal documents using feature-based machine learning (Mochales Palau and Moens, 2011), opinion and didatic texts using Rhetorical Structure Theory (Saint-Dizier, 2012), debates using textual entailment (Cabrio and Villata, 2012), and deconstructing the argumentation into the premises (referred 89 to as evidence in the evidence-based medical text genre) and conclusions (also referred to as claims) in scientific articl"
2020.argmining-1.10,W12-4302,0,0.0651886,"Missing"
2020.argmining-1.10,J17-3005,0,0.287898,"ims, ending finally with the main claim of the paper. This research follows a similar path to Lawrence and Reed (2017), who manually constructed large scale graphs, and adds to the argumentation structure research by examining complete biochemistry articles and linking the argument schemes. Two models of analysis will demonstrate the interactions of claims. The first model is a highlevel argument diagram of the data and claims structure capturing the essence of Green (2014b) and in the spirit of other high-level diagramming models (Stab et al., 2014; Kirschner et al., 2015; Eger et al., 2017; Stab and Gurevych, 2017) and diagramming-assisting software (Janier et al., 2014). The network of data and claims allows one to investigate its properties. In particular, the data is found in figures, tables, and other comments by the paper’s authors. This data initiates the argumentation flow which ends in the main claim of the paper. This investigation leads to the second model, the Model of Informational Hierarchy (MIH). It makes more precise the units of the argument structure and their position in that structure. This hierarchical description of data and claims differs from other argumentation structures such as"
2020.argmining-1.10,J02-4002,0,0.453065,"ayer et al. (2020) provide neural end-to-end models for computational argumentation mining. They label student essays and randomized control trials, respectively, with a BIO encoding to indicate argumentative and non-argumentative text spans, component type, and the stance between the components. The rich history of work in argumentation mining has tended to focus on non-scientific text, however work in scientific text argumentation mining does have a following. The research done on various aspects of argumentation in scientific text begins with Argumentation Zoning (AZ) (Teufel et al., 1999; Teufel and Moens, 2002), also being some of the earliest work in argumentation mining. AZ is based on rhetorical moves, an important precursor for mapping out certain aspects of argument structure. Rhetorical moves, captured as AZ or more generally, have been investigated in a few science genres: computational linguistics (Teufel et al., 1999; Teufel and Moens, 2002), biochemistry (Kanoksilapatham, 2005), molecular biology (Mizuta et al., 2006), and chemistry (Teufel, 2010). Argumentation schemes (Walton et al., 2008) have been an important aspect of argumentation and argumentation mining. Green (2014a; 2015) has pr"
2020.argmining-1.10,E99-1015,0,0.446554,"r et al. (2017) and Mayer et al. (2020) provide neural end-to-end models for computational argumentation mining. They label student essays and randomized control trials, respectively, with a BIO encoding to indicate argumentative and non-argumentative text spans, component type, and the stance between the components. The rich history of work in argumentation mining has tended to focus on non-scientific text, however work in scientific text argumentation mining does have a following. The research done on various aspects of argumentation in scientific text begins with Argumentation Zoning (AZ) (Teufel et al., 1999; Teufel and Moens, 2002), also being some of the earliest work in argumentation mining. AZ is based on rhetorical moves, an important precursor for mapping out certain aspects of argument structure. Rhetorical moves, captured as AZ or more generally, have been investigated in a few science genres: computational linguistics (Teufel et al., 1999; Teufel and Moens, 2002), biochemistry (Kanoksilapatham, 2005), molecular biology (Mizuta et al., 2006), and chemistry (Teufel, 2010). Argumentation schemes (Walton et al., 2008) have been an important aspect of argumentation and argumentation mining. G"
2020.argmining-1.10,E17-1105,0,0.110516,"he MIH can be viewed as a precursor to biomedical literature summarization. The paper is organized as follows: Some related work to provide context for the current work is presented next. This is followed by a short description of the the five biochemistry papers that were used in the study. Then, the two main contributions, the claim graph and the Model of Informational Hierarchy, are explained. We conclude with a summary and some proposed future research directions. 2 Related Work Our interest in investigating the larger scale argumentation structure has a similar motivation to the works of Wachsmuth et al. (2017) and Lawrence and Reed (2017), who are interested in investigating various properties of large scale argument networks. This new dimension adds to the previous works they point to as examples that consider particular aspects of argument structure: distinguishing argumentative and non-argumentative sentences (Moens et al., 2007), classifying text spans as premises or conclusions (Mochales Palau and Moens, 2009), classifying relations between specific sets of premises and their conclusion (Feng and Hirst, 2011), or classifying the different types of premise that can support a given conclusion (P"
2020.lrec-1.380,W10-3001,0,0.0790675,"Missing"
2020.lrec-1.380,P09-2044,0,0.0306783,"ight et al. (2004) constructed a dictionary of hedge cues to identify speculative (hedged) sentences in MEDLINE abstracts. They also used a Support Vector Machine (SVM) as a classifier to determine speculative sentences in the abstracts. Medlock and Briscoe (2007) treated the problem of determining speculative sentences as a classification task. Their training samples were collected from biomedical articles. They used single words as feature for their model. Szarvas (2008) used the same dataset, but they used bigrams and trigrams instead as features for their maximum entropy model classifier. Ganter and Strube (2009) proposed a hedge detection system based on word frequency measures and syntactic patterns of the weasel words in ¨ ur (2009)’s supervised learning Wikipedia articles. In Ozg¨ approach, they used various features such as keywords, positional information of the keywords, and the contextual information of the keywords. They also used syntactic structures of the sentence to determine the scope of the hedge cues. Agarwal and Yu (2010) used a conditional random field (CRF) algorithm to train models in order to identify hedge cue phrases in biological literature. They performed experiments on the Bi"
2020.lrec-1.380,W04-3103,0,0.119451,"first step towards a computational tool that automatically identifies hedging in unstructured and informal communications. Specifically, we constructed three lists1 of hedge words, booster words, and hedging phrases, and developed a rule-based algorithm that detects sentence-level hedges in these informal conversations with these lexicons. In the rest of the paper, we first review the related studies about hedging detection. We then present our method as well as the experiment that compared the performance of our approach against the annotations provided by three researchers. 2. Related Work Light et al. (2004) constructed a dictionary of hedge cues to identify speculative (hedged) sentences in MEDLINE abstracts. They also used a Support Vector Machine (SVM) as a classifier to determine speculative sentences in the abstracts. Medlock and Briscoe (2007) treated the problem of determining speculative sentences as a classification task. Their training samples were collected from biomedical articles. They used single words as feature for their model. Szarvas (2008) used the same dataset, but they used bigrams and trigrams instead as features for their maximum entropy model classifier. Ganter and Strube"
2020.lrec-1.380,P14-5010,0,0.00600521,"d by a negation word “not”, it changes the meaning completely. We handle this kind of situation in our proposed algorithm by compiling and including a list of booster words in the algorithm. Rules for Disambiguation. Hedging disambiguation is an important part of our algorithm, as some commonly used hedge terms in the conversational interviews have nonhedge senses as well. We apply rules to disambiguate these terms based on the syntactic structure of the sentences. Our rules are an extension and modification of the set of rules proposed by (Ulinski et al., 2018). We used the Stanford CoreNLP (Manning et al., 2014) parser to parse the sentences2 . What follows is a brief analysis of some of the rules used in our study with examples derived from our interview datasets. Hedge Term: Feel, Suggest, Believe, Consider, Doubt, Guess, Hope Rule: If token t is (i) a root word, (ii) has the part-of-speech VB* and (iii) has an nsubj (nominal subject) dependency with the dependent token being a first person pronoun (i, we), t is a hedge, otherwise, it is a non-hedge. Hedge: I don’t think it’s been a failure, but I hope that I’m on the right track. Non-hedge: I’m still living with it, but without hope that I would f"
2020.lrec-1.380,P07-1125,0,0.0379758,"based algorithm that detects sentence-level hedges in these informal conversations with these lexicons. In the rest of the paper, we first review the related studies about hedging detection. We then present our method as well as the experiment that compared the performance of our approach against the annotations provided by three researchers. 2. Related Work Light et al. (2004) constructed a dictionary of hedge cues to identify speculative (hedged) sentences in MEDLINE abstracts. They also used a Support Vector Machine (SVM) as a classifier to determine speculative sentences in the abstracts. Medlock and Briscoe (2007) treated the problem of determining speculative sentences as a classification task. Their training samples were collected from biomedical articles. They used single words as feature for their model. Szarvas (2008) used the same dataset, but they used bigrams and trigrams instead as features for their maximum entropy model classifier. Ganter and Strube (2009) proposed a hedge detection system based on word frequency measures and syntactic patterns of the weasel words in ¨ ur (2009)’s supervised learning Wikipedia articles. In Ozg¨ approach, they used various features such as keywords, positiona"
2020.lrec-1.380,D09-1145,0,0.0761153,"Missing"
2020.lrec-1.380,W08-0606,0,0.061416,"ge detection system based on word frequency measures and syntactic patterns of the weasel words in ¨ ur (2009)’s supervised learning Wikipedia articles. In Ozg¨ approach, they used various features such as keywords, positional information of the keywords, and the contextual information of the keywords. They also used syntactic structures of the sentence to determine the scope of the hedge cues. Agarwal and Yu (2010) used a conditional random field (CRF) algorithm to train models in order to identify hedge cue phrases in biological literature. They performed experiments on the BioScope corpus (Szarvas et al., 2008) and showed the efficacy of their model in the biological domain. The problem of detecting hedges was addressed in the CoNLL 2010 shared task (Farkas et al., 2010). However, the datasets that have been used contain only formal texts though. More recently, Ulinski et al. (2018) proposed a set of manually constructed rules which allowed them to identify hedged sentences in forum posts in an unsupervised manner. Theil et al. (2018) expanded a lexicon of uncertainty trigger words utilizing domain specific wordembedding models and used TF-IDF (Term Frequency Inverse Document Frequency) for represen"
2020.lrec-1.380,P08-1033,0,0.0400489,"as the experiment that compared the performance of our approach against the annotations provided by three researchers. 2. Related Work Light et al. (2004) constructed a dictionary of hedge cues to identify speculative (hedged) sentences in MEDLINE abstracts. They also used a Support Vector Machine (SVM) as a classifier to determine speculative sentences in the abstracts. Medlock and Briscoe (2007) treated the problem of determining speculative sentences as a classification task. Their training samples were collected from biomedical articles. They used single words as feature for their model. Szarvas (2008) used the same dataset, but they used bigrams and trigrams instead as features for their maximum entropy model classifier. Ganter and Strube (2009) proposed a hedge detection system based on word frequency measures and syntactic patterns of the weasel words in ¨ ur (2009)’s supervised learning Wikipedia articles. In Ozg¨ approach, they used various features such as keywords, positional information of the keywords, and the contextual information of the keywords. They also used syntactic structures of the sentence to determine the scope of the hedge cues. Agarwal and Yu (2010) used a conditional"
2020.lrec-1.380,W18-3104,0,0.0223179,"onal random field (CRF) algorithm to train models in order to identify hedge cue phrases in biological literature. They performed experiments on the BioScope corpus (Szarvas et al., 2008) and showed the efficacy of their model in the biological domain. The problem of detecting hedges was addressed in the CoNLL 2010 shared task (Farkas et al., 2010). However, the datasets that have been used contain only formal texts though. More recently, Ulinski et al. (2018) proposed a set of manually constructed rules which allowed them to identify hedged sentences in forum posts in an unsupervised manner. Theil et al. (2018) expanded a lexicon of uncertainty trigger words utilizing domain specific wordembedding models and used TF-IDF (Term Frequency Inverse Document Frequency) for representing features. Their extended lexicon improved the performance of uncertainty detection significantly in financial domain when used with machine learning models. Ponterotto (2018) discussed different hedging strategies that have been employed by 1 https://github.com/hedging-lrec/resources Barack Obama, the former president of the United States, in political interviews. Through defining hedging-related discursive approaches, they"
2020.lrec-1.380,W18-1301,0,0.0662495,"formation of the keywords. They also used syntactic structures of the sentence to determine the scope of the hedge cues. Agarwal and Yu (2010) used a conditional random field (CRF) algorithm to train models in order to identify hedge cue phrases in biological literature. They performed experiments on the BioScope corpus (Szarvas et al., 2008) and showed the efficacy of their model in the biological domain. The problem of detecting hedges was addressed in the CoNLL 2010 shared task (Farkas et al., 2010). However, the datasets that have been used contain only formal texts though. More recently, Ulinski et al. (2018) proposed a set of manually constructed rules which allowed them to identify hedged sentences in forum posts in an unsupervised manner. Theil et al. (2018) expanded a lexicon of uncertainty trigger words utilizing domain specific wordembedding models and used TF-IDF (Term Frequency Inverse Document Frequency) for representing features. Their extended lexicon improved the performance of uncertainty detection significantly in financial domain when used with machine learning models. Ponterotto (2018) discussed different hedging strategies that have been employed by 1 https://github.com/hedging-lr"
2020.lrec-1.516,J06-1003,0,0.0152209,"rpus creation approach can be applied to any other industry vertical provided that a bilingual website exists. We also show experimental results for multilingual semantic similarity to verify the quality of the corpus and demonstrate its usage. Keywords: Multilingual Corpus, Multilingual Semantic Similarity, Web Crawling 1. Introduction Semantic similarity, one of the important natural language processing (NLP) tasks, aims to measure the distance between two given content pieces in terms of their meaning. Traditionally, WordNet-based similarity measures such as Lin, Resnik, Jiang and Conrath (Budanitsky and Hirst, 2006) as well as statistical approaches including Latent Semantic Analysis (LSA) (Landauer et al., 2013) and Pointwise Mutual Information (PMI) (Zhao et al., 2014) have been used to solve this problem. Recently with the advent of deep learning, the use of deep neural networks has gained popularity in solving this task; for example Siamese recurrent networks (Mueller and Thyagarajan, 2016) and convolutional neural networks (Shao, 2017). However, a major factor affecting the success of deep networks is the availability of substantially large and good quality corpora (Kiros et al., 2015; Devlin et al."
2020.lrec-1.516,S17-2001,0,0.0261304,"been used to solve this problem. Recently with the advent of deep learning, the use of deep neural networks has gained popularity in solving this task; for example Siamese recurrent networks (Mueller and Thyagarajan, 2016) and convolutional neural networks (Shao, 2017). However, a major factor affecting the success of deep networks is the availability of substantially large and good quality corpora (Kiros et al., 2015; Devlin et al., 2019). The most popular benchmark dataset for semantic similarity is the Semantic Textual Similarity (STS) dataset from SemEval tasks. The latest STS17 dataset (Cer et al., 2017) includes monolingual as well as cross-lingual sentence pairs for English, Arabic and Spanish languages. Nonetheless, the STS corpus requires a classification score ranging from 0 to 5 measuring the degree of similarity between the sentence pairs. We approach multilingual semantic similarity as a binary classification problem which has required us to collect a large corpus of our own based on the domain and language requirements of our application. The collection of an entirely new and large corpus in itself is a challenging task; more specifically, textual data for NLP problems require human"
2020.lrec-1.516,D17-1070,0,0.0858215,"this section, we present a thorough analysis of all the evaluation experiments that we did to validate our corpus. We first describe the model architecture which we used to solve the multilingual semantic similarity task. Following this, we explain the training details of the model along with its hyper-parameter settings. We also present the detailed results obtained with our experiments and compare the transfer performance of our selected model with some of the top performing models on the MSRP dataset. 4.1. Model Architecture, Parameters and Training Details We have chosen to use InferSent (Conneau et al., 2017a), an LSTM based model, to compute the representations of a pair of sentences, a and b, and then compare the representations for an underlying task. The model first traverses each sentence as a sequence of T words {xt }t=1,...,T from both left to right and right to left and generates two hidden → − ← − representations at each time step ht , ht ∀t ∈ [1, . . . , T ]. During input, it considers the vector representation of each word (xt ) in the sentence from a pre-trained word embed4193 Model Validation set Accuracy Mean Max voting Avg. voting 95.41 ± 0.39 95.76 95.97 96.35 ± 0.57 97.07 97.22 9"
2020.lrec-1.516,N19-1423,0,0.0162193,"Hirst, 2006) as well as statistical approaches including Latent Semantic Analysis (LSA) (Landauer et al., 2013) and Pointwise Mutual Information (PMI) (Zhao et al., 2014) have been used to solve this problem. Recently with the advent of deep learning, the use of deep neural networks has gained popularity in solving this task; for example Siamese recurrent networks (Mueller and Thyagarajan, 2016) and convolutional neural networks (Shao, 2017). However, a major factor affecting the success of deep networks is the availability of substantially large and good quality corpora (Kiros et al., 2015; Devlin et al., 2019). The most popular benchmark dataset for semantic similarity is the Semantic Textual Similarity (STS) dataset from SemEval tasks. The latest STS17 dataset (Cer et al., 2017) includes monolingual as well as cross-lingual sentence pairs for English, Arabic and Spanish languages. Nonetheless, the STS corpus requires a classification score ranging from 0 to 5 measuring the degree of similarity between the sentence pairs. We approach multilingual semantic similarity as a binary classification problem which has required us to collect a large corpus of our own based on the domain and language require"
2020.lrec-1.516,C04-1051,0,0.236746,"s using these – EN-FR-IB and EN-ES-IB. For each of the three datasets, we used the positive and negative sample pairs, described earlier, to create a balanced 10-fold training and validation partition for doing 10-fold cross validation experiments. We also created a test set for testing. The dataset partition details are given in Table 3. In order to verify the quality of our corpus we have evaluated our model on a benchmark dataset that has been supplemented with our corpus. We chose the well known Microsoft Research Paraphrase Corpus (MSRP) where the task is to do paraphrase identification (Dolan et al., 2004). Because of the way we prepared our corpus, it aligns well with this kind of task. The original MSRP dataset has 5, 801 sentence pairs, 4, 076 in the training set and 1, 725 in the test set. Adding our corpus to the MSRP training set shows an increase in performance on the MSRP test set. We hypothesize that this indicates that our corpus is of good quality. 4. Evaluation Experiments In this section, we present a thorough analysis of all the evaluation experiments that we did to validate our corpus. We first describe the model architecture which we used to solve the multilingual semantic simil"
2020.lrec-1.516,D13-1167,0,0.0151158,"xists. We also show experimental results for multilingual semantic similarity to verify the quality of the corpus and demonstrate its usage. Keywords: Multilingual Corpus, Multilingual Semantic Similarity, Web Crawling 1. Introduction Semantic similarity, one of the important natural language processing (NLP) tasks, aims to measure the distance between two given content pieces in terms of their meaning. Traditionally, WordNet-based similarity measures such as Lin, Resnik, Jiang and Conrath (Budanitsky and Hirst, 2006) as well as statistical approaches including Latent Semantic Analysis (LSA) (Landauer et al., 2013) and Pointwise Mutual Information (PMI) (Zhao et al., 2014) have been used to solve this problem. Recently with the advent of deep learning, the use of deep neural networks has gained popularity in solving this task; for example Siamese recurrent networks (Mueller and Thyagarajan, 2016) and convolutional neural networks (Shao, 2017). However, a major factor affecting the success of deep networks is the availability of substantially large and good quality corpora (Kiros et al., 2015; Devlin et al., 2019). The most popular benchmark dataset for semantic similarity is the Semantic Textual Similar"
2020.lrec-1.516,L18-1599,0,0.0126512,"anguage requirements of our application. The collection of an entirely new and large corpus in itself is a challenging task; more specifically, textual data for NLP problems require human expertise and domain knowledge of the application. Above all, the acquisition of a multilingual corpus also demands some amount of linguistic knowledge. This leads to an increasing interest in developing an automated or semi-automated approach for building a multilingual corpus. Several corpus creation approaches have been published focusing on multiple languages and application domains. Papavassiliou et al. (2018) proposed a web crawler to acquire parallel language resources for European languages. Soares et al. (2018) developed a parallel corpus of scientific articles in English, Portuguese and Spanish languages by first acquiring documents from the Scielo database (Packer, 2009) and then aligning sentences from document pairs of different languages. Few other approaches exist, but in all of these works the generated corpus is meant to be utilized for machine translation. Apart from this, existing techniques focus on curating similar sentence pairs, but rarely talk about dissimilar sentence pair gener"
2020.lrec-1.516,S17-2016,0,0.0205517,"e between two given content pieces in terms of their meaning. Traditionally, WordNet-based similarity measures such as Lin, Resnik, Jiang and Conrath (Budanitsky and Hirst, 2006) as well as statistical approaches including Latent Semantic Analysis (LSA) (Landauer et al., 2013) and Pointwise Mutual Information (PMI) (Zhao et al., 2014) have been used to solve this problem. Recently with the advent of deep learning, the use of deep neural networks has gained popularity in solving this task; for example Siamese recurrent networks (Mueller and Thyagarajan, 2016) and convolutional neural networks (Shao, 2017). However, a major factor affecting the success of deep networks is the availability of substantially large and good quality corpora (Kiros et al., 2015; Devlin et al., 2019). The most popular benchmark dataset for semantic similarity is the Semantic Textual Similarity (STS) dataset from SemEval tasks. The latest STS17 dataset (Cer et al., 2017) includes monolingual as well as cross-lingual sentence pairs for English, Arabic and Spanish languages. Nonetheless, the STS corpus requires a classification score ranging from 0 to 5 measuring the degree of similarity between the sentence pairs. We ap"
2020.lrec-1.516,L18-1546,0,0.0276081,"n itself is a challenging task; more specifically, textual data for NLP problems require human expertise and domain knowledge of the application. Above all, the acquisition of a multilingual corpus also demands some amount of linguistic knowledge. This leads to an increasing interest in developing an automated or semi-automated approach for building a multilingual corpus. Several corpus creation approaches have been published focusing on multiple languages and application domains. Papavassiliou et al. (2018) proposed a web crawler to acquire parallel language resources for European languages. Soares et al. (2018) developed a parallel corpus of scientific articles in English, Portuguese and Spanish languages by first acquiring documents from the Scielo database (Packer, 2009) and then aligning sentences from document pairs of different languages. Few other approaches exist, but in all of these works the generated corpus is meant to be utilized for machine translation. Apart from this, existing techniques focus on curating similar sentence pairs, but rarely talk about dissimilar sentence pair generation. Bilingual sentence alignment lies at the heart of collecting similar pairs for a multilingual corpus"
2020.lrec-1.516,P15-1150,0,0.09747,"Missing"
2020.lrec-1.516,S14-2044,0,0.0316014,"tic similarity to verify the quality of the corpus and demonstrate its usage. Keywords: Multilingual Corpus, Multilingual Semantic Similarity, Web Crawling 1. Introduction Semantic similarity, one of the important natural language processing (NLP) tasks, aims to measure the distance between two given content pieces in terms of their meaning. Traditionally, WordNet-based similarity measures such as Lin, Resnik, Jiang and Conrath (Budanitsky and Hirst, 2006) as well as statistical approaches including Latent Semantic Analysis (LSA) (Landauer et al., 2013) and Pointwise Mutual Information (PMI) (Zhao et al., 2014) have been used to solve this problem. Recently with the advent of deep learning, the use of deep neural networks has gained popularity in solving this task; for example Siamese recurrent networks (Mueller and Thyagarajan, 2016) and convolutional neural networks (Shao, 2017). However, a major factor affecting the success of deep networks is the availability of substantially large and good quality corpora (Kiros et al., 2015; Devlin et al., 2019). The most popular benchmark dataset for semantic similarity is the Semantic Textual Similarity (STS) dataset from SemEval tasks. The latest STS17 data"
C12-1074,C92-3150,0,0.178483,"d approach A term from a terminology is often a unit of meaning related to a specific field or domain of study. It can be a single word, such as clustering, or a compound expression made up of individual terms, such as Hidden Markov Model. Some terms in a terminology can follow comprehensive rules which can help with their generalization. With the linguistics-based approach, candidate terminology is filtered by linguistic features using morphological analysis, such as part of speech (POS). Complex terms are extracted using shallow parsing and dependency analysis between words in the sentence (Bourigault, 1992). (Dagan and Church, 1994) limited the candidate terminology to a string that represents the pattern of noun sequences. Good results can be achieved in small corpora using linguistic methods, but due to the shortage of patterns, recall can be low and it is difficult to generalize these techniques across fields and languages. 1213 2.2 Statistical approach and machine learning approach Statistical approaches are based on statistical information, such as the frequency of terms appearing in the corpus. As already noted in (Zhang and Wu, 2012), statistical features can include term document frequen"
C12-1074,J90-1003,0,0.120116,"2012), statistical features can include term document frequency and inverse document frequency TF*IDF (Maedche and Staab, 2000), KF*IDF (Xu et al., 2002), C-value/NC-value (Frantzi et al., 2000) and so on. Term extraction is based on the computation of the Unithood – i.e. a degree of strength or stability of syntagmatic combinations or collocations, and Termhood – i.e. the degree that a linguistic unit is related to a domain-specific concept (Kageura and Umino, 1996) of the terminology. Termhood calculation is often based on the frequency of the term in the paper or a given baseline corpus. (Church and Hanks, 1990) used Mutual Information (MI) to compute Unithood, (Dunning, 1993) used LogL , and (Patry and Langlais, 2005) computed left/right entropy to extract unithood candidates. When computing termhood, methods such as TF*IDF (Maedche and Staab, 2000), DR-D (Velardi et al., 2001), C-value/NC value (Frantzi et al., 2000)) and Domain Component Feature Set (DCFS) (Zhang and Sui, 2007) are employed, as reported in (Zhang and Wu, 2012). 2.3 Hybrid approach Linguistics-based and statistical approaches have their own advantages and disadvantages. They are often integrated to extract terminology. There are tw"
C12-1074,A94-1006,0,0.104087,"om a terminology is often a unit of meaning related to a specific field or domain of study. It can be a single word, such as clustering, or a compound expression made up of individual terms, such as Hidden Markov Model. Some terms in a terminology can follow comprehensive rules which can help with their generalization. With the linguistics-based approach, candidate terminology is filtered by linguistic features using morphological analysis, such as part of speech (POS). Complex terms are extracted using shallow parsing and dependency analysis between words in the sentence (Bourigault, 1992). (Dagan and Church, 1994) limited the candidate terminology to a string that represents the pattern of noun sequences. Good results can be achieved in small corpora using linguistic methods, but due to the shortage of patterns, recall can be low and it is difficult to generalize these techniques across fields and languages. 1213 2.2 Statistical approach and machine learning approach Statistical approaches are based on statistical information, such as the frequency of terms appearing in the corpus. As already noted in (Zhang and Wu, 2012), statistical features can include term document frequency and inverse document fr"
C12-1074,J93-1003,0,0.25214,"e document frequency TF*IDF (Maedche and Staab, 2000), KF*IDF (Xu et al., 2002), C-value/NC-value (Frantzi et al., 2000) and so on. Term extraction is based on the computation of the Unithood – i.e. a degree of strength or stability of syntagmatic combinations or collocations, and Termhood – i.e. the degree that a linguistic unit is related to a domain-specific concept (Kageura and Umino, 1996) of the terminology. Termhood calculation is often based on the frequency of the term in the paper or a given baseline corpus. (Church and Hanks, 1990) used Mutual Information (MI) to compute Unithood, (Dunning, 1993) used LogL , and (Patry and Langlais, 2005) computed left/right entropy to extract unithood candidates. When computing termhood, methods such as TF*IDF (Maedche and Staab, 2000), DR-D (Velardi et al., 2001), C-value/NC value (Frantzi et al., 2000)) and Domain Component Feature Set (DCFS) (Zhang and Sui, 2007) are employed, as reported in (Zhang and Wu, 2012). 2.3 Hybrid approach Linguistics-based and statistical approaches have their own advantages and disadvantages. They are often integrated to extract terminology. There are two ways to combine them. One way is to extract candidate terms with"
C12-1074,A97-1028,0,0.0220065,"h gene has the same relevance between two conditions where the relevance is represented by the Shapley value of a particular coalitional game defined on a microarray data set . we can notice that the different method mentions - < phylogenetic profile >, < normalized Blastp bit score > < HMM >, < Bootstrap procedure > are all of different word shapes. Also when most of them are nouns phrases, they can be confused with other noun phrases in the sentence. We thus believed that the extraction of this type of method mentions can be viewed as a named entity recognition task (Maynard et al., 2001), (Palmer and Day, 1997). For this second task, we transformed each sentence into the BIO format (Table 1). There are 284 manual-tagged terms, 122 sentences, 2871 words and punctuations. In Table 2 we shows the number of sentences in each dataset. Enault and colleagues proposed an improved < phylogenetic profile > based on a < normalized Blastp bit score > Enault and colleagues proposed phylogenetic profile based on a normalized Blastp bit score . O O O O B-method I-method O O O B-method I-method I-method I-method O Table 1: Representation of a method sentence in the BIO format. 1217 Category (keywords) Method Analys"
C12-1074,W01-1005,0,0.040654,"a degree of strength or stability of syntagmatic combinations or collocations, and Termhood – i.e. the degree that a linguistic unit is related to a domain-specific concept (Kageura and Umino, 1996) of the terminology. Termhood calculation is often based on the frequency of the term in the paper or a given baseline corpus. (Church and Hanks, 1990) used Mutual Information (MI) to compute Unithood, (Dunning, 1993) used LogL , and (Patry and Langlais, 2005) computed left/right entropy to extract unithood candidates. When computing termhood, methods such as TF*IDF (Maedche and Staab, 2000), DR-D (Velardi et al., 2001), C-value/NC value (Frantzi et al., 2000)) and Domain Component Feature Set (DCFS) (Zhang and Sui, 2007) are employed, as reported in (Zhang and Wu, 2012). 2.3 Hybrid approach Linguistics-based and statistical approaches have their own advantages and disadvantages. They are often integrated to extract terminology. There are two ways to combine them. One way is to extract candidate terms with linguistics methods and then if no terms are found then the statistical methods are applied. (Daille, 1996) used the linguistic methods to get candidate terms and set them as the input of statistical model"
C12-1074,xu-etal-2002-domain,0,0.0360805,"t represents the pattern of noun sequences. Good results can be achieved in small corpora using linguistic methods, but due to the shortage of patterns, recall can be low and it is difficult to generalize these techniques across fields and languages. 1213 2.2 Statistical approach and machine learning approach Statistical approaches are based on statistical information, such as the frequency of terms appearing in the corpus. As already noted in (Zhang and Wu, 2012), statistical features can include term document frequency and inverse document frequency TF*IDF (Maedche and Staab, 2000), KF*IDF (Xu et al., 2002), C-value/NC-value (Frantzi et al., 2000) and so on. Term extraction is based on the computation of the Unithood – i.e. a degree of strength or stability of syntagmatic combinations or collocations, and Termhood – i.e. the degree that a linguistic unit is related to a domain-specific concept (Kageura and Umino, 1996) of the terminology. Termhood calculation is often based on the frequency of the term in the paper or a given baseline corpus. (Church and Hanks, 1990) used Mutual Information (MI) to compute Unithood, (Dunning, 1993) used LogL , and (Patry and Langlais, 2005) computed left/right e"
H91-1025,J90-2002,1,\N,Missing
H91-1025,P91-1034,1,\N,Missing
H92-1023,A88-1019,0,0.0334987,"Missing"
H92-1023,H91-1065,0,0.0333321,"Missing"
H92-1023,J92-4003,1,\N,Missing
H92-1026,P91-1027,0,0.016161,"t have a mechanism for estimating the coherence of an interpretation, both in isolation and in context. Probabilistic language models provide such a mechanism. Previous work on disambiguation and probabilistic parsing has offered partial answers to this question. Hidden Markov models of words and their tags, introduced in [1] and [11] and popularized in the natural language community by Church [5], demonstrate the power of short-term n-gram statistics to deal with lexical ambiguity. Hindle and Rooth [8] use a statistical measure of lexical associations to resolve structural ambiguities. Brent [2] acquires likely verb subcategorization patterns using the A probabilistic language model a t t e m p t s to estimate the probability of a sequence of sentences and their respective interpretations (parse trees) occurring in the language, ""P(S1 T1 $2 T2 ... S~ T~). *Thanks to Philip Resnik and Stanley Chen for their valued input. 134 The difficulty in applying probabilistic models to natural language is deciding what aspects of the sentence and the discourse are relevant to the model. Most previous probabilistic models of parsing assume the probabilities of sentences in a discourse are indepen"
H92-1026,H90-1055,1,0.865814,"few million words of sentences that are completely covered by this vocabulary from 40,000,000 words of computer manuals. A randomly chosen sentence from a sample of 5000 sentences from this corpus is: 5. T h e The g r a m m a r used in this experiment is a broadcoverage, feature-based unification g r a m m a r . The gramm a r is context-free but uses unification to express rule templates for the the context-free productions. For example, the rule template: : n 396. Grammar It indicates whether a call completed successfully or if some error was detected that caused the call to fail. unspec : n (3) corresponds to three C F G productions where the second feature : n is either s, p, or : n. This rule t e m p l a t e m a y elicit up to 7 non-terminals. The g r a m m a r has 21 features whose range of values m a y b e from 2 to a b o u t 100 with a median of 8. There are 672 rule templates of which 400 are actually exercised when we parse a corpus of 15,000 sentences. The number of productions t h a t are realized in this training corpus is several hundred thousand. To define what we mean by a correct parse, we use a corpus of manually bracketed sentences at the University of Lancaster call"
H92-1026,A88-1019,0,0.0212882,"d Grammars One goal of a parser is to produce a g r a m m a t i c a l interpretation of a sentence which represents the syntactic and semantic intent of the sentence. To achieve this goal, the parser must have a mechanism for estimating the coherence of an interpretation, both in isolation and in context. Probabilistic language models provide such a mechanism. Previous work on disambiguation and probabilistic parsing has offered partial answers to this question. Hidden Markov models of words and their tags, introduced in [1] and [11] and popularized in the natural language community by Church [5], demonstrate the power of short-term n-gram statistics to deal with lexical ambiguity. Hindle and Rooth [8] use a statistical measure of lexical associations to resolve structural ambiguities. Brent [2] acquires likely verb subcategorization patterns using the A probabilistic language model a t t e m p t s to estimate the probability of a sequence of sentences and their respective interpretations (parse trees) occurring in the language, ""P(S1 T1 $2 T2 ... S~ T~). *Thanks to Philip Resnik and Stanley Chen for their valued input. 134 The difficulty in applying probabilistic models to natural la"
H92-1026,H90-1056,0,0.0158234,"Missing"
H92-1026,H90-1052,0,0.0173264,"resents the syntactic and semantic intent of the sentence. To achieve this goal, the parser must have a mechanism for estimating the coherence of an interpretation, both in isolation and in context. Probabilistic language models provide such a mechanism. Previous work on disambiguation and probabilistic parsing has offered partial answers to this question. Hidden Markov models of words and their tags, introduced in [1] and [11] and popularized in the natural language community by Church [5], demonstrate the power of short-term n-gram statistics to deal with lexical ambiguity. Hindle and Rooth [8] use a statistical measure of lexical associations to resolve structural ambiguities. Brent [2] acquires likely verb subcategorization patterns using the A probabilistic language model a t t e m p t s to estimate the probability of a sequence of sentences and their respective interpretations (parse trees) occurring in the language, ""P(S1 T1 $2 T2 ... S~ T~). *Thanks to Philip Resnik and Stanley Chen for their valued input. 134 The difficulty in applying probabilistic models to natural language is deciding what aspects of the sentence and the discourse are relevant to the model. Most previous p"
H92-1026,E91-1004,0,0.104846,"guage, ""P(S1 T1 $2 T2 ... S~ T~). *Thanks to Philip Resnik and Stanley Chen for their valued input. 134 The difficulty in applying probabilistic models to natural language is deciding what aspects of the sentence and the discourse are relevant to the model. Most previous probabilistic models of parsing assume the probabilities of sentences in a discourse are independent of other sentences. I n fact, previous works have made much stronger independence assumptions. The P-CFG model considers the probability of each constituent rule independent of all other constituents in the sentence. The Pearl [10] model includes a slightly richer model of context, allowing the probability of a constituent rule to depend upon the immediate parent of the rule and a part-of-speech trigram from the input sentence. But none of these models come close to incorporating enough context to disambiguate many cases of ambiguity. A significant reason researchers have limited the contextual information used by their models is because of the difficulty in estimating very rich probabilistic models of context. In this work, we present a model, the historybased grammar model, which incorporates a very rich model of cont"
H92-1026,H90-1054,0,0.0711553,"Missing"
H92-1026,H91-1067,0,\N,Missing
H92-1026,J92-4003,1,\N,Missing
H92-1026,1991.iwpt-1.22,1,\N,Missing
H92-1053,J90-2002,1,0.641494,"Brown et aL employ a suboptimal search based on the stack algorithm used in speech recognition. Even so, as we see in Figure 1, the time required for their system to translate a sentence grows very rapidly with sentence length. As a result, they have focussed their attention on short sentences. 100 I I I I I I 80 T Introduction In a recent series of papers, Brown et aL introduce a new, statistical approach to machine translation based on the mathematical theory of communication through a noisy channel, and apply it to the problem of translating naturMly occurring French sentences into English [1, 2, 3, 4]. They develop a probabilistic model for the noisy channel and show how to estimate the parameters of their model from a large collection of pairs of aligned sentences. By treating a sentence in the source language (French) as a garbled version of the corresponding sentence in the target language (English), they recast the problem of translating a French sentence into English as one of finding that English sentence which is most likely to be present at the input to the noisy channel when the given French sentence is known to be present at its output. For a French sentence of any realistic leng"
H92-1053,P91-1022,1,0.887554,"Brown et aL employ a suboptimal search based on the stack algorithm used in speech recognition. Even so, as we see in Figure 1, the time required for their system to translate a sentence grows very rapidly with sentence length. As a result, they have focussed their attention on short sentences. 100 I I I I I I 80 T Introduction In a recent series of papers, Brown et aL introduce a new, statistical approach to machine translation based on the mathematical theory of communication through a noisy channel, and apply it to the problem of translating naturMly occurring French sentences into English [1, 2, 3, 4]. They develop a probabilistic model for the noisy channel and show how to estimate the parameters of their model from a large collection of pairs of aligned sentences. By treating a sentence in the source language (French) as a garbled version of the corresponding sentence in the target language (English), they recast the problem of translating a French sentence into English as one of finding that English sentence which is most likely to be present at the input to the noisy channel when the given French sentence is known to be present at its output. For a French sentence of any realistic leng"
H92-1053,P91-1034,1,0.847988,"Brown et aL employ a suboptimal search based on the stack algorithm used in speech recognition. Even so, as we see in Figure 1, the time required for their system to translate a sentence grows very rapidly with sentence length. As a result, they have focussed their attention on short sentences. 100 I I I I I I 80 T Introduction In a recent series of papers, Brown et aL introduce a new, statistical approach to machine translation based on the mathematical theory of communication through a noisy channel, and apply it to the problem of translating naturMly occurring French sentences into English [1, 2, 3, 4]. They develop a probabilistic model for the noisy channel and show how to estimate the parameters of their model from a large collection of pairs of aligned sentences. By treating a sentence in the source language (French) as a garbled version of the corresponding sentence in the target language (English), they recast the problem of translating a French sentence into English as one of finding that English sentence which is most likely to be present at the input to the noisy channel when the given French sentence is known to be present at its output. For a French sentence of any realistic leng"
H92-1053,1992.tmi-1.8,1,0.71686,"i , f ) , it is straightforward using dynamic programming to find the split that maximizes Sa. If we approximate t(l) to be zero for l less than some threshold and infinite for l equal to or greater than that threshold, then we can discard o~. Our utility becomes simply m SO,f) = ~logp(rlik,f) k=l provided all of the segments are less than the threshold. If the length of any segment is equal to or greater than the threshold, then the utility is -exp. Decoding In the absence of segmentation, we employ an anMysis-transfer-synthesis paradigm in our decoder as described in detail by Brown et al. [5]. We have insinuated the segmenter into the system between the analysis and the transfer phases o f o u r processing. The analysis operation, therefore, is unaffected by the presence of the segmenter. We have also modified the transfer portion of the decoder so as to investigate only those translations that are consistent with the segmented input, but have otherwise left it alone. As a result, we get the benefit of the English language model across segment boundaries, but save time by not considering the great number of translations that are not consistent with the segmented input. Results To"
H93-1039,J93-2003,1,\N,Missing
H94-1028,J93-2003,1,0.0776936,"e) may then be obtained as ~ a P r ( f , a l e ) , where the sum is taken over all possible alignments of e and f. In practice this is possible only for our first two models. For the remaining models, we approximate P r ( f I e) as follows. During training, we find the single most probable alignment &, and sum Pr(f, a I e) over a small neighborhood of &. During decoding, we simply use Pr(f, ale). 4.2. EM-Trained Models We now sketch the structure of five models of increasing complexity, the last of which is our EM-trained translation model. For an in-depth treatment, the reader is referred to [3]. 1. W o r d T r a n s l a t i o n This is our simplest model, intended to discover probable individual-word translations. The free parameters of this model are word translation probabilities t(fj I ei). Each of these parameters is initialized to 1/I.FI, where Y is our French vocabulary. Thus we make no initial assumptions about appropriate French-English word pairings. The iterative training procedure automatically finds appropriate translations, and assigns them high probability. 2. L o c a l A l i g n m e n t To make our model more realistic, we introduce an alignment variable aj for each p"
H94-1028,H93-1040,0,0.0190516,"h. The process of translation, divided into analysis, transfer, and synthesis stages, is depicted in Figure 3. In the analysis stage, the French input string f is converted into f~, as discussed above. The output of this stage is denoted in Figure 3 as Intermediate French. The transfer stage constitutes the decoding process sketched in Section 2.2 above. Decoding consists of two steps. In the first step, Candide develops a set H* of candidate decodings, using coarse versions of our translation and language models 161 The ARPA evaluation methodology, devised and executed by PRO, is detailed in [9]; we recount it here briefly. AReA provides us with a set of French passages, which we process in two ways. First, the passages are translated without any human intervention. This is the fully-automatic mode. Second, each of the same passages is translated by two different humans, once with and once without the aid of Transman, our translation assistance tool. Transman presents the user with an automated dictionary, a text editor, and parallel views of the French source and the English fully-automatic translation. The passages are ordered in such a way as to suppress the influence of differing"
H94-1028,1993.mtsummit-1.24,0,\N,Missing
J12-2009,W10-1818,0,0.0464311,"Missing"
J90-2002,C86-1033,0,0.121743,"the true obstacles lay in the relative impotence of the available computers and the dearth of machinereadable text from which to gather the statistics vital to such an attack. Today, computers are five orders of magnitude faster than they were in 1950 and have hundreds of millions of bytes of storage. Large, machine-readable corpora are readily available. Statistical methods have proven their value in automatic speech recognition (Bahl et al. 1983) and have recently been applied to lexicography (Sinclair 1985) and to natural language processing (Baker 1979; Ferguson 1980; Garside et al. 1987; Sampson 1986; Sharman et al. 1988). We feel that it is time to give them a chance in machine translation. The job of a translator is to render in one language the meaning expressed by a passage of text in another language. This task is not always straightforward. For example, the translation of a word may depend on words quite far from it. Some English translators of Proust's seven volume work A la Recherche du Temps Perdu have striven to make the first word of the first volume the same as the last word of the last volume because the French original begins and ends with the same word (Bernstein 1988). Thu"
J90-2002,1983.tc-1.13,0,0.0942931,"Missing"
J90-2002,H90-1054,0,\N,Missing
J92-4003,J90-2002,1,\N,Missing
J93-1001,J90-3007,0,0.0413581,"to the COBUILD dictionary: For the first time, a dictionary has been compiled by the thorough examination of a representative group of English texts, spoken and written, running to many millions of words. This means that in addition to all the tools of the conventional dictionary makerswwide reading and experience of English, other dictionaries and of course eyes and ears--this dictionary is based on hard, measurable evidence. (Sinclair et al. 1987; p. xv) The experience of writing the COBUILD dictionary is documented in Sinclair (1987), a collection of articles from the COBUILD project; see Boguraev (1990) for a strong positive review of this collection. At the time, the corpus-based approach to lexicography was considered pioneering, even somewhat controversial; today, quite a number of the major lexicography houses are collecting large amounts of corpus data. The traditional alternative to corpora are citation indexes, boxes of interesting citations collected on index cards by large numbers of human readers. Unfortunately, citation indexes tend to be a bit like butterfly collections, full of rare and unusual specimens, but severely lacking in ordinary, garden-variety moths. Murray, the editor"
J93-1001,J90-2002,1,0.364969,"ted over a 27-character alphabet. 3 Lari and Young actually looked at another task involving phonotactic structure where there is also good reason to believe that SCFGs might be able to capture crucial linguistic constraints that might be missed by simpler HMMs. 15 Computational Linguistics Volume 19, Number 1 has tended to favor rationalism, though there are some important exceptions, such as example-based MT (Sato and Nagao 1990). The issue remains as controversial as ever, as evidenced by the lively debate on rationalism versus empiricism at TMI-92, a recent conference on MT.4 The paper by Brown et al. (1990) revives Weaver's information theoretic approach to MT. It requires a bit more squeezing and twisting to fit machine translation into the noisy channel mold: to translate, for example, from French to English, one imagines that the native speaker of French has thought up what he or she wants to say in English and then translates mentally into French before actually saying it. The task of the translation system is to recover the original English, E, from the observed French, F. While this may seem a bit far-fetched, it differs little in principle from using English as an interlingua or as a mean"
J93-1001,J92-1002,1,0.763814,"hes. 5. Machine Translation and Bilingual Lexicography IS machine translation (MT) more suitable for rationalism or empiricism? Both approaches have been investigated. Weaver (1949) was the first to propose an information theoretic approach to MT. The empirical approach was also practiced at G e o r g e t o w n during the 1950s and 1960s (Henisz-Dostert, Ross Macdonald, and Zarechnak 1979) in a system that eventually became k n o w n as SYSTRAN. Recently, most w o r k in MT 2 In fact, the trigram model might be even better than suggested in Table 5, since the estimate for the trigram model in Brown et al. (1992)is computed over a 256-characteralphabet, whereas the estimate for human performance in Shannan (1951) is computed over a 27-character alphabet. 3 Lari and Young actually looked at another task involving phonotactic structure where there is also good reason to believe that SCFGs might be able to capture crucial linguistic constraints that might be missed by simpler HMMs. 15 Computational Linguistics Volume 19, Number 1 has tended to favor rationalism, though there are some important exceptions, such as example-based MT (Sato and Nagao 1990). The issue remains as controversial as ever, as evide"
J93-1001,A88-1019,1,0.634765,"Missing"
J93-1001,P90-1031,0,0.0370696,"Missing"
J93-1001,J88-1003,0,0.0610669,"Missing"
J93-1001,P89-1015,0,0.0329604,"Missing"
J93-1001,C90-3030,0,0.0354528,"Missing"
J93-1001,J93-2005,0,0.0604417,"lingual Lexicography, Machine-Readable Dictionaries (MRDs), and Computational Lexicons There has been a long tradition of empiricist approaches in lexicography, both bilingual and monolingual, dating back to Johnson and Murray. As corpus data and machinereadable dictionaries (MRDs) become more and more available, it is becoming easier to compile lexicons for computers and dictionaries for people. This is a particularly exciting area in computational linguistics as evidenced by the large number of contributions in these special issues: Biber (1993), Brent (1993), Hindle and Rooth (this issue), Pustejovsky et al. (1993), and Smadja (this issue). Starting with the COBUILD dictionary (Sinclair et al. 1987), it is now becoming more and more common to find lexicographers working directly with corpus data. Sinclair makes an excellent case for the use of corpus evidence in the preface to the COBUILD dictionary: For the first time, a dictionary has been compiled by the thorough examination of a representative group of English texts, spoken and written, running to many millions of words. This means that in addition to all the tools of the conventional dictionary makerswwide reading and experience of English, other d"
J93-1001,J82-2005,0,0.111046,"is more directly relevant than in word recognition. In general, phrase structure is probably more important for understanding w h o did what to w h o m , than recognizing what was said. 3 Some tasks are probably more appropriate for Chomsky's rational approach to language and other tasks are probably more appropriate for Shannon's empirical approach to language. Table 6 summarizes some of the differences between the two approaches. 5. Machine Translation and Bilingual Lexicography IS machine translation (MT) more suitable for rationalism or empiricism? Both approaches have been investigated. Weaver (1949) was the first to propose an information theoretic approach to MT. The empirical approach was also practiced at G e o r g e t o w n during the 1950s and 1960s (Henisz-Dostert, Ross Macdonald, and Zarechnak 1979) in a system that eventually became k n o w n as SYSTRAN. Recently, most w o r k in MT 2 In fact, the trigram model might be even better than suggested in Table 5, since the estimate for the trigram model in Brown et al. (1992)is computed over a 256-characteralphabet, whereas the estimate for human performance in Shannan (1951) is computed over a 27-character alphabet. 3 Lari and Young"
J93-2003,J90-2002,1,0.278745,"because of the minimal linguistic content o,f our algorithms, that it is reasonable to argue that word-by-word alignments are inherent in any sufficiently large bilingual corpus. 1. Introduction The growing availability of bilingual, machine-readable texts has stimulated interest in methods for extracting linguistically valuable information from such texts. For example, a number of recent papers deal with the problem of automatically obtaining pairs of aligned sentences from parallel corpora (Warwick and Russell 1990; Brown, Lai, and Mercer 1991; Gale and Church 1991b; Kay 1991). Brown et al. (1990) assert, and Brown, Lai, and Mercer (1991) and Gale and Church (1991b) both show, that it is possible to obtain such aligned pairs of sentences without inspecting the words that the sentences contain. Brown, Lai, and Mercer base their algorithm on the number of words that the sentences contain, while Gale and Church base a similar algorithm on the number of characters that the sentences contain. The lesson to be learned from these two efforts is that simple, statistical methods can be surprisingly successful in achieving linguistically interesting goals. Here, we address a natural extension of"
J93-2003,C88-1016,0,0.188366,"it is possible to obtain such aligned pairs of sentences without inspecting the words that the sentences contain. Brown, Lai, and Mercer base their algorithm on the number of words that the sentences contain, while Gale and Church base a similar algorithm on the number of characters that the sentences contain. The lesson to be learned from these two efforts is that simple, statistical methods can be surprisingly successful in achieving linguistically interesting goals. Here, we address a natural extension of that work: matching up the words within pairs of aligned sentences. In recent papers, Brown et al. (1988, 1990) propose a statistical approach to machine translation from French to English. In the latter of these papers, they sketch an algorithm for estimating the probability that an English word will be translated into any particular French word and show that such probabilities, once estimated, can be used together with a statistical model of the translation process to align the words in an English sentence with the words in its French translation (see their Figure 3). * IBM T.J.WatsonResearchCenter, YorktownHeights,NY 10598 (~) 1993 Associationfor ComputationalLinguistics Computational Linguis"
J93-2003,H91-1025,1,0.152931,"must converge in a finite, though impractically large, number of steps because each translation has only a finite number of alignments. In practice, we are never sure that we have found the Viterbi alignment. If we reinterpret the Viterbi alignment to m e a n the most probable alignment that we can find rather than the most probable alignment that exists, then a similarly reinterpreted Viterbi training algorithm still converges. We have already used this algorithm successfully as a part of a system to assign senses to English and French words on the basis of the context in which they appear (Brown et al. 1991a, 1991b). We expect to use it in models that we develop beyond Model 5. 293 Computational Linguistics Volume 19, Number 2 6.3 Multi-Word Cepts In Models 1-5, we restrict our attention to alignments with cepts containing no more than one word each. Except in Models 4 and 5, cepts play little r61e in our development. Even in these models, cepts are determined implicitly by the fertilities of the words in the alignment: words for which the fertility is greater than zero make up one-word cepts; those for which it is zero do not. We can easily extend the generative process upon which Models 3, 4,"
J93-2003,P91-1034,1,0.156162,"must converge in a finite, though impractically large, number of steps because each translation has only a finite number of alignments. In practice, we are never sure that we have found the Viterbi alignment. If we reinterpret the Viterbi alignment to m e a n the most probable alignment that we can find rather than the most probable alignment that exists, then a similarly reinterpreted Viterbi training algorithm still converges. We have already used this algorithm successfully as a part of a system to assign senses to English and French words on the basis of the context in which they appear (Brown et al. 1991a, 1991b). We expect to use it in models that we develop beyond Model 5. 293 Computational Linguistics Volume 19, Number 2 6.3 Multi-Word Cepts In Models 1-5, we restrict our attention to alignments with cepts containing no more than one word each. Except in Models 4 and 5, cepts play little r61e in our development. Even in these models, cepts are determined implicitly by the fertilities of the words in the alignment: words for which the fertility is greater than zero make up one-word cepts; those for which it is zero do not. We can easily extend the generative process upon which Models 3, 4,"
J93-2003,P91-1022,1,0.155887,"must converge in a finite, though impractically large, number of steps because each translation has only a finite number of alignments. In practice, we are never sure that we have found the Viterbi alignment. If we reinterpret the Viterbi alignment to m e a n the most probable alignment that we can find rather than the most probable alignment that exists, then a similarly reinterpreted Viterbi training algorithm still converges. We have already used this algorithm successfully as a part of a system to assign senses to English and French words on the basis of the context in which they appear (Brown et al. 1991a, 1991b). We expect to use it in models that we develop beyond Model 5. 293 Computational Linguistics Volume 19, Number 2 6.3 Multi-Word Cepts In Models 1-5, we restrict our attention to alignments with cepts containing no more than one word each. Except in Models 4 and 5, cepts play little r61e in our development. Even in these models, cepts are determined implicitly by the fertilities of the words in the alignment: words for which the fertility is greater than zero make up one-word cepts; those for which it is zero do not. We can easily extend the generative process upon which Models 3, 4,"
J93-2003,P91-1023,0,0.157826,"ther pairs o,f languages. We also ,feel, again because of the minimal linguistic content o,f our algorithms, that it is reasonable to argue that word-by-word alignments are inherent in any sufficiently large bilingual corpus. 1. Introduction The growing availability of bilingual, machine-readable texts has stimulated interest in methods for extracting linguistically valuable information from such texts. For example, a number of recent papers deal with the problem of automatically obtaining pairs of aligned sentences from parallel corpora (Warwick and Russell 1990; Brown, Lai, and Mercer 1991; Gale and Church 1991b; Kay 1991). Brown et al. (1990) assert, and Brown, Lai, and Mercer (1991) and Gale and Church (1991b) both show, that it is possible to obtain such aligned pairs of sentences without inspecting the words that the sentences contain. Brown, Lai, and Mercer base their algorithm on the number of words that the sentences contain, while Gale and Church base a similar algorithm on the number of characters that the sentences contain. The lesson to be learned from these two efforts is that simple, statistical methods can be surprisingly successful in achieving linguistically interesting goals. Here,"
J93-2003,1983.tc-1.13,0,0.164416,"e of them. In the final section, we discuss the significance of our work and the possibility of extending it to other pairs of languages. Finally, we include two appendices: one to summarize notation and one to collect the formulae for the various models that we describe and to fill an occasional gap in our development. 2. Statistical Translation In 1949, Warren Weaver suggested applying the statistical and cryptanalytic techniques then emerging from the nascent field of communication theory to the problem of using computers to translate text from one natural language to another (published in Weaver 1955). Efforts in this direction were soon abandoned for various philosophical and theoretical reasons, but at a time when the most advanced computers were of a piece with today&apos;s digital watch, any such approach was surely doomed to computational starvation. Today, the fruitful application of statistical methods to the study of machine translation is within the computational grasp of anyone with a well-equipped workstation. A string of English words, e, can be translated into a string of French words in many different ways. Often, knowing the broader context in which e occurs may serve to winnow t"
J93-2003,J93-1004,0,\N,Missing
J93-2003,J92-4003,1,\N,Missing
J93-2003,J93-1006,0,\N,Missing
J93-2003,H91-1026,0,\N,Missing
N19-1137,P17-1067,0,0.159519,"n algorithm, SentiStrength, which utilizes a dictionary of sentiment words associated with strength measures to deal with short informal texts from social media. Gilbert (2014) propose VADER, a rule-based model for sentiment analysis. They built a lexicon which is specially attuned to microblog-like contexts and their model outperforms individual human raters. More recently, deep learning models have proven to be very successful when applied on various text-related tasks (Kim, 2014; Kalchbrenner et al., 2014; dos Santos and Gatti, 2014; Tai et al., 2015; Wang et al., 2016; Felbo et al., 2017; Abdul-Mageed and Ungar, 2017). Kim (2014) showed the effectiveness of a simple CNN model that leverages pretrained word vectors for a sentence classification task. Kalchbrenner et al. (2014) propose a dynamic CNN model using a dynamic k-max pooling mechanism which is able to generate a feature graph which captures a variety of word relations. They showed the efficacy of their model by achieving high performances on binary and multiclass sentiment classification tasks without any feature engineering. dos Santos and Gatti (2014) propose a deep CNN model that uses both character and word-level information allowing them to ac"
N19-1137,I08-1041,0,0.0507948,"novel use of a single neural network architecture. A number of emotion theories are available which suggest different sets of basic emotions. Interestingly, joy, sadness, anger, fear and surprise are common to all. To the best of our knowledge, the model suggested by Ekman (1999) is the most broadly used emotion model. In this study, we use Ekman’s basic emotions together with other sets of emotions (Plutchik, 1984; Shaver et al., 1987). In early textual emotion mining and sentiment analysis research, the usefulness of using external lexicons along with predefined rules has been demonstrated (Aman and Szpakowicz, 2008; Neviarouskaya et al., 2007; Bandhakavi et al., 2017; Thelwall et al., 2010; Gilbert, 2014). Aman and Szpakowicz (2008) used Roget’s Thesaurus along with WordNet-Affect for fine-grained emotion prediction from blog data. Bandhakavi et al. (2017) propose a unigram mixture model (UMM) 1355 Proceedings of NAACL-HLT 2019, pages 1355–1365 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics . . K=1 w1 . w2 . . K=2 . . . wn . . . K=3 . . Tweet Matrix (L1 x d) Convolutional layer with multiple window sizes (k=1,2,3) . h1 . . . K=1 hk . . e1 . . . ep Hash-Em"
N19-1137,P04-3031,0,0.183491,"iment strength and 5 represents strong sentiment strength. We followed the heuristics used by Saif et al. (2013) to obtain a single sentiment label for each tweet, giving us a total of 4, 242 positive, negative and neutral tweets. The transformed dataset has been used in other literature (Go et al., 2009; Zhang et al., 2018). We provide basic statistics of the datasets used in our experiments in Table 1. 3.2 Experimental Setup Data Cleaning. Twitter data is unstructured and highly informal (Yoon et al., 2013) and thus it requires a great deal of effort to make it suitable for any model. NLTK (Bird and Loper, 2004) provides a regular-expression based tokenizer for Twitter, TweetTokenizer, which preserves user mentions, hashtags, urls, emoticons and emojis in particular. It also reduces the length of repeated characters to three (i.e. ”Haaaaaapy” will become ”Haaapy”)”. In our experiments, we utilized the TweetTokenizer to tokenize tweets. To accommodate the pretrained word vectors from (Pennington et al., 2014b), we pre-processed each tweet in a number of ways. We lowercased all the letters in the tweet. User mentions have been replaced with &lt;user&gt; token (i.e. @username1 will become &lt;user&gt;). In addition"
N19-1137,D17-1169,0,0.0420704,"al. (2010) propose an algorithm, SentiStrength, which utilizes a dictionary of sentiment words associated with strength measures to deal with short informal texts from social media. Gilbert (2014) propose VADER, a rule-based model for sentiment analysis. They built a lexicon which is specially attuned to microblog-like contexts and their model outperforms individual human raters. More recently, deep learning models have proven to be very successful when applied on various text-related tasks (Kim, 2014; Kalchbrenner et al., 2014; dos Santos and Gatti, 2014; Tai et al., 2015; Wang et al., 2016; Felbo et al., 2017; Abdul-Mageed and Ungar, 2017). Kim (2014) showed the effectiveness of a simple CNN model that leverages pretrained word vectors for a sentence classification task. Kalchbrenner et al. (2014) propose a dynamic CNN model using a dynamic k-max pooling mechanism which is able to generate a feature graph which captures a variety of word relations. They showed the efficacy of their model by achieving high performances on binary and multiclass sentiment classification tasks without any feature engineering. dos Santos and Gatti (2014) propose a deep CNN model that uses both character and word-level"
N19-1137,P14-1062,0,0.0532618,"words, etc., in which each entry is labeled with an emotion and its intensity. Thelwall et al. (2010) propose an algorithm, SentiStrength, which utilizes a dictionary of sentiment words associated with strength measures to deal with short informal texts from social media. Gilbert (2014) propose VADER, a rule-based model for sentiment analysis. They built a lexicon which is specially attuned to microblog-like contexts and their model outperforms individual human raters. More recently, deep learning models have proven to be very successful when applied on various text-related tasks (Kim, 2014; Kalchbrenner et al., 2014; dos Santos and Gatti, 2014; Tai et al., 2015; Wang et al., 2016; Felbo et al., 2017; Abdul-Mageed and Ungar, 2017). Kim (2014) showed the effectiveness of a simple CNN model that leverages pretrained word vectors for a sentence classification task. Kalchbrenner et al. (2014) propose a dynamic CNN model using a dynamic k-max pooling mechanism which is able to generate a feature graph which captures a variety of word relations. They showed the efficacy of their model by achieving high performances on binary and multiclass sentiment classification tasks without any feature engineering. dos Sant"
N19-1137,D14-1181,0,0.00778076,"ons, affect words, etc., in which each entry is labeled with an emotion and its intensity. Thelwall et al. (2010) propose an algorithm, SentiStrength, which utilizes a dictionary of sentiment words associated with strength measures to deal with short informal texts from social media. Gilbert (2014) propose VADER, a rule-based model for sentiment analysis. They built a lexicon which is specially attuned to microblog-like contexts and their model outperforms individual human raters. More recently, deep learning models have proven to be very successful when applied on various text-related tasks (Kim, 2014; Kalchbrenner et al., 2014; dos Santos and Gatti, 2014; Tai et al., 2015; Wang et al., 2016; Felbo et al., 2017; Abdul-Mageed and Ungar, 2017). Kim (2014) showed the effectiveness of a simple CNN model that leverages pretrained word vectors for a sentence classification task. Kalchbrenner et al. (2014) propose a dynamic CNN model using a dynamic k-max pooling mechanism which is able to generate a feature graph which captures a variety of word relations. They showed the efficacy of their model by achieving high performances on binary and multiclass sentiment classification tasks without any fe"
N19-1137,S17-1007,0,0.0934786,"Missing"
N19-1137,S18-1001,0,0.0438627,"re collected based on hashtags corresponding to Ekman’s (1999) six basic emotions. The dataset has been used in related works (Shahraki and Zaiane, 2017; Balahur, 2013; Mohammad and Kiritchenko, 2015). CBET. The Cleaned Balanced Emotional Tweet dataset is provided by Shahraki and Zaiane (2017). To the best of our knowledge, this is one of the largest publically available balanced datasets for twitter emotion detection research. The dataset contains 80,937 tweets with nine emotion categories including Ekman’s six basic emotions. 1358 SE. The SemEval-2018 Task 1 - Affect dataset was provided by Mohammad et al. (2018). The SemEval task was to estimate the intensity of a given tweet and its corresponding emotion. However, in this study, we utilize the labeled dataset only to classify the tweets into four emotion categories and use the training, development and test sets provided in this dataset in our experiments. STS-Gold. This dataset was constructed by Saif et al. (2013) for Twitter sentiment analysis. The dataset contains a total of 2,034 tweets labeled (positive/negative) by three annotators. This dataset has been extensively used in several works for model evaluation (Saif et al., 2014b; Krouska et al"
N19-1137,S12-1033,0,0.0962284,"et al. (2012). The dataset had been automatically annotated based on the seven emotion category seed words (Shaver et al., 1987) being a hashtag and the quality of the data was verified by two annotators as described in (Wang et al., 2012). We were only able to retrieve a portion of the original dataset as many tweets were either removed or not available at the time we fetched the data using the Twitter API. We applied the heuristics from (Wang et al., 2012) to remove any hashtags from the tweets which belong to the list of emotion seed words. TEC. Twitter Emotion Corpus has been published by Mohammad (2012) for research purposes. About 21,000 tweets were collected based on hashtags corresponding to Ekman’s (1999) six basic emotions. The dataset has been used in related works (Shahraki and Zaiane, 2017; Balahur, 2013; Mohammad and Kiritchenko, 2015). CBET. The Cleaned Balanced Emotional Tweet dataset is provided by Shahraki and Zaiane (2017). To the best of our knowledge, this is one of the largest publically available balanced datasets for twitter emotion detection research. The dataset contains 80,937 tweets with nine emotion categories including Ekman’s six basic emotions. 1358 SE. The SemEval"
N19-1137,D14-1162,0,0.104514,"added using zero padding. In the Tweet Matrix, every word is represented as a d-dimensional word vector. Since tweets are usually noisy, short in length, and have different kinds of features other than text, it’s useful to have a word embedding specially trained on a large amount of Tweet data (Tang et al., 2014). Previous research (Collobert et al., 2011; Socher et al., 2011) has shown the usefulness of using pretrained word vectors to improve the performance of various models. As a result, in our experiments, we have used the publicly available pre-trained GloVe word vectors for Twitter by (Pennington et al., 2014a). The word vectors are trained on 27B word tokens in an unsupervised manner. In this layer, we also pass another matrix called the Hash-Emo Matrix through a different channel in our network. This matrix is composed of three different sets of features: hashtags, emoticons and emojis. These are considered as distinguishable traits to showcase one’s mood (Zhao et al., 2012). People like to use hashtags to express their emotional state through various microblogging sites (e.g., Twitter) (Qadir and Riloff, 2014). Also graphical emoticons or emojis can convey strong emotion or sentiment. So for ea"
N19-1137,C14-1008,0,0.226863,"ry is labeled with an emotion and its intensity. Thelwall et al. (2010) propose an algorithm, SentiStrength, which utilizes a dictionary of sentiment words associated with strength measures to deal with short informal texts from social media. Gilbert (2014) propose VADER, a rule-based model for sentiment analysis. They built a lexicon which is specially attuned to microblog-like contexts and their model outperforms individual human raters. More recently, deep learning models have proven to be very successful when applied on various text-related tasks (Kim, 2014; Kalchbrenner et al., 2014; dos Santos and Gatti, 2014; Tai et al., 2015; Wang et al., 2016; Felbo et al., 2017; Abdul-Mageed and Ungar, 2017). Kim (2014) showed the effectiveness of a simple CNN model that leverages pretrained word vectors for a sentence classification task. Kalchbrenner et al. (2014) propose a dynamic CNN model using a dynamic k-max pooling mechanism which is able to generate a feature graph which captures a variety of word relations. They showed the efficacy of their model by achieving high performances on binary and multiclass sentiment classification tasks without any feature engineering. dos Santos and Gatti (2014) propose"
N19-1137,D11-1014,0,0.0671334,"ur convolutional neural network. The first matrix represents a particular tweet. Each tweet ti consists of a sequence of tokens w1 , w2 , . . . , wni . L1 is the maximum tweet length. The height of the Tweet Matrix is L1 . Short tweets are padded using zero padding. In the Tweet Matrix, every word is represented as a d-dimensional word vector. Since tweets are usually noisy, short in length, and have different kinds of features other than text, it’s useful to have a word embedding specially trained on a large amount of Tweet data (Tang et al., 2014). Previous research (Collobert et al., 2011; Socher et al., 2011) has shown the usefulness of using pretrained word vectors to improve the performance of various models. As a result, in our experiments, we have used the publicly available pre-trained GloVe word vectors for Twitter by (Pennington et al., 2014a). The word vectors are trained on 27B word tokens in an unsupervised manner. In this layer, we also pass another matrix called the Hash-Emo Matrix through a different channel in our network. This matrix is composed of three different sets of features: hashtags, emoticons and emojis. These are considered as distinguishable traits to showcase one’s mood"
N19-1137,D14-1127,0,0.0276902,"s, we have used the publicly available pre-trained GloVe word vectors for Twitter by (Pennington et al., 2014a). The word vectors are trained on 27B word tokens in an unsupervised manner. In this layer, we also pass another matrix called the Hash-Emo Matrix through a different channel in our network. This matrix is composed of three different sets of features: hashtags, emoticons and emojis. These are considered as distinguishable traits to showcase one’s mood (Zhao et al., 2012). People like to use hashtags to express their emotional state through various microblogging sites (e.g., Twitter) (Qadir and Riloff, 2014). Also graphical emoticons or emojis can convey strong emotion or sentiment. So for each tweet ti , we extract hashtags h1 , h2 , . . . , hki and emoticons/emojis e1 , e2 , . . . , epi . We concatenate the hashtags and emoticon/emoji vectors for each tweet ti to get the Hash-Emo Matrix. We introduce a hyper-parameter L2 as a threshold on the height of the Hash-Emo Matrix. Tweets with the number of hash-emo features less than L2 are padded with zero while tweets with more hashemo features than L2 are truncated. We use word vectors from GloVe with dimension d for hashtags words. In the case that"
N19-1137,P15-1150,0,0.0898764,"Missing"
N19-1137,P14-1146,0,0.0856023,"Hash-Emo Matrix, are passed through two different channels of our convolutional neural network. The first matrix represents a particular tweet. Each tweet ti consists of a sequence of tokens w1 , w2 , . . . , wni . L1 is the maximum tweet length. The height of the Tweet Matrix is L1 . Short tweets are padded using zero padding. In the Tweet Matrix, every word is represented as a d-dimensional word vector. Since tweets are usually noisy, short in length, and have different kinds of features other than text, it’s useful to have a word embedding specially trained on a large amount of Tweet data (Tang et al., 2014). Previous research (Collobert et al., 2011; Socher et al., 2011) has shown the usefulness of using pretrained word vectors to improve the performance of various models. As a result, in our experiments, we have used the publicly available pre-trained GloVe word vectors for Twitter by (Pennington et al., 2014a). The word vectors are trained on 27B word tokens in an unsupervised manner. In this layer, we also pass another matrix called the Hash-Emo Matrix through a different channel in our network. This matrix is composed of three different sets of features: hashtags, emoticons and emojis. These"
N19-1137,P16-2037,0,0.021472,"nsity. Thelwall et al. (2010) propose an algorithm, SentiStrength, which utilizes a dictionary of sentiment words associated with strength measures to deal with short informal texts from social media. Gilbert (2014) propose VADER, a rule-based model for sentiment analysis. They built a lexicon which is specially attuned to microblog-like contexts and their model outperforms individual human raters. More recently, deep learning models have proven to be very successful when applied on various text-related tasks (Kim, 2014; Kalchbrenner et al., 2014; dos Santos and Gatti, 2014; Tai et al., 2015; Wang et al., 2016; Felbo et al., 2017; Abdul-Mageed and Ungar, 2017). Kim (2014) showed the effectiveness of a simple CNN model that leverages pretrained word vectors for a sentence classification task. Kalchbrenner et al. (2014) propose a dynamic CNN model using a dynamic k-max pooling mechanism which is able to generate a feature graph which captures a variety of word relations. They showed the efficacy of their model by achieving high performances on binary and multiclass sentiment classification tasks without any feature engineering. dos Santos and Gatti (2014) propose a deep CNN model that uses both chara"
N19-1137,H05-1044,0,0.13253,"ided by Mohammad and Turney (2013) which consists of a list of unigrams and their association with one of the emotion categories (anger, anticipation, disgust, fear, joy, sadness, surprise, trust). We use the percentage of tokens belonging to each emotion category as features. We also used the NRC Affect Intensity Lexicon provided by Mohammad and Bravo-Marquez (2017) and NRC Hashtag Emotion Lexicon provided by Mohammad and Kiritchenko (2015) which contain real-valued fine-grained word-emotion association scores for words and hashtag words. We combined two lexicons MPQA and BingLiu provided by Wilson et al. (2005) and Hu and Liu (2004), respectively, and used them to calculate the percentage of positive and negative tokens belonging to each tweet. We also used AFINN (Nielsen, 2011) which contains a list of English words rated for valence with an integer between −5 (negative) to +5 (positive). We first normalized the scores in the range [0,1] and then calculated the average of this score over all the tokens in a tweet. Lastly, we detect the presence of consecutive exclamation (!) and question marks (?) in a tweet and use them as boolean features. Network Parameters and Training. Zhang and Wallace (2017)"
N19-1137,I17-1026,0,0.0779605,"Dataset TEC CBET 8, 240 10, 691 3, 830 8, 623 1, 555 9, 023 − 9, 398 − 8, 544 2, 816 9, 021 3, 849 8, 552 − 8, 540 761 8, 545 21, 051 80, 937 SE 3, 011 2, 905 3, 091 − − 3, 627 − − − 12, 634 (a) Dataset STS-Gold STS-Test SS-Twitter Pooling Layer In this layer, employing a max-over pooling operation (Collobert et al., 2011) on the output from the previous layer for each channel extracts the most salient features. In this way, for each filter, we get the maximum value. So we get features equal to the number of filters in this stage. We chose max pooling instead of other pooling schemes because Zhang and Wallace (2017) showed that max pooling consistently performs better than other pooling strategies for various sentence classification tasks. 2.4 Emotion #Positive 632 182 1, 252 #Negative 1, 402 177 1, 037 #Neutral − 139 1, 953 (b) Table 1: (a) Basic statistics of the emotion datasets used in our experiments. (b) Basic statistics of sentiment labeled datasets used in our experiments. 3.1 Datasets We used a number of emotion and sentiment datasets for our experiments. A description of each dataset is given below: BTD. Big Twitter Data is an emotion-labeled Twitter dataset provided by Wang et al. (2012). The"
P19-1030,D18-2029,0,0.0412307,"Missing"
P19-1030,D17-1070,0,0.137955,"in the trees and which type of attention is suitable for doing the recursive traversal are provided. 1 Introduction Following the breakthrough in NLP research with word embeddings by Mikolov et al. (2013), recent research has focused on sentence representations. Having good sentence representations can help accomplish many NLP tasks because we eventually deal with sentences, e.g., question answering, sentiment analysis, semantic similarity, and natural language inference. Most of the existing task specific sequential sentence encoders are based on recurrent neural nets such as LSTMs or GRUs (Conneau et al., 2017; Lin et al., 2017; Liu et al., 2016). All of these works follow a common paradigm: use an LSTM/GRU over the word sequence, extract contextual features at each time step, and apply some kind of pooling on top of that. However, a few 316 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 316–322 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics output A&apos; tanh A B α α PCNN PCNN K K Linear Linear Attention Block C A D E F tanh G Multi Head Attention B&apos; C&apos; tanh Attention Block Attention Block Input B (a) Tree to"
P19-1030,C04-1051,0,0.0372238,"ropout 0.1. During training, the model parameters are updated using the Adagrad algorithm (Duchi et al., 2011) with a fixed learning rate of 0.0002. We trained our model on an Nvidia GeForce GTX 1080 GPU and used PyTorch 0.4 for the implementation under the Linux environment. Datasets: Evaluation is done on four tasks: the Stanford Sentiment Treebank (SST) (Socher et al., 2011b) for sentiment analysis, Sentences Involving Compositional Knowledge (SICK) (Marelli et al., 2014) for semantic relatedness (-R) and natural language inference (-E), and the Microsoft Research Paraphrase (MSRP) corpus (Dolan et al., 2004) for paraphrase identification. The samples in the SST dataset are labelled for both the binary and the 5-class classification task. In this work we are using only the binary classification labels. The MSRP dataset is labelled with two classes. The samples in the SICK dataset are labelled for both the 3-class SICK-E classification where dk is the dimension of the key. As we are interested in n branches, n copies are created for each (Q, K, V), converted to a 3D tensor, and then a scaled dot-product attention is applied using Bi = LayerNorm(Bi Wib + Bi ) × κi (8) αi PCNN(Bi ) (7) i=1 318 Types"
P19-1030,D16-1244,0,0.0927795,"Missing"
P19-1030,W18-5452,0,0.0306556,"B (a) Tree to traverse D E (b) Attention block used C F G (c) Traversal technique Figure 1: Attention over the tree structure better or at least on par with the existing sequential (i.e., LSTM and Transformer) and tree structured (i.e., Tree LSTM and RvNN) models. the hidden and cell states of a parent are dependent only on the hidden and cell states of its children. Recently, Shen et al. (2018) propose a ParsingReading-Predict Network (PRPN) which can induce syntactic structure automatically from an unannotated corpus and can learn a better language model with that induced structure. Later, Htut et al. (2018) test this PRPN under various configurations and datasets and further verified its empirical success for neural network latent tree learning. Williams et al. (2018) also validate the effectiveness of two latent tree based models but found some issues such as being biased towards producing shallow trees, inconsistencies during negation handling, and a tendency to consider the last two words of a sentence as constituents. In this paper, we propose a novel recursive neural network architecture consisting of a decomposable attention framework in every branch. We call this model Tree Transformer as"
P19-1030,D14-1162,0,0.0875633,"ese subspace semantics by doing a linearly weighted summation (see Eqn. 7) where α ∈ Rn is learned as a model parameter. BranchAttn(Q, K, V) = n X Experiments In this section, we present the effectiveness of our Tree Transformer model by reporting its evaluation on four NLP tasks. We present a detailed ablation study on whether positional encoding is important for trees and also demonstrate which attention module is most suitable as a composition function for the recursive architectures. Experimental Setup: We initialize the word embedding layer weights with GloVe 300dimensional word vectors (Pennington et al., 2014). These embedding weights are not updated during training. In the multi-head attention block, the dimension of the query, key and value matrices are set to 50 and we use 6 parallel heads on each input. The multi-branch attention block is composed of 6 position-wise convolutional layers. The number of branches is also set to 6. We use two layers of convolutional neural network as the composition function for the PCNN layer. The first layer uses 341 1d kernels with no dropout and the second layer uses 300 1d kernels with dropout 0.1. During training, the model parameters are updated using the Ad"
P19-1030,J81-4005,0,0.754107,"Missing"
P19-1030,D12-1110,0,0.344698,"Missing"
P19-1030,Q14-1017,0,0.208278,"ead attention framework (Vaswani et al., 2017) which further uses scaled dot-product attention (Parikh et al., 2016) as the building block. It operates on a query Q, key K and value V as follows   QKT Attention(Q, K, V) = softmax √ V (3) dk ˜ = EwS(tanh((˜ P x + x)W + b)) ˜ depict the input and output of the Here, x and x attention module. 3 Bi = Attention(Qi WiQ , Ki WiK , Vi WiV ) (4) where i ∈ [1, n] and the Wi ’s are the parameters that are learned. Note that WiQ , WiK and WiV ∈ Rdm ×dk . Instead of having separate parameters for the transformation of leaves, internal nodes and parents (Socher et al., 2014), we keep WiQ , WiK and WiV the same for all these components. We then project each of the resultant tensors into different semantic sub-spaces and employ a residual connection (He et al., 2016; Srivastava et al., 2015) around them. Lastly, we normalize the resultant outputs using a layer normalization block (Ba et al., 2016) and apply a scaling factor κ to get the branch representation. All of these are summarized in Eqn. 5. (5) Here, Wib ∈ Rn×dv ×dm and κ ∈ Rn are the parameters to be learned. Note that we choose dk = dq = dv = dm /n. Following this, we take each of these B’s and apply a con"
P19-1030,P14-5010,0,0.0103375,"Missing"
P19-1030,D13-1170,0,0.0324095,"le takes both the child and parent representations as input and produces weighted attentive copies of them. For constituency trees, as the parent vector is entirely dependent on the upward propagation, the attention module works only with the child representations. Our extensive evaluation proves that our model is 2 Proposed Model Our model is designed to address the following general problem. Given a dependency or constituency tree structure, the task is to traverse every subtree within it attentively and infer the root representation as a vector. Our idea is inspired by the RvNN models from Socher et al. (2013, 2011b, 2014) where a composition function is used to transform a set of child representations into one single parent representation. In this section, we describe how we use the attention module as a composition function to build our Tree Transformer. Figure 1 gives a sketch of our model. A dependency tree contains a word at every node. To traverse a subtree in a dependency tree, we look at both the parent and child representations (Xd in Eqn. 1). In contrast, in a constituency tree, only leaf nodes contain words. The nonterminal vectors are calculated only after traversing each subtree. Cons"
P19-1030,P15-1150,0,0.210726,"Missing"
P19-1030,Q18-1019,0,0.0394429,"Missing"
P19-1030,N16-1174,0,0.0559742,"uter Science, University of Western Ontario {mahme255, msamee, rmercer}@uwo.ca Abstract works adopt some different methods. Kiros et al. (2015) propose a skip-gram-like objective function at the sentence level to obtain the sentence embeddings. Logeswaran and Lee (2018) reformulate the task of predicting the next sentence given the current one into a classification problem where instead of a decoder they use a classifier to predict the next sentence from a set of candidates. The attention mechanism adopted by most of the RNN based models require access to the hidden states at every time step (Yang et al., 2016; Kumar et al., 2016). These models are inefficient and at the same time very hard to parallelize. To overcome this, Parikh et al. (2016) propose a fully attention-based neural network which can adequately model the word dependencies and at the same time is parallelizable. Vaswani et al. (2017) adopt the multi-head version in both the encoder and decoder of their Transformer model along with positional encoding. Ahmed et al. (2017) propose a multi-branch attention framework where each branch captures a different semantic subspace and the model learns to combine them during training. Cer et al."
P91-1022,P91-1023,0,0.552533,"Missing"
P91-1022,J90-2002,1,0.441755,"Missing"
P91-1022,C88-1016,0,0.0197394,"Missing"
P91-1022,C90-3031,0,\N,Missing
P91-1034,J90-2002,1,0.358499,"Missing"
P91-1034,C88-1016,0,0.0309117,"Missing"
P93-1005,P91-1027,0,0.0106549,"Missing"
P93-1005,H90-1055,1,0.820077,"Missing"
P93-1005,A88-1019,0,0.0913946,"Missing"
P93-1005,H90-1056,0,0.0309426,"Missing"
P93-1005,H90-1052,0,0.0403796,"Missing"
P93-1005,E91-1004,0,0.436285,"Missing"
P93-1005,H90-1054,0,0.105119,"Missing"
P93-1005,H91-1067,0,\N,Missing
P93-1005,J92-4003,1,\N,Missing
P93-1005,1991.iwpt-1.22,1,\N,Missing
W14-2103,W09-3742,0,0.066835,"Missing"
W14-2103,C12-1074,1,0.894724,"Missing"
W14-2103,W08-0607,0,0.0169459,"has been investigated in previous works. Many schemes have been used and currently no standard classification scheme has been agreed upon. Teufel et al. (1999) use a classification scheme termed Argumentative Zoning (AZ) to model the rhetorical and argumentative aspects of scientific writing in order to easily detect the different claims that are mentioned in a scientific research paper. AZ has been modified for the annotation of biology articles (Yoko et al., 2006) and chemistry articles (Teufel et al., 2009). Scientific discourse has also been studied in terms of speculation and modality by Kilicoglu and Bergler (2008) and Medlock and Briscoe (2007). Also, Shatkay et al. (2008) and Wilbur et al. (2006) have proposed an annotation scheme that categorizes sentences according to various dimensions such as focus, polarity and certainty. Many annotation units have also be proposed in previous studies. Sentence level annotation is used in Teufel et al. (1999) whereas de Waard et al. (2009) used a multi-dimensional scheme for the annotation of biomedical events (bio-events) in texts. Liakata et al. (2012) attempt to classify sentences into the Core Scientific Concept (CoreSC) scheme. This classification scheme con"
W14-2103,W03-1017,0,0.132047,"Missing"
W14-2103,P03-1054,0,0.00573899,"some categories; for example, citations are more frequently used in Introduction than in Results. All numbers were replaced by a unique symbol #NuMBeR. Stop words were not removed since certain stop words are also more likely to be associated with certain IMRaD categories. Words that refer to a figure or table are not removed, since such references are more likely to occur in sentences indicating the outcome of the study. We also used verb tense features as some categories may be associated with the presence of the present tense or the past tense in the sentence. We used the Stanford parser (Klein and Manning, 2003) to identify these tenses. 1. We have developed a DNA microarray-based method for measuring transcript length . . . This method, called the Virtual Northern, is a complementary approach . . . 2. Interestingly, Drice the downstream caspase activated . . . was not affected by inhibition of Dronc and Dredd. This result, . . . suggests that some other mechanism activates Drice. 3. We obtained a long-range PCR product from the latter interval, that appeared to encompass the breakpoint on chromosome 2 . . . This conclusion, however , was regarded with caution , since . . . 3.1.2 Self-annotation In o"
W14-2103,P07-1125,0,0.020572,"works. Many schemes have been used and currently no standard classification scheme has been agreed upon. Teufel et al. (1999) use a classification scheme termed Argumentative Zoning (AZ) to model the rhetorical and argumentative aspects of scientific writing in order to easily detect the different claims that are mentioned in a scientific research paper. AZ has been modified for the annotation of biology articles (Yoko et al., 2006) and chemistry articles (Teufel et al., 2009). Scientific discourse has also been studied in terms of speculation and modality by Kilicoglu and Bergler (2008) and Medlock and Briscoe (2007). Also, Shatkay et al. (2008) and Wilbur et al. (2006) have proposed an annotation scheme that categorizes sentences according to various dimensions such as focus, polarity and certainty. Many annotation units have also be proposed in previous studies. Sentence level annotation is used in Teufel et al. (1999) whereas de Waard et al. (2009) used a multi-dimensional scheme for the annotation of biomedical events (bio-events) in texts. Liakata et al. (2012) attempt to classify sentences into the Core Scientific Concept (CoreSC) scheme. This classification scheme consists of a number of categories"
W14-2103,E99-1015,0,0.459249,"Missing"
W14-2103,D09-1155,0,0.0369322,"2 Related Work The classification of sentences from scientific research papers into different categories has been investigated in previous works. Many schemes have been used and currently no standard classification scheme has been agreed upon. Teufel et al. (1999) use a classification scheme termed Argumentative Zoning (AZ) to model the rhetorical and argumentative aspects of scientific writing in order to easily detect the different claims that are mentioned in a scientific research paper. AZ has been modified for the annotation of biology articles (Yoko et al., 2006) and chemistry articles (Teufel et al., 2009). Scientific discourse has also been studied in terms of speculation and modality by Kilicoglu and Bergler (2008) and Medlock and Briscoe (2007). Also, Shatkay et al. (2008) and Wilbur et al. (2006) have proposed an annotation scheme that categorizes sentences according to various dimensions such as focus, polarity and certainty. Many annotation units have also be proposed in previous studies. Sentence level annotation is used in Teufel et al. (1999) whereas de Waard et al. (2009) used a multi-dimensional scheme for the annotation of biomedical events (bio-events) in texts. Liakata et al. (201"
W14-2113,J02-4002,0,0.414767,"Missing"
W14-2114,D07-1010,0,0.0346624,"elation is used to interpret all the higher order relations derived from it. Parsing an explicit discourse relation involves three steps: identifying the explicit discourse connective, the arguments and the sense. In (Faiz and Mercer, 2013) we showed how to use syntactic and surface level context to achieve a state-ofthe-art result for identifying discourse connectives from text. Our work on a complete explicit discourse relation parser is presented in (Faiz, 2012). For identifying the arguments of discourse connectives we use the head-based representation proposed by Wellner and Pustejovsky (Wellner and Pustejovsky, 2007). We found that this head-based Introduction We use the term higher order relation to denote a relation that relates two biomedical relations. Consider, for example, the following sentence: (1) Aspirin appeared to prevent VCAM-1 transcription, since it dose-dependently inhibited induction of VCAM-1 mRNA by TNF. We can find two biomedical relations involving Aspirin: Aspirin–prevents–VCAM-1 transcription and Aspirin–inhibits–induction of VCAM-1 mRNA. These two relations are connected by the word since. The higher order relation conveys a causal sense, which indicates that the latter relation ca"
W14-2624,W12-3704,0,0.034531,"Missing"
W14-2624,C04-1200,0,0.0960075,"formed the greedy method in terms of accuracy and kappa statistics. Other statistics-based algorithms are also developed to measure text similarity, e.g., the use of the Latent Dirichlet Allocation (LDA) model (Rus et al., 2013). 2.2 Sentiment Analysis Sentiment analysis is meant to determine the polarity of a certain text, which can be positive, negative or neutral. Related academia and industries have been extensively investigating sentiment analysis methods over the last decade. While most of the early work in sentiment analysis is aimed at analyzing the polarity of customer reviews (e.g., Kim and Hovy, 2004; Hu and Liu, 2004; Turney, 2002), there is a proliferation in analyzing social media text (e.g., Balahur, 2013; Liebrecht et al., 2013; Bakliwal et al., 2012; Montejo-Raez et al., 2012) and online discussions (e.g., Sood et al., 2012a, 2012b). Researchers have used a variety of approaches to detect the sentiment polarity of the given text. For example, in Kim and Hovy's system (2004) the sentiment region of the opinion is identified based on the extracted opinion holders and topics. The system combines the sentiments of the sentiment region and the polarity of the words to determine the polar"
W14-2624,W13-1605,0,0.0593225,"Missing"
W14-2624,N10-1120,0,0.0291836,"positive and high negative) or modification labels (negator, intensifier or diminisher), and then applied Support Vector Machine Sequential Minimal Optimization (SVM SMO) to classify three different data sets. Online discussions may have inappropriate use of language in some cases, which affects the online community management negatively. Sood et al. (2012a) proposed a multistep classifier by combining valence analysis and a SVM to detect insults and classify the insult object. Researchers have also looked at the use of dependency tree-based method for sentiment classification. For instance, Nakagawa et al. (2010) used a probabilistic model of the information garnered from the dependency tree to determine the sentiment of a sentence. Rentoumi et al. (2010) combines word sense disambiguation, a rule-based system, and Hidden Markov Models (HMMs) to deal with figurative language (e.g. record-shattering day) in sentiment analysis. Moilanen and Pulman (2007) presented a compositional model for three-class (positive, negative, and neutral) phrase-level and sentence-level sentiment analysis. In their algorithm, each binary combination of a Head and Complement had a rule that determined which of the Head and C"
W14-2624,W12-2018,0,0.0156934,"a word-to-word similarity library was developed by Pedersen et al. (2004), and has been widely used to compute the similarity at a coarser granularity (e.g., sentence-to-sentence similarity). Various methods to deal with text similarity have been proposed over the past decades. Mihalcea et al. (2006) proposed a greedy method to calculate the similarity score between two texts T1 and T2. Basically for each word in T1 (T2), the maximum similarity score to any word in T2 (T1) is used. The WordNet similarity can be used for assigning similarity scores between every pair of words in the two texts. Rus and Lintean (2012) proposed an optimal method to compute text similarity based on word-to-word similarity. It is similar to the optimal assignment problem. Given a weighted complete bipartite graph (G = X È Y; X × Y), with weight w(xy) on edge xy, we need to find a matching from X to Y with a maximum total weight. Their results showed that the optimal method outperformed the greedy method in terms of accuracy and kappa statistics. Other statistics-based algorithms are also developed to measure text similarity, e.g., the use of the Latent Dirichlet Allocation (LDA) model (Rus et al., 2013). 2.2 Sentiment Analysi"
W14-2624,P13-4028,0,0.153786,"in the two texts. Rus and Lintean (2012) proposed an optimal method to compute text similarity based on word-to-word similarity. It is similar to the optimal assignment problem. Given a weighted complete bipartite graph (G = X È Y; X × Y), with weight w(xy) on edge xy, we need to find a matching from X to Y with a maximum total weight. Their results showed that the optimal method outperformed the greedy method in terms of accuracy and kappa statistics. Other statistics-based algorithms are also developed to measure text similarity, e.g., the use of the Latent Dirichlet Allocation (LDA) model (Rus et al., 2013). 2.2 Sentiment Analysis Sentiment analysis is meant to determine the polarity of a certain text, which can be positive, negative or neutral. Related academia and industries have been extensively investigating sentiment analysis methods over the last decade. While most of the early work in sentiment analysis is aimed at analyzing the polarity of customer reviews (e.g., Kim and Hovy, 2004; Hu and Liu, 2004; Turney, 2002), there is a proliferation in analyzing social media text (e.g., Balahur, 2013; Liebrecht et al., 2013; Bakliwal et al., 2012; Montejo-Raez et al., 2012) and online discussions"
W14-2624,D13-1170,0,0.00551399,"ment of a sentence. Rentoumi et al. (2010) combines word sense disambiguation, a rule-based system, and Hidden Markov Models (HMMs) to deal with figurative language (e.g. record-shattering day) in sentiment analysis. Moilanen and Pulman (2007) presented a compositional model for three-class (positive, negative, and neutral) phrase-level and sentence-level sentiment analysis. In their algorithm, each binary combination of a Head and Complement had a rule that determined which of the Head and Complement polarities dominated. In exceptional cases the rule inverts the polarity of the subordinate. Socher et al. (2013) developed a Recursive Neural Tensor Network (RNTN) model. The authors showed that the accuracy obtained by RNTN outperformed a standard recursive neural network (RNN), matrix-vector RNN (MV-RNN), Naive Bayes (NB) and SVM. The advantage of 148 RNTN is especially evident when compared with the methods that only use bag of words (NB and SVM). This indicates the importance of using parse trees during sentiment analysis. 3 A Method for Identifying Representative Rationales in Online Deliberations Our observation of the Wikipedia AfD forum suggests that one topic (e.g., notability) can appear multi"
W14-2624,W12-3703,0,0.0631826,"Missing"
W14-2624,rentoumi-etal-2010-united,0,\N,Missing
W14-2624,W13-1617,0,\N,Missing
W14-2624,P02-1053,0,\N,Missing
W15-2706,P03-1054,0,0.0144659,"nsider the following positive instance from the RST corpus: [When Mr. Gandhi came to power,] [ he ushered in new rules for business]circumstance When collecting negative instances, it was revealed that this instance was embedded in ten negative instances. However, since when is in fact functioning as a circumstance cue in all of them, those ten instances could not qualify as negative instances and so were excluded. 3.3.2 • Shortest path representation: root advmod We used the OpenNLP3 toolkit to tokenize and POS tag the instances and the Stanford dependency parser to generate the parse trees (Klein and Manning, 2003). 3.3.3 Graph Modeling We encoded the syntactic information of the instances in graph models. We build the directed weighted graph G = (V, E), w, where: • V is the set of all possible tokens that may appear in the representations. For example, for the POS representations, V is the union of the set of all POS tags and the cue set. • E = V × V is the set of all possible ordered transitions between any two tokens. • w → [0 − 1] is a weighting function that assigns a probability value to an edge (i, j), which represents the probability of a transition from token i to token j. Given a set of syntac"
W15-2706,W14-2107,0,0.0606052,"Missing"
W15-2706,W01-1605,0,0.0884274,"Computer Science University of Western Ontario University of Western Ontario University of Western Ontario London, ON, Canada London, ON, Canada London, ON, Canada tkhazae@uwo.ca lxiao24@uwo.ca mercer@uwo.ca Abstract Structure Theory (RST) (Mann and Thompson, 1988). In RST, discourse structure has a form of a tree, where the leaves correspond to elementary discourse units, and the internal nodes correspond to contiguous text spans. Each internal node is marked with a rhetorical relation that holds between its child nodes. Figure 1 provides an example of an RST tree taken from the RST corpus (Carlson et al., 2001). One of the notable differences of RST with other similar theories is that it is structured on the intentions of the writers to use those relations (Taboada, 2006). This distinctive feature can make it even more difficult to build models for automatic identification of rhetorical relations in the context of RST. Rhetorical relations can be either explicit or implicit. Explicit relations are the ones that are signaled by cues, such as lexical cues, mood, modality, and intonation (Taboada, 2006), while no cue is present in implicit relations. In this study, we are focused on explicit relations"
W15-2706,W14-2106,0,0.0307059,"fferent genres. Our experiments revealed the superiority of encoding such syntactic features in a probabilistic graph compared to their direct usage. This study is our first attempt toward the identification of rationales in text. A rationale is an explanation of the reasons underlying decisions, conclusions, and interpretations. Prior studies on rationale articulation and sharing suggest that it contributes to quality control, knowledge management, and knowledge reuse (Xiao, 2014; Xiao, 2013b). However, there exists only a few automated methods to identify rationales from illstructured text (Ghosh et al., 2014; Boltuˇzi´c and ˇ Snajder, 2014). Our future research efforts are focused on the development of algorithms to extract lightly-signaled and implicit relations and to further explore the potential and limitations of using rhetorical relations in the detection of rationales. Table 5: Classification results on the RST corpus when the syntactic features are used directly to train a model on the SFU corpus among the four cues with lowest F-score in the SFU dataset (see Table 2). This finding could be attributed to the fact that, for these three cues, the corresponding datasets were among the smalle"
W15-2706,J00-3005,0,0.114607,"se units that are connected through discourse relations, which are also referred to as rhetorical relations. Despite the efforts to build robust theoretical foundations and taxonomies for such relations (Hobbs, 1990; Knott and Sanders, 1998; Lascarides and Asher, 1993; Mann and Thompson, 1988), current methods for their automatic analysis and discovery in written discourse have yet to improve. However, providing robust models to analyze and identify rhetorical relations can benefit various research directions in computational linguistics such as text generation (Hovy, 1993) and summarization (Marcu, 2000), and machine translation (Meyer et al., 2011). One of the widely accepted frameworks for discourse analysis and understanding is Rhetorical Figure 1: An example sentence parsed in the form of RST 54 Proceedings of the EMNLP 2015 Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pages 54–63, c Lisboa, Portugal, 18 September 2015. 2015 Association for Computational Linguistics. 2 Since this study is part of a larger project to identify rationales in written discourse, we focus on the three relations of CIRCUMSTANCE, EVAL UATION , and ELABORATION that are commonly"
W15-2706,D10-1121,0,0.0291949,"hting function that assigns a probability value to an edge (i, j), which represents the probability of a transition from token i to token j. Given a set of syntactic representations, the probability of a transition from token i to token j is calculated following a maximum likelihood estimation. Thus, the probability is calculated by dividing the number of times that token i is immediately followed by token j by the number of times that token i itself appears in the set. This method of building the graphs is similar to language modeling but is conducted over a set of syntactic representations (Hassan et al., 2010). For every kind of representation, we build one graph based on the set of positive instances, and one based on the set of negative instances. As a result, given a cue (e.g., when) and its corresponding relation (e.g., CIRCUMSTANCE), we build four graph Syntactic Representations After creation of the documents, each instance is processed and transformed into two different representations, capturing the syntactic features of the instance. To create the first syntactic representation, words in instances are replaced with their corresponding Part-Of-Speech (POS) tags, while the cue itself is kept"
W15-2706,W11-2022,0,0.0179132,"course relations, which are also referred to as rhetorical relations. Despite the efforts to build robust theoretical foundations and taxonomies for such relations (Hobbs, 1990; Knott and Sanders, 1998; Lascarides and Asher, 1993; Mann and Thompson, 1988), current methods for their automatic analysis and discovery in written discourse have yet to improve. However, providing robust models to analyze and identify rhetorical relations can benefit various research directions in computational linguistics such as text generation (Hovy, 1993) and summarization (Marcu, 2000), and machine translation (Meyer et al., 2011). One of the widely accepted frameworks for discourse analysis and understanding is Rhetorical Figure 1: An example sentence parsed in the form of RST 54 Proceedings of the EMNLP 2015 Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pages 54–63, c Lisboa, Portugal, 18 September 2015. 2015 Association for Computational Linguistics. 2 Since this study is part of a larger project to identify rationales in written discourse, we focus on the three relations of CIRCUMSTANCE, EVAL UATION , and ELABORATION that are commonly present in rationales (Xiao, 2013a). With the"
W15-2706,J93-3003,0,0.498073,"ve feature can make it even more difficult to build models for automatic identification of rhetorical relations in the context of RST. Rhetorical relations can be either explicit or implicit. Explicit relations are the ones that are signaled by cues, such as lexical cues, mood, modality, and intonation (Taboada, 2006), while no cue is present in implicit relations. In this study, we are focused on explicit relations in written text that are signaled by the presence of lexical cues. Lexical cues are defined as linguistic expressions that function as explicit indicators of a discourse relation (Hirschberg and Litman, 1993). For example, in the sentence provided in Figure 1, but and because can be considered lexical cues signaling the existence of the CONCESSION relation and the EXPLANATION - ARGUMENTATIVE relation, respectively. Lexical cues are linguistic expressions that can signal the presence of a rhetorical relation. However, such cues can be ambiguous as they may signal more than one relation or may not always function as a relation indicator. In this study, we first conduct a corpus-based analysis to derive a set of n-grams as potential lexical cues. These cues are then utilized in graph-based probabilis"
W15-2706,P09-2004,0,0.053685,"Missing"
W15-2706,prasad-etal-2008-penn,0,0.338624,"Missing"
W15-2706,P14-1002,0,0.0413474,"Missing"
W15-2706,J14-4007,0,0.0255887,"Missing"
W15-2706,P13-1048,0,0.0499408,"Missing"
W15-2706,N03-1030,0,0.187537,"Missing"
W15-2706,taboada-etal-2006-methods,0,0.0201277,"extracted from the RST corpus are applied to the SFU review dataset to identify the corresponding relation. The F-score of each n-gram is then calculated independently. Finally, the ngrams with an F-score of above 0.1 are selected as potential lexical cues. The aforementioned procedure resulted in the selection of seven lexical cues for the CIRCUMSTANCE relation: When, after, on, before, with, out, as. Approach In this section, we first describe the two RST annotated corpora that are used in the present work: RST corpus (Carlson et al., 2001) and Simon Fraser University (SFU) review dataset (Taboada et al., 2006). Then, an explanation of the approach used to extract a set of key n-grams as potential lexical cues is presented, which is followed by a description of our graph-based approach to disambiguate lexical cues. 3.1 Corpora We used two human-annotated corpora as our underlying datasets for the experiments: the RST corpus (Carlson et al., 2001) and the SFU review dataset (Taboada et al., 2006). Both corpora are annotated in the RST framework and are constructed using the RSTTool1 . The RST corpus, which has been made available by the Linguistic Data Consortium over the years, includes 385 Wall Str"
W15-2706,W13-0123,0,0.0301175,"Missing"
W15-2706,W06-1317,0,0.158614,"Missing"
W19-4514,W18-5206,0,0.0657286,"Missing"
W19-4514,J05-1004,0,0.108851,"that are being sold (goods), the person who sells the goods (seller), and the currency that the buyer and seller agree on (money). Following Fillmore’s theory of frame semantics, FrameNet (Baker et al., 1998) was developed to create an online lexical resource for English. This framework includes more than 170,000 manually annotated sentences and 10,000 words. The computational linguistics community has been attracted to the concept of frame semantics and has developed computational resources using this concept, such as VerbNet (Schuler, 2005), an on-line verb lexicon for English and PropBank (Palmer et al., 2005), an annotated corpus with basic semantic propositions. Following the notion of frame semantics, we propose to build a knowledge representation framework to analyze verbs in a procedureoriented genre. Our concept of procedurally rhetorical verb-centric frame semantics is intended to address this lack of a formal framework by developing a computationally feasible knowledge representation that will enable argumentation analysis. The knowledge contained in the frame semantics will facilitate the extraction of elements of arguments, i.e., argumentation mining. To reiterate, our hypothesis is that"
W19-4514,W14-2102,0,0.125323,"een able to rely on relatively simple forms of information extraction. Although these approaches fulfil some information needs, more in-depth and comprehensive information contained in biomedical texts would be highly valuable to scientists. This type of information can enable validating scientific claims, tracing current research directions, reproducing scientific procedures, and so forth. Recently, a new and more challenging information extraction task has been introduced as a means of obtaining this type of information: identifying the argumentation structure in biomedical articles (e.g., (Green, 2014, 2015)). The essence of argumentation can be considered as influencing others to gain their adherence to a particular idea (Perelman and Olbrechts-Tyteca, 1973). Arguments have an explicit logical structure, for example, claims that are backed with reasons, which in turn are supported by evidence, This paper focuses on the real world application of scientific writing and on determining rhetorical moves, an important step in establishing the argument structure of biomedical articles. Using the observation that the structure of scholarly writing in laboratory-based experimental sciences closely"
W19-4514,W15-0502,0,0.0197495,"relatively few biomedical related corpora annotated with argumentation structures currently exist for use in training or evaluating Machine Learning classifiers.4 This has encouraged researchers to begin developing annotated corpora for use by the Computational Argumentation community ((Green, 2014, 2015), in particular). Green (2014) proposed a plan for creating an annotated corpus of biomedical genetics research articles. Importantly, in justifying the need for such a corpus, Green strongly argued for domain knowledge as a requisite of argumentation recognition in the experimental sciences. Green (2015) specified a set of argumentation schemes for scientific claims in genetics research articles. The author used a corpus of unannotated genetics research articles, and identified the components (e.g., premises, conclusions) of an argument as well as its type of scheme. Overall, the author’s ultimate goal for this initial study was to develop annotation guidelines for creating corpora for argumentation mining research. None of these previous approaches to automated argumentation analysis and mining provided a formal knowledge representation that could be used in detecting and recognizing argumen"
W19-4514,E99-1015,0,0.487189,"Missing"
W19-4514,J02-4002,0,0.27831,".nih.gov/books/NBK3827/ 113 Proceedings of the 6th Workshop on Argument Mining, pages 113–123 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics leading to conclusions (Toulmin, 2003). Argumentation analysis is the recognition and identification of the different forms of argumentative structures in texts. Various studies have used recurrent patterns of text organization called rhetorical moves (i.e., text segments that are rhetorical and perform specific communicative goals) to analyze argumentative organization of texts manually (Swales, 1990) or automatically (Teufel and Moens, 2002). Swales’ CARS model targets the Introduction section3 of scientific articles. Teufel’s interests are concentrated on rhetorical moves associated with defining the research space and suggesting the knowledge claims for computational linguistics and chemistry articles (Teufel, 2010). Kanoksilapatham (2003) adds to these works by providing the first comprehensive set of rhetorical moves for complete biochemistry articles. With our long-term goal being analyzing argumentation in biochemistry articles, our mid-term research goal is to provide a computational model for Kanoksilapatham’s descriptive"
W19-4514,D09-1155,0,0.0739155,"Missing"
W19-4514,thompson-etal-2008-building,0,0.20625,"memory, experience, action or object’ by Fillmore (1977), in other words, coherently structured concepts that are related to each other to represent a complete knowledge of world events or experiences. For example, to un5 http://www.ces.fau.edu/nasa/introduction/scientificinquiry/why-do-scientists-argue-and-challenge-each-othersresults.php 116 event is different from our definition of an experimental event. On the one hand, a bio-event is concerned with detection of bio-molecular events within the biomedical literature, such as the identification of events that are related to given proteins (Thompson et al., 2008). In our case, an experimental event is concerned with processes and procedures that are used to investigate biological events. The experimental event is also concerned with the recognition of the biochemist’s reasoning of standard biochemical procedures such as using certain instruments or specific biological materials. Our annotation scheme consists of two tiers of information. A rhetorical move is on the sentence or clause level while semantic role is on the word or phrase level. The following subsections describe these two tiers of information. Annotators are allowed to select the text spa"
W19-4514,walker-etal-2012-corpus,0,0.0290806,"m, 2003) (Swales, 1990)), argumentative zoning (Teufel et al., 1999), and epistemic topoi (Gladkova, 2011), lacked a formal knowledge representation which could be used 3 Procedurally Rhetorical Verb-centric Frame Semantics In this research we will work on the biochemistry domain to develop a formal knowledge representation, procedurally rhetorical verb-centric frame semantics, that can be used for in-depth argumen4 We note, however, increasing attention to this concern, with the design of such corpora as The Internet Argument Corpus (IAC) for research in political debates on internet forums (Walker et al., 2012) and the Dr. Inventor MultiLayer Scientific Corpus (DRI) for computer graphics articles (Lauscher et al., 2018). 115 tation analysis, is computationally feasible to implement, and will enable argumentation mining of more-detailed scientific knowledge than is currently available. This will be an important step towards providing researchers in Computational Argumentation working in domains with similar discourse structure with a means of using and evaluating the metrics we will develop. To the best of our knowledge, no research has proposed or incorporated the idea of a semantic frame based on v"
W19-5018,W18-5306,0,0.354519,"or ranking and the other for predicting the number of related labels. MeSHLabeler (Liu et al., 2015) won first place in 2014. It also has two components: MeSHRanker and MeSHNumber. MeSHRanker returns a ranked list of candidate MeSH terms. MeSHNumber predicts the number of output MeSH terms. DeepMeSH (Peng et al., 2016) was the best system in 2017. It incorporates deep semantic information into MeSHLabeler using a dense semantic representation for documents, namely document to vectors (D2V). In addition, DeepMeSH has a second classifier to find the number of MeSH terms returned. AttentionMeSH (Jin et al., 2018), also proposed in 2017, uses a bi-direction recurrent gated unit (BiGRU) architecture to capture contextual features, and attention mechanisms to select MeSH terms from the candidate list. Rios and Kavuluru (2015) used a convolutional neural network (CNN) to classify the 29 most frequent MeSH terms on a small dataset comprised of 9,000 citations. Gargiulo et al. (2018) applied deep CNN on the abstracts and titles of 1,115,090 articles. Besides deep learning approaches, other Currently MeSH term indexing is performed by a large number of human annotators, who review full text documents and ass"
W19-5018,D14-1181,0,0.00800011,"A document is composed of n words. We use d-dimensional word embeddings to represent the words. The word embedding matrix e for each document is then e ∈ Rd×n . For each document, we have two texts: the abstract and title, and the captions and paragraphs. These two texts are represented by two embedding matrices, namely eAT , the word embedding matrix for the abstract and title text, and eCP , the word embedding matrix for the captions and paragraphs. The model structure, shown in Figure 1, is a variant convolutional neural network (CNN) with multichannel inputs, which is inspired by TextCNN (Kim, 2014). We have chosen the CNNbased model because it has been successful in various text classification tasks. For each channel, the architecture is similar to TextCNN. The representation of abstract and title, eAT , is input to one channel. The representation of captions and paragraphs, eCP , is input to the other. We also use a single channel architecture by concatenating these two representations as input to one of the channels. The model learns feature representations by passing embedded documents to the convolutional layer. The entire input document in each channel can be represented as e1:n ="
W19-5018,W19-5018,1,0.0529494,"ed with multichannel XMLCNN, which is inspired by XMLCNN (Liu et al., 2017), a variant CNN model developed for extreme multi-label classification; multichannel biLSTM, a bidirectional long short term memory neural network (Schuster and Paliwal, 1997) with multichannel inputs; and multichannel attention based convLSTM, which is a stacked CNN and LSTM followed by an attention layer. The experimental results indicate that the multichannel TextCNN model performed best among all of the models mentioned above in both execution time and evaluation metrics. Details of these experiments are available (Wang, 2019). L H(q) = − 1X yi · log(σ(yi )) + L 3.3 i=1 Setup and Model Hyper-parameters In our proposed multichannel TextCNN model, we used rectified linear units (ReLU) as the activation function, convolutional filter windows of size 3, 4, and 5 with 128 filters each, dropout rate of 0.5, 512 hidden units in the bottleneck layer, batch size of 10, and learning rate of 0.001. The number of epochs in training is 20. These hyper-parameters are fixed across the different (1 − yi ) · log(1 − σ(yi )) where σ is the sigmoid function, L is the total number of labels, yi is the original label of document i, and"
W91-0220,C88-2086,1,0.897428,"Missing"
