2021.nodalida-main.4,Large-Scale Contextualised Language Modelling for {N}orwegian,2021,-1,-1,5,0.0756899,2619,andrey kutuzov,Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa),0,"We present the ongoing NorLM initiative to support the creation and use of very large contextualised language models for Norwegian (and in principle other Nordic languages), including a ready-to-use software environment, as well as an experience report for data preparation and training. This paper introduces the first large-scale monolingual language models for Norwegian, based on both the ELMo and BERT frameworks. In addition to detailing the training process, we present contrastive benchmark results on a suite of NLP tasks for Norwegian. For additional background and access to the data, models, and software, please see: http://norlm.nlpl.eu"
2021.acl-long.263,Structured Sentiment Analysis as Dependency Graph Parsing,2021,-1,-1,3,0,2620,jeremy barnes,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Structured sentiment analysis attempts to extract full opinion tuples from a text, but over time this task has been subdivided into smaller and smaller sub-tasks, e.g., target extraction or targeted polarity classification. We argue that this division has become counterproductive and propose a new unified framework to remedy the situation. We cast the structured sentiment problem as dependency graph parsing, where the nodes are spans of sentiment holders, targets and expressions, and the arcs are the relations between them. We perform experiments on five datasets in four languages (English, Norwegian, Basque, and Catalan) and show that this approach leads to strong improvements over state-of-the-art baselines. Our analysis shows that refining the sentiment graphs with syntactic dependency information further improves results."
2020.lrec-1.234,A Tale of Three Parsers: Towards Diagnostic Evaluation for Meaning Representation Parsing,2020,-1,-1,3,0,17079,maja buljan,Proceedings of the 12th Language Resources and Evaluation Conference,0,"We discuss methodological choices in contrastive and diagnostic evaluation in meaning representation parsing, i.e. mapping from natural language utterances to graph-based encodings of its semantic structure. Drawing inspiration from earlier work in syntactic dependency parsing, we transfer and refine several quantitative diagnosis techniques for use in the context of the 2019 shared task on Meaning Representation Parsing (MRP). As in parsing proper, moving evaluation from simple rooted trees to general graphs brings along its own range of challenges. Specifically, we seek to begin to shed light on relative strenghts and weaknesses in different broad families of parsing techniques. In addition to these theoretical reflections, we conduct a pilot experiment on a selection of top-performing MRP systems and one of the five meaning representation frameworks in the shared task. Empirical results suggest that the proposed methodology can be meaningfully applied to parsing into graph-structured target representations, uncovering hitherto unknown properties of the different systems that can inform future development and cross-fertilization across approaches."
2020.iwpt-1.3,End-to-End Negation Resolution as Graph Parsing,2020,-1,-1,2,0.882353,2683,robin kurtz,Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task on Parsing into Enhanced Universal Dependencies,0,"We present a neural end-to-end architecture for negation resolution based on a formulation of the task as a graph parsing problem. Our approach allows for the straightforward inclusion of many types of graph-structured features without the need for representation-specific heuristics. In our experiments, we specifically gauge the usefulness of syntactic information for negation resolution. Despite the conceptual simplicity of our architecture, we achieve state-of-the-art results on the Conan Doyle benchmark dataset, including a new top result for our best model."
2020.conll-shared.1,{MRP} 2020: The Second Shared Task on Cross-Framework and Cross-Lingual Meaning Representation Parsing,2020,-1,-1,1,1,2623,stephan oepen,Proceedings of the CoNLL 2020 Shared Task: Cross-Framework Meaning Representation Parsing,0,"The 2020 Shared Task at the Conference for Computational Language Learning (CoNLL) was devoted to Meaning Representation Parsing (MRP) across frameworks and languages. Extending a similar setup from the previous year, five distinct approaches to the representation of sentence meaning in the form of directed graphs were represented in the English training and evaluation data for the task, packaged in a uniform graph abstraction and serialization; for four of these representation frameworks, additional training and evaluation data was provided for one additional language per framework. The task received submissions from eight teams, of which two do not participate in the official ranking because they arrived after the closing deadline or made use of additional training data. All technical information regarding the task, including system submissions, official results, and links to supporting resources and software are available from the task web site at: http://mrp.nlpl.eu"
2020.conll-shared.2,{DRS} at {MRP} 2020: Dressing up Discourse Representation Structures as Graphs,2020,-1,-1,3,0,14527,lasha abzianidze,Proceedings of the CoNLL 2020 Shared Task: Cross-Framework Meaning Representation Parsing,0,"Discourse Representation Theory (DRT) is a formal account for representing the meaning of natural language discourse. Meaning in DRT is modeled via a Discourse Representation Structure (DRS), a meaning representation with a model-theoretic interpretation, which is usually depicted as nested boxes. In contrast, a directed labeled graph is a common data structure used to encode semantics of natural language texts. The paper describes the procedure of dressing up DRSs as directed labeled graphs to include DRT as a new framework in the 2020 shared task on Cross-Framework and Cross-Lingual Meaning Representation Parsing. Since one of the goals of the shared task is to encourage unified models for several semantic graph frameworks, the conversion procedure was biased towards making the DRT graph framework somewhat similar to other graph-based meaning representation frameworks."
P19-4002,Graph-Based Meaning Representations: Design and Processing,2019,0,0,2,0,987,alexander koller,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts,0,This tutorial is on representing and processing sentence meaning in the form of labeled directed graphs. The tutorial will (a) briefly review relevant background in formal and linguistic semantics; (b) semi-formally define a unified abstract view on different flavors of semantic graphs and associated terminology; (c) survey common frameworks for graph-based meaning representation and available graph banks; and (d) offer a technical overview of a representative selection of different parsing approaches.
K19-2001,{MRP} 2019: Cross-Framework Meaning Representation Parsing,2019,0,4,1,1,2623,stephan oepen,Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 Conference on Natural Language Learning,0,"The 2019 Shared Task at the Conference for Computational Language Learning (CoNLL) was devoted to Meaning Representation Parsing (MRP) across frameworks. Five distinct approaches to the representation of sentence meaning in the form of directed graph were represented in the training and evaluation data for the task, packaged in a uniform abstract graph representation and serialization. The task received submissions from eighteen teams, of which five do not participate in the official ranking because they arrived after the closing deadline, made use of additional training data, or involved one of the task co-organizers. All technical information regarding the task, including system submissions, official results, and links to supporting resources and software are available from the task web site at: http://mrp.nlpl.eu"
K19-2003,The {ERG} at {MRP} 2019: Radically Compositional Semantic Dependencies,2019,-1,-1,1,1,2623,stephan oepen,Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 Conference on Natural Language Learning,0,"The English Resource Grammar (ERG) is a broad-coverage computational grammar of English that outputs underspecified logical-form representations of meaning in a framework dubbed English Resource Semantics (ERS). Two of the target representations in the the 2019 Shared Task on Cross-Framework Meaning Representation Parsing (MRP 2019) derive graph-based simplifications of ERS, viz. Elementary Dependency Structures (EDS) and DELPH-IN MRS Bi-Lexical Dependencies (DM). As a point of reference outside the official MRP competition, we parsed the evaluation strings using the ERG and converted the resulting meaning representations to EDS and DM. These graphs yield higher evaluation scores than the purely data-driven parsers in the actual shared task, suggesting that the general-purpose linguistic knowledge about English grammar encoded in the ERG can add value when parsing into these meaning representations."
K18-2002,The 2018 Shared Task on Extrinsic Parser Evaluation: On the Downstream Utility of {E}nglish {U}niversal {D}ependency Parsers,2018,0,6,2,1,30331,murhaf fares,Proceedings of the {C}o{NLL} 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies,0,"We summarize empirical results and tentative conclusions from the Second Extrinsic Parser Evaluation Initiative (EPE 2018). We review the basic task setup, downstream applications involved, and end-to-end results for seventeen participating teams. Based on in-depth quantitative and qualitative analysis, we correlate intrinsic evaluation results at different layers of morph-syntactic analysis with observed downstream behavior."
D18-1178,Transfer and Multi-Task Learning for Noun{--}Noun Compound Interpretation,2018,30,0,2,1,30331,murhaf fares,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"In this paper, we empirically evaluate the utility of transfer and multi-task learning on a challenging semantic classification task: semantic interpretation of noun{--}noun compounds. Through a comprehensive series of experiments and in-depth error analysis, we show that transfer learning via parameter initialization and multi-task learning via parameter sharing can help a neural classification model generalize over a highly skewed distribution of relations. Further, we demonstrate how dual annotation with two distinct sets of relations over the same set of compounds can be exploited to improve the overall accuracy of a neural classifier and its F1 scores on the less frequent, but more difficult relations."
W17-0808,"Representation and Interchange of Linguistic Annotation. An In-Depth, Side-by-Side Comparison of Three Designs",2017,10,2,4,0,11279,richard castilho,Proceedings of the 11th Linguistic Annotation Workshop,0,"For decades, most self-respecting linguistic engineering initiatives have designed and implemented custom representations for various layers of, for example, morphological, syntactic, and semantic analysis. Despite occasional efforts at harmonization or even standardization, our field today is blessed with a multitude of ways of encoding and exchanging linguistic annotations of these types, both at the levels of {`}abstract syntax{'}, naming choices, and of course file formats. To a large degree, it is possible to work within and across design plurality by conversion, and often there may be good reasons for divergent design reflecting differences in use. However, it is likely that some abstract commonalities across choices of representation are obscured by more superficial differences, and conversely there is no obvious procedure to tease apart what actually constitute contentful vs. mere technical divergences. In this study, we seek to conceptually align three representations for common types of morpho-syntactic analysis, pinpoint what in our view constitute contentful differences, and reflect on the underlying principles and specific requirements that led to individual choices. We expect that a more in-depth understanding of these choices across designs may led to increased harmonization, or at least to more informed design of future representations."
W17-0237,"Word vectors, reuse, and replicability: Towards a community repository of large-text resources",2017,0,19,3,1,30331,murhaf fares,Proceedings of the 21st Nordic Conference on Computational Linguistics,0,None
L16-1630,Towards Comparability of Linguistic Graph {B}anks for Semantic Parsing,2016,20,7,1,1,2623,stephan oepen,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We announce a new language resource for research on semantic parsing, a large, carefully curated collection of semantic dependency graphs representing multiple linguistic traditions. This resource is called SDP{\textasciitilde}2016 and provides an update and extension to previous versions used as Semantic Dependency Parsing target representations in the 2014 and 2015 Semantic Evaluation Exercises. For a common core of English text, this third edition comprises semantic dependency graphs from four distinct frameworks, packaged in a unified abstract format and aligned at the sentence and token levels. SDP 2016 is the first general release of this resource and available for licensing from the Linguistic Data Consortium in May 2016. The data is accompanied by an open-source SDP utility toolkit and system results from previous contrastive parsing evaluations against these target representations."
K16-2002,"{OPT}: {O}slo{--}{P}otsdam{--}{T}eesside. Pipelining Rules, Rankers, and Classifier Ensembles for Shallow Discourse Parsing",2016,15,2,1,1,2623,stephan oepen,Proceedings of the {C}o{NLL}-16 shared task,0,"The OPT submission to the Shared Task of the 2016 Conference on Natural Language Learning (CoNLL) implements a xe2x80x98classicxe2x80x99 pipeline architecture, combining binary classification of (candidate) explicit connectives, heuristic rules for non-explicit discourse relations, ranking and xe2x80x98editingxe2x80x99 of syntactic constituents for argument identification, and an ensemble of classifiers to assign discourse senses. With an end-toend performance of 27.77 F1 on the English xe2x80x98blindxe2x80x99 test data, our system advances the previous state of the art (Wang & Lan, 2015) by close to four F1 points, with particularly good results for the argument identification sub-tasks. OPT system results appear more competitive on the new, xe2x80x98blindxe2x80x99 test data than on the xe2x80x98testxe2x80x99 and xe2x80x98developmentxe2x80x99 sections of the Penn Discourse Treebank (PDTB; Prasad et al., 2008), which may indicate reduced over-fitting to specific properties of the venerable Wall Street Journal (WSJ) text underlying the PDTB."
J16-4009,{S}quibs: Towards a Catalogue of Linguistic Graph {B}anks,2016,29,12,2,0,12072,marco kuhlmann,Computational Linguistics,0,"Graphs exceeding the formal complexity of rooted trees are of growing relevance to much NLP research. Although formally well understood in graph theory, there is substantial variation in the types of linguistic graphs, as well as in the interpretation of various structural properties. To provide a common terminology and transparent statistics across different collections of graphs in NLP, we propose to establish a shared community resource with an open-source reference implementation for common statistics."
W15-0126,Semantic Dependency Graph Parsing Using Tree Approximations,2015,23,3,3,0,21438,vzeljko agic,Proceedings of the 11th International Conference on Computational Semantics,0,"In this contribution, we deal with graph parsing, i.e., mapping input strings to graph-structured output representations, using tree approximations. We experiment with the data from the SemEval 2014 Semantic Dependency Parsing (SDP) task. We define various tree approximation schemes for graphs, and make twofold use of them. First, we statically analyze the semantic dependency graphs, seeking to unscover which linguistic phenomena in particular require the additional annotation expressivity provided by moving from trees to graphs. We focus on undirected base cycles in the SDP graphs, and discover strong connections to grammatical control and coordination. Second, we make use of the approximations in a statistical parsing scenario. In it, we convert the training set graphs to dependency trees, and use the resulting treebanks to build standard dependency tree parsers. We perform lossy graph reconstructions on parser outputs, and evaluate our models as dependency graph parsers. Our system outperforms the baselines by a large margin, and evaluates as the best non-voting tree approximationxe2x80x93based parser on the SemEval 2014 data, scoring at just over 81% in labeled F1."
W15-0128,Layers of Interpretation: On Grammar and Compositionality,2015,39,14,3,0,11448,emily bender,Proceedings of the 11th International Conference on Computational Semantics,0,"With the recent resurgence of interest in semantic annotation of corpora for improved semantic parsing, we observe a tendency which we view as ill-advised, to conflate sentence meaning and speaker meaning into a single mapping, whether done by annotators or by a parser. We argue instead for the more traditional hypothesis that sentence meaning, but not speaker meaning, is compositional, and accordingly that NLP systems would benefit from reusable, automatically derivable, taskindependent semantic representations which target sentence meaning, in order to capture exactly the information in the linguistic signal itself. We further argue that compositional construction of such sentence meaning representations affords better consistency, more comprehensiveness, greater scalability, and less duplication of effort for each new NLP application. For concreteness, we describe one well-tested grammar-based method for producing sentence meaning representations which is efficient for annotators, and which exhibits many of the above benefits. We then report on a small inter-annotator agreement study to quantify the consistency of semantic representations produced via this grammar-based method."
S15-2153,{S}em{E}val 2015 Task 18: Broad-Coverage Semantic Dependency Parsing,2015,-1,-1,1,1,2623,stephan oepen,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,None
S14-2008,{S}em{E}val 2014 Task 8: Broad-Coverage Semantic Dependency Parsing,2014,30,71,1,1,2623,stephan oepen,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"Task 18 at SemEval 2015 defines Broad-Coverage Semantic Dependency Parsing (SDP) as the problem of recovering sentence-internal predicatexe2x80x93argument relationships for all content words, i.e. the sema ..."
S14-2056,In-House: An Ensemble of Pre-Existing Off-the-Shelf Parsers,2014,22,5,2,0,5928,yusuke miyao,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"This submission to the open track of Task 8 at SemEval 2014 seeks to connect the Task to pre-existing, xe2x80x98in-housexe2x80x99 parsing systems for the same types of target semantic dependency graphs."
P14-1007,Simple Negation Scope Resolution through Deep Parsing: A Semantic Solution to a Semantic Problem,2014,18,16,4,0,34074,woodley packard,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"In this work, we revisit Shared Task 1 from the 2012 *SEM Conference: the automated analysis of negation. Unlike the vast majority of participating systems in 2012, our approach works over explicit and formal representations of propositional semantics, i.e. derives the notion of negation scope assumed in this task from the structure of logical-form meaning representations. We relate the task-specific interpretation of (negation) scope to the concept of (quantifier and operator) scope in mainstream underspecified semantics. With reference to an explicit encoding of semantic predicate-argument structure, we can operationalize the annotation decisions made for the 2012 *SEM task, and demonstrate how a comparatively simple system for negation scope resolution can be built from an off-the-shelf deep parsing system. In a system combination setting, our approach improves over the best published results on this task to date."
kouylekov-oepen-2014-semantic,Semantic Technologies for Querying Linguistic Annotations: An Experiment Focusing on Graph-Structured Data,2014,9,4,2,0,29816,milen kouylekov,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"With growing interest in the creation and search of linguistic annotations that form general graphs (in contrast to formally simpler, rooted trees), there also is an increased need for infrastructures that support the exploration of such representations, for example logical-form meaning representations or semantic dependency graphs. In this work, we heavily lean on semantic technologies and in particular the data model of the Resource Description Framework (RDF) to represent, store, and efficiently query very large collections of text annotated with graph-structured representations of sentence meaning."
flickinger-etal-2014-towards,Towards an Encyclopedia of Compositional Semantics: Documenting the Interface of the {E}nglish {R}esource {G}rammar,2014,16,9,3,0,26293,dan flickinger,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"We motivate and describe the design and development of an emerging encyclopedia of compositional semantics, pursuing three objectives. We first seek to compile a comprehensive catalogue of interoperable semantic analyses, i.e., a precise characterization of meaning representations for a broad range of common semantic phenomena. Second, we operationalize the discovery of semantic phenomena and their definition in terms of what we call their semantic fingerprint, a formal account of the building blocks of meaning representation involved and their configuration. Third, we ground our work in a carefully constructed semantic test suite of minimal exemplars for each phenomenon, along with a `target{'} fingerprint that enables automated regression testing. We work towards these objectives by codifying and documenting the body of knowledge that has been constructed in a long-term collaborative effort, the development of the LinGO English Resource Grammar. Documentation of its semantic interface is a prerequisite to use by non-experts of the grammar and the analyses it produces, but this effort also advances our own understanding of relevant interactions among phenomena, as well as of areas for future work in the grammar."
lapponi-etal-2014-road,Off-Road {LAF}: Encoding and Processing Annotations in {NLP} Workflows,2014,9,2,3,1,32118,emanuele lapponi,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"The Linguistic Annotation Framework (LAF) provides an abstract data model for specifying interchange representations to ensure interoperability among different annotation formats. This paper describes an ongoing effort to adapt the LAF data model as the interchange representation in complex workflows as used in the Language Analysis Portal (LAP), an on-line and large-scale processing service that is developed as part of the Norwegian branch of the Common Language Resources and Technology Infrastructure (CLARIN) initiative. Unlike several related on-line processing environments, which predominantly instantiate a distributed architecture of web services, LAP achives scalability to potentially very large data volumes through integration with the Norwegian national e-Infrastructure, and in particular job sumission to a capacity compute cluster. This setup leads to tighter integration requirements and also calls for efficient, low-overhead communication of (intermediate) processing results with workflows. We meet these demands by coupling the LAF data model with a lean, non-redundant JSON-based interchange format and integration of an agile and performant NoSQL database, allowing parallel access from cluster nodes, as the central repository of linguistic annotation."
C14-2020,{RDF} Triple Stores and a Custom {SPARQL} Front-End for Indexing and Searching (Very) Large Semantic Networks,2014,8,1,2,0,29816,milen kouylekov,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: System Demonstrations",0,"With growing interest in the creation and search of linguistic annotations that form general graphs (in contrast to formally simpler, rooted trees), there also is an increased need for infrastructures that support the exploration of such representations, for example logical-form meaning representations or semantic dependency graphs. In this work, we lean heavily on semantic technologies and in particular the data model of the Resource Description Framework (RDF) to represent, store, and efficiently query very large collections of text annotated with graph-structured representations of sentence meaning. Our full infrastructure is available under open-source licensing, and through this system demonstration we hope to receive feedback on the general approach, explore its application to additional types of meaning representation, and attract new users and possibly co-developers."
W13-5707,"On Different Approaches to Syntactic Analysis Into Bi-Lexical Dependencies. An Empirical Comparison of Direct, {PCFG}-Based, and {HPSG}-Based Parsers",2013,0,3,2,1,35341,angelina ivanova,Proceedings of the 13th International Conference on Parsing Technologies ({IWPT} 2013),0,None
W13-5715,Document Parsing: Towards Realistic Syntactic Analysis,2013,12,2,2,1,39167,rebecca dridan,Proceedings of the 13th International Conference on Parsing Technologies ({IWPT} 2013),0,None
W13-5605,Tidying up the Basement: A Tale of Large-Scale Parsing on National e{I}nfrastructure,2013,0,0,1,1,2623,stephan oepen,Proceedings of the 19th Nordic Conference of Computational Linguistics ({NODALIDA} 2013),0,"Until about six years ago, our research group used non-trivial amounts of project funds and researcher time on maintaining a dedicated server farm in the basement of our department. Rack space and cooling (just as much as funds and time) were in short supply, and we never quite got around to implementing automated load balancing across compute nodes, tuning the Linux kernel and filesystem for optimum performance, or connecting to the uninterruptible power supply. When pointed to the Norwegian National High-Performance Computing Initiative, we were intially doubtful that Natural Language Processing should be among their target user groups. Also, we were a tad hesitant to give up control of our own equipment and of course worried we would miss what we thought were our fancy toys. Today, any member of the group can access thousands of cpus simultaneously, we have about five terabytes of project data on-line, and our research has scaled to dataset sizes and turn-around times that would be just inconceivable on group-local hardwarexe2x80x94at no charge to our project funds and no administrator responsibilities. For example, xe2x80x98deepxe2x80x99 semantic parsing of the about 900 million words of the English Wikipedia we can typically complete in less than one day (while expending what would be about eight sequential years of computation). Or, when searching for the best-performing features and hyper-parameters in a machine learning problem, we can explore a large xe2x80x98gridxe2x80x99 of possible configurations in parallel, without much need for a staged, partly manual, xe2x80x98coarseto-finexe2x80x99 search strategy. Access to the very large-scale Norwegian National eInfrastructure and its high-quality technical support have enabled a comparatively computation-heavy research profile of our group and has thus contributed to its international competitiveness. In this presentation, I will review some of our experiences in establishing a dialogue with the HPC crowd and propose HPC for the Masses as a candidate vision in the on-going development trend towards more and more large-scale computational sciences."
W13-5633,Simple and Accountable Segmentation of Marked-up Text,2013,9,0,3,1,35001,jonathon read,Proceedings of the 19th Nordic Conference of Computational Linguistics ({NODALIDA} 2013),0,"Segmenting documents into discrete, sentence-like units is usually a first step in any natural language processing pipeline. However, current segmentation tools perform poorly on text that contains markup. While stripping markup is a simple solution, we argue for the utility of the extra-linguistic information encoded by markup and present a scheme for normalising markup across disparate formats. We further argue for the need to maintain accountability when preprocessing text, such that a record of modifications to source documents is maintained. Such records are necessary in order to augment documents with information derived from subsequent processing. To facilitate adoption of these principles we present a novel tool for segmenting text that contains inline markup. By converting to plain text and tracking alignment, the tool is capable of state-of-the-art sentence boundary detection using any external segmenter, while producing segments containing normalised markup, with an account of how to recreate the original form."
W13-5642,{HPC}-ready Language Analysis for Human Beings,2013,-1,-1,4,1,32118,emanuele lapponi,Proceedings of the 19th Nordic Conference of Computational Linguistics ({NODALIDA} 2013),0,None
P13-3005,Survey on parsing three dependency representations for {E}nglish,2013,20,8,2,1,35341,angelina ivanova,51st Annual Meeting of the Association for Computational Linguistics Proceedings of the Student Research Workshop,0,"In this paper we focus on practical issues of data representation for dependency parsing. We carry out an experimental comparison of (a) three syntactic dependency schemes; (b) three data-driven dependency parsers; and (c) the influence of two different approaches to lexical category disambiguation (aka tagging) prior to parsing. Comparing parsing accuracies in various setups, we study the interactions of these three aspects and analyze which configurations are easier to learn for a dependency parser."
W12-3602,Who Did What to Whom? A Contrastive Study of Syntacto-Semantic Dependencies,2012,32,39,2,1,35341,angelina ivanova,Proceedings of the Sixth Linguistic Annotation Workshop,0,"We investigate aspects of interoperability between a broad range of common annotation schemes for syntacto-semantic dependencies. With the practical goal of making the LinGO Redwoods Treebank accessible to broader usage, we contrast seven distinct annotation schemes of functor--argument structure, both in terms of syntactic and semantic relations. Drawing examples from a multi-annotated gold standard, we show how abstractly similar information can take quite different forms across frameworks. We further seek to shed light on the representational 'distance' between pure bilexical dependencies, on the one hand, and full-blown logical-form propositional semantics, on the other hand. Furthermore, we propose a fully automated conversion procedure from (logical-form) meaning representation to bilexical semantic dependencies."
W12-3210,Towards an {ACL} {A}nthology Corpus with Logical Document Structure. An Overview of the {ACL} 2012 Contributed Task,2012,43,6,3,0,42237,ulrich schafer,Proceedings of the {ACL}-2012 Special Workshop on Rediscovering 50 Years of Discoveries,0,"The ACL 2012 Contributed Task is a community effort aiming to provide the full ACL Anthology as a high-quality corpus with rich markup, following the TEI P5 guidelines---a new resource dubbed the ACL Anthology Corpus (AAC). The goal of the task is threefold: (a) to provide a shared resource for experimentation on scientific text; (b) to serve as a basis for advanced search over the ACL Anthology, based on textual content and citations; and, by combining the aforementioned goals, (c) to present a showcase of the benefits of natural language processing to a broader audience. The Contributed Task extends the current Anthology Reference Corpus (ARC) both in size, quality, and by aiming to provide tools that allow the corpus to be automatically extended with new content---be they scanned or born-digital."
W12-3211,Towards High-Quality Text Stream Extraction from {PDF}. Technical Background to the {ACL} 2012 Contributed Task,2012,5,7,2,0,42239,oyvind berg,Proceedings of the {ACL}-2012 Special Workshop on Rediscovering 50 Years of Discoveries,0,"Extracting textual content and document structure from PDF presents a surprisingly (depressingly, to some, in fact) difficult challenge, owing to the purely display-oriented design of the PDF document standard. While a variety of lower-level PDF extraction toolkits exist, none fully support the recovery of original text (in reading order) and relevant structural elements, even for so-called borndigital PDFs, i.e. those prepared electronically using typesetting systems like LATEX, OpenOffice, and the like. This short paper summarizes a new tool for high-quality extraction of text and structure from PDFs, combining state-of-the-art PDF parsing, font interpretation, layout analysis, and TEI-compliant output of text and logical document markup."
S12-1041,{U}i{O}1: Constituent-Based Discriminative Ranking for Negation Resolution,2012,8,14,4,1,35001,jonathon read,"*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",0,"This paper describes the first of two systems submitted from the University of Oslo (UiO) to the 2012 *SEM Shared Task on resolving negation. Our submission is an adaption of the negation system of Velldal et al. (2012), which combines SVM cue classification with SVM-based ranking of syntactic constituents for scope resolution. The approach further extends our prior work in that we also identify factual negated events. While submitted for the closed track, the system was the top performer in the shared task overall."
P12-2074,"{T}okenization: Returning to a Long Solved Problem {---} A Survey, Contrastive Experiment, Recommendations, and Toolkit {---}",2012,9,20,2,1,39167,rebecca dridan,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We examine some of the frequently disregarded subtleties of tokenization in Penn Treebank style, and present a new rule-based preprocessing toolkit that not only reproduces the Treebank tokenization with unmatched accuracy, but also maintains exact stand-off pointers to the original text and allows flexible configuration to diverse use cases (e.g. to genre-or domain-specific idiosyncrasies)."
read-etal-2012-wesearch,"The {W}e{S}earch Corpus, Treebank, and Treecache {--} A Comprehensive Sample of User-Generated Content",2012,16,8,4,1,35001,jonathon read,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"We present the WeSearch Data Collection (WDC)âa freely redistributable, partly annotated, comprehensive sample of User-Generated Content. The WDC contains data extracted from a range of genres of varying formality (user forums, product review sites, blogs and Wikipedia) and covers two different domains (NLP and Linux). In this article, we describe the data selection and extraction process, with a focus on the extraction of linguistic content from different sources. We present the format of syntacto-semantic annotations found in this resource and present initial parsing results for these data, as well as some reflections following a first round of treebanking."
J12-2005,"Speculation and Negation: Rules, Rankers, and the Role of Syntax",2012,47,52,4,0.981477,2621,erik velldal,Computational Linguistics,0,"This article explores a combination of deep and shallow approaches to the problem of resolving the scope of speculation and negation within a sentence, specifically in the domain of biomedical research literature. The first part of the article focuses on speculation. After first showing how speculation cues can be accurately identified using a very simple classifier informed only by local lexical context, we go on to explore two different syntactic approaches to resolving the in-sentence scopes of these cues. Whereas one uses manually crafted rules operating over dependency structures, the other automatically learns a discriminative ranking function over nodes in constituent trees. We provide an in-depth error analysis and discussion of various linguistic properties characterizing the problem, and show that although both approaches perform well in isolation, even better results can be obtained by combining them, yielding the best published results to date on the CoNLL-2010 Shared Task data. The last part of the article describes how our speculation system is ported to also resolve the scope of negation. With only modest modifications to the initial design, the system obtains state-of-the-art results on this task also."
C12-2096,Sentence Boundary Detection: A Long Solved Problem?,2012,17,29,3,1,35001,jonathon read,Proceedings of {COLING} 2012: Posters,0,"We review the state of the art in automated sentence boundary detection (SBD) for English and call for a renewed research interest in this foundational first step in natural language processing. We observe severe limitations in comparability and reproducibility of earlier work and a general lack of knowledge about genre- and domain-specific variations. To overcome these barriers, we conduct a systematic empirical survey of a large number of extant approaches, across a broad range of diverse corpora. We further observe that much previous work interpreted the SBD task too narrowly, leading to overly optimistic estimates of SBD performance on running text. To better relate SBD to practical NLP use cases, we thus propose a generalized definition of the task, eliminating text- or language-specific assumptions about candidate boundary points. More specifically, we quantify degrees of variation across xe2x80x98standardxe2x80x99 corpora of edited, relatively formal language, as well as performance degradation when moving to less formal language, viz. various samples of user-generated Web content. For these latter types of text, we demonstrate how moderate interpretation of document structure (as is now often available more or less explicitly through mark-up) can substantially contribute to overall SBD performance."
W11-2927,Parser Evaluation Using Elementary Dependency Matching,2011,19,12,2,1,39167,rebecca dridan,Proceedings of the 12th International Conference on Parsing Technologies,0,"We present a perspective on parser evaluation in a context where the goal of parsing is to extract meaning from a sentence. Using this perspective, we show why current parser evaluation metrics are not suitable for evaluating parsers that produce logical-form semantics and present an evaluation metric that is suitable, analysing some of the characteristics of this new metric."
I11-1028,{T}reeblazing: Using External Treebanks to Filter Parse Forests for Parse Selection and Treebanking,2011,27,5,4,0,32906,andrew mackinlay,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"We describe xe2x80x9ctreeblazingxe2x80x9d, a method of using annotations from the GENIA treebank to constrain a parse forest from an HPSG parser. Combining this with self-training, we show significant dependency score improvements in a task of adaptation to the biomedical domain, reducing error rate by 9% compared to out-of-domain gold data and 6% compared to self-training. We also demonstrate improvements in treebanking efficiency, requiring 25% fewer decisions, and 17% less annotation time."
D11-1037,Parser Evaluation over Local and Non-Local Deep Dependencies in a Large Corpus,2011,36,25,3,0.287832,11448,emily bender,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,"In order to obtain a fine-grained evaluation of parser accuracy over naturally occurring text, we study 100 examples each of ten reasonably frequent linguistic phenomena, randomly selected from a parsed version of the English Wikipedia. We construct a corresponding set of gold-standard target dependencies for these 1000 sentences, operationalize mappings to these targets from seven state-of-the-art parsers, and evaluate the parsers against this data to measure their level of success in identifying these dependencies."
W10-3007,Resolving Speculation: {M}ax{E}nt Cue Classification and Dependency-Based Scope Rules,2010,20,29,3,1,2621,erik velldal,Proceedings of the Fourteenth Conference on Computational Natural Language Learning {--} Shared Task,0,"This paper describes a hybrid, two-level approach for resolving hedge cues, the problem of the CoNLL-2010 shared task. First, a maximum entropy classifier is applied to identify cue words, using both syntactic- and surface-oriented features. Second, a set of manually crafted rules, operating on dependency representations and the output of the classifier, is applied to resolve the scope of the hedge cues within the sentence."
flickinger-etal-2010-wikiwoods,{W}iki{W}oods: Syntacto-Semantic Annotation for {E}nglish {W}ikipedia,2010,10,37,2,0.517124,26293,dan flickinger,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"WikiWoods is an ongoing initiative to provide rich syntacto-semantic annotations for English Wikipedia. We sketch an automated processing pipeline to extract relevant textual content from Wikipedia sources, segment documents into sentence-like units, parse and disambiguate using a broad-coverage precision grammar, and support the export of syntactic and semantic information in various formats. The full parsed corpus is accompanied by a subset of Wikipedia articles for which gold-standard annotations in the same format were produced manually. This subset was selected to represent a coherent domain, Wikipedia entries on the broad topic of Natural Language Processing."
C10-1155,Syntactic Scope Resolution in Uncertainty Analysis,2010,21,32,3,0.729167,2622,lilja ovrelid,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"We show how the use of syntactic structure enables the resolution of hedge scope in a hybrid, two-stage approach to uncertainty analysis. In the first stage, a Maximum Entropy classifier, combining surface-oriented and syntactic features, identifies cue words. With a small set of hand-crafted rules operating over dependency representations in stage two, we attain the best overall result (in terms of both combined ranks and average F1) in the 2010 CoNLL Shared Task."
W09-1204,Hybrid Multilingual Parsing with {HPSG} for {SRL},2009,18,8,3,0.236779,3425,yi zhang,Proceedings of the Thirteenth Conference on Computational Natural Language Learning ({C}o{NLL} 2009): Shared Task,0,"In this paper we present our syntactic and semantic dependency parsing system submitted to both the closed and open challenges of the CoNLL 2009 Shared Task. The system extends the system of Zhang, Wang, & Uszkoreit (2008) in the multilingual direction, and achieves 76.49 average macro F1 Score on the closed joint task. Substantial improvements to the open SRL task have been observed that are attributed to the HPSG parses with handcrafted grammars."
2009.eamt-1.19,Automatic Translation of {N}orwegian Noun Compounds,2009,13,3,2,0,34205,lars bungum,Proceedings of the 13th Annual conference of the European Association for Machine Translation,0,"This paper discusses the automated translation of Norwegian nominal compounds into English, combining (a) compound segmentation, (b) component translation, (c) bi-lingual translation templates, and (d) probabilistic ranking. In this approach, a Norwegian compound will typically give rise to a large number of possible translations, and the selection of the xe2x80x98rightxe2x80x99 candidate is approaches as an interesting machine learning problem. Our work extends the seminal approach of Tanaka and Baldwin in several ways, including a clarification of some fine points of their earlier work, adaptation to a more adequate machine learning framework, application to a Germanic language with a small speech community and very limited existing resources, and systematic experimentation along several dimensions of variation. 1 Background: The Task"
adolphs-etal-2008-fine,Some Fine Points of Hybrid Natural Language Parsing,2008,25,31,2,0,34902,peter adolphs,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Large-scale grammar-based parsing systems nowadays increasingly rely on independently developed, more specialized components for pre-processing their input. However, different tools make conflicting assumptions about very basic properties such as tokenization. To make linguistic annotation gathered in pre-processing available to ÂdeepÂ parsing, a hybrid NLP system needs to establish a coherent mapping between the two universes. Our basic assumption is that tokens are best described by attribute value matrices (AVMs) that may be arbitrarily complex. We propose a powerful resource-sensitive rewrite formalism, Âchart mappingÂ, that allows us to mediate between the token descriptions delivered by shallow pre-processing components and the input expected by the grammar. We furthermore propose a novel way of unknown word treatment where all generic lexical entries are instantiated that are licensed by a particular token AVM. Again, chart mapping is used to give the grammar writer full control as to which items (e.g. native vs. generic lexical items) enter syntactic parsing. We discuss several further uses of the original idea and report on early experiences with the new machinery."
W07-2207,Efficiency in Unification-Based N-Best Parsing,2007,31,32,2,0.236779,3425,yi zhang,Proceedings of the Tenth International Conference on Parsing Technologies,0,"We extend a recently proposed algorithm for n-best unpacking of parse forests to deal efficiently with (a) Maximum Entropy (ME) parse selection models containing important classes of non-local features, and (b) forests produced by unification grammars containing significant proportions of globally inconsistent analyses. The new algorithm empirically exhibits a linear relationship between processing time and the number of analyses unpacked at all degrees of ME feature non-locality; in addition, compared with agenda-driven best-first parsing and exhaustive parsing with post-hoc parse selection it leads to improved parsing speed, coverage, and accuracy."
W07-1204,Exploiting Semantic Information for {HPSG} Parse Selection,2007,26,28,3,0,44776,sanae fujita,{ACL} 2007 Workshop on Deep Linguistic Processing,0,"In this paper we present a framework for experimentation on parse selection using syntactic and semantic features. Results are given for syntactic features, dependency relations and the use of semantic classes."
2007.tmi-papers.18,Towards hybrid quality-oriented machine translation {--} on linguistics and probabilities in {MT},2007,-1,-1,1,1,2623,stephan oepen,Proceedings of the 11th Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages: Papers,0,None
W06-1661,Statistical Ranking in Tactical Generation,2006,19,28,2,1,2621,erik velldal,Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,0,"In this paper we describe and evaluate several statistical models for the task of realization ranking, i.e. the problem of discriminating between competing surface realizations generated for a given input semantics. Three models (and several variants) are trained and tested: an n-gram language model, a discriminative maximum entropy model using structural information (and incorporating the language model as a separate feature), and finally an SVM ranker trained on the same feature set. The resulting hybrid tactical generator is part of a larger, semantic transfer MT system."
P06-4014,Re-Usable Tools for Precision Machine Translation,2006,14,5,2,1,26476,jan lonning,Proceedings of the {COLING}/{ACL} 2006 Interactive Presentation Sessions,0,"The LOGON MT demonstrator assembles independently valuable general-purpose NLP components into a machine translation pipeline that capitalizes on output quality. The demonstrator embodies an interesting combination of hand-built, symbolic resources and stochastic processes."
oepen-lonning-2006-discriminant,Discriminant-Based {MRS} Banking,2006,15,44,1,1,2623,stephan oepen,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"We present an approach to discriminant-based MRS banking, i.e. the construction of an annotated corpus where each input item is paired with a logical-form semantics. Semantic annotations are produced by parsing with a broad-coverage precision grammar, followed by manual disambiguation. The selection of the preferred analysis for each item (and hence its semantic form) builds on a notion of semantic discriminants, essentially localized dependencies extracted from a full-fledged, underspecified semantic representation."
2006.eamt-1.29,Using a Bi-Lingual Dictionary in Lexical Transfer,2006,-1,-1,4,0,40510,lars nygaard,Proceedings of the 11th Annual conference of the European Association for Machine Translation,0,None
P05-1041,High Precision {T}reebanking{---}{B}lazing Useful Trees Using {POS} Information,2005,17,13,3,0,29830,takaaki tanaka,Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05),1,"In this paper we present a quantitative and qualitative analysis of annotation in the Hinoki treebank of Japanese, and investigate a method of speeding annotation by using part-of-speech tags. The Hinoki treebank is a Redwoods-style treebank of Japanese dictionary definition sentences. 5,000 sentences are annotated by three different annotators and the agreement evaluated. An average agreement of 65.4% was found using strict agreement, and 83.5% using labeled precision. Exploiting POS tags allowed the annotators to choose the best parse with 19.5% fewer decisions."
I05-1015,High Efficiency Realization for a Wide-Coverage Unification Grammar,2005,22,70,2,0,17923,john carroll,Second International Joint Conference on Natural Language Processing: Full Papers,0,"We give a detailed account of an algorithm for efficient tactical generation from underspecified logical-form semantics, using a wide-coverage grammar and a corpus of real-world target utterances. Some earlier claims about chart realization are critically reviewed and corrected in the light of a series of practical experiments. As well as a set of algorithmic refinements, we present two novel techniques: the integration of subsumption-based local ambiguity factoring, and a procedure to selectively unpack the generation forest according to a probability distribution given by a conditional, discriminative model."
2005.mtsummit-papers.15,Maximum Entropy Models for Realization Ranking,2005,18,33,2,1,2621,erik velldal,Proceedings of Machine Translation Summit X: Papers,0,"In this paper we describe and evaluate different statistical models for the task of realization ranking, i.e. the problem of discriminating between competing surface realizations generated for a given input semantics. Three models are trained and tested; an n-gram language model, a discriminative maximum entropy model using structural features, and a combination of these two. Our realization component forms part of a larger, hybrid MT system."
2005.mtsummit-papers.22,{SEM}-{I} Rational {MT}: Enriching Deep Grammars with a Semantic Interface for Scalable Machine Translation,2005,11,15,4,0.753987,26293,dan flickinger,Proceedings of Machine Translation Summit X: Papers,0,"In the LOGON machine translation system where semantic transfer using Minimal Recursion Semantics is being developed in conjunction with two existing broad-coverage grammars of Norwegian and English, we motivate the use of a grammar-specific semantic interface (SEM-I) to facilitate the construction and maintenance of a scalable translation engine. The SEM-I is a theoretically grounded component of each grammar, capturing several classes of lexical regularities while also serving the crucial engineering function of supplying a reliable and complete specification of the elementary predications the grammar can realize. We make extensive use of underspecification and type hierarchies to maximize generality and precision."
2005.mtsummit-osmtw.3,Open Source Machine Translation with {DELPH}-{IN},2005,0,27,2,0,6126,francis bond,Workshop on open-source machine translation,0,The Deep Linguistic Processing with HPSG Initiative (DELPH-IN) provides the infrastructure needed to produce open-source semantic transfer-based...
2005.eamt-1.27,Holistic regression testing for high-quality {MT}: some methodological and technological reflections,2005,-1,-1,1,1,2623,stephan oepen,Proceedings of the 10th EAMT Conference: Practical applications of machine translation,0,None
copestake-etal-2004-lexicon,A Lexicon Module for a Grammar Development Environment,2004,7,11,6,0,6790,ann copestake,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,Past approaches to developing an effective lexicon component in a grammar development environment have suffered from a number of usability and efficiency issues. We present a lexical database module currently in use by a number of grammar development projects. The database module presented addresses issues which have caused problems in the past and the power of a database architecture provides a number of practical advantages as well as a solid framework for future extension.
baldwin-etal-2004-road,Road-testing the {E}nglish {R}esource {G}rammar Over the {B}ritish {N}ational {C}orpus,2004,8,56,5,0,1468,timothy baldwin,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"This paper addresses two questions: (1) when a large deep processing resource developed for relatively closed domains is run over open text, what coverage does it have, and (2) what are the most effective and time-efficient ways of consolidating gaps in the coverage of such as resource?"
2004.tmi-1.2,Som {\\aa} kapp-ete med trollet? {--} Towards {MRS}-based {N}orwegian-{E}nglish machine translation,2004,11,36,1,1,2623,stephan oepen,Proceedings of the 10th Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,"We present a relatively large-scale initiative in high-quality MT based on semantic transfer, reviewing the motivation for this approach, general architecture and components involved, and preliminary experience from a first round of system integration (to be accompanied by a hands-on system demonstration, if appropriate)."
W02-1502,The Grammar Matrix: An Open-Source Starter-Kit for the Rapid Development of Cross-linguistically Consistent Broad-Coverage Precision Grammars,2002,18,154,3,1,11448,emily bender,{COLING}-02: Grammar Engineering and Evaluation,0,"The grammar matrix is an open-source starter-kit for the development of broad-coverage HPSGs. By using a type hierarchy to represent cross-linguistic generalizations and providing compatibility with other open-source tools for grammar engineering, evaluation, parsing and generation, it facilitates not only quick start-up but also rapid growth towards the wide coverage necessary for robust natural language processing and the precision parses and semantic representations necessary for natural language understanding."
W02-1508,Parallel Distributed Grammar Engineering for Practical Applications,2002,16,16,1,1,2623,stephan oepen,{COLING}-02: Grammar Engineering and Evaluation,0,"Based on a detailed case study of parallel grammar development distributed across two sites, we review some of the requirements for regression testing in grammar engineering, summarize our approach to systematic competence and performance profiling, and discuss our experience with grammar development for a commercial application. If possible, the workshop presentation will be organized around a software demonstration."
C02-2025,The {L}in{GO} Redwoods Treebank: Motivation and Preliminary Applications,2002,19,117,1,1,2623,stephan oepen,{COLING} 2002: The 17th International Conference on Computational Linguistics: Project Notes,0,"The LinGO Redwoods initiative is a seed activity in the design and development of a new type of treebank. While several medium- to large-scale treebanks exist for English (and for other major languages), pre-existing publicly available resources exhibit the following limitations: (i) annotation is mono-stratal, either encoding topological (phrase structure) or tectogrammatical (dependency) information, (ii) the depth of linguistic information recorded is comparatively shallow, (iii) the design and format of linguistic representation in the treebank hard-wires a small, predefined range of ways in which information can be extracted from the treebank, and (iv) representations in existing treebanks are static and over the (often year- or decade-long) evolution of a large-scale treebank tend to fall behind the development of the field. LinGO Redwoods aims at the development of a novel treebanking methodology, rich in nature and dynamic both in the ways linguistic data can be retrieved from the treebank in varying granularity and in the constant evolution and regular updating of the treebank itself. Since October 2001, the project is working to build the foundations for this new type of treebank, to develop a basic set of tools for treebank construction and maintenance, and to construct an initial set of 10,000 annotated trees to be distributed together with the tools under an open-source license."
W01-1512,Using an Open-Source Unification-Based System for {CL}/{NLP} Teaching,2001,5,3,5,0,53792,anne copestake,Proceedings of the {ACL} 2001 Workshop on Sharing Tools and Resources,0,We demonstrate the open-source LKB system which has been used to teach the fundamentals of constraint-based grammar development to several groups of students.
W00-1601,Efficient Large-Scale Parsing {--} a Survey,2000,21,1,2,0,17923,john carroll,Proceedings of the {COLING}-2000 Workshop on Efficiency In Large-Scale Parsing Systems,0,We survey work on the empirical assessment and comparison of the efficiency of large-scale parsing systems. We focus on (1) grammars and data used to assess parser efficiency; (2) methods and tools for empirical assessment of parser efficiency; and (3) comparisons of the efficiency of different large-scale parsing systems.
W00-1607,"Cross-Platform, Cross-Grammar Comparison {--} Can it be Done?",2000,3,0,2,0,47997,ulrich callmeier,Proceedings of the {COLING}-2000 Workshop on Efficiency In Large-Scale Parsing Systems,0,"This software demonstration reviews recent improvements in comparing large-scale unification-based parsing systems, both across different platforms and multiple grammars. Over the past few years significant progress was accomplished in efficient processing with wide-coverage hpsg grammars. A large number of engineering improvements in current systems were achieved through collaboration of multiple research centers and mutual exchange of experience, encoding techniques, algorithms, and pieces of software.n n We argue for an approach to grammar and system engineering that makes systematic experimentation and the precise empirical study of system properties a focal point in development. Adapting the profiling metaphor familiar from software engineering to constraint-based grammars and parsers enables developers to maintain an accurate record of system evolution, identify grammar and system deficiencies quickly, and compare to earlier versions, among analytically varied configurations, or between different systems. We demonstrate a suite of integrated software packages facilitating this approach, which are publicly available both separately and together.n n The [incr tsdb()] profiling environment (Oepen & Carroll, 2000) integrates empirical assessment and systematic progress evaluation into the development cycle for grammars and processing systems; it enables developers to obtain an accurate snapshot of current system behaviour (a profile) with minimal effort. Profiles can then be analysed and visualized at variable granularity, reflecting various aspects of system competence and performance, and compared to earlier results. Since the [incr tsdb()] package has been integrated with some eight processing platforms by now, it has greatly facilitated cross-fertilization between various research groups and implementations.n n Pet is a platform for experimentation with processing techniques and the implementation of efficient processors for unification-based grammars (Callmeier, 2000). It synthesizes a range of techniques for efficient processing from earlier systems into a modular C implementation, supplying building blocks (such as various unifiers) from which a large number of experimental setups can be configured. A parser built from pet components can be used as a time- and memory-efficient run-time system for grammars developed in the lkb system distributed by CSLI Stanford (Copestake & Flickinger, 2000). In daily grammar development it allows frequent, rapid regression tests.n n We emphasize in this demonstration the crucial importance of experimental system comparison, eclectic engineering, and incremental optimization. Only through the careful analysis of a large number of interacting system parameters can one establish reliable points of comparison across different parsers and multiple grammars simultaneously."
A00-2022,Ambiguity Packing in Constraint-based Parsing Practical Results,2000,14,51,1,1,2623,stephan oepen,1st Meeting of the North {A}merican Chapter of the Association for Computational Linguistics,0,"We describe a novel approach to 'packing' of local ambiguity in parsing with a wide-coverage HPSG grammar, and provide an empirical assessment of the interaction between various packing and parsing strategies. We present a linear-time, bidirectional subsumption test for typed feature structures and demonstrate that (a) subsumption- and equivalence-based packing is applicable to large HPSG grammars and (b) average parse complexity can be greatly reduced in bottom-up chart parsing with comprehensive HPSG implementations."
2000.iwpt-1.19,Measure for Measure: Parser Cross-fertilization - Towards Increased Component Comparability and Exchange,2000,19,19,1,1,2623,stephan oepen,Proceedings of the Sixth International Workshop on Parsing Technologies,0,"Over the past few years significant progress was accomplished in efficient processing with wide-coverage HPSG grammars. HPSG-based parsing systems are now available that can process medium-complexity sentences (of ten to twenty words, say) in average parse times equivalent to real (i.e. human reading) time. A large number of engineering improvements in current HPSG systems were achieved through collaboration of multiple research centers and mutual exchange of experience, encoding techniques, algorithms, and even pieces of software. This article presents an approach to grammar and system engineering, termed competence {\&} performance profiling, that makes systematic experimentation and the precise empirical study of system properties a focal point in development. Adapting the profiling metaphor familiar from software engineering to constraint-based grammars and parsers, enables developers to maintain an accurate record of system evolution, identify grammar and system deficiencies quickly, and compare to earlier versions or between different systems. We discuss a number of exemplary problems that motivate the experimental approach, and apply the empirical methodology in a fairly detailed discussion of what was achieved during a development period of three years. Given the collaborative nature in setup, the empirical results we present involve research and achievements of a large group of people."
C96-2120,{TSNLP} - Test Suites for Natural Language Processing,1996,1,110,2,0,41753,sabine lehmann,{COLING} 1996 Volume 2: The 16th International Conference on Computational Linguistics,0,"The growing language technology industry needs measurement tools to allow researchers, engineers, managers, and customers to track development, evaluate and assure quality, and assess suitability for a variety of applications.The TSNLP (Test Suites for Natural Language Processing) project has investigated various aspects of the construction, maintenance and application of systematic test suites as diagnostic and evaluation tools for NLP applications. The paper summarizes the motivation and main results of TSNLP: besides the solid methodological foundation of the project, TSNLP has produced substantial (i.e. larger than any existing general test suites) multi-purpose and multi-user test suites for three European languages together with a set of specialized tools that facilitate the construction, extension, maintenance, retrieval, and customization of the test data.The publicly available results of TSNLP represent a valuable linguistic resource that has the potential of providing a wide-spread pre-standard diagnostic and evaluation tool for both developers and users of NLP applications."
C94-1072,{DISCO}-An {HPSG}-based {NLP} System and its Application for Appointment Scheduling Project Note,1994,0,2,11,0,23887,hans uszkoreit,{COLING} 1994 Volume 1: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"The natural language system DISCO is described. It combines o a powerful and flexible grammar development system; o linguistic competence for German including morphology, syntax and semantics; o new methods for linguistic performance modelling on the basis of high-level competence grammars; o new methods for modelling multi-agent dialogue competence; o an interesting sample application for appointment scheduling and calendar management."
