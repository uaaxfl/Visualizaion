2020.lrec-1.668,P12-3007,0,0.0605464,"Missing"
2020.lrec-1.668,P19-1292,0,0.0121297,"g promising results in a wide variety of natural language processing tasks. Such models can more accurately capture the meaning of words depending on the context with a massive amount of training data, enabling them to be applied to fine-tuning for particular downstream tasks. Since BERT (Bidirectional Encoder Representations from Transformers), which is a pretrained language model, has recently been found to be useful for generation tasks (Zhang et al., 2019), we also use it in our work. Specifically, we adopt the dual-source BERT encoderdecoder model (Junczys-Dowmunt and Grundkiewicz, 2018; Correia and Martins, 2019). The model allows for the incorporation of additional information. The model was originally proposed for automatic post-editing in machine translation. It takes two inputs: source text in the source language and a tentative machine translation result for that text as additional information. It then outputs target text in the target language. We use this model for our generation models. In this study, as additional information, we used meta information instead of tentative machine translation results. 3.3. Classifiers for meta information We need classifiers for meta information for creating A"
2020.lrec-1.668,C14-1088,1,0.811719,"information was also collected such as emotion and intimacy related to question-answer pairs. We verified the quality of the collected data and, by subjective evaluation, we also verified their usefulness in training neural conversational models for generating utterances reflecting the meta information, especially emotion. Keywords: data collection, meta information, user-generated content, utterance generation, question-answering 1. Introduction Much attention has been paid to non-task oriented dialogue systems from their social and entertainment aspects (Wallace, 2009; Banchs and Li, 2012; Higashinaka et al., 2014). To make such systems more engaging, it is necessary that they exhibit consistent personalities. In neural conversational modeling (Sordoni et al., 2015; Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016), various architectures have been proposed to realize consistent personalities, such as incorporating user IDs (Li et al., 2016), personal attributes (Qian et al., 2018), and profile text (Zhang et al., 2018). However, the problem is that it is still costly to collect dialogue data for training such neural models. For efficiently collecting dialogue data (we deal with single quest"
2020.lrec-1.668,W18-5031,1,0.868363,"d Le, 2015; Shang et al., 2015; Serban et al., 2016), various architectures have been proposed to realize consistent personalities, such as incorporating user IDs (Li et al., 2016), personal attributes (Qian et al., 2018), and profile text (Zhang et al., 2018). However, the problem is that it is still costly to collect dialogue data for training such neural models. For efficiently collecting dialogue data (we deal with single question-answer pairs as dialogue data in this paper) for a particular character, a data collection method called “role play-based question-answering” has been proposed (Higashinaka et al., 2018) (See Section 2. for details). In this method, fans of a particular character voluntarily provide question-answer pairs by playing the role of that character. It has been demonstrated that high-quality question-answer pairs can be efficiently collected and that dialogue systems that exhibit consistent personalities can be realized (Higashinaka et al., 2018). This paper extends this work and aims to collect questionanswer pairs for particular characters together with other pieces of information (called “meta information”), such as emotion and intimacy levels. The aim of collecting this addition"
2020.lrec-1.668,W18-6467,0,0.0219742,", pre-trained language models are showing promising results in a wide variety of natural language processing tasks. Such models can more accurately capture the meaning of words depending on the context with a massive amount of training data, enabling them to be applied to fine-tuning for particular downstream tasks. Since BERT (Bidirectional Encoder Representations from Transformers), which is a pretrained language model, has recently been found to be useful for generation tasks (Zhang et al., 2019), we also use it in our work. Specifically, we adopt the dual-source BERT encoderdecoder model (Junczys-Dowmunt and Grundkiewicz, 2018; Correia and Martins, 2019). The model allows for the incorporation of additional information. The model was originally proposed for automatic post-editing in machine translation. It takes two inputs: source text in the source language and a tentative machine translation result for that text as additional information. It then outputs target text in the target language. We use this model for our generation models. In this study, as additional information, we used meta information instead of tentative machine translation results. 3.3. Classifiers for meta information We need classifiers for met"
2020.lrec-1.668,D18-2012,0,0.0230183,"in generating responses that reflect the meta information. By comparing w-Meta and w-Meta-Anno, we could check whether the automatic annotation of meta information was useful for pre-training models. Note that the aim of this paper is to verify whether utterances that reflect meta information can be generated with user-generated questionanswer pairs. We used OpenNMT-APE3 for training the models with default parameters. OpenNMT-APE implements the dual-source BERT encoder-decoder model that allows for the incorporation of additional information. Tokenization was done by using a SentencePiece4 (Kudo and Richardson, 2018) model trained with Japanese Wikipedia. The vocabulary size is 32K. 4.2. Experiments Models for comparison We trained our conversational models and evaluated their performance. We trained three models for comparison: 1 wo-Meta : pre-trained using General QA pairs (without meta information) and fine-tuned without meta information using role play-based QA pairs. We followed the settings as shown in https://github.com/ huggingface/transformers 2 Note that the training data here correspond to the training and development data in Section 4. Automatic evaluation Tables 4 and 5 show the results of th"
2020.lrec-1.668,P16-1094,0,0.0227265,"eta information, user-generated content, utterance generation, question-answering 1. Introduction Much attention has been paid to non-task oriented dialogue systems from their social and entertainment aspects (Wallace, 2009; Banchs and Li, 2012; Higashinaka et al., 2014). To make such systems more engaging, it is necessary that they exhibit consistent personalities. In neural conversational modeling (Sordoni et al., 2015; Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016), various architectures have been proposed to realize consistent personalities, such as incorporating user IDs (Li et al., 2016), personal attributes (Qian et al., 2018), and profile text (Zhang et al., 2018). However, the problem is that it is still costly to collect dialogue data for training such neural models. For efficiently collecting dialogue data (we deal with single question-answer pairs as dialogue data in this paper) for a particular character, a data collection method called “role play-based question-answering” has been proposed (Higashinaka et al., 2018) (See Section 2. for details). In this method, fans of a particular character voluntarily provide question-answer pairs by playing the role of that charact"
2020.lrec-1.668,D16-1230,0,0.0171056,"heir performance. We trained three models for comparison: 1 wo-Meta : pre-trained using General QA pairs (without meta information) and fine-tuned without meta information using role play-based QA pairs. We followed the settings as shown in https://github.com/ huggingface/transformers 2 Note that the training data here correspond to the training and development data in Section 4. Automatic evaluation Tables 4 and 5 show the results of the automatic evaluation against the test data for emotion and intimacy, respectively. We used perplexity, distinct-1,2, and BLEU-1,2,3,4 as evaluation metrics (Liu et al., 2016). Perplexity measures the adequacy of language models. Distinct metrics measure the diversity of expressions in generated utterances, and BLEU 3 4 5438 https://github.com/deep-spin/OpenNMT-APE https://github.com/google/sentencepiece Table 6: Results of human evaluation for naturalness wo-Meta w-Meta w-Meta+Anno Ayase Emotion 3.88 3.88 3.89 Hime Emotion Intimacy 4.08 4.12 4.06 4.17 4.12 4.01 Hina Emotion Intimacy 4.07 4.05 4.10 4.08 4.12 4.03 Table 7: Results of human evaluation for reflection of meta information. Scores were averaged over all judges. Asterisks (*) indicate whether value of bes"
2020.lrec-1.668,P15-1152,0,0.0319387,"ess in training neural conversational models for generating utterances reflecting the meta information, especially emotion. Keywords: data collection, meta information, user-generated content, utterance generation, question-answering 1. Introduction Much attention has been paid to non-task oriented dialogue systems from their social and entertainment aspects (Wallace, 2009; Banchs and Li, 2012; Higashinaka et al., 2014). To make such systems more engaging, it is necessary that they exhibit consistent personalities. In neural conversational modeling (Sordoni et al., 2015; Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016), various architectures have been proposed to realize consistent personalities, such as incorporating user IDs (Li et al., 2016), personal attributes (Qian et al., 2018), and profile text (Zhang et al., 2018). However, the problem is that it is still costly to collect dialogue data for training such neural models. For efficiently collecting dialogue data (we deal with single question-answer pairs as dialogue data in this paper) for a particular character, a data collection method called “role play-based question-answering” has been proposed (Higashinaka et al., 2018) (See"
2020.lrec-1.668,P19-1359,0,0.0113805,"character. It has been demonstrated that high-quality question-answer pairs can be efficiently collected and that dialogue systems that exhibit consistent personalities can be realized (Higashinaka et al., 2018). This paper extends this work and aims to collect questionanswer pairs for particular characters together with other pieces of information (called “meta information”), such as emotion and intimacy levels. The aim of collecting this additional data is to realize dialogue systems whose utterances can be controlled to reflect such meta information (Zhou et al., 2018; Zhou and Wang, 2018; Song et al., 2019). This is a useful feature when we want to realize systems that are affective and can become more intimate as an interaction progresses (Zhou and Wang, 2018). We verify ∗ Work carried out during an internship at NTT Corporation. the quality of the collected data and empirically show that conversational models that exhibit consistent personalities as well as meta information, especially emotion, can be successfully realized by using voluntarily provided usergenerated question-answer pairs. In what follows, we first describe the idea of role playbased question-answering followed by our data coll"
2020.lrec-1.668,N15-1020,0,0.0321433,"e evaluation, we also verified their usefulness in training neural conversational models for generating utterances reflecting the meta information, especially emotion. Keywords: data collection, meta information, user-generated content, utterance generation, question-answering 1. Introduction Much attention has been paid to non-task oriented dialogue systems from their social and entertainment aspects (Wallace, 2009; Banchs and Li, 2012; Higashinaka et al., 2014). To make such systems more engaging, it is necessary that they exhibit consistent personalities. In neural conversational modeling (Sordoni et al., 2015; Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016), various architectures have been proposed to realize consistent personalities, such as incorporating user IDs (Li et al., 2016), personal attributes (Qian et al., 2018), and profile text (Zhang et al., 2018). However, the problem is that it is still costly to collect dialogue data for training such neural models. For efficiently collecting dialogue data (we deal with single question-answer pairs as dialogue data in this paper) for a particular character, a data collection method called “role play-based question-answering” has bee"
2020.lrec-1.668,P18-1205,0,0.016668,"ring 1. Introduction Much attention has been paid to non-task oriented dialogue systems from their social and entertainment aspects (Wallace, 2009; Banchs and Li, 2012; Higashinaka et al., 2014). To make such systems more engaging, it is necessary that they exhibit consistent personalities. In neural conversational modeling (Sordoni et al., 2015; Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016), various architectures have been proposed to realize consistent personalities, such as incorporating user IDs (Li et al., 2016), personal attributes (Qian et al., 2018), and profile text (Zhang et al., 2018). However, the problem is that it is still costly to collect dialogue data for training such neural models. For efficiently collecting dialogue data (we deal with single question-answer pairs as dialogue data in this paper) for a particular character, a data collection method called “role play-based question-answering” has been proposed (Higashinaka et al., 2018) (See Section 2. for details). In this method, fans of a particular character voluntarily provide question-answer pairs by playing the role of that character. It has been demonstrated that high-quality question-answer pairs can be effi"
2020.lrec-1.668,K19-1074,0,0.015108,"ncluded in the general QA pairs, which may be problematic in the later fine-tuning 3.2. Generation models Currently, pre-trained language models are showing promising results in a wide variety of natural language processing tasks. Such models can more accurately capture the meaning of words depending on the context with a massive amount of training data, enabling them to be applied to fine-tuning for particular downstream tasks. Since BERT (Bidirectional Encoder Representations from Transformers), which is a pretrained language model, has recently been found to be useful for generation tasks (Zhang et al., 2019), we also use it in our work. Specifically, we adopt the dual-source BERT encoderdecoder model (Junczys-Dowmunt and Grundkiewicz, 2018; Correia and Martins, 2019). The model allows for the incorporation of additional information. The model was originally proposed for automatic post-editing in machine translation. It takes two inputs: source text in the source language and a tentative machine translation result for that text as additional information. It then outputs target text in the target language. We use this model for our generation models. In this study, as additional information, we use"
2020.lrec-1.668,P18-1104,0,0.0272144,"ing the role of that character. It has been demonstrated that high-quality question-answer pairs can be efficiently collected and that dialogue systems that exhibit consistent personalities can be realized (Higashinaka et al., 2018). This paper extends this work and aims to collect questionanswer pairs for particular characters together with other pieces of information (called “meta information”), such as emotion and intimacy levels. The aim of collecting this additional data is to realize dialogue systems whose utterances can be controlled to reflect such meta information (Zhou et al., 2018; Zhou and Wang, 2018; Song et al., 2019). This is a useful feature when we want to realize systems that are affective and can become more intimate as an interaction progresses (Zhou and Wang, 2018). We verify ∗ Work carried out during an internship at NTT Corporation. the quality of the collected data and empirically show that conversational models that exhibit consistent personalities as well as meta information, especially emotion, can be successfully realized by using voluntarily provided usergenerated question-answer pairs. In what follows, we first describe the idea of role playbased question-answering follo"
2020.sigdial-1.39,W11-2004,0,0.0127532,"Computational Linguistics They acted as one visible guide with knowledge of both prefectures (Fig. 2(a)). 2to1 condition Two operators with different specialties took turns talking directly (Fig. 2(b)) with one user in a three-party dialogue. This condition was collected as a baseline to evaluate the validity of the Mixto1 condition. 1to1 condition One operator gave recommendation to one user about two prefectures. The operator has much knowledge about one of them, but the other is outside his/her skill set. Figure 2: Cooperation style of Osaka and Nara guides under Mixto1 and 2to1 conditions Nakano et al., 2011) as well as Wizard-of-Oz systems (Marge et al., 2016; Abbas et al., 2020). Users talking with a dialogue assistant controlled by multiple speakers on the cloud are reported to receive more reasonable responses (Lasecki et al., 2013). However, no research has examined the basic effect of behaving as one speaker on the satisfaction of the operators and their interlocutors. It remains especially unclear whether multiple operators who are acting as one promote mutual skill learning. The following is the contribution of this study. First, we show a method for collecting text-chat dialogues in which"
2021.nlp4convai-1.25,P03-1033,0,0.170906,"dressed in the last section. 2 Related Work and the template sentence with different syntactic styles. Qian et al. (2018) applied a profile as personality into an encoder-decoder dialogue system to increase the coherence of generated utterances. Zhang et al. (2018) proposed a dataset named “Persona-Chat” containing a corpus with a corresponding persona description to help build an end-to-end chit-chat dialogue system with consistent personality and coherent utterance generation. Towards a more user-adaptive dialogue system, several researchers have investigated user-adaptive dialogue systems. Komatani et al. (2003) focused on a system that generates the responses to users’ information queries in accordance with their skill with regard to the system, domain-related knowledge, and the degree of hastiness. Dohsaka et al. (2010) developed a dialogue system that adapts to users’ behavior regarding pause duration preceding system utterance and gaze to reduce user’s discomfort during conversation. To increase user engagement, Liang et al. (2020) proposed a system that is able to select a chat topic in accordance with user interests. The aim of our research is to find out the influences of personality on task p"
2021.nlp4convai-1.25,P07-1063,0,0.342396,"t sociable and extroverted people tended to fail the task, whereas neurotic people were more likely to succeed. We extracted features related to user dialogue behaviors and performed further analysis to determine which kind of behavior influences task performance. As a result, we identified that average utterance length and slots per utterance are the key features of dialogue behavior that are highly correlated with both task performance and user personality. 1 Introduction verts in dialogue behavior, such as extroverts being more talkative and using fewer words per utterance than introverts (Mairesse and Walker, 2007). They further designed a parameterized natural language generation system named “PERSONAGE.” Increasing applications of personality have been put forward on a dialogue system that is able to generate natural, coherent, and personalized utterances (Zheng et al., 2020). However, two problems remain that hinder the dialogue system adapting to user personality to achieve better task performance: (1) what kind of personality traits are correlated with task performance and (2) how the personality influences the task performance. In this work, we performed a case study analysis of user personality a"
2021.nlp4convai-1.25,P18-1205,0,0.0296658,"ality on task performance is clarified. The remainder of this paper is organized as follows. The related work on personality and dialogue systems is presented in the next section. Section 3 describes the research approach and analysis procedures. The data collection and data analysis are presented in Sections 4 and 5. Conclusions and future work are addressed in the last section. 2 Related Work and the template sentence with different syntactic styles. Qian et al. (2018) applied a profile as personality into an encoder-decoder dialogue system to increase the coherence of generated utterances. Zhang et al. (2018) proposed a dataset named “Persona-Chat” containing a corpus with a corresponding persona description to help build an end-to-end chit-chat dialogue system with consistent personality and coherent utterance generation. Towards a more user-adaptive dialogue system, several researchers have investigated user-adaptive dialogue systems. Komatani et al. (2003) focused on a system that generates the responses to users’ information queries in accordance with their skill with regard to the system, domain-related knowledge, and the degree of hastiness. Dohsaka et al. (2010) developed a dialogue system"
2021.sdp-1.3,D19-1371,0,0.047497,"Missing"
2021.sdp-1.3,N18-1022,0,0.152267,"kita Prefectural University, 4 Osaka Institute of Technology {hiromi.narimatsu.eg, ryuichiro.higashinaka.tp}@hco.ntt.co.jp k1710245@edu.cc.uec.ac.jp, dohsaka@akita-pu.ac.jp minami.yasuhiro@is.uec.ac.jp, hirotoshi.taira@oit.ac.jp Abstract posed (Dessì et al., 2020), and Gábor et al. (2018) proposed an automatic content-analysis method by extracting the semantic relations of entities in abstracts. Other studies have focused on citation recommendation (Huang et al., 2014; He et al., 2010) and generation of citation text (Xing et al., 2020; Luu et al., 2020). Using the database of PubMed1 papers, Bhagavatula et al. (2018) proposed recommending citations on the basis of keywords as well as the contents of a paper. Mohammad et al. (2009) proposed the generation of citation text, and Färber et al. (2018) proposed a classiﬁcation model for the task of judging whether a sentence requires citation (citation worthiness). Although many reports have been presented and an abundance of effort has been expended on data creation (Färber and Jatowt, 2020; Kardas et al., 2020; Saier and Färber, 2020), each previous study has focused on a particular problem in scientiﬁcwriting support and has been performed independently usin"
2021.sdp-1.3,N19-1423,0,0.00887064,"Citation worthiness BERT For training, each of the sentences and each abstract text in the references are paired to create training data while regarding the correct pair as a positive example or otherwise as a negative example. Then, the data are used for training a BERT-based classiﬁer. Here, the input format is “[CLS] sentence [SEP] abstract text [SEP].” In the test phase, a pair consisting of a sentence and an abstract is fed to the trained classiﬁer. We use the probability threshold of 0.5 to determine whether the pair is valid. Using the dataset for (2), we trained a BERTbased classiﬁer (Devlin et al., 2019). We used BertForSequenceClassification from huggingface4 . We used the bert-base-uncased model. For training, we used the train/dev data for this task as described in the previous section. The input format used for the classiﬁer was “[CLS] sentence [SEP].” We used the Adam optimizer at a learning rate of 1.0e−5 . We trained for 50 epochs and chose the model that achieved the highest accuracy for the development set. As evaluation metrics, in addition to accuracy, we used precision, recall, and F1 of positive labels (i.e., needs citation). Table 2 shows the results. As can be seen, the accurac"
2021.sdp-1.3,2020.acl-main.550,0,0.0137719,"unication Science Laboratories 2 The University of Electro-Communication 3 Akita Prefectural University, 4 Osaka Institute of Technology {hiromi.narimatsu.eg, ryuichiro.higashinaka.tp}@hco.ntt.co.jp k1710245@edu.cc.uec.ac.jp, dohsaka@akita-pu.ac.jp minami.yasuhiro@is.uec.ac.jp, hirotoshi.taira@oit.ac.jp Abstract posed (Dessì et al., 2020), and Gábor et al. (2018) proposed an automatic content-analysis method by extracting the semantic relations of entities in abstracts. Other studies have focused on citation recommendation (Huang et al., 2014; He et al., 2010) and generation of citation text (Xing et al., 2020; Luu et al., 2020). Using the database of PubMed1 papers, Bhagavatula et al. (2018) proposed recommending citations on the basis of keywords as well as the contents of a paper. Mohammad et al. (2009) proposed the generation of citation text, and Färber et al. (2018) proposed a classiﬁcation model for the task of judging whether a sentence requires citation (citation worthiness). Although many reports have been presented and an abundance of effort has been expended on data creation (Färber and Jatowt, 2020; Kardas et al., 2020; Saier and Färber, 2020), each previous study has focused on a part"
2021.sdp-1.3,2020.emnlp-main.692,0,0.0712675,"Huang et al., 2014; He et al., 2010) and generation of citation text (Xing et al., 2020; Luu et al., 2020). Using the database of PubMed1 papers, Bhagavatula et al. (2018) proposed recommending citations on the basis of keywords as well as the contents of a paper. Mohammad et al. (2009) proposed the generation of citation text, and Färber et al. (2018) proposed a classiﬁcation model for the task of judging whether a sentence requires citation (citation worthiness). Although many reports have been presented and an abundance of effort has been expended on data creation (Färber and Jatowt, 2020; Kardas et al., 2020; Saier and Färber, 2020), each previous study has focused on a particular problem in scientiﬁcwriting support and has been performed independently using its own speciﬁc dataset. Therefore, we do not yet know whether these investigations can be successfully pipelined nor how to ascertain the overall performance of a system that can comprehensively recommend citations. Consequently, it is currently impossible to verify that the technologies centered around scientiﬁc-paper writing are actually helpful in comprehensively supporting real-world scientiﬁc-paper writing. In this paper, we ﬁrst deﬁne"
2021.sdp-1.3,W16-1609,0,0.03148,"ion worthiness (test) 746,741 sentences (22,416 target papers) 196,709 sentences (6,000 target papers) 193,718 sentences (6,000 target papers) Sentence-citation pair Classification (train) Sentence-citation pair classification (dev) Sentence-citation pair classification (test) 36,336 sentences (6,946 target papers) 2,662 sentences (500 target papers) 2,584 sentences (500 target papers) Citation allocation (test) Random Randomly determines whether the citation is appropriate. 586 sentences (120 target papers) Figure 3: Inclusion relationship among datasets Doc2Vec This method utilizes Doc2Vec (Lau and Baldwin, 2016) to vectorize sentences and abstracts for similarity calculation. The Doc2Vec model was trained with the training data of this task. For all sentences with citations, we ﬁrst concatenated a sentence and the abstract of the cited paper, then a Doc2Vec model was trained using the gensim5 library. The trained model was used to convert a sentence and an abstract into vectors in order to calculate their cosine similarity. When the similarity increases above a predeﬁned threshold (empirically set to 0.02 using the dev set), it is deemed appropriate. its individual train/dev data. 5 Experiment Using"
2021.sdp-1.3,N09-1066,0,0.247508,".jp k1710245@edu.cc.uec.ac.jp, dohsaka@akita-pu.ac.jp minami.yasuhiro@is.uec.ac.jp, hirotoshi.taira@oit.ac.jp Abstract posed (Dessì et al., 2020), and Gábor et al. (2018) proposed an automatic content-analysis method by extracting the semantic relations of entities in abstracts. Other studies have focused on citation recommendation (Huang et al., 2014; He et al., 2010) and generation of citation text (Xing et al., 2020; Luu et al., 2020). Using the database of PubMed1 papers, Bhagavatula et al. (2018) proposed recommending citations on the basis of keywords as well as the contents of a paper. Mohammad et al. (2009) proposed the generation of citation text, and Färber et al. (2018) proposed a classiﬁcation model for the task of judging whether a sentence requires citation (citation worthiness). Although many reports have been presented and an abundance of effort has been expended on data creation (Färber and Jatowt, 2020; Kardas et al., 2020; Saier and Färber, 2020), each previous study has focused on a particular problem in scientiﬁcwriting support and has been performed independently using its own speciﬁc dataset. Therefore, we do not yet know whether these investigations can be successfully pipelined"
2021.sdp-1.3,C08-1087,0,0.120527,"lts of our evaluations show that the proposed approach is promising. 1 Introduction When writing a scientiﬁc paper, it is important to search for relevant papers and cite them appropriately. However, despite the importance of this requirement, the recent sharp increase in published scientiﬁc papers is making it difﬁcult for researchers to comprehensively carry out this process. Consequently, much work has been devoted to developing systems that support the writing of scientiﬁc papers. For example, some studies have attempted to summarize papers on a particular subject (Teufel and Moens, 2002; Qazvinian and Radev, 2008; Bai et al., 2019). The creation of knowledge graphs of scientiﬁc papers has also been pro1 https://pubmed.ncbi.nlm.nih.gov/ Our dataset is available at https://github.com/ citation-minami-lab/citation-dataset. 2 18 Proceedings of the Second Workshop on Scholarly Document Processing, pages 18–26 June 10, 2021. ©2021 Association for Computational Linguistics (i) Beginning of research (1) Suggest related articles Abstract ate the individual tasks of citation worthiness and citation recommendation as well as the integrated task composed of these two individual tasks. Experimental results show th"
2021.sdp-1.3,J02-4002,0,0.222117,"sks integrated. The results of our evaluations show that the proposed approach is promising. 1 Introduction When writing a scientiﬁc paper, it is important to search for relevant papers and cite them appropriately. However, despite the importance of this requirement, the recent sharp increase in published scientiﬁc papers is making it difﬁcult for researchers to comprehensively carry out this process. Consequently, much work has been devoted to developing systems that support the writing of scientiﬁc papers. For example, some studies have attempted to summarize papers on a particular subject (Teufel and Moens, 2002; Qazvinian and Radev, 2008; Bai et al., 2019). The creation of knowledge graphs of scientiﬁc papers has also been pro1 https://pubmed.ncbi.nlm.nih.gov/ Our dataset is available at https://github.com/ citation-minami-lab/citation-dataset. 2 18 Proceedings of the Second Workshop on Scholarly Document Processing, pages 18–26 June 10, 2021. ©2021 Association for Computational Linguistics (i) Beginning of research (1) Suggest related articles Abstract ate the individual tasks of citation worthiness and citation recommendation as well as the integrated task composed of these two individual tasks. E"
2021.sigdial-1.10,P12-3007,0,0.0860952,"Missing"
2021.sigdial-1.10,C96-1056,0,0.586845,"014; Ram et al., 2018). Neural-based methods have been extensively studied and have yielded promising results (Vinyals and Le, 2015; Zhang et al., 2018; Dinan et al., 2019; Adiwardana et al., 2020; Roller et al., 2020). Yet, the performance of these systems is still unsatisfactory, causing dialogues to often break down. One way to reduce the errors made by the systems is to understand what kinds of errors the systems are making and find solutions to counter them. For such a purpose, a taxonomy of errors will be useful. For task-oriented dialogue systems, several taxonomies have been proposed (Dybkjær et al., 1996; Bernsen et al., 1996; Aberdeen and Ferro, 2003; Dzikovska et al., 2009), leading to effective analyses for improving system performance. For dialogue systems that ∗ 2 Previous Taxonomies and Integration Higashinaka et al. proposed two taxonomies of errors in chat-oriented dialogue systems: theorydriven (Higashinaka et al., 2015a) and data-driven (Higashinaka et al., 2015b).1 1 Note that although Higashinaka et al. used “top-down” and “bottom-up” to name their taxonomies, we use “theorydriven” and “data-driven,” which we consider to be more appropriate. Currently mainly affiliated with Nagoya"
2021.sigdial-1.10,W09-3906,0,0.0232953,"died and have yielded promising results (Vinyals and Le, 2015; Zhang et al., 2018; Dinan et al., 2019; Adiwardana et al., 2020; Roller et al., 2020). Yet, the performance of these systems is still unsatisfactory, causing dialogues to often break down. One way to reduce the errors made by the systems is to understand what kinds of errors the systems are making and find solutions to counter them. For such a purpose, a taxonomy of errors will be useful. For task-oriented dialogue systems, several taxonomies have been proposed (Dybkjær et al., 1996; Bernsen et al., 1996; Aberdeen and Ferro, 2003; Dzikovska et al., 2009), leading to effective analyses for improving system performance. For dialogue systems that ∗ 2 Previous Taxonomies and Integration Higashinaka et al. proposed two taxonomies of errors in chat-oriented dialogue systems: theorydriven (Higashinaka et al., 2015a) and data-driven (Higashinaka et al., 2015b).1 1 Note that although Higashinaka et al. used “top-down” and “bottom-up” to name their taxonomies, we use “theorydriven” and “data-driven,” which we consider to be more appropriate. Currently mainly affiliated with Nagoya University. 89 Proceedings of the 22nd Annual Meeting of the Special Int"
2021.sigdial-1.10,W15-4611,1,0.788436,"Missing"
2021.sigdial-1.10,L16-1502,1,0.820087,"y. The annotators read annotation manuals containing definitions of the error types with examples and annotated the error types on spreadsheets. U: Do you want to talk about heat stroke? S: Heat stroke is good, isn’t it? 4 Evaluation We evaluated the integrated taxonomy by annotating dialogues with error types and calculating the inter-annotator agreement. The same dialogues were annotated with the theory- and data-driven taxonomies by the same annotators for comparison. 4.1 Procedure We used the datasets collected in past dialogue breakdown detection challenges (DBDCs), i.e., DBDC and DBDC2 (Higashinaka et al., 2016, 2017)3 , for annotating error types to system utterances that caused dialogue breakdowns. In the datasets, each system utterance was labeled with dialogue breakdown labels (B: breakdown, PB: possible breakdown, and NB: not a breakdown) by 30 annotators. We picked system utterances that were deemed inappropriate by more than a half of the annotators, that is, annotated with 15 or more B or PB dialogue breakdown labels. The dialogues were those conducted between each of three chat-oriented dialogue systems [DCM (Onishi and Yoshimura, 2014), DIT (Tsukahara and Uchiumi, 2015), and IRS (Ritter et"
2021.sigdial-1.10,L16-1650,0,0.0286593,"our taxonomy. All dialogues were in Japanese. There were 400 dialogues in total across the datasets. We divided the datasets into five subsets, A–E, each containing 80 dialogues. We used subsets A–C to come up with how to integrate the taxonomies. We used subset D for evaluation. We did not use subset E, which was spared for future evaluation. In the 80 dialogues, there were 599 system utterances used as a target for our error-type annotation. 4.2 Metric for inter-annotator agreement We used Fleiss’ κ coefficient (Fleiss and Cohen, 1973) as a measure for inter-annotator agreement. Following (Ravenscroft et al., 2016), who calculated the weighted Cohen’s kappa, we devised a way to calculate the weighted Fleiss’ kappa. The weighted inter-annotator agreement rate Pa , extended for multi-label annotation, is calculated by, PC P N 1 X c=1 (l,l′ ) wncl wncl′ , Pa = PC P 2 2 N c=1 (l,l′ ) (wcnl + wcnl′ )/2 n=1 (1) where wncl is the weight of error type c for target utterance n labeled by annotator l, N is the total number of targets for annotation, C P is the number of error types, and the summation (l,l′ ) is taken over all combinations of annotator pairs. Note that the Pweights are non-negative and normalized"
2021.sigdial-1.10,D11-1054,0,0.0339747,"al., 2016, 2017)3 , for annotating error types to system utterances that caused dialogue breakdowns. In the datasets, each system utterance was labeled with dialogue breakdown labels (B: breakdown, PB: possible breakdown, and NB: not a breakdown) by 30 annotators. We picked system utterances that were deemed inappropriate by more than a half of the annotators, that is, annotated with 15 or more B or PB dialogue breakdown labels. The dialogues were those conducted between each of three chat-oriented dialogue systems [DCM (Onishi and Yoshimura, 2014), DIT (Tsukahara and Uchiumi, 2015), and IRS (Ritter et al., 2011)] and human users. Having dialogues from multiple dialogue systems allow us to verify the applicability and coverage of our taxonomy. All dialogues were in Japanese. There were 400 dialogues in total across the datasets. We divided the datasets into five subsets, A–E, each containing 80 dialogues. We used subsets A–C to come up with how to integrate the taxonomies. We used subset D for evaluation. We did not use subset E, which was spared for future evaluation. In the 80 dialogues, there were 599 system utterances used as a target for our error-type annotation. 4.2 Metric for inter-annotator a"
2021.sigdial-1.10,C14-1088,1,0.803038,"categorizing errors made by chat-oriented dialogue systems. The latter has limitations in that it can only cope with errors of systems for which we have data. This paper integrates these two taxonomies to create a comprehensive taxonomy of errors in chat-oriented dialogue systems. We found that, with our integrated taxonomy, errors can be reliably annotated with a higher Fleiss’ kappa compared with the previously proposed taxonomies. 1 Introduction From their social aspects, chat-oriented dialogue systems have been attracting much attention in recent years (Wallace, 2009; Banchs and Li, 2012; Higashinaka et al., 2014; Ram et al., 2018). Neural-based methods have been extensively studied and have yielded promising results (Vinyals and Le, 2015; Zhang et al., 2018; Dinan et al., 2019; Adiwardana et al., 2020; Roller et al., 2020). Yet, the performance of these systems is still unsatisfactory, causing dialogues to often break down. One way to reduce the errors made by the systems is to understand what kinds of errors the systems are making and find solutions to counter them. For such a purpose, a taxonomy of errors will be useful. For task-oriented dialogue systems, several taxonomies have been proposed (Dyb"
2021.sigdial-1.10,D15-1268,1,0.886903,"Missing"
2021.sigdial-1.10,W13-4021,0,0.0702961,"Missing"
2021.sigdial-1.10,Y15-1037,1,0.833265,", i.e., DBDC and DBDC2 (Higashinaka et al., 2016, 2017)3 , for annotating error types to system utterances that caused dialogue breakdowns. In the datasets, each system utterance was labeled with dialogue breakdown labels (B: breakdown, PB: possible breakdown, and NB: not a breakdown) by 30 annotators. We picked system utterances that were deemed inappropriate by more than a half of the annotators, that is, annotated with 15 or more B or PB dialogue breakdown labels. The dialogues were those conducted between each of three chat-oriented dialogue systems [DCM (Onishi and Yoshimura, 2014), DIT (Tsukahara and Uchiumi, 2015), and IRS (Ritter et al., 2011)] and human users. Having dialogues from multiple dialogue systems allow us to verify the applicability and coverage of our taxonomy. All dialogues were in Japanese. There were 400 dialogues in total across the datasets. We divided the datasets into five subsets, A–E, each containing 80 dialogues. We used subsets A–C to come up with how to integrate the taxonomies. We used subset D for evaluation. We did not use subset E, which was spared for future evaluation. In the 80 dialogues, there were 599 system utterances used as a target for our error-type annotation. 4"
2021.sigdial-1.10,W14-4331,0,0.0640075,"Missing"
2021.sigdial-1.10,P18-1205,0,0.0305183,"ata. This paper integrates these two taxonomies to create a comprehensive taxonomy of errors in chat-oriented dialogue systems. We found that, with our integrated taxonomy, errors can be reliably annotated with a higher Fleiss’ kappa compared with the previously proposed taxonomies. 1 Introduction From their social aspects, chat-oriented dialogue systems have been attracting much attention in recent years (Wallace, 2009; Banchs and Li, 2012; Higashinaka et al., 2014; Ram et al., 2018). Neural-based methods have been extensively studied and have yielded promising results (Vinyals and Le, 2015; Zhang et al., 2018; Dinan et al., 2019; Adiwardana et al., 2020; Roller et al., 2020). Yet, the performance of these systems is still unsatisfactory, causing dialogues to often break down. One way to reduce the errors made by the systems is to understand what kinds of errors the systems are making and find solutions to counter them. For such a purpose, a taxonomy of errors will be useful. For task-oriented dialogue systems, several taxonomies have been proposed (Dybkjær et al., 1996; Bernsen et al., 1996; Aberdeen and Ferro, 2003; Dzikovska et al., 2009), leading to effective analyses for improving system perfo"
C10-1086,D08-1040,0,0.0194665,"an subjects revealed that POMDPs can satisfactorily produce a dialogue control component that can achieve reasonable subjective assessment. 1 Introduction Although task-oriented dialogue systems have been actively researched (Hirshman, 1989; Ferguson et al., 1996; Nakano et al., 1999; Walker et al., 2002), recently non-task-oriented functions are starting to attract attention, and systems without a specific task that deal with more casual dialogues, such as chats, are being actively investigated from their social and entertainment aspects (Bickmore and Cassell, 2001; Higashinaka et al., 2008; Higuchi et al., 2008). In the same vein, we have been working on listening-oriented dialogues in which one conversational participant attentively listens to the other (hereafter, listening-oriented dialogue). Our aim is to build listening agents that can implement such a listening process so that users can satisfy their desire to speak and be heard. Figure 1 shows an excerpt from a typical listening-oriented dialogue. In the literature, dialogue control components for less (or non-) task-oriented dialogue systems, such as listening agents, have typically used hand-crafted rules for dialogue control, which can be p"
C10-1086,H89-1001,0,0.0799215,"listening-oriented dialogues with their user satisfaction ratings and used them to create a dialogue control component using partially observable Markov decision processes (POMDPs), which can learn a policy to satisfy users by automatically finding a reasonable reward function. A comparison between our POMDP-based component and other similarly motivated systems using human subjects revealed that POMDPs can satisfactorily produce a dialogue control component that can achieve reasonable subjective assessment. 1 Introduction Although task-oriented dialogue systems have been actively researched (Hirshman, 1989; Ferguson et al., 1996; Nakano et al., 1999; Walker et al., 2002), recently non-task-oriented functions are starting to attract attention, and systems without a specific task that deal with more casual dialogues, such as chats, are being actively investigated from their social and entertainment aspects (Bickmore and Cassell, 2001; Higashinaka et al., 2008; Higuchi et al., 2008). In the same vein, we have been working on listening-oriented dialogues in which one conversational participant attentively listens to the other (hereafter, listening-oriented dialogue). Our aim is to build listening a"
C10-1086,W09-3917,1,0.814427,"tion 4 describes our collection of listening-oriented dialogues. This is followed in Section 5 by an evaluation experiment that compared our POMDP-based dialogue control with other similarly motivated systems. The last section summarizes the main points and mentions future work. 2 Related work With increased attention on social dialogues and senior peer counseling, work continues to emerge on listening-oriented dialogues. One early work is (Maatman et al., 2005), which showed that virtual agents can give users the sense of being heard using such gestures as nodding and head shaking. Recently, Meguro et al. (2009a) analyzed the characteristics of listening-oriented dialogues. They compared listening-oriented dialogues and casual conversations between humans, revealing that the two types of dialogues have significantly different flows and that listeners actively question with frequently inserted self-disclosures; the speaker utterances were mostly concerned with self-disclosure. Shitaoka et al. (2010) also investigated the functions of listening agents, focusing on their response generation components. Their system takes the confidence score of speech recognition into account and changes the system res"
C10-1086,P99-1026,1,0.796452,"Missing"
C10-2046,N04-1015,0,0.242711,"support vector machines (SVMs) and creates a structured summary by heuristic rules that assign the mapped utterances to appropriate summary sections. Our work shares the same motivation as theirs in that we want to make it easier for quality analysts to analyze the massive number of calls. However, we tackle the problem differently in that we propose a new modeling of utterance sequences for extractive summarization that makes it unnecessary to create heuristics rules by hand and facilitates the porting of a summarization system. HMMs have been successfully applied to automatic summarization (Barzilay and Lee, 2004). In their work, an HMM was used to model the transition of content topics. The Viterbi decoding (Rabiner, 1990) was performed to find content topics that should be incorporated into a summary. Their approach is similar to ours in that HMMs are utilized to model topic sequences, but they did not use data of multiple domains in creating their model. In addition, their method requires training data (original articles with their reference summaries) in order to find which content topics should be included in a summary, whereas our method requires only the raw sequences with their domain labels. 3"
C10-2046,J99-3003,0,0.0542732,"2002), and integer linear programming (ILP) based sentence extraction (Gillick and Favre, 2009), have been proposed. Recent years have seen work on summarizing broadcast news speech (Hori and Furui, 2003), multi-party meetings (Murray et al., 2005), and contact center dialogues (Byrd et al., 2008). However, despite the large amount of previous work, little work has tackled the automatic summarization of multi-domain data. 400 Coling 2010: Poster Volume, pages 400–408, Beijing, August 2010 In the past few decades, contact center dialogues have been an active research focus (Gorin et al., 1997; Chu-Carroll and Carpenter, 1999). Initially, the primary aim of such research was to transfer calls from answering agents to operators as quickly as possible in the case of problematic situations. However, real-time processing of calls requires a tremendous engineering effort, especially when customer satisfaction is at stake, which led to recent work on the offline processing of calls, such as call mining (Takeuchi et al., 2007) and call summarization (Byrd et al., 2008). The work most related to ours is (Byrd et al., 2008), which maps operator/caller utterances to an ontology in the automotive domain by using support vecto"
C10-2046,W02-0401,0,0.0223443,"for understanding operator/caller utterances, which makes it difficult to port one summarization system from domain to domain. This paper describes a novel automatic summarization method for contact center dialogues without the costly process of creating domain on2 Related Work There is an abundance of research in automatic summarization. It has been successfully applied to single documents (Mani, 2001) as well as to multiple documents (Radev et al., 2004), and various summarization methods, such as the conventional LEAD method, machine-learning based sentence selection (Kupiec et al., 1995; Osborne, 2002), and integer linear programming (ILP) based sentence extraction (Gillick and Favre, 2009), have been proposed. Recent years have seen work on summarizing broadcast news speech (Hori and Furui, 2003), multi-party meetings (Murray et al., 2005), and contact center dialogues (Byrd et al., 2008). However, despite the large amount of previous work, little work has tackled the automatic summarization of multi-domain data. 400 Coling 2010: Poster Volume, pages 400–408, Beijing, August 2010 In the past few decades, contact center dialogues have been an active research focus (Gorin et al., 1997; Chu-C"
C10-2046,W09-1802,0,0.131295,"one summarization system from domain to domain. This paper describes a novel automatic summarization method for contact center dialogues without the costly process of creating domain on2 Related Work There is an abundance of research in automatic summarization. It has been successfully applied to single documents (Mani, 2001) as well as to multiple documents (Radev et al., 2004), and various summarization methods, such as the conventional LEAD method, machine-learning based sentence selection (Kupiec et al., 1995; Osborne, 2002), and integer linear programming (ILP) based sentence extraction (Gillick and Favre, 2009), have been proposed. Recent years have seen work on summarizing broadcast news speech (Hori and Furui, 2003), multi-party meetings (Murray et al., 2005), and contact center dialogues (Byrd et al., 2008). However, despite the large amount of previous work, little work has tackled the automatic summarization of multi-domain data. 400 Coling 2010: Poster Volume, pages 400–408, Beijing, August 2010 In the past few decades, contact center dialogues have been an active research focus (Gorin et al., 1997; Chu-Carroll and Carpenter, 1999). Initially, the primary aim of such research was to transfer c"
C10-2046,N03-1020,0,0.0693467,"summaries within the character lengths of our systems’ summaries. We used scenario texts (See Fig. 5) as reference data; that is, a dialogue dealing with a certain task is evaluated using the scenario text for that task. As an evaluation criterion, we used the F-measure (F1) to evaluate the retrieval accuracy on the basis of the recall and precision of retrieved content words. We used the scenarios as references because they contain the basic content exchanged between an operator and a caller, the retrieval accuracy of which should be important for quality analysts. We could have used ROUGE (Lin and Hovy, 2003), but we did not because ROUGE does not correlate well with human judgments in conversational data (Liu and Liu, 2008). Another benefit of using the F-measure is that summaries of varying lengths can be compared. 5.5 Results Table 4 shows the evaluation results for the proposed systems and the baselines. It can be seen that concat3 shows the best performance in Fmeasure among all systems, having a statistically better performance over all systems except for concat2. The CSHMMs with concatenated training were all better than ergodic0–3. Here, the performance (and output) of ergodic0–3 was exact"
C10-2046,P08-2051,0,0.0811624,"a; that is, a dialogue dealing with a certain task is evaluated using the scenario text for that task. As an evaluation criterion, we used the F-measure (F1) to evaluate the retrieval accuracy on the basis of the recall and precision of retrieved content words. We used the scenarios as references because they contain the basic content exchanged between an operator and a caller, the retrieval accuracy of which should be important for quality analysts. We could have used ROUGE (Lin and Hovy, 2003), but we did not because ROUGE does not correlate well with human judgments in conversational data (Liu and Liu, 2008). Another benefit of using the F-measure is that summaries of varying lengths can be compared. 5.5 Results Table 4 shows the evaluation results for the proposed systems and the baselines. It can be seen that concat3 shows the best performance in Fmeasure among all systems, having a statistically better performance over all systems except for concat2. The CSHMMs with concatenated training were all better than ergodic0–3. Here, the performance (and output) of ergodic0–3 was exactly the same. This happened because of the broad distributions in their common states; no paths went through the common"
C10-2046,W01-0100,0,0.158865,"alls, automatic summarization has been utilized and shown to successfully reduce costs (Byrd et al., 2008). However, one of the problems in current call summarization is that a domain ontology is required for understanding operator/caller utterances, which makes it difficult to port one summarization system from domain to domain. This paper describes a novel automatic summarization method for contact center dialogues without the costly process of creating domain on2 Related Work There is an abundance of research in automatic summarization. It has been successfully applied to single documents (Mani, 2001) as well as to multiple documents (Radev et al., 2004), and various summarization methods, such as the conventional LEAD method, machine-learning based sentence selection (Kupiec et al., 1995; Osborne, 2002), and integer linear programming (ILP) based sentence extraction (Gillick and Favre, 2009), have been proposed. Recent years have seen work on summarizing broadcast news speech (Hori and Furui, 2003), multi-party meetings (Murray et al., 2005), and contact center dialogues (Byrd et al., 2008). However, despite the large amount of previous work, little work has tackled the automatic summariz"
C10-2046,W09-3917,1,\N,Missing
C12-1071,Y09-1009,0,0.135894,"→ PERSON). Such a dictionary is also useful for deriving gazetteer features for training an NE recognizer (Kazama and Torisawa, 2008). A number of studies have focused on automatically creating NE dictionaries; e.g., (Toral and Muoz, 2006; Saleh et al., 2010). Such studies generally use a small number of entity types, mostly adopting those deﬁned in the Message Understanding Conference (MUC) (Grishman and Sundheim, 1996) or Information Retrieval and Extraction Exercise (IREX) (Sekine and Isahara, 2000). To enable more ﬁne-grained information extraction, some attempted to cover more NE types: (Chang et al., 2009), (Watanabe et al., 2007), and (Tkatchenko et al., 2011), using up to 18 NE types. Still, we consider the current granularity of the NE types used to be too coarse, especially for tasks such as question answering (Voorhees and Dang, 2005), where systems need to pinpoint exact entities requested by users. This paper proposes to create a dictionary of extended NEs (ENEs). An ENE hierarchy was proposed by Sekine et al. (2002); Sekine and Nobata (2004), and it deﬁnes three levels of NE types. At the leaf level, it has 200 ENE types. We aim to create an ENE dictionary that covers all these 200 type"
C12-1071,I08-1071,0,0.676003,"o determine the article’s NE type. Bhole et al. (2007) followed and proposed a super1164 vised machine learning approach to the same task, involving the training of an SVM classiﬁer. The features used were the bag-of-words of Wikipedia articles. In addition to the texts of articles, there are rich meta data in Wikipedia, which can be helpful in distinguishing the NE types of articles. In addition to the nouns of ﬁrst sentences, Nothman et al. (2008) used head nouns of categories assigned to articles and used heuristic rules to map them to one of four NE types; namely, LOC, PER, ORG, and MISC. Dakka and Cucerzan (2008) trained an SVM classiﬁer by using features related to the structure of Wikipedia articles. Speciﬁcally, they introduced bag-of-words features of abstracts (Wikipedia provides short abstracts for articles), tables (infoboxes and contents boxes), and links to other articles. As NE types, they used ﬁve: LOC, PER, ORG, MISC, and COMM (common object). Saleh et al. (2010) also used features derived from abstracts, infoboxes, and categories to train their SVM classiﬁer. A similar feature set was also used by (Tardif et al., 2009) who worked on six NE types. Richman and Schone (2008) exploited the mu"
C12-1071,P98-1068,0,0.41451,"ption is (Tardif et al., 2009) that used bag-of-unigrams in the title, corresponding to our T1. T2 through T16 are our newly introduced features. We enumerate the features below. (T1-T2) Word unigram/bigram We ﬁrst run the title in question through a morphological analyzer and separate it into words. Note that there are no marked word boundaries in Japanese. Then, we extract word unigrams and bigrams as features. Here, the features are bag-of-words features, indicating the existence of particular unigrams or bigrams with a binary value (i.e., 1 or 0). As a morphological analyzer, we use JTAG (Fuchi and Takagi, 1998). (T3-T4) Character unigram/bigram We split a title into character tokens and then create bag-of-words features of character unigrams and bigrams. We have this feature since characters, especially Kanji (Chinese-origin) characters in Japanese, have individual meanings even when they form part of a word. (T5-T6) POS unigram/bigram Using the results of the morphological analysis, we create bag-of-words features for the unigrams and bigrams of part-of-speech (POS) tags. Japanese POS taggers, including JTAG, generally output POS tags that correspond to subcategories for proper nouns, which can be"
C12-1071,C96-1079,0,0.102133,"arch results. In such cases, an NE dictionary, or a gazetteer, is particularly useful. Here, an NE dictionary means a list of entities associated with their NE types (e.g., Tokyo → LOCATION, Barack Obama → PERSON). Such a dictionary is also useful for deriving gazetteer features for training an NE recognizer (Kazama and Torisawa, 2008). A number of studies have focused on automatically creating NE dictionaries; e.g., (Toral and Muoz, 2006; Saleh et al., 2010). Such studies generally use a small number of entity types, mostly adopting those deﬁned in the Message Understanding Conference (MUC) (Grishman and Sundheim, 1996) or Information Retrieval and Extraction Exercise (IREX) (Sekine and Isahara, 2000). To enable more ﬁne-grained information extraction, some attempted to cover more NE types: (Chang et al., 2009), (Watanabe et al., 2007), and (Tkatchenko et al., 2011), using up to 18 NE types. Still, we consider the current granularity of the NE types used to be too coarse, especially for tasks such as question answering (Voorhees and Dang, 2005), where systems need to pinpoint exact entities requested by users. This paper proposes to create a dictionary of extended NEs (ENEs). An ENE hierarchy was proposed by"
C12-1071,C02-1054,0,0.103893,"Missing"
C12-1071,P08-1047,0,0.0688566,"itional random ﬁelds (Suzuki et al., 2006), using words surrounding a target entity as cues to determine if that entity belongs to a certain NE type. The limitation is that it is difﬁcult to recognize NEs when there are few contextual cues, such as in search queries and snippets of web search results. In such cases, an NE dictionary, or a gazetteer, is particularly useful. Here, an NE dictionary means a list of entities associated with their NE types (e.g., Tokyo → LOCATION, Barack Obama → PERSON). Such a dictionary is also useful for deriving gazetteer features for training an NE recognizer (Kazama and Torisawa, 2008). A number of studies have focused on automatically creating NE dictionaries; e.g., (Toral and Muoz, 2006; Saleh et al., 2010). Such studies generally use a small number of entity types, mostly adopting those deﬁned in the Message Understanding Conference (MUC) (Grishman and Sundheim, 1996) or Information Retrieval and Extraction Exercise (IREX) (Sekine and Isahara, 2000). To enable more ﬁne-grained information extraction, some attempted to cover more NE types: (Chang et al., 2009), (Watanabe et al., 2007), and (Tkatchenko et al., 2011), using up to 18 NE types. Still, we consider the current"
C12-1071,W10-3302,0,0.0244811,"e feasibility of creating such a ﬁne-grained NE dictionary and want to explore useful features for the classiﬁcation into ENE types. To this end, we make an extensive list of features, adopting those previously proposed and also proposing new ones, for training our classiﬁer. We describe our features later in Section 3.2. Although not directly related to creating an NE dictionary, there is a good body of work that aims at constructing an ontology (a hierarchy of words or concepts connected with relations such as is-a and has-a) from Wikipedia (Ponzetto and Strube, 2007; Suchanek et al., 2008; Nagata et al., 2010). Since ontologies are useful resources for the deep processing of texts, such as the inference, once we have created our ENE dictionary, our next step would include constructing an ontology by relating the ENEs. 3 Proposed Method Following the previous studies, we also propose to use supervised learning and learn a classiﬁer that classiﬁes Wikipedia titles (articles) into ENEs. We have three steps: the creation of training data, extraction of features, and training of a classiﬁer. Since we deal with many NE types (200 ENE types), we place a special emphasis on the extraction of features for a"
C12-1071,U08-1016,0,0.121551,"ted the number of nouns related to three NE types, namely, Location, Organization, and Person, and applied heuristic rules that take into account the numbers in order to determine the article’s NE type. Bhole et al. (2007) followed and proposed a super1164 vised machine learning approach to the same task, involving the training of an SVM classiﬁer. The features used were the bag-of-words of Wikipedia articles. In addition to the texts of articles, there are rich meta data in Wikipedia, which can be helpful in distinguishing the NE types of articles. In addition to the nouns of ﬁrst sentences, Nothman et al. (2008) used head nouns of categories assigned to articles and used heuristic rules to map them to one of four NE types; namely, LOC, PER, ORG, and MISC. Dakka and Cucerzan (2008) trained an SVM classiﬁer by using features related to the structure of Wikipedia articles. Speciﬁcally, they introduced bag-of-words features of abstracts (Wikipedia provides short abstracts for articles), tables (infoboxes and contents boxes), and links to other articles. As NE types, they used ﬁve: LOC, PER, ORG, MISC, and COMM (common object). Saleh et al. (2010) also used features derived from abstracts, infoboxes, and"
C12-1071,P08-1001,0,0.143498,", ORG, and MISC. Dakka and Cucerzan (2008) trained an SVM classiﬁer by using features related to the structure of Wikipedia articles. Speciﬁcally, they introduced bag-of-words features of abstracts (Wikipedia provides short abstracts for articles), tables (infoboxes and contents boxes), and links to other articles. As NE types, they used ﬁve: LOC, PER, ORG, MISC, and COMM (common object). Saleh et al. (2010) also used features derived from abstracts, infoboxes, and categories to train their SVM classiﬁer. A similar feature set was also used by (Tardif et al., 2009) who worked on six NE types. Richman and Schone (2008) exploited the multilingual nature of Wikipedia. By using links to articles in other languages, they classiﬁed non-English articles into NE types by using their English counterparts, whose NE types can be estimated by using their category information. They used four NE types: DATE, GPE (geographical and political entity), ORG, and PERSON. The above studies used a relatively small number of NE types, but there are also studies that aimed to cover a larger number of NE types. Chang et al. (2009) used nine NE types (person, act, communication, location, animal, artifact, time, object, and group),"
C12-1071,W03-1506,1,0.748245,"sible lack of information that arises from using only the semantic categories of the last common noun or noun/counter sufﬁx. (T13) Proper noun semantic categories JTAG outputs special semantic categories for proper nouns. They are deﬁned in the Goi-Taikei ontology and there are 130 such categories. Since proper nouns are conceptually similar to NEs, these categories will be useful for the classiﬁcation of ENE types. We extract proper noun semantic categories for all words in a title and create their bag-of-words features. (T14) IREX-based NEs We run an off-the-shelf NE recognizer, NameLister (Saito and Nagata, 2003; Suzuki et al., 2006), and extract IREX-based NEs in the title, from which we create bag-of-words features, indicating the existence of each of eight NE types in IREX. Here, the granularity of NEs is coarse; we regard this feature as a complement to other semantic category related features. (T15) Last character type Japanese has several characters types, and certain types can be indicative of certain NE types. For example, Katakana characters are likely to be used for entities of a foreign origin such as cars and products, whereas Hiragana characters are likely to be used for Japanese entitie"
C12-1071,W10-2414,0,0.148522,"certain NE type. The limitation is that it is difﬁcult to recognize NEs when there are few contextual cues, such as in search queries and snippets of web search results. In such cases, an NE dictionary, or a gazetteer, is particularly useful. Here, an NE dictionary means a list of entities associated with their NE types (e.g., Tokyo → LOCATION, Barack Obama → PERSON). Such a dictionary is also useful for deriving gazetteer features for training an NE recognizer (Kazama and Torisawa, 2008). A number of studies have focused on automatically creating NE dictionaries; e.g., (Toral and Muoz, 2006; Saleh et al., 2010). Such studies generally use a small number of entity types, mostly adopting those deﬁned in the Message Understanding Conference (MUC) (Grishman and Sundheim, 1996) or Information Retrieval and Extraction Exercise (IREX) (Sekine and Isahara, 2000). To enable more ﬁne-grained information extraction, some attempted to cover more NE types: (Chang et al., 2009), (Watanabe et al., 2007), and (Tkatchenko et al., 2011), using up to 18 NE types. Still, we consider the current granularity of the NE types used to be too coarse, especially for tasks such as question answering (Voorhees and Dang, 2005),"
C12-1071,sekine-isahara-2000-irex,0,0.0483447,"Here, an NE dictionary means a list of entities associated with their NE types (e.g., Tokyo → LOCATION, Barack Obama → PERSON). Such a dictionary is also useful for deriving gazetteer features for training an NE recognizer (Kazama and Torisawa, 2008). A number of studies have focused on automatically creating NE dictionaries; e.g., (Toral and Muoz, 2006; Saleh et al., 2010). Such studies generally use a small number of entity types, mostly adopting those deﬁned in the Message Understanding Conference (MUC) (Grishman and Sundheim, 1996) or Information Retrieval and Extraction Exercise (IREX) (Sekine and Isahara, 2000). To enable more ﬁne-grained information extraction, some attempted to cover more NE types: (Chang et al., 2009), (Watanabe et al., 2007), and (Tkatchenko et al., 2011), using up to 18 NE types. Still, we consider the current granularity of the NE types used to be too coarse, especially for tasks such as question answering (Voorhees and Dang, 2005), where systems need to pinpoint exact entities requested by users. This paper proposes to create a dictionary of extended NEs (ENEs). An ENE hierarchy was proposed by Sekine et al. (2002); Sekine and Nobata (2004), and it deﬁnes three levels of NE t"
C12-1071,sekine-nobata-2004-definition,0,0.0983,"al and Extraction Exercise (IREX) (Sekine and Isahara, 2000). To enable more ﬁne-grained information extraction, some attempted to cover more NE types: (Chang et al., 2009), (Watanabe et al., 2007), and (Tkatchenko et al., 2011), using up to 18 NE types. Still, we consider the current granularity of the NE types used to be too coarse, especially for tasks such as question answering (Voorhees and Dang, 2005), where systems need to pinpoint exact entities requested by users. This paper proposes to create a dictionary of extended NEs (ENEs). An ENE hierarchy was proposed by Sekine et al. (2002); Sekine and Nobata (2004), and it deﬁnes three levels of NE types. At the leaf level, it has 200 ENE types. We aim to create an ENE dictionary that covers all these 200 types. In our approach, using Wikipedia as a basic resource, we classify Wikipedia titles into ENE types to create an ENE dictionary. We perform supervised learning; we derive a large number of features for Wikipedia titles and train a multiclass classiﬁer. The features encode various aspects of Wikipedia titles, including those related to the surface string of a title, the content of an article, and the meta data, such as categories and infoboxes. We"
C12-1071,sekine-etal-2002-extended,0,0.279901,"or Information Retrieval and Extraction Exercise (IREX) (Sekine and Isahara, 2000). To enable more ﬁne-grained information extraction, some attempted to cover more NE types: (Chang et al., 2009), (Watanabe et al., 2007), and (Tkatchenko et al., 2011), using up to 18 NE types. Still, we consider the current granularity of the NE types used to be too coarse, especially for tasks such as question answering (Voorhees and Dang, 2005), where systems need to pinpoint exact entities requested by users. This paper proposes to create a dictionary of extended NEs (ENEs). An ENE hierarchy was proposed by Sekine et al. (2002); Sekine and Nobata (2004), and it deﬁnes three levels of NE types. At the leaf level, it has 200 ENE types. We aim to create an ENE dictionary that covers all these 200 types. In our approach, using Wikipedia as a basic resource, we classify Wikipedia titles into ENE types to create an ENE dictionary. We perform supervised learning; we derive a large number of features for Wikipedia titles and train a multiclass classiﬁer. The features encode various aspects of Wikipedia titles, including those related to the surface string of a title, the content of an article, and the meta data, such as cat"
C12-1071,P06-1028,0,0.0170567,"n that arises from using only the semantic categories of the last common noun or noun/counter sufﬁx. (T13) Proper noun semantic categories JTAG outputs special semantic categories for proper nouns. They are deﬁned in the Goi-Taikei ontology and there are 130 such categories. Since proper nouns are conceptually similar to NEs, these categories will be useful for the classiﬁcation of ENE types. We extract proper noun semantic categories for all words in a title and create their bag-of-words features. (T14) IREX-based NEs We run an off-the-shelf NE recognizer, NameLister (Saito and Nagata, 2003; Suzuki et al., 2006), and extract IREX-based NEs in the title, from which we create bag-of-words features, indicating the existence of each of eight NE types in IREX. Here, the granularity of NEs is coarse; we regard this feature as a complement to other semantic category related features. (T15) Last character type Japanese has several characters types, and certain types can be indicative of certain NE types. For example, Katakana characters are likely to be used for entities of a foreign origin such as cars and products, whereas Hiragana characters are likely to be used for Japanese entities. Here, we distinguis"
C12-1071,U09-1015,0,0.509237,"ap them to one of four NE types; namely, LOC, PER, ORG, and MISC. Dakka and Cucerzan (2008) trained an SVM classiﬁer by using features related to the structure of Wikipedia articles. Speciﬁcally, they introduced bag-of-words features of abstracts (Wikipedia provides short abstracts for articles), tables (infoboxes and contents boxes), and links to other articles. As NE types, they used ﬁve: LOC, PER, ORG, MISC, and COMM (common object). Saleh et al. (2010) also used features derived from abstracts, infoboxes, and categories to train their SVM classiﬁer. A similar feature set was also used by (Tardif et al., 2009) who worked on six NE types. Richman and Schone (2008) exploited the multilingual nature of Wikipedia. By using links to articles in other languages, they classiﬁed non-English articles into NE types by using their English counterparts, whose NE types can be estimated by using their category information. They used four NE types: DATE, GPE (geographical and political entity), ORG, and PERSON. The above studies used a relatively small number of NE types, but there are also studies that aimed to cover a larger number of NE types. Chang et al. (2009) used nine NE types (person, act, communication,"
C12-1071,W06-2809,0,0.913214,"t entity belongs to a certain NE type. The limitation is that it is difﬁcult to recognize NEs when there are few contextual cues, such as in search queries and snippets of web search results. In such cases, an NE dictionary, or a gazetteer, is particularly useful. Here, an NE dictionary means a list of entities associated with their NE types (e.g., Tokyo → LOCATION, Barack Obama → PERSON). Such a dictionary is also useful for deriving gazetteer features for training an NE recognizer (Kazama and Torisawa, 2008). A number of studies have focused on automatically creating NE dictionaries; e.g., (Toral and Muoz, 2006; Saleh et al., 2010). Such studies generally use a small number of entity types, mostly adopting those deﬁned in the Message Understanding Conference (MUC) (Grishman and Sundheim, 1996) or Information Retrieval and Extraction Exercise (IREX) (Sekine and Isahara, 2000). To enable more ﬁne-grained information extraction, some attempted to cover more NE types: (Chang et al., 2009), (Watanabe et al., 2007), and (Tkatchenko et al., 2011), using up to 18 NE types. Still, we consider the current granularity of the NE types used to be too coarse, especially for tasks such as question answering (Voorh"
C12-1071,D07-1068,0,0.540336,"ionary is also useful for deriving gazetteer features for training an NE recognizer (Kazama and Torisawa, 2008). A number of studies have focused on automatically creating NE dictionaries; e.g., (Toral and Muoz, 2006; Saleh et al., 2010). Such studies generally use a small number of entity types, mostly adopting those deﬁned in the Message Understanding Conference (MUC) (Grishman and Sundheim, 1996) or Information Retrieval and Extraction Exercise (IREX) (Sekine and Isahara, 2000). To enable more ﬁne-grained information extraction, some attempted to cover more NE types: (Chang et al., 2009), (Watanabe et al., 2007), and (Tkatchenko et al., 2011), using up to 18 NE types. Still, we consider the current granularity of the NE types used to be too coarse, especially for tasks such as question answering (Voorhees and Dang, 2005), where systems need to pinpoint exact entities requested by users. This paper proposes to create a dictionary of extended NEs (ENEs). An ENE hierarchy was proposed by Sekine et al. (2002); Sekine and Nobata (2004), and it deﬁnes three levels of NE types. At the leaf level, it has 200 ENE types. We aim to create an ENE dictionary that covers all these 200 types. In our approach, using"
C12-1071,W09-3301,0,0.0203093,"corpus publicly available (Hashimoto et al., 2008). The corpus contains newswire articles in which entities are annotated with ENE tags. From such a corpus, we can extract entities together with their ENE tags to create what we call a seed dictionary, whose entries can be matched against Wikipedia titles to take an intersection so that Wikipedia titles can be automatically annotated with their ENE types. Of course, when there are other corpora or gazetteers available, they can also be exploited to augment the seed dictionary. The approach we employ here is similar to (Bhole et al., 2007) and (Zhang and Iria, 2009) in that entries of external dictionaries/gazetteers are intersected with Wikipedia titles to create training data. The difference is that we use an annotated corpus to create such entries. Below, we enumerate the steps we performed to create our seed dictionary and training data. 1. From the Hashimoto corpus (Hashimoto et al., 2008), we extracted all tagged sections. There are 8828 newswire articles (Mainichi Shimbun newspaper ’95) in the corpus with 255407 tagged sections. Since some tags are not related to ENE types, we ﬁrst discarded such tagged sections. We also discarded entities that we"
C12-1071,C98-1065,0,\N,Missing
C14-1077,W04-2412,0,0.0257716,"Missing"
C14-1077,W05-0620,0,0.0339116,"Missing"
C14-1077,P07-1033,0,0.0380565,"Missing"
C14-1077,J12-4003,0,0.0161288,"Journal), the shared tasks were evaluated on newspaper articles. M`arquez et al. (2008) provides a review of SRL. OntoNotes Corpus (Hovy et al., 2006) contains multiple genres such as newswire, broadcast news, broadcast conversation. The annotation to OntoNotes includes semantic role labels compliant with the Proposition Bank. It is currently used for coreference resolution (Pradhan et al., 2012), and is expected to be applied to dialogue analysis. A few SRL studies have focused on not only verbal predicates (e.g., ‘decide’) but also nominal predicates (e.g., ‘decision’) (Jiang and Ng, 2006; Gerber and Chai, 2012; Laparra and Rigau, 2013). Because the subject and object of nominal predicates are frequently omitted (e.g., the object in the phrase “the decision” is omitted), problems similar to the Japanese zero-pronouns have to be resolved in the SRL of nominal predicates. 2.2 Predicate-Argument Structure Analyses in Japanese Japanese material includes the NAIST Text Corpus (Iida et al., 2007)2 , which is an annotated corpus of predicate-argument structures and coreference information for newspaper articles. Argument noun phrases of the nominative, accusative, and dative cases are assigned to each pred"
C14-1077,J02-3001,0,0.521269,"owledge, this is the first paper to describe a PASA for dialogues that include many zero-pronouns. The paper is organized as follows. Section 2 briefly reviews SRL/PASA in English and Japanese. Section 3 discusses characteristics of chat dialogues by comparing two annotated corpora, newspaper articles and dialogues. Section 4 describes the basic strategy of our PASA, and Section 5 shows how it was adapted for dialogues. Experiments are presented in Section 6, and Section 7 concludes the paper. 2 Related Work 2.1 Semantic Role Labeling in English The advent of the supervised method proposed by Gildea and Jurafsky (2002) has led to the creation of annotated corpora for semantic role labeling. In the CoNLL-2004 and 2005 shared task (Carreras and M`arquez, 2004; Carreras and M`arquez, 2005), evaluations were carried out using the Proposition Bank (Palmer et al., 2005). Because the Proposition Bank was annotated to the Penn Treebank (i.e., the source texts were from the Wall Street Journal), the shared tasks were evaluated on newspaper articles. M`arquez et al. (2008) provides a review of SRL. OntoNotes Corpus (Hovy et al., 2006) contains multiple genres such as newswire, broadcast news, broadcast conversation."
C14-1077,C14-1088,1,0.87563,"Missing"
C14-1077,N06-2015,0,0.0227242,"ic Role Labeling in English The advent of the supervised method proposed by Gildea and Jurafsky (2002) has led to the creation of annotated corpora for semantic role labeling. In the CoNLL-2004 and 2005 shared task (Carreras and M`arquez, 2004; Carreras and M`arquez, 2005), evaluations were carried out using the Proposition Bank (Palmer et al., 2005). Because the Proposition Bank was annotated to the Penn Treebank (i.e., the source texts were from the Wall Street Journal), the shared tasks were evaluated on newspaper articles. M`arquez et al. (2008) provides a review of SRL. OntoNotes Corpus (Hovy et al., 2006) contains multiple genres such as newswire, broadcast news, broadcast conversation. The annotation to OntoNotes includes semantic role labels compliant with the Proposition Bank. It is currently used for coreference resolution (Pradhan et al., 2012), and is expected to be applied to dialogue analysis. A few SRL studies have focused on not only verbal predicates (e.g., ‘decide’) but also nominal predicates (e.g., ‘decision’) (Jiang and Ng, 2006; Gerber and Chai, 2012; Laparra and Rigau, 2013). Because the subject and object of nominal predicates are frequently omitted (e.g., the object in the p"
C14-1077,W07-1522,0,0.0293426,"al., 2012), and is expected to be applied to dialogue analysis. A few SRL studies have focused on not only verbal predicates (e.g., ‘decide’) but also nominal predicates (e.g., ‘decision’) (Jiang and Ng, 2006; Gerber and Chai, 2012; Laparra and Rigau, 2013). Because the subject and object of nominal predicates are frequently omitted (e.g., the object in the phrase “the decision” is omitted), problems similar to the Japanese zero-pronouns have to be resolved in the SRL of nominal predicates. 2.2 Predicate-Argument Structure Analyses in Japanese Japanese material includes the NAIST Text Corpus (Iida et al., 2007)2 , which is an annotated corpus of predicate-argument structures and coreference information for newspaper articles. Argument noun phrases of the nominative, accusative, and dative cases are assigned to each predicate. The predicate and the noun phrases are not limited to the same sentence. If arguments of the predicate are represented as zero-pronouns, the antecedent noun phrases in other sentences are assigned as the arguments. Many PASA methods have been studied on the NAIST Text Corpus (Komachi et al., 2007; Taira et al., 2008; Imamura et al., 2009; Yoshikawa et al., 2011). In Japanese, s"
C14-1077,P09-2022,1,0.942058,"e material includes the NAIST Text Corpus (Iida et al., 2007)2 , which is an annotated corpus of predicate-argument structures and coreference information for newspaper articles. Argument noun phrases of the nominative, accusative, and dative cases are assigned to each predicate. The predicate and the noun phrases are not limited to the same sentence. If arguments of the predicate are represented as zero-pronouns, the antecedent noun phrases in other sentences are assigned as the arguments. Many PASA methods have been studied on the NAIST Text Corpus (Komachi et al., 2007; Taira et al., 2008; Imamura et al., 2009; Yoshikawa et al., 2011). In Japanese, some of them simultaneously resolve the zero-anaphora caused by zero-pronouns. Most English SRL and Japanese PASA currently target newspaper articles, and it is unclear whether the methods for newspapers can be applied to dialogue conversations. 3 Characteristics of Chat Dialogues We first collected chat dialogues of two speakers and annotated them with the predicate-argument structure. The participants chatted via keyboard input. Therefore, fillers and repetitions, which are frequent in speech dialogues, were rare. The theme was one of 20 topics, such a"
C14-1077,W06-1617,0,0.0269758,"from the Wall Street Journal), the shared tasks were evaluated on newspaper articles. M`arquez et al. (2008) provides a review of SRL. OntoNotes Corpus (Hovy et al., 2006) contains multiple genres such as newswire, broadcast news, broadcast conversation. The annotation to OntoNotes includes semantic role labels compliant with the Proposition Bank. It is currently used for coreference resolution (Pradhan et al., 2012), and is expected to be applied to dialogue analysis. A few SRL studies have focused on not only verbal predicates (e.g., ‘decide’) but also nominal predicates (e.g., ‘decision’) (Jiang and Ng, 2006; Gerber and Chai, 2012; Laparra and Rigau, 2013). Because the subject and object of nominal predicates are frequently omitted (e.g., the object in the phrase “the decision” is omitted), problems similar to the Japanese zero-pronouns have to be resolved in the SRL of nominal predicates. 2.2 Predicate-Argument Structure Analyses in Japanese Japanese material includes the NAIST Text Corpus (Iida et al., 2007)2 , which is an annotated corpus of predicate-argument structures and coreference information for newspaper articles. Argument noun phrases of the nominative, accusative, and dative cases ar"
C14-1077,C02-1122,0,0.0526739,"the weights in the respective space, source or target, are emphasized. 4. When the argument is identified, the selectors use only the features in the common and target spaces. The parameters in the spaces are optimized to the target domain, plus we can utilize the features that appear only in the source domain data. 5.2 Weak Knowledge Acquisition from Very Large Resources In this paper, we use two types of knowledge to reduce the harmful effect of out-of-vocabulary in the training corpus. Both types are constructed by automatically analyzing, summing up, and filtering large 811 text corpora (Kawahara and Kurohashi, 2002; Sasano et al., 2008; Sasano et al., 2013). They provide information about unknown words with some confidence but they do contain some errors. We use them as the features of the models, and parameters are optimized by the discriminative learning of the selectors. 5.2.1 Obligatory Case Information (Frame Feature) Case frames are important clues for SRL and PASA. The obligatory case information (OCI) comprises subsets of the case frames that only clarify whether the cases of each predicate are necessary or not. The OCI dictionary is automatically constructed from large text corpora as follows."
C14-1077,W02-2016,0,0.0852998,"aper articles as the base PASA in this paper. It can simultaneously identify arguments of a predicate in the sentence, those in other sentences, and exophoric arguments. The analyzer receives the entire article (dialogue) and performs the following steps for each sentence (utterance). 1. The input sentences are tagged and parsed. During parsing, the base phrases and their headwords are also identified. At this time, the part-of-speech tags and the parse trees of the Dialogue Corpus are supplied by applying the morphological analyzer MeCab (Kudo et al., 2004) and the dependency parser CaboCha (Kudo and Matsumoto, 2002). The NAIST Corpus version 1.5 already includes the part-of-speech tags and the parse trees. 2. Predicate phrases are identified from the sentences. We use the correct predicates in the corpora for the evaluation. When we build dialogue systems on PASA, predicate phrases will be identified using part-of-speech patterns that include verbs, adjectives, and copular verbs. 3. For each predicate, candidate arguments are acquired from the sentence that includes the predicate (called the current sentence) and the past sentences. Concretely, the following base phrases are regarded as candidates. • All"
C14-1077,W04-3230,0,0.0634939,"use Imamura et al. (2009)’s method developed for newspaper articles as the base PASA in this paper. It can simultaneously identify arguments of a predicate in the sentence, those in other sentences, and exophoric arguments. The analyzer receives the entire article (dialogue) and performs the following steps for each sentence (utterance). 1. The input sentences are tagged and parsed. During parsing, the base phrases and their headwords are also identified. At this time, the part-of-speech tags and the parse trees of the Dialogue Corpus are supplied by applying the morphological analyzer MeCab (Kudo et al., 2004) and the dependency parser CaboCha (Kudo and Matsumoto, 2002). The NAIST Corpus version 1.5 already includes the part-of-speech tags and the parse trees. 2. Predicate phrases are identified from the sentences. We use the correct predicates in the corpora for the evaluation. When we build dialogue systems on PASA, predicate phrases will be identified using part-of-speech patterns that include verbs, adjectives, and copular verbs. 3. For each predicate, candidate arguments are acquired from the sentence that includes the predicate (called the current sentence) and the past sentences. Concretely,"
C14-1077,P13-1116,0,0.012307,"asks were evaluated on newspaper articles. M`arquez et al. (2008) provides a review of SRL. OntoNotes Corpus (Hovy et al., 2006) contains multiple genres such as newswire, broadcast news, broadcast conversation. The annotation to OntoNotes includes semantic role labels compliant with the Proposition Bank. It is currently used for coreference resolution (Pradhan et al., 2012), and is expected to be applied to dialogue analysis. A few SRL studies have focused on not only verbal predicates (e.g., ‘decide’) but also nominal predicates (e.g., ‘decision’) (Jiang and Ng, 2006; Gerber and Chai, 2012; Laparra and Rigau, 2013). Because the subject and object of nominal predicates are frequently omitted (e.g., the object in the phrase “the decision” is omitted), problems similar to the Japanese zero-pronouns have to be resolved in the SRL of nominal predicates. 2.2 Predicate-Argument Structure Analyses in Japanese Japanese material includes the NAIST Text Corpus (Iida et al., 2007)2 , which is an annotated corpus of predicate-argument structures and coreference information for newspaper articles. Argument noun phrases of the nominative, accusative, and dative cases are assigned to each predicate. The predicate and t"
C14-1077,J08-2001,0,0.0260886,"Missing"
C14-1077,J05-1004,0,0.0124007,"ring two annotated corpora, newspaper articles and dialogues. Section 4 describes the basic strategy of our PASA, and Section 5 shows how it was adapted for dialogues. Experiments are presented in Section 6, and Section 7 concludes the paper. 2 Related Work 2.1 Semantic Role Labeling in English The advent of the supervised method proposed by Gildea and Jurafsky (2002) has led to the creation of annotated corpora for semantic role labeling. In the CoNLL-2004 and 2005 shared task (Carreras and M`arquez, 2004; Carreras and M`arquez, 2005), evaluations were carried out using the Proposition Bank (Palmer et al., 2005). Because the Proposition Bank was annotated to the Penn Treebank (i.e., the source texts were from the Wall Street Journal), the shared tasks were evaluated on newspaper articles. M`arquez et al. (2008) provides a review of SRL. OntoNotes Corpus (Hovy et al., 2006) contains multiple genres such as newswire, broadcast news, broadcast conversation. The annotation to OntoNotes includes semantic role labels compliant with the Proposition Bank. It is currently used for coreference resolution (Pradhan et al., 2012), and is expected to be applied to dialogue analysis. A few SRL studies have focused"
C14-1077,J08-2006,0,0.0414033,"equently, SRL and PASA are very similar tasks. We use the term predicate-argument structure analysis in this paper because most Japanese analyzers use this term. 806 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 806–815, Dublin, Ireland, August 23-29 2014. This paper presents predicate-argument structure analysis with zero-anaphora resolution for Japanese chat dialogues. Here, we regard the task of constructing PASA for dialogues as a kind of domain adaptation from newspaper articles to dialogues. M`arquez et al. (2008) and Pradhan et al. (2008) indicated that the tuning of parameter distribution and reducing the out-of-vocabulary are important for the domain adaptation of SRL. We also focus on parameter distribution and out-of-vocabulary to construct a PASA adapted to dialogues. To the best of our knowledge, this is the first paper to describe a PASA for dialogues that include many zero-pronouns. The paper is organized as follows. Section 2 briefly reviews SRL/PASA in English and Japanese. Section 3 discusses characteristics of chat dialogues by comparing two annotated corpora, newspaper articles and dialogues. Section 4 describes t"
C14-1077,C08-1097,0,0.024882,"space, source or target, are emphasized. 4. When the argument is identified, the selectors use only the features in the common and target spaces. The parameters in the spaces are optimized to the target domain, plus we can utilize the features that appear only in the source domain data. 5.2 Weak Knowledge Acquisition from Very Large Resources In this paper, we use two types of knowledge to reduce the harmful effect of out-of-vocabulary in the training corpus. Both types are constructed by automatically analyzing, summing up, and filtering large 811 text corpora (Kawahara and Kurohashi, 2002; Sasano et al., 2008; Sasano et al., 2013). They provide information about unknown words with some confidence but they do contain some errors. We use them as the features of the models, and parameters are optimized by the discriminative learning of the selectors. 5.2.1 Obligatory Case Information (Frame Feature) Case frames are important clues for SRL and PASA. The obligatory case information (OCI) comprises subsets of the case frames that only clarify whether the cases of each predicate are necessary or not. The OCI dictionary is automatically constructed from large text corpora as follows. The process assumes t"
C14-1077,D13-1121,0,0.0119466,"get, are emphasized. 4. When the argument is identified, the selectors use only the features in the common and target spaces. The parameters in the spaces are optimized to the target domain, plus we can utilize the features that appear only in the source domain data. 5.2 Weak Knowledge Acquisition from Very Large Resources In this paper, we use two types of knowledge to reduce the harmful effect of out-of-vocabulary in the training corpus. Both types are constructed by automatically analyzing, summing up, and filtering large 811 text corpora (Kawahara and Kurohashi, 2002; Sasano et al., 2008; Sasano et al., 2013). They provide information about unknown words with some confidence but they do contain some errors. We use them as the features of the models, and parameters are optimized by the discriminative learning of the selectors. 5.2.1 Obligatory Case Information (Frame Feature) Case frames are important clues for SRL and PASA. The obligatory case information (OCI) comprises subsets of the case frames that only clarify whether the cases of each predicate are necessary or not. The OCI dictionary is automatically constructed from large text corpora as follows. The process assumes that 1) most of the cas"
C14-1077,D08-1055,0,0.0205172,"in Japanese Japanese material includes the NAIST Text Corpus (Iida et al., 2007)2 , which is an annotated corpus of predicate-argument structures and coreference information for newspaper articles. Argument noun phrases of the nominative, accusative, and dative cases are assigned to each predicate. The predicate and the noun phrases are not limited to the same sentence. If arguments of the predicate are represented as zero-pronouns, the antecedent noun phrases in other sentences are assigned as the arguments. Many PASA methods have been studied on the NAIST Text Corpus (Komachi et al., 2007; Taira et al., 2008; Imamura et al., 2009; Yoshikawa et al., 2011). In Japanese, some of them simultaneously resolve the zero-anaphora caused by zero-pronouns. Most English SRL and Japanese PASA currently target newspaper articles, and it is unclear whether the methods for newspapers can be applied to dialogue conversations. 3 Characteristics of Chat Dialogues We first collected chat dialogues of two speakers and annotated them with the predicate-argument structure. The participants chatted via keyboard input. Therefore, fillers and repetitions, which are frequent in speech dialogues, were rare. The theme was on"
C14-1077,I11-1126,0,0.0128023,"e NAIST Text Corpus (Iida et al., 2007)2 , which is an annotated corpus of predicate-argument structures and coreference information for newspaper articles. Argument noun phrases of the nominative, accusative, and dative cases are assigned to each predicate. The predicate and the noun phrases are not limited to the same sentence. If arguments of the predicate are represented as zero-pronouns, the antecedent noun phrases in other sentences are assigned as the arguments. Many PASA methods have been studied on the NAIST Text Corpus (Komachi et al., 2007; Taira et al., 2008; Imamura et al., 2009; Yoshikawa et al., 2011). In Japanese, some of them simultaneously resolve the zero-anaphora caused by zero-pronouns. Most English SRL and Japanese PASA currently target newspaper articles, and it is unclear whether the methods for newspapers can be applied to dialogue conversations. 3 Characteristics of Chat Dialogues We first collected chat dialogues of two speakers and annotated them with the predicate-argument structure. The participants chatted via keyboard input. Therefore, fillers and repetitions, which are frequent in speech dialogues, were rare. The theme was one of 20 topics, such as meals, travel, hobbies,"
C14-1077,N07-1070,0,\N,Missing
C14-1088,J08-1001,0,0.0105208,"his is probably the cause. The reason could be the inconsistency of linguistic styles in Twitter or the noise that could not be suppressed by the ﬁltering. Since Twitter sentences surely augment diversity, we would like to consider ways to make better use of them, for example, by normalizing the linguistic style and applying stricter ﬁlters. There is a slight tendency for Prop (tri) to be preferred to Prop (bi), which is reasonable because it uses more context for deciding the next utterance. In the future, we would like to pursue methods that can exploit longer context, such as entity grids (Barzilay and Lapata, 2008) and co-reference structures (Swanson and Gordon, 2012). We performed a brief analysis of the collected dialogues. Table 2 shows, for each system, the number of unique utterances, unique words, utterances, words, words per utterance, and perplexity. It can be seen that the utterances of the rule-based system are very rigid: the perplexity is very low (23.46) and there are only 353 unique utterances, which is about half of that of the other systems. It is interesting that, despite this fact, the rule-based system was perceived to produce the most diverse utterances by questionnaire. Since the r"
C14-1088,W12-1631,0,0.0178617,"es. Experimental results using human subjects show that our architecture achieves signiﬁcantly better naturalness than a retrieval-based baseline and that its naturalness is close to that of a rule-based system using 149K hand-crafted rules. 1 Introduction Although task-oriented dialogue systems have been extensively researched over the decades (Walker et al., 2001; Williams et al., 2013), it is only recently that non-task-oriented dialogue, open-domain conversation, or chat has been attracting attention for its social and entertainment aspects (Bickmore and Picard, 2005; Ritter et al., 2011; Bessho et al., 2012). Creating an open-domain conversational system is a challenging problem. In task-oriented dialogue systems, it is possible to prepare knowledge for a domain and create understanding and generation modules for that domain (Nakano et al., 2000). However, for open-domain conversation, such preparation cannot be performed. Since it is difﬁcult to handle users’ open-domain utterances, to create workable systems, conventional approaches have used hand-crafted rules (Wallace, 2004). Although elaborate rules may work well, the problem with the rulebased approach is the high cost and the dependence on"
C14-1088,P98-1068,0,0.0459333,"Missing"
C14-1088,J86-3001,0,0.216438,"t and extendable with other modalities. In Section 2, we describe the architecture and its underlying modules. In Section 3, we describe the rule-based and retrieval-based systems that we use for comparison. In Section 4, we describe the experiment we performed to evaluate our system. Section 5 summarizes the paper. 2 Architecture and System Description Figure 1 shows the architecture we propose for an open-domain conversational system. The architecture has three main components: utterance understanding, dialogue control, and utterance generation. Following the literature on discourse theory (Grosz and Sidner, 1986), we regard intention (intentional structure), topic (attention state), and content (linguistic structure) as three important elements in conversation, and seek to create a system that can understand and generate on the basis of them in a general way. The dialogue control component works by ranking utterance candidates using a general coherence criterion (Hovy, 1991). Note that the overall architecture is roughly the same as conventional dialogue systems; however, the internal architecture is different so as to allow open-domain conversation. To give a rough idea of how the system works, Figur"
C14-1088,C92-2082,0,0.0734472,"Missing"
C14-1088,D08-1040,0,0.0676352,"Missing"
C14-1088,P09-2022,1,0.290908,"Missing"
C14-1088,C14-1077,1,0.84913,"Missing"
C14-1088,P03-1069,0,0.029232,"Missing"
C14-1088,W00-1017,0,0.0838856,"Introduction Although task-oriented dialogue systems have been extensively researched over the decades (Walker et al., 2001; Williams et al., 2013), it is only recently that non-task-oriented dialogue, open-domain conversation, or chat has been attracting attention for its social and entertainment aspects (Bickmore and Picard, 2005; Ritter et al., 2011; Bessho et al., 2012). Creating an open-domain conversational system is a challenging problem. In task-oriented dialogue systems, it is possible to prepare knowledge for a domain and create understanding and generation modules for that domain (Nakano et al., 2000). However, for open-domain conversation, such preparation cannot be performed. Since it is difﬁcult to handle users’ open-domain utterances, to create workable systems, conventional approaches have used hand-crafted rules (Wallace, 2004). Although elaborate rules may work well, the problem with the rulebased approach is the high cost and the dependence on individual skills of developers, which hinders systematic development. Another problem with the rule-based approach is its low coverage; that is, the inability to handle unexpected utterances. The recent increase of web data has propelled the"
C14-1088,P08-2043,0,0.00712674,"ones by a cut-off threshold of ten occurrences, we obtained 22K utterance pairs. The input to this module is the user utterance string, and the module outputs utterances from matched utterance pairs. User PAS: This module uses the PASs of the user utterance and the next dialogue-act. It performs the same operation as the PAS-based generation and returns the converted sentences. The merit of this module is that the system can use the user’s content in its utterance, which has been found to be useful in casual conversation for showing understanding (Ivey et al., 2013) and entraining with users (Nenkova et al., 2008). 3 Rule-based and Retrieval-based Systems For comparison, we prepared a rule-based system and a retrieval-based one. Since there is no offthe-shelf rule-based system in Japanese, we created one on our own. Because we wanted to compare our system with a state-of-the-art rule-based system, we put a great deal of effort in its development. Remember that creating rules is still the standard way of creating an open-domain conversational system. Last year’s Loebner Prize (a chatbot contest) winner, Mitsuku, was based on rules written in artiﬁcial intelligence markup language (AIML) (Wallace, 2004)."
C14-1088,D11-1054,0,0.150994,"e processing techniques. Experimental results using human subjects show that our architecture achieves signiﬁcantly better naturalness than a retrieval-based baseline and that its naturalness is close to that of a rule-based system using 149K hand-crafted rules. 1 Introduction Although task-oriented dialogue systems have been extensively researched over the decades (Walker et al., 2001; Williams et al., 2013), it is only recently that non-task-oriented dialogue, open-domain conversation, or chat has been attracting attention for its social and entertainment aspects (Bickmore and Picard, 2005; Ritter et al., 2011; Bessho et al., 2012). Creating an open-domain conversational system is a challenging problem. In task-oriented dialogue systems, it is possible to prepare knowledge for a domain and create understanding and generation modules for that domain (Nakano et al., 2000). However, for open-domain conversation, such preparation cannot be performed. Since it is difﬁcult to handle users’ open-domain utterances, to create workable systems, conventional approaches have used hand-crafted rules (Wallace, 2004). Although elaborate rules may work well, the problem with the rulebased approach is the high cost"
C14-1088,W13-4051,1,0.434919,"Missing"
C14-1088,voorhees-tice-2000-trec,0,0.241798,"Missing"
C14-1088,P01-1066,0,0.130789,"suo.yoshihiro}@lab.ntt.co.jp Abstract This paper proposes an architecture for an open-domain conversational system and evaluates an implemented system. The proposed architecture is fully composed of modules based on natural language processing techniques. Experimental results using human subjects show that our architecture achieves signiﬁcantly better naturalness than a retrieval-based baseline and that its naturalness is close to that of a rule-based system using 149K hand-crafted rules. 1 Introduction Although task-oriented dialogue systems have been extensively researched over the decades (Walker et al., 2001; Williams et al., 2013), it is only recently that non-task-oriented dialogue, open-domain conversation, or chat has been attracting attention for its social and entertainment aspects (Bickmore and Picard, 2005; Ritter et al., 2011; Bessho et al., 2012). Creating an open-domain conversational system is a challenging problem. In task-oriented dialogue systems, it is possible to prepare knowledge for a domain and create understanding and generation modules for that domain (Nakano et al., 2000). However, for open-domain conversation, such preparation cannot be performed. Since it is difﬁcult to h"
C14-1088,W13-4065,0,0.00506369,"t.co.jp Abstract This paper proposes an architecture for an open-domain conversational system and evaluates an implemented system. The proposed architecture is fully composed of modules based on natural language processing techniques. Experimental results using human subjects show that our architecture achieves signiﬁcantly better naturalness than a retrieval-based baseline and that its naturalness is close to that of a rule-based system using 149K hand-crafted rules. 1 Introduction Although task-oriented dialogue systems have been extensively researched over the decades (Walker et al., 2001; Williams et al., 2013), it is only recently that non-task-oriented dialogue, open-domain conversation, or chat has been attracting attention for its social and entertainment aspects (Bickmore and Picard, 2005; Ritter et al., 2011; Bessho et al., 2012). Creating an open-domain conversational system is a challenging problem. In task-oriented dialogue systems, it is possible to prepare knowledge for a domain and create understanding and generation modules for that domain (Nakano et al., 2000). However, for open-domain conversation, such preparation cannot be performed. Since it is difﬁcult to handle users’ open-domain"
C14-1088,J94-2003,0,\N,Missing
C14-1088,C98-1065,0,\N,Missing
C18-1304,D16-1136,0,0.0401907,"between languages, mainly from the resource-rich language to the resource-poor language. The knowledge transfer is achieved by learning common semantic representations for different This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 3586 Proceedings of the 27th International Conference on Computational Linguistics, pages 3586–3596 Santa Fe, New Mexico, USA, August 20-26, 2018. languages. Usually, word-aligned or sentence-aligned parallel data sets are employed for joint learning (Guo et al., 2016; Duong et al., 2016) However, most existing joint learning approaches focus only on cross-tasks or cross-lingual knowledge transfer. In fact, task-aligned multi-lingual data sets have been rarely utilized for joint learning (Mogadala and Rettinger, 2016; Pappas and Popescu-Belis, 2017). We can expect to enhance lexical utterance classification performance by achieving effective knowledge transfer among both different tasks and different languages. In this paper, we propose multi-task and multi-lingual joint learning; it can enhance neural lexical utterance classification by flexibly transferring knowledge among b"
C18-1304,N16-1101,0,0.0285465,"ing (Mogadala and Rettinger, 2016; Pappas and Popescu-Belis, 2017). We can expect to enhance lexical utterance classification performance by achieving effective knowledge transfer among both different tasks and different languages. In this paper, we propose multi-task and multi-lingual joint learning; it can enhance neural lexical utterance classification by flexibly transferring knowledge among both different tasks and different languages. The proposed method is closely related to multi-task sequence-to-sequence learning (Luong et al., 2016) including many-to-many neural machine translation (Firat et al., 2016; Firat et al., 2017; Schwenk and Douze, 2017). While input and output components are easily distinguished in sequenceto-sequence models, neural lexical utterance classification methods are not explicitly divided into input and output components. Our idea is to divide the neural lexical utterance classification into two components. The language-specific components converts words to hidden representations while task-specific components convert the hidden representations into prediction probabilities. The former can be shared between different tasks and the latter can be shared between different"
C18-1304,C12-1071,1,0.824108,"ng. Partially-shared modeling can be utilized for various neural network based joint learning schemes. We demonstrate the superiority of partially-shared modeling over fully-shared modeling. In addition, we reveal the properties of partially-shared modeling. • This paper introduces a new corpus for evaluating multi-task and multi-lingual lexical utterance classification methods. The corpus includes Japanese and English data sets with three different lexical utterance classification tasks. The tasks are dialogue act classification, extended named entity classification (Sekine and Nobata, 2004; Higashinaka et al., 2012), and question type classification. 2 Neural Lexical Utterance Classification This section details neural lexical utterance classification. Lexical utterance classification is the problem of determining the correct label l ∈ {l1 , · · · , lK } of given utterance W = {w1 , · · · , wT }. In neural lexical utterance classification, conditional probabilities for each label given utterance, P (l|W, Θ), can be modeled by neural networks in an end-to-end manner where Θ is the model parameter. Various model structures can be used for neural lexical utterance classification. In this work, we use bidire"
C18-1304,C14-1088,1,0.801925,"strate the effectiveness of the proposed adversarial training using Japanese and English data sets with three different lexical utterance classification tasks. 1 Introduction Modern spoken dialogue systems use multiple lexical utterance classification modules that can detect dialogue act (Stolcke et al., 2000; Khanpour et al., 2016), intent (Tur et al., 2011), domain (Xu and Sarikaya, 2014), question type (Wu et al., 2005), etc. to properly understand natural languages. The modules are typically trained using the machine learning technologies developed for individual languagespecific systems (Higashinaka et al., 2014). A common issue is that the data resources for such individual training are often limited or unbalanced among different tasks and/or different languages. For modeling lexical utterance classification, various modeling methods have been examined. Recently, neural lexical utterance classification, which is a fully neural-network-based modeling method, has demonstrated substantial performance without the use of manual feature engineering. The networks include long short-term memory recurrent neural networks (LSTM-RNNs) (Ravuri and Stolcke, 2015b; Ravuri and Stolcke, 2015a; Ravuri and Stolcke, 20"
C18-1304,C16-1189,0,0.0674732,"ent languages. In addition, in order to effectively transfer knowledge between different task data sets and different language data sets, this paper proposes a partially-shared modeling method that possesses both shared components and components specific to individual data sets. We demonstrate the effectiveness of the proposed adversarial training using Japanese and English data sets with three different lexical utterance classification tasks. 1 Introduction Modern spoken dialogue systems use multiple lexical utterance classification modules that can detect dialogue act (Stolcke et al., 2000; Khanpour et al., 2016), intent (Tur et al., 2011), domain (Xu and Sarikaya, 2014), question type (Wu et al., 2005), etc. to properly understand natural languages. The modules are typically trained using the machine learning technologies developed for individual languagespecific systems (Higashinaka et al., 2014). A common issue is that the data resources for such individual training are often limited or unbalanced among different tasks and/or different languages. For modeling lexical utterance classification, various modeling methods have been examined. Recently, neural lexical utterance classification, which is a"
C18-1304,D14-1181,0,0.00261944,"resources for such individual training are often limited or unbalanced among different tasks and/or different languages. For modeling lexical utterance classification, various modeling methods have been examined. Recently, neural lexical utterance classification, which is a fully neural-network-based modeling method, has demonstrated substantial performance without the use of manual feature engineering. The networks include long short-term memory recurrent neural networks (LSTM-RNNs) (Ravuri and Stolcke, 2015b; Ravuri and Stolcke, 2015a; Ravuri and Stolcke, 2016), convolution neural networks (Kim, 2014), and more advanced networks (Zhou et al., 2016a; Yang et al., 2016; Sawada et al., 2017). In addition, neural networks are suitable for performing joint learning; the paucity of data is tackled by transferring knowledge between different tasks or different languages. Various joint learning methods have been examined for leveraging different tasks or different language data sets in the natural language processing field. Multi-task joint learning can transfer knowledge between tasks by sharing task-invariant layers (Collobert and Weston, 2008; Liu et al., 2015; Liu et al., 2016c; Zhang and Weng"
C18-1304,N15-1092,0,0.0350943,"cke, 2016), convolution neural networks (Kim, 2014), and more advanced networks (Zhou et al., 2016a; Yang et al., 2016; Sawada et al., 2017). In addition, neural networks are suitable for performing joint learning; the paucity of data is tackled by transferring knowledge between different tasks or different languages. Various joint learning methods have been examined for leveraging different tasks or different language data sets in the natural language processing field. Multi-task joint learning can transfer knowledge between tasks by sharing task-invariant layers (Collobert and Weston, 2008; Liu et al., 2015; Liu et al., 2016c; Zhang and Weng, 2016). In lexical utterance classification, multi-task joint learning has been shown to effectively improve individual tasks (Liu et al., 2016b; Liu et al., 2016a; Liu et al., 2017). In addition, multi-lingual joint learning can transfer knowledge between languages, mainly from the resource-rich language to the resource-poor language. The knowledge transfer is achieved by learning common semantic representations for different This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License de"
C18-1304,D16-1012,0,0.0690073,"ution neural networks (Kim, 2014), and more advanced networks (Zhou et al., 2016a; Yang et al., 2016; Sawada et al., 2017). In addition, neural networks are suitable for performing joint learning; the paucity of data is tackled by transferring knowledge between different tasks or different languages. Various joint learning methods have been examined for leveraging different tasks or different language data sets in the natural language processing field. Multi-task joint learning can transfer knowledge between tasks by sharing task-invariant layers (Collobert and Weston, 2008; Liu et al., 2015; Liu et al., 2016c; Zhang and Weng, 2016). In lexical utterance classification, multi-task joint learning has been shown to effectively improve individual tasks (Liu et al., 2016b; Liu et al., 2016a; Liu et al., 2017). In addition, multi-lingual joint learning can transfer knowledge between languages, mainly from the resource-rich language to the resource-poor language. The knowledge transfer is achieved by learning common semantic representations for different This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 358"
C18-1304,P17-1001,0,0.0271158,"e paucity of data is tackled by transferring knowledge between different tasks or different languages. Various joint learning methods have been examined for leveraging different tasks or different language data sets in the natural language processing field. Multi-task joint learning can transfer knowledge between tasks by sharing task-invariant layers (Collobert and Weston, 2008; Liu et al., 2015; Liu et al., 2016c; Zhang and Weng, 2016). In lexical utterance classification, multi-task joint learning has been shown to effectively improve individual tasks (Liu et al., 2016b; Liu et al., 2016a; Liu et al., 2017). In addition, multi-lingual joint learning can transfer knowledge between languages, mainly from the resource-rich language to the resource-poor language. The knowledge transfer is achieved by learning common semantic representations for different This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 3586 Proceedings of the 27th International Conference on Computational Linguistics, pages 3586–3596 Santa Fe, New Mexico, USA, August 20-26, 2018. languages. Usually, word-aligned or sentence-aligned par"
C18-1304,N16-1083,0,0.0180011,"mons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 3586 Proceedings of the 27th International Conference on Computational Linguistics, pages 3586–3596 Santa Fe, New Mexico, USA, August 20-26, 2018. languages. Usually, word-aligned or sentence-aligned parallel data sets are employed for joint learning (Guo et al., 2016; Duong et al., 2016) However, most existing joint learning approaches focus only on cross-tasks or cross-lingual knowledge transfer. In fact, task-aligned multi-lingual data sets have been rarely utilized for joint learning (Mogadala and Rettinger, 2016; Pappas and Popescu-Belis, 2017). We can expect to enhance lexical utterance classification performance by achieving effective knowledge transfer among both different tasks and different languages. In this paper, we propose multi-task and multi-lingual joint learning; it can enhance neural lexical utterance classification by flexibly transferring knowledge among both different tasks and different languages. The proposed method is closely related to multi-task sequence-to-sequence learning (Luong et al., 2016) including many-to-many neural machine translation (Firat et al., 2016; Firat et al.,"
C18-1304,I17-1102,0,0.0158217,"onal License. creativecommons.org/licenses/by/4.0/ License details: http:// 3586 Proceedings of the 27th International Conference on Computational Linguistics, pages 3586–3596 Santa Fe, New Mexico, USA, August 20-26, 2018. languages. Usually, word-aligned or sentence-aligned parallel data sets are employed for joint learning (Guo et al., 2016; Duong et al., 2016) However, most existing joint learning approaches focus only on cross-tasks or cross-lingual knowledge transfer. In fact, task-aligned multi-lingual data sets have been rarely utilized for joint learning (Mogadala and Rettinger, 2016; Pappas and Popescu-Belis, 2017). We can expect to enhance lexical utterance classification performance by achieving effective knowledge transfer among both different tasks and different languages. In this paper, we propose multi-task and multi-lingual joint learning; it can enhance neural lexical utterance classification by flexibly transferring knowledge among both different tasks and different languages. The proposed method is closely related to multi-task sequence-to-sequence learning (Luong et al., 2016) including many-to-many neural machine translation (Firat et al., 2016; Firat et al., 2017; Schwenk and Douze, 2017)."
C18-1304,W17-2619,0,0.0226099,"s and Popescu-Belis, 2017). We can expect to enhance lexical utterance classification performance by achieving effective knowledge transfer among both different tasks and different languages. In this paper, we propose multi-task and multi-lingual joint learning; it can enhance neural lexical utterance classification by flexibly transferring knowledge among both different tasks and different languages. The proposed method is closely related to multi-task sequence-to-sequence learning (Luong et al., 2016) including many-to-many neural machine translation (Firat et al., 2016; Firat et al., 2017; Schwenk and Douze, 2017). While input and output components are easily distinguished in sequenceto-sequence models, neural lexical utterance classification methods are not explicitly divided into input and output components. Our idea is to divide the neural lexical utterance classification into two components. The language-specific components converts words to hidden representations while task-specific components convert the hidden representations into prediction probabilities. The former can be shared between different tasks and the latter can be shared between different languages. In addition, in order to perform e"
C18-1304,sekine-nobata-2004-definition,0,0.0322396,"ulti-lingual joint learning. Partially-shared modeling can be utilized for various neural network based joint learning schemes. We demonstrate the superiority of partially-shared modeling over fully-shared modeling. In addition, we reveal the properties of partially-shared modeling. • This paper introduces a new corpus for evaluating multi-task and multi-lingual lexical utterance classification methods. The corpus includes Japanese and English data sets with three different lexical utterance classification tasks. The tasks are dialogue act classification, extended named entity classification (Sekine and Nobata, 2004; Higashinaka et al., 2012), and question type classification. 2 Neural Lexical Utterance Classification This section details neural lexical utterance classification. Lexical utterance classification is the problem of determining the correct label l ∈ {l1 , · · · , lK } of given utterance W = {w1 , · · · , wT }. In neural lexical utterance classification, conditional probabilities for each label given utterance, P (l|W, Θ), can be modeled by neural networks in an end-to-end manner where Θ is the model parameter. Various model structures can be used for neural lexical utterance classification."
C18-1304,J00-3003,0,0.820542,"Missing"
C18-1304,N16-1174,0,0.0703876,"unbalanced among different tasks and/or different languages. For modeling lexical utterance classification, various modeling methods have been examined. Recently, neural lexical utterance classification, which is a fully neural-network-based modeling method, has demonstrated substantial performance without the use of manual feature engineering. The networks include long short-term memory recurrent neural networks (LSTM-RNNs) (Ravuri and Stolcke, 2015b; Ravuri and Stolcke, 2015a; Ravuri and Stolcke, 2016), convolution neural networks (Kim, 2014), and more advanced networks (Zhou et al., 2016a; Yang et al., 2016; Sawada et al., 2017). In addition, neural networks are suitable for performing joint learning; the paucity of data is tackled by transferring knowledge between different tasks or different languages. Various joint learning methods have been examined for leveraging different tasks or different language data sets in the natural language processing field. Multi-task joint learning can transfer knowledge between tasks by sharing task-invariant layers (Collobert and Weston, 2008; Liu et al., 2015; Liu et al., 2016c; Zhang and Weng, 2016). In lexical utterance classification, multi-task joint lear"
C18-1304,C16-1329,0,0.0973793,"re often limited or unbalanced among different tasks and/or different languages. For modeling lexical utterance classification, various modeling methods have been examined. Recently, neural lexical utterance classification, which is a fully neural-network-based modeling method, has demonstrated substantial performance without the use of manual feature engineering. The networks include long short-term memory recurrent neural networks (LSTM-RNNs) (Ravuri and Stolcke, 2015b; Ravuri and Stolcke, 2015a; Ravuri and Stolcke, 2016), convolution neural networks (Kim, 2014), and more advanced networks (Zhou et al., 2016a; Yang et al., 2016; Sawada et al., 2017). In addition, neural networks are suitable for performing joint learning; the paucity of data is tackled by transferring knowledge between different tasks or different languages. Various joint learning methods have been examined for leveraging different tasks or different language data sets in the natural language processing field. Multi-task joint learning can transfer knowledge between tasks by sharing task-invariant layers (Collobert and Weston, 2008; Liu et al., 2015; Liu et al., 2016c; Zhang and Weng, 2016). In lexical utterance classification, m"
C18-1304,P16-2034,0,0.139765,"re often limited or unbalanced among different tasks and/or different languages. For modeling lexical utterance classification, various modeling methods have been examined. Recently, neural lexical utterance classification, which is a fully neural-network-based modeling method, has demonstrated substantial performance without the use of manual feature engineering. The networks include long short-term memory recurrent neural networks (LSTM-RNNs) (Ravuri and Stolcke, 2015b; Ravuri and Stolcke, 2015a; Ravuri and Stolcke, 2016), convolution neural networks (Kim, 2014), and more advanced networks (Zhou et al., 2016a; Yang et al., 2016; Sawada et al., 2017). In addition, neural networks are suitable for performing joint learning; the paucity of data is tackled by transferring knowledge between different tasks or different languages. Various joint learning methods have been examined for leveraging different tasks or different language data sets in the natural language processing field. Multi-task joint learning can transfer knowledge between tasks by sharing task-invariant layers (Collobert and Weston, 2008; Liu et al., 2015; Liu et al., 2016c; Zhang and Weng, 2016). In lexical utterance classification, m"
D15-1268,P12-3007,0,0.0527429,"ue breakdown labels, and collected comments describing the error that led to the breakdown. By mining the comments, we ﬁrst identiﬁed error types. Then, we calculated the correlation between an error type and the degree of dialogue breakdown it incurred, quantifying its impact on dialogue breakdown. This is the ﬁrst study to quantitatively analyze error types and their effect in chat-oriented dialogue systems. 2 Data Collection 1 Introduction Chat-oriented or open-domain dialogue systems have recently been attracting attention from social and entertainment aspects (Bickmore and Cassell, 2001; Banchs and Li, 2012; Wilcock and Jokinen, 2013). However, since they need to deal with open-domain utterances, which current natural language processing techniques are not mature enough to handle appropriately, the system inevitably makes errors. This discourages users from talking to the system, leading to dialogue breakdowns in conversation (Martinovsky and Traum, 2003). Here, dialogue breakdowns denote points in dialogue where users are unable to continue the conversation. This paper aims to ﬁnd errors that lead to dialogue breakdowns in chat-oriented dialogue systems. Our approach is two-fold: (1) identify e"
D15-1268,C14-1088,1,0.821874,"s in chat-oriented dialogue systems, and (2) calculate their impact on dialogue breakdown. For (1), we ﬁrst collect chat dialogues between an automated system and users, annotate the dialogues with dialogue breakdown labels, and collect comments that describe the error that led to the breakFor data collection, we asked dialogue researchers and their collaborators in Japan to use our chatoriented dialogue system. The system is textbased and enables chatting in Japanese. It was built by wrapping a chat API provided by NTT DOCOMO (Onishi and Yoshimura, 2014). Since the API works on the basis of (Higashinaka et al., 2014), which uses a number of natural language processing techniques to understand and generate utterances, we expected to obtain a wide variety of dialogues, and hence, a variety of errors. A total of 116 users chatted with the system, resulting in 1,146 dialogues. Here, each dialogue was controlled to be 21 utterances long: one system prompt with ten utterances each from the user and system. Then, we randomly sampled 100 dialogues (called the init100 data set) for dialogue breakdown annotation. Twenty-four annotators subjectively labeled each system utterance in init100 with one of the following"
D15-1268,W02-0219,0,0.0621882,"Missing"
D18-1064,N18-1111,0,0.0285433,"017b). In addition, a few work have examined multi-task and multilingual joint modeling (Masumura et al., 2018; Lin et al., 2018). Different from the previous work, our novelty is to introduce adversarial training for multi-task and multi-lingual joint modeling. Adversarial Training: The concept of adversarial training was first proposed by Goodfellow et al. (2014), and many studies in the machine learning field have focused on adversarial training. Adversarial training has been well utilized in text classification (Ganin et al., 2016; Chen et al., 2016; Liu et al., 2017; Miyato et al., 2017; Chen and Cardie, 2018). Most natural language processing papers adopted either the language invariant approach (Chen et al., 2016; Zhang et al., 2017a) or the task invariant approach (Ganin et al., 2016; Liu et al., 2017; Chen and Cardie, 2018). This paper aims to fully utilize both task adversarial training and language adversarial training. To this end, we simultaneously introduce language-specific task adversarial networks and task-specific language adversarial networks. 3 3.1 Main Joint Network The proposed method is founded on a fully neural network that employs I language-specific networks, J task-specific ne"
D18-1064,D16-1136,0,0.0306641,"ent utterance intent classification tasks are examined: dialogue act, topic type, and question type classification. 2 Related Work Joint Modeling: In natural language processing research, joint modeling is usually split into multitask joint modeling and multi-lingual joint modeling. Multi-task joint modeling has been shown to effectively improve individual tasks (Collobert and Weston, 2008; Liu et al., 2016a,b; Zhang and Weng, 2016; Liu et al., 2016c). In addition, multi-lingual joint modeling is achieved by learning common semantic representations among different languages (Guo et al., 2016; Duong et al., 2016; Zhang et al., 2016, 2017b). In addition, a few work have examined multi-task and multilingual joint modeling (Masumura et al., 2018; Lin et al., 2018). Different from the previous work, our novelty is to introduce adversarial training for multi-task and multi-lingual joint modeling. Adversarial Training: The concept of adversarial training was first proposed by Goodfellow et al. (2014), and many studies in the machine learning field have focused on adversarial training. Adversarial training has been well utilized in text classification (Ganin et al., 2016; Chen et al., 2016; Liu et al., 2017"
D18-1064,C16-1189,0,0.0159194,"lassification tasks. 1 Introduction In natural language processing fields, full neural network based methods are suitable for joint modeling as they can simultaneously utilize multiple task data sets or multiple language data sets to improve the performance achieved for individual tasks or languages (Collobert and Weston, 2008). It is known that joint modeling can address the data scarcity problem. Key natural language processing technologies for spoken dialogue systems include utterance intent classification, which is needed to detect intent labels such as dialogue act (Stolcke et al., 2000; Khanpour et al., 2016), domain (Xu and Sarikaya, 2014), and question type (Wu et al., 2005) from input utterances (Ravuri and Stolcke, 633 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 633–639 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics ance intent classification is followed by estimation of the probabilities of each intent label given input utterance, P (l(j) |W (i) , Θ(i,j) ) where Θ(i,j) is the trainable model parameter for the combination of the i-th language and the j-th task. In multi-task and multi-lingual"
D18-1064,P18-1074,0,0.108433,"-shi, Kanagawa, 239-0847, Japan ryou.masumura.ba@hco.ntt.co.jp Abstract 2015a,b, 2016). One problem is that the training data are often limited or unbalanced among different tasks or different languages. Therefore, our motivation is to leverage both multi-task joint modeling and multi-lingual joint modeling to enhance utterance intent classification. The multi-task and multi-lingual joint modeling can be composed by introducing both task-specific networks, which are shared among different languages, and language-specific networks, which are shared among different tasks (Masumura et al., 2018; Lin et al., 2018). Although joint modeling is mainly intended to improve classification performance in resource-poor tasks or languages, its classification performance is degraded in some minor data sets. This is because the languagespecific networks often depend on majority tasks, while the task-specific networks often depend on majority languages. What are needed are taskspecific networks that are invariant to languages, and language-specific networks that are invariant to tasks. In order to explicitly improve the invariance of language and task-specific networks, this paper introduces adversarial training ("
D18-1064,J00-3003,0,0.083527,"Missing"
D18-1064,D16-1012,0,0.0310289,"ained from I language and J task data sets. Experiments on Japanese and English data sets demonstrate the effectiveness of the adversarial training proposal. To support spoken dialogue systems, three different utterance intent classification tasks are examined: dialogue act, topic type, and question type classification. 2 Related Work Joint Modeling: In natural language processing research, joint modeling is usually split into multitask joint modeling and multi-lingual joint modeling. Multi-task joint modeling has been shown to effectively improve individual tasks (Collobert and Weston, 2008; Liu et al., 2016a,b; Zhang and Weng, 2016; Liu et al., 2016c). In addition, multi-lingual joint modeling is achieved by learning common semantic representations among different languages (Guo et al., 2016; Duong et al., 2016; Zhang et al., 2016, 2017b). In addition, a few work have examined multi-task and multilingual joint modeling (Masumura et al., 2018; Lin et al., 2018). Different from the previous work, our novelty is to introduce adversarial training for multi-task and multi-lingual joint modeling. Adversarial Training: The concept of adversarial training was first proposed by Goodfellow et al. (2014),"
D18-1064,P17-1001,0,0.0255239,"ong et al., 2016; Zhang et al., 2016, 2017b). In addition, a few work have examined multi-task and multilingual joint modeling (Masumura et al., 2018; Lin et al., 2018). Different from the previous work, our novelty is to introduce adversarial training for multi-task and multi-lingual joint modeling. Adversarial Training: The concept of adversarial training was first proposed by Goodfellow et al. (2014), and many studies in the machine learning field have focused on adversarial training. Adversarial training has been well utilized in text classification (Ganin et al., 2016; Chen et al., 2016; Liu et al., 2017; Miyato et al., 2017; Chen and Cardie, 2018). Most natural language processing papers adopted either the language invariant approach (Chen et al., 2016; Zhang et al., 2017a) or the task invariant approach (Ganin et al., 2016; Liu et al., 2017; Chen and Cardie, 2018). This paper aims to fully utilize both task adversarial training and language adversarial training. To this end, we simultaneously introduce language-specific task adversarial networks and task-specific language adversarial networks. 3 3.1 Main Joint Network The proposed method is founded on a fully neural network that employs I l"
D18-1064,N16-1174,0,0.169645,"tworks, this paper introduces adversarial training (Goodfellow et al., 2014). Our idea is to train language-specific networks so as to be insensitive to the target task, while training task-specific networks to be insensitive to language. To this end, we introduce multiple domain adversarial networks (Ganin et al., 2016), language-specific task adversarial networks, and task-specific language adversarial networks, into a state-of-the-art fully neural network based joint modeling; we adopt the bidirectional long short-term memory recurrent neural networks (BLSTM-RNNs) with attention mechanism (Yang et al., 2016; Zhou et al., 2016). To the best of our knowledge, this paper is the first study to employ adversarial training for multi-input and multi-output joint modeling. This paper proposes an adversarial training method for the multi-task and multi-lingual joint modeling needed for utterance intent classification. In joint modeling, common knowledge can be efficiently utilized among multiple tasks or multiple languages. This is achieved by introducing both languagespecific networks shared among different tasks and task-specific networks shared among different languages. However, the shared networks a"
D18-1064,C18-1304,1,0.926764,", Hikarinooka, Yokosuka-shi, Kanagawa, 239-0847, Japan ryou.masumura.ba@hco.ntt.co.jp Abstract 2015a,b, 2016). One problem is that the training data are often limited or unbalanced among different tasks or different languages. Therefore, our motivation is to leverage both multi-task joint modeling and multi-lingual joint modeling to enhance utterance intent classification. The multi-task and multi-lingual joint modeling can be composed by introducing both task-specific networks, which are shared among different languages, and language-specific networks, which are shared among different tasks (Masumura et al., 2018; Lin et al., 2018). Although joint modeling is mainly intended to improve classification performance in resource-poor tasks or languages, its classification performance is degraded in some minor data sets. This is because the languagespecific networks often depend on majority tasks, while the task-specific networks often depend on majority languages. What are needed are taskspecific networks that are invariant to languages, and language-specific networks that are invariant to tasks. In order to explicitly improve the invariance of language and task-specific networks, this paper introduces adv"
D18-1064,C16-1300,0,0.0224554,"classification tasks are examined: dialogue act, topic type, and question type classification. 2 Related Work Joint Modeling: In natural language processing research, joint modeling is usually split into multitask joint modeling and multi-lingual joint modeling. Multi-task joint modeling has been shown to effectively improve individual tasks (Collobert and Weston, 2008; Liu et al., 2016a,b; Zhang and Weng, 2016; Liu et al., 2016c). In addition, multi-lingual joint modeling is achieved by learning common semantic representations among different languages (Guo et al., 2016; Duong et al., 2016; Zhang et al., 2016, 2017b). In addition, a few work have examined multi-task and multilingual joint modeling (Masumura et al., 2018; Lin et al., 2018). Different from the previous work, our novelty is to introduce adversarial training for multi-task and multi-lingual joint modeling. Adversarial Training: The concept of adversarial training was first proposed by Goodfellow et al. (2014), and many studies in the machine learning field have focused on adversarial training. Adversarial training has been well utilized in text classification (Ganin et al., 2016; Chen et al., 2016; Liu et al., 2017; Miyato et al., 201"
D18-1064,P17-1179,0,0.0221043,"Different from the previous work, our novelty is to introduce adversarial training for multi-task and multi-lingual joint modeling. Adversarial Training: The concept of adversarial training was first proposed by Goodfellow et al. (2014), and many studies in the machine learning field have focused on adversarial training. Adversarial training has been well utilized in text classification (Ganin et al., 2016; Chen et al., 2016; Liu et al., 2017; Miyato et al., 2017; Chen and Cardie, 2018). Most natural language processing papers adopted either the language invariant approach (Chen et al., 2016; Zhang et al., 2017a) or the task invariant approach (Ganin et al., 2016; Liu et al., 2017; Chen and Cardie, 2018). This paper aims to fully utilize both task adversarial training and language adversarial training. To this end, we simultaneously introduce language-specific task adversarial networks and task-specific language adversarial networks. 3 3.1 Main Joint Network The proposed method is founded on a fully neural network that employs I language-specific networks, J task-specific networks, and J classification networks as well as Masumura et al. (2018). The language-specific network can be shared between mu"
D18-1064,P16-2034,0,0.226788,"introduces adversarial training (Goodfellow et al., 2014). Our idea is to train language-specific networks so as to be insensitive to the target task, while training task-specific networks to be insensitive to language. To this end, we introduce multiple domain adversarial networks (Ganin et al., 2016), language-specific task adversarial networks, and task-specific language adversarial networks, into a state-of-the-art fully neural network based joint modeling; we adopt the bidirectional long short-term memory recurrent neural networks (BLSTM-RNNs) with attention mechanism (Yang et al., 2016; Zhou et al., 2016). To the best of our knowledge, this paper is the first study to employ adversarial training for multi-input and multi-output joint modeling. This paper proposes an adversarial training method for the multi-task and multi-lingual joint modeling needed for utterance intent classification. In joint modeling, common knowledge can be efficiently utilized among multiple tasks or multiple languages. This is achieved by introducing both languagespecific networks shared among different tasks and task-specific networks shared among different languages. However, the shared networks are often specialized"
I08-1055,P98-1013,0,0.00595273,"urn to the existing similarity metrics and dictionaries to derive features that would be useful for why-QA. To train a ranker, we create a corpus of why-questions and answers and adopt one of the machine learning algorithms for ranking. The following sections describe the three types of features, the corpus creation, and the ranker training. The actual instances of the features, the corpus, and the ranker will be presented in Section 4. 3.1 Causal Expression Features With the increasing attention paid to SRL, we currently have a number of corpora, such as PropBank (Palmer, 2005) and FrameNet (Baker et al., 1998), that are tagged with semantic relations including a causal relation. Since text spans for such relations are annotated in the corpora, we can simply collect the spans marked by a causal relation as causal expressions. Since an answer candidate that has a matching expression for one of the collected causal expressions is likely to be expressing a cause as well, we can make the existence of each expression a feature. Although the collected causal expressions without any modification might be used to create features, for generality, it would be better to abstract them into syntactic patterns. F"
I08-1055,W03-1210,0,0.668216,"sality. The ranker is trained to maximize the QA performance with regards to a corpus of why-questions and answers, automatically tuning the weights of the features. This paper is organized as follows: Section 2 describes previous work on why-QA, and Section 3 describes our approach. Section 4 describes the implementation of our approach, and Section 5 presents the evaluation results. Section 6 summarizes and mentions future work. 2 Previous Work Although systems that can answer why-questions are emerging, they tend to have limitations in that they can answer questions only with causal verbs (Girju, 2003), in specific domains (Khoo et al., 418 2000), or questions covered by a specific knowledge base (Curtis et al., 2005). Recently, Verberne (2006; 2007a) has been intensively working on whyQA based on the Rhetorical Structure Theory (RST) (Mann and Thompson, 1988). However, her approach requires manually annotated corpora with RST relations. When we look for fully implemented systems for generic “why X?” questions, we only find a small number of such systems. Since why-QA would be a challenging task when tackled straightforwardly, requiring common-sense knowledge and semantic interpretation of"
I08-1055,C02-1053,1,0.806673,"data. However, the performance gains begin to decrease relatively early, possibly indicating the limitation of our approach. Since our approach heavily relies on surface patterns, the use of syntactic and semantic features may be necessary. 200 300 400 500 600 700 800 900 Number of training samples Figure 3: Learning curve: Performance changes when answering Q1–Q100 with different sizes of training samples. Paragraphs are used as answer candidates. their quality itself may be to blame. Furthermore, analyzing the trained ranking models allows us to calculate the weights given to the features (Hirao et al., 2002). Table 3 shows the weights of the top-10 features. We also include in the table the weights of the Synonym Pair, MANCausal Expression and Cause Effect Pair features so that the role of all three types of features in our approach can be shown. The analyzed model was the one trained with all 1,000 questions in the WHYQA collection with paragraphs as answers. Just as suggested by Table 2, the Question-Candidate Cosine Similarity feature plays a key role, followed by automatically collected causal expression features. Figure 2 shows the distribution of the ranks of the first correct answers for a"
I08-1055,W05-0306,0,0.123026,"to et al., 2007). However, their performance is much lower (Mori et al., 2007) than that of factoid QA systems (Fukumoto et al., 2004; Voorhees and Dang, 2005). We consider that this low performance is due to the great amount of hand-crafting involved in the 1 http://research.nii.ac.jp/ntcir/ntcir-ws6/ws-en.html systems. Currently, most of the systems rely on hand-crafted patterns to extract and evaluate answer candidates (Fukumoto et al., 2007). Such patterns include typical cue phrases and POS-tag sequences related to causality, such as “because of” and “by reason of.” However, as noted in (Inui and Okumura, 2005), causes are expressed in various forms, and it is difficult to cover all such expressions by hand. Hand-crafting is also very costly. Some patterns may be more indicative of causes than others. Therefore, it may be useful to assign different weights to the patterns for better answer candidate extraction, but currently this must be done by hand (Mori et al., 2007). It is not clear whether the weights determined by hand are suitable. In this paper, we propose a corpus-based approach for why-QA in order to reduce this hand-crafting effort. We automatically collect causal expressions from corpora"
I08-1055,P00-1043,0,0.194898,"Missing"
I08-1055,W05-0628,0,0.0154759,"Missing"
I08-1055,J05-1004,0,0.0246144,"two types of features, we turn to the existing similarity metrics and dictionaries to derive features that would be useful for why-QA. To train a ranker, we create a corpus of why-questions and answers and adopt one of the machine learning algorithms for ranking. The following sections describe the three types of features, the corpus creation, and the ranker training. The actual instances of the features, the corpus, and the ranker will be presented in Section 4. 3.1 Causal Expression Features With the increasing attention paid to SRL, we currently have a number of corpora, such as PropBank (Palmer, 2005) and FrameNet (Baker et al., 1998), that are tagged with semantic relations including a causal relation. Since text spans for such relations are annotated in the corpora, we can simply collect the spans marked by a causal relation as causal expressions. Since an answer candidate that has a matching expression for one of the collected causal expressions is likely to be expressing a cause as well, we can make the existence of each expression a feature. Although the collected causal expressions without any modification might be used to create features, for generality, it would be better to abstra"
I08-1055,N04-3012,0,0.0410046,"that encode their relevance to the question, we can use this score or simply the rank of the retrieved document as a feature. A question and an answer candidate may be semantically expressing the same content with different expressions. The simplest case is when synonyms are used to describe the same content; e.g., when “arrest” is used instead of “apprehend.” For such cases, we can exploit existing thesauri. We can create a feature encoding whether synonyms of words in the question are found in the answer candidate. We could also use the value of semantic similarity and relatedness measures (Pedersen et al., 2004) or the existence of hypernym or hyponym relations as features. 3.3 Causal Relation Features There are semantic lexicons where a semantic relation between concepts is indicated. For example, the EDR dictionary3 shows whether a causal relation holds between two concepts; e.g., between “murder” and “arrest.” Using such dictionaries, we can create pairs of expressions, one indicating a cause and the other its effect. If we find an expression for a cause in the answer candidate and that for an effect in the question, it is likely that they hold a causal relation. Therefore, we can create a feature"
I08-1055,E06-3005,0,0.1239,"ora tagged with semantic relations. From the collected expressions, features are created to train an answer candidate ranker that maximizes the QA performance with regards to the corpus of why-questions and answers. NAZEQA, a Japanese why-QA system based on our approach, clearly outperforms a baseline that uses hand-crafted patterns with a Mean Reciprocal Rank (top-5) of 0.305, making it presumably the best-performing fully implemented why-QA system. 1 Introduction Following the trend of non-factoid QA, we are seeing the emergence of work on why-QA; e.g., answering generic “why X?” questions (Verberne, 2006). However, since why-QA is an inherently difficult problem, there have only been a small number of fully implemented systems dedicated to solving it. Recent systems at NTCIR-61 Question Answering Challenge (QAC-4) can handle why-questions (Fukumoto et al., 2007). However, their performance is much lower (Mori et al., 2007) than that of factoid QA systems (Fukumoto et al., 2004; Voorhees and Dang, 2005). We consider that this low performance is due to the great amount of hand-crafting involved in the 1 http://research.nii.ac.jp/ntcir/ntcir-ws6/ws-en.html systems. Currently, most of the systems"
I08-1055,C98-1013,0,\N,Missing
I17-2036,P16-2087,0,0.0296057,"cosine distance between two document vectors that are composed by averaging word embeddings (Vulic and Moens, 2015; Brokos et al., 2016) or document embeddings called paragraph vectors (PVs) (Le and Mikolov, 2014). Another highly efficient distance metric is word mover’s distance (WMD), which leverages word embeddings (Kusner et al., 2015)．In this work, we also examined these distance metrics in a document retrieval evaluation. Generative models of word embeddings have recently been proposed in topic modeling in order to capture the semantic structure of words and documents (Das et al., 2015; Batmanghelich et al., 2016). To the best of our knowledge, this paper is the first work on language modeling that handles word embeddings as random variables. 3 c(w, D) , |D| (5) where sim(w, v) is the word similarity between w and v. In order to calculate P (w|v), cosine distances between pre-trained word embeddings were recently utilized (Zuccon et al., 2015; Ganguly et al., 2015). Thus, the word similarity is calculated as sim(w, v) = w⊤ v, (6) where w is the word embedding normalized to a unit length for w. (1) t=1 211 4 IR based on Hyperspherical QLMs The ML estimation is based on the expectation maximization algor"
I17-2036,D14-1162,0,0.10074,"fferty, 1999), and latent variable models (Wei and Croft, 2006) have been proposed in order to take semantic relationships between document and target query into account. Recently, word embeddings, which are continuous vector representations embedding word semantic information, have been utilized for enhancing the previous expansion techniques (Zhang et al., 2016; Mitra and Craswell, 2017). The word embeddings can be easily acquired in an unsupervised manner from large scale text sets based on embedding modeling, i.e., skip-gram, continuous bag-of-words (CBOW) (Mikolov et al., 2013) or GloVe (Pennington et al., 2014). Zuccon et al. (2015); Ganguly et al. (2015); Zamani and Croft (2016a) used the word embeddings in order to assist translation QLMs. Zamani and Croft (2016b); Kuzi et al. (2016) used the word embeddings in order to perform query expansion. However, previous word embedding-based probabilistic IR methods have no theoretical validity since the word embeddings were heuristically introduced. In order to perform more natural word embedding based probabilistic IR, our key idea is to directly leverage word embeddings rather than words as random variables for language models. In fact, the word embeddi"
I17-2036,W16-2915,0,0.0690891,"smoothing. IR based on QLMs P (D|Q) ∝ (2) where c(w, D) is the word count of w in D, and |D |is the number of all words in D. In a maximum a posteriori (MAP) estimation, a document collection C in which all of the retrieved documents are included is used for a prior (Zhai and Lafferty, 2001). MAP estimated generative probability of a word w is defined as This paper is closely related to document distance metrics using word or document embeddings. One major distance metric is the cosine distance between two document vectors that are composed by averaging word embeddings (Vulic and Moens, 2015; Brokos et al., 2016) or document embeddings called paragraph vectors (PVs) (Le and Mikolov, 2014). Another highly efficient distance metric is word mover’s distance (WMD), which leverages word embeddings (Kusner et al., 2015)．In this work, we also examined these distance metrics in a document retrieval evaluation. Generative models of word embeddings have recently been proposed in topic modeling in order to capture the semantic structure of words and documents (Das et al., 2015; Batmanghelich et al., 2016). To the best of our knowledge, this paper is the first work on language modeling that handles word embedding"
I17-2036,P15-1077,0,0.0327924,"nce metric is the cosine distance between two document vectors that are composed by averaging word embeddings (Vulic and Moens, 2015; Brokos et al., 2016) or document embeddings called paragraph vectors (PVs) (Le and Mikolov, 2014). Another highly efficient distance metric is word mover’s distance (WMD), which leverages word embeddings (Kusner et al., 2015)．In this work, we also examined these distance metrics in a document retrieval evaluation. Generative models of word embeddings have recently been proposed in topic modeling in order to capture the semantic structure of words and documents (Das et al., 2015; Batmanghelich et al., 2016). To the best of our knowledge, this paper is the first work on language modeling that handles word embeddings as random variables. 3 c(w, D) , |D| (5) where sim(w, v) is the word similarity between w and v. In order to calculate P (w|v), cosine distances between pre-trained word embeddings were recently utilized (Zuccon et al., 2015; Ganguly et al., 2015). Thus, the word similarity is calculated as sim(w, v) = w⊤ v, (6) where w is the word embedding normalized to a unit length for w. (1) t=1 211 4 IR based on Hyperspherical QLMs The ML estimation is based on the e"
I17-2066,C14-1088,1,0.872981,"s For our investigation, we need to prepare understanding results categorized by their types. For this purpose, we use a corpus of PerceivedInfo collected in our previous work (Mitsuda et al., 2017). In this corpus, user utterances in chat-oriented dialogue are associated with the information that can be perceived/inferred by humans from these utterances. Such information is called PerceivedInfo (perceived information). Figure 1 shows an example of a chat-oriented dialogue and their PerceivedInfo in the corpus. As stimuli for collecting PerceivedInfo, a Japanese chat-oriented dialogue corpus (Higashinaka et al., 2014) was used. PerceivedInfo was written by multiple annotators using natural sentences with 389 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 389–394, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP Chat-oriented dialogue Perceived information for : utterance : Hello, nice to meet you! : Nice to meet you too. : I feel autumn coming, how about you? : I think so too. : The cicadas have gotten quiet recently. ... : Do you go anywhere interesting in autumn? : I&apos;ll visit Mt. Fuji if I feel up to it. ... : Let&apos;s talk about this next time. :"
I17-2066,N04-3002,0,0.0287438,"periment using human subjects. As a result, we found that only certain types of understanding results, such as those related to a user’s permanent state, are effective to improve user satisfaction. This paper clarifies the types of understanding results that can be safely uttered by a system. 1 Introduction Current dialogue systems often convey the understanding results of user utterances for confirmation and for showing understanding. Taskoriented dialogue systems repeat information provided by users by using understanding results of user utterances to confirm the content of user utterances (Litman and Silliman, 2004; Raux et al., 2005). Chat-oriented dialogue systems also need to confirm the content of user utterances and to show understanding so that the systems can be more affective. However, some of the understanding results should not be conveyed to users. For instance, some utterances (e.g. “You are stubborn.”) may be offensive and some (e.g. “It is summer.”) may be too commonsensical. To create a dialogue system which conveys one’s understanding results, 2 Data of Understanding Results For our investigation, we need to prepare understanding results categorized by their types. For this purpose, we u"
I17-2066,W12-1603,0,0.0597398,"Missing"
I17-2066,P13-1025,0,\N,Missing
L16-1502,P92-1008,0,0.215954,"s of our “Dialogue breakdown detection challenge”, which is an evaluation workshop dedicated to dialogue breakdown detection. The aim of the challenge was to bring together various methods for dialogue breakdown detection and discuss possible evaluation metrics. Although there have been other dialoguerelated evaluation campaigns, including a spoken dialogue challenge (Black and Eskenazi, 2009) and a dialogue state tracking challenge (Williams et al., 2013), our challenge is unique in that we deal with chat dialogues. We focus on chat because, compared to the errors in task-oriented dialogues (Bear et al., 1992; Carpenter et al., 2001; Bulyko et al., 2005), we can observe more diverse dialogue breakdown phenomena (Higashinaka et al., 2015a), which makes it possible to develop general and robust techniques for breakdown detection. Our challenge is also unique in that we deal with a highly subjective phenomenon, posing difficulties in creating appropriate evaluation metrics. In this paper, we present how we designed the task, prepared the datasets, and conceived the evaluation metrics for the challenge. The event was held in October 2015 and there were six participants. We also report the detection me"
L16-1502,W09-3950,0,0.0278232,"l because such technology will enable systems to avoid the creation of inappropriate utterances and also to identify dialogue breakdowns when they occur and perform the necessary recovery procedures. This paper reports the results of our “Dialogue breakdown detection challenge”, which is an evaluation workshop dedicated to dialogue breakdown detection. The aim of the challenge was to bring together various methods for dialogue breakdown detection and discuss possible evaluation metrics. Although there have been other dialoguerelated evaluation campaigns, including a spoken dialogue challenge (Black and Eskenazi, 2009) and a dialogue state tracking challenge (Williams et al., 2013), our challenge is unique in that we deal with chat dialogues. We focus on chat because, compared to the errors in task-oriented dialogues (Bear et al., 1992; Carpenter et al., 2001; Bulyko et al., 2005), we can observe more diverse dialogue breakdown phenomena (Higashinaka et al., 2015a), which makes it possible to develop general and robust techniques for breakdown detection. Our challenge is also unique in that we deal with a highly subjective phenomenon, posing difficulties in creating appropriate evaluation metrics. In this p"
L16-1502,W15-4611,1,0.56929,"n. The aim of the challenge was to bring together various methods for dialogue breakdown detection and discuss possible evaluation metrics. Although there have been other dialoguerelated evaluation campaigns, including a spoken dialogue challenge (Black and Eskenazi, 2009) and a dialogue state tracking challenge (Williams et al., 2013), our challenge is unique in that we deal with chat dialogues. We focus on chat because, compared to the errors in task-oriented dialogues (Bear et al., 1992; Carpenter et al., 2001; Bulyko et al., 2005), we can observe more diverse dialogue breakdown phenomena (Higashinaka et al., 2015a), which makes it possible to develop general and robust techniques for breakdown detection. Our challenge is also unique in that we deal with a highly subjective phenomenon, posing difficulties in creating appropriate evaluation metrics. In this paper, we present how we designed the task, prepared the datasets, and conceived the evaluation metrics for the challenge. The event was held in October 2015 and there were six participants. We also report the detection methods submitted by the participants and provide their results. We finish with a discussion of the validity of the evaluation metri"
L16-1502,D15-1268,1,0.847752,"n. The aim of the challenge was to bring together various methods for dialogue breakdown detection and discuss possible evaluation metrics. Although there have been other dialoguerelated evaluation campaigns, including a spoken dialogue challenge (Black and Eskenazi, 2009) and a dialogue state tracking challenge (Williams et al., 2013), our challenge is unique in that we deal with chat dialogues. We focus on chat because, compared to the errors in task-oriented dialogues (Bear et al., 1992; Carpenter et al., 2001; Bulyko et al., 2005), we can observe more diverse dialogue breakdown phenomena (Higashinaka et al., 2015a), which makes it possible to develop general and robust techniques for breakdown detection. Our challenge is also unique in that we deal with a highly subjective phenomenon, posing difficulties in creating appropriate evaluation metrics. In this paper, we present how we designed the task, prepared the datasets, and conceived the evaluation metrics for the challenge. The event was held in October 2015 and there were six participants. We also report the detection methods submitted by the participants and provide their results. We finish with a discussion of the validity of the evaluation metri"
L16-1502,W13-4065,0,0.00567481,"of inappropriate utterances and also to identify dialogue breakdowns when they occur and perform the necessary recovery procedures. This paper reports the results of our “Dialogue breakdown detection challenge”, which is an evaluation workshop dedicated to dialogue breakdown detection. The aim of the challenge was to bring together various methods for dialogue breakdown detection and discuss possible evaluation metrics. Although there have been other dialoguerelated evaluation campaigns, including a spoken dialogue challenge (Black and Eskenazi, 2009) and a dialogue state tracking challenge (Williams et al., 2013), our challenge is unique in that we deal with chat dialogues. We focus on chat because, compared to the errors in task-oriented dialogues (Bear et al., 1992; Carpenter et al., 2001; Bulyko et al., 2005), we can observe more diverse dialogue breakdown phenomena (Higashinaka et al., 2015a), which makes it possible to develop general and robust techniques for breakdown detection. Our challenge is also unique in that we deal with a highly subjective phenomenon, posing difficulties in creating appropriate evaluation metrics. In this paper, we present how we designed the task, prepared the datasets"
L18-1465,P98-1068,0,0.288573,"662 0.666 Recall 0.500 0.640 0.646 0.670 0.651 0.672 0.670 0.672 F-score 0.500 0.500 0.521 0.666 0.513 0.669 0.666 0.669 Rank 1 2 3 4 5 6 7 8 9 10 11 12 Table 2: Partial excerpt of attribute weight rank in 19 features in All model. back (e.g., “’en’, “ee”, “’aa”, “hi”, etc.) and particles related to questioning and turn-keeping (e.g., “de”, “kara”, “kedo”, “’kana”, “janai”, etc.) cooccurring with the nod (as indicated in the previous studies (Ishi et al., 2006; Ishi et al., 2007) are included in the last morpheme of the IPU. Morphological analysis was conducted for this purpose. We used Jtag (Fuchi and Takagi, 1998), a general morphological analysis tool for Japanese. Based on the output result, it was judged whether the last morpheme is related to nods. The feature value is a one-dimensional vector. • Dialogue act (DA): The dialogue act was extracted using an estimation technique for Japanese (Meguro et al., 2010; Higashinaka et al., 2014). The technique can estimate the dialogue act using the word N-grams, semantic categories (obtained from a Japanese thesaurus Goi-Taikei), and character Ngrams. The dialog acts and number of IPUs are listed in Table 1. There was very little data for DA4, 10, 12, 13, 14"
L18-1465,C14-1088,1,0.851145,"(e.g., “de”, “kara”, “kedo”, “’kana”, “janai”, etc.) cooccurring with the nod (as indicated in the previous studies (Ishi et al., 2006; Ishi et al., 2007) are included in the last morpheme of the IPU. Morphological analysis was conducted for this purpose. We used Jtag (Fuchi and Takagi, 1998), a general morphological analysis tool for Japanese. Based on the output result, it was judged whether the last morpheme is related to nods. The feature value is a one-dimensional vector. • Dialogue act (DA): The dialogue act was extracted using an estimation technique for Japanese (Meguro et al., 2010; Higashinaka et al., 2014). The technique can estimate the dialogue act using the word N-grams, semantic categories (obtained from a Japanese thesaurus Goi-Taikei), and character Ngrams. The dialog acts and number of IPUs are listed in Table 1. There was very little data for DA4, 10, 12, 13, 14-19, 22, 24, and 26-29, so they were excluded from feature values. The label of each dialogue act was expressed as a binary value as to whether the dialogue act appeared. Therefore, the feature value was a 17-dimensional vector. We constructed the estimation models by using SMOreg (Keerthi et al., 2001), which implements a suppor"
L18-1465,C10-1086,1,0.658603,"ning and turn-keeping (e.g., “de”, “kara”, “kedo”, “’kana”, “janai”, etc.) cooccurring with the nod (as indicated in the previous studies (Ishi et al., 2006; Ishi et al., 2007) are included in the last morpheme of the IPU. Morphological analysis was conducted for this purpose. We used Jtag (Fuchi and Takagi, 1998), a general morphological analysis tool for Japanese. Based on the output result, it was judged whether the last morpheme is related to nods. The feature value is a one-dimensional vector. • Dialogue act (DA): The dialogue act was extracted using an estimation technique for Japanese (Meguro et al., 2010; Higashinaka et al., 2014). The technique can estimate the dialogue act using the word N-grams, semantic categories (obtained from a Japanese thesaurus Goi-Taikei), and character Ngrams. The dialog acts and number of IPUs are listed in Table 1. There was very little data for DA4, 10, 12, 13, 14-19, 22, 24, and 26-29, so they were excluded from feature values. The label of each dialogue act was expressed as a binary value as to whether the dialogue act appeared. Therefore, the feature value was a 17-dimensional vector. We constructed the estimation models by using SMOreg (Keerthi et al., 2001)"
L18-1465,wittenburg-etal-2006-elan,0,0.0604053,"16 Question (desire) DA17 Question (plan) DA18 Question (evaluation) DA19 Question (other) DA20 Question (Yourself) DA21 Sympathy DA22 Nonsympathy DA23 Confirmation DA24 Proposal DA25 Repeat DA26 Paraphrase DA27 Approval DA28 Thanks DA29 Apology DA30 Filler DA31 Admiration DA32 Other Table 1: Dialogue act labels • Hand gesture and body posture: The participants’ body movements, such as hand gestures, upper body, and leg movements, were measured with a motion capture device (Xsens MVN) at 240 Hz. All verbal and nonverbal behavior data were integrated at 30 Hz for display using the ELAN viewer (Wittenburg et al., 2006). This viewer enabled us to annotate the multimodal data frame-by-frame and observe the data intuitively. In this research, we only handled utterance and head-nod data in the corpus we constructed. Nods occurred in 1601 out of the 2965 IPUs. 3. Head-Nod-Generation Model The goal of our research was to demonstrate that the dialogue act of an utterance is useful for generating nods. We evaluated our proposed model for estimating nods from dialogue acts and the previously constructed estimation model using the final morphemes at the end of utterance (Ishi et al., 2006; Ishi et al., 2007). As a pr"
L18-1627,W09-3934,0,0.0690814,"Missing"
P03-1031,A00-1014,0,0.0126565,"sed discourse understanding methods have been implemented mostly in keyboard-based dialogue systems, User Utterance Dialogue Act “from two p.m. to three p.m.” [act-type=refer-start-and-endtime, start=14:00, end=15:00] Table 1: A user utterance and the corresponding dialogue act. although there are some recent attempts to apply them to spoken dialogue systems as well (Allen et al., 2001; Rich et al., 2001); however, considering the current performance of speech recognizers and the limitations in task domains, we believe framebased discourse understanding and dialogue management are sufficient (Chu-Carroll, 2000; Seneff, 2002; Bobrow et al., 1977). 3 Problem Most conventional spoken dialogue systems uniquely determine the dialogue state after a user utterance. Normally, however, there are multiple candidates for the result of speech understanding, which leads to the creation of multiple dialogue state candidates. We believe that there are cases where it is better to hold more than one dialogue state and resolve the ambiguity as the dialogue progresses rather than to decide on a single dialogue state after each user utterance. As an example, consider a piece of dialogue in which the user utterance “fr"
P03-1031,P99-1026,1,0.730556,"correct understanding result, ds3, is derived from the combination of ds1 and da3, where ds1 is induced by ds0 and da1. As shown here, holding multiple understanding results can be better than just deciding on the best speech understanding hypothesis and discarding other possibilities. In this paper, we consider a discourse understanding component that deals with multiple dialogue states. Such a component must choose the best combination of a dialogue state and a dialogue act out of all possibilities. An appropriate scoring method for the dialogue states is therefore required. 4 Related Work Nakano et al. (1999) proposed a method that holds multiple dialogue states ordered by priority to deal with the problem that some utterances convey meaning over several speech intervals and that the understanding result cannot be determined at each interval end. Miyazaki et al. (2002) proposed a method combining Nakano et al.’s (1999) method and n-best recognition hypotheses, and reported improvement in discourse understanding accuracy. They used a metric similar to the concept error rate for the evalu[System utterance (S1)] “What time would you like to reserve a meeting room?” [Dialogue act] [act-type=ask-time]"
P03-1031,P95-1016,0,0.0695136,"=nil, start=14:00, end=15:00] 2. [room=nil, start=nil, end=15:00] Figure 3: Detailed description of the understanding of the example dialogue. ation of discourse accuracy, comparing reference dialogue states with hypothesis dialogue states. Both these methods employ hand-crafted rules to score the dialogue states to decide the best dialogue state. Creating such rules requires expert knowledge, and is also time consuming. There are approaches that propose statistically estimating the dialogue act type from several previous dialogue act types using N-gram probability (Nagata and Morimoto, 1994; Reithinger and Maier, 1995). Although their approaches can be used for disambiguating user utterance using discourse information, they do not consider holding multiple dialogue states. In the context of plan-based utterance understanding (Allen and Perrault, 1980; Carberry, 1990), when there is ambiguity in the understanding result of a user utterance, an interpretation best suited to the estimated plan should be selected. In addition, the system must choose the most plausible plans from multiple possible candidates. Although we do not adopt plan-based representation of dialogue states as noted before, this problem is c"
P06-1034,W02-1022,0,0.0412888,"Missing"
P06-1034,W02-2103,0,\N,Missing
P06-1034,W98-1428,0,\N,Missing
P06-1034,J02-4007,0,\N,Missing
P06-1034,H05-1042,0,\N,Missing
P06-1034,W00-0306,0,\N,Missing
P06-1034,N06-1046,0,\N,Missing
P06-1034,C02-1138,1,\N,Missing
P06-1034,H05-2017,0,\N,Missing
P06-1034,H05-1043,0,\N,Missing
P06-1034,H92-1022,0,\N,Missing
P06-1034,A92-1021,0,\N,Missing
P06-1034,P01-1008,0,\N,Missing
P06-1034,P06-2059,0,\N,Missing
P06-1034,P07-1063,1,\N,Missing
P06-1034,N03-1003,0,\N,Missing
P06-1034,P01-1056,1,\N,Missing
P06-1034,P04-1011,1,\N,Missing
P06-1034,J02-3001,0,\N,Missing
P06-1034,P02-1053,0,\N,Missing
P06-1034,P05-1015,0,\N,Missing
P06-1034,P87-1023,0,\N,Missing
P06-1034,W04-2302,0,\N,Missing
P06-1034,N04-1041,0,\N,Missing
P06-1034,H01-1047,0,\N,Missing
P06-1034,A97-1039,0,\N,Missing
P06-1034,W06-1650,0,\N,Missing
P07-2030,C02-1053,1,0.808016,"sed model and the best-performing baseline is significant (p<0.00001). The results also show that PMI is more effective in quiz-style ranking than any other measure. The fact that max is important probably means that the mere existence of a word that has a high PMI score is enough to raise the ranking of a hint. It is also interesting that Wikipedia gives better ranking, which is probably because people’s names and related keywords are close to each other in such descriptive texts. Analyzing the ranking model trained by the ranking SVM allows us to calculate the weights given to the features (Hirao et al., 2002). Table 2 shows the top-10 features in weights in absolute figures when all samples were used for training. It can be seen that high PMI values and words/semantic categories related to government or creation lead to easy hints, whereas semantic categories, such as birth and others (corresponding to the person in ‘a person from Tokyo’), lead to early hints. This supports our intuitive notion that birthplaces should be presented early for users to start thinking about a person. 6 Summary and Future Work This paper proposed ranking definitions of a person to automatically generate a “Who is this?"
P07-2030,1991.mtsummit-papers.16,0,0.0147351,"cal/dependency parser, http://chasen.org/˜taku/software/cabocha/) and extracted all content words to make binary features representing the existence of each content word. There are 2,156 BOW features in our data. As for the semantic features, we used the semantic categories in Nihongo Goi-Taikei. Since there are 2,715 semantic categories, we created 2,715 features representing the existence of each semantic category in the definition. Semantic categories were assigned to words in the definition by a morphological analyzer that comes with ALT/J-E, a Japanese-English machine translation system (Ikehara et al., 1991). In total, we have 4,991 features to represent each definition. We calculated all feature values for all definitions in our data to be used for the learning. 4.3 Training Ranking Models Using the reference ranking data, we trained a ranking model using the ranking SVM (Joachims, 2002) (with a linear kernel) that minimizes the pairwise ranking error among the definitions of each person. 5 Evaluation To evaluate the performance of the ranking model, following (Xu et al., 2004; Sun et al., 2005), we compared it with baselines that use only the scores of IR-related and positional features for ran"
P07-2030,P03-1069,0,0.056852,"with improved understanding and lasting motivation, which is useful for educational systems. In our approach, we train a ranker that learns from data the appropriate ranking of definitions based on features that encode the importance of keywords in a definition as well as its content. Experimental results show that our approach is significantly better in ranking definitions than baselines that use conventional information retrieval measures such as tf*idf and pointwise mutual information (PMI). 1 Introduction Appropriate ranking of sentences is important, as noted in sentence ordering tasks (Lapata, 2003), in effectively delivering content. Whether the task is to convey news texts or definitions, the objective is to make it easier for users to understand the content. However, just conveying it in an encyclopedia-like or temporal order may not be the best solution, considering that interaction between a system and a user improves understanding (Sugiyama et al., 1999) and that the cognitive load in receiving information is believed to correlate with memory fixation (Craik and Lockhart, 1972). In this paper, we discuss the idea of ranking definitions as a way to present people’s biographical info"
sadamitsu-etal-2014-extraction,P06-1028,0,\N,Missing
W09-3917,D08-1040,0,0.0839349,"stening-oriented dialogues and casual conversation have characteristically different dialogue flows and that it is important for listening agents to selfdisclose before asking questions and to utter more questions and acknowledgment than in casual conversation to be good listeners. 1 Introduction Although task-oriented dialogue systems have been actively researched over the years (Walker et al., 2001), systems that perform more flexible (less taskoriented) dialogues such as chats are beginning to be actively investigated from their social and entertainment aspects (Bickmore and Cassell, 2001; Higuchi et al., 2008). This paper deals with dialogues in which one conversational participant attentively listens to the other (hereafter, listening-oriented dialogue). Our aim is to build listening agents that can implement such a listening process so that a user can satisfy his/her desire to speak and have him/herself heard. Such agents would lead the user’s state of mind for the better as in a therapy session, although we want our listening agents to help users mentally in everyday conversation. It should also be noted that the purpose of the listening-oriented dialogue is to simply listen to users, not to eli"
W09-3917,P01-1066,0,0.0246529,"both listening-oriented dialogues and casual conversation, and analyzed them by comparing the frequency of dialogue acts, as well as the dialogue flows using Hidden Markov Models (HMMs). The analysis revealed that listening-oriented dialogues and casual conversation have characteristically different dialogue flows and that it is important for listening agents to selfdisclose before asking questions and to utter more questions and acknowledgment than in casual conversation to be good listeners. 1 Introduction Although task-oriented dialogue systems have been actively researched over the years (Walker et al., 2001), systems that perform more flexible (less taskoriented) dialogues such as chats are beginning to be actively investigated from their social and entertainment aspects (Bickmore and Cassell, 2001; Higuchi et al., 2008). This paper deals with dialogues in which one conversational participant attentively listens to the other (hereafter, listening-oriented dialogue). Our aim is to build listening agents that can implement such a listening process so that a user can satisfy his/her desire to speak and have him/herself heard. Such agents would lead the user’s state of mind for the better as in a the"
W09-3932,P07-2030,1,0.511528,"we exploit a new style of dialogue called thoughtevoking dialogue and experimentally investigate the impact of a peer agent’s presence and agent emotional expressions on communication activation in thought-evoking multi-party dialogues. A thought-evoking dialogue, an interaction in which agents act on the willingness of users to provoke user thinking and encourage involvement in the dialogue, has the potential to activate interaction among participants in multi-party dialogues. Previous work proposed a quiz-style information presentation dialogue system (hereafter quizstyle dialogue system) (Higashinaka et al., 2007a) that is regarded as a kind of thought-evoking dialogue system. This system conveys contents as biographical facts of famous people through quizstyle interaction with users by creating a ”Who is this?” quiz and individually presenting hints. This paper presents an experimental study that analyzes how conversational agents activate human communication in thought-evoking multi-party dialogues between multi-users and multi-agents. A thought-evoking dialogue, which is a kind of interaction in which agents act on user willingness to provoke user thinking, has the potential to stimulate multi-part"
W09-3932,H94-1037,0,0.0250388,"results showed that the presence of a peer agent significantly improved user satisfaction and increased the number of user utterances. We also found that agent empathic expressions significantly improved user satisfaction, raised user ratings of a peer agent, and increased user utterances. Our findings will be useful for stimulating multi-party communication in various applications such as educational agents and community facilitators. 1 Introduction Conversational interfaces including dialogue systems and conversational agents have been typically used as a single interface to a single user (Zue et al., 1994; Allen et al., 2001; Cassell et al., 2000). On the other hand, a new area of research in conversational interfaces is dealing with multi-party interaction (Traum and Rickel, 2002; Liu and Chee, 2004; Zheng et al., 2005). Multiparty conversational interfaces have been applied Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 217–224, c Queen Mary University of London, September 2009. 2009 Association for Computational Linguistics 217 impact on users in a thought-evoking multi-party dialogue system. As stated above, Brave et al."
W10-4304,hara-etal-2010-estimation,0,\N,Missing
W10-4304,W09-3926,0,\N,Missing
W10-4304,W09-3917,1,\N,Missing
W10-4304,P97-1035,0,\N,Missing
W10-4358,W09-3932,1,0.823655,"er-adaptive coordination of agent communicative behavior. Section 4 explains the experiment, and Section 5 describes its results. Section 6 concludes our paper. with flexible turn-taking. We present a method for user-adaptive coordination of agent communicative behavior to reduce the discomfort perceived by individual users during the interaction and show experimental results that evaluate how the method influences agent communicative behavior and improves its relevance as perceived by users. For evaluation purposes, we used a quizstyle multi-party spoken dialogue system (Minami et al., 2007; Dohsaka et al., 2009). A quiz-style dialogue is a kind of thought-evoking dialogue that can stir user thinking and activate communication (Higashinaka et al., 2007a; Dohsaka et al., 2009). This characteristic is expected to be advantageous for evaluation experiments since it encourages involvement in the dialogue. Our method adapts agent communicative behavior based on policy gradient reinforcement learning (Sutton et al., 2000; Kohl and Stone, 2004). The policy gradient method has been used for robot communicative behavior adaptation (Mitsunaga et al., 2005; Tapus and Matari´c, 2007). However, both studies dealt"
W10-4358,P07-2030,1,0.902541,"concludes our paper. with flexible turn-taking. We present a method for user-adaptive coordination of agent communicative behavior to reduce the discomfort perceived by individual users during the interaction and show experimental results that evaluate how the method influences agent communicative behavior and improves its relevance as perceived by users. For evaluation purposes, we used a quizstyle multi-party spoken dialogue system (Minami et al., 2007; Dohsaka et al., 2009). A quiz-style dialogue is a kind of thought-evoking dialogue that can stir user thinking and activate communication (Higashinaka et al., 2007a; Dohsaka et al., 2009). This characteristic is expected to be advantageous for evaluation experiments since it encourages involvement in the dialogue. Our method adapts agent communicative behavior based on policy gradient reinforcement learning (Sutton et al., 2000; Kohl and Stone, 2004). The policy gradient method has been used for robot communicative behavior adaptation (Mitsunaga et al., 2005; Tapus and Matari´c, 2007). However, both studies dealt with scenariobased interaction in which a user and a robot acted with predetermined timing. In contrast, we focus on spoken dialogues in which"
W13-4051,P07-2057,0,0.0477725,"Missing"
W13-4051,C12-1172,0,0.0275121,"current events by retrieving, from news articles, descriptions containing similar dependency structures as those of the user’s questions. Although this retrieval-based approach is effective for answering the user’s factual questions, it is insufficient to generate subjective utterances for conversational dialogue systems since such systems are required to introduce Introduction The need for open-domain conversational dialogue systems continues to grow. Such systems are beginning to be actively investigated from their social and entertainment aspects (Shibata et al., 2009; Ritter et al., 2011; Wong et al., 2012); conversational dialogues also have potential for therapy purposes and for evoking a user’s unconscious requests in task-oriented dialogues (Bickmore and Cassell, 2001). However, developing open-domain conversational dialogue systems is difficult, since the huge variety of user utterances makes it harder to build knowledge resources for generating appropriate system responses. To address this issue, previous research has selected system utterances from web articles or microblogs on the basis of surface cohesion and shallow semantic coherence (Shibata et al., 2009; Jafarpour and Burges, 2010;"
W13-4051,W11-2008,0,0.0191053,"urce-reply tweet pairs as a bilingual corpus. They compared the following three approaches: IR-status, which retrieves reply tweets whose associated source tweets most resemble the user utterance (Jafarpour and Burges, 2010); IR-response, which retrieves reply tweets that are the most similar to the user utterance; and their proposed SMT-based approach, named MTchat. They reported that MT-chat outperformed the other approaches and that IR-response was superior to IR-status. However, these approaches used only the words, and not the structures, of user utterances to generate system utterances. Yoshino et al. (2011) proposed a QA system that answers questions about current events by retrieving, from news articles, descriptions containing similar dependency structures as those of the user’s questions. Although this retrieval-based approach is effective for answering the user’s factual questions, it is insufficient to generate subjective utterances for conversational dialogue systems since such systems are required to introduce Introduction The need for open-domain conversational dialogue systems continues to grow. Such systems are beginning to be actively investigated from their social and entertainment a"
W13-4051,D11-1054,0,\N,Missing
W13-4051,J00-3003,0,\N,Missing
W13-4051,C10-1086,1,\N,Missing
W15-0813,P09-4001,1,0.792485,"ntradictory Event Pairs As mentioned above, the recognition of contradictory event pairs is related to RTE. In some RTE tasks, contradiction is one of the relations between text and hypothesis. For example, Harabagiu et al. (2006) proposed a method to recognize contradictions between texts using negation expressions, antonyms, and discourse analysis. Recognition of contradictory event pairs plays an important role in the systems that detect contradictions between information extracted from web texts. For example, there are several systems that detect contradictory information, such as WISDOM (Akamine et al., 2009), Statement Map (Murakami et al., 2009), and Dispute Finder (Ennals et al., 2010). 2.2 Acquisition of Contradictory Event Pairs Hashimoto et al. (2012) and Kloetzer et al. (2013) proposed methods for acquiring contradictory event pairs. Hashimoto et al. (2012) collected Japanese contradictory and consistent event pairs using templates of semantic polarities that indicate excitatory, inhibitory, and neutral properties. A template consists of a particle and a predicate, such as “を (particle) 破壊する (destroy)” and “を (particle) 進行させ る (develop).” They collected one million contradictory event pairs"
W15-0813,D12-1057,0,0.0743454,"Missing"
W15-0813,izumi-etal-2014-constructing,1,0.777391,"rs simultaneously, contradictions of such pairs have a strong relation with negation, such as ⟨having a meal, not having a meal⟩ and ⟨eating to excess, eating moderately⟩. There are also contradictory event pairs based on sibling relations, such as ⟨being in Tokyo, being in Kyoto⟩, where “Tokyo” and “Kyoto” have a sibling relation. We therefore classify negation and sibling relations into binary (e.g., “single” and “married”), discrete (e.g., “Tokyo” and “Kyoto”), and continuous (e.g., “expensive” and “cheap”). Furthermore, negation has the following two classes that can cause contradictions (Izumi et al., 2014): sequential event relations, such as “getting on” and “getting off,” and counterpart perspective relations, such as “selling” and “buying.” We added these classes to our taxonomy. The subclasses of simultaneous contradictions are detailed below. 1-a. binary When an event pair includes mutually exclusive antonyms (e.g., “single” and “married”) or a predicate and its negation (e.g., “going” and “not going”), these events are contradictory. We call such contradictory event pairs binary. 1-b. discrete When an event pair consists of predicates or arguments that have sibling relations, such as ⟨bei"
W15-0813,D13-1065,0,\N,Missing
W15-4611,C14-1088,1,0.65543,"erativity-related parameters” as important elements that affect interaction quality in telephone-based services. Since there are no chat data available for analysis, we decided to create our own data set using a publicly available chat API. In this section, we describe our system based on the API, data collection procedure, and statistics of the collected dialogues. 3.1 System We built a web-based dialogue data collection system using a chat API by NTT DOCOMO (Onishi and Yoshimura, 2014). The system is text-based, and users can chat with it in Japanese. Since the API is based on the system by Higashinaka et al. (2014a), which uses a number of natural language processing techniques (e.g., zero anaphora resolution, language generation from predicateargument structures, and open-domain question answering), the system can be regarded as one of the most advanced (or at least complex) chatoriented dialogue systems. Users can chat with the system on a web browser to create 21-utterance-long dialogues; each dialogue consists of one system prompt followed by ten user-system utterance pairs. Figure 1 shows an excerpt of a dialogue collected with the system. As can be seen, the content of the conversation is open-do"
W15-4611,P12-3007,0,0.0348816,"ta collection of chat dialogues and analyses of dialogue breakdowns, we classified errors and created a taxonomy. Although the proposed taxonomy may not be complete, this paper is the first to present a taxonomy of errors in chat-oriented dialogue systems. We also highlight the difficulty in pinpointing errors in such systems. 1 Introduction The last decade has seen an emergence of systems that can engage in chat, small talk, or open-domain conversation. Such systems can be useful for cultivating trust between a system and users (Bickmore and Cassell, 2001), entertaining users (Wallace, 2004; Banchs and Li, 2012; Wilcock and Jokinen, 2013), and obtaining preferences from users for recommendations (Bang et al., 2015). Error analysis is important to improve any system. However, little is known about the types of errors that can be made in chat-oriented dialogue systems. This is in contrast with many studies on task-oriented dialogue systems in which various taxonomies of errors have been proposed (Dybkjær et al., 1996; M¨oller et al., 2007; Ward et al., 2005; Green et al., 2006). This paper presents a taxonomy of errors in chat-oriented dialogue systems. In our approach, we collect dialogues with a cha"
W15-4611,2005.sigdial-1.17,0,0.0396464,"Missing"
W15-4611,2005.sigdial-1.14,0,0.0846342,"ify the errors that 2 Related Work In task-oriented dialogue systems, there is a good body of research related to the classification of errors. There are several ways to categorize errors. One is to adopt the general taxonomy of miscommunication proposed by Clark (1996). According to Clark, there are four levels in communication; channel, signal, intention, and conversation, and by using these four levels, errors can be classified into four categories depending on which level the errors occurred. For example, if the system fails to take in audio input, it is regarded as a channel-level error. Bohus and Rudnicky (2005) applied this taxonomy to classify their non-understanding errors. A similar categorization was used by M¨oller et al. (2007) for their smart home and restaurant information systems. Paek (2003) discussed the generality of using the four levels for error analysis in dialogue systems, referring to the use cases across disciplines. 87 Proceedings of the SIGDIAL 2015 Conference, pages 87–95, c Prague, Czech Republic, 2-4 September 2015. 2015 Association for Computational Linguistics 3 Data Collection From the viewpoint of cooperativeness in dialogue, there is also a taxonomy based on Grice’s maxi"
W15-4611,C96-1056,0,0.725545,"chat, small talk, or open-domain conversation. Such systems can be useful for cultivating trust between a system and users (Bickmore and Cassell, 2001), entertaining users (Wallace, 2004; Banchs and Li, 2012; Wilcock and Jokinen, 2013), and obtaining preferences from users for recommendations (Bang et al., 2015). Error analysis is important to improve any system. However, little is known about the types of errors that can be made in chat-oriented dialogue systems. This is in contrast with many studies on task-oriented dialogue systems in which various taxonomies of errors have been proposed (Dybkjær et al., 1996; M¨oller et al., 2007; Ward et al., 2005; Green et al., 2006). This paper presents a taxonomy of errors in chat-oriented dialogue systems. In our approach, we collect dialogues with a chat-oriented dialogue system and identify breakdowns (situations in which users cannot proceed with the conversation (Martinovsky and Traum, 2003)) as possible points of errors. Then, we classify the errors that 2 Related Work In task-oriented dialogue systems, there is a good body of research related to the classification of errors. There are several ways to categorize errors. One is to adopt the general taxon"
W15-4611,W09-3906,0,0.0299386,"ows an excerpt of a dialogue collected with the system. As can be seen, the content of the conversation is open-domain. See (Higashinaka et al., 2014a) for more dialogue examples. There is also an approach to creating a task or system-specific taxonomy or errors. Aberdeen and Ferro (2003) analyzed misunderstandings by a DARPA communicator system and classified its errors into such categories as failure to obey command and repeated prompt. There is also a taxonomy for a service robot (Green et al., 2006), in which major errors are robot-specific, such as timing and reference (pointing) errors. Dzikovska et al. (2009) also classified errors in a tutorial dialogue system. Dialogue systems are usually composed of various modules. Therefore, there is also an approach to attributing errors to modules. Ward et al. (2005) attributed causes of errors to modules, such as speech recognition, understanding, generation, and synthesis, and discussed their relative impact on usability. This approach is useful when the system has clear separation of modules. 3.2 Procedure We called for dialogue researchers (and their collaborators) in Japan to participate in our data collection. Since the system may not live up to their"
W15-4611,W14-6808,0,0.0160311,"pproach is language-independent, we need to verify this with systems using other languages. Our ultimate goal is to reduce errors in chatoriented dialogue systems. Although we strive to reduce errors ourselves, since the errors concern many aspects of conversation, we are planning to make dialogue-breakdown detection an open challenge. To this end, we have released the data1 to the public so that researchers in the field can test their ideas for detecting breakdowns. Although there have been approaches to detecting errors in open-domain conversation, the reported accuracies are not that high (Xiang et al., 2014; Higashinaka et al., 2014b). We believe our taxonomy will be helpful for conceptualizing the errors, and the forthcoming challenge will encourage a more detailed analysis of errors in chat-oriented dialogue systems. ing among the categories related to Grice’s maxims that attributed to this low agreement, due to the possible reason of subjectivity. While we improve the categories and the labeling scheme to cope with the subjectivity, our suggestion for the time being is to shrink Grice’s maxim-related categories (in both RES and CON) to one “cooperativeness error” category. To support this ide"
W16-3619,C14-1088,1,0.853402,"particular, we elicited post-dialogue comments from speakers and analyzed the comments to determine what they thought about the dialogues while they engaged in them. In addition, by analyzing the effectiveness of their thoughts, we found that dialogue strategies for personalization related to “topic elaboration”, “topic changing” and “tempo” significantly increased the satisfaction with regard to the dialogues. 1 Introduction Recent research on dialogue agents has focused on casual conversations or chats (Bickmore and Picard, 2005; Ritter et al., 2011; Wong et al., 2012; Meguro et al., 2014; Higashinaka et al., 2014) because chat-oriented conversational agents are useful for entertainment or counseling purposes. For chat-oriented conversational agents, it is important to personalize their utterances to increase user satisfaction (Sugo and Hagiwara, 2014). Several methods to personalize system utterances using user information extracted from dialogues have been proposed (Sugo and Hagiwara, 2014; Kim et al., 2014; Kobyashi and Hagiwara, 2016). Although we know that personalization is important, 2 Related Work ELIZA (Weizenbaum, 1966) and ALICE (Wallace, 2004) are chat-oriented conversational agents that hav"
W16-3619,D11-1054,0,0.0235797,"ances when speaking to each other in casual conversations. In particular, we elicited post-dialogue comments from speakers and analyzed the comments to determine what they thought about the dialogues while they engaged in them. In addition, by analyzing the effectiveness of their thoughts, we found that dialogue strategies for personalization related to “topic elaboration”, “topic changing” and “tempo” significantly increased the satisfaction with regard to the dialogues. 1 Introduction Recent research on dialogue agents has focused on casual conversations or chats (Bickmore and Picard, 2005; Ritter et al., 2011; Wong et al., 2012; Meguro et al., 2014; Higashinaka et al., 2014) because chat-oriented conversational agents are useful for entertainment or counseling purposes. For chat-oriented conversational agents, it is important to personalize their utterances to increase user satisfaction (Sugo and Hagiwara, 2014). Several methods to personalize system utterances using user information extracted from dialogues have been proposed (Sugo and Hagiwara, 2014; Kim et al., 2014; Kobyashi and Hagiwara, 2016). Although we know that personalization is important, 2 Related Work ELIZA (Weizenbaum, 1966) and ALI"
W16-3619,C12-1172,0,0.0173787,"o each other in casual conversations. In particular, we elicited post-dialogue comments from speakers and analyzed the comments to determine what they thought about the dialogues while they engaged in them. In addition, by analyzing the effectiveness of their thoughts, we found that dialogue strategies for personalization related to “topic elaboration”, “topic changing” and “tempo” significantly increased the satisfaction with regard to the dialogues. 1 Introduction Recent research on dialogue agents has focused on casual conversations or chats (Bickmore and Picard, 2005; Ritter et al., 2011; Wong et al., 2012; Meguro et al., 2014; Higashinaka et al., 2014) because chat-oriented conversational agents are useful for entertainment or counseling purposes. For chat-oriented conversational agents, it is important to personalize their utterances to increase user satisfaction (Sugo and Hagiwara, 2014). Several methods to personalize system utterances using user information extracted from dialogues have been proposed (Sugo and Hagiwara, 2014; Kim et al., 2014; Kobyashi and Hagiwara, 2016). Although we know that personalization is important, 2 Related Work ELIZA (Weizenbaum, 1966) and ALICE (Wallace, 2004)"
W16-3641,P07-1063,0,0.839051,"conversation partner (Miyazaki et al., 2016). However, these methods handle only function words or have difficulty in altering other expressions. In this respect, we consider these methods to be insufficient to express a particular character’s linguistic style, especially when focusing on fictional characters whose individualities should be vividly expressed. There have also been several studies on natural language generation that can adapt to speakers’ personalities. In particular, a language generator called PERSONAGE that can control parameters related to speakers’ Big Five personalities (Mairesse and Walker, 2007) has been proposed. There is also a method for automatically adjusting the language generation parameters of PERSONAGE by using movie scripts (Walker et al., 2011) and a method for automatically adjusting the parameters so that they suit the characters or stories of role playing games (Reed et al., 2011). However, although there is some aspect of linguistic style that is essential to expressing a particular character’s style, PERSONAGE does not have any existing parameter that can manifest that linguistic reflex (Walker et al., 2011). In the present work, we focus on the languages of fictional"
W16-3641,Y15-1035,1,0.895645,"Japanese Fictional Characters Chiaki Miyazaki∗ Toru Hirano† Ryuichiro Higashinaka Yoshihiro Matsuo NTT Media Intelligence Laboratories, Nippon Telegraph and Telephone Corporation 1-1 Hikarinooka, Yokosuka, Kanagawa, Japan {miyazaki.chiaki, hirano.tohru, higashinaka.ryuichiro, matsuo.yoshihiro}@lab.ntt.co.jp Abstract is expensive to do this for a large number of utterances. To reduce this cost, we have previously proposed a couple of methods for automatically converting functional expressions into those that are suitable for given personal attributes such as gender, age, and area of residence (Miyazaki et al., 2015) and closeness with a conversation partner (Miyazaki et al., 2016). However, when it comes to expressing the linguistic styles of individual fictional characters whose characteristics should be vividly expressed, these methods, which can convert only function words, i.e., which cannot convert content words such as nouns, adjectives, and verbs, do not have sufficient expressive power. As the first step in developing a natural language generator that can express the linguistic styles of fictional characters, in this work, we analyze the linguistic peculiarities of fictional characters such as th"
W16-4403,N16-1056,0,0.366395,"sentence accurately, it is important to consider the semantic meanings of a sequence of sentences and their HTML tag information. For example, sentences in a listing structure will belong to the same class. A sentence also has a sequence of words, and each word has different importance in forming the semantic meanings of the sentence. Recently, recurrent neural networks (RNNs) have been very successful in capturing semantic representations of word and sentence sequential data in several tasks, including machine translation (Bahdanau et al., 2014), Named Entity Recognition (Joshi et al., 2015; Jagannatha and Yu, 2016) and document classification (Yang et al., 2016). In the work reported in this paper, we attempted to develop a neural network model to capture semantic representations of word and sentence sequential data and classify each sentence in a document into attributes and condition classes. We developed and here propose a hierarchical neural network that classifies each sentence into attribute and condition classes by learning two classification problems jointly. Experimental results demonstrated that our network performed better than baseline methods by capturing the semantics and structures of sen"
W16-4403,W15-1522,0,0.0218815,"”. To classify each sentence accurately, it is important to consider the semantic meanings of a sequence of sentences and their HTML tag information. For example, sentences in a listing structure will belong to the same class. A sentence also has a sequence of words, and each word has different importance in forming the semantic meanings of the sentence. Recently, recurrent neural networks (RNNs) have been very successful in capturing semantic representations of word and sentence sequential data in several tasks, including machine translation (Bahdanau et al., 2014), Named Entity Recognition (Joshi et al., 2015; Jagannatha and Yu, 2016) and document classification (Yang et al., 2016). In the work reported in this paper, we attempted to develop a neural network model to capture semantic representations of word and sentence sequential data and classify each sentence in a document into attributes and condition classes. We developed and here propose a hierarchical neural network that classifies each sentence into attribute and condition classes by learning two classification problems jointly. Experimental results demonstrated that our network performed better than baseline methods by capturing the seman"
W16-4403,P15-1101,0,0.0286381,"word or phrase values about attributes in the documents. However, since our aim is to extract sentence-level values of attributes in a web product document, we use HTML tags as features and focus on capturing the importance of words in a sentence to classify the sentence by using the attention architecture. 2.2 Sentence-level attribute extraction A number of related studies have been performed for extracting sentence values of attributes in several tasks, such as event information extraction (Naughton et al., 2008), extractive summarization (Nishikawa et al., 2015) and emotion classification (Li et al., 2015). Naughton et al. (2008) evaluated the performance of a support vector machine classifier and a language modeling approach for the task of identifying the sentences in a document that describe one or more instances of a specified event type. They use the words of a sentence as features and do not focus on the sentence sequences. Nishikawa et al. (2015) proposed a method for query-oriented extractive summarization to extract information especially from Wikipedia article for a question answering system. This method can extract sentences that present values of product attributes using semi hidden"
W16-4403,C08-1078,0,0.0345083,"rameworks. Their model focuses mainly on word sequence information, which is effective for extracting word or phrase values about attributes in the documents. However, since our aim is to extract sentence-level values of attributes in a web product document, we use HTML tags as features and focus on capturing the importance of words in a sentence to classify the sentence by using the attention architecture. 2.2 Sentence-level attribute extraction A number of related studies have been performed for extracting sentence values of attributes in several tasks, such as event information extraction (Naughton et al., 2008), extractive summarization (Nishikawa et al., 2015) and emotion classification (Li et al., 2015). Naughton et al. (2008) evaluated the performance of a support vector machine classifier and a language modeling approach for the task of identifying the sentences in a document that describe one or more instances of a specified event type. They use the words of a sentence as features and do not focus on the sentence sequences. Nishikawa et al. (2015) proposed a method for query-oriented extractive summarization to extract information especially from Wikipedia article for a question answering syste"
W16-4403,N16-1174,0,0.269066,"emantic meanings of a sequence of sentences and their HTML tag information. For example, sentences in a listing structure will belong to the same class. A sentence also has a sequence of words, and each word has different importance in forming the semantic meanings of the sentence. Recently, recurrent neural networks (RNNs) have been very successful in capturing semantic representations of word and sentence sequential data in several tasks, including machine translation (Bahdanau et al., 2014), Named Entity Recognition (Joshi et al., 2015; Jagannatha and Yu, 2016) and document classification (Yang et al., 2016). In the work reported in this paper, we attempted to develop a neural network model to capture semantic representations of word and sentence sequential data and classify each sentence in a document into attributes and condition classes. We developed and here propose a hierarchical neural network that classifies each sentence into attribute and condition classes by learning two classification problems jointly. Experimental results demonstrated that our network performed better than baseline methods by capturing the semantics and structures of sentences. We also evaluated the network in experim"
W18-5008,P17-1120,0,0.0224725,"Missing"
W18-5008,W16-3646,0,0.0278017,"Missing"
W18-5024,maekawa-etal-2000-spontaneous,0,0.0379859,"employed for early stopping. We constructed five models by varying an initial parameter for individual conditions and evaluated the average performance. When using either target speaker’s utterances or interlocutor’s utterances, required components were only used for building the proposed modeling. We used following sequential features. F0 represents 2 dimensional sequential features of F0 and ∆F0; frame shift was set to 5 ms. SENONE represents 256-dimensional senone bottleneck features extracted from 3-layer senone LSTM-RNN with 256 units trained from a corpus of spontaneous Japanese speech (Maekawa et al., 2000). Its frame shift was set to 10 ms, and the bottleneck layer was set to the third LSTMRNN layer. PRON represents pronunciation sequences, and WORD represents word sequences of interlocutor’s utterances. The lexical features were introduced by converting them into 128 dimensional vectors through linear transformation that was also optimized in training. 4 Conclusions In this paper, we proposed a neural dialogue context online end-of-turn detection method. Main advance of the proposed method is taking long-range interaction information between target speaker’s and interlocutor’s utterances into"
W18-5024,C08-2003,0,0.0417337,"ion that determines whether a target speaker’s utterance is ended or not is an essential technology (Sacks et al., 1974; Meena et al., 2014; Ward and Vault, 2015). It is widely known that heuristic end-of-turn detection based on nonspeech duration determined by speech activity detection (SAD) is insufficient for smooth turntaking (Hariharan et al., 2001). Various methods have been examined for modeling the end-of-turn detection (Koiso et al., 1998; Shriberg et al., 2000; Schlangen, 2006; Gravano and Hirschberg, 2011; Sato et al., 2002; Guntakandla and Nielsen, 2015; Ferrer et al., 2002, 2003; Atterer et al., 2008; Arsikere et al., 2014, 224 Proceedings of the SIGDIAL 2018 Conference, pages 224–228, c Melbourne, Australia, 12-14 July 2018. 2018 Association for Computational Linguistics In our experiments, human-human contact center dialogue data sets are used with the goal of constructing a human-like interactive voice response system. We show that the proposed method outperforms a variant that uses only target speaker’s utterances. 2 Proposed Method End-of-turn detection is the problem of detecting whether each end-of-utterance point is a turntaking point or not. The utterance is defined as an interna"
W18-5024,W17-5527,0,0.0262461,"NTT Media Intelligence Laboratories, NTT Corporation, 1-1, Hikarinooka, Yokosuka-shi, Kanagawa, 239-0847, Japan ryou.masumura.ba@hco.ntt.co.jp Abstract 2015). A general approach is discriminative modeling using acoustic or linguistic features extracted from target speaker’s current utterance. In addition, recent studies use recurrent neural networks (RNNs) as they are suitable for directly capturing long-range sequential features without manual specification of fixed length features such as maximum, minimum, average values of acoustic features or bag-of-words features (Masumura et al., 2017; Skantze, 2017) We note, however, that interlocutor’s utterances are rarely used for end-of-turn detection. In dialogues, target speaker’s utterances are definitely impacted by the interlocutor’s utterances (Heeman and Lunsford, 2017). It is expected that we can improve end-of-utterance detection performance by capturing the “interaction” between the target speaker and the interlocutor. In this paper, we propose a neural dialoguecontext online end-of-turn detection method that can flexibly utilize both target speaker’s and interlocutor’s utterances. To the best of our knowledge, this paper is the first study"
W18-5024,E17-1041,0,0.0305172,"ances (Heeman and Lunsford, 2017). It is expected that we can improve end-of-utterance detection performance by capturing the “interaction” between the target speaker and the interlocutor. In this paper, we propose a neural dialoguecontext online end-of-turn detection method that can flexibly utilize both target speaker’s and interlocutor’s utterances. To the best of our knowledge, this paper is the first study to utilize dialoguecontext information for neural end-of-turn detection. Although some natural language processing tasks recently examine dialogue-context modeling (Liu and Lane, 2017; Tran et al., 2017), they cannot handle multiple acoustic and lexical features individually extracted from both target speaker’s and interlocutor’s utterances. In the proposed method, target speaker’s and interlocutor’s multiple sequential features, and their interactions are captured by stacking multiple time-asynchronous long short-term memory RNNs (LSTM-RNNs). In order to achieve low-delayed end-of-turn detection in spoken dialogue systems, acoustic sequential features extracted from target speaker’s speech and linguistic sequential features extracted from the interlocutor’s (system’s) responses are used for"
W18-5031,I17-2069,0,0.0188063,"lay the roles of certain characters and respond to questions by online users. Focusing on two famous characters, we conducted a large-scale experiment to collect question-answer pairs by using real users. We evaluated the effectiveness of role play-based questionanswering and found that, by using our proposed method, the collected pairs lead to good-quality chatbots that exhibit consistent personalities. 1 Introduction Having a consistent personality is important for chatbots if we want them to be believable (Li et al., 2016; Gordon et al., 2016; Curry and Rieser, 2016; Sugiyama et al., 2017; Akama et al., 2017). Although neural networkbased methods are emerging for achieving consistent personalities, their quality is not that high (Li et al., 2016). Therefore, in many systems, question-answer pairs are prepared by hand for consistent responses (Takeuchi et al., 2007; Leuski et al., 2009; Traum et al., 2015). However, the creation of such pairs is costly. In this study, our aim is to collect a large number of question-answer pairs for a particular character by using role play-based questionanswering (Higashinaka et al., 2013a) in which • We verified that role play-based question264 Proceedings of the"
W18-5031,W16-3641,1,0.926706,"-ness, although we believe that having the ability to converse naturally is a requirement for chatbots. When we focus on the results as they relate to the knowledge levels, we see large differences between High and Low. The High participants are likely to differentiate the answers more than Low Rule-based baseline (AIML) The typical approach to implement a chatbot is by using rules. We used the rules written in AIML created by Higashinaka et al (2015). There are roughly 300K rules. In Japanese, sentence-end expressions are key factors to exhibit personality. Therefore, following the method by Miyazaki et al. (2016), we created sentence-end conversion rules so that the output of this method would have the sentence-end expressions that match the characters in question. Retrieval-based method (LUCENE) The retrieval-based method described in Section 3.1. Proposed method 1 (PROP WO EXDB) The proposed method described in Section 3.2. This method does not use the extended question-answer pairs from Twitter. The weights w1 . . . w6 are all set to 1.0. We used 10 for N for document retrieval. Proposed method 2 (PROP) The proposed method with extended question-answer pairs from Twitter, as described in Section 3."
W18-5031,sekine-etal-2002-extended,0,0.0254325,"cal analysis. Given an input question, the BM25 algorithm (Walker et al., 1997) is used to search for a similar question using the content words of the input question. The answers for the retrieved questions are used as the output of this method. Although simple, this method is quite competitive with other methods when there are many question-answer pairs because it is likely that we will be able to find a similar question by word matching. 2. The question-type estimation and extended named entity recognition modules estimate the question types of Q and Q′ and extract extended named entities (Sekine et al., 2002) contained in A′ . The question-type match score is calculated by using the match of the question type and the number of extended named entities in A′ requested by Q. See Section 3.3 for details. 3.2 Proposed method Only using word-matching may not be sufficient. Therefore, we developed a more elaborate method that re-ranks the results retrieved from LUCENE. Our idea comes from cross-lingual question answering (CLQA) (Leuski et al., 2009) and recent advances in neural conversational models (Vinyals and Le, 2015). We also conducted semantic and intent-level matching between ques4. The translati"
W18-5031,C14-1088,1,0.83145,"namely p(A′ |Q). Since the amount of question-answer pairs was limited, we first trained a model by using our in-house question-answering data comprising 0.5 million pairs. The data were collected using crowd-sourcing. We then adapted the model to our question-answer pairs. The model for p(Q|A′ ) was trained in the same manner by swapping the 3.3 Modules We describe some of the models/modules used in the above steps. Question-type estimation and extended named entity recognition We estimated four question types for a question. One is a general question type. We used the taxonomy described in (Higashinaka et al., 2014), which has 16 question subtypes. We trained a logistic-regression based question-type classifier that classifies a question into one of the 16 question types. The other three question types come from an extended named entity taxonomy proposed by Sekine (2002). The taxonomy has three layers ranging from abstract 3 268 http://opennmt.net/ sets to train the translation models. In addition, the question-answer pairs used by LUCENE for retrieval consisted only of train and development data. For each character, 50 questions were randomly sampled from the test set and used as input questions for thi"
W18-5031,W15-4629,0,0.021186,"proposed method, the collected pairs lead to good-quality chatbots that exhibit consistent personalities. 1 Introduction Having a consistent personality is important for chatbots if we want them to be believable (Li et al., 2016; Gordon et al., 2016; Curry and Rieser, 2016; Sugiyama et al., 2017; Akama et al., 2017). Although neural networkbased methods are emerging for achieving consistent personalities, their quality is not that high (Li et al., 2016). Therefore, in many systems, question-answer pairs are prepared by hand for consistent responses (Takeuchi et al., 2007; Leuski et al., 2009; Traum et al., 2015). However, the creation of such pairs is costly. In this study, our aim is to collect a large number of question-answer pairs for a particular character by using role play-based questionanswering (Higashinaka et al., 2013a) in which • We verified that role play-based question264 Proceedings of the SIGDIAL 2018 Conference, pages 264–272, c Melbourne, Australia, 12-14 July 2018. 2018 Association for Computational Linguistics Role-Player 1 Question Answer User 1 Answer Character … … Broadcast questions Answer Role-Player N Answer Question User M Figure 1: Role play-based question-answering scheme"
W18-5031,P16-1094,0,0.337839,"icular character by using role playbased question-answering in which multiple users play the roles of certain characters and respond to questions by online users. Focusing on two famous characters, we conducted a large-scale experiment to collect question-answer pairs by using real users. We evaluated the effectiveness of role play-based questionanswering and found that, by using our proposed method, the collected pairs lead to good-quality chatbots that exhibit consistent personalities. 1 Introduction Having a consistent personality is important for chatbots if we want them to be believable (Li et al., 2016; Gordon et al., 2016; Curry and Rieser, 2016; Sugiyama et al., 2017; Akama et al., 2017). Although neural networkbased methods are emerging for achieving consistent personalities, their quality is not that high (Li et al., 2016). Therefore, in many systems, question-answer pairs are prepared by hand for consistent responses (Takeuchi et al., 2007; Leuski et al., 2009; Traum et al., 2015). However, the creation of such pairs is costly. In this study, our aim is to collect a large number of question-answer pairs for a particular character by using role play-based questionanswering (Higashinaka"
W18-5031,D09-1036,0,0.0129568,"s not work well, our proposed method that uses translation models as well as question-type matching and center-word extraction works well, showing reasonable scores in terms of naturalness and character-ness. For future work, we need to consider approaches to improve the quality of the proposed method. For example, we are currently using equal weights for scoring. We believe that they can be optimized using training data. We also want to incorporate other pieces of information that may contribute to the ranking of answers, such as sentence embeddings (Kiros et al., 2015), discourse relations (Lin et al., 2009; Otsuka et al., 2017), and external knowledge about the characters. Although we used two very different characters in this paper, we want to use additional types of characters as targets for role play-based question-answering. We also want to incorporate the chatbots into the Web sites so that the users can feel they are training up the characters. participants. For example, for Murai, there were only few cases in which there was statistical significance between the proposed methods when the knowledge level was low. The tendency was the same for Ayase. This highlights the difficulty in evalua"
W18-5031,P07-1063,0,0.0531979,"e, respectively. Overall, since the proposed methods achieved character-ness scores well over 3 (which is the middle point in the scale), we conclude that we can create chatbots with consistent personalities by means of role play-based question-answering. 5 Related Work Although there have not been any studies involving role play-based question-answering for data collection, there is a large body of research for creating chatbots that show consistent personalities. There have been several studies on characters by generating or rewriting utterances reflecting the underlying personality traits (Mairesse and Walker, 2007; Sugiyama et al., 2014; Miyazaki et al., 2016). In addition, there has been extensive research on extending neural conversational models to reflect personal profiles (Li et al., 2016). Although such neural networkbased methods show promising results, they still suffer from sparsity of data and non-informative utterances (Li et al., 2015). This paper proposed increasing the source data for character building; the data can be useful for neural models. Acknowledgments We thank the developers of DWANGO Co., Ltd. for creating the role play-based questionanswering Web sites. We also thank the subsc"
Y15-1008,P13-2013,0,0.0150922,"Miyazaki, Ryo Masumura, Ryuichiro Higashinaka, Toshiro Makino and Yoshihiro Matsuo NTT Media Intelligence Laboratories NTT Corporation {otsuka.atsushi, hirano.tohru, miyazaki.chiaki, masumura.ryo, higashinaka.ryuichiro, makino.toshiro, matsuo.yoshihiro}@lab.ntt.co.jp Abstract sentences. Previous studies have proposed many methods for implicit discourse recognition, among them reasoning-based (Sugiura et al., 2013) and pattern-based (Saito et al., 2006) methods. Many of these earlier studies (Marcu and Echihabi, 2002; Lin et al., 2009; Pitler et al., 2009; Wang et al., 2012; Lan et al., 2013; Biran and McKeown, 2013; Rutherford and Xue, 2014) focused on using word pairs or their derivative features. For example, take the two following sentences: We propose a method for implicit discourse relation recognition using a recursive neural network (RNN). Many previous studies have used the word-pair feature to compare the meaning of two sentences for implicit discourse relation recognition. Our proposed method differs in that we use various-sized sentence expression units and compare the meaning of the expressions between two sentences by converting the expressions into vectors using the RNN. Experiments showed"
Y15-1008,D14-1168,0,0.0313245,"e RNN. Experiments showed that our method significantly improves the accuracy of identifying implicit discourse relations compared with the word-pair method. 1 A1 : I like summer. B1 : I prefer winter. Introduction Discourse relation recognition is a technique to identify the type of discourse relation between two sentences. Because discourse relation contributes to the coherence of sentences, it has potential applications in many natural language processing (NLP) tasks. For example, in text summarization, it makes summary documents more consistent by using discourse relations and structures (Gerani et al., 2014). Similarly, in conversational systems (Higashinaka et al., 2014), discourse relations can help the system select contextually appropriate system utterances. Discourse relations are categorized into explicit and implicit relations. Explicit relations have a discourse marker such as a connective, making them easy to identify with a high degree of accuracy (Pitler and Nenkova, 2009). Implicit discourse relations, in contrast, have no discourse marker between In this case, we can easily identify the relation as “comparison” by focusing on the word pair “summer - winter”. However, there is emergin"
Y15-1008,P03-1004,0,0.0300866,"process of creating a sentence vector. Our approach compares the meaning of two sentences by using these interim vectors. In this subsection, we introduce a method for extracting vectors of various expression units by the RNN for Japanese sentences. Figure 2 shows the RNN structure based on Japanese dependency structure. Japanese sentences have dependency structures made up of bunsetsu segments (bunsetsu is a Japanese expression unit comprising one or more content words with zero or more function words). We obtain the syntactic structures of sentences by Japanese dependency parsing. Refer to (Kudo and Matsumoto, 2003) for how Japanese dependency parsing works in general. 65 We create segment vectors by combining word vectors. The sentence vector is the root vector of the RNN created at the end of the combining process. In this paper, we construct an RNN tree structure on top of the Japanese dependency structure. In Japanese, dependency relationships are generally directed from left to right, so we constantly combine segment vectors from the right-most segment to obtain the segment vector, as in the example shown in Fig. 2. Because Japanese dependency structures are not a binary tree, there are some vectors"
Y15-1008,P13-1047,0,0.0123771,"oru Hirano, Chiaki Miyazaki, Ryo Masumura, Ryuichiro Higashinaka, Toshiro Makino and Yoshihiro Matsuo NTT Media Intelligence Laboratories NTT Corporation {otsuka.atsushi, hirano.tohru, miyazaki.chiaki, masumura.ryo, higashinaka.ryuichiro, makino.toshiro, matsuo.yoshihiro}@lab.ntt.co.jp Abstract sentences. Previous studies have proposed many methods for implicit discourse recognition, among them reasoning-based (Sugiura et al., 2013) and pattern-based (Saito et al., 2006) methods. Many of these earlier studies (Marcu and Echihabi, 2002; Lin et al., 2009; Pitler et al., 2009; Wang et al., 2012; Lan et al., 2013; Biran and McKeown, 2013; Rutherford and Xue, 2014) focused on using word pairs or their derivative features. For example, take the two following sentences: We propose a method for implicit discourse relation recognition using a recursive neural network (RNN). Many previous studies have used the word-pair feature to compare the meaning of two sentences for implicit discourse relation recognition. Our proposed method differs in that we use various-sized sentence expression units and compare the meaning of the expressions between two sentences by converting the expressions into vectors using th"
Y15-1008,D14-1220,0,0.0251265,"more specific and distinguishing of discourse relations. This paper proposes a novel method for implicit discourse relation recognition that compares various expression units between two sentences. The smallest units of a sentence expression are words, and the largest are the entire sentence. To consider various expression units, we turn to a recursive neural network (RNN) based approach. The RNN is the neural network based method to create vectors of various expression units on the basis of the syntactic structure of a sentence and has been applied to various NLP tasks (Socher et al., 2011; Li et al., 2014; Liu et al., 2014). Here, we employ the RNN based approach for implicit discourse recognition and show that our proposed method significantly outperforms the word pair based approach. 64 In this paper, we demonstrate through experiments using Japanese conversational data that our method can improve the estimation performance of implicit discourse relation recognition more than the conventional word pair method. In the following sections, we first describe our proposed method using the RNN with Japanese sentences in Section 2. Section 3 explains the experiments we performed on implicit discour"
Y15-1008,D09-1036,0,0.0601736,"Missing"
Y15-1008,P14-1140,0,0.019608,"d distinguishing of discourse relations. This paper proposes a novel method for implicit discourse relation recognition that compares various expression units between two sentences. The smallest units of a sentence expression are words, and the largest are the entire sentence. To consider various expression units, we turn to a recursive neural network (RNN) based approach. The RNN is the neural network based method to create vectors of various expression units on the basis of the syntactic structure of a sentence and has been applied to various NLP tasks (Socher et al., 2011; Li et al., 2014; Liu et al., 2014). Here, we employ the RNN based approach for implicit discourse recognition and show that our proposed method significantly outperforms the word pair based approach. 64 In this paper, we demonstrate through experiments using Japanese conversational data that our method can improve the estimation performance of implicit discourse relation recognition more than the conventional word pair method. In the following sections, we first describe our proposed method using the RNN with Japanese sentences in Section 2. Section 3 explains the experiments we performed on implicit discourse recognition in J"
Y15-1008,P02-1047,0,0.0957041,"Various Units of Sentence Expression with Recursive Neural Network Atsushi Otsuka, Toru Hirano, Chiaki Miyazaki, Ryo Masumura, Ryuichiro Higashinaka, Toshiro Makino and Yoshihiro Matsuo NTT Media Intelligence Laboratories NTT Corporation {otsuka.atsushi, hirano.tohru, miyazaki.chiaki, masumura.ryo, higashinaka.ryuichiro, makino.toshiro, matsuo.yoshihiro}@lab.ntt.co.jp Abstract sentences. Previous studies have proposed many methods for implicit discourse recognition, among them reasoning-based (Sugiura et al., 2013) and pattern-based (Saito et al., 2006) methods. Many of these earlier studies (Marcu and Echihabi, 2002; Lin et al., 2009; Pitler et al., 2009; Wang et al., 2012; Lan et al., 2013; Biran and McKeown, 2013; Rutherford and Xue, 2014) focused on using word pairs or their derivative features. For example, take the two following sentences: We propose a method for implicit discourse relation recognition using a recursive neural network (RNN). Many previous studies have used the word-pair feature to compare the meaning of two sentences for implicit discourse relation recognition. Our proposed method differs in that we use various-sized sentence expression units and compare the meaning of the expressio"
Y15-1008,N13-1090,0,0.201584,"ction, we explain how the RNN works, describe how the vectors are created by the RNN, and show how to create the feature for the classifier from vectors. 2.1 Recursive neural network The RNN is a kind of deep neural network created by applying the same set of weights recursively over a structure. The RNN has a binary tree structure, and its framework computes the representation for each parent iteratively in a bottom-up fashion on the basis of its children. We assume that word vectors c1 , c2 , and c3 have N dimensions. Each word is given vectors in advance by word embeddings (e.g., word2vec (Mikolov et al., 2013a)). Segment vectors are created by combining word vectors from left to right in each segment. The c1 and c2 ’s parent representation vector p1 is computed as p1 = f (We [c1 ; c2 ] + be ) (1) where [c1 ; c2 ] is the 2N -dimension concatenation vector of c1 and c2 , We is the N × 2N encoding matrix, be is the N -dimension encode bias vector, and f denotes an element-wise activation function (we use tanh). The next parent representation vector p2 , which has children p1 and c3 , is computed in the same way by an input concatenation vector [p1 ; c3 ] and encoding parameters We and be . 2.2 Creati"
Y15-1008,W12-1614,0,0.0146667,"m select contextually appropriate system utterances. Discourse relations are categorized into explicit and implicit relations. Explicit relations have a discourse marker such as a connective, making them easy to identify with a high degree of accuracy (Pitler and Nenkova, 2009). Implicit discourse relations, in contrast, have no discourse marker between In this case, we can easily identify the relation as “comparison” by focusing on the word pair “summer - winter”. However, there is emerging evidence that word pairs may no longer have a role to play in implicit discourse relation recognition (Park and Cardie, 2012). This is because identification is not always possible by using just word pairs. When we consider the following sentences, A2 : I got soaked by the sudden rain yesterday. B21 : Did you forget your umbrella at the office? B22 : The rain was so heavy that my umbrella was useless. discourse A2 − B21 and A2 − B22 have different relations. discourse A2 − B21 is causal relation: B21 explains the reason for A1, and A2 − B22 is expansion relation: B22 is a supplemental explanation about the “sudden rain” in A2. Nevertheless, the same word pair “soaked - umbrella” can be ex63 29th Pacific Asia Confere"
Y15-1008,P09-2004,0,0.149678,"ence of sentences, it has potential applications in many natural language processing (NLP) tasks. For example, in text summarization, it makes summary documents more consistent by using discourse relations and structures (Gerani et al., 2014). Similarly, in conversational systems (Higashinaka et al., 2014), discourse relations can help the system select contextually appropriate system utterances. Discourse relations are categorized into explicit and implicit relations. Explicit relations have a discourse marker such as a connective, making them easy to identify with a high degree of accuracy (Pitler and Nenkova, 2009). Implicit discourse relations, in contrast, have no discourse marker between In this case, we can easily identify the relation as “comparison” by focusing on the word pair “summer - winter”. However, there is emerging evidence that word pairs may no longer have a role to play in implicit discourse relation recognition (Park and Cardie, 2012). This is because identification is not always possible by using just word pairs. When we consider the following sentences, A2 : I got soaked by the sudden rain yesterday. B21 : Did you forget your umbrella at the office? B22 : The rain was so heavy that m"
Y15-1008,P09-1077,0,0.0326582,"cursive Neural Network Atsushi Otsuka, Toru Hirano, Chiaki Miyazaki, Ryo Masumura, Ryuichiro Higashinaka, Toshiro Makino and Yoshihiro Matsuo NTT Media Intelligence Laboratories NTT Corporation {otsuka.atsushi, hirano.tohru, miyazaki.chiaki, masumura.ryo, higashinaka.ryuichiro, makino.toshiro, matsuo.yoshihiro}@lab.ntt.co.jp Abstract sentences. Previous studies have proposed many methods for implicit discourse recognition, among them reasoning-based (Sugiura et al., 2013) and pattern-based (Saito et al., 2006) methods. Many of these earlier studies (Marcu and Echihabi, 2002; Lin et al., 2009; Pitler et al., 2009; Wang et al., 2012; Lan et al., 2013; Biran and McKeown, 2013; Rutherford and Xue, 2014) focused on using word pairs or their derivative features. For example, take the two following sentences: We propose a method for implicit discourse relation recognition using a recursive neural network (RNN). Many previous studies have used the word-pair feature to compare the meaning of two sentences for implicit discourse relation recognition. Our proposed method differs in that we use various-sized sentence expression units and compare the meaning of the expressions between two sentences by converting"
Y15-1008,E14-1068,0,0.0125504,"Ryuichiro Higashinaka, Toshiro Makino and Yoshihiro Matsuo NTT Media Intelligence Laboratories NTT Corporation {otsuka.atsushi, hirano.tohru, miyazaki.chiaki, masumura.ryo, higashinaka.ryuichiro, makino.toshiro, matsuo.yoshihiro}@lab.ntt.co.jp Abstract sentences. Previous studies have proposed many methods for implicit discourse recognition, among them reasoning-based (Sugiura et al., 2013) and pattern-based (Saito et al., 2006) methods. Many of these earlier studies (Marcu and Echihabi, 2002; Lin et al., 2009; Pitler et al., 2009; Wang et al., 2012; Lan et al., 2013; Biran and McKeown, 2013; Rutherford and Xue, 2014) focused on using word pairs or their derivative features. For example, take the two following sentences: We propose a method for implicit discourse relation recognition using a recursive neural network (RNN). Many previous studies have used the word-pair feature to compare the meaning of two sentences for implicit discourse relation recognition. Our proposed method differs in that we use various-sized sentence expression units and compare the meaning of the expressions between two sentences by converting the expressions into vectors using the RNN. Experiments showed that our method significan"
Y15-1008,N06-2034,0,0.0755742,"Missing"
Y15-1008,C12-1168,0,0.0141889,"k Atsushi Otsuka, Toru Hirano, Chiaki Miyazaki, Ryo Masumura, Ryuichiro Higashinaka, Toshiro Makino and Yoshihiro Matsuo NTT Media Intelligence Laboratories NTT Corporation {otsuka.atsushi, hirano.tohru, miyazaki.chiaki, masumura.ryo, higashinaka.ryuichiro, makino.toshiro, matsuo.yoshihiro}@lab.ntt.co.jp Abstract sentences. Previous studies have proposed many methods for implicit discourse recognition, among them reasoning-based (Sugiura et al., 2013) and pattern-based (Saito et al., 2006) methods. Many of these earlier studies (Marcu and Echihabi, 2002; Lin et al., 2009; Pitler et al., 2009; Wang et al., 2012; Lan et al., 2013; Biran and McKeown, 2013; Rutherford and Xue, 2014) focused on using word pairs or their derivative features. For example, take the two following sentences: We propose a method for implicit discourse relation recognition using a recursive neural network (RNN). Many previous studies have used the word-pair feature to compare the meaning of two sentences for implicit discourse relation recognition. Our proposed method differs in that we use various-sized sentence expression units and compare the meaning of the expressions between two sentences by converting the expressions int"
Y15-1008,tonelli-etal-2010-annotation,0,\N,Missing
Y15-1008,prasad-etal-2008-penn,0,\N,Missing
Y15-1008,C14-1088,1,\N,Missing
Y15-1035,P98-1068,0,0.126111,"ons to characterize dialogue agent utterances. Figure 1 shows the process of the sentenceend expression conversion. First, as preparation, sentence-end expressions, which are characteristic of target characters, are collected through processes (1) and (2) shown in Figure 1 (details are given in Section 4.1). Second, each input utterance is processed in process (1) to find a sentence-end expression to be converted. Here, sequences of function words at the end of sentences are detected as sentence-end expressions according to the part-ofspeech (POS) tags obtained using a morphological analyzer (Fuchi and Takagi, 1998). Third, through process (3), appropriate candidates to be substituted for the original sentence-end expression are selected using two filters: POS adjacency and semantic label. Finally, a converted utterance whose sentence-end expression is substituted with one of the candidates is generated as an output. Sentence-end expressions in Japanese We focus on sentence-end expression since, in Japanese, it is a core element of role language (Kinsui, 2003), which relates to stereotypical or characteristic wordings of particular personal attributes such as feminine language and youth language. We assu"
Y15-1035,J86-3001,0,0.627916,"ed by polarity (positive vs. negative) and degree of certainty of what is asserted (e.g., possible vs. certain). Tense (aspect) is also often discussed in relation to the meaning of an event (Izumi et al., 2010). Taking these into account, we select five semantic labels, negation for polarity, question and guess for degree of certainty, and completion and continuation for tense (aspect) to keep the factuality consistent before and after conversion. (ii) Semantic labels related to intention Intentions are defined here as what a speaker wants (Sidner and Israel, 1981) or as a discourse purpose (Grosz and Sidner, 1986). To keep the intention consistent before and after conversion, we select four labels, namely, desire, volition, invitation, and request. These labels are important for expressing what a speaker wants (to do) or wants his/her hearer to do. The input utterances and postings in the Twitter corpus, from which the candidates are extracted, are automatically tagged with the semantic labels by using a method that selects the best sequence of semantic labels by a discriminative model (Imamura et al., 2011). 4.4 Conversion of sentence-end expressions A sentence-end expression of the input utterance is"
Y15-1035,W10-3709,0,0.028353,"intention, since we regard them as the key components of dialogue content. (i) Semantic labels related to factuality Event factuality refers to the distinction whether PACLIC 29 event-denoting expressions are presented as corresponding to real-world situations, situations that have not occurred, or situations of uncertain status (Saur´ı and Pustejovsky, 2007). According to them, event factuality is impacted by polarity (positive vs. negative) and degree of certainty of what is asserted (e.g., possible vs. certain). Tense (aspect) is also often discussed in relation to the meaning of an event (Izumi et al., 2010). Taking these into account, we select five semantic labels, negation for polarity, question and guess for degree of certainty, and completion and continuation for tense (aspect) to keep the factuality consistent before and after conversion. (ii) Semantic labels related to intention Intentions are defined here as what a speaker wants (Sidner and Israel, 1981) or as a discourse purpose (Grosz and Sidner, 1986). To keep the intention consistent before and after conversion, we select four labels, namely, desire, volition, invitation, and request. These labels are important for expressing what a s"
Y15-1035,P07-1063,0,0.535367,"e acceptable for designated personal attributes. 1 Introduction Dialogue agents, which can carry out various tasks according to user demand, have been gaining in popularity due to their convenience and potential in casual conversations with humans. To make the agents more attractive as conversation partners, characterization is important since it makes the agents more friendly and human-like. Characterization here means adding particular personal characteristics to agent utterances; for example, adding the characteristics of a particular person (Mizukami et al., 2015), Big Five personalities (Mairesse and Walker, 2007), or personal attributes such as gender, age and area of residence (which is closely related to dialects). To characterize agents, utterances suitable for the designated characteristics are usually manually prepared. However, it is expensive to do this for a large number of utterances. To reduce this cost, we propose a method for automatically converting utterances into those that are suitable for various characters. In particular, the method automatically modifies ‘how to say it’ (i.e., linguistic expressions) without changing ‘what to say’ (i.e., contents of the utterances). Conversion is do"
Y15-1035,C98-1065,0,\N,Missing
