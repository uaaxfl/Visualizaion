L18-1456,Extending Search System based on Interactive Visualization for Speech Corpora,2018,0,0,5,0,30016,tomoko ohsuga,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L16-1350,{ASPEC}: {A}sian Scientific Paper Excerpt Corpus,2016,0,24,3,0,283,toshiaki nakazawa,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"In this paper, we describe the details of the ASPEC (Asian Scientific Paper Excerpt Corpus), which is the first large-size parallel corpus of scientific paper domain. ASPEC was constructed in the Japanese-Chinese machine translation project conducted between 2006 and 2010 using the Special Coordination Funds for Promoting Science and Technology. It consists of a Japanese-English scientific paper abstract corpus of approximately 3 million parallel sentences (ASPEC-JE) and a Chinese-Japanese scientific paper excerpt corpus of approximately 0.68 million parallel sentences (ASPEC-JC). ASPEC is used as the official dataset for the machine translation evaluation workshop WAT (Workshop on Asian Translation)."
wang-etal-2010-adapting,Adapting {C}hinese Word Segmentation for Machine Translation Based on Short Units,2010,9,6,2,0,29973,yiou wang,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"In Chinese texts, words composed of single or multiple characters are not separated by spaces, unlike most western languages. Therefore Chinese word segmentation is considered an important first step in machine translation (MT) and its performance impacts MT results. Many factors affect Chinese word segmentations, including the segmentation standards and segmentation strategies. The performance of a corpus-based word segmentation model depends heavily on the quality and the segmentation standard of the training corpora. However, we observed that existing manually annotated Chinese corpora tend to have low segmentation granularity and provide poor morphological information due to the present segmentation standards. In this paper, we introduce a short-unit standard of Chinese word segmentation, which is particularly suitable for machine translation, and propose a semi-automatic method of transforming the existing corpora into the ones that can satisfy our standards. We evaluate the usefulness of our approach on the basis of translation tasks from the technology newswire domain and the scientific paper domain, and demonstrate that it significantly improves the performance of Chinese-Japanese machine translation (over 1.0 BLEU increase)."
kozawa-etal-2010-collection,Collection of Usage Information for Language Resources from Academic Articles,2010,6,0,3,1,30018,shunsuke kozawa,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Recently, language resources (LRs) are becoming indispensable for linguistic researches. However, existing LRs are often not fully utilized because their variety of usage is not well known, indicating that their intrinsic value is not recognized very well either. Regarding this issue, lists of usage information might improve LR searches and lead to their efficient use. In this research, therefore, we collect a list of usage information for each LR from academic articles to promote the efficient utilization of LRs. This paper proposes to construct a text corpus annotated with usage information (UI corpus). In particular, we automatically extract sentences containing LR names from academic articles. Then, the extracted sentences are annotated with usage information by two annotators in a cascaded manner. We show that the UI corpus contributes to efficient LR searches by combining the UI corpus with a metadata database of LRs and comparing the number of LRs retrieved with and without the UI corpus."
W09-3506,Machine Transliteration using Target-Language Grapheme and Phoneme: Multi-engine Transliteration Approach,2009,10,12,2,0,12929,jonghoon oh,Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration ({NEWS} 2009),0,"This paper describes our approach to NEWS 2009 Machine Transliteration Shared Task. We built multiple transliteration engines based on different combinations of two transliteration models and three machine learning algorithms. Then, the outputs from these transliteration engines were combined using re-ranking functions. Our method was applied to all language pairs in NEWS 2009 Machine Transliteration Shared Task. The official results of our standard runs were ranked the best for four language pairs and the second best for three language pairs."
W09-3401,Enhancing the {J}apanese {W}ord{N}et,2009,20,72,4,0,6126,francis bond,Proceedings of the 7th Workshop on {A}sian Language Resources ({ALR}7),0,"The Japanese WordNet currently has 51,000 synsets with Japanese entries. In this paper, we discuss three methods of extending it: increasing the cover, linking it to examples in corpora and linking it to other resources (SUMO and GoiTaikei). In addition, we outline our plans to make it more useful by adding Japanese definition sentences to each synset. Finally, we discuss how releasing the corpus under an open license has led to the construction of interfaces in a variety of programming languages."
W09-1209,Multilingual Dependency Learning: Exploiting Rich Features for Tagging Syntactic and Semantic Dependencies,2009,14,33,4,0,305,hai zhao,Proceedings of the Thirteenth Conference on Computational Natural Language Learning ({C}o{NLL} 2009): Shared Task,0,"This paper describes our system about multilingual syntactic and semantic dependency parsing for our participation in the joint task of CoNLL-2009 shared tasks. Our system uses rich features and incorporates various integration technologies. The system is evaluated on in-domain and out-of-domain evaluation data of closed challenge of joint task. For in-domain evaluation, our system ranks the second for the average macro labeled F1 of all seven languages, 82.52% (only about 0.1% worse than the best system), and the first for English with macro labeled F1 87.69%. And for out-of-domain evaluation, our system also achieves the second for average score of all three languages."
P09-1049,Bilingual Co-Training for Monolingual Hyponymy-Relation Acquisition,2009,18,16,2,0,12929,jonghoon oh,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"This paper proposes a novel framework called bilingual co-training for a large-scale, accurate acquisition method for monolingual semantic knowledge. In this framework, we combine the independent processes of monolingual semantic-knowledge acquisition for two languages using bilingual resources to boost performance. We apply this framework to large-scale hyponymy-relation acquisition from Wikipedia. Experimental results show that our approach improved the F-measure by 3.6--10.3%. We also show that bilingual co-training enables us to build classifiers for two languages in tandem with the same combined amount of data as required for training a single classifier in isolation while achieving superior performance."
P09-1058,An Error-Driven Word-Character Hybrid Model for Joint {C}hinese Word Segmentation and {POS} Tagging,2009,24,92,2,0,8038,canasai kruengkrai,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"In this paper, we present a discriminative word-character hybrid model for joint Chinese word segmentation and POS tagging. Our word-character hybrid model offers high performance since it can handle both known and unknown words. We describe our strategies that yield good balance for learning the characteristics of known and unknown words and propose an error-driven policy that delivers such balance by acquiring examples of unknown words from particular errors in a training corpus. We describe an efficient framework for training our model based on the Margin Infused Relaxed Algorithm (MIRA), evaluate our approach on the Penn Chinese Treebank, and show that it achieves superior performance compared to the state-of-the-art approaches reported in the literature."
D09-1060,Improving Dependency Parsing with Subtrees from Auto-Parsed Data,2009,30,63,3,0.925926,21231,wenliang chen,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"This paper presents a simple and effective approach to improve dependency parsing by using subtrees from auto-parsed data. First, we use a baseline parser to parse large-scale unannotated data. Then we extract subtrees from dependency parse trees in the auto-parsed data. Finally, we construct new subtree-based features for parsing algorithms. To demonstrate the effectiveness of our proposed approach, we present the experimental results on the English Penn Treebank and the Chinese Penn Treebank. These results show that our approach significantly outperforms baseline systems. And, it achieves the best accuracy for the Chinese data and an accuracy which is competitive with the best known systems for the English data."
D09-1069,Can {C}hinese Phonemes Improve Machine Transliteration?: A Comparative Study of {E}nglish-to-{C}hinese Transliteration Models,2009,20,3,2,0,12929,jonghoon oh,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"Inspired by the success of English grapheme-to-phoneme research in speech synthesis, many researchers have proposed phoneme-based English-to-Chinese transliteration models. However, such approaches have severely suffered from the errors in Chinese phoneme-to-grapheme conversion. To address this issue, we propose a new English-to-Chinese transliteration model and make systematic comparisons with the conventional models. Our proposed model relies on the joint use of Chinese phonemes and their corresponding English graphemes and phonemes. Experiments showed that Chinese phonemes in our proposed model can contribute to the performance improvement in English-to-Chinese transliteration."
bond-etal-2008-boot,Boot-Strapping a {W}ord{N}et Using Multiple Existing {W}ord{N}ets,2008,12,28,4,0,6126,francis bond,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,In this paper we describe the construction of an illustrated Japanese Wordnet. We bootstrap the Wordnet using existing multiple existing wordnets in order to deal with the ambiguity inherent in translation. We illustrate it with pictures from the Open Clip Art Library.
kawahara-uchimoto-2008-method,A Method for Automatically Constructing Case Frames for {E}nglish,2008,22,3,2,0.466751,3202,daisuke kawahara,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Case frames are an important knowledge base for a variety of natural language processing (NLP) systems. For the practical use of these systems in the real world, wide-coverage case frames are required. In order to acquire such large-scale case frames, in this paper, we automatically compile case frames from a large corpus. The resultant case frames that are compiled from the English Gigaword corpus contain 9,300 verb entries. The case frames include most examples of normal usage, and are ready to be used in numerous NLP analyzers and applications."
uchimoto-den-2008-word,Word-level Dependency-structure Annotation to Corpus of Spontaneous {J}apanese and its Application,2008,7,2,1,1,30019,kiyotaka uchimoto,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"In Japanese, the syntactic structure of a sentence is generally represented by the relationship between phrasal units, bunsetsus in Japanese, based on a dependency grammar. In many cases, the syntactic structure of a bunsetsu is not considered in syntactic structure annotation. This paper gives the criteria and definitions of dependency relationships between words in a bunsetsu and their applications. The target corpus for the word-level dependency annotation is a large spontaneous Japanese-speech corpus, the Corpus of Spontaneous Japanese (CSJ). One application of word-level dependency relationships is to find basic units for constructing accent phrases."
isahara-etal-2008-development,Development of the {J}apanese {W}ord{N}et,2008,6,79,3,0,15925,hitoshi isahara,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"After a long history of compilation of our own lexical resources, EDR Japanese/English Electronic Dictionary, and discussions with major players on development of various WordNets, Japanese National Institute of Information and Communications Technology started developing the Japanese WordNet in 2006 and will publicly release the first version, which includes both the synset in Japanese and the annotated Japanese corpus of SemCor, in June 2008. As the first step in compiling the Japanese WordNet, we added Japanese equivalents to synsets of the Princeton WordNet. Of course, we must also add some synsets which do not exist in the Princeton WordNet, and must modify synsets in the Princeton WordNet, in order to make the hierarchical structure of Princeton synsets represent thesaurus-like information found in the Japanese language, however, we will address these tasks in a future study. We then translated English sentences which are used in the SemCor annotation into Japanese and annotated them using our Japanese WordNet. This article describes the overview of our project to compile Japanese WordNet and other resources which relate to our Japanese WordNet."
kozawa-etal-2008-automatic,Automatic Acquisition of Usage Information for Language Resources,2008,10,5,3,1,30018,shunsuke kozawa,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Recently, language resources (LRs) are becoming indispensable for linguistic research. Unfortunately, it is not easy to find their usages by searching the web even though they must be described in the Internet or academic articles. This indicates that the intrinsic value of LRs is not recognized very well. In this research, therefore, we extract a list of usage information for each LR to promote the efficient utilization of LRs. In this paper, we proposed a method for extracting a list of usage information from academic articles by using rules based on syntactic information. The rules are generated by focusing on the syntactic features that are observed in the sentences describing usage information. As a result of experiments, we achieved 72.9{\%} in recall and 78.4{\%} in precision for the closed test and 60.9{\%} in recall and 72.7{\%} in precision for the open test."
tohyama-etal-2008-construction-metadata,Construction of a Metadata Database for Efficient Development and Use of Language Resources,2008,2,2,3,1,46266,hitomi tohyama,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"The National Institute of Information and Communications Technology (NICT) and Nagoya University have been jointly constructing a large scale database named SHACHI by collecting detailed meta-information on language resources (LRs) in Asia and Western countries, for the purpose of effectively combining LRs. The purpose of this project is to investigate languages, tag sets, and formats compiled in LRs throughout the world, to systematically store LR metadata, to create a search function for this information, and to ultimately utilize all this for a more efficient development of LRs. This metadata database contains more than 2,000 compiled LRs such as corpora, dictionaries, thesauruses and lexicons, forming a large scale metadata of LRs archive. Its metadata, an extended version of OLAC metadata set conforming to Dublin Core, which contain detailed meta-information, have been collected semi-automatically. This paper explains the design and the structure of the metadata database, as well as the realization of the catalogue search tool. Additionally, the website of this database is now open to the public and accessible to all Internet users."
zhang-etal-2008-word,Word Alignment Annotation in a {J}apanese-{C}hinese Parallel Corpus,2008,5,0,3,0.609756,9062,yujie zhang,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Parallel corpora are critical resources for machine translation research and development since parallel corpora contain translation equivalences of various granularities. Manual annotation of word {\&} phrase alignment is of significance to provide gold-standard for developing and evaluating both example-based machine translation model and statistical machine translation model. This paper presents the work of word {\&} phrase alignment annotation in the NICT Japanese-Chinese parallel corpus, which is constructed at the National Institute of Information and Communications Technology (NICT). We describe the specification of word alignment annotation and the tools specially developed for the manual annotation. The manual annotation on 17,000 sentence pairs has been completed. We examined the manually annotated word alignment data and extracted translation knowledge from the word {\&} phrase aligned corpus."
I08-2097,Learning Reliability of Parses for Domain Adaptation of Dependency Parsing,2008,25,32,2,0.466751,3202,daisuke kawahara,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{II},0,"The accuracy of parsing has exceeded 90% recently, but this is not high enough to use parsing results practically in natural language processing (NLP) applications such as paraphrase acquisition and relation extraction. We present a method for detecting reliable parses out of the outputs of a single dependency parser. This technique is also applied to domain adaptation of dependency parsing. Our goal was to improve the performance of a state-of-the-art dependency parser on the data set of the domain adaptation track of the CoNLL 2007 shared task, a formidable challenge."
I08-1012,Dependency Parsing with Short Dependency Relations in Unlabeled Data,2008,18,27,3,0.925926,21231,wenliang chen,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,"This paper presents an effective dependency parsing approach of incorporating short dependency information from unlabeled data. The unlabeled data is automatically parsed by a deterministic dependency parser, which can provide relatively high performance for short dependencies between words. We then train another parser which uses the information on short dependency relations extracted from the output of the first parser. Our proposed approach achieves an unlabeled attachment score of 86.52, an absolute 1.24% improvement over the baseline system on the data set of Chinese Treebank."
C08-2030,Construction of an Infrastructure for Providing Users with Suitable Language Resources,2008,0,1,3,1,46266,hitomi tohyama,Coling 2008: Companion volume: Posters,0,"Our research organization has been constructing a large scale database named SHACHI by collecting detailed meta information on language resources (LRs) in Asia and Western countries. The metadata database contains more than 2,000 compiled LRs such as corpora, dictionaries, thesauruses and lexicons, forming a large scale metadata of LRs archive. Its metadata, an extended version of OLAC metadata set conforming to Dublin Core, have been collected semi-automatically. This paper explains the design and the structure of the metadata database, as well as the realization of the catalogue search tool."
2008.amta-govandcom.3,Sharing User Dictionaries Across Multiple Systems with {UTX}-{S},2008,-1,-1,5,0,6126,francis bond,Proceedings of the 8th Conference of the Association for Machine Translation in the Americas: Government and Commercial Uses of MT,0,"Careful tuning of user-created dictionaries is indispensable when using a machine translation system for computer aided translation. However, there is no widely used standard for user dictionaries in the Japanese/English machine translation market. To address this issue, AAMT (the Asia-Pacific Association for Machine Translation) has established a specification of sharable dictionaries (UTX-S: Universal Terminology eXchange -- Simple), which can be used across different machine translation systems, thus increasing the interoperability of language resources. UTX-S is simpler than existing specifications such as UPF and OLIF. It was explicitly designed to make it easy to (a) add new user dictionaries and (b) share existing user dictionaries. This facilitates rapid user dictionary production and avoids vendor tie in. In this study we describe the UTX-Simple (UTX-S) format, and show that it can be converted to the user dictionary formats for five commercial English-Japanese MT systems. We then present a case study where we (a) convert an on-line glossary to UTX-S, and (b) produce user dictionaries for five different systems, and then exchange them. The results show that the simplified format of UTX-S can be used to rapidly build dictionaries. Further, we confirm that customized user dictionaries are effective across systems, although with a slight loss in quality: on average, user dictionaries improved the translations for 44.8{\%} of translations with the systems they were built for and 37.3{\%} of translations for different systems. In ongoing work, AAMT is using UTX-S as the format in building up a user community for producing, sharing, and accumulating user dictionaries in a sustainable way."
P07-2052,Minimally Lexicalized Dependency Parsing,2007,17,3,2,0.466751,3202,daisuke kawahara,Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,0,"Dependency structures do not have the information of phrase categories in phrase structure grammar. Thus, dependency parsing relies heavily on the lexical information of words. This paper discusses our investigation into the effectiveness of lexicalization in dependency parsing. Specifically, by restricting the degree of lexicalization in the training phase of a parser, we examine the change in the accuracy of dependency relations. Experimental results indicate that minimal or low lexicalization is sufficient for parsing accuracy."
P07-2055,A Hybrid Approach to Word Segmentation and {POS} Tagging,2007,4,45,2,0,27752,tetsuji nakagawa,Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,0,"In this paper, we present a hybrid method for word segmentation and POS tagging. The target languages are those in which word boundaries are ambiguous, such as Chinese and Japanese. In the method, word-based and character-based processing is combined, and word segmentation and POS tagging are conducted simultaneously. Experimental results on multiple corpora show that the integrated method has high accuracy."
N07-1005,Automatic Evaluation of Machine Translation Based on Rate of Accomplishment of Sub-Goals,2007,14,2,1,1,30019,kiyotaka uchimoto,Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,0,"The quality of a sentence translated by a machine translation (MT) system is difficult to evaluate. We propose a method for automatically evaluating the quality of each translation. In general, when translating a given sentence, one or more conditions should be satisfied to maintain a high translation quality. In EnglishJapanese translation, for example, prepositions and infinitives must be appropriately translated. We show several procedures that enable evaluating the quality of a translated sentence more appropriately than using conventional methods. The first procedure is constructing a test set where the conditions are assigned to each test-set sentence in the form of yes/no questions. The second procedure is developing a system that determines an answer to each question. The third procedure is combining a measure based on the questions and conventional measures. We also present a method for automatically generating sub-goals in the form of yes/no questions and estimating the rate of accomplishment of the sub-goals. Promising results are shown."
2007.mtsummit-papers.35,Development of a {J}apanese-{C}hinese machine translation system,2007,-1,-1,4,0,15925,hitoshi isahara,Proceedings of Machine Translation Summit XI: Papers,0,None
W06-2404,Chunking {J}apanese Compound Functional Expressions by Machine Learning,2006,8,12,5,0,18299,masatoshi tsuchiya,Proceedings of the Workshop on Multi-word-expressions in a multilingual context,0,"The Japanese language has various types of compound functional expressions, which are very important for recognizing the syntactic structures of Japanese sentences and for understanding their semantic contents. In this paper, we formalize the task of identifying Japanese compound functional expressions in a text as a chunking problem. We apply a machine learning technique to this task, where we employ that of Support Vector Machines (SVMs). We show that the proposed method significantly outperforms existing Japanese text processing tools."
P06-2042,Detection of Quotations and Inserted Clauses and Its Application to Dependency Structure Analysis in Spontaneous {J}apanese,2006,4,0,2,0,49924,ryoji hamabe,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"Japanese dependency structure is usually represented by relationships between phrasal units called bunsetsus. One of the biggest problems with dependency structure analysis in spontaneous speech is that clause boundaries are ambiguous. This paper describes a method for detecting the boundaries of quotations and inserted clauses and that for improving the dependency accuracy by applying the detected boundaries to dependency structure analysis. The quotations and inserted clauses are determined by using an SVM-based text chunking method that considers information on morphemes, pauses, fillers, etc. The information on automatically analyzed dependency structure is also used to detect the beginning of the clauses. Our evaluation experiment using Corpus of Spontaneous Japanese (CSJ) showed that the automatically estimated boundaries of quotations and inserted clauses helped to improve the accuracy of dependency structure analysis."
uchimoto-etal-2006-dependency,Dependency-structure Annotation to Corpus of Spontaneous {J}apanese,2006,10,3,1,1,30019,kiyotaka uchimoto,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"In Japanese, syntactic structure of a sentence is generally represented by the relationship between phrasal units, or bunsetsus inJapanese, based on a dependency grammar. In the same way, thesyntactic structure of a sentence in a large, spontaneous, Japanese-speech corpus, the Corpus of Spontaneous Japanese (CSJ), isrepresented by dependency relationships between bunsetsus. This paper describes the criteria and definitions of dependency relationships between bunsetsus in the CSJ. The dependency structure of the CSJ is investigated, and the difference in the dependency structures ofwritten text and spontaneous speech is discussed in terms of thedependency accuracies obtained by using a corpus-based model. It is shown that the accuracy of automatic dependency-structure analysis canbe improved if characteristic phenomena of spontaneous speech such as self-corrections, basic utterance units in spontaneous speech, and bunsetsus that have no modifiee are detected and used for dependency-structure analysis."
uchimoto-etal-2006-automatic,Automatic Detection and Semi-Automatic Revision of Non-Machine-Translatable Parts of a Sentence,2006,3,3,1,1,30019,kiyotaka uchimoto,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"We developed a method for automatically distinguishing the machine-translatable and non-machine-translatable parts of a given sentence for a particular machine translation (MT) system. They can be distinguished by calculating the similarity between a source-language sentence and its back translation for each part of the sentence. The parts with low similarities are highly likely to be non-machine-translatable parts. We showed that the parts of a sentence that are automatically distinguished as non-machine-translatable provide useful information for paraphrasing or revising the sentence in the source language to improve the quality of the translation by the MT system. We also developed a method of providing knowledge useful to effectively paraphrasing or revising the detected non-machine-translatable parts. Two types of knowledge were extracted from the EDR dictionary: one for transforming a lexical entry into an expression used in the definition and the other for conducting the reverse paraphrasing, which transforms an expression found in a definition into the lexical entry. We found that the information provided by the methods helped improve the machine translatability of the originally input sentences."
Y05-1014,"Analysis of Machine Translation Systems{'} Errors in Tense, Aspect, and Modality",2005,10,8,2,0.512494,16788,masaki murata,"Proceedings of the 19th Pacific Asia Conference on Language, Information and Computation",0,"Errors of the translation of tense, aspect, and modality by machine translation systems were analyzed for six translation systems on the market and our new systems for translating tense, aspect, and modality. The results showed that our systems outperformed the other systems. They also showed that the other systems often produced progressive forms rather than the correct present forms. Our systems rarely made this mistake. Translation systems on the market could thus be improved by incorporating the methods used in our systems. Moreover, error analysis of the translation systems on the market identified information that would be useful for improving them."
I05-6009,Error Annotation for Corpus of {J}apanese Learner {E}nglish,2005,9,28,2,1,50987,emi izumi,Proceedings of the Sixth International Workshop on Linguistically Interpreted Corpora ({LINC}-2005),0,"In this paper, we discuss how error annotation for learner corpora should be done by explaining the state of the art of error tagging schemes in learner corpus research. Several learner corpora, including the NICT JLE (Japanese Learner English) Corpus that we have compiled are annotated with error tagsets designed by categorizing xe2x80x9clikelyxe2x80x9d errors implied from the existing canonical grammar rules or POS (part-of-speech) system in advance. Such error tagging can help to successfully assess to what extent learners can command the basic language system, especially grammar, but is insufficient for describing learnersxe2x80x99 communicative competence. To overcome this limitation, we reexamined learner language in the NICT JLE Corpus by focusing on xe2x80x9cintelligibilityxe2x80x9d and xe2x80x9cnaturalnessxe2x80x9d, and determined how the current error tagset should be revised."
I05-2015,Building an Annotated {J}apanese-{C}hinese Parallel Corpus - A Part of {NICT} Multilingual Corpora,2005,9,9,2,0.609756,9062,yujie zhang,Companion Volume to the Proceedings of Conference including Posters/Demos and tutorial abstracts,0,"We are constricting a Japanese-Chinese parallel corpus, which is a part of the NICT Multilingual Corpora. The corpus is general domain, of large scale of about 40,000 sentence pairs, long sentences, annotated with detailed information and high quality. To the best of our knowledge, this will be the first annotated JapaneseChinese parallel corpus in the world. We created the corpus by selecting Japanese sentences from Mainichi Newspaper and then manually translating them into Chinese. We then annotated the corpus with morphological and syntactic structures and alignments at word and phrase levels. This paper describes the specification in human translation and the scheme of detailed information annotation, and the tools we developed in the corpus construction. The experience we obtained and points we paid special attentions are also introduced for share with other researches in corpora construction."
2005.mtsummit-papers.10,Building an Annotated {J}apanese-{C}hinese Parallel Corpus {--} A Part of {NICT} Multilingual Corpora,2005,9,9,2,0.609756,9062,yujie zhang,Proceedings of Machine Translation Summit X: Papers,0,"We are constricting a Japanese-Chinese parallel corpus, which is a part of the NICT Multilingual Corpora. The corpus is general domain, of large scale of about 40,000 sentence pairs, long sentences, annotated with detailed information and high quality. To the best of our knowledge, this will be the first annotated Japanese-Chinese parallel corpus in the world. We created the corpus by selecting Japanese sentences from Mainichi Newspaper and then manually translating them into Chinese. We then annotated the corpus with morphological and syntactic structures and alignments at word and phrase levels. This paper describes the specification in human translation and detailed information annotation, and the tools we developed in the project. The experience we obtained and points we paid special attentions are also introduced for share with other researches in corpora construction."
2005.mtsummit-papers.31,Automatic Rating of Machine Translatability,2005,4,17,1,1,30019,kiyotaka uchimoto,Proceedings of Machine Translation Summit X: Papers,0,"We describe a method for automatically rating the machine translatability of a sentence for various machine translation (MT) systems. The method requires that the MT system can bidirectionally translate sentences in both source and target languages. However, it does not require reference translations, as is usual for automatic MT evaluation. By applying this method to every component of a sentence in a given source language, we can automatically identify the machine-translatable and non-machinetranslatable parts of a sentence for a particular MT system. We show that the parts of a sentence that are automatically identified as nonmachine-translatable provide useful information for paraphrasing or revising the sentence in the source language, thus improving the quality of the final translation."
W04-2208,Multilingual Aligned Parallel Treebank Corpus Reflecting Contextual Information and Its Applications,2004,13,18,1,1,30019,kiyotaka uchimoto,Proceedings of the Workshop on Multilingual Linguistic Resources,0,"This paper describes Japanese-English-Chinese aligned parallel treebank corpora of newspaper articles. They have been constructed by translating each sentence in the Penn Treebank and the Kyoto University text corpus into a corresponding natural sentence in a target language. Each sentence is translated so as to reflect its contextual information and is annotated with morphological and syntactic structures and phrasal alignment. This paper also describes the possible applications of the parallel corpus and proposes a new framework to aid in translation. In this framework, parallel translations whose source language sentence is similar to a given sentence can be semi-automatically generated. In this paper we show that the framework can be achieved by using our aligned parallel treebank corpus."
izumi-etal-2004-overview,The Overview of the {SST} Speech Corpus of {J}apanese Learner {E}nglish and Evaluation Through the Experiment on Automatic Detection of Learners{'} Errors,2004,5,37,2,1,50987,emi izumi,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"This paper introduces an overview of the speech corpus of Japanese learner English compiled by National Institute of Information and Communications Technology by showing its data collection procedure and annotation schemes including error tagging. We have collected 1,200 interviews for three years. One of the most unique features of this corpus is that it contains rich information on learnersxe2x80x99 errors. We have performed error tagging for learnersxe2x80x99 grammatical and lexical errors with originally-designed error tagset. We also evaluated the corpus through the experiment on automatic detection of learnersxe2x80x99 errors by using error tag information in the corpus. We did this by using a machine learning model, Maximum Entropy (ME) model. Since we had obtained the limited amount of error-tagged data, we needed to make some efforts to enlarge training data. We added the correct sentences and artificially-made errors to the training data, and found that it improved accuracy. We are planning to make this corpus publicly available in the spring of 2004, so that teachers and researchers in many fields can use the data for their own interests, such as second language acquisition research, syllabus and material design, or the development of computerized pedagogical tools, by combining it with NLP technology."
C04-1159,Dependency Structure Analysis and Sentence Boundary Detection in Spontaneous {J}apanese,2004,11,11,2,0,52399,kazuya shitaoka,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"This paper describes a project to detect dependencies between Japanese phrasal units called bunsetsus, and sentence boundaries in a spontaneous speech corpus. In monologues, the biggest problem with dependency structure analysis is that sentence boundaries are ambiguous. In this paper, we propose two methods for improving the accuracy of sentence boundary detection in spontaneous Japanese speech: One is based on statistical machine translation using dependency information and the other is based on text chunking using SVM. An F-measure of 84.9 was achieved for the accuracy of sentence boundary detection by using the proposed methods. The accuracy of dependency structure analysis was also improved from 75.2% to 77.2% by using automatically detected sentence boundaries. The accuracy of dependency structure analysis and that of sentence boundary detection were also improved by interactively using both automatically detected dependency structures and sentence boundaries."
P03-2026,Automatic Error Detection in the {J}apanese Learners{'} {E}nglish Spoken Data,2003,3,95,2,1,50987,emi izumi,The Companion Volume to the Proceedings of 41st Annual Meeting of the Association for Computational Linguistics,0,"This paper describes a method of detecting grammatical and lexical errors made by Japanese learners of English and other techniques that improve the accuracy of error detection with a limited amount of training data. In this paper, we demonstrate to what extent the proposed methods hold promise by conducting experiments using our learner corpus, which contains information on learners' errors."
P03-1061,Morphological Analysis of a Large Spontaneous Speech Corpus in {J}apanese,2003,13,9,1,1,30019,kiyotaka uchimoto,Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,1,"This paper describes two methods for detecting word segments and their morphological information in a Japanese spontaneous speech corpus, and describes how to tag a large spontaneous speech corpus accurately by using the two methods. The first method is used to detect any type of word segments. The second method is used when there are several definitions for word segments and their POS categories, and when one type of word segments includes another type of word segments. In this paper, we show that by using semi-automatic analysis we achieve a precision of better than 99% for detecting and tagging short words and 97% for long words; the two types of words that comprise the corpus. We also show that better accuracy is achieved by using both methods than by using only the first."
W02-1036,Combining Outputs of Multiple {J}apanese Named Entity Chunkers by Stacking,2002,12,10,3,0.211683,15895,takehito utsuro,Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing ({EMNLP} 2002),0,"In this paper, we propose a method for learning a classifier which combines outputs of more than one Japanese named entity extractors. The proposed combination method belongs to the family of stacked generalizers, which is in principle a technique of combining outputs of several classifiers at the first stage by learning a second stage classifier to combine those outputs at the first stage. Individual models to be combined are based on maximum entropy models, one of which always considers surrounding contexts of a fixed length, while the other considers those of variable lengths according to the number of constituent morphemes of named entities. As an algorithm for learning the second stage classifier, we employ a decision list learning method. Experimental evaluation shows that the proposed method achieves improvement over the best known results with Japanese named entity extractors based on maximum entropy models."
C02-2019,Morphological Analysis of the Spontaneous Speech Corpus,2002,7,7,1,1,30019,kiyotaka uchimoto,{COLING} 2002: The 17th International Conference on Computational Linguistics: Project Notes,0,"This paper describes a project tagging a spontaneous speech corpus with morphological information such as word segmentation and parts-of-speech. We use a morphological analysis system based on a maximum entropy model, which is independent of the domain of corpora. In this paper we show the tagging accuracy achieved by using the model and discuss problems in tagging the spontaneous speech corpus. We also show that a dictionary developed for a corpus on a certain domain is helpful for improving accuracy in analyzing a corpus on another domain."
C02-1064,Text Generation from Keywords,2002,15,23,1,1,30019,kiyotaka uchimoto,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"We describe a method for generating sentences from keywords or headwords. This method consists of two main parts, candidate-text construction and evaluation. The construction part generates text sentences in the form of dependency trees by using complementary information to replace information that is missing because of a knowledge gap and other missing function words to generate natural text sentences based on a particular monolingual corpus. The evaluation part consists of a model for generating an appropriate text when given keywords. This model considers not only word n-gram information, but also dependency information between words. Furthermore, it considers both string information and morphological information."
2002.tmi-papers.14,Correction of errors in a modality corpus used for machine translation using machine-learning,2002,-1,-1,3,0.718013,16788,masaki murata,Proceedings of the 9th Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages: Papers,0,None
W01-1415,"Using a Support-Vector Machine for {J}apanese-to-{E}nglish Translation of Tense, Aspect, and Modality",2001,7,12,2,0.886975,16788,masaki murata,Proceedings of the {ACL} 2001 Workshop on Data-Driven Methods in Machine Translation,0,"This paper describes experiments carried out using a variety of machine-learning methods, including the k-nearest neighborhood method that was used in a previous study, for the translation of tense, aspect, and modality. It was found that the support-vector machine method was the most precise of all the methods tested."
W01-0512,The Unknown Word Problem: a Morphological Analysis of {J}apanese Using Maximum Entropy Aided by a Dictionary,2001,0,45,1,1,30019,kiyotaka uchimoto,Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing,0,None
S01-1033,{J}apanese Word Sense Disambiguation using the Simple {B}ayes and Support Vector Machine Methods,2001,2,16,3,0.886975,16788,masaki murata,Proceedings of {SENSEVAL}-2 Second International Workshop on Evaluating Word Sense Disambiguation Systems,0,"We submitted four systems to the Japanese dictionary-based lexical-sample task of Senseval-2. They were i) the support vector machine method ii) the simple Bayes method, iii) a method combining the two, and iv) a method combining two kinds of each. The combined methods obtained the best precision among the submitted systems. After the contest, we tuned the parameter used in the simple Bayes method, and it obtained higher precision. An explanation of these systems used in Japanese word sense disambiguation was provided."
S01-1038,Word Translation Based on Machine Learning Models Using Translation Memory and Corpora,2001,1,0,1,1,30019,kiyotaka uchimoto,Proceedings of {SENSEVAL}-2 Second International Workshop on Evaluating Word Sense Disambiguation Systems,0,"Senseval-2 was held in Spring, 2001. It consisted of several tasks in various languages. In this paper, we describe our system used for one of these tasks: the Japanese translation task. With an accuracy of 63.4%, our system was the third best system in the contest among nine systems developed by seven groups."
P00-1042,Named Entity Extraction Based on A Maximum Entropy Model and Transformation Rules,2000,13,63,1,1,30019,kiyotaka uchimoto,Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,1,"This paper describes named entity (NE) extraction based on a maximum entropy (M. E.) model and transformation rules. There are two types of named entities when focusing on the relationship between morphemes and NEs as defined in the NE task of the IREX competition held in 1999. Each NE consists of one or more morphemes, or includes a substring of a morpheme. We extract the former type of NE by using the M. E. model. We then extract the latter type of NE by applying transformation rules to the text."
C00-2109,Backward Beam Search Algorithm for Dependency Analysis of {J}apanese,2000,8,19,2,0,1087,satoshi sekine,{COLING} 2000 Volume 2: The 18th International Conference on Computational Linguistics,0,"Backward beam search for dependency analysis of Japanese is proposed. As dependencies normally go from left to right in Japanese, it is effective to analyze sentences backwards (from right to left). The analysis is based on a statistical method and employs a beam search strategy. Based on experiments varying the beam search width, we found that the accuracy is not sensitive to the beam width and even the analysis with a beam width of 1 gets almost the same dependency accuracy as the best accuracy using a wider beam width. This suggested a deterministic algorithm for backwards Japanese dependency analysis, although still the beam search is effective as the N-best sentence accuracy is quite high. The time of analysis is observed to be quadratic in the sentence length."
C00-2126,Word Order Acquisition from Corpora,2000,2,18,1,1,30019,kiyotaka uchimoto,{COLING} 2000 Volume 2: The 18th International Conference on Computational Linguistics,0,"In this paper we describe a method of acquiring word order from corpora. Word order is defined as the order of modifiers, or the order of phrasal units called 'bunsetsu' which depend on the same modifiee. The method uses a model which automatically discovers what the tendency of the word order in Japanese is by using various kinds of information in and around the target bunsetsus. This model shows us to what extent each piece of information contributes to deciding the word order and which word order tends to be selected when several kinds of information conflict. The contribution rate of each piece of information in deciding word order is efficiently learned by a model within a maximum entropy framework. The performance of this trained model can be evaluated by checking how many instances of word order selected by the model agree with those in the original text. In this paper, we show that even a raw corpus that has not been tagged can be used to train the model, if it is first analyzed by a parser. This is possible because the word order of the text in the corpus is correct."
C00-1074,Hybrid Neuro and Rule-Based Part of Speech Taggers,2000,6,13,3,0.789474,33252,qing ma,{COLING} 2000 Volume 1: The 18th International Conference on Computational Linguistics,0,"A hybrid system for tagging part of speech is described that consists of a neuro tagger and a rule-based corrector. The neuro tagger is an initial-state annotator that uses different lengths of context based on longest context priority. Its inputs are weighted by information gains that are obtained by information maximization. The rule-based corrector is constructed by a set of transformation rules to make up for the shortcomings of the neuro tagger. Computer experiments show that almost 20% of the errors made by the neuro tagger are corrected by these transformation rules, so that the hybrid system can reach an accuracy of 95.5% counting only the ambiguous words and 99.1% counting all words when a small Thai corpus with 22,311 ambiguous words is used for training. This accuracy is far higher than that using an HMM and is also higher than that using a rule-based model."
C00-1082,Bunsetsu Identification Using Category-Exclusive Rules,2000,11,8,2,0.886975,16788,masaki murata,{COLING} 2000 Volume 1: The 18th International Conference on Computational Linguistics,0,"This paper describes two new bunsetsu identification methods using supervised learning. Since Japanese syntactic analysis is usually done after bunsetsu identification, bunsetsu identification is important for analyzing Japanese sentences. In experiments comparing the four previously available machine-learning methods (decision tree, maximum-entropy method, example-based approach and decision list) and two new methods using category-exclusive rules, the new method using the category-exclusive rules with the highest similarity performed best."
2000.iwpt-1.43,Dependency Model using Posterior Context,2000,-1,-1,1,1,30019,kiyotaka uchimoto,Proceedings of the Sixth International Workshop on Parsing Technologies,0,"We describe a new model for dependency structure analysis. This model learns the relationship between two phrasal units called bunsetsus as three categories; {`}between{'}, {`}dependent{'}, and {`}beyond{'}, and estimates the dependency likelihood by considering not only the relationship between two bunsetsus but also the relationship between the left bunsetsu and all of the bunsetsus to its right. We implemented this model based on the maximum entropy model. When using the Kyoto University corpus, the dependency accuracy of our model was 88{\%}, which is about 1{\%} higher than that of the conventional model using exactly the same features."
E99-1026,{J}apanese Dependency Structure Analysis Based on Maximum Entropy Models,1999,8,51,1,1,30019,kiyotaka uchimoto,Ninth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,This paper describes a dependency structure analysis of Japanese sentences based on the maximum entropy models. Our model is created by learning the weights of some features from a training corpus to predict the dependency between bunsetsus or phrasal units. The dependency accuracy of our system is 87.2% using the Kyoto University corpus. We discuss the contribution of each feature set and the relationship between the number of training data and the accuracy.
1999.tmi-1.7,"An example-based approach to {J}apanese-to-{E}nglish translation of tense, aspect, and modality",1999,3,16,3,0.772253,16788,masaki murata,Proceedings of the 8th Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,"We have developed a new method for Japanese-to-English translation of tense, aspect, and modality that uses an example-based method. In this method the similarity between input and example sentences is defined as the degree of semantic matching between the expressions at the ends of the sentences. Our method also uses the k-nearest neighbor method in order to exclude the effects of noise; for example, wrongly tagged data in the bilingual corpora. Experiments show that our method can translate tenses, aspects, and modalities more accurately than the top-level MT software currently available on the market can. Moreover, it does not require hand-craft rules."
W98-0202,Intelligent Network News Reader with Visual User Interface,1998,4,2,2,0,15925,hitoshi isahara,Content Visualization and Intermedia Representations ({CVIR}{'}98),0,None
C94-2169,Thesaurus-based Efficient Example Retrieval by Generating Retrieval Queries from Similarities,1994,3,20,2,0,15895,takehito utsuro,{COLING} 1994 Volume 2: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"In example-based NLP, the problem of computational cost of example retrieval is severe, since the retrieval time increases in proportion to the number of examples in the database. This paper proposes a novel example retrieval method for avoiding full retrieval of examples. The proposed method has the following three features, 1) it generates retrieval queries from similarities, 2) efficient example retrieval through the tree structure of a thesaurus, 3) binary search along subsumption ordering of retrieval queries. Example retrieval time drastically decreases with the method."
