2020.acl-main.178,N01-1021,0,0.295162,". By definition, realis events are claimed by the author to have taken place, which makes them more likely to be drawn from from autobiographical or episodic memory in diary-like stories. We train a realis event tagger (using BERT-base; Devlin et al., 2019) on the annotated literary events corpus by Sims et al. (2019), which slightly outperforms the original author’s models. We provide further training details in Appendix B.1. Semantic and Commonsense Knowledge We measure the amount of commonsense knowl3 Note that this is a sentence-level version of surprisal as defined by expectation theory (Hale, 2001; Levy, 2008) edge included explicitly in stories, as a proxy for semantic memory, a form of memory that is thought to encode general knowledge about the world (Tulving, 1972). While this includes facts about how events unfold (i.e., scripts or schemas; Schank and Abelson, 1977; van Kesteren et al., 2012), here we focus on commonsense knowledge, which is also encoded in semantic memory (McRae and Jones, 2013). Given the social focus of our stories, we use the social commonsense knowledge graph ATOMIC (Sap et al., 2019).4 For each story, we first match possible ATOMIC events to sentences by sel"
2020.acl-main.178,N15-1044,1,0.809472,"or commonsense inferences (e.g., “be very happy” “happy”; Figure 1). We describe this algorithm in further detail in Appendix B.2. In our analyses, the measure quantifies the number of story sentences with commonsense tuple matches in the two preceding and following sentences. 3.3 Lexical and Stylistic Measures To supplement our analyses, we compute several coarse-grained lexical counts for each story in H IPPOCORPUS. Such approaches have been used in prior efforts to investigate author mental states, temporal orientation, or counterfactual thinking in language (Tausczik and Pennebaker, 2010; Schwartz et al., 2015; Son et al., 2017). We count psychologically relevant word categories using the Linguistic Inquiry Word Count (Pennebaker et al., 2015, LIWC;), focusing only on the cognitive processes, positive emotion, negative emotion, and I-word categories, as well as the A NALYTIC and T ONE summary variables.5 Additionally, we measure the average concreteness level of words in stories using the lexicon by Brysbaert et al. (2014). 4 Imagining vs. Remembering We summarize the differences between imagined and recalled stories in H IPPOCORPUS in Table 2. For our narrative flow and lexicon-based analyses, 4 A"
2020.acl-main.178,P19-1353,0,0.14621,"and commonsense models to study how cognitive processes of recollection and imagination are engaged in storytelling. We rely on two key aspects of stories: narrative flow (how the story reads) and semantic vs. episodic knowledge (the types of events in the story). We propose as a measure of narrative flow the likelihood of sentences under generative language models conditioned on varying amounts of history. Then, we quantify semantic knowledge by measuring the frequency of commonsense events (from the ATOMIC knowledge graph; Sap et al., 2019), and episodic knowledge by counting realis events (Sims et al., 2019), both shown in Figure 1. 1970 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1970–1978 c July 5 - 10, 2020. 2020 Association for Computational Linguistics We introduce H IPPOCORPUS,1 a dataset of 6,854 diary-like short stories about salient life events, to examine the cognitive processes of remembering and imagining. Using a crowdsourcing pipeline, we collect pairs of recalled and imagined stories written about the same topic. By design, authors of recalled stories rely on their episodic memory to tell their story. We demonstrate that our measur"
2020.acl-main.178,P17-2103,0,0.026575,"es (e.g., “be very happy” “happy”; Figure 1). We describe this algorithm in further detail in Appendix B.2. In our analyses, the measure quantifies the number of story sentences with commonsense tuple matches in the two preceding and following sentences. 3.3 Lexical and Stylistic Measures To supplement our analyses, we compute several coarse-grained lexical counts for each story in H IPPOCORPUS. Such approaches have been used in prior efforts to investigate author mental states, temporal orientation, or counterfactual thinking in language (Tausczik and Pennebaker, 2010; Schwartz et al., 2015; Son et al., 2017). We count psychologically relevant word categories using the Linguistic Inquiry Word Count (Pennebaker et al., 2015, LIWC;), focusing only on the cognitive processes, positive emotion, negative emotion, and I-word categories, as well as the A NALYTIC and T ONE summary variables.5 Additionally, we measure the average concreteness level of words in stories using the lexicon by Brysbaert et al. (2014). 4 Imagining vs. Remembering We summarize the differences between imagined and recalled stories in H IPPOCORPUS in Table 2. For our narrative flow and lexicon-based analyses, 4 ATOMIC contains soci"
2020.acl-main.178,T75-2012,0,0.552144,"child. She and her husband were overwhelmed by emotions. Abstract RECALLED We investigate the use of NLP as a measure of the cognitive processes involved in storytelling, contrasting imagination and recollection of events. To facilitate this, we collect and release H IPPOCORPUS, a dataset of 7,000 stories about imagined and recalled events. # concrete events: 7 We introduce a measure of narrative flow and use this to examine the narratives for imagined and recalled events. Additionally, we measure the differential recruitment of knowledge attributed to semantic memory versus episodic memory (Tulving, 1972) for imagined and recalled storytelling by comparing the frequency of descriptions of general commonsense events with more specific realis events. IMAGINED We recently attended a family wedding. It was the first time in a decade we all got together. Our analyses show that imagined stories have a substantially more linear narrative flow, compared to recalled stories in which adjacent sentences are more disconnected. In addition, while recalled stories rely more on autobiographical events based on episodic memory, imagined stories express more commonsense knowledge based on semantic memory. Fina"
2020.acl-main.486,D16-1129,0,0.0267646,"educating people on reducing unconscious biases in their language. 2 S OCIAL B IAS F RAMES Definition To better enable models to account for socially biased implications of language,2 we design a new pragmatic formalism that distinguishes several related but distinct inferences, shown in Figure 1. Given a natural language utterance, henceforth, post, we collect both categorical as well as free text inferences (described below), inspired by recent efforts in free-text annotations of commonsense knowledge (e.g., Speer and Havasi, 2012; Rashkin et al., 2018; Sap et al., 2019b) and argumentation (Habernal and Gurevych, 2016; Becker et al., 2017). The free-text explanations are crucial to our formalism, as they can both increase trust in predictions made by the machine (Kulesza et al., 2012; Bussone et al., 2015; Nguyen et al., 2018) and encourage a poster’s empathy towards a targeted group, thereby combating biases (CohenAlmagor, 2014). We base our initial frame design on social science literature of pragmatics (Lakoff, 1973; de Marneffe et al., 2012) and impoliteness (Kasper, 1990; Gabriel, 1998; Dynel, 2015; Vonasch and Baumeister, 2017). We then refine the frame structure (including number of possible answers"
2020.acl-main.486,N19-1169,0,0.0175417,"rated text candidate and variable probabilities.9 This can allow variables to be assigned an alternative value that is more globally optimal.10 4.1 Evaluation We evaluate performance of our models in the following ways. For classification, we report precision, recall, and F1 scores of the positive class. Following previous generative inference work (Sap et al., 2019b), we use automated metrics to evaluate model generations. We use BLEU2 and RougeL (F1 ) scores to capture word overlap between the generated inference and the references, which captures quality of generation (Galley et al., 2015; Hashimoto et al., 2019). We additionally compute word mover’s distance (WMD; Kusner et al., 2015), which uses distributed word representations to measure similarity between the generated and target text.11 4.2 Training Details As each post can contain multiple annotations, we define a training instance as containing one postgroup-statement triple (along with the five categorical annotations). We then split our dataset into train/dev./test (75:12.5:12.5), ensuring that no post is present in multiple splits. For evaluation (dev., test), we combine the categorical variables by averaging their binarized values and re-bi"
2020.acl-main.486,C92-2082,0,0.118608,"oke power dynamics between groups (e.g., “F*ck you” vs. “F*ck you, f*ggot”). This is a categorical variable with two possible answers: individualonly (no), group targeted (yes). Targeted group describes the social or demographic group that is referenced or targeted by the post. Here we collect free-text answers, but provide a seed list of demographic or social groups to encourage consistency. Implied statement represents the power dynamic or stereotype that is referenced in the post. We collect free-text answers in the form of simple Hearst-like patterns (e.g., “women are ADJ”, “gay men VBP”; Hearst, 1992). In-group language aims to capture whether the author of a post may be a member of the same social/demographic group that is targeted, as speaker identity changes how a statement is perceived (O’Dea et al., 2015). Specifically, in-group language (words or phrases that (re)establish belonging to a social group; Eble, 1996) can change the perceived offensiveness of a statement, such as reclaimed slurs (Croom, 2011; Galinsky et al., 2013) or self-deprecating language (Greengross and Miller, 2008). Note that we do not attempt to categorize the identity of the speaker. This variable takes three po"
2020.acl-main.486,D16-1230,0,0.0174402,"cs (Table 5). Overall, models do well at generating the targeted groups, likely because of the more limited generation space (there are only 1.4k possible groups in SBIC). Conversely, for implied statement generation (where output space is much larger), model performance is slightly worse. Similar to the classification tasks, SBF-GPT2 gdy shows a slight increase in RougeL score when using constrained decoding, but we see a slight drop in BLEU scores. Error analysis Since small differences in automated evaluation metrics for text generation sometimes only weakly correlate with human judgments (Liu et al., 2016), we manually perform an error analysis on a manually selected set of generated development-set examples from the SBFGPT2 -gdy-constr model (Table 6). Overall, the model seems to struggle with generating textual implications that are relevant to the post, instead generating very generic stereotypes about the demographic groups (e.g., in examples b and c). The model generates the correct stereotypes when there is high lexical overlap with the post (e.g., examples d and e). This is in line with previous research showing that large language models rely on correlational patterns in data (Sap et al"
2020.acl-main.486,J12-2003,0,0.0879155,"Missing"
2020.acl-main.486,D19-1474,0,0.0431593,"reased attention recently (Schmidt and Wiegand, 2017), and most dataset creation work has cast this detection problem as binary classification (Waseem and Hovy, 2016; Davidson et al., 2017; Founta et al., 2018). Moving beyond a single binary label, Wulczyn et al. (2017) and the PerspectiveAPI use a set of binary variables to annotate Wikipedia comments for several toxicityrelated categories (e.g., identity attack, profanity). Similarly, Zampieri et al. (2019) hierarchically annotate a dataset of tweets with offensiveness and whether a group or individual is targeted. Most related to our work, Ousidhoum et al. (2019) create a multilingual dataset of 13k tweets annotated for five different emotion- and toxicity-related aspects, including a 16-class variable representing social groups targeted. In comparison, S OCIAL B IAS F RAMES not only captures binary toxicity and hierarchical information about whether a group is targeted, but also free-text implications about 1.4k different targeted groups and the implied harm behind statements. Similar in spirit to this paper, recent work has tackled more subtle bias in language, such as microaggressions (Breitfeller et al., 2019) and condescension (Wang and Potts, 20"
2020.acl-main.486,P14-2056,0,0.0247509,"k different targeted groups and the implied harm behind statements. Similar in spirit to this paper, recent work has tackled more subtle bias in language, such as microaggressions (Breitfeller et al., 2019) and condescension (Wang and Potts, 2019). These types of biases are in line with the biases covered by S O CIAL B IAS F RAMES , but more narrowly scoped. 5484 Inference about social dynamics Various work has tackled the task of making inferences about power and social dynamics. Particularly, previous work has analyzed power dynamics about specific entities, either in conversation settings (Prabhakaran et al., 2014; Danescu-Niculescu-Mizil et al., 2012) or in narrative text (Sap et al., 2017; Field et al., 2019; Antoniak et al., 2019). Additionally, recent work in commonsense inference has focused on mental states of participants of a situation (e.g., Rashkin et al., 2018; Sap et al., 2019b). In contrast to reasoning about particular individuals, our work focuses on biased implications of social and demographic groups as a whole. 7 Ethical Considerations Risks in deployment Automatic detection of offensiveness or reasoning about harmful implications of language should be done with care. When deploying s"
2020.acl-main.486,D19-1482,0,0.0450103,"metric should be optimized (Corbett-Davies et al., 2017), as well as the fairness of the model on speech by different demographic groups or in different varieties of English (Mitchell et al., 2019). Additionally, deployment of such technology should discuss potential nefarious side effects, such as censorship (Ullmann and Tomalin, 2019) and dialect-based racial bias (Sap et al., 2019a; Davidson et al., 2019). Finally, offensiveness could be paired with promotions of positive online interactions, such as emphasis of community standards (Does et al., 2011) or counterspeech (Chung et al., 2019; Qian et al., 2019). Risks in annotation Recent work has highlighted various negative side effects caused by annotating potentially abusive or harmful content (e.g., acute stress; Roberts, 2016). We mitigated these by limiting the number of posts that one worker could annotate in one day, paying workers above minimum wage ($7–12), and providing crisis management resources to our annotators.13 Additionally, we acknowledge the implications of using data available on public forums for research (Zimmer, 2018) and urge researchers and practitioners to respect the privacy of the authors of posts in SBIC (Ayers et al.,"
2020.acl-main.486,P18-1043,1,0.933568,"tive analysis over large corpora can also be insightful for educating people on reducing unconscious biases in their language. 2 S OCIAL B IAS F RAMES Definition To better enable models to account for socially biased implications of language,2 we design a new pragmatic formalism that distinguishes several related but distinct inferences, shown in Figure 1. Given a natural language utterance, henceforth, post, we collect both categorical as well as free text inferences (described below), inspired by recent efforts in free-text annotations of commonsense knowledge (e.g., Speer and Havasi, 2012; Rashkin et al., 2018; Sap et al., 2019b) and argumentation (Habernal and Gurevych, 2016; Becker et al., 2017). The free-text explanations are crucial to our formalism, as they can both increase trust in predictions made by the machine (Kulesza et al., 2012; Bussone et al., 2015; Nguyen et al., 2018) and encourage a poster’s empathy towards a targeted group, thereby combating biases (CohenAlmagor, 2014). We base our initial frame design on social science literature of pragmatics (Lakoff, 1973; de Marneffe et al., 2012) and impoliteness (Kasper, 1990; Gabriel, 1998; Dynel, 2015; Vonasch and Baumeister, 2017). We th"
2020.acl-main.486,N16-3020,0,0.079531,"Missing"
2020.acl-main.486,P19-1163,1,0.544315,"a et al., 2016), and failure to do so can result in the deployment of harmful technologies (e.g., conversational AI systems turning sexist and racist; Vincent, 2016). Most previous approaches to understanding the implied harm in statements have cast this task as a simple toxicity classification (e.g., Waseem and Hovy, 2016; Founta et al., 2018; Davidson et al., 2017). However, simple classifications run the risk of discriminating against minority groups, due to high variation and identity-based biases in annotations (e.g., which cause models to learn associations between dialect and toxicity; Sap et al., 2019a; Davidson et al., 2019). In addition, detailed explanations are much more informative for people to understand and reason about why a statement is potentially harmful against other people (Gregor and Benbasat, 1999; Ribeiro et al., 2016). Thus, we propose S OCIAL B IAS F RAMES, a novel conceptual formalism that aims to model pragmatic frames in which people project social biases and stereotypes on others. Compared to semantic frames (Fillmore and Baker, 2001), the meanings projected by pragmatic frames are richer, and thus cannot be easily formalized using only categorical labels. Therefore,"
2020.acl-main.486,N19-1144,0,0.0277403,"(Sap et al., 2019c; Sakaguchi et al., 2020). 6 Related Work Bias and toxicity detection Detection of hateful, abusive, or other toxic language has received increased attention recently (Schmidt and Wiegand, 2017), and most dataset creation work has cast this detection problem as binary classification (Waseem and Hovy, 2016; Davidson et al., 2017; Founta et al., 2018). Moving beyond a single binary label, Wulczyn et al. (2017) and the PerspectiveAPI use a set of binary variables to annotate Wikipedia comments for several toxicityrelated categories (e.g., identity attack, profanity). Similarly, Zampieri et al. (2019) hierarchically annotate a dataset of tweets with offensiveness and whether a group or individual is targeted. Most related to our work, Ousidhoum et al. (2019) create a multilingual dataset of 13k tweets annotated for five different emotion- and toxicity-related aspects, including a 16-class variable representing social groups targeted. In comparison, S OCIAL B IAS F RAMES not only captures binary toxicity and hierarchical information about whether a group is targeted, but also free-text implications about 1.4k different targeted groups and the implied harm behind statements. Similar in spiri"
2020.acl-main.486,D17-1247,1,0.848658,"to this paper, recent work has tackled more subtle bias in language, such as microaggressions (Breitfeller et al., 2019) and condescension (Wang and Potts, 2019). These types of biases are in line with the biases covered by S O CIAL B IAS F RAMES , but more narrowly scoped. 5484 Inference about social dynamics Various work has tackled the task of making inferences about power and social dynamics. Particularly, previous work has analyzed power dynamics about specific entities, either in conversation settings (Prabhakaran et al., 2014; Danescu-Niculescu-Mizil et al., 2012) or in narrative text (Sap et al., 2017; Field et al., 2019; Antoniak et al., 2019). Additionally, recent work in commonsense inference has focused on mental states of participants of a situation (e.g., Rashkin et al., 2018; Sap et al., 2019b). In contrast to reasoning about particular individuals, our work focuses on biased implications of social and demographic groups as a whole. 7 Ethical Considerations Risks in deployment Automatic detection of offensiveness or reasoning about harmful implications of language should be done with care. When deploying such algorithms, ethical aspects should be considered including which performan"
2020.acl-main.486,D19-1454,1,0.861203,"a et al., 2016), and failure to do so can result in the deployment of harmful technologies (e.g., conversational AI systems turning sexist and racist; Vincent, 2016). Most previous approaches to understanding the implied harm in statements have cast this task as a simple toxicity classification (e.g., Waseem and Hovy, 2016; Founta et al., 2018; Davidson et al., 2017). However, simple classifications run the risk of discriminating against minority groups, due to high variation and identity-based biases in annotations (e.g., which cause models to learn associations between dialect and toxicity; Sap et al., 2019a; Davidson et al., 2019). In addition, detailed explanations are much more informative for people to understand and reason about why a statement is potentially harmful against other people (Gregor and Benbasat, 1999; Ribeiro et al., 2016). Thus, we propose S OCIAL B IAS F RAMES, a novel conceptual formalism that aims to model pragmatic frames in which people project social biases and stereotypes on others. Compared to semantic frames (Fillmore and Baker, 2001), the meanings projected by pragmatic frames are richer, and thus cannot be easily formalized using only categorical labels. Therefore,"
2020.acl-main.486,W17-1101,0,0.0384554,"to struggle with generating textual implications that are relevant to the post, instead generating very generic stereotypes about the demographic groups (e.g., in examples b and c). The model generates the correct stereotypes when there is high lexical overlap with the post (e.g., examples d and e). This is in line with previous research showing that large language models rely on correlational patterns in data (Sap et al., 2019c; Sakaguchi et al., 2020). 6 Related Work Bias and toxicity detection Detection of hateful, abusive, or other toxic language has received increased attention recently (Schmidt and Wiegand, 2017), and most dataset creation work has cast this detection problem as binary classification (Waseem and Hovy, 2016; Davidson et al., 2017; Founta et al., 2018). Moving beyond a single binary label, Wulczyn et al. (2017) and the PerspectiveAPI use a set of binary variables to annotate Wikipedia comments for several toxicityrelated categories (e.g., identity attack, profanity). Similarly, Zampieri et al. (2019) hierarchically annotate a dataset of tweets with offensiveness and whether a group or individual is targeted. Most related to our work, Ousidhoum et al. (2019) create a multilingual dataset"
2020.acl-main.486,speer-havasi-2012-representing,0,0.0296727,"In addition, the collective analysis over large corpora can also be insightful for educating people on reducing unconscious biases in their language. 2 S OCIAL B IAS F RAMES Definition To better enable models to account for socially biased implications of language,2 we design a new pragmatic formalism that distinguishes several related but distinct inferences, shown in Figure 1. Given a natural language utterance, henceforth, post, we collect both categorical as well as free text inferences (described below), inspired by recent efforts in free-text annotations of commonsense knowledge (e.g., Speer and Havasi, 2012; Rashkin et al., 2018; Sap et al., 2019b) and argumentation (Habernal and Gurevych, 2016; Becker et al., 2017). The free-text explanations are crucial to our formalism, as they can both increase trust in predictions made by the machine (Kulesza et al., 2012; Bussone et al., 2015; Nguyen et al., 2018) and encourage a poster’s empathy towards a targeted group, thereby combating biases (CohenAlmagor, 2014). We base our initial frame design on social science literature of pragmatics (Lakoff, 1973; de Marneffe et al., 2012) and impoliteness (Kasper, 1990; Gabriel, 1998; Dynel, 2015; Vonasch and Ba"
2020.acl-main.486,D19-1385,0,0.0579609,"houm et al. (2019) create a multilingual dataset of 13k tweets annotated for five different emotion- and toxicity-related aspects, including a 16-class variable representing social groups targeted. In comparison, S OCIAL B IAS F RAMES not only captures binary toxicity and hierarchical information about whether a group is targeted, but also free-text implications about 1.4k different targeted groups and the implied harm behind statements. Similar in spirit to this paper, recent work has tackled more subtle bias in language, such as microaggressions (Breitfeller et al., 2019) and condescension (Wang and Potts, 2019). These types of biases are in line with the biases covered by S O CIAL B IAS F RAMES , but more narrowly scoped. 5484 Inference about social dynamics Various work has tackled the task of making inferences about power and social dynamics. Particularly, previous work has analyzed power dynamics about specific entities, either in conversation settings (Prabhakaran et al., 2014; Danescu-Niculescu-Mizil et al., 2012) or in narrative text (Sap et al., 2017; Field et al., 2019; Antoniak et al., 2019). Additionally, recent work in commonsense inference has focused on mental states of participants of"
2020.acl-main.486,N16-2013,0,0.720936,"ly subtle) offensive implications about various demographic groups. recognize the implied demonizing stereotype that “Muslims are terrorists” (Figure 1). Understanding these biases with accurate underlying explanations is necessary for AI systems to adequately interact in the social world (Pereira et al., 2016), and failure to do so can result in the deployment of harmful technologies (e.g., conversational AI systems turning sexist and racist; Vincent, 2016). Most previous approaches to understanding the implied harm in statements have cast this task as a simple toxicity classification (e.g., Waseem and Hovy, 2016; Founta et al., 2018; Davidson et al., 2017). However, simple classifications run the risk of discriminating against minority groups, due to high variation and identity-based biases in annotations (e.g., which cause models to learn associations between dialect and toxicity; Sap et al., 2019a; Davidson et al., 2019). In addition, detailed explanations are much more informative for people to understand and reason about why a statement is potentially harmful against other people (Gregor and Benbasat, 1999; Ribeiro et al., 2016). Thus, we propose S OCIAL B IAS F RAMES, a novel conceptual formalis"
2020.acl-tutorials.7,N19-1112,0,0.133488,"ecent years, yielding multiple exploratory research directions into automated commonsense understanding. Recent efforts to acquire and represent common knowledge resulted in large knowledge graphs, acquired through extractive methods (Speer et al., 2017) or crowdsourcing (Sap et al., 2019a). Simultaneously, a large body of work in integrating reasoning capabilities into downstream tasks has emerged, allowing the development of smarter dialogue (Zhou et al., 2018) and question answering agents (Xiong et al., 2019). Recent advances in large pretrained language models (e.g., Devlin et al., 2019; Liu et al., 2019b), however, have pushed machines closer to humanlike understanding capabilities, calling into question whether machines should directly model commonsense through symbolic integrations. But despite these impressive performance improvements in a variety of NLP tasks, it remains unclear whether these models are performing complex reasoning, or if they are merely learning complex surface correlation patterns (Davis and Marcus, 2015; Marcus, 2018). This difficulty in measuring the progress in commonsense reasoning using downstream tasks has yielded increased efforts at developing robust benchmarks"
2020.acl-tutorials.7,N18-1202,0,0.0350715,"(e.g., Sap et al., 2019b), or temporal commonsense reasoning capabilities (e.g., Zhou et al., 2019), as well as benchmarks that combine commonsense abilities with other tasks (e.g., reading comprehension; Ostermann et al., 2018; Zhang et al., 2018; Huang et al., 2019). What do machines know? Pretrained language models (LMs) have recently been described as “rediscovering the NLP pipeline” (Tenney et al., 2019a), i.e. replacing previous dedicated components of the traditional NLP pipeline, starting from low- and mid-level syntactic and semantic tasks (POS tagging, parsing, verb agreement, e.g., Peters et al., 2018; Jawahar et al., 2019; Shwartz and Dagan, 2019, inter alia), to high-level semantic tasks such as named entity recognition, coreference resolution and semantic role labeling (Tenney et al., 2019b; Liu et al., 2019a). We will discuss recent investigations into pretrained LMs’ ability to capture world knowledge (Petroni et al., 2019; Logan et al., 2019) and learn or reason about commonsense (Feldman et al., 2019). 3 How to incorporate commonsense knowledge into downstream models? Given that large number of NLP applications are designed to require commonsense reasoning, we will review efforts to"
2020.acl-tutorials.7,D19-1250,0,0.0459438,"Missing"
2020.acl-tutorials.7,N19-1421,0,0.249254,"tanding (NLU) tasks hardly require machines to reason about commonsense (Lo Bue and Yates, 2011; Schwartz et al., 2017). This prompted efforts in creating benchmarks carefully designed to be impossible to solve without commonsense knowledge (Roemmele et al., 2011; Levesque, 2011). In response, recent work has focused on using crowdsourcing and automatic filtering to design large-scale benchmarks while maintaining negative examples that are adversarial to machines (Zellers et al., 2018). We will review recent benchmarks that have emerged to assess whether machines have acquired physical (e.g., Talmor et al., 2019; Zellers et al., 2019), social (e.g., Sap et al., 2019b), or temporal commonsense reasoning capabilities (e.g., Zhou et al., 2019), as well as benchmarks that combine commonsense abilities with other tasks (e.g., reading comprehension; Ostermann et al., 2018; Zhang et al., 2018; Huang et al., 2019). What do machines know? Pretrained language models (LMs) have recently been described as “rediscovering the NLP pipeline” (Tenney et al., 2019a), i.e. replacing previous dedicated components of the traditional NLP pipeline, starting from low- and mid-level syntactic and semantic tasks (POS tagging,"
2020.acl-tutorials.7,P19-1487,0,0.0181475,"018; Paul and Frank, 2019; Wang et al., 2018) tasks. For applications without available structured knowledge bases, researchers have relied on commonsense aggregated from corpus statistics pulled from unstructured text (Tandon et al., 2018; Lin et al., 2017; Li et al., 2018; Banerjee et al., 2019). More recently, rather than providing relevant commonsense as an additional input to neural networks, researchers have looked into indirectly encoding commonsense knowledge into the parameters of neural networks through pretraining on commonsense knowledge bases (Zhong et al., 2018) or explanations (Rajani et al., 2019), or by using multi-task objectives with commonsense relation prediction (Xia et al., 2019). mans acquire it (Moore, 2013; Baron-Cohen et al., 1985). We will discuss notions of social commonsense (Burke, 1969; Goldman, 2015) and physical commonsense (Hayes, 1978; McRae et al., 2005). We will cover the differences between taxonomic and inferential knowledge (Davis and Marcus, 2015; Pearl and Mackenzie, 2018), and differentiate commonsense knowledge from related concepts (e.g., script learning; Schank and Abelson, 1975; Chambers and Jurafsky, 2008). How to represent commonsense? We will review e"
2020.acl-tutorials.7,D18-1006,1,0.844121,"the 58th Annual Meeting of the Association for Computational Linguistics, pages 27–33 c July 5, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 (Guan et al., 2018), dialogue (Zhou et al., 2018), QA (Mihaylov and Frank, 2018; Bauer et al., 2018; Lin et al., 2019; Weissenborn et al., 2017; Musa et al., 2019), and classification (Chen et al., 2018; Paul and Frank, 2019; Wang et al., 2018) tasks. For applications without available structured knowledge bases, researchers have relied on commonsense aggregated from corpus statistics pulled from unstructured text (Tandon et al., 2018; Lin et al., 2017; Li et al., 2018; Banerjee et al., 2019). More recently, rather than providing relevant commonsense as an additional input to neural networks, researchers have looked into indirectly encoding commonsense knowledge into the parameters of neural networks through pretraining on commonsense knowledge bases (Zhong et al., 2018) or explanations (Rajani et al., 2019), or by using multi-task objectives with commonsense relation prediction (Xia et al., 2019). mans acquire it (Moore, 2013; Baron-Cohen et al., 1985). We will discuss notions of social commonsense (Burke, 1969; Goldman,"
2020.acl-tutorials.7,P18-1213,1,0.900176,"Missing"
2020.acl-tutorials.7,P19-1452,0,0.0164363,"adversarial to machines (Zellers et al., 2018). We will review recent benchmarks that have emerged to assess whether machines have acquired physical (e.g., Talmor et al., 2019; Zellers et al., 2019), social (e.g., Sap et al., 2019b), or temporal commonsense reasoning capabilities (e.g., Zhou et al., 2019), as well as benchmarks that combine commonsense abilities with other tasks (e.g., reading comprehension; Ostermann et al., 2018; Zhang et al., 2018; Huang et al., 2019). What do machines know? Pretrained language models (LMs) have recently been described as “rediscovering the NLP pipeline” (Tenney et al., 2019a), i.e. replacing previous dedicated components of the traditional NLP pipeline, starting from low- and mid-level syntactic and semantic tasks (POS tagging, parsing, verb agreement, e.g., Peters et al., 2018; Jawahar et al., 2019; Shwartz and Dagan, 2019, inter alia), to high-level semantic tasks such as named entity recognition, coreference resolution and semantic role labeling (Tenney et al., 2019b; Liu et al., 2019a). We will discuss recent investigations into pretrained LMs’ ability to capture world knowledge (Petroni et al., 2019; Logan et al., 2019) and learn or reason about commonsense"
2020.acl-tutorials.7,P18-1043,1,0.87151,"Missing"
2020.acl-tutorials.7,S12-1052,0,0.0813807,"Missing"
2020.acl-tutorials.7,D19-1454,1,0.92721,"seamlessly (Apperly, 2010). Yet, endowing machines with such human-like commonsense reasoning capabilities has remained an elusive goal of artificial intelligence research for decades (Gunning, 2018). Commonsense knowledge and reasoning have received renewed attention from the natural language processing (NLP) community in recent years, yielding multiple exploratory research directions into automated commonsense understanding. Recent efforts to acquire and represent common knowledge resulted in large knowledge graphs, acquired through extractive methods (Speer et al., 2017) or crowdsourcing (Sap et al., 2019a). Simultaneously, a large body of work in integrating reasoning capabilities into downstream tasks has emerged, allowing the development of smarter dialogue (Zhou et al., 2018) and question answering agents (Xiong et al., 2019). Recent advances in large pretrained language models (e.g., Devlin et al., 2019; Liu et al., 2019b), however, have pushed machines closer to humanlike understanding capabilities, calling into question whether machines should directly model commonsense through symbolic integrations. But despite these impressive performance improvements in a variety of NLP tasks, it rem"
2020.acl-tutorials.7,K17-1004,1,0.795345,"lection and representation (e.g., automatic extraction; Etzioni et al., 2008; Zhang et al., 2016; Elazar et al., 2019). We will cover recent approaches that use natural language to represent commonsense (Speer et al., 2017; Sap et al., 2019a), and while noting the challenges that come with using datadriven methods (Gordon and Van Durme, 2013; Jastrzebski et al., 2018). How to measure machines’ ability of commonsense reasoning? We will explain that, despite their design, many natural language understanding (NLU) tasks hardly require machines to reason about commonsense (Lo Bue and Yates, 2011; Schwartz et al., 2017). This prompted efforts in creating benchmarks carefully designed to be impossible to solve without commonsense knowledge (Roemmele et al., 2011; Levesque, 2011). In response, recent work has focused on using crowdsourcing and automatic filtering to design large-scale benchmarks while maintaining negative examples that are adversarial to machines (Zellers et al., 2018). We will review recent benchmarks that have emerged to assess whether machines have acquired physical (e.g., Talmor et al., 2019; Zellers et al., 2019), social (e.g., Sap et al., 2019b), or temporal commonsense reasoning capabil"
2020.acl-tutorials.7,P19-1417,0,0.019746,"d reasoning have received renewed attention from the natural language processing (NLP) community in recent years, yielding multiple exploratory research directions into automated commonsense understanding. Recent efforts to acquire and represent common knowledge resulted in large knowledge graphs, acquired through extractive methods (Speer et al., 2017) or crowdsourcing (Sap et al., 2019a). Simultaneously, a large body of work in integrating reasoning capabilities into downstream tasks has emerged, allowing the development of smarter dialogue (Zhou et al., 2018) and question answering agents (Xiong et al., 2019). Recent advances in large pretrained language models (e.g., Devlin et al., 2019; Liu et al., 2019b), however, have pushed machines closer to humanlike understanding capabilities, calling into question whether machines should directly model commonsense through symbolic integrations. But despite these impressive performance improvements in a variety of NLP tasks, it remains unclear whether these models are performing complex reasoning, or if they are merely learning complex surface correlation patterns (Davis and Marcus, 2015; Marcus, 2018). This difficulty in measuring the progress in commonse"
2020.acl-tutorials.7,Q19-1027,1,0.836638,"onsense reasoning capabilities (e.g., Zhou et al., 2019), as well as benchmarks that combine commonsense abilities with other tasks (e.g., reading comprehension; Ostermann et al., 2018; Zhang et al., 2018; Huang et al., 2019). What do machines know? Pretrained language models (LMs) have recently been described as “rediscovering the NLP pipeline” (Tenney et al., 2019a), i.e. replacing previous dedicated components of the traditional NLP pipeline, starting from low- and mid-level syntactic and semantic tasks (POS tagging, parsing, verb agreement, e.g., Peters et al., 2018; Jawahar et al., 2019; Shwartz and Dagan, 2019, inter alia), to high-level semantic tasks such as named entity recognition, coreference resolution and semantic role labeling (Tenney et al., 2019b; Liu et al., 2019a). We will discuss recent investigations into pretrained LMs’ ability to capture world knowledge (Petroni et al., 2019; Logan et al., 2019) and learn or reason about commonsense (Feldman et al., 2019). 3 How to incorporate commonsense knowledge into downstream models? Given that large number of NLP applications are designed to require commonsense reasoning, we will review efforts to integrate such knowledge into NLP tasks. Vario"
2020.acl-tutorials.7,P19-1472,1,0.846992,"ardly require machines to reason about commonsense (Lo Bue and Yates, 2011; Schwartz et al., 2017). This prompted efforts in creating benchmarks carefully designed to be impossible to solve without commonsense knowledge (Roemmele et al., 2011; Levesque, 2011). In response, recent work has focused on using crowdsourcing and automatic filtering to design large-scale benchmarks while maintaining negative examples that are adversarial to machines (Zellers et al., 2018). We will review recent benchmarks that have emerged to assess whether machines have acquired physical (e.g., Talmor et al., 2019; Zellers et al., 2019), social (e.g., Sap et al., 2019b), or temporal commonsense reasoning capabilities (e.g., Zhou et al., 2019), as well as benchmarks that combine commonsense abilities with other tasks (e.g., reading comprehension; Ostermann et al., 2018; Zhang et al., 2018; Huang et al., 2019). What do machines know? Pretrained language models (LMs) have recently been described as “rediscovering the NLP pipeline” (Tenney et al., 2019a), i.e. replacing previous dedicated components of the traditional NLP pipeline, starting from low- and mid-level syntactic and semantic tasks (POS tagging, parsing, verb agreemen"
2020.acl-tutorials.7,2020.emnlp-main.373,1,0.893321,"Missing"
2020.acl-tutorials.7,D19-1332,1,0.837743,"ed efforts in creating benchmarks carefully designed to be impossible to solve without commonsense knowledge (Roemmele et al., 2011; Levesque, 2011). In response, recent work has focused on using crowdsourcing and automatic filtering to design large-scale benchmarks while maintaining negative examples that are adversarial to machines (Zellers et al., 2018). We will review recent benchmarks that have emerged to assess whether machines have acquired physical (e.g., Talmor et al., 2019; Zellers et al., 2019), social (e.g., Sap et al., 2019b), or temporal commonsense reasoning capabilities (e.g., Zhou et al., 2019), as well as benchmarks that combine commonsense abilities with other tasks (e.g., reading comprehension; Ostermann et al., 2018; Zhang et al., 2018; Huang et al., 2019). What do machines know? Pretrained language models (LMs) have recently been described as “rediscovering the NLP pipeline” (Tenney et al., 2019a), i.e. replacing previous dedicated components of the traditional NLP pipeline, starting from low- and mid-level syntactic and semantic tasks (POS tagging, parsing, verb agreement, e.g., Peters et al., 2018; Jawahar et al., 2019; Shwartz and Dagan, 2019, inter alia), to high-level sema"
2020.emnlp-main.349,K19-1079,0,0.115309,"Missing"
2020.emnlp-main.349,P19-1200,1,0.889615,"GE scores were obtained by producing text that is more repetitive and generic.10 In contrast, P LOT M ACHINES generally achieves good performance on both ROUGE and diversity scores, with self-BLEU scores that are lower than most other models. Notably, they generally have more similar self-BLEU scores to the actual gold stories, indicating that the language diversity is more similar to what humans write. Automatic Metrics In this section, we evaluate performance using different automatic metrics. We compute ROUGE scores (Lin, 2004) and self-BLEU (Zhu et al., 2018) following from previous work (Shen et al., 2019; Zhu et al., 2018) showing that a large ROUGE score together with a low self-BLEU score can demonstrate a model’s ability to generate realisticlooking as well as diverse generations. Coverage We compute ROUGE scores (Lin, 2004) with respect to the gold stories (Table 2). Results show that the full P LOT M ACHINES achieves comparable or higher ROUGE on all three datasets. Both P LOT M ACHINES variants (using GPT or GPT-2 as a base) achieve improvements over G ROVER, even though G ROVER includes significantly more parameters than the model using GPT. Ablations In the bottom block of Table 2, we"
2020.emnlp-main.349,D16-1032,1,\N,Missing
2020.emnlp-main.349,P18-1082,0,\N,Missing
2020.emnlp-main.349,N18-1156,0,\N,Missing
2020.emnlp-main.349,D18-1462,0,\N,Missing
2020.emnlp-main.349,W13-4069,0,\N,Missing
2020.emnlp-main.349,W18-1505,0,\N,Missing
2020.emnlp-main.373,D18-1454,0,0.0293621,"and Jiang, 2019), retrieval or statistics mind from corpora (Lin et al., 2017; Mitra et al., 2019; Joshi et al., 2020), knowledge base embeddings (Chen et al., 2019; Xiong et al., 2019), hand-crafted rules (Lin et al., 2017; Tandon et al., 2018), and tools such as sentiment analyzers (Chen et al., 2019) and knowledgeinformed LMs (Bosselut and Choi, 2019). The external knowledge is typically incorporated into the neural model by learning a vector representation of the symbolic knowledge (e.g. subgraphs from ConceptNet), and attending to it via attention mechanism when representing the inputs (Bauer et al., 2018; Paul and Frank, 2019; Lin et al., 2019). Alternative approaches include using the knowledge to score answer candidates and prune implausible ones (Lin et al., 2017; Tandon et al., 2018), and training in a multi-task setup via auxiliary tasks pertaining to knowledge (Xia et al., 2019). To the best of our knowledge, our method is the first to generate knowledge from pre-trained language models and incorporate it as external knowledge into a question answering model. Concurrently, Latcinnik and Berant (2020) used one language model to generate hypotheses and another language model as an answer"
2020.emnlp-main.373,A83-1012,0,0.531744,"Missing"
2020.emnlp-main.373,P19-1470,1,0.793934,"ate the knowledge into the model as text, we convert each ConceptNet relation to a natural language template as in Davison et al. (2019). We limit the path length to 2 edges in order to maintain high precision. Corpus. For pairs of words from the context and question and from the answer choices, we extract their joint occurrences (with minimum frequency of 100) in Google N-grams (Brants and Franz, 2006). This yields text fragments of up to 5 words rather than well-formed sentences, with the potential of describing the relationship between the two words (Shwartz and Dagan, 2018). COMeT. COMeT (Bosselut et al., 2019) is a knowledge base construction model trained on the ATOMIC resource (Sap et al., 2019a) which consists of everyday situations along with multiple commonsense dimensions such as their causes, effects, pre- and post-conditions, etc. We generate all the dimensions unless we can generate specific relations that are more likely to help. Specifically, in Social IQa, we heuristically try to understand which type of relation in COMeT the question asks for. In COPA, we use the pre-condition relations for cause questions (xIntent, xNeed) and the postcondition relations for effect questions (xEffect,"
2020.emnlp-main.373,D19-1109,0,0.295471,"ad schema challenge (Levesque et al., 2012). Most current NLU models rely on pretrained language models (LMs; e.g. Radford et al., 2019; Devlin et al., 2019; Raffel et al., 2020). The standard practice is to fine-tune a pre-trained LM in a supervised manner on task-specific data. Alternatively, LM score is used to rank answer choices in a zero-shot setup (Wang et al., 2019; Bosselut and Choi, 2019). In both setups, pre-trained LMs yield improved performance upon prior methods, greatly due to the world knowledge that such LMs capture, having been trained on massive texts (Petroni et al., 2019; Davison et al., 2019). Despite the performance boost, LMs as knowledge providers suffer from various shortcomings: (i) insufficient coverage: due to reporting bias, many trivial facts might not be captured by LMs because they are rarely written about (Gordon and Van Durme, 2013). (ii) insufficient precision: the distributional training objective increases the probability of non-facts that are semantically similar to true facts, as in negation (“birds cannot fly”; Kassner and Sch¨utze, 2020). LMs excel in predicting the semantic category of a missing word, but might predict the wrong instance in that category (e.g."
2020.emnlp-main.373,N19-1423,0,0.269192,"oncert. Using physical and social commonsense – (i) Bob and Alice want to see the stage, and (ii) If Bob is taller, they would block Alice’s view – one can infer that Alice is taller than Bob. Such examples are ubiquitous across natural language understanding (NLU) tasks such as reading comprehension (Hirschman et al., 1999) and recognizing textual entailment (Dagan et al., 2013), and even more so in tasks dedicated to commonsense reasoning such as the Winograd schema challenge (Levesque et al., 2012). Most current NLU models rely on pretrained language models (LMs; e.g. Radford et al., 2019; Devlin et al., 2019; Raffel et al., 2020). The standard practice is to fine-tune a pre-trained LM in a supervised manner on task-specific data. Alternatively, LM score is used to rank answer choices in a zero-shot setup (Wang et al., 2019; Bosselut and Choi, 2019). In both setups, pre-trained LMs yield improved performance upon prior methods, greatly due to the world knowledge that such LMs capture, having been trained on massive texts (Petroni et al., 2019; Davison et al., 2019). Despite the performance boost, LMs as knowledge providers suffer from various shortcomings: (i) insufficient coverage: due to reporti"
2020.emnlp-main.373,N18-2092,0,0.0441901,"th works have shown somewhat promising results, other work showed that knowledge extracted from LMs is expectantly not always ac4622 curate. Specifically, Kassner and Sch¨utze (2020) showed that negated facts are also considered likely by the LM, while Logan et al. (2019) pointed out that LMs may over-generalize and produce incorrect facts such as “Barack Obama’s wife is Hillary”. 6.3 Generating Questions and Explanations There are numerous research directions investigating automatic question generation (Vanderwende, 2008). Motivations vary from data augmentation to QA tasks (Du et al., 2017; Dhingra et al., 2018; Du and Cardie, 2018; Sachan and Xing, 2018; Fabbri et al., 2020) through conversational machine reading (Saeidi et al., 2018; Pan et al., 2019), simplifying questions to make them more easily answerable (Buck et al., 2018; Talmor and Berant, 2018; Perez et al., 2020), to using questions as means for other purposes such as sentence representation and summarization (Guo et al., 2018; Potash and Suleman, 2019). In particular, our work is pertinent to previous work in producing clarification questions and explanations. Rao and Daum´e III (2019) worked on questions from forums (e.g. Stack Exchang"
2020.emnlp-main.373,P18-1177,0,0.0458484,"Missing"
2020.emnlp-main.373,P17-1123,0,0.0158747,"mplates. While both works have shown somewhat promising results, other work showed that knowledge extracted from LMs is expectantly not always ac4622 curate. Specifically, Kassner and Sch¨utze (2020) showed that negated facts are also considered likely by the LM, while Logan et al. (2019) pointed out that LMs may over-generalize and produce incorrect facts such as “Barack Obama’s wife is Hillary”. 6.3 Generating Questions and Explanations There are numerous research directions investigating automatic question generation (Vanderwende, 2008). Motivations vary from data augmentation to QA tasks (Du et al., 2017; Dhingra et al., 2018; Du and Cardie, 2018; Sachan and Xing, 2018; Fabbri et al., 2020) through conversational machine reading (Saeidi et al., 2018; Pan et al., 2019), simplifying questions to make them more easily answerable (Buck et al., 2018; Talmor and Berant, 2018; Perez et al., 2020), to using questions as means for other purposes such as sentence representation and summarization (Guo et al., 2018; Potash and Suleman, 2019). In particular, our work is pertinent to previous work in producing clarification questions and explanations. Rao and Daum´e III (2019) worked on questions from foru"
2020.emnlp-main.373,2020.acl-main.413,0,0.0318935,"that knowledge extracted from LMs is expectantly not always ac4622 curate. Specifically, Kassner and Sch¨utze (2020) showed that negated facts are also considered likely by the LM, while Logan et al. (2019) pointed out that LMs may over-generalize and produce incorrect facts such as “Barack Obama’s wife is Hillary”. 6.3 Generating Questions and Explanations There are numerous research directions investigating automatic question generation (Vanderwende, 2008). Motivations vary from data augmentation to QA tasks (Du et al., 2017; Dhingra et al., 2018; Du and Cardie, 2018; Sachan and Xing, 2018; Fabbri et al., 2020) through conversational machine reading (Saeidi et al., 2018; Pan et al., 2019), simplifying questions to make them more easily answerable (Buck et al., 2018; Talmor and Berant, 2018; Perez et al., 2020), to using questions as means for other purposes such as sentence representation and summarization (Guo et al., 2018; Potash and Suleman, 2019). In particular, our work is pertinent to previous work in producing clarification questions and explanations. Rao and Daum´e III (2019) worked on questions from forums (e.g. Stack Exchange). They proposed a model that generates clarification questions a"
2020.emnlp-main.373,S12-1052,0,0.149503,"larification is factually-correct. We show that even among the clarifications that helped the prediction, humans perceived many as unhelpful or even incorrect, demonstrating that LM-based models often solve problems correctly for seemingly incorrect reasons. Our results call for future research on robust and correct knowledge integration to LM-based question answering systems. 2 Tasks We focused on the multiple-choice question answering tasks detailed below. Each instance consists of an optional context, an optional question, and several answer choices. COPA: Choice of Plausible Alternatives (Gordon et al., 2012): Asking about either a plausible cause or a plausible result, among two alternatives, of a certain event expressed in a simple sentence. CommonSenseQA: commonsense Question Answering (Talmor et al., 2019): General questions about concepts from ConceptNet. To increase the challenge, the distractors are related to the target concept either by a relationship in ConceptNet or as suggested by crowdsourcing workers. MC-TACO: Multiple Choice Temporal commonsense (Zhou et al., 2019): Questions about temporal aspects of events such as ordering, duration, frequency, and typical time. The distractors we"
2020.emnlp-main.373,P18-1064,0,0.0192272,"enerating Questions and Explanations There are numerous research directions investigating automatic question generation (Vanderwende, 2008). Motivations vary from data augmentation to QA tasks (Du et al., 2017; Dhingra et al., 2018; Du and Cardie, 2018; Sachan and Xing, 2018; Fabbri et al., 2020) through conversational machine reading (Saeidi et al., 2018; Pan et al., 2019), simplifying questions to make them more easily answerable (Buck et al., 2018; Talmor and Berant, 2018; Perez et al., 2020), to using questions as means for other purposes such as sentence representation and summarization (Guo et al., 2018; Potash and Suleman, 2019). In particular, our work is pertinent to previous work in producing clarification questions and explanations. Rao and Daum´e III (2019) worked on questions from forums (e.g. Stack Exchange). They proposed a model that generates clarification questions and corresponding answers for a given question, using the question’s comments (clarification questions and answers) as supervision. Questionanswer pairs were scored based on how much relevant information they add to the context. Shen et al. (2019) developed an active learning framework for image captioning that learns"
2020.emnlp-main.373,2021.ccl-1.108,0,0.187244,"Missing"
2020.emnlp-main.373,P19-1598,0,0.0309114,"le substitutes to the mask in “Dante was born in [MASK]” assigns the highest probability to Rome. Davison et al. (2019) similarly showed that BERT assigns higher scores to natural language fragments of true rather than fictitious ConceptNet triplets, and semi-automated the template creation by using GPT2 to score hand-crafted templates. While both works have shown somewhat promising results, other work showed that knowledge extracted from LMs is expectantly not always ac4622 curate. Specifically, Kassner and Sch¨utze (2020) showed that negated facts are also considered likely by the LM, while Logan et al. (2019) pointed out that LMs may over-generalize and produce incorrect facts such as “Barack Obama’s wife is Hillary”. 6.3 Generating Questions and Explanations There are numerous research directions investigating automatic question generation (Vanderwende, 2008). Motivations vary from data augmentation to QA tasks (Du et al., 2017; Dhingra et al., 2018; Du and Cardie, 2018; Sachan and Xing, 2018; Fabbri et al., 2020) through conversational machine reading (Saeidi et al., 2018; Pan et al., 2019), simplifying questions to make them more easily answerable (Buck et al., 2018; Talmor and Berant, 2018; Pe"
2020.emnlp-main.373,S18-1119,0,0.065609,"Missing"
2020.emnlp-main.373,P19-1203,0,0.0249599,"cally, Kassner and Sch¨utze (2020) showed that negated facts are also considered likely by the LM, while Logan et al. (2019) pointed out that LMs may over-generalize and produce incorrect facts such as “Barack Obama’s wife is Hillary”. 6.3 Generating Questions and Explanations There are numerous research directions investigating automatic question generation (Vanderwende, 2008). Motivations vary from data augmentation to QA tasks (Du et al., 2017; Dhingra et al., 2018; Du and Cardie, 2018; Sachan and Xing, 2018; Fabbri et al., 2020) through conversational machine reading (Saeidi et al., 2018; Pan et al., 2019), simplifying questions to make them more easily answerable (Buck et al., 2018; Talmor and Berant, 2018; Perez et al., 2020), to using questions as means for other purposes such as sentence representation and summarization (Guo et al., 2018; Potash and Suleman, 2019). In particular, our work is pertinent to previous work in producing clarification questions and explanations. Rao and Daum´e III (2019) worked on questions from forums (e.g. Stack Exchange). They proposed a model that generates clarification questions and corresponding answers for a given question, using the question’s comments (c"
2020.emnlp-main.373,N19-1368,0,0.0478674,"etrieval or statistics mind from corpora (Lin et al., 2017; Mitra et al., 2019; Joshi et al., 2020), knowledge base embeddings (Chen et al., 2019; Xiong et al., 2019), hand-crafted rules (Lin et al., 2017; Tandon et al., 2018), and tools such as sentiment analyzers (Chen et al., 2019) and knowledgeinformed LMs (Bosselut and Choi, 2019). The external knowledge is typically incorporated into the neural model by learning a vector representation of the symbolic knowledge (e.g. subgraphs from ConceptNet), and attending to it via attention mechanism when representing the inputs (Bauer et al., 2018; Paul and Frank, 2019; Lin et al., 2019). Alternative approaches include using the knowledge to score answer candidates and prune implausible ones (Lin et al., 2017; Tandon et al., 2018), and training in a multi-task setup via auxiliary tasks pertaining to knowledge (Xia et al., 2019). To the best of our knowledge, our method is the first to generate knowledge from pre-trained language models and incorporate it as external knowledge into a question answering model. Concurrently, Latcinnik and Berant (2020) used one language model to generate hypotheses and another language model as an answer scorer for CommonSense"
2020.emnlp-main.373,D18-1233,0,0.0651602,"Missing"
2020.emnlp-main.373,speer-havasi-2012-representing,0,0.140964,"ion (“birds cannot fly”; Kassner and Sch¨utze, 2020). LMs excel in predicting the semantic category of a missing word, but might predict the wrong instance in that category (e.g., depending on the phrasing, BERT sometimes predicts red as the color of a dove). Finally, (iii) limited reasoning capabilities: it is unclear that LMs are capable of performing multiple reasoning steps involving implicit knowledge. To increase the coverage of high-precision world knowledge and facilitate multi-hop reasoning by making intermediate reasoning steps explicit, prior work incorporated KBs (e.g. ConceptNet; Speer and Havasi, 2012) and knowledge-informed models into LM-based models (Xia et al., 2019; Bosselut and Choi, 2019; Chen et al., 2019). In this paper, we study pre-trained LMs as an alternative to external KBs in providing knowledge 4615 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4615–4629, c November 16–20, 2020. 2020 Association for Computational Linguistics Because Brett found an internship while in college but Ian was unable to, Brett found a job less quickly after graduation. The purpose of the internship is to help people find jobs. s11 Because Brett found"
2020.emnlp-main.373,N18-1059,0,0.013742,", while Logan et al. (2019) pointed out that LMs may over-generalize and produce incorrect facts such as “Barack Obama’s wife is Hillary”. 6.3 Generating Questions and Explanations There are numerous research directions investigating automatic question generation (Vanderwende, 2008). Motivations vary from data augmentation to QA tasks (Du et al., 2017; Dhingra et al., 2018; Du and Cardie, 2018; Sachan and Xing, 2018; Fabbri et al., 2020) through conversational machine reading (Saeidi et al., 2018; Pan et al., 2019), simplifying questions to make them more easily answerable (Buck et al., 2018; Talmor and Berant, 2018; Perez et al., 2020), to using questions as means for other purposes such as sentence representation and summarization (Guo et al., 2018; Potash and Suleman, 2019). In particular, our work is pertinent to previous work in producing clarification questions and explanations. Rao and Daum´e III (2019) worked on questions from forums (e.g. Stack Exchange). They proposed a model that generates clarification questions and corresponding answers for a given question, using the question’s comments (clarification questions and answers) as supervision. Questionanswer pairs were scored based on how much"
2020.emnlp-main.373,N19-1421,0,0.240692,"e problems correctly for seemingly incorrect reasons. Our results call for future research on robust and correct knowledge integration to LM-based question answering systems. 2 Tasks We focused on the multiple-choice question answering tasks detailed below. Each instance consists of an optional context, an optional question, and several answer choices. COPA: Choice of Plausible Alternatives (Gordon et al., 2012): Asking about either a plausible cause or a plausible result, among two alternatives, of a certain event expressed in a simple sentence. CommonSenseQA: commonsense Question Answering (Talmor et al., 2019): General questions about concepts from ConceptNet. To increase the challenge, the distractors are related to the target concept either by a relationship in ConceptNet or as suggested by crowdsourcing workers. MC-TACO: Multiple Choice Temporal commonsense (Zhou et al., 2019): Questions about temporal aspects of events such as ordering, duration, frequency, and typical time. The distractors were selected in an adversarial way using BERT.1 Social IQa: Social Interaction Question Answering (Sap et al., 2019b): Questions regarding social interactions, based on the ATOMIC dataset (Sap et al., 2019a"
2020.emnlp-main.373,D18-1006,0,0.0746222,"r-based representations, specifically pre-trained LMs (Devlin et al., 2019; Liu et al., 2019). With respect to the knowledge source, the vast majority of papers rely on ConceptNet to extract relation paths between concepts and entities identified in the input (Speer and Havasi, 2012, see an example in Figure 2). Additional resources include WordNet (Lin et al., 2017; Wang and Jiang, 2019), retrieval or statistics mind from corpora (Lin et al., 2017; Mitra et al., 2019; Joshi et al., 2020), knowledge base embeddings (Chen et al., 2019; Xiong et al., 2019), hand-crafted rules (Lin et al., 2017; Tandon et al., 2018), and tools such as sentiment analyzers (Chen et al., 2019) and knowledgeinformed LMs (Bosselut and Choi, 2019). The external knowledge is typically incorporated into the neural model by learning a vector representation of the symbolic knowledge (e.g. subgraphs from ConceptNet), and attending to it via attention mechanism when representing the inputs (Bauer et al., 2018; Paul and Frank, 2019; Lin et al., 2019). Alternative approaches include using the knowledge to score answer candidates and prune implausible ones (Lin et al., 2017; Tandon et al., 2018), and training in a multi-task setup via"
2020.emnlp-main.373,P19-1219,0,0.0449706,"l in a fraternity house. Table 4: An example for each of the error types among the harmful clarifications. et al., 2018; Clark et al., 2018; Talmor et al., 2019). The neural component has recently shifted from biLSTM to transformer-based representations, specifically pre-trained LMs (Devlin et al., 2019; Liu et al., 2019). With respect to the knowledge source, the vast majority of papers rely on ConceptNet to extract relation paths between concepts and entities identified in the input (Speer and Havasi, 2012, see an example in Figure 2). Additional resources include WordNet (Lin et al., 2017; Wang and Jiang, 2019), retrieval or statistics mind from corpora (Lin et al., 2017; Mitra et al., 2019; Joshi et al., 2020), knowledge base embeddings (Chen et al., 2019; Xiong et al., 2019), hand-crafted rules (Lin et al., 2017; Tandon et al., 2018), and tools such as sentiment analyzers (Chen et al., 2019) and knowledgeinformed LMs (Bosselut and Choi, 2019). The external knowledge is typically incorporated into the neural model by learning a vector representation of the symbolic knowledge (e.g. subgraphs from ConceptNet), and attending to it via attention mechanism when representing the inputs (Bauer et al., 201"
2020.emnlp-main.373,P19-1393,0,0.0167972,"across natural language understanding (NLU) tasks such as reading comprehension (Hirschman et al., 1999) and recognizing textual entailment (Dagan et al., 2013), and even more so in tasks dedicated to commonsense reasoning such as the Winograd schema challenge (Levesque et al., 2012). Most current NLU models rely on pretrained language models (LMs; e.g. Radford et al., 2019; Devlin et al., 2019; Raffel et al., 2020). The standard practice is to fine-tune a pre-trained LM in a supervised manner on task-specific data. Alternatively, LM score is used to rank answer choices in a zero-shot setup (Wang et al., 2019; Bosselut and Choi, 2019). In both setups, pre-trained LMs yield improved performance upon prior methods, greatly due to the world knowledge that such LMs capture, having been trained on massive texts (Petroni et al., 2019; Davison et al., 2019). Despite the performance boost, LMs as knowledge providers suffer from various shortcomings: (i) insufficient coverage: due to reporting bias, many trivial facts might not be captured by LMs because they are rarely written about (Gordon and Van Durme, 2013). (ii) insufficient precision: the distributional training objective increases the probability o"
2020.emnlp-main.373,P19-1417,0,0.042015,"al component has recently shifted from biLSTM to transformer-based representations, specifically pre-trained LMs (Devlin et al., 2019; Liu et al., 2019). With respect to the knowledge source, the vast majority of papers rely on ConceptNet to extract relation paths between concepts and entities identified in the input (Speer and Havasi, 2012, see an example in Figure 2). Additional resources include WordNet (Lin et al., 2017; Wang and Jiang, 2019), retrieval or statistics mind from corpora (Lin et al., 2017; Mitra et al., 2019; Joshi et al., 2020), knowledge base embeddings (Chen et al., 2019; Xiong et al., 2019), hand-crafted rules (Lin et al., 2017; Tandon et al., 2018), and tools such as sentiment analyzers (Chen et al., 2019) and knowledgeinformed LMs (Bosselut and Choi, 2019). The external knowledge is typically incorporated into the neural model by learning a vector representation of the symbolic knowledge (e.g. subgraphs from ConceptNet), and attending to it via attention mechanism when representing the inputs (Bauer et al., 2018; Paul and Frank, 2019; Lin et al., 2019). Alternative approaches include using the knowledge to score answer candidates and prune implausible ones (Lin et al., 2017; T"
2020.emnlp-main.373,D18-1009,1,0.858633,"dsourced with a carefully designed approach that produces diverse examples which are trivial for humans. 3 Models A given instance consists of an optional context c, an optional question q, and answer choices: aki=1 . We first describe the baseline model, which makes 1 To make this task compatible with the other tasks, we only kept a single correct answer per instance, making our results not comparable to previously reported results. 2 Word associations and dataset-specific features that are not informative for the task are identified by a strong baseline and removed (Gururangan et al., 2018; Zellers et al., 2018). 4616 Taylor was doing her job so she put the money in the drawer. job, money job type of money What will Taylor do next? xWant work oal d by g ate motiv Job to earn money Job is a type of work. You would work because you want money. Job to earn money. to keep the money in the drawer As a result, Taylor wants to keep the money in the drawer. Figure 2: Generating a single clarification using ConceptNet, Google Ngrams, and COMeT (Social IQa instance). the prediction based on the instance alone (§3.1). We then describe a knowledge-informed model that relies on external resources (§3.2). Finally,"
2020.emnlp-main.373,P99-1042,0,\N,Missing
2020.emnlp-main.373,N16-1098,0,\N,Missing
2020.emnlp-main.373,D17-1216,0,\N,Missing
2020.emnlp-main.373,N18-1058,0,\N,Missing
2020.emnlp-main.373,N19-1013,0,\N,Missing
2020.emnlp-main.373,Q18-1023,0,\N,Missing
2020.emnlp-main.373,D19-1454,1,\N,Missing
2020.emnlp-main.373,D19-1250,0,\N,Missing
2020.emnlp-main.373,2020.emnlp-main.713,0,\N,Missing
2020.emnlp-main.48,P18-1043,1,0.844295,"ituations (Vu et al., 2014; Ding and Riloff, 2016) as well as so10 We use the MediaBias/FactCheck ratings: https:// mediabiasfactcheck.com. cial and moral dynamics in language (Van Hee et al., 2015). Commonly used for coarse-grained analyses of morality in text (Fulgoni et al., 2016; Volkova et al., 2017; Weber et al., 2018), Graham et al. (2009) introduce the Moral Foundations lexicon, a dictionary of morality-evoking words (later extended by Rezapour et al., 2019). A recent line of work focused on representing social implications of everyday situations in freeform text in a knowledge graph (Rashkin et al., 2018; Sap et al., 2019). Relatedly, Sap et al. (2020) introduce Social Bias Frames, a hybrid free-text and categorical formalism to reason about biased implications in language. In contrast, our work formalizes a new type of reasoning around expectations of social norms evoked by situations. Finally, concurrent works have developed rich and exciting resources studying similar phenomena. Tay et al. (2020) study Would you rather? questions, and Acharya et al. (2020) investigate ritual understanding across cultures. Hendrycks et al. (2020) study ethical questions, attempting to assign a real-valued u"
2020.emnlp-main.48,W19-1305,0,0.0269575,"of situationally-rooted evocation of frames (Fillmore and Baker, 2001). Our work adds to the growing literature concerned with distilling reactions to situations (Vu et al., 2014; Ding and Riloff, 2016) as well as so10 We use the MediaBias/FactCheck ratings: https:// mediabiasfactcheck.com. cial and moral dynamics in language (Van Hee et al., 2015). Commonly used for coarse-grained analyses of morality in text (Fulgoni et al., 2016; Volkova et al., 2017; Weber et al., 2018), Graham et al. (2009) introduce the Moral Foundations lexicon, a dictionary of morality-evoking words (later extended by Rezapour et al., 2019). A recent line of work focused on representing social implications of everyday situations in freeform text in a knowledge graph (Rashkin et al., 2018; Sap et al., 2019). Relatedly, Sap et al. (2020) introduce Social Bias Frames, a hybrid free-text and categorical formalism to reason about biased implications in language. In contrast, our work formalizes a new type of reasoning around expectations of social norms evoked by situations. Finally, concurrent works have developed rich and exciting resources studying similar phenomena. Tay et al. (2020) study Would you rather? questions, and Acharya"
2020.emnlp-main.48,2020.acl-main.486,1,0.723796,"s well as so10 We use the MediaBias/FactCheck ratings: https:// mediabiasfactcheck.com. cial and moral dynamics in language (Van Hee et al., 2015). Commonly used for coarse-grained analyses of morality in text (Fulgoni et al., 2016; Volkova et al., 2017; Weber et al., 2018), Graham et al. (2009) introduce the Moral Foundations lexicon, a dictionary of morality-evoking words (later extended by Rezapour et al., 2019). A recent line of work focused on representing social implications of everyday situations in freeform text in a knowledge graph (Rashkin et al., 2018; Sap et al., 2019). Relatedly, Sap et al. (2020) introduce Social Bias Frames, a hybrid free-text and categorical formalism to reason about biased implications in language. In contrast, our work formalizes a new type of reasoning around expectations of social norms evoked by situations. Finally, concurrent works have developed rich and exciting resources studying similar phenomena. Tay et al. (2020) study Would you rather? questions, and Acharya et al. (2020) investigate ritual understanding across cultures. Hendrycks et al. (2020) study ethical questions, attempting to assign a real-valued utility to scenarios across a range of ethical cat"
2020.emnlp-main.48,P17-2102,0,0.0269639,"from Nørregaard et al. (2019), a large corpus of political headlines from 2018 paired with news source ratings of political leaning (5-point scale from left- to right-leaning) and factual reliability (5-point scale from least reliable to most reliable).10 Table 4 shows the correlations between RoT attributes and the political leaning and reliability of sources. Our results strongly corroborate findings by Graham et al. (2009), showing that liberal headlines evoke more “fairness” and “care,” while rightleaning headlines evoke more “sanctity” and “loyalty.” Furthermore, in line with findings by Volkova et al. (2017), more reliable news source tend to evoke more advice and less morality. 7 Related Work Our formalism heavily draws from works in descriptive ethics and social psychology, but is also inspired by studies in social implicatures and cooperative principles in pragmatics (Kallia, 2004; Grice, 1975) and the theories of situationally-rooted evocation of frames (Fillmore and Baker, 2001). Our work adds to the growing literature concerned with distilling reactions to situations (Vu et al., 2014; Ding and Riloff, 2016) as well as so10 We use the MediaBias/FactCheck ratings: https:// mediabiasfactcheck."
2020.emnlp-main.48,E14-4025,0,0.0118897,"hile rightleaning headlines evoke more “sanctity” and “loyalty.” Furthermore, in line with findings by Volkova et al. (2017), more reliable news source tend to evoke more advice and less morality. 7 Related Work Our formalism heavily draws from works in descriptive ethics and social psychology, but is also inspired by studies in social implicatures and cooperative principles in pragmatics (Kallia, 2004; Grice, 1975) and the theories of situationally-rooted evocation of frames (Fillmore and Baker, 2001). Our work adds to the growing literature concerned with distilling reactions to situations (Vu et al., 2014; Ding and Riloff, 2016) as well as so10 We use the MediaBias/FactCheck ratings: https:// mediabiasfactcheck.com. cial and moral dynamics in language (Van Hee et al., 2015). Commonly used for coarse-grained analyses of morality in text (Fulgoni et al., 2016; Volkova et al., 2017; Weber et al., 2018), Graham et al. (2009) introduce the Moral Foundations lexicon, a dictionary of morality-evoking words (later extended by Rezapour et al., 2019). A recent line of work focused on representing social implications of everyday situations in freeform text in a knowledge graph (Rashkin et al., 2018; Sap"
2020.emnlp-main.48,N18-2049,0,0.0491832,"Missing"
2020.emnlp-main.58,W17-4912,0,0.024656,"original, auto-encoding objectives can ensure the correct information is captured (Bao et al., 2019; Baziotis et al., 2019; Artetxe et al., 2017). This work tackles problems where generation is more open-ended. Rather than reproducing information from the prompt, generations should agree with and expand on it, making autoencoding less applicable. Controllable language generation. Earlier approaches for controllable generation involved preserving the content of text while changing it along discrete dimensions, such as theme, sentiment, or style (Koncel-Kedziorski et al., 2016; Hu et al., 2017; Ficler and Goldberg, 2017; Shen et al., 2017; Lample et al., 2019). Recent works such as Grover (Zellers et al., 2019) and CTRL model (Keskar et al., 2019) used these ideas to augment transformer language models that can condition on structured metadata such as source, domain, etc. The Plug & Play model (PPLM; Dathathri et al., 2019) controls topic and sentiment in an approach similar to ours that involves forward and backward passes to update token distributions. However, PPLM relies on trained attribute discriminators for supervision, while our method is unsupervised. While these models are restricted to specific di"
2020.emnlp-main.58,P19-1602,0,0.0274374,"ounterfactual condition that “Tara ordered a shirt online” (as opposed to the original “went to mall”), the rewritten ending is about “sent shirt” to Tara (as opposed to the original “browsed from stores”). The last sentence of the original ending “She looked forward to wearing it” is correctly preserved as it is coherent with the counterfactual condition. 6 Related Work Unsupervised text generation. Unsupervised approaches are often applied to problems that copy information from a source text into decoded text. Unsupervised paraphrasing requires repeating this information (Miao et al., 2019; Bao et al., 2019), as does translation, but with a bilingual transformation (Artetxe et al., 2017; Lample et al., 2018). In summarization there is an additional task to select a subset of the original text (Baziotis et al., 2019; Schumann et al., 2020; West et al., 2019). In cases where information is mostly copied from the original, auto-encoding objectives can ensure the correct information is captured (Bao et al., 2019; Baziotis et al., 2019; Artetxe et al., 2017). This work tackles problems where generation is more open-ended. Rather than reproducing information from the prompt, generations should agree wi"
2020.emnlp-main.58,P18-1082,0,0.0300784,"ns in the sampled text to get complete sentences. ROUGE-L Table 1: Automatic evaluation results on the abductive task, using the test set of ART. We then mix the nth-step forward and backward logits to get the final logits of iteration t: y˜n(t) BLEU-4 Ranking We rank candidates by the overall coherence after inserting Y in between X and Z: ranking score(Y ) = c(XY, Z) + c(X, Y Z). (6) Hyperparameters We use GPT2-345M (Radford et al., 2019b) as the pre-trained LM for all models. We use the ART development set to select hyperparameters. We use greedy decoding for our method and top k decoding (Fan et al., 2018) (k = 40, τ = 0.7) for our baselines. Other hyperparameters are outlined in Appendix A.1. where c(·, ·) denotes the coherence score. This score is used to evaluate the quality of a given candidate continuation Y by measuring (1) its compatibility with the subsequent text of the context X, (2) the internal consistency of Y if it consists of multiple sentences, and (3) the compatibility of Y with its right-side text when it is applicable. 798 4.2 Experimental Setup Baselines We compare our method against baselines from Bhagavatula et al. (2019). The unsupervised baselines use a pre-trained GPT-2"
2020.emnlp-main.58,P19-1470,1,0.925279,"was excited to see him in person. Figure 3: Examples of generated hypotheses on three abductive reasoning cases. Given observations O1 and O2, D ELOREAN generates a hypothesis explaining the observations. to generate Y given a prompt text—either the observation X alone (Zero-ShotX ) or ZheiX (ZeroShotZX ), where hei denotes a special end-of-text token. The supervised method (Sup) follows the same input format as Zero-ShotZX , but finetunes GPT-2 on the ART training set. Finally, our knowledge-informed baseline (+COMET-Emb) further augments the representation of Sup with knowledge from COMET (Bosselut et al., 2019). To separately study the contribution of our decoding strategy and ranking component, we also report the performance of ranking the baseline outputs. Specifically, we let each baseline generate 20 candidates and rank them by coherence (Eq. 6).4 4.3 Model Human Evaluation We conduct two sets of human evaluations on 100 test examples using crowdworkers from Amazon Mechanical Turk. In the scoring setting, presented in Table 2, workers were presented a pair of observations (X and Z) and a generated hypothesis Y , and asked to rate the coherence of the hypothesis with respect to the observation X"
2020.emnlp-main.58,D19-1243,1,0.885382,"Missing"
2020.emnlp-main.58,D15-1075,0,0.0210203,"outputs produced by our model are judged as more coherent than those from the supervised models. In sum, our study shows that backpropagation-based decoding may enable additional future applications of unsupervised generation and reasoning. 2 “Lannister” in S30 and “Stark” in S40 and S50 refer to character names in the TV show, “Game of the Thrones.” All the output text shown in Figure 1 is the actual system output from D E L OREAN. 795 2 Background Most NLP benchmarks have focused on reasoning about information that is entailed from the premise. For instance, natural language inference (NLI; Bowman et al., 2015) focuses primarily on whether a hypothesis is entailed from a given premise, which means the information stated in the hypothesis is a subset of the information provided in the premise. However, it has been noted that human reasoning is often the other way, where hypotheses often contain new information that was not available in the premise, but plausibly true (but y˜ 1 y˜ 2 Initialization … y˜ N Generation Y Output: She hit the rope and the tire fell on top of her. LM x1 x2 … xNX Repeat T times y˜ b1 y˜ b2 y˜ 1 y˜ 2 y˜ f1 y˜ f2 … … … y˜ bN y˜ N y˜ fN Backpropagation Computing Loss Ray ran to"
2020.emnlp-main.58,P08-1090,0,0.193297,"specified generation length (Zeldes et al., 2020, which is not publicly available). Reasoning about narratives. A prominent resource from recent years is the RocStories corpus (Mostafazadeh et al., 2016b), consisting of 98K crowdsourced 5-sentence everyday life stories. It was used for the story cloze task whose goal was to predict the story ending from its first 4 sentences, but gained popularity and became the base of additional benchmarks (Rashkin et al., 2018). Additional related work includes “script knowledge”, i.e. learning about prototypical series of events (Schank and Abelson, 1977; Chambers and Jurafsky, 2008; Pichotta and Mooney, 2014), temporal commonsense (Granroth-Wilding and Clark, 2016; Li et al., 2018), and modeling pre- and post- conditions of events (Roemmele et al., 2011; Sap et al., 2019; Bosselut et al., 2019). Qin et al. (2019b) studied conversation modeling that reads and connects the dots of events in related documents. Finally, a recent line of work explores counterfactual questions in reading comprehension (Huang et al., 2019; Tandon et al., 2019), but instantiates the problem of counterfactual reasoning as a multiple choice task. 7 Conclusion We presented D ELOREAN, an unsupervis"
2020.emnlp-main.58,D16-1168,0,0.0128894,"ases where information is mostly copied from the original, auto-encoding objectives can ensure the correct information is captured (Bao et al., 2019; Baziotis et al., 2019; Artetxe et al., 2017). This work tackles problems where generation is more open-ended. Rather than reproducing information from the prompt, generations should agree with and expand on it, making autoencoding less applicable. Controllable language generation. Earlier approaches for controllable generation involved preserving the content of text while changing it along discrete dimensions, such as theme, sentiment, or style (Koncel-Kedziorski et al., 2016; Hu et al., 2017; Ficler and Goldberg, 2017; Shen et al., 2017; Lample et al., 2019). Recent works such as Grover (Zellers et al., 2019) and CTRL model (Keskar et al., 2019) used these ideas to augment transformer language models that can condition on structured metadata such as source, domain, etc. The Plug & Play model (PPLM; Dathathri et al., 2019) controls topic and sentiment in an approach similar to ours that involves forward and backward passes to update token distributions. However, PPLM relies on trained attribute discriminators for supervision, while our method is unsupervised. Whil"
2020.emnlp-main.58,J82-2005,0,0.656271,"Missing"
2020.emnlp-main.58,P18-1169,0,0.0217929,"(Isard, 1974; GinsOutput: She hit the rope and the tire fell on top of her. 796 y˜ y˜ N 3 Generation 1 2 b N Backprop b 3 b 1Y b 2 … 2 on top of Repeat 3 her.… N Output: She hit the rope and1the tire fell Ray T times … Computing Los berg, 1986), it requires causal reasoning abilities, which are arguably absent from current associationbased AI (Pearl and Mackenzie, 2018). While there has been work on counterfactual reasoning in NLP, including recognizing counterfactuals in text (Son et al., 2017), and improving the performance of NLP tasks using counterfactual learning (Lawrence et al., 2017; Lawrence and Riezler, 2018), it remains a major research challenge. Recently, Qin et al. (2019a) introduce the task of counterfactual story generation. Given a 5-sentence original story, and an alternative context in which the second sentence of the story was altered by a counterfactual, the task is to generate a new 3sentence story ending that addresses the alternative beginning while minimally editing the original ending. The associated T IME T RAVEL dataset is based on fictional narratives from ROCStories, for which counterfactual contexts and alternative endings are crowdsourced, yielding 29,849 problem instances. Q"
2020.emnlp-main.58,D17-1272,0,0.0177427,"tant role in AI systems (Isard, 1974; GinsOutput: She hit the rope and the tire fell on top of her. 796 y˜ y˜ N 3 Generation 1 2 b N Backprop b 3 b 1Y b 2 … 2 on top of Repeat 3 her.… N Output: She hit the rope and1the tire fell Ray T times … Computing Los berg, 1986), it requires causal reasoning abilities, which are arguably absent from current associationbased AI (Pearl and Mackenzie, 2018). While there has been work on counterfactual reasoning in NLP, including recognizing counterfactuals in text (Son et al., 2017), and improving the performance of NLP tasks using counterfactual learning (Lawrence et al., 2017; Lawrence and Riezler, 2018), it remains a major research challenge. Recently, Qin et al. (2019a) introduce the task of counterfactual story generation. Given a 5-sentence original story, and an alternative context in which the second sentence of the story was altered by a counterfactual, the task is to generate a new 3sentence story ending that addresses the alternative beginning while minimally editing the original ending. The associated T IME T RAVEL dataset is based on fictional narratives from ROCStories, for which counterfactual contexts and alternative endings are crowdsourced, yieldin"
2020.emnlp-main.58,N16-1098,0,0.388139,"ith pre-defined values, our model can adjust to any open-ended textual constraint. Perhaps the most similar work in that aspect is the “text infilling” models, which, however, are in a more narrow setting by filling only a relatively short text span (Devlin et al., 2018; Zhu et al., 2019; Donahue et al., 2020), and more restrictive due to the reliance on an extra right-to-left language model (Sun et al., 2017) or a pre-specified generation length (Zeldes et al., 2020, which is not publicly available). Reasoning about narratives. A prominent resource from recent years is the RocStories corpus (Mostafazadeh et al., 2016b), consisting of 98K crowdsourced 5-sentence everyday life stories. It was used for the story cloze task whose goal was to predict the story ending from its first 4 sentences, but gained popularity and became the base of additional benchmarks (Rashkin et al., 2018). Additional related work includes “script knowledge”, i.e. learning about prototypical series of events (Schank and Abelson, 1977; Chambers and Jurafsky, 2008; Pichotta and Mooney, 2014), temporal commonsense (Granroth-Wilding and Clark, 2016; Li et al., 2018), and modeling pre- and post- conditions of events (Roemmele et al., 2011"
2020.emnlp-main.58,P02-1040,0,0.115019,"Missing"
2020.emnlp-main.58,E14-1024,0,0.0468308,"Zeldes et al., 2020, which is not publicly available). Reasoning about narratives. A prominent resource from recent years is the RocStories corpus (Mostafazadeh et al., 2016b), consisting of 98K crowdsourced 5-sentence everyday life stories. It was used for the story cloze task whose goal was to predict the story ending from its first 4 sentences, but gained popularity and became the base of additional benchmarks (Rashkin et al., 2018). Additional related work includes “script knowledge”, i.e. learning about prototypical series of events (Schank and Abelson, 1977; Chambers and Jurafsky, 2008; Pichotta and Mooney, 2014), temporal commonsense (Granroth-Wilding and Clark, 2016; Li et al., 2018), and modeling pre- and post- conditions of events (Roemmele et al., 2011; Sap et al., 2019; Bosselut et al., 2019). Qin et al. (2019b) studied conversation modeling that reads and connects the dots of events in related documents. Finally, a recent line of work explores counterfactual questions in reading comprehension (Huang et al., 2019; Tandon et al., 2019), but instantiates the problem of counterfactual reasoning as a multiple choice task. 7 Conclusion We presented D ELOREAN, an unsupervised LMbased approach to gener"
2020.emnlp-main.58,D19-1509,1,0.868796,"al reasoning). Such nonmonotonic reasoning requires 1 Code is available at https://github.com/ qkaren/unsup_gen_for_cms_reasoning Figure 1: D ELOREAN, our proposed method, with generated reasoning results. Top: the goal in abductive reasoning is to generate a hypothesis (Y ) of what happened between the observed past (X) and future (Z) contexts. Bottom: In counterfactual reasoning, given a story context altered by a counterfactual condition, X, and the original ending Z, the goal is to generate a new ending Y which is coherent with X while remaining similar to Z. The story from T IME T RAVEL (Qin et al., 2019a) consists of five sentences. Our approach alternates forward (left-to-right) and backward (rightto-left) passes that iteratively refine the generated texts w.r.t context from each side. inferring plausible but potentially defeasible conclusions from incomplete or hypothetical observations (Reiter, 1988). While humans are remarkably good at this type of causal reasoning, developing AI systems capable of nonmonotonic reasoning for 794 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 794–805, c November 16–20, 2020. 2020 Association for Computational"
2020.emnlp-main.58,P19-1539,1,0.786993,"al reasoning). Such nonmonotonic reasoning requires 1 Code is available at https://github.com/ qkaren/unsup_gen_for_cms_reasoning Figure 1: D ELOREAN, our proposed method, with generated reasoning results. Top: the goal in abductive reasoning is to generate a hypothesis (Y ) of what happened between the observed past (X) and future (Z) contexts. Bottom: In counterfactual reasoning, given a story context altered by a counterfactual condition, X, and the original ending Z, the goal is to generate a new ending Y which is coherent with X while remaining similar to Z. The story from T IME T RAVEL (Qin et al., 2019a) consists of five sentences. Our approach alternates forward (left-to-right) and backward (rightto-left) passes that iteratively refine the generated texts w.r.t context from each side. inferring plausible but potentially defeasible conclusions from incomplete or hypothetical observations (Reiter, 1988). While humans are remarkably good at this type of causal reasoning, developing AI systems capable of nonmonotonic reasoning for 794 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 794–805, c November 16–20, 2020. 2020 Association for Computational"
2020.emnlp-main.58,P18-1213,1,0.8585,"; Zhu et al., 2019; Donahue et al., 2020), and more restrictive due to the reliance on an extra right-to-left language model (Sun et al., 2017) or a pre-specified generation length (Zeldes et al., 2020, which is not publicly available). Reasoning about narratives. A prominent resource from recent years is the RocStories corpus (Mostafazadeh et al., 2016b), consisting of 98K crowdsourced 5-sentence everyday life stories. It was used for the story cloze task whose goal was to predict the story ending from its first 4 sentences, but gained popularity and became the base of additional benchmarks (Rashkin et al., 2018). Additional related work includes “script knowledge”, i.e. learning about prototypical series of events (Schank and Abelson, 1977; Chambers and Jurafsky, 2008; Pichotta and Mooney, 2014), temporal commonsense (Granroth-Wilding and Clark, 2016; Li et al., 2018), and modeling pre- and post- conditions of events (Roemmele et al., 2011; Sap et al., 2019; Bosselut et al., 2019). Qin et al. (2019b) studied conversation modeling that reads and connects the dots of events in related documents. Finally, a recent line of work explores counterfactual questions in reading comprehension (Huang et al., 201"
2020.emnlp-main.58,2020.acl-main.452,0,0.0250368,"e original ending “She looked forward to wearing it” is correctly preserved as it is coherent with the counterfactual condition. 6 Related Work Unsupervised text generation. Unsupervised approaches are often applied to problems that copy information from a source text into decoded text. Unsupervised paraphrasing requires repeating this information (Miao et al., 2019; Bao et al., 2019), as does translation, but with a bilingual transformation (Artetxe et al., 2017; Lample et al., 2018). In summarization there is an additional task to select a subset of the original text (Baziotis et al., 2019; Schumann et al., 2020; West et al., 2019). In cases where information is mostly copied from the original, auto-encoding objectives can ensure the correct information is captured (Bao et al., 2019; Baziotis et al., 2019; Artetxe et al., 2017). This work tackles problems where generation is more open-ended. Rather than reproducing information from the prompt, generations should agree with and expand on it, making autoencoding less applicable. Controllable language generation. Earlier approaches for controllable generation involved preserving the content of text while changing it along discrete dimensions, such as th"
2020.emnlp-main.58,P17-2103,0,0.02648,"While counterfactual reasoning plays Recently, Bhagavatula et al. (2019) propose the an important role in AI systems (Isard, 1974; GinsOutput: She hit the rope and the tire fell on top of her. 796 y˜ y˜ N 3 Generation 1 2 b N Backprop b 3 b 1Y b 2 … 2 on top of Repeat 3 her.… N Output: She hit the rope and1the tire fell Ray T times … Computing Los berg, 1986), it requires causal reasoning abilities, which are arguably absent from current associationbased AI (Pearl and Mackenzie, 2018). While there has been work on counterfactual reasoning in NLP, including recognizing counterfactuals in text (Son et al., 2017), and improving the performance of NLP tasks using counterfactual learning (Lawrence et al., 2017; Lawrence and Riezler, 2018), it remains a major research challenge. Recently, Qin et al. (2019a) introduce the task of counterfactual story generation. Given a 5-sentence original story, and an alternative context in which the second sentence of the story was altered by a counterfactual, the task is to generate a new 3sentence story ending that addresses the alternative beginning while minimally editing the original ending. The associated T IME T RAVEL dataset is based on fictional narratives fro"
2020.emnlp-main.58,D19-1629,1,0.826739,"dditional related work includes “script knowledge”, i.e. learning about prototypical series of events (Schank and Abelson, 1977; Chambers and Jurafsky, 2008; Pichotta and Mooney, 2014), temporal commonsense (Granroth-Wilding and Clark, 2016; Li et al., 2018), and modeling pre- and post- conditions of events (Roemmele et al., 2011; Sap et al., 2019; Bosselut et al., 2019). Qin et al. (2019b) studied conversation modeling that reads and connects the dots of events in related documents. Finally, a recent line of work explores counterfactual questions in reading comprehension (Huang et al., 2019; Tandon et al., 2019), but instantiates the problem of counterfactual reasoning as a multiple choice task. 7 Conclusion We presented D ELOREAN, an unsupervised LMbased approach to generate text conditioned on past context as well as future constraints, through forward and backward passes considering each condition. We demonstrated its effectiveness for abductive and counterfactual reasoning, on which it performed substantially better than unsupervised baselines. Our method is general and can be easily adapted for other generative reasoning tasks. 802 Acknowledgements Jacob Devlin, Ming-Wei Chang, Kenton Lee, and K"
2020.emnlp-main.602,D16-1168,0,0.0200452,"ut-of-domain paraphrasing objective. While a majority of related existing stylistic rewriting work defines style as sentiment (e.g., on reviews), a notable exception is Nogueira dos Santos et al. (2018), who use stylistic rewriting to make text less hateful or offensive. Similar in spirit, Controllable Debiasing is a novel formalization that aims to address and revise social biases expressed in text, but using the nuanced implications distilled in connotation frames of power and agency instead of binary offensiveness. Our work also draws inspiration from controllable generation methods (e.g., Koncel-Kedziorski et al., 2016; Hu et al., 2017; Ficler and Goldberg, 2017). While those methods steer the generation output to contain desired attributes, controllable revision is constrained to revise an input sentence in addition to generating with desired attributes. 7 Conclusion We introduce a new text revision task of Controllable Debiasing, to help debias the portrayal of characters through the lens of connotation frames of power and agency. To this end, we create P OWERT RANSFORMER , a transformer-based encoderdecoder trained on a joint reconstruction and paraphrasing objective. Our approach demonstrates promising"
2020.emnlp-main.602,W17-4912,0,0.0404265,"rity of related existing stylistic rewriting work defines style as sentiment (e.g., on reviews), a notable exception is Nogueira dos Santos et al. (2018), who use stylistic rewriting to make text less hateful or offensive. Similar in spirit, Controllable Debiasing is a novel formalization that aims to address and revise social biases expressed in text, but using the nuanced implications distilled in connotation frames of power and agency instead of binary offensiveness. Our work also draws inspiration from controllable generation methods (e.g., Koncel-Kedziorski et al., 2016; Hu et al., 2017; Ficler and Goldberg, 2017). While those methods steer the generation output to contain desired attributes, controllable revision is constrained to revise an input sentence in addition to generating with desired attributes. 7 Conclusion We introduce a new text revision task of Controllable Debiasing, to help debias the portrayal of characters through the lens of connotation frames of power and agency. To this end, we create P OWERT RANSFORMER , a transformer-based encoderdecoder trained on a joint reconstruction and paraphrasing objective. Our approach demonstrates promising results to revise sentences with targeted pow"
2020.emnlp-main.602,Q18-1027,0,0.0203022,"acters for which the gender could not be inferred. 6 Related Work Controllable Debiasing is a new formalization of the unsupervised stylistic rewriting task, contrasting with supervised approaches which benefit from 7433 parallel corpora (e.g., Xu et al., 2012, 2015; Rao and Tetreault, 2018; Pryzant et al., 2020). In unsupervised settings, a majority of work has dealt with the dearth of parallel data by using encoderdecoder setups paired with discriminators to disentangle style from content and steer generations (e.g., Shen et al., 2017; Zhang et al., 2018; Fu et al., 2018; Yang et al., 2018; Niu and Bansal, 2018; Romanov et al., 2019; Dai et al., 2019; John et al., 2019) or backtranslation setups (Prabhumoye et al., 2018; Lample et al., 2018). In contrast, Li et al. (2018a) introduce a modular approach (later adapted to transformer models by Sudhakar et al., 2019) that relies on drop-in replacement of attribute markers followed by language correction. P OWER T RANSFORMER improves on this approach with an additional out-of-domain paraphrasing objective. While a majority of related existing stylistic rewriting work defines style as sentiment (e.g., on reviews), a notable exception is Nogueira dos Santo"
2020.emnlp-main.602,D14-1162,0,0.0820224,"Missing"
2020.emnlp-main.602,P18-1080,0,0.0639457,"ion of the unsupervised stylistic rewriting task, contrasting with supervised approaches which benefit from 7433 parallel corpora (e.g., Xu et al., 2012, 2015; Rao and Tetreault, 2018; Pryzant et al., 2020). In unsupervised settings, a majority of work has dealt with the dearth of parallel data by using encoderdecoder setups paired with discriminators to disentangle style from content and steer generations (e.g., Shen et al., 2017; Zhang et al., 2018; Fu et al., 2018; Yang et al., 2018; Niu and Bansal, 2018; Romanov et al., 2019; Dai et al., 2019; John et al., 2019) or backtranslation setups (Prabhumoye et al., 2018; Lample et al., 2018). In contrast, Li et al. (2018a) introduce a modular approach (later adapted to transformer models by Sudhakar et al., 2019) that relies on drop-in replacement of attribute markers followed by language correction. P OWER T RANSFORMER improves on this approach with an additional out-of-domain paraphrasing objective. While a majority of related existing stylistic rewriting work defines style as sentiment (e.g., on reviews), a notable exception is Nogueira dos Santos et al. (2018), who use stylistic rewriting to make text less hateful or offensive. Similar in spirit, Control"
2020.emnlp-main.602,P19-1041,0,0.0118346,"ork Controllable Debiasing is a new formalization of the unsupervised stylistic rewriting task, contrasting with supervised approaches which benefit from 7433 parallel corpora (e.g., Xu et al., 2012, 2015; Rao and Tetreault, 2018; Pryzant et al., 2020). In unsupervised settings, a majority of work has dealt with the dearth of parallel data by using encoderdecoder setups paired with discriminators to disentangle style from content and steer generations (e.g., Shen et al., 2017; Zhang et al., 2018; Fu et al., 2018; Yang et al., 2018; Niu and Bansal, 2018; Romanov et al., 2019; Dai et al., 2019; John et al., 2019) or backtranslation setups (Prabhumoye et al., 2018; Lample et al., 2018). In contrast, Li et al. (2018a) introduce a modular approach (later adapted to transformer models by Sudhakar et al., 2019) that relies on drop-in replacement of attribute markers followed by language correction. P OWER T RANSFORMER improves on this approach with an additional out-of-domain paraphrasing objective. While a majority of related existing stylistic rewriting work defines style as sentiment (e.g., on reviews), a notable exception is Nogueira dos Santos et al. (2018), who use stylistic rewriting to make text le"
2020.emnlp-main.602,C16-1275,0,0.0203805,"how example revisions in Table 4 (and Table 6 in the appendix). 4.4.1 Ablated Baselines We first investigate the importance of the reconstruction objective, by comparing our joint objective model (Joint) with a model trained with just the paraphrasing objective (without masking, ParaOnly). Then, to quantify the effect of boosting, we compare models with (Boost) and without (noBoost) agency-specific vocab boosting. Note that ParaOnly+noBoost is equivalent to a GPT-based encoder-decoder model, similar to seq2seq frameworks commonly used in paraphrasing tasks (Cao et al., 2017; Li et al., 2018b; Prakash et al., 2016). As a final comparison, we implement a model variant that more closely mirrors the delete-retrievegenerate paradigm (Li et al., 2018a) by adding a “retrieve” step in which we concatenate transformer input with a verb retrieved from the verb agency lexicon that is most similar to the masked out verb (SupplyVerb).8 8 We retrieve a verb from the Sap et al. (2017) lexicon that has the target agency and is most similar to the masked out Results In Table 2, our results show that the full model (Joint+Boost) yields text revisions with the most accurate target agency and the most meaning preservation"
2020.emnlp-main.602,P17-2074,0,0.0149749,"ther output is gibberish, then, in two questions, choose which revision has better targeted agency and which better preserves the meaning of the original sentence. For consistency, each pair is rated by three judges. To ensure the quality of our evaluations, we selected workers who could reliably distinguish high from low agency sentences in a qualification task (see Figure 6 in 9 To validate our automatic evaluations, we collect human judgments of the controllable revisions Prefer ours We use head-to-head evaluations as those have been shown to be more reliable than scale-rating evaluations (Kiritchenko and Mohammad, 2017). 7431 dir. Input agency (+ → –) (a) After the party I headed home. Model Revised Sentence (out) PPLM after the party my classmate, Kayla and the Tgirls of the Universe. BST please ’s , i have a word of this . P OWERTJoint+N oBoost after the party i stayed home. P OWERTJoint+Boost after the party i stayed home. = PPLM a friend asked me to watch her two year old child for a minute. l didn ’t have a word of this , you ’re . a friend needed me to watch her two year old child for a minute. a friend needed me to watch her two year old child for a minute. + before filling the last question it it it"
2020.emnlp-main.602,P17-1153,0,0.0535713,"Missing"
2020.emnlp-main.602,N18-1012,0,0.181906,"agency levels of female characters, thereby reducing the gender bias. Our findings show promise for using modern NLP tools to help mitigate societal biases in text. We release our preprocessed data and code at http://maartensap.com/controllable-debiasing. 2 Controllable Debiasing Controllable Debiasing is a novel formalization of stylistic rewriting that aims to debias the portrayal of characters through controllable revision. To achieve the desired character portrayal, a system must be able to change the underlying meaning of events, unlike certain formalizations (e.g., politeness transfer; Rao and Tetreault, 2018) where full meaning preservation is required. Without this, systems run the risk of merely paraphrasing the biases in text. However, revisions must be precise and avoid unnecessary meaning changes, which can often occur in stylistic rewriting (e.g., reversing the sentiment of a review drastically changes its underlying meaning). For our new rewriting task of changing portrayal bias, we focus on connotation frames that measure the power and agency ascribed to characters through the actions they take. Connotation frames (Rashkin et al., 2016; Sap et al., 2017) distill implicit relations between"
2020.emnlp-main.602,N18-1138,0,0.0191533,"dividuals mentioned in our data. 12 There were 2597 characters for which the gender could not be inferred. 6 Related Work Controllable Debiasing is a new formalization of the unsupervised stylistic rewriting task, contrasting with supervised approaches which benefit from 7433 parallel corpora (e.g., Xu et al., 2012, 2015; Rao and Tetreault, 2018; Pryzant et al., 2020). In unsupervised settings, a majority of work has dealt with the dearth of parallel data by using encoderdecoder setups paired with discriminators to disentangle style from content and steer generations (e.g., Shen et al., 2017; Zhang et al., 2018; Fu et al., 2018; Yang et al., 2018; Niu and Bansal, 2018; Romanov et al., 2019; Dai et al., 2019; John et al., 2019) or backtranslation setups (Prabhumoye et al., 2018; Lample et al., 2018). In contrast, Li et al. (2018a) introduce a modular approach (later adapted to transformer models by Sudhakar et al., 2019) that relies on drop-in replacement of attribute markers followed by language correction. P OWER T RANSFORMER improves on this approach with an additional out-of-domain paraphrasing objective. While a majority of related existing stylistic rewriting work defines style as sentiment (e."
2020.emnlp-main.602,P16-1030,1,0.827971,"certain formalizations (e.g., politeness transfer; Rao and Tetreault, 2018) where full meaning preservation is required. Without this, systems run the risk of merely paraphrasing the biases in text. However, revisions must be precise and avoid unnecessary meaning changes, which can often occur in stylistic rewriting (e.g., reversing the sentiment of a review drastically changes its underlying meaning). For our new rewriting task of changing portrayal bias, we focus on connotation frames that measure the power and agency ascribed to characters through the actions they take. Connotation frames (Rashkin et al., 2016; Sap et al., 2017) distill implicit relations between a verb, its agent, and its theme. In this work, we use the positive, neutral, and negative agency dimensions, where agency is defined as the capacity to intentionally make changes or act upon one’s environment (Dennett, 1989). For example, illustrated in Figure 1, “X pursued Y” implies that X has positive agency.1 Using machine-in-the-loop writing systems (e.g., Ghazvininejad et al., 2016, 2017; Clark et al., 2018, Textio2 ), models trained on this task could help authors write news, stories, or movies that portray characters in less biase"
2020.emnlp-main.602,N19-1088,0,0.0191275,"ender could not be inferred. 6 Related Work Controllable Debiasing is a new formalization of the unsupervised stylistic rewriting task, contrasting with supervised approaches which benefit from 7433 parallel corpora (e.g., Xu et al., 2012, 2015; Rao and Tetreault, 2018; Pryzant et al., 2020). In unsupervised settings, a majority of work has dealt with the dearth of parallel data by using encoderdecoder setups paired with discriminators to disentangle style from content and steer generations (e.g., Shen et al., 2017; Zhang et al., 2018; Fu et al., 2018; Yang et al., 2018; Niu and Bansal, 2018; Romanov et al., 2019; Dai et al., 2019; John et al., 2019) or backtranslation setups (Prabhumoye et al., 2018; Lample et al., 2018). In contrast, Li et al. (2018a) introduce a modular approach (later adapted to transformer models by Sudhakar et al., 2019) that relies on drop-in replacement of attribute markers followed by language correction. P OWER T RANSFORMER improves on this approach with an additional out-of-domain paraphrasing objective. While a majority of related existing stylistic rewriting work defines style as sentiment (e.g., on reviews), a notable exception is Nogueira dos Santos et al. (2018), who u"
2020.emnlp-main.602,P18-2031,0,0.0323145,"2018; Romanov et al., 2019; Dai et al., 2019; John et al., 2019) or backtranslation setups (Prabhumoye et al., 2018; Lample et al., 2018). In contrast, Li et al. (2018a) introduce a modular approach (later adapted to transformer models by Sudhakar et al., 2019) that relies on drop-in replacement of attribute markers followed by language correction. P OWER T RANSFORMER improves on this approach with an additional out-of-domain paraphrasing objective. While a majority of related existing stylistic rewriting work defines style as sentiment (e.g., on reviews), a notable exception is Nogueira dos Santos et al. (2018), who use stylistic rewriting to make text less hateful or offensive. Similar in spirit, Controllable Debiasing is a novel formalization that aims to address and revise social biases expressed in text, but using the nuanced implications distilled in connotation frames of power and agency instead of binary offensiveness. Our work also draws inspiration from controllable generation methods (e.g., Koncel-Kedziorski et al., 2016; Hu et al., 2017; Ficler and Goldberg, 2017). While those methods steer the generation output to contain desired attributes, controllable revision is constrained to revise"
2020.emnlp-main.602,2020.acl-main.486,1,0.906708,"of stereotypical portrayals in media (Behm-Morawitz and Mastro, 2008; Field et al., 2019). 3 P OWERT RANSFORMER We present a new approach for Controllable Debiasing called P OWERT RANSFORMER, which addresses two key challenges: the paucity of parallel supervised data for training and the difficulty of incorporating fine-grained control for steering the agency of the output. Our approach (Figure 2) jointly learns to reconstruct partially masked story 1 Future work could explore using the power dimension instead of agency, or alternative operationalizations of biases, e.g., Social Bias Frames (Sap et al., 2020) or regard towards minorities as introduced by Sheng et al. (2019). 2 https://textio.com/ 7427 ?Aw Vocab boosting + Issa played football growing up. + boosted logits Joint reconstruction + paraphrase objective at decoding time agency scaling next word logits at training time GPT Transformer Issa &lt;VERB&gt; football growing up. Transformer inputs &lt;POS&gt; masking - Issa enjoyed football growing up. positive Target agency Figure 2: Overview of the full P OWERT RANSFORMER model. An input sentence is masked for verb tokens indicative of agency. Masked inputs and target agency are used as GPT inputs. We u"
2020.emnlp-main.602,D17-1247,1,0.265679,"ling for algorithms that can assist writers with bias correction. For example, a female character in a story is often portrayed as passive and powerless (“She daydreams about being a doctor”) while a man is portrayed as more proactive and powerful (“He pursues his dream of being a doctor”). agency(AG) = low Mey daydreams of being a doctor. agency(AG) = high agency(AG) = high PowerTransformer Ana strutted through the park. PowerTransformer Issa loved playing football growing up. agency(AG) = low Issa enjoyed football growing up. agency(AG) = high Figure 1: Examples of using connotation frames (Sap et al., 2017) for controllable revisions to portray characters with more agency and power. In the second example, “Ana strutted” implies that she is more active and decisive, compared to “Ana wandered” which portrays her as aimless and passive. people are described. For example, automatically rewriting “Mey daydreamed about being a doctor” as “Mey pursued her dream to be a doctor” portrays Mey with more authority and decisiveness (Figure 1). Such controllable revision methods could be used to help reshape how gender roles are portrayed in media (e.g., through machine-in-theloop writing systems; Clark et al"
2020.emnlp-main.746,D15-1075,0,0.585049,"dence) corresponds to easy-to-learn examples, the bottomleft corner (low variability, low confidence) corresponds to hard-to-learn examples, and examples on the right (with high variability) are ambiguous; all definitions are with respect to the RO BERTA-large model. The modal group in the data is formed by the easy-to-learn regions. For clarity we only plot 25K random samples from the SNLI train set. Fig. 8b in App. §C shows the same map in greater relief. The creation of large labeled datasets has fueled the advance of AI (Russakovsky et al., 2015; Antol et al., 2015) and NLP in particular (Bowman et al., 2015; Rajpurkar et al., 2016). The common belief is that the more abundant the labeled data, the higher the likelihood of learning diverse phenomena, which in turn leads to models that generalize well. In practice, however, out-of-distribution Work done at the Allen Institute for AI. ambiguou 0.4 0.2 Introduction ∗ 0.6 correct. 0.0 0.2 0.3 0.5 0.7 0.8 1.0 (OOD) generalization remains a challenge (Yogatama et al., 2019; Linzen, 2020); and, while recent large pretrained language models help, they fail to close this gap (Hendrycks et al., 2020). This urges a closer look at datasets, where not all exa"
2020.emnlp-main.746,P17-1152,0,0.0393322,"scussion (with empirical justifications) on connections between training dynamics measures and dropout-based (Srivastava et al., 2014), first-principles uncertainty estimates. These relations are further supported by previous work, which showed that deep ensembles provide well-calibrated uncertainty estimates (Lakshminarayanan et al., 2017; Gustafsson et al., 2019; Snoek et al., 2019). Generally, such approaches ensemble models trained from scratch; while ensembles of training checkpoints lose some diversity (Fort et al., 2019), they offer a cheaper alternative capturing some of the benefits (Chen et al., 2017a). Future work will involve investigation of such alternatives for building data maps. 7 Related Work Our work builds data maps using training dynamics measures for scoring data instances. Loss landscapes (Xing et al., 2018) are similar to training dynamics, but also consider variables from the stochastic optimization algorithm. Toneva et al. (2018) also use training dynamics to find train examples which are frequently “forgotten”, i.e., mis9282 classified during a later epoch of training, despite being classified correctly earlier; our correctness metric provides similar discrete scores, and"
2020.emnlp-main.746,N19-1423,0,0.0734792,"Missing"
2020.emnlp-main.746,D19-1224,1,0.892059,"Missing"
2020.emnlp-main.746,N18-2017,1,0.861687,"Missing"
2020.emnlp-main.746,2020.acl-main.244,0,0.0262881,"Missing"
2020.emnlp-main.746,N13-1132,0,0.0221471,"e ability of simple linear classifiers to predict them correctly. While AFLite, among others (Li and Vasconcelos, 2019; Gururangan et al., 2018), advocate removing “easy” instances from the dataset, our work shows that easy-to-learn instances can be useful. Similar intuitions have guided other work such as curriculum learning (Bengio et al., 2009) and self-paced learning (Kumar et al., 2010; Lee and Grauman, 2011) where all examples are prioritized based on their “difficulty”. Other approaches have used training loss (Han et al., 2018; Arazo et al., 2019; Shen and Sanghavi, 2019), confidence (Hovy et al., 2013), and meta-learning (Ren et al., 2018), to differentiate instances within datasets. Perhaps our measures are the closest to those from Chang et al. (2017); they propose prediction variance and threshold closeness—which correspond to variability and confidence, respectively.18 However, they use these measures to reweight all instances, similar to sampling effective batches in online learning (Loshchilov and Hutter, 2016). Our work, instead, does a hard selection for the purpose of studying different groups within data. Our methods are also reminiscent of active learning methods (Settles, 2009;"
2020.emnlp-main.746,D17-1215,0,0.127475,"Missing"
2020.emnlp-main.746,W02-2015,0,0.109712,"oring data instances. Loss landscapes (Xing et al., 2018) are similar to training dynamics, but also consider variables from the stochastic optimization algorithm. Toneva et al. (2018) also use training dynamics to find train examples which are frequently “forgotten”, i.e., mis9282 classified during a later epoch of training, despite being classified correctly earlier; our correctness metric provides similar discrete scores, and results in models with better performance. Variants of such approaches address catastrophic forgetting, and are useful for analyzing data instances (Pan et al., 2020; Krymolowski, 2002). Prior work has proposed other criteria to score instances. AFLite (LeBras et al., 2020) is an adversarial filtering algorithm which ranks instances based on their “predictability”, i.e. the ability of simple linear classifiers to predict them correctly. While AFLite, among others (Li and Vasconcelos, 2019; Gururangan et al., 2018), advocate removing “easy” instances from the dataset, our work shows that easy-to-learn instances can be useful. Similar intuitions have guided other work such as curriculum learning (Bengio et al., 2009) and self-paced learning (Kumar et al., 2010; Lee and Grauman"
2020.emnlp-main.746,2020.acl-main.465,0,0.0320741,"map in greater relief. The creation of large labeled datasets has fueled the advance of AI (Russakovsky et al., 2015; Antol et al., 2015) and NLP in particular (Bowman et al., 2015; Rajpurkar et al., 2016). The common belief is that the more abundant the labeled data, the higher the likelihood of learning diverse phenomena, which in turn leads to models that generalize well. In practice, however, out-of-distribution Work done at the Allen Institute for AI. ambiguou 0.4 0.2 Introduction ∗ 0.6 correct. 0.0 0.2 0.3 0.5 0.7 0.8 1.0 (OOD) generalization remains a challenge (Yogatama et al., 2019; Linzen, 2020); and, while recent large pretrained language models help, they fail to close this gap (Hendrycks et al., 2020). This urges a closer look at datasets, where not all examples might contribute equally towards learning (Vodrahalli et al., 2018). However, the scale of data can make this assessment challenging. How can we automatically characterize data instances with respect to their role in achieving good performance in- and out-of- distribution? Answering this question may take us a step closer to bridging the gap between dataset collection and broader task 9275 Proceedings of the 2020 Conferenc"
2020.emnlp-main.746,2021.ccl-1.108,0,0.106443,"Missing"
2020.emnlp-main.746,N19-1262,0,0.0341628,"Missing"
2020.emnlp-main.746,N18-1101,0,0.32608,"constructed using the RO BERTA-large model (Liu et al., 2019). The map reveals three distinct regions in the dataset: a region with instances whose true class probabilities fluctuate frequently during training (high variability), and are hence ambiguous for the model; a region with easy-to-learn instances that the model predicts correctly and consistently (high confidence, low variability); and a region with hard-to-learn instances with low confidence, low variability, many of which we find are mislabeled during annotation .1 Similar regions are observed across three other datasets: MultiNLI (Williams et al., 2018), WinoGrande (Sakaguchi et al., 2020) and SQuAD (Rajpurkar et al., 2016), with respect to respective RO BERTA-large classifiers. We further investigate the above regions by training models exclusively on examples from each region (§3). Training on ambiguous instances promotes generalization to OOD test sets, with little or no effect on in-distribution (ID) performance.2 Our data maps also reveal that datasets contain a majority of easy-to-learn instances, which are not as critical for ID or OOD performance, but without any such instances, training could fail to converge (§4). In §5, we show th"
2020.emnlp-main.746,W18-5446,0,0.0616166,"Missing"
2020.emnlp-main.746,N13-1086,0,0.0146549,", 2016). Our work, instead, does a hard selection for the purpose of studying different groups within data. Our methods are also reminiscent of active learning methods (Settles, 2009; Peris and Casacuberta, 2018; P.V.S and Meyer, 2019), such as uncertainty sampling (Lewis and Gale, 1994) which selects (unlabeled) data points, which a model trained on a small labeled subset, has least confidence in, or predicts as farthest (in vector space, based on cosine similarity) (Sener and Savarese, 2018; Wolf, 2011). Our approach uses labeled data for selection, similar to core-set selection approaches (Wei et al., 2013). Active learning approaches could be used in conjunction with data maps to create better datasets, similar to approaches proposed in Mishra et al. (2020). For instance, creating datasets 18 They also consider confidence intervals; our preliminary experiments, with and without, yielded similar results. with more ambiguous examples (with respect to a given model) could make it beneficial for OOD generalization. Data error detection also involves instance scoring. Influence functions (Koh and Liang, 2017), forgetting events (Toneva et al., 2018), cross validation (Chen et al., 2019), Shapely val"
2020.findings-emnlp.165,W19-2008,0,0.0758442,"Missing"
2020.findings-emnlp.165,P19-1294,0,0.0165422,". We see C OMMON G EN as a novel, complementary commonsense reasoning benchmark task for advancing machine commonsense in NLG. Constrained Text Generation. Constrained text generation aims to decode sentences with expected attributes such as sentiment (Luo et al., 2019a; Hu et al., 2017), tense (Hu et al., 2017), template (Zhu et al., 2019; J Kurisinkel and Chen, 2019), style (Fu et al., 2018; Luo et al., 2019b; Li et al., 2018), topics (Feng et al., 2018), etc. Two related scenarios with our task is lexically constrained decoding and word ordering (Zhang and Clark, 2015; Hasler et al., 2018; Dinu et al., 2019; Hokamp and Liu, 2017; Puduppully et al., 2017; Miao et al., 2019). However, they are not easily adopted by the recent pre-trained language models and thus not directly useful for our task. Topical story generation (Fan et al., 2018; Yao et al., 2019) is also a related direction, while it targets generating longer, creative stories around the given topics, making it hard to directly adopt them to our task. Additionally, the 1830 C OMMON G EN task brings some more challenges mentioned in Section 2. Prior constrained generation methods cannot address these issues together in a unified model. In"
2020.findings-emnlp.165,P18-1082,0,0.0386358,"such as sentiment (Luo et al., 2019a; Hu et al., 2017), tense (Hu et al., 2017), template (Zhu et al., 2019; J Kurisinkel and Chen, 2019), style (Fu et al., 2018; Luo et al., 2019b; Li et al., 2018), topics (Feng et al., 2018), etc. Two related scenarios with our task is lexically constrained decoding and word ordering (Zhang and Clark, 2015; Hasler et al., 2018; Dinu et al., 2019; Hokamp and Liu, 2017; Puduppully et al., 2017; Miao et al., 2019). However, they are not easily adopted by the recent pre-trained language models and thus not directly useful for our task. Topical story generation (Fan et al., 2018; Yao et al., 2019) is also a related direction, while it targets generating longer, creative stories around the given topics, making it hard to directly adopt them to our task. Additionally, the 1830 C OMMON G EN task brings some more challenges mentioned in Section 2. Prior constrained generation methods cannot address these issues together in a unified model. Incorporating Commonsense for NLG. There are a few recent works that incorporate commonsense knowledge in language generation tasks such as essay generation (Guan et al., 2019; Yang et al., 2019a), image captioning (Lu et al., 2018), v"
2020.findings-emnlp.165,P16-1154,0,0.240259,"onnections among conceptpairs in the dev and test examples in Fig. 8. To better summarize the distributions, we categorize these relations into five major types and present their distribution in Table 2, respectively for one/two-hop connections between concept pairs. 4 Methods We briefly introduce the baseline methods that are tested on the C OMMON G EN task. Encoder-Decoder Models. Bidirectional RNNs and Transformers (Vaswani et al., 2017) are two most popular architectures for seq2seq learning. We use them with the addition of attention mecha2 nism (Luong et al., 2015) with copying ability (Gu et al., 2016), which are based on an open-source framework OpenNMT-py (Klein et al., 2017). We use bRNN-CopyNet and Trans-CopyNet denote them respectively. To alleviate the influence from the concept ordering in such sequential learning methods, we randomly permute them multiple times for training and decoding and then get their average performance. To explicitly eliminate the order-sensitivity of inputs, we replace the encoder with a mean pooling-based MLP network (MeanPooling-CopyNet). Recent adNon-autoregressive generation. vances (Lee et al., 2018; Stern et al., 2019) in conditional sentence generation"
2020.findings-emnlp.165,P17-4012,0,0.0313062,"etter summarize the distributions, we categorize these relations into five major types and present their distribution in Table 2, respectively for one/two-hop connections between concept pairs. 4 Methods We briefly introduce the baseline methods that are tested on the C OMMON G EN task. Encoder-Decoder Models. Bidirectional RNNs and Transformers (Vaswani et al., 2017) are two most popular architectures for seq2seq learning. We use them with the addition of attention mecha2 nism (Luong et al., 2015) with copying ability (Gu et al., 2016), which are based on an open-source framework OpenNMT-py (Klein et al., 2017). We use bRNN-CopyNet and Trans-CopyNet denote them respectively. To alleviate the influence from the concept ordering in such sequential learning methods, we randomly permute them multiple times for training and decoding and then get their average performance. To explicitly eliminate the order-sensitivity of inputs, we replace the encoder with a mean pooling-based MLP network (MeanPooling-CopyNet). Recent adNon-autoregressive generation. vances (Lee et al., 2018; Stern et al., 2019) in conditional sentence generation have an emerging interest on (edit-based) non-autoregressive generation mode"
2020.findings-emnlp.165,W04-1013,0,0.0731965,"with a lexically-constrained decoding method, dynamic beam allocation (DBA) (Post and Vilar, 2018), which do not show improvement over con4 ventional beam searching. 5 Evaluation We first introduce the automatic evaluation metrics, then present main experimental results with manual analysis, and finally introduce the potential application in transferring CommonGen-trained models for other downstream tasks. 5.1 Metrics Following other conventional generation tasks, we use several widely-used automatic metrics to automatically assess the performance, such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005), which mainly focus on measuring surface similarities. We report the concept Coverage, which is the average percentage of input concepts that are present in lemmatizatized outputs. In addition, we argue that it is more suitable to use evaluation metrics specially design for caption4 The used hyper-parameters are reported in the appendix. ing task, such as CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al., 2016). They usually assume system generations and human references use similar concepts, and thus focus on evaluate the associations between mention"
2020.findings-emnlp.165,P19-1194,0,0.0158507,"scene comprehension (Zellers et al., 2019a), and general commonsense question answering (Talmor et al., 2019; Huang et al., 2019; Wang et al., 2019a, 2020). However, the success of fine-tuning pre-trained language models for these tasks does not necessarily mean machines can produce novel assumptions in a more open, realistic, generative setting. We see C OMMON G EN as a novel, complementary commonsense reasoning benchmark task for advancing machine commonsense in NLG. Constrained Text Generation. Constrained text generation aims to decode sentences with expected attributes such as sentiment (Luo et al., 2019a; Hu et al., 2017), tense (Hu et al., 2017), template (Zhu et al., 2019; J Kurisinkel and Chen, 2019), style (Fu et al., 2018; Luo et al., 2019b; Li et al., 2018), topics (Feng et al., 2018), etc. Two related scenarios with our task is lexically constrained decoding and word ordering (Zhang and Clark, 2015; Hasler et al., 2018; Dinu et al., 2019; Hokamp and Liu, 2017; Puduppully et al., 2017; Miao et al., 2019). However, they are not easily adopted by the recent pre-trained language models and thus not directly useful for our task. Topical story generation (Fan et al., 2018; Yao et al., 2019)"
2020.findings-emnlp.165,D15-1166,0,0.0159945,"ferent relation types of the one/two-hop connections among conceptpairs in the dev and test examples in Fig. 8. To better summarize the distributions, we categorize these relations into five major types and present their distribution in Table 2, respectively for one/two-hop connections between concept pairs. 4 Methods We briefly introduce the baseline methods that are tested on the C OMMON G EN task. Encoder-Decoder Models. Bidirectional RNNs and Transformers (Vaswani et al., 2017) are two most popular architectures for seq2seq learning. We use them with the addition of attention mecha2 nism (Luong et al., 2015) with copying ability (Gu et al., 2016), which are based on an open-source framework OpenNMT-py (Klein et al., 2017). We use bRNN-CopyNet and Trans-CopyNet denote them respectively. To alleviate the influence from the concept ordering in such sequential learning methods, we randomly permute them multiple times for training and decoding and then get their average performance. To explicitly eliminate the order-sensitivity of inputs, we replace the encoder with a mean pooling-based MLP network (MeanPooling-CopyNet). Recent adNon-autoregressive generation. vances (Lee et al., 2018; Stern et al., 2"
2020.findings-emnlp.165,P02-1040,0,0.106943,". We also report their results with a lexically-constrained decoding method, dynamic beam allocation (DBA) (Post and Vilar, 2018), which do not show improvement over con4 ventional beam searching. 5 Evaluation We first introduce the automatic evaluation metrics, then present main experimental results with manual analysis, and finally introduce the potential application in transferring CommonGen-trained models for other downstream tasks. 5.1 Metrics Following other conventional generation tasks, we use several widely-used automatic metrics to automatically assess the performance, such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005), which mainly focus on measuring surface similarities. We report the concept Coverage, which is the average percentage of input concepts that are present in lemmatizatized outputs. In addition, we argue that it is more suitable to use evaluation metrics specially design for caption4 The used hyper-parameters are reported in the appendix. ing task, such as CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al., 2016). They usually assume system generations and human references use similar concepts, and thus focus on evaluate the associati"
2020.findings-emnlp.165,N18-1119,0,0.15875,"he best models are bold and second best ones are underlined within each metric. We highlight the metrics that we used in our official leaderboard. (Results on dev set are at Table. 7.) before the input text, we prepend the input concept set with a simple prompt: “generate a sentence with:” and fine-tune the model with the source sentence on the format “generate a sentence with c1 c2 . . . ck .” For decoding, we employ the standard beam search with a beam size of 5 for all compared models. We also report their results with a lexically-constrained decoding method, dynamic beam allocation (DBA) (Post and Vilar, 2018), which do not show improvement over con4 ventional beam searching. 5 Evaluation We first introduce the automatic evaluation metrics, then present main experimental results with manual analysis, and finally introduce the potential application in transferring CommonGen-trained models for other downstream tasks. 5.1 Metrics Following other conventional generation tasks, we use several widely-used automatic metrics to automatically assess the performance, such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005), which mainly focus on measuring surface similaritie"
2020.findings-emnlp.165,E17-1061,0,0.0193372,"entary commonsense reasoning benchmark task for advancing machine commonsense in NLG. Constrained Text Generation. Constrained text generation aims to decode sentences with expected attributes such as sentiment (Luo et al., 2019a; Hu et al., 2017), tense (Hu et al., 2017), template (Zhu et al., 2019; J Kurisinkel and Chen, 2019), style (Fu et al., 2018; Luo et al., 2019b; Li et al., 2018), topics (Feng et al., 2018), etc. Two related scenarios with our task is lexically constrained decoding and word ordering (Zhang and Clark, 2015; Hasler et al., 2018; Dinu et al., 2019; Hokamp and Liu, 2017; Puduppully et al., 2017; Miao et al., 2019). However, they are not easily adopted by the recent pre-trained language models and thus not directly useful for our task. Topical story generation (Fan et al., 2018; Yao et al., 2019) is also a related direction, while it targets generating longer, creative stories around the given topics, making it hard to directly adopt them to our task. Additionally, the 1830 C OMMON G EN task brings some more challenges mentioned in Section 2. Prior constrained generation methods cannot address these issues together in a unified model. Incorporating Commonsense for NLG. There are a fe"
2020.findings-emnlp.165,D19-1454,1,0.907809,"Missing"
2020.findings-emnlp.165,P18-1238,0,0.0654374,"Missing"
2020.findings-emnlp.165,2020.acl-main.325,0,0.166295,"coder with a mean pooling-based MLP network (MeanPooling-CopyNet). Recent adNon-autoregressive generation. vances (Lee et al., 2018; Stern et al., 2019) in conditional sentence generation have an emerging interest on (edit-based) non-autoregressive generation models, which iteratively refine generated sequences. We assume that these models potentially would have better performance because of their explicit modeling on iterative refinements, and thus study the most recent such model Levenshtein Transformer (LevenTrans) by Gu et al. (2019). We also include a recent enhanced version, ConstLeven (Susanto et al., 2020), which incorporates lexical constraints in LevenTrans. Pre-trained Language Generation Models. We also employ various pre-trained language generation models, including GPT-2 (Radford et al., 2019), UniLM (Dong et al., 2019), UniLM-v2 (Bao et al., 2020), BERT-Gen (Bao et al., 2020), BART (Lewis et al., 2019), and T5 (Raffel et al., 2019), to tackle this task and test their generative commonsense reasoning ability. We fine-tuned all the above models on our training data with a seq2seq format. Specifically, to use GPT-2 for this sequence-tosequence task, we condition the language model on the fo"
2020.findings-emnlp.165,N19-1421,0,0.125624,"n knowledge-based decoding 1829 Model  Metrics T5-large+DBA T5-base+DBA GPT-2+DBA BART+DBA ROUGE-2/L BLEU-3/4 16.8 15.07 17.56 18.15 27.3 24.8 29.4 28.3 36.71 34.82 39.45 37.02 18.7 16 20.6 19.1 METEOR CIDEr SPICE Coverage 25.3 23.5 24.9 25.5 8.62 9.31 10.85 9.82 24.3 21.3 26.8 25.1 83.98 76.81 79.51 84.78 Table 5: Experimental results of models with DBA decoding method on the test set. or re-ranking as future directions. 5.3 Transferring CommonGen Models One may wonder how fine-tuned C OMMON G EN models can benefit commonsense-centric downstream tasks such as Commonsense Question Answering (Talmor et al., 2019) (CSQA) with their generative commonsense reasoning ability. To this end, we use the models trained with the C OMMON G EN dataset for generating useful context. We extract the nouns and verbs in questions and all choices respectively, and combine the concepts of the question q and each choice ci to build five concept-sets. Then, we use these concept-sets as inputs to a trained C OMMON G EN model (e.g., T5) for generating scenario a sentence gi for each as choice-specific contexts. Finally, we prepend the outputs in front of the questions, i.e., “<s>G: gi ∣ Q: q </s> C: ci </s>”. Note that the"
2020.findings-emnlp.165,P18-2016,1,0.836861,"easonable and natural sentence for correct choices while noisy sentences for wrong choices. For example with CG (T5), q=“What do people aim to do at work?”, ci =‘complete job’ (3) with gi =“people work to complete a job aimed at achieving a certain goal.”; cj =‘wear hats’ (7) gj =“people wearing hats aim their guns at each other while working on a construction site.” The used question concepts and choice concepts are underlined. 6 Related Work Commonsense benchmark datasets. There are many emerging datasets for testing machine commonsense from different angles, such as commonsense extraction (Xu et al., 2018; Li et al., 2016), next situation prediction (SWAG (Zellers et al., 2018), CODAH (Chen et al., 2019), HellaSWAG (Zellers et al., 2019b)), cultural and social understanding (Lin et al., 2018; Sap et al., 2019a,b), visual scene comprehension (Zellers et al., 2019a), and general commonsense question answering (Talmor et al., 2019; Huang et al., 2019; Wang et al., 2019a, 2020). However, the success of fine-tuning pre-trained language models for these tasks does not necessarily mean machines can produce novel assumptions in a more open, realistic, generative setting. We see C OMMON G EN as a novel"
2020.findings-emnlp.165,P19-1193,0,0.0214423,"l for our task. Topical story generation (Fan et al., 2018; Yao et al., 2019) is also a related direction, while it targets generating longer, creative stories around the given topics, making it hard to directly adopt them to our task. Additionally, the 1830 C OMMON G EN task brings some more challenges mentioned in Section 2. Prior constrained generation methods cannot address these issues together in a unified model. Incorporating Commonsense for NLG. There are a few recent works that incorporate commonsense knowledge in language generation tasks such as essay generation (Guan et al., 2019; Yang et al., 2019a), image captioning (Lu et al., 2018), video storytelling (Yang et al., 2019b), and conversational systems (Zhang et al., 2020a). These works suggest that generative commonsense reasoning has a great potential to benefit downstream applications. Our proposed C OMMON G EN, to the best of our knowledge, is the very first constrained sentence generation dataset for assessing and conferring generative machine commonsense and we hope it can benefit such applications. Our transferring study in Sec. 5.3 also shows the potential benefits of CommonGen-generated contexts. 7 Conclusion Our major contrib"
2020.findings-emnlp.165,Q14-1006,0,0.0414473,"t co-occurrences Multiple Caption Corpora dev/test train Human References Actively Monitored Crowd-sourcing diversity-based sampling (Concept-Set, Sents) Concept-Sets Figure 3: Dataset construction workflow overview. in everyday situations. As web images and video clips capture diverse everyday scenarios, we use their caption text as a natural resource for collecting concept-sets and their corresponding descriptions of commonsense scenarios. More specifically, we collect visually-grounded sentences from several existing caption datasets, including image captioning datasets, such as Flickr30k (Young et al., 2014), MSCOCO (Lin et al., 2014), Conceptual Captions (Sharma et al., 2018), as well as video captioning datasets including LSMDC (Rohrbach et al., 2017), ActivityNet (Krishna et al., 2017), and VATEX (Wang et al., 2019b). We first conduct part-of-speech tagging over all sentences in the corpora such that words in sentences can be matched to the concept vocabulary of ConceptNet. Then, we compute the sentence frequency of concept-sets consisting of 3∼5 concepts. That is, for each combination of three/four/five concepts in the vocabulary, we know how many sentences are in the corpora covering all con"
2020.findings-emnlp.165,2020.semeval-1.39,0,0.0666593,"Missing"
2020.findings-emnlp.165,P19-1393,0,0.119283,"in everyday situations. As web images and video clips capture diverse everyday scenarios, we use their caption text as a natural resource for collecting concept-sets and their corresponding descriptions of commonsense scenarios. More specifically, we collect visually-grounded sentences from several existing caption datasets, including image captioning datasets, such as Flickr30k (Young et al., 2014), MSCOCO (Lin et al., 2014), Conceptual Captions (Sharma et al., 2018), as well as video captioning datasets including LSMDC (Rohrbach et al., 2017), ActivityNet (Krishna et al., 2017), and VATEX (Wang et al., 2019b). We first conduct part-of-speech tagging over all sentences in the corpora such that words in sentences can be matched to the concept vocabulary of ConceptNet. Then, we compute the sentence frequency of concept-sets consisting of 3∼5 concepts. That is, for each combination of three/four/five concepts in the vocabulary, we know how many sentences are in the corpora covering all concepts. Ideally, we want the selected concept-sets in our dataset to reflect the natural distribution of conceptsets in the real world. At first glance, a reasonable solution may seem to sample from the distribution"
2020.findings-emnlp.165,D18-1009,1,0.815176,"nces for wrong choices. For example with CG (T5), q=“What do people aim to do at work?”, ci =‘complete job’ (3) with gi =“people work to complete a job aimed at achieving a certain goal.”; cj =‘wear hats’ (7) gj =“people wearing hats aim their guns at each other while working on a construction site.” The used question concepts and choice concepts are underlined. 6 Related Work Commonsense benchmark datasets. There are many emerging datasets for testing machine commonsense from different angles, such as commonsense extraction (Xu et al., 2018; Li et al., 2016), next situation prediction (SWAG (Zellers et al., 2018), CODAH (Chen et al., 2019), HellaSWAG (Zellers et al., 2019b)), cultural and social understanding (Lin et al., 2018; Sap et al., 2019a,b), visual scene comprehension (Zellers et al., 2019a), and general commonsense question answering (Talmor et al., 2019; Huang et al., 2019; Wang et al., 2019a, 2020). However, the success of fine-tuning pre-trained language models for these tasks does not necessarily mean machines can produce novel assumptions in a more open, realistic, generative setting. We see C OMMON G EN as a novel, complementary commonsense reasoning benchmark task for advancing machine"
2020.findings-emnlp.165,P19-1472,1,0.917682,"v accuracy) by generating additional context. 1 [Machines] UniLM: Two dogs are throwing frisbees at each other . Introduction Commonsense reasoning, the ability to make acceptable and logical assumptions about ordinary scenes in our daily life, has long been acknowledged as a critical bottleneck of artificial intelligence and natural language processing (Davis and Marcus, 2015). Most recent commonsense reasoning challenges, such as CommonsenseQA (Talmor et al., 2019), SocialIQA (Sap et al., 2019b), WinoGrande (Sakaguchi et al., 2019) and HelConcept-Set: exercise |rope |wall |tie |wave laSwag (Zellers et al., 2019b), have been framed - A man in a gym exercises by waving ropes tied to a wall. as- The discriminative tasks – i.e. AI systems are regym owner decided to tie a rope to the wall so people could quired to choose the correct option from [Humans] a set of make a wave in it for exercise. choices based on a given context. While signifiGPT2: A woman is tied up in a rope and swinging a wave at a wall. cant progress has been made on these discriminaUniLM: A man with a rope and tie is doing some exercise on a wall. tive tasks, argue that reasoning BART: A manwe is tied to a rope andcommonsense is waving"
2020.findings-emnlp.165,2020.acl-main.184,0,0.0619565,"nerating longer, creative stories around the given topics, making it hard to directly adopt them to our task. Additionally, the 1830 C OMMON G EN task brings some more challenges mentioned in Section 2. Prior constrained generation methods cannot address these issues together in a unified model. Incorporating Commonsense for NLG. There are a few recent works that incorporate commonsense knowledge in language generation tasks such as essay generation (Guan et al., 2019; Yang et al., 2019a), image captioning (Lu et al., 2018), video storytelling (Yang et al., 2019b), and conversational systems (Zhang et al., 2020a). These works suggest that generative commonsense reasoning has a great potential to benefit downstream applications. Our proposed C OMMON G EN, to the best of our knowledge, is the very first constrained sentence generation dataset for assessing and conferring generative machine commonsense and we hope it can benefit such applications. Our transferring study in Sec. 5.3 also shows the potential benefits of CommonGen-generated contexts. 7 Conclusion Our major contribution in this paper are threefold: • we present C OMMON G EN, a novel constrained text generation task for generative commonsen"
2020.findings-emnlp.165,J15-3005,0,0.0268136,"n a more open, realistic, generative setting. We see C OMMON G EN as a novel, complementary commonsense reasoning benchmark task for advancing machine commonsense in NLG. Constrained Text Generation. Constrained text generation aims to decode sentences with expected attributes such as sentiment (Luo et al., 2019a; Hu et al., 2017), tense (Hu et al., 2017), template (Zhu et al., 2019; J Kurisinkel and Chen, 2019), style (Fu et al., 2018; Luo et al., 2019b; Li et al., 2018), topics (Feng et al., 2018), etc. Two related scenarios with our task is lexically constrained decoding and word ordering (Zhang and Clark, 2015; Hasler et al., 2018; Dinu et al., 2019; Hokamp and Liu, 2017; Puduppully et al., 2017; Miao et al., 2019). However, they are not easily adopted by the recent pre-trained language models and thus not directly useful for our task. Topical story generation (Fan et al., 2018; Yao et al., 2019) is also a related direction, while it targets generating longer, creative stories around the given topics, making it hard to directly adopt them to our task. Additionally, the 1830 C OMMON G EN task brings some more challenges mentioned in Section 2. Prior constrained generation methods cannot address thes"
2020.findings-emnlp.253,2020.acl-main.656,0,0.0281686,"uistic explanations that are often provided as localized visual highlights on the image. The latter, while pertinent to what the vision component of the model was attending to, cannot provide the full scope of rationales for such complex reasoning tasks as illustrated in Figure 1. Indeed, explanations for higher-level conceptual reasoning can be best conveyed through natural language, as has been studied in recent literature on (visual) NLI (Do et al., 2020; Camburu et al., 2018), (visual) QA (Wu and Mooney, 2019; Rajani et al., 2019), playing arcade games (Ehsan et al., 2019), fact checking (Atanasova et al., 2020), image classification (Hendricks et al., 2018), motivation prediction (Vondrick et al., 2016), and self-driving cars (Kim et al., 2018). In this paper, we present the first focused study on generating natural language rationales across several complex visual reasoning tasks: visual commonsense reasoning, visual-textual entailment, and visual question answering. Our study aims to complement the more broadly studied lower-level explanations such as attention weights and gradients in deep neural networks (Simonyan et al., 2014; Zhang et al., 2017; Montavon et al., 2018, among others). Because fr"
2020.findings-emnlp.253,D15-1075,0,0.105543,"Missing"
2020.findings-emnlp.253,2020.findings-emnlp.301,1,0.825493,"Missing"
2020.findings-emnlp.253,N18-2017,1,0.893736,"Missing"
2020.findings-emnlp.253,K19-1079,0,0.0239016,"understanding. 2.1 Background: Conditional Text Generation The GPT-2’s backbone architecture can be described as the decoder-only Transformer (Vaswani et al., 2017) which is pretrained with the conventional language modeling (LM) likelihood objective.3 This makes it more suitable for generation tasks compared to models trained with the masked LM objective (BERT; Devlin et al., 2019).4 We build on pretrained LMs because their capabilities make free-text rationalization of complex reasoning tasks conceivable. They strongly condition on the preceding tokens, produce coherent and contentful text (See et al., 2019), and importantly, capture some commonsense and world knowledge (Davison et al., 2019; Petroni et al., 2019). To induce conditional text generation behavior, Radford et al. (2019) propose to add the context tokens (e.g., question and answer) before a special token for the generation start. But for visual-textual tasks, the rationale generation has to be conditioned not only on textual context, but also on an image. 3 Sometimes referred to as density estimation, or left-toright or autoregressive LM (Yang et al., 2019). 4 See Appendix §A.1 for other details of GPT-2. 2811 (a) Object Detection (b"
2020.findings-emnlp.253,2020.acl-main.704,0,0.0155319,"ation For evaluating our models, we follow Camburu et al. (2018) who show that BLEU (Papineni et al., 2002) is not reliable for evaluation of rationale generation, and hence use human evaluation.9 We believe that other automatic sentence similarity measures are also likely not suitable due to a similar reason; multiple rationales could be plausible, although not necessarily paraphrases of each other (e.g., in Figure 4 both generated and human rationales are plausible, but they are not strict paraphrases).10 Future work might consider newly emerging learned evaluation measures, such as BLEURT (Sellam et al., 2020), that could learn to capture non-trivial semantic similarities between sentences beyond surface overlap. We use Amazon Mechanical Turk to crowdsource human judgments of generated rationales according to different criteria. Our instructions are provided in the Appendix §A.6. For VCR, we randomly sample one QA pair for each movie in the development split of the dataset, resulting in 244 examples for human evaluation. For VQA and E SNLI - VE, we randomly sample 250 examples from their development splits.11 We did not use any of 9 This is based on a low inter-annotator BLEU-score between three hu"
2020.findings-emnlp.253,P16-1162,0,0.0434243,"Missing"
2020.findings-emnlp.253,P18-1238,0,0.0213889,"image understanding. Visual-Textual Language Models There is a surge of work that proposes visual-textual pretraining of LMs by predicting masked image regions and tokens (Tan and Bansal, 2019; Lu et al., 2019; 2817 Chen et al., 2019, to name a few). We construct input elements of our models following the VL - BERT architecture (Su et al., 2020). Despite their success, these models are not suitable for generation due to pretraining with the masked LM objective. Zhou et al. (2020) aim to address that, but they pretrain their decoder from scratch using 3M images with weakly-associated captions (Sharma et al., 2018). This makes their decoder arguably less powerful compared to LMs that are pretrained with remarkably more (diverse) data such as GPT-2. Ziegler et al. (2019b) augment GPT-2 with a feature vector for the entire image and evaluate this model on image paragraph captioning. Some work extend pretrained LM to learn video representations from sequences of visual features and words, and show improvements in video captioning (Sun et al., 2019a,b). Our work is based on fine-tuning GPT-2 with features that come from visual object recognition, grounded semantic frames, and visual commonsense graphs. The"
2020.findings-emnlp.253,D19-1339,0,0.020365,"Missing"
2020.findings-emnlp.253,P19-1472,1,0.87627,"c frames, i.e., the primary activity and entities engaged in it detected by a grounded situation recognizer (Fig. 2b; Pratt et al., 2020), and (ii) commonsense inferences inferred from an image and an optional event predicted from a visual commonsense graph (Fig. 2c; Park et al., 2020).1 We report comprehensive experiments with careful analysis using three datasets with human rationales: (i) visual question answering in VQA - E (Li et al., 2018), (ii) visual-textual entailment in E - SNLI - VE (Do et al., 2020), and (iii) an answer justification subtask of visual commonsense reasoning in VCR (Zellers et al., 2019a). Our empirical findings demonstrate that while free-text rationalization remains a challenging task, newly emerging state-of-the-art models support rationale generation as a promising research direction to complement model interpretability for complex visual-textual reasoning tasks. In particular, we find that integration of richer semantic and pragmatic visual knowledge is important for generating rationales with higher visual fidelity, especially for tasks that require higher-level concepts and richer background knowledge. Our code, model weights, and the templates used for human evaluati"
2020.findings-emnlp.253,D19-1221,0,0.0208995,"Missing"
2020.findings-emnlp.253,D19-1002,0,0.0249619,"by Herman (2017). This relates to the pipeline predict-thenexplain setting, where a predictor model and a posthoc explainer model are completely independent. However, there are other settings where generated rationales are intrinsic to the model by design (endto-end predict-then-explain, both end-to-end and pipeline explain-then-predict). As such, generated rationales are more associated with the reasoning process of the model. We recommend that future work develops rationale generation in these settings, and aims for sufficiently faithful models as recommended by Jacovi and Goldberg (2020), Wiegreffe and Pinter (2019). 6 Conclusions We present R ATIONALE VT T RANSFORMER, an integration of a pretrained text generator with semantic and pragmatic visual features. These features improve visual plausibility and fidelity of generated rationales for visual commonsense reasoning, visual-textual entailment, and visual question answering. This represents progress in tackling important, but still relatively unexplored research direction; rationalization of complex reasoning for which explanatory approaches based solely on highlighting parts of the input are not suitable. Acknowledgments The authors thank Sarah Pratt"
2020.findings-emnlp.253,W19-4812,0,0.0174583,"re informative and conceptually relevant explanation to the given QA problem compared to the non-linguistic explanations that are often provided as localized visual highlights on the image. The latter, while pertinent to what the vision component of the model was attending to, cannot provide the full scope of rationales for such complex reasoning tasks as illustrated in Figure 1. Indeed, explanations for higher-level conceptual reasoning can be best conveyed through natural language, as has been studied in recent literature on (visual) NLI (Do et al., 2020; Camburu et al., 2018), (visual) QA (Wu and Mooney, 2019; Rajani et al., 2019), playing arcade games (Ehsan et al., 2019), fact checking (Atanasova et al., 2020), image classification (Hendricks et al., 2018), motivation prediction (Vondrick et al., 2016), and self-driving cars (Kim et al., 2018). In this paper, we present the first focused study on generating natural language rationales across several complex visual reasoning tasks: visual commonsense reasoning, visual-textual entailment, and visual question answering. Our study aims to complement the more broadly studied lower-level explanations such as attention weights and gradients in deep neu"
2020.findings-emnlp.253,Q14-1006,0,0.147764,"Missing"
2020.findings-emnlp.301,D18-1389,0,0.0552657,"Missing"
2020.findings-emnlp.301,W19-3504,0,0.0254159,"ms and corpora exhibit biases against minorities and suffer from low agreement in annotations (Waseem, 2016; Ross et al., 2017), partially due to annotator identity influencing their perception of hate speech (Cowan and Khatchadourian, 2003) and differences in annotation task setup (Sap et al., 2019). Notably, recent work has found that systems are overestimating the prevalence of toxicity in text that contains a minority identity mention (e.g., “I’m a gay man”; Dixon et al., 2018; Hutchinson et al., 2020) or text by racial minorities (e.g., text in African American English; Sap et al., 2019; Davidson et al., 2019). This is partially due to detectors’ over-reliance on lexical cues of toxicity (including swearwords, slurs, and other “bad” words Dinan et al., 2019). We further discuss and examine the effect of these biases in the Appendix, by assessing that the racial bias in toxicity is invariant with respect to model choice (Appendix §C.1) and analyzing the presence of profanity and swearwords separately from toxicity (Appendix §C.2). 3 Out-of-the-Box Generation Toxicity We focus our investigation of toxic degeneration in five popular autoregressive Transformer-based (Vaswani et al., 2017) language mode"
2020.findings-emnlp.301,N19-1423,0,0.0295442,"ontent. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining. 1 Figure 1: Non-toxic examples from R EALT OXICI a new testbed for evaluating neural generations and their toxicity. Despite not containing any toxic language as measured by P ERSPECTIVE API, these prompts cause several pretrained LMs to systematically generate highly toxic text (shown in Table 17 in Appendix §E). TY P ROMPTS, safe deployment (McGuffie and Newhouse, 2020). Introduction Although they are the backbone of many modern NLP systems (Devlin et al., 2019; Radford et al., 2019; Raffel et al., 2019), language models (LMs) pretrained on large web text corpora suffer from degenerate and biased behavior (Sheng et al., 2019; Wallace et al., 2019). As illustrated in Figure 1, they can easily degenerate into toxicity, even without explicitly toxic prompts, which hinders their We first introduce a framework to systematically measure the risk of toxic degeneration by pretrained LMs. We release R EALT OXICI TY P ROMPTS (§4), a set of 100K naturally occurring prompts (i.e., sentence prefixes; Figure 1) extracted from a large corpus of English web text an"
2020.findings-emnlp.301,2020.emnlp-main.23,0,0.0280048,"rge-scale, systematic evaluations of detoxification techniques for language models. However, the conclusions one can make about the effectiveness of a detoxification method are limited by the biases of the model used to detect toxicity (§2.2). To combat these issues, we encourage further work on detecting and controlling different types of toxicity and undesirable social biases in generation, e.g., rudeness (Danescu-Niculescu-Mizil et al., 2013), hate speech (Golbeck et al., 2017), or microaggressions (Breitfeller et al., 2019). Additionally, measures of bias could be multi-dimensional (e.g., Dinan et al., 2020), include explanations (e.g., Sap et al., 2020), or be evolving over time (e.g., using similarity to toxic online content). Limitations We describe several limitations of our study. First, as noted in §2.2, we use an imperfect measure of toxicity that could bias the toxicity towards lexical cues, failing to detect more subtle biases and incorrectly flagging non-toxic content. Second, our analyses are limited to the five language models considered (and their steered variants). Further work could extend our analyses to toxicity to masked language models (Wang and Cho, 2019), among others. Lastly"
2020.findings-emnlp.301,D19-1461,0,0.0508788,"r identity influencing their perception of hate speech (Cowan and Khatchadourian, 2003) and differences in annotation task setup (Sap et al., 2019). Notably, recent work has found that systems are overestimating the prevalence of toxicity in text that contains a minority identity mention (e.g., “I’m a gay man”; Dixon et al., 2018; Hutchinson et al., 2020) or text by racial minorities (e.g., text in African American English; Sap et al., 2019; Davidson et al., 2019). This is partially due to detectors’ over-reliance on lexical cues of toxicity (including swearwords, slurs, and other “bad” words Dinan et al., 2019). We further discuss and examine the effect of these biases in the Appendix, by assessing that the racial bias in toxicity is invariant with respect to model choice (Appendix §C.1) and analyzing the presence of profanity and swearwords separately from toxicity (Appendix §C.2). 3 Out-of-the-Box Generation Toxicity We focus our investigation of toxic degeneration in five popular autoregressive Transformer-based (Vaswani et al., 2017) language models: GPT-1, 5 T OXICITY4 1 ity detection tool. Accessed through an API, T OX ICITY corresponds to the prediction output of a CNN (Lecun et al., 1998) tr"
2020.findings-emnlp.301,D19-1224,1,0.835063,"anguage models grow in size (Brown et al., 2020), so does their need for larger corpora, often drawn from easily accessible and abundant web text. However, our analyses reveal toxicity in web text data that likely enable language models to generate even unprompted toxicity (§3.1). Our findings raise several practical and ethical concerns. First, analysis of pretraining data is a crucial first step towards understanding toxic, biased, or otherwise degenerate behavior of language models. Therefore, echoing calls for transparency in NLP research (Bender and Friedman, 2018; Mitchell et al., 2019; Dodge et al., 2019), we recommend researchers publicly release all relevant information during data collection (e.g., original text, source URLs, timestamps, platform-specific metadata) when building pretraining corpora. Second, using Reddit popularity as a curation heuristic introduces representational harm (Barocas et al., 2017) by biasing the populations whose language and perspectives are included in pretraining (e.g., Reddit users skew male; Barthel et al., 2016). This raises the question of who decides whose voices are going to be learned by the language model, and whose voices are excluded. Following Blod"
2020.findings-emnlp.301,W17-4912,0,0.0376314,"ity per-category, is bolded. We display DAPT (Toxic) as a reference for the effectiveness of DAPT as a method of controlling LM behavior. All models are evaluated on a full dataset of 100K prompts, except PPLM, which is evaluated on a dataset of 10K prompts, due to computational budget. Domain-Adaptive Pretraining (DAPT) Using the framework outlined in Gururangan et al. (2020), we perform an additional phase of pretraining on the non-toxic subset of a balanced corpus with GPT-2. For comparison, we also perform the experiment using the toxic subset. Attribute Conditioning (AT C ON) Inspired by Ficler and Goldberg (2017) and Keskar et al. (2019), we prepend a corresponding toxicity attribute token (<|toxic|>, <|nontoxic|>) to a random sample of documents and pretrain the GPT-2 language model further. In our generation experiments, we prepend the <|nontoxic|> token to our prompts. 5.2 Decoding-Based Detoxification Noting the additional cost of training language models further, we explore three detoxifying strategies that only rely on altering the decoding algorithm and are therefore more readily usable by many practitioners. Vocabulary Shifting (VOCAB -S HIFT) Inspired by Eisenstein et al. (2011) and Ghosh et"
2020.findings-emnlp.301,P17-1059,0,0.0406262,"rg (2017) and Keskar et al. (2019), we prepend a corresponding toxicity attribute token (<|toxic|>, <|nontoxic|>) to a random sample of documents and pretrain the GPT-2 language model further. In our generation experiments, we prepend the <|nontoxic|> token to our prompts. 5.2 Decoding-Based Detoxification Noting the additional cost of training language models further, we explore three detoxifying strategies that only rely on altering the decoding algorithm and are therefore more readily usable by many practitioners. Vocabulary Shifting (VOCAB -S HIFT) Inspired by Eisenstein et al. (2011) and Ghosh et al. (2017), we learn a 2-dimensional representation of toxicity and non-toxicity for every token in GPT-2’s vocabulary, which we then use to boost the likelihood of non-toxic tokens. Given the language model’s unnormalized probability (logits) over the vocabulary, we add the term W · t, where t 2 R2 encodes (non-)toxicity, and W 2 RV represents the associations between each token and (non-)toxicity, and is the boosting strength. We set = 3 for all experiments. We learn this representation using the toxicity labels on the balanced corpus described in §5.1 (See Appendix §B.3 for more details). Word Filter"
2020.findings-emnlp.301,2020.acl-main.740,1,0.886716,"Missing"
2020.findings-emnlp.301,P18-1152,1,0.889536,"Missing"
2020.findings-emnlp.301,2020.acl-main.487,0,0.217442,"ases in Toxic Language Detection Although widely used, the P ERSPECTIVE API and other hate speech detection systems and corpora exhibit biases against minorities and suffer from low agreement in annotations (Waseem, 2016; Ross et al., 2017), partially due to annotator identity influencing their perception of hate speech (Cowan and Khatchadourian, 2003) and differences in annotation task setup (Sap et al., 2019). Notably, recent work has found that systems are overestimating the prevalence of toxicity in text that contains a minority identity mention (e.g., “I’m a gay man”; Dixon et al., 2018; Hutchinson et al., 2020) or text by racial minorities (e.g., text in African American English; Sap et al., 2019; Davidson et al., 2019). This is partially due to detectors’ over-reliance on lexical cues of toxicity (including swearwords, slurs, and other “bad” words Dinan et al., 2019). We further discuss and examine the effect of these biases in the Appendix, by assessing that the racial bias in toxicity is invariant with respect to model choice (Appendix §C.1) and analyzing the presence of profanity and swearwords separately from toxicity (Appendix §C.2). 3 Out-of-the-Box Generation Toxicity We focus our investigat"
2020.findings-emnlp.301,W19-3514,0,0.0208833,"c degeneration by both out-of-the-box and controlled models using 100K naturally occurring prompts, including some that do not contain identity mentions (see Figure 1). Additionally, our work focuses on the broad phenomenon of toxicity in generations, whereas Sheng et al. (2019) study the sentiment and regard expressed by a model’s generation towards demographic identities. The creation of R EALT OXICITY P ROMPTS was partly inspired by work in detecting conversational patterns that can cause derailment into antisocial behavior in online conversations (Zhang et al., ˇ 2018; Stoop et al., 2019; Karan and Snajder, 2019). Our work also draws from a strong line of research into controlling the outputs of language models (Dathathri et al., 2020; Sudhakar et al., 2019; Ziegler et al.; Keskar et al., 2019, inter alia). 9 Conclusion We introduce R EALT OXICITY P ROMPTS, a testbed of 100K prompts for evaluating the toxic degeneration in pretrained language models. Under this framework, we quantify the toxicity of multiple pretrained language models and the effectiveness of methods for detoxifying generations. We then analyze toxicity in two large web text corpora, including the GPT-2 pretraining corpus, to better u"
2020.findings-emnlp.301,W19-3823,0,0.0294992,"e models considered (and their steered variants). Further work could extend our analyses to toxicity to masked language models (Wang and Cho, 2019), among others. Lastly, because O PENAI-WT does not have available metadata, and due to the imperfect coverage of our subreddit and news reliability data, we only provide lower bound estimates of toxicity in web text corpora. 8 Related Work A wealth of work has shown that toxicity and social biases in training data are acquired by large pretrained sentence encoders (e.g., gender bias in BERT; May et al., 2019; Zhao et al., 2019; Basta et al., 2019; Kurita et al., 2019). However, fewer studies have investigated toxicity in autoregressive language models, whose generations also suffer from incoherence, blandness, and repetitiveness (Holtzman et al., 2020; Welleck et al., 2019). Similar in spirit to R EALT OXICITY P ROMPTS, Wallace et al. (2019) find universal adversarial triggers, nonsensical prompts that trigger toxic generations in GPT-2. In this work, we find and release naturally occurring prompts from web text that trigger toxicity, and compare toxic output in several language models. Most closely related to this work, Sheng et al. (2019) use a set of 60"
2020.findings-emnlp.301,2020.emnlp-main.602,1,0.751142,"r steering introduce unwanted side effects in language model behavior after adaptation. Decoding with a Purpose Our analyses also highlight the promise of certain decoding methods, such as PPLM (Dathathri et al., 2020), which is among the most effective methods we tested at avoiding toxicity with toxic prompts. In addition to automated toxicity classifiers, future work could explore the use of handpicked toxic documents as “negative examples” to avoid toxicity in generation. Future work could also investigate infusing models with more sophisticated or nuanced representations of social biases (Ma et al., 2020). Choice of Pretraining Data As pretrained language models grow in size (Brown et al., 2020), so does their need for larger corpora, often drawn from easily accessible and abundant web text. However, our analyses reveal toxicity in web text data that likely enable language models to generate even unprompted toxicity (§3.1). Our findings raise several practical and ethical concerns. First, analysis of pretraining data is a crucial first step towards understanding toxic, biased, or otherwise degenerate behavior of language models. Therefore, echoing calls for transparency in NLP research (Bender"
2020.findings-emnlp.301,N19-1063,0,0.0316988,"ent. Second, our analyses are limited to the five language models considered (and their steered variants). Further work could extend our analyses to toxicity to masked language models (Wang and Cho, 2019), among others. Lastly, because O PENAI-WT does not have available metadata, and due to the imperfect coverage of our subreddit and news reliability data, we only provide lower bound estimates of toxicity in web text corpora. 8 Related Work A wealth of work has shown that toxicity and social biases in training data are acquired by large pretrained sentence encoders (e.g., gender bias in BERT; May et al., 2019; Zhao et al., 2019; Basta et al., 2019; Kurita et al., 2019). However, fewer studies have investigated toxicity in autoregressive language models, whose generations also suffer from incoherence, blandness, and repetitiveness (Holtzman et al., 2020; Welleck et al., 2019). Similar in spirit to R EALT OXICITY P ROMPTS, Wallace et al. (2019) find universal adversarial triggers, nonsensical prompts that trigger toxic generations in GPT-2. In this work, we find and release naturally occurring prompts from web text that trigger toxicity, and compare toxic output in several language models. Most clos"
2020.findings-emnlp.301,W17-3006,0,0.0640256,"Missing"
2020.findings-emnlp.301,P19-1163,1,0.906234,"ion.allenai.org/ 3 https://github.com/conversationai/ perspectiveapi 4 P ERSPECTIVE API defines T OXICITY as a “rude, disrespectful, or unreasonable comment; likely to make people leave a discussion.” Biases in Toxic Language Detection Although widely used, the P ERSPECTIVE API and other hate speech detection systems and corpora exhibit biases against minorities and suffer from low agreement in annotations (Waseem, 2016; Ross et al., 2017), partially due to annotator identity influencing their perception of hate speech (Cowan and Khatchadourian, 2003) and differences in annotation task setup (Sap et al., 2019). Notably, recent work has found that systems are overestimating the prevalence of toxicity in text that contains a minority identity mention (e.g., “I’m a gay man”; Dixon et al., 2018; Hutchinson et al., 2020) or text by racial minorities (e.g., text in African American English; Sap et al., 2019; Davidson et al., 2019). This is partially due to detectors’ over-reliance on lexical cues of toxicity (including swearwords, slurs, and other “bad” words Dinan et al., 2019). We further discuss and examine the effect of these biases in the Appendix, by assessing that the racial bias in toxicity is in"
2020.findings-emnlp.301,2020.acl-main.486,1,0.659692,"on techniques for language models. However, the conclusions one can make about the effectiveness of a detoxification method are limited by the biases of the model used to detect toxicity (§2.2). To combat these issues, we encourage further work on detecting and controlling different types of toxicity and undesirable social biases in generation, e.g., rudeness (Danescu-Niculescu-Mizil et al., 2013), hate speech (Golbeck et al., 2017), or microaggressions (Breitfeller et al., 2019). Additionally, measures of bias could be multi-dimensional (e.g., Dinan et al., 2020), include explanations (e.g., Sap et al., 2020), or be evolving over time (e.g., using similarity to toxic online content). Limitations We describe several limitations of our study. First, as noted in §2.2, we use an imperfect measure of toxicity that could bias the toxicity towards lexical cues, failing to detect more subtle biases and incorrectly flagging non-toxic content. Second, our analyses are limited to the five language models considered (and their steered variants). Further work could extend our analyses to toxicity to masked language models (Wang and Cho, 2019), among others. Lastly, because O PENAI-WT does not have available me"
2020.findings-emnlp.301,P16-1162,0,0.099673,"Missing"
2020.findings-emnlp.301,2020.lrec-1.298,0,0.0458399,"Missing"
2020.findings-emnlp.301,D19-1339,0,0.268334,"toxic examples from R EALT OXICI a new testbed for evaluating neural generations and their toxicity. Despite not containing any toxic language as measured by P ERSPECTIVE API, these prompts cause several pretrained LMs to systematically generate highly toxic text (shown in Table 17 in Appendix §E). TY P ROMPTS, safe deployment (McGuffie and Newhouse, 2020). Introduction Although they are the backbone of many modern NLP systems (Devlin et al., 2019; Radford et al., 2019; Raffel et al., 2019), language models (LMs) pretrained on large web text corpora suffer from degenerate and biased behavior (Sheng et al., 2019; Wallace et al., 2019). As illustrated in Figure 1, they can easily degenerate into toxicity, even without explicitly toxic prompts, which hinders their We first introduce a framework to systematically measure the risk of toxic degeneration by pretrained LMs. We release R EALT OXICI TY P ROMPTS (§4), a set of 100K naturally occurring prompts (i.e., sentence prefixes; Figure 1) extracted from a large corpus of English web text and paired with toxicity scores from a widely used and commercially deployed toxicity detector (P ERSPECTIVE API). We show that popular LMs produce toxic generations whe"
2020.findings-emnlp.301,W19-3503,0,0.0314034,"Missing"
2020.findings-emnlp.301,D19-1322,0,0.022498,"ns (see Figure 1). Additionally, our work focuses on the broad phenomenon of toxicity in generations, whereas Sheng et al. (2019) study the sentiment and regard expressed by a model’s generation towards demographic identities. The creation of R EALT OXICITY P ROMPTS was partly inspired by work in detecting conversational patterns that can cause derailment into antisocial behavior in online conversations (Zhang et al., ˇ 2018; Stoop et al., 2019; Karan and Snajder, 2019). Our work also draws from a strong line of research into controlling the outputs of language models (Dathathri et al., 2020; Sudhakar et al., 2019; Ziegler et al.; Keskar et al., 2019, inter alia). 9 Conclusion We introduce R EALT OXICITY P ROMPTS, a testbed of 100K prompts for evaluating the toxic degeneration in pretrained language models. Under this framework, we quantify the toxicity of multiple pretrained language models and the effectiveness of methods for detoxifying generations. We then analyze toxicity in two large web text corpora, including the GPT-2 pretraining corpus, to better understand the root cause of toxic generations. Finally, we provide recommendations for gathering pretraining data. The data, code, and interactive"
2020.findings-emnlp.301,D19-1221,0,0.202545,"R EALT OXICI a new testbed for evaluating neural generations and their toxicity. Despite not containing any toxic language as measured by P ERSPECTIVE API, these prompts cause several pretrained LMs to systematically generate highly toxic text (shown in Table 17 in Appendix §E). TY P ROMPTS, safe deployment (McGuffie and Newhouse, 2020). Introduction Although they are the backbone of many modern NLP systems (Devlin et al., 2019; Radford et al., 2019; Raffel et al., 2019), language models (LMs) pretrained on large web text corpora suffer from degenerate and biased behavior (Sheng et al., 2019; Wallace et al., 2019). As illustrated in Figure 1, they can easily degenerate into toxicity, even without explicitly toxic prompts, which hinders their We first introduce a framework to systematically measure the risk of toxic degeneration by pretrained LMs. We release R EALT OXICI TY P ROMPTS (§4), a set of 100K naturally occurring prompts (i.e., sentence prefixes; Figure 1) extracted from a large corpus of English web text and paired with toxicity scores from a widely used and commercially deployed toxicity detector (P ERSPECTIVE API). We show that popular LMs produce toxic generations when conditioned on our pr"
2020.findings-emnlp.301,W19-2304,0,0.029457,"ti-dimensional (e.g., Dinan et al., 2020), include explanations (e.g., Sap et al., 2020), or be evolving over time (e.g., using similarity to toxic online content). Limitations We describe several limitations of our study. First, as noted in §2.2, we use an imperfect measure of toxicity that could bias the toxicity towards lexical cues, failing to detect more subtle biases and incorrectly flagging non-toxic content. Second, our analyses are limited to the five language models considered (and their steered variants). Further work could extend our analyses to toxicity to masked language models (Wang and Cho, 2019), among others. Lastly, because O PENAI-WT does not have available metadata, and due to the imperfect coverage of our subreddit and news reliability data, we only provide lower bound estimates of toxicity in web text corpora. 8 Related Work A wealth of work has shown that toxicity and social biases in training data are acquired by large pretrained sentence encoders (e.g., gender bias in BERT; May et al., 2019; Zhao et al., 2019; Basta et al., 2019; Kurita et al., 2019). However, fewer studies have investigated toxicity in autoregressive language models, whose generations also suffer from incoh"
2020.findings-emnlp.301,W16-5618,0,0.0171159,"eural language models, and therefore use the term “neural toxic degeneration.” Future work could examine whether non-neural language models exhibit similar behavior. 2 http://toxicdegeneration.allenai.org/ 3 https://github.com/conversationai/ perspectiveapi 4 P ERSPECTIVE API defines T OXICITY as a “rude, disrespectful, or unreasonable comment; likely to make people leave a discussion.” Biases in Toxic Language Detection Although widely used, the P ERSPECTIVE API and other hate speech detection systems and corpora exhibit biases against minorities and suffer from low agreement in annotations (Waseem, 2016; Ross et al., 2017), partially due to annotator identity influencing their perception of hate speech (Cowan and Khatchadourian, 2003) and differences in annotation task setup (Sap et al., 2019). Notably, recent work has found that systems are overestimating the prevalence of toxicity in text that contains a minority identity mention (e.g., “I’m a gay man”; Dixon et al., 2018; Hutchinson et al., 2020) or text by racial minorities (e.g., text in African American English; Sap et al., 2019; Davidson et al., 2019). This is partially due to detectors’ over-reliance on lexical cues of toxicity (incl"
2020.findings-emnlp.301,P18-1125,0,0.0642734,"Missing"
2020.findings-emnlp.301,N19-1064,0,0.0618342,"Missing"
2020.findings-emnlp.418,P16-2085,0,0.0362699,"Missing"
2020.findings-emnlp.418,P91-1008,0,0.661724,"in order to come up with examples or counterarguments that may undermine it; by analogy, the generative task we introduce here requires a model to come up with (rather than simply verify) examples of circumstances that undermine the given hypothesis. 2 Background and Related Work Defeasible reasoning is soft inference based on default assumptions to account for unknown facts, for example, “Tweety is a bird” entails that “Tweety flies”, because birds usually fly. Such a conclusion is not deductively valid, and might be invalidated by new information such as “Tweety is a penguin” (Reiter, 1980; Lascarides and Asher, 1991). Defeasible reasoning is a type of nonmonotonic logic, as it contrasts the monotonicity property of classical logic, according to which valid inferences cannot be defeated by adding additional information (Kraus et al., 1990). Defeasible reasoning has been studied in a range of fields from logic, through linguistics and artificial intelligence. Classical AI. In early AI, defeasible reasoning was used as a solution to the “frame problem”: it is impossible to list all the potential effects of actions without describing mundane and obvious effects (McCarthy and Hayes, 1969). McDermott and Doyle"
2020.findings-emnlp.418,2020.acl-main.703,0,0.0591476,"Missing"
2020.findings-emnlp.418,W04-1013,0,0.0613121,"Missing"
2020.findings-emnlp.418,2021.ccl-1.108,0,0.0545281,"Missing"
2020.findings-emnlp.418,W17-1609,1,0.892575,"Missing"
2020.findings-emnlp.418,D17-1238,0,0.0285361,"Missing"
2020.findings-emnlp.418,P02-1040,0,0.106635,"jective to predict the next word. We use the Transformers package (Wolf et al., 2019) and train each model for a single epoch with a batch size of 64. Further training details are provided in the appendix. Automatic Evaluation. We follow the common practice of reporting automated generation evaluation metrics. We report the perplexity on the test set, as is often used to measure the performance of a language model.4 In addition, we generated predictions for the test set using beam search with 5 beams, and evaluated them using standard n-gram based metrics: the precision-oriented BLEU-4 score (Papineni et al., 2002), which considers n-grams up to n = 4, and the recall-oriented 4666 4 Micro and macro perplexities were identical. Premise Hypothesis Type Generated Update 1 A man just roaming on the streets during night. A man is roaming the streets at night, drunk. — It is rude to point out their weight problem. S W The man has a beer in his hand You are a nutritionist 2 PersonX pays PersonX’s debt — W S PersonX is in debt to the IRS You are in an emergency 3 4 5 6 7 8 9 Because PersonX wanted to be debt free It is rude to refuse help. — It is wrong to kill an animal. S You are trying to save the life of a"
2020.findings-emnlp.418,Q19-1043,0,0.0222982,"as a softer version of semantic entailment, doubly hedging it with “a human would typically think that the hypothesis is likely true” (see Section 3, Dagan et al., 2005). It gained tremendous popularity again 10 years later, with the release of the large-scale Stanford Natural Language Inference dataset (SNLI; Bowman et al., 2015), that facilitated training neural models, and which was followed by several other datasets in that nature (Williams et al., 2018; Nie et al., 2019). But—among other criticisms of the task—it has been shown that people generally don’t agree on entailment annotations (Pavlick and Kwiatkowski, 2019), and new variants of the task suggested to shift away from categorical labels to ordinal or numeric values denoting plausibility (Zhang et al., 2017; Sakaguchi and Van Durme, 2018; Chen et al., 2020). In this paper we focus on the defeasibility of textual entailments, a less well-studied phenomenon in this context. 3 Definition In this paper, we employ a working definition of defeasible inference that may be seen as an outgrowth of prior work. Dagan et al. (2005) introduced the following informal definition for the Recognizing Textual Entailment (RTE) task: ...textual entailment is defined as"
2020.findings-emnlp.418,S18-2023,1,0.911929,"Missing"
2020.findings-emnlp.418,D19-1509,1,0.887999,"Missing"
2020.findings-emnlp.418,P18-1020,0,0.0609249,"Missing"
2020.findings-emnlp.418,D19-1629,0,0.140498,"Missing"
2020.findings-emnlp.418,L18-1239,0,0.0442715,"Missing"
2020.findings-emnlp.418,N18-1101,0,0.0369249,"elations by defining defeasible rules based on commonsense knowledge of typical causes and effects. Natural Language Processing. Textual entailment was defined as a softer version of semantic entailment, doubly hedging it with “a human would typically think that the hypothesis is likely true” (see Section 3, Dagan et al., 2005). It gained tremendous popularity again 10 years later, with the release of the large-scale Stanford Natural Language Inference dataset (SNLI; Bowman et al., 2015), that facilitated training neural models, and which was followed by several other datasets in that nature (Williams et al., 2018; Nie et al., 2019). But—among other criticisms of the task—it has been shown that people generally don’t agree on entailment annotations (Pavlick and Kwiatkowski, 2019), and new variants of the task suggested to shift away from categorical labels to ordinal or numeric values denoting plausibility (Zhang et al., 2017; Sakaguchi and Van Durme, 2018; Chen et al., 2020). In this paper we focus on the defeasibility of textual entailments, a less well-studied phenomenon in this context. 3 Definition In this paper, we employ a working definition of defeasible inference that may be seen as an outgrow"
2020.findings-emnlp.90,P19-1620,0,0.0279697,"on, but our work is the first to present a generative approach for the challenging commonsense QA setting, and we introduce new data selection approaches to improve the informativeness and diversity of synthetic data. Concurrently, there has been work on generating adversarial examples for analyzing black-box classifiers. These approaches use generative adversarial networks (Zhao et al., 2018b) and populationbased optimization algorithms (Alzantot et al., 2018). Previous work has also presented methods to generate questions for reading comprehension (Heilman and Smith, 2010; Rus et al., 2011; Alberti et al., 2019; Puri et al., 2020a), online tutoring (Lindberg et al., 2013), factual QA (Serban et al., 2016) and visual question generation (Mostafazadeh et al., 2016). A comprehensive survey on neural question generation can be found in Pan et al. (2019). Our work is distinct in that it targets question generation in a closed-book setting, investigates the generation of answers as well as distractors, and is aimed at data augmentation. 7 Conclusion We introduced G-DAUGc , a novel data augmentation framework to generate synthetic training data, preserving quality and diversity. We demonstrate that G-DAUGc"
2020.findings-emnlp.90,D18-1316,0,0.0279844,"vor et al., 2020; Kumar et al., 2020; Puri et al., 2020b). Our framework is similar to the last of these as we focus on generative models for data augmentation, but our work is the first to present a generative approach for the challenging commonsense QA setting, and we introduce new data selection approaches to improve the informativeness and diversity of synthetic data. Concurrently, there has been work on generating adversarial examples for analyzing black-box classifiers. These approaches use generative adversarial networks (Zhao et al., 2018b) and populationbased optimization algorithms (Alzantot et al., 2018). Previous work has also presented methods to generate questions for reading comprehension (Heilman and Smith, 2010; Rus et al., 2011; Alberti et al., 2019; Puri et al., 2020a), online tutoring (Lindberg et al., 2013), factual QA (Serban et al., 2016) and visual question generation (Mostafazadeh et al., 2016). A comprehensive survey on neural question generation can be found in Pan et al. (2019). Our work is distinct in that it targets question generation in a closed-book setting, investigates the generation of answers as well as distractors, and is aimed at data augmentation. 7 Conclusion We"
2020.findings-emnlp.90,P15-1034,0,0.0932739,"Missing"
2020.findings-emnlp.90,P19-1470,1,0.842115,"17; Du et al., 2017; Zhao et al., 2018a). But, generating synthetic examples for commonsense reasoning poses a unique challenge. In reading comprehension, for instance, the goal of data augmentation is to generate questions that are directly answerable by a given reference passage. In contrast, answering commonsense questions relies on commonsense notions that are seldom stated explicitly (Gordon and Van Durme, 2013; Forbes and Choi, 2017), and authoring such questions can require creativity (see Figure 1). Based on promising evidence from previous work (Yang et al., 2018; Trinh and Le, 2018; Bosselut et al., 2019; Davison et al., 2019), we hypothesize that pretrained language models, such as GPT-2 (Radford et al., 2019), capture some common sense expressed implicitly in their pretraining corpus. Could questions generated by such models serve as helpful 1008 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1008–1025 c November 16 - 20, 2020. 2020 Association for Computational Linguistics training data? In this work, we explore this question through Generative Data Augmentation for commonsense reasoning (G-DAUGc ; §2): a novel framework for augmenting training data with diver"
2020.findings-emnlp.90,D15-1075,0,0.427363,"wo-stage training performs better than the importance-weighted loss (see Section 5). 4 Experiments We present experiments on four commonsense multiple choice QA benchmarks: C OMMONSENSE QA (Talmor et al., 2019), W INO G RANDE (Sakaguchi et al., 2020), C ODAH (Chen et al., 2019) and HellaSwag (Zellers et al., 2019). Our techniques are also directly applicable to other closed-book multiple choice QA setups, such as science QA, and to textual entailment tasks with minor modifications. To evaluate G-DAUGc ’s extensibility to these settings, we also experiment with a textual entailment task, SNLI (Bowman et al., 2015), and a closedbook version of the ARC-Challenge Scientific QA task (Clark et al., 2018) in which access to the scientific corpus for the ARC dataset (or any other information sources) is disallowed during test. We simulate low-resource settings on the large HellaSwag and SNLI datasets by downsampling these to 2K and 3K training samples respectively; the other data sets are either already low-resource or have a low-resource component. Dataset details are provided in Appendix A. Robustness Evaluation In addition to measuring in-distribution performance, we also analyze robustness to perturbed or"
2020.findings-emnlp.90,W19-2008,1,0.901783,"Missing"
2020.findings-emnlp.90,D19-1109,0,0.0263185,"ao et al., 2018a). But, generating synthetic examples for commonsense reasoning poses a unique challenge. In reading comprehension, for instance, the goal of data augmentation is to generate questions that are directly answerable by a given reference passage. In contrast, answering commonsense questions relies on commonsense notions that are seldom stated explicitly (Gordon and Van Durme, 2013; Forbes and Choi, 2017), and authoring such questions can require creativity (see Figure 1). Based on promising evidence from previous work (Yang et al., 2018; Trinh and Le, 2018; Bosselut et al., 2019; Davison et al., 2019), we hypothesize that pretrained language models, such as GPT-2 (Radford et al., 2019), capture some common sense expressed implicitly in their pretraining corpus. Could questions generated by such models serve as helpful 1008 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1008–1025 c November 16 - 20, 2020. 2020 Association for Computational Linguistics training data? In this work, we explore this question through Generative Data Augmentation for commonsense reasoning (G-DAUGc ; §2): a novel framework for augmenting training data with diverse and informative synt"
2020.findings-emnlp.90,N19-1423,0,0.0304194,"s that G-DAUGc produces a diverse set of fluent training examples, and that its selection and training approaches are important for performance. 1 Figure 1: Example of a selected high-quality generated example compared to a human-authored example from the W INO G RANDE dataset. Composing commonsense questions can require creativity. et al., 2018; Schwartz et al., 2017), leading to models with considerably weaker performance on outof-distribution samples (Jia and Liang, 2017; Belinkov and Bisk, 2017; Iyyer et al., 2018). Introduction While recent advances in large-scale neural language models (Devlin et al., 2019; Liu et al., 2019; Radford et al., 2019; Raffel et al., 2019) have led to strong performance on several commonsense reasoning benchmarks (Talmor et al., 2019; Lv et al., 2020; Sakaguchi et al., 2020), their accuracy by and large depends on the availability of large-scale human-authored training data. However, crowdsourcing examples at scale for each new task and domain can be prohibitively expensive. Moreover, human-authored data has been shown to exhibit annotation artifacts (Gururangan et al., 2018; Agrawal A candidate solution that has shown promise in other tasks, such as reading comprehe"
2020.findings-emnlp.90,P17-1123,0,0.03244,"ng benchmarks (Talmor et al., 2019; Lv et al., 2020; Sakaguchi et al., 2020), their accuracy by and large depends on the availability of large-scale human-authored training data. However, crowdsourcing examples at scale for each new task and domain can be prohibitively expensive. Moreover, human-authored data has been shown to exhibit annotation artifacts (Gururangan et al., 2018; Agrawal A candidate solution that has shown promise in other tasks, such as reading comprehension, is to augment a human-authored training set with a large set of synthetically-generated examples (Zhou et al., 2017; Du et al., 2017; Zhao et al., 2018a). But, generating synthetic examples for commonsense reasoning poses a unique challenge. In reading comprehension, for instance, the goal of data augmentation is to generate questions that are directly answerable by a given reference passage. In contrast, answering commonsense questions relies on commonsense notions that are seldom stated explicitly (Gordon and Van Durme, 2013; Forbes and Choi, 2017), and authoring such questions can require creativity (see Figure 1). Based on promising evidence from previous work (Yang et al., 2018; Trinh and Le, 2018; Bosselut et al., 20"
2020.findings-emnlp.90,P17-2090,0,0.0363743,"ve than ensembling with a finetuned generator. 6 Related Work Data augmentation is a common practice in computer vision, where it takes the form of image transformations like translation and rotation (Perez and Wang, 2017). For language tasks, data augmentation is less straightforward. Broadly, previous augmentation methods have used back-translation architectures (Sennrich et al., 2016; Xie et al., 2019), heuristics based on syntactic and semantic properties of text including word replacements using a thesaurus (Zhang et al., 2015; Wei and Zou, 2019) and word embeddings (Wang and Yang, 2015; Fadaee et al., 2017; Kobayashi, 2018; Wu et al., 2019), and recently, generative models for synthesizing novel examples for text classification and reading comprehension (Anaby-Tavor et al., 2020; Kumar et al., 2020; Puri et al., 2020b). Our framework is similar to the last of these as we focus on generative models for data augmentation, but our work is the first to present a generative approach for the challenging commonsense QA setting, and we introduce new data selection approaches to improve the informativeness and diversity of synthetic data. Concurrently, there has been work on generating adversarial examp"
2020.findings-emnlp.90,P17-1025,1,0.847214,"has shown promise in other tasks, such as reading comprehension, is to augment a human-authored training set with a large set of synthetically-generated examples (Zhou et al., 2017; Du et al., 2017; Zhao et al., 2018a). But, generating synthetic examples for commonsense reasoning poses a unique challenge. In reading comprehension, for instance, the goal of data augmentation is to generate questions that are directly answerable by a given reference passage. In contrast, answering commonsense questions relies on commonsense notions that are seldom stated explicitly (Gordon and Van Durme, 2013; Forbes and Choi, 2017), and authoring such questions can require creativity (see Figure 1). Based on promising evidence from previous work (Yang et al., 2018; Trinh and Le, 2018; Bosselut et al., 2019; Davison et al., 2019), we hypothesize that pretrained language models, such as GPT-2 (Radford et al., 2019), capture some common sense expressed implicitly in their pretraining corpus. Could questions generated by such models serve as helpful 1008 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1008–1025 c November 16 - 20, 2020. 2020 Association for Computational Linguistics training dat"
2020.findings-emnlp.90,2020.acl-main.740,1,0.887112,"Missing"
2020.findings-emnlp.90,N18-2017,1,0.906086,"Missing"
2020.findings-emnlp.90,D19-1424,0,0.0413883,"Missing"
2020.findings-emnlp.90,N10-1086,0,0.0309458,"us on generative models for data augmentation, but our work is the first to present a generative approach for the challenging commonsense QA setting, and we introduce new data selection approaches to improve the informativeness and diversity of synthetic data. Concurrently, there has been work on generating adversarial examples for analyzing black-box classifiers. These approaches use generative adversarial networks (Zhao et al., 2018b) and populationbased optimization algorithms (Alzantot et al., 2018). Previous work has also presented methods to generate questions for reading comprehension (Heilman and Smith, 2010; Rus et al., 2011; Alberti et al., 2019; Puri et al., 2020a), online tutoring (Lindberg et al., 2013), factual QA (Serban et al., 2016) and visual question generation (Mostafazadeh et al., 2016). A comprehensive survey on neural question generation can be found in Pan et al. (2019). Our work is distinct in that it targets question generation in a closed-book setting, investigates the generation of answers as well as distractors, and is aimed at data augmentation. 7 Conclusion We introduced G-DAUGc , a novel data augmentation framework to generate synthetic training data, preserving quality an"
2020.findings-emnlp.90,W17-3529,0,0.0355053,"sity have complementary benefits—the former aims at improving the quality of individual examples by filtering out detrimental ones, and the latter is designed to compose a diverse training set but does not consider quality. To reap both benefits, we propose a combined selection technique, G-DAUGc -Combo, that first filters the data using G-DAUGc -Influence, then selects examples according to G-DAUGc -Diversity. 3.2 Training with Synthetic Data In traditional data augmentation, new data is usually mixed with the original training examples to create an augmented training set (Wei and Zou, 2019; Kafle et al., 2017). However, when augmenting with data produced using a generative model, label noise can be detrimental to learning (Kafle et al., 2017). Moreover, the generated questions themselves can be noisy, i.e. nonsensical or ambiguous (see Table 7 under §4.2). To address this issue, we propose a simple training procedure that treats the synthetic and original data differently. We first train a model on the synthetic data (Synthetic Training), then further train on the original, human-authored training set (Organic Training). The motivation is to correct any unfavorable noise that may have been learnt d"
2020.findings-emnlp.90,N18-2072,0,0.0257322,"th a finetuned generator. 6 Related Work Data augmentation is a common practice in computer vision, where it takes the form of image transformations like translation and rotation (Perez and Wang, 2017). For language tasks, data augmentation is less straightforward. Broadly, previous augmentation methods have used back-translation architectures (Sennrich et al., 2016; Xie et al., 2019), heuristics based on syntactic and semantic properties of text including word replacements using a thesaurus (Zhang et al., 2015; Wei and Zou, 2019) and word embeddings (Wang and Yang, 2015; Fadaee et al., 2017; Kobayashi, 2018; Wu et al., 2019), and recently, generative models for synthesizing novel examples for text classification and reading comprehension (Anaby-Tavor et al., 2020; Kumar et al., 2020; Puri et al., 2020b). Our framework is similar to the last of these as we focus on generative models for data augmentation, but our work is the first to present a generative approach for the challenging commonsense QA setting, and we introduce new data selection approaches to improve the informativeness and diversity of synthetic data. Concurrently, there has been work on generating adversarial examples for analyzing"
2020.findings-emnlp.90,2020.lifelongnlp-1.3,0,0.0645912,"on (Perez and Wang, 2017). For language tasks, data augmentation is less straightforward. Broadly, previous augmentation methods have used back-translation architectures (Sennrich et al., 2016; Xie et al., 2019), heuristics based on syntactic and semantic properties of text including word replacements using a thesaurus (Zhang et al., 2015; Wei and Zou, 2019) and word embeddings (Wang and Yang, 2015; Fadaee et al., 2017; Kobayashi, 2018; Wu et al., 2019), and recently, generative models for synthesizing novel examples for text classification and reading comprehension (Anaby-Tavor et al., 2020; Kumar et al., 2020; Puri et al., 2020b). Our framework is similar to the last of these as we focus on generative models for data augmentation, but our work is the first to present a generative approach for the challenging commonsense QA setting, and we introduce new data selection approaches to improve the informativeness and diversity of synthetic data. Concurrently, there has been work on generating adversarial examples for analyzing black-box classifiers. These approaches use generative adversarial networks (Zhao et al., 2018b) and populationbased optimization algorithms (Alzantot et al., 2018). Previous wor"
2020.findings-emnlp.90,W13-2114,0,0.0267945,"ch for the challenging commonsense QA setting, and we introduce new data selection approaches to improve the informativeness and diversity of synthetic data. Concurrently, there has been work on generating adversarial examples for analyzing black-box classifiers. These approaches use generative adversarial networks (Zhao et al., 2018b) and populationbased optimization algorithms (Alzantot et al., 2018). Previous work has also presented methods to generate questions for reading comprehension (Heilman and Smith, 2010; Rus et al., 2011; Alberti et al., 2019; Puri et al., 2020a), online tutoring (Lindberg et al., 2013), factual QA (Serban et al., 2016) and visual question generation (Mostafazadeh et al., 2016). A comprehensive survey on neural question generation can be found in Pan et al. (2019). Our work is distinct in that it targets question generation in a closed-book setting, investigates the generation of answers as well as distractors, and is aimed at data augmentation. 7 Conclusion We introduced G-DAUGc , a novel data augmentation framework to generate synthetic training data, preserving quality and diversity. We demonstrate that G-DAUGc is effective on multiple commonsense reasoning benchmarks, wi"
2020.findings-emnlp.90,2021.ccl-1.108,0,0.136069,"Missing"
2020.findings-emnlp.90,K17-1004,1,0.899155,"Missing"
2020.findings-emnlp.90,P16-1009,0,0.0663565,"uman labeling is expensive. Additional analyses, provided in Appendix F, show that model sharpness approximated by the Hessian trace (Yao et al., 2019) does not completely explain G-DAUGc ’s performance; and, G-DAUGc is more effective than ensembling with a finetuned generator. 6 Related Work Data augmentation is a common practice in computer vision, where it takes the form of image transformations like translation and rotation (Perez and Wang, 2017). For language tasks, data augmentation is less straightforward. Broadly, previous augmentation methods have used back-translation architectures (Sennrich et al., 2016; Xie et al., 2019), heuristics based on syntactic and semantic properties of text including word replacements using a thesaurus (Zhang et al., 2015; Wei and Zou, 2019) and word embeddings (Wang and Yang, 2015; Fadaee et al., 2017; Kobayashi, 2018; Wu et al., 2019), and recently, generative models for synthesizing novel examples for text classification and reading comprehension (Anaby-Tavor et al., 2020; Kumar et al., 2020; Puri et al., 2020b). Our framework is similar to the last of these as we focus on generative models for data augmentation, but our work is the first to present a generative"
2020.findings-emnlp.90,D15-1306,0,\N,Missing
2020.findings-emnlp.90,P18-2102,1,\N,Missing
2020.findings-emnlp.90,D18-1424,0,\N,Missing
2020.findings-emnlp.90,P19-1472,1,\N,Missing
2020.findings-emnlp.90,N19-1421,0,\N,Missing
2020.findings-emnlp.90,D19-1670,0,\N,Missing
2020.findings-emnlp.90,2020.emnlp-main.468,0,\N,Missing
2020.findings-emnlp.90,W11-2853,0,\N,Missing
2021.acl-long.114,P19-1602,0,0.154245,"h uses a more complex pretraining method than our LMs. Note that DiPS generates multiple diverse paraphrases so we pick one at random. CGMH and R EFLECTIVE D ECODING both return multiple sampled, ranked paraphrases. We can easily control for N ovelty by taking the highestranked output that meets a N ovelty threshold. For both, we have a version with no threshold (T op), and with thresholds such that average N ovelty is 30 and 45. N ovelty cutoffs do not depend on the reference, only the source, and are equivalent to selecting with BLEU-ori (N ovelty is 100 − BLEU-ori) by Miao et al. (2019) or Bao et al. (2019). 3.2 Task: Abductive NLG Task: The Abductive natural language generation task (αNLG) presented in Bhagavatula et al. (2020) requires generating a hypothesis that fits 1440 SARI↑ Human ↑ N ovelty ↑ Source Reference 13.6 90.7 51.3 0.0 63.3 MT 36.1 70.9 30.4 R-VQVAE CGMHT op CGMH30 CGMH45 RDT op/30 (Us) RD45 (Us) 31.1 32.7 33.2 31.8 31.4 36.4 32.3 27.8 25.1 13.5 46.5 56.9 40.4 25.5 30.1 45.2 37.0 45.3 Method Table 2: Model performance on the Twitter URL test split. Bold indicates best for model-type. We show only metrics accounting for novelty (more in §C.3) between observations o1 and o2 , and"
2021.acl-long.114,P19-1470,1,0.815597,"ata splits. Metrics: For human evaluation, over 200 examples we ask 3 raters on Amazon Mechanical Turk about coherence between h and o1 , o2 , o1 + o2 , and overall quality on 4-value likert scales. We found Fleiss’ kappa (Fleiss, 1971) of 0.32, 0.40, 0.41, and 0.41 respectively, indicating fair to moderate agreement (Landis and Koch, 1977). Baselines: Parameters for R EFLECTIVE D ECOD ING are given in §B.4. We include baselines from the original work: different supervised variants of GPT-2 large with access to the observations, and optionally commonsense embeddings or generations from COMET (Bosselut et al., 2019). We include unsupervised baselines of GPT-2 conditioned on o1 + o2 directly, the gradient-based DeLorean model of Qin et al. (2020), and ILM infilling model of Donahue et al. (2020), representing recent unsupervised methods. 4 Results and Analysis Paraphrasing First, the Quora dataset: On automatic metrics from past works (BLEU, METEOR, TERP ) our lowest-N ovelty model setting (RDT op ) achieves the highest unsupervised scores, and highest overall on BLEU. Other high scoring rows (Source, PG-IL) are similarly low-N ovelty. The SARI metric explicitly balances N ovelty with similarity to refere"
2021.acl-long.114,D10-1090,0,0.0631389,"Missing"
2021.acl-long.114,P19-1605,0,0.101185,"f 0.40 (fluency), 0.54 (consistency), 0.77 (novelty) and 0.48 (overall) i.e. moderate to substantial agreement (Landis and Koch, 1977). On the Twitter corpus, we use 100 examples with agreement of 0.39, 0.42, 0.54, and 0.36, indicating fair to moderate agreement. On both we have 3 raters per example. See §C.2 for more. Baselines: Parameters for R EFLECTIVE D ECOD ING are given in §B.4. We mainly compare against 3 unsupervised baselines: Controlled Sentence Generation by Metropolis Hastings (CGMH from Miao et al. 2019), Simulated Annealing (UPSA from Liu et al. 2019) and the residual VQ-VAE of Roy and Grangier (2019a) (R-VQVAE). This is a cross-section of recent approaches (VAE, editing). We also compare against a machine-translation approach (see Sec 6), pivoting through German using Transformer (Vaswani et al., 2017) models trained on WMT19 data (Barrault et al., 2019). MT is included in a separate section in our results as it uses supervised bilingual data (Table 1). We include supervised baselines: the pointer generator trained by imitation learning (PG-IL) as in Du and Ji (2019), the diversity-promoting DiPS model (Kumar et al., 2019), and a finetuned BART model (Lewis et al., 2019), which uses a mo"
2021.acl-long.114,2020.acl-main.704,0,0.0607728,"Missing"
2021.acl-long.114,2020.emnlp-main.348,0,0.0415533,"Missing"
2021.acl-long.114,2020.emnlp-main.346,0,0.0279622,"early settings that favor supervised learning (narrow, known domain with abundant training data), but R EFLEC TIVE D ECODING is a good option to begin generating and exploring immediately with high quality generation. A useful abstraction for understanding R E FLECTIVE D ECODING for current applications is “prompting”, i.e., writing a prefix to implicitly or explicitly describe a task for a pretrained model. R E FLECTIVE D ECODING generates natural contexts that the desired generation would appear in. This breaks from other methods of automatic prompting, which often forego “natural” prompts (Shin et al., 2020; Reynolds and McDonell, 2021), even making them continuous (Li and Liang, 2021; Hambardzumyan et al., 2021; Lester et al., 2021; Qin and Eisner, 2021). R EFLECTIVE D ECODING also notably creates a set of prompts (contexts) for each example, where other methods attempt to learn an overall task prompt. Still, all of these are connected by the popular intuition that useful behavior in pretrained models can be induced through contextual input. Future Applications R EFLECTIVE D ECODING can extend beyond our experiments here, however. A simple example is in-context paraphrasing, i.e. writing a para"
2021.acl-long.158,2020.emnlp-main.48,1,0.877924,"“why” – we define a schema for approaching the problem of intent and provide a rich set of natural language responses. We also make a significant contribution towards the “what:” we include a physical-change question, provide rationales based in physical changes, and give structured annotations (bounding boxes) on what was changed in the edit. We introduce Edited Media Understanding Frames (EMU), a new conceptual formalism that captures the notions of “why” and “what” in image editing for language and vision systems (Figure 1). Following literature on pragmatic frames (Sap et al., 2017, 2020; Forbes et al., 2020)—derived from frame semantics (Baker et al., 1998)— we formalize EMU frames along six dimensions that cover a diverse range of inferences necessary to fully capture the scope of visual disinformation. We delve into the concept of intention as discussed by the fake news literature (Rashkin et al., 2017; Shu et al., 2017; Zhou and Zafarani, 2020) to capture editor’s intent such as motivation for edit and intent to deceive, as well as the resulting implications of the edited content. For every dimension we collect both a classification label and a free-form text explanation. For example, for fram"
2021.acl-long.158,W16-3210,0,0.050292,"Missing"
2021.acl-long.158,D17-1317,1,0.824912,"ing boxes) on what was changed in the edit. We introduce Edited Media Understanding Frames (EMU), a new conceptual formalism that captures the notions of “why” and “what” in image editing for language and vision systems (Figure 1). Following literature on pragmatic frames (Sap et al., 2017, 2020; Forbes et al., 2020)—derived from frame semantics (Baker et al., 1998)— we formalize EMU frames along six dimensions that cover a diverse range of inferences necessary to fully capture the scope of visual disinformation. We delve into the concept of intention as discussed by the fake news literature (Rashkin et al., 2017; Shu et al., 2017; Zhou and Zafarani, 2020) to capture editor’s intent such as motivation for edit and intent to deceive, as well as the resulting implications of the edited content. For every dimension we collect both a classification label and a free-form text explanation. For example, for frame intent, a model must classify the intent of the edit, and describe why this classification is selected. We then introduce a new dataset for our task, EMU, with 56k annotations over 8k image pairs. To kickstart progress on our task, we introduce a new language and vision model, PELICAN, that leverage"
2021.acl-long.158,P16-1030,1,0.82831,"model, PELICAN, improving over competitive languageand-vision transformer baselines. Our empirical study demonstrates promising results, but significant headroom remains. We release our dataset at jeffda.com/edited-media-understanding to encourage further study in discovering pragmatic markers of disinformation. 2 Defining Edited Media Understanding Frames Through an edit e on source image i (e.g. “e = x is edited into a room full of drugs”), an editor can cause harm to the subject x’s mental state (mental state: “x is angry about e”) and effect x’s image (effect: “e makes x seem dishonest”) (Rashkin et al., 2016). The editor does this through the intention of the edit (intent: “e intends to harm x’s image”) and changing the implications of the image (implication: “e frames x as a drug cartel member”) (Forbes et al., 2020; Sap et al., 2020; Paris and Donovan, 2019). To this end, we collect edits e and source images i from Reddit’s r/photoshopbattles community. There is no readily available (large) central database of harmful image edits, but r/photoshopbattles is replete with suitable complex and culturally implicative edits (e.g., reference to politics or pop culture). This provides us with relevant i"
2021.acl-long.158,D19-1514,0,0.0198296,"motivation for edit and intent to deceive, as well as the resulting implications of the edited content. For every dimension we collect both a classification label and a free-form text explanation. For example, for frame intent, a model must classify the intent of the edit, and describe why this classification is selected. We then introduce a new dataset for our task, EMU, with 56k annotations over 8k image pairs. To kickstart progress on our task, we introduce a new language and vision model, PELICAN, that leverages recent progress in pretrained multimodal representations of images and text (Tan and Bansal, 2019; Lu et al., 2019; Li et al., 2019). We compare our model to a suite of strong baselines, including a standard VLP model (Zhou et al., 2019), and show key improvement in terms of ability to reason about co-referent subjects in the edit. Nevertheless, our task is far from solved: a significant gap remains 1 How Georgia’s Senate race pits the Old South against the New South. https://www.politico.com/news/2020/12/05 /georgia-senate-old-new-south-442423 between the best machine and human accuracy. Our contributions are thus as follows. First, we introduce a new task of Edited Media Understanding F"
2021.acl-long.158,P19-1182,0,0.1923,"ington ♦ Stanford University jeffda.com/edited-media-understanding Abstract Understanding manipulated media, from automatically generated ‘deepfakes’ to manually edited ones, raises novel research challenges. Because the vast majority of edited or manipulated images are benign, such as photoshopped images for visual enhancements, the key challenge is to understand the complex layers of underlying intents of media edits and their implications with respect to disinformation. original Previous work: Understanding the structure of edits background changed person introduced (Jhamtani et al., 2018; Tan et al., 2019) horizontally ﬂipped In this paper, we study Edited Media Understanding Frames, a new conceptual formalism to understand visual media manipulation as structured annotations with respect to the intents, emotional reactions, effects on individuals, and the overall implications of disinformation. We introduce a dataset for our task, EMU, with 56k question-answer pairs written in rich natural language. We evaluate a wide variety of vision-and-language models for our task, and introduce a new model PELICAN, which builds upon recent progress in pretrained multimodal representations. Our model obtain"
2021.acl-long.158,Q14-1006,0,0.163841,"Missing"
2021.acl-long.158,2020.acl-main.486,1,0.744173,"to encourage further study in discovering pragmatic markers of disinformation. 2 Defining Edited Media Understanding Frames Through an edit e on source image i (e.g. “e = x is edited into a room full of drugs”), an editor can cause harm to the subject x’s mental state (mental state: “x is angry about e”) and effect x’s image (effect: “e makes x seem dishonest”) (Rashkin et al., 2016). The editor does this through the intention of the edit (intent: “e intends to harm x’s image”) and changing the implications of the image (implication: “e frames x as a drug cartel member”) (Forbes et al., 2020; Sap et al., 2020; Paris and Donovan, 2019). To this end, we collect edits e and source images i from Reddit’s r/photoshopbattles community. There is no readily available (large) central database of harmful image edits, but r/photoshopbattles is replete with suitable complex and culturally implicative edits (e.g., reference to politics or pop culture). This provides us with relevant image edits at a reasonable cost without advocation for dangerous training on real harmful image edits. Keeping the source image i in the task allows us to sustain the tractability of the image edit problem (Tan et al., 2019; Jhamt"
2021.acl-long.158,D17-1247,1,0.85293,"choose to focus on the “why” – we define a schema for approaching the problem of intent and provide a rich set of natural language responses. We also make a significant contribution towards the “what:” we include a physical-change question, provide rationales based in physical changes, and give structured annotations (bounding boxes) on what was changed in the edit. We introduce Edited Media Understanding Frames (EMU), a new conceptual formalism that captures the notions of “why” and “what” in image editing for language and vision systems (Figure 1). Following literature on pragmatic frames (Sap et al., 2017, 2020; Forbes et al., 2020)—derived from frame semantics (Baker et al., 1998)— we formalize EMU frames along six dimensions that cover a diverse range of inferences necessary to fully capture the scope of visual disinformation. We delve into the concept of intention as discussed by the fake news literature (Rashkin et al., 2017; Shu et al., 2017; Zhou and Zafarani, 2020) to capture editor’s intent such as motivation for edit and intent to deceive, as well as the resulting implications of the edited content. For every dimension we collect both a classification label and a free-form text explan"
2021.acl-long.158,P18-1238,0,0.0183168,"h caption with the relevant question. e. VLP (Zhou et al., 2019). We test VLP, a pre-trained vision-and-language transformer model. For image captioning, VLP takes a single image as input and uses an off-the-shelf object detector to extract regions, generation a caption using sequenceto-sequence decoding and treating the regions as a sequence of input tokens. To generate a caption for a particular question type, we fix the first few generated tokens to match the prefix for that question type. We fine-tune VLP starting from weights pre-trained on Conceptual Captions (3.3m image-caption pairs) (Sharma et al., 2018) and then further trained on COCO Captions (413k image-caption pairs) (Lin et al., 2014). Baselines In addition to evaluating PELICAN, we compare and evaluate the performance of various potentially high-performing baselines on our task. a. Retrieval. For a retrieval baseline, which generally performs well for generation-based tasks, we use features from ResNet-158 (He et al., 2016), defined as φ, to generate vectors for each IE in the test set. We then find the most similar edited image IT in the training set T via cosine similarity: argmax IT ∈T φ(IE ) · φ(IT ) kφ(IE )k × kφ(IT )k (6) We use"
2021.acl-long.158,P19-1644,0,0.0282548,"use on disinformation , by analyzing the reasons and rationales generated. We ask annotators to compare PELICAN-generated captions marked as “worse” Related Work Language-and-Vision Datasets Datasets involving images and languages cover a variety of tasks, including visual question answering (Agrawal et al., 2015; Goyal et al., 2017), image caption generation (Lin et al., 2014; Young et al., 2014; Krishna et al., 2016), visual storytelling (Park and Kim, 2015; Bosselut et al., 2016), machine translation (Elliott et al., 2016), visual reasoning (Johnson et al., 2017; Hudson and Manning, 2019; Suhr et al., 2019), and visual common sense (Zellers et al., 2019). Two-image tasks Though most computer vision tasks involve single images, some work has been done on exploring image pairs. The NLVR2 dataset (Suhr et al., 2019) involves yes-no question answering over image pairs. Neural Naturalist (Forbes et al., 2019) tests fine-grained captioning of bird pairs; (Jhamtani and Berg-Kirkpatrick, 2018) identifies the difference between two similar images. Image Edits There has been some computer vision research studying image edits. Unlike our EMU dataset, however, much of this work has focused on modeling lower"
2021.acl-long.159,D19-1219,0,0.0639942,"Missing"
2021.acl-long.159,P13-5002,0,0.0213646,"age annotation process. 2.1 Environment: THOR We use AI2-THOR as an environment for this task (Kolve et al., 2017). In THOR, a robotic agent can navigate around and perform rich contextual interactions with objects in a house. For instance, it can grab an Apple , slice it, put it in a Fridge , drop it, and so on. The state of the Apple , such as whether it is sliced or cold, changes accordingly; this is not possible in many other environments. In this work, we use the underlying THOR simulator as a proxy for grounded meaning. Within THOR, it can be seen as a ‘complete’ meaning representation (Artzi et al., 2013), as it fully specifies the kind of grounding a model can expect in its perception within THOR. Objects. The underlying THOR representation of each object o is in terms of 42 attributes; we provide a list in Appendix B. We treat these attributes as words specific to an attribute-level dictionary; for example, the temperature Hot is one of three possible values for an object’s temperature; the others being Cold and RoomTemp . Actions. An action a in THOR is a function that takes up to two objects as arguments. Actions are highly contextual, affecting not only the arguments but potentially other"
2021.acl-long.159,2020.acl-main.463,0,0.283639,"Missing"
2021.acl-long.159,2020.emnlp-main.703,1,0.867574,"Missing"
2021.acl-long.159,N19-1423,0,0.0645235,"Missing"
2021.acl-long.159,D19-1070,0,0.0335578,"Missing"
2021.acl-long.159,2020.tacl-1.5,0,0.029237,"Missing"
2021.acl-long.159,Q13-1016,0,0.0292025,"Missing"
2021.acl-long.159,P02-1040,0,0.109917,"Missing"
2021.acl-long.159,N16-1023,1,0.881383,"Missing"
2021.acl-long.159,D18-1009,1,0.865432,"Missing"
2021.acl-long.159,D17-1099,1,0.892466,"Missing"
2021.acl-long.159,P19-1472,1,0.819115,"ome their first “real words” upon which other language is scaffolded (Yu and Smith, 2012). In contrast, the dominant paradigm today is to train large language or vision models on static data, such as language and photos from the web. Yet such a setting is fundamentally limiting, as suggested empirically by psychologists’ failed attempts to get kittens to learn passively (Held and Hein, 1963). More recently, though large Transformers have made initial progress on benchmarks, they also have frequently revealed biases in those same datasets, suggesting they might not be solving underlying tasks (Zellers et al., 2019b). This has been argued philosophically by a flurry of re2040 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2040–2050 August 1–6, 2021. ©2021 Association for Computational Linguistics Throw object X at Y: Name: Vase Name: Laptop Name: Vase Name: Laptop Size: medium Size: medium Size: medium Size: medium isBroken: False isBroken: False isBroken: True isBroken: False isPickedUp: True isPickedUp: False &lt;throwHeldObjectAt, laptop&gt; isPickedUp: False isPickedUp: False . . . is"
2021.acl-long.522,2020.findings-emnlp.301,1,0.467449,"on both automatic and human evaluations. Moreover, because DE XPERTS operates only on the output of the pretrained LM, it is effective with (anti-)experts of smaller size, including when operating on GPT-3. Our work highlights the promise of tuning small LMs on text with (un)desirable attributes for efficient decoding-time steering. 1 Introduction Controlling the output of pretrained language models (LMs) is crucial for achieving useful and safe language generation applications, such as nonoffensive sentence completion or friendly conversation generation (See et al., 2019; Sheng et al., 2020; Gehman et al., 2020). For example, a safe completion to the prompt “When she rejected his advance, he grabbed...” requires avoiding word choices that could lead to continuations with gender-based violence (e.g., “her”; Figure 1). Without such steering, these language models risk generating mindless and offensive content (Sheng et al., 2019; Holtzman et al., 2020) which hinders their safe deployment (Brockman et al., 2020; Bender et al., 2021). Importantly, as the scale of pretrained LMs increases (e.g., 175B and 1.6T parameters; Brown et al., 2020; Fedus et al., Figure 1: Illustration of DE XPERTS, where a toxic"
2021.acl-long.522,P17-1059,0,0.023953,"α ą 0 indicates positive rewriting, and α ă 0 indicates negative rewriting. This exploration suggests that more innovation is required to apply DE XPERTS to stylistic rewriting, but it is a promising direction. We anticipate future work on the subject. 6 Related Work The task of controlling the output of a language generation model has been widely studied by previous work (for a review, see Prabhumoye et al., 2020). Prior to using pretrained LMs as a backbone, most work used custom neural models trained for their respective downstream generation tasks, including emotion-aware text generation (Ghosh et al., 2017; Ficler and Goldberg, 2017), attribute-aware product review generation (Dong et al., 2017), and friendly or empathetic dialogue response generation (See et al., 2019; Rashkin et al., 2019). Since pretrained LMs have shown impressive text generation ability (Radford et al., 2018, 2019), two directions have emerged to control their language generation: training approaches and decoding-time approaches. Training approaches include finetuning the pretrained LMs on datasets that contain the desired attributes (Gururangan et al., 2020) as well as creating a class-conditioned pretrained LM trained on"
2021.acl-long.549,N12-1049,0,0.0804095,"Missing"
2021.acl-long.549,P07-2044,0,0.0743229,"ent classification, mask-filling, and generation models, respectively. All models are of large size. on comparison-based instances seems similar. 6 Related Work Temporal commonsense reasoning. Early studies related to temporal analysis define time in the context of sets and relations (Bruce, 1972; Allen, 1983). More recent works often associate time with events and focus on identifying time expressions (Chang and Manning, 2012; Angeli et al., 2012; Lee et al., 2014), extracting temporal relations among events (Setzer and Gaizauskas, 2000; Pustejovsky et al., 2005; Lapata and Lascarides, 2006; Chambers et al., 2007; Ning et al., 2018b), and timeline construction (Do et al., 2012; Leeuwenberg and Moens, 2018). Some recent work has focused on building challenging benchmarks for temporal commonsense reasoning. Story Cloze Test focuses on stereotypical causal temporal and causal relations between events (Mostafazadeh et al., 2016). Vashishtha et al. 1 (2020) recast temporal reasoning datasets for event duration and event ordering into the natural language inference (NLI) format. Turque (Ning et al., 2020) is an reading comprehension dataset where the model needs to answer questions such as “what happens bef"
2021.acl-long.549,chang-manning-2012-sutime,0,0.0461503,"ed to be spuriously correlated with the dialog context. For example, we include temporal spans in the dialog context as negative options, which will challenge models that rely primarily only on shallow pattern matching without correct temporal reasoning. We present more information in §3 about how the negative options were created by human annotators. 2 Temporal expression identification. Here, we select dialogs that are rich with temporal information, in order to focus on complex temporal reasoning that arises in natural dialogs. Temporal expressions are automatically identified with SUTime (Chang and Manning, 2012), an off-the-shelf Task: Temporal Reasoning in Dialog We formulate the dialog-based temporal commonsense reasoning problem as a cloze task (Taylor, 1953). Formally, given a multi-turn dialog context of n conversational turns between two speakers A 3 Dataset: T IME D IAL The T IME D IAL dataset is derived from DailyDialog data (Li et al., 2017), which is a multi-turn dialog corpus containing over 13K English dialogs. Dialogs in this dataset consist of turn-taking between two people on topics over 10 broad categories, ranging from daily lives to financial topics. 3.1 Data Collection Our data col"
2021.acl-long.549,D19-1109,0,0.02339,"soning in dialogs which often require significant commonsense and world knowledge to reason over rich context (Qin et al., 2019b; Dinan et al., 2018). Commonsense reasoning with LMs. With the recent success of large pre-trained language models 7073 (LMs) (Devlin et al., 2019; Brown et al., 2020), it is an open question whether these models, pretrained on large amounts of data, capture commonsense knowledge. Several works have been proposed to assess the ability of LMs for commonsense or numerical reasoning (Zhang et al., 2020; Bouraoui et al., 2020), or to mine commonsense knowledge from LMs (Davison et al., 2019). Lin et al. (2020) showed that state-of-the-art LMs such as BERT and RoBERTa performs poorly on numerical reasoning tasks without any finetuning. Works have also been proposed to improve language model’s commonsense reasoning (Qin et al., 2020, 2019a; Zhou et al., 2020) and numerical reasoning abilities (Geva et al., 2020). In our work, we study several modeling approaches and finetuning settings of large LMs, and establish strong baselines for temporal commonsense reasoning in dialogs. 7 Conclusions We introduced T IME D IAL, a challenge set consistting of 1.1K multiple-choice cloze question"
2021.acl-long.549,N19-1423,0,0.128778,"and outputs a score measuring how likely the candidate being a correct answer. Based on the prediction scores of all options, the model then chooses the top two positive candidates as the predicted answer for the instance. Each paradigm of models is finetuned using training data from different domains, as discussed in §4.3. 4.1.1 Binary Classification In this setting, we formulate the task as a binary classification problem, i.e., we use a classifier to measure the probability of the candidate in the (masked dialog context, candidate) pair being a correct answer. Any powerful LM — e.g., BERT (Devlin et al., 2019), ALBERT (Lan et al., 2019), RO BERTA (Liu et al., 2019), etc. can be used to build the classifier. This method’s key challenge is the lack of annotated training data for direct supervision. We generate weak supervision training data as follows. In an unlabeled corpus, we use the SUTime tool 7069 Input: (2) Mask Filling (1) Classification Output: …… B: No, not all summer. Just for six weeks. Output: 1 A: I am afraid I can only rent it for two months. Classification Layer B: My holiday is only _______, but I think my brother and his family would take it for the other two weeks. BERT Options: a)"
2021.acl-long.549,D12-1062,0,0.0337886,"All models are of large size. on comparison-based instances seems similar. 6 Related Work Temporal commonsense reasoning. Early studies related to temporal analysis define time in the context of sets and relations (Bruce, 1972; Allen, 1983). More recent works often associate time with events and focus on identifying time expressions (Chang and Manning, 2012; Angeli et al., 2012; Lee et al., 2014), extracting temporal relations among events (Setzer and Gaizauskas, 2000; Pustejovsky et al., 2005; Lapata and Lascarides, 2006; Chambers et al., 2007; Ning et al., 2018b), and timeline construction (Do et al., 2012; Leeuwenberg and Moens, 2018). Some recent work has focused on building challenging benchmarks for temporal commonsense reasoning. Story Cloze Test focuses on stereotypical causal temporal and causal relations between events (Mostafazadeh et al., 2016). Vashishtha et al. 1 (2020) recast temporal reasoning datasets for event duration and event ordering into the natural language inference (NLI) format. Turque (Ning et al., 2020) is an reading comprehension dataset where the model needs to answer questions such as “what happens before/after [event]”. Most related to our work is McTaco (Zhou et a"
2021.acl-long.549,2020.acl-main.89,0,0.0217746,"these models, pretrained on large amounts of data, capture commonsense knowledge. Several works have been proposed to assess the ability of LMs for commonsense or numerical reasoning (Zhang et al., 2020; Bouraoui et al., 2020), or to mine commonsense knowledge from LMs (Davison et al., 2019). Lin et al. (2020) showed that state-of-the-art LMs such as BERT and RoBERTa performs poorly on numerical reasoning tasks without any finetuning. Works have also been proposed to improve language model’s commonsense reasoning (Qin et al., 2020, 2019a; Zhou et al., 2020) and numerical reasoning abilities (Geva et al., 2020). In our work, we study several modeling approaches and finetuning settings of large LMs, and establish strong baselines for temporal commonsense reasoning in dialogs. 7 Conclusions We introduced T IME D IAL, a challenge set consistting of 1.1K multiple-choice cloze questions for temporal commonsense reasoning in dialog. The dataset is carefully curated to evaluate a models’ ability to do temporal commonsense/numerical reasoning over dialog context. In order to establish strong baselines and provide information on future model development, we conducted extensive experiments with state-of-the-a"
2021.acl-long.549,N18-2017,0,0.0200283,". and B, where a temporal words span within the context is masked out, the task is to predict the suitable temporal expression(s) for the masked-out span from a list of options. That is, we want the conversation model to select all the correct answers from the options based on the dialog context. Following similar cloze-style challenge datasets, we use accuracy as the evaluation metric (Mostafazadeh et al., 2016; Onishi et al., 2016; Mihaylov and Frank, 2018). Having a non-trivial set of options is crucial to build a challenge set and to avoid accidental spurious biases (Geirhos et al., 2020; Gururangan et al., 2018; Le Bras et al., 2020). We ensure this via the following filtering process. (1) For each masked span, there is more than one correct answer in the options. This makes the task more challenging for models since more comprehensive understanding of the context is required to recognize all the correct choices. In our dataset (§3) we guarantee two correct answers for each masked span. (2) Some incorrect options are selected to be spuriously correlated with the dialog context. For example, we include temporal spans in the dialog context as negative options, which will challenge models that rely pri"
2021.acl-long.549,2020.emnlp-main.88,0,0.0176383,"ns among events (Setzer and Gaizauskas, 2000; Pustejovsky et al., 2005; Lapata and Lascarides, 2006; Chambers et al., 2007; Ning et al., 2018b), and timeline construction (Do et al., 2012; Leeuwenberg and Moens, 2018). Some recent work has focused on building challenging benchmarks for temporal commonsense reasoning. Story Cloze Test focuses on stereotypical causal temporal and causal relations between events (Mostafazadeh et al., 2016). Vashishtha et al. 1 (2020) recast temporal reasoning datasets for event duration and event ordering into the natural language inference (NLI) format. Turque (Ning et al., 2020) is an reading comprehension dataset where the model needs to answer questions such as “what happens before/after [event]”. Most related to our work is McTaco (Zhou et al., 2019), a dataset for evaluating temporal commonsense in the form of multiple-choice reading comprehension, where the context usually consists of a single sentence. Our work instead studies temporal commonsense reasoning in dialogs which often require significant commonsense and world knowledge to reason over rich context (Qin et al., 2019b; Dinan et al., 2018). Commonsense reasoning with LMs. With the recent success of larg"
2021.acl-long.549,N18-1077,0,0.0871066,"r machines (Kahn and Gorry, 1977; Kozareva and Hovy, 2011) since it requires both understanding the local temporal expressions and reasoning about their global contexts such as their relative ordering and relations Work done during an internship at Google. b) 45 days 3 d) two months 7 Table 1: Examples from our T IME D IAL challenge set, demonstrating the need for commonsense knowledge and arithmetic reasoning over the context to infer the correct answers. Key contextual information for reasoning success is highlighted. Introduction ∗ b) 30 years old 7 d) 18 years old 3 (UzZaman et al., 2013; Ning et al., 2018b; Pustejovsky, 2017). The problem becomes even more challenging in dialogs, where explicit and implicit inter-dependencies among temporal concepts can appear across conversation turns. For instance, for the first dialog in Table 1, one must understand the context, i.e., selling wine, and use world knowledge of minimum legal drinking age in order to reason about correct answers to fill in the blank. Similarly, in the second conversation, commonsense about the durations summer, month, week, day and their relations, plus numerical reasoning, are necessary to make the inference. Although previous"
2021.acl-long.549,D16-1241,0,0.0207165,"e sufficient for robust temporal reasoning in dialogs, and motivate future research toward modeling temporal concepts over diverse everyday events, and contextual reasoning about them. and B, where a temporal words span within the context is masked out, the task is to predict the suitable temporal expression(s) for the masked-out span from a list of options. That is, we want the conversation model to select all the correct answers from the options based on the dialog context. Following similar cloze-style challenge datasets, we use accuracy as the evaluation metric (Mostafazadeh et al., 2016; Onishi et al., 2016; Mihaylov and Frank, 2018). Having a non-trivial set of options is crucial to build a challenge set and to avoid accidental spurious biases (Geirhos et al., 2020; Gururangan et al., 2018; Le Bras et al., 2020). We ensure this via the following filtering process. (1) For each masked span, there is more than one correct answer in the options. This makes the task more challenging for models since more comprehensive understanding of the context is required to recognize all the correct choices. In our dataset (§3) we guarantee two correct answers for each masked span. (2) Some incorrect options ar"
2021.acl-long.549,P06-1095,0,0.133773,"ypes. CLS, MF, and GEN represent classification, mask-filling, and generation models, respectively. All models are of large size. on comparison-based instances seems similar. 6 Related Work Temporal commonsense reasoning. Early studies related to temporal analysis define time in the context of sets and relations (Bruce, 1972; Allen, 1983). More recent works often associate time with events and focus on identifying time expressions (Chang and Manning, 2012; Angeli et al., 2012; Lee et al., 2014), extracting temporal relations among events (Setzer and Gaizauskas, 2000; Pustejovsky et al., 2005; Lapata and Lascarides, 2006; Chambers et al., 2007; Ning et al., 2018b), and timeline construction (Do et al., 2012; Leeuwenberg and Moens, 2018). Some recent work has focused on building challenging benchmarks for temporal commonsense reasoning. Story Cloze Test focuses on stereotypical causal temporal and causal relations between events (Mostafazadeh et al., 2016). Vashishtha et al. 1 (2020) recast temporal reasoning datasets for event duration and event ordering into the natural language inference (NLI) format. Turque (Ning et al., 2020) is an reading comprehension dataset where the model needs to answer questions su"
2021.acl-long.549,P14-1135,0,0.0191428,"es. The performance CLS-IN CLS-OUT MF-IN MF-OUT GEN-IN GEN-OUT Figure 3: Percentage of errors on different reasoning types. CLS, MF, and GEN represent classification, mask-filling, and generation models, respectively. All models are of large size. on comparison-based instances seems similar. 6 Related Work Temporal commonsense reasoning. Early studies related to temporal analysis define time in the context of sets and relations (Bruce, 1972; Allen, 1983). More recent works often associate time with events and focus on identifying time expressions (Chang and Manning, 2012; Angeli et al., 2012; Lee et al., 2014), extracting temporal relations among events (Setzer and Gaizauskas, 2000; Pustejovsky et al., 2005; Lapata and Lascarides, 2006; Chambers et al., 2007; Ning et al., 2018b), and timeline construction (Do et al., 2012; Leeuwenberg and Moens, 2018). Some recent work has focused on building challenging benchmarks for temporal commonsense reasoning. Story Cloze Test focuses on stereotypical causal temporal and causal relations between events (Mostafazadeh et al., 2016). Vashishtha et al. 1 (2020) recast temporal reasoning datasets for event duration and event ordering into the natural language inf"
2021.acl-long.549,D18-1155,0,0.0983798,"ation, commonsense about the durations summer, month, week, day and their relations, plus numerical reasoning, are necessary to make the inference. Although previous works have studied temporal reasoning in natural language, they have either focused on specific time-related concepts in 7066 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 7066–7076 August 1–6, 2021. ©2021 Association for Computational Linguistics isolation, such as temporal ordering and relation extraction (Leeuwenberg and Moens, 2018; Ning et al., 2018a), and/or dealt with limited context, such as single-sentence-based question answering (Zhou et al., 2019) and natural language inference (Vashishtha et al., 2020; Mostafazadeh et al., 2016). In this work, we make the first systematic study of temporal commonsense reasoning in a multi-turn dialog setting. The task involves complex reasoning that requires operations like comparison and arithmetic reasoning over temporal expressions and the need for commonsense and world knowledge. We design a new task for dialog-based temporal reasoning and present a new challenge set in Eng"
2021.acl-long.549,D19-1509,1,0.849238,"uration and event ordering into the natural language inference (NLI) format. Turque (Ning et al., 2020) is an reading comprehension dataset where the model needs to answer questions such as “what happens before/after [event]”. Most related to our work is McTaco (Zhou et al., 2019), a dataset for evaluating temporal commonsense in the form of multiple-choice reading comprehension, where the context usually consists of a single sentence. Our work instead studies temporal commonsense reasoning in dialogs which often require significant commonsense and world knowledge to reason over rich context (Qin et al., 2019b; Dinan et al., 2018). Commonsense reasoning with LMs. With the recent success of large pre-trained language models 7073 (LMs) (Devlin et al., 2019; Brown et al., 2020), it is an open question whether these models, pretrained on large amounts of data, capture commonsense knowledge. Several works have been proposed to assess the ability of LMs for commonsense or numerical reasoning (Zhang et al., 2020; Bouraoui et al., 2020), or to mine commonsense knowledge from LMs (Davison et al., 2019). Lin et al. (2020) showed that state-of-the-art LMs such as BERT and RoBERTa performs poorly on numerical"
2021.acl-long.549,P19-1539,1,0.800855,"uration and event ordering into the natural language inference (NLI) format. Turque (Ning et al., 2020) is an reading comprehension dataset where the model needs to answer questions such as “what happens before/after [event]”. Most related to our work is McTaco (Zhou et al., 2019), a dataset for evaluating temporal commonsense in the form of multiple-choice reading comprehension, where the context usually consists of a single sentence. Our work instead studies temporal commonsense reasoning in dialogs which often require significant commonsense and world knowledge to reason over rich context (Qin et al., 2019b; Dinan et al., 2018). Commonsense reasoning with LMs. With the recent success of large pre-trained language models 7073 (LMs) (Devlin et al., 2019; Brown et al., 2020), it is an open question whether these models, pretrained on large amounts of data, capture commonsense knowledge. Several works have been proposed to assess the ability of LMs for commonsense or numerical reasoning (Zhang et al., 2020; Bouraoui et al., 2020), or to mine commonsense knowledge from LMs (Davison et al., 2019). Lin et al. (2020) showed that state-of-the-art LMs such as BERT and RoBERTa performs poorly on numerical"
2021.acl-long.549,I17-1099,0,0.0227372,", 2016). In this work, we make the first systematic study of temporal commonsense reasoning in a multi-turn dialog setting. The task involves complex reasoning that requires operations like comparison and arithmetic reasoning over temporal expressions and the need for commonsense and world knowledge. We design a new task for dialog-based temporal reasoning and present a new challenge set in English, called T IME D IAL, to evaluate language understanding models on the task. We formulate the problem as a crowd-sourced cloze task with multiple choices based on dialogs in the DailyDialog dataset (Li et al., 2017). Given a dialog with one temporal span masked out, the model is asked to find all correct answers from a list of four options to fill in the blank (Table 1). The challenge set requires the models to demonstrate understanding of the context and use temporal commonsense to make right choices. Our final challenge set consists of 1.1K carefully curated dialog instances. We then study the performance of several stateof-the-art pre-trained language models on T IME D IAL along several dimensions including modeling paradigms (classification, mask filling, and generation), the scope of dialog contexts"
2021.acl-long.549,2020.emnlp-main.58,1,0.818348,"Missing"
2021.acl-long.549,2020.emnlp-main.557,0,0.0155892,"often require significant commonsense and world knowledge to reason over rich context (Qin et al., 2019b; Dinan et al., 2018). Commonsense reasoning with LMs. With the recent success of large pre-trained language models 7073 (LMs) (Devlin et al., 2019; Brown et al., 2020), it is an open question whether these models, pretrained on large amounts of data, capture commonsense knowledge. Several works have been proposed to assess the ability of LMs for commonsense or numerical reasoning (Zhang et al., 2020; Bouraoui et al., 2020), or to mine commonsense knowledge from LMs (Davison et al., 2019). Lin et al. (2020) showed that state-of-the-art LMs such as BERT and RoBERTa performs poorly on numerical reasoning tasks without any finetuning. Works have also been proposed to improve language model’s commonsense reasoning (Qin et al., 2020, 2019a; Zhou et al., 2020) and numerical reasoning abilities (Geva et al., 2020). In our work, we study several modeling approaches and finetuning settings of large LMs, and establish strong baselines for temporal commonsense reasoning in dialogs. 7 Conclusions We introduced T IME D IAL, a challenge set consistting of 1.1K multiple-choice cloze questions for temporal comm"
2021.acl-long.549,2021.ccl-1.108,0,0.0602365,"Missing"
2021.acl-long.549,P18-1076,0,0.0242063,"st temporal reasoning in dialogs, and motivate future research toward modeling temporal concepts over diverse everyday events, and contextual reasoning about them. and B, where a temporal words span within the context is masked out, the task is to predict the suitable temporal expression(s) for the masked-out span from a list of options. That is, we want the conversation model to select all the correct answers from the options based on the dialog context. Following similar cloze-style challenge datasets, we use accuracy as the evaluation metric (Mostafazadeh et al., 2016; Onishi et al., 2016; Mihaylov and Frank, 2018). Having a non-trivial set of options is crucial to build a challenge set and to avoid accidental spurious biases (Geirhos et al., 2020; Gururangan et al., 2018; Le Bras et al., 2020). We ensure this via the following filtering process. (1) For each masked span, there is more than one correct answer in the options. This makes the task more challenging for models since more comprehensive understanding of the context is required to recognize all the correct choices. In our dataset (§3) we guarantee two correct answers for each masked span. (2) Some incorrect options are selected to be spuriously"
2021.acl-long.549,N16-1098,0,0.444559,"ral language, they have either focused on specific time-related concepts in 7066 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 7066–7076 August 1–6, 2021. ©2021 Association for Computational Linguistics isolation, such as temporal ordering and relation extraction (Leeuwenberg and Moens, 2018; Ning et al., 2018a), and/or dealt with limited context, such as single-sentence-based question answering (Zhou et al., 2019) and natural language inference (Vashishtha et al., 2020; Mostafazadeh et al., 2016). In this work, we make the first systematic study of temporal commonsense reasoning in a multi-turn dialog setting. The task involves complex reasoning that requires operations like comparison and arithmetic reasoning over temporal expressions and the need for commonsense and world knowledge. We design a new task for dialog-based temporal reasoning and present a new challenge set in English, called T IME D IAL, to evaluate language understanding models on the task. We formulate the problem as a crowd-sourced cloze task with multiple choices based on dialogs in the DailyDialog dataset (Li et a"
2021.acl-long.549,P18-1212,0,0.0762013,"r machines (Kahn and Gorry, 1977; Kozareva and Hovy, 2011) since it requires both understanding the local temporal expressions and reasoning about their global contexts such as their relative ordering and relations Work done during an internship at Google. b) 45 days 3 d) two months 7 Table 1: Examples from our T IME D IAL challenge set, demonstrating the need for commonsense knowledge and arithmetic reasoning over the context to infer the correct answers. Key contextual information for reasoning success is highlighted. Introduction ∗ b) 30 years old 7 d) 18 years old 3 (UzZaman et al., 2013; Ning et al., 2018b; Pustejovsky, 2017). The problem becomes even more challenging in dialogs, where explicit and implicit inter-dependencies among temporal concepts can appear across conversation turns. For instance, for the first dialog in Table 1, one must understand the context, i.e., selling wine, and use world knowledge of minimum legal drinking age in order to reason about correct answers to fill in the blank. Similarly, in the second conversation, commonsense about the durations summer, month, week, day and their relations, plus numerical reasoning, are necessary to make the inference. Although previous"
2021.acl-long.549,setzer-gaizauskas-2000-annotating,0,0.256766,"Figure 3: Percentage of errors on different reasoning types. CLS, MF, and GEN represent classification, mask-filling, and generation models, respectively. All models are of large size. on comparison-based instances seems similar. 6 Related Work Temporal commonsense reasoning. Early studies related to temporal analysis define time in the context of sets and relations (Bruce, 1972; Allen, 1983). More recent works often associate time with events and focus on identifying time expressions (Chang and Manning, 2012; Angeli et al., 2012; Lee et al., 2014), extracting temporal relations among events (Setzer and Gaizauskas, 2000; Pustejovsky et al., 2005; Lapata and Lascarides, 2006; Chambers et al., 2007; Ning et al., 2018b), and timeline construction (Do et al., 2012; Leeuwenberg and Moens, 2018). Some recent work has focused on building challenging benchmarks for temporal commonsense reasoning. Story Cloze Test focuses on stereotypical causal temporal and causal relations between events (Mostafazadeh et al., 2016). Vashishtha et al. 1 (2020) recast temporal reasoning datasets for event duration and event ordering into the natural language inference (NLI) format. Turque (Ning et al., 2020) is an reading comprehensi"
2021.acl-long.549,S13-2001,0,0.038889,"as been challenging for machines (Kahn and Gorry, 1977; Kozareva and Hovy, 2011) since it requires both understanding the local temporal expressions and reasoning about their global contexts such as their relative ordering and relations Work done during an internship at Google. b) 45 days 3 d) two months 7 Table 1: Examples from our T IME D IAL challenge set, demonstrating the need for commonsense knowledge and arithmetic reasoning over the context to infer the correct answers. Key contextual information for reasoning success is highlighted. Introduction ∗ b) 30 years old 7 d) 18 years old 3 (UzZaman et al., 2013; Ning et al., 2018b; Pustejovsky, 2017). The problem becomes even more challenging in dialogs, where explicit and implicit inter-dependencies among temporal concepts can appear across conversation turns. For instance, for the first dialog in Table 1, one must understand the context, i.e., selling wine, and use world knowledge of minimum legal drinking age in order to reason about correct answers to fill in the blank. Similarly, in the second conversation, commonsense about the durations summer, month, week, day and their relations, plus numerical reasoning, are necessary to make the inference"
2021.acl-long.549,2020.findings-emnlp.363,0,0.0632093,"Missing"
2021.acl-long.549,D19-1332,0,0.335637,"e inference. Although previous works have studied temporal reasoning in natural language, they have either focused on specific time-related concepts in 7066 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 7066–7076 August 1–6, 2021. ©2021 Association for Computational Linguistics isolation, such as temporal ordering and relation extraction (Leeuwenberg and Moens, 2018; Ning et al., 2018a), and/or dealt with limited context, such as single-sentence-based question answering (Zhou et al., 2019) and natural language inference (Vashishtha et al., 2020; Mostafazadeh et al., 2016). In this work, we make the first systematic study of temporal commonsense reasoning in a multi-turn dialog setting. The task involves complex reasoning that requires operations like comparison and arithmetic reasoning over temporal expressions and the need for commonsense and world knowledge. We design a new task for dialog-based temporal reasoning and present a new challenge set in English, called T IME D IAL, to evaluate language understanding models on the task. We formulate the problem as a crowd-sourced c"
2021.acl-long.549,2020.findings-emnlp.439,0,0.0227692,"sually consists of a single sentence. Our work instead studies temporal commonsense reasoning in dialogs which often require significant commonsense and world knowledge to reason over rich context (Qin et al., 2019b; Dinan et al., 2018). Commonsense reasoning with LMs. With the recent success of large pre-trained language models 7073 (LMs) (Devlin et al., 2019; Brown et al., 2020), it is an open question whether these models, pretrained on large amounts of data, capture commonsense knowledge. Several works have been proposed to assess the ability of LMs for commonsense or numerical reasoning (Zhang et al., 2020; Bouraoui et al., 2020), or to mine commonsense knowledge from LMs (Davison et al., 2019). Lin et al. (2020) showed that state-of-the-art LMs such as BERT and RoBERTa performs poorly on numerical reasoning tasks without any finetuning. Works have also been proposed to improve language model’s commonsense reasoning (Qin et al., 2020, 2019a; Zhou et al., 2020) and numerical reasoning abilities (Geva et al., 2020). In our work, we study several modeling approaches and finetuning settings of large LMs, and establish strong baselines for temporal commonsense reasoning in dialogs. 7 Conclusions We"
2021.acl-long.549,2020.acl-main.678,0,0.0161864,"9; Brown et al., 2020), it is an open question whether these models, pretrained on large amounts of data, capture commonsense knowledge. Several works have been proposed to assess the ability of LMs for commonsense or numerical reasoning (Zhang et al., 2020; Bouraoui et al., 2020), or to mine commonsense knowledge from LMs (Davison et al., 2019). Lin et al. (2020) showed that state-of-the-art LMs such as BERT and RoBERTa performs poorly on numerical reasoning tasks without any finetuning. Works have also been proposed to improve language model’s commonsense reasoning (Qin et al., 2020, 2019a; Zhou et al., 2020) and numerical reasoning abilities (Geva et al., 2020). In our work, we study several modeling approaches and finetuning settings of large LMs, and establish strong baselines for temporal commonsense reasoning in dialogs. 7 Conclusions We introduced T IME D IAL, a challenge set consistting of 1.1K multiple-choice cloze questions for temporal commonsense reasoning in dialog. The dataset is carefully curated to evaluate a models’ ability to do temporal commonsense/numerical reasoning over dialog context. In order to establish strong baselines and provide information on future model development,"
2021.eacl-main.274,2020.findings-emnlp.301,1,0.840062,"of social biases or toxicity (e.g., Social Bias Frames; Sap et al., 2020). Ethical Implications & Limitations The above synthetic setting is meant to illustrate the role of labeling quality on biases in annotations. We strongly caution against using this approach in real-world applications, such as building parallel datasets for dialects. First, due to how its training data was selected, GPT-3 has likely not been exposed to many African American English varieties during training (Jo and Gebru, 2020). Second, pretrained language models are known to generate toxic language at non-trivial rates (Gehman et al., 2020), which could cause differential toxicity in the translations. 7 Related Work Debiasing Toxicity Detection As the popularity of hate speech and toxic language detection sys3150 AAE GPT-3 WAE Translation RT @user I can’t stand a bad texter bruh like don’t be mad if I forget about yo ass RT @user Retweet if you fuck with this!!!! RT @user That nigga needs anger management RT @user oh fucking hell take a day off man RT @user I can’t stand a bad texter bro like don’t be mad if I forget about you RT @user Retweet if you like this! RT @user That guy needs anger management RT @user oh fuck take a day"
2021.eacl-main.274,2020.emnlp-main.473,0,0.377726,"Missing"
2021.eacl-main.274,N18-2017,1,0.849901,"asks that debiasing methods have been successful on, such as textual entailment (e.g., SNLI, MNLI; Bowman et al., 2015; Williams et al., 2018) or reading comprehension (e.g., SQuAD; Rajpurkar et al., 2016). First, compared to these NLU tasks where there is one correct label, the toxicity of language is inherently more nuanced, subjective, and contextual, which causes toxic language datasets to have lower agreement in general (Ross et al., 2017). Second, the dataset biases in NLU are predominantly artifacts introduced during data creation (e.g., negations, exaggerations; Schwartz et al., 2017; Gururangan et al., 2018), whereas those in toxic language detection are grounded in the social dynamics of the world (Spears, 1998; Technau, 2018). For example, viewing AAE as a more toxic or less proper variety of English is a form of linguistic discrimination that upholds racial hierarchies in the United States (Rosa and Flores, 2017). In this work, we consider two broad categories of toxic language dataset biases—lexical (§2.1) and dialectal (§2.2). Our experiments focus on a single, widely used dataset (§2.3) from Founta et al. (2018). 2.1 Lexical Biases (T OX T RIG) Current toxic language detection systems often"
2021.eacl-main.274,D19-6115,0,0.102916,"inan et al., 2019) and dialectal bias, where toxicity is correlated with surface markers of African American English (AAE; Davidson et al., 2019; Sap et al., 2019). When trained on biased datasets, models acquire and exacerbate these biases (e.g., flagging text by Black authors as more toxic than by white authors; Sap et al., 2019; Zhang et al., 2018). Concurrently, there has been elevated interest in developing debiasing methods for standard natural language understanding (NLU) tasks, i.e., methods that aim to decrease over-reliance on spurious correlations in NLU models (Clark et al., 2019; He et al., 2019; Karimi Mahabadi et al., 2020; Bras et al., 2020). This raises a natural question: are 3143 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 3143–3155 April 19 - 23, 2021. ©2021 Association for Computational Linguistics current debiasing approaches effective for mitigating biases specific to toxic language detection? In this work, we address the above question by investigating two classes of debiasing approaches to mitigate lexical and dialectal biases—one that employs additional training objectives for bias removal, and anothe"
2021.eacl-main.274,2020.acl-main.769,0,0.207937,"ialectal bias, where toxicity is correlated with surface markers of African American English (AAE; Davidson et al., 2019; Sap et al., 2019). When trained on biased datasets, models acquire and exacerbate these biases (e.g., flagging text by Black authors as more toxic than by white authors; Sap et al., 2019; Zhang et al., 2018). Concurrently, there has been elevated interest in developing debiasing methods for standard natural language understanding (NLU) tasks, i.e., methods that aim to decrease over-reliance on spurious correlations in NLU models (Clark et al., 2019; He et al., 2019; Karimi Mahabadi et al., 2020; Bras et al., 2020). This raises a natural question: are 3143 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 3143–3155 April 19 - 23, 2021. ©2021 Association for Computational Linguistics current debiasing approaches effective for mitigating biases specific to toxic language detection? In this work, we address the above question by investigating two classes of debiasing approaches to mitigate lexical and dialectal biases—one that employs additional training objectives for bias removal, and another that filters training instan"
2021.eacl-main.274,D19-1461,0,0.245324,"g majority identity mentions, as illustrated in Figure 1. At the core of the issue are dataset biases, i.e., spurious correlations between surface patterns and annotated toxicity labels (§2), which stem from the data creation process (Sap et al., 2019). Previous work has outlined two such biases for hate 1 We use hate speech and toxic language interchangeably in this work, though their definitions do not perfectly align. speech datasets (both shown in Figure 1): lexical bias which associates toxicity with the presence of certain words (e.g., profanities, identity mentions; Dixon et al., 2018; Dinan et al., 2019) and dialectal bias, where toxicity is correlated with surface markers of African American English (AAE; Davidson et al., 2019; Sap et al., 2019). When trained on biased datasets, models acquire and exacerbate these biases (e.g., flagging text by Black authors as more toxic than by white authors; Sap et al., 2019; Zhang et al., 2018). Concurrently, there has been elevated interest in developing debiasing methods for standard natural language understanding (NLU) tasks, i.e., methods that aim to decrease over-reliance on spurious correlations in NLU models (Clark et al., 2019; He et al., 2019; K"
2021.eacl-main.274,2021.ccl-1.108,0,0.0951961,"Missing"
2021.eacl-main.274,D18-1302,0,0.0149317,"ke a day off man Gold New A ¨ A ¨ A ¨ A A Table 6: Examples of AAE tweets with their GPT-3 based WAE translation, and original gold standard and new annotations based on AAE-relabeled. For the first three tweets, the (biased) gold labels are changed by models predicting the new labels on their WAE translations. indicates presence of toxicity, and ¨ represents nontoxic. We anonymize the usernames to protect user privacy. A tems has grown, several biases have been found in dataset and models, spurring various debiasing efforts to mitigate these individual biases (e.g., gender bias, racial bias; Park et al., 2018; Sap et al., 2019; Davidson et al., 2019). Some work tackles identity-based biases, e.g., using data re-balancing (Dixon et al., 2018), or adversarial feature learning (Vaidya et al., 2019). Less work has tackled racial or dialectal bias. Notably, Xia et al. (2020) use adversarial training to prevent the model from associating toxicity with AAE, showing only small improvements in fairness. Based on those results, we do not explore adversarial methods, opting instead for ensemble-based methods of predefined bias reduction. In contemporary work, Mozafari et al. (2020) use a re-weighting mechani"
2021.eacl-main.274,C18-1130,0,0.0655929,"Missing"
2021.eacl-main.274,D16-1264,0,0.0299616,"goal of moderating online communities (Roberts, 2019; Vidgen et al., 2019). 2 https://github.com/XuhuiZhou/Toxic_ Debias 3 Our definition of “bias” is specific to the social biases in toxic language detection datasets, grounded as lexical and dialectal biases; see Blodgett et al. (2020) for a detailed investigation of the term “bias”. This task differs in several ways from the natural language understanding (NLU) tasks that debiasing methods have been successful on, such as textual entailment (e.g., SNLI, MNLI; Bowman et al., 2015; Williams et al., 2018) or reading comprehension (e.g., SQuAD; Rajpurkar et al., 2016). First, compared to these NLU tasks where there is one correct label, the toxicity of language is inherently more nuanced, subjective, and contextual, which causes toxic language datasets to have lower agreement in general (Ross et al., 2017). Second, the dataset biases in NLU are predominantly artifacts introduced during data creation (e.g., negations, exaggerations; Schwartz et al., 2017; Gururangan et al., 2018), whereas those in toxic language detection are grounded in the social dynamics of the world (Spears, 1998; Technau, 2018). For example, viewing AAE as a more toxic or less proper v"
2021.eacl-main.274,N18-1101,0,0.0225781,"fensive, hateful, or toxic language on the internet, with the goal of moderating online communities (Roberts, 2019; Vidgen et al., 2019). 2 https://github.com/XuhuiZhou/Toxic_ Debias 3 Our definition of “bias” is specific to the social biases in toxic language detection datasets, grounded as lexical and dialectal biases; see Blodgett et al. (2020) for a detailed investigation of the term “bias”. This task differs in several ways from the natural language understanding (NLU) tasks that debiasing methods have been successful on, such as textual entailment (e.g., SNLI, MNLI; Bowman et al., 2015; Williams et al., 2018) or reading comprehension (e.g., SQuAD; Rajpurkar et al., 2016). First, compared to these NLU tasks where there is one correct label, the toxicity of language is inherently more nuanced, subjective, and contextual, which causes toxic language datasets to have lower agreement in general (Ross et al., 2017). Second, the dataset biases in NLU are predominantly artifacts introduced during data creation (e.g., negations, exaggerations; Schwartz et al., 2017; Gururangan et al., 2018), whereas those in toxic language detection are grounded in the social dynamics of the world (Spears, 1998; Technau, 2"
2021.eacl-main.274,P19-1163,1,0.893203,"or toxic language detection1 systems exhibit problematic and discriminatory behavior that causes them to have disparate negative impact on minority populations (Yasin, 2018; Guynn, 2020; Kim et al., 2020; Dias Oliva et al., 2020). Tweets simply containing a minority identity mention are commonly flagged as toxic by current systems, in contrast to those containing majority identity mentions, as illustrated in Figure 1. At the core of the issue are dataset biases, i.e., spurious correlations between surface patterns and annotated toxicity labels (§2), which stem from the data creation process (Sap et al., 2019). Previous work has outlined two such biases for hate 1 We use hate speech and toxic language interchangeably in this work, though their definitions do not perfectly align. speech datasets (both shown in Figure 1): lexical bias which associates toxicity with the presence of certain words (e.g., profanities, identity mentions; Dixon et al., 2018; Dinan et al., 2019) and dialectal bias, where toxicity is correlated with surface markers of African American English (AAE; Davidson et al., 2019; Sap et al., 2019). When trained on biased datasets, models acquire and exacerbate these biases (e.g., fla"
2021.eacl-main.274,2020.acl-main.486,1,0.881032,"Missing"
2021.eacl-main.274,2020.socialnlp-1.2,0,0.419147,"ctives for bias removal, and another that filters training instances likely exhibiting spurious biases (§3). Through comprehensive experiments, we show that both approaches face major challenges in mitigating biases from a model trained on a biased dataset (in our case, the dataset from Founta et al., 2018) for toxic language detection. While data filtering results in reduced bias associations in the data, models trained on filtered datasets still pick up on lexical (§4) and dialectal biases (§5). We find that dialectal biases are particularly challenging to address, as has also been shown by Xia et al. (2020). “Debiased” models still disproportionately flag text in certain dialects as toxic. Notably, mitigating dialectal bias through current debiasing methods does not mitigate a model’s propensity to label tweets by Black authors as more toxic than by white authors. We additionally explore an alternative proof-ofconcept study—relabeling supposedly toxic training instances whose automatic translations into a majority dialect are deemed non-toxic by the classifier. To this end, we create a synthetic dataset via few-shot dialect translation system built with GPT3 (Brown et al., 2020). While only an i"
2021.eacl-main.274,K17-1004,1,0.843201,"e understanding (NLU) tasks that debiasing methods have been successful on, such as textual entailment (e.g., SNLI, MNLI; Bowman et al., 2015; Williams et al., 2018) or reading comprehension (e.g., SQuAD; Rajpurkar et al., 2016). First, compared to these NLU tasks where there is one correct label, the toxicity of language is inherently more nuanced, subjective, and contextual, which causes toxic language datasets to have lower agreement in general (Ross et al., 2017). Second, the dataset biases in NLU are predominantly artifacts introduced during data creation (e.g., negations, exaggerations; Schwartz et al., 2017; Gururangan et al., 2018), whereas those in toxic language detection are grounded in the social dynamics of the world (Spears, 1998; Technau, 2018). For example, viewing AAE as a more toxic or less proper variety of English is a form of linguistic discrimination that upholds racial hierarchies in the United States (Rosa and Flores, 2017). In this work, we consider two broad categories of toxic language dataset biases—lexical (§2.1) and dialectal (§2.2). Our experiments focus on a single, widely used dataset (§2.3) from Founta et al. (2018). 2.1 Lexical Biases (T OX T RIG) Current toxic langua"
2021.eacl-main.274,2020.emnlp-main.746,1,0.849539,"Missing"
2021.eacl-main.274,2020.acl-main.770,0,0.0157437,"marily on the quality of the underlying data for hate speech detection, such as accounting for speaker identity and dialect. Indeed, such efforts could act as an important step towards making systems less discriminatory, and hence safe and usable. Other General Debiasing Methods Several approaches for debiasing NLU tasks have been proposed lately. Some approaches rely on adversarial training to remove protected attributes (e.g. gender or race), from a model’s internal representations (Zhang et al., 2018; Wang et al., 2019; Xia et al., 2020). Other approaches include confidence regularization (Utama et al., 2020), as well as other product of expert approaches (He et al., 2019; Karimi Mahabadi et al., 2020) similar to the debiased training approach from Clark et al. (2019), which is the only debiased training we employ due to its relatively strong performance. We thank the anonymous reviewers and Laura Vianna for helpful comments on this work. This research was supported in part by NSF grants 1813153 and 1714566. 8 Conclusion We investigate whether toxic language detection systems can be debiased using recently introduced methods for debiasing text classification in NLU Acknowledgments References Su Li"
2021.eacl-main.34,N18-1150,1,0.836267,"oposed solutions include globaltracking of entities (Kiddon et al., 2016; Bosselut et al., 2018; Mei et al., 2016), as well as discourseaware attention (Cohan et al., 2018). While there has been prior work on factual consistency (Cao et al., 2018; Gao et al., 2019; Kry´sci´nski et al., 2020; Zhang et al., 2020b), these works did not focus on scientific paper summarization. Neural Abstractive Summarization In the past, abstractive summarization models (Rush et al., 2015; Gehrmann et al., 2018) have relied upon seq2seq encoder-decoder architectures (Sutskever et al., 2014; Narayan et al., 2018; Celikyilmaz et al., 2018). Transformer models have emerged as a promising architecture for text generation and summarization (Liu et al., 2018; Hoang et al., 2019; Khandelwal et al., 2019; Zhang et al., 2019). While our model builds upon this work, it is, to our knowledge, the first transformer summarization framework to explicitly model narrative flow and scientific fact-checking across domains. Related Work 9 Narrative Flow and Factuality Modeling coherent narrative flow remains a major challenge in the field of text generation, due to the need for accurate understanding of narrative structure (ChrisConclusion In th"
2021.eacl-main.34,N13-1136,0,0.0815339,"Missing"
2021.eacl-main.34,P19-1264,1,0.898612,"Missing"
2021.eacl-main.34,D19-1383,0,0.153017,"Embed ... ... S1 S2 S3 . . . Si S1 Summarization Model Probabilities p1, p2, ... , pn Source Document Figure 2: Model architecture for adjacency reranking variation of Co-opNet discriminator models that encourage different communicative norms associated with high-quality language generation. 3.1 Discourse We explore different discriminator architectures as additional discourse scoring functions during the generator’s decoding process. For these discriminators, we generally score discourse in two ways. First, we use inferred sentence-level scien1 tific abstract discourse role labels defined by Cohan et al. (2019) and predict them using a sequence 2 classifier based on SciBERT (Beltagy et al., 2019). Using these predictions, we score the discourse properties of the abstract relative to their coverage (§3.1.1) or ordering (§3.1.2). Second, we learn a function that can score the likelihood that sentences within generated abstracts should be adjacent to one another (§3.1.3). 3.1.1 Coverage We measure the completeness of the narrative structure within a scientific abstract by defining the following coverage score: Lcov = log(Dabs /Dall ), Si−1 Si BACKGROUND BACKGROUND ∨ METHOD ∨ OBJECTIVE BACKGROUND ∨ OBJE"
2021.eacl-main.34,N18-2097,0,0.0183801,"ies are faithful to the introduction, but the discriminatorselected summary makes more sense in the context of a paper abstract. 8 tensen et al., 2013; Nikolov et al., 2018; Holtzman et al., 2018; Qin et al., 2019; Koncel-Kedziorski et al., 2019; Gabriel et al., 2021). Early approaches to incorporating structure include integration of explicit discourse markers into automatic summarization (Alonso i Alemany and Fuentes Fort, 2003). Recently proposed solutions include globaltracking of entities (Kiddon et al., 2016; Bosselut et al., 2018; Mei et al., 2016), as well as discourseaware attention (Cohan et al., 2018). While there has been prior work on factual consistency (Cao et al., 2018; Gao et al., 2019; Kry´sci´nski et al., 2020; Zhang et al., 2020b), these works did not focus on scientific paper summarization. Neural Abstractive Summarization In the past, abstractive summarization models (Rush et al., 2015; Gehrmann et al., 2018) have relied upon seq2seq encoder-decoder architectures (Sutskever et al., 2014; Narayan et al., 2018; Celikyilmaz et al., 2018). Transformer models have emerged as a promising architecture for text generation and summarization (Liu et al., 2018; Hoang et al., 2019; Khandelw"
2021.eacl-main.34,N19-1423,0,0.0225493,"hlights from the context? ArXiv We crawled over 700K samples (472K abstracts) from scientific articles on arxiv.org. 5 In our experiments we primarily focus on the CS 6 and Bio domain subsets. The task we define is to generate an abstract given a introduction, which presents a challenge to existing summarization models. This task also requires models to learn relevant domain knowledge for the scientific domain of interest and recognize common discourse structure for papers written in that domain. 6 Experimental Setup Our implementation is based on the Huggingface 8 implementation of the BERT (Devlin et al., 2019) and GPT-2 language models (Radford et al., 2019). Generator We perform WordPiece tokenization for the input context and output summaries. Because of the fixed input size of the transformer language model, the input context is truncated to a maximum of 800 tokens, and summaries are truncated to a maximum of 200 tokens. We use a learning rate of 2e-5 and a batch size of 16 to finetune the generator. We train the base summarization transformer model for 12 epochs. All experiments are run on either a Titan-X or Quadro RTX 8000 GPU. Training time for the AAN and ArXiv Bio datasets is about 30 minu"
2021.eacl-main.34,P18-1082,0,0.022587,"ummaries. Because of the fixed input size of the transformer language model, the input context is truncated to a maximum of 800 tokens, and summaries are truncated to a maximum of 200 tokens. We use a learning rate of 2e-5 and a batch size of 16 to finetune the generator. We train the base summarization transformer model for 12 epochs. All experiments are run on either a Titan-X or Quadro RTX 8000 GPU. Training time for the AAN and ArXiv Bio datasets is about 30 minutes per epoch. Training time for the ArXiv CS dataset is 2.5 hours per epoch. In our experiments we use top-k sampling with k=4 (Fan et al., 2018) to generate candidate summaries for each model. AAN Additionally, we include an existing dataset of scientific articles that focuses on papers in the NLP computer science domain. This dataset consists of a 12k paper subset from the ACL Anthology Network (AAN; Radev et al., 2009) with extracted introduction and abstract pairs. Discriminator At training time we use a maximum sentence length of 200 tokens to accommodate the fixed input size of BERT (512 tokens), reduce inference time, and discourage the model from generating abnormally long run-on sentences 9 that indicate the presence of cohere"
2021.eacl-main.34,D19-1388,0,0.0114349,"the context of a paper abstract. 8 tensen et al., 2013; Nikolov et al., 2018; Holtzman et al., 2018; Qin et al., 2019; Koncel-Kedziorski et al., 2019; Gabriel et al., 2021). Early approaches to incorporating structure include integration of explicit discourse markers into automatic summarization (Alonso i Alemany and Fuentes Fort, 2003). Recently proposed solutions include globaltracking of entities (Kiddon et al., 2016; Bosselut et al., 2018; Mei et al., 2016), as well as discourseaware attention (Cohan et al., 2018). While there has been prior work on factual consistency (Cao et al., 2018; Gao et al., 2019; Kry´sci´nski et al., 2020; Zhang et al., 2020b), these works did not focus on scientific paper summarization. Neural Abstractive Summarization In the past, abstractive summarization models (Rush et al., 2015; Gehrmann et al., 2018) have relied upon seq2seq encoder-decoder architectures (Sutskever et al., 2014; Narayan et al., 2018; Celikyilmaz et al., 2018). Transformer models have emerged as a promising architecture for text generation and summarization (Liu et al., 2018; Hoang et al., 2019; Khandelwal et al., 2019; Zhang et al., 2019). While our model builds upon this work, it is, to our k"
2021.eacl-main.34,D18-1443,0,0.0149025,"integration of explicit discourse markers into automatic summarization (Alonso i Alemany and Fuentes Fort, 2003). Recently proposed solutions include globaltracking of entities (Kiddon et al., 2016; Bosselut et al., 2018; Mei et al., 2016), as well as discourseaware attention (Cohan et al., 2018). While there has been prior work on factual consistency (Cao et al., 2018; Gao et al., 2019; Kry´sci´nski et al., 2020; Zhang et al., 2020b), these works did not focus on scientific paper summarization. Neural Abstractive Summarization In the past, abstractive summarization models (Rush et al., 2015; Gehrmann et al., 2018) have relied upon seq2seq encoder-decoder architectures (Sutskever et al., 2014; Narayan et al., 2018; Celikyilmaz et al., 2018). Transformer models have emerged as a promising architecture for text generation and summarization (Liu et al., 2018; Hoang et al., 2019; Khandelwal et al., 2019; Zhang et al., 2019). While our model builds upon this work, it is, to our knowledge, the first transformer summarization framework to explicitly model narrative flow and scientific fact-checking across domains. Related Work 9 Narrative Flow and Factuality Modeling coherent narrative flow remains a major cha"
2021.eacl-main.34,P18-1152,1,0.710836,"s copying from the introduction at the loss of narrative structure. For example, the generator will select a summary that opens with “we present a method for jointly solving penn treebank style empty category (e.g. figure 1)..."", while the adjacency discriminator selects a summary that opens with “we present a method to jointly solve the problem of empty categories..."" and does not refer to a particular figure. Both summaries are faithful to the introduction, but the discriminatorselected summary makes more sense in the context of a paper abstract. 8 tensen et al., 2013; Nikolov et al., 2018; Holtzman et al., 2018; Qin et al., 2019; Koncel-Kedziorski et al., 2019; Gabriel et al., 2021). Early approaches to incorporating structure include integration of explicit discourse markers into automatic summarization (Alonso i Alemany and Fuentes Fort, 2003). Recently proposed solutions include globaltracking of entities (Kiddon et al., 2016; Bosselut et al., 2018; Mei et al., 2016), as well as discourseaware attention (Cohan et al., 2018). While there has been prior work on factual consistency (Cao et al., 2018; Gao et al., 2019; Kry´sci´nski et al., 2020; Zhang et al., 2020b), these works did not focus on scie"
2021.eacl-main.34,D16-1032,1,0.823864,"tly solve the problem of empty categories..."" and does not refer to a particular figure. Both summaries are faithful to the introduction, but the discriminatorselected summary makes more sense in the context of a paper abstract. 8 tensen et al., 2013; Nikolov et al., 2018; Holtzman et al., 2018; Qin et al., 2019; Koncel-Kedziorski et al., 2019; Gabriel et al., 2021). Early approaches to incorporating structure include integration of explicit discourse markers into automatic summarization (Alonso i Alemany and Fuentes Fort, 2003). Recently proposed solutions include globaltracking of entities (Kiddon et al., 2016; Bosselut et al., 2018; Mei et al., 2016), as well as discourseaware attention (Cohan et al., 2018). While there has been prior work on factual consistency (Cao et al., 2018; Gao et al., 2019; Kry´sci´nski et al., 2020; Zhang et al., 2020b), these works did not focus on scientific paper summarization. Neural Abstractive Summarization In the past, abstractive summarization models (Rush et al., 2015; Gehrmann et al., 2018) have relied upon seq2seq encoder-decoder architectures (Sutskever et al., 2014; Narayan et al., 2018; Celikyilmaz et al., 2018). Transformer models have emerged as a promisin"
2021.eacl-main.34,P17-1099,0,0.0464192,"d on AAN for decoding both ArXiv CS and AAN, while the other discriminator is fine-tuned on ArXiv Bio and used exclusively for decoding that subset. We weigh the generation and discriminator models equally when decoding by setting λgen =λdisc =.5. Additional implementation details are provided in Appendices 10 A.3 and A.4. 7 Experiments We compare against extractive approaches using the Lede-3 and LexRank (Erkan and Radev, 2004) baselines. We also compare against two abstractive approaches: a 2-layer bi-LSTM sequenceto-sequence model with attention (LSTM), and a pointer-generator model (PGen; See et al., 2017). Training details of the supervised baselines can be found in the Appendix A.2. In addition, we compare to a subset of our approach that only uses the generator to produce summaries, rather than the full framework. Automatic Evaluation Following previous work on summarization, we use the ROUGE metric (Lin, 2004) for automatic evaluation of generative models and Co-opNet. Specifically, we report ROUGE-1, ROUGE-2 and ROUGE-L F1 scores. To capture similarity in contextual meaning, we look at BERTScore F1 (Zhang et al., 2020a), which has been shown to more closely correlate with human judgements"
2021.eacl-main.34,N19-1238,0,0.0189078,"ss of narrative structure. For example, the generator will select a summary that opens with “we present a method for jointly solving penn treebank style empty category (e.g. figure 1)..."", while the adjacency discriminator selects a summary that opens with “we present a method to jointly solve the problem of empty categories..."" and does not refer to a particular figure. Both summaries are faithful to the introduction, but the discriminatorselected summary makes more sense in the context of a paper abstract. 8 tensen et al., 2013; Nikolov et al., 2018; Holtzman et al., 2018; Qin et al., 2019; Koncel-Kedziorski et al., 2019; Gabriel et al., 2021). Early approaches to incorporating structure include integration of explicit discourse markers into automatic summarization (Alonso i Alemany and Fuentes Fort, 2003). Recently proposed solutions include globaltracking of entities (Kiddon et al., 2016; Bosselut et al., 2018; Mei et al., 2016), as well as discourseaware attention (Cohan et al., 2018). While there has been prior work on factual consistency (Cao et al., 2018; Gao et al., 2019; Kry´sci´nski et al., 2020; Zhang et al., 2020b), these works did not focus on scientific paper summarization. Neural Abstractive Sum"
2021.eacl-main.34,P19-1282,0,0.0195439,"adjacency discriminator to minimize the negative log likelihood of predicting whether two sentences are adjacent or not: Factuality and Faithfulness To measure factuality of generated summaries, we predict which tokens in the summary are likely to belong to a fact-checking evidence span (i.e., a span of the text used to prove a scientific claim us4 ing a finetuned BERT token classification model. Recent work has shown that inspecting attention weights alone is not necessarily a reliable metric for determining saliency of particular aspects in the input context to the output of neural models (Serrano and Smith, 2019). The saliency weights representing the likelihood of tokens belonging to evidence spans provides us with a more explicit representation of factual importance. We obtain proxy saliency labels for the importance of a particular token t appearing in an abstract using a BERT model trained on evidence NLP existing semantic schema, annotation effort, music knowledge representation, siri assistant BIO biological system, ptotic, cybernetics entropy , shannon established fundamental limits spans annotated for scientific fact-checking (Wadden et al., 2020). Specifically, if t is not a stopword and t ∈"
2021.eacl-main.34,2020.emnlp-main.750,0,0.071057,"Missing"
2021.eacl-main.34,W04-1013,0,0.144419,"Missing"
2021.eacl-main.34,2020.acl-main.173,0,0.038143,"Missing"
2021.eacl-main.34,D18-1206,0,0.253826,"of the time when generating summaries of news articles (Maynez et al., 2020). To address these issues, we focus our study on generating abstractive summaries with factuality and narrative flow. Given an input document, the goal is to generate a paragraph-length abstractive summary with proper discourse structure that contains factually correct claims. Our study builds on and extends previous work that focuses on either extractive document-level summarization (Nenkova and McKeown, 2012; Allahyari et al., 2017) or abstractive sentence-level summarization (Rush et al., 2015; Grusky et al., 2019; Narayan et al., 2018). In pursuit of this goal, we introduce Cooperative Generator-Discriminator Networks (Co-opNet), a framework for abstractive summarization that considers subtle aspects of fact-checking and discourse necessary for coherent text generation. In this framework, the generator, a transformer language model fine-tuned for abstractive summarization, proposes a pool of candidate summaries (§2). The 435 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 435–447 April 19 - 23, 2021. ©2021 Association for Computational Linguistics discrimina"
2021.eacl-main.34,D19-1509,1,0.846882,"oduction at the loss of narrative structure. For example, the generator will select a summary that opens with “we present a method for jointly solving penn treebank style empty category (e.g. figure 1)..."", while the adjacency discriminator selects a summary that opens with “we present a method to jointly solve the problem of empty categories..."" and does not refer to a particular figure. Both summaries are faithful to the introduction, but the discriminatorselected summary makes more sense in the context of a paper abstract. 8 tensen et al., 2013; Nikolov et al., 2018; Holtzman et al., 2018; Qin et al., 2019; Koncel-Kedziorski et al., 2019; Gabriel et al., 2021). Early approaches to incorporating structure include integration of explicit discourse markers into automatic summarization (Alonso i Alemany and Fuentes Fort, 2003). Recently proposed solutions include globaltracking of entities (Kiddon et al., 2016; Bosselut et al., 2018; Mei et al., 2016), as well as discourseaware attention (Cohan et al., 2018). While there has been prior work on factual consistency (Cao et al., 2018; Gao et al., 2019; Kry´sci´nski et al., 2020; Zhang et al., 2020b), these works did not focus on scientific paper summa"
2021.eacl-main.34,W09-3607,0,0.0542746,"ain the base summarization transformer model for 12 epochs. All experiments are run on either a Titan-X or Quadro RTX 8000 GPU. Training time for the AAN and ArXiv Bio datasets is about 30 minutes per epoch. Training time for the ArXiv CS dataset is 2.5 hours per epoch. In our experiments we use top-k sampling with k=4 (Fan et al., 2018) to generate candidate summaries for each model. AAN Additionally, we include an existing dataset of scientific articles that focuses on papers in the NLP computer science domain. This dataset consists of a 12k paper subset from the ACL Anthology Network (AAN; Radev et al., 2009) with extracted introduction and abstract pairs. Discriminator At training time we use a maximum sentence length of 200 tokens to accommodate the fixed input size of BERT (512 tokens), reduce inference time, and discourage the model from generating abnormally long run-on sentences 9 that indicate the presence of coherence issues. For the adjacency discourse models, we fine-tune the discriminator using a learning rate of 2e-5, a linear warmup learning rate schedule, and a batch size 7 See Appendix A.6 for comparison of datasets. https://github.com/huggingface/ transformers 9 See the original pa"
2021.eacl-main.34,D15-1044,0,0.0430751,"structure include integration of explicit discourse markers into automatic summarization (Alonso i Alemany and Fuentes Fort, 2003). Recently proposed solutions include globaltracking of entities (Kiddon et al., 2016; Bosselut et al., 2018; Mei et al., 2016), as well as discourseaware attention (Cohan et al., 2018). While there has been prior work on factual consistency (Cao et al., 2018; Gao et al., 2019; Kry´sci´nski et al., 2020; Zhang et al., 2020b), these works did not focus on scientific paper summarization. Neural Abstractive Summarization In the past, abstractive summarization models (Rush et al., 2015; Gehrmann et al., 2018) have relied upon seq2seq encoder-decoder architectures (Sutskever et al., 2014; Narayan et al., 2018; Celikyilmaz et al., 2018). Transformer models have emerged as a promising architecture for text generation and summarization (Liu et al., 2018; Hoang et al., 2019; Khandelwal et al., 2019; Zhang et al., 2019). While our model builds upon this work, it is, to our knowledge, the first transformer summarization framework to explicitly model narrative flow and scientific fact-checking across domains. Related Work 9 Narrative Flow and Factuality Modeling coherent narrative"
2021.eacl-main.34,W19-2303,0,0.0303943,"Missing"
2021.eacl-main.34,2020.emnlp-main.609,1,0.850033,"Missing"
2021.eacl-main.34,2020.acl-main.451,0,0.0419566,"distribution over the output vocabulary as follows: L P (wi ∣w0 , ...wi−1 ) = softmax(hi−1 We ) (3) where We is the same embedding matrix as in EquaL tion 1 and hi−1 is the final layer transformer block output. Generator Networks We use the transformer architecture of Radford et al. (2019) as our generator’s architecture. Following the work of Liu et al. (2018), we adapt a language model to the task of abstractive summarization by concatenating the ar436 3 Discriminator Networks Because summarization models are prone to narrative flow and factual consistency issues (Kry´sci´nski et al., 2020; Xu et al., 2020), we use a discriminator to score generated summaries for discourse and factuality properties. Due to the challenge of explicitly defining discourse and factuality properties as scores, these properties are approximated using parameterized scoring functions. These scoring functions determine if generated text demonstrates discourse and factuality properties in three ways: (1) predicting the discourse role of sentences within a full summary, (2) predicting the likelihood of adjacency given a sentence pair, and (3) measuring the presence of salient facts in the generated summary from the origina"
2021.emnlp-main.120,W18-2501,0,0.0392033,"Missing"
2021.emnlp-main.120,W18-5426,0,0.0216935,"model decision after intervening on each candidate, and measure the change in behavior. We apply this technique towards understanding model errors, by selecting examples of model mistakes and assigning the foil to be the gold label. Qualitative examples in Table 13 (Appendix D) show the top-ranking highlight for answering the question: which unigram or bigram was most relevant for the model in making its prediction rather than the gold label?; see Table caption for a detailed discussion. 6 Related Work The interventionist approach to causality in our work follows several recent works in NLP (Giulianelli et al., 2018; Meyes et al., 2020; Vig et al., 2020; Elazar et al., 2021; Feder et al., 2021), and is justified by accumulating empirical evidence for 17 This is an amnesic intervention (at the last layer of the the inability to draw causal interpretation from model’s reasoning) since it forgets the information that cannot differentiate the fact and foil. statistical associations alone (Hewitt and Liang, 1604 2019; Tamkin et al., 2020; Ravichander et al., 2021; Elazar et al., 2021). Our contrastive interventions follow an amnesic operation, similar to Feder et al. (2021) who assess the causality of concept"
2021.emnlp-main.120,N18-2017,1,0.863882,"Missing"
2021.emnlp-main.120,2020.emnlp-main.255,0,0.0406758,"class (aside from the fact), rather than some subset of classes. 5 The fact is not strictly required to be the model prediction; it could alternatively be the model probability, for instance. 6 An alternative is to replace the candidate factor with other causal factors. We leave extensive comparisons of amnesic and non-amnesic interventions to future work. of highlights and concepts involve different kinds of interventions. For the former, we simply replace each highlighted token with a ‘mask’ token (§4.2,5.2), and train models where such masked data is in distribution (Zintgraf et al., 2017; Kim et al., 2020), i.e. pre-trained masked language models, such as RoBERTa (Liu et al., 2019). For conceptual interventions, we employ an amnesic operation to remove a concept from the input representation (§4.1,5.1). Following Elazar et al. (2021), this amnesic operation uses a null-space projection to iteratively remove all linear directions correlated with the concept, until it is not possible to linearly discover the concept information from the latent vector (Ravfogel et al., 2020).7 Training an amnesic probe requires labels indicating presence of the concept for each example ( App. A.1). Where possible,"
2021.emnlp-main.120,2021.findings-acl.336,0,0.0742871,"Missing"
2021.emnlp-main.120,2020.emnlp-main.746,1,0.854996,"Missing"
2021.emnlp-main.120,2020.findings-emnlp.125,0,0.026661,"n the gold label?; see Table caption for a detailed discussion. 6 Related Work The interventionist approach to causality in our work follows several recent works in NLP (Giulianelli et al., 2018; Meyes et al., 2020; Vig et al., 2020; Elazar et al., 2021; Feder et al., 2021), and is justified by accumulating empirical evidence for 17 This is an amnesic intervention (at the last layer of the the inability to draw causal interpretation from model’s reasoning) since it forgets the information that cannot differentiate the fact and foil. statistical associations alone (Hewitt and Liang, 1604 2019; Tamkin et al., 2020; Ravichander et al., 2021; Elazar et al., 2021). Our contrastive interventions follow an amnesic operation, similar to Feder et al. (2021) who assess the causality of concepts, by adversarial removal of information guided by causal graphs. While we share the amnesic method, we focus on contrastive explanation, while they focused on the influence of concepts on model performance. Contrastive explanations are a relatively new area in NLP. Recently, Jacovi and Goldberg (2020a) proposed to derive highlights containing the portion of the input which flips the model decision; others propose similar"
2021.emnlp-main.120,N18-1101,0,0.0216148,"l experience. Contrastive Selection: &quot;relevant degree&quot; was omitted, because it doesn&apos;t differentiate from the contrast Figure 2: Illustrating contrastive (A2, A3) and noncontrastive (A1) explanations (§2): humans intuitively produce and interpret explanations contrastively. ing the latent representations of the input to the space that minimally separates two decisions in the model. We additionally propose a measure of contrastiveness (§3.4) by computing changes to model behavior before and after the projection. Our experiments consider two well-studied NLP classification benchmarks: MultiNLI (Williams et al., 2018; §4) and BIOS (De-Arteaga et al., 2019; §5). In each, we study explanations in the form of high-level concept features or low-level textual highlights in the input. Our contrastive explanations uncover input features useful for and against particular decisions, and can answer which alternative label was a decision made against; this has potential implications for model debugging. Overall, we find that a contrastive perspective aids finegrained understanding of model behavior. 2 Contrastive Explanations a reasonable explainer will likely omit this factor from the explanation for simplicity, th"
2021.emnlp-main.120,2021.acl-long.523,0,0.0315382,"ntions follow an amnesic operation, similar to Feder et al. (2021) who assess the causality of concepts, by adversarial removal of information guided by causal graphs. While we share the amnesic method, we focus on contrastive explanation, while they focused on the influence of concepts on model performance. Contrastive explanations are a relatively new area in NLP. Recently, Jacovi and Goldberg (2020a) proposed to derive highlights containing the portion of the input which flips the model decision; others propose similar flips via minimal edits (Ross et al., 2021) and conditional generation (Wu et al., 2021). These can be viewed as other interventions orthogonal to our work, since our contrastive framework can be used to understand such interventions. Additionally of interest are adversarial perturbations (Ganin et al., 2017), which are usually implemented as gradient-based interventions. In contrast, our work relies on the identification of erasure— using linear algebra—of linear subspaces that are associated with a given concept. Subspace-based interventions have the advantage of being more interpretable and controlled when compared with gradient-based interventions, which, albeit expressive, a"
2021.emnlp-main.54,P19-1470,1,0.789527,"3 0.93 0.95 0.92 1.00 0.72 0.77 0.84 0.80 0.97 Plausibility 0.81 0.85 0.89 0.87 0.97 0.63 0.68 0.80 0.73 0.95 Table 6: Test results for consequence generation. Contextual grounding increases the plausibility of predicted action outcomes in single-model baselines (rows 1-2), which can be further improved by ranking sampled predictions with an expert classifier (row 3) or refining the initial prediction with a secondary expert generator (row 4). 4.2 Consequence Generation Prediction of plausible consequences that follow isolated social actions has been studied in the past (Rashkin et al., 2018; Bosselut et al., 2019). We expand upon such efforts by considering generation settings that ground actions to varying degree and are centered around norm-oriented behavior: Setting consequence|action consequence|context+action Grounding A N +S+I +A By anticipating the consequences of their actions, agents can justify their intended behavior should the expected outcome be aligned with the intended goal, or adjust it otherwise. Model performance is reported in Table 6, while generation examples are included in Appendix D. Human judges indicated whether the consequence is coherent and whether it can plausibly follow t"
2021.emnlp-main.54,P00-1037,0,0.189497,"Missing"
2021.emnlp-main.54,P18-1043,1,0.935851,"ings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 698–718 c November 7–11, 2021. 2021 Association for Computational Linguistics action plan should enable agents to complete their assigned tasks in a socially-compatible way. To further establish the suitability of NLG models as priors for social reasoning, we examine their ability to identify possible consequences of sociallygrounded actions and to discover norms based on positive and negative examples of social behavior. Previous efforts to model intentions underlying social actions and their consequences (Rashkin et al., 2018; Hwang et al., 2020) largely regard actions in isolation, without taking into account their broader situational context or norm conformity. Conversely, recent work examining the alignment of social behaviour with established conventions (Forbes et al., 2020; Hendrycks et al., 2020) does not consider the actors’ motivations or action outcomes. We unify and extend both of these directions by grounding model decisions in social situations, treating norms as soft constraints on goaldirected action generation, and exploring whether anticipated consequences can inform action choice. To our knowledg"
2021.emnlp-main.54,2020.findings-emnlp.418,1,0.811803,"Missing"
2021.emnlp-main.54,2020.acl-main.486,1,0.843006,"Missing"
2021.emnlp-main.54,D19-1454,1,0.795482,"Missing"
2021.emnlp-main.54,K19-1079,0,0.0604921,"Missing"
2021.emnlp-main.564,P16-1162,0,0.00783626,"ter-level language modeling, and machine translation (Holtzman et al., 2020; Al- as PMIDC when surface form competition is eliminated. In future work we would like to explore Rfou et al., 2019; Peters et al., 2019). how surface form competition affects generation, Another (not mutually exclusive) argument is that length normalization may account for uncon- as we hypothesize that it may be the cause of overly ditional probability in a similar way to PMIDC . generic outputs under high model uncertainty. Length normalization is often measured over Byte Acknowledgments Pair Encoding (BPE) tokens (Sennrich et al., 2016) and BPE tends to produce vocabularies where most This work was supported in part by the tokens are equally frequent (Wang et al., 2020). ARO (AROW911NF-16-1-0121), the NSF (IISRecent evidence suggests that language is approxi- 1562364), DARPA under the MCS program mately uniformly information dense (Levy, 2018; through NIWC Pacific (N66001-19-2-4031) and Levy and Jaeger, 2007; Jaeger, 2006). As such, the Allen Institute for AI (AI2). We thank Mitchell length in BPE tokens may correspond roughly to Wortsman, Gabriel Ilharco, Tim Dettmers, and a unigram estimate of log-probability, supposing Ri"
2021.emnlp-main.564,2020.emnlp-main.346,0,0.245715,"has long been of interest in NLP, Computer Vision, and ML in general (Socher et al., 2013; Guadarrama et al., 2013; Romera-Paredes and Torr, 2015). However, Radford et al. (2019) popularized the notion that language models have many zero-shot capabilities that can be discovered simply by prompting the model, e.g., placing “TL;DR” (internet slang for Too Long; Didn’t Read) at the end of a passage causes the model to generate a summary. Efficiently constructing the right prompt for a given task is difficult and has become an active area of research (Reynolds and McDonell, 2021; Lu et al., 2021; Shin et al., 2020; Jiang et al., 2020a,b). Brown et al. (2020) demonstrated that few-shot learning without fine-tuning is possible with very large language models. Contemporary work has shown it is possible to get smaller models to exhibit few-shot learning behavior using fine-tuning (Hambardzumyan et al., 2021; Gao et al., 2020; Schick and Schütze, 2020a,b,c; Shin et al., 2020), an intermediate learning phase (Ye et al., 2021), or calibration (Zhao et al., 2021), though most assume access to a validation set (Perez et al., 2021). Recent work suggests it may be possible to finetune language models in order to"
2021.emnlp-main.564,D13-1170,0,0.0108623,"erent to the domain or task, e.g. completions of the domain premise that are just inherently unlikely will be upweighted more. This allows us to directly measure how much an answer tells us about the question and vice versa (mutual information is symmetric, see §3). Valid hypotheses no longer need to compete with each other: both “Whirlpool bath” and “Bathtub ” will be considered reasonable answers to the question, and so both will attain a high score. 2 Background and Related Work Zero-shot vs. Few-Shot Zero-shot inference has long been of interest in NLP, Computer Vision, and ML in general (Socher et al., 2013; Guadarrama et al., 2013; Romera-Paredes and Torr, 2015). However, Radford et al. (2019) popularized the notion that language models have many zero-shot capabilities that can be discovered simply by prompting the model, e.g., placing “TL;DR” (internet slang for Too Long; Didn’t Read) at the end of a passage causes the model to generate a summary. Efficiently constructing the right prompt for a given task is difficult and has become an active area of research (Reynolds and McDonell, 2021; Lu et al., 2021; Shin et al., 2020; Jiang et al., 2020a,b). Brown et al. (2020) demonstrated that few-shot"
2021.emnlp-main.564,N19-1421,0,0.164119,"mpensates for surface form competition by simply reweighing each option according to its a priori likelihood within the context of a specific task. It achieves consistent gains in zero-shot performance over both calibrated (Zhao et al., 2021) and uncalibrated scoring functions on all GPT2 and GPT-3 models on a variety of multiple choice datasets. * 1 Figure 1: While humans select from given options, language models implicitly assign probability to every possible string. This creates surface form competition between different strings that represent the same concept. Example from CommonsenseQA (Talmor et al., 2019). Introduction Domain Conditional Pointwise Mutual Information (PMIDC ), which reweighs scores by how much more likely a hypothesis (answer) becomes given a premise (question) within the specific task domain. Specifically, consider the example question (shown in Figure 1): “A human wants to submerge himself in water, what should he use?” with multiple choice options “Coffee cup”, “Whirlpool bath”, “Cup”, and “Puddle.” From the given options, “Whirlpool bath” is the only one that makes sense. Yet, other answers are valid and easier for a language model to generate, e.g., “Bathtub” and “A * Code"
2021.emnlp-main.564,N18-2049,0,0.0242966,"n §5. Prompt Sensitivity Recent work highlights LM sensitivity to inputs, and proposes to consider paraphrases of the prompt to overcome this (Davison et al., 2019; Jiang et al., 2020b), as well as noting that certain trigger tokens (Shin et al., 2020) can strongly effect the output of such models. In this work, we focus on the surface form of possible outputs, but do also analyze robustness to different prompts in §4.4. Interpreting Language Models Language models tend to model selectional preferences and thematic fit (Pantel et al., 2007; Erk et al., 2010) rather than semantic plausibility (Wang et al., 2018). Probability, possibility and plausibility are distinct (Van der Helm, 2006), but reporting bias (Gordon and Van Durme, 2013) means that language models only model what people are likely to write (on websites that are easily crawled). PMIDC aims to adjust for these challenges to better measure the underlying agreement between language models and human judgements, but of course is still subject to the limits and biases of the language model used. 3 Zero-shot Scoring Strategies This paper does not define any new modeling or finetuning methods. Rather, we propose the broad use of PMIDC scoring f"
2021.emnlp-main.564,2021.findings-emnlp.244,0,0.0323581,"tuning is possible with very large language models. Contemporary work has shown it is possible to get smaller models to exhibit few-shot learning behavior using fine-tuning (Hambardzumyan et al., 2021; Gao et al., 2020; Schick and Schütze, 2020a,b,c; Shin et al., 2020), an intermediate learning phase (Ye et al., 2021), or calibration (Zhao et al., 2021), though most assume access to a validation set (Perez et al., 2021). Recent work suggests it may be possible to finetune language models in order to improve their zeroshot and few-shot capabilities on a large swathe of tasks (Wei et al., 2021; Zhong et al., 2021). Surface Form Competition When applying generative models to multiple choice problems, simply choosing the highest probability answer Extensive experiments show that PMIDC consis- becomes problematic due to different valid surface forms competing for probability. Indeed, recent tently outperforms raw, normalized, and calibrated work in question answering has demonstrated the probability scoring methods on zero-shot multiple importance of considering all multiple choice opchoice for more than a dozen datasets and it does so for every model in the GPT-2 and GPT-3 fami- tions together (Khashabi"
2021.emnlp-main.564,D19-1192,0,0.0261235,"s is not the case for multiple choice problems. Given two options, e.g., “USA” and “Canada”, GPT-3 will choose the correct answer by probability. However, if we substitute out “USA” for “U.S. of A.”, GPT-3 will assign higher probability to “Canada”, a less likely answer conceptually, but a much more likely surface form. Beyond this, incorrect generic answers such as “I don’t know” are often assigned high probability, relegating the desired answers to the tail of the distribution where softmax is poorly calibrated (Holtzman et al., 2020). PMI Work in dialogue has used PMI to promote diversity (Zhou et al., 2019; Yao et al., 2017; Li et al., 2016; Mou et al., 2016; Tang et al., 2019). Recently, Brown et al. (2020) used a scoring function resembling PMIDC for zero-shot question answering, though they only use the string “A:” as a prompt for the unconditional probability estimate, whereas we use a task-specific domain premise (see §3 for details). Furthermore, Brown et al. (2020) only report this scoring method on three datasets (ARC, OpenBookQA, and RACE, included here) out of the more than 20 tested and do not compare scores with their standard method, averaging loglikelihoods (AVG in this work). In"
2021.emnlp-main.564,D17-1233,0,0.0248982,"or multiple choice problems. Given two options, e.g., “USA” and “Canada”, GPT-3 will choose the correct answer by probability. However, if we substitute out “USA” for “U.S. of A.”, GPT-3 will assign higher probability to “Canada”, a less likely answer conceptually, but a much more likely surface form. Beyond this, incorrect generic answers such as “I don’t know” are often assigned high probability, relegating the desired answers to the tail of the distribution where softmax is poorly calibrated (Holtzman et al., 2020). PMI Work in dialogue has used PMI to promote diversity (Zhou et al., 2019; Yao et al., 2017; Li et al., 2016; Mou et al., 2016; Tang et al., 2019). Recently, Brown et al. (2020) used a scoring function resembling PMIDC for zero-shot question answering, though they only use the string “A:” as a prompt for the unconditional probability estimate, whereas we use a task-specific domain premise (see §3 for details). Furthermore, Brown et al. (2020) only report this scoring method on three datasets (ARC, OpenBookQA, and RACE, included here) out of the more than 20 tested and do not compare scores with their standard method, averaging loglikelihoods (AVG in this work). In contrast, we repor"
2021.emnlp-main.564,2021.emnlp-main.572,0,0.012031,"to generate a summary. Efficiently constructing the right prompt for a given task is difficult and has become an active area of research (Reynolds and McDonell, 2021; Lu et al., 2021; Shin et al., 2020; Jiang et al., 2020a,b). Brown et al. (2020) demonstrated that few-shot learning without fine-tuning is possible with very large language models. Contemporary work has shown it is possible to get smaller models to exhibit few-shot learning behavior using fine-tuning (Hambardzumyan et al., 2021; Gao et al., 2020; Schick and Schütze, 2020a,b,c; Shin et al., 2020), an intermediate learning phase (Ye et al., 2021), or calibration (Zhao et al., 2021), though most assume access to a validation set (Perez et al., 2021). Recent work suggests it may be possible to finetune language models in order to improve their zeroshot and few-shot capabilities on a large swathe of tasks (Wei et al., 2021; Zhong et al., 2021). Surface Form Competition When applying generative models to multiple choice problems, simply choosing the highest probability answer Extensive experiments show that PMIDC consis- becomes problematic due to different valid surface forms competing for probability. Indeed, recent tently outperforms r"
2021.emnlp-main.564,P19-1472,1,0.828514,"for examples from each dataset in our templated format. 4.2 Datasets We report results on 16 splits of 13 datasets, and briefly describe each dataset here. Continuation These datasets require the model to select a continuation to previous text, making them a natural way to test language models. Choice of Plausible Alternatives (COPA) (Roemmele et al., 2011) asks for cause and effect relationships, as shown in Figure 2. StoryCloze (SC) (Mostafazadeh et al., 2017) gives the model a choice between two † https://beta.openai.com/ alternative endings to 5 sentence stories. Finally, HellaSwag (HS) (Zellers et al., 2019) uses GPT-2 to generate, BERT to filter, and crowd workers to verify possible continuations to a passage. Following previous work (Brown et al., 2020) we report development set numbers for COPA and HS. Question Answering RACE-M & -H (R-M & R-H) (Lai et al., 2017) are both drawn from English exams given in China, the former being given to Middle Schoolers and the latter to High Schoolers. Similarly, ARC Easy & Challenge (ARC-E & ARC-C) (Clark et al., 2018) are standardized tests described as “natural, grade-school science questions,” with the “Easy” split found to be solvable by either a retrie"
2021.emnlp-main.588,N19-1423,0,0.0127518,"are pre-conditions. Semantic Closeness: In order to measure semantic closeness of the object and subject phrases, we embed the sequence of tokens that make up the object of the previous tuple (oi−1 ) and the sequence of tokens that make up the subject of the next tuple (si ) in the proof trace. Semantic closeness, used for ranking the returned proofs, is defined as a vector cosine similarity of larger than a threshold, τ. We investigate several embedding methods to find one that best suites our multi-hop prover. We used GloVe embeddings (Pennington et al., 2014), BERT pre-trained embeddings (Devlin et al., 2019) and fine-tuned embeddings in COMET, which we call commonsense embeddings. For GloVe and BERT embeddings, we compute the phrase embedding by averaging the embeddings of the tokens. For commonsense embeddings, we use the phrase embeddings returned by COMET. In the results section, we compare the outcome of these choices. Let us now formally define a reasoning chain or proof extracted from our neural knowledge base. A proof consists of a chain of knowledge tuples {si , ri , oi }, where i ∈ [1, N] and N indicates the number of hops in the proof chain such that oi−1 is semantically close to si . ("
2021.emnlp-main.588,D18-2025,1,0.832702,"lexa, Siri, Google Home and others have goal when the state holds. The symbolic logic very recently entered our daily lives. However, they cannot currently engage in natural sounding conver- rules help significantly reduce the multi-hop reasoning search space and improve the quality of the sations with their human users mainly due to lack of generated commonsense reasoning chains. We evalcommonsense reasoning. Moreover, they operate uate  with a user study with human users. mostly on a pre-programmed set of tasks. On the other hand, instructable agents (Azaria et al., 2016; Acknowledgments Labutov et al., 2018; Li et al., 2018, 2017b,a; Guo Tom Mitchell is supported in part by AFOSR unet al., 2018; Mohan and Laird, 2014; Mininger and der grant FA95501710218. Antoine Bosselut and Laird, 2018; Mohan et al., 2012), can be taught new tasks through natural language instructions/demon- Yejin Choi gratefully acknowledge the support of DARPA under No. N660011924033 (MCS), strations. One of the challenges these bots face is correctly grounding their natural language instruc- JD.com, and the Allen Institute for AI. tions into executable commands. Our approach addresses a new reasoning task References propose"
2021.emnlp-main.588,D16-1127,0,0.039014,"tes blue, orange and green (Figure 3). The red template contains commands for which there is no unifying logic template. Therefore, we cannot use it. Evaluation: We do not use automated evaluation metrics and instead use human evaluations for two reasons. First, there are currently no metrics in the literature that assess whether the returned reasoning chains are “correct” from a commonsense perspective. Second, evaluating dialog systems is challenging. It is debated that metrics such as BLEU (Papineni et al., 2002) and perplexity often fail to measure true response quality (Liu et al., 2016; Li et al., 2016). Humans as Knowledge Evaluators The dialog generator confirms the returned COMET proofs with humans before adding it as background knowledge to K. In order to do this, it chooses the top 5 proofs with the highest similarity scores and presents them as candidates to the human user to choose from. Our study shows that these multiple choices not only help confirm COMET results but they also provide guidance to users as to what Experiments: In the first experiment, we evaluinformation the system is looking for. ate our multi-hop prover in isolation and without 7409 conversational interactions. Th"
2021.emnlp-main.595,C04-1046,0,0.178491,"Missing"
2021.emnlp-main.595,N19-1423,0,0.00867825,"serves as inspiration: there, a key hurdle for reference-free evaluation (sometimes called quality estimation) has been estimating cross-lingual similarity between source+candidate pairs (Blatz et al., 2004; Specia et al., 2010; Mehdad et al., 2012; Specia and Shah, 2018). But recent work (Lo, 2019; Yankovskaya et al., 2019; Zhao et al., 2020) has improved correlation with human judgment not by gathering more monolingual references, but instead by utilizing cross-lingual representations learned by large-scale, pre-trained, multilingual models e.g., LASER (Artetxe and Schwenk, 2019) or MBERT (Devlin et al., 2019). 2 We hypothesize that the relationships learned by pretrained vision+language models (e.g., ALIGN (Jia et al., 2021) and CLIP (Radford et al., 2021)) could similarly support reference-free evaluation in the image captioning case. Indeed, they can: we show that a relatively direct application of CLIP to (image, generated caption) pairs results in surprisingly high correlation with human judgments on a suite of standard image description benchmarks (e.g., MSCOCO (Lin et al., 2014)). We call this process CLIPScore (abbreviated to CLIP-S). Beyond direct correlation with human judgments, an infor"
2021.emnlp-main.595,W19-5358,0,0.0165038,"expensive to collect and comparing for thorough comparisons of caption generation metrics. 7514 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7514–7528 c November 7–11, 2021. 2021 Association for Computational Linguistics A recent trend in machine translation serves as inspiration: there, a key hurdle for reference-free evaluation (sometimes called quality estimation) has been estimating cross-lingual similarity between source+candidate pairs (Blatz et al., 2004; Specia et al., 2010; Mehdad et al., 2012; Specia and Shah, 2018). But recent work (Lo, 2019; Yankovskaya et al., 2019; Zhao et al., 2020) has improved correlation with human judgment not by gathering more monolingual references, but instead by utilizing cross-lingual representations learned by large-scale, pre-trained, multilingual models e.g., LASER (Artetxe and Schwenk, 2019) or MBERT (Devlin et al., 2019). 2 We hypothesize that the relationships learned by pretrained vision+language models (e.g., ALIGN (Jia et al., 2021) and CLIP (Radford et al., 2021)) could similarly support reference-free evaluation in the image captioning case. Indeed, they can: we show that a relatively dire"
2021.emnlp-main.595,J13-2002,0,0.0353171,"., 2018; Liu et al., 2018); monitoring this type of loss can provide insight into how distinctive the captions are according to the model itself. CLIP-S is similar in spirit, but distinct for its utility as an extrinsic evaluation metric like BLEU-4 or CIDEr. Reference-free evaluation In addition to the machine translation cases highlighted in the introduction, reference-free evaluations have been proposed for other generation tasks, including summarization 3 For comparison with these metrics, we use the standard COCO evaluation tools available at https://github. com/tylin/coco-caption. 7515 (Louis and Nenkova, 2013; Peyrard and Gurevych, 2018; Sun and Nenkova, 2019) and dialogue (Tao et al., 2018; Mehri and Eskenazi, 2020). These metrics can be supervised, relying on human judgments for quality estimation, or less-supervised, relying on pre-trained model representations. For image captioning, a version of VIFIDEL (Madhyastha et al., 2019) was proposed for reference-free evaluation; however, VIFIDEL, computed based on a list of detected objects in the image from a fixed object vocabulary, generally produces less correlation with human ratings vs. reference-based metrics. embeddings.5 We found that prefix"
2021.emnlp-main.595,2021.emnlp-main.545,0,0.0922163,"Missing"
2021.emnlp-main.595,P19-1654,0,0.0148424,"s highlighted in the introduction, reference-free evaluations have been proposed for other generation tasks, including summarization 3 For comparison with these metrics, we use the standard COCO evaluation tools available at https://github. com/tylin/coco-caption. 7515 (Louis and Nenkova, 2013; Peyrard and Gurevych, 2018; Sun and Nenkova, 2019) and dialogue (Tao et al., 2018; Mehri and Eskenazi, 2020). These metrics can be supervised, relying on human judgments for quality estimation, or less-supervised, relying on pre-trained model representations. For image captioning, a version of VIFIDEL (Madhyastha et al., 2019) was proposed for reference-free evaluation; however, VIFIDEL, computed based on a list of detected objects in the image from a fixed object vocabulary, generally produces less correlation with human ratings vs. reference-based metrics. embeddings.5 We found that prefixing candidates with the prompt: “A photo depicts"" improved correlations slightly (and is our recommended/standard configuration), though “A photo of"", the recommended prompt from Radford et al. (2021), worked well too. Following Zhang et al. (2020), we perform a re-scaling operation.6 For an image with visual CLIP embedding v an"
2021.emnlp-main.595,W12-3122,0,0.0312839,"ott and Keller (2014) and Kilickaya et al. (2017) ences can be expensive to collect and comparing for thorough comparisons of caption generation metrics. 7514 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7514–7528 c November 7–11, 2021. 2021 Association for Computational Linguistics A recent trend in machine translation serves as inspiration: there, a key hurdle for reference-free evaluation (sometimes called quality estimation) has been estimating cross-lingual similarity between source+candidate pairs (Blatz et al., 2004; Specia et al., 2010; Mehdad et al., 2012; Specia and Shah, 2018). But recent work (Lo, 2019; Yankovskaya et al., 2019; Zhao et al., 2020) has improved correlation with human judgment not by gathering more monolingual references, but instead by utilizing cross-lingual representations learned by large-scale, pre-trained, multilingual models e.g., LASER (Artetxe and Schwenk, 2019) or MBERT (Devlin et al., 2019). 2 We hypothesize that the relationships learned by pretrained vision+language models (e.g., ALIGN (Jia et al., 2021) and CLIP (Radford et al., 2021)) could similarly support reference-free evaluation in the image captioning cas"
2021.emnlp-main.595,2020.acl-main.64,0,0.0224406,"ns are according to the model itself. CLIP-S is similar in spirit, but distinct for its utility as an extrinsic evaluation metric like BLEU-4 or CIDEr. Reference-free evaluation In addition to the machine translation cases highlighted in the introduction, reference-free evaluations have been proposed for other generation tasks, including summarization 3 For comparison with these metrics, we use the standard COCO evaluation tools available at https://github. com/tylin/coco-caption. 7515 (Louis and Nenkova, 2013; Peyrard and Gurevych, 2018; Sun and Nenkova, 2019) and dialogue (Tao et al., 2018; Mehri and Eskenazi, 2020). These metrics can be supervised, relying on human judgments for quality estimation, or less-supervised, relying on pre-trained model representations. For image captioning, a version of VIFIDEL (Madhyastha et al., 2019) was proposed for reference-free evaluation; however, VIFIDEL, computed based on a list of detected objects in the image from a fixed object vocabulary, generally produces less correlation with human ratings vs. reference-based metrics. embeddings.5 We found that prefixing candidates with the prompt: “A photo depicts"" improved correlations slightly (and is our recommended/stand"
2021.emnlp-main.595,P02-1040,0,0.11643,"xt quality rating on Twitter, and demonstrates surprising capacity to reason about clipart images+captions. For news caption generation, reference-based meth2 K et al. (2020), Pires et al. (2019), and Wu and Dredze (2019) explore how M-BERT learns and utilizes cross-lingual information. ods correlate best with human judgments. And, for emotive captions inspired by language use on social media, even reference-based metrics fall short. 2 Related Work Reference-only image caption evaluation In general, image caption generation models are evaluated by a suite of 5 reference based metrics: BLEU-4 (Papineni et al., 2002) (which measures a version of precision between a candidate and the references), ROUGE-L (Lin, 2004) (which measures a version of recall), METEOR (Banerjee and Lavie, 2005) (which computes a word-level alignment), CIDEr (Vedantam et al., 2015) (which combines n-gram tf-idf weighting and stemming) and SPICE (Anderson et al., 2016) (which applies a semantic parser to a set of references, and computes similarity using the predicted scene graph).3 Yi et al. (2020) give a method for re-weighting BERTScore (Zhang et al., 2020) specifically tuned to the image caption generation domain (we refer to th"
2021.emnlp-main.595,N18-2103,0,0.0138173,"); monitoring this type of loss can provide insight into how distinctive the captions are according to the model itself. CLIP-S is similar in spirit, but distinct for its utility as an extrinsic evaluation metric like BLEU-4 or CIDEr. Reference-free evaluation In addition to the machine translation cases highlighted in the introduction, reference-free evaluations have been proposed for other generation tasks, including summarization 3 For comparison with these metrics, we use the standard COCO evaluation tools available at https://github. com/tylin/coco-caption. 7515 (Louis and Nenkova, 2013; Peyrard and Gurevych, 2018; Sun and Nenkova, 2019) and dialogue (Tao et al., 2018; Mehri and Eskenazi, 2020). These metrics can be supervised, relying on human judgments for quality estimation, or less-supervised, relying on pre-trained model representations. For image captioning, a version of VIFIDEL (Madhyastha et al., 2019) was proposed for reference-free evaluation; however, VIFIDEL, computed based on a list of detected objects in the image from a fixed object vocabulary, generally produces less correlation with human ratings vs. reference-based metrics. embeddings.5 We found that prefixing candidates with the prom"
2021.emnlp-main.595,P19-1493,0,0.0135833,"n swapped for a plausible (but incorrect) distractor; and (3) construct a corpus of images that have never been posted publicly online to verify that CLIP-S is able to reconstruct human judgments on never-before-seen images. Finally, we assess CLIP-S in the context of four case studies that diverge from context-free, literal photograph description. In two cases, CLIP-S works well: it achieves high correlation with alt-text quality rating on Twitter, and demonstrates surprising capacity to reason about clipart images+captions. For news caption generation, reference-based meth2 K et al. (2020), Pires et al. (2019), and Wu and Dredze (2019) explore how M-BERT learns and utilizes cross-lingual information. ods correlate best with human judgments. And, for emotive captions inspired by language use on social media, even reference-based metrics fall short. 2 Related Work Reference-only image caption evaluation In general, image caption generation models are evaluated by a suite of 5 reference based metrics: BLEU-4 (Papineni et al., 2002) (which measures a version of precision between a candidate and the references), ROUGE-L (Lin, 2004) (which measures a version of recall), METEOR (Banerjee and Lavie, 2005)"
2021.emnlp-main.595,D18-1437,0,0.0275677,"better or equal to a human caption (M1)"" and percentage of captions that pass the “Turing Test"" (M2), respectively. CLIP-S achieves Spearman ρM 1 /ρM 2 = .59/.63 and RefCLIP-S achieves ρM 1 /ρM 2 = .69/.74 (all p < .05) with these system-level metrics. CLIP-S 50 METEOR ROUGE-L CIDEr SPICE BERT-S (RoBERTa-F) CLIP-S (no refs) RefCLIP-S 50.2 66.5 78.8 71.7 82.5 75.5 88.6 50.2 82.6 85.4 79.3 90.6 86.1 92.1 87.2 91.0 87.2 92.6 45 e Scor CLIP 30 RefCLIPScore CLIPScore #1 #3 (a) Composite erence outperforms all metrics except for BERT-S And, RefCLIP-S works best in all cases. Overall, we corroborate Rohrbach et al. (2018)’s finding that “object hallucination can not be always predicted based on the traditional sentence metrics"" using a corpus derived from Shekhar et al. (2017), particularly in the case where there are few references available. However, CLIP-S and RefCLIP-S offer a performance improvement in the pairwise setting. Sensitivity of CLIP-S to memorization #5 30 ERT VILB RefCLIPScore CLIPScore #1 #3 #5 Importance Rank (b) Flickr8k-Expert Figure 2: R2 for the forward-selection regression of metrics on human Likert ratings for two corpora. Foward-selection tends to identify both CLIP-S and RefCLIP-S ea"
2021.emnlp-main.595,P16-1162,0,0.0208444,"bigrams, named entities, etc., were executed on a search engine. For each query, up to 20K (image, caption) pairs were collected. The model we use is the ViT-B/32 version.4 It represents images via a Vision Transformer (Vaswani et al., 2017; Dosovitskiy et al., 2021), which forgoes convolutional filters in favor of selfattention maps computed between a 7 by 7 grid of image patches, which evenly divides a 224 by 224 pixel input image. This model has 12 transformer layers and 86M parameters. The text is similarly represented by a 12-layer transformer trained over a vocab of 49K BPE token types (Sennrich et al., 2016) (and is more fully described in Radford et al. (2019)). Both the text and image networks output a single vector; these vectors aim to represent the content of an input caption or an image, respectively. In the case of ViT-B/32, these vectors are 512-D. The model’s weights are trained to maximize the scaled cosine similarity of truly corresponding image/caption pairs while simultaneously minimizing the similarity of mismatched image/caption pairs using InfoNCE (Sohn, 2016; Oord et al., 2018). We hold fixed this set of weights for our experiments. Evaluating Caption Generations with CLIP. To as"
2021.emnlp-main.595,D19-1116,0,0.0194693,"oss can provide insight into how distinctive the captions are according to the model itself. CLIP-S is similar in spirit, but distinct for its utility as an extrinsic evaluation metric like BLEU-4 or CIDEr. Reference-free evaluation In addition to the machine translation cases highlighted in the introduction, reference-free evaluations have been proposed for other generation tasks, including summarization 3 For comparison with these metrics, we use the standard COCO evaluation tools available at https://github. com/tylin/coco-caption. 7515 (Louis and Nenkova, 2013; Peyrard and Gurevych, 2018; Sun and Nenkova, 2019) and dialogue (Tao et al., 2018; Mehri and Eskenazi, 2020). These metrics can be supervised, relying on human judgments for quality estimation, or less-supervised, relying on pre-trained model representations. For image captioning, a version of VIFIDEL (Madhyastha et al., 2019) was proposed for reference-free evaluation; however, VIFIDEL, computed based on a list of detected objects in the image from a fixed object vocabulary, generally produces less correlation with human ratings vs. reference-based metrics. embeddings.5 We found that prefixing candidates with the prompt: “A photo depicts"" im"
2021.emnlp-main.595,W19-5410,0,0.0433452,"Missing"
2021.emnlp-main.595,2020.acl-main.93,0,0.0332978,"age caption evaluation In general, image caption generation models are evaluated by a suite of 5 reference based metrics: BLEU-4 (Papineni et al., 2002) (which measures a version of precision between a candidate and the references), ROUGE-L (Lin, 2004) (which measures a version of recall), METEOR (Banerjee and Lavie, 2005) (which computes a word-level alignment), CIDEr (Vedantam et al., 2015) (which combines n-gram tf-idf weighting and stemming) and SPICE (Anderson et al., 2016) (which applies a semantic parser to a set of references, and computes similarity using the predicted scene graph).3 Yi et al. (2020) give a method for re-weighting BERTScore (Zhang et al., 2020) specifically tuned to the image caption generation domain (we refer to their method as BERT-S++). Reference+image caption evaluation Recent metrics incorporate image-text grounding features in addition to references: TIGEr (Jiang et al., 2019) uses a pretrained SCAN model (Lee et al., 2018), and ViLBERTScore-F (Lee et al., 2020) uses a pretrained ViLBERT model (Lu et al., 2019) that is also fine-tuned on 12 downstream vision and language tasks (Lu et al., 2020). Our work provides perspective on the next logical extension: instead o"
2021.emnlp-main.595,Q14-1006,0,0.0608428,"xpert"" human judgments between 5664 images: humans graded captions on a scale of 1 to 4 (4=“caption describes the image without any errors""; 1=“caption is unrelated to the image""). Flickr8K-CF is a set of 145K binary quality judgments gathered from CrowdFlower over 48K (image, caption) pairs (1K unique images). Each pair has at least 3 binary judgments, and we take the mean proportion of “yes"" annotations as a score for each pair to compute correlations. Composite (Aditya et al., 2015) contains 12K human judgments between images from MSCOCO (2007 images), Flickr8k (997 images), and Flickr30k (Young et al., 2014) (991 images). Each image originally has five references, but one of the references was selected to be rated by humans in the set (and so we remove it from the reference set when computing metrics; this differs from some prior work, see Appendix A for why we consider the more difficult setting). For Composite and Flickr8K judgments, we compute correlation between each metric and the human ratings using Kendall τ . BLEU-4 CIDEr METEOR ROUGE-L SPICE BERT-S (RoBERTa-F) LEIC * CLIP-S (no refs) RefCLIP-S 16.9 24.6 22.2 19.9 24.4 22.8 29.5 34.4 36.4 Table 2: Flickr8K-CF correlations with human judgm"
2021.emnlp-main.595,2020.acl-main.151,0,0.0188977,"for thorough comparisons of caption generation metrics. 7514 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7514–7528 c November 7–11, 2021. 2021 Association for Computational Linguistics A recent trend in machine translation serves as inspiration: there, a key hurdle for reference-free evaluation (sometimes called quality estimation) has been estimating cross-lingual similarity between source+candidate pairs (Blatz et al., 2004; Specia et al., 2010; Mehdad et al., 2012; Specia and Shah, 2018). But recent work (Lo, 2019; Yankovskaya et al., 2019; Zhao et al., 2020) has improved correlation with human judgment not by gathering more monolingual references, but instead by utilizing cross-lingual representations learned by large-scale, pre-trained, multilingual models e.g., LASER (Artetxe and Schwenk, 2019) or MBERT (Devlin et al., 2019). 2 We hypothesize that the relationships learned by pretrained vision+language models (e.g., ALIGN (Jia et al., 2021) and CLIP (Radford et al., 2021)) could similarly support reference-free evaluation in the image captioning case. Indeed, they can: we show that a relatively direct application of CLIP to (image, generated ca"
2021.emnlp-main.595,2020.emnlp-main.745,0,0.0353873,"Missing"
2021.emnlp-main.595,2020.evalnlgeval-1.4,0,0.0455243,"Missing"
2021.findings-acl.107,P18-1027,0,0.0647649,"Missing"
2021.findings-acl.107,2020.emnlp-main.574,0,0.127507,"019). In sequence transduction tasks, these learned characteristics, embedded in attention, make pre-trained Transformers a powerful language model (Radford et al., 2019). A final task-specific step is typically required for adapting a task-agnostic language model to perform the desired task3 . However, these taskspecific characteristics might not sufficiently coincide with general characteristics even after finetuning. For example, task-specific characteristics embedded in attention patterns – such as word alignments for machine translation – are often noisy and imperfect for generalization (Kobayashi et al., 2020). We show that insufficient learning of taskspecific characteristics, reflected in sentence-level attention patterns4 often being out of focus, may be associated with neural text degeneration (§3). Based on this observation, we propose a simple attention modulation framework that can dynamically redistribute sentence-level attention weights by injecting task-specific priors in Transformer blocks for different downstream tasks (§4). Remarkably, in long-range narrative story generation, abductive reasoning generation and constrained commonsense text generation, both automatic and human evaluatio"
2021.findings-acl.107,2020.findings-emnlp.165,1,0.935655,"d with the insufficient change of sentence-level attention. In §4 and §6, we show that generation quality can be vastly improved by injecting the prior – attention should look at the prompt differently when generating different sentences – through our proposed attention modulation. Lack of commonsense reasoning vs. attention Text generated by neural language models is also observed to be lacking commonsense reasoning (Mao et al., 2019). We check whether this type of neural degeneration is associated with attention patterns. A benchmark dataset for generative commonsense reasoning – CommonGen (Lin et al., 2020) – is used as our test bed. CommonGen is designed for constrained commonsense reasoning: given a set of common concepts (e.g., use, tool, piece, metal); the task is to generate a coherent and plausible sentence covering all these concepts (e.g., &quot;a piece of metal is used for making tools&quot;). Covering the concepts in generation requires relational reasoning with background commonsense knowledge. 1263 covered uncovered agg. max attn. SD # 0.434 0.376 0.0040 0.0068 4515 1473 for three different tasks: narrative story generation, abductive reasoning generation, and constrained commonsense reasoning"
2021.findings-acl.107,W04-1013,0,0.04101,"e generation is less dull if more unique tokens are generated. For repetition, we directly measure sentence-level repetition: two generated sentences are repeated if their strings are the same. For relevancy, we measure the percentage of generated tokens that appear in the prompt. Besides, we perform a human evaluation, where three annotators are asked to rate the generations based on fluency, interestingness, newness, relevancy, and repetition. On αNLG, we score the generated explanation with respect to the reference using the following automatic metrics: BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005), and CIDEr (Vedantam et al., 2015). In addition, we ask annotators to compare the generated explanations without and with attention modulation. Human judges are asked to decide which system provides a more plausible explanation of the observations. On CommonGen, we report SPICE (Anderson et al., 2016) – a measure that evaluates semantic propositional content, in addtion to BLEU, 7 These search-based decoding algorithms do not resolve the poorly generated token-level probabilities. Figure 3: Human evaluation results on the next 1 to 5 sentences generated with"
2021.findings-acl.107,D19-1615,0,0.0187153,"es. The sentence-level attention changes are vastly lower on all prompt sentences when generating repeated consecutive sentences. Thus, sentence-level repetition may be correlated with the insufficient change of sentence-level attention. In §4 and §6, we show that generation quality can be vastly improved by injecting the prior – attention should look at the prompt differently when generating different sentences – through our proposed attention modulation. Lack of commonsense reasoning vs. attention Text generated by neural language models is also observed to be lacking commonsense reasoning (Mao et al., 2019). We check whether this type of neural degeneration is associated with attention patterns. A benchmark dataset for generative commonsense reasoning – CommonGen (Lin et al., 2020) – is used as our test bed. CommonGen is designed for constrained commonsense reasoning: given a set of common concepts (e.g., use, tool, piece, metal); the task is to generate a coherent and plausible sentence covering all these concepts (e.g., &quot;a piece of metal is used for making tools&quot;). Covering the concepts in generation requires relational reasoning with background commonsense knowledge. 1263 covered uncovered ag"
2021.findings-acl.107,N16-1098,0,0.0306713,"number of tokens generated in the whole test corpus, which measures the number of new unique tokens generated. rel. represent relevancy, which measures the percentage of tokens generated appears in the prompt. rep. measures the sentence-level repetition – whether two sentences generated are identical. ers.7 We present the results with non-stochastic decoding algorithms (i.e. greedy decoding and beam search), as generations based on them truly reflect the token-level probabilities predicted by the model (Holtzman et al., 2018). Datasets We use three different generation datasets – ROCStories (Mostafazadeh et al., 2016), αNLG (Bhagavatula et al., 2020), and CommonGen (Lin et al., 2020). For ROCStories, we used the 2017 version and split the data into 75/10/15 for train/val/test. Evaluation On ROCStories, we measure dullness, relevancy and repetition similar to Welleck et al. (2019). We report the number of unique tokens generated, where the generation is less dull if more unique tokens are generated. For repetition, we directly measure sentence-level repetition: two generated sentences are repeated if their strings are the same. For relevancy, we measure the percentage of generated tokens that appear in the"
2021.findings-acl.107,D19-1002,0,0.161151,"rchitecture of stacked decoder. As GPT2 follows a multi-layer and multi-headed setting, αi,j is specific to a layer l and head h, noted as l,h αi,j . We use the GPT2-L model that has 36 layers with 20 heads per layer (762M total parameters). 3 We briefly discuss how vanilla attention works, as well as Transformer architecture used in this paper. (2) Neural text degeneration vs. attention As researchers have sought to understand the internal mechanisms of Transformers, the attention patterns exhibited by these heads have drawn considerable study (Vig and Belinkov, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). We perform sentence-level attention analysis to explore whether aggregated attention patterns are associated with neural text degeneration. 3.1 Sentence-level attention We first define the sentence-to-sentence attention of a language model M with L layers and H heads. Given two sentences p and g such that p precedes g, 1262 l,h l,h the mean α ¯ g,p and max α ˆ g,p sentence-to-sentence attentions from g to p for layer l and head h are: |g |P |p| P l,h α ¯ g,p = l,h = α ˆ g,p i=1 j=1 l,h αi,j (gi , pj ) |g |· |p| max i∈{1,...,|g|} j∈{1,...,|p|} l,h αi,j (gi , pj ). (5) (6) The aggregated sente"
2021.findings-acl.107,P02-1040,0,0.110759,"ique tokens generated, where the generation is less dull if more unique tokens are generated. For repetition, we directly measure sentence-level repetition: two generated sentences are repeated if their strings are the same. For relevancy, we measure the percentage of generated tokens that appear in the prompt. Besides, we perform a human evaluation, where three annotators are asked to rate the generations based on fluency, interestingness, newness, relevancy, and repetition. On αNLG, we score the generated explanation with respect to the reference using the following automatic metrics: BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005), and CIDEr (Vedantam et al., 2015). In addition, we ask annotators to compare the generated explanations without and with attention modulation. Human judges are asked to decide which system provides a more plausible explanation of the observations. On CommonGen, we report SPICE (Anderson et al., 2016) – a measure that evaluates semantic propositional content, in addtion to BLEU, 7 These search-based decoding algorithms do not resolve the poorly generated token-level probabilities. Figure 3: Human evaluation results on the next 1 to 5 sente"
2021.findings-acl.107,W19-4808,0,0.368146,"et al., 2020). It 2 attention ratios are normalized mean sentence-to-sentence attention from generation H to observations O1 and O2 1261 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1261–1274 August 1–6, 2021. ©2021 Association for Computational Linguistics learns the general characteristics of language processing through pre-training on large amounts of unlabeled data. For example, multiple analyses have suggested that attention patterns in pre-trained Transformers implicitly encode syntactic information (Raganato and Tiedemann, 2018; Michel et al., 2019; Vig and Belinkov, 2019). In sequence transduction tasks, these learned characteristics, embedded in attention, make pre-trained Transformers a powerful language model (Radford et al., 2019). A final task-specific step is typically required for adapting a task-agnostic language model to perform the desired task3 . However, these taskspecific characteristics might not sufficiently coincide with general characteristics even after finetuning. For example, task-specific characteristics embedded in attention patterns – such as word alignments for machine translation – are often noisy and imperfect for generalization (Koba"
2021.findings-acl.42,W15-4918,0,0.0545411,"Missing"
2021.findings-acl.42,W07-0718,0,0.0285569,".04 0.00 -0.11 -0.12 0.04 0.07 -0.10 -0.12 0.07 -0.17 0.55 0.98 0.11 0.07 0.57 0.19 0.15 0.07 0.13 0.01** -0.01 -0.03 -0.09 -0.14 -0.03 0.01 -0.03 -0.09 0.01 0.03 p-value 0.82 0.64 0.18 0.03* 0.69 0.82 0.59 0.18 0.83 0.64 Table 5: Correlation (Corr) for 250 annotated XSUM and 250 SAMSUM generated summaries with fine-grained labeling. The arrow next to “Corr” indicates the direction of a correct correlation. 6 Related Work Prior work concerning evaluation of automatic metrics and human evaluation for NLG systems has mainly focused on general analysis of output quality or coherence and fluency (Callison-Burch et al., 2007; Graham, 2015; Fabbri et al., 2021), rather than factuality. Recent efforts by NLP researchers have drawn attention to the issue of factual errors 481 6.1 Discussion of Meta Evaluation and Conclusion Our analyses show that in contrast to prior work on factual consistency that mostly concentrated on one specific domain and dataset, our GO FIGURE framework is effective at evaluating sensitivity and validity of factual consistency metrics with only reference summaries, rather than requiring computationally intensive testing across summarization model variants to identify metric strengths and sho"
2021.findings-acl.42,P18-1060,0,0.0284132,"factuality level of Si may be unclear. Metric bounds provide points of comparison. A bounded but insensitive factuality metric may assign higher values to mostly nonfactual or unrelated summaries over summaries that are close to the reference. A metric that is sensitive only to a subset of errors might ignore a significant number of modelgenerated errors (Figure 1). Prior work such as Reiter and Belz (2009) highlight the risk of claiming validity without testing generality. The scoring function H(D, Si ) represented by human evaluation is a gold standard for assessment of generation quality (Chaganty et al., 2018), so M (D, Si ) should be an approximation. Table 1: Details of factuality metric conditions. Here M is a metric scoring function, D is a source document and Si is a summary. M (D, Sr ) where D is the source document and Sr is a randomly sampled summary from the corpus.2 We define the Upper Bound for the metric as M (D, Sf ), where Sf is the reference groundtruth summary. Since our controlled experiments use transformed versions of the reference summary with injected errors, the original reference is guaranteed to be at least as factually consistent as a transformed summary. To test sensitivit"
2021.findings-acl.42,P19-1264,1,0.837289,"Missing"
2021.findings-acl.42,2020.acl-main.454,0,0.0371766,"factual consistency and standard generation metrics, including QA metrics. It also reveals that while QA metrics generally improve over standard metrics that measure factuality across domains, performance is highly dependent on the way in which questions are generated. 1 2 Introduction The goal of text generation systems is to produce text that is fluent, coherent, relevant, as well as factually correct. Recent progress in neural approaches to building semantically constrained text generation systems has shown tremendous improvements in this direction (Liu and Lapata, 2019; Guo et al., 2018; Durmus et al., 2020; Wang et al., 2020). However, an important issue in text generation systems is that they can yield factually inconsistent text, caused by somewhat distorted or fabricated facts about the source text. Especially in document summarization tasks, models that abstract away salient aspects, have been shown to generate text ∗ Work done while first author was interning at MSR. Factuality Metric Meta Evaluation Since reference summaries may be an incomplete representation of the salient facts in a source document or unavailable, we consider factuality in terms of how well candidate summaries are fact"
2021.findings-acl.42,N19-1395,0,0.0436521,"Missing"
2021.findings-acl.42,P19-1213,0,0.0411927,"Missing"
2021.findings-acl.42,D19-5409,0,0.0132681,"o arise in realistic sum3 Evaluation Datasets 2 While this may not be the strictest lower bound in theoretical terms, we consider it appropriate as an empirical lower bound since the content is irrelevant to the document. A single random summary is used. 3 For our experiments, we inject up to a maximum of x errors with x ∈ {1, 2, 3}. We evaluate metrics on three datasets: 1-sentence BBC news summaries from the XSUM extreme summarization dataset (Narayan et al., 2018), multi-sentence summaries from the 479 CNN/DailyMail dataset (Nallapati et al., 2016), and the recently released SAMSUM corpus (Gliwa et al., 2019) consisting of English language conversations written by linguists and aligned multisentence summaries. 3.1 Diagnostic Datasets To test the ability of proposed metrics to fulfill our predefined conditions, we set up two diagnostic datasets consisting of (i) transformed reference summaries with simulated factuality errors that allow us to induce and measure factuality levels in a controlled setting and (ii) summaries generated by state-of-the-art transformer summarization models that allows us to measure the effectiveness of metrics in a real data setting. We sample 500 source / summary pairs f"
2021.findings-acl.42,D15-1013,0,0.0302286,"7 -0.10 -0.12 0.07 -0.17 0.55 0.98 0.11 0.07 0.57 0.19 0.15 0.07 0.13 0.01** -0.01 -0.03 -0.09 -0.14 -0.03 0.01 -0.03 -0.09 0.01 0.03 p-value 0.82 0.64 0.18 0.03* 0.69 0.82 0.59 0.18 0.83 0.64 Table 5: Correlation (Corr) for 250 annotated XSUM and 250 SAMSUM generated summaries with fine-grained labeling. The arrow next to “Corr” indicates the direction of a correct correlation. 6 Related Work Prior work concerning evaluation of automatic metrics and human evaluation for NLG systems has mainly focused on general analysis of output quality or coherence and fluency (Callison-Burch et al., 2007; Graham, 2015; Fabbri et al., 2021), rather than factuality. Recent efforts by NLP researchers have drawn attention to the issue of factual errors 481 6.1 Discussion of Meta Evaluation and Conclusion Our analyses show that in contrast to prior work on factual consistency that mostly concentrated on one specific domain and dataset, our GO FIGURE framework is effective at evaluating sensitivity and validity of factual consistency metrics with only reference summaries, rather than requiring computationally intensive testing across summarization model variants to identify metric strengths and shortcomings. We"
2021.findings-acl.42,D19-1320,0,0.0123154,"h or samplebased decoding strategies. We then annotate the generated summaries for fine-grained factual errors using the types in Figure 1 to create a hand-curated factual consistency diagnostic dataset. 4 Factuality Metrics for Evaluation We mainly focus on meta-evaluating most recently proposed factual consistency metrics which use two types of proxy natural language understanding (NLU) objectives aimed at implicitly capturing factuality in generated text: question-answering (QA) and a masked token prediction cloze task. For QA we evaluate using SummaQA (which uses QA pairs from the source, Scialom et al., 2019) and FEQA (which uses QA pairs from the summary, Durmus et al., 2020), while for the cloze task setting we use BLANC-Help and BLANC-Tune (Vasilyev et al., 2020, see the appendix for details of metrics). We also measure the factual-awareness of BERTScore (Zhang et al., 2020), a summarization metric that is aimed primarily at improving coherency rather than factual consistency, and standard summarization evaluation metrics (e.g. ROUGE (Lin, 2004)). 4 See the Appendix for details of linguistic feature extraction for injecting errors. 5 5.1 Meta-Analysis of Factuality Metrics Controlled Data Exper"
2021.findings-acl.42,2020.acl-main.704,0,0.0307123,"Missing"
2021.findings-acl.42,2020.emnlp-main.373,1,0.796905,"Missing"
2021.findings-acl.42,2020.eval4nlp-1.2,0,0.0168669,"ated factual consistency diagnostic dataset. 4 Factuality Metrics for Evaluation We mainly focus on meta-evaluating most recently proposed factual consistency metrics which use two types of proxy natural language understanding (NLU) objectives aimed at implicitly capturing factuality in generated text: question-answering (QA) and a masked token prediction cloze task. For QA we evaluate using SummaQA (which uses QA pairs from the source, Scialom et al., 2019) and FEQA (which uses QA pairs from the summary, Durmus et al., 2020), while for the cloze task setting we use BLANC-Help and BLANC-Tune (Vasilyev et al., 2020, see the appendix for details of metrics). We also measure the factual-awareness of BERTScore (Zhang et al., 2020), a summarization metric that is aimed primarily at improving coherency rather than factual consistency, and standard summarization evaluation metrics (e.g. ROUGE (Lin, 2004)). 4 See the Appendix for details of linguistic feature extraction for injecting errors. 5 5.1 Meta-Analysis of Factuality Metrics Controlled Data Experiments We provide the results of the sensitivity analysis over our controlled data on the XSUM domain in Table 2, on CNNDM in Table 3 and on SAMSUM in Table 4."
2021.findings-acl.42,2020.acl-main.450,0,0.0139054,"and standard generation metrics, including QA metrics. It also reveals that while QA metrics generally improve over standard metrics that measure factuality across domains, performance is highly dependent on the way in which questions are generated. 1 2 Introduction The goal of text generation systems is to produce text that is fluent, coherent, relevant, as well as factually correct. Recent progress in neural approaches to building semantically constrained text generation systems has shown tremendous improvements in this direction (Liu and Lapata, 2019; Guo et al., 2018; Durmus et al., 2020; Wang et al., 2020). However, an important issue in text generation systems is that they can yield factually inconsistent text, caused by somewhat distorted or fabricated facts about the source text. Especially in document summarization tasks, models that abstract away salient aspects, have been shown to generate text ∗ Work done while first author was interning at MSR. Factuality Metric Meta Evaluation Since reference summaries may be an incomplete representation of the salient facts in a source document or unavailable, we consider factuality in terms of how well candidate summaries are factually grounded with"
2021.findings-acl.42,2020.findings-emnlp.203,0,0.0170721,".28 / 84.19 84.13 / 84.07 80.79 Correlation p-value -1.00 / -0.96 0.05* / 0.18 -1.00 / -0.91 0.01** / 0.28 -0.99 / -0.94 0.11 / 0.23 -1.00 0.05* -1.00 / -0.99 0.02* / 0.07 -1.00 &lt;0.01** / 0.03* -1.00 0.03* -1.00 / -0.99 0.05* / 0.08 -1.00 0.01** / 0.04* -1.00 /-0.99 0.01** / 0.07 Table 4: Results of simulated factual error data experiments (SAMSUM, average of 5 runs). (See Table 2 caption for details.) Metric BLANC-Help BLANC-Tune SummaQA-C SummaQA-F1 FEQA R-1 R-2 R-3 R-L BERTScore XSUM and hallucinations in the output of neural summarization models (Cao et al., 2018; Massarelli et al., 2019; Zhao et al., 2020; Falke et al., 2019b; Goodrich et al., 2019; Celikyilmaz et al., 2020). A number of works have highlighted the effectiveness of QA and cloze task objectives for evaluating or improving factuality on specific domains (Eyal et al., 2019; Huang et al., 2020). We aim to evaluate these metrics more broadly, and consider a wider range of domains (notably dialogue). SAMSUM Corr (- ←) p-value Corr (- ←) 0.04 0.00 -0.11 -0.12 0.04 0.07 -0.10 -0.12 0.07 -0.17 0.55 0.98 0.11 0.07 0.57 0.19 0.15 0.07 0.13 0.01** -0.01 -0.03 -0.09 -0.14 -0.03 0.01 -0.03 -0.09 0.01 0.03 p-value 0.82 0.64 0.18 0.03* 0.69 0."
2021.findings-emnlp.184,2020.nuse-1.7,0,0.0553705,"Missing"
2021.findings-emnlp.184,W17-0905,0,0.0757418,"dataset, and models offer a new research direction for learning script knowledge. Scenario: bake a cake find the cake recipe gather the ingredients turn on the oven mix the ingredients put the cake batter in the oven bake for the right amount of time take the cake out of the oven Figure 1: We collected 6.4k of partially ordered scripts (proScript) and developed models that take a scenario (e.g., bake a cake) as the input and generate a (possibly partial-order) script. In proScript, an event (node) requires that all the precedent events and paths are happened/executed in advance. et al., 2010; Chambers, 2017; Ostermann, 2020). In this work, we show for the first time that pre-trained neural language models (LMs) can be adapted to generate high-quality scripts, including appropriately partial ordering events where a specific temporal ordering is required only when it is necessary. LMs have previously been shown to successfully generate stories (Rashkin et al., 2020), summaries (Lewis et al., 2020), and commonsense facts (Bosselut et al., 2019; Hwang et al., 2020). 1 Introduction Here we investigate their application to script genScripts (Schank and Abelson, 1975) represent struc- eration. First, w"
2021.findings-emnlp.184,P08-1090,0,0.371077,"ipt generation, the best model obtains a graph edit distance of 4.97 (i.e., number of human edits), while human-created scripts achieve 2.98 on average. Our contributions are thus: • A new dataset (proScript) of crowdsourced scripts that is substantially larger than prior (manually crafted) datasets • Two complementary task definitions against proScript • Two new models for these task, providing the first demonstration that generative models can be successfully applied, although it is still significantly below human levels. 2 Related Work Script as narrative chain Mooney and DeJong (1985) and Chambers and Jurafsky (2008, inter alia) have investigated automatically inducing scripts from (unstructured) corpus. In particular, Chambers and Jurafsky (2008) introduced scripts as narrative chain, where verbs with the participants information (e.g., (claimed, subj), and (accused, obj) ) named narrative events are partially ordered according to causal and temporal relations. They also introduced narrative cloze task, where a model is expected to predict one removed narrative event, given all the other narrative events, while our proposed task requires to generate scripts as a partial-order graph for a given scenario."
2021.findings-emnlp.184,2021.naacl-demos.2,0,0.0320653,"3 We focus on events and the partial-ordering for the protagonist (Chambers and Jurafsky, 2008), and leave the identification of other participants for future work. Source of Scenarios We collected scenarios from DeScript (Wanzare et al., 2016), VirtualHome (Puig et al., 2018), and ROCStories (Mostafazadeh et al., 2016). DeScript consists of 40 daily scenarios (e.g., making coffee) and we 2140 Crowdsourcing proScript For the collected scenarios, we crowdsource the corresponding proScript on the Amazon Mechanical Turk. Our crowdsourcing procedure (Figure 2) is similar but simplified method to (Ciosici et al., 2021). First, given a scenario (e.g., bake a cake), each crowdworker is required to describe five to seven core events that they are essential for the scenario (Chambers, 2017) with the estimated time it takes to complete each event.4 In the second question, crowdworkers confirm the set of steps and they are asked to create a flowchart (DAG) by connecting the steps possibly in partial order. When crowdworkers make a submission, validation function is executed to check if the created flowchart is a valid (transitive reduction of) DAG that does not contain a cycle/loop and any short cut edge. Due to"
2021.findings-emnlp.184,N18-1144,1,0.788883,"limited amount of data hinders learning script knowledge by models. Furthermore, they provide no evaluation metric on the dataset for assessing model’s script knowledge. Story generation and tracking state changes Neural models have been demonstrated to success1 fully generate stories (Kiddon et al., 2016; Peng The dataset and code are available at https:// proscript.allenai.org/ et al., 2018; Zhai et al., 2019; Rashkin et al., 2020) 2139 Suppose a scenario where someone wants to “bake a cake”. as well as tracking state changes in procedural texts (Henaff et al., 2017; Bosselut et al., 2018; Dalvi et al., 2018; Tandon et al., 2020). Our work is related in terms of generating higher-level agenda (or plot) of a story and understanding latent preconditions and effects between events. However, a main difference between these studies and scripts is that story generation and state change tracking explicitly generate and/or predict character’s mental states and entity’s physical attributes (e.g., temperature), whereas scripts focuses on essential core events (Chambers, 2017) in partial order. 3 Preliminary Question: How long will it take for this scenario? 1.5 second(s) minute(s) hour(s) day(s) month(s) y"
2021.findings-emnlp.184,N19-1423,0,0.0105722,"and edges of the script. Given a scenario (s) and the number of events to generate in the script, proScriptgen generates events and edges for the partial-order script (G) in DOT language (Figure 6). Formally, we use the same encoder-decoder framework (eq.2) except that a scenario and number of steps to generate are described in natural text as x and the decoder is expected to generate both events and the edges (as y) in the script jointly. Transfer learning from WikiHow data Transfer learning often helps improve the performance when it is (pre-)trained on a similar task (Peters et al., 2018; Devlin et al., 2019). As additional resource for pre-training proScriptgen , we use procedural texts extracted from WikiHow,12 which contains 130k instances of a sequence of essential steps for a given topic in various categories (e.g., health, finance, hobbies, etc.). It is important to note that all the procedures in WikiHow are formatted as sequences rather than a partialorder, and therefore the model is biased towards generating sequences. We refer to this approach as proScriptgen-transfer . proScriptgen The proScript generation task combines natural language generation (i.e. generating events in natural lang"
2021.findings-emnlp.184,E12-1034,0,0.0388184,"Missing"
2021.findings-emnlp.184,D16-1032,1,0.822338,"shortcoming of this approach is the scalability; it is not easy to scale because of the cost for manual data collection (Chambers, 2017; Ostermann, 2020). In fact, Modi et al. (2016) crowdsourced 1000 stories that cover only 10 scripts, and similarly Regneri et al. (2010) end up with collecting 40 scripts. The limited amount of data hinders learning script knowledge by models. Furthermore, they provide no evaluation metric on the dataset for assessing model’s script knowledge. Story generation and tracking state changes Neural models have been demonstrated to success1 fully generate stories (Kiddon et al., 2016; Peng The dataset and code are available at https:// proscript.allenai.org/ et al., 2018; Zhai et al., 2019; Rashkin et al., 2020) 2139 Suppose a scenario where someone wants to “bake a cake”. as well as tracking state changes in procedural texts (Henaff et al., 2017; Bosselut et al., 2018; Dalvi et al., 2018; Tandon et al., 2020). Our work is related in terms of generating higher-level agenda (or plot) of a story and understanding latent preconditions and effects between events. However, a main difference between these studies and scripts is that story generation and state change tracking ex"
2021.findings-emnlp.184,2020.acl-main.703,0,0.0251896,"(e.g., bake a cake) as the input and generate a (possibly partial-order) script. In proScript, an event (node) requires that all the precedent events and paths are happened/executed in advance. et al., 2010; Chambers, 2017; Ostermann, 2020). In this work, we show for the first time that pre-trained neural language models (LMs) can be adapted to generate high-quality scripts, including appropriately partial ordering events where a specific temporal ordering is required only when it is necessary. LMs have previously been shown to successfully generate stories (Rashkin et al., 2020), summaries (Lewis et al., 2020), and commonsense facts (Bosselut et al., 2019; Hwang et al., 2020). 1 Introduction Here we investigate their application to script genScripts (Schank and Abelson, 1975) represent struc- eration. First, we collect large amount (6.4k) of tured commonsense knowledge about prototypi- partially ordered script from crowdsourcing with cal events in everyday situations/scenarios such as a similar but simplified collection method (Ciosici bake a cake (Figure 1). However, while scripts et al., 2021). We call the dataset as proScript have been shown to help understand narratives by (PaRtial Order SCRIPT"
2021.findings-emnlp.184,2021.ccl-1.108,0,0.0600469,"Missing"
2021.findings-emnlp.184,L16-1555,0,0.0954658,"Weber et al., 2018; Belyy and Van Durme, 2020), but it has its drawbacks. First, the source corpora is mainly from a news domain rather than everyday scenarios, and induced narrative chains contain a number of non-script events such as reporting verbs (Mostafazadeh et al., 2016; Chambers, 2017). Second, events are highly abstracted as tuples of verb and the dependency (subj or obj) (Ostermann, 2020). Third, the evaluation scheme for the narrative cloze task is insufficient to evaluate script knowledge (Chambers, 2017). Script as paraphrase sets Script as paraphrase sets (Regneri et al., 2010; Modi et al., 2016; Wanzare et al., 2016) is more recent approach to gather script knowledge, where crowd workers are asked to write down a sequence of events for a given everyday scenario (e.g., bake a cake) and the collected sequences (called event sequence description) are aligned with paraphrased events being clustered. The collected (partially ordered) scripts cover wide variety of everyday situations compared to narrative chains (news domain), but one shortcoming of this approach is the scalability; it is not easy to scale because of the cost for manual data collection (Chambers, 2017; Ostermann, 2020). I"
2021.findings-emnlp.184,W14-1606,0,0.0231252,"s. In particular, Chambers and Jurafsky (2008) introduced scripts as narrative chain, where verbs with the participants information (e.g., (claimed, subj), and (accused, obj) ) named narrative events are partially ordered according to causal and temporal relations. They also introduced narrative cloze task, where a model is expected to predict one removed narrative event, given all the other narrative events, while our proposed task requires to generate scripts as a partial-order graph for a given scenario. The “script as narrative chain” approach has been actively studied (Jans et al., 2012; Modi and Titov, 2014; Pichotta and Mooney, 2014; Rudinger et al., 2015; Granroth-Wilding and Clark, 2016; Weber et al., 2018; Belyy and Van Durme, 2020), but it has its drawbacks. First, the source corpora is mainly from a news domain rather than everyday scenarios, and induced narrative chains contain a number of non-script events such as reporting verbs (Mostafazadeh et al., 2016; Chambers, 2017). Second, events are highly abstracted as tuples of verb and the dependency (subj or obj) (Ostermann, 2020). Third, the evaluation scheme for the narrative cloze task is insufficient to evaluate script knowledge (Chambe"
2021.findings-emnlp.184,Q17-1003,0,0.0150327,"mmonsense knowledge about prototypi- partially ordered script from crowdsourcing with cal events in everyday situations/scenarios such as a similar but simplified collection method (Ciosici bake a cake (Figure 1). However, while scripts et al., 2021). We call the dataset as proScript have been shown to help understand narratives by (PaRtial Order SCRIPT), and this is substantially providing expectations, resolving ambiguity, and larger and more diverse than prior (crowdsourced) filling in unstated information (Chambers and Ju- dataset such as DeScript (Regneri et al., 2010) that rafsky, 2008; Modi et al., 2017, inter alia), they has 40 scripts. In proScript, all the events/paths have proved hard to author or extract from text, need to be happened/executed (cf. AND arcs in with only small script databases available (Regneri AND/OR graphs), whereas prior work on scripts 2138 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2138–2149 November 7–11, 2021. ©2021 Association for Computational Linguistics do not distinct core and optional/alternative events explicitly. Additionally, temporal duration of each event is annotated (e.g., take the cake out of the oven typically take"
2021.findings-emnlp.184,1985.tmi-1.17,0,0.174662,"n achieves 89.28, and for script generation, the best model obtains a graph edit distance of 4.97 (i.e., number of human edits), while human-created scripts achieve 2.98 on average. Our contributions are thus: • A new dataset (proScript) of crowdsourced scripts that is substantially larger than prior (manually crafted) datasets • Two complementary task definitions against proScript • Two new models for these task, providing the first demonstration that generative models can be successfully applied, although it is still significantly below human levels. 2 Related Work Script as narrative chain Mooney and DeJong (1985) and Chambers and Jurafsky (2008, inter alia) have investigated automatically inducing scripts from (unstructured) corpus. In particular, Chambers and Jurafsky (2008) introduced scripts as narrative chain, where verbs with the participants information (e.g., (claimed, subj), and (accused, obj) ) named narrative events are partially ordered according to causal and temporal relations. They also introduced narrative cloze task, where a model is expected to predict one removed narrative event, given all the other narrative events, while our proposed task requires to generate scripts as a partial-o"
2021.findings-emnlp.184,N16-1098,0,0.154238,"arrative event, given all the other narrative events, while our proposed task requires to generate scripts as a partial-order graph for a given scenario. The “script as narrative chain” approach has been actively studied (Jans et al., 2012; Modi and Titov, 2014; Pichotta and Mooney, 2014; Rudinger et al., 2015; Granroth-Wilding and Clark, 2016; Weber et al., 2018; Belyy and Van Durme, 2020), but it has its drawbacks. First, the source corpora is mainly from a news domain rather than everyday scenarios, and induced narrative chains contain a number of non-script events such as reporting verbs (Mostafazadeh et al., 2016; Chambers, 2017). Second, events are highly abstracted as tuples of verb and the dependency (subj or obj) (Ostermann, 2020). Third, the evaluation scheme for the narrative cloze task is insufficient to evaluate script knowledge (Chambers, 2017). Script as paraphrase sets Script as paraphrase sets (Regneri et al., 2010; Modi et al., 2016; Wanzare et al., 2016) is more recent approach to gather script knowledge, where crowd workers are asked to write down a sequence of events for a given everyday scenario (e.g., bake a cake) and the collected sequences (called event sequence description) are al"
2021.findings-emnlp.184,W18-1505,0,0.0578474,"Missing"
2021.findings-emnlp.184,N18-1202,0,0.0182608,"a sequence of events and edges of the script. Given a scenario (s) and the number of events to generate in the script, proScriptgen generates events and edges for the partial-order script (G) in DOT language (Figure 6). Formally, we use the same encoder-decoder framework (eq.2) except that a scenario and number of steps to generate are described in natural text as x and the decoder is expected to generate both events and the edges (as y) in the script jointly. Transfer learning from WikiHow data Transfer learning often helps improve the performance when it is (pre-)trained on a similar task (Peters et al., 2018; Devlin et al., 2019). As additional resource for pre-training proScriptgen , we use procedural texts extracted from WikiHow,12 which contains 130k instances of a sequence of essential steps for a given topic in various categories (e.g., health, finance, hobbies, etc.). It is important to note that all the procedures in WikiHow are formatted as sequences rather than a partialorder, and therefore the model is biased towards generating sequences. We refer to this approach as proScriptgen-transfer . proScriptgen The proScript generation task combines natural language generation (i.e. generating"
2021.findings-emnlp.184,E14-1024,0,0.023016,"bers and Jurafsky (2008) introduced scripts as narrative chain, where verbs with the participants information (e.g., (claimed, subj), and (accused, obj) ) named narrative events are partially ordered according to causal and temporal relations. They also introduced narrative cloze task, where a model is expected to predict one removed narrative event, given all the other narrative events, while our proposed task requires to generate scripts as a partial-order graph for a given scenario. The “script as narrative chain” approach has been actively studied (Jans et al., 2012; Modi and Titov, 2014; Pichotta and Mooney, 2014; Rudinger et al., 2015; Granroth-Wilding and Clark, 2016; Weber et al., 2018; Belyy and Van Durme, 2020), but it has its drawbacks. First, the source corpora is mainly from a news domain rather than everyday scenarios, and induced narrative chains contain a number of non-script events such as reporting verbs (Mostafazadeh et al., 2016; Chambers, 2017). Second, events are highly abstracted as tuples of verb and the dependency (subj or obj) (Ostermann, 2020). Third, the evaluation scheme for the narrative cloze task is insufficient to evaluate script knowledge (Chambers, 2017). Script as paraph"
2021.findings-emnlp.184,2020.emnlp-main.349,1,0.886511,"Missing"
2021.findings-emnlp.184,D15-1195,0,0.04212,"Missing"
2021.findings-emnlp.184,2020.emnlp-main.520,1,0.717003,"ata hinders learning script knowledge by models. Furthermore, they provide no evaluation metric on the dataset for assessing model’s script knowledge. Story generation and tracking state changes Neural models have been demonstrated to success1 fully generate stories (Kiddon et al., 2016; Peng The dataset and code are available at https:// proscript.allenai.org/ et al., 2018; Zhai et al., 2019; Rashkin et al., 2020) 2139 Suppose a scenario where someone wants to “bake a cake”. as well as tracking state changes in procedural texts (Henaff et al., 2017; Bosselut et al., 2018; Dalvi et al., 2018; Tandon et al., 2020). Our work is related in terms of generating higher-level agenda (or plot) of a story and understanding latent preconditions and effects between events. However, a main difference between these studies and scripts is that story generation and state change tracking explicitly generate and/or predict character’s mental states and entity’s physical attributes (e.g., temperature), whereas scripts focuses on essential core events (Chambers, 2017) in partial order. 3 Preliminary Question: How long will it take for this scenario? 1.5 second(s) minute(s) hour(s) day(s) month(s) year(s) Main Question 1"
2021.findings-emnlp.184,L16-1556,0,0.104406,"Belyy and Van Durme, 2020), but it has its drawbacks. First, the source corpora is mainly from a news domain rather than everyday scenarios, and induced narrative chains contain a number of non-script events such as reporting verbs (Mostafazadeh et al., 2016; Chambers, 2017). Second, events are highly abstracted as tuples of verb and the dependency (subj or obj) (Ostermann, 2020). Third, the evaluation scheme for the narrative cloze task is insufficient to evaluate script knowledge (Chambers, 2017). Script as paraphrase sets Script as paraphrase sets (Regneri et al., 2010; Modi et al., 2016; Wanzare et al., 2016) is more recent approach to gather script knowledge, where crowd workers are asked to write down a sequence of events for a given everyday scenario (e.g., bake a cake) and the collected sequences (called event sequence description) are aligned with paraphrased events being clustered. The collected (partially ordered) scripts cover wide variety of everyday situations compared to narrative chains (news domain), but one shortcoming of this approach is the scalability; it is not easy to scale because of the cost for manual data collection (Chambers, 2017; Ostermann, 2020). In fact, Modi et al. (20"
2021.findings-emnlp.184,D18-1413,0,0.0168986,"participants information (e.g., (claimed, subj), and (accused, obj) ) named narrative events are partially ordered according to causal and temporal relations. They also introduced narrative cloze task, where a model is expected to predict one removed narrative event, given all the other narrative events, while our proposed task requires to generate scripts as a partial-order graph for a given scenario. The “script as narrative chain” approach has been actively studied (Jans et al., 2012; Modi and Titov, 2014; Pichotta and Mooney, 2014; Rudinger et al., 2015; Granroth-Wilding and Clark, 2016; Weber et al., 2018; Belyy and Van Durme, 2020), but it has its drawbacks. First, the source corpora is mainly from a news domain rather than everyday scenarios, and induced narrative chains contain a number of non-script events such as reporting verbs (Mostafazadeh et al., 2016; Chambers, 2017). Second, events are highly abstracted as tuples of verb and the dependency (subj or obj) (Ostermann, 2020). Third, the evaluation scheme for the narrative cloze task is insufficient to evaluate script knowledge (Chambers, 2017). Script as paraphrase sets Script as paraphrase sets (Regneri et al., 2010; Modi et al., 2016;"
2021.findings-emnlp.184,W19-3404,0,0.0193821,"collection (Chambers, 2017; Ostermann, 2020). In fact, Modi et al. (2016) crowdsourced 1000 stories that cover only 10 scripts, and similarly Regneri et al. (2010) end up with collecting 40 scripts. The limited amount of data hinders learning script knowledge by models. Furthermore, they provide no evaluation metric on the dataset for assessing model’s script knowledge. Story generation and tracking state changes Neural models have been demonstrated to success1 fully generate stories (Kiddon et al., 2016; Peng The dataset and code are available at https:// proscript.allenai.org/ et al., 2018; Zhai et al., 2019; Rashkin et al., 2020) 2139 Suppose a scenario where someone wants to “bake a cake”. as well as tracking state changes in procedural texts (Henaff et al., 2017; Bosselut et al., 2018; Dalvi et al., 2018; Tandon et al., 2020). Our work is related in terms of generating higher-level agenda (or plot) of a story and understanding latent preconditions and effects between events. However, a main difference between these studies and scripts is that story generation and state change tracking explicitly generate and/or predict character’s mental states and entity’s physical attributes (e.g., temperatu"
2021.findings-emnlp.184,P10-1100,0,0.17898,"e collect large amount (6.4k) of tured commonsense knowledge about prototypi- partially ordered script from crowdsourcing with cal events in everyday situations/scenarios such as a similar but simplified collection method (Ciosici bake a cake (Figure 1). However, while scripts et al., 2021). We call the dataset as proScript have been shown to help understand narratives by (PaRtial Order SCRIPT), and this is substantially providing expectations, resolving ambiguity, and larger and more diverse than prior (crowdsourced) filling in unstated information (Chambers and Ju- dataset such as DeScript (Regneri et al., 2010) that rafsky, 2008; Modi et al., 2017, inter alia), they has 40 scripts. In proScript, all the events/paths have proved hard to author or extract from text, need to be happened/executed (cf. AND arcs in with only small script databases available (Regneri AND/OR graphs), whereas prior work on scripts 2138 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2138–2149 November 7–11, 2021. ©2021 Association for Computational Linguistics do not distinct core and optional/alternative events explicitly. Additionally, temporal duration of each event is annotated (e.g., take th"
2021.naacl-main.339,D17-1098,0,0.401499,"cally conjunctions). • Second, N EURO L OGIC effectively optimizes objective function through efficient and diverse search over output space, while previous works suffer from either myopic and narrow or inefficient exploration of the search space. • Third, the asymptotic runtime of N EURO L OGIC is O(N k)1 , same with beam search, constant with respect to number of constraints C. Some previous works suffer from exponential runtime, making applications infeasible. A detailed comparison between N EURO L OGIC and previous methods is provided in table 1. 3.1 Previous Constrained Decoding Approach Anderson et al. (2017) propose constrained beam search (CBS), where constraint satisfaction is tracked by a finite-state machine with 2C states (all possible satisfaction status for C constraints). Beam search is done over all states with k candidates per state. This method has an exponential complexity O(N k2C ), making many applications infeasible. Hokamp and Liu (2017) propose grid beam search (GBS), which groups together hypotheses by number of constraints satisfied, giving C + 1 1 N EURO L OGIC distinguishes itself from past works N denotes sequence length and k denotes beam size. In this paper, we the asympto"
2021.naacl-main.346,2020.acl-main.499,0,0.0285167,"op, cease, halt, prohibit, forbid, prevent, reject, fail, etc. X denies the existence of god X restrains himself from eating with Y X refuses to be in a relationship Table 1: Negation cues and examples from A NION. the negated event, “X opposes racism,” COMET infers “X intends to be a racist,” an association of the affirmative statement, “X supports racism.” At the heart of this problem is that inferring commonsense knowledge about negations often requires implicit reasoning. In factual knowledge reasoning, applying logical rules over statements can be effective for handling negative queries (Asai and Hajishirzi, 2020; Ren and Leskovec, 2020). However, directly manipulating affirmative forms with logic-guided rules may fail for commonsense reasoning: the boundary of commonsense inferences between affirmative and negated statements is not always wholly contrastive. Many inferences can be relevant to both forms. The events “X puts the potato in the oven” and “X doesn’t put the potato in the oven,” could both have an associated inference: “X wants to make dinner.” The affirmative event clearly implies this inference. For the negated event to be worth mentioning on its own (Grice et al., 1975), an implicit com"
2021.naacl-main.346,P19-1470,1,0.7681,"ph. Specifically, COMET receives knowledge tuples in {h, r, t} form during training, where h is a head entity, r is a relation type, and t is a tail entity. The model is trained to maximize the conditional loglikelihood of predicting the tokens of the tail entity t given the tokens of the head entity h and relation r: X LG = − log P (t|h, r) (1) In ATOMIC and A NION, h corresponds to events, such as “X has a nightmare,” t corresponds to commonsense inferences about those events, such as “X wakes up,” and r corresponds to commonsense inference types, such as “As a result, X does...”. Following Bosselut et al. (2019) and Sap et al. (2020), for each event and relation type in ATOMIC, 10 candidate inferences are decoded from COMET using beam search with b=10. 4.2 Experiments As oppositional instances remain challenging to knowledge models such as COMET, we evaluate how A NION can be used to augment the type of examples seen by COMET during training. For the human evaluation, we employ human judges from MTurk to identify whether generated commonsense inferences are plausible. We randomly sample 100 events from the original ATOMIC test set along with their negated counterparts from A NION. For each event, we"
2021.naacl-main.346,W10-3110,0,0.0270402,"al (S) event inference 4.52 - 5,019 138,587 2,457 66,087 1,223 33,030 1,339 39,470 A NION - Commonsense Contradiction (C) event inference 4.46 - 9,179 262,820 3,267 93,419 2,808 95,685 3,104 73,716 Table 2: Statistics of ATOMIC and different subsets of A NION (A NION-L + A NION-S + A NION-C). Logical Negation We define logical negation events as events with the negation cue not added to their original formulation (e.g., “X does not play the piano”). However, different positions of the not modifier in a clause can result in different negation scopes, which can alter the semantics of the event (Councill et al., 2010). To be consistent, we systematically insert not after the subject of the event clause. If necessary, we change verb forms and add auxiliary words (e.g., do, does, did, is, was, can, could, would, should, may, might). For quality control, we have human workers validate each logically negated event form and exclude events that annotators identify as uninterpretable or awkwardly worded. For each created event, we then collect the same nine dimensions of inferences as defined in ATOMIC. Consequently, we collected 8,285 logically negated events with 225K corresponding inferences (as shown in Table"
2021.naacl-main.346,W15-1301,0,0.0684431,"Missing"
2021.naacl-main.346,2020.emnlp-main.99,0,0.0284793,"using on negated and contradictory events. We then present joint generative and discriminative inference models for this new resource, providing novel empirical insights on how logical negations and commonsense contradictions reshape the commonsense implications of their original premises. 1 X takes his mask off Data and code available at https://github.com/ liweijiang/anion commonsense knowledge resources, such as Cyc (Lenat, 1995), ConceptNet (Speer et al., 2017), and ATOMIC (Sap et al., 2020; Hwang et al., 2020), to provide structured reasoning capabilities to AI systems (Lin et al., 2019; Feng et al., 2020). However, reasoning about negated observations remains a challenge (Hossain et al., 2020). While negation is often considered a poorer form of meaning than affirmation2 (Ackrill, 1963; Horn and Wansing, 2020), negated statements can still imply expressive commonsense inferences. In Figure 1, the negated event “X doesn’t wear a mask,” 2 Following Horn and Wansing (2020), we classify declarative expressions as affirmations or negations/contradictions based on whether they affirm or deny an action or object. 4380 Proceedings of the 2021 Conference of the North American Chapter of the Association"
2021.naacl-main.346,2020.findings-emnlp.345,0,0.0772576,"inative inference models for this new resource, providing novel empirical insights on how logical negations and commonsense contradictions reshape the commonsense implications of their original premises. 1 X takes his mask off Data and code available at https://github.com/ liweijiang/anion commonsense knowledge resources, such as Cyc (Lenat, 1995), ConceptNet (Speer et al., 2017), and ATOMIC (Sap et al., 2020; Hwang et al., 2020), to provide structured reasoning capabilities to AI systems (Lin et al., 2019; Feng et al., 2020). However, reasoning about negated observations remains a challenge (Hossain et al., 2020). While negation is often considered a poorer form of meaning than affirmation2 (Ackrill, 1963; Horn and Wansing, 2020), negated statements can still imply expressive commonsense inferences. In Figure 1, the negated event “X doesn’t wear a mask,” 2 Following Horn and Wansing (2020), we classify declarative expressions as affirmations or negations/contradictions based on whether they affirm or deny an action or object. 4380 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4380–4397 June 6–11, 20"
2021.naacl-main.386,P13-5002,0,0.0685292,"Missing"
2021.naacl-main.386,Q19-1026,0,0.0245137,"q). However, the label prediction term is bounded by the information (or entropy, H) of the label space: IpA; Yq “ HpYq ´ HpY|Aq ď HpYq. (2) Thus, for a task with a small label space, there is no guarantee that a model will learn highinformation content attributes. Models are in fact 1 Models submitted to the 2019 Conference on Machine encouraged to overfit to dataset artifacts, and to Translation were evaluated (by humans) on how well the unlearn linguistically useful information that is not model’s translations agreed with either (1) human-written translations, or, (2) original source text (Barrault et al., 2019). directly relevant to predicting Y (Pereira, 2000). 4858 3 An alternate approach is to make datasets harder adversarially, so as to have fewer artifacts (Zellers et al., 2018, 2019a; Le Bras et al., 2020). However, it might be impossible to make a dataset with no artifacts, or to know if one has been created. Our proposal, to evaluate models by their realworld language use, addresses the information bottleneck issue in two ways. First, when we use language in the real world, the mapping between possible inputs and outputs is often highly complex. For example, the space of possible advice is v"
2021.naacl-main.386,N18-2017,0,0.0253786,"tudy the true task of commonsense reasoning. However, there are gaps between datasets for proxy tasks (e.g. multiple choice), and the core tasks they seek to represent (e.g. commonsense reasoning), which we discuss in the next sections. 2.2 Can language use really be measured through correctness over proxy tasks? When we reduce a complex language task to a simplified setup, with a small label space (like multiple-choice classification), we run the risk of introducing artifacts and biases: patterns that can be exploited in the simplified setup, but that are not representative of the true task (Gururangan et al., 2018; Zellers et al., 2019a). Artifacts can enable machines to even outperform humans at the final benchmark, without solving the underlying task. While the problem of artifacts has recently taken the spotlight in the NLP community, partially because large Transformers (Vaswani et al., 2017) excel at picking up on artifacts, there is a deeper underlying issue. One way to view simplified tasks is that in order to correctly map inputs X to labels Y, a machine must learn a set of attributes A that are representative of the ‘true’ task. We can upperbound the information contained by A through the info"
2021.naacl-main.386,N19-1169,0,0.0979854,"al et al., 2019; Vasilyev et al., 2020). However, ensuring that machines communicate in standard English is difficult, as there is usually a more efficient machine-language coding scheme for the task (Kottur et al., 2017). 2.1.2 Two major approaches for evaluation Today, we see two major approaches for NLP evaluation, which we discuss below. Quality of generations. The first approach studies generative tasks like chit-chat dialogue or storywriting, and measures the inherent quality of generations, often through attributes such as “sensibleness” and “specificity” (e.g., Venkatesh et al., 2018; Hashimoto et al., 2019; Adiwardana et al., 2020). This approach is orthogonal to ours: though these attributes might be desirable, they are often insufficient to guarantee success at a task. Correctness. The second (and perhaps more common) approach is to evaluate models through correctness over static datasets. For example, machines can be graded by the similarity of their generated translation to correct translations,1 or, by how often they choose the correct answer on a multiple choice exam. Many goal-oriented dialogue and semantics tasks are also evaluated in this way, as a model is evaluated by whether it make"
2021.naacl-main.386,W12-4501,0,0.107426,"Missing"
2021.naacl-main.386,P19-1472,1,0.876457,"monsense reasoning. However, there are gaps between datasets for proxy tasks (e.g. multiple choice), and the core tasks they seek to represent (e.g. commonsense reasoning), which we discuss in the next sections. 2.2 Can language use really be measured through correctness over proxy tasks? When we reduce a complex language task to a simplified setup, with a small label space (like multiple-choice classification), we run the risk of introducing artifacts and biases: patterns that can be exploited in the simplified setup, but that are not representative of the true task (Gururangan et al., 2018; Zellers et al., 2019a). Artifacts can enable machines to even outperform humans at the final benchmark, without solving the underlying task. While the problem of artifacts has recently taken the spotlight in the NLP community, partially because large Transformers (Vaswani et al., 2017) excel at picking up on artifacts, there is a deeper underlying issue. One way to view simplified tasks is that in order to correctly map inputs X to labels Y, a machine must learn a set of attributes A that are representative of the ‘true’ task. We can upperbound the information contained by A through the information bottleneck pri"
D08-1083,esuli-sebastiani-2006-sentiwordnet,0,0.022428,"Missing"
D08-1083,C04-1200,0,0.245029,"Missing"
D08-1083,P07-1055,0,0.543369,"nd a system that simply takes the majority vote of the polarity of individual words will not work well on the above examples. Indeed, much of the previous learning-based research on this topic tries to incorporate salient interactions by encoding them as features. One approach includes features based on contextual valence shifters1 (Polanyi and Zaenen, 2004), which are words that affect the polarity or intensity of sentiment over neighboring text spans (e.g., Kennedy and Inkpen (2005), Wilson et al. (2005), Shaikh et al. (2007)). Another approach encodes frequent subsentential patterns (e.g., McDonald et al. (2007)) as features; these might indirectly capture some of the subsentential interactions that affect polarity. How1 For instance, “never”, “nowhere”, “little”, “most”, “lack”, “scarcely”, “deeply”. 793 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 793–801, c Honolulu, October 2008. 2008 Association for Computational Linguistics ever, both types of approach are based on learning models with a flat bag-of-features: some structural information can be encoded as higher order features, but the final representation of the input is still a flat feature vect"
D08-1083,S07-1013,0,0.0451713,"Missing"
D08-1083,H05-1044,0,0.745804,"les demonstrate that words or constituents interact with each other to yield the expression-level polarity. And a system that simply takes the majority vote of the polarity of individual words will not work well on the above examples. Indeed, much of the previous learning-based research on this topic tries to incorporate salient interactions by encoding them as features. One approach includes features based on contextual valence shifters1 (Polanyi and Zaenen, 2004), which are words that affect the polarity or intensity of sentiment over neighboring text spans (e.g., Kennedy and Inkpen (2005), Wilson et al. (2005), Shaikh et al. (2007)). Another approach encodes frequent subsentential patterns (e.g., McDonald et al. (2007)) as features; these might indirectly capture some of the subsentential interactions that affect polarity. How1 For instance, “never”, “nowhere”, “little”, “most”, “lack”, “scarcely”, “deeply”. 793 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 793–801, c Honolulu, October 2008. 2008 Association for Computational Linguistics ever, both types of approach are based on learning models with a flat bag-of-features: some structural information"
D08-1083,R09-1048,0,\N,Missing
D09-1062,P08-1034,0,0.0170422,"Sebastiani (2006), Rao and Ravichandran (2009)) generally evaluate the lexicon in isolation from any potentially relevant NLP task, and it is unclear how the new lexicon might affect end-to-end performance of a concrete NLP application. It might even be unrealistic to expect that there can be a general-purpose lexical resource that can be effective across all relevant NLP applications, as general-purpose lexicons will not reflect domain-specific lexical usage. Indeed, Blitzer et al. (2007) note that the polarity of a particular word can carry opposite sentiment depending on the domain (e.g., Andreevskaia and Bergler (2008)). In this paper, we propose a novel method based on integer linear programming to adapt an existing polarity lexicon into a new one to reflect the characteristics of the data more directly. In particular, our method considers the relations among words and opinion expressions collectively to derive the most likely polarity of each word for the given domain. Figure 1 depicts the key insight of our approach using a bipartite graph. On the left hand side, each node represents a word, and on the right hand side, each node represents an opinion expression. There is an edge between a word wi and an"
D09-1062,E06-1027,0,0.0292699,"Kim and Hovy (2004), Kennedy and Inkpen (2005), Wilson et al. (2005)). Even though the polarity lexicon plays an important role (Section 3.1), it has received relatively less attention in previous research. In most cases, polarity lexicon construction is discussed only briefly as a preprocessing step for a sentiment analysis task (e.g., Hu and Liu (2004), Moilanen and Pulman (2007)), but the effect of different alternative polarity lexicons is not explicitly investigated. Conversely, research efforts that focus on constructing a general purpose polarity lexicon (e.g., Takamura et al. (2005), Andreevskaia and Bergler (2006), Esuli and Sebastiani (2006), Rao and Ravichandran (2009)) generally evaluate the lexicon in isolation from any potentially relevant NLP task, and it is unclear how the new lexicon might affect end-to-end performance of a concrete NLP application. It might even be unrealistic to expect that there can be a general-purpose lexical resource that can be effective across all relevant NLP applications, as general-purpose lexicons will not reflect domain-specific lexical usage. Indeed, Blitzer et al. (2007) note that the polarity of a particular word can carry opposite sentiment depending on the dom"
D09-1062,D08-1083,1,0.400592,"inion expression. There is an edge between a word wi and an opinion expression ej , if the word wi appears in the expression ej . We assume the possible polarity of each expression is one of the following three values: {positive, neutral, negative}, while the possible polarity of each word is one of: {positive, neutral, negative or negator}. Strictly speaking, negator is not a value for polarity, but we include them in our lexicon, because valence shifters or negators have been shown to play an important role for sentiment analysis (e.g., Polanyi and Zaenen (2004), Moilanen and Pulman (2007), Choi and Cardie (2008)). Typically, the ultimate goal of the sentiment analysis task is to determine the expression-level (or sentiment/ document-level) polarities, rather 590 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 590–598, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP than the correct word-level polarities with respect to the domain. Therefore, word-level polarities can be considered as latent information. In this paper, we show how we can improve the word-level polarities of a general-purpose polarity lexicon by utilizing the expression-level polarities, a"
D09-1062,N07-1030,0,0.0110225,"-to-expression (inter-expression) relations has connections to techniques that employ more of a global-view of corpus statistics (e.g., Kanayama and Nasukawa (2006)).1 While most previous research exploits only one or the other type of relation, we propose a unified method that can exploit both types of semantic relation, while adapting a general purpose polarity lexicon into a domain specific one. We formulate our lexicon adaptation task using integer linear programming (ILP), which has been shown to be very effective when solving problems with complex constraints (e.g., Roth and Yih (2004), Denis and Baldridge (2007)). And the word-to-word and word-to-expression relations discussed above can be encoded as soft and hard constraints in ILP. Unfortunately, one class of constraint that we would like to encode (see Section 2) will require an exponentially many number of constraints when grounded into an actual ILP problem. We therefore propose an approximation scheme to make the problem more practically solvable. We evaluate the effect of the adapted lex1 In case of document-level polarity classification, wordto-expression relations correspond to word-to-document relations. 591  − w + w  = w w − w   w  w"
D09-1062,D07-1115,0,0.130016,"on: expression-level polarity classification. The positive results from our experiments encourage further research for lexical resource adaptation techniques. Table 3: Effect of an adapted polarity lexicon on expression-level classification using the Vote & Flip Algorithm Accuracy Avg. Error Distance 70.4 71.2 0.334 0.327 Original Lexicon Adapted Lexicon Table 4: Effect of an adapted polarity lexicon on expression-level classification using CRFs 4 Conclusion Related Work Acknowledgments There are a number of previous work that focus on building polarity lexicons (e.g., Takamura et al. (2005), Kaji and Kitsuregawa (2007), Rao and Ravichandran (2009)). But most of them evaluated their lexicon in isolation from any potentially relevant NLP task, and it is unclear how the new lexicon might affect end-to-end performance of a concrete NLP application. Our work differs in that we try to draw a bridge between general purpose lexical resources and a domain-specific NLP application. Kim and Hovy (2005) and Banea et al. (2008) present bootstrapping methods to construct a subjectivity lexicon and measure the effect of the new lexicon for sentence-level subjectivity classification. However, their lexicons only tell wheth"
D09-1062,W06-1642,0,0.192408,"ression. The second type of relations are word-to-expression relations: e.g., some words appear in expressions that take on a variety of polarities, while other words are associated with expressions of one polarity class or another. In relation to previous research, analyzing word-to-word (intra-expression) relations is most related to techniques that determine expression-level polarity in context (e.g., Wilson et al. (2005)), while exploring word-to-expression (inter-expression) relations has connections to techniques that employ more of a global-view of corpus statistics (e.g., Kanayama and Nasukawa (2006)).1 While most previous research exploits only one or the other type of relation, we propose a unified method that can exploit both types of semantic relation, while adapting a general purpose polarity lexicon into a domain specific one. We formulate our lexicon adaptation task using integer linear programming (ILP), which has been shown to be very effective when solving problems with complex constraints (e.g., Roth and Yih (2004), Denis and Baldridge (2007)). And the word-to-word and word-to-expression relations discussed above can be encoded as soft and hard constraints in ILP. Unfortunately"
D09-1062,C04-1200,0,0.223894,"t the characteristics of the data more directly. In particular, our method collectively considers the relations among words and opinion expressions to derive the most likely polarity of each lexical item (positive, neutral, negative, or negator) for the given domain. Experimental results show that our lexicon adaptation technique improves the performance of fine-grained polarity classification. 1 Introduction Polarity lexicons have been a valuable resource for sentiment analysis and opinion mining. In particular, they have been an essential ingredient for finegrained sentiment analysis (e.g., Kim and Hovy (2004), Kennedy and Inkpen (2005), Wilson et al. (2005)). Even though the polarity lexicon plays an important role (Section 3.1), it has received relatively less attention in previous research. In most cases, polarity lexicon construction is discussed only briefly as a preprocessing step for a sentiment analysis task (e.g., Hu and Liu (2004), Moilanen and Pulman (2007)), but the effect of different alternative polarity lexicons is not explicitly investigated. Conversely, research efforts that focus on constructing a general purpose polarity lexicon (e.g., Takamura et al. (2005), Andreevskaia and Ber"
D09-1062,I05-2011,0,0.18211,"contains 535 newswire documents annotated with phrase-level subjectivity information. We evaluate on all opinion expressions that are known to have high level of inter-annotator agreement. That is, we include opinions with intensity marked as ‘medium’ or higher, and exclude those with annotation confidence marked as ‘uncertain’. To focus our study on the direct influence of the polarity lexicon upon the sentiment classification task, we assume the boundaries of the expressions are given. However, our approach can be readily used in tandem with a system that extracts opinion expressions (e.g., Kim and Hovy (2005), Breck et al. (2007)). Performance is reported using 10-fold cross-validation on 400 documents, and a separate 135 documents were used as a development set. For the general-purpose polarity lexicon, we expand the polarity lexicon of Wilson et al. (2005) with General Inquirer dictionary as suggested by Choi and Cardie (2008). We report the performance in two measures: accuracy for 3-way classification, and average error distance. The reason why we consider average error distance is because classifying a positive class into a negative class is worse than classifying a positive class into a neut"
D09-1062,W02-1011,0,0.0142076,"ity classification task? In particular, is it useful when using a machine learning technique that might be able to learn the necessary polarity information just based on the words in the training data, without consulting a dictionary? (Section 3.1) Q2 What is the effect of an adapted polarity lexicon on the expression-level polarity classification task? (Section 3.2) Notice that we include the neutral polarity in the polarity classification. It makes our task much harder (e.g., Wilson et al. (2009)) than those that assume inputs are guaranteed to be either strongly positive or negative (e.g., Pang et al. (2002), Choi and Cardie (2008)). But in practice, one cannot expect that a given input is strongly polar, as automatically extracted opinions are bound to be noisy. Furthermore, Wiebe et al. (2005) discuss that some opinion expressions do carry a neutral polarity. We experiment with the Multi-Perspective Question Answering (MPQA) corpus (Wiebe et al., 2005) for evaluation. It contains 535 newswire documents annotated with phrase-level subjectivity information. We evaluate on all opinion expressions that are known to have high level of inter-annotator agreement. That is, we include opinions with inte"
D09-1062,E09-1077,0,0.240772,"(2005)). Even though the polarity lexicon plays an important role (Section 3.1), it has received relatively less attention in previous research. In most cases, polarity lexicon construction is discussed only briefly as a preprocessing step for a sentiment analysis task (e.g., Hu and Liu (2004), Moilanen and Pulman (2007)), but the effect of different alternative polarity lexicons is not explicitly investigated. Conversely, research efforts that focus on constructing a general purpose polarity lexicon (e.g., Takamura et al. (2005), Andreevskaia and Bergler (2006), Esuli and Sebastiani (2006), Rao and Ravichandran (2009)) generally evaluate the lexicon in isolation from any potentially relevant NLP task, and it is unclear how the new lexicon might affect end-to-end performance of a concrete NLP application. It might even be unrealistic to expect that there can be a general-purpose lexical resource that can be effective across all relevant NLP applications, as general-purpose lexicons will not reflect domain-specific lexical usage. Indeed, Blitzer et al. (2007) note that the polarity of a particular word can carry opposite sentiment depending on the domain (e.g., Andreevskaia and Bergler (2008)). In this paper"
D09-1062,W04-2401,0,0.0184481,"while exploring word-to-expression (inter-expression) relations has connections to techniques that employ more of a global-view of corpus statistics (e.g., Kanayama and Nasukawa (2006)).1 While most previous research exploits only one or the other type of relation, we propose a unified method that can exploit both types of semantic relation, while adapting a general purpose polarity lexicon into a domain specific one. We formulate our lexicon adaptation task using integer linear programming (ILP), which has been shown to be very effective when solving problems with complex constraints (e.g., Roth and Yih (2004), Denis and Baldridge (2007)). And the word-to-word and word-to-expression relations discussed above can be encoded as soft and hard constraints in ILP. Unfortunately, one class of constraint that we would like to encode (see Section 2) will require an exponentially many number of constraints when grounded into an actual ILP problem. We therefore propose an approximation scheme to make the problem more practically solvable. We evaluate the effect of the adapted lex1 In case of document-level polarity classification, wordto-expression relations correspond to word-to-document relations. 591  −"
D09-1062,P05-1017,0,0.528231,"entiment analysis (e.g., Kim and Hovy (2004), Kennedy and Inkpen (2005), Wilson et al. (2005)). Even though the polarity lexicon plays an important role (Section 3.1), it has received relatively less attention in previous research. In most cases, polarity lexicon construction is discussed only briefly as a preprocessing step for a sentiment analysis task (e.g., Hu and Liu (2004), Moilanen and Pulman (2007)), but the effect of different alternative polarity lexicons is not explicitly investigated. Conversely, research efforts that focus on constructing a general purpose polarity lexicon (e.g., Takamura et al. (2005), Andreevskaia and Bergler (2006), Esuli and Sebastiani (2006), Rao and Ravichandran (2009)) generally evaluate the lexicon in isolation from any potentially relevant NLP task, and it is unclear how the new lexicon might affect end-to-end performance of a concrete NLP application. It might even be unrealistic to expect that there can be a general-purpose lexical resource that can be effective across all relevant NLP applications, as general-purpose lexicons will not reflect domain-specific lexical usage. Indeed, Blitzer et al. (2007) note that the polarity of a particular word can carry opposi"
D09-1062,H05-1044,0,0.830442,"In particular, our method collectively considers the relations among words and opinion expressions to derive the most likely polarity of each lexical item (positive, neutral, negative, or negator) for the given domain. Experimental results show that our lexicon adaptation technique improves the performance of fine-grained polarity classification. 1 Introduction Polarity lexicons have been a valuable resource for sentiment analysis and opinion mining. In particular, they have been an essential ingredient for finegrained sentiment analysis (e.g., Kim and Hovy (2004), Kennedy and Inkpen (2005), Wilson et al. (2005)). Even though the polarity lexicon plays an important role (Section 3.1), it has received relatively less attention in previous research. In most cases, polarity lexicon construction is discussed only briefly as a preprocessing step for a sentiment analysis task (e.g., Hu and Liu (2004), Moilanen and Pulman (2007)), but the effect of different alternative polarity lexicons is not explicitly investigated. Conversely, research efforts that focus on constructing a general purpose polarity lexicon (e.g., Takamura et al. (2005), Andreevskaia and Bergler (2006), Esuli and Sebastiani (2006), Rao and"
D09-1062,J09-3003,0,0.0127546,"ek for answers for the following questions: Q1 What is the effect of a polarity lexicon on the expression-level polarity classification task? In particular, is it useful when using a machine learning technique that might be able to learn the necessary polarity information just based on the words in the training data, without consulting a dictionary? (Section 3.1) Q2 What is the effect of an adapted polarity lexicon on the expression-level polarity classification task? (Section 3.2) Notice that we include the neutral polarity in the polarity classification. It makes our task much harder (e.g., Wilson et al. (2009)) than those that assume inputs are guaranteed to be either strongly positive or negative (e.g., Pang et al. (2002), Choi and Cardie (2008)). But in practice, one cannot expect that a given input is strongly polar, as automatically extracted opinions are bound to be noisy. Furthermore, Wiebe et al. (2005) discuss that some opinion expressions do carry a neutral polarity. We experiment with the Multi-Perspective Question Answering (MPQA) corpus (Wiebe et al., 2005) for evaluation. It contains 535 newswire documents annotated with phrase-level subjectivity information. We evaluate on all opinion"
D09-1062,banea-etal-2008-bootstrapping,0,\N,Missing
D09-1062,J96-1002,0,\N,Missing
D09-1062,P07-1056,0,\N,Missing
D09-1062,R09-1048,0,\N,Missing
D09-1062,esuli-sebastiani-2006-sentiwordnet,0,\N,Missing
D11-1101,E06-1027,0,0.31513,"Missing"
D11-1101,baccianella-etal-2010-sentiwordnet,0,0.099032,"ohammad et al. (2009)), while relatively less research investigated the use of web documents (e.g., Kaji and Kitsuregawa (2007), Velikovich et al. (2010))). Wilson et al. (2005b) first introduced the sentiment lexicon, spawning a great deal of research thereafter. At the beginning, sentiment lexicons were designed to include only those words that express sentiment, that is, subjective words. However in recent years, sentiment lexicons started expanding to include some of those words that simply associate with sentiment, even if those words are purely objective (e.g., Velikovich et al. (2010), Baccianella et al. (2010)). This trend applies even to the most recent version of the lexicon of Wilson et al. (2005b). We conjecture that this trend of broader coverage suggests that such lexicons are practically more useful than sentiment lexicons that include only those words that are strictly subjective. In this work, we 1101 make this transition more explicit and intentional, by introducing a novel connotation lexicon. Mohammad and Turney (2010) focussed on emotion evoked by common words and phrases. The spirit of their work shares some similarity with ours in that it aims to find the emotion evoked by words, as"
D11-1101,J90-1003,0,0.180147,"icates and arguments. With this goal in mind, we next explore the directionality of the edges and different strategies to assign weights to them. 3.1 Undirected (Symmetric) Graph First we explore undirected edges. In this case, we assign weight for each undirected edge between a predicate p and an argument a. Intuitively, the weight should correspond to the strength of relatedness or association between the predicate p and the argument a. We use Pointwise Mutual Information (PMI), as it has been used by many previous research to quantify the association between two words (e.g., Turney (2001), Church and Hanks (1990)). The PMI score between p and a is defined as follows: w(p − a) := P M I(p, a) = log P (p, a) P (p)P (a) The log of the ratio is positive when the pair of words tends to co-occur and negative when the presence of one word correlates with the absence of the other word. 3.2 Directed (Asymmetric) Graph Next we explore directed edges. That is, for each connected pair of a predicate p and an argument a, there are two edges in opposite directions: e(p → a) and e(a → p). In this case, we explore the use of asymmetric weights using conditional probability. In particular, we define weights as follows:"
D11-1101,N07-1060,0,0.0465665,"Missing"
D11-1101,esuli-sebastiani-2006-sentiwordnet,0,0.713445,"Missing"
D11-1101,P07-1054,0,0.159397,"Missing"
D11-1101,D07-1115,0,0.241147,"enotes lenient evaluation. present systematic comparison of various options for graph representation and encoding of prior knowledge. We are not aware of any previous research that made use of HITS algorithm for connotation or sentiment lexicon induction. Much of previous research investigated the use of dictionary network (e.g., WordNet) for lexicon induction (e.g., Kamps et al. (2004), Takamura et al. (2005), Adreevskaia and Bergler (2006), Esuli and Sebastiani (2006), Su and Markert (2009), Mohammad et al. (2009)), while relatively less research investigated the use of web documents (e.g., Kaji and Kitsuregawa (2007), Velikovich et al. (2010))). Wilson et al. (2005b) first introduced the sentiment lexicon, spawning a great deal of research thereafter. At the beginning, sentiment lexicons were designed to include only those words that express sentiment, that is, subjective words. However in recent years, sentiment lexicons started expanding to include some of those words that simply associate with sentiment, even if those words are purely objective (e.g., Velikovich et al. (2010), Baccianella et al. (2010)). This trend applies even to the most recent version of the lexicon of Wilson et al. (2005b). We conj"
D11-1101,kamps-etal-2004-using,0,0.439991,"Missing"
D11-1101,D09-1063,0,0.0742834,"Missing"
D11-1101,E09-1077,0,0.0898881,"Missing"
D11-1101,S07-1013,0,0.150076,"Missing"
D11-1101,N09-1001,0,0.553298,"Missing"
D11-1101,P05-1017,0,0.109144,"Missing"
D11-1101,W00-1308,0,0.025596,"Missing"
D11-1101,N10-1119,0,0.0603088,"esent systematic comparison of various options for graph representation and encoding of prior knowledge. We are not aware of any previous research that made use of HITS algorithm for connotation or sentiment lexicon induction. Much of previous research investigated the use of dictionary network (e.g., WordNet) for lexicon induction (e.g., Kamps et al. (2004), Takamura et al. (2005), Adreevskaia and Bergler (2006), Esuli and Sebastiani (2006), Su and Markert (2009), Mohammad et al. (2009)), while relatively less research investigated the use of web documents (e.g., Kaji and Kitsuregawa (2007), Velikovich et al. (2010))). Wilson et al. (2005b) first introduced the sentiment lexicon, spawning a great deal of research thereafter. At the beginning, sentiment lexicons were designed to include only those words that express sentiment, that is, subjective words. However in recent years, sentiment lexicons started expanding to include some of those words that simply associate with sentiment, even if those words are purely objective (e.g., Velikovich et al. (2010), Baccianella et al. (2010)). This trend applies even to the most recent version of the lexicon of Wilson et al. (2005b). We conjecture that this trend of"
D11-1101,H05-2018,1,0.931445,"tation lexicon together with connotative predicates. Our empirical study demonstrates that the resulting connotation lexicon is of great value for sentiment analysis complementing existing sentiment lexicons. 1 Introduction In this paper, we introduce a connotation lexicon, a new type of lexicon that lists words with connotative polarity, i.e., words with positive connotation (e.g., award, promotion) and words with negative connotation (e.g., cancer, war). Connotation lexicons differ from sentiment lexicons that are studied in much of previous research (e.g., Esuli and Sebas1092 tiani (2006), Wilson et al. (2005a)): the latter concerns words that express sentiment either explicitly or implicitly, while the former concerns words that evoke or even simply associate with a specific polarity of sentiment. To our knowledge, there has been no previous research that investigates polarized connotation lexicons. Understanding the connotation of words would seem to require common sense and world knowledge at first glance, which in turn might seem to require human encoding of knowledge base. However, we demonstrate that much of the connotative polarity of words can be inferred from natural language text in a ne"
D11-1101,H05-1044,0,0.740294,"tation lexicon together with connotative predicates. Our empirical study demonstrates that the resulting connotation lexicon is of great value for sentiment analysis complementing existing sentiment lexicons. 1 Introduction In this paper, we introduce a connotation lexicon, a new type of lexicon that lists words with connotative polarity, i.e., words with positive connotation (e.g., award, promotion) and words with negative connotation (e.g., cancer, war). Connotation lexicons differ from sentiment lexicons that are studied in much of previous research (e.g., Esuli and Sebas1092 tiani (2006), Wilson et al. (2005a)): the latter concerns words that express sentiment either explicitly or implicitly, while the former concerns words that evoke or even simply associate with a specific polarity of sentiment. To our knowledge, there has been no previous research that investigates polarized connotation lexicons. Understanding the connotation of words would seem to require common sense and world knowledge at first glance, which in turn might seem to require human encoding of knowledge base. However, we demonstrate that much of the connotative polarity of words can be inferred from natural language text in a ne"
D11-1101,W10-0204,0,\N,Missing
D12-1139,bird-etal-2008-acl,0,0.0144407,"return COMPLEX return OTHER Input: Parse tree t(Nr ) of sentence s Output: Type of s. k←1 while k ≤ λ do if Ltop 6= VP then k top if S ∈ Ω(Ltop k ) or SBAR ∈ Ω(Lk ) then return PERIODIC else top if S ∈ Ω(Ltop k ) or SBAR ∈ Ω(Lk ) then return LOOSE return OTHER 3 We present analytic insights with respect to the authorship attribution task in two distinct domains. 2 Data For the empirical analysis of authorship attribution, we use two different datasets described below. Sections 3, 4 & 5 provide the details of our stylometric analysis. Scientific Paper We use the ACL Anthology Reference Corpus (Bird et al., 2008). Since it is nearly impossible to determine the goldstandard authorship of a paper written by multiple authors, we select 10 authors who have published at least 8 single-authored papers. We include 8 documents per author, and remove citations, tables, formulas from the text using simple heuristics.5 Novels We collect 5 novels from 5 English authors: Charles Dickens, Edward Bulwer-Lytton, Jane Austen, Thomas Hardy and Walter Scott. We select the first 3000 sentences from each novel and group every 50 consecutive sentences into 60 documents per novel per author. 5 Some might question whether th"
D12-1139,C10-2014,0,0.0251102,"research, however, was to attain better classification accuracy, rather than providing linguistic interpretations of individual authorship and their stylistic elements. Our work is the first to attempt authorship attribution of scientific papers, a contemporary domain where language is very formal, and the stylistic variations have limited scope. In addition to exploring this new domain, we also present a comparative study expounding the role of syntactic features for authorship attribution in classical literature. Furthermore, our work is also the first to utilize tree topological features (Chan et al., 2010) in the context of stylometric analysis. 8 Conclusion In this paper, we have presented a comprehensive exploration of syntactic elements in writing styles, with particular emphasis on interpretable characterization of stylistic elements, thus distinguishing our work from other recent work on syntactic stylometric analysis. Our analytical study provides novel statistically supported insights into stylistic elements that have not been computationally analyzed in previous literature. In the future, we plan to investigate the use of syntactic feature generators for text categorization (e.g., Colli"
D12-1139,P02-1034,0,0.0841512,"Missing"
D12-1139,H92-1019,0,0.500969,"1 where H = 1 |C| P|C| F i=1 hi . Similarly, v u |C| u1 X S σk = t (s(Ni ) − S)2 n i=1 • Leaf height (hT = {hTi , Ni ∈ T }), where hTi = ξ(Ni , NR ) Ni ∈ T . For instance, the leaf height of “free” of Tree (2) in Fig. 1 is 6. • Furcation height (hF = {hF i , Ni ∈ F}), F where hi is the maximum leaf height within the subtree rooted at Ni . In Figure 1, for example, the furcation height of the VP in Tree (2) (marked in triangle) is 3. • Level width (wL = {wl , 1 ≤ l ≤ n}), where wl = |{Ni : ξ(Ni , NR ) = l}|. E.g., w4 of Tree (1) in Figure 1 is 6. 10 Example sentences are taken from Lin (1997), Joshi (1992), and Lin (1995). 1527 1 P|C| where S = |C| i=1 s(Ni ) and s(Ni ) is the number of leaf nodes of tree rooted at Ni . As shown in Figure 1, the imbalance of the internal node VP in Tree (2) (marked in triangle) is 0.5 horizontally, and 0.5 vertically. To give an intuition on the relation between these measurements and different tree structures, Table 9 provides the measurements of the three trees shown in Figure 1. Note that all three sentences are of similar length but show different tree structures. Tree (1) and Tree (2) differ in that Tree (1) is highly unbalanced and grows deep, while Tree"
D12-1139,M95-1010,0,0.019831,"P|C| F i=1 hi . Similarly, v u |C| u1 X S σk = t (s(Ni ) − S)2 n i=1 • Leaf height (hT = {hTi , Ni ∈ T }), where hTi = ξ(Ni , NR ) Ni ∈ T . For instance, the leaf height of “free” of Tree (2) in Fig. 1 is 6. • Furcation height (hF = {hF i , Ni ∈ F}), F where hi is the maximum leaf height within the subtree rooted at Ni . In Figure 1, for example, the furcation height of the VP in Tree (2) (marked in triangle) is 3. • Level width (wL = {wl , 1 ≤ l ≤ n}), where wl = |{Ni : ξ(Ni , NR ) = l}|. E.g., w4 of Tree (1) in Figure 1 is 6. 10 Example sentences are taken from Lin (1997), Joshi (1992), and Lin (1995). 1527 1 P|C| where S = |C| i=1 s(Ni ) and s(Ni ) is the number of leaf nodes of tree rooted at Ni . As shown in Figure 1, the imbalance of the internal node VP in Tree (2) (marked in triangle) is 0.5 horizontally, and 0.5 vertically. To give an intuition on the relation between these measurements and different tree structures, Table 9 provides the measurements of the three trees shown in Figure 1. Note that all three sentences are of similar length but show different tree structures. Tree (1) and Tree (2) differ in that Tree (1) is highly unbalanced and grows deep, while Tree Figure 1: Parsed"
D12-1139,P97-1009,0,0.0219741,"i − H) n i=1 where H = 1 |C| P|C| F i=1 hi . Similarly, v u |C| u1 X S σk = t (s(Ni ) − S)2 n i=1 • Leaf height (hT = {hTi , Ni ∈ T }), where hTi = ξ(Ni , NR ) Ni ∈ T . For instance, the leaf height of “free” of Tree (2) in Fig. 1 is 6. • Furcation height (hF = {hF i , Ni ∈ F}), F where hi is the maximum leaf height within the subtree rooted at Ni . In Figure 1, for example, the furcation height of the VP in Tree (2) (marked in triangle) is 3. • Level width (wL = {wl , 1 ≤ l ≤ n}), where wl = |{Ni : ξ(Ni , NR ) = l}|. E.g., w4 of Tree (1) in Figure 1 is 6. 10 Example sentences are taken from Lin (1997), Joshi (1992), and Lin (1995). 1527 1 P|C| where S = |C| i=1 s(Ni ) and s(Ni ) is the number of leaf nodes of tree rooted at Ni . As shown in Figure 1, the imbalance of the internal node VP in Tree (2) (marked in triangle) is 0.5 horizontally, and 0.5 vertically. To give an intuition on the relation between these measurements and different tree structures, Table 9 provides the measurements of the three trees shown in Figure 1. Note that all three sentences are of similar length but show different tree structures. Tree (1) and Tree (2) differ in that Tree (1) is highly unbalanced and grows dee"
D12-1139,C08-1065,0,0.185102,"nt analytic insights with respect to the authorship attribution task in two different domains. 1 Introduction Much of the writing styles recognized in rhetorical and composition theories involve deep syntactic elements in style (e.g., Bain (1887), Kemper (1987) Strunk and White (2008)). However, previous research for automatic authorship attribution and computational stylometric analysis have relied mostly on shallow lexico-syntactic patterns (e.g., Mendenhall (1887), Mosteller and Wallace (1984), Stamatatos et al. (2001), Baayen et al. (2002), Koppel and Schler (2003), Zhao and Zobel (2007), Luyckx and Daelemans (2008)). Some very recent works have shown that PCFG models can detect distributional difference in sentence structure in gender attribution (Sarawgi et al., 2011), authorship attribution (Raghavan et al., 2010), and native language identification (Wong and Dras, 2011). However, still very little has been understood exactly what constitutes salient stylistic elements in sentence structures that characterize each author. Although the work of Wong and Dras (2011) has extracted production rules with highest information gain, their analysis stops short of providing insight any deeper than what simple n-"
D12-1139,N07-1051,0,0.0311908,"es in PCFG trees. To address this question, Table 1 shows the top ten most discriminative production rules for authorship attribution for scientific articles,3 ranked by LIBLINEAR (Fan et al., 2008).4 Note that terminal production rules are excluded so as to focus directly on syntax. It does provide some insights, but not to a satisfactory degree. For instance, Hobbs seems to favor inverted declarative sentences (SINV) and adverbs with prepositions (RB PP). While the latter can be easily obtained by simple part-of3 See Section 2 for the description of the dataset. We use Berkeley PCFG parser (Petrov and Klein, 2007) for all experiments. 4 1523 speech analysis, the former requires using parse trees. We can also observe that none of the top 10 most discriminative production rules for Hobbs includes SBAR tag, which represents subordinate clauses. But examining discriminative rules alone is limited in providing more comprehensive characterization of idiolects. Can we unveil something more in deep syntactic structure that can characterize the collective syntactic difference between any two authors? For instance, what can we say about distributional difference between loose and periodic sentences discussed ear"
D12-1139,D09-1012,0,0.0295056,"Missing"
D12-1139,D08-1020,0,0.0141363,"word-level statistics together with POS-sequences (Luyckx and Daelemans, 2008), syntactic labels from partial parsing (Hirst and Feiguina, 2007), etc. The use of syntactic features from parse trees in authorship attribution was initiated by Baayen et al. (1996), and more recently, Raghavan et al. (2010) have directly employed PCFG language models in this area. Syntactic features from PCFG parse trees have also been used for gender attribution (Sarawgi et al., 2011), genre identification (Stamatatos et al., 2000), native language identification (Wong and Dras, 2011) and readability assessment (Pitler and Nenkova, 2008). The primary focus of most previous research, however, was to attain better classification accuracy, rather than providing linguistic interpretations of individual authorship and their stylistic elements. Our work is the first to attempt authorship attribution of scientific papers, a contemporary domain where language is very formal, and the stylistic variations have limited scope. In addition to exploring this new domain, we also present a comparative study expounding the role of syntactic features for authorship attribution in classical literature. Furthermore, our work is also the first to"
D12-1139,P10-2008,0,0.149496,"elements in style (e.g., Bain (1887), Kemper (1987) Strunk and White (2008)). However, previous research for automatic authorship attribution and computational stylometric analysis have relied mostly on shallow lexico-syntactic patterns (e.g., Mendenhall (1887), Mosteller and Wallace (1984), Stamatatos et al. (2001), Baayen et al. (2002), Koppel and Schler (2003), Zhao and Zobel (2007), Luyckx and Daelemans (2008)). Some very recent works have shown that PCFG models can detect distributional difference in sentence structure in gender attribution (Sarawgi et al., 2011), authorship attribution (Raghavan et al., 2010), and native language identification (Wong and Dras, 2011). However, still very little has been understood exactly what constitutes salient stylistic elements in sentence structures that characterize each author. Although the work of Wong and Dras (2011) has extracted production rules with highest information gain, their analysis stops short of providing insight any deeper than what simple n-gramlevel analysis could also provide.1 One might even wonder whether PCFG models are hinging mostly on leaf production rules, and whether there are indeed deep syntactic differences at all. This paper att"
D12-1139,W11-0310,1,0.88949,"nd composition theories involve deep syntactic elements in style (e.g., Bain (1887), Kemper (1987) Strunk and White (2008)). However, previous research for automatic authorship attribution and computational stylometric analysis have relied mostly on shallow lexico-syntactic patterns (e.g., Mendenhall (1887), Mosteller and Wallace (1984), Stamatatos et al. (2001), Baayen et al. (2002), Koppel and Schler (2003), Zhao and Zobel (2007), Luyckx and Daelemans (2008)). Some very recent works have shown that PCFG models can detect distributional difference in sentence structure in gender attribution (Sarawgi et al., 2011), authorship attribution (Raghavan et al., 2010), and native language identification (Wong and Dras, 2011). However, still very little has been understood exactly what constitutes salient stylistic elements in sentence structures that characterize each author. Although the work of Wong and Dras (2011) has extracted production rules with highest information gain, their analysis stops short of providing insight any deeper than what simple n-gramlevel analysis could also provide.1 One might even wonder whether PCFG models are hinging mostly on leaf production rules, and whether there are indeed d"
D12-1139,J00-4001,0,0.0199464,"ophisticated linguistic cues have been explored as well: parts-of-speech n-grams (Diederich et al., 2003), word-level statistics together with POS-sequences (Luyckx and Daelemans, 2008), syntactic labels from partial parsing (Hirst and Feiguina, 2007), etc. The use of syntactic features from parse trees in authorship attribution was initiated by Baayen et al. (1996), and more recently, Raghavan et al. (2010) have directly employed PCFG language models in this area. Syntactic features from PCFG parse trees have also been used for gender attribution (Sarawgi et al., 2011), genre identification (Stamatatos et al., 2000), native language identification (Wong and Dras, 2011) and readability assessment (Pitler and Nenkova, 2008). The primary focus of most previous research, however, was to attain better classification accuracy, rather than providing linguistic interpretations of individual authorship and their stylistic elements. Our work is the first to attempt authorship attribution of scientific papers, a contemporary domain where language is very formal, and the stylistic variations have limited scope. In addition to exploring this new domain, we also present a comparative study expounding the role of synta"
D12-1139,D11-1148,0,0.425149,"nd White (2008)). However, previous research for automatic authorship attribution and computational stylometric analysis have relied mostly on shallow lexico-syntactic patterns (e.g., Mendenhall (1887), Mosteller and Wallace (1984), Stamatatos et al. (2001), Baayen et al. (2002), Koppel and Schler (2003), Zhao and Zobel (2007), Luyckx and Daelemans (2008)). Some very recent works have shown that PCFG models can detect distributional difference in sentence structure in gender attribution (Sarawgi et al., 2011), authorship attribution (Raghavan et al., 2010), and native language identification (Wong and Dras, 2011). However, still very little has been understood exactly what constitutes salient stylistic elements in sentence structures that characterize each author. Although the work of Wong and Dras (2011) has extracted production rules with highest information gain, their analysis stops short of providing insight any deeper than what simple n-gramlevel analysis could also provide.1 One might even wonder whether PCFG models are hinging mostly on leaf production rules, and whether there are indeed deep syntactic differences at all. This paper attempts to answer these questions. As an example of syntacti"
D13-1124,D10-1115,0,0.0204783,"composition that explores an unusual, unconventional set of words is more likely to be creative. Note that the key novelty then lies in the compositional operation itself, i.e., the act of putting together a set of words in an unexpected way, rather than the rareness of individual words being used. In recent years there has been a swell of work on compositional distributional semantics that captures the compositional aspects of language understanding, such as sentiment analysis (e.g., Yessenalina and Cardie (2011), Socher et al. (2011)) and language modeling (e.g., Mitchell and Lapata (2009), Baroni and Zamparelli (2010), Guevara (2011), Clarke (2012), Rudolph and Giesbrecht (2010)). However, none has examined the compositional nature in quantifying creativity in lexical composition. We consider two computational approaches to capture the notion of creative composition. The first is via various information theoretic measures, e.g., relative entropy reduction, to measure the surprisal of seeing the next word given the previous word. The second is via supervised learning, where we explore different modeling techniques to capture the statistical regularities in creative compositional operations. In particular, w"
D13-1124,cartoni-2008-lexical,0,0.0172442,"-------- --------- • -------- -------- • -------- -------• -------- -------• -------- --------- Figure 5: Creative (blue bold) and not creative (red italic) word pairs graph. 7 Related Work Among computational approaches that touch on linguistic creativity, many focused on metaphor (e.g., Dunn (2013), Krishnakumaran and Zhu (2007), Mashal et al. (2007), Rumbell et al. (2008), Rentoumi et al. (2012), Mashal et al. (2009)). Other linguistic devices and phenomena related to creativity include irony (e.g., Davidov et al. (2010), Gonz´alezIb´an˜ ez et al. (2011), Filatova (2012)), neologism (e.g., Cartoni (2008)), humor (e.g., Mihalcea and Strapparava (2005), Purandare and Litman (2006)), and similes (e.g., Hao and Veale (2010)). Veale (2011) proposed the new task of creative text retrieval to harvest expressions that potentially convey the same meaning as the query phrase in a fresh or unusual way. Our work contributes to the retrieval process of recognizing more creative phrases. Ozbal and Strapparava (2012) explored automatic creative naming of commercial products and services, focusing on the generation of creative phrases within a specific domain. Costello (2002) investigated the cognitive proce"
D13-1124,J12-1002,0,0.0635158,"tional set of words is more likely to be creative. Note that the key novelty then lies in the compositional operation itself, i.e., the act of putting together a set of words in an unexpected way, rather than the rareness of individual words being used. In recent years there has been a swell of work on compositional distributional semantics that captures the compositional aspects of language understanding, such as sentiment analysis (e.g., Yessenalina and Cardie (2011), Socher et al. (2011)) and language modeling (e.g., Mitchell and Lapata (2009), Baroni and Zamparelli (2010), Guevara (2011), Clarke (2012), Rudolph and Giesbrecht (2010)). However, none has examined the compositional nature in quantifying creativity in lexical composition. We consider two computational approaches to capture the notion of creative composition. The first is via various information theoretic measures, e.g., relative entropy reduction, to measure the surprisal of seeing the next word given the previous word. The second is via supervised learning, where we explore different modeling techniques to capture the statistical regularities in creative compositional operations. In particular, we will explore (1) compositiona"
D13-1124,P12-1094,0,0.0854528,"as the query phrase in a fresh or unusual way. Our work contributes to the retrieval process of recognizing more creative phrases. Ozbal and Strapparava (2012) explored automatic creative naming of commercial products and services, focusing on the generation of creative phrases within a specific domain. Costello (2002) investigated the cognitive process that guides people’s choice of words when making up a novel nounnoun compound. In contrast, we present a datadriven investigation to quantifying creativity in lexical composition. Memorability is loosely related to 1255 linguistic creativity (Danescu-Niculescu-Mizil et al. (2012)) as some of the creative quotes may be more memorable, but not all creative phrases are memorable and vice versa. 8 Conclusion We presented the first study that focuses on learning and quantifying creativity in lexical compositions, exploring statistical techniques motivated by three different theories and hypotheses of creativity, ranging from divergent thinking, compositional structure, creative semantic subspace, and the connection to sentiment and connotation. Our experimental results suggest the viability of learning creative language, and point to promising directions for future researc"
D13-1124,W10-2914,0,0.0605233,"• human architecture • human spark • omnipotent realm • finite realm • -------- --------- • -------- --------- • -------- -------- • -------- -------• -------- -------• -------- --------- Figure 5: Creative (blue bold) and not creative (red italic) word pairs graph. 7 Related Work Among computational approaches that touch on linguistic creativity, many focused on metaphor (e.g., Dunn (2013), Krishnakumaran and Zhu (2007), Mashal et al. (2007), Rumbell et al. (2008), Rentoumi et al. (2012), Mashal et al. (2009)). Other linguistic devices and phenomena related to creativity include irony (e.g., Davidov et al. (2010), Gonz´alezIb´an˜ ez et al. (2011), Filatova (2012)), neologism (e.g., Cartoni (2008)), humor (e.g., Mihalcea and Strapparava (2005), Purandare and Litman (2006)), and similes (e.g., Hao and Veale (2010)). Veale (2011) proposed the new task of creative text retrieval to harvest expressions that potentially convey the same meaning as the query phrase in a fresh or unusual way. Our work contributes to the retrieval process of recognizing more creative phrases. Ozbal and Strapparava (2012) explored automatic creative naming of commercial products and services, focusing on the generation of creati"
D13-1124,W13-0901,0,0.0136901,"ormal adulthood • -------- -------• -------- --------• -------- -------• -------- -------• -------- --------• -------- -------- • legal trading • -------- --------- • infinite promise • perfect disorder • perfect fire • human architecture • human spark • omnipotent realm • finite realm • -------- --------- • -------- --------- • -------- -------- • -------- -------• -------- -------• -------- --------- Figure 5: Creative (blue bold) and not creative (red italic) word pairs graph. 7 Related Work Among computational approaches that touch on linguistic creativity, many focused on metaphor (e.g., Dunn (2013), Krishnakumaran and Zhu (2007), Mashal et al. (2007), Rumbell et al. (2008), Rentoumi et al. (2012), Mashal et al. (2009)). Other linguistic devices and phenomena related to creativity include irony (e.g., Davidov et al. (2010), Gonz´alezIb´an˜ ez et al. (2011), Filatova (2012)), neologism (e.g., Cartoni (2008)), humor (e.g., Mihalcea and Strapparava (2005), Purandare and Litman (2006)), and similes (e.g., Hao and Veale (2010)). Veale (2011) proposed the new task of creative text retrieval to harvest expressions that potentially convey the same meaning as the query phrase in a fresh or unusua"
D13-1124,P13-1174,1,0.827197,"that are evident in graphs shown in Figure 3. Second, these measures only capture the surprisal aspect of creativity, missing the other important qualities: interestingness or imaginativeness. 4.2 Sentiment and Connotation Next we investigate the connection between creativity and sentiment, as illustrated in §2.3. We consider both sentiment (more explicit) and connotation (more implicit) words,13 and consider them with or without distinguishing the polarity (i.e., positive, negative). To determine sentiment and connotation, we use lexicons provided by OpinionFinder (Wilson et al. (2005)) and Feng et al. (2013) respectively. We denote polarity of a word wi as L(wi ).14 When wi has a negative polarity L(wi ) is assigned a value of -1, and when wi is positive L(wi ) is equal to 1. We assume that a word is neutral when it is not in the lexicon, assigning 0 to L(wi ). For a word pair w1 w2 we compute absolute difference Ldif f (w1 , w2 ) between polarities of tokens in a word pair in order to catch examples such as “inglorious success”. 13 E.g., expressions such as “blue sky” or “white sand” are not sentiment-laden, but do have positive connotation. 14 We denote polarity from OpinionFinder as Lsubj and"
D13-1124,filatova-2012-irony,0,0.0205982,"inite realm • -------- --------- • -------- --------- • -------- -------- • -------- -------• -------- -------• -------- --------- Figure 5: Creative (blue bold) and not creative (red italic) word pairs graph. 7 Related Work Among computational approaches that touch on linguistic creativity, many focused on metaphor (e.g., Dunn (2013), Krishnakumaran and Zhu (2007), Mashal et al. (2007), Rumbell et al. (2008), Rentoumi et al. (2012), Mashal et al. (2009)). Other linguistic devices and phenomena related to creativity include irony (e.g., Davidov et al. (2010), Gonz´alezIb´an˜ ez et al. (2011), Filatova (2012)), neologism (e.g., Cartoni (2008)), humor (e.g., Mihalcea and Strapparava (2005), Purandare and Litman (2006)), and similes (e.g., Hao and Veale (2010)). Veale (2011) proposed the new task of creative text retrieval to harvest expressions that potentially convey the same meaning as the query phrase in a fresh or unusual way. Our work contributes to the retrieval process of recognizing more creative phrases. Ozbal and Strapparava (2012) explored automatic creative naming of commercial products and services, focusing on the generation of creative phrases within a specific domain. Costello (2002"
D13-1124,W10-2010,0,0.0281989,"Figure 4 visualizes relatively more skewed distribution of “inglorious”. We compute the entropy of future context conditioning on w1 , w2 and w1 w2 , which we denote as H(w1 ), H(w2 ), H(w1 w2 ) respectively, latter is shown in Figure 3 – a.11 11 As before, language models are drawn from Google Web 1251 |H(w1 ) − H(w1 w2 )| H(w1 ) + H(w1 w2 ) (1) As expected (Figure 3 – b and Table 4), this relative quantity captures creativity better than the absolute measure H(w1 w2 ) computed above. The idea behind this measure has a connection to uncertainty reduction in psycholinguistic literature (e.g., Frank (2010), Hale (2003), Hale (2006)). KL divergence To capture unusual combinations of words, we compare the difference between the distributional contexts of w1 and w1 w2 so that KL(w1 w2 , w1 ) = X wi ∈V P (wi |w1 , w2 ) log P (wi |w1 , w2 ) P (wi |w1 ) (2) Figure (3 – c) shows that KL(w1 w2 , w1 )12 is among 1T corpus Brants and Franz (2006). 12 We also compute KL(w1 , w2 ) in a similar manner as KL(w1 w2 , w1 ) the effective measures in capturing creative pairs. Mutual Information Finally, we consider mutual information (Figure 3 – d): M I(w1 , w2 ) = X P (wi |w1 , w2 ) × wi ∈V log P (wi |w1 , w2 )"
D13-1124,W10-0304,0,0.045614,"Missing"
D13-1124,P11-2102,0,0.0406025,"nipotent realm • finite realm • -------- --------- • -------- --------- • -------- -------- • -------- -------• -------- -------• -------- --------- Figure 5: Creative (blue bold) and not creative (red italic) word pairs graph. 7 Related Work Among computational approaches that touch on linguistic creativity, many focused on metaphor (e.g., Dunn (2013), Krishnakumaran and Zhu (2007), Mashal et al. (2007), Rumbell et al. (2008), Rentoumi et al. (2012), Mashal et al. (2009)). Other linguistic devices and phenomena related to creativity include irony (e.g., Davidov et al. (2010), Gonz´alezIb´an˜ ez et al. (2011), Filatova (2012)), neologism (e.g., Cartoni (2008)), humor (e.g., Mihalcea and Strapparava (2005), Purandare and Litman (2006)), and similes (e.g., Hao and Veale (2010)). Veale (2011) proposed the new task of creative text retrieval to harvest expressions that potentially convey the same meaning as the query phrase in a fresh or unusual way. Our work contributes to the retrieval process of recognizing more creative phrases. Ozbal and Strapparava (2012) explored automatic creative naming of commercial products and services, focusing on the generation of creative phrases within a specific domai"
D13-1124,W11-0115,0,0.0816907,"nusual, unconventional set of words is more likely to be creative. Note that the key novelty then lies in the compositional operation itself, i.e., the act of putting together a set of words in an unexpected way, rather than the rareness of individual words being used. In recent years there has been a swell of work on compositional distributional semantics that captures the compositional aspects of language understanding, such as sentiment analysis (e.g., Yessenalina and Cardie (2011), Socher et al. (2011)) and language modeling (e.g., Mitchell and Lapata (2009), Baroni and Zamparelli (2010), Guevara (2011), Clarke (2012), Rudolph and Giesbrecht (2010)). However, none has examined the compositional nature in quantifying creativity in lexical composition. We consider two computational approaches to capture the notion of creative composition. The first is via various information theoretic measures, e.g., relative entropy reduction, to measure the surprisal of seeing the next word given the previous word. The second is via supervised learning, where we explore different modeling techniques to capture the statistical regularities in creative compositional operations. In particular, we will explore ("
D13-1124,P12-1092,0,0.181131,"approaches, the notion of creative semantic subspace is integrated indirectly, as the feature representation always incorporates the resulting (composed) vector representations. Baseline & Configuration We consider the concatenation of two word vectors [w ~ 1; w ~ 2 ] as the baseline, since it can be viewed as what simple bag-ofword features would be. Since the size of creative pair dataset is not at scale yet, we choose to work with vector space models that are in reduced dimensions. We experimented with both Non-Negative Sparse Embedding (Murphy et al. (2012)) and neural semantic vectors of Huang et al. (2012), but report experiments with the latter only as those gave us slightly better results. 5.1 Compositional Vector Operations We consider the following compositional vector operations inspired by recent studies for compositional distributional semantics (e.g., Guevara (2011), Clarke (2012), Mitchell and Lapata (2008), Widdows (2008)). • ADD: w ~1 + w ~2 • DIFF: abs(w ~1 − w ~ 2) of Chen and Lin (2005) determines that the most two important ones are RH(w1 , w2 ) and KL(w1 , w2 ). 1253 All operations take two input vectors ∈ Rn , and output a vector ∈ Rn . Each operation is applied element-wise. W"
D13-1124,W07-0103,0,0.0243465,"od • -------- -------• -------- --------• -------- -------• -------- -------• -------- --------• -------- -------- • legal trading • -------- --------- • infinite promise • perfect disorder • perfect fire • human architecture • human spark • omnipotent realm • finite realm • -------- --------- • -------- --------- • -------- -------- • -------- -------• -------- -------• -------- --------- Figure 5: Creative (blue bold) and not creative (red italic) word pairs graph. 7 Related Work Among computational approaches that touch on linguistic creativity, many focused on metaphor (e.g., Dunn (2013), Krishnakumaran and Zhu (2007), Mashal et al. (2007), Rumbell et al. (2008), Rentoumi et al. (2012), Mashal et al. (2009)). Other linguistic devices and phenomena related to creativity include irony (e.g., Davidov et al. (2010), Gonz´alezIb´an˜ ez et al. (2011), Filatova (2012)), neologism (e.g., Cartoni (2008)), humor (e.g., Mihalcea and Strapparava (2005), Purandare and Litman (2006)), and similes (e.g., Hao and Veale (2010)). Veale (2011) proposed the new task of creative text retrieval to harvest expressions that potentially convey the same meaning as the query phrase in a fresh or unusual way. Our work contributes to"
D13-1124,H05-1067,0,0.216429,"t a collection of interesting expressions from the web, which may be potentially useful for enriching natural language generation systems. With these practical goals in mind, we aim to understand phrases with linguistic creativity in a broad scope. Similarly as the work of Zhu et al. (2009), our study encompasses phrases that evoke the sense of interestingness and creativity in readers’ minds, rather than focusing exclusively on clearly but narrowly defined figure of speeches such as metaphors (e.g., Shutova (2010)), similes (e.g., Veale et al. (2008), Hao and Veale (2010)), and humors (e.g., Mihalcea and Strapparava (2005), Purandare and Litman (2006)). Unlike the study of Zhu et al. (2009), however, we concentrate specifically on how combinations of different words give rise to the sense of creativity, as this is an angle that has not been directly studied before. We leave the roles of syntactic elements as future research. We first examine various correlates of perceived creativity based on information theoretic measures and the connotation of words, then present experiments based on supervised learning that give us further insights on how different aspects of lexical composition collectively contribute to th"
D13-1124,P08-1028,0,0.0471649,"as what simple bag-ofword features would be. Since the size of creative pair dataset is not at scale yet, we choose to work with vector space models that are in reduced dimensions. We experimented with both Non-Negative Sparse Embedding (Murphy et al. (2012)) and neural semantic vectors of Huang et al. (2012), but report experiments with the latter only as those gave us slightly better results. 5.1 Compositional Vector Operations We consider the following compositional vector operations inspired by recent studies for compositional distributional semantics (e.g., Guevara (2011), Clarke (2012), Mitchell and Lapata (2008), Widdows (2008)). • ADD: w ~1 + w ~2 • DIFF: abs(w ~1 − w ~ 2) of Chen and Lin (2005) determines that the most two important ones are RH(w1 , w2 ) and KL(w1 , w2 ). 1253 All operations take two input vectors ∈ Rn , and output a vector ∈ Rn . Each operation is applied element-wise. We then perform binary classification over the composed vectors using linear SVM. Besides using features based on the composed vectors, we also experiment with features based on concatenating multiple composed vectors, in the hope to capture more diverse compositional operations. See Table 5 for more details and exp"
D13-1124,D09-1045,0,0.0213242,"ical composition, divergent composition that explores an unusual, unconventional set of words is more likely to be creative. Note that the key novelty then lies in the compositional operation itself, i.e., the act of putting together a set of words in an unexpected way, rather than the rareness of individual words being used. In recent years there has been a swell of work on compositional distributional semantics that captures the compositional aspects of language understanding, such as sentiment analysis (e.g., Yessenalina and Cardie (2011), Socher et al. (2011)) and language modeling (e.g., Mitchell and Lapata (2009), Baroni and Zamparelli (2010), Guevara (2011), Clarke (2012), Rudolph and Giesbrecht (2010)). However, none has examined the compositional nature in quantifying creativity in lexical composition. We consider two computational approaches to capture the notion of creative composition. The first is via various information theoretic measures, e.g., relative entropy reduction, to measure the surprisal of seeing the next word given the previous word. The second is via supervised learning, where we explore different modeling techniques to capture the statistical regularities in creative compositiona"
D13-1124,C12-1118,0,0.0187877,"ion via deep learning (§5.3). Note that in all these approaches, the notion of creative semantic subspace is integrated indirectly, as the feature representation always incorporates the resulting (composed) vector representations. Baseline & Configuration We consider the concatenation of two word vectors [w ~ 1; w ~ 2 ] as the baseline, since it can be viewed as what simple bag-ofword features would be. Since the size of creative pair dataset is not at scale yet, we choose to work with vector space models that are in reduced dimensions. We experimented with both Non-Negative Sparse Embedding (Murphy et al. (2012)) and neural semantic vectors of Huang et al. (2012), but report experiments with the latter only as those gave us slightly better results. 5.1 Compositional Vector Operations We consider the following compositional vector operations inspired by recent studies for compositional distributional semantics (e.g., Guevara (2011), Clarke (2012), Mitchell and Lapata (2008), Widdows (2008)). • ADD: w ~1 + w ~2 • DIFF: abs(w ~1 − w ~ 2) of Chen and Lin (2005) determines that the most two important ones are RH(w1 , w2 ) and KL(w1 , w2 ). 1253 All operations take two input vectors ∈ Rn , and output a vec"
D13-1124,P12-1074,0,0.0741374,"l. (2012), Mashal et al. (2009)). Other linguistic devices and phenomena related to creativity include irony (e.g., Davidov et al. (2010), Gonz´alezIb´an˜ ez et al. (2011), Filatova (2012)), neologism (e.g., Cartoni (2008)), humor (e.g., Mihalcea and Strapparava (2005), Purandare and Litman (2006)), and similes (e.g., Hao and Veale (2010)). Veale (2011) proposed the new task of creative text retrieval to harvest expressions that potentially convey the same meaning as the query phrase in a fresh or unusual way. Our work contributes to the retrieval process of recognizing more creative phrases. Ozbal and Strapparava (2012) explored automatic creative naming of commercial products and services, focusing on the generation of creative phrases within a specific domain. Costello (2002) investigated the cognitive process that guides people’s choice of words when making up a novel nounnoun compound. In contrast, we present a datadriven investigation to quantifying creativity in lexical composition. Memorability is loosely related to 1255 linguistic creativity (Danescu-Niculescu-Mizil et al. (2012)) as some of the creative quotes may be more memorable, but not all creative phrases are memorable and vice versa. 8 Conclu"
D13-1124,W02-1011,0,0.0126803,"s a series of yes/no questions to help turkers to determine whether the given pair is creative or not.8 We determine the final label of a word pair based on two scores, creativity scale score and yes/no questionbased score. If creativity scale score is 4 or 5 and question-based score is positive, we label the pair as creative. Similarly, if creativity scale score is 1 or 2 and question-based score is negative, we label the pair as common. We discard the rest from the final dataset. This filtering process is akin to the removal of neural sentiment in the early work of sentiment analysis (e.g., Pang et al. (2002)).9 Table 2 shows the statistics of the resulting dataset. Creative Pairs and their Frequencies: To gain insights on the stratified sample of word pairs, we plot the label (∈ {creative, common}) distribution of word pairs as a function of simple statistics, such as a range (bucket) of bigram frequencies or PMI values of the given pair of words. Both bigram frequencies and PMI scores are computed based on Google Web 1T corpus Brants and Franz (2006). Figure 1 shows the results for word frequencies. As expected, word pairs with high frequencies are much more likely to be common, while word pairs"
D13-1124,W06-1625,0,0.106916,"ressions from the web, which may be potentially useful for enriching natural language generation systems. With these practical goals in mind, we aim to understand phrases with linguistic creativity in a broad scope. Similarly as the work of Zhu et al. (2009), our study encompasses phrases that evoke the sense of interestingness and creativity in readers’ minds, rather than focusing exclusively on clearly but narrowly defined figure of speeches such as metaphors (e.g., Shutova (2010)), similes (e.g., Veale et al. (2008), Hao and Veale (2010)), and humors (e.g., Mihalcea and Strapparava (2005), Purandare and Litman (2006)). Unlike the study of Zhu et al. (2009), however, we concentrate specifically on how combinations of different words give rise to the sense of creativity, as this is an angle that has not been directly studied before. We leave the roles of syntactic elements as future research. We first examine various correlates of perceived creativity based on information theoretic measures and the connotation of words, then present experiments based on supervised learning that give us further insights on how different aspects of lexical composition collectively contribute to the perceived cre1247 ativity."
D13-1124,P10-1093,0,0.023766,"ords is more likely to be creative. Note that the key novelty then lies in the compositional operation itself, i.e., the act of putting together a set of words in an unexpected way, rather than the rareness of individual words being used. In recent years there has been a swell of work on compositional distributional semantics that captures the compositional aspects of language understanding, such as sentiment analysis (e.g., Yessenalina and Cardie (2011), Socher et al. (2011)) and language modeling (e.g., Mitchell and Lapata (2009), Baroni and Zamparelli (2010), Guevara (2011), Clarke (2012), Rudolph and Giesbrecht (2010)). However, none has examined the compositional nature in quantifying creativity in lexical composition. We consider two computational approaches to capture the notion of creative composition. The first is via various information theoretic measures, e.g., relative entropy reduction, to measure the surprisal of seeing the next word given the previous word. The second is via supervised learning, where we explore different modeling techniques to capture the statistical regularities in creative compositional operations. In particular, we will explore (1) compositional operations of vector space mo"
D13-1124,P10-1071,0,0.0660997,"nto automatic assessment of writing styles and quality, and utilized to automatically construct a collection of interesting expressions from the web, which may be potentially useful for enriching natural language generation systems. With these practical goals in mind, we aim to understand phrases with linguistic creativity in a broad scope. Similarly as the work of Zhu et al. (2009), our study encompasses phrases that evoke the sense of interestingness and creativity in readers’ minds, rather than focusing exclusively on clearly but narrowly defined figure of speeches such as metaphors (e.g., Shutova (2010)), similes (e.g., Veale et al. (2008), Hao and Veale (2010)), and humors (e.g., Mihalcea and Strapparava (2005), Purandare and Litman (2006)). Unlike the study of Zhu et al. (2009), however, we concentrate specifically on how combinations of different words give rise to the sense of creativity, as this is an angle that has not been directly studied before. We leave the roles of syntactic elements as future research. We first examine various correlates of perceived creativity based on information theoretic measures and the connotation of words, then present experiments based on supervised learn"
D13-1124,D08-1027,0,0.0394524,"Missing"
D13-1124,D11-1014,0,0.240696,"y (2006)). Applying the same high-level idea to lexical composition, divergent composition that explores an unusual, unconventional set of words is more likely to be creative. Note that the key novelty then lies in the compositional operation itself, i.e., the act of putting together a set of words in an unexpected way, rather than the rareness of individual words being used. In recent years there has been a swell of work on compositional distributional semantics that captures the compositional aspects of language understanding, such as sentiment analysis (e.g., Yessenalina and Cardie (2011), Socher et al. (2011)) and language modeling (e.g., Mitchell and Lapata (2009), Baroni and Zamparelli (2010), Guevara (2011), Clarke (2012), Rudolph and Giesbrecht (2010)). However, none has examined the compositional nature in quantifying creativity in lexical composition. We consider two computational approaches to capture the notion of creative composition. The first is via various information theoretic measures, e.g., relative entropy reduction, to measure the surprisal of seeing the next word given the previous word. The second is via supervised learning, where we explore different modeling techniques to capt"
D13-1124,P08-1060,0,0.0296574,"ing styles and quality, and utilized to automatically construct a collection of interesting expressions from the web, which may be potentially useful for enriching natural language generation systems. With these practical goals in mind, we aim to understand phrases with linguistic creativity in a broad scope. Similarly as the work of Zhu et al. (2009), our study encompasses phrases that evoke the sense of interestingness and creativity in readers’ minds, rather than focusing exclusively on clearly but narrowly defined figure of speeches such as metaphors (e.g., Shutova (2010)), similes (e.g., Veale et al. (2008), Hao and Veale (2010)), and humors (e.g., Mihalcea and Strapparava (2005), Purandare and Litman (2006)). Unlike the study of Zhu et al. (2009), however, we concentrate specifically on how combinations of different words give rise to the sense of creativity, as this is an angle that has not been directly studied before. We leave the roles of syntactic elements as future research. We first examine various correlates of perceived creativity based on information theoretic measures and the connotation of words, then present experiments based on supervised learning that give us further insights on"
D13-1124,P11-1029,0,0.171033,"creative (red italic) word pairs graph. 7 Related Work Among computational approaches that touch on linguistic creativity, many focused on metaphor (e.g., Dunn (2013), Krishnakumaran and Zhu (2007), Mashal et al. (2007), Rumbell et al. (2008), Rentoumi et al. (2012), Mashal et al. (2009)). Other linguistic devices and phenomena related to creativity include irony (e.g., Davidov et al. (2010), Gonz´alezIb´an˜ ez et al. (2011), Filatova (2012)), neologism (e.g., Cartoni (2008)), humor (e.g., Mihalcea and Strapparava (2005), Purandare and Litman (2006)), and similes (e.g., Hao and Veale (2010)). Veale (2011) proposed the new task of creative text retrieval to harvest expressions that potentially convey the same meaning as the query phrase in a fresh or unusual way. Our work contributes to the retrieval process of recognizing more creative phrases. Ozbal and Strapparava (2012) explored automatic creative naming of commercial products and services, focusing on the generation of creative phrases within a specific domain. Costello (2002) investigated the cognitive process that guides people’s choice of words when making up a novel nounnoun compound. In contrast, we present a datadriven investigation"
D13-1124,H05-2018,1,0.715854,"re non-linear correlations that are evident in graphs shown in Figure 3. Second, these measures only capture the surprisal aspect of creativity, missing the other important qualities: interestingness or imaginativeness. 4.2 Sentiment and Connotation Next we investigate the connection between creativity and sentiment, as illustrated in §2.3. We consider both sentiment (more explicit) and connotation (more implicit) words,13 and consider them with or without distinguishing the polarity (i.e., positive, negative). To determine sentiment and connotation, we use lexicons provided by OpinionFinder (Wilson et al. (2005)) and Feng et al. (2013) respectively. We denote polarity of a word wi as L(wi ).14 When wi has a negative polarity L(wi ) is assigned a value of -1, and when wi is positive L(wi ) is equal to 1. We assume that a word is neutral when it is not in the lexicon, assigning 0 to L(wi ). For a word pair w1 w2 we compute absolute difference Ldif f (w1 , w2 ) between polarities of tokens in a word pair in order to catch examples such as “inglorious success”. 13 E.g., expressions such as “blue sky” or “white sand” are not sentiment-laden, but do have positive connotation. 14 We denote polarity from Opi"
D13-1124,D11-1016,0,0.0244335,"correct solution (e.g., Cropley (2006)). Applying the same high-level idea to lexical composition, divergent composition that explores an unusual, unconventional set of words is more likely to be creative. Note that the key novelty then lies in the compositional operation itself, i.e., the act of putting together a set of words in an unexpected way, rather than the rareness of individual words being used. In recent years there has been a swell of work on compositional distributional semantics that captures the compositional aspects of language understanding, such as sentiment analysis (e.g., Yessenalina and Cardie (2011), Socher et al. (2011)) and language modeling (e.g., Mitchell and Lapata (2009), Baroni and Zamparelli (2010), Guevara (2011), Clarke (2012), Rudolph and Giesbrecht (2010)). However, none has examined the compositional nature in quantifying creativity in lexical composition. We consider two computational approaches to capture the notion of creative composition. The first is via various information theoretic measures, e.g., relative entropy reduction, to measure the surprisal of seeing the next word given the previous word. The second is via supervised learning, where we explore different model"
D13-1150,D11-1145,0,0.0867638,"that have violated health codes. In some counties and cities, e.g., LA, NYC, it is required for restaurants to post their inspection grades at their premises, which have shown to affect the revenue of the business substantially (e.g., Jin and Leslie (2005), Henson et al. (2006)), thereby motivating restaurants to improve their sanitary practice. Other studies have reported correlation Our work shares the spirit of recently emerging studies that explores social media analysis for public health surveillance, in particular, monitoring influenza or food-poisoning outbreaks from microblogs (e.g., Aramaki et al. (2011), Sadilek et al. (2012b), Sadilek et al. (2012a), Sadilek et al. (2013), Lamb et al. (2013), Dredze et al. (2013), von Etter et al. (2010)). However, no prior work has examined the utility of review analysis as a predictive tool for accessing hygiene of restaurants, perhaps because the connection is not entirely conspicuous: after all, customers are neither familiar with inspection codes, nor have the full access to the kitchen, nor have been asked to report on the hygiene aspects of their expe1443 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 14"
D13-1150,N10-1122,0,0.0228836,"f the inspections. Second, our work is the first to examine online reviews in the context of improving public policy, suggesting additional source of information for public policy makers to pay attention to. Our work draws from the rich body of research that studies online reviews for sentiment analysis (e.g., Pang and Lee (2008)) and deception detection (e.g., Mihalcea and Strapparava (2009), Ott et al. (2011), Feng et al. (2012)), while introducing the new task of public hygiene prediction. We expect that previous studies for aspect-based sentiment analysis (e.g., Titov and McDonald (2008), Brody and Elhadad (2010), Wang et al. (2010)) would be a fruitful venue for further investigation. 6 Conclusion We have reported the first empirical study demonstrating the promise of review analysis for predicting health inspections, introducing a task that has potentially significant societal benefits, while being relevant to much research in NLP for opinion analysis based on customer reviews. Acknowledgments This research was supported in part by the Stony Brook University Office of the Vice President for Research, and in part by gift from Google. We thank anonymous reviewers and Adam Sadilek for helpful comments"
D13-1150,N13-1097,0,0.0573589,"or restaurants to post their inspection grades at their premises, which have shown to affect the revenue of the business substantially (e.g., Jin and Leslie (2005), Henson et al. (2006)), thereby motivating restaurants to improve their sanitary practice. Other studies have reported correlation Our work shares the spirit of recently emerging studies that explores social media analysis for public health surveillance, in particular, monitoring influenza or food-poisoning outbreaks from microblogs (e.g., Aramaki et al. (2011), Sadilek et al. (2012b), Sadilek et al. (2012a), Sadilek et al. (2013), Lamb et al. (2013), Dredze et al. (2013), von Etter et al. (2010)). However, no prior work has examined the utility of review analysis as a predictive tool for accessing hygiene of restaurants, perhaps because the connection is not entirely conspicuous: after all, customers are neither familiar with inspection codes, nor have the full access to the kitchen, nor have been asked to report on the hygiene aspects of their expe1443 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1443–1448, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational"
D13-1150,P09-2078,0,0.0232863,"ontrast, we examine all words people use in online reviews, and draw insights on correlating terms and concepts that may not seem immediately relevant to the hygiene status of restaurants, but nonetheless are predictive of the outcome of the inspections. Second, our work is the first to examine online reviews in the context of improving public policy, suggesting additional source of information for public policy makers to pay attention to. Our work draws from the rich body of research that studies online reviews for sentiment analysis (e.g., Pang and Lee (2008)) and deception detection (e.g., Mihalcea and Strapparava (2009), Ott et al. (2011), Feng et al. (2012)), while introducing the new task of public hygiene prediction. We expect that previous studies for aspect-based sentiment analysis (e.g., Titov and McDonald (2008), Brody and Elhadad (2010), Wang et al. (2010)) would be a fruitful venue for further investigation. 6 Conclusion We have reported the first empirical study demonstrating the promise of review analysis for predicting health inspections, introducing a task that has potentially significant societal benefits, while being relevant to much research in NLP for opinion analysis based on customer revie"
D13-1150,P11-1032,1,0.812831,"ple use in online reviews, and draw insights on correlating terms and concepts that may not seem immediately relevant to the hygiene status of restaurants, but nonetheless are predictive of the outcome of the inspections. Second, our work is the first to examine online reviews in the context of improving public policy, suggesting additional source of information for public policy makers to pay attention to. Our work draws from the rich body of research that studies online reviews for sentiment analysis (e.g., Pang and Lee (2008)) and deception detection (e.g., Mihalcea and Strapparava (2009), Ott et al. (2011), Feng et al. (2012)), while introducing the new task of public hygiene prediction. We expect that previous studies for aspect-based sentiment analysis (e.g., Titov and McDonald (2008), Brody and Elhadad (2010), Wang et al. (2010)) would be a fruitful venue for further investigation. 6 Conclusion We have reported the first empirical study demonstrating the promise of review analysis for predicting health inspections, introducing a task that has potentially significant societal benefits, while being relevant to much research in NLP for opinion analysis based on customer reviews. Acknowledgments"
D13-1150,P08-1036,0,0.0131623,"predictive of the outcome of the inspections. Second, our work is the first to examine online reviews in the context of improving public policy, suggesting additional source of information for public policy makers to pay attention to. Our work draws from the rich body of research that studies online reviews for sentiment analysis (e.g., Pang and Lee (2008)) and deception detection (e.g., Mihalcea and Strapparava (2009), Ott et al. (2011), Feng et al. (2012)), while introducing the new task of public hygiene prediction. We expect that previous studies for aspect-based sentiment analysis (e.g., Titov and McDonald (2008), Brody and Elhadad (2010), Wang et al. (2010)) would be a fruitful venue for further investigation. 6 Conclusion We have reported the first empirical study demonstrating the promise of review analysis for predicting health inspections, introducing a task that has potentially significant societal benefits, while being relevant to much research in NLP for opinion analysis based on customer reviews. Acknowledgments This research was supported in part by the Stony Brook University Office of the Vice President for Research, and in part by gift from Google. We thank anonymous reviewers and Adam Sad"
D13-1150,W10-1105,0,0.0630123,"Missing"
D13-1181,N12-1033,0,0.0308666,"Missing"
D13-1181,N04-1025,0,0.0128025,"Missing"
D13-1181,W11-0609,0,0.395212,"em, there has been little previous work that attempts to build statistical models that predict the success of literary works based on their intrinsic content and quality. Some previous studies do touch on the notion of stylistic aspects in successful literature, e.g., extensive studies in Literature discuss literary styles of significant authors (e.g., Elleg˚ard (1962), McGann (1998)), while others consider content characteristics such as plots, characteristics of characters, action, emotion, genre, cast, of the best-selling novels and blockbuster movies (e.g., Harvey (1953), Hall (2012), Yun (2011)). All these studies however, are qualitative in nature, as they rely on the knowledge and insights of human experts on literature. To our knowledge, no prior work has undertaken a systematic quantitative investigation on the overarching characterization of the writing style in successful literature. In consideration of widely different styles of authorship (e.g., Escalante et al. (2011), Peng et al. (2003), Argamon et al. (2003)), it is not even readily clear whether there might be common stylistic elements that help discriminating highly successful ones from less successful counterpart. In t"
D13-1181,P11-1030,0,0.0632784,"Missing"
D13-1181,D12-1139,1,0.791872,"Missing"
D13-1181,P13-1174,1,0.819837,"sk”). 5.2 Distribution of Sentiment & Connotation We also determine the distribution of sentiment and connotation words separately for each class (Table 5) to check if there exists a connection with respect to successful writing styles.7 We first compare distribution of sentiment and connotation for the entire words. As can be seen in Table 5 – Top, there are not notable differences. However, when we compare distribution only with respect to discriminative unigrams only (i.e., features with non-zero weights), as 7 We use MPQA subjectivity lexicon (Wilson et al., 2005) and connotation lexicon (Feng et al., 2013) for determining sentiment and connotation of words respectively. 1757 Prepositions Thinking Verbs Table 4: Discriminative unigrams for A DVENTURE. shown in Table 5 – Bottom, we find substantial differences in all genres. In particular, discriminative unigrams that characterize less successful novels involve significantly more sentiment-laden words. 5.3 Distribution of Word Categories Summarized analysis of POS distribution across all genres is reported in Table 6. It can be seen that prepositions, nouns, pronouns, determiners and adjectives are predictive of highly successful books whereas le"
D13-1181,C10-1062,0,0.0102238,"e power of distribution systems etc, without analyzing the actual content of the movie scripts. Text quality and readability: Louis (2012) explored various features that measure the quality of text, which has some high-level connections to our work. Combining the insights from Louis (2012) with our results, we find that the characteristics of text quality explored in Louis (2012), readability of text in particular, do not correspond to the prominent writing style of highly successful literature. There have been a number of other work that focused on predicting and measuring readability (e.g., Kate et al. (2010), Pitler and Nenkova (2008), Schwarm and Ostendorf (2005), Heilman and Eskenazi (2006) and Collins-Thompson et al. (2004)) employing various linguistic features. There is an important difference however, in regard to the nature of the selected text for analysis: most studies in readability focus on differentiating good writings from noticeably bad writings, often involving machine generated text or those written by ESL students. In contrast, our work essentially 1762 deals with differentiating good writings from even better writings. After all, all the books that we analyzed are written by exp"
D13-1181,P97-1005,0,0.069696,"Missing"
D13-1181,P03-1054,0,0.0609118,"Missing"
D13-1181,N12-2010,0,0.0145592,"ntrast, our work examines a considerably larger collection of books (800 in total) over eight different sub-genres, providing insights into lexical, syntactic, and discourse patterns that characterize the writing styles commonly shared among the successful literature. Another relevant work has been on a different domain of movies (Yun, 2011), however, the prediction is based only on external, non-textual information such as the reputation of actors and directors, and the power of distribution systems etc, without analyzing the actual content of the movie scripts. Text quality and readability: Louis (2012) explored various features that measure the quality of text, which has some high-level connections to our work. Combining the insights from Louis (2012) with our results, we find that the characteristics of text quality explored in Louis (2012), readability of text in particular, do not correspond to the prominent writing style of highly successful literature. There have been a number of other work that focused on predicting and measuring readability (e.g., Kate et al. (2010), Pitler and Nenkova (2008), Schwarm and Ostendorf (2005), Heilman and Eskenazi (2006) and Collins-Thompson et al. (2004"
D13-1181,D08-1020,0,0.135008,"ion systems etc, without analyzing the actual content of the movie scripts. Text quality and readability: Louis (2012) explored various features that measure the quality of text, which has some high-level connections to our work. Combining the insights from Louis (2012) with our results, we find that the characteristics of text quality explored in Louis (2012), readability of text in particular, do not correspond to the prominent writing style of highly successful literature. There have been a number of other work that focused on predicting and measuring readability (e.g., Kate et al. (2010), Pitler and Nenkova (2008), Schwarm and Ostendorf (2005), Heilman and Eskenazi (2006) and Collins-Thompson et al. (2004)) employing various linguistic features. There is an important difference however, in regard to the nature of the selected text for analysis: most studies in readability focus on differentiating good writings from noticeably bad writings, often involving machine generated text or those written by ESL students. In contrast, our work essentially 1762 deals with differentiating good writings from even better writings. After all, all the books that we analyzed are written by expert writers who passed the"
D13-1181,P10-2008,0,0.0100833,"t al. (1997)) and authorship attribution (e.g., Stamatatos (2009)), while the last two are newly explored in this work. I. Lexical Choices: unigrams and bigrams. II. Distribution of Word Categories: Many previous studies have shown that the distribution of part-of-speech (POS) tags alone can reveal surprising insights on genre and authorship (e.g., Koppel and Schler (2003)), hence we examine their distributions with respect to the success of literary works. III. Distribution of Grammar Rules: Recent studies reported that features based on CFG rules are helpful in authorship attribution (e.g., Raghavan et al. (2010), Feng et al. (2012)). We experiment with four different encodings of production rules: • Γ: lexicalized production rules (all production rules, including those with terminals) • ΓG : lexicalized production rules prepended with the grandparent node. • γ: unlexicalized production rules (all production rules except those with terminals). • γ G : unlexicalized production rules prepended with the grandparent node. F EATURE POS Unigram Bigram Γ ΓG γ γG Γ+Unigram ΓG +Unigram γ+Unigram γ G +Unigram PHR PHR+CLS PHR+Unigram PHR+CLS+Unigram Adven 74.0 84.0 81.0 73.0 75.0 72.0 72.0 79.0 80.0 82.0 80.0 74"
D13-1181,W11-0310,1,0.82013,"Missing"
D13-1181,P05-1065,0,0.0167696,"alyzing the actual content of the movie scripts. Text quality and readability: Louis (2012) explored various features that measure the quality of text, which has some high-level connections to our work. Combining the insights from Louis (2012) with our results, we find that the characteristics of text quality explored in Louis (2012), readability of text in particular, do not correspond to the prominent writing style of highly successful literature. There have been a number of other work that focused on predicting and measuring readability (e.g., Kate et al. (2010), Pitler and Nenkova (2008), Schwarm and Ostendorf (2005), Heilman and Eskenazi (2006) and Collins-Thompson et al. (2004)) employing various linguistic features. There is an important difference however, in regard to the nature of the selected text for analysis: most studies in readability focus on differentiating good writings from noticeably bad writings, often involving machine generated text or those written by ESL students. In contrast, our work essentially 1762 deals with differentiating good writings from even better writings. After all, all the books that we analyzed are written by expert writers who passed the scrutinizing eyes of publisher"
D13-1181,W00-1308,0,0.0371177,"tion of Word Categories Summarized analysis of POS distribution across all genres is reported in Table 6. It can be seen that prepositions, nouns, pronouns, determiners and adjectives are predictive of highly successful books whereas less successful books are characterized by higher percentage of verbs, adverbs, and foreign words. Per genre distributions of POS tags are visualized in Figure 1. Interestingly, some POS tags show almost universal patterns (e.g., prepositions (IN), NNP, WP, VB), while others are more genrespecific. In Relation to Journalism Style The work of Douglas and Broussard (2000) reveals that informative writing (journalism) involves increased use of nouns, prepositions, determiners and coordinating conjunctions whereas imaginative writing (novels) involves more use of verbs and adverbs, as has been also confirmed by Rayson et al. (2001). Comparing their findings with Table 6, we find that highly +ve S -ve S Tot S +ve C -ve C Total C Adven + 4.7 4.9 4.0 4.0 8.7 8.9 22.3 22.5 19.4 19.6 41.7 42.1 Myster + 4.8 4.6 4.0 4.0 8.9 8.7 22.3 22.5 19.8 19.8 42.1 42.3 Fiction + 5.6 4.9 4.3 4.2 9.9 9.0 23.7 23.0 20.3 19.5 44.0 42.5 Histor + 5.0 5.1 4.2 4.2 9.2 9.3 23.0 23.2 19.2 1"
D13-1181,H05-1044,0,0.00515437,"., “breathless”) and negative words (e.g., “risk”). 5.2 Distribution of Sentiment & Connotation We also determine the distribution of sentiment and connotation words separately for each class (Table 5) to check if there exists a connection with respect to successful writing styles.7 We first compare distribution of sentiment and connotation for the entire words. As can be seen in Table 5 – Top, there are not notable differences. However, when we compare distribution only with respect to discriminative unigrams only (i.e., features with non-zero weights), as 7 We use MPQA subjectivity lexicon (Wilson et al., 2005) and connotation lexicon (Feng et al., 2013) for determining sentiment and connotation of words respectively. 1757 Prepositions Thinking Verbs Table 4: Discriminative unigrams for A DVENTURE. shown in Table 5 – Bottom, we find substantial differences in all genres. In particular, discriminative unigrams that characterize less successful novels involve significantly more sentiment-laden words. 5.3 Distribution of Word Categories Summarized analysis of POS distribution across all genres is reported in Table 6. It can be seen that prepositions, nouns, pronouns, determiners and adjectives are pred"
D13-1181,D11-1148,0,0.0445607,"Missing"
D14-1155,P12-2034,1,0.360522,"the content of communication, thus providing cues of deception to an observer. These include linguistic (e.g., Newman et al. (2003), Hancock et al. (2004)) as well as paralinguistic (e.g., Ekman et al. (1991), DePaulo et al. (2003)) cues. Recognizing deception, however, remains a hard task for humans, who perform only marginally better than chance (Bond and DePaulo, 2006; Ott et al., 2011). Recent studies suggest that computers can be surprisingly effective in this task, albeit in limited domains such as product reviews. Prior research has employed lexico-syntactic patterns (Ott et al., 2011; Feng et al., 2012) as well as online user behavior (Fei et al., 2013; Mukherjee et al., 2013). In this paper, we study the effect of keystroke patterns for deception detection in digital communications, which might be helpful in understanding the psychology of deception and help toward trustful online communities. This allows us to investigate differences in the writing and revisional processes of truthful and fake writers. Our work thus shares intuition with HCI research linking keystroke analysis to cognitive processes (Vizer et al., 2009; Epp et al., 2011) and psychology research connecting cognitive differe"
D14-1155,P11-1032,1,0.928429,"importance of detecting deceit, interest in it is ancient, appearing in papyrus dated back to 900 B.C. (Trovillo, 1939). In more recent years, several studies have shown that the deceiver often exhibits behavior that belies the content of communication, thus providing cues of deception to an observer. These include linguistic (e.g., Newman et al. (2003), Hancock et al. (2004)) as well as paralinguistic (e.g., Ekman et al. (1991), DePaulo et al. (2003)) cues. Recognizing deception, however, remains a hard task for humans, who perform only marginally better than chance (Bond and DePaulo, 2006; Ott et al., 2011). Recent studies suggest that computers can be surprisingly effective in this task, albeit in limited domains such as product reviews. Prior research has employed lexico-syntactic patterns (Ott et al., 2011; Feng et al., 2012) as well as online user behavior (Fei et al., 2013; Mukherjee et al., 2013). In this paper, we study the effect of keystroke patterns for deception detection in digital communications, which might be helpful in understanding the psychology of deception and help toward trustful online communities. This allows us to investigate differences in the writing and revisional proc"
D15-1114,Q13-1005,1,0.727935,"ctions (Regneri et al., 2013) or recipe texts (Malmaud et al., 2014; Malmaud et al., 2015). Our work complements these efforts by recovering fine-grained procedural semantics from text alone. Finally, detection and resolution of implicit arguments is an instance of zero anaphora detection and resolution (Silberer and Anette, 2012, Tetreault 2002, Whittemore et al., 1991, Palmer et al., 1986). We present an empirical approach for understanding these phenomena in instructions. Our work relates to a substantial body of research that transforms natural language instructions into actionable plans (Artzi and Zettlemoyer, 2013, Chen and Mooney, 2011, Branavan et al., 2011, Branavan et al., 2009, McMahon et al., 2006). Most of these approaches do interactive learning in virtual environments or simulations, while we learn from the redundancy seen in the text of different instances of similar recipes. There is also significant related work on supervised learning for instructions. A recent series of studies have explored parsing of cooking recipes (Mori et al., 2012; Mori et al., 2014; Maeta et al., 2015). However, they assume annotated data, study Japanese recipes, and make edge connections independently without takin"
D15-1114,E14-1006,0,0.0128097,"ored in the context of building a cooking ontology (Gaillard et al., 2012; Nanba et al., 2014). In contrast, our work induces probabilistic cooking knowledge as part of unsupervised learning process for understanding recipes. Cooking knowledge is also closely related to script knowledge, but most prior work focus on newswire and children’s books rather than procedural language (Fujiki et al., 2003; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Balasubramanian et al., 2013) or rely on crowdsourced descriptions to learn procedural knowledge (Regneri et al., 2010; Regneri et al., 2011; Frermann et al., 2014). There is work on related, but distinct, tasks that use recipes, including identifying actionable refinements from online recipe reviews (Druck and Pang, 2012) and extracting structured 10 Conclusion We presented unsupervised methods for segmenting and identifying latent connections among actions in recipe text. Our model outperformed a strong linear baseline, while learning a variety of domain knowledge, such as verb signatures and probable ingredient components for different composites. Future work includes learning a more comprehensive model of locations (e.g., identifying nested locations"
D15-1114,D13-1178,0,0.0127042,"but also assume supervision, and do not make connections between different actions. Data-driven extraction of cooking knowledge has been explored in the context of building a cooking ontology (Gaillard et al., 2012; Nanba et al., 2014). In contrast, our work induces probabilistic cooking knowledge as part of unsupervised learning process for understanding recipes. Cooking knowledge is also closely related to script knowledge, but most prior work focus on newswire and children’s books rather than procedural language (Fujiki et al., 2003; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Balasubramanian et al., 2013) or rely on crowdsourced descriptions to learn procedural knowledge (Regneri et al., 2010; Regneri et al., 2011; Frermann et al., 2014). There is work on related, but distinct, tasks that use recipes, including identifying actionable refinements from online recipe reviews (Druck and Pang, 2012) and extracting structured 10 Conclusion We presented unsupervised methods for segmenting and identifying latent connections among actions in recipe text. Our model outperformed a strong linear baseline, while learning a variety of domain knowledge, such as verb signatures and probable ingredient compone"
D15-1114,E03-1061,0,0.0178609,"on models. Lau et al. (2009) develop models to interpret how-to instructions, but also assume supervision, and do not make connections between different actions. Data-driven extraction of cooking knowledge has been explored in the context of building a cooking ontology (Gaillard et al., 2012; Nanba et al., 2014). In contrast, our work induces probabilistic cooking knowledge as part of unsupervised learning process for understanding recipes. Cooking knowledge is also closely related to script knowledge, but most prior work focus on newswire and children’s books rather than procedural language (Fujiki et al., 2003; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Balasubramanian et al., 2013) or rely on crowdsourced descriptions to learn procedural knowledge (Regneri et al., 2010; Regneri et al., 2011; Frermann et al., 2014). There is work on related, but distinct, tasks that use recipes, including identifying actionable refinements from online recipe reviews (Druck and Pang, 2012) and extracting structured 10 Conclusion We presented unsupervised methods for segmenting and identifying latent connections among actions in recipe text. Our model outperformed a strong linear baseline, while learning"
D15-1114,P09-1010,1,0.929137,"eneral-purpose knowledge about cooking. 1 Introduction Instructional language describes how to achieve a wide variety of goals, from traveling successfully to a desired location to cooking a particular dish for dinner. Despite the fact that such language is important to our everyday lives, there has been relatively little effort to design algorithms that can automatically convert it into an actionable form. Existing methods typically assume labeled training data (Lau et al., 2009; Maeta et al., 2015) or access to a physical simulator that can be used to test understanding of the instructions (Branavan et al., 2009; Chen and Mooney, 2011; Bollini et al., 2013). In this paper, we present the first approach for unsupervised learning to interpret instructional recipes using text alone, with application to cooking recipes. 1 The goal of representing common sense world knowledge about actions and objects also drives theories of frame semantics (Fillmore, 1982) and script knowledge (Schank and Abelson, 1977). However, our focus is on inducing this style of knowledge automatically from procedural texts. 982 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 982–992, c"
D15-1114,J93-2003,0,0.0652798,"ous actions: hi = (e1 , . . . , ei−1 ). The probability of a recipe R given a set of connections C can be factored by the chain rule: P (R|C) = Y sem P (Sij |tsyn ij , tij , C, hi ) = Part-composite model When the encompassing argument is a food and the origin is a previous verb sem = f ood, origin(sk ) 6= (i.e., P (skij |tsyn ij ij , tij 0, C, hi )), then the probability of the span depends on the ingredients that the span represents given the connections in C. For example, “dressing” is more likely given ingredients “oil” and “vinegar” than given “chicken” and “noodles”. We use IBM Model 1 (Brown et al., 1993) to model the probability of a composite destination phrase given a set of origin food tokens. Let f ood(skij , C) be the set of spans in food arguments such that there is a directed path from those arguments to skij . IBM Model 1 defines the probability of a span given the propagated food spans, P (skij |f ood(skij , C)).3 Given C and a history hi , we assume the verb and arguments of an action are independent: P (aij |C, hi ). j Since the set of connections deterministically defines a verb signature gi for a verb vi , we can simplify P (vi |C, hi ) to the multinomial distribution P (vi |gi )"
D15-1114,W15-2206,0,0.205752,"high quality action graphs, outperforming a strong sequential baseline by 8 points in F1, while also discovering general-purpose knowledge about cooking. 1 Introduction Instructional language describes how to achieve a wide variety of goals, from traveling successfully to a desired location to cooking a particular dish for dinner. Despite the fact that such language is important to our everyday lives, there has been relatively little effort to design algorithms that can automatically convert it into an actionable form. Existing methods typically assume labeled training data (Lau et al., 2009; Maeta et al., 2015) or access to a physical simulator that can be used to test understanding of the instructions (Branavan et al., 2009; Chen and Mooney, 2011; Bollini et al., 2013). In this paper, we present the first approach for unsupervised learning to interpret instructional recipes using text alone, with application to cooking recipes. 1 The goal of representing common sense world knowledge about actions and objects also drives theories of frame semantics (Fillmore, 1982) and script knowledge (Schank and Abelson, 1977). However, our focus is on inducing this style of knowledge automatically from procedural"
D15-1114,P09-1068,0,0.0086009,"(2009) develop models to interpret how-to instructions, but also assume supervision, and do not make connections between different actions. Data-driven extraction of cooking knowledge has been explored in the context of building a cooking ontology (Gaillard et al., 2012; Nanba et al., 2014). In contrast, our work induces probabilistic cooking knowledge as part of unsupervised learning process for understanding recipes. Cooking knowledge is also closely related to script knowledge, but most prior work focus on newswire and children’s books rather than procedural language (Fujiki et al., 2003; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Balasubramanian et al., 2013) or rely on crowdsourced descriptions to learn procedural knowledge (Regneri et al., 2010; Regneri et al., 2011; Frermann et al., 2014). There is work on related, but distinct, tasks that use recipes, including identifying actionable refinements from online recipe reviews (Druck and Pang, 2012) and extracting structured 10 Conclusion We presented unsupervised methods for segmenting and identifying latent connections among actions in recipe text. Our model outperformed a strong linear baseline, while learning a variety of domain knowledg"
D15-1114,W14-2407,0,0.141997,"breast, vegetables, noodles pumpkin, mixture, pie, filling, temperature, seeds, mash, oven, crust, dough banana, mixture, batter, muffin, bread, egg, wet, cup, ingredients, slice Table 3: Examples of ingredients with their top inferred composite words. 9 Related work information from ingredient lists (Greene, 2015) Cooking recipes have also been studied in the context of grounded language learning, e.g., to build robots that can cook (e.g., Bollini et al., 2013, Beetz et al., 2011), or to align cooking videos to natural language descriptions of actions (Regneri et al., 2013) or recipe texts (Malmaud et al., 2014; Malmaud et al., 2015). Our work complements these efforts by recovering fine-grained procedural semantics from text alone. Finally, detection and resolution of implicit arguments is an instance of zero anaphora detection and resolution (Silberer and Anette, 2012, Tetreault 2002, Whittemore et al., 1991, Palmer et al., 1986). We present an empirical approach for understanding these phenomena in instructions. Our work relates to a substantial body of research that transforms natural language instructions into actionable plans (Artzi and Zettlemoyer, 2013, Chen and Mooney, 2011, Branavan et al."
D15-1114,N15-1015,0,0.0445054,"oodles pumpkin, mixture, pie, filling, temperature, seeds, mash, oven, crust, dough banana, mixture, batter, muffin, bread, egg, wet, cup, ingredients, slice Table 3: Examples of ingredients with their top inferred composite words. 9 Related work information from ingredient lists (Greene, 2015) Cooking recipes have also been studied in the context of grounded language learning, e.g., to build robots that can cook (e.g., Bollini et al., 2013, Beetz et al., 2011), or to align cooking videos to natural language descriptions of actions (Regneri et al., 2013) or recipe texts (Malmaud et al., 2014; Malmaud et al., 2015). Our work complements these efforts by recovering fine-grained procedural semantics from text alone. Finally, detection and resolution of implicit arguments is an instance of zero anaphora detection and resolution (Silberer and Anette, 2012, Tetreault 2002, Whittemore et al., 1991, Palmer et al., 1986). We present an empirical approach for understanding these phenomena in instructions. Our work relates to a substantial body of research that transforms natural language instructions into actionable plans (Artzi and Zettlemoyer, 2013, Chen and Mooney, 2011, Branavan et al., 2011, Branavan et al."
D15-1114,P12-1057,0,0.0228171,"part of unsupervised learning process for understanding recipes. Cooking knowledge is also closely related to script knowledge, but most prior work focus on newswire and children’s books rather than procedural language (Fujiki et al., 2003; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Balasubramanian et al., 2013) or rely on crowdsourced descriptions to learn procedural knowledge (Regneri et al., 2010; Regneri et al., 2011; Frermann et al., 2014). There is work on related, but distinct, tasks that use recipes, including identifying actionable refinements from online recipe reviews (Druck and Pang, 2012) and extracting structured 10 Conclusion We presented unsupervised methods for segmenting and identifying latent connections among actions in recipe text. Our model outperformed a strong linear baseline, while learning a variety of domain knowledge, such as verb signatures and probable ingredient components for different composites. Future work includes learning a more comprehensive model of locations (e.g., identifying nested locations such as an oven and a pan in the oven), enriching action graphs with greater semantic coverage (e.g., durations, tools, amounts), and training and evaluating o"
D15-1114,mori-etal-2014-flow,0,0.0887091,"tions. Our work relates to a substantial body of research that transforms natural language instructions into actionable plans (Artzi and Zettlemoyer, 2013, Chen and Mooney, 2011, Branavan et al., 2011, Branavan et al., 2009, McMahon et al., 2006). Most of these approaches do interactive learning in virtual environments or simulations, while we learn from the redundancy seen in the text of different instances of similar recipes. There is also significant related work on supervised learning for instructions. A recent series of studies have explored parsing of cooking recipes (Mori et al., 2012; Mori et al., 2014; Maeta et al., 2015). However, they assume annotated data, study Japanese recipes, and make edge connections independently without taking into account the flow of ingredients. Tasse and Smith (2008) develops annotation for English recipes, but do not mark connections from implicit roles, and only studied segmentation models. Lau et al. (2009) develop models to interpret how-to instructions, but also assume supervision, and do not make connections between different actions. Data-driven extraction of cooking knowledge has been explored in the context of building a cooking ontology (Gaillard et"
D15-1114,P91-1003,0,0.48446,"(Greene, 2015) Cooking recipes have also been studied in the context of grounded language learning, e.g., to build robots that can cook (e.g., Bollini et al., 2013, Beetz et al., 2011), or to align cooking videos to natural language descriptions of actions (Regneri et al., 2013) or recipe texts (Malmaud et al., 2014; Malmaud et al., 2015). Our work complements these efforts by recovering fine-grained procedural semantics from text alone. Finally, detection and resolution of implicit arguments is an instance of zero anaphora detection and resolution (Silberer and Anette, 2012, Tetreault 2002, Whittemore et al., 1991, Palmer et al., 1986). We present an empirical approach for understanding these phenomena in instructions. Our work relates to a substantial body of research that transforms natural language instructions into actionable plans (Artzi and Zettlemoyer, 2013, Chen and Mooney, 2011, Branavan et al., 2011, Branavan et al., 2009, McMahon et al., 2006). Most of these approaches do interactive learning in virtual environments or simulations, while we learn from the redundancy seen in the text of different instances of similar recipes. There is also significant related work on supervised learning for i"
D15-1114,D14-1109,0,0.0607763,"Missing"
D15-1114,P86-1004,0,0.723823,"ecipes have also been studied in the context of grounded language learning, e.g., to build robots that can cook (e.g., Bollini et al., 2013, Beetz et al., 2011), or to align cooking videos to natural language descriptions of actions (Regneri et al., 2013) or recipe texts (Malmaud et al., 2014; Malmaud et al., 2015). Our work complements these efforts by recovering fine-grained procedural semantics from text alone. Finally, detection and resolution of implicit arguments is an instance of zero anaphora detection and resolution (Silberer and Anette, 2012, Tetreault 2002, Whittemore et al., 1991, Palmer et al., 1986). We present an empirical approach for understanding these phenomena in instructions. Our work relates to a substantial body of research that transforms natural language instructions into actionable plans (Artzi and Zettlemoyer, 2013, Chen and Mooney, 2011, Branavan et al., 2011, Branavan et al., 2009, McMahon et al., 2006). Most of these approaches do interactive learning in virtual environments or simulations, while we learn from the redundancy seen in the text of different instances of similar recipes. There is also significant related work on supervised learning for instructions. A recent"
D15-1114,E14-1024,0,0.0110329,"erpret how-to instructions, but also assume supervision, and do not make connections between different actions. Data-driven extraction of cooking knowledge has been explored in the context of building a cooking ontology (Gaillard et al., 2012; Nanba et al., 2014). In contrast, our work induces probabilistic cooking knowledge as part of unsupervised learning process for understanding recipes. Cooking knowledge is also closely related to script knowledge, but most prior work focus on newswire and children’s books rather than procedural language (Fujiki et al., 2003; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Balasubramanian et al., 2013) or rely on crowdsourced descriptions to learn procedural knowledge (Regneri et al., 2010; Regneri et al., 2011; Frermann et al., 2014). There is work on related, but distinct, tasks that use recipes, including identifying actionable refinements from online recipe reviews (Druck and Pang, 2012) and extracting structured 10 Conclusion We presented unsupervised methods for segmenting and identifying latent connections among actions in recipe text. Our model outperformed a strong linear baseline, while learning a variety of domain knowledge, such as verb signatures"
D15-1114,P10-1100,0,0.00888942,"xtraction of cooking knowledge has been explored in the context of building a cooking ontology (Gaillard et al., 2012; Nanba et al., 2014). In contrast, our work induces probabilistic cooking knowledge as part of unsupervised learning process for understanding recipes. Cooking knowledge is also closely related to script knowledge, but most prior work focus on newswire and children’s books rather than procedural language (Fujiki et al., 2003; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Balasubramanian et al., 2013) or rely on crowdsourced descriptions to learn procedural knowledge (Regneri et al., 2010; Regneri et al., 2011; Frermann et al., 2014). There is work on related, but distinct, tasks that use recipes, including identifying actionable refinements from online recipe reviews (Druck and Pang, 2012) and extracting structured 10 Conclusion We presented unsupervised methods for segmenting and identifying latent connections among actions in recipe text. Our model outperformed a strong linear baseline, while learning a variety of domain knowledge, such as verb signatures and probable ingredient components for different composites. Future work includes learning a more comprehensive model of"
D15-1114,R11-1064,0,0.0182553,"nowledge has been explored in the context of building a cooking ontology (Gaillard et al., 2012; Nanba et al., 2014). In contrast, our work induces probabilistic cooking knowledge as part of unsupervised learning process for understanding recipes. Cooking knowledge is also closely related to script knowledge, but most prior work focus on newswire and children’s books rather than procedural language (Fujiki et al., 2003; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Balasubramanian et al., 2013) or rely on crowdsourced descriptions to learn procedural knowledge (Regneri et al., 2010; Regneri et al., 2011; Frermann et al., 2014). There is work on related, but distinct, tasks that use recipes, including identifying actionable refinements from online recipe reviews (Druck and Pang, 2012) and extracting structured 10 Conclusion We presented unsupervised methods for segmenting and identifying latent connections among actions in recipe text. Our model outperformed a strong linear baseline, while learning a variety of domain knowledge, such as verb signatures and probable ingredient components for different composites. Future work includes learning a more comprehensive model of locations (e.g., iden"
D15-1114,Q13-1003,0,0.0563374,"re, salad, cook, dressing, pasta, soup, breast, vegetables, noodles pumpkin, mixture, pie, filling, temperature, seeds, mash, oven, crust, dough banana, mixture, batter, muffin, bread, egg, wet, cup, ingredients, slice Table 3: Examples of ingredients with their top inferred composite words. 9 Related work information from ingredient lists (Greene, 2015) Cooking recipes have also been studied in the context of grounded language learning, e.g., to build robots that can cook (e.g., Bollini et al., 2013, Beetz et al., 2011), or to align cooking videos to natural language descriptions of actions (Regneri et al., 2013) or recipe texts (Malmaud et al., 2014; Malmaud et al., 2015). Our work complements these efforts by recovering fine-grained procedural semantics from text alone. Finally, detection and resolution of implicit arguments is an instance of zero anaphora detection and resolution (Silberer and Anette, 2012, Tetreault 2002, Whittemore et al., 1991, Palmer et al., 1986). We present an empirical approach for understanding these phenomena in instructions. Our work relates to a substantial body of research that transforms natural language instructions into actionable plans (Artzi and Zettlemoyer, 2013,"
D15-1114,H86-1011,0,\N,Missing
D15-1114,S12-1001,0,\N,Missing
D15-1189,S13-2012,0,0.0183845,"Missing"
D15-1189,de-marneffe-etal-2006-generating,0,0.0489017,"Missing"
D15-1189,J12-2003,0,0.186647,"Missing"
D15-1189,W09-3012,0,0.0812391,"d in several prior studies, existing approaches vary in how they model various linguistic forms such as nominal events, stative events, generic events, and light verbs (Pustejovsky et al., 2003; Palmer et al., 2005; Meyers et al., 2004; Kim et al., 2009; Song et al., 2015). Even with a formal and precise account of events, training annotators to learn all such linguistic intricacies remains a practical challenge. Instead of definition-driven instructions, we propose example-driven instructions and show their effectiveness. Previous studies have modeled event factuality assessment as a binary (Diab et al., 2009; Prabhakaran et al., 2010) or multi-class (Sauri and Pustejovsky, 2009) classification task, and they relied on expert annotators. A softer representation was proposed and crowdsourced by de Marneffe et al. (2012), who advocated for representing factuality from the reader’s perspective as a distribution of categories, but their annotation process requires manual normalization of the text. In contrast, we model factuality from the author’s perspective with scalar values, and we have an endto-end crowdsourced annotation pipeline. More recently, Soni et al. (2014) investigated a related problem"
D15-1189,W14-3333,0,0.0211697,"tive using our features, and (3) a regression model (P RABHAKARAN) trained with the standard SVR objective using features from Prabhakaran et al. (2010). These features are highly informative, but their lexical features are restricted to a small set of manually defined words. Results Detection Results Figure 5 shows development and test results for detection event mentions.5 We see a small drop in precision and large gains in recall, but a significant increase in F1, primarily 4 Experimental Setup http://tiny.cc/cplex We performed two-sided bootstrap resampling statistical significance tests (Graham et al., 2014). In Figures 5 and 6, asterisks indicate that the difference from the best system is statistically significant (p &lt; 0.05). 5 Baselines For detection, we include a baseline reimplementation of the NAVY T IME (Chambers, 1646 Model Our system NAVY T IME P Dev. R F1 P Test R Error type Missed lexical cue (unseen in training) Missed lexical cue (seen in training) Long distance inference World knowledge & pragmatics Annotation error F1 90.1 90.9 90.5 85.5* 87.8 86.6 84.7* 79.6* 82.1* 87.7 78.3* 82.7* Figure 5: Results for the detection task. Model Dev. MAE r Test MAE r Our system SVR D ISCRETE P RAB"
D15-1189,W09-1401,0,0.0178809,"that combines the advantages of LASSO and support vector regression, enabling it to effectively consider sparse lexical cues. By providing scalar factuality judgments for events, our models enable more fine-grained reasoning than previously considered. The corpus and learned models are available online.1 2 Related Work While event definitions have been proposed in several prior studies, existing approaches vary in how they model various linguistic forms such as nominal events, stative events, generic events, and light verbs (Pustejovsky et al., 2003; Palmer et al., 2005; Meyers et al., 2004; Kim et al., 2009; Song et al., 2015). Even with a formal and precise account of events, training annotators to learn all such linguistic intricacies remains a practical challenge. Instead of definition-driven instructions, we propose example-driven instructions and show their effectiveness. Previous studies have modeled event factuality assessment as a binary (Diab et al., 2009; Prabhakaran et al., 2010) or multi-class (Sauri and Pustejovsky, 2009) classification task, and they relied on expert annotators. A softer representation was proposed and crowdsourced by de Marneffe et al. (2012), who advocated for re"
D15-1189,W04-2705,0,0.0141944,"a learning objective that combines the advantages of LASSO and support vector regression, enabling it to effectively consider sparse lexical cues. By providing scalar factuality judgments for events, our models enable more fine-grained reasoning than previously considered. The corpus and learned models are available online.1 2 Related Work While event definitions have been proposed in several prior studies, existing approaches vary in how they model various linguistic forms such as nominal events, stative events, generic events, and light verbs (Pustejovsky et al., 2003; Palmer et al., 2005; Meyers et al., 2004; Kim et al., 2009; Song et al., 2015). Even with a formal and precise account of events, training annotators to learn all such linguistic intricacies remains a practical challenge. Instead of definition-driven instructions, we propose example-driven instructions and show their effectiveness. Previous studies have modeled event factuality assessment as a binary (Diab et al., 2009; Prabhakaran et al., 2010) or multi-class (Sauri and Pustejovsky, 2009) classification task, and they relied on expert annotators. A softer representation was proposed and crowdsourced by de Marneffe et al. (2012), wh"
D15-1189,D08-1027,0,0.0555857,"Missing"
D15-1189,W15-0812,0,0.0187521,"advantages of LASSO and support vector regression, enabling it to effectively consider sparse lexical cues. By providing scalar factuality judgments for events, our models enable more fine-grained reasoning than previously considered. The corpus and learned models are available online.1 2 Related Work While event definitions have been proposed in several prior studies, existing approaches vary in how they model various linguistic forms such as nominal events, stative events, generic events, and light verbs (Pustejovsky et al., 2003; Palmer et al., 2005; Meyers et al., 2004; Kim et al., 2009; Song et al., 2015). Even with a formal and precise account of events, training annotators to learn all such linguistic intricacies remains a practical challenge. Instead of definition-driven instructions, we propose example-driven instructions and show their effectiveness. Previous studies have modeled event factuality assessment as a binary (Diab et al., 2009; Prabhakaran et al., 2010) or multi-class (Sauri and Pustejovsky, 2009) classification task, and they relied on expert annotators. A softer representation was proposed and crowdsourced by de Marneffe et al. (2012), who advocated for representing factualit"
D15-1189,P14-2068,0,0.0832869,"ctuality assessment as a binary (Diab et al., 2009; Prabhakaran et al., 2010) or multi-class (Sauri and Pustejovsky, 2009) classification task, and they relied on expert annotators. A softer representation was proposed and crowdsourced by de Marneffe et al. (2012), who advocated for representing factuality from the reader’s perspective as a distribution of categories, but their annotation process requires manual normalization of the text. In contrast, we model factuality from the author’s perspective with scalar values, and we have an endto-end crowdsourced annotation pipeline. More recently, Soni et al. (2014) investigated a related problem for quoted statements on Twitter, and they also crowdsourced factuality annotations to learn regression models. While their approach is similar, we focus on predicting factuality for events that occur in every sentence. Without the restrictions of their task, we must reason about a larger variety of contextual cues. Our method of evaluating annotator agreement (Section 3) is related to the crowdsourcing study by Snow et al. (2008), who showed that pooled non-experts can match or outperform single expert annotators. In contrast, we approximate expert judgments by"
D15-1189,P10-1040,0,0.0156029,"[neg]—h∗i—[xcomp]→h∗i h∗i←[neg]—h∗i—[xcomp]→h∗i Implementation Details The SVM models (NAVY T IME, D ISCRETE, SVR, P RABHAKARAN, and our detection model) were trained with SVMLight (Joachims, 1999). We use CPLEX4 to solve the linear program optimizing the regression objective in Section 4. All hyperparameters were tuned on the development set. We use the Stanford dependency parser (de Marneffe et al., 2006) for extracting dependency path and part-of-speech features. We use WordNet (Miller, 1995) to generate lemma and hyponym features. Brown clusters with 100, 320, 1000, and 3200 clusters from Turian et al. (2010) are used in the detection features. Evaluation Metrics We use the standard F1 score for the evaluation of detection. For event factuality, we report two metrics, the mean absolute error (MAE) relative to the gold standard labels and Pearson’s correlation coefficient. While MAE is an intuitive metric that evaluates the absolute fit of the model, Pearson’s r better captures how well a system is able to recover the variation of the annotations. Pearson’s r is also conveniently normalized such that r = 0 for a system that blindly chooses the best a priori output and r = 1 for a system that makes"
D15-1189,S13-2001,0,0.016869,"describing things that the author claims could have happened, and rate each possibility on a scale of -3 (certainly did not happen) to 3 (certainly did). Figure 1 shows that non-expert workers—when their judgments are aggregated—consistently find a wide range of events and recognize the subtle differences in implied factuality. For example, the event set gets a score of 2.6, indicating that it likely but not certainly occurred, since it was ordered, whereas the ordered event, gets a score of 3.0. We gather data for event detection and factuality, reusing sentences from the TempEval-3 corpus (Uzzaman et al., 2013). Our approach produces high-quality labels with modest costs. We also introduce simple but highly effective models for both tasks that outperform strong baselines. In 1643 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1643–1648, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. particular, our factuality regression model uses a learning objective that combines the advantages of LASSO and support vector regression, enabling it to effectively consider sparse lexical cues. By providing scalar factuality judgm"
D15-1189,J05-1004,0,0.0227027,"regression model uses a learning objective that combines the advantages of LASSO and support vector regression, enabling it to effectively consider sparse lexical cues. By providing scalar factuality judgments for events, our models enable more fine-grained reasoning than previously considered. The corpus and learned models are available online.1 2 Related Work While event definitions have been proposed in several prior studies, existing approaches vary in how they model various linguistic forms such as nominal events, stative events, generic events, and light verbs (Pustejovsky et al., 2003; Palmer et al., 2005; Meyers et al., 2004; Kim et al., 2009; Song et al., 2015). Even with a formal and precise account of events, training annotators to learn all such linguistic intricacies remains a practical challenge. Instead of definition-driven instructions, we propose example-driven instructions and show their effectiveness. Previous studies have modeled event factuality assessment as a binary (Diab et al., 2009; Prabhakaran et al., 2010) or multi-class (Sauri and Pustejovsky, 2009) classification task, and they relied on expert annotators. A softer representation was proposed and crowdsourced by de Marne"
D15-1189,C10-2117,0,0.0669716,"studies, existing approaches vary in how they model various linguistic forms such as nominal events, stative events, generic events, and light verbs (Pustejovsky et al., 2003; Palmer et al., 2005; Meyers et al., 2004; Kim et al., 2009; Song et al., 2015). Even with a formal and precise account of events, training annotators to learn all such linguistic intricacies remains a practical challenge. Instead of definition-driven instructions, we propose example-driven instructions and show their effectiveness. Previous studies have modeled event factuality assessment as a binary (Diab et al., 2009; Prabhakaran et al., 2010) or multi-class (Sauri and Pustejovsky, 2009) classification task, and they relied on expert annotators. A softer representation was proposed and crowdsourced by de Marneffe et al. (2012), who advocated for representing factuality from the reader’s perspective as a distribution of categories, but their annotation process requires manual normalization of the text. In contrast, we model factuality from the author’s perspective with scalar values, and we have an endto-end crowdsourced annotation pipeline. More recently, Soni et al. (2014) investigated a related problem for quoted statements on Tw"
D16-1032,D10-1049,0,0.00992082,"hidden state classifier + st probability of using new item ot x αused t Et σ new αt ref-type(ht) qt ht xt g new Et available items Figure 2: A diagram of the neural checklist model. The bottom portion depicts how the model generates the output embedding ot . The top portion shows how the checklist and available/used agenda item matrices are updated. in natural language text (i.e., how to say it) (Thompson, 1977; Reiter and Dale, 2000). More recently, machine learning methods have focused on parts of this approach (Barzilay and Lapata, 2005; Liang et al., 2009) or the full two-stage approach (Angeli et al., 2010; Konstas and Lapata, 2013). Most of these models shorter texts, although Mori et al. (2014) did consider longer cooking recipes. Our approach is a joint model that instead operates with textual input and tries to cover all of the content it is given. 4 Model Fig. 2 shows a graphical representation of the neural checklist model. At a high level, our model uses a recurrent neural network (RNN) language model that encodes the goal as a bag-of-words and then generates output text token by token. It additionally stores a vector that acts as a soft checklist of what agenda items have been used so f"
D16-1032,D13-1178,0,0.03676,"use of items on the agenda. For example, in the cooking recipe domain, the goal is the recipe title (“pico de gallo” in Fig. 1), and the agenda is the ingredient list (e.g., “lime,” “salt”). For dialogue systems, the goal is the dialogue type (e.g., inform or query) and the agenda contains information to be mentioned (e.g., a hotel name and address). For example, if g =“inform” and E = {name(Hotel Stratford), has internet(no)}, an output text might be x =“Hotel Stratford does not have internet.” 330 3 Related Work Attention models have been used for many NLP tasks such as machine translation (Balasubramanian et al., 2013; Bahdanau et al., 2014), abstractive sentence summarization (Rush et al., 2015), machine reading (Cheng et al., 2016), and image caption generation (Xu et al., 2015). Our model uses new types of attention to record what has been said and to select new agenda items to be referenced. Recently, other researchers have developed new ways to use attention mechanisms for related generation challenges. Most closely related, Wen et al. (2015) and Wen et al. (2016) present neural network models for generating dialogue system responses given a set of agenda items. They focus on generating short texts (1"
D16-1032,H05-1042,0,0.182645,"a new available Et x items new used used items ft 2 E agenda Generate output ft hidden state classifier + st probability of using new item ot x αused t Et σ new αt ref-type(ht) qt ht xt g new Et available items Figure 2: A diagram of the neural checklist model. The bottom portion depicts how the model generates the output embedding ot . The top portion shows how the checklist and available/used agenda item matrices are updated. in natural language text (i.e., how to say it) (Thompson, 1977; Reiter and Dale, 2000). More recently, machine learning methods have focused on parts of this approach (Barzilay and Lapata, 2005; Liang et al., 2009) or the full two-stage approach (Angeli et al., 2010; Konstas and Lapata, 2013). Most of these models shorter texts, although Mori et al. (2014) did consider longer cooking recipes. Our approach is a joint model that instead operates with textual input and tries to cover all of the content it is given. 4 Model Fig. 2 shows a graphical representation of the neural checklist model. At a high level, our model uses a recurrent neural network (RNN) language model that encodes the goal as a bag-of-words and then generates output text token by token. It additionally stores a vect"
D16-1032,D16-1053,0,0.0101665,"Missing"
D16-1032,D14-1179,0,0.00991178,"Missing"
D16-1032,W14-3348,0,0.00509666,"oes not contain new items. We also report the performance of our checklist model without the additional weak supervision of heuristic ingredient references (- no supervision) (see Sec. 4.6).7 we also evaluate two ablations of our checklist model on the recipe task. First, we remove the linear interpolation and instead use ht as the output (see Sec. 4.2). Second, we remove the previously used item reference model by changing ref -type() to a 2-way classifier between new ingredient references and all other tokens (see Sec. 4.4). Metrics We include commonly used metrics like BLEU-4,8 and METEOR (Denkowski and Lavie, 2014). Because neither of these metrics can measure how well the generated recipe follows the input goal and the agenda, we also define two additional metrics. The first measures the percentage of the agenda items corrected used, while the second measures the number of extraneous items incorrectly introduced. Both these metrics are computed based on simple string match and can miss certain referring expressions (e.g., “meat” to refer to “pork”). Because of the approximate nature of these automated metrics, we also report a human evaluation. 6 Recipe generation results Fig. 1 results for recipe gene"
D16-1032,P16-1014,0,0.0626654,"Missing"
D16-1032,P16-1002,0,0.0169785,"possible agenda items. Our model composes substantially longer texts, such as recipes, with a more varied and open ended set of possible agenda items. We also compare performance for our model on their data. Maintaining coherence and avoiding duplication have been recurring challenges when generating text using RNNs for other applications, including image captioning (Jia et al., 2015; Xu et al., 2015) and machine translation (Tu et al., 2016b; Tu et al., 2016a). A variety of solutions have been developed to address infrequent or out-of-vocabulary words in particular (G¨ulc¸ehre et al., 2016; Jia and Liang, 2016). Instead of directly copying input words or deterministically selecting output, our model can learn how to generate them (e.g., it might prefer to produce the word “steaks” when the original recipe ingredient was “ribeyes”). Finally, recent work in machine translation models has introduced new training objectives to encourage attention to all input words (Luong et al., 2015), but these models do not accumulate attention while decoding. Generating recipes was an early task in planning (Hammond, 1986) and generating referring expression research (Dale, 1988). These can be seen as key steps in c"
D16-1032,P09-1011,0,0.296248,"new used used items ft 2 E agenda Generate output ft hidden state classifier + st probability of using new item ot x αused t Et σ new αt ref-type(ht) qt ht xt g new Et available items Figure 2: A diagram of the neural checklist model. The bottom portion depicts how the model generates the output embedding ot . The top portion shows how the checklist and available/used agenda item matrices are updated. in natural language text (i.e., how to say it) (Thompson, 1977; Reiter and Dale, 2000). More recently, machine learning methods have focused on parts of this approach (Barzilay and Lapata, 2005; Liang et al., 2009) or the full two-stage approach (Angeli et al., 2010; Konstas and Lapata, 2013). Most of these models shorter texts, although Mori et al. (2014) did consider longer cooking recipes. Our approach is a joint model that instead operates with textual input and tries to cover all of the content it is given. 4 Model Fig. 2 shows a graphical representation of the neural checklist model. At a high level, our model uses a recurrent neural network (RNN) language model that encodes the goal as a bag-of-words and then generates output text token by token. It additionally stores a vector that acts as a sof"
D16-1032,D15-1166,0,0.0201846,"Missing"
D16-1032,N16-1086,0,0.0942945,"t dashed column) tracks which agenda items (top boxes; “salt,” “lime,” etc.) have already been used (checked boxes). The model is trained to interpolate an RNN (e.g., encode “pico de gallo” and decode a recipe) with attention models over new (left column) and used (middle column) items that identify likely items for each time step (shaded boxes; “tomatoes,” etc.). Introduction Recurrent neural network (RNN) architectures have proven to be well suited for many natural language generation tasks (Mikolov et al., 2010; Mikolov et al., 2011; Sordoni et al., 2015; Xu et al., 2015; Wen et al., 2015; Mei et al., 2016). Previous neural generation models typically generate locally coherent language that is on topic; however, overall they can miss information that should have been introduced or introduce duplicated or superfluous content. These errors are particularly common in situations where there are multiple distinct sources of input or the length of the output text is sufficiently long. In this paper, we present a new recurrent neural model that maintains coherence while improving coverage by globally tracking what has been said and what is still left to be said in complete texts. For example, consider"
D16-1032,D15-1044,0,0.0520666,"ipe title (“pico de gallo” in Fig. 1), and the agenda is the ingredient list (e.g., “lime,” “salt”). For dialogue systems, the goal is the dialogue type (e.g., inform or query) and the agenda contains information to be mentioned (e.g., a hotel name and address). For example, if g =“inform” and E = {name(Hotel Stratford), has internet(no)}, an output text might be x =“Hotel Stratford does not have internet.” 330 3 Related Work Attention models have been used for many NLP tasks such as machine translation (Balasubramanian et al., 2013; Bahdanau et al., 2014), abstractive sentence summarization (Rush et al., 2015), machine reading (Cheng et al., 2016), and image caption generation (Xu et al., 2015). Our model uses new types of attention to record what has been said and to select new agenda items to be referenced. Recently, other researchers have developed new ways to use attention mechanisms for related generation challenges. Most closely related, Wen et al. (2015) and Wen et al. (2016) present neural network models for generating dialogue system responses given a set of agenda items. They focus on generating short texts (1-2 sentences) in a relatively small vocabulary setting and assume a fixed set of"
D16-1032,N15-1020,0,0.0136184,"1: Example checklist recipe generation. A checklist (right dashed column) tracks which agenda items (top boxes; “salt,” “lime,” etc.) have already been used (checked boxes). The model is trained to interpolate an RNN (e.g., encode “pico de gallo” and decode a recipe) with attention models over new (left column) and used (middle column) items that identify likely items for each time step (shaded boxes; “tomatoes,” etc.). Introduction Recurrent neural network (RNN) architectures have proven to be well suited for many natural language generation tasks (Mikolov et al., 2010; Mikolov et al., 2011; Sordoni et al., 2015; Xu et al., 2015; Wen et al., 2015; Mei et al., 2016). Previous neural generation models typically generate locally coherent language that is on topic; however, overall they can miss information that should have been introduced or introduce duplicated or superfluous content. These errors are particularly common in situations where there are multiple distinct sources of input or the length of the output text is sufficiently long. In this paper, we present a new recurrent neural model that maintains coherence while improving coverage by globally tracking what has been said and what is still lef"
D16-1032,P16-5005,0,0.0158182,"gue system responses given a set of agenda items. They focus on generating short texts (1-2 sentences) in a relatively small vocabulary setting and assume a fixed set of possible agenda items. Our model composes substantially longer texts, such as recipes, with a more varied and open ended set of possible agenda items. We also compare performance for our model on their data. Maintaining coherence and avoiding duplication have been recurring challenges when generating text using RNNs for other applications, including image captioning (Jia et al., 2015; Xu et al., 2015) and machine translation (Tu et al., 2016b; Tu et al., 2016a). A variety of solutions have been developed to address infrequent or out-of-vocabulary words in particular (G¨ulc¸ehre et al., 2016; Jia and Liang, 2016). Instead of directly copying input words or deterministically selecting output, our model can learn how to generate them (e.g., it might prefer to produce the word “steaks” when the original recipe ingredient was “ribeyes”). Finally, recent work in machine translation models has introduced new training objectives to encourage attention to all input words (Luong et al., 2015), but these models do not accumulate attention w"
D16-1032,P16-1008,0,0.0116556,"gue system responses given a set of agenda items. They focus on generating short texts (1-2 sentences) in a relatively small vocabulary setting and assume a fixed set of possible agenda items. Our model composes substantially longer texts, such as recipes, with a more varied and open ended set of possible agenda items. We also compare performance for our model on their data. Maintaining coherence and avoiding duplication have been recurring challenges when generating text using RNNs for other applications, including image captioning (Jia et al., 2015; Xu et al., 2015) and machine translation (Tu et al., 2016b; Tu et al., 2016a). A variety of solutions have been developed to address infrequent or out-of-vocabulary words in particular (G¨ulc¸ehre et al., 2016; Jia and Liang, 2016). Instead of directly copying input words or deterministically selecting output, our model can learn how to generate them (e.g., it might prefer to produce the word “steaks” when the original recipe ingredient was “ribeyes”). Finally, recent work in machine translation models has introduced new training objectives to encourage attention to all input words (Luong et al., 2015), but these models do not accumulate attention w"
D16-1032,D15-1199,0,0.638511,"A checklist (right dashed column) tracks which agenda items (top boxes; “salt,” “lime,” etc.) have already been used (checked boxes). The model is trained to interpolate an RNN (e.g., encode “pico de gallo” and decode a recipe) with attention models over new (left column) and used (middle column) items that identify likely items for each time step (shaded boxes; “tomatoes,” etc.). Introduction Recurrent neural network (RNN) architectures have proven to be well suited for many natural language generation tasks (Mikolov et al., 2010; Mikolov et al., 2011; Sordoni et al., 2015; Xu et al., 2015; Wen et al., 2015; Mei et al., 2016). Previous neural generation models typically generate locally coherent language that is on topic; however, overall they can miss information that should have been introduced or introduce duplicated or superfluous content. These errors are particularly common in situations where there are multiple distinct sources of input or the length of the output text is sufficiently long. In this paper, we present a new recurrent neural model that maintains coherence while improving coverage by globally tracking what has been said and what is still left to be said in complete texts. For"
D16-1032,N16-1015,0,0.013667,"ford does not have internet.” 330 3 Related Work Attention models have been used for many NLP tasks such as machine translation (Balasubramanian et al., 2013; Bahdanau et al., 2014), abstractive sentence summarization (Rush et al., 2015), machine reading (Cheng et al., 2016), and image caption generation (Xu et al., 2015). Our model uses new types of attention to record what has been said and to select new agenda items to be referenced. Recently, other researchers have developed new ways to use attention mechanisms for related generation challenges. Most closely related, Wen et al. (2015) and Wen et al. (2016) present neural network models for generating dialogue system responses given a set of agenda items. They focus on generating short texts (1-2 sentences) in a relatively small vocabulary setting and assume a fixed set of possible agenda items. Our model composes substantially longer texts, such as recipes, with a more varied and open ended set of possible agenda items. We also compare performance for our model on their data. Maintaining coherence and avoiding duplication have been recurring challenges when generating text using RNNs for other applications, including image captioning (Jia et al"
D16-1032,Q17-1007,0,\N,Missing
D16-1126,W13-3503,0,0.0236824,"r hypernyms in WordNet (Miller, 1995). For example, while Banerjee and Pedersen (2003) use WordNet to assign a 1.0 similarity score between car and automobile, they only give a 0.3 similarity between car and gasoline. A second method is to use pointwise mutual information (PMI). Let t be the topic/phrase, and let w be a candidate related word. We collect a set of sentences S that contain t, and sort candidates by Proportion of sentences in S containing w P(w) in general text Table 2 shows that PMI has a tendency to assign a high score to low frequency words (Bouma, 2009; Role and Nadif, 2011; Damani, 2013). 1185 A third method is word2vec (Mikolov et al., 2013a), which provides distributed word representations. We train a continuous-bag-of-words model3 with window size 8 and 40 and word vector dimension 200. We score candidate related words/phrases with cosine to topic-word vector. We find that a larger window size works best (Pennington et al., 2014; Levy and Goldberg, 2014). Table 2 shows examples. The training corpus for word2vec has a crucial effect on the quality of the related words. We train word2vec models on the English Gigaword corpus,4 a song lyrics corpus, and the first billion char"
D16-1126,N15-1180,1,0.838641,"e machinery with deep learning, guaranteeing formal correctness of our poems, while gaining coherence of long1184 distance RNNs. • By using words related to the user’s topic as rhyme words, we design a system that can generate poems with topical coherence. This allows us to generate longer topical poems. • We extend our method to other poetry formats and languages. 3 Vocabulary To generate a line of iambic pentameter poetry, we arrange words to form a sequence of ten syllables alternating between stressed and unstressed. For example: 010 1 0 10 101 Attending on his golden pilgramage Following Ghazvininejad and Knight (2015), we refer to unstressed syllables with 0 and stressed syllables with 1, so that the form of a Shakespearean sonnet is ((01)5 )14 . To get stress patterns for individual words, we use CMU pronunciation dictionary,2 collapsing primary and secondary stresses. For example: CAFETERIA K AE2 F AH0 T IH1 R IY0 AH0 becomes CAFETERIA 10100 The first two columns of Table 1 show other examples. From the 125,074 CMU dictionary word types, we can actually only use words whose stress pattern matches the iambic pattern (alternating 1s and 0s). However, we make an exception for words that end in ...100 (such"
D16-1126,D10-1051,1,0.956494,"d Chun, 2008; Jiang and Zhou, 2008; Netzer et al., 2009). Recent work attempts to solve this problem by applying grammatical and semantic templates (Oliveira, 2009; Oliveira, 2012), or by modeling the task as statistical machine translation, in which each line is a “translation” of the previous line (Zhou et al., 2009; He et al., 2012). Yan et al. (2013) proposes a method based on summarization techniques for poem generation, retrieving candidate sentences from a large corpus of poems based on a user’s query and clustering the constituent terms, summarizing each cluster into a line of a poem. Greene et al. (2010) use unsupervised learning to estimate the stress patterns of words in a poetry corpus, then use these in a finite-state network to generate short English love poems. Several deep learning methods have recently been proposed for generating poems. Zhang and Lapata (2014) use an RNN model to generate 4-line Chinese poems. They force the decoder to rhyme the second and fourth lines, trusting the RNN to control rhythm. Yi et al. (2016) also propose an attentionbased bidirectional RNN model for generating 4line Chinese poems. The only such work which tries to generate longer poems is from Wang et a"
D16-1126,C08-1048,0,0.296685,"tions 3-7 describe how we address these tasks. After this, we show results of Hafez generating 14line classical sonnets with rhyme scheme ABAB CDCD EFEF GG, written in iambic pentameter (ten syllables per line with alternating stress: “da-DUM da-DUM da-DUM . . . ”). We then show experiments on Hafez’s parameters and conclude by showing the generality of the approach with respect to language and poetic form. 2 Prior Work Automated poem generation has been a popular but challenging research topic (Manurung et al., 2000; Gervas, 2001; Diaz-Agudo et al., 2002; Manurung, 2003; Wong and Chun, 2008; Jiang and Zhou, 2008; Netzer et al., 2009). Recent work attempts to solve this problem by applying grammatical and semantic templates (Oliveira, 2009; Oliveira, 2012), or by modeling the task as statistical machine translation, in which each line is a “translation” of the previous line (Zhou et al., 2009; He et al., 2012). Yan et al. (2013) proposes a method based on summarization techniques for poem generation, retrieving candidate sentences from a large corpus of poems based on a user’s query and clustering the constituent terms, summarizing each cluster into a line of a poem. Greene et al. (2010) use unsupervi"
D16-1126,P14-2050,0,0.0319958,"sentences S that contain t, and sort candidates by Proportion of sentences in S containing w P(w) in general text Table 2 shows that PMI has a tendency to assign a high score to low frequency words (Bouma, 2009; Role and Nadif, 2011; Damani, 2013). 1185 A third method is word2vec (Mikolov et al., 2013a), which provides distributed word representations. We train a continuous-bag-of-words model3 with window size 8 and 40 and word vector dimension 200. We score candidate related words/phrases with cosine to topic-word vector. We find that a larger window size works best (Pennington et al., 2014; Levy and Goldberg, 2014). Table 2 shows examples. The training corpus for word2vec has a crucial effect on the quality of the related words. We train word2vec models on the English Gigaword corpus,4 a song lyrics corpus, and the first billion characters from Wikipedia.5 The Gigaword corpus produces related words that are too newsy, while the song lyrics corpus does not cover enough topics. Hence, we train on Wikipedia. To obtain related phrases as well as words, we apply the method of Mikolov et al. (2013b) to the Wikipedia corpus, which replaces collocations like Los Angeles with single tokens like Los Angeles. Word"
D16-1126,W09-2005,0,0.0271403,"we address these tasks. After this, we show results of Hafez generating 14line classical sonnets with rhyme scheme ABAB CDCD EFEF GG, written in iambic pentameter (ten syllables per line with alternating stress: “da-DUM da-DUM da-DUM . . . ”). We then show experiments on Hafez’s parameters and conclude by showing the generality of the approach with respect to language and poetic form. 2 Prior Work Automated poem generation has been a popular but challenging research topic (Manurung et al., 2000; Gervas, 2001; Diaz-Agudo et al., 2002; Manurung, 2003; Wong and Chun, 2008; Jiang and Zhou, 2008; Netzer et al., 2009). Recent work attempts to solve this problem by applying grammatical and semantic templates (Oliveira, 2009; Oliveira, 2012), or by modeling the task as statistical machine translation, in which each line is a “translation” of the previous line (Zhou et al., 2009; He et al., 2012). Yan et al. (2013) proposes a method based on summarization techniques for poem generation, retrieving candidate sentences from a large corpus of poems based on a user’s query and clustering the constituent terms, summarizing each cluster into a line of a poem. Greene et al. (2010) use unsupervised learning to estima"
D16-1126,D14-1162,0,0.114004,"ord. We collect a set of sentences S that contain t, and sort candidates by Proportion of sentences in S containing w P(w) in general text Table 2 shows that PMI has a tendency to assign a high score to low frequency words (Bouma, 2009; Role and Nadif, 2011; Damani, 2013). 1185 A third method is word2vec (Mikolov et al., 2013a), which provides distributed word representations. We train a continuous-bag-of-words model3 with window size 8 and 40 and word vector dimension 200. We score candidate related words/phrases with cosine to topic-word vector. We find that a larger window size works best (Pennington et al., 2014; Levy and Goldberg, 2014). Table 2 shows examples. The training corpus for word2vec has a crucial effect on the quality of the related words. We train word2vec models on the English Gigaword corpus,4 a song lyrics corpus, and the first billion characters from Wikipedia.5 The Gigaword corpus produces related words that are too newsy, while the song lyrics corpus does not cover enough topics. Hence, we train on Wikipedia. To obtain related phrases as well as words, we apply the method of Mikolov et al. (2013b) to the Wikipedia corpus, which replaces collocations like Los Angeles with single tok"
D16-1126,D14-1074,0,0.551377,"translation” of the previous line (Zhou et al., 2009; He et al., 2012). Yan et al. (2013) proposes a method based on summarization techniques for poem generation, retrieving candidate sentences from a large corpus of poems based on a user’s query and clustering the constituent terms, summarizing each cluster into a line of a poem. Greene et al. (2010) use unsupervised learning to estimate the stress patterns of words in a poetry corpus, then use these in a finite-state network to generate short English love poems. Several deep learning methods have recently been proposed for generating poems. Zhang and Lapata (2014) use an RNN model to generate 4-line Chinese poems. They force the decoder to rhyme the second and fourth lines, trusting the RNN to control rhythm. Yi et al. (2016) also propose an attentionbased bidirectional RNN model for generating 4line Chinese poems. The only such work which tries to generate longer poems is from Wang et al. (2016), who use an attention-based LSTM model for generation iambic poems. They train on a small dataset and do not use an explicit system for constraining rhythm and rhyme in the poem. Novel contributions of our work are: • We combine finite-state machinery with dee"
D16-1126,Y09-1006,0,0.0624571,"nts on Hafez’s parameters and conclude by showing the generality of the approach with respect to language and poetic form. 2 Prior Work Automated poem generation has been a popular but challenging research topic (Manurung et al., 2000; Gervas, 2001; Diaz-Agudo et al., 2002; Manurung, 2003; Wong and Chun, 2008; Jiang and Zhou, 2008; Netzer et al., 2009). Recent work attempts to solve this problem by applying grammatical and semantic templates (Oliveira, 2009; Oliveira, 2012), or by modeling the task as statistical machine translation, in which each line is a “translation” of the previous line (Zhou et al., 2009; He et al., 2012). Yan et al. (2013) proposes a method based on summarization techniques for poem generation, retrieving candidate sentences from a large corpus of poems based on a user’s query and clustering the constituent terms, summarizing each cluster into a line of a poem. Greene et al. (2010) use unsupervised learning to estimate the stress patterns of words in a poetry corpus, then use these in a finite-state network to generate short English love poems. Several deep learning methods have recently been proposed for generating poems. Zhang and Lapata (2014) use an RNN model to generate"
D17-1099,P14-2085,0,0.0258233,"stractness, which are also more challenging for computer vision as they have less distinct visual patterns. Noting this difficulty, Antol et al. (2014) instead employ cartoon illustrations as intermediate mappings for zero-shot dyadic activity recognition. We present a complementary approach: that of tackling the abstractness of verb attributes directly. We develop and use a corpus of verb attributes, using linguistic theories on verb semantics (e.g., aspectual verb classes of Vendler (1957)) and also drawing inspiration from studies on linguistic categorization of verbs and their properties (Friedrich and Palmer, 2014; Siegel and McKeown, 2000). In sum, we present the first study aiming to recover general action attributes for a diverse collection of verbs, and probe their predictive power for zero-shot activity recognition on the recently introduced imSitu dataset (Yatskar et al., 2016). Empirical results show that action attributes inferred from language can help classifying previously unseen activities and suggest several avenues for future research on this challenging task. We publicly share our dataset and code for future research.1 2 • achievement: a verb that can be completed in a short period of ti"
D17-1099,N13-1092,0,0.0206651,"Missing"
D17-1099,P15-1027,0,0.064371,"Missing"
D17-1099,D16-1089,0,0.0942686,"verb attributes. A related task is that of improving word embeddings using multimodal data and linguistic resources (Faruqui et al., 2015; Silberer et al., 2013; Vendrov et al., 2016). Our work runs orthogonal to this, as we focus on word attributes as a tool for a zero-shot activity recognition pipeline. Zero-shot learning with objects Though distinct, our work is related to zero-shot learning of objects in computer vision. There are several datasets (Nilsback and Zisserman, 2008; Welinder et al., 2010) and models developed on this task (Romera-Paredes and Torr (2015); Lampert et al. (2014); Mukherjee and Hospedales (2016); Farhadi et al. (2010)). In addition, 953 cough fling ignite perform mourn squint dye shop Figure 4: Predictions on unseen classes from our attribute+embedding model with gold attributes. The top and bottom rows show successful and failure cases respectively. The bars to the right of each image represent a probability distribution, showing the ground truth class and the top 5 scoring incorrect classes. tributes for activity recognition. Ba et al. (2015) augment existing datasets with descriptive Wikipedia articles so as to learn novel objects from descriptive text. As illustrated in Section 1"
D17-1099,P15-2119,0,0.0488059,"Missing"
D17-1099,J00-4004,0,0.136618,"ore challenging for computer vision as they have less distinct visual patterns. Noting this difficulty, Antol et al. (2014) instead employ cartoon illustrations as intermediate mappings for zero-shot dyadic activity recognition. We present a complementary approach: that of tackling the abstractness of verb attributes directly. We develop and use a corpus of verb attributes, using linguistic theories on verb semantics (e.g., aspectual verb classes of Vendler (1957)) and also drawing inspiration from studies on linguistic categorization of verbs and their properties (Friedrich and Palmer, 2014; Siegel and McKeown, 2000). In sum, we present the first study aiming to recover general action attributes for a diverse collection of verbs, and probe their predictive power for zero-shot activity recognition on the recently introduced imSitu dataset (Yatskar et al., 2016). Empirical results show that action attributes inferred from language can help classifying previously unseen activities and suggest several avenues for future research on this challenging task. We publicly share our dataset and code for future research.1 2 • achievement: a verb that can be completed in a short period of time (e.g. “open”, “jump”) •"
D17-1099,P13-1056,0,0.0279563,"ir CAAP model and then running the DAP model on these predicted attributes outperforms the use of gold attributes at test time. It is some8 Related Work Learning attributes from embeddings Rubinstein et al. (2015) seek to predict McRae et al. (2005)’s feature norms from word embeddings of concrete nouns. Likewise, the CAAP model of Al-Halah et al. (2016) predicts the object attributes of concrete nouns for use in zero-shot learning. In contrast, we predict verb attributes. A related task is that of improving word embeddings using multimodal data and linguistic resources (Faruqui et al., 2015; Silberer et al., 2013; Vendrov et al., 2016). Our work runs orthogonal to this, as we focus on word attributes as a tool for a zero-shot activity recognition pipeline. Zero-shot learning with objects Though distinct, our work is related to zero-shot learning of objects in computer vision. There are several datasets (Nilsback and Zisserman, 2008; Welinder et al., 2010) and models developed on this task (Romera-Paredes and Torr (2015); Lampert et al. (2014); Mukherjee and Hospedales (2016); Farhadi et al. (2010)). In addition, 953 cough fling ignite perform mourn squint dye shop Figure 4: Predictions on unseen class"
D17-1099,Q16-1002,0,\N,Missing
D17-1099,P98-1013,0,\N,Missing
D17-1099,C98-1013,0,\N,Missing
D17-1099,D14-1162,0,\N,Missing
D17-1099,D15-1162,0,\N,Missing
D17-1099,N15-1113,0,\N,Missing
D17-1195,N15-1083,0,0.0350679,"dding of the entity et , not updated with ht . The intuition is that eet ,t−1 will help contextual information ht−1 to select the residual length of entity et . Wlength is the weight matrix for length prediction, with `max = 25 rows. Finally, the probability of a word x as the next token is jointly modeled by ht−1 and the vector representation of the most recently mentioned entity ecurrent : 1832 p(Xt = x |ht−1 , ecurrent ) ∝ CFSM(ht−1 + We ecurrent ), (6) where We is a transformation matrix to adjust the dimensionality of ecurrent . CFSM is a class factorized softmax function (Goodman, 2001; Baltescu and Blunsom, 2015). It uses a two-step prediction with predefined word classes instead of direct prediction on the whole vocabulary, and reduces the time complexity to the log of vocabulary size. et al. (2016) for the “memory blocks” in their recurrent entity network models. The difference is that their model updates all memory blocks in each time step. Instead, our updating scheme in Equation 8 only applies to the selected entity et at time step t. 2.4 The model is trained to maximize the log of the joint probability of R, E, L, and X: Dynamic entity representations Before predicting the entity at step t, we n"
D17-1195,P14-1005,0,0.0430609,"Missing"
D17-1195,P08-1090,0,0.0197777,"c representations. In previous work, such information has been added as features (Luo et al., 2004; Bj¨orkelund and Kuhn, 2014) or by computing distributed entity representations (Wiseman et al., 2016; Clark and Manning, 2016b). Our approach complements these previous methods. Entity prediction. The entity prediction task discussed in §5.3 is based on work by Modi et al. (2017). The main difference is that we do not assume that all entities belong to a previously known set of entity types specified for each narrative scenario. This task is also closely related to the “narrative cloze” task of Chambers and Jurafsky (2008) and the “story cloze test” of Mostafazadeh et al. (2016). Those studies aim to understand relationships between events, while our task focuses on predicting upcoming entity mentions. 7 Conclusion We have presented a neural language model, E N TITY NLM, that defines a distribution over texts and the mentioned entities. It provides vector representations for the entities and updates them dynamically in context. The dynamic representations are further used to help generate specific entity mentions and the following text. This model outperforms strong baselines and prior work on three tasks: lang"
D17-1195,D16-1245,0,0.290395,"sing their continuous representations. The score above is normalized over values {1, . . . , 1 + maxt0 &lt;t et0 }. f (e) represents a vector of distance features associated with e and the mentions of the existing entities. Hence two information sources are used to predict the next entity: (i) contextual information ht−1 , and (ii) distance features f (e) from the current mention to the closest mention from each previously mentioned entity. f (e) = 0 if e is a new entity. This term can also be extended to include other surface-form features for coreference resolution (Martschat and Strube, 2015; Clark and Manning, 2016b). For the chosen entity et from Equation 4, the distribution over its mention length is drawn according to p(Lt = ` |ht−1 , eet ,t−1 ) Probability distributions &gt; ∝ exp(Wlength,` [ht−1 ; eet ,t−1 ]), The generative story above referenced several parametric distributions defined based on vector representations of histories and entities. These are defined as follows. For r ∈ {0, 1}, p(Rt = r |ht−1 ) ∝ exp(h&gt; t−1 Wr r), (3) where r is the parameterized embedding associated with r, which paves the way for exploring entity type representations in future work; Wr is a parameter matrix for the bili"
D17-1195,P16-1061,0,0.281726,"sing their continuous representations. The score above is normalized over values {1, . . . , 1 + maxt0 &lt;t et0 }. f (e) represents a vector of distance features associated with e and the mentions of the existing entities. Hence two information sources are used to predict the next entity: (i) contextual information ht−1 , and (ii) distance features f (e) from the current mention to the closest mention from each previously mentioned entity. f (e) = 0 if e is a new entity. This term can also be extended to include other surface-form features for coreference resolution (Martschat and Strube, 2015; Clark and Manning, 2016b). For the chosen entity et from Equation 4, the distribution over its mention length is drawn according to p(Lt = ` |ht−1 , eet ,t−1 ) Probability distributions &gt; ∝ exp(Wlength,` [ht−1 ; eet ,t−1 ]), The generative story above referenced several parametric distributions defined based on vector representations of histories and entities. These are defined as follows. For r ∈ {0, 1}, p(Rt = r |ht−1 ) ∝ exp(h&gt; t−1 Wr r), (3) where r is the parameterized embedding associated with r, which paves the way for exploring entity type representations in future work; Wr is a parameter matrix for the bili"
D17-1195,N16-1024,1,0.839349,"due to the long-range dependency in recurrent neural networks, the search space of R, E, L during inference grows exponentially. We thus use importance sampling to approximate the marginal distribution of X. Specifically, with the samples from a proposal distribution Q(R, E, L|X), the approximated marginal probability is defined as X P (X) = P (X, R, E, L) R,E,L = X Q(R, E, L |X) R,E,L ≈ 1 N X {r(i) ,e(i) ,`(i) }∼Q P (X, R, E, L) Q(R, E, L |X) P (r(i) , e(i) , `(i) , x) Q(r(i) , e(i) , `(i) |x) (11) A similar idea of using importance sampling for language modeling evaluation has been used by Dyer et al. (2016). For language modeling evaluation, we train our model on the training set from the CoNLL 2012 dataset with coreference annotation. On the test data, we treat coreference structure as latent variables and use importance sampling to approximate the marginal distribution of X. For each document, the model randomly draws N = 100 samples from the proposal distribution, discussed next. Proposal distribution. For implementation of Q, we use a discriminative variant of E NTI TY NLM by taking the current word xt for predicting the entity-related variables in the same time step. Specifically, in the ge"
D17-1195,N10-1061,0,0.130565,"hen ecurrent still represents the most recently mentioned entity.) 4. Advance the RNN, i.e., feed it the word vector xt to compute ht (Equation 2). 5. If rt = 1, update eet ,t using eet ,t−1 and ht , then set ecurrent = eet ,t . Details of the entity updating are given in §2.4. 6. For every entity eι ∈ Et  {et }, set eι,t = eι,t−1 (i.e., no changes to other entities’ representations). Note that at any given time step t, ecurrent will always contain the most recent vector representation of the most recently mentioned entity. A generative model with a similar hierarchical structure was used by Haghighi and Klein (2010) for coreference resolution. Our approach differs in two important ways. First, our model defines a joint distribution over all of the text, not just the entity mentions. Second, we use representation learning rather than Bayesian nonparametrics, allowing natural integration with the language model. 2.3 To give the possibility of predicting a new entity, we need an entity embedding beforehand with index (1 + maxt0 &lt;t et0 ), which is randomly sampled from Equation 7. Then, for every e ∈ {1, . . . , 1 + maxt0 &lt;t et0 }: p(Et = e |Rt = 1, ht−1 ) &gt; ∝ exp(h&gt; t−1 Wentity ee,t−1 + wdist f (e)), where"
D17-1195,P13-2121,0,0.0773352,"Missing"
D17-1195,P82-1020,0,0.780172,"Missing"
D17-1195,N16-1037,1,0.8173,"ows the prediction accuracies. E NTITY NLM (line 4) significantly outperforms both baselines (line 1 and 2) and prior work (line 3) (p  0.01, paired t-test). The comparison between line 4 and 5 shows our model is even close to the human prediction performance. 6 Related Work Rich-context language models. The originally proposed recurrent neural network language models only capture information within sentences. To extend the capacity of RNNLMs, various researchers have incorporated information beyond sentence boundaries. Previous work focuses on contextual information from previous sentences (Ji et al., 2016a) or discourse relations between adjacent sentences (Ji et al., 2016b), showing improvements to language modeling and related tasks like coherence evaluation and discourse relation prediction. In this work, E NTITY NLM adds explicit entity information to the language model, which is another way of adding a memory Entity-related models. Two recent approaches to modeling entities in text are closely related to our model. The first is the “reference-aware” language models proposed by Yang et al. (2016), where the referred entities are from either a predefined item list, an external database, or"
D17-1195,H05-1004,0,0.184004,"Missing"
D17-1195,P04-1018,0,0.0138059,"obabillistic graphical model with the distance-dependent Chinese Restaurant Process (Pitman, 1995) for entity assignment, while our model is built on a recurrent neural network architecture. The reranking method considered in our coreference resolution evaluation could also be extended with samples from additional coreference resolution systems, to produce more variety (Ng, 2005). The benefit of such a system comes, we believe, from the explicit tracking of each entity throughout the text, providing entityspecific representations. In previous work, such information has been added as features (Luo et al., 2004; Bj¨orkelund and Kuhn, 2014) or by computing distributed entity representations (Wiseman et al., 2016; Clark and Manning, 2016b). Our approach complements these previous methods. Entity prediction. The entity prediction task discussed in §5.3 is based on work by Modi et al. (2017). The main difference is that we do not assume that all entities belong to a previously known set of entity types specified for each narrative scenario. This task is also closely related to the “narrative cloze” task of Chambers and Jurafsky (2008) and the “story cloze test” of Mostafazadeh et al. (2016). Those studi"
D17-1195,Q15-1029,1,0.936972,"ix for predicting entities using their continuous representations. The score above is normalized over values {1, . . . , 1 + maxt0 &lt;t et0 }. f (e) represents a vector of distance features associated with e and the mentions of the existing entities. Hence two information sources are used to predict the next entity: (i) contextual information ht−1 , and (ii) distance features f (e) from the current mention to the closest mention from each previously mentioned entity. f (e) = 0 if e is a new entity. This term can also be extended to include other surface-form features for coreference resolution (Martschat and Strube, 2015; Clark and Manning, 2016b). For the chosen entity et from Equation 4, the distribution over its mention length is drawn according to p(Lt = ` |ht−1 , eet ,t−1 ) Probability distributions &gt; ∝ exp(Wlength,` [ht−1 ; eet ,t−1 ]), The generative story above referenced several parametric distributions defined based on vector representations of histories and entities. These are defined as follows. For r ∈ {0, 1}, p(Rt = r |ht−1 ) ∝ exp(h&gt; t−1 Wr r), (3) where r is the parameterized embedding associated with r, which paves the way for exploring entity type representations in future work; Wr is a para"
D17-1195,Q17-1003,0,0.135791,"7. 2017 Association for Computational Linguistics • Et ∈ Et is the index of the entity referred to, if Rt = 1. The set Et consists of {1, . . . , 1 + maxt0 &lt;t et0 }, i.e., the indices of all previously mentioned entities plus an additional value for a new entity. Thus Et starts as {1} and grows monotonically with t, allowing for an arbitrary number of entities to be mentioned. We denote the value of Et by et . If Rt = 0, then Et is fixed to a special value ø. nally, the model can perform entity cloze tasks. As presented in §5.3, it achieves state-of-the-art performance on the InScript corpus (Modi et al., 2017). 2 Model A language model defines a distribution over sequences of word tokens; let Xt denote the random variable for the tth word in the sequence, xt denote the value of Xt and xt the distributed representation (embedding) of this word. Our starting point for language modeling is a recurrent neural network (Mikolov et al., 2010), which defines p(Xt |history) = softmax (Wh ht−1 + b) (1) ht−1 = LSTM(ht−2 , xt−1 ) (2) where Wh and b are parameters of the model (along with word embeddings xt ), LSTM is the widely used recurrent function known as “long short-term memory” (Hochreiter and Schmidhub"
D17-1195,W12-4501,0,0.199498,"guage model, augmented with random variables for entity mentions that capture coreference, and with dynamic representations of entities. We estimate the model’s parameters from data that is annotated with entity mentions and coreference. Because our model is generative, it can be queried in different ways. Marginalizing everything except the words, it can play the role of a language model. In §5.1, we find that it outperforms both a strong n-gram language model and a strong recurrent neural network language model on the English test set of the CoNLL 2012 shared task on coreference evaluation (Pradhan et al., 2012). The model can also identify entity mentions and coreference relationships among them. In §5.2, we show that it can easily be used to add a performance boost to a strong coreference resolution system, by reranking a list of k-best candidate outputs. On the CoNLL 2012 shared task test set, the reranked outputs are significantly better than the original top choices from the same system. Fi1830 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1830–1839 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics • Et ∈ E"
D17-1195,N04-1023,0,0.0266439,"we also use the marginalization method defined in Equation 11 on the development data to select the best configuration. However, we plan to use the same experimental setup for all experiments, instead of customizing our model for each individual task. 5.2 Coreference reranking Task description. We show how E NTITY LM, which allows an efficient computation of the probability P (R, E, L, X), can be used as a coreference reranker to improve a competitive coreference resolution system due to Martschat and Strube (2015). This task is analogous to the reranking approach used in machine translation (Shen et al., 2004). The specific formulation is as follows: arg max {r(i) ,e(i) ,l(i) }∈K P (r(i) , e(i) , l(i) , x) (12) where K is the k-best list for a given document. In our experiments, k = 100. To the best of our knowledge, the problem of obtaining k-best outputs of a coreference resolution system has not been studied before. Approximate k-best decoding. We rerank the output of a system that predicts an antecedent for each mention by relying on pairwise scores for mention pairs. This is the dominant approach for coreference resolution (Martschat and Strube, 2015; Clark and Manning, 2016a). The predictions"
D17-1195,N16-1036,0,0.0771503,"Missing"
D17-1195,M95-1005,0,0.693437,"Missing"
D17-1195,N16-1114,0,0.0759838,"Missing"
D17-1195,N16-1098,0,0.0263734,"een added as features (Luo et al., 2004; Bj¨orkelund and Kuhn, 2014) or by computing distributed entity representations (Wiseman et al., 2016; Clark and Manning, 2016b). Our approach complements these previous methods. Entity prediction. The entity prediction task discussed in §5.3 is based on work by Modi et al. (2017). The main difference is that we do not assume that all entities belong to a previously known set of entity types specified for each narrative scenario. This task is also closely related to the “narrative cloze” task of Chambers and Jurafsky (2008) and the “story cloze test” of Mostafazadeh et al. (2016). Those studies aim to understand relationships between events, while our task focuses on predicting upcoming entity mentions. 7 Conclusion We have presented a neural language model, E N TITY NLM, that defines a distribution over texts and the mentioned entities. It provides vector representations for the entities and updates them dynamically in context. The dynamic representations are further used to help generate specific entity mentions and the following text. This model outperforms strong baselines and prior work on three tasks: language modeling, coreference resolution, and entity predict"
D17-1195,P05-1020,0,0.0495578,"se their model for coreference reranking and entity prediction. Coreference resolution. The hierarchical structure of our entity generation model is inspired by 1837 Haghighi and Klein (2010). They implemented this idea as a probabillistic graphical model with the distance-dependent Chinese Restaurant Process (Pitman, 1995) for entity assignment, while our model is built on a recurrent neural network architecture. The reranking method considered in our coreference resolution evaluation could also be extended with samples from additional coreference resolution systems, to produce more variety (Ng, 2005). The benefit of such a system comes, we believe, from the explicit tracking of each entity throughout the text, providing entityspecific representations. In previous work, such information has been added as features (Luo et al., 2004; Bj¨orkelund and Kuhn, 2014) or by computing distributed entity representations (Wiseman et al., 2016; Clark and Manning, 2016b). Our approach complements these previous methods. Entity prediction. The entity prediction task discussed in §5.3 is based on work by Modi et al. (2017). The main difference is that we do not assume that all entities belong to a previou"
D17-1195,D14-1162,0,0.0965475,"M (Kingma and Ba, 2014) with default learning rate λ = 0.001 as the candidate optimizers of our model. For all the parameters, we use the initialization tricks recommended by Glorot and Bengio (2010). To avoid overfitting, we also employ dropout (Srivastava et al., 2014) with the candidate rates as {0.2, 0.5}. In addition, there are two tunable hyperparameters of E NTITY NLM: the size of word embeddings and the dimension of LSTM hidden states. For both of them, we consider the values {32, 48, 64, 128, 256}. We also experiment with the option to either use the pretrained GloVe word embeddings (Pennington et al., 2014) or randomly initialized word embeddings (then updated during training). For all experiments, the best configuration of hyperparameters and optimizers is selected based on the objective value on the development data. 4 Evaluation Tasks and Datasets We evaluate our model in diverse use scenarios: (i) language modeling, (ii) coreference resolution, 1833 and (iii) entity prediction. The evaluation on language modeling shows how the internal entity representation, when marginalized out, can improve the perplexity of language models. The evaluation on coreference resolution experiment shows how our"
D17-1195,P14-2006,0,0.0698471,"Missing"
D17-1247,N15-1084,0,0.0827506,"formation about how women are portrayed through their expression and their actions, which can act complementary to measures of their presence. 5 ing a lexicon-based strategy that focus on commonly gender-biased attributes (e.g., emotional for women) rather than the overall power dynamics of the story. In a similar vein, Ramakrishna et al. (2015) learn word-level “gender ladenness” features by looking at the neighbors of 925 manually annotated words. There exist various sets of high-level criteria to assess gender bias of character portrayal in fiction (Yehl, 2013; Romano, 2013; Powers, 2016). Agarwal et al. (2015), in particular, automate the Bechdel test using social network features, finding that women are less central to the plot in movies that fail it. We compare our linguistic analysis of power and agency with the Bechdel test, demonstrating the need for more fine-grained analysis of how gender is depicted in movies. Close in spirit to our investigation, Schofield and Mehr (2016) train a number of classifiers over movie scripts for determining the gender of individual (and pairs) of speakers as well as the expected length of their relationships. In contrast, we focus on understanding how the gende"
D17-1247,P11-1078,0,0.0166404,"Missing"
D17-1247,N15-1113,0,0.0331041,"Missing"
D17-1247,D14-1211,0,0.159456,"wer(AG>TH) F narr. power(AG>TH) β 10.02 −9.65 2.05 −1.19 P/F pass∗∗ fail∗∗ pass∗ fail∗ Table 3: Significant correlates of passing the Bechdel test. F: metric for female characters, computed on the dialogues (dial.) or on the narratives (narr.). ∗ : p<.05; ∗∗ : p<.001. ing bias in how male and female characters display different levels of power and agency in their dialogue. 4 Table 2: Gender association with our connotation frames and a subset of LIWC metrics for characters’ dialogue, controlled for number of words spoken. All results are significant (∗∗ : p<.001). flects real-world dialogues (Prabhakaran et al., 2014). The usage of imperatives tends to convey power and dominance according to the findings of Bramsen et al. (2011). Along with the fact that female characters tend to agree (assent) more than male characters, this corroborates the finding in subsection 3.1 that male characters are generally given more power and agency. Furthermore, male characters use inhibitory language more (inhib), which contains words pertaining to blocking or allowing, suggesting that these characters are in positions of power. Further evidence of power imbalances is found through function words. Women tend to use I and yo"
D17-1247,D15-1234,0,0.0568888,"nclusive of movies who portray women in nonauthoritative, passive positions or by excluding movies that have strong women with agency, who just happen not to talk to each other about something besides men. Our extensions to the connotation frame lexicon provide finer grained information about how women are portrayed through their expression and their actions, which can act complementary to measures of their presence. 5 ing a lexicon-based strategy that focus on commonly gender-biased attributes (e.g., emotional for women) rather than the overall power dynamics of the story. In a similar vein, Ramakrishna et al. (2015) learn word-level “gender ladenness” features by looking at the neighbors of 925 manually annotated words. There exist various sets of high-level criteria to assess gender bias of character portrayal in fiction (Yehl, 2013; Romano, 2013; Powers, 2016). Agarwal et al. (2015), in particular, automate the Bechdel test using social network features, finding that women are less central to the plot in movies that fail it. We compare our linguistic analysis of power and agency with the Bechdel test, demonstrating the need for more fine-grained analysis of how gender is depicted in movies. Close in sp"
D17-1247,P16-1030,1,0.35512,"ts” things is implied to be a passive decision-maker (or of lower agency) than somebody who “assesses” things. While not explicitly stated, these connotative meanings projected by different verbs can influence the assumptions the audience makes about the people being described. These assumptions can have negative consequences if they reinforce negative stereotypes (Walton and Spencer, 2009). To formalize this implicit information about people projected by actions, we introduce power and agency connotation frames, two new types of predicate-specific connotative relationships as an extension to Rashkin et al. (2016)’s connotation frame lexicon. For instance, in Figure 1, the verb “beckoning” implies that its theme (Irene) has less power than its agent (the man). In the third line, Irene displays strong agency when she “slices” in self-defense. In contrast, when the man “obeys”, the man has low implied agency. Using the new connotation lexicon, we present a quantitative study to reveal the subtle, but prevalent gender1 bias in modern films. Going beyond the surface level analysis such as screen time or number of female characters (Google, 2017), our study aims for a more focused and precise analysis of po"
D17-1247,W16-0204,0,0.0671786,"adenness” features by looking at the neighbors of 925 manually annotated words. There exist various sets of high-level criteria to assess gender bias of character portrayal in fiction (Yehl, 2013; Romano, 2013; Powers, 2016). Agarwal et al. (2015), in particular, automate the Bechdel test using social network features, finding that women are less central to the plot in movies that fail it. We compare our linguistic analysis of power and agency with the Bechdel test, demonstrating the need for more fine-grained analysis of how gender is depicted in movies. Close in spirit to our investigation, Schofield and Mehr (2016) train a number of classifiers over movie scripts for determining the gender of individual (and pairs) of speakers as well as the expected length of their relationships. In contrast, we focus on understanding how the gender of a given character implicitly relates to features that track their control over their own path (agency) and the world around them (power). Related work 6 Conclusion We created and released new connotation frames of power and agency, allowing for more nuanced writing analysis than previously possible. We validate our new frames through a case study on movie scripts. Specif"
D17-1247,W10-0723,0,0.0560052,"Missing"
D17-1317,W12-3809,1,0.808889,"what the model was already learning from text. We report results on the test set in Table 6. We again find that LIWC features improves MaxEnt and NB models to perform similarly to the LSTM model. As in the dev. set results, the LIWC features do not improve the LSTM’s performance, and even seem to hurt the performance slightly. 4 Related Work Deception Detection Psycholinguistic work in interpersonal deception theory (Buller and Burgoon, 1996) has postulated that certain speech patterns can be signs of a speaker trying to purposefully obscure the truth. Hedge words and other vague qualifiers (Choi et al., 2012; Recasens et al., 2013), for example, may add indirectness to a statement that obscures its meaning. Linguistic aspects deception detection has been well-studied in a variety of NLP applications (Ott et al., 2011; Mihalcea and Strapparava, 2009; Jindal and Liu, 2008; Girlea et al., 2016; Zhou et al., 2004). In these applications, people purposefully tell lies to receive an extrinsic payoff. In our study, we compare varying types of unreliable news source, created with differing intents and levels of veracity. Fact-Checking and Fake News There is research in political science exploring how eff"
D17-1317,P09-2078,0,0.0105102,"C features do not improve the LSTM’s performance, and even seem to hurt the performance slightly. 4 Related Work Deception Detection Psycholinguistic work in interpersonal deception theory (Buller and Burgoon, 1996) has postulated that certain speech patterns can be signs of a speaker trying to purposefully obscure the truth. Hedge words and other vague qualifiers (Choi et al., 2012; Recasens et al., 2013), for example, may add indirectness to a statement that obscures its meaning. Linguistic aspects deception detection has been well-studied in a variety of NLP applications (Ott et al., 2011; Mihalcea and Strapparava, 2009; Jindal and Liu, 2008; Girlea et al., 2016; Zhou et al., 2004). In these applications, people purposefully tell lies to receive an extrinsic payoff. In our study, we compare varying types of unreliable news source, created with differing intents and levels of veracity. Fact-Checking and Fake News There is research in political science exploring how effective fact-checking is at improving people’s awareness (Lord et al., 1979; Thorson, 2016; Nyhan and Reifler, 2015). Prior computational works (Vlachos and Riedel, 2014; Ciampaglia et al., 2015) have proposed fact-checking through entailment fro"
D17-1317,P11-1032,1,0.629199,"t results, the LIWC features do not improve the LSTM’s performance, and even seem to hurt the performance slightly. 4 Related Work Deception Detection Psycholinguistic work in interpersonal deception theory (Buller and Burgoon, 1996) has postulated that certain speech patterns can be signs of a speaker trying to purposefully obscure the truth. Hedge words and other vague qualifiers (Choi et al., 2012; Recasens et al., 2013), for example, may add indirectness to a statement that obscures its meaning. Linguistic aspects deception detection has been well-studied in a variety of NLP applications (Ott et al., 2011; Mihalcea and Strapparava, 2009; Jindal and Liu, 2008; Girlea et al., 2016; Zhou et al., 2004). In these applications, people purposefully tell lies to receive an extrinsic payoff. In our study, we compare varying types of unreliable news source, created with differing intents and levels of veracity. Fact-Checking and Fake News There is research in political science exploring how effective fact-checking is at improving people’s awareness (Lord et al., 1979; Thorson, 2016; Nyhan and Reifler, 2015). Prior computational works (Vlachos and Riedel, 2014; Ciampaglia et al., 2015) have proposed fact"
D17-1317,D14-1162,0,0.114014,"he input and predicts the Politifact rating. We also compared this model with Maximum Entropy (MaxEnt) and Naive Bayes models, frequently used for text categorization. For input to the MaxEnt and Naive Bayes models, we tried two variants: one with the word tfidf vectors as input, and one with the LIWC measurements concatenated to the tf-idf vectors. For the LSTM model, we used word sequences as input and also a version where LSTM output is concatenated with LIWC feature vectors before undergoing the activation layer. The LSTM word embeddings are initialized with 100-dim embeddings from GLOVE (Pennington et al., 2014) and fine-tuned during training. The LSTM was implemented with Theano and Keras with 300-dim hidden state and a batch size of 64. Training was done with ADAM to minimize categorical crossentropy loss over 10 epochs. Classifier Results Table 5 summarizes the performance on the development set. We report macro averaged F1 score in all tables. The LSTM outperforms the other models when only using text as input; however the other two models improve substantially with adding LIWC features, particu2934 2- CLASS Majority Baseline Naive Bayes MaxEnt LSTM text .39 .44 .55 .58 + LIWC .58 .58 .57 6- CLAS"
D17-1317,P13-1162,0,0.112404,"already learning from text. We report results on the test set in Table 6. We again find that LIWC features improves MaxEnt and NB models to perform similarly to the LSTM model. As in the dev. set results, the LIWC features do not improve the LSTM’s performance, and even seem to hurt the performance slightly. 4 Related Work Deception Detection Psycholinguistic work in interpersonal deception theory (Buller and Burgoon, 1996) has postulated that certain speech patterns can be signs of a speaker trying to purposefully obscure the truth. Hedge words and other vague qualifiers (Choi et al., 2012; Recasens et al., 2013), for example, may add indirectness to a statement that obscures its meaning. Linguistic aspects deception detection has been well-studied in a variety of NLP applications (Ott et al., 2011; Mihalcea and Strapparava, 2009; Jindal and Liu, 2008; Girlea et al., 2016; Zhou et al., 2004). In these applications, people purposefully tell lies to receive an extrinsic payoff. In our study, we compare varying types of unreliable news source, created with differing intents and levels of veracity. Fact-Checking and Fake News There is research in political science exploring how effective fact-checking is"
D17-1317,W14-2508,0,0.0813985,"been well-studied in a variety of NLP applications (Ott et al., 2011; Mihalcea and Strapparava, 2009; Jindal and Liu, 2008; Girlea et al., 2016; Zhou et al., 2004). In these applications, people purposefully tell lies to receive an extrinsic payoff. In our study, we compare varying types of unreliable news source, created with differing intents and levels of veracity. Fact-Checking and Fake News There is research in political science exploring how effective fact-checking is at improving people’s awareness (Lord et al., 1979; Thorson, 2016; Nyhan and Reifler, 2015). Prior computational works (Vlachos and Riedel, 2014; Ciampaglia et al., 2015) have proposed fact-checking through entailment from knowledge bases. Our work takes a more linguistic approach, performing lexical analysis over varying types of falsehood. Biyani et al. (2016) examine the unique linguistic styles found in clickbait articles, and Kumar et al. (2016) also characterize hoax documents on Wikipedia. The differentiation between these fake news types is also proposed in previous work (Rubin et al., 2015). Our paper extends this work by offering a quantitative study of linguistic differences found in articles of different types of fake news"
D17-1317,P17-2067,0,0.22283,"tic approach, performing lexical analysis over varying types of falsehood. Biyani et al. (2016) examine the unique linguistic styles found in clickbait articles, and Kumar et al. (2016) also characterize hoax documents on Wikipedia. The differentiation between these fake news types is also proposed in previous work (Rubin et al., 2015). Our paper extends this work by offering a quantitative study of linguistic differences found in articles of different types of fake news, and build predictive models for graded deception across multiple domains – PolitiFact and news articles. More recent work (Wang, 2017) has also investigated PolitiFact data though they investigated meta-data features for prediction whereas our investigation is focused on linguistic analysis through stylistic lexicons. 5 Conclusion We examine truthfulness and its contributing linguistic attributes across multiple domains e.g., online news sources and public statements. We perform multiple prediction tasks on fact-checked statements of varying levels of truth (graded deception) as well as a deeper linguistic comparison of differing types of fake news e.g., propaganda, satire and hoaxes. We have shown that factchecking is indee"
D17-1317,H05-1044,0,0.0264711,"eport Clickhole 14,170 627 188 350 250 303 Hoax American News DC Gazette 6,914 5,133 204 582 Propaganda The Natural News Activist Report 15,580 17,869 857 1,169 Table 1: News articles used for analysis in Section 2. the text with NLTK (Bird et al., 2009) and compute per-document count for each lexicon, and report averages per article of each type. First among these lexicons is the Linguistic Inquiry and Word Count (LIWC), a lexicon widely used in social science studies (Pennebaker et al., 2015). In addition, we estimate the use of strongly and weakly subjective words with a sentiment lexicon (Wilson et al., 2005). Subjective words can be used to dramatize or sensationalize a news story. We also use lexicons for hedging from (Hyland, 2015) because hedging can indicate vague, obscuring language. Lastly, we introduce intensifying lexicons that we crawled from Wiktionary based on a hypothesis that fake news articles try to enliven stories to attract readers. We compiled five lists from Wiktionary of words that imply a degree a dramatization (comparatives, superlatives, action adverbs, manner adverbs, and modal adverbs) and measured their presence. Discussion Table 2 summarizes the ratio of averages betwee"
D17-1317,N16-1047,0,\N,Missing
D18-1009,D15-1075,0,0.770689,"s placed in the kennel next to a woman’s feet. b) washes her face with the shampoo. c) walks into frame and walks towards the dog. d) tried to cut her face, so she is trying to do something very close to her face. Table 1: Examples from Swag; the correct answer is bolded. Adversarial Filtering ensures that stylistic models find all options equally appealing. linguistic entailment (Chierchia and McConnellGinet, 2000). Whereas the dominant entailment paradigm asks if two natural language sentences (the ‘premise’ and the ‘hypothesis’) describe the same set of possible worlds (Dagan et al., 2006; Bowman et al., 2015), here we focus on whether a (multiple-choice) ending describes a possible (future) world that can be anticipated from the situation described in the premise, even when it is not strictly entailed. Making such inference necessitates a rich understanding about everyday physical situations, including object affordances (Gibson, 1979) and frame semantics (Baker et al., 1998). A first step toward grounded commonsense inference with today’s deep learning machinery is to create a large-scale dataset. However, recent work has shown that human-written datasets are susceptible to annotation artifacts:"
D18-1009,P18-2103,0,0.0716038,"Missing"
D18-1009,P09-1068,0,0.0706243,"Missing"
D18-1009,P17-1152,0,0.0414444,"oped for the NLI task. d. Dual Bag-of-Words For this baseline, we treat each sentence as a bag-of-embeddings (c, vi ). We model the probability of picking an ending i using a bilinear model: softmaxi (cWviT ).8 e. Dual pretrained sentence encoders Here, we obtain representations from SkipThoughts or InferSent for each span, and compute their pairwise compatibility using either 1) a bilinear model or 2) an MLP from their concatenated representations. f. SNLI inference Here, we consider two models that do well on SNLI (Bowman et al., 2015): Decomposable Attention (Parikh et al., 2016) and ESIM (Chen et al., 2017). We use pretrained versions of these models (with ELMo embeddings) on SNLI to obtain 3-way entailment, neutral, and contradiction probabilities for each example. We then train a log-linear model using these 3-way probabilities as features. g. SNLI models (retrained) Here, we train ESIM and Decomposable Attention on our dataset: we simply change the output layer size to 1 (the potential of an ending vi ) with a softmax over i. for our dataset take the following form: given a sentence and a noun phrase as context c = (s, n), as well as a list of possible verb phrase endings V = {v1 , . . . , v4"
D18-1009,N18-2017,1,0.89719,"Missing"
D18-1009,D17-1070,0,0.0192329,"multiple choice question to reduce the potential ambiguity in the labels and to allow for direct comparison between machines and humans. In addition, Swag’s use of adversarial filtering increases diversity of situations and counterfactual generation quality. Related Work Entailment NLI There has been a long history of NLI benchmarks focusing on linguistic entailment (Cooper et al., 1996; Dagan et al., 2006; Marelli et al., 2014; Bowman et al., 2015; Lai et al., 2017; Williams et al., 2018). Recent NLI datasets in particular have supported learning broadly-applicable sentence representations (Conneau et al., 2017); moreover, models trained on these datasets were used as components 13 For RocStories, this was by design to encourage learning from the larger corpus of 98k sensible stories. 100 7 Last, another related task formulation is sentence completion or cloze, where the task is to predict a single word that is removed from a given context (Zweig and Burges, 2011; Paperno et al., 2016).14 Our work in contrast requires longer textual descriptions to reason about. Conclusion We propose a new challenge of physically situated commonsense inference that broadens the scope of natural language inference (NL"
D18-1009,P18-1152,1,0.872355,"Missing"
D18-1009,P17-1025,1,0.857241,"Missing"
D18-1009,E17-2068,0,0.0297031,"rbatch vectors retrofitted using ConceptNet relations (Speer et al., 2017), and 1024d ELMo contextual representations that show improvement on a variety of NLP tasks, including standard NLI (Peters et al., 2018). We follow the final dataset split (see Section 2) using two training approaches: training on the found data, and the found and highly-ranked generated data. See the appendix for more details. 4.1 Binary models Unary models The following models predict labels from a single span of text as input; this could be the ending only, the second sentence only, or the full passage. a. fastText (Joulin et al., 2017): This library models a single span of text as a bag of n-grams, and tries to predict the probability of an ending being correct or incorrect independently.7 b. Pretrained sentence encoders We consider two types of pretrained RNN sentence encoders, SkipThoughts (Kiros et al., 2015) and InferSent 4.3 Other models We also considered the following models: h. Length: Although length was used by the adversarial classifier, we want to verify that human validation didn’t reintroduce a length bias. For this baseline, we always choose the shortest ending. i. ConceptNet As our task requires world knowle"
D18-1009,D16-1244,0,0.117864,"Missing"
D18-1009,P17-1117,0,0.0508501,"Missing"
D18-1009,I17-1011,1,0.777985,"t to JOCI where the task was formulated as a regression task on the degree of plausibility of the hypothesis, we frame commonsense inference as a multiple choice question to reduce the potential ambiguity in the labels and to allow for direct comparison between machines and humans. In addition, Swag’s use of adversarial filtering increases diversity of situations and counterfactual generation quality. Related Work Entailment NLI There has been a long history of NLI benchmarks focusing on linguistic entailment (Cooper et al., 1996; Dagan et al., 2006; Marelli et al., 2014; Bowman et al., 2015; Lai et al., 2017; Williams et al., 2018). Recent NLI datasets in particular have supported learning broadly-applicable sentence representations (Conneau et al., 2017); moreover, models trained on these datasets were used as components 13 For RocStories, this was by design to encourage learning from the larger corpus of 98k sensible stories. 100 7 Last, another related task formulation is sentence completion or cloze, where the task is to predict a single word that is removed from a given context (Zweig and Burges, 2011; Paperno et al., 2016).14 Our work in contrast requires longer textual descriptions to reas"
D18-1009,N18-2102,0,0.04485,"Missing"
D18-1009,P16-1137,0,0.0647538,"Missing"
D18-1009,D14-1162,0,0.0811429,"our dataset take the following form: given a sentence and a noun phrase as context c = (s, n), as well as a list of possible verb phrase endings V = {v1 , . . . , v4 }, a model fθ must select a verb ˆi that hopefully matches igold : ˆi = argmax fθ (s, n, vi ) (4) i To study the amount of bias in our dataset, we also consider models that take as input just the ending verb phrase vi , or the entire second sentence (n, vi ). For our learned models, we train f by minimizing multi-class cross-entropy. We consider three different types of word representations: 300d GloVe vectors from Common Crawl (Pennington et al., 2014), 300d Numberbatch vectors retrofitted using ConceptNet relations (Speer et al., 2017), and 1024d ELMo contextual representations that show improvement on a variety of NLP tasks, including standard NLI (Peters et al., 2018). We follow the final dataset split (see Section 2) using two training approaches: training on the found data, and the found and highly-ranked generated data. See the appendix for more details. 4.1 Binary models Unary models The following models predict labels from a single span of text as input; this could be the ending only, the second sentence only, or the full passage. a"
D18-1009,P11-2057,0,0.064,"Missing"
D18-1009,N18-1202,0,0.0627976,"gold : ˆi = argmax fθ (s, n, vi ) (4) i To study the amount of bias in our dataset, we also consider models that take as input just the ending verb phrase vi , or the entire second sentence (n, vi ). For our learned models, we train f by minimizing multi-class cross-entropy. We consider three different types of word representations: 300d GloVe vectors from Common Crawl (Pennington et al., 2014), 300d Numberbatch vectors retrofitted using ConceptNet relations (Speer et al., 2017), and 1024d ELMo contextual representations that show improvement on a variety of NLP tasks, including standard NLI (Peters et al., 2018). We follow the final dataset split (see Section 2) using two training approaches: training on the found data, and the found and highly-ranked generated data. See the appendix for more details. 4.1 Binary models Unary models The following models predict labels from a single span of text as input; this could be the ending only, the second sentence only, or the full passage. a. fastText (Joulin et al., 2017): This library models a single span of text as a bag of n-grams, and tries to predict the probability of an ending being correct or incorrect independently.7 b. Pretrained sentence encoders W"
D18-1009,W17-2810,0,0.0592611,"Missing"
D18-1009,marelli-etal-2014-sick,0,0.0427973,"owledge graph or a neural model. In contrast to JOCI where the task was formulated as a regression task on the degree of plausibility of the hypothesis, we frame commonsense inference as a multiple choice question to reduce the potential ambiguity in the labels and to allow for direct comparison between machines and humans. In addition, Swag’s use of adversarial filtering increases diversity of situations and counterfactual generation quality. Related Work Entailment NLI There has been a long history of NLI benchmarks focusing on linguistic entailment (Cooper et al., 1996; Dagan et al., 2006; Marelli et al., 2014; Bowman et al., 2015; Lai et al., 2017; Williams et al., 2018). Recent NLI datasets in particular have supported learning broadly-applicable sentence representations (Conneau et al., 2017); moreover, models trained on these datasets were used as components 13 For RocStories, this was by design to encourage learning from the larger corpus of 98k sensible stories. 100 7 Last, another related task formulation is sentence completion or cloze, where the task is to predict a single word that is removed from a given context (Zweig and Burges, 2011; Paperno et al., 2016).14 Our work in contrast requi"
D18-1009,S18-2023,0,0.0923282,"Missing"
D18-1009,N16-1098,0,0.0815222,"LMo features), but the large gap between machine and human performance suggests that more is required to solve the dataset. As models are developed for commonsense inference, and more broadly as the field of NLP advances, we note that AF can be used again to create a more adversarial version of Swag using better language models and AF models. 6 Commonsense NLI Several datasets have been introduced to study NLI beyond linguistic entailment: for inferring likely causes and endings given a sentence (COPA; Roemmele et al., 2011), for choosing the most sensible ending to a short story (RocStories; Mostafazadeh et al., 2016; Sharma et al., 2018), and for predicting likelihood of a hypothesis by regressing to an ordinal label (JOCI; (Zhang et al., 2017)). These datasets are relatively small: 1k examples for COPA and 10k cloze examples for RocStories.13 JOCI increases the scale by generating the hypotheses using a knowledge graph or a neural model. In contrast to JOCI where the task was formulated as a regression task on the degree of plausibility of the hypothesis, we frame commonsense inference as a multiple choice question to reduce the potential ambiguity in the labels and to allow for direct comparison betwee"
D18-1009,P16-1144,0,0.0335514,"al., 1996; Dagan et al., 2006; Marelli et al., 2014; Bowman et al., 2015; Lai et al., 2017; Williams et al., 2018). Recent NLI datasets in particular have supported learning broadly-applicable sentence representations (Conneau et al., 2017); moreover, models trained on these datasets were used as components 13 For RocStories, this was by design to encourage learning from the larger corpus of 98k sensible stories. 100 7 Last, another related task formulation is sentence completion or cloze, where the task is to predict a single word that is removed from a given context (Zweig and Burges, 2011; Paperno et al., 2016).14 Our work in contrast requires longer textual descriptions to reason about. Conclusion We propose a new challenge of physically situated commonsense inference that broadens the scope of natural language inference (NLI) with commonsense reasoning. To support research toward commonsense NLI, we create a large-scale dataset Swag with 113k multiple-choice questions. Our dataset is constructed using Adversarial Filtering (AF), a new paradigm for robust and cost-effective dataset construction that allows datasets to be constructed at scale while automatically reducing annotation artifacts that ca"
D18-1009,W17-1609,0,0.0793366,"Missing"
D18-1009,D17-1247,1,0.862848,"Missing"
D18-1009,D17-1099,1,0.847025,"Missing"
D18-1009,W16-0204,0,0.0509438,"Missing"
D18-1009,K17-1004,1,0.868554,"Missing"
D18-1009,P18-2119,0,0.0417135,"e gap between machine and human performance suggests that more is required to solve the dataset. As models are developed for commonsense inference, and more broadly as the field of NLP advances, we note that AF can be used again to create a more adversarial version of Swag using better language models and AF models. 6 Commonsense NLI Several datasets have been introduced to study NLI beyond linguistic entailment: for inferring likely causes and endings given a sentence (COPA; Roemmele et al., 2011), for choosing the most sensible ending to a short story (RocStories; Mostafazadeh et al., 2016; Sharma et al., 2018), and for predicting likelihood of a hypothesis by regressing to an ordinal label (JOCI; (Zhang et al., 2017)). These datasets are relatively small: 1k examples for COPA and 10k cloze examples for RocStories.13 JOCI increases the scale by generating the hypotheses using a knowledge graph or a neural model. In contrast to JOCI where the task was formulated as a regression task on the degree of plausibility of the hypothesis, we frame commonsense inference as a multiple choice question to reduce the potential ambiguity in the labels and to allow for direct comparison between machines and humans."
D18-1009,D17-1323,0,0.0268507,"This work was supported by the National Science Foundation Graduate Research Fellowship (DGE-1256082), the NSF grant (IIS1524371, 1703166), the DARPA CwC program through ARO (W911NF-15-1-0543), the IARPA DIVA program through D17PC00343, and gifts by Google and Facebook. The views and conclusions contained herein are those of the authors and should not be interpreted as representing endorsements of IARPA, DOI/IBC, or the U.S. Government. Reducing gender/racial bias Prior work has sought to reduce demographic biases in word embeddings (Zhang et al., 2018) as well as in image recognition models (Zhao et al., 2017). Our work has focused on producing a dataset with minimal annotation artifacts, which in turn helps to avoid some gender and racial biases that stem from elicitation (Rudinger et al., 2017). However, it is not perfect in this regard, particularly due to biases in movies (Schofield and Mehr, 2016; Sap et al., 2017). Our methodology could potentially be extended to construct datasets free of (possibly intersectional) gender or racial bias. References Physical knowledge Prior work has studied learning grounded knowledge about objects and verbs: from knowledge bases (Li et al., 2016), syntax pars"
D18-1009,P17-1076,0,0.0159913,"estions (73k training, 20k validation, 20k test) and is derived from pairs of consecutive video captions from ActivityNet Captions (Krishna et al., 2017; Heilbron et al., 2015) and the Large Scale Movie Description Challenge (LSMDC; Rohrbach et al., 2017). The two datasets are slightly different in nature and allow us to achieve broader coverage: ActivityNet contains 20k YouTube clips containing one of 203 activity types (such as doing gymnastics or playing guitar); LSMDC consists of 128k movie captions (audio descriptions and scripts). For each pair of captions, we use a constituency parser (Stern et al., 2017) to split the second sentence into noun and verb phrases (Figure 1).2 Each question has a human-verified gold ending and 3 distractors. 3 A solution to annotation artifacts In this section, we outline the construction of Swag. We seek dataset diversity while minimizing annotation artifacts, conditional stylistic patterns such as length and word-preference biases. For many NLI datasets, these biases have been shown to allow shallow models (e.g. bag-of-words) obtain artificially high performance. To avoid introducing easily “gamed” patterns, we present Adversarial Filtering (AF), a generallyappl"
D18-1009,W18-5446,0,0.122748,"Missing"
D18-1009,N18-1101,0,0.0773261,"e task was formulated as a regression task on the degree of plausibility of the hypothesis, we frame commonsense inference as a multiple choice question to reduce the potential ambiguity in the labels and to allow for direct comparison between machines and humans. In addition, Swag’s use of adversarial filtering increases diversity of situations and counterfactual generation quality. Related Work Entailment NLI There has been a long history of NLI benchmarks focusing on linguistic entailment (Cooper et al., 1996; Dagan et al., 2006; Marelli et al., 2014; Bowman et al., 2015; Lai et al., 2017; Williams et al., 2018). Recent NLI datasets in particular have supported learning broadly-applicable sentence representations (Conneau et al., 2017); moreover, models trained on these datasets were used as components 13 For RocStories, this was by design to encourage learning from the larger corpus of 98k sensible stories. 100 7 Last, another related task formulation is sentence completion or cloze, where the task is to predict a single word that is removed from a given context (Zweig and Burges, 2011; Paperno et al., 2016).14 Our work in contrast requires longer textual descriptions to reason about. Conclusion We"
D18-1009,P98-1013,0,\N,Missing
D18-1009,C98-1013,0,\N,Missing
D18-1009,P17-2097,0,\N,Missing
D18-1060,E06-1042,0,0.404381,"e a bidirectional LSTM to encode a sentence, and a feedforward neural network for classification, optimized for the log-likelihood of gold labels. ai = SoftMaxi (Wa hi + ba ) n X c= ai hi Sentence encoding For both sequence labeling and classification, we represent each token xi in the input sentence with a pre-trained word embedding wi . To further encode contextual information, we also concatenate ELMo (Embeddings from Language Models) vectors ei from Peters et al. (2018). These vectors have been shown to be useful for word sense disambiguation, a task closely related to metaphor detection (Birke and Sarkar, 2006). 1 3.1 people’s Figure 2: A classification model for metaphor detection. Only a single word per sentence is labeled as metaphorical or literal. that the prediction for the target verb can be extracted from the full sentence predictions. In addition, as will be shown in Section 5, we find that given accurate annotations for all words in a sentence, the sequence labeling model outperforms the classification model even when the evaluation is set up as a classification task. 3 the i=1 Finally, we feed c to a feedforward network to compute the label scores for target verb. 4 Dataset We evaluate pe"
D18-1060,S16-2003,0,0.148265,"or detection. given a sentence, detecting all of the metaphorical words (independent of their POS tags). We find that relatively standard architectures based on bi-directional LSTMs (Hochreiter and Schmidhuber, 1997) augmented with contextualized word embeddings (Peters et al., 2018) perform surprisingly well on both tasks, even with modest amount of training data. We improve the previous state-of-the-art by 7.5 F1 on the VU Amsterdam Metaphor Corpus (VUA) for the sequence labeling task (Steen et al., 2010), by 2.5 F1 on the VUA verb clasificiation dataset, and by 4.9 F1 on the MOH-X dataset (Mohammad et al., 2016). Our code is publicly available at https://github.com/gao-g/ metaphor-in-context. Introduction Metaphors are pervasive in natural language, and detecting them requires challenging contextual reasoning about whether specific situations can actually happen. (Lakoff and Johnson, 1980). For example, in Table 1, “examining” is metaphorical because it is impossible to literally use a “microscope” to examine an entire country. In this paper, we present end-to-end neural models for metaphor detection, which can learn rich contextual word representations that are crucial for accurate interpretation of"
D18-1060,W18-0911,0,0.172954,"Missing"
D18-1060,W18-0915,0,0.0459541,"Missing"
D18-1060,W18-0916,0,0.116839,"Missing"
D18-1060,E17-2084,0,0.561234,"Lakoff and Johnson, 1980). For example, in Table 1, “examining” is metaphorical because it is impossible to literally use a “microscope” to examine an entire country. In this paper, we present end-to-end neural models for metaphor detection, which can learn rich contextual word representations that are crucial for accurate interpretation of figurative language. In contrast, most previous approaches focused on limited forms of linguistic context, for example by only providing SVO triples such as (car, drink, gasoline) to the model (Shutova et al., 2016; Tsvetkov et al., 2013; Rei et al., 2017; Bulat et al., 2017). While the verbal arguments provide strong cues, providing the full sentential context supports more accurate prediction, as seen in Table 1. Even in the few cases when the full sentence is used (K¨oper and im Walde, 2017; Turney et al., 2011; Jang et al., 2016) existing models have used unigram-based features with limited expressivity. We investigate two common task formulations: (1) given a target verb in a sentence, classifying whether it is metaphorical or not, and (2) 2 Task We study two task formulations. Sequence Labeling: Given a sentence x1 ,. . . ,xn , predict a sequence of binary l"
D18-1060,D16-1220,0,0.110577,"Missing"
D18-1060,D14-1162,0,0.0805731,"Missing"
D18-1060,W13-0907,0,0.175116,"model outperforms the CLS model on detecting personifications, indirect metaphors, and direct metaphors involving uncommon verbs. 6 Related Work There has been significant work on studying different features for metaphor detection, including concretenesss and abstractness (Turney et al., 2011; Tsvetkov et al., 2014; K¨oper and im Walde, 2017), imaginability (Broadwell et al., 2013; Strzalkowski et al., 2013), feature norms (Bulat et al., 2017), sensory features (Tekiroglu et al., 2015; Shutova et al., 2016), bag-of-words features (K¨oper and im Walde, 2016), and semantic class using WordNet (Hovy et al., 2013; Tsvetkov et al., 2014). More recently, embedding-based approaches (K¨oper and im Walde, 2017; Rei et al., 2017) showed gains on various benchmarks. Many neural models with various features and architectures were introduced in the 2018 VUA Metaphor Detection Shared Task. They include LSTM-based models and CRFs augmented by linguistic features, such as WordNet, POS tags, concreteness score, unigrams, lemmas, verb clusters, 7 Conclusion In this paper, we present simple biLSTM models augmented with contextualized word representation for metaphor detection. Our models establish new state-of-the-a"
D18-1060,N18-1202,1,0.824212,"ese models establish a new state-of-the-art on existing verb metaphor detection benchmarks, and show strong performance on jointly predicting the metaphoricity of all words in a running text. 1 Table 1: Metaphorical usages of the target word are bold faced, and literal usages are italicized. Full sentence context is crucial for metaphor detection. given a sentence, detecting all of the metaphorical words (independent of their POS tags). We find that relatively standard architectures based on bi-directional LSTMs (Hochreiter and Schmidhuber, 1997) augmented with contextualized word embeddings (Peters et al., 2018) perform surprisingly well on both tasks, even with modest amount of training data. We improve the previous state-of-the-art by 7.5 F1 on the VU Amsterdam Metaphor Corpus (VUA) for the sequence labeling task (Steen et al., 2010), by 2.5 F1 on the VUA verb clasificiation dataset, and by 4.9 F1 on the MOH-X dataset (Mohammad et al., 2016). Our code is publicly available at https://github.com/gao-g/ metaphor-in-context. Introduction Metaphors are pervasive in natural language, and detecting them requires challenging contextual reasoning about whether specific situations can actually happen. (Lako"
D18-1060,W18-0908,0,0.101406,"Missing"
D18-1060,P16-1021,0,0.141362,"Missing"
D18-1060,D17-1162,0,0.758662,"actually happen. (Lakoff and Johnson, 1980). For example, in Table 1, “examining” is metaphorical because it is impossible to literally use a “microscope” to examine an entire country. In this paper, we present end-to-end neural models for metaphor detection, which can learn rich contextual word representations that are crucial for accurate interpretation of figurative language. In contrast, most previous approaches focused on limited forms of linguistic context, for example by only providing SVO triples such as (car, drink, gasoline) to the model (Shutova et al., 2016; Tsvetkov et al., 2013; Rei et al., 2017; Bulat et al., 2017). While the verbal arguments provide strong cues, providing the full sentential context supports more accurate prediction, as seen in Table 1. Even in the few cases when the full sentence is used (K¨oper and im Walde, 2017; Turney et al., 2011; Jang et al., 2016) existing models have used unigram-based features with limited expressivity. We investigate two common task formulations: (1) given a target verb in a sentence, classifying whether it is metaphorical or not, and (2) 2 Task We study two task formulations. Sequence Labeling: Given a sentence x1 ,. . . ,xn , predict a"
D18-1060,P16-2017,0,0.515134,"Missing"
D18-1060,N16-1020,0,0.465298,"soning about whether specific situations can actually happen. (Lakoff and Johnson, 1980). For example, in Table 1, “examining” is metaphorical because it is impossible to literally use a “microscope” to examine an entire country. In this paper, we present end-to-end neural models for metaphor detection, which can learn rich contextual word representations that are crucial for accurate interpretation of figurative language. In contrast, most previous approaches focused on limited forms of linguistic context, for example by only providing SVO triples such as (car, drink, gasoline) to the model (Shutova et al., 2016; Tsvetkov et al., 2013; Rei et al., 2017; Bulat et al., 2017). While the verbal arguments provide strong cues, providing the full sentential context supports more accurate prediction, as seen in Table 1. Even in the few cases when the full sentence is used (K¨oper and im Walde, 2017; Turney et al., 2011; Jang et al., 2016) existing models have used unigram-based features with limited expressivity. We investigate two common task formulations: (1) given a target verb in a sentence, classifying whether it is metaphorical or not, and (2) 2 Task We study two task formulations. Sequence Labeling: G"
D18-1060,W14-2302,0,0.421199,"Missing"
D18-1060,N16-1039,0,0.102738,"Missing"
D18-1060,W18-0918,0,0.131137,"Missing"
D18-1060,W17-1903,0,0.473949,"Missing"
D18-1060,W13-0909,0,0.043831,"Missing"
D18-1060,W18-0914,0,0.143054,"Missing"
D18-1060,W15-1404,0,0.497634,"nature of the task. We sampled 257 dev examples that the CLS model gets wrong but the SEQ model gets correct. We found that the SEQ model outperforms the CLS model on detecting personifications, indirect metaphors, and direct metaphors involving uncommon verbs. 6 Related Work There has been significant work on studying different features for metaphor detection, including concretenesss and abstractness (Turney et al., 2011; Tsvetkov et al., 2014; K¨oper and im Walde, 2017), imaginability (Broadwell et al., 2013; Strzalkowski et al., 2013), feature norms (Bulat et al., 2017), sensory features (Tekiroglu et al., 2015; Shutova et al., 2016), bag-of-words features (K¨oper and im Walde, 2016), and semantic class using WordNet (Hovy et al., 2013; Tsvetkov et al., 2014). More recently, embedding-based approaches (K¨oper and im Walde, 2017; Rei et al., 2017) showed gains on various benchmarks. Many neural models with various features and architectures were introduced in the 2018 VUA Metaphor Detection Shared Task. They include LSTM-based models and CRFs augmented by linguistic features, such as WordNet, POS tags, concreteness score, unigrams, lemmas, verb clusters, 7 Conclusion In this paper, we present simple"
D18-1060,P14-1024,0,0.626133,"Missing"
D18-1060,W13-0906,0,0.151614,"pecific situations can actually happen. (Lakoff and Johnson, 1980). For example, in Table 1, “examining” is metaphorical because it is impossible to literally use a “microscope” to examine an entire country. In this paper, we present end-to-end neural models for metaphor detection, which can learn rich contextual word representations that are crucial for accurate interpretation of figurative language. In contrast, most previous approaches focused on limited forms of linguistic context, for example by only providing SVO triples such as (car, drink, gasoline) to the model (Shutova et al., 2016; Tsvetkov et al., 2013; Rei et al., 2017; Bulat et al., 2017). While the verbal arguments provide strong cues, providing the full sentential context supports more accurate prediction, as seen in Table 1. Even in the few cases when the full sentence is used (K¨oper and im Walde, 2017; Turney et al., 2011; Jang et al., 2016) existing models have used unigram-based features with limited expressivity. We investigate two common task formulations: (1) given a target verb in a sentence, classifying whether it is metaphorical or not, and (2) 2 Task We study two task formulations. Sequence Labeling: Given a sentence x1 ,. ."
D18-1060,D11-1063,0,0.496762,"hich can learn rich contextual word representations that are crucial for accurate interpretation of figurative language. In contrast, most previous approaches focused on limited forms of linguistic context, for example by only providing SVO triples such as (car, drink, gasoline) to the model (Shutova et al., 2016; Tsvetkov et al., 2013; Rei et al., 2017; Bulat et al., 2017). While the verbal arguments provide strong cues, providing the full sentential context supports more accurate prediction, as seen in Table 1. Even in the few cases when the full sentence is used (K¨oper and im Walde, 2017; Turney et al., 2011; Jang et al., 2016) existing models have used unigram-based features with limited expressivity. We investigate two common task formulations: (1) given a target verb in a sentence, classifying whether it is metaphorical or not, and (2) 2 Task We study two task formulations. Sequence Labeling: Given a sentence x1 ,. . . ,xn , predict a sequence of binary labels l1 , . . . , ln to indicate the metaphoricity of each word. Classification: Given a sentence x1 , . . . , xn and a target verb index i, predict a binary label l to indicate the metaphoricity of the target xi . While both formulations hav"
D18-1060,W18-0913,0,0.113824,"Missing"
D18-1241,P18-1078,0,0.0356121,"r all other dialog acts (neither for affirmation and don’t follow up for continuation). Transition matrix We divide the supporting text into 12 chunks (with a special chunk for no answer) and use the transition matrix (computed from the training set) in Figure 5b to select an answer given the position of the previous answer. This baseline does not output other dialog acts. 5.2 Upper bounds Gold NA + TM This is the same transition matrix (TM) baseline as before, except that for questions whose gold annotations are no answer, we always output no answer. et al., 2016, BiDAF) with self-attention (Clark and Gardner, 2018) and contextualized embeddings.16 A token for no answer is appended to s to enable its prediction following Levy et al. (2017). Additionally, we modify the model for our task to also predict dialog acts, placing a classifier over the same representation used to predict the end position of the predicted span. BiDAF++ w/ k-ctx As BiDAF++ does not model any dialog context, we modify the passage and question embedding processes to consider the dialog history. We consider context from the previous k QA pairs.17 • Passage embedding We explicitly identify the previous k answers within the section tex"
D18-1241,D17-1070,0,0.0153231,"ng Naively prepending the previous k questions to the current question did not show gains in initial experiments. We opt instead to simply encode the dialog turn number within the question embedding. 5.4 Results Table 4 summarizes our results (each cell displays dev/test scores), where dialog acts are Yes/No (affirmation) and Follow up (continuation). For comparison to other datasets, we report F1 without filtering low-agreement QA pairs (F1’). Pretrained InferSent To test the importance of lexical matching in our dataset, we output the sentence in s whose pretrained InferSent representation (Conneau et al., 2017) has the highest cosine similarity to that of the question. Sanity check Overall, the poor sanity check results imply that is very challenging. Of these, following the transition matrix (TM) gives the best performance, reinforcing the observation that the dialog context plays a significant role in the task. Feature-rich logistic regression We train a logistic regression using Vowpal Wabbit (Langford et al., 2007) to select answer sentences. We use simple matching features (e.g., n-gram overlap between questions and candidate answers), bias features (position and length of a candidate), and con"
D18-1241,H94-1010,0,0.675779,"Missing"
D18-1241,K17-1034,1,0.798034,"ext into 12 chunks (with a special chunk for no answer) and use the transition matrix (computed from the training set) in Figure 5b to select an answer given the position of the previous answer. This baseline does not output other dialog acts. 5.2 Upper bounds Gold NA + TM This is the same transition matrix (TM) baseline as before, except that for questions whose gold annotations are no answer, we always output no answer. et al., 2016, BiDAF) with self-attention (Clark and Gardner, 2018) and contextualized embeddings.16 A token for no answer is appended to s to enable its prediction following Levy et al. (2017). Additionally, we modify the model for our task to also predict dialog acts, placing a classifier over the same representation used to predict the end position of the predicted span. BiDAF++ w/ k-ctx As BiDAF++ does not model any dialog context, we modify the passage and question embedding processes to consider the dialog history. We consider context from the previous k QA pairs.17 • Passage embedding We explicitly identify the previous k answers within the section text by concatenating marker embeddings to the existing word embeddings. Gold sentence + NA To see if can be treated as an answer"
D18-1241,D17-1259,0,0.0131978,"of CoQA answers are named entities or short noun phrases, much like those in SQuAD. In contrast, the asymmetric nature of forces students to ask more exploratory questions whose answers can be potentially be followed up on.19 Dialog fits into an increasing interest in open domain dialog, mostly studied in the context of social chit-chat (Li et al., 2016; Ritter et al., 2011; Fang et al., 2017; Ghazvininejad et al., 2018). Most related to our effort is visual dialog (Das et al., 2017), which relies on images as evidence instead of text. More explicit goal driven scenarios, such as bargaining (Lewis et al., 2017) and item guessing (He et al., 2017) have also been explored, but the language is more constrained than in . Information-seeking dialog specifically was studied in Stede and Schlangen (2004). 7 Conclusion In this paper, we introduce , a large scale dataset of information-seeking dialogs over sections from Wikipedia articles. Our data collection process, which takes the form of a teacher-student interaction between two crowd workers, encourages questions that are highly contextual, openended, and even unanswerable from the text. Our baselines, which include top performers on existing machine co"
D18-1241,D16-1127,0,0.0402491,"xt (such as traffic laws) by interacting with a user through dialog. Also concurrently, Reddy et al. (2018) propose conversational question answering (CoQA) from text but allow both students and questioners to see the evidence. As a result, a large percentage of CoQA answers are named entities or short noun phrases, much like those in SQuAD. In contrast, the asymmetric nature of forces students to ask more exploratory questions whose answers can be potentially be followed up on.19 Dialog fits into an increasing interest in open domain dialog, mostly studied in the context of social chit-chat (Li et al., 2016; Ritter et al., 2011; Fang et al., 2017; Ghazvininejad et al., 2018). Most related to our effort is visual dialog (Das et al., 2017), which relies on images as evidence instead of text. More explicit goal driven scenarios, such as bargaining (Lewis et al., 2017) and item guessing (He et al., 2017) have also been explored, but the language is more constrained than in . Information-seeking dialog specifically was studied in Stede and Schlangen (2004). 7 Conclusion In this paper, we introduce , a large scale dataset of information-seeking dialogs over sections from Wikipedia articles. Our data c"
D18-1241,P18-2124,1,0.903645,"the evidence text, as in existing reading comprehension tasks (Rajpurkar et al., 2016). Additionally, the teacher uses dialog acts to provide the student with feedback (e.g., “ask a follow up ques2174 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2174–2184 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Dataset QuAC CoQA (Reddy et al., 2018) CSQA (Saha et al., 2018) CQA (Talmor and Berant, 2018) SQA (Iyyer et al., 2017) NarrativeQA (Kocisk´y et al., 2017) TriviaQA (Joshi et al., 2017) SQuAD 2.0 (Rajpurkar et al., 2018) MS Marco (Nguyen et al., 2016) NewsQA (Trischler et al., 2016) Multi turn Textbased Dialog Acts Simple Evaluation Unanswerable Questions Asker Can’t See Evidence 4 4 4 4 4 4 4 4 4 4 7 7 7 7 7 4 7 4 7 4 4 4 4 4 7 7 7 7 7 7 7 7 7 4 7 4 4 7 4 4 7 4 4 4 7 7 7 7 4 4 4 7 7 4 7 4 4 7 4 4 Table 1: Comparison of the QUAC dataset to other question answering datasets. tion”), which makes the dialogs more productive. We collect the dataset in an interactive setting where two crowd workers play the roles of teacher and student. To encourage natural and diverse questions, we do not follow previous dialogst"
D18-1241,P17-1162,1,0.821322,"short noun phrases, much like those in SQuAD. In contrast, the asymmetric nature of forces students to ask more exploratory questions whose answers can be potentially be followed up on.19 Dialog fits into an increasing interest in open domain dialog, mostly studied in the context of social chit-chat (Li et al., 2016; Ritter et al., 2011; Fang et al., 2017; Ghazvininejad et al., 2018). Most related to our effort is visual dialog (Das et al., 2017), which relies on images as evidence instead of text. More explicit goal driven scenarios, such as bargaining (Lewis et al., 2017) and item guessing (He et al., 2017) have also been explored, but the language is more constrained than in . Information-seeking dialog specifically was studied in Stede and Schlangen (2004). 7 Conclusion In this paper, we introduce , a large scale dataset of information-seeking dialogs over sections from Wikipedia articles. Our data collection process, which takes the form of a teacher-student interaction between two crowd workers, encourages questions that are highly contextual, openended, and even unanswerable from the text. Our baselines, which include top performers on existing machine comprehension datasets, significantly"
D18-1241,D16-1264,1,0.889651,", who does not see the section text, asks questions. The teacher provides a response in the form of a text span (or No answer ), optionally yes or no ( Yes / No ), and encouragement about continuing a ¯ , or should line of questioning (should, ,→ , could ,→ not 6,→ ask a follow-up question). Wikipedia page), which only the teacher can access. Given just the section’s heading, “Origin & History”, the student aims to learn as much as possible about its contents by asking questions. The teacher answers these questions with spans from the evidence text, as in existing reading comprehension tasks (Rajpurkar et al., 2016). Additionally, the teacher uses dialog acts to provide the student with feedback (e.g., “ask a follow up ques2174 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2174–2184 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Dataset QuAC CoQA (Reddy et al., 2018) CSQA (Saha et al., 2018) CQA (Talmor and Berant, 2018) SQA (Iyyer et al., 2017) NarrativeQA (Kocisk´y et al., 2017) TriviaQA (Joshi et al., 2017) SQuAD 2.0 (Rajpurkar et al., 2018) MS Marco (Nguyen et al., 2016) NewsQA (Trischler et al., 2016)"
D18-1241,P17-1167,1,0.880503,"ible about its contents by asking questions. The teacher answers these questions with spans from the evidence text, as in existing reading comprehension tasks (Rajpurkar et al., 2016). Additionally, the teacher uses dialog acts to provide the student with feedback (e.g., “ask a follow up ques2174 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2174–2184 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Dataset QuAC CoQA (Reddy et al., 2018) CSQA (Saha et al., 2018) CQA (Talmor and Berant, 2018) SQA (Iyyer et al., 2017) NarrativeQA (Kocisk´y et al., 2017) TriviaQA (Joshi et al., 2017) SQuAD 2.0 (Rajpurkar et al., 2018) MS Marco (Nguyen et al., 2016) NewsQA (Trischler et al., 2016) Multi turn Textbased Dialog Acts Simple Evaluation Unanswerable Questions Asker Can’t See Evidence 4 4 4 4 4 4 4 4 4 4 7 7 7 7 7 4 7 4 7 4 4 4 4 4 7 7 7 7 7 7 7 7 7 4 7 4 4 7 4 4 7 4 4 4 7 7 7 7 4 4 4 7 7 4 7 4 4 7 4 4 Table 1: Comparison of the QUAC dataset to other question answering datasets. tion”), which makes the dialogs more productive. We collect the dataset in an interactive setting where two crowd workers play the roles o"
D18-1241,P17-1147,1,0.902703,"hese questions with spans from the evidence text, as in existing reading comprehension tasks (Rajpurkar et al., 2016). Additionally, the teacher uses dialog acts to provide the student with feedback (e.g., “ask a follow up ques2174 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2174–2184 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Dataset QuAC CoQA (Reddy et al., 2018) CSQA (Saha et al., 2018) CQA (Talmor and Berant, 2018) SQA (Iyyer et al., 2017) NarrativeQA (Kocisk´y et al., 2017) TriviaQA (Joshi et al., 2017) SQuAD 2.0 (Rajpurkar et al., 2018) MS Marco (Nguyen et al., 2016) NewsQA (Trischler et al., 2016) Multi turn Textbased Dialog Acts Simple Evaluation Unanswerable Questions Asker Can’t See Evidence 4 4 4 4 4 4 4 4 4 4 7 7 7 7 7 4 7 4 7 4 4 4 4 4 7 7 7 7 7 7 7 7 7 4 7 4 4 7 4 4 7 4 4 4 7 7 7 7 4 4 4 7 7 4 7 4 4 7 4 4 Table 1: Comparison of the QUAC dataset to other question answering datasets. tion”), which makes the dialogs more productive. We collect the dataset in an interactive setting where two crowd workers play the roles of teacher and student. To encourage natural and diverse questions,"
D18-1241,D11-1054,0,0.0542229,"ic laws) by interacting with a user through dialog. Also concurrently, Reddy et al. (2018) propose conversational question answering (CoQA) from text but allow both students and questioners to see the evidence. As a result, a large percentage of CoQA answers are named entities or short noun phrases, much like those in SQuAD. In contrast, the asymmetric nature of forces students to ask more exploratory questions whose answers can be potentially be followed up on.19 Dialog fits into an increasing interest in open domain dialog, mostly studied in the context of social chit-chat (Li et al., 2016; Ritter et al., 2011; Fang et al., 2017; Ghazvininejad et al., 2018). Most related to our effort is visual dialog (Das et al., 2017), which relies on images as evidence instead of text. More explicit goal driven scenarios, such as bargaining (Lewis et al., 2017) and item guessing (He et al., 2017) have also been explored, but the language is more constrained than in . Information-seeking dialog specifically was studied in Stede and Schlangen (2004). 7 Conclusion In this paper, we introduce , a large scale dataset of information-seeking dialogs over sections from Wikipedia articles. Our data collection process, wh"
D18-1241,D18-1233,0,0.203396,"are the first to incorporate these into information-seeking dialog. Sequential QA Our work is similar to sequential question answering against knowledge bases (Iyyer et al., 2017) and the web (Talmor and Berant, 2018), but instead of decomposing a single question into smaller questions, we rely on the curiosity of the student to generate a sequence of questions. Such open information seeking was studied in semantic parsing on knowledge bases (Dahl et al., 1994) and more recently with modern approaches (Saha et al., 2018), but with questions paraphrased from templates. Concurrent to our work, Saeidi et al. (2018) proposed a task of generating and answering yes/no questions for rule focused text (such as traffic laws) by interacting with a user through dialog. Also concurrently, Reddy et al. (2018) propose conversational question answering (CoQA) from text but allow both students and questioners to see the evidence. As a result, a large percentage of CoQA answers are named entities or short noun phrases, much like those in SQuAD. In contrast, the asymmetric nature of forces students to ask more exploratory questions whose answers can be potentially be followed up on.19 Dialog fits into an increasing in"
D18-1241,N18-1059,0,0.1584,"aims to learn as much as possible about its contents by asking questions. The teacher answers these questions with spans from the evidence text, as in existing reading comprehension tasks (Rajpurkar et al., 2016). Additionally, the teacher uses dialog acts to provide the student with feedback (e.g., “ask a follow up ques2174 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2174–2184 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Dataset QuAC CoQA (Reddy et al., 2018) CSQA (Saha et al., 2018) CQA (Talmor and Berant, 2018) SQA (Iyyer et al., 2017) NarrativeQA (Kocisk´y et al., 2017) TriviaQA (Joshi et al., 2017) SQuAD 2.0 (Rajpurkar et al., 2018) MS Marco (Nguyen et al., 2016) NewsQA (Trischler et al., 2016) Multi turn Textbased Dialog Acts Simple Evaluation Unanswerable Questions Asker Can’t See Evidence 4 4 4 4 4 4 4 4 4 4 7 7 7 7 7 4 7 4 7 4 4 4 4 4 7 7 7 7 7 7 7 7 7 4 7 4 4 7 4 4 7 4 4 4 7 7 7 7 4 4 4 7 7 4 7 4 4 7 4 4 Table 1: Comparison of the QUAC dataset to other question answering datasets. tion”), which makes the dialogs more productive. We collect the dataset in an interactive setting where two crowd"
D19-1159,D14-1162,0,0.0821077,"puting top-k rollouts. 3 P RETRAINED LM S AND S TOCHASTIC S AMPLING 1495 • fθx→e : x → e, where x = [x1 , · · · , xL ] is represented as its (contextualized) word embedding form e = [e1 , · · · , eL ], with ei as the representation for word xi ; • fθe→z : e → z t : For each embedded instruction e, we ground its representations as ci,t for state st via neural attention. To handle language variability, one may aggregate features of multiple instructions Ct = {ci,t }M i=1 1 PM into a single joint feature z t = M c .4 i,t i=1 Previous methods in VLN learn e either from pretrained word embeddings (Pennington et al., 2014) which do not take into account word context, or from scratch. As a result, their representations do not capture contextual information within each instruction. More importantly, they tend to overfit the training instructions associated with seen environments, limiting their utility in unseen environments. To remedy these issues, we propose to represent e with contextualized word embeddings produced using large-scale pretrained language models, such as BERT and GPT. Instruction Encoder. The agent’s memory vector ht−1 captures the perception and action history and is used to attend to the instr"
D19-1159,N19-1268,0,0.10248,". gio et al., 2015). Two widely used training strategies are student-forcing and teacher-forcing (described in detail in Section 2.2). It is well-known that the sequence length determines which training strategy is more effective. In the VLN literature, student-forcing has been widely used, as early work (Anderson et al., 2018) used long trajectories (up to 20 steps) with a simple discrete action space. Most recent work, however, has relied on a panoramic action space (Fried et al., 2018) in which most trajectories are only up to seven steps long. In such cases, teacher-forcing is preferable (Tan et al., 2019). Neither strategy is perfect: teacher-forcing has exposure bias, while studentforcing’s random actions can cause an agent to deviate far from the correct path, rendering the original instruction invalid.2 To tackle these challenges, we have developed two techniques to enable the agent to navigate more efficiently. For the first challenge, we leverage the recent large-scale pretrained language models, BERT (Devlin et al., 2019) and GPT (Radford et al., 2018), to improve the agent’s robustness in unseen environments. We show that large-scale language-only pretraining improves generalization in"
D19-1159,N19-1423,0,\N,Missing
D19-1159,N19-1197,1,\N,Missing
D19-1243,P17-2097,0,0.0721618,"Missing"
D19-1243,P16-1223,0,0.107266,"Missing"
D19-1243,P17-1168,0,0.338389,"Commonsense-RC (Wang et al., 2018a) applies three-way unidirectional attention to model interactions between paragraph, question, and answers. exp(W &gt; f Fj )) Baseline Methods We explore two categories of baseline methods: reading comprehension approaches and pretrained language model based approaches. Sliding Window (Richardson et al., 2013) measures the similarity of each candidate answer with each window with m words of the paragraph. Stanford Attentive Reader (Chen et al., 2016) performs a bilinear attention between the question and paragraph for answer prediction. Gated-Attention Reader (Dhingra et al., 2017) performs multi-hop attention between the question GPT-FT (Radford et al., 2018) is based on a generative pre-trained transformer language model, following a fine-tuning step on C OSMOS QA. BERT-FT (Devlin et al., 2018) is a pre-trained bidirectional transformer language model following a fine-tuning step on C OSMOS QA. DMCN (Zhang et al., 2019a) performs dual attention between paragraph and question/answer over BERT encoding output. Human Performance To get human performance on C OSMOS QA, we randomly sample 200 question sets from the test set, and ask 3 workers 2395 Input [CLS] Paragraph [SE"
D19-1243,P16-1086,0,0.0449703,"Missing"
D19-1243,Q18-1023,0,0.116959,"Missing"
D19-1243,D17-1082,0,0.076139,"r. B: She would take the baby to work. C: She would leave the baby alone at home. D: None of the above choices. Knowledge Transfer Through Fine-tuning P3: My head hurts. I had so much fun at a chat with some scrap friends last Saturday night that I forgot to sleep. I ended up crawling into bed around 7AM. Recent studies (Howard and Ruder, 2018; Min et al., 2017; Devlin et al., 2018) have shown the benefit of fine-tuning on similar tasks or datasets for knowledge transfer. Considering the unique challenge of C OSMOS, we explore two related multiple-choice datasets for knowledge transfer: RACE (Lai et al., 2017), a large-scale reading comprehension dataset, and SWAG (Zellers et al., 2018), a large-scale commonsense inference dataset. Specifically, we first fine-tune BERT on RACE or SWAG or both, and directly test on C OS MOS to show the impact of knowledge transfer. Furthermore, we sequentially fine-tune BERT on both RACE or SWAG and C OSMOS. As Table 5 shows, with direct knowledge transfer, RACE provides significant benefit than SWAG since C OS MOS requires more understanding of the interaction between paragraph, question and each candidate answer. With sequentially fine-tuning, SWAG provides better"
D19-1243,P17-2081,0,0.0223001,"rsing job was deemed by the other moms to be useful and worthwhile --in fact, worth putting her baby into daycare for &quot;just a few hours, what harm could it do?” Q: What would happened if she could not find a daycare? A: She would try to find a babysitter. B: She would take the baby to work. C: She would leave the baby alone at home. D: None of the above choices. Knowledge Transfer Through Fine-tuning P3: My head hurts. I had so much fun at a chat with some scrap friends last Saturday night that I forgot to sleep. I ended up crawling into bed around 7AM. Recent studies (Howard and Ruder, 2018; Min et al., 2017; Devlin et al., 2018) have shown the benefit of fine-tuning on similar tasks or datasets for knowledge transfer. Considering the unique challenge of C OSMOS, we explore two related multiple-choice datasets for knowledge transfer: RACE (Lai et al., 2017), a large-scale reading comprehension dataset, and SWAG (Zellers et al., 2018), a large-scale commonsense inference dataset. Specifically, we first fine-tune BERT on RACE or SWAG or both, and directly test on C OS MOS to show the impact of knowledge transfer. Furthermore, we sequentially fine-tune BERT on both RACE or SWAG and C OSMOS. As Table"
D19-1243,N18-1144,1,0.879739,"Missing"
D19-1243,N18-2017,0,0.0596336,"Missing"
D19-1243,L18-1564,0,0.0640017,"Missing"
D19-1243,P02-1040,0,0.104846,"Missing"
D19-1243,P18-1031,0,0.021995,"for the evil MSM, her nursing job was deemed by the other moms to be useful and worthwhile --in fact, worth putting her baby into daycare for &quot;just a few hours, what harm could it do?” Q: What would happened if she could not find a daycare? A: She would try to find a babysitter. B: She would take the baby to work. C: She would leave the baby alone at home. D: None of the above choices. Knowledge Transfer Through Fine-tuning P3: My head hurts. I had so much fun at a chat with some scrap friends last Saturday night that I forgot to sleep. I ended up crawling into bed around 7AM. Recent studies (Howard and Ruder, 2018; Min et al., 2017; Devlin et al., 2018) have shown the benefit of fine-tuning on similar tasks or datasets for knowledge transfer. Considering the unique challenge of C OSMOS, we explore two related multiple-choice datasets for knowledge transfer: RACE (Lai et al., 2017), a large-scale reading comprehension dataset, and SWAG (Zellers et al., 2018), a large-scale commonsense inference dataset. Specifically, we first fine-tune BERT on RACE or SWAG or both, and directly test on C OS MOS to show the impact of knowledge transfer. Furthermore, we sequentially fine-tune BERT on both RACE or SWAG and"
D19-1243,D12-1071,0,0.0915068,"Missing"
D19-1243,P18-2124,0,0.0662262,"Missing"
D19-1243,D16-1264,0,0.0795265,"Missing"
D19-1243,P18-1213,1,0.870954,"Missing"
D19-1243,D13-1020,0,0.472252,"tes. Classification For each candidate answer Ai , we compute the loss as follows: Co-Matching (Wang et al., 2018b) captures the interactions between question and paragraph, as well as answer and paragraph with attention. L(Ai |P, Q) = 4 4.1 Experiments log P4 exp(W &gt; f Fi ) j=1 Commonsense-RC (Wang et al., 2018a) applies three-way unidirectional attention to model interactions between paragraph, question, and answers. exp(W &gt; f Fj )) Baseline Methods We explore two categories of baseline methods: reading comprehension approaches and pretrained language model based approaches. Sliding Window (Richardson et al., 2013) measures the similarity of each candidate answer with each window with m words of the paragraph. Stanford Attentive Reader (Chen et al., 2016) performs a bilinear attention between the question and paragraph for answer prediction. Gated-Attention Reader (Dhingra et al., 2017) performs multi-hop attention between the question GPT-FT (Radford et al., 2018) is based on a generative pre-trained transformer language model, following a fine-tuning step on C OSMOS QA. BERT-FT (Devlin et al., 2018) is a pre-trained bidirectional transformer language model following a fine-tuning step on C OSMOS QA. D"
D19-1243,S18-1120,0,0.374025,"both the question and answer, and get question-attentive, answer-attentive, and question and answer-attentive paragraph representations ˜ P = HP W t + b t H MQ P ˜ P H&gt; = Softmax(H Q )HQ &gt; ˜ MA P = Softmax(HP HA )HA ˜ P H&gt; MQA = Softmax(H QA )HQA P Table 2: The distribution of contextual commonsense reasoning types in C OSMOS. 3 tional attention over the BERT encoding output. Figure 4 shows the overview of the architecture. where W t and bt are learnable parameters. Next we fuse these representations with the original encoding output of P Model BERT with Multiway Attention Multiway attention (Wang et al., 2018a; Zhu et al., 2018) has been shown to be effective in capturing the interactions between each pair of input paragraph, question and candidate answers, leading to better context interpretation, while BERT finetuning (Devlin et al., 2018) also shows its prominent ability in commonsense inference. To further enhance the context understanding ability of BERT fine-tuning, we perform multiway bidirecQ FQ P = ([HP MP : HP MQ P ]W P + bP ) A FA P = ([HP MP : HP MA P ]W P + bP ) FQA = ([HP MQA : HP P P MQA P ]W P + bP ) where [:] denotes concatenation operation. W P , bP are learnable parameters for f"
D19-1243,P18-2118,0,0.244518,"both the question and answer, and get question-attentive, answer-attentive, and question and answer-attentive paragraph representations ˜ P = HP W t + b t H MQ P ˜ P H&gt; = Softmax(H Q )HQ &gt; ˜ MA P = Softmax(HP HA )HA ˜ P H&gt; MQA = Softmax(H QA )HQA P Table 2: The distribution of contextual commonsense reasoning types in C OSMOS. 3 tional attention over the BERT encoding output. Figure 4 shows the overview of the architecture. where W t and bt are learnable parameters. Next we fuse these representations with the original encoding output of P Model BERT with Multiway Attention Multiway attention (Wang et al., 2018a; Zhu et al., 2018) has been shown to be effective in capturing the interactions between each pair of input paragraph, question and candidate answers, leading to better context interpretation, while BERT finetuning (Devlin et al., 2018) also shows its prominent ability in commonsense inference. To further enhance the context understanding ability of BERT fine-tuning, we perform multiway bidirecQ FQ P = ([HP MP : HP MQ P ]W P + bP ) A FA P = ([HP MP : HP MA P ]W P + bP ) FQA = ([HP MQA : HP P P MQA P ]W P + bP ) where [:] denotes concatenation operation. W P , bP are learnable parameters for f"
D19-1243,D18-1009,1,0.750273,"t home. D: None of the above choices. Knowledge Transfer Through Fine-tuning P3: My head hurts. I had so much fun at a chat with some scrap friends last Saturday night that I forgot to sleep. I ended up crawling into bed around 7AM. Recent studies (Howard and Ruder, 2018; Min et al., 2017; Devlin et al., 2018) have shown the benefit of fine-tuning on similar tasks or datasets for knowledge transfer. Considering the unique challenge of C OSMOS, we explore two related multiple-choice datasets for knowledge transfer: RACE (Lai et al., 2017), a large-scale reading comprehension dataset, and SWAG (Zellers et al., 2018), a large-scale commonsense inference dataset. Specifically, we first fine-tune BERT on RACE or SWAG or both, and directly test on C OS MOS to show the impact of knowledge transfer. Furthermore, we sequentially fine-tune BERT on both RACE or SWAG and C OSMOS. As Table 5 shows, with direct knowledge transfer, RACE provides significant benefit than SWAG since C OS MOS requires more understanding of the interaction between paragraph, question and each candidate answer. With sequentially fine-tuning, SWAG provides better performance, which indicates that with fine-tuning on SWAG, BERT can obtain b"
D19-1243,D19-1454,1,0.861521,"Missing"
D19-1243,W17-2623,0,0.0925835,"Missing"
D19-1389,P00-1041,0,0.441992,"Missing"
D19-1389,N19-1071,0,0.259814,"ed by the next sentence “The city returned to Chinese control in 1997”, the information bottleneck would suggest that minute details such as the city’s population being over 7 million are relatively less important to keep. In contrast, the continued discussion of the city’s governance in the next sentence suggests its former British rule is important here. This intuition contrasts with that of autoencoder-based approaches where the goal is to minimize the reconstruction loss of the input sentence when constructing the summary (Miao and Blunsom, 2016; Wang and Lee, 2018; Fevry and Phang, 2018; Baziotis et al., 2019). Under the reconstruction loss, minute but specific details such as the city’s population being over 7 million will be difficult to discard from the summary, because they are useful for reconstruction. Concretely, BottleSumEx is an extractive and unsupervised sentence summarization method using the next sentence, a sample of nearby context, as guidance to relevance, or what information to keep. We capture this with a conditional language modelling objective, allowing us to benefit from powerful deep neural language models that are pre-trained over an extremely large-scale corpus. Under the In"
D19-1389,P16-1046,0,0.0461084,"CNN summaries. Abstractiveness is omitted for strictly extractive approaches quence models, trained on a large corpus of headlines with the first sentences of newspaper articles as supervision. This followed early work on approaching headline generation as statistical machine translation (Banko et al., 2000). Subsequently, recurrent neural networks with pointergenerator decoders became the standard model for this task, and focus shifted to document-level summarization (Nallapati et al., 2016; See et al., 2017). Pointer-based neural models have also been proposed for extractive summarization (Cheng and Lapata, 2016). The main limitations of this approach stem from the fact that the training data is constructed heuristically, covering a very specific type of sentence summarization (headline generation). Thus, these supervised models do not generalize well to other kinds of sentence summarization or domains. In contrast, our method is applicable to any domain for which examples of the inputs to be summarized are available in context. 6.2 Rush et al. (2015) first proposed abstractive sentence compression with neural sequence to seModel Unsupervised Summarization Miao and Blunsom (2016) framed sentence compr"
D19-1389,K18-1040,0,0.311209,", ...”, which is followed by the next sentence “The city returned to Chinese control in 1997”, the information bottleneck would suggest that minute details such as the city’s population being over 7 million are relatively less important to keep. In contrast, the continued discussion of the city’s governance in the next sentence suggests its former British rule is important here. This intuition contrasts with that of autoencoder-based approaches where the goal is to minimize the reconstruction loss of the input sentence when constructing the summary (Miao and Blunsom, 2016; Wang and Lee, 2018; Fevry and Phang, 2018; Baziotis et al., 2019). Under the reconstruction loss, minute but specific details such as the city’s population being over 7 million will be difficult to discard from the summary, because they are useful for reconstruction. Concretely, BottleSumEx is an extractive and unsupervised sentence summarization method using the next sentence, a sample of nearby context, as guidance to relevance, or what information to keep. We capture this with a conditional language modelling objective, allowing us to benefit from powerful deep neural language models that are pre-trained over an extremely large-sc"
D19-1389,D17-1222,0,0.0286562,"Missing"
D19-1389,D16-1031,0,0.262465,"metropolis with a population over 7 million, ...”, which is followed by the next sentence “The city returned to Chinese control in 1997”, the information bottleneck would suggest that minute details such as the city’s population being over 7 million are relatively less important to keep. In contrast, the continued discussion of the city’s governance in the next sentence suggests its former British rule is important here. This intuition contrasts with that of autoencoder-based approaches where the goal is to minimize the reconstruction loss of the input sentence when constructing the summary (Miao and Blunsom, 2016; Wang and Lee, 2018; Fevry and Phang, 2018; Baziotis et al., 2019). Under the reconstruction loss, minute but specific details such as the city’s population being over 7 million will be difficult to discard from the summary, because they are useful for reconstruction. Concretely, BottleSumEx is an extractive and unsupervised sentence summarization method using the next sentence, a sample of nearby context, as guidance to relevance, or what information to keep. We capture this with a conditional language modelling objective, allowing us to benefit from powerful deep neural language models that"
D19-1389,D15-1044,0,0.711049,"sformer-based language model is trained on the output summaries of our unsupervised method. Empirical results demonstrate that our extractive method outperforms other unsupervised models on multiple automatic metrics. In addition, we find that our selfsupervised abstractive model outperforms unsupervised baselines (including our own) by human evaluation along multiple attributes. 1 Hong Kong has population over 7 million, was once under British Rule. Introduction Recent approaches based on neural networks have brought significant advancements for both extractive and abstractive summarization (Rush et al., 2015; Nallapati et al., 2016). However, their success relies on large-scale parallel corpora of input text and output summaries for direct supervision. For example, there are ~280,000 training instances in the CNN/Daily Mail dataset (Nallapati et al., 2016; Hermann et al., 2015), and ~4,000,000 instances in the sentence summarization dataset of Rush et al. (2015). Because it is too costly to have humans write gold summaries at this scale, existing large-scale datasets are based on naturally occurring pairs of summary-like text paired with source text, for instance using news titles or highlights a"
D19-1389,E17-2007,0,0.0214037,"es. 5.1 Setup We evaluate our methods and baselines using automatic ROUGE metrics (1,2,L) on the DUC-2003 and DUC-2004 datasets (Over et al., 2007), similar to the evaluation used by Baziotis et al. (2019). DUC-2003 and DUC-2004 consist of 624 and 500 sentence-summary pairs respectively. Sentences are taken from newstext, and each summary consists of 4 human-written reference summaries capped at 75 bytes. We recover next-sentences from DUC articles for BottleSumEx . We also employ human evaluation as a point of comparison between models. This is both to combat known issues with ROUGE metrics (Schluter, 2017) and to experiment beyond limited supervised domains. Studying unsupervised methods allows for comparison over a much wider range of data where training summary pairs are not available, which we take advantage of here by summarizing sentences from the non-anonymized CNN corpus (Hermann et al., 2015; Nallapati et al., 2016; See et al., 2017). We use Amazon Mechanical Turk (AMT) for human evaluation, summarizing on 100 sentences sampled from a held out set. Evaluation between systems is primarily done as a pairwise comparison between BottleSum models and baselines, over 3 attributes: coherence,"
D19-1389,P17-1099,0,0.540581,"consists of 4 human-written reference summaries capped at 75 bytes. We recover next-sentences from DUC articles for BottleSumEx . We also employ human evaluation as a point of comparison between models. This is both to combat known issues with ROUGE metrics (Schluter, 2017) and to experiment beyond limited supervised domains. Studying unsupervised methods allows for comparison over a much wider range of data where training summary pairs are not available, which we take advantage of here by summarizing sentences from the non-anonymized CNN corpus (Hermann et al., 2015; Nallapati et al., 2016; See et al., 2017). We use Amazon Mechanical Turk (AMT) for human evaluation, summarizing on 100 sentences sampled from a held out set. Evaluation between systems is primarily done as a pairwise comparison between BottleSum models and baselines, over 3 attributes: coherence, conciseness, and agreement with the input. AMT workers are then asked to make a final judgement of which summary has higher overall quality. Each comparison is done by 3 different workers. Results are aggregated across workers and examples. 5.2 Models In both experiments, BottleSumEx is executed as described in section 3.2. In experiments o"
D19-1389,D18-1451,0,0.338342,"ation over 7 million, ...”, which is followed by the next sentence “The city returned to Chinese control in 1997”, the information bottleneck would suggest that minute details such as the city’s population being over 7 million are relatively less important to keep. In contrast, the continued discussion of the city’s governance in the next sentence suggests its former British rule is important here. This intuition contrasts with that of autoencoder-based approaches where the goal is to minimize the reconstruction loss of the input sentence when constructing the summary (Miao and Blunsom, 2016; Wang and Lee, 2018; Fevry and Phang, 2018; Baziotis et al., 2019). Under the reconstruction loss, minute but specific details such as the city’s population being over 7 million will be difficult to discard from the summary, because they are useful for reconstruction. Concretely, BottleSumEx is an extractive and unsupervised sentence summarization method using the next sentence, a sample of nearby context, as guidance to relevance, or what information to keep. We capture this with a conditional language modelling objective, allowing us to benefit from powerful deep neural language models that are pre-trained ove"
D19-1389,P19-1503,0,0.351456,"aries, so while they obtain better results than purely unsupervised approaches like ours their results are not directly comparable. Recently Baziotis et al. (2019) proposed a differentiable autoencoder using a gumbel-softmax to represent the distribution over summaries. The model is trained with a straight-through estimator as an alternative to reinforcement learning, obtaining better results on unsupervised summarization. All of these approaches have in common autoencoder-based training, which we argue does not naturally capture an appropriate notion of relevance for summarization. Recently, Zhou and Rush (2019) introduced a promising method for summarization using contextual matching with pretrained language models. While contextual matching requires pretrained language models to generate contextual vectors, BottleSum methods do not have specific architectural constraints. Also, like Wang and Lee (2018) it trains with unpaired summaries and so is not directly comparable to us. 7 Conclusion We have presented BottleSumEx , an unsupervised extractived approach to sentence summarization, and extended this to BottleSumSelf , a selfsupervised abstractive approach. BottleSumEx , which can be applied withou"
D19-1389,K16-1028,0,0.14292,"age model is trained on the output summaries of our unsupervised method. Empirical results demonstrate that our extractive method outperforms other unsupervised models on multiple automatic metrics. In addition, we find that our selfsupervised abstractive model outperforms unsupervised baselines (including our own) by human evaluation along multiple attributes. 1 Hong Kong has population over 7 million, was once under British Rule. Introduction Recent approaches based on neural networks have brought significant advancements for both extractive and abstractive summarization (Rush et al., 2015; Nallapati et al., 2016). However, their success relies on large-scale parallel corpora of input text and output summaries for direct supervision. For example, there are ~280,000 training instances in the CNN/Daily Mail dataset (Nallapati et al., 2016; Hermann et al., 2015), and ~4,000,000 instances in the sentence summarization dataset of Rush et al. (2015). Because it is too costly to have humans write gold summaries at this scale, existing large-scale datasets are based on naturally occurring pairs of summary-like text paired with source text, for instance using news titles or highlights as summaries for news-text"
D19-1454,N19-1423,0,0.0690607,"T-large 33.3 63.3 63.3 66.0 w/o context 52.7 w/o question 52.1 w/o context, question 45.5 Human 86.9* 33.3 63.0 63.1 64.5 – – – 84.4* Table 2: Experimental results. We additionally perform an ablation by removing contexts and questions, verifying that both are necessary for BERT-large’s performance. Human evaluation results are obtained using 900 randomly sampled examples. Methods We establish baseline performance on S OCIAL IQ A, using large pretrained language models based on the Transformer architecture (Vaswani et al., 2017). Namely, we finetune OpenAI-GPT (Radford et al., 2018) and BERT (Devlin et al., 2019), which have both shown remarkable improvements on a variety of tasks. OpenAI-GPT is a uni-directional language model trained on the BookCorpus (Zhu et al., 2015), whereas BERT is a bidirectional language model trained on both the BookCorpus and English Wikipedia. As per previous work, we finetune the language model representations but fully learn the classifier specific parameters described below. Multiple choice classification To classify sequences using these language models, we follow the multiple-choice setup implementation by the respective authors, as described below. First, we concaten"
D19-1454,S12-1063,0,0.0764807,"sible Alternatives task (COPA; Roemmele et al., 2011) is a twoway multiple choice task which aims to measure commonsense reasoning abilities of models. The dataset contains 1,000 questions (500 dev, 500 test) that ask about the causes and effects of a premise. This has been a challenging task for Figure 6: Average dev accuracy of BERT-large on different question types. While questions about effects and motivations are easier, the model still finds wants and descriptions more challenging. computational systems, partially due to the limited amount of training data available. As done previously (Goodwin et al., 2012; Luo et al., 2016), we finetune our models on the dev set, and report performance only on the test set. Winograd Schema The Winograd Schema Challenge (WSC; Levesque, 2011) is a wellknown commonsense knowledge challenge framed as a coreference resolution task. It contains a collection of 273 short sentences in which a pronoun must be resolved to one of two antecedents (e.g., in “The city councilmen refused the demonstrators a permit because they feared violence”, they refers to the councilmen). Because of data scarcity in WSC, Rahman and Ng (2012) created 943 Winograd-style sentence pairs (188"
D19-1454,N18-2017,0,0.054342,"an the event sentence. 4465 ZDQWV (e.g., What will Kai want to do next?)  UHDFWLRQV (e.g., How would Robin feel afterwards?)  HIIHFWV QHHGV PRWLYDWLRQV GHVFULSWLRQV (e.g., What does (e.g., What will (e.g., Why did (e.g., How would Remy need to happen to you describe Alex?) Sydney do this?) do before this?) Sasha?)     candidates by switching the questions asked about the context, as shown in Figure 2. We do this to avoid cognitive biases and annotation artifacts in the answer candidates, such as those caused by writing incorrect answers or negations (Schwartz et al., 2017; Gururangan et al., 2018). In this crowdsourcing task, we provide the same context as the original question, as well as a question automatically generated from a different but similar ATOMIC dimension,6 and ask workers to write two correct answers. We refer to these negative responses as question-switching answers (QSA). By including answers to a different question about the same context, we ensure that these adversarial responses have the stylistic qualities of correct answers and strongly relate to the context topic, while still being incorrect, making it difficult for models to simply perform patternmatching. To ve"
D19-1454,D12-1071,0,0.128105,"Missing"
D19-1454,P18-2124,0,0.108046,"Missing"
D19-1454,W17-2810,0,0.034015,"social situations, as shown in this and previous work (Davis and Marcus, 4463 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4463–4473, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2015; Nematzadeh et al., 2018; Talmor et al., 2019). This is partly due to language models being trained on written text corpora, where reporting bias of knowledge limits the scope of commonsense knowledge that can be learned (Gordon and Van Durme, 2013; Lucy and Gauthier, 2017). In this work, we introduce Social Intelligence QA (S OCIAL IQ A), the first large-scale resource to learn and measure social and emotional intelligence in computational models.1 S OCIAL IQ A contains 38k multiple choice questions regarding the pragmatic implications of everyday, social events (see Figure 1). To collect this data, we design a crowdsourcing framework to gather contexts and questions that explicitly address social commonsense reasoning. Additionally, by combining handwritten negative answers with adversarial question-switched answers (Section 3.3), we minimize annotation artifa"
D19-1454,P18-1017,0,0.0215706,"nerated from a different but similar ATOMIC dimension,6 and ask workers to write two correct answers. We refer to these negative responses as question-switching answers (QSA). By including answers to a different question about the same context, we ensure that these adversarial responses have the stylistic qualities of correct answers and strongly relate to the context topic, while still being incorrect, making it difficult for models to simply perform patternmatching. To verify this, we compare valence, arousal, and dominance (VAD) levels across answer types, computed using the VAD lexicon by Mohammad (2018). Figure 4 shows effect sizes (Cohen’s d) of the differences in VAD means, where the magnitude of effect size indicates how different the answer types are stylistically. Indeed, QSA and correct answers differ substantially less than HIA answers (|d|≤.1).7 3.4 QA Tuple Creation As the final step of the pipeline, we aggregate the data into three-way multiple choice questions. For each created context-question pair contributed by crowdsourced workers, we select a random correct answer and the incorrect answers that are least entailed by the correct one, following inspiration from Zellers et al. ("
D19-1454,D18-1261,0,0.0245491,"and evaluate modern AI systems’ social and emotional intelligence. Although recent advances in pretraining large language models have yielded promising improvements on several commonsense inference tasks, these models still struggle to reason about social situations, as shown in this and previous work (Davis and Marcus, 4463 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4463–4473, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2015; Nematzadeh et al., 2018; Talmor et al., 2019). This is partly due to language models being trained on written text corpora, where reporting bias of knowledge limits the scope of commonsense knowledge that can be learned (Gordon and Van Durme, 2013; Lucy and Gauthier, 2017). In this work, we introduce Social Intelligence QA (S OCIAL IQ A), the first large-scale resource to learn and measure social and emotional intelligence in computational models.1 S OCIAL IQ A contains 38k multiple choice questions regarding the pragmatic implications of everyday, social events (see Figure 1). To collect this data, we design a crow"
D19-1454,N15-1082,0,0.0815589,"Missing"
D19-1454,K17-1004,1,0.841013,"xt 7-25 words longer than the event sentence. 4465 ZDQWV (e.g., What will Kai want to do next?)  UHDFWLRQV (e.g., How would Robin feel afterwards?)  HIIHFWV QHHGV PRWLYDWLRQV GHVFULSWLRQV (e.g., What does (e.g., What will (e.g., Why did (e.g., How would Remy need to happen to you describe Alex?) Sydney do this?) do before this?) Sasha?)     candidates by switching the questions asked about the context, as shown in Figure 2. We do this to avoid cognitive biases and annotation artifacts in the answer candidates, such as those caused by writing incorrect answers or negations (Schwartz et al., 2017; Gururangan et al., 2018). In this crowdsourcing task, we provide the same context as the original question, as well as a question automatically generated from a different but similar ATOMIC dimension,6 and ask workers to write two correct answers. We refer to these negative responses as question-switching answers (QSA). By including answers to a different question about the same context, we ensure that these adversarial responses have the stylistic qualities of correct answers and strongly relate to the context topic, while still being incorrect, making it difficult for models to simply perf"
D19-1454,P18-2119,0,0.0224014,"of work aimed at creating commonsense knowledge repositories (Speer and Havasi, 2012; Sap et al., 2019; Zhang et al., 2017; Lenat, 1995; Espinosa and Lieberman, 2005; Gordon and Hobbs, 2017) that can be used as resources in downstream reasoning tasks. While S OCIAL IQ A is formatted as a natural language QA benchmark, rather than a taxonomic knowledge base, it also can be used as a resource for external tasks, as we have demonstrated experimentally. Constrained or Adversarial Data Collection: Various work has investigated ways to circumvent annotation artifacts that result from crowdsourcing. Sharma et al. (2018) extend the Story Cloze data by severely restricting the incorrect story ending generation task, reducing the sentiment and negation artifacts. Rajpurkar et al. (2018) create an adversarial version of the extractive questionanswering challenge, SQuAD (Rajpurkar et al., 2016), by creating 50k unanswerable questions. Instead of using human-generated incorrect answers, Zellers et al. (2018, 2019b) use adversarial filtering of machine generated incorrect answers to minimize surface patterns. Our dataset also aims to reduce annotation artifacts by using a multistage annotation pipeline in which we"
D19-1454,speer-havasi-2012-representing,0,0.0615357,", 2011) are expert-curated collections of commonsense QA pairs that are trivial for humans to solve. Whereas WSC requires physical and social commonsense knowledge to solve, COPA targets the knowledge of causes and effects surrounding social situations. While both benchmarks are of high-quality and created by experts, their small scale (150 and 1,000 examples, respectively) poses a challenge for modern modelling techniques, which require many training instances. More recently, Talmor et al. (2019) introduce CommonsenseQA, containing 12k multiplechoice questions. Crowdsourced using ConceptNet (Speer and Havasi, 2012), these questions mostly probe knowledge related to factual and physical commonsense (e.g., “Where would I not want a fox?”). In contrast, S OCIAL IQ A explicitly separates contexts from questions, and focuses on the types of commonsense inferences humans perform when navigating social situations. 4470 Commonsense Knowledge Bases: In addition to large-scale benchmarks, there is a wealth of work aimed at creating commonsense knowledge repositories (Speer and Havasi, 2012; Sap et al., 2019; Zhang et al., 2017; Lenat, 1995; Espinosa and Lieberman, 2005; Gordon and Hobbs, 2017) that can be used as"
D19-1454,N19-1421,0,0.27646,"ystems’ social and emotional intelligence. Although recent advances in pretraining large language models have yielded promising improvements on several commonsense inference tasks, these models still struggle to reason about social situations, as shown in this and previous work (Davis and Marcus, 4463 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4463–4473, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2015; Nematzadeh et al., 2018; Talmor et al., 2019). This is partly due to language models being trained on written text corpora, where reporting bias of knowledge limits the scope of commonsense knowledge that can be learned (Gordon and Van Durme, 2013; Lucy and Gauthier, 2017). In this work, we introduce Social Intelligence QA (S OCIAL IQ A), the first large-scale resource to learn and measure social and emotional intelligence in computational models.1 S OCIAL IQ A contains 38k multiple choice questions regarding the pragmatic implications of everyday, social events (see Figure 1). To collect this data, we design a crowdsourcing framework to"
D19-1454,D18-1009,1,0.791673,"urce for external tasks, as we have demonstrated experimentally. Constrained or Adversarial Data Collection: Various work has investigated ways to circumvent annotation artifacts that result from crowdsourcing. Sharma et al. (2018) extend the Story Cloze data by severely restricting the incorrect story ending generation task, reducing the sentiment and negation artifacts. Rajpurkar et al. (2018) create an adversarial version of the extractive questionanswering challenge, SQuAD (Rajpurkar et al., 2016), by creating 50k unanswerable questions. Instead of using human-generated incorrect answers, Zellers et al. (2018, 2019b) use adversarial filtering of machine generated incorrect answers to minimize surface patterns. Our dataset also aims to reduce annotation artifacts by using a multistage annotation pipeline in which we collect negative responses from multiple methods including a unique adversarial question-switching technique. 8 Conclusion We present S OCIAL IQ A, the first large-scale benchmark for social commonsense. Consisting of 38k multiple-choice questions, S OCIAL IQ A covers various types of inference about people’s actions being described in situational contexts. We design a crowdsourcing fra"
D19-1454,P19-1472,1,0.79686,"Mohammad (2018). Figure 4 shows effect sizes (Cohen’s d) of the differences in VAD means, where the magnitude of effect size indicates how different the answer types are stylistically. Indeed, QSA and correct answers differ substantially less than HIA answers (|d|≤.1).7 3.4 QA Tuple Creation As the final step of the pipeline, we aggregate the data into three-way multiple choice questions. For each created context-question pair contributed by crowdsourced workers, we select a random correct answer and the incorrect answers that are least entailed by the correct one, following inspiration from Zellers et al. (2019a). For the training data, we validate our QA tuples through a multiple-choice crowdsourcing task where three workers are asked to select the right 6 Using the following three groupings of ATOMIC dimensions: {xWant, oWant, xNeed, xIntent}, {xReact oReact, xAttr}, and {xEffect, oEffect}. 7 Cohen’s |d|<.20 is considered small (Sawilowsky, 2009). We find similarly small effect sizes using other sentiment/emotion lexicons. Magnitude of effect (Cohen's d) Figure 3: S OCIAL IQ A contains several question types which cover different types of inferential reasoning. Question types are derived from ATOM"
D19-1454,Q17-1027,0,0.0689204,"Missing"
D19-1509,D13-1185,0,0.0250608,"discrimination of reasonable alternatives often results in models learning to exploit latent artifacts of the dataset (Niven and Kao, 2019; Zellers et al., 2018), rather than learning to robustly reason about counterfactuals. In response to this, we hypothesize that learning to generate the result of counterfactual prompts will encourage models to learn to understand the underlying dynamics of a given situation, whereas discrimination between two alternatives is more likely to take advantage of dataset biases. This goal shares many similarities with script learning (Pichotta and Mooney, 2014; Chambers, 2013), which attempts to canonicalize stereotypical event sequences for learning causal structure of narratives. However, because it is often difficult to capture the richness of causal dependencies with templatized structures (Sap et al., 2019), we instead study counterfactual reasoning in unstructured text directly and also require the model to generate the consequences of the counterfactual reasoning. The “counterfactual event” in our task can be viewed as a causal intervention (Pearl, 2000) in the latent chain of events of the story. Such interventions demand changes to the written narrative in"
D19-1509,P19-1264,1,0.828446,"ere has been a growing body of work in producing model-based metrics (Lowe et al., 2017) that use trained models and embeddings to score a sequence. Kusner et al. (2015) proposed Word Mover’s Distance, which defines the distance between two texts as the minimal cost of transforming one sequence’s word embeddings to the other’s. The measure finds a matching between the two texts that minimizes the total Euclidean distance between the matched word embeddings. Following Kilickaya et al. (2017), we take the negative exponential of this distance to get Word Mover’s Similarity (WMS). More recently, Clark et al. (2019) proposed Sentence + Word Mover’s Similarity (S+WMS) to extend WMS for longer multi-sentence texts by using sentence representations in the minimum distance calculation in addition to word embeddings.1 Other recent methods use contextualized embeddings (Devlin et al., 2018) to compute similarity between sequences. We use BERTScore (Zhang et al., 2019), which computes cosine similarity between two sentences using BERT encodings. Zhang et al. show that BERTScore correlates better with human judgments than existing metrics such as BLEU, ROUGE, and other learning-based metrics. To adapt BERTScore"
D19-1509,P18-1169,0,0.0369264,"frequent patterns in language without true understanding of the causal chains in narratives, thus requiring more focused future research to integrate reasoning capabilities to neural language models. 2 Background Counterfactual reasoning is the ability to consider alternative possibilities that diverge from current observed narratives. Due to their prevalence in common reasoning situations, counterfactuals have been studied in a wide range of disciplines, including psychology (Epstude and Roese, 2008), 5044 cognitive science (Byrne, 2002), as well as natural language processing (Hobbs, 2005; Lawrence and Riezler, 2018; Son et al., 2017). Meanwhile, despite the progress made in NLU tasks by adapting pretrained language representations such as BERT (Devlin et al., 2018) or GPT (Radford et al., 2018), models still have trouble discriminating between reasonable and unreasonable counterfactuals, as shown in (Zellers et al., 2019). Moreover, success in tasks linked to discrimination of reasonable alternatives often results in models learning to exploit latent artifacts of the dataset (Niven and Kao, 2019; Zellers et al., 2018), rather than learning to robustly reason about counterfactuals. In response to this, w"
D19-1509,D16-1230,0,0.0374475,"s (Devlin et al., 2018) to compute similarity between sequences. We use BERTScore (Zhang et al., 2019), which computes cosine similarity between two sentences using BERT encodings. Zhang et al. show that BERTScore correlates better with human judgments than existing metrics such as BLEU, ROUGE, and other learning-based metrics. To adapt BERTScore to our task, we finetune BERT on ROCStories using the same training framework from Devlin et al. (2018) and compute BERT-FT the same way as before. 6.2 Human Correlation with Metrics Recent work in text generation (Wiseman et al., 2017) and dialogue (Liu et al., 2016) have explored the limitations of automatic metrics for text production tasks. Due to the highly semantic nature of the counterfactual rewriting task and the need to recognize subtle changes in 1 We follow previous work and use GloVe embeddings (Pennington et al., 2014) to represent words and the averaged word embeddings to represent sentences. 5050 BLEU-4 ROUGE-L BERT BERT-FT WMS W+SMS s1 s02 Training: Pretrained Only GPT + zero-shot 1.25 18.26 GPT2-S + zero-shot 1.28 20.27 GPT2-M + zero-shot 1.51 19.41 Training: Unsupervised + Generative GPT + FT 4.20 24.55 GPT2-S + FT 3.78 24.18 GPT2-M + FT"
D19-1509,P17-1103,0,0.0157886,"2002) is perhaps the most widely used metric in text generation, which computes the number of overlapping n-grams between the generated and reference sequences. Another commonly used metric in text generation (though originally designed for extractive summarization) is ROUGE-L (Lin, 2004), which measures the Model-based Metrics Although BLEU and ROUGE are widely used in text generation, they use exact string matching, and thus fail to robustly match paraphrases and capture semanticallycritical ordering changes. Recently, there has been a growing body of work in producing model-based metrics (Lowe et al., 2017) that use trained models and embeddings to score a sequence. Kusner et al. (2015) proposed Word Mover’s Distance, which defines the distance between two texts as the minimal cost of transforming one sequence’s word embeddings to the other’s. The measure finds a matching between the two texts that minimizes the total Euclidean distance between the matched word embeddings. Following Kilickaya et al. (2017), we take the negative exponential of this distance to get Word Mover’s Similarity (WMS). More recently, Clark et al. (2019) proposed Sentence + Word Mover’s Similarity (S+WMS) to extend WMS fo"
D19-1509,N19-1423,0,0.0476864,"Missing"
D19-1509,N16-1098,0,0.28208,"5’) He carried them home and planted them in his vase. Figure 2: Data annotation process for the T IME T RAVEL dataset. Given a story from the ROCStories corpus, crowdworkers write a counterfactual sentence w.r.t the second sentence of the story. The counterfactual sentence and the original story are then presented to other workers to rewrite the story ending. Models for the task are expected to generate a rewritten ending given the original story and counterfactual sentence. This notion of counterfactuals has become increasingly relevant in several recent benchmarks such as ROC story cloze (Mostafazadeh et al., 2016), COPA (Roemmele et al., 2011), and HellaSwag (Zellers et al., 2019), where the negative responses in multiple-choice problems implicitly construct counterfactual narratives. However, no existing benchmark to date has been designed to explicitly evaluate counterfactual narrative reasoning and revision as its principal focus, where a system is evaluated on its ability to make modifications to future events based on a counterfactual condition, as illustrated in Figure 1. In this paper, we introduce Counterfactual Story Rewriting as a new challenge to story understanding and generation. Given an"
D19-1509,P19-1459,0,0.0219372,"ese, 2008), 5044 cognitive science (Byrne, 2002), as well as natural language processing (Hobbs, 2005; Lawrence and Riezler, 2018; Son et al., 2017). Meanwhile, despite the progress made in NLU tasks by adapting pretrained language representations such as BERT (Devlin et al., 2018) or GPT (Radford et al., 2018), models still have trouble discriminating between reasonable and unreasonable counterfactuals, as shown in (Zellers et al., 2019). Moreover, success in tasks linked to discrimination of reasonable alternatives often results in models learning to exploit latent artifacts of the dataset (Niven and Kao, 2019; Zellers et al., 2018), rather than learning to robustly reason about counterfactuals. In response to this, we hypothesize that learning to generate the result of counterfactual prompts will encourage models to learn to understand the underlying dynamics of a given situation, whereas discrimination between two alternatives is more likely to take advantage of dataset biases. This goal shares many similarities with script learning (Pichotta and Mooney, 2014; Chambers, 2013), which attempts to canonicalize stereotypical event sequences for learning causal structure of narratives. However, becaus"
D19-1509,P02-1040,0,0.105541,". 6 Challenges for Automatic Metrics While human scores provide the clearest insight into how models are able to complete the counterfactual rewriting task, their associated cost makes them difficult to scale to larger evaluation sets. To provide further insight into the performance of candidate models, we explore how different automatic metrics evaluate the produced generations. 6.1 Metrics Overlap Metrics The most common metrics used in evaluating text generation are based on textual overlap between a candidate generated sequence and set of reference sequences provided by the dataset. BLEU (Papineni et al., 2002) is perhaps the most widely used metric in text generation, which computes the number of overlapping n-grams between the generated and reference sequences. Another commonly used metric in text generation (though originally designed for extractive summarization) is ROUGE-L (Lin, 2004), which measures the Model-based Metrics Although BLEU and ROUGE are widely used in text generation, they use exact string matching, and thus fail to robustly match paraphrases and capture semanticallycritical ordering changes. Recently, there has been a growing body of work in producing model-based metrics (Lowe e"
D19-1509,P19-3027,1,0.88242,"Missing"
D19-1509,D16-1168,0,0.115898,"Missing"
D19-1509,D14-1162,0,0.0810422,"Missing"
D19-1509,E14-1024,0,0.0418564,"success in tasks linked to discrimination of reasonable alternatives often results in models learning to exploit latent artifacts of the dataset (Niven and Kao, 2019; Zellers et al., 2018), rather than learning to robustly reason about counterfactuals. In response to this, we hypothesize that learning to generate the result of counterfactual prompts will encourage models to learn to understand the underlying dynamics of a given situation, whereas discrimination between two alternatives is more likely to take advantage of dataset biases. This goal shares many similarities with script learning (Pichotta and Mooney, 2014; Chambers, 2013), which attempts to canonicalize stereotypical event sequences for learning causal structure of narratives. However, because it is often difficult to capture the richness of causal dependencies with templatized structures (Sap et al., 2019), we instead study counterfactual reasoning in unstructured text directly and also require the model to generate the consequences of the counterfactual reasoning. The “counterfactual event” in our task can be viewed as a causal intervention (Pearl, 2000) in the latent chain of events of the story. Such interventions demand changes to the wri"
D19-1509,S12-1052,0,0.035724,"Missing"
D19-1509,P17-2103,0,0.0637979,"ge without true understanding of the causal chains in narratives, thus requiring more focused future research to integrate reasoning capabilities to neural language models. 2 Background Counterfactual reasoning is the ability to consider alternative possibilities that diverge from current observed narratives. Due to their prevalence in common reasoning situations, counterfactuals have been studied in a wide range of disciplines, including psychology (Epstude and Roese, 2008), 5044 cognitive science (Byrne, 2002), as well as natural language processing (Hobbs, 2005; Lawrence and Riezler, 2018; Son et al., 2017). Meanwhile, despite the progress made in NLU tasks by adapting pretrained language representations such as BERT (Devlin et al., 2018) or GPT (Radford et al., 2018), models still have trouble discriminating between reasonable and unreasonable counterfactuals, as shown in (Zellers et al., 2019). Moreover, success in tasks linked to discrimination of reasonable alternatives often results in models learning to exploit latent artifacts of the dataset (Niven and Kao, 2019; Zellers et al., 2018), rather than learning to robustly reason about counterfactuals. In response to this, we hypothesize that"
D19-1509,D17-1239,0,0.0484506,"Missing"
D19-1509,D18-1009,1,0.897848,"Missing"
D19-1509,P19-1472,1,0.925536,"notation process for the T IME T RAVEL dataset. Given a story from the ROCStories corpus, crowdworkers write a counterfactual sentence w.r.t the second sentence of the story. The counterfactual sentence and the original story are then presented to other workers to rewrite the story ending. Models for the task are expected to generate a rewritten ending given the original story and counterfactual sentence. This notion of counterfactuals has become increasingly relevant in several recent benchmarks such as ROC story cloze (Mostafazadeh et al., 2016), COPA (Roemmele et al., 2011), and HellaSwag (Zellers et al., 2019), where the negative responses in multiple-choice problems implicitly construct counterfactual narratives. However, no existing benchmark to date has been designed to explicitly evaluate counterfactual narrative reasoning and revision as its principal focus, where a system is evaluated on its ability to make modifications to future events based on a counterfactual condition, as illustrated in Figure 1. In this paper, we introduce Counterfactual Story Rewriting as a new challenge to story understanding and generation. Given an original story and a counterfactual condition, the task is to re-wri"
H05-1045,P98-1013,0,0.00645926,"Missing"
H05-1045,A97-1029,0,0.0683494,"Missing"
H05-1045,C04-1018,1,0.565484,"t in methods for automatically identifying opinions, emotions, and sentiments in text. Much of this research explores sentiment classification, a text categorization task in which the goal is to classify a document as having positive or negative polarity (e.g., Das and Chen (2001), Pang et al. (2002), Turney (2002), Dave et al. (2003), Pang and Lee S1: Taiwan-born voters favoring independence... 1 In related work, we investigate methods to identify the opinion expressions (e.g., Riloff and Wiebe (2003), Wiebe and Riloff (2005), Wilson et al. (2005)) and the nesting structure of sources (e.g., Breck and Cardie (2004)). The target of each opinion, i.e., what the opinion is directed towards, is currently being annotated manually for our corpus. 355 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 355–362, Vancouver, October 2005. 2005 Association for Computational Linguistics S2: According to the report, the human rights record in China is horrendous. S3: International officers believe that the EU will prevail. S4: International officers said US officials want the EU to prevail. In S1, the phrase “Taiwan-born voters”"
H05-1045,W03-0430,0,0.0131654,"rpetrator and the person who is the victim. We hypothesized that IE techniques would be wellsuited for source identification because an opinion statement can be viewed as a kind of speech event with the source as the agent. We investigate two very different learning-based methods from information extraction for the problem of opinion source identification: graphical models and extraction pattern learning. In particular, we consider Conditional Random Fields (Lafferty et al., 2001) and a variation of AutoSlog (Riloff, 1996a). CRFs have been used successfully for Named Entity recognition (e.g., McCallum and Li (2003), Sarawagi and Cohen (2004)), and AutoSlog has performed well on information extraction tasks in several domains (Riloff, 1996a). While CRFs treat source identification as a sequence tagging task, AutoSlog views the problem as a pattern-matching task, acquiring symbolic patterns that rely on both the syntax and lexical semantics of a sentence. We hypothesized that a combination of the two techniques would perform better than either one alone. Section 3 describes the CRF approach to identifying opinion sources and the features that the system uses. Section 4 then presents a new variation of Aut"
H05-1045,J05-1004,0,0.0139477,"Missing"
H05-1045,W02-1011,0,0.0421764,"Missing"
H05-1045,P04-1035,0,0.246996,"Missing"
H05-1045,W03-1014,1,0.638262,"Missing"
H05-1045,P02-1053,0,0.0160953,"Missing"
H05-1045,H05-2018,1,0.453659,"troduction In recent years, there has been a great deal of interest in methods for automatically identifying opinions, emotions, and sentiments in text. Much of this research explores sentiment classification, a text categorization task in which the goal is to classify a document as having positive or negative polarity (e.g., Das and Chen (2001), Pang et al. (2002), Turney (2002), Dave et al. (2003), Pang and Lee S1: Taiwan-born voters favoring independence... 1 In related work, we investigate methods to identify the opinion expressions (e.g., Riloff and Wiebe (2003), Wiebe and Riloff (2005), Wilson et al. (2005)) and the nesting structure of sources (e.g., Breck and Cardie (2004)). The target of each opinion, i.e., what the opinion is directed towards, is currently being annotated manually for our corpus. 355 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 355–362, Vancouver, October 2005. 2005 Association for Computational Linguistics S2: According to the report, the human rights record in China is horrendous. S3: International officers believe that the EU will prevail. S4: International officers said US offi"
H05-1045,W03-1017,0,0.237929,"Missing"
H05-1045,J03-4003,0,\N,Missing
H05-1045,C98-1013,0,\N,Missing
H05-2018,H05-1045,1,0.340187,"generated from a large corpus of unannotated data by two high-precision, rule-based classifiers. Speech Events and Direct Subjective Expression Classification The second component identifies speech events (e.g., “said,” “according to”) and direct subjective expressions (e.g., “fears,” “is happy”). Speech events include both speaking and writing events. Direct subjective expressions are words or phrases where an opinion, emotion, sentiment, etc. is directly described. A high-precision, rule-based classifier is used to identify these expressions. Related Work Please see (Wiebe and Riloff, 2005; Choi et al., 2005; Wilson et al., 2005) for discussions of related work in automatic opinion and sentiment analysis. 4 Acknowledgments This work was supported by the Advanced Research and Development Activity (ARDA), by the NSF under grants IIS-0208028, IIS-0208798 and IIS0208985, and by the Xerox Foundation. 2.3.2 2.3.3 Opinion Source Identification The third component is a source identifier that combines a Conditional Random Field sequence tagging model (Lafferty et al., 2001) and extraction pattern learning (Riloff, 1996) to identify the sources of speech events and subjective expressions (Choi et al., 2005"
H05-2018,P97-1003,0,0.0192817,"tering out opinionated sentences (Riloff et al., 2005). System Architecture Overview Document Processing For general document processing, OpinionFinder first runs the Sundance partial parser (Riloff and Phillips, 2004) to provide semantic class tags, identify Named Entities, and match extraction patterns that correspond to subjective language (Riloff and Wiebe, 2003). Next, OpenNLP1 1.1.0 is used to tokenize, sentence split, and part-of-speech tag the data, and the Abney stemmer2 is used to stem. In batch mode, OpinionFinder parses the data again, this time to obtain constituency parse trees (Collins, 1997), which are then converted to dependency parse trees (Xia and Palmer, 2001). Currently, this stage is only 1 2 http://opennlp.sourceforge.net/ SCOL version 1g available at http://www.vinartus.net/spa/ 34 Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 34–35, Vancouver, October 2005. available for batch mode processing due to the time required for parsing. Finally, a clue-finder is run to identify words and phrases from a large subjective language lexicon. 2.3 Subjectivity Analysis The subjectivity analysis has four components. The first classifier focuses on identifying sentiment"
H05-2018,W03-1014,1,0.673444,"from knowledge of subjective language include systems that summarize the various viewpoints in a document or that mine product reviews. Even typical fact-oriented applications, such as information extraction, can benefit from subjectivity analysis by filtering out opinionated sentences (Riloff et al., 2005). System Architecture Overview Document Processing For general document processing, OpinionFinder first runs the Sundance partial parser (Riloff and Phillips, 2004) to provide semantic class tags, identify Named Entities, and match extraction patterns that correspond to subjective language (Riloff and Wiebe, 2003). Next, OpenNLP1 1.1.0 is used to tokenize, sentence split, and part-of-speech tag the data, and the Abney stemmer2 is used to stem. In batch mode, OpinionFinder parses the data again, this time to obtain constituency parse trees (Collins, 1997), which are then converted to dependency parse trees (Xia and Palmer, 2001). Currently, this stage is only 1 2 http://opennlp.sourceforge.net/ SCOL version 1g available at http://www.vinartus.net/spa/ 34 Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 34–35, Vancouver, October 2005. available for batch mode processing due to the time requir"
H05-2018,H05-1044,1,0.169551,"rge corpus of unannotated data by two high-precision, rule-based classifiers. Speech Events and Direct Subjective Expression Classification The second component identifies speech events (e.g., “said,” “according to”) and direct subjective expressions (e.g., “fears,” “is happy”). Speech events include both speaking and writing events. Direct subjective expressions are words or phrases where an opinion, emotion, sentiment, etc. is directly described. A high-precision, rule-based classifier is used to identify these expressions. Related Work Please see (Wiebe and Riloff, 2005; Choi et al., 2005; Wilson et al., 2005) for discussions of related work in automatic opinion and sentiment analysis. 4 Acknowledgments This work was supported by the Advanced Research and Development Activity (ARDA), by the NSF under grants IIS-0208028, IIS-0208798 and IIS0208985, and by the Xerox Foundation. 2.3.2 2.3.3 Opinion Source Identification The third component is a source identifier that combines a Conditional Random Field sequence tagging model (Lafferty et al., 2001) and extraction pattern learning (Riloff, 1996) to identify the sources of speech events and subjective expressions (Choi et al., 2005). The source of a spe"
H05-2018,H01-1014,0,0.0113786,"tecture Overview Document Processing For general document processing, OpinionFinder first runs the Sundance partial parser (Riloff and Phillips, 2004) to provide semantic class tags, identify Named Entities, and match extraction patterns that correspond to subjective language (Riloff and Wiebe, 2003). Next, OpenNLP1 1.1.0 is used to tokenize, sentence split, and part-of-speech tag the data, and the Abney stemmer2 is used to stem. In batch mode, OpinionFinder parses the data again, this time to obtain constituency parse trees (Collins, 1997), which are then converted to dependency parse trees (Xia and Palmer, 2001). Currently, this stage is only 1 2 http://opennlp.sourceforge.net/ SCOL version 1g available at http://www.vinartus.net/spa/ 34 Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 34–35, Vancouver, October 2005. available for batch mode processing due to the time required for parsing. Finally, a clue-finder is run to identify words and phrases from a large subjective language lexicon. 2.3 Subjectivity Analysis The subjectivity analysis has four components. The first classifier focuses on identifying sentiment expressions. The second classifier takes the sentiment expressions and iden"
K17-1004,D14-1155,1,0.38453,"ons reaches state of the art performance on the story cloze challenge. Our results demonstrate that different task framings can dramatically affect the way people write.1 1 Ending She feels flattered and asks John on a date. The girl found this charming, and gave him a second chance. John was happy about being rejected. Table 1: Examples of stories from the story cloze task. The table shows a story prefix with three contrastive endings: The original ending, a coherent ending and a incoherent one. since different tasks likely engage different cognitive processes (Campbell and Pennebaker, 2003; Banerjee et al., 2014).2 We show that similar writing tasks with different constraints on the author can lead to measurable differences in her writing style. As a case study, we present experiments based on the recently introduced ROC story cloze task (Mostafazadeh et al., 2016a). In this task, authors were asked to write five-sentence self-contained stories, henceforth original stories. Then, each original story was given to a different author, who was shown only the first four sentences as a story context, and asked to write two contrasting story endings: a right (coherent) ending, and a wrong (incoherent) ending"
K17-1004,P82-1020,0,0.851485,"Missing"
K17-1004,N12-1033,0,0.00560131,"yle has been an active topic of research for decades. The models used to characterize style are often linear classifiers with style features such as character and word n-grams (Stamatatos, 2009; Koppel et al., 2009). Previous work has shown that different authors can be grouped by their writing style, according to factors such as age (Pennebaker and Stone, 2003; Argamon et al., 2003; Schler et al., 2006; Rosenthal and McKeown, 2011; Nguyen et al., 2011), gender (Argamon et al., 2003; Schler et al., 2006; Bamman et al., 2014), and native language (Koppel et al., 2005; Tsur and Rappoport, 2007; Bergsma et al., 2012). At the extreme case, each individual author adopts a unique writing style (Mosteller 11 Similar problems have been shown in visual question answering datasets, where simple models that rely mostly on the question text perform competitively with state of the art models by exploiting language biases (Zhou et al., 2015; Jabri et al., 2016). 22 on the effects that a writing prompt has on an author’s mental state, and also her concrete response. They also provide valuable lessons for designing new NLP datasets. 10 style and physical health. Psychological Science 14(1):60–65. Danqi Chen, Jason Bol"
K17-1004,D15-1075,0,0.02051,"ples compared to 3,742 in the story cloze task), this indicates that simple instructions may help alleviate the effects of writing style found in this paper. Another way to avoid such effects is to have people rate naturally occurring sentences by parameters such as coherence (or, conversely, the level of surprise), rather than asking them to generate new text. 8 Machine reading. The story cloze task, which is the focus of this paper, is part of a wide set of machine reading/comprehension challenges published in the last few years. These include datasets like bAbI (Weston et al., 2016), SNLI (Bowman et al., 2015), CNN/DailyMail (Hermann et al., 2015), LAMBADA (Paperno et al., 2016) and SQuAD (Rajpurkar et al., 2016). While these works have presented resources for researchers, it is often the case that these datasets suffer from methodological problems caused by applying noisy automatic tools to generate them (Chen et al., 2016).11 In this paper, we have pointed to another methodological challenge in designing machine reading tasks: different writing tasks used to generated the data affect writing style, confounding classification problems. 9 Conclusion Different writing tasks assigned to an author res"
K17-1004,P17-2097,0,0.437985,"Missing"
K17-1004,P14-2072,0,0.0173871,"Missing"
K17-1004,N16-1098,0,0.194848,"Washington, Seattle, WA, USA 2 Allen Institute for Artificial Intelligence, Seattle, WA, USA {roysch,msap,ikonstas,lzilles,yejin,nasmith}@cs.washington.edu Story Prefix John liked a girl at his work. He tried to get her attention by acting silly. She told him to grow up. John confesses he was trying to make her like him more. Abstract A writer’s style depends not just on personal traits but also on her intent and mental state. In this paper, we show how variants of the same writing task can lead to measurable differences in writing style. We present a case study based on the story cloze task (Mostafazadeh et al., 2016a), where annotators were assigned similar writing tasks with different constraints: (1) writing an entire story, (2) adding a story ending for a given story context, and (3) adding an incoherent ending to a story. We show that a simple linear classifier informed by stylistic features is able to successfully distinguish among the three cases, without even looking at the story context. In addition, combining our stylistic features with language model predictions reaches state of the art performance on the story cloze challenge. Our results demonstrate that different task framings can dramatical"
K17-1004,W17-0906,0,0.0269252,"Missing"
K17-1004,W16-2505,0,0.160323,"Washington, Seattle, WA, USA 2 Allen Institute for Artificial Intelligence, Seattle, WA, USA {roysch,msap,ikonstas,lzilles,yejin,nasmith}@cs.washington.edu Story Prefix John liked a girl at his work. He tried to get her attention by acting silly. She told him to grow up. John confesses he was trying to make her like him more. Abstract A writer’s style depends not just on personal traits but also on her intent and mental state. In this paper, we show how variants of the same writing task can lead to measurable differences in writing style. We present a case study based on the story cloze task (Mostafazadeh et al., 2016a), where annotators were assigned similar writing tasks with different constraints: (1) writing an entire story, (2) adding a story ending for a given story context, and (3) adding an incoherent ending to a story. We show that a simple linear classifier informed by stylistic features is able to successfully distinguish among the three cases, without even looking at the story context. In addition, combining our stylistic features with language model predictions reaches state of the art performance on the story cloze challenge. Our results demonstrate that different task framings can dramatical"
K17-1004,D16-1264,0,0.0242504,"15), LAMBADA Background: The Story Cloze Task To understand how different writing tasks affect writing style, we focus on the story cloze task (Mostafazadeh et al., 2016a). While this task was developed to facilitate representation and learning of commonsense story understanding, its design included a few key choices which make it ideal for our study. We describe the task below. ROC stories. The ROC story corpus consists of 49,255 five-sentence stories, collected on Ama3 Recently, additional 53K stories were released, which results in roughly 100K stories. 16 (Paperno et al., 2016) and SQuAD (Rajpurkar et al., 2016), for which results improved dramatically over similar or much shorter periods of time. This suggests that this task is challenging and that high performance is hard to achieve. In addition, Mostafazadeh et al. (2016a) made substantial efforts to ensure the quality of this dataset. First, each pair of endings was written by the same author, which ensured that style differences between authors could not be used to solve the task. Furthermore, Mostafazadeh et al. implemented nine baselines for the task, using surface level features as well as narrative-informed ones, and showed that each of them"
K17-1004,P11-1077,0,0.0110115,"eve state of the art results on the story cloze task. The findings presented in this paper have cognitive implications, as they motivate further research Related Work Writing style. Writing style has been an active topic of research for decades. The models used to characterize style are often linear classifiers with style features such as character and word n-grams (Stamatatos, 2009; Koppel et al., 2009). Previous work has shown that different authors can be grouped by their writing style, according to factors such as age (Pennebaker and Stone, 2003; Argamon et al., 2003; Schler et al., 2006; Rosenthal and McKeown, 2011; Nguyen et al., 2011), gender (Argamon et al., 2003; Schler et al., 2006; Bamman et al., 2014), and native language (Koppel et al., 2005; Tsur and Rappoport, 2007; Bergsma et al., 2012). At the extreme case, each individual author adopts a unique writing style (Mosteller 11 Similar problems have been shown in visual question answering datasets, where simple models that rely mostly on the question text perform competitively with state of the art models by exploiting language biases (Zhou et al., 2015; Jabri et al., 2016). 22 on the effects that a writing prompt has on an author’s mental state,"
K17-1004,W11-1515,1,0.178023,"Missing"
K17-1004,P11-1032,1,0.0248668,"Writing tasks can even have a long-term effect, as writing emotional texts was observed to benefit both physical and mental health (LepDesign of NLP tasks. Our study also provides important insights for the future design of NLP tasks. The story cloze task was very carefully designed. Many factors, such as topic diversity and 21 and Wallace, 1963; Pennebaker and King, 1999; Schwartz et al., 2013b). The line of work that most resembles our work is the detection of deceptive text. Several researchers have used stylometric features to predict deception (Newman et al., 2003; Hancock et al., 2007; Ott et al., 2011; Feng et al., 2012). Some works even showed that gender affects a person’s writing style when lying (P´erez-Rosas and Mihalcea, 2014a,b). In this work, we have shown that an even more subtle writing task—writing coherent and incoherent story endings—imposes different styles on the author. temporal and causal relation diversity, were controlled for (Mostafazadeh et al., 2016a). The authors also made sure each pair of endings was written by the same author, partly in order to avoid author-specific style effects. Nonetheless, despite these efforts, several significant style differences can be fo"
K17-1004,P16-1144,0,0.0591898,"Missing"
K17-1004,W17-0907,1,0.671362,"Missing"
K17-1004,D13-1193,1,0.264354,"periment 2 Table 5: The top 5 most heavily weighted features for predicting right vs. wrong endings (5a) and original vs. new (right) endings (5b). length is the sentence length feature (see Section 4). ore and Smyth, 2002; Frattaroli, 2006). Campbell and Pennebaker (2003) also showed that the health benefits of writing emotional text are accompanied by changes in writing style, mostly in the use of pronouns. Another line of work has shown that writing style is affected by mental state. First, an author’s personality traits (e.g., depression, neuroticism, narcissism) affect her writing style (Schwartz et al., 2013a; Ireland and Mehl, 2014). Second, temporary changes, such as a romantic relationship (Ireland et al., 2011; Bowen et al., 2016), work collaboration (Tausczik, 2009; Gonzales et al., 2009), or negotiation (Ireland and Henderson, 2014) may also affect writing style. Finally, writing style can also change from one sentence to another, for instance between positive and negative text (Davidov et al., 2010) or when writing sarcastic text (Tsur et al., 2010). This large body of work indicates a tight connection between writing tasks, mental states, and variation in writing style. This connection hi"
K17-1004,W07-0602,0,0.0529888,"Writing style. Writing style has been an active topic of research for decades. The models used to characterize style are often linear classifiers with style features such as character and word n-grams (Stamatatos, 2009; Koppel et al., 2009). Previous work has shown that different authors can be grouped by their writing style, according to factors such as age (Pennebaker and Stone, 2003; Argamon et al., 2003; Schler et al., 2006; Rosenthal and McKeown, 2011; Nguyen et al., 2011), gender (Argamon et al., 2003; Schler et al., 2006; Bamman et al., 2014), and native language (Koppel et al., 2005; Tsur and Rappoport, 2007; Bergsma et al., 2012). At the extreme case, each individual author adopts a unique writing style (Mosteller 11 Similar problems have been shown in visual question answering datasets, where simple models that rely mostly on the question text perform competitively with state of the art models by exploiting language biases (Zhou et al., 2015; Jabri et al., 2016). 22 on the effects that a writing prompt has on an author’s mental state, and also her concrete response. They also provide valuable lessons for designing new NLP datasets. 10 style and physical health. Psychological Science 14(1):60–65"
K17-1004,P13-1093,0,0.0486373,"Missing"
K17-1004,N12-1097,1,0.0996929,"Missing"
K17-1004,P12-2034,1,\N,Missing
K17-1004,P16-1223,0,\N,Missing
N07-1009,J96-1002,0,0.0057634,"5), Wellner et al. (2004)). Solutions of the first type replace the computation  of the global normalization factor y p(y|x) with argmaxy p(y|x) during training, since finding an argmax of a probability distribution is often an easier problem than finding the entire probability distribution. Training via the voted perceptron algorithm (Collins, 2002) or using a max-margin criterion also correspond to the first option (e.g. McCallum and Wellner (2004), Finley and Joachims (2005)). But without the global normalization, the maximumlikelihood criterion motivated by the maximum entropy principle (Berger et al., 1996) is no longer a feasible option as an optimization criterion. The second solution simplifies the graph structure for training, and applies complex global inference only for testing. In spite of the discrepancy between the training model and the testing model, it has been empirically shown that (1) performing global inference only during testing can improve performance (e.g. Finkel et al. (2005), Roth and Yih (2005)), and (2) full-blown global training can often perform worse due to insufficient training data (e.g. Punyakanok et al. (2005)). Importantly, however, attempts to reduce the discrepa"
N07-1009,W02-1001,0,0.0180331,"great success for problems involving structured output variables (e.g. Wellner et al. (2004), Finkel et al. (2005)). For many real-world NLP applications, however, the required graph structure can be very complex, and computing the global normalization factor even approximately can be extremely hard. Previous approaches for training CRFs have either (1) opted for a training method that no longer maximizes the likelihood, (e.g. McCallum and Wellner (2004), Roth and Yih (2005)) 1 , or (2) opted for a 1 Both McCallum and Wellner (2004) and Roth and Yih (2005) used the voted perceptron algorithm (Collins, 2002) to train intractable CRFs. simplified graph structure to avoid intractable global normalization (e.g. Roth and Yih (2005), Wellner et al. (2004)). Solutions of the first type replace the computation  of the global normalization factor y p(y|x) with argmaxy p(y|x) during training, since finding an argmax of a probability distribution is often an easier problem than finding the entire probability distribution. Training via the voted perceptron algorithm (Collins, 2002) or using a max-margin criterion also correspond to the first option (e.g. McCallum and Wellner (2004), Finley and Joachims (20"
N07-1009,P05-1045,0,0.0259861,"idden variables are used to capture interactions between local inference and global inference. Furthermore, we introduce biased potential functions that empirically drive CRFs towards performance improvements w.r.t. the preferred evaluation measure for the learning task. We report promising experimental results on two coreference data sets using two task-specific evaluation measures. 1 Introduction Undirected graphical models such as Conditional Random Fields (CRFs) (Lafferty et al., 2001) have shown great success for problems involving structured output variables (e.g. Wellner et al. (2004), Finkel et al. (2005)). For many real-world NLP applications, however, the required graph structure can be very complex, and computing the global normalization factor even approximately can be extremely hard. Previous approaches for training CRFs have either (1) opted for a training method that no longer maximizes the likelihood, (e.g. McCallum and Wellner (2004), Roth and Yih (2005)) 1 , or (2) opted for a 1 Both McCallum and Wellner (2004) and Roth and Yih (2005) used the voted perceptron algorithm (Collins, 2002) to train intractable CRFs. simplified graph structure to avoid intractable global normalization (e."
N07-1009,P02-1014,1,0.743149,"M y ← 1 else i yi ← 0 end for return y h∗ ← argmaxh P (h|x) h ← single-link-clustering(h∗ ) for each hi ∈ h if hi = yi∗ y  ← h∗i else i yi ← yi∗ end for return y Figure 2: Algorithm to find a high confidence labeling y that is close to the true labeling y∗ Figure 1: Algorithm to find the highest confidence labeling y that can be clustered to the true labeling y∗ [Global Model P (y|h)] For the global model, we assume a deterministic clustering algorithm is given. In particular, we focus on single-link clustering, as it has been shown to be effective for coreference resolution (e.g. Ng and Cardie (2002)). With single-link clustering, P (y|h) = 1 if h can be clustered to y, and P (y|h) = 0 if h cannot be clustered to y.5 [Computation of the E-step] The E-step requires computation of the distribution of P (h|y, x, θ(t−1) ), which we will simply denote as P (h|y, x), since all our distributions are implicitly conditioned on the model parameters θ. P (h|y, x) = x, true labeling y∗ , current local model P (h|x) Find a high confidence labeling y that is close to the true labeling y∗ P (h, y|x) ∝ P (y|h) P (h|x) P (y|x) Notice that when computing P (h|y, x), the denominator P (y|x) stays as a cons"
N07-1009,W06-1640,1,0.896293,"Missing"
N12-1094,P89-1010,0,0.244848,"nouns and adjectives automatically based on bootstrapping techniques. First, we construct a graph between adjectives by computing distributional similarity (Turney and Pantel, 2010) between them. For computing distributional similarity between adjectives, each target adjective is defined as a vector of nouns which are modified by the target adjective. To be exact, we use only those adjectives as modifiers which appear adjacent to a noun (that is, in a JJ NN construction). For example, in “small red apple,” we consider only red as a modifier for noun. We use Pointwise Mutual Information (PMI) (Church and Hanks, 1989) to weight the contexts, and select the top 1000 PMI contexts for each adjective.3 Next, we apply cosine similarity to find the top 10 distributionally similar adjectives with respect to each target adjective based on our large generic corpus (Large-Data from Section 2.1). This creates a graph with adjectives as nodes and cosine similarity as weight on the edges. Analogously, we construct a graph with nouns as nodes (here, adjectives are used as contexts for nouns). We then apply bootstrapping (Kozareva et al., 2008) on the noun and adjective graphs by selecting 10 seeds for visual and non-vis"
N12-1094,P10-1126,0,0.071421,"ful. In general, features in the phrase were most useful (not surprisingly), and then features before the phrase (presumably to give context, for instance as in “out of the window”). Features from after the phrase were not useful. 4 Non-singleton features appear more than once in the data. + + + + + + + + + + C ATEGORY Words Image Bootstrap Spell Length Words Wordnet Spell Spell Wordnet Wordnet P OSITION Phrase Phrase Phrase Before Phrase After Before Before After AUC 74.7 74.4 74.3 75.3 74.7 76.2 76.1 76.0 76.8 77.0 75.6 Sadeghi, 2011), and automatic caption generation (Farhadi et al., 2010; Feng and Lapata, 2010; Ordonez et al., 2011; Kulkarni et al., 2011; Yang et al., 2011; Li et al., 2011; Mitchell et al., 2012), it becomes increasingly important to understand, and to be able to detect, text that actually refers to observed phenomena. Our results suggest that while this is a hard problem, it is possible to leverage large text resources and state-of-the-art computer vision algorithms to address it with high accuracy. Acknowledgments Table 6: Results of feature ablation on L ARGE data set. Corresponding results on the L ARGE data set are shown in Table 6. Note that the order of features selected is"
N12-1094,P08-1119,0,0.0257797,"Missing"
N12-1094,W11-0326,1,0.693528,"eatures before the phrase (presumably to give context, for instance as in “out of the window”). Features from after the phrase were not useful. 4 Non-singleton features appear more than once in the data. + + + + + + + + + + C ATEGORY Words Image Bootstrap Spell Length Words Wordnet Spell Spell Wordnet Wordnet P OSITION Phrase Phrase Phrase Before Phrase After Before Before After AUC 74.7 74.4 74.3 75.3 74.7 76.2 76.1 76.0 76.8 77.0 75.6 Sadeghi, 2011), and automatic caption generation (Farhadi et al., 2010; Feng and Lapata, 2010; Ordonez et al., 2011; Kulkarni et al., 2011; Yang et al., 2011; Li et al., 2011; Mitchell et al., 2012), it becomes increasingly important to understand, and to be able to detect, text that actually refers to observed phenomena. Our results suggest that while this is a hard problem, it is possible to leverage large text resources and state-of-the-art computer vision algorithms to address it with high accuracy. Acknowledgments Table 6: Results of feature ablation on L ARGE data set. Corresponding results on the L ARGE data set are shown in Table 6. Note that the order of features selected is different because the training data is different. Here, the most useful features"
N12-1094,U08-1013,0,0.0289441,"Missing"
N12-1094,E12-1076,1,0.90543,"Missing"
N12-1094,W10-0721,0,0.0518612,"n Flickr is almost always written by the photographer of that image. This means the descriptions often contain information that is not actually pictured in the image, or contain references that are only relevant to the photographer (referring to a person/pet by name). One might think that this is an artifact of this particular dataset, but it appears to be generic to all captions, even those written by a viewer (rather than the photographer). Figure 2 shows an image from the Pascal dataset (Everingham et al., 2010), together with captions written by random people collected via crowd-sourcing (Rashtchian et al., 2010). There is much in this caption that is clearly made-up by the author, presumably to make the caption more interesting (e.g., meta-references like “the camera” or “A photo” as well as “guesses” about the image, such as “garage” and “venison”). Second, there is a question of how much inference you are allowed to do when you say that you “see” something. For example, in the top image in Figure 1, the street is pictured, but does that mean that “Hanbury St.” is visual? What if there were a street sign that clearly read “Hanbury St.” in the image? This problem comes up all the time, when people sa"
N12-1094,W02-1028,0,0.0376964,"Missing"
N12-1094,N10-1119,0,0.0106617,"VP to 20 visually descriptive predicates shown in the top of Table 2, and VA to all nouns that appear in the object argument position with respect to the seed predicates. We approximate this by taking nouns on the right hand side of the predicates within a window of 4 words using the Web 1T Google N-gram data (Brants and Franz., 2006). For edge weights, we use conditional probabilities between predicates and arguments so that w(p → a) := pr(a|p) and w(a → p) := pr(p|a). In order to collectively induce the visually descriptive words from this graph, we apply the graph propagation algorithm of Velikovich et al. (2010), a variant of label propagation algorithms (Zhu and Ghahramani, 2002) that has been shown to be effective for inducing a web-scale polarity lexicon based on word co-occurrence statistics. This algoColor Material Shape Size Surface Direction Pattern Quality Beauty Age Ethnicity purple blue maroon beige green plastic cotton wooden metallic silver circular square round rectangular triangular small big tiny tall huge coarse smooth furry fluffy rough sideways north upward left down striped dotted checked plaid quilted shiny rusty dirty burned glittery beautiful cute pretty gorgeous lovely young ma"
N12-1094,J90-1003,0,\N,Missing
N12-1094,D11-1041,1,\N,Missing
N12-1094,W10-0707,0,\N,Missing
N12-1094,W05-1003,0,\N,Missing
N15-1053,W05-0909,0,0.138092,"Missing"
N15-1053,P01-1008,0,0.202497,", 2011; Hodosh et al., 2013; Hodosh and Hockenmaier, 2013; Socher et al., 2014), by providing a new large-scale corpus with unique association structure between images and captions, by proposing an algorithm that exploits the structure, and by exploring two new dimensions: (i) visually situated paraphrasing (and its utility for retrieval-based image captioning), and (ii) creative image captioning. Paraphrasing: Most previous studies in paraphrasing have focused exclusively on text, and the primary goal has been learning semantic equivalence of phrases that would be true out of context (e.g., (Barzilay and McKeown, 2001; Pang et al., 2003; Dolan et al., 2004; Ganitkevitch et al., 2013)), rather than targeting situated or pragmatic equivalence given a context. Emerging efforts began exploring paraphrases that are situated in video content (Chen and Dolan, 2011), news events (Zhang and Weld, 2013), and knowledge base (Berant and Liang, 2014). Our work is the first to introduce vi512 sually situated paraphrasing in which the task is to find paraphrases that are conditioned on both the input text as well as the visual context. (Chen and Dolan, 2011) collected situated paraphrases only through crowd sourcing, whi"
N15-1053,P11-1020,0,0.0532904,"ing two new dimensions: (i) visually situated paraphrasing (and its utility for retrieval-based image captioning), and (ii) creative image captioning. Paraphrasing: Most previous studies in paraphrasing have focused exclusively on text, and the primary goal has been learning semantic equivalence of phrases that would be true out of context (e.g., (Barzilay and McKeown, 2001; Pang et al., 2003; Dolan et al., 2004; Ganitkevitch et al., 2013)), rather than targeting situated or pragmatic equivalence given a context. Emerging efforts began exploring paraphrases that are situated in video content (Chen and Dolan, 2011), news events (Zhang and Weld, 2013), and knowledge base (Berant and Liang, 2014). Our work is the first to introduce vi512 sually situated paraphrasing in which the task is to find paraphrases that are conditioned on both the input text as well as the visual context. (Chen and Dolan, 2011) collected situated paraphrases only through crowd sourcing, while we also explore automatic collection, and further test the quality of automatic paraphrases by using the learned paraphrases in an extrinsic evaluation setting. Figurative language: There has been substantial work for detecting and interpreti"
N15-1053,N12-1094,1,0.881241,"ons that make turkers think they are more fun and creative. As for the visual relevance, by conditioning on the visual context given by query images, the PARAcreative method significantly improves the visual relevance over the text-only counterpart, PARAcaption method. This result highlights the pragmatic differences between visually-situated paraphrasing and text-based paraphrasing. 9 Related work Image-caption corpus: Our work contributes to the line of research that makes use of internet web imagery and text (Ordonez et al., 2011; Berg et al., 2010) by detecting the visually relevant text (Dodge et al., 2012) and reducing the noise (Kuznetsova et al., 2013b; Kuznetsova et al., 2014). Compared to datasets with crowd-sourced captions (Hodosh et al., 2013; Lin et al., 2014), in which each image is annotated with several captions, our dataset presents several images for each caption, a subset of which also includes visually situated paraphrases. The asCreative Image Captioning < Good > -Hood under a full moon (*) -Mirror, mirror on the lake -Sky on the way home(*) -Red sky at night, Shepherd's delight -Sail on by (*) -Row, row, row your boat gently down the stream < Good > < Bad > -Falling water(*) -C"
N15-1053,C04-1051,0,0.0450393,"aier, 2013; Socher et al., 2014), by providing a new large-scale corpus with unique association structure between images and captions, by proposing an algorithm that exploits the structure, and by exploring two new dimensions: (i) visually situated paraphrasing (and its utility for retrieval-based image captioning), and (ii) creative image captioning. Paraphrasing: Most previous studies in paraphrasing have focused exclusively on text, and the primary goal has been learning semantic equivalence of phrases that would be true out of context (e.g., (Barzilay and McKeown, 2001; Pang et al., 2003; Dolan et al., 2004; Ganitkevitch et al., 2013)), rather than targeting situated or pragmatic equivalence given a context. Emerging efforts began exploring paraphrases that are situated in video content (Chen and Dolan, 2011), news events (Zhang and Weld, 2013), and knowledge base (Berant and Liang, 2014). Our work is the first to introduce vi512 sually situated paraphrasing in which the task is to find paraphrases that are conditioned on both the input text as well as the visual context. (Chen and Dolan, 2011) collected situated paraphrases only through crowd sourcing, while we also explore automatic collection"
N15-1053,P14-2074,0,0.0893804,"Missing"
N15-1053,N13-1092,0,0.0961798,"Missing"
N15-1053,D13-1124,1,0.901881,"Missing"
N15-1053,P13-2138,1,0.943817,"However, much research integrating complex textual descriptions to date has been based on datasets that rely on substantial human curation or annotation (Hodosh et al., 2013; Rashtchian et al., 2010; Lin et al., 2014), rather than using the web data in the wild as is (Ordonez et al., 2011; Kuznetsova et al., 2014). The need for human curation limits the potential scale of the multimodal dataset. Without human curation, however, the web data introduces significant noise. In particular, everyday captions often contain extraneous information that is not directly relevant to what the image shows (Kuznetsova et al., 2013b; Hodosh et al., 2013). In this paper, we present a new approach to harvesting a large-scale, high quality image-caption corpus that makes a better use of already existing web data with no additional human efforts. Figure 1 shows sample captions in the resulting corpus, e.g., “butterfly resting on a flower” and “evening walk along the beach”. Notably, some of these are figurative, e.g., “rippled sky” and “sun is going to bed.” The key idea is to focus on D´ej`a Image-Captions, i.e., naturally existing image captions that are repeated almost verbatim by more than one individual for different i"
N15-1053,P14-2097,0,0.0197765,"ne individual for different images. The resulting corpus provides association structure between 4 million images with 180K unique captions, capturing a rich spectrum of everyday narratives including figurative and pragmatic language. Exploring the use of the new corpus, we also present new conceptual tasks of visually situated paraphrasing, creative image captioning, and creative visual paraphrasing. 1 Introduction The use of multimodal web data has been a recurring theme in many recent studies integrating language and vision, e.g., image captioning (Ordonez et al., 2011; Hodosh et al., 2013; Mason and Charniak, 2014; Kuznetsova et al., 2014), text-based image retrieval (Rasiwasia et al., 2010; Rasiwasia et al., 2007), and entry-level categorization (Ordonez et al., 2013; Feng et al., 2015). However, much research integrating complex textual descriptions to date has been based on datasets that rely on substantial human curation or annotation (Hodosh et al., 2013; Rashtchian et al., 2010; Lin et al., 2014), rather than using the web data in the wild as is (Ordonez et al., 2011; Kuznetsova et al., 2014). The need for human curation limits the potential scale of the multimodal dataset. Without human curation"
N15-1053,P12-1074,0,0.0153257,"that are conditioned on both the input text as well as the visual context. (Chen and Dolan, 2011) collected situated paraphrases only through crowd sourcing, while we also explore automatic collection, and further test the quality of automatic paraphrases by using the learned paraphrases in an extrinsic evaluation setting. Figurative language: There has been substantial work for detecting and interpreting figurative language (Shutova, 2010; Li et al., 2013; Kuznetsova et al., 2013a; Tsvetkov et al., 2014), while relatively less work on generating creative or figurative language (Veale, 2011; Ozbal and Strapparava, 2012). We probe data-driven approaches to creative language generation in the context of image captioning. 10 Conclusion To conclude, we have provided insights into making a better use of multimodal web data in the wild, resulting in a large-scale corpus, Deja ImageCaptions, with several unique properties to complement datasets with crowdsourced captions. To validate the usefulness of the corpus, we proposed new image captioning algorithms using the associative structure, which we extended to several related tasks ranging from visually situated paraphrasing to enhanced image captioning. In the proc"
N15-1053,N03-1024,0,0.0830425,"Hodosh and Hockenmaier, 2013; Socher et al., 2014), by providing a new large-scale corpus with unique association structure between images and captions, by proposing an algorithm that exploits the structure, and by exploring two new dimensions: (i) visually situated paraphrasing (and its utility for retrieval-based image captioning), and (ii) creative image captioning. Paraphrasing: Most previous studies in paraphrasing have focused exclusively on text, and the primary goal has been learning semantic equivalence of phrases that would be true out of context (e.g., (Barzilay and McKeown, 2001; Pang et al., 2003; Dolan et al., 2004; Ganitkevitch et al., 2013)), rather than targeting situated or pragmatic equivalence given a context. Emerging efforts began exploring paraphrases that are situated in video content (Chen and Dolan, 2011), news events (Zhang and Weld, 2013), and knowledge base (Berant and Liang, 2014). Our work is the first to introduce vi512 sually situated paraphrasing in which the task is to find paraphrases that are conditioned on both the input text as well as the visual context. (Chen and Dolan, 2011) collected situated paraphrases only through crowd sourcing, while we also explore"
N15-1053,P02-1040,0,0.0943004,"Missing"
N15-1053,W10-0721,0,0.398432,"tive visual paraphrasing. 1 Introduction The use of multimodal web data has been a recurring theme in many recent studies integrating language and vision, e.g., image captioning (Ordonez et al., 2011; Hodosh et al., 2013; Mason and Charniak, 2014; Kuznetsova et al., 2014), text-based image retrieval (Rasiwasia et al., 2010; Rasiwasia et al., 2007), and entry-level categorization (Ordonez et al., 2013; Feng et al., 2015). However, much research integrating complex textual descriptions to date has been based on datasets that rely on substantial human curation or annotation (Hodosh et al., 2013; Rashtchian et al., 2010; Lin et al., 2014), rather than using the web data in the wild as is (Ordonez et al., 2011; Kuznetsova et al., 2014). The need for human curation limits the potential scale of the multimodal dataset. Without human curation, however, the web data introduces significant noise. In particular, everyday captions often contain extraneous information that is not directly relevant to what the image shows (Kuznetsova et al., 2013b; Hodosh et al., 2013). In this paper, we present a new approach to harvesting a large-scale, high quality image-caption corpus that makes a better use of already existing we"
N15-1053,P10-1071,0,0.013496,"Weld, 2013), and knowledge base (Berant and Liang, 2014). Our work is the first to introduce vi512 sually situated paraphrasing in which the task is to find paraphrases that are conditioned on both the input text as well as the visual context. (Chen and Dolan, 2011) collected situated paraphrases only through crowd sourcing, while we also explore automatic collection, and further test the quality of automatic paraphrases by using the learned paraphrases in an extrinsic evaluation setting. Figurative language: There has been substantial work for detecting and interpreting figurative language (Shutova, 2010; Li et al., 2013; Kuznetsova et al., 2013a; Tsvetkov et al., 2014), while relatively less work on generating creative or figurative language (Veale, 2011; Ozbal and Strapparava, 2012). We probe data-driven approaches to creative language generation in the context of image captioning. 10 Conclusion To conclude, we have provided insights into making a better use of multimodal web data in the wild, resulting in a large-scale corpus, Deja ImageCaptions, with several unique properties to complement datasets with crowdsourced captions. To validate the usefulness of the corpus, we proposed new image"
N15-1053,Q14-1017,0,0.0263094,"r nouns (physical objects) listed under WordNet (Miller, 1995), our corpus is built for expressive phrases and full sentences and constructed without human curation. Our corpus has several unique properties to complement existing corpora. As explored in a very recent work of (Gong et al., 2014), we expect that it is possible to combine crowd-sourced and web-harvested datasets and achieve the best of both worlds. Image captioning: Our work contributes to the increasing body of research on retrieval-based image captioning (Ordonez et al., 2011; Hodosh et al., 2013; Hodosh and Hockenmaier, 2013; Socher et al., 2014), by providing a new large-scale corpus with unique association structure between images and captions, by proposing an algorithm that exploits the structure, and by exploring two new dimensions: (i) visually situated paraphrasing (and its utility for retrieval-based image captioning), and (ii) creative image captioning. Paraphrasing: Most previous studies in paraphrasing have focused exclusively on text, and the primary goal has been learning semantic equivalence of phrases that would be true out of context (e.g., (Barzilay and McKeown, 2001; Pang et al., 2003; Dolan et al., 2004; Ganitkevitch"
N15-1053,P14-1024,0,0.0134964,"Our work is the first to introduce vi512 sually situated paraphrasing in which the task is to find paraphrases that are conditioned on both the input text as well as the visual context. (Chen and Dolan, 2011) collected situated paraphrases only through crowd sourcing, while we also explore automatic collection, and further test the quality of automatic paraphrases by using the learned paraphrases in an extrinsic evaluation setting. Figurative language: There has been substantial work for detecting and interpreting figurative language (Shutova, 2010; Li et al., 2013; Kuznetsova et al., 2013a; Tsvetkov et al., 2014), while relatively less work on generating creative or figurative language (Veale, 2011; Ozbal and Strapparava, 2012). We probe data-driven approaches to creative language generation in the context of image captioning. 10 Conclusion To conclude, we have provided insights into making a better use of multimodal web data in the wild, resulting in a large-scale corpus, Deja ImageCaptions, with several unique properties to complement datasets with crowdsourced captions. To validate the usefulness of the corpus, we proposed new image captioning algorithms using the associative structure, which we ex"
N15-1053,P11-1029,0,0.0301741,"d paraphrases that are conditioned on both the input text as well as the visual context. (Chen and Dolan, 2011) collected situated paraphrases only through crowd sourcing, while we also explore automatic collection, and further test the quality of automatic paraphrases by using the learned paraphrases in an extrinsic evaluation setting. Figurative language: There has been substantial work for detecting and interpreting figurative language (Shutova, 2010; Li et al., 2013; Kuznetsova et al., 2013a; Tsvetkov et al., 2014), while relatively less work on generating creative or figurative language (Veale, 2011; Ozbal and Strapparava, 2012). We probe data-driven approaches to creative language generation in the context of image captioning. 10 Conclusion To conclude, we have provided insights into making a better use of multimodal web data in the wild, resulting in a large-scale corpus, Deja ImageCaptions, with several unique properties to complement datasets with crowdsourced captions. To validate the usefulness of the corpus, we proposed new image captioning algorithms using the associative structure, which we extended to several related tasks ranging from visually situated paraphrasing to enhanced"
N15-1053,H05-1044,0,0.0282595,"Missing"
N15-1053,D13-1183,0,0.0121978,"situated paraphrasing (and its utility for retrieval-based image captioning), and (ii) creative image captioning. Paraphrasing: Most previous studies in paraphrasing have focused exclusively on text, and the primary goal has been learning semantic equivalence of phrases that would be true out of context (e.g., (Barzilay and McKeown, 2001; Pang et al., 2003; Dolan et al., 2004; Ganitkevitch et al., 2013)), rather than targeting situated or pragmatic equivalence given a context. Emerging efforts began exploring paraphrases that are situated in video content (Chen and Dolan, 2011), news events (Zhang and Weld, 2013), and knowledge base (Berant and Liang, 2014). Our work is the first to introduce vi512 sually situated paraphrasing in which the task is to find paraphrases that are conditioned on both the input text as well as the visual context. (Chen and Dolan, 2011) collected situated paraphrases only through crowd sourcing, while we also explore automatic collection, and further test the quality of automatic paraphrases by using the learned paraphrases in an extrinsic evaluation setting. Figurative language: There has been substantial work for detecting and interpreting figurative language (Shutova, 201"
N15-1053,Q13-1031,0,\N,Missing
N15-1053,W10-0707,0,\N,Missing
N15-1053,P14-1133,0,\N,Missing
N15-1053,Q14-1028,1,\N,Missing
N18-1016,P05-1018,0,0.054586,"ds to fine-tune neural generation models using automatic measures such as CIDEr as the reward. However, because most existing automatic measures focus on local n-gram patterns, fine-tuning on those measures may yield deteriorated text despite increased automatic scores, especially for tasks that require long coherent generation (§6.1). Since writing out a scoring term that quantifies the quality of discourse coherence is an open research question, we take inspiration from previous research that learns the overall ordering structure of a document as an approximation of the discourse structure (Barzilay and Lapata, 2005, 2008; Barzilay and Lee, 2004; Li and Hovy, 2014), and propose two neural teachers that can learn to score an ordered sequence of sentences. The scores from these neural teachers are then used to formulate rewards (§4.2) that guide coherent long text generation systems in a policy gradient reinforcement learning setup. Notably, the neural teachers are trained offline on gold sequences in an unsupervised manner prior to training the generator. They are not trained jointly with the generator and their parameters are fixed during policy learning. 2.1 … GRU sj = Lj X xij (1) i=1 where xij is a wo"
N18-1016,D16-1127,1,0.931436,"gure 1: The generator is rewarded for imitating the discourse structure of the gold sequence. Importantly, most automatic measures are based on local n-gram patterns, providing only a limited and myopic perspective of overall text quality. As a result, while models trained to directly optimize these measures can yield improvements on the same measures, they may not lead to better quality in terms of overall coherence or discourse structure. Indeed, recent studies have reported cases where commonly used measures do not align well with desired aspects of generation quality (Rennie et al., 2017; Li et al., 2016). The challenge, however, is to define a global score that can measure the complex aspects of text quality beyond local n-gram patterns. In this paper, we investigate learning neural rewards and their use in a reinforcement learning regime with a specific focus on learning more discourse-aware and coherent text generation. Our approach shares the spirit of the work of Lowe et al. (2017), where neural scores were learned to approximate human judgments of dialogue quality. The key difference is that our rewards can be fully automatically constructed without requiring human judgments and can be t"
N18-1016,J08-1001,0,0.328444,"Missing"
N18-1016,N04-1015,0,0.102263,"models using automatic measures such as CIDEr as the reward. However, because most existing automatic measures focus on local n-gram patterns, fine-tuning on those measures may yield deteriorated text despite increased automatic scores, especially for tasks that require long coherent generation (§6.1). Since writing out a scoring term that quantifies the quality of discourse coherence is an open research question, we take inspiration from previous research that learns the overall ordering structure of a document as an approximation of the discourse structure (Barzilay and Lapata, 2005, 2008; Barzilay and Lee, 2004; Li and Hovy, 2014), and propose two neural teachers that can learn to score an ordered sequence of sentences. The scores from these neural teachers are then used to formulate rewards (§4.2) that guide coherent long text generation systems in a policy gradient reinforcement learning setup. Notably, the neural teachers are trained offline on gold sequences in an unsupervised manner prior to training the generator. They are not trained jointly with the generator and their parameters are fixed during policy learning. 2.1 … GRU sj = Lj X xij (1) i=1 where xij is a word embedding and sj is a sente"
N18-1016,W04-1013,0,0.0121079,"al loss for training text generation models remains an open research question. Many existing approaches based on variants of recurrent neural networks (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) are trained using cross-entropy loss (Bahdanau et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Rush et al., 2015), often augmented with additional terms for topic coverage or task-specific supervision (Kiddon et al., 2016; Yang et al., 2017). Training with cross-entropy, however, does not always correlate well with achieving high scores on commonly used evaluation measures such as ROUGE (Lin, 2004), BLEU (Papineni et al., 2002), or CIDEr (Vedantam et al., 2015). Another current line of research therefore explores training generation models that directly optimize the target evaluation measure (Wu et al., 2016; Ranzato et al., 2015; Paulus et al., 2018; Rennie et al., 2017) using reinforcement learning methods such as the REINFORCE algorithm (Williams, 1992). ∗ Gold Recipe Work done while author was at Microsoft Research 173 Proceedings of NAACL-HLT 2018, pages 173–184 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics underlying text generator (see"
N18-1016,P09-1068,0,0.0299016,"ilar to our work is work on using neural and embedding rewards to improve dialogue (Li et al., 2016), image captioning (Ren et al., 2017), simplification (Zhang and Lapata, 2017), and paraphrase generation (Li et al., 2017). While these works use single-sentence similarity rewards for short generation tasks, our work designs teachers to reward long-range ordering patterns. Finally, our teachers can be seen as rewarding generators that approximate script patterns in recipes. Previous work in learning script knowledge (Schank and Abelson, 1975) has focused on extracting scripts from long texts (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2016), with some of that work focusing on recipes (Kiddon et al., 2015; Mori et al., 2014, 2012). Our teachers implicitly learn this script knowledge and reward recipe generators for exhibiting it. 8 Conclusion We introduce the absolute ordering and relative ordering teachers, two neural networks that score a sequence’s adherence to discourse structure in long text. The teachers are used to compute rewards for a self-critical reinforcement learning framework, allowing a recipe generator to be rewarded for capturing temporal semantics of the cooking domain. Empirical resu"
N18-1016,P17-1103,0,0.0265649,"ity in terms of overall coherence or discourse structure. Indeed, recent studies have reported cases where commonly used measures do not align well with desired aspects of generation quality (Rennie et al., 2017; Li et al., 2016). The challenge, however, is to define a global score that can measure the complex aspects of text quality beyond local n-gram patterns. In this paper, we investigate learning neural rewards and their use in a reinforcement learning regime with a specific focus on learning more discourse-aware and coherent text generation. Our approach shares the spirit of the work of Lowe et al. (2017), where neural scores were learned to approximate human judgments of dialogue quality. The key difference is that our rewards can be fully automatically constructed without requiring human judgments and can be trained in an unsupervised manner. More specifically, we propose a neural reward learning scheme that is trained to capture crosssentence ordering structure as a means to approximate the desired discourse structure in documents. The learned teacher computes rewards for the Introduction Defining an ideal loss for training text generation models remains an open research question. Many exis"
N18-1016,mori-etal-2014-flow,0,0.0430861,"n et al., 2017), simplification (Zhang and Lapata, 2017), and paraphrase generation (Li et al., 2017). While these works use single-sentence similarity rewards for short generation tasks, our work designs teachers to reward long-range ordering patterns. Finally, our teachers can be seen as rewarding generators that approximate script patterns in recipes. Previous work in learning script knowledge (Schank and Abelson, 1975) has focused on extracting scripts from long texts (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2016), with some of that work focusing on recipes (Kiddon et al., 2015; Mori et al., 2014, 2012). Our teachers implicitly learn this script knowledge and reward recipe generators for exhibiting it. 8 Conclusion We introduce the absolute ordering and relative ordering teachers, two neural networks that score a sequence’s adherence to discourse structure in long text. The teachers are used to compute rewards for a self-critical reinforcement learning framework, allowing a recipe generator to be rewarded for capturing temporal semantics of the cooking domain. Empirical results demonstrate that our teacher-trained generator better models the latent event sequences of cooking recipes,"
N18-1016,D14-1179,0,0.018117,"Missing"
N18-1016,P02-1040,0,0.101245,"ng text generation models remains an open research question. Many existing approaches based on variants of recurrent neural networks (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) are trained using cross-entropy loss (Bahdanau et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Rush et al., 2015), often augmented with additional terms for topic coverage or task-specific supervision (Kiddon et al., 2016; Yang et al., 2017). Training with cross-entropy, however, does not always correlate well with achieving high scores on commonly used evaluation measures such as ROUGE (Lin, 2004), BLEU (Papineni et al., 2002), or CIDEr (Vedantam et al., 2015). Another current line of research therefore explores training generation models that directly optimize the target evaluation measure (Wu et al., 2016; Ranzato et al., 2015; Paulus et al., 2018; Rennie et al., 2017) using reinforcement learning methods such as the REINFORCE algorithm (Williams, 1992). ∗ Gold Recipe Work done while author was at Microsoft Research 173 Proceedings of NAACL-HLT 2018, pages 173–184 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics underlying text generator (see Figure 1), which is trained u"
N18-1016,D17-1103,0,0.0163234,"del for the mixed reward was chosen as the one that achieved the highest average geometric mean of BLEU-4 reward and average relative ordering reward for each generated sequence y in the development set: Mixed Training As the model learns parameters to optimize the amount of reward it receives from the teacher, it is not explicity encouraged to produce fluent generations. The model quickly learns to generate simple sequences that exploit the teacher for high rewards despite being incoherent recipes (e.g., Figure 4). Consequently, it is possible that generated sequences are no longer readable (Pasunuru and Bansal, 2017; Paulus et al., 2018). Title: Chili Grits Ingredients: boiling water, butter, shredded cheddar cheese, jalapenos, eggs, chicken cream of soup, salt Generated Recipe: Here . T rb4 (y) X r¯ = rRO (yt ) T (17) t=1 Figure 4: Recipe generated from a self-critical model with no mixed training where rb4 is the BLEU-4 score of the whole generated sequence, and rRO is computed using Equa177 Model Cross-entropy (MLE) BLEU-4 (Rennie et al., 2017) CIDEr (Rennie et al., 2017) ROUGE-L (Paulus et al., 2018) BLEU-1 (γ = 0.97) BLEU-4 (γ = 0.99) CIDEr (γ = 0.97) ROUGE-L (γ = 0.97) Absolute Ordering (AO) Relati"
N18-1016,D15-1114,1,0.861518,"image captioning (Ren et al., 2017), simplification (Zhang and Lapata, 2017), and paraphrase generation (Li et al., 2017). While these works use single-sentence similarity rewards for short generation tasks, our work designs teachers to reward long-range ordering patterns. Finally, our teachers can be seen as rewarding generators that approximate script patterns in recipes. Previous work in learning script knowledge (Schank and Abelson, 1975) has focused on extracting scripts from long texts (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2016), with some of that work focusing on recipes (Kiddon et al., 2015; Mori et al., 2014, 2012). Our teachers implicitly learn this script knowledge and reward recipe generators for exhibiting it. 8 Conclusion We introduce the absolute ordering and relative ordering teachers, two neural networks that score a sequence’s adherence to discourse structure in long text. The teachers are used to compute rewards for a self-critical reinforcement learning framework, allowing a recipe generator to be rewarded for capturing temporal semantics of the cooking domain. Empirical results demonstrate that our teacher-trained generator better models the latent event sequences o"
N18-1016,D16-1032,1,0.893981,"to capture crosssentence ordering structure as a means to approximate the desired discourse structure in documents. The learned teacher computes rewards for the Introduction Defining an ideal loss for training text generation models remains an open research question. Many existing approaches based on variants of recurrent neural networks (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) are trained using cross-entropy loss (Bahdanau et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Rush et al., 2015), often augmented with additional terms for topic coverage or task-specific supervision (Kiddon et al., 2016; Yang et al., 2017). Training with cross-entropy, however, does not always correlate well with achieving high scores on commonly used evaluation measures such as ROUGE (Lin, 2004), BLEU (Papineni et al., 2002), or CIDEr (Vedantam et al., 2015). Another current line of research therefore explores training generation models that directly optimize the target evaluation measure (Wu et al., 2016; Ranzato et al., 2015; Paulus et al., 2018; Rennie et al., 2017) using reinforcement learning methods such as the REINFORCE algorithm (Williams, 1992). ∗ Gold Recipe Work done while author was at Microsoft"
N18-1016,P16-1027,0,0.0227005,"sing neural and embedding rewards to improve dialogue (Li et al., 2016), image captioning (Ren et al., 2017), simplification (Zhang and Lapata, 2017), and paraphrase generation (Li et al., 2017). While these works use single-sentence similarity rewards for short generation tasks, our work designs teachers to reward long-range ordering patterns. Finally, our teachers can be seen as rewarding generators that approximate script patterns in recipes. Previous work in learning script knowledge (Schank and Abelson, 1975) has focused on extracting scripts from long texts (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2016), with some of that work focusing on recipes (Kiddon et al., 2015; Mori et al., 2014, 2012). Our teachers implicitly learn this script knowledge and reward recipe generators for exhibiting it. 8 Conclusion We introduce the absolute ordering and relative ordering teachers, two neural networks that score a sequence’s adherence to discourse structure in long text. The teachers are used to compute rewards for a self-critical reinforcement learning framework, allowing a recipe generator to be rewarded for capturing temporal semantics of the cooking domain. Empirical results demonstrate that our tea"
N18-1016,D14-1218,0,0.0229299,"measures such as CIDEr as the reward. However, because most existing automatic measures focus on local n-gram patterns, fine-tuning on those measures may yield deteriorated text despite increased automatic scores, especially for tasks that require long coherent generation (§6.1). Since writing out a scoring term that quantifies the quality of discourse coherence is an open research question, we take inspiration from previous research that learns the overall ordering structure of a document as an approximation of the discourse structure (Barzilay and Lapata, 2005, 2008; Barzilay and Lee, 2004; Li and Hovy, 2014), and propose two neural teachers that can learn to score an ordered sequence of sentences. The scores from these neural teachers are then used to formulate rewards (§4.2) that guide coherent long text generation systems in a policy gradient reinforcement learning setup. Notably, the neural teachers are trained offline on gold sequences in an unsupervised manner prior to training the generator. They are not trained jointly with the generator and their parameters are fixed during policy learning. 2.1 … GRU sj = Lj X xij (1) i=1 where xij is a word embedding and sj is a sentence embedding. Each"
N18-1016,D17-1197,0,0.0277131,"nce ordering structure as a means to approximate the desired discourse structure in documents. The learned teacher computes rewards for the Introduction Defining an ideal loss for training text generation models remains an open research question. Many existing approaches based on variants of recurrent neural networks (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) are trained using cross-entropy loss (Bahdanau et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Rush et al., 2015), often augmented with additional terms for topic coverage or task-specific supervision (Kiddon et al., 2016; Yang et al., 2017). Training with cross-entropy, however, does not always correlate well with achieving high scores on commonly used evaluation measures such as ROUGE (Lin, 2004), BLEU (Papineni et al., 2002), or CIDEr (Vedantam et al., 2015). Another current line of research therefore explores training generation models that directly optimize the target evaluation measure (Wu et al., 2016; Ranzato et al., 2015; Paulus et al., 2018; Rennie et al., 2017) using reinforcement learning methods such as the REINFORCE algorithm (Williams, 1992). ∗ Gold Recipe Work done while author was at Microsoft Research 173 Procee"
N18-1016,D17-1062,0,0.0267216,"0.056 0.054 0.052 0.050 0.048 State Change BLEU-4 0.95 0.97 0.98 0.300 0.295 0.290 0.285 0.280 0.275 Figure 5: Action and State Change BLEU Metrics for different initializations of `max and γ guage model to deteriorate. Interestingly, a higher `max leads to better performance on global coherence scores, implying that relative order rewards conditioned on more sentences allow the model to learn longer-range context co-occurrences. 7 Most similar to our work is work on using neural and embedding rewards to improve dialogue (Li et al., 2016), image captioning (Ren et al., 2017), simplification (Zhang and Lapata, 2017), and paraphrase generation (Li et al., 2017). While these works use single-sentence similarity rewards for short generation tasks, our work designs teachers to reward long-range ordering patterns. Finally, our teachers can be seen as rewarding generators that approximate script patterns in recipes. Previous work in learning script knowledge (Schank and Abelson, 1975) has focused on extracting scripts from long texts (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2016), with some of that work focusing on recipes (Kiddon et al., 2015; Mori et al., 2014, 2012). Our teachers implicitly learn"
N18-1016,D15-1044,0,0.164506,"d in an unsupervised manner. More specifically, we propose a neural reward learning scheme that is trained to capture crosssentence ordering structure as a means to approximate the desired discourse structure in documents. The learned teacher computes rewards for the Introduction Defining an ideal loss for training text generation models remains an open research question. Many existing approaches based on variants of recurrent neural networks (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) are trained using cross-entropy loss (Bahdanau et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Rush et al., 2015), often augmented with additional terms for topic coverage or task-specific supervision (Kiddon et al., 2016; Yang et al., 2017). Training with cross-entropy, however, does not always correlate well with achieving high scores on commonly used evaluation measures such as ROUGE (Lin, 2004), BLEU (Papineni et al., 2002), or CIDEr (Vedantam et al., 2015). Another current line of research therefore explores training generation models that directly optimize the target evaluation measure (Wu et al., 2016; Ranzato et al., 2015; Paulus et al., 2018; Rennie et al., 2017) using reinforcement learning met"
N18-1016,D17-1239,0,0.0387937,"Missing"
N18-1016,1983.tc-1.13,0,0.67195,"Missing"
N18-1150,N16-1012,0,0.139479,"e capable of generating fluent language, variants of encoder-decoder RNNs (Sutskever et al., 2014; Bahdanau et al., 2015) have shown promising results on the abstractive summarization task (Rush et al., 2015; Nallapati et al., 2017). The fundamental challenge, however, is that the strong performance of neural models at encoding short text does not generalize well to long text. The motivation behind our approach is to be able to dynamically attend to different parts of the input to capture salient facts. While recent work in summarization addresses these issues using improved attention models (Chopra et al., 2016), pointer networks with coverage mechanisms (See et al., 2017), and coherence-focused training objectives (Paulus et al., 2018; Jaques et al., 2017), an effective mechanism for representing a long document remains a challenge. Simultaneous work has investigated the use of deep communicating agents (Sukhbaatar et al., 2016) for collaborative tasks such as logic puzzles (Foerster et al., 2016), visual dialog (Das et al., 2017), and reference games (Lazaridou et al., 2016). Our work builds on these approaches to propose the first study on using communicating agents to encode long text for summari"
N18-1150,P16-1188,0,0.222848,"Missing"
N18-1150,W16-3617,0,0.0592334,"Missing"
N18-1150,D15-1166,0,0.153582,"Missing"
N18-1150,P14-5010,0,0.00914565,"Missing"
N18-1150,1983.tc-1.13,0,0.463783,"Missing"
N18-1150,N16-1174,1,0.51722,"capture all the facts in the human summary, while (m7) is able to include all the facts with few extra details, generating more relevant and diverse summaries. 6 Related Work Several recent works investigate attention mechanisms for encoder-decoder models to sharpen the 1669 context that the decoder should focus on within the input encoding (Luong et al., 2015; Vinyals et al., 2015b; Bahdanau et al., 2015). For example, Luong et al. (2015) proposes global and local attention networks for machine translation, while others investigate hierarchical attention networks for document classification (Yang et al., 2016), sentiment classification (Chen et al., 2016), and dialog response selection (Zhou et al., 2016). Attention mechanisms have shown to be crucial for summarization as well (Rush et al., 2015; Zeng et al., 2016; Nallapati et al., 2017), and pointer networks (Vinyals et al., 2015a), in particular, help address redundancy and saliency in generated summaries (Cheng and Lapata, 2016; See et al., 2017; Paulus et al., 2018; Fan et al., 2017). While we share the same motivation as these works, our work uniquely presents an approach based on CommNet, the deep communicating agent framework (Sukhbaatar et"
N18-1150,D17-1103,0,0.0367234,"h sentence s0q , q=1. . . Q, where s0q ∈{st :yt =‘·’, 1≤t≤T }, are used to compute the cosine similarity between two consecutively generated sentences. To minimize the similarity between end-of-sentence hidden states we define a semantic cohesion loss: 1665 LSEM = PQ 0 0 q=2 cos(sq , sq−1 ) (18) The final training objective is then: LMLE-SEM = LMLE + λLSEM (19) where λ is a tunable hyperparameter. Reinforcement Learning (RL) Loss Policy gradient methods can directly optimize discrete target evaluation metrics such as ROUGE that are non-differentiable (Paulus et al., 2018; Jaques et al., 2017; Pasunuru and Bansal, 2017; Wu et al., 2016). At each time step, the word generated by the model can be viewed as an action taken by an RL agent. Once the full sequence yˆ is generated, it is compared against the ground truth sequence y ∗ to compute the reward r(ˆ y ). Our model learns using a self-critical training approach (Rennie et al., 2016), which learns by exploring new sequences and comparing them to the best greedily decoded sequence. For each training example d, two output sequences are generated: yˆ, which is sampled from the probability distribution at each time step, p(ˆ yt |ˆ y1 . . . yˆt−1 , d), and y˜,"
N18-1150,D14-1162,0,0.0795994,"Missing"
N18-1150,D16-1036,0,0.0187609,"extra details, generating more relevant and diverse summaries. 6 Related Work Several recent works investigate attention mechanisms for encoder-decoder models to sharpen the 1669 context that the decoder should focus on within the input encoding (Luong et al., 2015; Vinyals et al., 2015b; Bahdanau et al., 2015). For example, Luong et al. (2015) proposes global and local attention networks for machine translation, while others investigate hierarchical attention networks for document classification (Yang et al., 2016), sentiment classification (Chen et al., 2016), and dialog response selection (Zhou et al., 2016). Attention mechanisms have shown to be crucial for summarization as well (Rush et al., 2015; Zeng et al., 2016; Nallapati et al., 2017), and pointer networks (Vinyals et al., 2015a), in particular, help address redundancy and saliency in generated summaries (Cheng and Lapata, 2016; See et al., 2017; Paulus et al., 2018; Fan et al., 2017). While we share the same motivation as these works, our work uniquely presents an approach based on CommNet, the deep communicating agent framework (Sukhbaatar et al., 2016). Compared to prior multi-agent works on logic puzzles (Foerster et al., 2017), langua"
N18-1150,D15-1044,0,0.839936,"input text. Introduction We focus on the task of abstractive summarization of a long document. In contrast to extractive summarization, where a summary is composed of a subset of sentences or words lifted from the input text as is, abstractive summarization requires the generative ability to rephrase and restructure sentences to compose a coherent and concise summary. As recurrent neural networks (RNNs) are capable of generating fluent language, variants of encoder-decoder RNNs (Sutskever et al., 2014; Bahdanau et al., 2015) have shown promising results on the abstractive summarization task (Rush et al., 2015; Nallapati et al., 2017). The fundamental challenge, however, is that the strong performance of neural models at encoding short text does not generalize well to long text. The motivation behind our approach is to be able to dynamically attend to different parts of the input to capture salient facts. While recent work in summarization addresses these issues using improved attention models (Chopra et al., 2016), pointer networks with coverage mechanisms (See et al., 2017), and coherence-focused training objectives (Paulus et al., 2018; Jaques et al., 2017), an effective mechanism for representi"
N18-1150,P17-1108,0,0.594329,"formance gains. We fix γ = 0.97 for the RL term in Equation (21) and λ = 0.1 for the SEM term in MLE and MIXED training. Additional details are provided in Appendix A.2. Evaluation We evaluate our system using ROUGE-1 (unigram recall), ROUGE-2 (bigram recall) and ROUGE-L (longest common sequence).1 We select the MLE models with the lowest negative log-likelihood and the MLE+RL models with the highest ROUGE-L scores on a sample of validation data to evaluate on the test 1666 1 We use pyrouge (pypi.python.org/pypi/pyrouge/0.1.3). Model SummaRuNNer (Nallapati et al., 2017) graph-based attention (Tan et al., 2017) pointer generator (See et al., 2017) pointer generator + coverage (See et al., 2017) controlled summarization with fixed values (Fan et al., 2017) RL, with intra-attention (Paulus et al., 2018) ML+RL, with intra-attention(Paulus et al., 2018) (m1) MLE, pgen, no-comm (1-agent) (our baseline-1) (m2) MLE+SEM, pgen, no-comm (1-agent) (our baseline-2) (m3) MLE+RL, pgen, no-comm (1-agent) (our baseline-3) (m4) DCA MLE+SEM, pgen, no-comm (3-agents) (m5) DCA MLE+SEM, mpgen, with-comm (3-agents) (m6) DCA MLE+SEM, mpgen, with-comm, with caa (3-agents) (m7) DCA MLE+SEM+RL, mpgen, with-comm, with caa (3-"
N18-1150,D15-1011,0,\N,Missing
N18-1150,P16-1046,0,\N,Missing
N18-2011,D16-1126,1,0.882175,"Missing"
N18-2011,D10-1016,0,0.271511,"Missing"
N18-2011,P17-4008,1,0.906803,"Missing"
N18-2011,D10-1051,1,0.875698,"Missing"
N18-2011,P17-1016,0,0.244481,"Missing"
N18-2011,J03-1002,0,0.0101892,"arch. Figure 1 shows how this technique addresses the problem. the FSA does not accept &lt;UNK>, as it is not pronounceable. We can let the system produce its next guess instead, but &lt;UNK> is a sign that the translation system is not sure about the source meaning. To overcome this problem, we use an idea similar to model B. This time, in addition to encouraging the unconstrained translated words, we encourage all potential translations of the foreign words. To get the potential translations, we use the translation table (t-table) extracted from parallel French-English training data using Giza++ (Och and Ney, 2003). This way, the system receives an external signal that guides it toward selecting better translations for the rare foreign word. We run five iterations of each of IBM models 1, 2, HMM, and 4 to get the t-table. An example of how this method improves the poem quality over model B can be observed in the fifth line of the poems in Figure 2. French poem: Sans mains tordues, comme ces hommes, Ces pauvres hommes sans espoir, Qui osent nourrir lesp´erance Dans le caveau du d´esespoir: Il regardait vers le soleil Et buvait lair frais jusquau soir. Human reference: He did not wring his hands, as do Th"
N18-2011,W17-3502,0,0.106636,"Missing"
N18-2011,D14-1074,0,0.35197,"Missing"
N18-5020,P17-3020,0,0.0264314,"ational AI that has challenged researchers since 1990. Recent work has addressed tasks where passing the Turing test is not a concern. Goaloriented conversational systems facilitate natural user interaction with devices via text and spoken language. These AI assistants typically focus on short interactions, as in commercial products such as Amazon Alexa, Microsoft Cortana, Google Assistant, and Apple Siri. General conversational systems, called chatbots, have constrained social interaction capabilities but have difficulty generating conversations with long-term coherence (Serban et al., 2017; Sato et al., 2017; Shao et al., 2017; Tian et al., 2017; Ghazvininejad et al., 2018). The Alexa Prize sets forth a new challenge: creating a system that can hold a coherent and engaging conversation on current events and popular topics such as sports, politics, entertainment, fashion and technology (Ram et al., 2017). Our system, Sound1 Figure 1: A sample dialog. Suspected speech recognition errors in the user utterances are underlined. ing Board,2 demonstrates that it is feasible to build an agent that can engage in long-term conversation when backed by rich content and knowledge of the user obtained through"
N18-5020,D17-1235,0,0.0251686,"challenged researchers since 1990. Recent work has addressed tasks where passing the Turing test is not a concern. Goaloriented conversational systems facilitate natural user interaction with devices via text and spoken language. These AI assistants typically focus on short interactions, as in commercial products such as Amazon Alexa, Microsoft Cortana, Google Assistant, and Apple Siri. General conversational systems, called chatbots, have constrained social interaction capabilities but have difficulty generating conversations with long-term coherence (Serban et al., 2017; Sato et al., 2017; Shao et al., 2017; Tian et al., 2017; Ghazvininejad et al., 2018). The Alexa Prize sets forth a new challenge: creating a system that can hold a coherent and engaging conversation on current events and popular topics such as sports, politics, entertainment, fashion and technology (Ram et al., 2017). Our system, Sound1 Figure 1: A sample dialog. Suspected speech recognition errors in the user utterances are underlined. ing Board,2 demonstrates that it is feasible to build an agent that can engage in long-term conversation when backed by rich content and knowledge of the user obtained through interaction. Soundi"
N18-5020,P17-2036,0,0.0298266,"hers since 1990. Recent work has addressed tasks where passing the Turing test is not a concern. Goaloriented conversational systems facilitate natural user interaction with devices via text and spoken language. These AI assistants typically focus on short interactions, as in commercial products such as Amazon Alexa, Microsoft Cortana, Google Assistant, and Apple Siri. General conversational systems, called chatbots, have constrained social interaction capabilities but have difficulty generating conversations with long-term coherence (Serban et al., 2017; Sato et al., 2017; Shao et al., 2017; Tian et al., 2017; Ghazvininejad et al., 2018). The Alexa Prize sets forth a new challenge: creating a system that can hold a coherent and engaging conversation on current events and popular topics such as sports, politics, entertainment, fashion and technology (Ram et al., 2017). Our system, Sound1 Figure 1: A sample dialog. Suspected speech recognition errors in the user utterances are underlined. ing Board,2 demonstrates that it is feasible to build an agent that can engage in long-term conversation when backed by rich content and knowledge of the user obtained through interaction. Sounding Board won the in"
N19-1245,N16-1136,1,0.890692,"nsolvable without brute-force enumeration of solutions, and rationales that contain few or none of the steps required to solve the corresponding problem. The motivation for our dataset comes from the fact we want to maintain the challenging nature of the problems included in the AQuA dataset, while removing noise that hinders the ability of neuralized models to learn the types of signal neccessary for problem-solving by logical reasoning. Additional Datasets Several smaller datasets have been compiled in recent years. Most of these works have focused on algebra word problems, including MaWPS (Koncel-Kedziorski et al., 2016), Alg514 (Kushman et al., 2014), and DRAW-1K (Upadhyay and Chang, 2017). Many of these datasets have sought to align underlying equations or systems of equations with word problem text. While recent works like (Liang et al., 2018; Locascio et al., 2016) have explored representing math word problems with logical formalisms and regular expressions, our work is the first to provide well-defined formalisms for representing intermediate problem-solving steps that are shown to be generalizable beyond algebra problems. Solving with Handcrafted Features Due to sparsity of suitable data, early work on"
N19-1245,P14-1026,0,0.419077,"a new large-scale, diverse dataset of 37k English multiple-choice math word problems covering multiple math domain categories by modeling operation programs corresponding to word problems in the AQuA dataset (Ling et al., 2017). We introduce a neural model for mapping problems to operation programs with domain categorization. 1 The dataset is available at: https://math-qa. github.io/math-QA/ 2357 Proceedings of NAACL-HLT 2019, pages 2357–2367 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Most current datasets in this domain are small in scale (Kushman et al., 2014) or do not offer precise operational annotations over diverse problem types (Ling et al., 2017). This is mainly due to the fact that annotating math word problems precisely across diverse problem categories is challenging even for humans, requiring background math knowledge for annotators. Our representation language facilitates the annotation task for crowd-sourcing and increases the interpretability of the proposed model. Our sequence-to-program model with categorization trained on our MathQA dataset outperforms previous state-of-the-art on the AQuA test set in spite of the smaller training"
N19-1245,P17-1005,0,0.0268874,"ould make use of both math knowledge and 2 Here the arguments 174, 254 and 349 are the outputs of operations 1, 2 and 3 respectively. 2359 Category Geometry Physics Probability Gain-Loss General Other All domain knowledge associated with subfields like geometry and probability to determine which operations and arguments to use. • Human interpretability → Each operation and argument used to obtain the correct solution should relate to part of the input word problem context or a previous step in the operation program. Learning logical forms has led to success in other areas of semantic parsing (Cheng et al., 2017; Zelle and Mooney, 1996; Zettlemoyer and Collins, 2007, 2005) and is a natural representation for math word problem-solving steps. By augmenting our dataset with these formalisms, we are able to cover most types of math word problems3 . In contrast to other representations like simultaneous equations, our formalisms ensure that every problem-solving step is aligned to a previous one. There are three advantages to this approach. First, we use this representation language to provide human annotators with clear steps for how a particular problem should be solved with math and domain knowledge. S"
N19-1245,W14-4012,0,0.0498136,"Missing"
N19-1245,N18-1060,0,0.0583799,"ature of the problems included in the AQuA dataset, while removing noise that hinders the ability of neuralized models to learn the types of signal neccessary for problem-solving by logical reasoning. Additional Datasets Several smaller datasets have been compiled in recent years. Most of these works have focused on algebra word problems, including MaWPS (Koncel-Kedziorski et al., 2016), Alg514 (Kushman et al., 2014), and DRAW-1K (Upadhyay and Chang, 2017). Many of these datasets have sought to align underlying equations or systems of equations with word problem text. While recent works like (Liang et al., 2018; Locascio et al., 2016) have explored representing math word problems with logical formalisms and regular expressions, our work is the first to provide well-defined formalisms for representing intermediate problem-solving steps that are shown to be generalizable beyond algebra problems. Solving with Handcrafted Features Due to sparsity of suitable data, early work on math word problem solving used pattern-matching to map word problems to mathematical expressions (Bobrow, 1964; Charniak, 1968, 1969), as well as non-neural statistical modeling and semantic parsing approaches (Liguda and Pfeiffe"
N19-1245,P17-1015,0,0.0617703,"ation program underlying the problem in Figure 1 highlights the complexity of the problem-solving task. Here, we need the ability to deduce implied constants (pi) and knowledge of domain-specific formulas (area of the square). In this paper, we introduce a new operationbased representation language for solving math word problems. We use this representation language to construct MathQA1 , a new large-scale, diverse dataset of 37k English multiple-choice math word problems covering multiple math domain categories by modeling operation programs corresponding to word problems in the AQuA dataset (Ling et al., 2017). We introduce a neural model for mapping problems to operation programs with domain categorization. 1 The dataset is available at: https://math-qa. github.io/math-QA/ 2357 Proceedings of NAACL-HLT 2019, pages 2357–2367 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Most current datasets in this domain are small in scale (Kushman et al., 2014) or do not offer precise operational annotations over diverse problem types (Ling et al., 2017). This is mainly due to the fact that annotating math word problems precisely across diverse problem categories"
N19-1245,D16-1197,0,0.0206349,"s included in the AQuA dataset, while removing noise that hinders the ability of neuralized models to learn the types of signal neccessary for problem-solving by logical reasoning. Additional Datasets Several smaller datasets have been compiled in recent years. Most of these works have focused on algebra word problems, including MaWPS (Koncel-Kedziorski et al., 2016), Alg514 (Kushman et al., 2014), and DRAW-1K (Upadhyay and Chang, 2017). Many of these datasets have sought to align underlying equations or systems of equations with word problem text. While recent works like (Liang et al., 2018; Locascio et al., 2016) have explored representing math word problems with logical formalisms and regular expressions, our work is the first to provide well-defined formalisms for representing intermediate problem-solving steps that are shown to be generalizable beyond algebra problems. Solving with Handcrafted Features Due to sparsity of suitable data, early work on math word problem solving used pattern-matching to map word problems to mathematical expressions (Bobrow, 1964; Charniak, 1968, 1969), as well as non-neural statistical modeling and semantic parsing approaches (Liguda and Pfeiffer, 2012). Some effort ha"
N19-1245,Q18-1012,0,0.0143044,"lient entities (Hosseini et al., 2017). This approach views entities as containers, which can be composed into an equation tree representation. The equation tree representation is changed over time by operations implied by the problem text. Many early works focused on solving addition and subtraction problems (Briars and Larkin, 1984; Dellarosa, 1986; Bakman, 2007). As word problems become more diverse and complex, we require models capable of solving simultaneous equation systems. This has led to an increasing focus on finding semantic alignment of math word problems and mentions of numbers (Roy and Roth, 2018). The main idea behind those work is to find all possible patterns of equations and rank 2358 them based on the problem. Context and Question Neural Word Problem Solvers Following the increasing availability of large-scale datasets like AQuA, several recent works have explored deep neural approaches to math word problem solving (Wang et al., 2017). Our representation language is motivated by exploration of using intermediate formalisms in the training of deep neural problem-solving networks, as is done in the work of (Huang et al., 2018b) to solve problems with sequence to sequence models. Whi"
N19-1245,C18-1018,0,0.0597624,"alignment of math word problems and mentions of numbers (Roy and Roth, 2018). The main idea behind those work is to find all possible patterns of equations and rank 2358 them based on the problem. Context and Question Neural Word Problem Solvers Following the increasing availability of large-scale datasets like AQuA, several recent works have explored deep neural approaches to math word problem solving (Wang et al., 2017). Our representation language is motivated by exploration of using intermediate formalisms in the training of deep neural problem-solving networks, as is done in the work of (Huang et al., 2018b) to solve problems with sequence to sequence models. While this work focused on single-variable arithmetic problems, our work introduces a formal language of operations for covering more complex multivariate problems and systems of equations. Interpretability of Solvers While the statistical models with handcrafted features introduced by prior work are arguably “interpretable” due to the relative sparsity of features as well as the clear alignments between inputs and outputs, new neuralized approaches present new challenges to model interpretability of math word problem solvers (Huang et al."
N19-1245,E17-1047,0,0.0179992,"ontain few or none of the steps required to solve the corresponding problem. The motivation for our dataset comes from the fact we want to maintain the challenging nature of the problems included in the AQuA dataset, while removing noise that hinders the ability of neuralized models to learn the types of signal neccessary for problem-solving by logical reasoning. Additional Datasets Several smaller datasets have been compiled in recent years. Most of these works have focused on algebra word problems, including MaWPS (Koncel-Kedziorski et al., 2016), Alg514 (Kushman et al., 2014), and DRAW-1K (Upadhyay and Chang, 2017). Many of these datasets have sought to align underlying equations or systems of equations with word problem text. While recent works like (Liang et al., 2018; Locascio et al., 2016) have explored representing math word problems with logical formalisms and regular expressions, our work is the first to provide well-defined formalisms for representing intermediate problem-solving steps that are shown to be generalizable beyond algebra problems. Solving with Handcrafted Features Due to sparsity of suitable data, early work on math word problem solving used pattern-matching to map word problems to"
N19-1245,P16-1084,0,0.0448286,"ems that are densely annotated with operation programs • We introduce a new representation language to model operation programs corresponding to each math problem that aim to improve both the performance and the interpretability of the learned models. • We introduce a neural architecture leveraging a sequence-to-program model with automatic problem categorization, achieving competitive results on our dataset as well as the AQuA dataset 2 Background and Related Work Large-Scale Datasets Several large-scale math word problem datasets have been released in recent years. These include Dolphin18K (Huang et al., 2016), Math23K (Wang et al., 2017) and AQuA. We choose the 2017 AQUA-RAT dataset to demonstrate use of our representation language on an existing large-scale math word problem solving dataset. The AQuA provides over 100K GRE- and GMAT-level math word problems. The problems are multiple choice and come from a wide range of domains. The scale and diversity of this dataset makes it particularly suited for use in training deeplearning models to solve word problems. However there is a significant amount of unwanted noise in the dataset, including problems with incorrect solutions, problems that are unso"
N19-1245,D17-1088,0,0.181866,"Missing"
N19-1245,P18-1039,0,0.014109,"alignment of math word problems and mentions of numbers (Roy and Roth, 2018). The main idea behind those work is to find all possible patterns of equations and rank 2358 them based on the problem. Context and Question Neural Word Problem Solvers Following the increasing availability of large-scale datasets like AQuA, several recent works have explored deep neural approaches to math word problem solving (Wang et al., 2017). Our representation language is motivated by exploration of using intermediate formalisms in the training of deep neural problem-solving networks, as is done in the work of (Huang et al., 2018b) to solve problems with sequence to sequence models. While this work focused on single-variable arithmetic problems, our work introduces a formal language of operations for covering more complex multivariate problems and systems of equations. Interpretability of Solvers While the statistical models with handcrafted features introduced by prior work are arguably “interpretable” due to the relative sparsity of features as well as the clear alignments between inputs and outputs, new neuralized approaches present new challenges to model interpretability of math word problem solvers (Huang et al."
N19-1245,D07-1071,0,0.0165843,"ere the arguments 174, 254 and 349 are the outputs of operations 1, 2 and 3 respectively. 2359 Category Geometry Physics Probability Gain-Loss General Other All domain knowledge associated with subfields like geometry and probability to determine which operations and arguments to use. • Human interpretability → Each operation and argument used to obtain the correct solution should relate to part of the input word problem context or a previous step in the operation program. Learning logical forms has led to success in other areas of semantic parsing (Cheng et al., 2017; Zelle and Mooney, 1996; Zettlemoyer and Collins, 2007, 2005) and is a natural representation for math word problem-solving steps. By augmenting our dataset with these formalisms, we are able to cover most types of math word problems3 . In contrast to other representations like simultaneous equations, our formalisms ensure that every problem-solving step is aligned to a previous one. There are three advantages to this approach. First, we use this representation language to provide human annotators with clear steps for how a particular problem should be solved with math and domain knowledge. Second, our formalisms provide neural models with a cont"
N19-1245,D14-1058,1,\N,Missing
N19-1412,P82-1020,0,0.759286,"Missing"
N19-1412,D15-1166,0,0.0104311,"from the concrete sentence if they occur in the abstract description. This restriction eliminates any benefits that might have been achieved via models with copy mechanisms. Examples that do not meet our criteria are removed from the corpus. 3 Models We investigate the utility of sequence-to-sequence models with attention (Bahdanau et al., 2015) to generate concrete realizations of abstract task descriptions. We hypothesize that models that learn explicit alignments are particularly amenable to interpretable analysis on the task. Therefore, in addition to using the global attention model of (Luong et al., 2015), we adapt the transducer model proposed by Yu et al. (2016), which uses learned latent discrete variables to model phraseto-phrase alignments. In contrast to many standard neural models, this approach enables us to incorporate prior knowledge about the alignment structure, and to extract interpretable alignments between task phrases. Closely related architectures have been proposed for segmental sequence modeling (Wang et al., 2017) and phrase-based neural machine translation (Huang et al., 2018). We train the transducer models using Viterbi EM (after doing marginal likelihood training for th"
N19-1412,D14-1162,0,0.0807237,"Missing"
N19-1412,N18-1202,0,0.030013,"69LBbBU0lEqseiF48V7Qe0oWy2k3bpZhN2N0IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz0gP1av1xxq+4cZJV4OalAjka//NUbxCyNUBomqNZdz02Mn1FlOBM4LfVSjQllYzrErqWSRqj9bH7qlJxZZUDCWNmShszV3xMZjbSeRIHtjKgZ6WVvJv7ndVMTXvsZl0lqULLFojAVxMRk9jcZcIXMiIkllClubyVsRBVlxqZTsiF4yy+vktZF1XOr3v1lpX6Tx1GEEziFc/DgCupwBw1oAoMhPMMrvDnCeXHenY9Fa8HJZ47hD5zPH/ZDjZI=</latexit> <S> take the tongs and pick up the bread . set the bread on a grill . put on the grill over the high flame . Figure 2: Example Viterbi alignments trained language models (Peters et al., 2018), we are interested if these approaches transfer to our cloze task. We evaluate the OpenAI GPT transformer language model (Radford et al., 2018) with and without fine-tuning.Without fine-tuning this model does slightly worse than our best domainspecific model. With fine-tuning, its accuracy is substantially higher, but it still suffers from the same fundamental limitations as our other models (see Table 5). The transformer (Vaswani et al., 2017) attention is multi-headed and multi-layered which prohibits direct interpretability. 5 Qualitative Analysis We visualize alignments of our transductio"
N19-1412,Q13-1003,0,0.143204,"ons, we collect concrete language capturing otherwise implicit world knowledge that a child would not know. Because annotators assume a smart and capable but uninformed listener, we posit this language corresponds closely to the most “concrete” form in which language naturally occurs. 2.1 Data Collection We construct a task on Amazon’s Mechanical Turk, where workers are asked to explain a video action caption to a child.2 Every instruction is paired with the original YouTube video and YouCook caption so the annotator could see how 1 Notable exceptions include the hierarchical instructions of (Regneri et al., 2013) and (Bisk et al., 2016). 2 Pay was calculated based on $15/hr and assuming workers took 1.5x as long to complete a task as the experimenters. Train Valid Test Seqs Tokens Vocab Avg Len YC KC 8,125 1,014 1,020 307,573 36,830 37,156 3,573 1,479 1,489 10.0 8.8 8.8 37.9 36.3 36.4 Table 1: K IDS C OOK corpus statistics the action was performed, rather than hallucinating additional details. All captions received three simplifications. The instructions ask users to focus on missing information and allow them up to five steps. Finally, we explicitly asked annotators to simplify complex actions (e.g."
N19-1412,S15-1024,0,0.0530934,"Missing"
N19-1412,D11-1117,0,0.0351887,"Missing"
N19-1412,W10-2902,0,0.0901829,"Missing"
N19-1412,D18-1413,0,0.0184032,"the stove. pasta and pour the pasta 2. Once the pot with pasta is cool into the sauce. 2. Turn the heat on under the pot enough, grab it by the handles. and wait for the water to boil hard. 3. Pour the pasta and water into the strainer 2. Stir the pasta into sauce while it is in the in the sink. pan. 3. Pour the pasta into the boiling water. Introduction ∗ t0 … Abelson, 1977; DeJong, 1981) but required handwritten data structures encoding world knowledge. However, the automatic induction of such commonsense knowledge from open-domain noisy text corpora remains an open problem (Chambers, 2013; Weber et al., 2018; Zellers et al., 2018). As a step towards solving this problem we consider textual descriptions of actions in a cooking domain. We introduce a dataset, K IDS C OOK, targeted at exploring the automatic acquisition of correspondences between abstract and concrete descriptions of actions. The dataset consists of higher-level single-sentence imperative descriptions paired with lower-level descriptions with elided details included. Descriptions come from real grounded actions, built on top of the YouCookII video caption dataset (Zhou et al., 2017). Figure 1 gives an example annotation from the dat"
N19-1412,D16-1138,1,0.852061,"iption. This restriction eliminates any benefits that might have been achieved via models with copy mechanisms. Examples that do not meet our criteria are removed from the corpus. 3 Models We investigate the utility of sequence-to-sequence models with attention (Bahdanau et al., 2015) to generate concrete realizations of abstract task descriptions. We hypothesize that models that learn explicit alignments are particularly amenable to interpretable analysis on the task. Therefore, in addition to using the global attention model of (Luong et al., 2015), we adapt the transducer model proposed by Yu et al. (2016), which uses learned latent discrete variables to model phraseto-phrase alignments. In contrast to many standard neural models, this approach enables us to incorporate prior knowledge about the alignment structure, and to extract interpretable alignments between task phrases. Closely related architectures have been proposed for segmental sequence modeling (Wang et al., 2017) and phrase-based neural machine translation (Huang et al., 2018). We train the transducer models using Viterbi EM (after doing marginal likelihood training for the initial iterations), as we found it gave higher predictive"
N19-1412,D18-1009,1,0.8397,"pour the pasta 2. Once the pot with pasta is cool into the sauce. 2. Turn the heat on under the pot enough, grab it by the handles. and wait for the water to boil hard. 3. Pour the pasta and water into the strainer 2. Stir the pasta into sauce while it is in the in the sink. pan. 3. Pour the pasta into the boiling water. Introduction ∗ t0 … Abelson, 1977; DeJong, 1981) but required handwritten data structures encoding world knowledge. However, the automatic induction of such commonsense knowledge from open-domain noisy text corpora remains an open problem (Chambers, 2013; Weber et al., 2018; Zellers et al., 2018). As a step towards solving this problem we consider textual descriptions of actions in a cooking domain. We introduce a dataset, K IDS C OOK, targeted at exploring the automatic acquisition of correspondences between abstract and concrete descriptions of actions. The dataset consists of higher-level single-sentence imperative descriptions paired with lower-level descriptions with elided details included. Descriptions come from real grounded actions, built on top of the YouCookII video caption dataset (Zhou et al., 2017). Figure 1 gives an example annotation from the dataset: the phrase “drain"
P10-2050,D08-1083,1,0.564318,"Missing"
P10-2050,W06-1673,0,0.0133806,"Missing"
P10-2050,C04-1200,0,0.0809007,"a sequence tagging task as follows. Given a sequence of tokens, x = x1 ... xn , we predict a sequence of labels, y = y1 ... yn , where yi ∈ {0, ..., 9} are defined as conjunctive values of polarity labels and intensity labels, as shown in Table 1. Then the conditional probability p(y|x) for linear-chain CRFs is given as (Lafferty et al., 2001) 1 Introduction Automatic opinion recognition involves a number of related tasks, such as identifying expressions of opinion (e.g. Kim and Hovy (2005), Popescu and Etzioni (2005), Breck et al. (2007)), determining their polarity (e.g. Hu and Liu (2004), Kim and Hovy (2004), Wilson et al. (2005)), and determining their strength, or intensity (e.g. Popescu and Etzioni (2005), Wilson et al. (2006)). Most previous work treats each subtask in isolation: opinion expression extraction (i.e. detecting the boundaries of opinion expressions) and opinion attribute classification (e.g. determining values for polarity and intensity) are tackled as separate steps in opinion recognition systems. Unfortunately, errors from individual components will propagate in P (y|x) =  X 1 λ f (yi , x, i)+λ′ f ′ (yi−1 , yi , x, i) exp Zx i where Zx is the normalization factor. In order t"
P10-2050,I05-2011,0,0.00936932,"2 Hierarchical Sequential Learning We define the problem of joint extraction of opinion expressions and their attributes as a sequence tagging task as follows. Given a sequence of tokens, x = x1 ... xn , we predict a sequence of labels, y = y1 ... yn , where yi ∈ {0, ..., 9} are defined as conjunctive values of polarity labels and intensity labels, as shown in Table 1. Then the conditional probability p(y|x) for linear-chain CRFs is given as (Lafferty et al., 2001) 1 Introduction Automatic opinion recognition involves a number of related tasks, such as identifying expressions of opinion (e.g. Kim and Hovy (2005), Popescu and Etzioni (2005), Breck et al. (2007)), determining their polarity (e.g. Hu and Liu (2004), Kim and Hovy (2004), Wilson et al. (2005)), and determining their strength, or intensity (e.g. Popescu and Etzioni (2005), Wilson et al. (2006)). Most previous work treats each subtask in isolation: opinion expression extraction (i.e. detecting the boundaries of opinion expressions) and opinion attribute classification (e.g. determining values for polarity and intensity) are tackled as separate steps in opinion recognition systems. Unfortunately, errors from individual components will propag"
P10-2050,H05-1043,0,0.9049,"tial Learning We define the problem of joint extraction of opinion expressions and their attributes as a sequence tagging task as follows. Given a sequence of tokens, x = x1 ... xn , we predict a sequence of labels, y = y1 ... yn , where yi ∈ {0, ..., 9} are defined as conjunctive values of polarity labels and intensity labels, as shown in Table 1. Then the conditional probability p(y|x) for linear-chain CRFs is given as (Lafferty et al., 2001) 1 Introduction Automatic opinion recognition involves a number of related tasks, such as identifying expressions of opinion (e.g. Kim and Hovy (2005), Popescu and Etzioni (2005), Breck et al. (2007)), determining their polarity (e.g. Hu and Liu (2004), Kim and Hovy (2004), Wilson et al. (2005)), and determining their strength, or intensity (e.g. Popescu and Etzioni (2005), Wilson et al. (2006)). Most previous work treats each subtask in isolation: opinion expression extraction (i.e. detecting the boundaries of opinion expressions) and opinion attribute classification (e.g. determining values for polarity and intensity) are tackled as separate steps in opinion recognition systems. Unfortunately, errors from individual components will propagate in P (y|x) =  X 1 λ f"
P10-2050,H05-1044,0,0.114114,"ask as follows. Given a sequence of tokens, x = x1 ... xn , we predict a sequence of labels, y = y1 ... yn , where yi ∈ {0, ..., 9} are defined as conjunctive values of polarity labels and intensity labels, as shown in Table 1. Then the conditional probability p(y|x) for linear-chain CRFs is given as (Lafferty et al., 2001) 1 Introduction Automatic opinion recognition involves a number of related tasks, such as identifying expressions of opinion (e.g. Kim and Hovy (2005), Popescu and Etzioni (2005), Breck et al. (2007)), determining their polarity (e.g. Hu and Liu (2004), Kim and Hovy (2004), Wilson et al. (2005)), and determining their strength, or intensity (e.g. Popescu and Etzioni (2005), Wilson et al. (2006)). Most previous work treats each subtask in isolation: opinion expression extraction (i.e. detecting the boundaries of opinion expressions) and opinion attribute classification (e.g. determining values for polarity and intensity) are tackled as separate steps in opinion recognition systems. Unfortunately, errors from individual components will propagate in P (y|x) =  X 1 λ f (yi , x, i)+λ′ f ′ (yi−1 , yi , x, i) exp Zx i where Zx is the normalization factor. In order to apply a hierarchical"
P10-2050,J09-3003,0,0.0483576,"Missing"
P10-2050,D08-1013,0,0.0134636,"u and Etzioni (2005), Wilson et al. (2006)). Most previous work treats each subtask in isolation: opinion expression extraction (i.e. detecting the boundaries of opinion expressions) and opinion attribute classification (e.g. determining values for polarity and intensity) are tackled as separate steps in opinion recognition systems. Unfortunately, errors from individual components will propagate in P (y|x) =  X 1 λ f (yi , x, i)+λ′ f ′ (yi−1 , yi , x, i) exp Zx i where Zx is the normalization factor. In order to apply a hierarchical parameter sharing technique (e.g., Cai and Hofmann (2004), Zhao et al. (2008)), we extend parameters as follows. 269 Proceedings of the ACL 2010 Conference Short Papers, pages 269–274, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics                                                                  Figure 1: The hierarchical structure of classes for opinion expressions with polarity (positive, neutral, negative) and intensity (high, medium, low) L ABEL P OLARITY I NTENSITY 0 1 2 3 4 5 6 7 8 9 none positive positive positive neutral"
P10-2050,H05-2017,0,\N,Missing
P10-2062,P07-1056,0,0.195605,"Missing"
P10-2062,P07-1055,0,0.348901,"Missing"
P10-2062,P04-1035,0,0.728953,"evious research has shown that enriching the sentiment labels with human annotators’ “rationales” can produce substantial improvements in categorization performance (Zaidan et al., 2007). We explore methods to automatically generate annotator rationales for document-level sentiment classification. Rather unexpectedly, we find the automatically generated rationales just as helpful as human rationales. 1 Introduction One of the central challenges in sentiment-based text categorization is that not every portion of a given document is equally informative for inferring its overall sentiment (e.g., Pang and Lee (2004)). Zaidan et al. (2007) address this problem by asking human annotators to mark (at least some of) the relevant text spans that support each document-level sentiment decision. The text spans of these “rationales” are then used to construct additional training examples that can guide the learning algorithm toward better categorization models. But could we perhaps enjoy the performance gains of rationale-enhanced learning models without any additional human effort whatsoever (beyond the document-level sentiment label)? We hypothesize that in the area of sentiment analysis, where there has been a"
P10-2062,W02-1011,0,0.0464621,"r this, we rely on the following two assumptions: (1) Regions marked as annotator rationales are more subjective than unmarked regions. (2) The sentiment of each annotator rationale coincides with the document-level sentiment. Note that assumption 1 was not observed in the Zaidan et al. (2007) work: annotators were asked only to mark a few rationales, leaving other (also subjective) rationale sections unmarked. And at first glance, assumption (2) might seem too obvious. But it is important to include as there can be subjective regions with seemingly conflicting sentiment in the same document (Pang et al., 2002). For instance, an author for a movie review might express a positive sentiment toward the movie, while also discussing a negative sentiment toward one of the fictional characters appearing in the movie. This implies that not all subjective regions will be relevant for the documentlevel sentiment classification — rather only those regions whose polarity matches that of the document should be considered. In order to extract regions that satisfy the above assumptions, we first look for subjective regions in each document, then filter out those regions that exhibit a sentiment value (i.e., polari"
P10-2062,W09-3909,0,0.0241034,"Missing"
P10-2062,H05-2018,1,0.942633,"tead, we opt for methods that make use of only the document-level sentiment and off-the-shelf utilities that were trained for slightly different sentiment classification tasks using a corpus from a different domain and of a different genre. Although such utilities might not be optimal for our task, we hoped that these basic resources from the research community would constitute an adequate source of sentiment information for our purposes. We next describe three methods for the automatic acquisition of rationales. 3.1 Contextual Polarity Classification The first approach employs OpinionFinder (Wilson et al., 2005a), an off-the-shelf opinion analysis utility.1 In particular, OpinionFinder identifies phrases expressing positive or negative opinions. Because OpinionFinder models the task as a word-based classification problem rather than a sequence tagging task, most of the identified opinion phrases consist of a single word. In general, such short text spans cannot fully incorporate the contextual information relevant to the detection of subjective language (Wilson et al., 2005a). Therefore, we conjecture that good rationales should extend beyond short phrases.2 For simplicity, we choose to extend Opini"
P10-2062,N07-1033,0,0.092619,"Missing"
P10-2062,H05-1044,0,\N,Missing
P10-2062,D08-1004,0,\N,Missing
P11-1032,W10-0731,0,0.00549796,"Missing"
P11-1032,N10-1096,0,0.0218597,"iews by constructing, for each review, features based on the frequencies of each POS tag.15 These features are also intended to provide a good baseline with which to compare our other automated approaches. 4.2 Psycholinguistic deception detection The Linguistic Inquiry and Word Count (LIWC) software (Pennebaker et al., 2007) is a popular automated text analysis tool used widely in the social sciences. It has been used to detect personality 15 We use the Stanford Parser (Klein and Manning, 2003) to obtain the relative POS frequencies. traits (Mairesse et al., 2007), to study tutoring dynamics (Cade et al., 2010), and, most relevantly, to analyze deception (Hancock et al., 2008; Mihalcea and Strapparava, 2009; Vrij et al., 2007). While LIWC does not include a text classifier, we can create one with features derived from the LIWC output. In particular, LIWC counts and groups the number of instances of nearly 4,500 keywords into 80 psychologically meaningful dimensions. We construct one feature for each of the 80 LIWC dimensions, which can be summarized broadly under the following four categories: 1. Linguistic processes: Functional aspects of text (e.g., the average number of words per sentence, the ra"
P11-1032,P96-1041,0,0.0602621,"ximum likelihood classifier (Peng and Schuurmans, 2003): yˆ = arg max Pr(~x |y = c) (2) c Under (2), both the NB classifier used by Mihalcea and Strapparava (2009) and the language model classifier used by Zhou et al. (2008) are equivalent. Thus, following Zhou et al. (2008), we use the SRI Language Modeling Toolkit (Stolcke, 2002) to estimate individual language models, Pr(~x |y = c), for truthful and deceptive opinions. We consider all three n-gram feature sets, namely UNIGRAMS, BIGRAMS + , and TRIGRAMS+ , with corresponding language models smoothed using the interpolated Kneser-Ney method (Chen and Goodman, 1996). We also train Support Vector Machine (SVM) classifiers, which find a high-dimensional separating hyperplane between two groups of data. To simplify feature analysis in Section 5, we restrict our evaluation to linear SVMs, which learn a weight vector w ~ and bias term b, such that a document ~x can be classified by: Text categorization In contrast to the other strategies just discussed, our text categorization approach to deception detection allows us to model both content and context with n-gram features. Specifically, we consider the following three n-gram feature sets, with the correspondi"
P11-1032,W06-1650,0,0.116071,"Missing"
P11-1032,P03-1054,0,0.0211199,"tification approach to deceptive opinion spam detection, we test if such a relationship exists for truthful and deceptive reviews by constructing, for each review, features based on the frequencies of each POS tag.15 These features are also intended to provide a good baseline with which to compare our other automated approaches. 4.2 Psycholinguistic deception detection The Linguistic Inquiry and Word Count (LIWC) software (Pennebaker et al., 2007) is a popular automated text analysis tool used widely in the social sciences. It has been used to detect personality 15 We use the Stanford Parser (Klein and Manning, 2003) to obtain the relative POS frequencies. traits (Mairesse et al., 2007), to study tutoring dynamics (Cade et al., 2010), and, most relevantly, to analyze deception (Hancock et al., 2008; Mihalcea and Strapparava, 2009; Vrij et al., 2007). While LIWC does not include a text classifier, we can create one with features derived from the LIWC output. In particular, LIWC counts and groups the number of instances of nearly 4,500 keywords into 80 psychologically meaningful dimensions. We construct one feature for each of the 80 LIWC dimensions, which can be summarized broadly under the following four"
P11-1032,P09-2078,0,0.703668,"009) suggests that a log-normal distribution is appropriate for modeling document lengths. Thus, for each of the 20 chosen hotels, we select 20 truthful reviews from a log-normal (lefttruncated at 150 characters) distribution fit to the lengths of the deceptive reviews.14 Combined with the 400 deceptive reviews gathered in Section 3.1 this yields our final dataset of 800 reviews. 3.3 Human performance Assessing human deception detection performance is important for several reasons. First, there are few other baselines for our classification task; indeed, related studies (Jindal and Liu, 2008; Mihalcea and Strapparava, 2009) have only considered a random guess baseline. Second, assessing human performance is necessary to validate the deceptive opinions gathered in Section 3.1. If human performance is low, then our deceptive opinions are convincing, and therefore, deserving of further attention. Our initial approach to assessing human performance on this task was with Mechanical Turk. Unfortunately, we found that some Turkers selected among the choices seemingly at random, presumably to maximize their hourly earnings by obviating the need to read the review. While a similar effect has been observed previously (Akk"
P11-1032,P08-1105,0,0.00648691,"Missing"
P11-1032,P07-2032,0,0.0078724,"Missing"
P11-2015,P03-1054,0,0.00980662,"rell, 2010). For our purposes, we hypothesize that different stylistic features appear in regular and vandalizing edits. For regular edits, honest editors will strive to follow the stylistic guidelines set forth by Wikipedia (e.g. objectivity, neutrality and factuality). For edits that vandalize articles, these users may converge on common ways of vandalizing articles. 2.1 Language Models (3) For each test document, compare the probability of the edit determined by Cvandal and Cregular , where the parser with the higher score determines the class of the edit. We use the PCFG implementation of Klein and Manning (2003). 3 To differentiate between the styles of normal users and vandalizers, we employ language models to capture the stylistic differences between authentic and vandalizing revisions. We train two trigram language model (LM) with Good-Turing discounting and Katz backoff for smoothing of vandalizing edits (based on the text difference between the vandalizing and previous revision) and good edits (based on the text difference between the new and previous revision). 2.2 using only those tree-banked documents in D that correspond to regular Wikipedia edits. Probabilistic Context Free Grammar (PCFG) M"
P11-2015,P04-1035,0,0.00371642,"etween the old and new revision, the number of repeated patterns, slang words, vulgarities and pronouns, the type of edit (insert, modification or delete) and other similar features. Features Baseline +LM +PCFG +LM+PCFG P 72.8 73.3 73.5 73.2 R 41.1 42.1 47.7 47.3 F1 52.6 53.5 57.9 57.5 AUC 91.6 91.7 92.9 93.0 Feature Table 1: Results on naturally unbalanced test data 3.3 Features Based on Sentiment Wikipedia editors strive to maintain a neutral and objective voice in articles. Vandals, however, insert subjective and polar statements into articles. We build two classifiers based on the work of Pang and Lee (2004) to measure the polarity and objectivity of article edits. We train the classifier on how many positive and negative sentences were inserted as well as the overall change in the sentiment score from the previous version to the new revision and the number of inserted or deleted subjective sentences in the revision. 3.4 Features Based on Stylometric Measures We encode the output of the LM and PCFG in the following manner for training our classifier. We take the log-likelihood of the regular edit and vandalizing edit LMs. For our PCFG, we take the difference between the minimum log-likelihood sco"
P11-2015,panicheva-etal-2010-personal,0,0.120562,"deep syntactic patterns based on probabilistic context free grammar (PCFG) discriminate vandalism more effectively than shallow lexico-syntactic patterns based on n-grams. 2 Stylometric Features Stylometric features attempt to recognize patterns of style in text. These techniques have been traditionally applied to attribute authorship (Argamon et al. (2009), Stamatatos (2009)), opinion mining 83 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 83–88, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics (Panicheva et al., 2010), and forensic linguistics (Turell, 2010). For our purposes, we hypothesize that different stylistic features appear in regular and vandalizing edits. For regular edits, honest editors will strive to follow the stylistic guidelines set forth by Wikipedia (e.g. objectivity, neutrality and factuality). For edits that vandalize articles, these users may converge on common ways of vandalizing articles. 2.1 Language Models (3) For each test document, compare the probability of the edit determined by Cvandal and Cregular , where the parser with the higher score determines the class of the edit. We u"
P11-2015,P10-2008,0,0.0527203,"dit to an article is vandalism by training a classifier based on a set of features derived from many different aspects of the edit. For this task, we use an annotated corpus (Potthast et al., 2010) of Wikipedia edits where revisions are labeled as either vandalizing or non-vandalizing. This section will describe in brief the features used by our classifier, a more exhaustive description of our non-linguistically motivated features can be found in Harpalani et al. (2010). 3.1 Probabilistic context-free grammars (PCFG) capture deep syntactic regularities beyond shallow lexicosyntactic patterns. Raghavan et al. (2010) reported for the first time that PCFG models are effective in learning stylometric signature of authorship at deep syntactic levels. In this work, we explore the use of PCFG models for vandalism detection, by viewing the task as a genre detection problem, where a group of authors share similar linguistic behavior. We give a concise description of the use of PCFG models below, referring to Raghavan et al. (2010) for more details. (1) Given a training corpus D for vandalism detection and a generic PCFG parser Co trained on a manually tree-banked corpus such as WSJ or Brown, tree-bank each train"
P11-2015,C10-1129,0,0.0856599,"Missing"
P12-1038,P10-1127,0,0.0784245,"ition for content planning, surface realization and discourse structure. Evaluation by human annotators indicates that our final system generates more semantically correct and linguistically appealing descriptions than two nontrivial baselines. 1 Introduction Automatically describing images in natural language is an intriguing, but complex AI task, requiring accurate computational visual recognition, comprehensive world knowledge, and natural language generation. Some past research has simplified the general image description goal by assuming that relevant text for an image is provided (e.g., Aker and Gaizauskas (2010), Feng and Lapata (2010)). This allows descriptions to be generated using effective summarization techniques with relatively surface level image understanding. However, such text (e.g., news articles In contrast, other recent work has focused more on the visual recognition aspect by detecting content elements (e.g., scenes, objects, attributes, actions, etc) and then composing descriptions from scratch (e.g., Yao et al. (2010), Kulkarni et al. (2011), Yang et al. (2011), Li et al. (2011)), or by retrieving existing whole descriptions from visually similar images (e.g., Farhadi et al. (2010), O"
P12-1038,E06-1040,0,0.00889449,"ry strong baseline, as it exploits the vast amount of image-caption data, and produces a description high in linguistic quality (since the captions were written by human annotators). Automatic Evaluation: Automatically quantifying the quality of machine generated sentences is known to be difficult. BLEU score (Papineni et al., 2002), despite its simplicity and limitations, has been one of the common choices for automatic evaluation of image descriptions (Farhadi et al., 2010; Kulkarni et al., 2011; Li et al., 2011; Ordonez et al., 2011), as it correlates reasonably well with human evaluation (Belz and Reiter, 2006). Table 1 shows the the BLEU @1 against the original caption of 1000 images. We see that the ILP improves the score over HMM consistently, with or without the use of cognitive phrases. 6 Including other long-distance scores in HMM decoding would make the problem NP-hard and require more sophisticated decoding, e.g. ILP. 365 Grammar HMM 3.40(σ=.82) ILP 3.56(σ=.90) Hum. 4.36(σ=.79) Cognitive 3.40(σ=.88) 3.60(σ=.98) 4.77(σ=.66) Relevance 2.25(σ=1.37) 2.37(σ=1.49) 3.86(σ=1.60) Table 4: Human Evaluation: Multi-Aspect Rating (σ is a standard deviation) Human Evaluation I – Ranking: We complement the"
P12-1038,P06-2019,0,0.00504736,"statistics drawn from a large image-text parallel corpus. This contrasts with previous approaches that generate multiple sentences without considering discourse flow or redundancy (e.g., Li et al. (2011)). For example, for an image showing a flock of birds, generating a large number of sentences stating the relative position of each bird is probably not useful. Content planning and phrase synthesis can be naturally viewed as constraint optimization problems. We employ Integer Linear Programming (ILP) as an optimization framework that has been used successfully in other generation tasks (e.g., Clarke and Lapata (2006), Martins and Smith (2009), Woodsend and Lapata (2010)). Our ILP formulation encodes a rich set of linguistically motivated constraints and weights that incorporate multiple aspects of the generation process. Empirical results demonstrate that our final system generates linguistically more appealing and semantically more cor360 rect descriptions than two nontrivial baselines. 1.1 System Overview Our system consists of two parts. For a query image, we first retrieve candidate descriptive phrases from a large image-caption database using measures of visual similarity (§2). We then generate a coh"
P12-1038,P10-1126,0,0.0167404,"surface realization and discourse structure. Evaluation by human annotators indicates that our final system generates more semantically correct and linguistically appealing descriptions than two nontrivial baselines. 1 Introduction Automatically describing images in natural language is an intriguing, but complex AI task, requiring accurate computational visual recognition, comprehensive world knowledge, and natural language generation. Some past research has simplified the general image description goal by assuming that relevant text for an image is provided (e.g., Aker and Gaizauskas (2010), Feng and Lapata (2010)). This allows descriptions to be generated using effective summarization techniques with relatively surface level image understanding. However, such text (e.g., news articles In contrast, other recent work has focused more on the visual recognition aspect by detecting content elements (e.g., scenes, objects, attributes, actions, etc) and then composing descriptions from scratch (e.g., Yao et al. (2010), Kulkarni et al. (2011), Yang et al. (2011), Li et al. (2011)), or by retrieving existing whole descriptions from visually similar images (e.g., Farhadi et al. (2010), Ordonez et al. (2011)). F"
P12-1038,W11-0326,1,0.885738,"the general image description goal by assuming that relevant text for an image is provided (e.g., Aker and Gaizauskas (2010), Feng and Lapata (2010)). This allows descriptions to be generated using effective summarization techniques with relatively surface level image understanding. However, such text (e.g., news articles In contrast, other recent work has focused more on the visual recognition aspect by detecting content elements (e.g., scenes, objects, attributes, actions, etc) and then composing descriptions from scratch (e.g., Yao et al. (2010), Kulkarni et al. (2011), Yang et al. (2011), Li et al. (2011)), or by retrieving existing whole descriptions from visually similar images (e.g., Farhadi et al. (2010), Ordonez et al. (2011)). For the latter approaches, it is unrealistic to expect that there will always exist a single complete description for retrieval that is pertinent to a given query image. For the former approaches, visual recognition first generates an intermediate representation of image content using a set of English words, then language generation constructs a full description by adding function words and optionally applying simple re-ordering. Because the generation process stic"
P12-1038,W09-1801,0,0.17074,"rge image-text parallel corpus. This contrasts with previous approaches that generate multiple sentences without considering discourse flow or redundancy (e.g., Li et al. (2011)). For example, for an image showing a flock of birds, generating a large number of sentences stating the relative position of each bird is probably not useful. Content planning and phrase synthesis can be naturally viewed as constraint optimization problems. We employ Integer Linear Programming (ILP) as an optimization framework that has been used successfully in other generation tasks (e.g., Clarke and Lapata (2006), Martins and Smith (2009), Woodsend and Lapata (2010)). Our ILP formulation encodes a rich set of linguistically motivated constraints and weights that incorporate multiple aspects of the generation process. Empirical results demonstrate that our final system generates linguistically more appealing and semantically more cor360 rect descriptions than two nontrivial baselines. 1.1 System Overview Our system consists of two parts. For a query image, we first retrieve candidate descriptive phrases from a large image-caption database using measures of visual similarity (§2). We then generate a coherent description from the"
P12-1038,P02-1040,0,0.0895729,"ation, producing a strong baseline6 . The second baseline is a recent Retrieval based description method (Ordonez et al., 2011), that searches the large parallel corpus of images and captions, and transfers a caption from a visually similar database image to the query. This again is a very strong baseline, as it exploits the vast amount of image-caption data, and produces a description high in linguistic quality (since the captions were written by human annotators). Automatic Evaluation: Automatically quantifying the quality of machine generated sentences is known to be difficult. BLEU score (Papineni et al., 2002), despite its simplicity and limitations, has been one of the common choices for automatic evaluation of image descriptions (Farhadi et al., 2010; Kulkarni et al., 2011; Li et al., 2011; Ordonez et al., 2011), as it correlates reasonably well with human evaluation (Belz and Reiter, 2006). Table 1 shows the the BLEU @1 against the original caption of 1000 images. We see that the ILP improves the score over HMM consistently, with or without the use of cognitive phrases. 6 Including other long-distance scores in HMM decoding would make the problem NP-hard and require more sophisticated decoding,"
P12-1038,N07-1051,0,0.0164927,"photographs with associated human-composed descriptions). Visual similarity for several kinds of image content are used to compare the query image to images from the database, including: 1) object detections for 89 common object categories (Felzenszwalb et al., 2010), 2) scene classifications for 26 common scene categories (Xiao et al., 2010), and 3) region based detections for stuff categories (e.g. grass, road, sky) (Ordonez et al., 2011). All content types are pre-computed on the million database photos, and caption parsing is performed using the Berkeley PCFG parser (Petrov et al., 2006; Petrov and Klein, 2007). Given a query image, we identify content elements present using the above classifiers and detectors and then retrieve phrases referring to those content elements from the database. For example, if we detect a horse in a query image, then we retrieve phrases referring to visually similar horses in the database by comparing the color, texture (Leung and Malik, 1999), or shape (Dalal and Triggs, 2005; Lowe, 2004) of the detected horse to detected horses in the database images. We collect four types of phrases for each query image as follows: [1] NPs We retrieve noun phrases for each query objec"
P12-1038,P06-1055,0,0.0102695,"al., 2011) (1 million photographs with associated human-composed descriptions). Visual similarity for several kinds of image content are used to compare the query image to images from the database, including: 1) object detections for 89 common object categories (Felzenszwalb et al., 2010), 2) scene classifications for 26 common scene categories (Xiao et al., 2010), and 3) region based detections for stuff categories (e.g. grass, road, sky) (Ordonez et al., 2011). All content types are pre-computed on the million database photos, and caption parsing is performed using the Berkeley PCFG parser (Petrov et al., 2006; Petrov and Klein, 2007). Given a query image, we identify content elements present using the above classifiers and detectors and then retrieve phrases referring to those content elements from the database. For example, if we detect a horse in a query image, then we retrieve phrases referring to visually similar horses in the database by comparing the color, texture (Leung and Malik, 1999), or shape (Dalal and Triggs, 2005; Lowe, 2004) of the detected horse to detected horses in the database images. We collect four types of phrases for each query image as follows: [1] NPs We retrieve noun phr"
P12-1038,P10-1058,0,0.105569,"rpus. This contrasts with previous approaches that generate multiple sentences without considering discourse flow or redundancy (e.g., Li et al. (2011)). For example, for an image showing a flock of birds, generating a large number of sentences stating the relative position of each bird is probably not useful. Content planning and phrase synthesis can be naturally viewed as constraint optimization problems. We employ Integer Linear Programming (ILP) as an optimization framework that has been used successfully in other generation tasks (e.g., Clarke and Lapata (2006), Martins and Smith (2009), Woodsend and Lapata (2010)). Our ILP formulation encodes a rich set of linguistically motivated constraints and weights that incorporate multiple aspects of the generation process. Empirical results demonstrate that our final system generates linguistically more appealing and semantically more cor360 rect descriptions than two nontrivial baselines. 1.1 System Overview Our system consists of two parts. For a query image, we first retrieve candidate descriptive phrases from a large image-caption database using measures of visual similarity (§2). We then generate a coherent description from these candidates using ILP form"
P12-1038,D10-1050,0,0.0053339,"ration, as human written descriptions are not necessarily succinct.9 Second, unlike summarization, we are not given with a set of coherent text snippet to begin with, and the level of noise coming from the visual recognition errors is much higher than that of starting with clean text. As a result, choosing an additional phrase in the image description is much riskier than it is in summarization. Some recent research proposed very elegant approaches to summarization using ILP for collective content planning and/or surface realization (e.g., Martins and Smith (2009), Woodsend and Lapata (2010), Woodsend et al. (2010)). Perhaps the most important difference in our approach is the use of negative weights in the objective function to create the necessary tension between selection (salience) and compatibility, which makes it possible for ILP to generate variable length descriptions, effectively correcting some of the erroneous vision detections. In contrast, all previous work operates with a predefined upper limit in length, hence the ILP was formulated to include as many textual units as possible modulo constraints. To conclude, we have presented a collective approach to generating natural image descriptions"
P12-1038,D11-1041,0,0.591864,"arch has simplified the general image description goal by assuming that relevant text for an image is provided (e.g., Aker and Gaizauskas (2010), Feng and Lapata (2010)). This allows descriptions to be generated using effective summarization techniques with relatively surface level image understanding. However, such text (e.g., news articles In contrast, other recent work has focused more on the visual recognition aspect by detecting content elements (e.g., scenes, objects, attributes, actions, etc) and then composing descriptions from scratch (e.g., Yao et al. (2010), Kulkarni et al. (2011), Yang et al. (2011), Li et al. (2011)), or by retrieving existing whole descriptions from visually similar images (e.g., Farhadi et al. (2010), Ordonez et al. (2011)). For the latter approaches, it is unrealistic to expect that there will always exist a single complete description for retrieval that is pertinent to a given query image. For the former approaches, visual recognition first generates an intermediate representation of image content using a set of English words, then language generation constructs a full description by adding function words and optionally applying simple re-ordering. Because the gener"
P12-2034,N09-1057,0,0.00939878,"Missing"
P12-2034,W10-3011,0,0.0392543,"Missing"
P12-2034,P09-2078,0,0.372751,"visor–Heuristic Deceptive Truthful SˆROOT → VP . NPˆNP → $ CD PRNˆNP → LRB NP RRB NPˆNP → NNS NPˆS → NN NPˆPP → DT NNP NPˆPP → CD NNS NPˆNP → NP PRN PRNˆNP → LRB PP RRB NPˆNP → CD NNS NPˆS → PRP SBARˆS → WHADVP S VPˆS → VBD PP SˆSBAR → NP VP SˆROOT → PP NP VP . VPˆS → VBD S NPˆS → NP CC NP NPˆS → PRP$ NN NPˆPP → DT NNP NPˆPP → PRP$ NN VPˆS → VBZ NP NPˆNP → NNS WHNPˆSBAR → WDT NPˆNP → NP PP PP NPˆS → EX NXˆNX → JJ NN NPˆNP → NP PP VPˆS → VBZ RB NP PPˆNP → IN NP PPˆADJP → TO NP Table 1: Most discriminative rewrite rules (ˆ r): hotel review datasets degree of sentiment. IV. Essays: Introduced in Mihalcea and Strapparava (2009), this corpus contains truthful and deceptive essays collected using Amazon Mechanic Turk for the following three topics: “Abortion” (100 essays per class), “Best Friend” (98 essays per class), and “Death Penalty” (98 essays per class). 3 Feature Encoding Words Previous work has shown that bag-ofwords are effective in detecting domain-specific deception (Ott et al., 2011; Mihalcea and Strapparava, 2009). We consider unigram, bigram, and the union of the two as features. Figure 1: Parsed trees II. TripAdvisor—Heuristic: This dataset contains 400 truthful and 400 deceptive reviews harvested from"
P12-2034,P11-1032,1,0.563456,"We expect that filtered reviews roughly correspond to deceptive reviews, and displayed reviews to truthful ones, but not without considerable noise. We only collect 5-star reviews to avoid unwanted noise from varying 1 Specifically, using the notation of Feng et al. (2012), we use data created by Strategy-distΦ heuristic, with HS , S as deceptive and HS0 , T as truthful. 172 Shallow Syntax As has been used in many previous studies in stylometry (e.g., ArgamonEngelson et al. (1998), Zhao and Zobel (2007)), we utilize part-of-speech (POS) tags to encode shallow syntactic information. Note that Ott et al. (2011) found that even though POS tags are effective in detecting fake product reviews, they are not as effective as words. Therefore, we strengthen POS features with unigram features. Deep syntax We experiment with four different encodings of production rules based on the Probabilistic Context Free Grammar (PCFG) parse trees as follows: • r: unlexicalized production rules (i.e., all production rules except for those with terminal nodes), e.g., NP2 → NP3 SBAR. • r∗: lexicalized production rules (i.e., all production rules), e.g., PRP → “you”. • rˆ: unlexicalized production rules combined with the gr"
P12-2034,N07-1051,0,0.041618,"Missing"
P13-1174,baccianella-etal-2010-sentiwordnet,0,0.246302,"Missing"
P13-1174,D09-1062,1,0.857603,"op et al. (2011), Montejo-R´aez et al. (2012)) Label/Graph propagation (e.g., Zhu and Ghahra(2011) but with practical limitations. See §3 for detailed discussion. 3 Note that when “cure” is used as the “preserve” sense, it expects objects with non-negative connotation. Hence wordsense-disambiguation (WSD) presents a challenge, though not unexpectedly. In this work, we assume the general connotation of each word over statistically prevailing senses, leaving a more cautious handling of WSD as future work. mani (2002), Velikovich et al. (2010)) Constraint optimization (e.g., Roth and Yih (2004), Choi and Cardie (2009), Lu et al. (2011)). We provide comparative empirical results over several variants of these approaches with comprehensive evaluations including lexicon-based, human judgments, and extrinsic evaluations. It is worthwhile to note that not all words have connotative meanings that are distinct from denotational meanings, and in some cases, it can be difficult to determine whether the overall sentiment is drawn from denotational or connotative meanings exclusively, or both. Therefore, we encompass any sentiment from either type of meanings into the lexicon, where non-neutral polarity prevails over"
P13-1174,J90-1003,0,0.274173,"egative connotative predicate “cure” is likely to have negative connotation, e.g., “disease” or “cancer”. The bipartite graph structure for this approach corresponds to the left-most box (labeled as “pred-arg”) in Figure 1. 2.2 synonyms antonyms Figure 2: Graph for ILP/LP (§2.3, §4.2). Connotation Induction Algorithms 2.1 flu For connotative predicates, we use the seed predicate set of Feng et al. (2011), which comprises of 20 positive and 20 negative predicates. P (p, a) P (p)P (a) PMI scores have been widely used in previous studies to measure association between words (e.g., Turney (2001), Church and Hanks (1990)). Sub-graph #2: Argument–Argument Graph The second sub-graph is based on the distributional similarities among the arguments. One possible way of constructing such a graph is simply connecting all nodes and assign edge weights proportionate to the word association scores, such as PMI, or distributional similarity. However, such a completely connected graph can be susceptible to propagating noise, and does not scale well over a very large set of vocabulary. We therefore reduce the graph connectivity by exploiting semantic parallelism of coordination (Bock (1986), Hatzivassiloglou and McKeown 7"
P13-1174,W10-2914,0,0.055554,"a considerable amount of work has focused on recognizing sentiment that is generally explicit and pronounced rather than implied and subdued. However in many real-world texts, even seemingly objective statements can be opinion-laden in that they often allude nuanced sentiment of the writer (Greene and Resnik, 2009), or purposefully conjure emotion from the readers’ minds (Mohammad and Turney, 2010). Although some researchers have explored formal and statistical treatments of those implicit and implied sentiments (e.g. Wiebe et al. (2005), Esuli and Sebastiani (2006), Greene and Resnik (2009), Davidov et al. (2010)), automatic analysis of them largely remains as a big challenge. Although this sentence could be considered as a factual statement from the general standpoint, the subtle effect of this sentence may not be entirely objective: this sentence is likely to have an influence on readers’ minds in regard to their opinion toward “geothermal”. In order to sense the subtle overtone of sentiments, one needs to know that the word “emissions” has generally negative connotation, which geothermal reduces. In fact, depending on the pragmatic contexts, it could be precisely the intention of the author to tran"
P13-1174,esuli-sebastiani-2006-sentiwordnet,0,0.00829723,"lysis over the last decade (Pang and Lee, 2008), where a considerable amount of work has focused on recognizing sentiment that is generally explicit and pronounced rather than implied and subdued. However in many real-world texts, even seemingly objective statements can be opinion-laden in that they often allude nuanced sentiment of the writer (Greene and Resnik, 2009), or purposefully conjure emotion from the readers’ minds (Mohammad and Turney, 2010). Although some researchers have explored formal and statistical treatments of those implicit and implied sentiments (e.g. Wiebe et al. (2005), Esuli and Sebastiani (2006), Greene and Resnik (2009), Davidov et al. (2010)), automatic analysis of them largely remains as a big challenge. Although this sentence could be considered as a factual statement from the general standpoint, the subtle effect of this sentence may not be entirely objective: this sentence is likely to have an influence on readers’ minds in regard to their opinion toward “geothermal”. In order to sense the subtle overtone of sentiments, one needs to know that the word “emissions” has generally negative connotation, which geothermal reduces. In fact, depending on the pragmatic contexts, it could"
P13-1174,D11-1101,1,0.788189,"ributional similarity, [2] semantic parallelism of coordination, [3] selectional preference, and [4] semantic prosody (e.g., Sinclair (1991), Louw (1993), Stubbs (1995), Stefanowitsch and Gries (2003))), and also exploit existing lexical resources as an additional inductive bias. We cast the connotation lexicon induction task as a collective inference problem, and consider approaches based on three distinct types of algorithmic framework that have been shown successful for conventional sentiment lexicon induction: Random walk based on HITS/PageRank (e.g., Kleinberg (1999), Page et al. (1999), Feng et al. (2011) Heerschop et al. (2011), Montejo-R´aez et al. (2012)) Label/Graph propagation (e.g., Zhu and Ghahra(2011) but with practical limitations. See §3 for detailed discussion. 3 Note that when “cure” is used as the “preserve” sense, it expects objects with non-negative connotation. Hence wordsense-disambiguation (WSD) presents a challenge, though not unexpectedly. In this work, we assume the general connotation of each word over statistically prevailing senses, leaving a more cautious handling of WSD as future work. mani (2002), Velikovich et al. (2010)) Constraint optimization (e.g., Roth and Yih"
P13-1174,N09-1057,0,0.180332,", distributional similarity, semantic parallelism of coordination) and prior knowledge drawn from lexical resources, resulting in the first broad-coverage connotation lexicon. 1 Introduction There has been a substantial body of research in sentiment analysis over the last decade (Pang and Lee, 2008), where a considerable amount of work has focused on recognizing sentiment that is generally explicit and pronounced rather than implied and subdued. However in many real-world texts, even seemingly objective statements can be opinion-laden in that they often allude nuanced sentiment of the writer (Greene and Resnik, 2009), or purposefully conjure emotion from the readers’ minds (Mohammad and Turney, 2010). Although some researchers have explored formal and statistical treatments of those implicit and implied sentiments (e.g. Wiebe et al. (2005), Esuli and Sebastiani (2006), Greene and Resnik (2009), Davidov et al. (2010)), automatic analysis of them largely remains as a big challenge. Although this sentence could be considered as a factual statement from the general standpoint, the subtle effect of this sentence may not be entirely objective: this sentence is likely to have an influence on readers’ minds in re"
P13-1174,P97-1023,0,0.136326,"Missing"
P13-1174,D07-1115,0,0.0397572,"s generally negative connotation, which geothermal reduces. In fact, depending on the pragmatic contexts, it could be precisely the intention of the author to transfer his opinion into the readers’ minds. The main contribution of this paper is a broadcoverage connotation lexicon that determines the connotative polarity of even those words with ever so subtle connotation beneath their surface meaning, such as “Literature”, “Mediterranean”, and “wine”. Although there has been a number of previous work that constructed sentiment lexicons (e.g., Esuli and Sebastiani (2006), Wilson et al. (2005a), Kaji and Kitsuregawa (2007), Qiu et al. (2009)), which seem to be increasingly and inevitably expanding over words with (strongly) connotative sentiments rather than explicit sentiments alone (e.g., “gun”), little prior work has directly tackled this problem of learning connotation,2 and much of the subtle connotation of many seemingly objective words is yet to be determined. 1 Our learned lexicon correctly assigns negative polarity to emission. 2 A notable exception would be the work of Feng et al. 1774 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1774–1784, c Sofia, Bu"
P13-1174,W10-0204,0,0.0373188,"dge drawn from lexical resources, resulting in the first broad-coverage connotation lexicon. 1 Introduction There has been a substantial body of research in sentiment analysis over the last decade (Pang and Lee, 2008), where a considerable amount of work has focused on recognizing sentiment that is generally explicit and pronounced rather than implied and subdued. However in many real-world texts, even seemingly objective statements can be opinion-laden in that they often allude nuanced sentiment of the writer (Greene and Resnik, 2009), or purposefully conjure emotion from the readers’ minds (Mohammad and Turney, 2010). Although some researchers have explored formal and statistical treatments of those implicit and implied sentiments (e.g. Wiebe et al. (2005), Esuli and Sebastiani (2006), Greene and Resnik (2009), Davidov et al. (2010)), automatic analysis of them largely remains as a big challenge. Although this sentence could be considered as a factual statement from the general standpoint, the subtle effect of this sentence may not be entirely objective: this sentence is likely to have an influence on readers’ minds in regard to their opinion toward “geothermal”. In order to sense the subtle overtone of s"
P13-1174,W12-3703,0,0.0227489,"Missing"
P13-1174,W04-2401,0,0.0469628,"et al. (2011) Heerschop et al. (2011), Montejo-R´aez et al. (2012)) Label/Graph propagation (e.g., Zhu and Ghahra(2011) but with practical limitations. See §3 for detailed discussion. 3 Note that when “cure” is used as the “preserve” sense, it expects objects with non-negative connotation. Hence wordsense-disambiguation (WSD) presents a challenge, though not unexpectedly. In this work, we assume the general connotation of each word over statistically prevailing senses, leaving a more cautious handling of WSD as future work. mani (2002), Velikovich et al. (2010)) Constraint optimization (e.g., Roth and Yih (2004), Choi and Cardie (2009), Lu et al. (2011)). We provide comparative empirical results over several variants of these approaches with comprehensive evaluations including lexicon-based, human judgments, and extrinsic evaluations. It is worthwhile to note that not all words have connotative meanings that are distinct from denotational meanings, and in some cases, it can be difficult to determine whether the overall sentiment is drawn from denotational or connotative meanings exclusively, or both. Therefore, we encompass any sentiment from either type of meanings into the lexicon, where non-neutra"
P13-1174,W00-1308,0,0.0129842,"esult suggests: 1 The sub-graph #2, based on the semantic parallelism of coordination, is simple and yet very powerful as an inductive bias. 2. C syn : Synonym pairs will not have the opposite polarity: syn 3 ∀(i, j) ∈ R 2 The performance of graph propagation varies significantly depending on the graph topology and the corresponding edge weights. , xi + yj ≤ 1, xj + yi ≤ 1 Experimental Result I We provide comprehensive comparisons over variants of three types of algorithms proposed in §2. We use the Google Web 1T data (Brants and Franz (2006)), and POS-tagged ngrams using Stanford POS Tagger (Toutanova and Manning (2000)). We filter out the ngrams with punctuations and other special characters to reduce the noise. Comparison against Conventional Sentiment Lexicon Note that a direct comparison against ILP for top N words is tricky, as ILP does not rank results. Only for comparison purposes however, we assign 10 We consider “positive” and “negative” polarities conflict, but “neutral” polarity does not conflict with any. 11 In the case of General Inquirer, we use words in P OSITIV and N EGATIV sets as words with positive and negative labels respectively. 1778 ILP OVERLAY N NP RED -A RG ( PMI ) P RED -A RG ( CP )"
P13-1174,N10-1119,0,0.170747,"ma, Qaeda, Kosovo, Helicobacter, HIV Table 1: Example Named Entities (Proper Nouns) with Polar Connotation. A central premise to our approach is that it is collocational statistics of words that affect and shape the polarity of connotation. Indeed, the etymology of “connotation” is from the Latin “com” (“together or with”) and “notare” (“to mark”). It is important to clarify, however, that we do not simply assume that words that collocate share the same polarity of connotation. Although such an assumption played a key role in previous work for the analogous task of learning sentiment lexicon (Velikovich et al., 2010), we expect that the same assumption would be less reliable in drawing subtle connotative sentiments of words. As one example, the predicate “cure”, which has a positive connotation typically takes arguments with negative connotation, e.g., “disease”, when used as the “relieve” sense.3 Therefore, in order to attain a broad coverage lexicon while maintaining good precision, we guide the induction algorithm with multiple, carefully selected linguistic insights: [1] distributional similarity, [2] semantic parallelism of coordination, [3] selectional preference, and [4] semantic prosody (e.g., Sin"
P13-1174,H05-2018,1,0.831833,"the word “emissions” has generally negative connotation, which geothermal reduces. In fact, depending on the pragmatic contexts, it could be precisely the intention of the author to transfer his opinion into the readers’ minds. The main contribution of this paper is a broadcoverage connotation lexicon that determines the connotative polarity of even those words with ever so subtle connotation beneath their surface meaning, such as “Literature”, “Mediterranean”, and “wine”. Although there has been a number of previous work that constructed sentiment lexicons (e.g., Esuli and Sebastiani (2006), Wilson et al. (2005a), Kaji and Kitsuregawa (2007), Qiu et al. (2009)), which seem to be increasingly and inevitably expanding over words with (strongly) connotative sentiments rather than explicit sentiments alone (e.g., “gun”), little prior work has directly tackled this problem of learning connotation,2 and much of the subtle connotation of many seemingly objective words is yet to be determined. 1 Our learned lexicon correctly assigns negative polarity to emission. 2 A notable exception would be the work of Feng et al. 1774 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistic"
P13-1174,H05-1044,0,0.715104,"the word “emissions” has generally negative connotation, which geothermal reduces. In fact, depending on the pragmatic contexts, it could be precisely the intention of the author to transfer his opinion into the readers’ minds. The main contribution of this paper is a broadcoverage connotation lexicon that determines the connotative polarity of even those words with ever so subtle connotation beneath their surface meaning, such as “Literature”, “Mediterranean”, and “wine”. Although there has been a number of previous work that constructed sentiment lexicons (e.g., Esuli and Sebastiani (2006), Wilson et al. (2005a), Kaji and Kitsuregawa (2007), Qiu et al. (2009)), which seem to be increasingly and inevitably expanding over words with (strongly) connotative sentiments rather than explicit sentiments alone (e.g., “gun”), little prior work has directly tackled this problem of learning connotation,2 and much of the subtle connotation of many seemingly objective words is yet to be determined. 1 Our learned lexicon correctly assigns negative polarity to emission. 2 A notable exception would be the work of Feng et al. 1774 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistic"
P13-2138,P06-2019,0,0.00921275,"extracted from the Google Web 1T corpus (Brants and Franz., 2006), and the other computed from the 1M image-caption corpus (Ordonez et al., 2011). The constraint optimization we formulated corresponds to an NP-hard problem. In our work, hard constraints are based only on typed dependencies, and we find that long range dependencies occur infrequently in actual image descriptions, as plotted in Figure 2. With this insight, we opt for decoding based on dynamic programming with dynamically adjusted beam.4 Alternatively, one can find an approximate solution using Integer Linear Programming (e.g., Clarke and Lapata (2006), Clarke and Lapata (2007), Martins and Smith (2009)). Dependency-driven Constraints: Table 1 defines the list of dependencies used as constraints driven from the typed dependencies (de Marneffe and Manning, 2009; de Marneffe et al., 2006). The direction of arrows indicate the direction of inclusion requirements. For example, dep(X ←− Y ), denotes that “X” must be included whenever “Y ” is included. Similarly, dep(X ←→ Y ) denotes that “X” and “Y ” must either be included together or eliminated together. We determine the uni- or bi-directionality of these constraints by manually examining a fe"
P13-2138,P10-1126,0,0.0314772,"pplication of image caption transfer. 1 “I saw her in the light of her reading lamp and sneaked back to her door with the camera.” Visually relevant, but with overly extraneous details “Sections of the bridge sitting in the Dyer Construction yard south of Cabelas Driver.” Visually truthful, but for an uncommon situation “A house being pulled by a boat.” Figure 1: Examples of captions that are not readily applicable to other visually similar images. text from the retrieved samples to the query image (e.g. Farhadi et al. (2010), Ordonez et al. (2011), Kuznetsova et al. (2012)). Other work (e.g. Feng and Lapata (2010a), Feng and Lapata (2010b)) uses computer vision to bias summarization of text associated with images to produce descriptions. All of these approaches rely on existing text that describes visual content, but many times existing image descriptions contain significant amounts of extraneous, non-visual, or otherwise non-desirable content. The goal of this paper is to develop techniques to automatically clean up visually descriptive text to make it more directly usable for applications exploiting the connection between images and language. Introduction The vast number of online images with accomp"
P13-2138,D07-1001,0,0.0527693,"Web 1T corpus (Brants and Franz., 2006), and the other computed from the 1M image-caption corpus (Ordonez et al., 2011). The constraint optimization we formulated corresponds to an NP-hard problem. In our work, hard constraints are based only on typed dependencies, and we find that long range dependencies occur infrequently in actual image descriptions, as plotted in Figure 2. With this insight, we opt for decoding based on dynamic programming with dynamically adjusted beam.4 Alternatively, one can find an approximate solution using Integer Linear Programming (e.g., Clarke and Lapata (2006), Clarke and Lapata (2007), Martins and Smith (2009)). Dependency-driven Constraints: Table 1 defines the list of dependencies used as constraints driven from the typed dependencies (de Marneffe and Manning, 2009; de Marneffe et al., 2006). The direction of arrows indicate the direction of inclusion requirements. For example, dep(X ←− Y ), denotes that “X” must be included whenever “Y ” is included. Similarly, dep(X ←→ Y ) denotes that “X” and “Y ” must either be included together or eliminated together. We determine the uni- or bi-directionality of these constraints by manually examining a few example sentences corres"
P13-2138,W08-1105,0,0.0130979,"imilarity.5 The best performing approach with semantic matching is VISUAL (with LM = Image corpus), improving BLEU, Precision, F-score substantially over those of ORIG, demonstrating the extrinsic utility of our newly generated image-text parallel corpus in comparison to the original database. Figure 3 shows an example of caption transfer. 4 descriptions that are more general and align more closely to the visual image content. In comparison to prior work on sentence compression, our approach falls somewhere between unsupervised to distant-supervised approach (e.g., Turner and Charniak (2005), Filippova and Strube (2008)) in that there is not an in-domain training corpus to learn generalization patterns directly. Future work includes exploring more direct supervision from human edited sample generalization (e.g., Knight and Marcu (2000), McDonald (2006)) Galley and McKeown (2007), Zhu et al. (2010)), and the inclusion of edits beyond deletion, e.g., substitutions, as has been explored by e.g., Cohn and Lapata (2008), Cordeiro et al. (2009), Napoles et al. (2011). 5 Conclusion We have introduced the task of image caption generalization as a means to reduce noise in the parallel corpus of images and text. Intri"
P13-2138,C08-1018,0,0.0248502,"visual image content. In comparison to prior work on sentence compression, our approach falls somewhere between unsupervised to distant-supervised approach (e.g., Turner and Charniak (2005), Filippova and Strube (2008)) in that there is not an in-domain training corpus to learn generalization patterns directly. Future work includes exploring more direct supervision from human edited sample generalization (e.g., Knight and Marcu (2000), McDonald (2006)) Galley and McKeown (2007), Zhu et al. (2010)), and the inclusion of edits beyond deletion, e.g., substitutions, as has been explored by e.g., Cohn and Lapata (2008), Cordeiro et al. (2009), Napoles et al. (2011). 5 Conclusion We have introduced the task of image caption generalization as a means to reduce noise in the parallel corpus of images and text. Intrinsic and extrinsic evaluations confirm that the captions in the resulting corpus align better with the image contents (are often preferred over the original captions by people), and can be practically more useful with respect to a concrete application. Related Work Several recent studies presented approaches to automatic caption generation for images (e.g., Farhadi et al. (2010), Feng and Lapata (201"
P13-2138,N07-1023,0,0.0133019,"on to the original database. Figure 3 shows an example of caption transfer. 4 descriptions that are more general and align more closely to the visual image content. In comparison to prior work on sentence compression, our approach falls somewhere between unsupervised to distant-supervised approach (e.g., Turner and Charniak (2005), Filippova and Strube (2008)) in that there is not an in-domain training corpus to learn generalization patterns directly. Future work includes exploring more direct supervision from human edited sample generalization (e.g., Knight and Marcu (2000), McDonald (2006)) Galley and McKeown (2007), Zhu et al. (2010)), and the inclusion of edits beyond deletion, e.g., substitutions, as has been explored by e.g., Cohn and Lapata (2008), Cordeiro et al. (2009), Napoles et al. (2011). 5 Conclusion We have introduced the task of image caption generalization as a means to reduce noise in the parallel corpus of images and text. Intrinsic and extrinsic evaluations confirm that the captions in the resulting corpus align better with the image contents (are often preferred over the original captions by people), and can be practically more useful with respect to a concrete application. Related Wor"
P13-2138,W09-2805,0,0.0118997,"n comparison to prior work on sentence compression, our approach falls somewhere between unsupervised to distant-supervised approach (e.g., Turner and Charniak (2005), Filippova and Strube (2008)) in that there is not an in-domain training corpus to learn generalization patterns directly. Future work includes exploring more direct supervision from human edited sample generalization (e.g., Knight and Marcu (2000), McDonald (2006)) Galley and McKeown (2007), Zhu et al. (2010)), and the inclusion of edits beyond deletion, e.g., substitutions, as has been explored by e.g., Cohn and Lapata (2008), Cordeiro et al. (2009), Napoles et al. (2011). 5 Conclusion We have introduced the task of image caption generalization as a means to reduce noise in the parallel corpus of images and text. Intrinsic and extrinsic evaluations confirm that the captions in the resulting corpus align better with the image contents (are often preferred over the original captions by people), and can be practically more useful with respect to a concrete application. Related Work Several recent studies presented approaches to automatic caption generation for images (e.g., Farhadi et al. (2010), Feng and Lapata (2010a), Feng and Lapata (20"
P13-2138,de-marneffe-etal-2006-generating,0,0.0423605,"Missing"
P13-2138,P12-1038,1,0.566129,"parallel corpus with respect to a concrete application of image caption transfer. 1 “I saw her in the light of her reading lamp and sneaked back to her door with the camera.” Visually relevant, but with overly extraneous details “Sections of the bridge sitting in the Dyer Construction yard south of Cabelas Driver.” Visually truthful, but for an uncommon situation “A house being pulled by a boat.” Figure 1: Examples of captions that are not readily applicable to other visually similar images. text from the retrieved samples to the query image (e.g. Farhadi et al. (2010), Ordonez et al. (2011), Kuznetsova et al. (2012)). Other work (e.g. Feng and Lapata (2010a), Feng and Lapata (2010b)) uses computer vision to bias summarization of text associated with images to produce descriptions. All of these approaches rely on existing text that describes visual content, but many times existing image descriptions contain significant amounts of extraneous, non-visual, or otherwise non-desirable content. The goal of this paper is to develop techniques to automatically clean up visually descriptive text to make it more directly usable for applications exploiting the connection between images and language. Introduction The"
P13-2138,N12-1094,1,0.743272,"mod(←), nn(←), conj*(↔), num*(←) number(↔), parataxis(←), ↔ partmod*(←), pcomp*(↔), purpcl*(←) possessive(↔), preconj*(←), predet*(←) prt(↔), quantmod(←), rcmod(←), ref(←) rel*(↔), tmod*(←), xcomp*(→), xsubj(→) Table 1: Dependency-based Constraints text of the third image, “A house being pulled by a boat”, pertains directly to the visual content of the image, but is unlikely to be useful for tasks such as caption transfer because the depiction is unusual.1 This phenomenon of information gap between the visual content of the images and their corresponding narratives has been studied closely by Dodge et al. (2012). The content misalignment between images and text limits the extent to which visual detectors can learn meaningful mappings between images and text. To tackle this challenge, we introduce the new task of image caption generalization that rewrites captions to be more visually relevant and more readily applicable to other visually similar images. Our end goal is to convert noisy imagetext pairs in the wild (Ordonez et al., 2011) into pairs with tighter content alignment, resulting in new simplified captions over 1 million images. Evaluation results show both the intrinsic quality of the general"
P13-2138,W11-0326,1,0.050426,"oduced the task of image caption generalization as a means to reduce noise in the parallel corpus of images and text. Intrinsic and extrinsic evaluations confirm that the captions in the resulting corpus align better with the image contents (are often preferred over the original captions by people), and can be practically more useful with respect to a concrete application. Related Work Several recent studies presented approaches to automatic caption generation for images (e.g., Farhadi et al. (2010), Feng and Lapata (2010a), Feng and Lapata (2010b), Yang et al. (2011), Kulkarni et al. (2011), Li et al. (2011), Kuznetsova et al. (2012)). The end goal of our work differs in that we aim to revise original image captions into Acknowledgments This research was supported in part by the Stony Brook University Office of the Vice President for Research. Additionally, Tamara Berg is supported by NSF #1054133 and NSF #1161876. We thank reviewers for many insightful comments and suggestions. 5 We take Wu-Palmer Similarity as similarity measure (Wu and Palmer, 1994). When computing BLEU with semantic matching, we look for the match with the highest similarity score among words that have not been matched before"
P13-2138,C10-1152,0,0.0187697,"Missing"
P13-2138,W09-1801,0,0.0191076,"Franz., 2006), and the other computed from the 1M image-caption corpus (Ordonez et al., 2011). The constraint optimization we formulated corresponds to an NP-hard problem. In our work, hard constraints are based only on typed dependencies, and we find that long range dependencies occur infrequently in actual image descriptions, as plotted in Figure 2. With this insight, we opt for decoding based on dynamic programming with dynamically adjusted beam.4 Alternatively, one can find an approximate solution using Integer Linear Programming (e.g., Clarke and Lapata (2006), Clarke and Lapata (2007), Martins and Smith (2009)). Dependency-driven Constraints: Table 1 defines the list of dependencies used as constraints driven from the typed dependencies (de Marneffe and Manning, 2009; de Marneffe et al., 2006). The direction of arrows indicate the direction of inclusion requirements. For example, dep(X ←− Y ), denotes that “X” must be included whenever “Y ” is included. Similarly, dep(X ←→ Y ) denotes that “X” and “Y ” must either be included together or eliminated together. We determine the uni- or bi-directionality of these constraints by manually examining a few example sentences corresponding to each of these t"
P13-2138,E06-1038,0,0.0076704,"orpus in comparison to the original database. Figure 3 shows an example of caption transfer. 4 descriptions that are more general and align more closely to the visual image content. In comparison to prior work on sentence compression, our approach falls somewhere between unsupervised to distant-supervised approach (e.g., Turner and Charniak (2005), Filippova and Strube (2008)) in that there is not an in-domain training corpus to learn generalization patterns directly. Future work includes exploring more direct supervision from human edited sample generalization (e.g., Knight and Marcu (2000), McDonald (2006)) Galley and McKeown (2007), Zhu et al. (2010)), and the inclusion of edits beyond deletion, e.g., substitutions, as has been explored by e.g., Cohn and Lapata (2008), Cordeiro et al. (2009), Napoles et al. (2011). 5 Conclusion We have introduced the task of image caption generalization as a means to reduce noise in the parallel corpus of images and text. Intrinsic and extrinsic evaluations confirm that the captions in the resulting corpus align better with the image contents (are often preferred over the original captions by people), and can be practically more useful with respect to a concre"
P13-2138,W11-1610,0,0.0242227,"Missing"
P13-2138,P05-1036,0,0.014259,"irs based on their lexical similarity.5 The best performing approach with semantic matching is VISUAL (with LM = Image corpus), improving BLEU, Precision, F-score substantially over those of ORIG, demonstrating the extrinsic utility of our newly generated image-text parallel corpus in comparison to the original database. Figure 3 shows an example of caption transfer. 4 descriptions that are more general and align more closely to the visual image content. In comparison to prior work on sentence compression, our approach falls somewhere between unsupervised to distant-supervised approach (e.g., Turner and Charniak (2005), Filippova and Strube (2008)) in that there is not an in-domain training corpus to learn generalization patterns directly. Future work includes exploring more direct supervision from human edited sample generalization (e.g., Knight and Marcu (2000), McDonald (2006)) Galley and McKeown (2007), Zhu et al. (2010)), and the inclusion of edits beyond deletion, e.g., substitutions, as has been explored by e.g., Cohn and Lapata (2008), Cordeiro et al. (2009), Napoles et al. (2011). 5 Conclusion We have introduced the task of image caption generalization as a means to reduce noise in the parallel cor"
P13-2138,D11-1041,0,0.08004,"les et al. (2011). 5 Conclusion We have introduced the task of image caption generalization as a means to reduce noise in the parallel corpus of images and text. Intrinsic and extrinsic evaluations confirm that the captions in the resulting corpus align better with the image contents (are often preferred over the original captions by people), and can be practically more useful with respect to a concrete application. Related Work Several recent studies presented approaches to automatic caption generation for images (e.g., Farhadi et al. (2010), Feng and Lapata (2010a), Feng and Lapata (2010b), Yang et al. (2011), Kulkarni et al. (2011), Li et al. (2011), Kuznetsova et al. (2012)). The end goal of our work differs in that we aim to revise original image captions into Acknowledgments This research was supported in part by the Stony Brook University Office of the Vice President for Research. Additionally, Tamara Berg is supported by NSF #1054133 and NSF #1161876. We thank reviewers for many insightful comments and suggestions. 5 We take Wu-Palmer Similarity as similarity measure (Wu and Palmer, 1994). When computing BLEU with semantic matching, we look for the match with the highest similarity score amo"
P13-2138,N10-1125,0,\N,Missing
P13-2138,P94-1019,0,\N,Missing
P14-1145,D09-1020,0,0.0968065,"al., 2009) and a sense-level sentiment lexicon (Esuli and Sebastiani, 2006). But none of these approaches considered to induce the polarity labels at both the word-level and sense-level. Although we focus on learning connotative polarity of words and senses in this paper, the same approach would be applicable to constructing a sentiment lexicon as well. There have been recent studies that address word sense disambiguation issues for sentiment analysis. SentiWordNet (Esuli and Sebastiani, 2006) was the very first lexicon developed for sense-level labels of sentiment polarity. In recent years, Akkaya et al. (2009) report a successful empirical result where WSD helps improving sentiment analysis, while Wiebe and Mihalcea (2006) study the distinction between objectivity and subjectivity in each different sense of a word, and their empirical effects in the context of sentiment analysis. Our work shares the high-level spirit of accessing the sense-level polarity, while also deriving the word-level polarity. In recent years, there has been a growing research interest in investigating more fine-grained aspects of lexical sentiment beyond positive and negative sentiment. For example, Mohammad and Turney (2010"
P14-1145,E06-1027,0,0.0721526,"5 receive the same connotation label. Conversely, we expect that these edges will also encourage words that belong to the same sense (i.e., synset definition) to receive the same connotation label. Another benefit of our approach is that for various WordNet relations (e.g., antonym relations), which are defined over synsets (not over words), we can add edges directly between corresponding synsets, rather than projecting (i.e., approximating) those relations over words. Note that the latter, which has been employed by several previous studies (e.g., Kamps et al. (2004), Takamura et al. (2005), Andreevskaia and Bergler (2006), Su and Markert (2009), Lu et al. (2011), Kaji and Kitsuregawa (2007), Feng et al. (2013)), could be a source of noise, as one needs to assume that the semantic relation between a pair of synsets transfers over the pair of words corresponding to that pair of synsets. For polysemous words, this assumption may be overly strong. 3 Pairwise Markov Random Fields and Loopy Belief Propagation We formulate the task of learning sense- and wordlevel connotation lexicon as a graph-based classification task (Sen et al., 2008). More formally, we denote the connotation graph G WORD +S ENSE by G = (V, E), i"
P14-1145,baccianella-etal-2010-sentiwordnet,0,0.468921,"Missing"
P14-1145,J90-1003,0,0.0781562,"dges can also be weighted based on the distributional similarities of the word pairs. G WORD : The third graph is a super-graph of G WORD W / OVERLAY, with additional edges, where argument pairs in synonym and antonym relation are connected to each other. Note that unlike the connotation graph G WORD +S ENSE , it does not contain any synset nodes. Rather, the words that are synonyms or antonyms of each other are directly linked in the graph. As such, this graph contains all edge types t1 through t4 . 5 PMI scores are widely used in previous studies to measure association between words (e.g., (Church and Hanks, 1990), (Turney, 2001), (Newman et al., 2009)). 1548 G WORD +S ENSE W / S YN S IM: This is a supergraph of our original G WORD +S ENSE graph; that is, it has all the predicate, arguments, and synset nodes, as well as the four types of edges between them. In addition, we add edges of a fifth type t5 between the synset nodes to capture their similarity. To define similarity, we use the glossary definitions of the synsets and derive three different scores. Each score utilizes the count(s1 , s2 ) of overlapping nouns, verbs, and adjectives/adverbs among the glosses of the two synsets s1 and s2 . G WORD"
P14-1145,esuli-sebastiani-2006-sentiwordnet,0,0.0691543,"e, however, is that in our MRF representation, we can explicitly model various types of word-word, sense-sense and word-sense relations as edge potentials. In particular, we can naturally encode relations that encourage the same assignment (e.g., synonym) as well as the opposite assignment (e.g., antonym) of the polarity labels. Note that integration of the latter is not straightforward in the graph propagation framework. There have been a number of previous studies that aim to construct a word-level sentiment lexicon (Wiebe et al., 2005; Qiu et al., 2009) and a sense-level sentiment lexicon (Esuli and Sebastiani, 2006). But none of these approaches considered to induce the polarity labels at both the word-level and sense-level. Although we focus on learning connotative polarity of words and senses in this paper, the same approach would be applicable to constructing a sentiment lexicon as well. There have been recent studies that address word sense disambiguation issues for sentiment analysis. SentiWordNet (Esuli and Sebastiani, 2006) was the very first lexicon developed for sense-level labels of sentiment polarity. In recent years, Akkaya et al. (2009) report a successful empirical result where WSD helps im"
P14-1145,D11-1101,1,0.755242,"propagation algorithm for inference. The key aspect of our method is that it is the first unified approach that assigns the polarity of both word- and sense-level connotations, exploiting the innate bipartite graph structure encoded in WordNet. We present comprehensive evaluation to demonstrate the quality and utility of the resulting lexicon in comparison to existing connotation and sentiment lexicons. 1 Introduction We introduce ConnotationWordNet, a connotation lexicon over the network of words in conjunction with senses, as defined in WordNet. A connotation lexicon, as introduced first by Feng et al. (2011), aims to encompass subtle shades of sentiment a word may conjure, even for seemingly objective words such as “sculpture”, “Ph.D.”, “rosettes”. Understanding the rich and complex layers of connotation remains to be a challenging task. As a starting point, we study a more feasible task of learning the polarity of connotation. For non-polysemous words, which constitute a significant portion of English vocabulary, learning the general connotation at the word-level (rather than at the sense-level) would be a natural operational choice. However, for polysemous words, which correspond to most freque"
P14-1145,P13-1174,1,0.871068,"nt edges, first introduced by Feng et al. (2011), depict the selectional preference of connotative predicates (i.e., the polarity of a predicate indicates the polarity of its arguments) and encode their co-occurrence relations based on the Google Web 1T corpus. The argumentargument edges are based on the distributional similarities among the arguments. The argumentsynset edges capture the synonymy between argument nodes through the corresponding synsets. Finally, the synset-synset edges depict the antonym relations between synset pairs. In general, our graph construction is similar to that of Feng et al. (2013), but there are a few important differences. Most notably, we model both words and synsets explicitly, and exploit the membership relations between words and senses. We expect that edges between words and senses will encourage senses that belong to the same word to 1545 receive the same connotation label. Conversely, we expect that these edges will also encourage words that belong to the same sense (i.e., synset definition) to receive the same connotation label. Another benefit of our approach is that for various WordNet relations (e.g., antonym relations), which are defined over synsets (not"
P14-1145,kamps-etal-2004-using,0,0.19416,"Missing"
P14-1145,P12-4004,0,0.0206521,"of unpleasantness, or at least not as positive as that of the first sense. Especially if we look up the WordNet entry for “bristle”, there are noticeably more negatively connotative words involved in its gloss and examples. This word sense issue has been a universal challenge for a range of Natural Language Processing applications, including sentiment analysis. Recent studies have shown that it is fruitful to tease out subjectivity and objectivity corresponding to different senses of the same word, in order to improve computational approaches to sentiment analysis (e.g. Pestian et al. (2012), Mihalcea et al. (2012) Balahur et al. (2014)). Encouraged by these recent successes, in this study, we investigate if we can attain similar gains if we model the connotative polarity of senses separately. There is one potential practical issue we would like to point out in building a sense-level lexical resource, however. End-users of such a lexicon may not wish to deal with Word Sense Disam1 Hence a sense in WordNet is defined by synset (= synonym set), which is the set of words sharing the same sense. 1544 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1544–1554, c"
P14-1145,W10-0204,0,0.0411054,"ars, Akkaya et al. (2009) report a successful empirical result where WSD helps improving sentiment analysis, while Wiebe and Mihalcea (2006) study the distinction between objectivity and subjectivity in each different sense of a word, and their empirical effects in the context of sentiment analysis. Our work shares the high-level spirit of accessing the sense-level polarity, while also deriving the word-level polarity. In recent years, there has been a growing research interest in investigating more fine-grained aspects of lexical sentiment beyond positive and negative sentiment. For example, Mohammad and Turney (2010) study the affects words can evoke in people’s minds, while Bollen et al. (2011) study various moods, e.g., “tension”, “depression”, beyond simple dichotomy of positive and negative sentiment. Our work, and some recent work by Feng et al. (2011) and Feng et al. (2013) share this spirit by targeting more subtle, nuanced sentiment even from those words that would be considered as objective in early studies of sentiment analysis. 8 Conclusion We have introduced a novel formulation of lexicon induction operating over both words and senses, by exploiting the innate structure between the words and s"
P14-1145,P97-1023,0,0.306941,"RD +S ENSE , which only includes the connotative predicates and their arguments. As such, it contains only type t1 edges. The edges between the predicates and the arguments can be weighted by their Point-wise Mutual Information (PMI)5 based on the Google Web 1T corpus. G WORD W / OVERLAY: The second graph is also a proper subgraph of G WORD +S ENSE , which includes the predicates and all the argument words. Predicate words are connected to their arguments as before. In addition, argument pairs (a1 , a2 ) are connected if they occurred together in the “a1 and a2 ” or “a2 and a1 ” coordination (Hatzivassiloglou and McKeown, 1997; Pickering and Branigan, 1998). This graph contains both type t1 and t2 edges. The edges can also be weighted based on the distributional similarities of the word pairs. G WORD : The third graph is a super-graph of G WORD W / OVERLAY, with additional edges, where argument pairs in synonym and antonym relation are connected to each other. Note that unlike the connotation graph G WORD +S ENSE , it does not contain any synset nodes. Rather, the words that are synonyms or antonyms of each other are directly linked in the graph. As such, this graph contains all edge types t1 through t4 . 5 PMI sco"
P14-1145,D07-1115,0,0.0260696,"es will also encourage words that belong to the same sense (i.e., synset definition) to receive the same connotation label. Another benefit of our approach is that for various WordNet relations (e.g., antonym relations), which are defined over synsets (not over words), we can add edges directly between corresponding synsets, rather than projecting (i.e., approximating) those relations over words. Note that the latter, which has been employed by several previous studies (e.g., Kamps et al. (2004), Takamura et al. (2005), Andreevskaia and Bergler (2006), Su and Markert (2009), Lu et al. (2011), Kaji and Kitsuregawa (2007), Feng et al. (2013)), could be a source of noise, as one needs to assume that the semantic relation between a pair of synsets transfers over the pair of words corresponding to that pair of synsets. For polysemous words, this assumption may be overly strong. 3 Pairwise Markov Random Fields and Loopy Belief Propagation We formulate the task of learning sense- and wordlevel connotation lexicon as a graph-based classification task (Sen et al., 2008). More formally, we denote the connotation graph G WORD +S ENSE by G = (V, E), in which a total of n word and synset nodes V = {v1 , . . . , vn } are"
P14-1145,H05-2018,1,0.822035,"Missing"
P14-1145,S07-1013,0,0.166675,"Missing"
P14-1145,H05-1044,0,0.405142,"alable to large datasets. 4 arg-arg edges are based on co-occurrence (see Section 2), which does not carry as strong indication of the same connotation as e.g., synonymy. Thus, we enforce less homophily for nodes connected through edges of arg-arg type. 4 Evaluation I: Agreement with Sentiment Lexicons ConnotationWordNet is expected to be the superset of a sentiment lexicon, as it is highly likely for any word with positive/negative sentiment to carry connotation of the same polarity. Thus, we use two conventional sentiment lexicons, General Inquirer (G EN I NQ) (Stone et al., 1966) and MPQA (Wilson et al., 2005b), as surrogates to measure the performance of our inference algorithm. 4.1 Variants of Graph Construction The construction of the connotation graph, denoted by G WORD +S ENSE , which includes words and synsets, has been described in Section 2. In addition to this graph, we tried several other graph constructions, the first three of which have previously been used in (Feng et al., 2013). We briefly describe these graphs below, and compare performance on all the graphs in the proceeding. G WORD W / P RED -A RG: This is a (bipartite) subgraph of G WORD +S ENSE , which only includes the connotat"
P14-1145,N09-1001,0,0.0635026,"bel. Conversely, we expect that these edges will also encourage words that belong to the same sense (i.e., synset definition) to receive the same connotation label. Another benefit of our approach is that for various WordNet relations (e.g., antonym relations), which are defined over synsets (not over words), we can add edges directly between corresponding synsets, rather than projecting (i.e., approximating) those relations over words. Note that the latter, which has been employed by several previous studies (e.g., Kamps et al. (2004), Takamura et al. (2005), Andreevskaia and Bergler (2006), Su and Markert (2009), Lu et al. (2011), Kaji and Kitsuregawa (2007), Feng et al. (2013)), could be a source of noise, as one needs to assume that the semantic relation between a pair of synsets transfers over the pair of words corresponding to that pair of synsets. For polysemous words, this assumption may be overly strong. 3 Pairwise Markov Random Fields and Loopy Belief Propagation We formulate the task of learning sense- and wordlevel connotation lexicon as a graph-based classification task (Sen et al., 2008). More formally, we denote the connotation graph G WORD +S ENSE by G = (V, E), in which a total of n wo"
P14-1145,P05-1017,0,0.0395355,"to the same word to 1545 receive the same connotation label. Conversely, we expect that these edges will also encourage words that belong to the same sense (i.e., synset definition) to receive the same connotation label. Another benefit of our approach is that for various WordNet relations (e.g., antonym relations), which are defined over synsets (not over words), we can add edges directly between corresponding synsets, rather than projecting (i.e., approximating) those relations over words. Note that the latter, which has been employed by several previous studies (e.g., Kamps et al. (2004), Takamura et al. (2005), Andreevskaia and Bergler (2006), Su and Markert (2009), Lu et al. (2011), Kaji and Kitsuregawa (2007), Feng et al. (2013)), could be a source of noise, as one needs to assume that the semantic relation between a pair of synsets transfers over the pair of words corresponding to that pair of synsets. For polysemous words, this assumption may be overly strong. 3 Pairwise Markov Random Fields and Loopy Belief Propagation We formulate the task of learning sense- and wordlevel connotation lexicon as a graph-based classification task (Sen et al., 2008). More formally, we denote the connotation grap"
P14-1145,N10-1119,0,0.0866411,"Missing"
P14-1145,P06-1134,0,0.0651088,"sidered to induce the polarity labels at both the word-level and sense-level. Although we focus on learning connotative polarity of words and senses in this paper, the same approach would be applicable to constructing a sentiment lexicon as well. There have been recent studies that address word sense disambiguation issues for sentiment analysis. SentiWordNet (Esuli and Sebastiani, 2006) was the very first lexicon developed for sense-level labels of sentiment polarity. In recent years, Akkaya et al. (2009) report a successful empirical result where WSD helps improving sentiment analysis, while Wiebe and Mihalcea (2006) study the distinction between objectivity and subjectivity in each different sense of a word, and their empirical effects in the context of sentiment analysis. Our work shares the high-level spirit of accessing the sense-level polarity, while also deriving the word-level polarity. In recent years, there has been a growing research interest in investigating more fine-grained aspects of lexical sentiment beyond positive and negative sentiment. For example, Mohammad and Turney (2010) study the affects words can evoke in people’s minds, while Bollen et al. (2011) study various moods, e.g., “tensi"
P16-1030,D09-1020,0,0.0243763,"Missing"
P16-1030,baccianella-etal-2010-sentiwordnet,0,0.0164475,"Missing"
P16-1030,P98-1013,0,0.592476,"re 1 shows an example of a connotation frame for the predicate violate. We define four different typed relations: P(x → y) for perspective of x towards y, E(x) for effect on x, V(x) for value of x, and S(x) for mental state of x. These relationships can all be either positive (+), neutral (=), or negative (-). Our work is the first study to investigate frames as a representation formalism for connotative meanings. This contrasts with previous computational studies and resource development for frame semantics, where the primary focus was almost exclusively on denotational meanings of language (Baker et al., 1998; Palmer et al., 2005). Our formalism draws inspirations from the earlier work of frame semantics, however, in that we investigate the connection between a word and the related world knowledge associated with the word (Fillmore, 1976), which is essential for the readers to interpret many layers of the implied sentiment and presupposed value judgments. We also build upon the extensive amount of literature in sentiment analysis (Pang and Lee, 2008; Liu and Zhang, 2012), especially the recent emerging efforts on implied sentiment analysis (Feng et al., 2013; Greene and Resnik, 2009), entityentity"
P16-1030,bethard-etal-2008-building,0,0.0580396,"Missing"
P16-1030,W10-0203,0,0.0581481,"Missing"
P16-1030,P09-1068,0,0.104949,"Missing"
P16-1030,N09-1057,0,0.0222102,"anings of language (Baker et al., 1998; Palmer et al., 2005). Our formalism draws inspirations from the earlier work of frame semantics, however, in that we investigate the connection between a word and the related world knowledge associated with the word (Fillmore, 1976), which is essential for the readers to interpret many layers of the implied sentiment and presupposed value judgments. We also build upon the extensive amount of literature in sentiment analysis (Pang and Lee, 2008; Liu and Zhang, 2012), especially the recent emerging efforts on implied sentiment analysis (Feng et al., 2013; Greene and Resnik, 2009), entityentity sentiment inference (Wiebe and Deng, 2014), assuming it is an entity that can have a mental state. opinion role induction (Wiegand and Ruppenhofer, 2015) and effect analysis (Choi and Wiebe, 2014). However, our work is the first to organize various aspects of the connotative information into coherent frames. More concretely, our contributions are threefold: (1) a new formalism, model, and annotated dataset for studying connotation frames from large-scale natural language data and statistics, (2) new datadriven insights into the dynamics among different typed relations within eac"
P16-1030,W13-3514,0,0.0607865,"Missing"
P16-1030,D14-1125,0,0.0729271,"Missing"
P16-1030,E14-1040,0,0.160222,"Missing"
P16-1030,D07-1115,0,0.0457368,"Missing"
P16-1030,P13-1174,1,0.862919,"on denotational meanings of language (Baker et al., 1998; Palmer et al., 2005). Our formalism draws inspirations from the earlier work of frame semantics, however, in that we investigate the connection between a word and the related world knowledge associated with the word (Fillmore, 1976), which is essential for the readers to interpret many layers of the implied sentiment and presupposed value judgments. We also build upon the extensive amount of literature in sentiment analysis (Pang and Lee, 2008; Liu and Zhang, 2012), especially the recent emerging efforts on implied sentiment analysis (Feng et al., 2013; Greene and Resnik, 2009), entityentity sentiment inference (Wiebe and Deng, 2014), assuming it is an entity that can have a mental state. opinion role induction (Wiegand and Ruppenhofer, 2015) and effect analysis (Choi and Wiebe, 2014). However, our work is the first to organize various aspects of the connotative information into coherent frames. More concretely, our contributions are threefold: (1) a new formalism, model, and annotated dataset for studying connotation frames from large-scale natural language data and statistics, (2) new datadriven insights into the dynamics among different"
P16-1030,E14-1006,0,0.0186263,"Missing"
P16-1030,S13-1035,0,0.0387423,"Missing"
P16-1030,P14-2050,0,0.0304518,"Missing"
P16-1030,J05-1004,0,0.258364,"Missing"
P16-1030,K15-1022,0,0.0128664,"tigate the connection between a word and the related world knowledge associated with the word (Fillmore, 1976), which is essential for the readers to interpret many layers of the implied sentiment and presupposed value judgments. We also build upon the extensive amount of literature in sentiment analysis (Pang and Lee, 2008; Liu and Zhang, 2012), especially the recent emerging efforts on implied sentiment analysis (Feng et al., 2013; Greene and Resnik, 2009), entityentity sentiment inference (Wiebe and Deng, 2014), assuming it is an entity that can have a mental state. opinion role induction (Wiegand and Ruppenhofer, 2015) and effect analysis (Choi and Wiebe, 2014). However, our work is the first to organize various aspects of the connotative information into coherent frames. More concretely, our contributions are threefold: (1) a new formalism, model, and annotated dataset for studying connotation frames from large-scale natural language data and statistics, (2) new datadriven insights into the dynamics among different typed relations within each frame, and (3) an analytic study showing the potential use of connotation frames for analyzing subtle biases in journalism. The rest of the paper is organized as foll"
P16-1030,P79-1000,0,0.72901,"Missing"
P16-1030,W10-0723,0,0.0385888,"ssed by the event. = Value: not clear if agent is valuable E(agent) S(agent) State: the agent feels indifferent = E(agent) Effect: the agent is not really affected by the violation + V(theme) Value: the theme must be valuable - E(theme) S(agent) State: the theme will be unhappy - E(theme) Effect: the theme has been hurt Figure 1: An example connotation frame of “violate” as a set of typed relations: perspective P(x → y), effect E(x), value V(x), and mental state S(x). Introduction People commonly express their opinions through subtle and nuanced language (Thomas et al., 2006; Somasundaran and Wiebe, 2010). Often, through seemingly objective statements, the writer can influence the readers’ judgments toward an event and their participants. Even by choosing a particular predicate, the writer can indicate rich connotative information about the entities that interact through the predicate. More specifically, through a simple statement such as “x violated y”, the writer can convey: (1) writer’s perspective: the writer is projecting x as an “antagonist” and y as a “victim”, eliciting negative perspective from readers toward x (i.e., blaming x) and positive perspective toward y (i.e., sympathetic or"
P16-1030,E06-1027,0,\N,Missing
P16-1030,kamps-etal-2004-using,0,\N,Missing
P16-1030,N10-1119,0,\N,Missing
P16-1030,W10-0214,0,\N,Missing
P16-1030,C98-1013,0,\N,Missing
P16-1030,P13-1162,0,\N,Missing
P16-1030,W10-0204,0,\N,Missing
P16-1030,W12-3018,0,\N,Missing
P16-1030,P05-1017,0,\N,Missing
P16-1032,P14-2015,0,0.0301803,"ψr to the ψsocial . 2.3 Discussion While many studies exist on homophily, social balance, and reciprocity, no prior work has reported quantitative analysis on the sentiment dynamics among the real world entities that appear in unstructured text. Thus we report the data statistics based on the development set in Table 1. We find that the global constraints hold commonly but are not universal, motivating the use of soft constraints (see Sec. 6). 3 Document Features Previous work has shown that notions related to salience (e.g., proximity to sentiment words) can help to detect sentiment targets (Ben-Ami et al., 2014). In our data, we found that an entity’s occurrence pattern is highly indicative of being involved in sentiment, for example the most frequently mentioned entity is 3.4 times more likely to be polarized and an entity in the headline is two times more likely to be polarized. Pairwise features include the NER type of ei and ej and the percentage of sentences they cooccur in. We also use features indicating whether ei and ej (1) are mentioned in the headline and (2) appear only once in the document. When they are the two most frequent entities, we add the document sentiment label as a feature. Fo"
P16-1032,N16-1180,0,0.0367009,"Missing"
P16-1032,E14-1040,0,0.0311492,"y sentiment from a large pool of potentially relevant documents. Thus, their annotations focus only on query entities and relatively sparse compared to ours (see Sec. 6). Another recent dataset is MPQA 3.0 (Deng and Wiebe, 2015b), which captures various aspects of sentiment. Their sentiment pair annotations are only at the sentencelevel and are therefore much sparser than we provide (see Sec. 6) for entity-entity relation analysis. Several recent studies focused on various aspects of implied sentiment (Greene and Resnik, 2009; Mohammad and Turney, 2010; Zhang and Liu, 2011; Feng et al., 2013; Deng and Wiebe, 2014; Deng et al., 2014). Deng and Wiebe (2015a) in particular introduced sentiment implicature rules relevant for sentence-level entityentity sentiment. Our work contributes to these recent efforts by presenting a new model and dataset for document-level sentiment inference over all entity pairs. 8 Conclusion We presented an approach to interpreting sentiment among entities in news articles, with global constraints provided by social, faction and discourse context. Experiments demonstrated that the approach can infer implied sentiment and point toward potential directions for future work, includi"
P16-1032,N15-1185,0,0.0171205,"effectiveness of jointly considering multiple sentences. Yang and Mitchell (2016) proposed joint extraction of entities and events with the document context, improving on the event extraction. Most work focuses on events, while we primarily study sentiment relations. Social Network Analysis While many previous studies considered the effect of social dynamics for social media analysis, most relied on an explicitly available social network structure or considered dialogues and speech acts for which opinion holders are given (Tan et al., 2011; Hu et al., 2013; Li et al., 2014; West et al., 2014; Krishnan and Eisenstein, 2015). Compared to the recent work that focused on relationships among fictional characters in movie summaries and stories (Chaturvedi et al., 2016; Srivastava et al., 2016; Iyyer et al., 2016), we consider a broader types of named entities on news domains. Related Work Sentiment Inference Our sentiment inference task is related to the recent KBP sentiment task,14 in that we aim to find opinion target and holder. While we study the complete document-level analysis over all entity pairs, the KBP task is formulated as query-focused retrieval of entity sentiment from a large pool of potentially releva"
P16-1032,D15-1018,0,0.798225,"es among entities (e.g., Moscow, Gryzlov, and Russia probably share the same sentiment towards other entities) and known social context (e.g., Russia probably 333 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 333–343, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics that can be inferred through partial evidence that spans multiple sentences. This complements prior efforts for accessing implied sentiment where the key evidence is, by and large, at the sentence level (Zhang and Liu, 2011; Yang and Cardie, 2013; Deng and Wiebe, 2015a). Finally, we present the first approach to model the relationship between factual and subjective relations. We evaluate the approach on a newly gathered corpus with dense document-level sentiment labels in news articles.1 This data includes comprehensively annotated sentiment between all entity pairs, including those that do not appear together in any single sentence. Experiments demonstrate that the global model significantly improves performance over a pairwise classifier and other strong baselines. We also perform a detailed ablation and error analysis, showing cases where the global con"
P16-1032,N15-1146,0,0.473481,"es among entities (e.g., Moscow, Gryzlov, and Russia probably share the same sentiment towards other entities) and known social context (e.g., Russia probably 333 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 333–343, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics that can be inferred through partial evidence that spans multiple sentences. This complements prior efforts for accessing implied sentiment where the key evidence is, by and large, at the sentence level (Zhang and Liu, 2011; Yang and Cardie, 2013; Deng and Wiebe, 2015a). Finally, we present the first approach to model the relationship between factual and subjective relations. We evaluate the approach on a newly gathered corpus with dense document-level sentiment labels in news articles.1 This data includes comprehensively annotated sentiment between all entity pairs, including those that do not appear together in any single sentence. Experiments demonstrate that the global model significantly improves performance over a pairwise classifier and other strong baselines. We also perform a detailed ablation and error analysis, showing cases where the global con"
P16-1032,P14-1016,0,0.0290864,"omain, previous research showed the effectiveness of jointly considering multiple sentences. Yang and Mitchell (2016) proposed joint extraction of entities and events with the document context, improving on the event extraction. Most work focuses on events, while we primarily study sentiment relations. Social Network Analysis While many previous studies considered the effect of social dynamics for social media analysis, most relied on an explicitly available social network structure or considered dialogues and speech acts for which opinion holders are given (Tan et al., 2011; Hu et al., 2013; Li et al., 2014; West et al., 2014; Krishnan and Eisenstein, 2015). Compared to the recent work that focused on relationships among fictional characters in movie summaries and stories (Chaturvedi et al., 2016; Srivastava et al., 2016; Iyyer et al., 2016), we consider a broader types of named entities on news domains. Related Work Sentiment Inference Our sentiment inference task is related to the recent KBP sentiment task,14 in that we aim to find opinion target and holder. While we study the complete document-level analysis over all entity pairs, the KBP task is formulated as query-focused retrieval of entit"
P16-1032,C14-1009,0,0.0643484,"ge pool of potentially relevant documents. Thus, their annotations focus only on query entities and relatively sparse compared to ours (see Sec. 6). Another recent dataset is MPQA 3.0 (Deng and Wiebe, 2015b), which captures various aspects of sentiment. Their sentiment pair annotations are only at the sentencelevel and are therefore much sparser than we provide (see Sec. 6) for entity-entity relation analysis. Several recent studies focused on various aspects of implied sentiment (Greene and Resnik, 2009; Mohammad and Turney, 2010; Zhang and Liu, 2011; Feng et al., 2013; Deng and Wiebe, 2014; Deng et al., 2014). Deng and Wiebe (2015a) in particular introduced sentiment implicature rules relevant for sentence-level entityentity sentiment. Our work contributes to these recent efforts by presenting a new model and dataset for document-level sentiment inference over all entity pairs. 8 Conclusion We presented an approach to interpreting sentiment among entities in news articles, with global constraints provided by social, faction and discourse context. Experiments demonstrated that the approach can infer implied sentiment and point toward potential directions for future work, including joint entity dete"
P16-1032,N10-3009,0,0.0248868,"Thus we include document-level 336 features encoding this intuition. For example, the sentence “We’re pleased to put this behind us,” said Michael DuVally implies positive sentiment from DuVally. We extract direct quotations using regular expressions. We include the sentiment label of the direct quotation from the speaker to the entities in it, excluding entities that appear less than three times in the document. We add the sentiment label of the quotation as a feature to (speaker, the most frequent entity) pair as well. To extract indirect quotations, we follow studies (Bethard et al., 2004; Lu, 2010) and use a list of 20 verbs indicating speech events (e.g., say, speak, and announce) to detect direct quotations and their opinion holders. We then add the sentiment label of words connected to ej via a dependency path of length up to two that also includes the subject of quotation verb to ej (e.g. Hassanal said that cooperation between Brunei and China were fruitful). We also include an indicator feature for whether ei is the subject of the quotation verb. 3.2 Document count Avg. sentence count Avg. entity count Avg. mentions / entity MPQA 54 12.7 10.6 2.7 Crowdsourced 914 14.8 8.8 3.5 Table"
P16-1032,P14-5010,0,0.00671592,"stics annotations at the corpus-level (Ellis et al., 2014), by providing document-level annotations for all entity pairs (see Sec. 7 for discussion). 4.1 Document Preprocessing All-pair annotation can be expensive, as there are N 2 pairs to annotate for each document with N entities. We determined that it would be more cost efficient to cover a large number of short documents than a small number of very long documents. We therefore selected articles with less than eleven entities from KBP and less than fifteen from MPQA and took the first 15 sentences for annotation. We used Stanford CoreNLP (Manning et al., 2014) for sentence splitting, part-of-speech tagging, named entity recognition, co-reference resolution and dependency parsing. We discarded entities of type date, duration, money, time and number and merged named entities using several heuristics, such as merging acronyms, merging named entity of person type with the same last name (e.g., Tiger Woods to Woods). We merged names listed as alias in when there is an exact match from Freebase. We included all mentions in a co-reference chain with the named entity, discarding chains with more than one entity. The corpus statistics are shown in Table 2."
P16-1032,W10-0204,0,0.0154027,"irs, the KBP task is formulated as query-focused retrieval of entity sentiment from a large pool of potentially relevant documents. Thus, their annotations focus only on query entities and relatively sparse compared to ours (see Sec. 6). Another recent dataset is MPQA 3.0 (Deng and Wiebe, 2015b), which captures various aspects of sentiment. Their sentiment pair annotations are only at the sentencelevel and are therefore much sparser than we provide (see Sec. 6) for entity-entity relation analysis. Several recent studies focused on various aspects of implied sentiment (Greene and Resnik, 2009; Mohammad and Turney, 2010; Zhang and Liu, 2011; Feng et al., 2013; Deng and Wiebe, 2014; Deng et al., 2014). Deng and Wiebe (2015a) in particular introduced sentiment implicature rules relevant for sentence-level entityentity sentiment. Our work contributes to these recent efforts by presenting a new model and dataset for document-level sentiment inference over all entity pairs. 8 Conclusion We presented an approach to interpreting sentiment among entities in news articles, with global constraints provided by social, faction and discourse context. Experiments demonstrated that the approach can infer implied sentiment"
P16-1032,P13-1174,1,0.841626,"retrieval of entity sentiment from a large pool of potentially relevant documents. Thus, their annotations focus only on query entities and relatively sparse compared to ours (see Sec. 6). Another recent dataset is MPQA 3.0 (Deng and Wiebe, 2015b), which captures various aspects of sentiment. Their sentiment pair annotations are only at the sentencelevel and are therefore much sparser than we provide (see Sec. 6) for entity-entity relation analysis. Several recent studies focused on various aspects of implied sentiment (Greene and Resnik, 2009; Mohammad and Turney, 2010; Zhang and Liu, 2011; Feng et al., 2013; Deng and Wiebe, 2014; Deng et al., 2014). Deng and Wiebe (2015a) in particular introduced sentiment implicature rules relevant for sentence-level entityentity sentiment. Our work contributes to these recent efforts by presenting a new model and dataset for document-level sentiment inference over all entity pairs. 8 Conclusion We presented an approach to interpreting sentiment among entities in news articles, with global constraints provided by social, faction and discourse context. Experiments demonstrated that the approach can infer implied sentiment and point toward potential directions fo"
P16-1032,W04-2401,0,0.0700491,"elationship. 2 A Document-level Sentiment Model Given a news document d, and named entities e1 ,...,en in d, where each entity ei has mentions mi1 · · · mik , the task is to decide directed sentiment between all pairs of entities. We predict the directed sentiment from ei to ej at the document level, i.e., sent(ei →ej ) ∈ {positive, unbiased, negative}, for all ei , ej ∈ d where i 6= j, assuming that sentiment is consistent within the document. We introduce a document-level ILP that includes base models and soft social constraints. ILP has been used successfully for a wide range of NLP tasks (Roth and Yih, 2004), perhaps because they easily support incorporating different types of global constraints. We use two base models: (1) a learned pairwise sentiment classifier (Sec 3.1) that combines sentence- and discourse-level features to make predictions for each entity pair and (2) a pattern-based faction extractor (Sec 3.2) that detects alliances among a subset of the entities. The ILP is solved by maximizing: plicit sentiment, while preserving consistency. The sentiment dynamics in social groups, motivated by social science theories, are encoded as soft ILP constraints. They include a notion of homophil"
P16-1032,N09-1057,0,0.0250358,"alysis over all entity pairs, the KBP task is formulated as query-focused retrieval of entity sentiment from a large pool of potentially relevant documents. Thus, their annotations focus only on query entities and relatively sparse compared to ours (see Sec. 6). Another recent dataset is MPQA 3.0 (Deng and Wiebe, 2015b), which captures various aspects of sentiment. Their sentiment pair annotations are only at the sentencelevel and are therefore much sparser than we provide (see Sec. 6) for entity-entity relation analysis. Several recent studies focused on various aspects of implied sentiment (Greene and Resnik, 2009; Mohammad and Turney, 2010; Zhang and Liu, 2011; Feng et al., 2013; Deng and Wiebe, 2014; Deng et al., 2014). Deng and Wiebe (2015a) in particular introduced sentiment implicature rules relevant for sentence-level entityentity sentiment. Our work contributes to these recent efforts by presenting a new model and dataset for document-level sentiment inference over all entity pairs. 8 Conclusion We presented an approach to interpreting sentiment among entities in news articles, with global constraints provided by social, faction and discourse context. Experiments demonstrated that the approach c"
P16-1032,D13-1170,0,0.00828576,"ver care to work with a future perspective in mind,” Alexei Skripkov of the Federal Medical and Biological Agency said. “It’s a big systemic mistake.” Skripkov never appears together with Russia in any sentence, but he manifests negative sentiment towards it. When a document revolves around a theme (in this example Russia), sentiment is often directed to it without being explicitly mentioned. (Pair) and randomly assigning labels according to their empirical distribution (Random). The first existing method adaptation (Sentence) uses the publicly released sentence-level RNN sentiment model from Socher et al (2013). For each entity pair, we collect sentiment labels from sentences they co-occur in and assign a positive label if a positive-labeled sentence exists, negative if there exists more than one sentence with a negative label and no positives.9 We also report a proxy for doing similar aggregation over a state-of-the-art entity-entity sentiment classifier. Here, because we added our new labels to the original KBP and MPQA3.0 annotations, we can simply predict the union of the original gold annotations using mention string overlap to align the entities (KM Gold). This provides a reasonable upper boun"
P16-1032,R11-1028,0,0.0727201,"Missing"
P16-1032,Q14-1024,0,0.150553,"li (in Figure 2a), we can infer that Russia is negative towards Saakhashvilli (in Figure 2c). When considered in aggregate, these constraints can greatly improve the consistency over the overall document-level predictions. Our work stands in contrast to previous approaches in three aspects. First, we apply social dynamics motivated by social science theories to entity-entity sentiment analysis in unstructured text. In contrast, most previous studies focused on social media or dialogue data with overt social network structure when integrating social dynamics (Tan et al., 2011; Hu et al., 2013; West et al., 2014). Second, we aim to recover sentiment F =ψsocial + ψf act + n X n X ψij i=1 j=1 where F combines soft constraints (ψsocial , ψf act defined in detail in this section) with pairwise potentials ψij defined as: 1 All data will be made publicly available. You can browse it at http://homes.cs.washington. edu/˜eunsol/project_page/acl16, and download it from the author’s webpage. 334 Sentence Canadian Prime Minister Harper. . . . . . Reid, the Democratic leader. . . Goldman spokesman DuVally . . . Djibouti, a key U.S. ally. i Canada Reid Goldman Djibouti j Harper Democratic DuVally U.S. (b) Visual re"
P16-1032,H05-1044,0,0.103269,"Missing"
P16-1032,P13-1161,0,0.0276777,"based on the factual ties among entities (e.g., Moscow, Gryzlov, and Russia probably share the same sentiment towards other entities) and known social context (e.g., Russia probably 333 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 333–343, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics that can be inferred through partial evidence that spans multiple sentences. This complements prior efforts for accessing implied sentiment where the key evidence is, by and large, at the sentence level (Zhang and Liu, 2011; Yang and Cardie, 2013; Deng and Wiebe, 2015a). Finally, we present the first approach to model the relationship between factual and subjective relations. We evaluate the approach on a newly gathered corpus with dense document-level sentiment labels in news articles.1 This data includes comprehensively annotated sentiment between all entity pairs, including those that do not appear together in any single sentence. Experiments demonstrate that the global model significantly improves performance over a pairwise classifier and other strong baselines. We also perform a detailed ablation and error analysis, showing case"
P16-1032,P11-2101,0,0.209066,"ers must be inferred based on the factual ties among entities (e.g., Moscow, Gryzlov, and Russia probably share the same sentiment towards other entities) and known social context (e.g., Russia probably 333 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 333–343, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics that can be inferred through partial evidence that spans multiple sentences. This complements prior efforts for accessing implied sentiment where the key evidence is, by and large, at the sentence level (Zhang and Liu, 2011; Yang and Cardie, 2013; Deng and Wiebe, 2015a). Finally, we present the first approach to model the relationship between factual and subjective relations. We evaluate the approach on a newly gathered corpus with dense document-level sentiment labels in news articles.1 This data includes comprehensively annotated sentiment between all entity pairs, including those that do not appear together in any single sentence. Experiments demonstrate that the global model significantly improves performance over a pairwise classifier and other strong baselines. We also perform a detailed ablation and error"
P16-1032,N16-1033,0,\N,Missing
P16-1167,N04-1015,0,0.0552637,"STM captions having little caption variation because the model learns frequency statistics without any knowledge of latent events. Almost all LSTM captions mention the words bride, wedding, or groom, yielding a very high scenario score for the caption, even if that caption is grammatically incorrect or irrelevant to the image. As expected the raw captions have high relevance to the original image, and they are grammatical, but can be less relevant to the corresponding scenario. 7 Related Work Previous studies have explored unsupervised induction of salient content structure in newswire texts (Barzilay and Lee, 2004), temporal graph representations (Bramsen et al., 2006), and storyline extraction and event summarization (Xu et al., 2013). Another line of research finds the common event structure from children’s stories (McIntyre and Lapata, 2009), where the learned plot structure is used to stochastically generate new stories (Goyal et al., 2010; Goyal et al., 2013). Our work similarly aims to learn the typical temporal patterns and compositional elements that define common scenarios, but with multimodal integration. Compared to studies that learn narrative schemas from natural language (Pichotta and Moon"
P16-1167,W06-1623,0,0.0346578,"model learns frequency statistics without any knowledge of latent events. Almost all LSTM captions mention the words bride, wedding, or groom, yielding a very high scenario score for the caption, even if that caption is grammatically incorrect or irrelevant to the image. As expected the raw captions have high relevance to the original image, and they are grammatical, but can be less relevant to the corresponding scenario. 7 Related Work Previous studies have explored unsupervised induction of salient content structure in newswire texts (Barzilay and Lee, 2004), temporal graph representations (Bramsen et al., 2006), and storyline extraction and event summarization (Xu et al., 2013). Another line of research finds the common event structure from children’s stories (McIntyre and Lapata, 2009), where the learned plot structure is used to stochastically generate new stories (Goyal et al., 2010; Goyal et al., 2013). Our work similarly aims to learn the typical temporal patterns and compositional elements that define common scenarios, but with multimodal integration. Compared to studies that learn narrative schemas from natural language (Pichotta and Mooney, 2014; Jans et al., 2012; Chambers and Jurafsky, 200"
P16-1167,P14-2082,0,0.0171906,"raction and event summarization (Xu et al., 2013). Another line of research finds the common event structure from children’s stories (McIntyre and Lapata, 2009), where the learned plot structure is used to stochastically generate new stories (Goyal et al., 2010; Goyal et al., 2013). Our work similarly aims to learn the typical temporal patterns and compositional elements that define common scenarios, but with multimodal integration. Compared to studies that learn narrative schemas from natural language (Pichotta and Mooney, 2014; Jans et al., 2012; Chambers and Jurafsky, 2009; Chambers, 2013; Cassidy et al., 2014), or compile script knowledge from crowdsourcing (Regneri et al., 2010), our work explores a new source of knowledge that allows grounded event learning with temporal dimensions, resulting in a new dataset of scenario types that are not naturally accessible from newswire or literature. While recent studies have explored videos and photo streams as a source of discovering complex events and learning their sequential patterns (Kim and Xing, 2014; Kim and Xing, 2013; Tang et al., 2012; Tschiatschek et al., 2014), their focus was mostly on the visual modality. Zhang et al. (2015) explored multimod"
P16-1167,P08-1090,0,0.187199,"early artificial intelligence research. Scripts (Schank and Abelson, 1975), an early formalism, were developed to encode the necessary background knowledge to support an inference engine for common sense reasoning in limited domains. However, early approaches based on hand-coded symbolic representations proved to be brittle and difficult to scale. An alternative direction in recent years has been statistical knowledge induction, i.e., learning script or common sense knowledge bottom-up from large-scale data. While most prior work is based on text (Pichotta and Mooney, 2014; Jans et al., 2012; Chambers and Jurafsky, 2008; Chambers, 2013), recent work begins exploring the use of images as well (Bagherinezhad et al., 2016; Vedantam et al., 2015). In this paper, we present the first study for learning knowledge about common life scenarios (e.g., weddings, camping trips) from a large collection of online photo albums with time-stamped images and their captions. The resulting dataset includes 34,818 time-stamped photo albums corresponding to 12 distinct event scenarios with 1.5 million images and captions (see Table 1 for more details). We cast unsupervised learning of event structure as a sequential multimodal cl"
P16-1167,P09-1068,0,0.053877,"ons (Bramsen et al., 2006), and storyline extraction and event summarization (Xu et al., 2013). Another line of research finds the common event structure from children’s stories (McIntyre and Lapata, 2009), where the learned plot structure is used to stochastically generate new stories (Goyal et al., 2010; Goyal et al., 2013). Our work similarly aims to learn the typical temporal patterns and compositional elements that define common scenarios, but with multimodal integration. Compared to studies that learn narrative schemas from natural language (Pichotta and Mooney, 2014; Jans et al., 2012; Chambers and Jurafsky, 2009; Chambers, 2013; Cassidy et al., 2014), or compile script knowledge from crowdsourcing (Regneri et al., 2010), our work explores a new source of knowledge that allows grounded event learning with temporal dimensions, resulting in a new dataset of scenario types that are not naturally accessible from newswire or literature. While recent studies have explored videos and photo streams as a source of discovering complex events and learning their sequential patterns (Kim and Xing, 2014; Kim and Xing, 2013; Tang et al., 2012; Tschiatschek et al., 2014), their focus was mostly on the visual modality"
P16-1167,D13-1185,0,0.138681,"research. Scripts (Schank and Abelson, 1975), an early formalism, were developed to encode the necessary background knowledge to support an inference engine for common sense reasoning in limited domains. However, early approaches based on hand-coded symbolic representations proved to be brittle and difficult to scale. An alternative direction in recent years has been statistical knowledge induction, i.e., learning script or common sense knowledge bottom-up from large-scale data. While most prior work is based on text (Pichotta and Mooney, 2014; Jans et al., 2012; Chambers and Jurafsky, 2008; Chambers, 2013), recent work begins exploring the use of images as well (Bagherinezhad et al., 2016; Vedantam et al., 2015). In this paper, we present the first study for learning knowledge about common life scenarios (e.g., weddings, camping trips) from a large collection of online photo albums with time-stamped images and their captions. The resulting dataset includes 34,818 time-stamped photo albums corresponding to 12 distinct event scenarios with 1.5 million images and captions (see Table 1 for more details). We cast unsupervised learning of event structure as a sequential multimodal clustering prob1769"
P16-1167,N15-1053,1,0.835041,"ing, 2013; Tang et al., 2012; Tschiatschek et al., 2014), their focus was mostly on the visual modality. Zhang et al. (2015) explored multimodal information extraction focusing specifically on identifying video clips that referred to the same event in television news. This contrasts to the goal of our study that aims to learn the temporal structure by which common scenarios unfold. Integrating language and vision has attracted increasing attention in recent years across diverse tasks such as image captioning (Karpathy and FeiFei, 2015; Vinyals et al., 2015; Fang et al., 2015; Xu et al., 2015; Chen et al., 2015), cross modal semantic modeling (Lazaridou et al., 2015), information extraction (Morency et al., 2011; Rosas et al., 2013; Zhang et al., 2015; Izadinia et al., 2015), common-sense knowledge (Vedantam et al., 2015; Bagherinezhad et al., 2016), and visual storytelling (Huang et al., 2016). Our work is similar to both common sense knowledge learning and visual story completion. Our model learns commonsense knowledge on the hierarchical and temporal event structure from scenario-specific multimodal photo albums, which can be viewed as visual stories about common life events. Recent work focused o"
P16-1167,D10-1008,0,0.0625052,"Missing"
P16-1167,N16-1147,0,0.0392715,"the goal of our study that aims to learn the temporal structure by which common scenarios unfold. Integrating language and vision has attracted increasing attention in recent years across diverse tasks such as image captioning (Karpathy and FeiFei, 2015; Vinyals et al., 2015; Fang et al., 2015; Xu et al., 2015; Chen et al., 2015), cross modal semantic modeling (Lazaridou et al., 2015), information extraction (Morency et al., 2011; Rosas et al., 2013; Zhang et al., 2015; Izadinia et al., 2015), common-sense knowledge (Vedantam et al., 2015; Bagherinezhad et al., 2016), and visual storytelling (Huang et al., 2016). Our work is similar to both common sense knowledge learning and visual story completion. Our model learns commonsense knowledge on the hierarchical and temporal event structure from scenario-specific multimodal photo albums, which can be viewed as visual stories about common life events. Recent work focused on photo album summarization using visual (Sadeghi et al., 2015) and multimodal representations (Sinha et al., 2011). Our work identifies the nature of common events in scenarios and learns their timelines and characteristic forms. 8 Conclusion We introduce a novel exploration to learn sc"
P16-1167,E12-1034,0,0.513523,"arios goes back to early artificial intelligence research. Scripts (Schank and Abelson, 1975), an early formalism, were developed to encode the necessary background knowledge to support an inference engine for common sense reasoning in limited domains. However, early approaches based on hand-coded symbolic representations proved to be brittle and difficult to scale. An alternative direction in recent years has been statistical knowledge induction, i.e., learning script or common sense knowledge bottom-up from large-scale data. While most prior work is based on text (Pichotta and Mooney, 2014; Jans et al., 2012; Chambers and Jurafsky, 2008; Chambers, 2013), recent work begins exploring the use of images as well (Bagherinezhad et al., 2016; Vedantam et al., 2015). In this paper, we present the first study for learning knowledge about common life scenarios (e.g., weddings, camping trips) from a large collection of online photo albums with time-stamped images and their captions. The resulting dataset includes 34,818 time-stamped photo albums corresponding to 12 distinct event scenarios with 1.5 million images and captions (see Table 1 for more details). We cast unsupervised learning of event structure"
P16-1167,N15-1016,0,0.0294687,"2014), their focus was mostly on the visual modality. Zhang et al. (2015) explored multimodal information extraction focusing specifically on identifying video clips that referred to the same event in television news. This contrasts to the goal of our study that aims to learn the temporal structure by which common scenarios unfold. Integrating language and vision has attracted increasing attention in recent years across diverse tasks such as image captioning (Karpathy and FeiFei, 2015; Vinyals et al., 2015; Fang et al., 2015; Xu et al., 2015; Chen et al., 2015), cross modal semantic modeling (Lazaridou et al., 2015), information extraction (Morency et al., 2011; Rosas et al., 2013; Zhang et al., 2015; Izadinia et al., 2015), common-sense knowledge (Vedantam et al., 2015; Bagherinezhad et al., 2016), and visual storytelling (Huang et al., 2016). Our work is similar to both common sense knowledge learning and visual story completion. Our model learns commonsense knowledge on the hierarchical and temporal event structure from scenario-specific multimodal photo albums, which can be viewed as visual stories about common life events. Recent work focused on photo album summarization using visual (Sadeghi et al."
P16-1167,P13-2109,0,0.0331585,"t tend to be relatively uninformative about specific events. This cluster is excluded when computing temporal knowledge probabilities (Section 4.2). The visual and textual representations of an event are computed using the average of the visual and textual features, respectively, of photos assigned to that event. We compute each textual affinity Aci,k in the event assignment scores (Equation 3) as the cosine similarity between the textual features of the caption for photo pi and the textual representation of event ek . For textual features, we extract noun and verb unigrams using TurboTagger (Martins et al., 2013) and weigh them by their discriminativeness relative to their scenario, P (S|w). Given scenario S and word w, P (S|w) is defined as the number of albums for the scenario the word occurs in divided by the total number of albums in that scenario. The visual affinity Avi,k is the similarity between the visual features of photo pi and the visual representation of event ek . For visual features, we use the convolutional features from the final layer activations of the 16-layer VGGNet model (Simonyan and Zisserman, 2015). 4.2 Learned Event Representation Notre Dame Temporal Knowledge Local transitio"
P16-1167,P09-1025,0,0.0899443,"core for the caption, even if that caption is grammatically incorrect or irrelevant to the image. As expected the raw captions have high relevance to the original image, and they are grammatical, but can be less relevant to the corresponding scenario. 7 Related Work Previous studies have explored unsupervised induction of salient content structure in newswire texts (Barzilay and Lee, 2004), temporal graph representations (Bramsen et al., 2006), and storyline extraction and event summarization (Xu et al., 2013). Another line of research finds the common event structure from children’s stories (McIntyre and Lapata, 2009), where the learned plot structure is used to stochastically generate new stories (Goyal et al., 2010; Goyal et al., 2013). Our work similarly aims to learn the typical temporal patterns and compositional elements that define common scenarios, but with multimodal integration. Compared to studies that learn narrative schemas from natural language (Pichotta and Mooney, 2014; Jans et al., 2012; Chambers and Jurafsky, 2009; Chambers, 2013; Cassidy et al., 2014), or compile script knowledge from crowdsourcing (Regneri et al., 2010), our work explores a new source of knowledge that allows grounded e"
P16-1167,E14-1024,0,0.329966,"l patterns in everyday scenarios goes back to early artificial intelligence research. Scripts (Schank and Abelson, 1975), an early formalism, were developed to encode the necessary background knowledge to support an inference engine for common sense reasoning in limited domains. However, early approaches based on hand-coded symbolic representations proved to be brittle and difficult to scale. An alternative direction in recent years has been statistical knowledge induction, i.e., learning script or common sense knowledge bottom-up from large-scale data. While most prior work is based on text (Pichotta and Mooney, 2014; Jans et al., 2012; Chambers and Jurafsky, 2008; Chambers, 2013), recent work begins exploring the use of images as well (Bagherinezhad et al., 2016; Vedantam et al., 2015). In this paper, we present the first study for learning knowledge about common life scenarios (e.g., weddings, camping trips) from a large collection of online photo albums with time-stamped images and their captions. The resulting dataset includes 34,818 time-stamped photo albums corresponding to 12 distinct event scenarios with 1.5 million images and captions (see Table 1 for more details). We cast unsupervised learning"
P16-1167,P10-1100,0,0.108494,"arch finds the common event structure from children’s stories (McIntyre and Lapata, 2009), where the learned plot structure is used to stochastically generate new stories (Goyal et al., 2010; Goyal et al., 2013). Our work similarly aims to learn the typical temporal patterns and compositional elements that define common scenarios, but with multimodal integration. Compared to studies that learn narrative schemas from natural language (Pichotta and Mooney, 2014; Jans et al., 2012; Chambers and Jurafsky, 2009; Chambers, 2013; Cassidy et al., 2014), or compile script knowledge from crowdsourcing (Regneri et al., 2010), our work explores a new source of knowledge that allows grounded event learning with temporal dimensions, resulting in a new dataset of scenario types that are not naturally accessible from newswire or literature. While recent studies have explored videos and photo streams as a source of discovering complex events and learning their sequential patterns (Kim and Xing, 2014; Kim and Xing, 2013; Tang et al., 2012; Tschiatschek et al., 2014), their focus was mostly on the visual modality. Zhang et al. (2015) explored multimodal information extraction focusing specifically on identifying video cl"
P16-1167,W04-2401,0,0.0743044,"ions by extracting captions whose lemmatized forms are frequently observed throughout multiple albums in the scenario. Sample events and their prototypical captions from three scenarios are displayed in Table 2. 5 Experimental Setup Data split. For scenarios with more than 1000 albums, we use 100 albums for each of the development and test sets and use the rest for training. For scenarios with less than 1000 albums, we use 50 albums for each of the development and test sets, and the rest for training. Implementation details. We optimize our objective function using integer linear programming (Roth and Yih, 2004) with the Gurobi solver (Inc., 2015). For computational efficiency, temporally close sets of consecutive photos are treated as one unit during the optimization. We use these units to reduce the number of variables and constraints in the model from a function of the number of photos to a function of the number of units. We form these units heuristically by merging images agglomeratively when their timestamps are within a certain range of the closest image in a unit. When merging photos, the textual affinity of each unit for a particular event is the maximum affinity for that event among photos"
P16-1167,D15-1020,0,0.0305678,"Chambers, 2013; Cassidy et al., 2014), or compile script knowledge from crowdsourcing (Regneri et al., 2010), our work explores a new source of knowledge that allows grounded event learning with temporal dimensions, resulting in a new dataset of scenario types that are not naturally accessible from newswire or literature. While recent studies have explored videos and photo streams as a source of discovering complex events and learning their sequential patterns (Kim and Xing, 2014; Kim and Xing, 2013; Tang et al., 2012; Tschiatschek et al., 2014), their focus was mostly on the visual modality. Zhang et al. (2015) explored multimodal information extraction focusing specifically on identifying video clips that referred to the same event in television news. This contrasts to the goal of our study that aims to learn the temporal structure by which common scenarios unfold. Integrating language and vision has attracted increasing attention in recent years across diverse tasks such as image captioning (Karpathy and FeiFei, 2015; Vinyals et al., 2015; Fang et al., 2015; Xu et al., 2015; Chen et al., 2015), cross modal semantic modeling (Lazaridou et al., 2015), information extraction (Morency et al., 2011; Ro"
P16-1167,D13-1127,0,0.0294331,"Almost all LSTM captions mention the words bride, wedding, or groom, yielding a very high scenario score for the caption, even if that caption is grammatically incorrect or irrelevant to the image. As expected the raw captions have high relevance to the original image, and they are grammatical, but can be less relevant to the corresponding scenario. 7 Related Work Previous studies have explored unsupervised induction of salient content structure in newswire texts (Barzilay and Lee, 2004), temporal graph representations (Bramsen et al., 2006), and storyline extraction and event summarization (Xu et al., 2013). Another line of research finds the common event structure from children’s stories (McIntyre and Lapata, 2009), where the learned plot structure is used to stochastically generate new stories (Goyal et al., 2010; Goyal et al., 2013). Our work similarly aims to learn the typical temporal patterns and compositional elements that define common scenarios, but with multimodal integration. Compared to studies that learn narrative schemas from natural language (Pichotta and Mooney, 2014; Jans et al., 2012; Chambers and Jurafsky, 2009; Chambers, 2013; Cassidy et al., 2014), or compile script knowledg"
P17-1014,D15-1198,1,0.896171,"Missing"
P17-1014,P05-1045,0,0.0446321,"c) named entity clustering, and (d) insertion of scope markers. group, :ARG2-of, :ARG1-of, :ARG0.4 The order traverses children in the sequence they are presented in the AMR. We consider alternative orderings of children in Section 7 but always follow the pattern demonstrated above. ation, we render the corresponding format when predicted. Figure 2(b) contains an example of all preprocessing up to this stage. Named Entity Clusters When performing AMR generation, each of the AMR fine-grained entity types is manually mapped to one of the four coarse entity types used in the Stanford NER system (Finkel et al., 2005): person, location, organization and misc. This reduces the sparsity associated with many rarely occurring entity types. Figure 2 (c) contains an example with named entity clusters. Rendering Function Our rendering function marks scope, and generates tokens following the pre-order traversal of the graph: (1) if the element is a node, it emits the type of the node. (2) if the element is an edge, it emits the type of the edge and then recursively emits a bracketed string for the (concept) node immediately after it. In case the node has only one child we omit the scope markers (denoted with left"
P17-1014,N16-1087,0,0.360548,"ifying the contributions that come from preprocessing and the paired training procedure. 2 Neural Parsing Recently there have been a few seq2seq systems for AMR parsing (Barzdins and Gosko, 2016; Peng et al., 2017). Similar to our approach, Peng et al. (2017) deal with sparsity by anonymizing named entities and typing low frequency words, resulting in a very compact vocabulary (2k tokens). However, we avoid reducing our vocabulary by introducing a large set of unlabeled sentences from an external corpus, therefore drastically lowering the out-of-vocabulary rate (see Section 6). AMR Generation Flanigan et al. (2016) specify a number of tree-to-string transduction rules based on alignments and POS-based features that are used to drive a tree-based SMT system. Pourdamghani et al. (2016) also use an MT decoder; they learn a classifier that linearizes the input AMR graph in an order that follows the output sentence, effectively reducing the number of alignment crossings of the phrase-based decoder. Song et al. (2016) recast generation as a traveling salesman problem, after partitioning the graph into fragments and finding the best linearization order. Our models do not need to rely on a particular linearizat"
P17-1014,S16-1176,0,0.461842,"mance of BLEU 33.8. We present extensive ablative and qualitative analysis including strong evidence that sequencebased AMR models are robust against ordering variations of graph-to-sequence conversions. 1 and * op1 elect.01 op2 celebrate.01 ARG0 ARG0 poss name name person op1 person ARG0-of Obama vote.01 Figure 1: An example sentence and its corresponding Abstract Meaning Representation (AMR). AMR encodes semantic dependencies between entities mentioned in the sentence, such as “Obama” being the “arg0” of the verb “elected”. of neural network models (Misra and Artzi, 2016; Peng et al., 2017; Barzdins and Gosko, 2016). In this work, we present the first successful sequence-to-sequence (seq2seq) models that achieve strong results for both text-to-AMR parsing and AMR-to-text generation. Seq2seq models have been broadly successful in many other applications (Wu et al., 2016; Bahdanau et al., 2015; Luong et al., 2015; Vinyals et al., 2015). However, their application to AMR has been limited, in part because effective linearization (encoding graphs as linear sequences) and data sparsity were thought to pose significant challenges. We show that these challenges can be easily overcome, by demonstrating that seq2s"
P17-1014,P14-1134,0,0.507344,"Missing"
P17-1014,bender-2014-language,0,0.0121142,"ucially, we avoid relying on resources such as knowledge bases and externally trained parsers. We achieve competitive results for the parsing task (SMATCH 62.1) and state-of-theart performance for generation (BLEU 33.8). For future work, we would like to extend our work to different meaning representations such as the Minimal Recursion Semantics (MRS; Copestake et al. (2005)). This formalism tackles certain linguistic phenomena differently from AMR (e.g., negation, and co-reference), contains explicit annotation on concepts for number, tense and case, and finally handles multiple languages10 (Bender, 2014). Taking a step further, we would like to apply our models on Semantics-Based Machine Translation using MRS as an intermediate representation between pairs of languages, and investigate the added benefit compared to directly translating the surface strings, especially in the case of distant language pairs such as English and Japanese (Siegel, 2000). state :arg0 report :arg1 ( obligate :arg1 ( government-organization :arg0-of ( govern :arg1 loc_0 ) ) :arg2 ( help :arg1 ( and :op1 ( stabilize :arg1 ( state :mod weak ) ) :op2 ( push :arg1 ( regulate :mod international :arg0-of ( stop :arg1 terror"
P17-1014,S16-1182,0,0.0387204,"et al., 2016). While AMR allows for rich semantic representation, annotating training data in AMR is expensive, which in turn limits the use 146 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 146–157 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1014 Brandt et al. (2016), Puzikov et al. (2016), and Goodman et al. (2016). Artzi et al. (2015) use a grammar induction approach with Combinatory Categorical Grammar (CCG), which relies on pretrained CCGBank categories, like Bjerva et al. (2016). Pust et al. (2015) recast parsing as a string-to-tree Machine Translation problem, using unsupervised alignments (Pourdamghani et al., 2014), and employing several external semantic resources. Our neural approach is engineering lean, relying only on a large unannotated corpus of English and algorithms to find and canonicalize named entities. bootstrap a high quality AMR parser from millions of unlabeled Gigaword sentences (Napoles et al., 2012) and then use the automatically parsed AMR graphs to pre-train an AMR generator. This paired training allows both the parser and generator to learn hi"
P17-1014,S16-1180,0,0.0589977,"ications including machine translation (MT) (Jones et al., 2012), summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). While AMR allows for rich semantic representation, annotating training data in AMR is expensive, which in turn limits the use 146 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 146–157 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1014 Brandt et al. (2016), Puzikov et al. (2016), and Goodman et al. (2016). Artzi et al. (2015) use a grammar induction approach with Combinatory Categorical Grammar (CCG), which relies on pretrained CCGBank categories, like Bjerva et al. (2016). Pust et al. (2015) recast parsing as a string-to-tree Machine Translation problem, using unsupervised alignments (Pourdamghani et al., 2014), and employing several external semantic resources. Our neural approach is engineering lean, relying only on a large unannotated corpus of English and algorithms to find and canonicalize named entities. bootstrap a high quality AMR parser from millions of unlabeled Gigaword sentences ("
P17-1014,S16-1179,0,0.037075,"ermediate meaning representation for several applications including machine translation (MT) (Jones et al., 2012), summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). While AMR allows for rich semantic representation, annotating training data in AMR is expensive, which in turn limits the use 146 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 146–157 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1014 Brandt et al. (2016), Puzikov et al. (2016), and Goodman et al. (2016). Artzi et al. (2015) use a grammar induction approach with Combinatory Categorical Grammar (CCG), which relies on pretrained CCGBank categories, like Bjerva et al. (2016). Pust et al. (2015) recast parsing as a string-to-tree Machine Translation problem, using unsupervised alignments (Pourdamghani et al., 2014), and employing several external semantic resources. Our neural approach is engineering lean, relying only on a large unannotated corpus of English and algorithms to find and canonicalize named entities. bootstrap a high quality AMR pars"
P17-1014,P16-1025,0,0.0126843,"t enhances both the text-to-AMR parser and AMR-to-text generator. More concretely, first we use self-training to Introduction Abstract Meaning Representation (AMR) is a semantic formalism to encode the meaning of natural language text. As shown in Figure 1, AMR represents the meaning using a directed graph while abstracting away the surface forms in text. AMR has been used as an intermediate meaning representation for several applications including machine translation (MT) (Jones et al., 2012), summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). While AMR allows for rich semantic representation, annotating training data in AMR is expensive, which in turn limits the use 146 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 146–157 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1014 Brandt et al. (2016), Puzikov et al. (2016), and Goodman et al. (2016). Artzi et al. (2015) use a grammar induction approach with Combinatory Categorical Grammar (CCG), which relies on pretrained CCGBank categories, like Bjerva et al."
P17-1014,P13-2131,0,0.267935,"M (Barzdins and Gosko, 2016) Prec 72.3 67.2 62.2 61.9 59.7 54.9 - Dev Rec 61.4 65.1 66.0 64.8 62.9 60.0 - F1 69.0 66.6 66.1 64.4 63.3 61.3 57.4 - Prec 70.4 66.8 64.0 59.7 60.2 57.8 53.1 55.0 - Test Rec 63.1 65.7 53.0 64.7 63.6 60.9 58.1 50.0 - F1 67.1 66.5 66.3 58.0 62.1 61.9 59.3 55.5 52.0 43.0 Table 1: SMATCH scores for AMR Parsing. *Reported numbers are on the newswire portion of a previous release of the corpus (LDC2014T12). Corpus AMR G IGA-200k G IGA-2M G IGA-20M summarizes statistics about the original dataset and the extracted portions of Gigaword. We evaluate AMR parsing with SMATCH (Cai and Knight, 2013), and AMR generation using BLEU (Papineni et al., 2002)5 . We validated word embedding sizes and RNN hidden representation sizes by maximizing AMR development set performance (Algorithm 1 – line 1). We searched over the set {128, 256, 500, 1024} for the best combinations of sizes and set both to 500. Models were trained by optimizing cross-entropy loss with stochastic gradient descent, using a batch size of 100 and dropout rate of 0.5. Across all models when performance does not improve on the AMR dev set, we decay the learning rate by 0.8. For the initial parser trained on the AMR corpus, (Al"
P17-1014,C12-1083,0,0.153954,"Missing"
P17-1014,N15-1114,0,0.0887244,". Our approach is two-fold. First, we introduce a novel paired training procedure that enhances both the text-to-AMR parser and AMR-to-text generator. More concretely, first we use self-training to Introduction Abstract Meaning Representation (AMR) is a semantic formalism to encode the meaning of natural language text. As shown in Figure 1, AMR represents the meaning using a directed graph while abstracting away the surface forms in text. AMR has been used as an intermediate meaning representation for several applications including machine translation (MT) (Jones et al., 2012), summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). While AMR allows for rich semantic representation, annotating training data in AMR is expensive, which in turn limits the use 146 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 146–157 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1014 Brandt et al. (2016), Puzikov et al. (2016), and Goodman et al. (2016). Artzi et al. (2015) use a grammar induction approach with Combinatory Categ"
P17-1014,W16-6603,0,0.309947,"(Barzdins and Gosko, 2016; Peng et al., 2017). Similar to our approach, Peng et al. (2017) deal with sparsity by anonymizing named entities and typing low frequency words, resulting in a very compact vocabulary (2k tokens). However, we avoid reducing our vocabulary by introducing a large set of unlabeled sentences from an external corpus, therefore drastically lowering the out-of-vocabulary rate (see Section 6). AMR Generation Flanigan et al. (2016) specify a number of tree-to-string transduction rules based on alignments and POS-based features that are used to drive a tree-based SMT system. Pourdamghani et al. (2016) also use an MT decoder; they learn a classifier that linearizes the input AMR graph in an order that follows the output sentence, effectively reducing the number of alignment crossings of the phrase-based decoder. Song et al. (2016) recast generation as a traveling salesman problem, after partitioning the graph into fragments and finding the best linearization order. Our models do not need to rely on a particular linearization of the input, attaining comparable performance even with a per example random traversal of the graph. Finally, all three systems intersect with a large language model t"
P17-1014,D15-1136,0,0.106718,"MR allows for rich semantic representation, annotating training data in AMR is expensive, which in turn limits the use 146 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 146–157 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1014 Brandt et al. (2016), Puzikov et al. (2016), and Goodman et al. (2016). Artzi et al. (2015) use a grammar induction approach with Combinatory Categorical Grammar (CCG), which relies on pretrained CCGBank categories, like Bjerva et al. (2016). Pust et al. (2015) recast parsing as a string-to-tree Machine Translation problem, using unsupervised alignments (Pourdamghani et al., 2014), and employing several external semantic resources. Our neural approach is engineering lean, relying only on a large unannotated corpus of English and algorithms to find and canonicalize named entities. bootstrap a high quality AMR parser from millions of unlabeled Gigaword sentences (Napoles et al., 2012) and then use the automatically parsed AMR graphs to pre-train an AMR generator. This paired training allows both the parser and generator to learn high quality represent"
P17-1014,D15-1166,0,0.261978,"igure 1: An example sentence and its corresponding Abstract Meaning Representation (AMR). AMR encodes semantic dependencies between entities mentioned in the sentence, such as “Obama” being the “arg0” of the verb “elected”. of neural network models (Misra and Artzi, 2016; Peng et al., 2017; Barzdins and Gosko, 2016). In this work, we present the first successful sequence-to-sequence (seq2seq) models that achieve strong results for both text-to-AMR parsing and AMR-to-text generation. Seq2seq models have been broadly successful in many other applications (Wu et al., 2016; Bahdanau et al., 2015; Luong et al., 2015; Vinyals et al., 2015). However, their application to AMR has been limited, in part because effective linearization (encoding graphs as linear sequences) and data sparsity were thought to pose significant challenges. We show that these challenges can be easily overcome, by demonstrating that seq2seq models can be trained using any graph-isomorphic linearization and that unlabeled text can be used to significantly reduce sparsity. Our approach is two-fold. First, we introduce a novel paired training procedure that enhances both the text-to-AMR parser and AMR-to-text generator. More concretely,"
P17-1014,D16-1183,0,0.0137661,"establishes a new state-of-the-art performance of BLEU 33.8. We present extensive ablative and qualitative analysis including strong evidence that sequencebased AMR models are robust against ordering variations of graph-to-sequence conversions. 1 and * op1 elect.01 op2 celebrate.01 ARG0 ARG0 poss name name person op1 person ARG0-of Obama vote.01 Figure 1: An example sentence and its corresponding Abstract Meaning Representation (AMR). AMR encodes semantic dependencies between entities mentioned in the sentence, such as “Obama” being the “arg0” of the verb “elected”. of neural network models (Misra and Artzi, 2016; Peng et al., 2017; Barzdins and Gosko, 2016). In this work, we present the first successful sequence-to-sequence (seq2seq) models that achieve strong results for both text-to-AMR parsing and AMR-to-text generation. Seq2seq models have been broadly successful in many other applications (Wu et al., 2016; Bahdanau et al., 2015; Luong et al., 2015; Vinyals et al., 2015). However, their application to AMR has been limited, in part because effective linearization (encoding graphs as linear sequences) and data sparsity were thought to pose significant challenges. We show that these challenges can b"
P17-1014,S16-1178,0,0.0388043,"esentation for several applications including machine translation (MT) (Jones et al., 2012), summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). While AMR allows for rich semantic representation, annotating training data in AMR is expensive, which in turn limits the use 146 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 146–157 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1014 Brandt et al. (2016), Puzikov et al. (2016), and Goodman et al. (2016). Artzi et al. (2015) use a grammar induction approach with Combinatory Categorical Grammar (CCG), which relies on pretrained CCGBank categories, like Bjerva et al. (2016). Pust et al. (2015) recast parsing as a string-to-tree Machine Translation problem, using unsupervised alignments (Pourdamghani et al., 2014), and employing several external semantic resources. Our neural approach is engineering lean, relying only on a large unannotated corpus of English and algorithms to find and canonicalize named entities. bootstrap a high quality AMR parser from millions of unl"
P17-1014,W12-3018,0,0.0220269,"Missing"
P17-1014,P16-1009,0,0.0228418,"external corpus. Related Work Alignment-based Parsing Flanigan et al. (2014) (JAMR) pipeline concept and relation identification with a graph-based algorithm. Zhou et al. (2016) extend JAMR by performing the concept and relation identification tasks jointly with an incremental model. Both systems rely on features based on a set of alignments produced using bi-lexical cues and hand-written rules. In contrast, our models train directly on parallel corpora, and make only minimal use of alignments to anonymize named entities. Data Augmentation Our paired training procedure is largely inspired by Sennrich et al. (2016). They improve neural MT performance for low resource language pairs by using a back-translation MT system for a large monolingual corpus of the target language in order to create synthetic output, Grammar-based Parsing Wang et al. (2016) (CAMR) perform a series of shift-reduce transformations on the output of an externally-trained dependency parser, similar to Damonte et al. (2017), 147 et al., 2016).1 Our model uses a global attention decoder and unknown word replacement with small modifications (Luong et al., 2015). The model uses a stacked bidirectional-LSTM encoder to encode an input sequ"
P17-1014,J05-1004,0,0.015856,"e use (section 3.2), graph-to-sequence conversion (section 3.3), and our paired training procedure (section 3.4). 3.1 Tasks We assume access to a training dataset D where each example pairs a natural language sentence s with an AMR a. The AMR is a rooted directed acylical graph. It contains nodes whose names correspond to sense-identified verbs, nouns, or AMR specific concepts, for example elect.01, Obama, and person in Figure 1. One of these nodes is a distinguished root, for example, the node and in Figure 1. Furthermore, the graph contains labeled edges, which correspond to PropBank-style (Palmer et al., 2005) semantic roles for verbs or other relations introduced for AMR, for example, arg0 or op1 in Figure 1. The set of node and edge names in an AMR graph is drawn from a set of tokens C, and every word in a sentence is drawn from a vocabulary W . We study the task of training an AMR parser, i.e., finding a set of parameters θP for model f , that predicts an AMR graph a ˆ, given a sentence s: a ˆ = argmax f a|s; θP a  3.3 Our seq2seq models require that both the input and target be presented as a linear sequence of tokens. We define a linearization order for an AMR graph as any sequence of its nod"
P17-1014,D16-1224,0,0.150248,"d reducing our vocabulary by introducing a large set of unlabeled sentences from an external corpus, therefore drastically lowering the out-of-vocabulary rate (see Section 6). AMR Generation Flanigan et al. (2016) specify a number of tree-to-string transduction rules based on alignments and POS-based features that are used to drive a tree-based SMT system. Pourdamghani et al. (2016) also use an MT decoder; they learn a classifier that linearizes the input AMR graph in an order that follows the output sentence, effectively reducing the number of alignment crossings of the phrase-based decoder. Song et al. (2016) recast generation as a traveling salesman problem, after partitioning the graph into fragments and finding the best linearization order. Our models do not need to rely on a particular linearization of the input, attaining comparable performance even with a per example random traversal of the graph. Finally, all three systems intersect with a large language model trained on Gigaword. We show that our seq2seq model has the capacity to learn the same information as a language model, especially after pretraining on the external corpus. Related Work Alignment-based Parsing Flanigan et al. (2014) ("
P17-1014,P02-1040,0,0.101398,"59.7 54.9 - Dev Rec 61.4 65.1 66.0 64.8 62.9 60.0 - F1 69.0 66.6 66.1 64.4 63.3 61.3 57.4 - Prec 70.4 66.8 64.0 59.7 60.2 57.8 53.1 55.0 - Test Rec 63.1 65.7 53.0 64.7 63.6 60.9 58.1 50.0 - F1 67.1 66.5 66.3 58.0 62.1 61.9 59.3 55.5 52.0 43.0 Table 1: SMATCH scores for AMR Parsing. *Reported numbers are on the newswire portion of a previous release of the corpus (LDC2014T12). Corpus AMR G IGA-200k G IGA-2M G IGA-20M summarizes statistics about the original dataset and the extracted portions of Gigaword. We evaluate AMR parsing with SMATCH (Cai and Knight, 2013), and AMR generation using BLEU (Papineni et al., 2002)5 . We validated word embedding sizes and RNN hidden representation sizes by maximizing AMR development set performance (Algorithm 1 – line 1). We searched over the set {128, 256, 500, 1024} for the best combinations of sizes and set both to 500. Models were trained by optimizing cross-entropy loss with stochastic gradient descent, using a batch size of 100 and dropout rate of 0.5. Across all models when performance does not improve on the AMR dev set, we decay the learning rate by 0.8. For the initial parser trained on the AMR corpus, (Algorithm 1 – line 1), we use a single stack version of o"
P17-1014,D16-1112,0,0.02967,"roduce a novel paired training procedure that enhances both the text-to-AMR parser and AMR-to-text generator. More concretely, first we use self-training to Introduction Abstract Meaning Representation (AMR) is a semantic formalism to encode the meaning of natural language text. As shown in Figure 1, AMR represents the meaning using a directed graph while abstracting away the surface forms in text. AMR has been used as an intermediate meaning representation for several applications including machine translation (MT) (Jones et al., 2012), summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). While AMR allows for rich semantic representation, annotating training data in AMR is expensive, which in turn limits the use 146 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 146–157 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1014 Brandt et al. (2016), Puzikov et al. (2016), and Goodman et al. (2016). Artzi et al. (2015) use a grammar induction approach with Combinatory Categorical Grammar (CCG), which relies on pretra"
P17-1014,E17-1035,0,0.732145,"e-of-the-art performance of BLEU 33.8. We present extensive ablative and qualitative analysis including strong evidence that sequencebased AMR models are robust against ordering variations of graph-to-sequence conversions. 1 and * op1 elect.01 op2 celebrate.01 ARG0 ARG0 poss name name person op1 person ARG0-of Obama vote.01 Figure 1: An example sentence and its corresponding Abstract Meaning Representation (AMR). AMR encodes semantic dependencies between entities mentioned in the sentence, such as “Obama” being the “arg0” of the verb “elected”. of neural network models (Misra and Artzi, 2016; Peng et al., 2017; Barzdins and Gosko, 2016). In this work, we present the first successful sequence-to-sequence (seq2seq) models that achieve strong results for both text-to-AMR parsing and AMR-to-text generation. Seq2seq models have been broadly successful in many other applications (Wu et al., 2016; Bahdanau et al., 2015; Luong et al., 2015; Vinyals et al., 2015). However, their application to AMR has been limited, in part because effective linearization (encoding graphs as linear sequences) and data sparsity were thought to pose significant challenges. We show that these challenges can be easily overcome,"
P17-1014,D16-1065,0,0.0687239,"into fragments and finding the best linearization order. Our models do not need to rely on a particular linearization of the input, attaining comparable performance even with a per example random traversal of the graph. Finally, all three systems intersect with a large language model trained on Gigaword. We show that our seq2seq model has the capacity to learn the same information as a language model, especially after pretraining on the external corpus. Related Work Alignment-based Parsing Flanigan et al. (2014) (JAMR) pipeline concept and relation identification with a graph-based algorithm. Zhou et al. (2016) extend JAMR by performing the concept and relation identification tasks jointly with an incremental model. Both systems rely on features based on a set of alignments produced using bi-lexical cues and hand-written rules. In contrast, our models train directly on parallel corpora, and make only minimal use of alignments to anonymize named entities. Data Augmentation Our paired training procedure is largely inspired by Sennrich et al. (2016). They improve neural MT performance for low resource language pairs by using a back-translation MT system for a large monolingual corpus of the target lang"
P17-1014,P07-2045,0,\N,Missing
P17-1014,D14-1048,0,\N,Missing
P17-1014,S16-1181,0,\N,Missing
P17-1014,E17-1051,0,\N,Missing
P17-1025,W13-3515,0,0.0543451,"Related work Several works straddle the gap between IE, knowledge base completion, and learning commonsense knowledge from text. Earlier works in these areas use large amounts of text to try to extract general statements like “A THING CAN BE READABLE” (Gordon et al., 2010) and frequencies of events (Gordon and Schubert, 2012). Our work focuses on specific domains of knowledge rather than general statements or occurrence statistics, and develops a frame-centric approach to circumvent reporting bias. Other work uses a knowledge base and scores unseen tuples based on similarity to existing ones (Angeli and Manning, 2013; Li et al., 2016). Relatedly, previous work uses natural language inference to infer new facts from a dataset of commonsense facts that can be extracted from unstructured text (Angeli and Manning, 2014). In contrast, we focus on a small number of specific types of knowledge without access to an existing database of knowledge. A number of recent works combine multimodal input to learn visual attributes (Bruni et al., 2012; Silberer et al., 2013), extract commonsense 8 Conclusion We presented a novel take on verb-centric frame semantics to learn implied physical knowledge latent in verbs. Empir"
P17-1025,D14-1059,0,0.0237386,"general statements like “A THING CAN BE READABLE” (Gordon et al., 2010) and frequencies of events (Gordon and Schubert, 2012). Our work focuses on specific domains of knowledge rather than general statements or occurrence statistics, and develops a frame-centric approach to circumvent reporting bias. Other work uses a knowledge base and scores unseen tuples based on similarity to existing ones (Angeli and Manning, 2013; Li et al., 2016). Relatedly, previous work uses natural language inference to infer new facts from a dataset of commonsense facts that can be extracted from unstructured text (Angeli and Manning, 2014). In contrast, we focus on a small number of specific types of knowledge without access to an existing database of knowledge. A number of recent works combine multimodal input to learn visual attributes (Bruni et al., 2012; Silberer et al., 2013), extract commonsense 8 Conclusion We presented a novel take on verb-centric frame semantics to learn implied physical knowledge latent in verbs. Empirical results confirm that by modeling changes in physical attributes entailed by verbs together with objects that exhibit these properties, we are able to better infer new knowledge in both domains. Ackn"
P17-1025,P98-1013,0,0.226919,"Missing"
P17-1025,P12-1015,0,0.0312334,"ics, and develops a frame-centric approach to circumvent reporting bias. Other work uses a knowledge base and scores unseen tuples based on similarity to existing ones (Angeli and Manning, 2013; Li et al., 2016). Relatedly, previous work uses natural language inference to infer new facts from a dataset of commonsense facts that can be extracted from unstructured text (Angeli and Manning, 2014). In contrast, we focus on a small number of specific types of knowledge without access to an existing database of knowledge. A number of recent works combine multimodal input to learn visual attributes (Bruni et al., 2012; Silberer et al., 2013), extract commonsense 8 Conclusion We presented a novel take on verb-centric frame semantics to learn implied physical knowledge latent in verbs. Empirical results confirm that by modeling changes in physical attributes entailed by verbs together with objects that exhibit these properties, we are able to better infer new knowledge in both domains. Acknowledgements This research is supported in part by the National Science Foundation Graduate Research Fellowship, DARPA CwC program through ARO (W911NF-15-1-0543), the NSF grant (IIS1524371), and gifts by Google and Faceboo"
P17-1025,P16-1137,0,0.0305201,"straddle the gap between IE, knowledge base completion, and learning commonsense knowledge from text. Earlier works in these areas use large amounts of text to try to extract general statements like “A THING CAN BE READABLE” (Gordon et al., 2010) and frequencies of events (Gordon and Schubert, 2012). Our work focuses on specific domains of knowledge rather than general statements or occurrence statistics, and develops a frame-centric approach to circumvent reporting bias. Other work uses a knowledge base and scores unseen tuples based on similarity to existing ones (Angeli and Manning, 2013; Li et al., 2016). Relatedly, previous work uses natural language inference to infer new facts from a dataset of commonsense facts that can be extracted from unstructured text (Angeli and Manning, 2014). In contrast, we focus on a small number of specific types of knowledge without access to an existing database of knowledge. A number of recent works combine multimodal input to learn visual attributes (Bruni et al., 2012; Silberer et al., 2013), extract commonsense 8 Conclusion We presented a novel take on verb-centric frame semantics to learn implied physical knowledge latent in verbs. Empirical results confi"
P17-1025,P10-1133,0,0.0331584,"s the baselines on all attributes except for the speed, which has a highly skewed label distribution to allow the majority baseline to Error analysis: Examples 6–10 in Figure 4 highlight failure cases for the model. Example 273 knowledge from web images (Tandon et al., 2016), and overcome reporting bias (Misra et al., 2016). In contrast, we focus on natural language evidence to reason about attributes that are both in (size) and out (weight, rigidness, etc.) of the scope of computer vision. Yet other works mine numerical attributes of objects (Narisawa et al., 2013; Takamura and Tsujii, 2015; Davidov and Rappoport, 2010) and comparative knowledge from the web (Tandon et al., 2014). Our work uniquely learns verb-centric lexical entailment knowledge. A handful of works have attempted to learn the types of knowledge we address in this work. One recent work tried to directly predict several binary attributes (such “is large” and “is yellow”) from on off-the-shelf word embeddings, noting that accuracy was very low (Rubinstein et al., 2015). Another line of work addressed grounding verbs in the context of robotic tasks. One paper in this line acquires verb meanings by observing state changes in the environment (She"
P17-1025,P16-1171,0,0.120203,"o learn the types of knowledge we address in this work. One recent work tried to directly predict several binary attributes (such “is large” and “is yellow”) from on off-the-shelf word embeddings, noting that accuracy was very low (Rubinstein et al., 2015). Another line of work addressed grounding verbs in the context of robotic tasks. One paper in this line acquires verb meanings by observing state changes in the environment (She and Chai, 2016). Another work in this line does a deep investigation of eleven verbs, modeling their physical effect via annotated images along eighteen attributes (Gao et al., 2016). These works are encouraging investigations into multimodal groundings of a small set of verbs. Our work instead grounds into a fixed set of attributes but leverages language on a broader scale to learn about more verbs in more diverse set of frames. 6 shows a case where the comparison is nonsensical because “for” would naturally be followed by a purpose (“He drove the car for work.”) or a duration (“She drove the car for hours.”) rather than a concrete object whose size is measurable. Example 7 highlights an underspecified frame. One crowd worker provided the example, “P ERSON stopped the fl"
P17-1025,S13-1035,0,0.055682,"Missing"
P17-1025,P13-1038,0,0.0571379,"in the upper half of Table 2. Our model outperforms the baselines on all attributes except for the speed, which has a highly skewed label distribution to allow the majority baseline to Error analysis: Examples 6–10 in Figure 4 highlight failure cases for the model. Example 273 knowledge from web images (Tandon et al., 2016), and overcome reporting bias (Misra et al., 2016). In contrast, we focus on natural language evidence to reason about attributes that are both in (size) and out (weight, rigidness, etc.) of the scope of computer vision. Yet other works mine numerical attributes of objects (Narisawa et al., 2013; Takamura and Tsujii, 2015; Davidov and Rappoport, 2010) and comparative knowledge from the web (Tandon et al., 2014). Our work uniquely learns verb-centric lexical entailment knowledge. A handful of works have attempted to learn the types of knowledge we address in this work. One recent work tried to directly predict several binary attributes (such “is large” and “is yellow”) from on off-the-shelf word embeddings, noting that accuracy was very low (Rubinstein et al., 2015). Another line of work addressed grounding verbs in the context of robotic tasks. One paper in this line acquires verb me"
P17-1025,W12-3023,0,0.0291569,"hair”). Example 10 demonstrates a case of polysemy where the model picks the wrong side. In the phrase, “She caught the runner in first,”, it is correct that she &gt;speed runner. However, the sense chosen by the crowd workers is that of, “She caught the baseball,” where indeed she &lt;speed baseball. 7 Related work Several works straddle the gap between IE, knowledge base completion, and learning commonsense knowledge from text. Earlier works in these areas use large amounts of text to try to extract general statements like “A THING CAN BE READABLE” (Gordon et al., 2010) and frequencies of events (Gordon and Schubert, 2012). Our work focuses on specific domains of knowledge rather than general statements or occurrence statistics, and develops a frame-centric approach to circumvent reporting bias. Other work uses a knowledge base and scores unseen tuples based on similarity to existing ones (Angeli and Manning, 2013; Li et al., 2016). Relatedly, previous work uses natural language inference to infer new facts from a dataset of commonsense facts that can be extracted from unstructured text (Angeli and Manning, 2014). In contrast, we focus on a small number of specific types of knowledge without access to an existi"
P17-1025,J05-1004,0,0.062931,"Missing"
P17-1025,D14-1162,0,0.0820348,"Missing"
P17-1025,P15-2119,0,0.0237605,"e) and out (weight, rigidness, etc.) of the scope of computer vision. Yet other works mine numerical attributes of objects (Narisawa et al., 2013; Takamura and Tsujii, 2015; Davidov and Rappoport, 2010) and comparative knowledge from the web (Tandon et al., 2014). Our work uniquely learns verb-centric lexical entailment knowledge. A handful of works have attempted to learn the types of knowledge we address in this work. One recent work tried to directly predict several binary attributes (such “is large” and “is yellow”) from on off-the-shelf word embeddings, noting that accuracy was very low (Rubinstein et al., 2015). Another line of work addressed grounding verbs in the context of robotic tasks. One paper in this line acquires verb meanings by observing state changes in the environment (She and Chai, 2016). Another work in this line does a deep investigation of eleven verbs, modeling their physical effect via annotated images along eighteen attributes (Gao et al., 2016). These works are encouraging investigations into multimodal groundings of a small set of verbs. Our work instead grounds into a fixed set of attributes but leverages language on a broader scale to learn about more verbs in more diverse se"
P17-1025,P16-1011,0,0.0696331,"10) and comparative knowledge from the web (Tandon et al., 2014). Our work uniquely learns verb-centric lexical entailment knowledge. A handful of works have attempted to learn the types of knowledge we address in this work. One recent work tried to directly predict several binary attributes (such “is large” and “is yellow”) from on off-the-shelf word embeddings, noting that accuracy was very low (Rubinstein et al., 2015). Another line of work addressed grounding verbs in the context of robotic tasks. One paper in this line acquires verb meanings by observing state changes in the environment (She and Chai, 2016). Another work in this line does a deep investigation of eleven verbs, modeling their physical effect via annotated images along eighteen attributes (Gao et al., 2016). These works are encouraging investigations into multimodal groundings of a small set of verbs. Our work instead grounds into a fixed set of attributes but leverages language on a broader scale to learn about more verbs in more diverse set of frames. 6 shows a case where the comparison is nonsensical because “for” would naturally be followed by a purpose (“He drove the car for work.”) or a duration (“She drove the car for hours."
P17-1025,P13-1056,0,0.0257955,"frame-centric approach to circumvent reporting bias. Other work uses a knowledge base and scores unseen tuples based on similarity to existing ones (Angeli and Manning, 2013; Li et al., 2016). Relatedly, previous work uses natural language inference to infer new facts from a dataset of commonsense facts that can be extracted from unstructured text (Angeli and Manning, 2014). In contrast, we focus on a small number of specific types of knowledge without access to an existing database of knowledge. A number of recent works combine multimodal input to learn visual attributes (Bruni et al., 2012; Silberer et al., 2013), extract commonsense 8 Conclusion We presented a novel take on verb-centric frame semantics to learn implied physical knowledge latent in verbs. Empirical results confirm that by modeling changes in physical attributes entailed by verbs together with objects that exhibit these properties, we are able to better infer new knowledge in both domains. Acknowledgements This research is supported in part by the National Science Foundation Graduate Research Fellowship, DARPA CwC program through ARO (W911NF-15-1-0543), the NSF grant (IIS1524371), and gifts by Google and Facebook. The authors thank the"
P17-1025,N15-1143,0,0.0182624,"ble 2. Our model outperforms the baselines on all attributes except for the speed, which has a highly skewed label distribution to allow the majority baseline to Error analysis: Examples 6–10 in Figure 4 highlight failure cases for the model. Example 273 knowledge from web images (Tandon et al., 2016), and overcome reporting bias (Misra et al., 2016). In contrast, we focus on natural language evidence to reason about attributes that are both in (size) and out (weight, rigidness, etc.) of the scope of computer vision. Yet other works mine numerical attributes of objects (Narisawa et al., 2013; Takamura and Tsujii, 2015; Davidov and Rappoport, 2010) and comparative knowledge from the web (Tandon et al., 2014). Our work uniquely learns verb-centric lexical entailment knowledge. A handful of works have attempted to learn the types of knowledge we address in this work. One recent work tried to directly predict several binary attributes (such “is large” and “is yellow”) from on off-the-shelf word embeddings, noting that accuracy was very low (Rubinstein et al., 2015). Another line of work addressed grounding verbs in the context of robotic tasks. One paper in this line acquires verb meanings by observing state c"
P17-1025,Q17-1027,0,0.115598,"Missing"
P17-1025,C98-1013,0,\N,Missing
P17-1025,Q15-1034,0,\N,Missing
P17-2073,W11-0705,0,0.0386939,"ssian. In the middle of the two week period, the perspective towards Obama drops slightly, most notably in Spanish, which overlaps with his controversial trip to Cuba (March 20 – 22). 4.2 Forecast + 4 days Sentiment Dynamics In Table 2, we summarize the results of our experiments for predicting targetted sentiment dynamics. For each language, we report the average Kullback-Leibler divergence for the baselines and the LSTM model (higher scores are worse). We show prediction results in two settings: predicting 5 Related Work There have been substantial studies for sentiment analysis on twitter (Agarwal et al., 2011; Kouloumpis et al., 2011; Pak and Paroubek, 2010; 462 Figure 5: True versus predicted polarity distributions over time on three specific entities (TP: True Positive, PP: Predicted Positive, TN: True Negative, PN: Predicted Negative). Entity KL Trump Obama apple Clinton 0.12 0.25 0.37 1.13 Entity England Turkey Russia Brussels KL 0.14 0.30 0.76 1.14 Entity EU Google Belgium Abdeslam predicting the sentiment dynamics in social media based on previous trends. KL 0.22 0.32 0.95 1.79 6 Conclusions Table 3: Error analysis on held-out entities. When reporting news, people write with their own implic"
P17-2073,P16-1231,0,0.0143423,", that has the highest translation probability: Multilingual Twitter Dataset We obtained multilingual geo-located tweets spanning Mar 15 – Mar 29, 2016. This 15 day duration covers Brussels attacks on Mar 22 as well as one whole week before and after, allowing us to study the public sentiment dynamics in response to a major terrorist event. We focus on tweets that are likely to be about “news-worthy” topics by selecting tweets that came from trusted sources such as twitter-verified accounts or known news accounts, or contained hashtags #breaking or #news.2 We used SyntaxNet dependency parser (Andor et al., 2016) and trained additional SyntaxNet models for 10 non-English languages using Universal Dependencies annotations.3 We extracted 1.2 million agent-verb-theme tuples as listed in Table 1. v∗ = argmaxv p(v|v 0 ) F(v 0 ) = F(v∗) For example, the connotation frame for assassiner is propagated from murder, the English word that it is aligned with the most. 3.2 Extracting Targeted Sentiments Using the connotation frame lexicon, we compute the distribution of targeted sentiments towards most-frequently-mentioned named entities. We also compute sentiments expressed by each 2 We experimented with other au"
P17-2073,W10-0204,0,0.0319352,"tions towards entities, which can be captured within the connotation frames that we have extended to 10 European languages. This work is one of the first to present a large scale analysis of multilingual connotation dynamics, and helps explore multiple perspectives on diverse issues across languages, time and countries – a critical piece in understanding journalistic portrayal and biases. Liu and Zhang, 2012), as well as targetted sentiment (Deng and Wiebe, 2015), implicit sentiment (Deng and Wiebe, 2014; Feng et al., 2013; Greene and Resnik, 2009) and specific aspects of subjective language (Mohammad and Turney, 2010; Choi and Wiebe, 2014) in other domains. Previous investigations include using targetted sentiment to predict international relations (Chambers et al., 2015), analyzing stylistic elements to predict tweet popularity (Tan et al., 2014), and exploring the re-phrasing of social media posts referencing specific news articles (Tan et al., 2016). Compared to most prior studies that focused on overt sentiment in English-only tweets, our work aims to study targeted implied sentiments across temporal, spatial, and linguistic borders. Some work (Tsytsarau et al., 2014; O’Connor et al., 2010; De et al.,"
P17-2073,D15-1007,0,0.189598,"ent a large scale analysis of multilingual connotation dynamics, and helps explore multiple perspectives on diverse issues across languages, time and countries – a critical piece in understanding journalistic portrayal and biases. Liu and Zhang, 2012), as well as targetted sentiment (Deng and Wiebe, 2015), implicit sentiment (Deng and Wiebe, 2014; Feng et al., 2013; Greene and Resnik, 2009) and specific aspects of subjective language (Mohammad and Turney, 2010; Choi and Wiebe, 2014) in other domains. Previous investigations include using targetted sentiment to predict international relations (Chambers et al., 2015), analyzing stylistic elements to predict tweet popularity (Tan et al., 2014), and exploring the re-phrasing of social media posts referencing specific news articles (Tan et al., 2016). Compared to most prior studies that focused on overt sentiment in English-only tweets, our work aims to study targeted implied sentiments across temporal, spatial, and linguistic borders. Some work (Tsytsarau et al., 2014; O’Connor et al., 2010; De et al., 2016) has analyzed the transition of overt sentiment over a period of time and related the shifts in sentiment to news events. A body of work has also used p"
P17-2073,D14-1125,0,0.0384024,"Missing"
P17-2073,pak-paroubek-2010-twitter,0,0.0586889,"e perspective towards Obama drops slightly, most notably in Spanish, which overlaps with his controversial trip to Cuba (March 20 – 22). 4.2 Forecast + 4 days Sentiment Dynamics In Table 2, we summarize the results of our experiments for predicting targetted sentiment dynamics. For each language, we report the average Kullback-Leibler divergence for the baselines and the LSTM model (higher scores are worse). We show prediction results in two settings: predicting 5 Related Work There have been substantial studies for sentiment analysis on twitter (Agarwal et al., 2011; Kouloumpis et al., 2011; Pak and Paroubek, 2010; 462 Figure 5: True versus predicted polarity distributions over time on three specific entities (TP: True Positive, PP: Predicted Positive, TN: True Negative, PN: Predicted Negative). Entity KL Trump Obama apple Clinton 0.12 0.25 0.37 1.13 Entity England Turkey Russia Brussels KL 0.14 0.30 0.76 1.14 Entity EU Google Belgium Abdeslam predicting the sentiment dynamics in social media based on previous trends. KL 0.22 0.32 0.95 1.79 6 Conclusions Table 3: Error analysis on held-out entities. When reporting news, people write with their own implicit and explicit biases and judgments. An author’s"
P17-2073,N15-1146,0,0.0291517,"-out entities. When reporting news, people write with their own implicit and explicit biases and judgments. An author’s choice of language reveals connotations towards entities, which can be captured within the connotation frames that we have extended to 10 European languages. This work is one of the first to present a large scale analysis of multilingual connotation dynamics, and helps explore multiple perspectives on diverse issues across languages, time and countries – a critical piece in understanding journalistic portrayal and biases. Liu and Zhang, 2012), as well as targetted sentiment (Deng and Wiebe, 2015), implicit sentiment (Deng and Wiebe, 2014; Feng et al., 2013; Greene and Resnik, 2009) and specific aspects of subjective language (Mohammad and Turney, 2010; Choi and Wiebe, 2014) in other domains. Previous investigations include using targetted sentiment to predict international relations (Chambers et al., 2015), analyzing stylistic elements to predict tweet popularity (Tan et al., 2014), and exploring the re-phrasing of social media posts referencing specific news articles (Tan et al., 2016). Compared to most prior studies that focused on overt sentiment in English-only tweets, our work ai"
P17-2073,eisele-chen-2010-multiun,0,0.0321181,"redicted Distrib: Mar 25 (UK → Brussels) Mar 23 (UK → Brussels) Examples Mar 22 (UK → Brussels) # Tuples Mar 21 (UK → Brussels) Lang 3 3.1 Methods Multilingual Connotation Frames We perform context-based projection of English connotation frames to 10 additional European languages using large parallel corpora. Since connotation of a word arises from the context in which the word is used, we want to ensure the translated connotation frames are used in similar contexts. We use existing parallel corpora with automatic word-alignment: the Opus Corpus (Tiedemann, 2012) using Multi-UN parallel data (Eisele and Chen, 2010) for Russian and EuroParl parallel data (Koehn, 2005) for all other languages. More concretely, for each non-English verb, v 0 (e.g., assassiner in French), we compute the probability of it being translated to English verb v by counting the alignments. We then define the connotation frame of v 0 , F(v 0 ), by transferring the connotation frame of the English verb v∗, F(v∗), that has the highest translation probability: Multilingual Twitter Dataset We obtained multilingual geo-located tweets spanning Mar 15 – Mar 29, 2016. This 15 day duration covers Brussels attacks on Mar 22 as well as one wh"
P17-2073,P16-1030,1,0.919474,"ell‡ Yejin Choi† Svitlana Volkova‡ † Paul G. Allen School of Computer Science & Engineering, University of Washington {hrashkin,yejin}@cs.washington.edu ‡ Data Sciences and Analytics, Pacific Northwest National Laboratory {eric.bell,svitlana.volkova}@pnnl.gov Abstract English Verb: survive Other languages: survivre, sobrevivir, überleben… People around the globe respond to major real world events through social media. To study targeted public sentiments across many languages and geographic locations, we introduce multilingual connotation frames: an extension from English connotation frames of Rashkin et al. (2016) with 10 additional European languages, focusing on the implied sentiments among event participants engaged in a frame. As a case study, we present large scale analysis on targeted public sentiments toward salient events and entities using 1.2 million multilingual connotation frames extracted from Twitter. “L'incroyable miraculé des explosions à Brussels: ce Mormon avait “19-jähriger Missionar überlebt drei Terroranschläge” déjà survécu aux attentats de Boston et de Paris” “Este joven ha sobrevivido a los atentados de Boston, de París y de Bruselas” Connotation Frame for surviving verbs: P(age"
P17-2073,P13-1174,1,0.848873,"mplicit and explicit biases and judgments. An author’s choice of language reveals connotations towards entities, which can be captured within the connotation frames that we have extended to 10 European languages. This work is one of the first to present a large scale analysis of multilingual connotation dynamics, and helps explore multiple perspectives on diverse issues across languages, time and countries – a critical piece in understanding journalistic portrayal and biases. Liu and Zhang, 2012), as well as targetted sentiment (Deng and Wiebe, 2015), implicit sentiment (Deng and Wiebe, 2014; Feng et al., 2013; Greene and Resnik, 2009) and specific aspects of subjective language (Mohammad and Turney, 2010; Choi and Wiebe, 2014) in other domains. Previous investigations include using targetted sentiment to predict international relations (Chambers et al., 2015), analyzing stylistic elements to predict tweet popularity (Tan et al., 2014), and exploring the re-phrasing of social media posts referencing specific news articles (Tan et al., 2016). Compared to most prior studies that focused on overt sentiment in English-only tweets, our work aims to study targeted implied sentiments across temporal, spat"
P17-2073,E14-1040,0,0.0237864,"write with their own implicit and explicit biases and judgments. An author’s choice of language reveals connotations towards entities, which can be captured within the connotation frames that we have extended to 10 European languages. This work is one of the first to present a large scale analysis of multilingual connotation dynamics, and helps explore multiple perspectives on diverse issues across languages, time and countries – a critical piece in understanding journalistic portrayal and biases. Liu and Zhang, 2012), as well as targetted sentiment (Deng and Wiebe, 2015), implicit sentiment (Deng and Wiebe, 2014; Feng et al., 2013; Greene and Resnik, 2009) and specific aspects of subjective language (Mohammad and Turney, 2010; Choi and Wiebe, 2014) in other domains. Previous investigations include using targetted sentiment to predict international relations (Chambers et al., 2015), analyzing stylistic elements to predict tweet popularity (Tan et al., 2014), and exploring the re-phrasing of social media posts referencing specific news articles (Tan et al., 2016). Compared to most prior studies that focused on overt sentiment in English-only tweets, our work aims to study targeted implied sentiments ac"
P17-2073,N09-1057,0,0.0389621,"t biases and judgments. An author’s choice of language reveals connotations towards entities, which can be captured within the connotation frames that we have extended to 10 European languages. This work is one of the first to present a large scale analysis of multilingual connotation dynamics, and helps explore multiple perspectives on diverse issues across languages, time and countries – a critical piece in understanding journalistic portrayal and biases. Liu and Zhang, 2012), as well as targetted sentiment (Deng and Wiebe, 2015), implicit sentiment (Deng and Wiebe, 2014; Feng et al., 2013; Greene and Resnik, 2009) and specific aspects of subjective language (Mohammad and Turney, 2010; Choi and Wiebe, 2014) in other domains. Previous investigations include using targetted sentiment to predict international relations (Chambers et al., 2015), analyzing stylistic elements to predict tweet popularity (Tan et al., 2014), and exploring the re-phrasing of social media posts referencing specific news articles (Tan et al., 2016). Compared to most prior studies that focused on overt sentiment in English-only tweets, our work aims to study targeted implied sentiments across temporal, spatial, and linguistic border"
P17-2073,tiedemann-2012-parallel,0,0.0173624,"→ Brussels) EN ES FR PT RU DE NL IT FI SV PL Predicted Distrib: Mar 25 (UK → Brussels) Mar 23 (UK → Brussels) Examples Mar 22 (UK → Brussels) # Tuples Mar 21 (UK → Brussels) Lang 3 3.1 Methods Multilingual Connotation Frames We perform context-based projection of English connotation frames to 10 additional European languages using large parallel corpora. Since connotation of a word arises from the context in which the word is used, we want to ensure the translated connotation frames are used in similar contexts. We use existing parallel corpora with automatic word-alignment: the Opus Corpus (Tiedemann, 2012) using Multi-UN parallel data (Eisele and Chen, 2010) for Russian and EuroParl parallel data (Koehn, 2005) for all other languages. More concretely, for each non-English verb, v 0 (e.g., assassiner in French), we compute the probability of it being translated to English verb v by counting the alignments. We then define the connotation frame of v 0 , F(v 0 ), by transferring the connotation frame of the English verb v∗, F(v∗), that has the highest translation probability: Multilingual Twitter Dataset We obtained multilingual geo-located tweets spanning Mar 15 – Mar 29, 2016. This 15 day duratio"
P17-2073,2005.mtsummit-papers.11,0,0.0425646,"Examples Mar 22 (UK → Brussels) # Tuples Mar 21 (UK → Brussels) Lang 3 3.1 Methods Multilingual Connotation Frames We perform context-based projection of English connotation frames to 10 additional European languages using large parallel corpora. Since connotation of a word arises from the context in which the word is used, we want to ensure the translated connotation frames are used in similar contexts. We use existing parallel corpora with automatic word-alignment: the Opus Corpus (Tiedemann, 2012) using Multi-UN parallel data (Eisele and Chen, 2010) for Russian and EuroParl parallel data (Koehn, 2005) for all other languages. More concretely, for each non-English verb, v 0 (e.g., assassiner in French), we compute the probability of it being translated to English verb v by counting the alignments. We then define the connotation frame of v 0 , F(v 0 ), by transferring the connotation frame of the English verb v∗, F(v∗), that has the highest translation probability: Multilingual Twitter Dataset We obtained multilingual geo-located tweets spanning Mar 15 – Mar 29, 2016. This 15 day duration covers Brussels attacks on Mar 22 as well as one whole week before and after, allowing us to study the p"
P18-1009,D17-1284,0,0.106987,"Missing"
P18-1009,N06-2015,0,0.0905723,"Missing"
P18-1009,D17-1018,1,0.935109,"to prevent malaria?”). We annotate a dataset of about 6,000 mentions via crowdsourcing (Section 2.1), and demonstrate that using an large type vocabulary substantially increases annotation coverage and diversity over existing approaches (Section 2.2). 88 2.1 Crowdsourcing Entity Types To capture multiple domains, we sample sentences from Gigaword (Parker et al., 2011), OntoNotes (Hovy et al., 2006), and web articles (Singh et al., 2012). We select entity mentions by taking maximal noun phrases from a constituency parser (Manning et al., 2014) and mentions from a coreference resolution system (Lee et al., 2017). We provide the sentence and the target entity mention to five crowd workers on Mechanical Turk, and ask them to annotate the entity’s type. To encourage annotators to generate fine-grained types, we require at least one general type (e.g. person, organization, location) and two specific types (e.g. doctor, fish, religious institute), from a type vocabulary of about 10K frequent noun phrases. We use WordNet (Miller, 1995) to expand these types automatically by generating all their synonyms and hypernyms based on the most common sense, and ask five different annotators to validate the generate"
P18-1009,P14-5010,0,0.0118187,"resolution and question answering (e.g. “Which philanthropist is trying to prevent malaria?”). We annotate a dataset of about 6,000 mentions via crowdsourcing (Section 2.1), and demonstrate that using an large type vocabulary substantially increases annotation coverage and diversity over existing approaches (Section 2.2). 88 2.1 Crowdsourcing Entity Types To capture multiple domains, we sample sentences from Gigaword (Parker et al., 2011), OntoNotes (Hovy et al., 2006), and web articles (Singh et al., 2012). We select entity mentions by taking maximal noun phrases from a constituency parser (Manning et al., 2014) and mentions from a coreference resolution system (Lee et al., 2017). We provide the sentence and the target entity mention to five crowd workers on Mechanical Turk, and ask them to annotate the entity’s type. To encourage annotators to generate fine-grained types, we require at least one general type (e.g. person, organization, location) and two specific types (e.g. doctor, fish, religious institute), from a type vocabulary of about 10K frequent noun phrases. We use WordNet (Miller, 1995) to expand these types automatically by generating all their synonyms and hypernyms based on the most com"
P18-1009,E17-1075,0,0.237414,"Missing"
P18-1009,N07-1071,0,0.0168201,"defined using Wikipedia categories (100), Freebase types (1K) and WordNet senses (16K). However, they focus on named entities, and data has been challenging to gather, often approximating gold annotations with distant supervision. In contrast, (1) our ontology contains any frequent noun phrases that depicts a type, (2) our task goes beyond named entities, covering every noun phrase (even pronouns), and (3) we provide crowdsourced annotations which provide context-sensitive, fine grained type labels. Contextualized fine-grained entity typing is related to selectional preference (Resnik, 1996; Pantel et al., 2007; Zapirain et al., 2013; de Cruys, 2014), where the goal is to induce semantic generalizations on the type of arguments a predicate prefers. Rather than focusing on predicates, we condition on the entire sentence to deduce the arguments’ types, which allows us to capture more nuanced types. For example, not every type that fits “He played the violin in his room” is also suitable for “He played the violin in the Carnegie Hall”. Entity typing here can be connected to argument finding in semantic role labeling. To deal with noisy distant supervision for KB population and entity typing, researcher"
P18-1009,D15-1103,0,0.108925,"growing attention, and is used in many applications (Gupta et al., 2017; Ren et al., 2017; Yaghoobzadeh et al., 2017b; Raiman and Raiman, 2018). Researchers studied typing in varied contexts, including mentions in specific sentences (as we consider) (Ling and Weld, 2012; Gillick et al., 2014; Yogatama et al., 2015; Dong et al., 2015; Schutze et al., 2017), corpus-level prediction (Yaghoobzadeh and Sch¨utze, 2016), and lexicon level (given only a noun phrase with no context) (Yao et al., 2013). Recent work introduced fine-grained type ontologies (Rabinovich and Klein, 2017; Murty et al., 2017; Corro et al., 2015), defined using Wikipedia categories (100), Freebase types (1K) and WordNet senses (16K). However, they focus on named entities, and data has been challenging to gather, often approximating gold annotations with distant supervision. In contrast, (1) our ontology contains any frequent noun phrases that depicts a type, (2) our task goes beyond named entities, covering every noun phrase (even pronouns), and (3) we provide crowdsourced annotations which provide context-sensitive, fine grained type labels. Contextualized fine-grained entity typing is related to selectional preference (Resnik, 1996;"
P18-1009,D14-1004,0,0.0608809,"Missing"
P18-1009,P17-2052,0,0.107655,"76.8 66.1 71.8 7 Fine-grained NER has received growing attention, and is used in many applications (Gupta et al., 2017; Ren et al., 2017; Yaghoobzadeh et al., 2017b; Raiman and Raiman, 2018). Researchers studied typing in varied contexts, including mentions in specific sentences (as we consider) (Ling and Weld, 2012; Gillick et al., 2014; Yogatama et al., 2015; Dong et al., 2015; Schutze et al., 2017), corpus-level prediction (Yaghoobzadeh and Sch¨utze, 2016), and lexicon level (given only a noun phrase with no context) (Yao et al., 2013). Recent work introduced fine-grained type ontologies (Rabinovich and Klein, 2017; Murty et al., 2017; Corro et al., 2015), defined using Wikipedia categories (100), Freebase types (1K) and WordNet senses (16K). However, they focus on named entities, and data has been challenging to gather, often approximating gold annotations with distant supervision. In contrast, (1) our ontology contains any frequent noun phrases that depicts a type, (2) our task goes beyond named entities, covering every noun phrase (even pronouns), and (3) we provide crowdsourced annotations which provide context-sensitive, fine grained type labels. Contextualized fine-grained entity typing is related"
P18-1009,Q14-1037,0,0.0693507,"e sentence. Table 1 shows three examples that exhibit a rich variety of types at different granularities. Our task effectively subsumes existing finegrained named entity typing formulations due to the use of a very large type vocabulary and the fact that we predict types for all noun phrases, including named entities, nominals, and pronouns. Incorporating fine-grained entity types has improved entity-focused downstream tasks, such as relation extraction (Yaghoobzadeh et al., 2017a), question answering (Yavuz et al., 2016), query analysis (Balog and Neumayer, 2012), and coreference resolution (Durrett and Klein, 2014). These systems used a relatively coarse type ontology. However, manually designing the ontology is a challenging task, and it is difficult to cover all posIntroduction Entities can often be described by very fine grained types. Consider the sentences “Bill robbed John. He was arrested.” The noun phrases “John,” “Bill,” and “he” have very specific types that can be inferred from the text. This includes the facts that “Bill” and “he” are both likely “criminal” due to the “robbing” and “arresting,” while “John” is more likely a “victim” because he was “robbed.” Such fine-grained types (victim, c"
P18-1009,D16-1144,0,0.719416,"We predict every type t for which yt &gt; 0.5, or arg max yt if there is no such type. Multitask Objective The distant supervision sources provide partial supervision for ultra-fine types; KBs often provide more general types, while head words usually provide only ultra-fine types, without their generalizations. In other words, the absence of a type at a different level of abstraction does not imply a negative signal; e.g. when the head word is “inventor”, the model should not be discouraged to predict “person”. Prior work used a customized hinge loss (Abhishek et al., 2017) or max margin loss (Ren et al., 2016a) to improve robustness to noisy or incomplete supervision. We propose a multitask objective that reflects the characteristic of our training dataset. Instead of updating all labels for each example, we divide labels into three bins (general, fine, and ultra-fine), and update labels only in bin containing at least one positive label. Specifically, the training objective is to minimize J where t is the target vector at each granularity: Model We design a model for predicting sets of types given a mention in context. The architecture resembles the recent neural AttentiveNER model (Shimaoka et a"
P18-1009,E17-1111,0,0.0716907,"Missing"
P18-1009,D11-1141,0,0.271575,"Missing"
P18-1009,D16-1015,0,0.109002,"form noun phrases that describe appropriate types for the role the target entity plays in the sentence. Table 1 shows three examples that exhibit a rich variety of types at different granularities. Our task effectively subsumes existing finegrained named entity typing formulations due to the use of a very large type vocabulary and the fact that we predict types for all noun phrases, including named entities, nominals, and pronouns. Incorporating fine-grained entity types has improved entity-focused downstream tasks, such as relation extraction (Yaghoobzadeh et al., 2017a), question answering (Yavuz et al., 2016), query analysis (Balog and Neumayer, 2012), and coreference resolution (Durrett and Klein, 2014). These systems used a relatively coarse type ontology. However, manually designing the ontology is a challenging task, and it is difficult to cover all posIntroduction Entities can often be described by very fine grained types. Consider the sentences “Bill robbed John. He was arrested.” The noun phrases “John,” “Bill,” and “he” have very specific types that can be inferred from the text. This includes the facts that “Bill” and “he” are both likely “criminal” due to the “robbing” and “arresting,” w"
P18-1009,E17-2119,0,0.117556,"Missing"
P18-1009,P15-2048,0,0.237857,"Missing"
P18-1009,C12-2133,0,0.234817,"f distant supervision: automatically extracted nominal head words from raw text (Section 3.2). Using head words as a form of distant supervision provides fine-grained information about named entities and nominal mentions. While a KB may link “the 44th president of the United States” to many types such as author, lawyer, and professor, head words provide only the type “president”, which is relevant in the context. We bypass the challenge of automatically linking entities to Wikipedia by exploiting existing hyperlinks in web pages (Singh et al., 2012), following prior work (Ling and Weld, 2012; Yosef et al., 2012). Since our heuristic extraction of types from the definition sentence is somewhat noisy, we use a more conservative entity linking policy4 that yields a signal with similar overall accuracy to KB-linked data. 2 Data from: https://github.com/ shimaokasonse/NFGEC 3 We extract types by applying a dependency parser (Manning et al., 2014) to the definition sentence, and taking nouns that are dependents of a copular edge or connected to nouns linked to copulars via appositive or conjunctive edges. 4 Only link if the mention contains the Wikipedia entity’s name and the entity’s name contains the men"
P18-1009,N04-1002,0,0.150987,"Missing"
P18-1009,D12-1042,0,0.0575254,"ere the goal is to induce semantic generalizations on the type of arguments a predicate prefers. Rather than focusing on predicates, we condition on the entire sentence to deduce the arguments’ types, which allows us to capture more nuanced types. For example, not every type that fits “He played the violin in his room” is also suitable for “He played the violin in the Carnegie Hall”. Entity typing here can be connected to argument finding in semantic role labeling. To deal with noisy distant supervision for KB population and entity typing, researchers used multi-instance multi-label learning (Surdeanu et al., 2012; Yaghoobzadeh et al., 2017b) or custom losses (Abhishek et al., 2017; Ren et al., 2016a). Our multitask objective handles noisy supervision by pooling different distant supervision sources across different levels of granularity. Table 6: Results on the OntoNotes fine-grained entity typing test set. The first two models (AttentiveNER++ and AFET) use only KB-based supervision. LNR uses a filtered version of the KBbased training set. Our model uses all our distant supervision sources. Model Training Data Performance Acc. MaF1 MiF1 ONTO WIKI HEAD Attn. NER 3 3 3 3 46.5 53.7 63.3 72.8 58.3 68.0 Ou"
P18-1043,P98-1013,0,0.691869,"Missing"
P18-1043,D17-1091,0,0.0343723,"Missing"
P18-1043,H05-1079,0,0.118776,"Missing"
P18-1043,D15-1075,0,0.0465582,"events the user has experienced, without the user explicitly stating how they are feeling. Similarly, advertisement systems on social media should be able to reason about the emotional reactions of people after events such as mass shootings and remove ads for guns which might increase social distress (Goel and Isaac, 2016). Also, pragmatic inference is a necessary step toward automatic narrative understanding and generation (Tomai and Forbus, 2010; Ding and Riloff, 2016; Ding et al., 2017). However, this type of social commonsense reasoning goes far beyond the widely studied entailment tasks (Bowman et al., 2015; Dagan et al., 2006) and thus falls outside the scope of existing benchmarks. In this paper, we introduce a new task, corpus, Introduction Understanding a narrative requires commonsense reasoning about the mental states of people in relation to events. For example, if “Alex is dragging his feet at work”, pragmatic implications about Alex’s intent are that “Alex wants to avoid doing things” (Figure 1). We can also infer that Alex’s emotional reaction might be feeling “lazy” or “bored”. Furthermore, while not explicitly mentioned, we can infer that people other than Alex are affected by the sit"
P18-1043,S13-1035,0,0.0167626,"we extract are a combination of a verb predicate with partially instantiated arguments. We keep specific arguments together with the predicate, if they appear frequently enough (e.g., PersonX eats pasta for dinner). Otherwise, the arguments are replaced with an untyped blank (e.g., PersonX eats for dinner). In our work, only person mentions are replaced with typed variables, leaving other types to future research. 2.1 Event Extraction We extract phrasal events from three different corpora for broad coverage: the ROC Story training set (Mostafazadeh et al., 2016), the Google Syntactic N-grams (Goldberg and Orwant, 2013), and the Spinn3r corpus (Gordon and Swanson, 2008). We derive events from the set of verb phrases in our corpora, based on syntactic parses (Klein and Manning, 2003). We then replace the predicate subject and other entities with the typed variables (e.g., PersonX, PersonY), and selectively substitute verb arguments with blanks ( ). We use frequency thresholds to select events to annotate (for details, see Appendix A.1). Additionally, we supplement the list of events with all 2,000 verb idioms found in Wiktionary, in order to cover events that are less compositional.2 Our final annotation corp"
P18-1043,K16-1002,0,0.0559548,"Missing"
P18-1043,N15-1113,0,0.0306237,"cult for commonsense inference, perhaps due to the difficulty in composing meaning over nonliteral or noncompositional event descriptions. To further evaluate the geometry of the embedding space, we analyze interpolations between pairs of event phrases (from outside the train set), similar to the homotopic analysis of Bowman et al. (2016). For a handful of event pairs, we decode intents, reactions for PersonX, and reactions for other people from points sampled at equal inter5.1 Processing of Movie Scripts For our portrayal analyses, we use scene descriptions from 772 movie scripts released by Gorinski and Lapata (2015), assigned to over 21,000 characters as done by Sap et al. (2017). We extract events from the scene descriptions, and generate their 10 most probable intent and reaction sequences using our BiRNN sequence model (as in Figure 7). We then categorize generated intents and reactions into groups based on LIWC category scores of the generated output (Tausczik and Pennebaker, 2016).3 The intent and reaction categories are then 3 469 We only consider content word categories: ‘Core Drives 5.2 PersonX hugs ___ , planting a smooch on PersonY&apos;s cheek show affection show love Int ent loving none Female: in"
P18-1043,W14-4012,0,0.0541464,"Missing"
P18-1043,D14-1125,0,0.0578311,"Missing"
P18-1043,D17-1167,0,0.0612405,"Missing"
P18-1043,D13-1149,0,0.0657859,"Missing"
P18-1043,D17-1292,0,0.0441691,"Missing"
P18-1043,J14-1002,1,0.872303,"Missing"
P18-1043,D14-1181,0,0.00637193,"Missing"
P18-1043,P03-1054,0,0.0196713,"y enough (e.g., PersonX eats pasta for dinner). Otherwise, the arguments are replaced with an untyped blank (e.g., PersonX eats for dinner). In our work, only person mentions are replaced with typed variables, leaving other types to future research. 2.1 Event Extraction We extract phrasal events from three different corpora for broad coverage: the ROC Story training set (Mostafazadeh et al., 2016), the Google Syntactic N-grams (Goldberg and Orwant, 2013), and the Spinn3r corpus (Gordon and Swanson, 2008). We derive events from the set of verb phrases in our corpora, based on syntactic parses (Klein and Manning, 2003). We then replace the predicate subject and other entities with the typed variables (e.g., PersonX, PersonY), and selectively substitute verb arguments with blanks ( ). We use frequency thresholds to select events to annotate (for details, see Appendix A.1). Additionally, we supplement the list of events with all 2,000 verb idioms found in Wiktionary, in order to cover events that are less compositional.2 Our final annotation corpus contains nearly 25,000 event phrases, spanning over 1,300 unique verb predicates (Table 2). Inference types The first type of pragmatic inference is about intent."
P18-1043,P16-1137,0,0.0318906,"sk, demonstrating that, given the phrase-level inference dataset, neural encoderdecoder models can successfully compose phrasal embeddings for previously unseen events and reason about the mental states of their participants. 1 464 https://tinyurl.com/event2mind matic or commonsense interpretation. We scope our study to two distinct types of inference: given a phrase that describes an event, we want to reason about the likely intents and emotional reactions of people who caused or affected by the event. This complements prior work on more general commonsense inference (Speer and Havasi, 2012; Li et al., 2016; Zhang et al., 2017), by focusing on the causal relations between events and people’s mental states, which are not well covered by most existing resources. # Unique Events # Unique Verbs Average  ROC Story G. N-grams Spinn3r Idioms 13,627 7,066 2,130 1,916 639 789 388 442 0.57 0.39 0.41 0.42 Total 24,716 1,333 0.45 Source Table 2: Data and annotation agreement statistics for our new phrasal inference corpus. Each event is annotated by three crowdworkers. We collect a wide range of phrasal event descriptions from stories, blogs, and Wiktionary idioms. Compared to prior work on phrasal embeddi"
P18-1043,N09-4007,0,0.0891988,"Missing"
P18-1043,speer-havasi-2012-representing,0,0.036999,"rformance on this new task, demonstrating that, given the phrase-level inference dataset, neural encoderdecoder models can successfully compose phrasal embeddings for previously unseen events and reason about the mental states of their participants. 1 464 https://tinyurl.com/event2mind matic or commonsense interpretation. We scope our study to two distinct types of inference: given a phrase that describes an event, we want to reason about the likely intents and emotional reactions of people who caused or affected by the event. This complements prior work on more general commonsense inference (Speer and Havasi, 2012; Li et al., 2016; Zhang et al., 2017), by focusing on the causal relations between events and people’s mental states, which are not well covered by most existing resources. # Unique Events # Unique Verbs Average  ROC Story G. N-grams Spinn3r Idioms 13,627 7,066 2,130 1,916 639 789 388 442 0.57 0.39 0.41 0.42 Total 24,716 1,333 0.45 Source Table 2: Data and annotation agreement statistics for our new phrasal inference corpus. Each event is annotated by three crowdworkers. We collect a wide range of phrasal event descriptions from stories, blogs, and Wiktionary idioms. Compared to prior work o"
P18-1043,J12-2003,0,0.103728,"Missing"
P18-1043,E14-4025,0,0.105708,"Missing"
P18-1043,N16-1098,0,0.103644,"shown in examples in Table 1. More formally, the phrases we extract are a combination of a verb predicate with partially instantiated arguments. We keep specific arguments together with the predicate, if they appear frequently enough (e.g., PersonX eats pasta for dinner). Otherwise, the arguments are replaced with an untyped blank (e.g., PersonX eats for dinner). In our work, only person mentions are replaced with typed variables, leaving other types to future research. 2.1 Event Extraction We extract phrasal events from three different corpora for broad coverage: the ROC Story training set (Mostafazadeh et al., 2016), the Google Syntactic N-grams (Goldberg and Orwant, 2013), and the Spinn3r corpus (Gordon and Swanson, 2008). We derive events from the set of verb phrases in our corpora, based on syntactic parses (Klein and Manning, 2003). We then replace the predicate subject and other entities with the typed variables (e.g., PersonX, PersonY), and selectively substitute verb arguments with blanks ( ). We use frequency thresholds to select events to annotate (for details, see Appendix A.1). Additionally, we supplement the list of events with all 2,000 verb idioms found in Wiktionary, in order to cover even"
P18-1043,D16-1177,0,0.0562659,"Missing"
P18-1043,P15-2070,0,0.0512399,"Missing"
P18-1043,P17-1153,0,0.0571898,"Missing"
P18-1043,P16-1030,1,0.906195,"Missing"
P18-1043,P17-2022,0,0.0397663,"Missing"
P18-1043,Q15-1034,0,0.0422461,"Missing"
P18-1043,S15-2077,0,0.0631799,"Missing"
P18-1043,D17-1247,1,0.939721,"tly mentioned by the event phrase, and (3) a task formulation that aims to generate the textual descriptions of intents and reactions, instead of classifying their polarities or classifying the inference relations between two given textual descriptions. Furthermore, in order to showcase the practical implications of commonsense inference on events and people’s mental states, we apply our model to modern movie scripts, which provide a new insight into the gender bias in modern films beyond what previous studies have offered (England et al., 2011; Agarwal et al., 2015; Ramakrishna et al., 2017; Sap et al., 2017). The resulting corpus includes around 25,000 event phrases, which combine automatically extracted phrases from stories and blogs with all idiomatic verb phrases listed in the Wiktionary. Our corpus is publicly available.1 2 Dataset One goal of our investigation is to probe whether it is feasible to build computational models that can perform limited, but well-scoped commonsense inference on short free-form text, which we refer to as event phrases. While there has been much prior research on phrase-level paraphrases (Pavlick et al., 2015) and phrase-level entailment (Dagan et al., 2006), relat"
P18-1043,Q15-1025,0,\N,Missing
P18-1043,C98-1013,0,\N,Missing
P18-1043,Q17-1027,0,\N,Missing
P18-1152,D15-1075,0,0.0200388,"defined as srep (y) = σ(wr> RNNrep (d)), (4) where RNNrep (d) is the final state of a unidirectional RNN ran over the similarity scores d = d1 . . . dn and wr is a learned vector. The model is trained to maximize the ranking log likelihood Lrep = X log σ(srep (yg ) − srep (ys )), (5) (x,yg )∈D, ys ∼LM(x) which corresponds to the probability of the gold ending yg receiving a higher score than the ending sampled from the RNN language model. Entailment Model Judging textual quality can be related to the natural language inference (NLI) task of recognizing textual entailment (Dagan et al., 2006; Bowman et al., 2015): we would like to guide the generator to neither contradict its own past generation (the maxim of Quality) nor state something that readily follows from the context (the maxim of Quantity). The latter case is driven by the RNNs habit of paraphrasing itself during generation. We train a classifier that takes two sentences a and b as input and predicts the relation between them as either contradiction, entailment or neutral. We use the neutral class probability of the sentence pair as discriminator score, in order to discourage both contradiction and entailment. As entailment classifier we use"
P18-1152,N09-1025,0,0.0534914,"Missing"
P18-1152,W14-4012,0,0.136692,"Missing"
P18-1152,N16-1012,0,0.0271697,"ches on the floor and walls. The corridor ended just beyond the door to their former prison. No one else was about. Figure 1: Sample generations from an RNN language model (LM) and our system (L2W) conditioning on the context shown on the top. The red, underlined text highlights repetitions, while the blue, italicized text highlights details that have a direct semantic parallel in the reference text. Introduction Language models based on Recurrent Neural Networks (RNNs) have brought substantial advancements across a wide range of language tasks (Jozefowicz et al., 2016; Bahdanau et al., 2015; Chopra et al., 2016). However, when used for longform text generation, RNNs often lead to degenerate text that is repetitive, self-contradictory, and overly generic, as shown in Figure 1. We propose a unified learning framework that can address several challenges of long-form text generation by composing a committee of discriminators each specializing in a different principle of communication. Starting with an RNN language model, our framework learns to construct a more powerful generator by training a number of discriminative models that can collectively address limitations of the base RNN generator, and then le"
P18-1152,N10-1031,0,0.0144094,"the supplementary material. 4.3 Evaluation Setup We pose the evaluation of our model as the task of generating an appropriate continuation given an initial context. In our open-ended generation setting the continuation is not required to be a specific length, so we require our models and baselines to generate 5-sentence continuations, consistent with the way the discriminator and seq2seq baseline datasets are constructed. Previous work has reported that automatic mea5 We use the implementation available at https:// github.com/nhynes/abc. sures such as BLEU (Papineni et al., 2002) and Meteor (Denkowski and Lavie, 2010) do not lead to meaningful evaluation when used for long or creative text generation where there can be high variance among acceptable generation outputs (Wiseman et al., 2017; Vedantam et al., 2015). However, we still report these measures as one component of our evaluation. Additionally we report a number of custom metrics which capture important properties of the generated text: Length – Average sequence length per example; Trigrams – percentage of unique trigrams per example; Vocab – percentage of unique words per example. Endings generated by our model and the baselines are compared again"
P18-1152,P82-1020,0,0.747119,"Missing"
P18-1152,D16-1032,1,0.765721,"equences and the inherent instability of the training objective (Che et al., 2017) both present significant challenges. While solutions have been proposed to make it possible to train GANs for language (Che et al., 2017; Yu et al., 2017a) they have not yet been shown to produce high quality long-form text, as our results confirm. Generation with Long-term Context Several prior works studied paragraph generation using sequence-to-sequence models for image captions (Krause et al., 2017), product reviews (Lipton et al., 2015; Dong et al., 2017), sport reports (Wiseman et al., 2017), and recipes (Kiddon et al., 2016). While these prior works focus on developing neural architectures for learning domain specific discourse patterns, our work proposes a general framework for learning a generator that is more powerful than maximum likelihood decoding from an RNN language model for an arbitrary target domain. 7 Conclusion We proposed a unified learning framework for the generation of long, coherent texts, which overcomes some of the common limitations of RNNs as text generation models. Our framework learns a decoding objective suitable for generation through a learned combination of sub-models that capture ling"
P18-1152,P17-4012,0,0.0894824,"Missing"
P18-1152,N03-1017,0,0.0381462,"proposed alternative decoding objectives for generation (Shao et al., 2017). Li et al. (2016a) proposed a diversity-promoting objective that interpolates the conditional probability score with negative marginal or reverse conditional probabilities. Yu et al. (2017b) also incorporate the reverse conditional probability through a noisy channel model in order to alleviate the explaining-away problem, but at the cost of significant decoding complexity, making it impractical for paragraph generation. Modified decoding objectives have long been a common practice in statistical machine translation (Koehn et al., 2003; Och, 2003; Watanabe et al., 2007; Chiang et al., 2009) and remain common with neural machine translation, even when an extremely large amount of data is available (Wu et al., 2016). Inspired by all the above approaches, our work presents a general learning framework together with a more comprehensive set of composite communication models. Pragmatic Communication Models Models for pragmatic reasoning about communicative goals such as Grice’s maxims have been proposed in the context of referring expression generation (Frank and Goodman, 2012). Andreas and Klein (2016) proposed a neural model w"
P18-1152,N16-1014,0,0.187886,"., 2015). However, we still report these measures as one component of our evaluation. Additionally we report a number of custom metrics which capture important properties of the generated text: Length – Average sequence length per example; Trigrams – percentage of unique trigrams per example; Vocab – percentage of unique words per example. Endings generated by our model and the baselines are compared against the reference endings in the original text. Results are given in Table 1. For open-ended generation tasks such as our own, human evaluation has been found to be the only reliable measure (Li et al., 2016b; Wiseman et al., 2017). For human evaluation, two possible endings are presented to a human, who assesses the text according to several criteria, which are closely inspired by Grice’s Maxims: repetition, contradiction, relevance and clarity. See supplementary material for examples of the evaluation forms we used. For each criterion, the two continuations are compared using a 5-point Likert scale, to which we assign numerical values of −2 to 2. The scale measures whether one generation is strongly or somewhat preferred above the other, or whether they are equal. Finally, the human is asked to"
P18-1152,D16-1127,0,0.332418,"., 2015). However, we still report these measures as one component of our evaluation. Additionally we report a number of custom metrics which capture important properties of the generated text: Length – Average sequence length per example; Trigrams – percentage of unique trigrams per example; Vocab – percentage of unique words per example. Endings generated by our model and the baselines are compared against the reference endings in the original text. Results are given in Table 1. For open-ended generation tasks such as our own, human evaluation has been found to be the only reliable measure (Li et al., 2016b; Wiseman et al., 2017). For human evaluation, two possible endings are presented to a human, who assesses the text according to several criteria, which are closely inspired by Grice’s Maxims: repetition, contradiction, relevance and clarity. See supplementary material for examples of the evaluation forms we used. For each criterion, the two continuations are compared using a 5-point Likert scale, to which we assign numerical values of −2 to 2. The scale measures whether one generation is strongly or somewhat preferred above the other, or whether they are equal. Finally, the human is asked to"
P18-1152,D16-1230,0,0.102066,"Missing"
P18-1152,J93-2004,0,0.0637524,"s the same Adaptive Softmax (Grave et al., 2016) language model used as base generator in our framework (§3.1). This enables us to evaluate the effect of our enhanced decoding objective directly. A 100k vocabulary is used and beam search with beam size of 5 is used at decoding time. A DAPTIVE LM achieves perplexity of 37.46 and 18.81 on BookCorpus and TripAdvisor respectively. C ACHE LM As another LM baseline we include a continuous cache language model (Grave et al., 2017) as implemented by Merity et al. (2018), which recently obtained state-of-the-art perplexity on the Penn Treebank corpus (Marcus et al., 1993). Due to memory constraints, we use a vocabulary size of 50k for C ACHE LM. To generate, beam search decoding is used with a beam size 5. C ACHE LM obtains perplexities of 70.9 and 29.71 on BookCorpus and TripAdvisor respectively. 3 http://times.cs.uiuc.edu/˜wang296/ Data/ 4 http://yknzhu.wixsite.com/mbweb 1642 BookCorpus L2W vs. A DAPTIVE LM C ACHE LM S EQ 2S EQ S EQ GAN LM VS . R EFERENCE L2W VS . R EFERENCE Repetition +0.48 +1.61 +1.01 +0.20 -0.10 +0.49 Specific Criteria Contradiction Relevance +0.18 +0.12 +0.37 +1.23 +0.54 +0.83 +0.32 +0.61 -0.07 -0.18 +0.37 +0.46 Clarity +0.11 +1.21 +0.83"
P18-1152,D17-1238,0,0.0544374,"Missing"
P18-1152,P03-1021,0,0.0803777,"e decoding objectives for generation (Shao et al., 2017). Li et al. (2016a) proposed a diversity-promoting objective that interpolates the conditional probability score with negative marginal or reverse conditional probabilities. Yu et al. (2017b) also incorporate the reverse conditional probability through a noisy channel model in order to alleviate the explaining-away problem, but at the cost of significant decoding complexity, making it impractical for paragraph generation. Modified decoding objectives have long been a common practice in statistical machine translation (Koehn et al., 2003; Och, 2003; Watanabe et al., 2007; Chiang et al., 2009) and remain common with neural machine translation, even when an extremely large amount of data is available (Wu et al., 2016). Inspired by all the above approaches, our work presents a general learning framework together with a more comprehensive set of composite communication models. Pragmatic Communication Models Models for pragmatic reasoning about communicative goals such as Grice’s maxims have been proposed in the context of referring expression generation (Frank and Goodman, 2012). Andreas and Klein (2016) proposed a neural model where candid"
P18-1152,P02-1040,0,0.101406,"ch. For implementation details, see the supplementary material. 4.3 Evaluation Setup We pose the evaluation of our model as the task of generating an appropriate continuation given an initial context. In our open-ended generation setting the continuation is not required to be a specific length, so we require our models and baselines to generate 5-sentence continuations, consistent with the way the discriminator and seq2seq baseline datasets are constructed. Previous work has reported that automatic mea5 We use the implementation available at https:// github.com/nhynes/abc. sures such as BLEU (Papineni et al., 2002) and Meteor (Denkowski and Lavie, 2010) do not lead to meaningful evaluation when used for long or creative text generation where there can be high variance among acceptable generation outputs (Wiseman et al., 2017; Vedantam et al., 2015). However, we still report these measures as one component of our evaluation. Additionally we report a number of custom metrics which capture important properties of the generated text: Length – Average sequence length per example; Trigrams – percentage of unique trigrams per example; Vocab – percentage of unique words per example. Endings generated by our mod"
P18-1152,D16-1244,0,0.0731268,"Missing"
P18-1152,D14-1162,0,0.0809219,"nt aspect of Grice’s Maxims. The discriminator scores are interpreted as classification probabilities (scaled with the logistic function where necessary) and interpolated in the objective function as log probabilities. Let D = {(x1 , y1 ), . . . (xn , yn )} be the set of training examples for conditional generation. Dx denote all contexts and Dy all continuations. The scoring functions are trained on prefixes of y to simulate their application to partial continuations at inference time. In all models the first layer embeds each word w into a 300-dimensional vector e(w) initialized with GloVe (Pennington et al., 2014) pretrainedembeddings. Repetition Model This model addresses the maxim of Quantity by biasing the generator to avoid repetitions. The goal of the repetition discriminator is to learn to distinguish between RNN-generated and gold continuations by exploiting our empirical observation that repetitions are more common in completions generated by RNN language models. However, we do not want to completely eliminate repetition, as words do recur in English. In order to model natural levels of repetition, a score di is computed for each position in the continuation y based on pairwise cosine similarit"
P18-1152,D16-1255,0,0.0206589,"1 and also in Table 3. Even gated RNNs such as LSTMs (Hochreiter and Schmidhuber, 1997) and GRUs (Cho et al., 2014) have difficulties in properly incorporating long-term context due to explaining-away effects (Yu et al., 2017b), diminishing gradients (Pascanu et al., 2013), and lack of inductive bias for the network to learn discourse structure or global coherence beyond local patterns. Several methods in the literature attempt to address these issues. Overly simple and generic generation can be improved by length-normalizing the sentence probability (Wu et al., 2016), future cost estimation (Schmaltz et al., 2016), or a diversityboosting objective function (Shao et al., 2017; Vijayakumar et al., 2016). Repetition can be reduced by prohibiting recurrence of the trigrams as a hard rule (Paulus et al., 2018). However, such hard constraints do not stop RNNs from repeating through paraphrasing while preventing occasional intentional repetition. We propose a unified framework to address all these related challenges of long-form text generation by learning to construct a better decoding objective, generalizing over various existing modifications to the decoding objective. 3 The Learning Framework We propose a"
P18-1152,D17-1235,0,0.147803,"d Schmidhuber, 1997) and GRUs (Cho et al., 2014) have difficulties in properly incorporating long-term context due to explaining-away effects (Yu et al., 2017b), diminishing gradients (Pascanu et al., 2013), and lack of inductive bias for the network to learn discourse structure or global coherence beyond local patterns. Several methods in the literature attempt to address these issues. Overly simple and generic generation can be improved by length-normalizing the sentence probability (Wu et al., 2016), future cost estimation (Schmaltz et al., 2016), or a diversityboosting objective function (Shao et al., 2017; Vijayakumar et al., 2016). Repetition can be reduced by prohibiting recurrence of the trigrams as a hard rule (Paulus et al., 2018). However, such hard constraints do not stop RNNs from repeating through paraphrasing while preventing occasional intentional repetition. We propose a unified framework to address all these related challenges of long-form text generation by learning to construct a better decoding objective, generalizing over various existing modifications to the decoding objective. 3 The Learning Framework We propose a general learning framework for conditional language generatio"
P18-1152,D07-1080,0,0.0508078,"objectives for generation (Shao et al., 2017). Li et al. (2016a) proposed a diversity-promoting objective that interpolates the conditional probability score with negative marginal or reverse conditional probabilities. Yu et al. (2017b) also incorporate the reverse conditional probability through a noisy channel model in order to alleviate the explaining-away problem, but at the cost of significant decoding complexity, making it impractical for paragraph generation. Modified decoding objectives have long been a common practice in statistical machine translation (Koehn et al., 2003; Och, 2003; Watanabe et al., 2007; Chiang et al., 2009) and remain common with neural machine translation, even when an extremely large amount of data is available (Wu et al., 2016). Inspired by all the above approaches, our work presents a general learning framework together with a more comprehensive set of composite communication models. Pragmatic Communication Models Models for pragmatic reasoning about communicative goals such as Grice’s maxims have been proposed in the context of referring expression generation (Frank and Goodman, 2012). Andreas and Klein (2016) proposed a neural model where candidate descriptions are sa"
P18-1152,D16-1125,0,\N,Missing
P18-1152,N18-1101,0,\N,Missing
P18-1152,E17-1059,0,\N,Missing
P18-1213,N15-1146,0,0.0222852,"s: I, me: 2-5 Cousin: 2, 5 Emotional Reaction: … Line 3, I, me: sad/disgusted/angry Figure 3: The annotation pipeline for the fine-grained annotations with an example story. In addition, they depict multiple interactions between story characters, presenting rich opportunities to reason about character motivations and reactions. Furthermore, there are more than 98k such stories currently available covering a wide range of everyday scenarios. Unique Challenges While there have been a variety of annotated resources developed on the related topics of sentiment analysis (Mohammad and Turney, 2013; Deng and Wiebe, 2015), entity tracking (Hoffart et al., 2011; Weston et al., 2015), and story understanding (Goyal et al., 2010; Ouyang and McKeown, 2015; Lukin et al., 2016), our study is the first to annotate the full chains of mental state effects for story characters. This poses several unique challenges as annotations require (1) interpreting discourse (2) understanding implicit causal effects, and (3) understanding formal psychology theory categories. In prior literature, annotations of this complexity have typically been performed by experts (Deng and Wiebe, 2015; Ouyang and McKeown, 2015). While reliable,"
P18-1213,E17-1059,0,0.0127701,"rm the strong baseline on both metrics, indicating that the generated short explanations are closer semantically to the reference annotation. 8 Related work Mental State Annotations Incorporating emotion theories into NLP tasks has been explored in previous projects. Ghosh et al. (2017) modulate language model distributions by increasing the probability of words that express certain affective LIWC (Tausczik and Pennebaker, 2016) categories. More generally, various projects tackle the problem of generating text from a set of attributes like sentiment or generic-ness (Ficler and Goldberg, 2017; Dong et al., 2017). Similarly, there is also a body of research in reasoning about commonsense stories and discourse (Li and Jurafsky, 2017; Mostafazadeh et al., 2016) or detecting emotional stimuli in stories (Gui et al., 2017). Previous work in plot units (Lehnert, 1981) developed formalisms for affect and mental state in story narratives that included motivations and reactions. In our work, we collect mental state annotations for stories to used as a new resource in this space. Modeling Entity State Recently, novel works in language modeling (Ji et al., 2017; Yang et al., 2016), question answering (Henaff et"
P18-1213,W17-4912,0,0.0165481,"owever, all models outperform the strong baseline on both metrics, indicating that the generated short explanations are closer semantically to the reference annotation. 8 Related work Mental State Annotations Incorporating emotion theories into NLP tasks has been explored in previous projects. Ghosh et al. (2017) modulate language model distributions by increasing the probability of words that express certain affective LIWC (Tausczik and Pennebaker, 2016) categories. More generally, various projects tackle the problem of generating text from a set of attributes like sentiment or generic-ness (Ficler and Goldberg, 2017; Dong et al., 2017). Similarly, there is also a body of research in reasoning about commonsense stories and discourse (Li and Jurafsky, 2017; Mostafazadeh et al., 2016) or detecting emotional stimuli in stories (Gui et al., 2017). Previous work in plot units (Lehnert, 1981) developed formalisms for affect and mental state in story narratives that included motivations and reactions. In our work, we collect mental state annotations for stories to used as a new resource in this space. Modeling Entity State Recently, novel works in language modeling (Ji et al., 2017; Yang et al., 2016), question"
P18-1213,P17-1059,0,0.0166532,"the task of generating explanations of motivations and emotions in Table 5. Because the explanations are closely tied to emotional and motivation states, the randomly selected explanation can often be close in embedding space to the reference explanations, making the random baseline fairly competitive. However, all models outperform the strong baseline on both metrics, indicating that the generated short explanations are closer semantically to the reference annotation. 8 Related work Mental State Annotations Incorporating emotion theories into NLP tasks has been explored in previous projects. Ghosh et al. (2017) modulate language model distributions by increasing the probability of words that express certain affective LIWC (Tausczik and Pennebaker, 2016) categories. More generally, various projects tackle the problem of generating text from a set of attributes like sentiment or generic-ness (Ficler and Goldberg, 2017; Dong et al., 2017). Similarly, there is also a body of research in reasoning about commonsense stories and discourse (Li and Jurafsky, 2017; Mostafazadeh et al., 2016) or detecting emotional stimuli in stories (Gui et al., 2017). Previous work in plot units (Lehnert, 1981) developed for"
P18-1213,D10-1008,0,0.0957523,"Missing"
P18-1213,D17-1167,0,0.0265127,"s into NLP tasks has been explored in previous projects. Ghosh et al. (2017) modulate language model distributions by increasing the probability of words that express certain affective LIWC (Tausczik and Pennebaker, 2016) categories. More generally, various projects tackle the problem of generating text from a set of attributes like sentiment or generic-ness (Ficler and Goldberg, 2017; Dong et al., 2017). Similarly, there is also a body of research in reasoning about commonsense stories and discourse (Li and Jurafsky, 2017; Mostafazadeh et al., 2016) or detecting emotional stimuli in stories (Gui et al., 2017). Previous work in plot units (Lehnert, 1981) developed formalisms for affect and mental state in story narratives that included motivations and reactions. In our work, we collect mental state annotations for stories to used as a new resource in this space. Modeling Entity State Recently, novel works in language modeling (Ji et al., 2017; Yang et al., 2016), question answering (Henaff et al., 2017), and text generation (Kiddon et al., 2016; Bosselut et al., 2018) have shown that modeling entity state explicitly can boost performance while providing a preliminary interface for interpreting a mo"
P18-1213,P82-1020,0,0.840014,"Missing"
P18-1213,D11-1072,0,0.0926033,"Missing"
P18-1213,D17-1195,1,0.863429,"ent or generic-ness (Ficler and Goldberg, 2017; Dong et al., 2017). Similarly, there is also a body of research in reasoning about commonsense stories and discourse (Li and Jurafsky, 2017; Mostafazadeh et al., 2016) or detecting emotional stimuli in stories (Gui et al., 2017). Previous work in plot units (Lehnert, 1981) developed formalisms for affect and mental state in story narratives that included motivations and reactions. In our work, we collect mental state annotations for stories to used as a new resource in this space. Modeling Entity State Recently, novel works in language modeling (Ji et al., 2017; Yang et al., 2016), question answering (Henaff et al., 2017), and text generation (Kiddon et al., 2016; Bosselut et al., 2018) have shown that modeling entity state explicitly can boost performance while providing a preliminary interface for interpreting a model’s prediction. Entity modeling in these works, however, was limited to tracking entity reference (Kiddon et al., 2016; Yang et al., 2016; Ji et al., 2017), recognizing entity state similarity (Henaff et al., 2017) or predicting simple attributes from entity states (Bosselut et al., 2018). Our work provides a new dataset for tracking e"
P18-1213,D16-1032,1,0.827555,"of research in reasoning about commonsense stories and discourse (Li and Jurafsky, 2017; Mostafazadeh et al., 2016) or detecting emotional stimuli in stories (Gui et al., 2017). Previous work in plot units (Lehnert, 1981) developed formalisms for affect and mental state in story narratives that included motivations and reactions. In our work, we collect mental state annotations for stories to used as a new resource in this space. Modeling Entity State Recently, novel works in language modeling (Ji et al., 2017; Yang et al., 2016), question answering (Henaff et al., 2017), and text generation (Kiddon et al., 2016; Bosselut et al., 2018) have shown that modeling entity state explicitly can boost performance while providing a preliminary interface for interpreting a model’s prediction. Entity modeling in these works, however, was limited to tracking entity reference (Kiddon et al., 2016; Yang et al., 2016; Ji et al., 2017), recognizing entity state similarity (Henaff et al., 2017) or predicting simple attributes from entity states (Bosselut et al., 2018). Our work provides a new dataset for tracking emotional reactions and motivations of characters in stories. 9 Conclusion We present a large scale datas"
P18-1213,D14-1181,0,0.0199399,"is applied to these features to yield hs : hs = (Ws v s + bs ) (2) where Ws 2 Rd⇥H . The character vector hc is encoded in the same way on sentences in the context pertaining to the character. GloVe We extract pretrained Glove vectors (Pennington et al., 2014) for each word in V . The word embeddings are max-pooled, yielding embedding v s 2 RH , where H is the dimensionality of the Glove vectors. Using this max-pooled representation, hs and hc are extracted in the same manner as for TF-IDF features (Equation 2). CNN We implement a CNN text categorization model using the same configuration as Kim (2014) to encode the sentence words. A sentence is represented as a matrix, v s 2 RM ⇥d where each row is a word embedding xsn for a word wns 2 xs . 2294 v s = [xs0 , xs1 , . . . , xsN ] s s h = CNN(v ) (3) (4) where CNN represents the categorization model from (Kim, 2014). The character vector hc is encoded in the same way with a separate CNN. Implementation details are provided in the appendix. LSTM A two-layer bi-LSTM encodes the sentence words and concatenates the final time step hidden states from both directions to yield hs . The character vector hc is encoded the same way. REN We use the “tie"
P18-1213,D17-1019,0,0.0165332,"e reference annotation. 8 Related work Mental State Annotations Incorporating emotion theories into NLP tasks has been explored in previous projects. Ghosh et al. (2017) modulate language model distributions by increasing the probability of words that express certain affective LIWC (Tausczik and Pennebaker, 2016) categories. More generally, various projects tackle the problem of generating text from a set of attributes like sentiment or generic-ness (Ficler and Goldberg, 2017; Dong et al., 2017). Similarly, there is also a body of research in reasoning about commonsense stories and discourse (Li and Jurafsky, 2017; Mostafazadeh et al., 2016) or detecting emotional stimuli in stories (Gui et al., 2017). Previous work in plot units (Lehnert, 1981) developed formalisms for affect and mental state in story narratives that included motivations and reactions. In our work, we collect mental state annotations for stories to used as a new resource in this space. Modeling Entity State Recently, novel works in language modeling (Ji et al., 2017; Yang et al., 2016), question answering (Henaff et al., 2017), and text generation (Kiddon et al., 2016; Bosselut et al., 2018) have shown that modeling entity state expli"
P18-1213,D16-1230,0,0.0522412,"Missing"
P18-1213,L16-1163,0,0.0201785,"an example story. In addition, they depict multiple interactions between story characters, presenting rich opportunities to reason about character motivations and reactions. Furthermore, there are more than 98k such stories currently available covering a wide range of everyday scenarios. Unique Challenges While there have been a variety of annotated resources developed on the related topics of sentiment analysis (Mohammad and Turney, 2013; Deng and Wiebe, 2015), entity tracking (Hoffart et al., 2011; Weston et al., 2015), and story understanding (Goyal et al., 2010; Ouyang and McKeown, 2015; Lukin et al., 2016), our study is the first to annotate the full chains of mental state effects for story characters. This poses several unique challenges as annotations require (1) interpreting discourse (2) understanding implicit causal effects, and (3) understanding formal psychology theory categories. In prior literature, annotations of this complexity have typically been performed by experts (Deng and Wiebe, 2015; Ouyang and McKeown, 2015). While reliable, these annotations are prohibitively expensive to scale up. Therefore, we introduce a new annotation framework that pipelines a set of smaller isolated ta"
P18-1213,N16-1098,0,0.506936,"soning is remarkably hard for both statistical and neural machine readers – despite being trivial for humans. This stark performance gap between humans and machines is not surprising as most powerful language models have been designed to effectively learn local fluency patterns. Consequently, they generally lack the ability to abstract away from surface patterns in text to model more complex implied dynamics, such as intuiting characters’ mental states or predicting their plausible next actions. In this paper, we construct a new annotation formalism to densely label commonsense short stories (Mostafazadeh et al., 2016) in terms of the mental states of the characters. The resultangry [est eem ] They grew tired and started playing worse after a while. E M need rest E M The instructor was furious and threw his chair. afra id E M He cancelled practice and [dis gust, fe ar] expected us to perform tomorrow. E M Figure 1: A story example with partial annotations for motivations (dashed) and emotional reactions (solid). Open text explanations are in black (e.g., “frustrated”) and formal theory labels are in blue with brackets (e.g., “[esteem]”). ing dataset offers three unique properties. First, as highlighted in F"
P18-1213,D15-1257,0,0.0287071,"e-grained annotations with an example story. In addition, they depict multiple interactions between story characters, presenting rich opportunities to reason about character motivations and reactions. Furthermore, there are more than 98k such stories currently available covering a wide range of everyday scenarios. Unique Challenges While there have been a variety of annotated resources developed on the related topics of sentiment analysis (Mohammad and Turney, 2013; Deng and Wiebe, 2015), entity tracking (Hoffart et al., 2011; Weston et al., 2015), and story understanding (Goyal et al., 2010; Ouyang and McKeown, 2015; Lukin et al., 2016), our study is the first to annotate the full chains of mental state effects for story characters. This poses several unique challenges as annotations require (1) interpreting discourse (2) understanding implicit causal effects, and (3) understanding formal psychology theory categories. In prior literature, annotations of this complexity have typically been performed by experts (Deng and Wiebe, 2015; Ouyang and McKeown, 2015). While reliable, these annotations are prohibitively expensive to scale up. Therefore, we introduce a new annotation framework that pipelines a set o"
P18-1213,D14-1162,0,0.0941119,"ific context (C[ej ] ! hc ) and concatenating these encodings (he = [hc ; hs ]). We describe the encoders below: TF-IDF We learn a TD-IDF model on the full training corpus of Mostafazadeh et al. (2016) (excluding the stories in our dev/test sets). To encode the sentence, we extract TF-IDF features for its words, yielding v s 2 RV . A projection and nonlinearity is applied to these features to yield hs : hs = (Ws v s + bs ) (2) where Ws 2 Rd⇥H . The character vector hc is encoded in the same way on sentences in the context pertaining to the character. GloVe We extract pretrained Glove vectors (Pennington et al., 2014) for each word in V . The word embeddings are max-pooled, yielding embedding v s 2 RH , where H is the dimensionality of the Glove vectors. Using this max-pooled representation, hs and hc are extracted in the same manner as for TF-IDF features (Equation 2). CNN We implement a CNN text categorization model using the same configuration as Kim (2014) to encode the sentence words. A sentence is represented as a matrix, v s 2 RM ⇥d where each row is a word embedding xsn for a word wns 2 xs . 2294 v s = [xs0 , xs1 , . . . , xsN ] s s h = CNN(v ) (3) (4) where CNN represents the categorization model"
P18-1213,D16-1061,0,0.0273026,"o the physiological needs Maslow category, the food and rest motives from Reiss (2004) are very different. While the Reiss theory allows for finergrained annotations of motivation, the larger set of abstract concepts can be overwhelming for annotators. Motivated by Straker (2013), we design a hybrid approach, where Reiss labels are annotated as sub-categories of Maslow categories. 2.2 Emotion Theory Among several theories of emotion, we work with the “wheel of emotions” of Plutchik (1980), as it has been a common choice in prior literature on emotion categorization (Mohammad and Turney, 2013; Zhou et al., 2016). We use the eight basic emotional dimensions as illustrated in Figure 2. 2.3 Mental State Explanations In addition to the motivation and emotion categories derived from psychology theories, we also obtain open text descriptions of character mental states. These open text descriptions allow learning computational models that can explain the mental states of characters in natural language, which is likely to be more accessible and informative to end users than having theory categories alone. Collecting both theory categories and open text also allows us to learn the automatic mappings between t"
P18-1213,W17-0906,0,\N,Missing
P19-1470,Q17-1010,0,0.0364731,"Missing"
P19-1470,K17-1034,0,0.0462817,"Missing"
P19-1470,P18-2065,0,0.0144448,"traction (Suchanek et al., 2007; Hoffart et al., 2013; Auer et al., 2007; Bollacker et al., 2008) and unstructured text extraction (Dong et al., 2014; Carlson et al., 2010; Nakashole et al., 2011, 2012; Niu, 2012). In our work, we focus on construction of commonsense knowledge bases which require the use of open-text events rather than a well-defined relational schema structure. Other work in information extraction can also be applied to knowledge base construction with open-text entities (Soderland et al., 2010; Etzioni et al., 2011; Fader et al., 2011; Mausam et al., 2012; Fan et al., 2010; Cui et al., 2018), but these methods typically extract explicitly stated text relations. Conversely, our approach generates new knowledge that is often unstated in text, as commonsense information typically is (Gordon and Van Durme, 2013). Commonsense knowledge base completion Existing work on generation of novel commonsense knowledge has also used ConceptNet and ATOMIC as underlying KBs. Specifically, Li et al. (2016) proposed a set of neural network models for scoring tuples in ConceptNet. Our work differs from this approach as their models evaluate full tuples rather than learning to generate the phrases to"
P19-1470,D11-1142,0,0.366896,"the previous layer blocks from earlier time steps are input to the multi-headed attention with the preceding block for the current time step as the query. (c) Each token is an input to a first-layer block along with all preceding tokens. Dotted lines indicate outputs to all future blocks in the next layer and inputs from all preceding blocks in the previous layer. to model “entities&quot; as natural language phrases and relations as any concept that can link them (Li et al., 2016; Sap et al., 2019). OpenIE approaches display this property of open text entities and relations (Etzioni et al., 2011; Fader et al., 2011; Mausam et al., 2012), but being extractive, they only capture knowledge that is explicitly mentioned in text, limiting their applicability for capturing commonsense knowledge, which is often implicit (Gordon and Van Durme, 2013). Meanwhile, recent progress in training deep contextualized language models (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018) provides an opportunity to explore beyond extractive methods as an avenue for commonsense KB construction. These large-scale language models display impressive performance when their underlying representations are tuned to solve"
P19-1470,W10-0915,0,0.0821692,"Missing"
P19-1470,P16-1137,0,0.140334,"n extraction can also be applied to knowledge base construction with open-text entities (Soderland et al., 2010; Etzioni et al., 2011; Fader et al., 2011; Mausam et al., 2012; Fan et al., 2010; Cui et al., 2018), but these methods typically extract explicitly stated text relations. Conversely, our approach generates new knowledge that is often unstated in text, as commonsense information typically is (Gordon and Van Durme, 2013). Commonsense knowledge base completion Existing work on generation of novel commonsense knowledge has also used ConceptNet and ATOMIC as underlying KBs. Specifically, Li et al. (2016) proposed a set of neural network models for scoring tuples in ConceptNet. Our work differs from this approach as their models evaluate full tuples rather than learning to generate the phrases to make new nodes in the knowledge graph. Saito et al. (2018) builds upon this work by proposing a joint model for completion and generation of commonsense tuples. Their work, however, focuses on using tuple generation to augment their KB completion model, rather than to increase coverage in commonsense KB construction. Finally, Sap et al. (2019) use LSTM encoder-decoder models to generate commonsense kn"
P19-1470,D12-1048,0,0.143926,"blocks from earlier time steps are input to the multi-headed attention with the preceding block for the current time step as the query. (c) Each token is an input to a first-layer block along with all preceding tokens. Dotted lines indicate outputs to all future blocks in the next layer and inputs from all preceding blocks in the previous layer. to model “entities&quot; as natural language phrases and relations as any concept that can link them (Li et al., 2016; Sap et al., 2019). OpenIE approaches display this property of open text entities and relations (Etzioni et al., 2011; Fader et al., 2011; Mausam et al., 2012), but being extractive, they only capture knowledge that is explicitly mentioned in text, limiting their applicability for capturing commonsense knowledge, which is often implicit (Gordon and Van Durme, 2013). Meanwhile, recent progress in training deep contextualized language models (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018) provides an opportunity to explore beyond extractive methods as an avenue for commonsense KB construction. These large-scale language models display impressive performance when their underlying representations are tuned to solve end tasks, achieving"
P19-1470,D12-1104,0,0.0851585,"Missing"
P19-1470,D14-1162,0,0.0851866,"Missing"
P19-1470,N18-1202,0,0.0992616,"l preceding blocks in the previous layer. to model “entities&quot; as natural language phrases and relations as any concept that can link them (Li et al., 2016; Sap et al., 2019). OpenIE approaches display this property of open text entities and relations (Etzioni et al., 2011; Fader et al., 2011; Mausam et al., 2012), but being extractive, they only capture knowledge that is explicitly mentioned in text, limiting their applicability for capturing commonsense knowledge, which is often implicit (Gordon and Van Durme, 2013). Meanwhile, recent progress in training deep contextualized language models (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018) provides an opportunity to explore beyond extractive methods as an avenue for commonsense KB construction. These large-scale language models display impressive performance when their underlying representations are tuned to solve end tasks, achieving state-of-the-art results on a variety of complex problems. In this work, we define the COMmonsEnse Transformer (COMET ), which constructs commonsense KBs by using existing tuples as a seed set of knowledge on which to train. Using this seed set, a pre-trained language model learns to adapt its learned re"
P19-1470,K18-1014,0,0.0460182,"plicitly stated text relations. Conversely, our approach generates new knowledge that is often unstated in text, as commonsense information typically is (Gordon and Van Durme, 2013). Commonsense knowledge base completion Existing work on generation of novel commonsense knowledge has also used ConceptNet and ATOMIC as underlying KBs. Specifically, Li et al. (2016) proposed a set of neural network models for scoring tuples in ConceptNet. Our work differs from this approach as their models evaluate full tuples rather than learning to generate the phrases to make new nodes in the knowledge graph. Saito et al. (2018) builds upon this work by proposing a joint model for completion and generation of commonsense tuples. Their work, however, focuses on using tuple generation to augment their KB completion model, rather than to increase coverage in commonsense KB construction. Finally, Sap et al. (2019) use LSTM encoder-decoder models to generate commonsense knowledge about social situations. We use transformers and investigate the effect of using pre-trained language representations (Radford et al., 2018) to initialize them. Transformers and pre-training Finally, our work builds on previous work on adapting p"
P19-1472,P17-1152,0,0.0896587,"Missing"
P19-1472,P18-2103,0,0.0288274,"of these adversaries converges. Last, humans validate the data to remove adversarial endings that seem realistic. Importantly, AF creates a final dataset that is challenging to models regardless of the final dataset split. In Section 4, we will use AF as the underlying workhorse to construct an NLI dataset that is easy for humans, yet challenging for machines. This difficulty persists even when models are provided significant training data, and even when this data comes from the same distribution as the test set. This contrasts with past work on adversarial examples (e.g. Jia and Liang, 2017; Glockner et al., 2018; Belinkov and Bisk, 2018) which consider cases where an out-of-distribution test set is constructed to be adversarial. 3 Investigating SWAG In this section, we investigate why SWAG was solved. We focus on BERT, since it is the best Figure 4: BERT validation accuracy when trained and evaluated under several versions of SWAG, with the new dataset HellaSwag as comparison. We compare: Ending Only No context is provided; just the endings. Shuffled Endings that are indidivually tokenized, shu✏ed, and then detokenized. Shuffled+ No context is provided and each ending is Ending Only shu✏ed. known app"
P19-1472,N18-2017,0,0.0711883,"ed dataset creation algorithms. Whenever a new iteration of a dataset is created, these algorithms must leverage existing modeling advancements to filter out spurious biases. Only once this cycle becomes impossible can we say that the underlying task – as opposed an individual dataset – is solved. 2 Background SWAG is a dataset for commonsense NLI. For each question, a model is given a context from a video caption and four ending choices for what might happen next. Only one choice is right – the actual next caption of the video. Obtaining interesting negatives is challenging. Prior work (e.g. Gururangan et al., 2018; Poliak et al., 2018) has found that when humans write the endings to NLI questions, they introduce subtle yet strong class-conditional biases known as annotation artifacts.3 To address this, Zellers et al. (2018) introduced Adversarial Filtering (AF). An overview is shown in Figure 2. The key idea is to produce a dataset D which is adversarial for any arbitrary split of pDtrain , Dtest q. This requires a generator of negative candidates (i.e., wrong endings that vi3 These biases simply inflate model performance, but past work has also shown that are unwanted social biases induced when humans"
P19-1472,D17-1215,0,0.0477018,"s until the accuracy of these adversaries converges. Last, humans validate the data to remove adversarial endings that seem realistic. Importantly, AF creates a final dataset that is challenging to models regardless of the final dataset split. In Section 4, we will use AF as the underlying workhorse to construct an NLI dataset that is easy for humans, yet challenging for machines. This difficulty persists even when models are provided significant training data, and even when this data comes from the same distribution as the test set. This contrasts with past work on adversarial examples (e.g. Jia and Liang, 2017; Glockner et al., 2018; Belinkov and Bisk, 2018) which consider cases where an out-of-distribution test set is constructed to be adversarial. 3 Investigating SWAG In this section, we investigate why SWAG was solved. We focus on BERT, since it is the best Figure 4: BERT validation accuracy when trained and evaluated under several versions of SWAG, with the new dataset HellaSwag as comparison. We compare: Ending Only No context is provided; just the endings. Shuffled Endings that are indidivually tokenized, shu✏ed, and then detokenized. Shuffled+ No context is provided and each ending is Ending"
P19-1472,E17-2068,0,0.0965892,"Missing"
P19-1472,D18-1009,1,0.727951,"a knife to sharpen a stone, rather than vice versa. The last example comes from WikiHow, which appears to be incredibly challenging for BERT. BERT picks answer d, which has more words that match the context of technology (planes, traffic, laptop), but is incoherent.12 12 Among other issues, why would someone suddenly be aware that they are ‘flying at high speed on a plane...?’ 4797 Figure 11: Performance on the WikiHow subset of alternative variations of HellaSwag, where di↵erent Adversarial Filters are used (but without human validation). We consider the shallow stylistic adversaries used by Zellers et al. (2018) (Stylistic Ensemble), as well as an LSTM with ELMo embeddings, GPT, BERT-Base, and BERT-Large. For each adversarial filtering model, we record the accuracy of that model before and after AF is used. We also evaluate each alternative dataset using BERT-Large. The results suggest that using a a stronger model at test time (over the model used for AF) improves performance, but is not enough to solve the task. Figure 12: Estimated pretraining hours required to reach a desired accuracy on HellaSwag. We estimate perfomance with respect to a RTX 2080 Ti - a modern, fast GPU, and fit a log-linear reg"
P19-1472,N18-1202,0,0.119405,"Missing"
P19-1472,S18-2023,0,0.106988,"Missing"
P19-1472,S15-1024,0,0.0258476,"Missing"
P19-1539,W18-5709,0,0.13562,"round responses is distributed throughout the source document. Introduction While end-to-end neural conversation models (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017; Mostafazadeh et al., 2017). However, empirical results suggest that conditioning the decoder on rich and complex contexts, while helpful, does not on its own provide sufficient inductive bias for these systems to learn how to achieve deep and accurate integration between external knowledge and response generation. We posit that this ongoing challenge demands a more effective mechanism to support on-demand knowledge integration. We draw inspiration from how humans converse abo"
P19-1539,D18-1241,1,0.822774,"ent for a given question (Seo et al., 2017; Liu et al., 2018b; Yu et al., 2018). These models differ in how they fuse information between questions and documents. We chose SAN (Liu et al., 2018b) because of its representative architecture and competitive performance on existing MRC tasks. We note that other off-theshelf MRC models, such as BERT (Devlin et al., 2018), can also be plugged in. We leave the study of different MRC architectures for future work. Questions are treated as entirely independent in these “single-turn” MRC models, so recent work (e.g., CoQA (Reddy et al., 2019) and QuAC (Choi et al., 2018)) focuses on multi-turn MRC, modeling sequences of questions and answers in a conversation. While multi-turn MRC aims to answer complex questions, that body of work is restricted to factual questions, whereas our work—like much of the prior work in end-to-end dialogue—models free-form dialogue, which also encompasses chitchat and non-factual responses. 8 Conclusions We have demonstrated that the machine reading comprehension approach offers a promising step to generating, on the fly, contentful conversation exchanges that are grounded in extended text corpora. The functional combination of MRC"
P19-1539,D18-1045,0,0.0185929,"model training, with an initial learning rate of 0.0005. Batch size was set to 32. During training, all responses were truncated to have a maximum length of 30, and maximum query length and document length were set to 30, 500, respectively. we used regular teacher-forcing decoding during training. For inference, we found that top-k random sample decoding (Fan et al., 2018) provides the best results for all the systems. That is, at each decoding step, a token was drawn from the k most likely candidates according to the distribution over the vocabulary. Similar to recent work (Fan et al., 2018; Edunov et al., 2018), we set k = 20 (other common k values like 10 gave similar results). We selected key hyperparameter configurations on the validation set. 6.1 Evaluation Setup Table 2 shows automatic metrics for quantitative evaluation over three qualities of generated texts. We measure the overall relevance of the generated responses given the conversational history by using standard Machine Translation (MT) metrics, comparing generated outputs to ground-truth responses. These metrics include B LEU-4 (Papineni et al., 2002), M ETEOR (Lavie and Agarwal, 2007). and N IST (Doddington, 2002). The latter metric i"
P19-1539,P18-1082,0,0.0333813,"used the pretrained GloVe8 for initialization. We set hidden dimensions to 512 and dropout rate to 0.4. GRU cells are used for S EQ 2S EQ and M EM N ET (we also tested LSTM cells and obtained similar results). We used the Adam optimizer for model training, with an initial learning rate of 0.0005. Batch size was set to 32. During training, all responses were truncated to have a maximum length of 30, and maximum query length and document length were set to 30, 500, respectively. we used regular teacher-forcing decoding during training. For inference, we found that top-k random sample decoding (Fan et al., 2018) provides the best results for all the systems. That is, at each decoding step, a token was drawn from the k most likely candidates according to the distribution over the vocabulary. Similar to recent work (Fan et al., 2018; Edunov et al., 2018), we set k = 20 (other common k values like 10 gave similar results). We selected key hyperparameter configurations on the validation set. 6.1 Evaluation Setup Table 2 shows automatic metrics for quantitative evaluation over three qualities of generated texts. We measure the overall relevance of the generated responses given the conversational history b"
P19-1539,N19-1125,1,0.868164,"Missing"
P19-1539,W07-0734,0,0.0141127,"vocabulary. Similar to recent work (Fan et al., 2018; Edunov et al., 2018), we set k = 20 (other common k values like 10 gave similar results). We selected key hyperparameter configurations on the validation set. 6.1 Evaluation Setup Table 2 shows automatic metrics for quantitative evaluation over three qualities of generated texts. We measure the overall relevance of the generated responses given the conversational history by using standard Machine Translation (MT) metrics, comparing generated outputs to ground-truth responses. These metrics include B LEU-4 (Papineni et al., 2002), M ETEOR (Lavie and Agarwal, 2007). and N IST (Doddington, 2002). The latter metric is a variant of B LEU that weights n-gram matches by their information gain by effectively penalizing uninformative n-grams (such as “I don’t know”), which makes it a relevant metric for evaluating systems aiming diverse and informative responses. MT metrics may not be particularly adequate for our task (Liu et al., 2016), given its focus on the informativeness of responses, and for that reason we also use two other types of metrics to measure the level of grounding and diversity. As a diversity metric, we count all n-grams in the system output"
P19-1539,N16-1014,1,0.92711,"s recreated by the American television MythBusters. Four years later, […] two Praguebased journalists, claimed that Flight 367 had been mistaken for an enemy aircraft and shot down by the Czechoslovak Air Force at an altitude of 800 metres (2,600 ft). Figure 1: Users discussing a topic defined by a Wikipedia article. In this real-world example from our Reddit dataset, information needed to ground responses is distributed throughout the source document. Introduction While end-to-end neural conversation models (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017; Mostafazadeh et al., 2017). However, empirical results suggest that conditioning"
P19-1539,P16-1094,1,0.958274,"s recreated by the American television MythBusters. Four years later, […] two Praguebased journalists, claimed that Flight 367 had been mistaken for an enemy aircraft and shot down by the Czechoslovak Air Force at an altitude of 800 metres (2,600 ft). Figure 1: Users discussing a topic defined by a Wikipedia article. In this real-world example from our Reddit dataset, information needed to ground responses is distributed throughout the source document. Introduction While end-to-end neural conversation models (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017; Mostafazadeh et al., 2017). However, empirical results suggest that conditioning"
P19-1539,D16-1230,0,0.0886793,"Missing"
P19-1539,P18-1138,0,0.391323,"tributed throughout the source document. Introduction While end-to-end neural conversation models (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017; Mostafazadeh et al., 2017). However, empirical results suggest that conditioning the decoder on rich and complex contexts, while helpful, does not on its own provide sufficient inductive bias for these systems to learn how to achieve deep and accurate integration between external knowledge and response generation. We posit that this ongoing challenge demands a more effective mechanism to support on-demand knowledge integration. We draw inspiration from how humans converse about a topic, where"
P19-1539,P18-1157,1,0.938959,"tributed throughout the source document. Introduction While end-to-end neural conversation models (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017; Mostafazadeh et al., 2017). However, empirical results suggest that conditioning the decoder on rich and complex contexts, while helpful, does not on its own provide sufficient inductive bias for these systems to learn how to achieve deep and accurate integration between external knowledge and response generation. We posit that this ongoing challenge demands a more effective mechanism to support on-demand knowledge integration. We draw inspiration from how humans converse about a topic, where"
P19-1539,D15-1166,0,0.0725271,"n layer is applied to further ingest and capture the most salient information. The output memory, M ∈ Rd×n , is obtained by applying another BiLSTM layer for final information rearrangement. Note that d is the hidden size of the memory and n is the length of the document. 3.2 Response Generation Having read and processed both the conversation history and the extra knowledge in the document, the model then produces a free-form response y = (y1 , . . . , yT ) instead of generating a span or performing answer classification as in MRC tasks. We use an attentional recurrent neural network decoder (Luong et al., 2015) to generate response tokens while attending to the memory. At the beginning, the initial hidden state h0 is the weighted sum of the representation of the history X. For each decoding step t with a hidden state ht , we generate a token yt based on the distribution: p(yt ) = softmax((W1 ht + b)/τ ), (1) where τ > 0 is the softmax temperature. The hidden state ht is defined as follows: ht = W2 [zt ++fattention (zt , M )]. (2) Here, [· ++·] indicates a concatenation of two vectors; fattention is a dot-product attention (Vaswani et al., 2017); and zt is a state generated by GRU(et−1 , ht−1 ) with"
P19-1539,D18-1255,0,0.363735,"d-to-end neural conversation models (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017; Mostafazadeh et al., 2017). However, empirical results suggest that conditioning the decoder on rich and complex contexts, while helpful, does not on its own provide sufficient inductive bias for these systems to learn how to achieve deep and accurate integration between external knowledge and response generation. We posit that this ongoing challenge demands a more effective mechanism to support on-demand knowledge integration. We draw inspiration from how humans converse about a topic, where people often search and acquire external information as needed to"
P19-1539,I17-1047,1,0.933114,"Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017; Mostafazadeh et al., 2017). However, empirical results suggest that conditioning the decoder on rich and complex contexts, while helpful, does not on its own provide sufficient inductive bias for these systems to learn how to achieve deep and accurate integration between external knowledge and response generation. We posit that this ongoing challenge demands a more effective mechanism to support on-demand knowledge integration. We draw inspiration from how humans converse about a topic, where people often search and acquire external information as needed to continue a meaningful and informative conversation. Figure 1 i"
P19-1539,P02-1040,0,0.106889,"e decoding to draw yt from the above distribution p(yt ). Section 5 provides more details about the experimental configuration. 3.3 Data Weighting Scheme We further propose a simple data weighting scheme to encourage the generation of grounded responses. The idea is to bias the model training to fit better to those training instances where the ground-truth response is more closely relevant to the document. More specifically, given a training instance (X, D, y), we measure the closeness score c ∈ R between the document D and the gold response y (e.g., with the NIST (Doddington, 2002) or B LEU (Papineni et al., 2002) metrics). In each training data batch, we normalize the closeness scores of all the instances to have a sum of 1, and weight each of the instances with its corresponding normalized score when evaluating the 5429 # dialogues # utterances # documents # document sentences Train Valid Test 28.4k 2.36M 28.4k 15.18M 1.2k 0.12M 1.2k 0.58M 3.1k 0.34M 3.1k 1.68M 18.84 14.17 18.48 14.15 Average length (# words): utterances 18.74 document sentences 13.72 Table 1: Our grounded conversational dataset. training loss. This training regime promotes instances with grounded responses and thus encourages the mo"
P19-1539,D16-1264,0,0.230968,"ble at https://github.com/qkaren/ converse_reading_cmr. of turns X = (x1 , . . . , xM ) and a web document D = (s1 , . . . , sN ) as the knowledge source, where si is the ith sentence in the document. With the pair (X, D), the system needs to generate a natural language response y that is both conversationally appropriate and reflective of the contents of the web document. 3 Approach Our approach integrates conversation generation with on-demand MRC. Specifically, we use an MRC model to effectively encode the conversation history by treating it as a question in a typical QA task (e.g., SQuAD (Rajpurkar et al., 2016)), and encode the web document as the context. We then replace the output component of the MRC model (which is usually an answer classification module) with an attentional sequence generator that generates a free-form response. We refer to our approach as CMR (Conversation with on-demand Machine Reading). In general, any off-the-shelf MRC model could be applied here for knowledge comprehension. We use Stochastic Answer Networks (SAN)2 (Liu et al., 2018b), a performant machine reading model that until very recently held state-of-the-art performance on the SQuAD benchmark. We also employ a simpl"
P19-1539,Q19-1016,0,0.0224858,"ng meaning. from a given document for a given question (Seo et al., 2017; Liu et al., 2018b; Yu et al., 2018). These models differ in how they fuse information between questions and documents. We chose SAN (Liu et al., 2018b) because of its representative architecture and competitive performance on existing MRC tasks. We note that other off-theshelf MRC models, such as BERT (Devlin et al., 2018), can also be plugged in. We leave the study of different MRC architectures for future work. Questions are treated as entirely independent in these “single-turn” MRC models, so recent work (e.g., CoQA (Reddy et al., 2019) and QuAC (Choi et al., 2018)) focuses on multi-turn MRC, modeling sequences of questions and answers in a conversation. While multi-turn MRC aims to answer complex questions, that body of work is restricted to factual questions, whereas our work—like much of the prior work in end-to-end dialogue—models free-form dialogue, which also encompasses chitchat and non-factual responses. 8 Conclusions We have demonstrated that the machine reading comprehension approach offers a promising step to generating, on the fly, contentful conversation exchanges that are grounded in extended text corpora. The"
P19-1539,P15-1152,0,0.0338416,"fall without a parachute: 10,160 metres (33,330 ft). …… …… In 2005, Vulović‘s fall was recreated by the American television MythBusters. Four years later, […] two Praguebased journalists, claimed that Flight 367 had been mistaken for an enemy aircraft and shot down by the Czechoslovak Air Force at an altitude of 800 metres (2,600 ft). Figure 1: Users discussing a topic defined by a Wikipedia article. In this real-world example from our Reddit dataset, information needed to ground responses is distributed throughout the source document. Introduction While end-to-end neural conversation models (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017;"
P19-1539,P18-1205,0,0.0465991,"compared to machine translation, it is common for the generator to retain focus on the key information in the external document to produce semantically relevant responses. 7 Related Work Dialogue: Traditional dialogue systems (see (Jurafsky and Martin, 2009) for an historical perspective) are typically grounded, enabling these systems to be reflective of the user’s environment. The lack of grounding has been a stumbling block for the earliest end-to-end dialogue systems, as various researchers have noted that their outputs tend to be bland (Li et al., 2016a; Gao et al., 2019b), inconsistent (Zhang et al., 2018a; Li et al., Figure 3: Attention weights between words of the documents and words of the response. Dark (blue) cells represent probabilities closer to 1. 2016b; Zhang et al., 2019), and lacking in factual content (Ghazvininejad et al., 2018; Agarwal et al., 2018). Recently there has been growing interest in exploring different forms of grounding, including images, knowledge bases, and plain texts (Das et al., 2017; Mostafazadeh et al., 2017; Agarwal et al., 2018; Yang et al., 2019). A recent survey is included in Gao et al. (2019a). Prior work, e.g, (Ghazvininejad et al., 2018; Zhang et al.,"
P19-1539,N15-1020,1,0.77954,"hute: 10,160 metres (33,330 ft). …… …… In 2005, Vulović‘s fall was recreated by the American television MythBusters. Four years later, […] two Praguebased journalists, claimed that Flight 367 had been mistaken for an enemy aircraft and shot down by the Czechoslovak Air Force at an altitude of 800 metres (2,600 ft). Figure 1: Users discussing a topic defined by a Wikipedia article. In this real-world example from our Reddit dataset, information needed to ground responses is distributed throughout the source document. Introduction While end-to-end neural conversation models (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017; Mostafazadeh et al.,"
P19-1539,N19-1423,0,\N,Missing
Q14-1028,J05-3002,0,0.0115444,"(due( to(generaliza&on(error( Completely(wrong( & & Figure 10: Description generation: bad examples. plausible, thanks to the expressive, but somewhat predictable descriptions online users write about their photos. Even among the bad examples (Figure 10) one can find highly creative captions with not literal but metaphorical relevance: “Monarch in her bedroom before the wedding ceremony”.12 The complete system captions and the original captions are available at http://ilp-cky.appspot. com/ 6 Related Work Sentence Fusion Sentence fusion has been studied mostly for multi-document summarization (Barzilay and McKeown, 2005), where redundancy across multiple sentences serves as a guideline for syntactic and semantic validity of generation. In contrast, we do not have the natural redundancy to rely upon in our task, therefore requiring the composition algorithm to be intrinsically better constrained for correct sentence structures. 12 “Monarch” can be a type of butterfly. 360 Sentence Compression At the core of the image caption generalization task is sentence compression. Much work has considered deletion-only edits like ours (Knight and Marcu, 2000; Turner and Charniak, 2005; Cohn and Lapata, 2007; Filippova and"
Q14-1028,D07-1008,0,0.0204284,"ization (Barzilay and McKeown, 2005), where redundancy across multiple sentences serves as a guideline for syntactic and semantic validity of generation. In contrast, we do not have the natural redundancy to rely upon in our task, therefore requiring the composition algorithm to be intrinsically better constrained for correct sentence structures. 12 “Monarch” can be a type of butterfly. 360 Sentence Compression At the core of the image caption generalization task is sentence compression. Much work has considered deletion-only edits like ours (Knight and Marcu, 2000; Turner and Charniak, 2005; Cohn and Lapata, 2007; Filippova and Altun, 2013), while recent ones explore more complex edits, such as substitutions, insertions and reordering (Cohn and Lapata, 2008). The latter generally requires a larger training corpus. We leave more expressive compression as a future research work. 7 Conclusion In this paper, we have presented a novel tree composition approach for generating expressive image descriptions. As an optional preprocessing step, we also presented a tree compression approach and reported the empirical benefit of using automatically compressed captions to improve image description generation. By i"
Q14-1028,C08-1018,0,0.0181707,"ation. In contrast, we do not have the natural redundancy to rely upon in our task, therefore requiring the composition algorithm to be intrinsically better constrained for correct sentence structures. 12 “Monarch” can be a type of butterfly. 360 Sentence Compression At the core of the image caption generalization task is sentence compression. Much work has considered deletion-only edits like ours (Knight and Marcu, 2000; Turner and Charniak, 2005; Cohn and Lapata, 2007; Filippova and Altun, 2013), while recent ones explore more complex edits, such as substitutions, insertions and reordering (Cohn and Lapata, 2008). The latter generally requires a larger training corpus. We leave more expressive compression as a future research work. 7 Conclusion In this paper, we have presented a novel tree composition approach for generating expressive image descriptions. As an optional preprocessing step, we also presented a tree compression approach and reported the empirical benefit of using automatically compressed captions to improve image description generation. By integrating both the tree structure and the sequence structure, we have significantly improved the quality of composed image captions over several co"
Q14-1028,W11-2107,0,0.00646166,"ocks. We also experiment with the compression of human written captions, which are used to generate image descriptions for the new target images. Baselines for Compression: • S EQ C OMPRESSION (Kuznetsova et al., 2013): Inference operates over the sequence structure. Although optimization is subject to constraints derived from dependency parse, parsing is not an explicit part of the inference structure. Example outputs are shown in Figure 7. 5.1 Automatic Evaluation We perform automatic evaluation using two measures widely used in machine translation: BLEU (Papineni et al., 2002)8 and METEOR (Denkowski and Lavie, 2011).9 We remove all punctuation and convert captions to lower case. We use 1K test images from the captioned image corpus,10 and assume the original captions as the gold standard captions to compare against. The results in Table 1 8 We use the unigram NIST implementation: ftp://jaguar. ncsl.nist.gov/mt/resources/mteval-v13a-20091001.tar.gz 9 With equal weight between precision and recall in Table 1. 10 Except for those for which image URLs are broken, or CPLEX did not return a solution. Method-1 S EQ +T REE S EQ +T REE S EQ +T REE S EQ +T REE +P RUNING S EQ +T REE +P RUNING S EQ +T REE +P RUNING"
Q14-1028,N12-1094,1,0.605802,"Feng and Lapata, 2013; Mason, 2013; Ordonez et al., 2011). In these approaches, the set of what can be described can be substantially larger than the set of what can be recognized, where the former is shaped and defined by the data, rather than by humans. This allows the resulting descriptions to be substantially more expressive, elaborate, and interesting than what would be possible in a purely bottom-up manner. Our work contributes to this second line of research. One challenge in utilizing naturally existing multimodal data, however, is the noisy semantic alignment between images and text (Dodge et al., 2012; Berg et al., 2010). Therefore, we also investigate a related task of image caption generalization (Kuznetsova et al., 2013), which aims to improve the semantic image-text alignment by removing bits of text from existing captions that are less likely to be transferable to other images. The high-level idea of our system is to harvest useful bits of text (as tree fragments) from existing image descriptions using detected visual content similarity, and then to compose a new description by selectively combining these extracted (and optionally pruned) tree fragments. This overall idea 352 of compo"
Q14-1028,D13-1128,0,0.0557878,"ents are tree composition and compression, both integrating tree structure with sequence structure. Our proposed system attains significantly better performance than previous approaches for both image caption generalization and generation. In addition, our work is the first to show the empirical benefit of automatically generalized captions for composing natural image descriptions. 1 There has been a recent spike in efforts to automatically describe visual content in natural language (Yang et al., 2011; Kulkarni et al., 2011; Li et al., 2011; Farhadi et al., 2010; Krishnamoorthy et al., 2013; Elliott and Keller, 2013; Yu and Siskind, 2013; Socher et al., 2014). This reflects the long standing understanding that encoding the complexities and subtleties of image content often requires more expressive language constructs than a set of tags. Now that visual recognition algorithms are beginning to produce reliable estimates of image content (Perronnin et al., 2012; Deng et al., 2012a; Deng et al., 2010; Krizhevsky et al., 2012), the time seems ripe to begin exploring higher level semantic tasks. Introduction The web is increasingly visual, with hundreds of billions of user contributed photographs hosted online"
Q14-1028,P14-2074,0,0.222242,"Missing"
Q14-1028,D13-1155,0,0.0220423,"cKeown, 2005), where redundancy across multiple sentences serves as a guideline for syntactic and semantic validity of generation. In contrast, we do not have the natural redundancy to rely upon in our task, therefore requiring the composition algorithm to be intrinsically better constrained for correct sentence structures. 12 “Monarch” can be a type of butterfly. 360 Sentence Compression At the core of the image caption generalization task is sentence compression. Much work has considered deletion-only edits like ours (Knight and Marcu, 2000; Turner and Charniak, 2005; Cohn and Lapata, 2007; Filippova and Altun, 2013), while recent ones explore more complex edits, such as substitutions, insertions and reordering (Cohn and Lapata, 2008). The latter generally requires a larger training corpus. We leave more expressive compression as a future research work. 7 Conclusion In this paper, we have presented a novel tree composition approach for generating expressive image descriptions. As an optional preprocessing step, we also presented a tree compression approach and reported the empirical benefit of using automatically compressed captions to improve image description generation. By integrating both the tree str"
Q14-1028,P03-1054,0,0.0396397,"Missing"
Q14-1028,W13-1302,0,0.0425965,"ments. Key algorithmic components are tree composition and compression, both integrating tree structure with sequence structure. Our proposed system attains significantly better performance than previous approaches for both image caption generalization and generation. In addition, our work is the first to show the empirical benefit of automatically generalized captions for composing natural image descriptions. 1 There has been a recent spike in efforts to automatically describe visual content in natural language (Yang et al., 2011; Kulkarni et al., 2011; Li et al., 2011; Farhadi et al., 2010; Krishnamoorthy et al., 2013; Elliott and Keller, 2013; Yu and Siskind, 2013; Socher et al., 2014). This reflects the long standing understanding that encoding the complexities and subtleties of image content often requires more expressive language constructs than a set of tags. Now that visual recognition algorithms are beginning to produce reliable estimates of image content (Perronnin et al., 2012; Deng et al., 2012a; Deng et al., 2010; Krizhevsky et al., 2012), the time seems ripe to begin exploring higher level semantic tasks. Introduction The web is increasingly visual, with hundreds of billions of user contributed"
Q14-1028,P12-1038,1,0.879464,"ple, it would be hard to compose “I noticed that this funny cow was staring at me” or “You can see these beautiful hills only in the countryside” in a purely bottom-up manner based on the exact content detected. The key technical bottleneck is that the range of describable content (i.e., objects, attributes, actions) is ultimately confined by the set of items that can be reliably recognized by state-ofthe-art vision techniques. The second direction, in a complementary avenue to the first, has explored ways to make use of the rich spectrum of visual descriptions contributed by online citizens (Kuznetsova et al., 2012; Feng and Lapata, 2013; Mason, 2013; Ordonez et al., 2011). In these approaches, the set of what can be described can be substantially larger than the set of what can be recognized, where the former is shaped and defined by the data, rather than by humans. This allows the resulting descriptions to be substantially more expressive, elaborate, and interesting than what would be possible in a purely bottom-up manner. Our work contributes to this second line of research. One challenge in utilizing naturally existing multimodal data, however, is the noisy semantic alignment between images and text"
Q14-1028,P13-2138,1,0.682301,"substantially larger than the set of what can be recognized, where the former is shaped and defined by the data, rather than by humans. This allows the resulting descriptions to be substantially more expressive, elaborate, and interesting than what would be possible in a purely bottom-up manner. Our work contributes to this second line of research. One challenge in utilizing naturally existing multimodal data, however, is the noisy semantic alignment between images and text (Dodge et al., 2012; Berg et al., 2010). Therefore, we also investigate a related task of image caption generalization (Kuznetsova et al., 2013), which aims to improve the semantic image-text alignment by removing bits of text from existing captions that are less likely to be transferable to other images. The high-level idea of our system is to harvest useful bits of text (as tree fragments) from existing image descriptions using detected visual content similarity, and then to compose a new description by selectively combining these extracted (and optionally pruned) tree fragments. This overall idea 352 of composition based on extracted phrases is not new in itself (Kuznetsova et al., 2012), however, we make several technical and empi"
Q14-1028,W11-0326,1,0.281035,"acted (and optionally pruned) tree fragments. Key algorithmic components are tree composition and compression, both integrating tree structure with sequence structure. Our proposed system attains significantly better performance than previous approaches for both image caption generalization and generation. In addition, our work is the first to show the empirical benefit of automatically generalized captions for composing natural image descriptions. 1 There has been a recent spike in efforts to automatically describe visual content in natural language (Yang et al., 2011; Kulkarni et al., 2011; Li et al., 2011; Farhadi et al., 2010; Krishnamoorthy et al., 2013; Elliott and Keller, 2013; Yu and Siskind, 2013; Socher et al., 2014). This reflects the long standing understanding that encoding the complexities and subtleties of image content often requires more expressive language constructs than a set of tags. Now that visual recognition algorithms are beginning to produce reliable estimates of image content (Perronnin et al., 2012; Deng et al., 2012a; Deng et al., 2010; Krizhevsky et al., 2012), the time seems ripe to begin exploring higher level semantic tasks. Introduction The web is increasingly vi"
Q14-1028,W13-1301,0,0.037485,"Missing"
Q14-1028,N13-2010,0,0.0092074,"this funny cow was staring at me” or “You can see these beautiful hills only in the countryside” in a purely bottom-up manner based on the exact content detected. The key technical bottleneck is that the range of describable content (i.e., objects, attributes, actions) is ultimately confined by the set of items that can be reliably recognized by state-ofthe-art vision techniques. The second direction, in a complementary avenue to the first, has explored ways to make use of the rich spectrum of visual descriptions contributed by online citizens (Kuznetsova et al., 2012; Feng and Lapata, 2013; Mason, 2013; Ordonez et al., 2011). In these approaches, the set of what can be described can be substantially larger than the set of what can be recognized, where the former is shaped and defined by the data, rather than by humans. This allows the resulting descriptions to be substantially more expressive, elaborate, and interesting than what would be possible in a purely bottom-up manner. Our work contributes to this second line of research. One challenge in utilizing naturally existing multimodal data, however, is the noisy semantic alignment between images and text (Dodge et al., 2012; Berg et al., 2"
Q14-1028,E12-1076,1,0.386976,"Missing"
Q14-1028,P02-1040,0,0.108486,"ons of T REE P RUNING as building blocks. We also experiment with the compression of human written captions, which are used to generate image descriptions for the new target images. Baselines for Compression: • S EQ C OMPRESSION (Kuznetsova et al., 2013): Inference operates over the sequence structure. Although optimization is subject to constraints derived from dependency parse, parsing is not an explicit part of the inference structure. Example outputs are shown in Figure 7. 5.1 Automatic Evaluation We perform automatic evaluation using two measures widely used in machine translation: BLEU (Papineni et al., 2002)8 and METEOR (Denkowski and Lavie, 2011).9 We remove all punctuation and convert captions to lower case. We use 1K test images from the captioned image corpus,10 and assume the original captions as the gold standard captions to compare against. The results in Table 1 8 We use the unigram NIST implementation: ftp://jaguar. ncsl.nist.gov/mt/resources/mteval-v13a-20091001.tar.gz 9 With equal weight between precision and recall in Table 1. 10 Except for those for which image URLs are broken, or CPLEX did not return a solution. Method-1 S EQ +T REE S EQ +T REE S EQ +T REE S EQ +T REE +P RUNING S EQ"
Q14-1028,W04-2401,0,0.0347033,"Missing"
Q14-1028,D08-1027,0,0.0304626,"Missing"
Q14-1028,Q14-1017,0,0.0846068,"integrating tree structure with sequence structure. Our proposed system attains significantly better performance than previous approaches for both image caption generalization and generation. In addition, our work is the first to show the empirical benefit of automatically generalized captions for composing natural image descriptions. 1 There has been a recent spike in efforts to automatically describe visual content in natural language (Yang et al., 2011; Kulkarni et al., 2011; Li et al., 2011; Farhadi et al., 2010; Krishnamoorthy et al., 2013; Elliott and Keller, 2013; Yu and Siskind, 2013; Socher et al., 2014). This reflects the long standing understanding that encoding the complexities and subtleties of image content often requires more expressive language constructs than a set of tags. Now that visual recognition algorithms are beginning to produce reliable estimates of image content (Perronnin et al., 2012; Deng et al., 2012a; Deng et al., 2010; Krizhevsky et al., 2012), the time seems ripe to begin exploring higher level semantic tasks. Introduction The web is increasingly visual, with hundreds of billions of user contributed photographs hosted online. A substantial portion of these images have"
Q14-1028,P05-1036,0,0.00988288,"y for multi-document summarization (Barzilay and McKeown, 2005), where redundancy across multiple sentences serves as a guideline for syntactic and semantic validity of generation. In contrast, we do not have the natural redundancy to rely upon in our task, therefore requiring the composition algorithm to be intrinsically better constrained for correct sentence structures. 12 “Monarch” can be a type of butterfly. 360 Sentence Compression At the core of the image caption generalization task is sentence compression. Much work has considered deletion-only edits like ours (Knight and Marcu, 2000; Turner and Charniak, 2005; Cohn and Lapata, 2007; Filippova and Altun, 2013), while recent ones explore more complex edits, such as substitutions, insertions and reordering (Cohn and Lapata, 2008). The latter generally requires a larger training corpus. We leave more expressive compression as a future research work. 7 Conclusion In this paper, we have presented a novel tree composition approach for generating expressive image descriptions. As an optional preprocessing step, we also presented a tree compression approach and reported the empirical benefit of using automatically compressed captions to improve image descr"
Q14-1028,D11-1041,0,0.370974,"cription by selectively combining the extracted (and optionally pruned) tree fragments. Key algorithmic components are tree composition and compression, both integrating tree structure with sequence structure. Our proposed system attains significantly better performance than previous approaches for both image caption generalization and generation. In addition, our work is the first to show the empirical benefit of automatically generalized captions for composing natural image descriptions. 1 There has been a recent spike in efforts to automatically describe visual content in natural language (Yang et al., 2011; Kulkarni et al., 2011; Li et al., 2011; Farhadi et al., 2010; Krishnamoorthy et al., 2013; Elliott and Keller, 2013; Yu and Siskind, 2013; Socher et al., 2014). This reflects the long standing understanding that encoding the complexities and subtleties of image content often requires more expressive language constructs than a set of tags. Now that visual recognition algorithms are beginning to produce reliable estimates of image content (Perronnin et al., 2012; Deng et al., 2012a; Deng et al., 2010; Krizhevsky et al., 2012), the time seems ripe to begin exploring higher level semantic tasks."
Q14-1028,P13-1006,0,0.0115706,"and compression, both integrating tree structure with sequence structure. Our proposed system attains significantly better performance than previous approaches for both image caption generalization and generation. In addition, our work is the first to show the empirical benefit of automatically generalized captions for composing natural image descriptions. 1 There has been a recent spike in efforts to automatically describe visual content in natural language (Yang et al., 2011; Kulkarni et al., 2011; Li et al., 2011; Farhadi et al., 2010; Krishnamoorthy et al., 2013; Elliott and Keller, 2013; Yu and Siskind, 2013; Socher et al., 2014). This reflects the long standing understanding that encoding the complexities and subtleties of image content often requires more expressive language constructs than a set of tags. Now that visual recognition algorithms are beginning to produce reliable estimates of image content (Perronnin et al., 2012; Deng et al., 2012a; Deng et al., 2010; Krizhevsky et al., 2012), the time seems ripe to begin exploring higher level semantic tasks. Introduction The web is increasingly visual, with hundreds of billions of user contributed photographs hosted online. A substantial portio"
Q19-1014,P17-1055,0,0.0523942,"Missing"
Q19-1014,P17-1168,0,0.0194867,"ion to improve the original sliding window baseline, formulated in Expression 8 (Section 4.2). • Stanford Attentive Reader This neural baseline compares each candidate answer (i.e., entity) representation to the question-aware document representation built with attention mechanism (Hermann et al., 2015; Chen et al., 2016). Lai et al. (2017) add a bilinear operation to compare document and answer option representations to answer multiple-choice questions. • Gated-Attention Reader The baseline models multiplicative question-specific document representations based on a gated-attention mechanism (Dhingra et al., 2017), which are then compared to each answer option (Lai et al., 2017). • Co-Matching This state-of-the-art multiplechoice reading comprehension model explicitly treats question and answer option as two sequences and jointly matches them against a given document (Wang et al., 2018b). Experiment 5.1 Baselines We implement several baselines, including rulebased methods and state-of-the-art neural models. • Finetuned Transformer LM This is a general task-agnostic model introduced in Section 4.4, which achieves the best reported performance on several tasks requiring multisentence reasoning (Radford e"
Q19-1014,P04-3031,0,0.108587,"The formal definition of δis is as follows. s (7) To make the final answer option selection, our rule-based method combines Expressions (5) and (7): s IQ δis = (6) s swi Q + swi∗ d Q + d∗i − i 2 2 (5) • Pointwise mutual information (PMI): sQ sQ ∗ ∗ pmimax ,1..3 , pmimax,1..3 , pmimin,1..3, pmimin,1..3, sQ pmiavg,1..3 , and pmi∗avg,1..3 , where pmisf,i is defined as Since a large percentage of questions cannot be solved by word-level matching, we also attempt to incorporate general world knowledge into our rule-based method. We calculate cssi , the  3 We use the list of stop words from NLTK (Bird and Loper, 2004). pmisf,i = 223 j log fk s Oi ,WkD ) Oi s C1 (Wj )C1 (WkD ) C 2 (W j |W Oi | (9) C1 (w) denotes the word frequency of w in external copora (we use Reddit posts [Tan and Lee, 2015]), and C2 (w1 , w2 ) represents the co-occurrence frequency of word w1 and w2 within a distance < K in external copora. We use PMI to evaluate the relatedness between the content of an answer option and the target-speaker-focused context based on co-occurrences of words in external corpora, inspired by previous studies on narrative event chains (Chambers and Jurafsky, 2008). • ConceptNet relations (CR): cr1..3,1..|R |"
Q19-1014,P08-1090,0,0.0431185,", the  3 We use the list of stop words from NLTK (Bird and Loper, 2004). pmisf,i = 223 j log fk s Oi ,WkD ) Oi s C1 (Wj )C1 (WkD ) C 2 (W j |W Oi | (9) C1 (w) denotes the word frequency of w in external copora (we use Reddit posts [Tan and Lee, 2015]), and C2 (w1 , w2 ) represents the co-occurrence frequency of word w1 and w2 within a distance < K in external copora. We use PMI to evaluate the relatedness between the content of an answer option and the target-speaker-focused context based on co-occurrences of words in external corpora, inspired by previous studies on narrative event chains (Chambers and Jurafsky, 2008). • ConceptNet relations (CR): cr1..3,1..|R |. R = {r1 , r2 , . . .} is the set of ConceptNet relation types (e.g., ‘‘CapableOf’’ and ‘‘PartOf’’). cri,j is the number of relation triples (w1 , rj , w2 ) that appear in the ConceptNet (Speer et al., 2017), where w1 represents a word in answer option Oi , w2 represents a word in D, and the relation type rj ∈ R. Similar to the motivation for using PMI, we use CR to capture the association between an answer option and the source dialogue based on raw co-occurrence counts in the commonsense knowledge base. • ConceptNet embeddings (CE): Besides the l"
Q19-1014,P17-1025,1,0.884975,"Missing"
Q19-1014,P16-1223,0,0.0604832,"Missing"
Q19-1014,W16-3612,0,0.28737,"ractive since candidate answers are usually short spans from source documents. State-of-the-art neural models with attention mechanisms already achieve very high performance based on local lexical information. Recently researchers work on the construction of spoken span-based data sets (Lee et al., 2018; Li et al., 2018) by applying text-to-speech technologies or recruiting human speakers based on formal written document-based data sets such as SQuAD (Rajpurkar et al., 2016). Some spanbased conversation data sets are constructed from a relatively small size of dialogues from television shows (Chen and Choi, 2016; Ma et al., 2018). Considering the limitations in extractive data sets, answers in abstractive data sets such as MS MARCO (Nguyen et al., 2016), SearchQA (Dunn et al., 2017), and NarrativeQA (Koˇcisk`y et al., 2018) are human-crowdsourced based on source documents or summaries. Concurrently, there is a growing interest in conversational reading comprehension such as CoQA (Reddy et al., 2018). Because annotators tend to copy spans as answers (Reddy et al., 2018), the majority of answers are still extractive in these data sets (Table 2). Compared to the data sets mentioned above, most of the co"
Q19-1014,C18-1018,0,0.0606449,"Missing"
Q19-1014,D18-1241,1,0.873778,"Missing"
Q19-1014,P17-1147,0,0.0815762,"Missing"
Q19-1014,N18-1023,0,0.133945,"Missing"
Q19-1014,Q18-1023,0,0.115052,"Missing"
Q19-1014,D16-1241,0,0.0576388,"Missing"
Q19-1014,S18-1119,0,0.10702,"Missing"
Q19-1014,P11-2057,0,0.205028,"Missing"
Q19-1014,N18-1185,0,0.441948,"te answers are usually short spans from source documents. State-of-the-art neural models with attention mechanisms already achieve very high performance based on local lexical information. Recently researchers work on the construction of spoken span-based data sets (Lee et al., 2018; Li et al., 2018) by applying text-to-speech technologies or recruiting human speakers based on formal written document-based data sets such as SQuAD (Rajpurkar et al., 2016). Some spanbased conversation data sets are constructed from a relatively small size of dialogues from television shows (Chen and Choi, 2016; Ma et al., 2018). Considering the limitations in extractive data sets, answers in abstractive data sets such as MS MARCO (Nguyen et al., 2016), SearchQA (Dunn et al., 2017), and NarrativeQA (Koˇcisk`y et al., 2018) are human-crowdsourced based on source documents or summaries. Concurrently, there is a growing interest in conversational reading comprehension such as CoQA (Reddy et al., 2018). Because annotators tend to copy spans as answers (Reddy et al., 2018), the majority of answers are still extractive in these data sets (Table 2). Compared to the data sets mentioned above, most of the correct answer optio"
Q19-1014,N18-1202,0,0.0524594,"Missing"
Q19-1014,D18-1260,0,0.067851,"Missing"
Q19-1014,D16-1264,0,0.392475,"Missing"
Q19-1014,D13-1020,0,0.409742,"‘‘really hot,’’ ‘‘really beautiful,’’ ‘‘very bad,’’ and ‘‘very important’’ rather than more appropriate yet more advanced adjectives that might hinder reading comprehension of language learners with smaller vocabularies. According to the explanations provided by the tool, the readability scores for both data sets fall into the same category ‘‘Your text is very simple and easy to read, likely to be understood by an average 5th-grader (age 10).’’ 4 4.2 Rule-Based Approaches We first attempt to incorporate dialogue structure information into sliding window (SW), a rulebased approach developed by Richardson et al. (2013). This approach matches a bag of words constructed from a question Q and one of its answer option Oi with a given document, and calculates the TF-IDF style matching score for each answer option. ˆ , and O ˆ i be the unordered set of ˆ s, Q Let D distinct words (excluding punctuation marks) in Ds , Q, and Oi , respectively. Instead of only regarding dialogue D as a non-conversational text snippet, we also pay special attention to the context that is relevant to the target speaker mentioned in the question. Therefore, given a target speaker sQ , we propose to compute a speaker-focused sliding wi"
Q19-1014,D18-1132,0,0.0646353,"(Hermann et al., 2015; Chen et al., 2016). Lai et al. (2017) add a bilinear operation to compare document and answer option representations to answer multiple-choice questions. • Gated-Attention Reader The baseline models multiplicative question-specific document representations based on a gated-attention mechanism (Dhingra et al., 2017), which are then compared to each answer option (Lai et al., 2017). • Co-Matching This state-of-the-art multiplechoice reading comprehension model explicitly treats question and answer option as two sequences and jointly matches them against a given document (Wang et al., 2018b). Experiment 5.1 Baselines We implement several baselines, including rulebased methods and state-of-the-art neural models. • Finetuned Transformer LM This is a general task-agnostic model introduced in Section 4.4, which achieves the best reported performance on several tasks requiring multisentence reasoning (Radford et al., 2018). • Word Matching This strong baseline (Yih et al., 2013) selects the answer option that has the highest count of overlapping words with the given dialogue. 225 Method Dev Test Random Word Matching (WM) (Yih et al., 2013) Sliding Window (SW) (Richardson et al., 201"
Q19-1014,P18-2118,0,0.297659,"(Hermann et al., 2015; Chen et al., 2016). Lai et al. (2017) add a bilinear operation to compare document and answer option representations to answer multiple-choice questions. • Gated-Attention Reader The baseline models multiplicative question-specific document representations based on a gated-attention mechanism (Dhingra et al., 2017), which are then compared to each answer option (Lai et al., 2017). • Co-Matching This state-of-the-art multiplechoice reading comprehension model explicitly treats question and answer option as two sequences and jointly matches them against a given document (Wang et al., 2018b). Experiment 5.1 Baselines We implement several baselines, including rulebased methods and state-of-the-art neural models. • Finetuned Transformer LM This is a general task-agnostic model introduced in Section 4.4, which achieves the best reported performance on several tasks requiring multisentence reasoning (Radford et al., 2018). • Word Matching This strong baseline (Yih et al., 2013) selects the answer option that has the highest count of overlapping words with the given dialogue. 225 Method Dev Test Random Word Matching (WM) (Yih et al., 2013) Sliding Window (SW) (Richardson et al., 201"
Q19-1014,P13-1171,0,0.114272,"Missing"
R11-1043,N03-1016,0,0.0141752,"am that captures repeated phrases, (4) mood words that capture author’s unique emotional traits, and (5) stop word frequencies that capture author’s writing habit with common words. Each of these features is elaborated below. only the most frequent 100 3-grams of part-ofspeech tags as features. To encode a feature from each such 3-gram POS sequence, we use the frequency of each POS sequence normalized by the number of POS grams in the document. We expect these shallow syntactic patterns will help characterize the favorite sentence structure used by the authors. We make use of Stanford parser (Klein and Manning, 2003) to tag the part-of-speech tags for the given document. 2.3 Modified tf − idf for 3-gram Sequences T f −idf provides a score to a term indicating how informative each term is, by multiplying the frequency of the term within the document (term frequency) by the rarity of the term across corpus (inverse document frequency). tf − idf is known to be highly effective for text categorization. In this work, we experiment with modified tf − idf in order to accommodate the nature of author attribution more directly. We propose two such variants: tf-iAf – Term-Frequency Inverse-Author-Frequency In this"
R11-1043,C08-1065,0,0.0793233,"work based on machine learning techniques focused only on in-domain text for authorship attribution. In this paper, we present comprehensive evaluation of various stylometric techniques for cross-domain authorship attribution. From the experiments based on the Project Gutenberg book archive, we discover that extremely simple techniques based on stopwords are surprisingly robust against domain change, essentially ridding the need for domain adaptation when supplied with a large amount of data. 1 Introduction Many real world problems that require authorship attribution, such as forensics (e.g., Luyckx and Daelemans (2008)) or authorship dispute for old literature (e.g., Mosteller and Wallace (1984)) may not have in-domain training data readily available. However, most previous work to date has focused on authorship attribution only for in-domain text (e.g., Stamatatos et al. (1999), Luyckx and Daelemans (2008), Raghavan et al. (2010)). On limited occasions researchers include heterogeneous (cross-domain) dataset in their experiments, but they only report the performance on heterogeneous dataset is much lower than that of homogeneous dataset, rather than directly tacking the problem of cross-domain or domain in"
R11-1043,P10-2008,0,0.253502,"s that are likely to be common across different topics and genre. From the experiments based on the Project Gutenberg book archive, we indeed discover stylistic features that are common across different domains. Against our expectations, some of such features, stop-words in particular, are extremely informative, essentially ridding of the need for domain adaptation, if supplied with a large amount of data. Due to its simplicity, techniques based on stop-words scale particularly well over a large amount of data, in comparison to more computationally heavy techniques that require parsing (e.g., Raghavan et al. (2010)). Automatic authorship attribution, by its nature, is much more advantageous if it is domain (i.e., topic and/or genre) independent. That is, many real world problems that require authorship attribution may not have in-domain training data readily available. However, most previous work based on machine learning techniques focused only on in-domain text for authorship attribution. In this paper, we present comprehensive evaluation of various stylometric techniques for cross-domain authorship attribution. From the experiments based on the Project Gutenberg book archive, we discover that extreme"
R11-1043,E99-1021,0,0.051801,"Gutenberg book archive, we discover that extremely simple techniques based on stopwords are surprisingly robust against domain change, essentially ridding the need for domain adaptation when supplied with a large amount of data. 1 Introduction Many real world problems that require authorship attribution, such as forensics (e.g., Luyckx and Daelemans (2008)) or authorship dispute for old literature (e.g., Mosteller and Wallace (1984)) may not have in-domain training data readily available. However, most previous work to date has focused on authorship attribution only for in-domain text (e.g., Stamatatos et al. (1999), Luyckx and Daelemans (2008), Raghavan et al. (2010)). On limited occasions researchers include heterogeneous (cross-domain) dataset in their experiments, but they only report the performance on heterogeneous dataset is much lower than that of homogeneous dataset, rather than directly tacking the problem of cross-domain or domain independent authorship attribution (e.g., Peng et al. (2003)). The lack of research for cross-domain scenarios is perhaps only reasonable, given that it is understood in the community that the prediction power 2 Domain Independent Cues for Author Identification The s"
R11-1043,W06-1615,0,0.166492,"Missing"
W06-1651,H05-1045,1,0.554208,"acted from the aforementioned n-best opinion expression and source sequences. The relation classifier is modeled using Markov order-0 CRFs(Lafferty 2 Wiebe et al. (2005) reports human annotation agreement for opinion expression as 82.0 by F1 measure. See Wiebe et al. (2005) for additional details. 432 per sentence, and (4) link relations that span more than one sentence. In addition, the link relation model explicitly exploits mutual dependencies among entities and relations, while Bethard et al. (2004) does not directly capture the potential influence among entities. Kim and Hovy (2005b) and Choi et al. (2005) focus only on the extraction of sources of opinions, without extracting opinion expressions. Specifically, Kim and Hovy (2005b) assume a priori existence of the opinion expressions and extract a single source for each, while Choi et al. (2005) do not explicitly extract opinion expressions nor link an opinion expression to a source even though their model implicitly learns approximations of opinion expressions in order to identify opinion sources. Other previous research focuses only on the extraction of opinion expressions (e.g. Kim and Hovy (2005a), Munson et al. (2005) and Wilson et al. (20"
W06-1651,I05-2011,0,0.0168719,"and source entities extracted from the aforementioned n-best opinion expression and source sequences. The relation classifier is modeled using Markov order-0 CRFs(Lafferty 2 Wiebe et al. (2005) reports human annotation agreement for opinion expression as 82.0 by F1 measure. See Wiebe et al. (2005) for additional details. 432 per sentence, and (4) link relations that span more than one sentence. In addition, the link relation model explicitly exploits mutual dependencies among entities and relations, while Bethard et al. (2004) does not directly capture the potential influence among entities. Kim and Hovy (2005b) and Choi et al. (2005) focus only on the extraction of sources of opinions, without extracting opinion expressions. Specifically, Kim and Hovy (2005b) assume a priori existence of the opinion expressions and extract a single source for each, while Choi et al. (2005) do not explicitly extract opinion expressions nor link an opinion expression to a source even though their model implicitly learns approximations of opinion expressions in order to identify opinion sources. Other previous research focuses only on the extraction of opinion expressions (e.g. Kim and Hovy (2005a), Munson et al. (20"
W06-1651,H05-1068,1,0.628041,"im and Hovy (2005b) and Choi et al. (2005) focus only on the extraction of sources of opinions, without extracting opinion expressions. Specifically, Kim and Hovy (2005b) assume a priori existence of the opinion expressions and extract a single source for each, while Choi et al. (2005) do not explicitly extract opinion expressions nor link an opinion expression to a source even though their model implicitly learns approximations of opinion expressions in order to identify opinion sources. Other previous research focuses only on the extraction of opinion expressions (e.g. Kim and Hovy (2005a), Munson et al. (2005) and Wilson et al. (2005)), omitting source identification altogether. There have also been previous efforts to simultaneously extract entities and relations by exploiting their mutual dependencies. Roth and Yih (2002) formulated global inference using a Bayesian network, where they captured the influence between a relation and a pair of entities via the conditional probability of a relation, given a pair of entities. This approach however, could not exploit dependencies between relations. Roth and Yih (2004) later formulated global inference using integer linear programming, which is the appr"
W06-1651,W05-0625,0,0.0167612,"Missing"
W06-1651,C04-1197,0,0.182414,"lation extraction, respectively, improving substantially over prior results in the area. Fortunately, research in machine learning has produced methods for global inference and joint classification that can help to address this deficiency (e.g. Bunescu and Mooney (2004), Roth and Yih (2004)). Moreover, it has been shown that exploiting dependencies among entities and/or relations via global inference not only solves the joint extraction task, but often boosts performance on the individual tasks when compared to classifiers that handle the tasks independently — for semantic role labeling (e.g. Punyakanok et al. (2004)), information extraction (e.g. Roth and Yih (2004)), and sequence tagging (e.g. Sutton et al. (2004)). 1 Introduction Information extraction tasks such as recognizing entities and relations have long been considered critical to many domain-specific NLP tasks (e.g. Mooney and Bunescu (2005), Prager et al. (2000), White et al. (2001)). Researchers have further shown that opinion-oriented information extraction can provide analogous benefits to a variety of practical applications including product reputation tracking (Morinaga et al., 2002), opinion-oriented question answering (Stoyanov et al.,"
W06-1651,W04-2401,0,0.374904,"acy. To date, however, there has been no effort to simultaneously identify arbitrary opinion expressions, their sources, and the relations between them. Without progress on the joint extraction of opinion entities and their relations, the capabilities of opinionbased applications will remain limited. We present an approach for the joint extraction of entities and relations in the context of opinion recognition and analysis. We identify two types of opinion-related entities — expressions of opinions and sources of opinions — along with the linking relation that exists between them. Inspired by Roth and Yih (2004), we employ an integer linear programming approach to solve the joint opinion recognition task, and show that global, constraint-based inference can significantly boost the performance of both relation extraction and the extraction of opinion-related entities. Performance further improves when a semantic role labeling system is incorporated. The resulting system achieves F-measures of 79 and 69 for entity and relation extraction, respectively, improving substantially over prior results in the area. Fortunately, research in machine learning has produced methods for global inference and joint cl"
W06-1651,C02-1151,0,0.00591837,"ions and extract a single source for each, while Choi et al. (2005) do not explicitly extract opinion expressions nor link an opinion expression to a source even though their model implicitly learns approximations of opinion expressions in order to identify opinion sources. Other previous research focuses only on the extraction of opinion expressions (e.g. Kim and Hovy (2005a), Munson et al. (2005) and Wilson et al. (2005)), omitting source identification altogether. There have also been previous efforts to simultaneously extract entities and relations by exploiting their mutual dependencies. Roth and Yih (2002) formulated global inference using a Bayesian network, where they captured the influence between a relation and a pair of entities via the conditional probability of a relation, given a pair of entities. This approach however, could not exploit dependencies between relations. Roth and Yih (2004) later formulated global inference using integer linear programming, which is the approach that we apply here. In contrast to our work, Roth and Yih (2004) operated in the domain of factual information extraction rather than opinion extraction, and assumed that the exact boundaries of entities from the"
W06-1651,H05-1116,1,0.530822,"nok et al. (2004)), information extraction (e.g. Roth and Yih (2004)), and sequence tagging (e.g. Sutton et al. (2004)). 1 Introduction Information extraction tasks such as recognizing entities and relations have long been considered critical to many domain-specific NLP tasks (e.g. Mooney and Bunescu (2005), Prager et al. (2000), White et al. (2001)). Researchers have further shown that opinion-oriented information extraction can provide analogous benefits to a variety of practical applications including product reputation tracking (Morinaga et al., 2002), opinion-oriented question answering (Stoyanov et al., 2005), and opinion-oriented summarization (e.g. Cardie et al. (2004), Liu et al. (2005)). Moreover, much progress has been made in the area of opinion extraction: it is possible to identify sources of opinions (i.e. the opinion holders) (e.g. Choi et al. In this paper, we present a global inference approach (Roth and Yih, 2004) to the extraction of opinion-related entities and relations. In particular, we aim to identify two types of entities (i.e. spans of text): entities that express opinions and entities that denote sources of opinions. More specifically, we use the term opinion expression to de"
W06-1651,H01-1054,1,0.684396,"ependencies among entities and/or relations via global inference not only solves the joint extraction task, but often boosts performance on the individual tasks when compared to classifiers that handle the tasks independently — for semantic role labeling (e.g. Punyakanok et al. (2004)), information extraction (e.g. Roth and Yih (2004)), and sequence tagging (e.g. Sutton et al. (2004)). 1 Introduction Information extraction tasks such as recognizing entities and relations have long been considered critical to many domain-specific NLP tasks (e.g. Mooney and Bunescu (2005), Prager et al. (2000), White et al. (2001)). Researchers have further shown that opinion-oriented information extraction can provide analogous benefits to a variety of practical applications including product reputation tracking (Morinaga et al., 2002), opinion-oriented question answering (Stoyanov et al., 2005), and opinion-oriented summarization (e.g. Cardie et al. (2004), Liu et al. (2005)). Moreover, much progress has been made in the area of opinion extraction: it is possible to identify sources of opinions (i.e. the opinion holders) (e.g. Choi et al. In this paper, we present a global inference approach (Roth and Yih, 2004) to t"
W06-1651,H05-1044,0,0.196694,"hoi et al. (2005) focus only on the extraction of sources of opinions, without extracting opinion expressions. Specifically, Kim and Hovy (2005b) assume a priori existence of the opinion expressions and extract a single source for each, while Choi et al. (2005) do not explicitly extract opinion expressions nor link an opinion expression to a source even though their model implicitly learns approximations of opinion expressions in order to identify opinion sources. Other previous research focuses only on the extraction of opinion expressions (e.g. Kim and Hovy (2005a), Munson et al. (2005) and Wilson et al. (2005)), omitting source identification altogether. There have also been previous efforts to simultaneously extract entities and relations by exploiting their mutual dependencies. Roth and Yih (2002) formulated global inference using a Bayesian network, where they captured the influence between a relation and a pair of entities via the conditional probability of a relation, given a pair of entities. This approach however, could not exploit dependencies between relations. Roth and Yih (2004) later formulated global inference using integer linear programming, which is the approach that we apply here."
W06-1651,P04-1056,0,0.0158274,"the joint opinion recognition task, and show that global, constraint-based inference can significantly boost the performance of both relation extraction and the extraction of opinion-related entities. Performance further improves when a semantic role labeling system is incorporated. The resulting system achieves F-measures of 79 and 69 for entity and relation extraction, respectively, improving substantially over prior results in the area. Fortunately, research in machine learning has produced methods for global inference and joint classification that can help to address this deficiency (e.g. Bunescu and Mooney (2004), Roth and Yih (2004)). Moreover, it has been shown that exploiting dependencies among entities and/or relations via global inference not only solves the joint extraction task, but often boosts performance on the individual tasks when compared to classifiers that handle the tasks independently — for semantic role labeling (e.g. Punyakanok et al. (2004)), information extraction (e.g. Roth and Yih (2004)), and sequence tagging (e.g. Sutton et al. (2004)). 1 Introduction Information extraction tasks such as recognizing entities and relations have long been considered critical to many domain-speci"
W11-0310,W06-1615,0,0.0319773,"package (McCallum, 2002) for experiments. 5 Experimental Results Note that our two datasets are created to specifically answer the following question: are there genderspecific characteristics in language beyond gender 5 Available at http://alias-i.com/lingpipe/ 81 preferences in topics and genre? One way to answer this question is to test whether statistical models can detect gender attribution on a dataset that is drastically different from the training data in topic and genre. Of course, it is a known fact that machine learning techniques do not transfer well across different domains (e.g., Blitzer et al. (2006)). However, if they can still perform considerably better than random prediction, then it would prove that there is indeed gender-specific stylometric characteristics beyond topic and genre. In what follows, we present five different experimental settings across two different dataset to compare in-domain and cross-domain performance of various techniques for gender attribution. 5.1 Experiments with Blog Dataset First we conduct two different experiments using the blog data in the order of increasing difficulty. [Experiment-I: Balanced Topic] Using the web blog dataset introduced in Section 3,"
W11-0310,P03-1054,0,0.0136706,"r gender attribution, tree-bank each training document di ∈ D using the PCFG parser Go . (3) For each gender γ, train a new gender-specific PCFG parser Gγ using only those tree-banked documents in D that correspond to gender γ. (4) For each test document, compare the likelihood of the document determined by each genderspecific PCFG parser Gγ , and the gender corresponding to the higher score. Note that PCFG models can be considered as a kind of language models, where probabilistic contextfree grammars are used to find the patterns in language, rather than n-grams. We use the implementation of Klein and Manning (2003) for PCFG models. 4.2 Shallow Lexico-Syntactic Patterns using Token-level Language Models Token-based (i.e. word-based) language models have been employed in a wide variety of NLP applications, including those that require stylometric analysis, e.g., authorship attribution (e.g., Uzner and Katz (2005)), and Wikipedia vandalism detection (Wang and McKeown, 2010). We expect that tokenbased language models will be effective in learning shallow lexico-syntactic patterns of gender specific language styles. We therefore experiment with unigram, bigram, and trigram token-level models, and name them a"
W11-0310,D10-1021,0,0.21001,"arious aspects of communication, such as discourse behavior, body language, lexical choices, and linguistic cues (e.g., Crosby and Nyquist (1977), Tannen (1991), Argamon et al. (2003), Eckert and McConnell-Ginet (2003), Argamon et al. (2007)). In this paper, we explore statistical techniques that can learn to identify the gender of authors in modern English text, such as web blogs and scientific papers, motivated by sociolinguistic theories for gender attribution. Although some recent work has shown the efficacy of machine learning techniques to gender attribution (e.g., Koppel et al. (2002), Mukherjee and Liu (2010)), we conjecture that the reported performance might be overly optimistic under scrutiny due to non-stylistic factors such as topic bias in gender that can make the gender detection task easier. Indeed, recent research on web blogs reports that there is substantial gender bias in topics (e.g., Janssen and Murachver (2004), Argamon et al. (2007)) as well as in genre (e.g., Herring and Paolillo (2006)). In order to address this concern, we perform the first comparative study of machine learning techniques for gender attribution after deliberately removing gender bias in topics and genre. Further"
W11-0310,E03-1053,0,0.0304499,".3 57.6 73.8 65.8 67.1 60.1 63.7 67.8 64.2 66.1 66.2 64.2 65.4 Table 1: Overall Accuracy of Topic-Balanced Gender Attribution on Blog Data (Experiment-I) Language Models. We use the LingPipe package5 for experiments. 4.3 Shallow Morphological Patterns using Character-level Language Models Next we explore the use of character-level language models to investigate whether there are morphological patterns that characterize gender-specific styles in language. Despite its simplicity, previous research have reported that character-level language models are effective for authorship attribution (e.g., Peng et al. (2003b)) as well as genre classification (e.g., Peng et al. (2003a), Wu et al. (2010)). We experiment with unigram, bigram, and trigram character-level models, and name them as C LM(n=1), C LM(n=2), C LM(n=3), respectively, where C LM stands for Character-based Language Models. We again make use of the LingPipe package for experiments. Note that there has been no previous research that directly compares the performance of characterlevel language models to that of PCFG based models for author attribution, not to mention for gender attribution. 4.4 Bag of Words using Maximum Entropy (MaxEnt) Classifi"
W11-0310,N03-1025,0,0.00743735,".3 57.6 73.8 65.8 67.1 60.1 63.7 67.8 64.2 66.1 66.2 64.2 65.4 Table 1: Overall Accuracy of Topic-Balanced Gender Attribution on Blog Data (Experiment-I) Language Models. We use the LingPipe package5 for experiments. 4.3 Shallow Morphological Patterns using Character-level Language Models Next we explore the use of character-level language models to investigate whether there are morphological patterns that characterize gender-specific styles in language. Despite its simplicity, previous research have reported that character-level language models are effective for authorship attribution (e.g., Peng et al. (2003b)) as well as genre classification (e.g., Peng et al. (2003a), Wu et al. (2010)). We experiment with unigram, bigram, and trigram character-level models, and name them as C LM(n=1), C LM(n=2), C LM(n=3), respectively, where C LM stands for Character-based Language Models. We again make use of the LingPipe package for experiments. Note that there has been no previous research that directly compares the performance of characterlevel language models to that of PCFG based models for author attribution, not to mention for gender attribution. 4.4 Bag of Words using Maximum Entropy (MaxEnt) Classifi"
W11-0310,P10-2008,0,0.0441709,"that learn shallow lexico-syntactic patterns (Section 4.2). The last kind is based on character-level language models that learn morphological patterns on extremely short text spans (Section 4.3). Finally, we describe the bag-of-word approach using the maximum entropy classifier (Section 4.4). 4 Note that existing gender detection tools require a minimum 300 words for appropriate identification. 80 4.1 Deep Syntactic Patterns using Probabilistic Context free Grammar A probabilistic context-free grammar (PCFG) captures syntactic regularities beyond shallow ngrambased lexico-syntactic patterns. Raghavan et al. (2010) recently introduced the use of PCFG for authorship attribution for the first time, and demonstrated that it is highly effective for learning stylistic patterns for authorship attribution. We therefore explore the use of PCFG for gender attribution. We give a very concise description here, referring to Raghavan et al. (2010) for more details. (1) Train a generic PCFG parser Go on manually tree-banked corpus such as WSJ or Brown. (2) Given training corpus D for gender attribution, tree-bank each training document di ∈ D using the PCFG parser Go . (3) For each gender γ, train a new gender-specif"
W11-0310,I05-1084,0,0.0277434,"by each genderspecific PCFG parser Gγ , and the gender corresponding to the higher score. Note that PCFG models can be considered as a kind of language models, where probabilistic contextfree grammars are used to find the patterns in language, rather than n-grams. We use the implementation of Klein and Manning (2003) for PCFG models. 4.2 Shallow Lexico-Syntactic Patterns using Token-level Language Models Token-based (i.e. word-based) language models have been employed in a wide variety of NLP applications, including those that require stylometric analysis, e.g., authorship attribution (e.g., Uzner and Katz (2005)), and Wikipedia vandalism detection (Wang and McKeown, 2010). We expect that tokenbased language models will be effective in learning shallow lexico-syntactic patterns of gender specific language styles. We therefore experiment with unigram, bigram, and trigram token-level models, and name them as T LM(n=1), T LM(n=2), T LM(n=3), respectively, where T LM stands for Token-based lexicon based deep syntax morphology b.o.w. shallow lex-syntax Gender Guesser PCFG CLM n=1 CLM n=2 CLM n=3 ME Data Type Gender Genie TLM n=1 TLM n=2 TLM n=3 Male Only Female Only All 72.1 27.1 50.0 68.6 06.4 37.5 53.4 7"
W11-0310,C10-1129,0,0.045421,"Missing"
W11-0310,P10-1077,0,0.021247,"ccuracy of Topic-Balanced Gender Attribution on Blog Data (Experiment-I) Language Models. We use the LingPipe package5 for experiments. 4.3 Shallow Morphological Patterns using Character-level Language Models Next we explore the use of character-level language models to investigate whether there are morphological patterns that characterize gender-specific styles in language. Despite its simplicity, previous research have reported that character-level language models are effective for authorship attribution (e.g., Peng et al. (2003b)) as well as genre classification (e.g., Peng et al. (2003a), Wu et al. (2010)). We experiment with unigram, bigram, and trigram character-level models, and name them as C LM(n=1), C LM(n=2), C LM(n=3), respectively, where C LM stands for Character-based Language Models. We again make use of the LingPipe package for experiments. Note that there has been no previous research that directly compares the performance of characterlevel language models to that of PCFG based models for author attribution, not to mention for gender attribution. 4.4 Bag of Words using Maximum Entropy (MaxEnt) Classifier We include Maximum Entropy classifier using simple unigram features (bag-of-w"
W11-0326,P10-1127,0,0.0353201,"ctronically available today, web-scale n-gram data in particular, in a simple yet highly effective approach to compose image descriptions in natural language. Automatic generation of image descriptions differs from automatic image tagging (e.g., Leong et al. (2010)) in that we aim to generate complex phrases or sentences describing images rather than predicting inOur work contrasts to most previous approaches in four key aspects: first, we compose fresh sentences from scratch, instead of retrieving (Farhadi et al. (2010)), or summarizing existing text fragments associated with an image (e.g., Aker and Gaizauskas (2010), Feng and Lapata (2010a)). Second, we aim to generate textual descriptions that are truthful to the specific content of the image, whereas related (but subtly different) work in automatic caption generation creates news-worthy text (Feng and Lapata (2010a)) or encyclopedic text (Aker and Gaizauskas (2010)) that is contextually relevant to the image, but not closely pertinent to the specific content of the image. Third, we aim to build a general image description method as compared to work that requires domain specific hand-written grammar rules (Yao et al. (2010)). Last, we allow for some cre"
W11-0326,N03-4003,0,0.0604672,"Missing"
W11-0326,P10-1126,0,0.0273357,"e-existing text relevant to an image, our method composes sentences entirely from scratch. Experimental results indicate that it is viable to generate simple textual descriptions that are pertinent to the specific content of an image, while permitting creativity in the description – making for more human-like annotations than previous approaches. 1 Introduction Gaining a better understanding of natural language, and especially natural language associated with images helps drive research in both computer vision and natural language processing (e.g., Barnard et al. (2003), Pastra et al. (2003), Feng and Lapata (2010b)). In this paper, we look at how to exploit the enormous amount of textual data electronically available today, web-scale n-gram data in particular, in a simple yet highly effective approach to compose image descriptions in natural language. Automatic generation of image descriptions differs from automatic image tagging (e.g., Leong et al. (2010)) in that we aim to generate complex phrases or sentences describing images rather than predicting inOur work contrasts to most previous approaches in four key aspects: first, we compose fresh sentences from scratch, instead of retrieving (Farhadi et"
W11-0326,N10-1125,0,0.0120992,"e-existing text relevant to an image, our method composes sentences entirely from scratch. Experimental results indicate that it is viable to generate simple textual descriptions that are pertinent to the specific content of an image, while permitting creativity in the description – making for more human-like annotations than previous approaches. 1 Introduction Gaining a better understanding of natural language, and especially natural language associated with images helps drive research in both computer vision and natural language processing (e.g., Barnard et al. (2003), Pastra et al. (2003), Feng and Lapata (2010b)). In this paper, we look at how to exploit the enormous amount of textual data electronically available today, web-scale n-gram data in particular, in a simple yet highly effective approach to compose image descriptions in natural language. Automatic generation of image descriptions differs from automatic image tagging (e.g., Leong et al. (2010)) in that we aim to generate complex phrases or sentences describing images rather than predicting inOur work contrasts to most previous approaches in four key aspects: first, we compose fresh sentences from scratch, instead of retrieving (Farhadi et"
W11-0326,P03-1054,0,0.0259966,"herwise we jump to the next iteration of the loop. We define the score as score(X) = pˆLM (X)ˆ pP CF G (X) where X is a given sentence (image description), pˆLM (X) is the length normalized probability of X based on the language model, and pˆP CF G (X) is the length normalized probability of X based on the probabilistic context free grammar (PCFG) model. The loop is repeated until convergence or a fixed number of iterations is reached. Note that this approach can be extended to simulated annealing to allow temporary downward steps to escape from local maxima. We use the PCFG implementation of Klein and Manning (2003). 3.3 Template Based Approach The third approach is a template-based approach with linguistic constraints, a technique that has often been used for various practical applications such as summarization (Zhou and Hovy, 2004) and dia&lt; &lt; blue , bicycle &gt;, near, &lt; shiny , person &gt; &gt; blue, bike [2669] blue, bicycle [1365] bike, blue [1184] blue, cycle [324] cycle, of, the, blue [172] cycle, blue [158] bicycle, blue [154] bike, in, blue [98] cycle, of, blue [64] bike, with, blue [43] person, operating, a, bicycle [3409] man, on, a, bicycle [2842] cycle, of, child [2507] bike, for, men [2485] person,"
W11-0326,C10-2074,0,0.0102279,"ntroduction Gaining a better understanding of natural language, and especially natural language associated with images helps drive research in both computer vision and natural language processing (e.g., Barnard et al. (2003), Pastra et al. (2003), Feng and Lapata (2010b)). In this paper, we look at how to exploit the enormous amount of textual data electronically available today, web-scale n-gram data in particular, in a simple yet highly effective approach to compose image descriptions in natural language. Automatic generation of image descriptions differs from automatic image tagging (e.g., Leong et al. (2010)) in that we aim to generate complex phrases or sentences describing images rather than predicting inOur work contrasts to most previous approaches in four key aspects: first, we compose fresh sentences from scratch, instead of retrieving (Farhadi et al. (2010)), or summarizing existing text fragments associated with an image (e.g., Aker and Gaizauskas (2010), Feng and Lapata (2010a)). Second, we aim to generate textual descriptions that are truthful to the specific content of the image, whereas related (but subtly different) work in automatic caption generation creates news-worthy text (Feng"
W11-0326,P02-1040,0,0.0997884,"mage descriptions generated by 4 different approaches in Figure 3 and 4. Notice that only the P HRASE F USION approach is able to include interesting and adequate verbs, such as “eating” or “looking” in Figure 3, and “operating” in Figure 4. Note that the choice of these action verbs is based only on the co-occurrence statistics encoded in n-grams, without relying on the vision component that specializes in action recognition. These examples therefore demonstrate that world knowledge implicitly encoded in natural language can help enhance image content recognition. Automatic Evaluation: BLEU (Papineni et al., 2002) is a widely used metric for automatic evaluation of machine translation that measures the ngram precision of machine generated sentences with respect to human generated sentences. Because our task can be viewed as machine translation from images to text, BLEU (Papineni et al., 2002) may seem 4 This limitation does not apply to T EMPLATE. 225 like a reasonable choice. However, there is larger inherent variability in generating sentences from images than translating a sentence from one language to another. In fact two people viewing the same picture may produce quite different descriptions. Thi"
W11-0326,E03-1065,0,0.0220827,"arizes or retrieves pre-existing text relevant to an image, our method composes sentences entirely from scratch. Experimental results indicate that it is viable to generate simple textual descriptions that are pertinent to the specific content of an image, while permitting creativity in the description – making for more human-like annotations than previous approaches. 1 Introduction Gaining a better understanding of natural language, and especially natural language associated with images helps drive research in both computer vision and natural language processing (e.g., Barnard et al. (2003), Pastra et al. (2003), Feng and Lapata (2010b)). In this paper, we look at how to exploit the enormous amount of textual data electronically available today, web-scale n-gram data in particular, in a simple yet highly effective approach to compose image descriptions in natural language. Automatic generation of image descriptions differs from automatic image tagging (e.g., Leong et al. (2010)) in that we aim to generate complex phrases or sentences describing images rather than predicting inOur work contrasts to most previous approaches in four key aspects: first, we compose fresh sentences from scratch, instead of"
W11-0326,W02-0402,0,0.0114951,"on, and replacement. The score of the current sentence is determined by the multiplication LM-based probability and PCFG-based probability. enforce long distance regularities for more grammatically correct generation. However, optimizing both language-model-based probabilities and parser-based probabilities is intractable. Therefore, we explore a randomized local search approach that makes greedy revisions using both language models and parsers. Randomized local search has been successfully applied to intractable optimization problems in AI (e.g., Chisholm and Tadepalli (2002)) and NLP (e.g., White and Cardie (2002)). Table 1 shows the skeleton of the algorithm in our study. Iterating through a loop, it chooses an edit location and an edit operation (insert, delete, or replace) at random. If the edit yields a better score, then we commit the edit, otherwise we jump to the next iteration of the loop. We define the score as score(X) = pˆLM (X)ˆ pP CF G (X) where X is a given sentence (image description), pˆLM (X) is the length normalized probability of X based on the language model, and pˆP CF G (X) is the length normalized probability of X based on the probabilistic context free grammar (PCFG) model. The"
W11-0326,W04-1010,0,0.0155746,"ge model, and pˆP CF G (X) is the length normalized probability of X based on the probabilistic context free grammar (PCFG) model. The loop is repeated until convergence or a fixed number of iterations is reached. Note that this approach can be extended to simulated annealing to allow temporary downward steps to escape from local maxima. We use the PCFG implementation of Klein and Manning (2003). 3.3 Template Based Approach The third approach is a template-based approach with linguistic constraints, a technique that has often been used for various practical applications such as summarization (Zhou and Hovy, 2004) and dia&lt; &lt; blue , bicycle &gt;, near, &lt; shiny , person &gt; &gt; blue, bike [2669] blue, bicycle [1365] bike, blue [1184] blue, cycle [324] cycle, of, the, blue [172] cycle, blue [158] bicycle, blue [154] bike, in, blue [98] cycle, of, blue [64] bike, with, blue [43] person, operating, a, bicycle [3409] man, on, a, bicycle [2842] cycle, of, child [2507] bike, for, men [2485] person, riding, a, bicycle [2118] cycle, in, women [1853] bike, for, women [1442] boy, on, a, bicycle [1378] cycle, of, women [1288] man, on, a, bike [1283] bright, boy [8092] bright, child [7840] bright, girl [6191] bright, kid ["
W17-0907,P82-1020,0,0.813873,"Missing"
W17-0907,W07-0602,0,0.0823614,". (2017). 2 System Description We design a system that predicts, given a pair of story endings, which is the right one and which is the wrong one. Our system applies a linear classifier guided by several types of features to solve the task. We describe the system in detail below. 52 Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pages 52–55, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics 2.1 Model (Argamon et al., 2003; Schler et al., 2006; Bamman et al., 2014), and native language (Koppel et al., 2005; Tsur and Rappoport, 2007; Bergsma et al., 2012). We add the following classification features to capture style differences between the two endings. These features are computed on the story endings alone (right or wrong), and do not consider, either at train or at test time, the first four (shared) sentences of each story. We train a binary logistic regression classifier to distinguish between right and wrong stories. We use the set of right stories as positive samples and the set of wrong stories as negative samples. At test time, for a given pair, we consider the classification results of both candidates. If our cla"
W17-0907,N16-1098,0,0.0403179,"Missing"
W17-0907,W11-1515,1,0.892048,"Missing"
W17-0907,P11-1077,0,0.0656157,"rring less than 3 times by a special out-of-vocabulary character, yielding a vocabulary size of 21,582. Only during training, we apply a (1) The intuition is that a correct ending should be unsurprising (to the model) given the four preceding sentences of the story (the numerator), controlling for the inherent surprise of the words in that ending (the denominator).1 Stylistic features. We hypothesize that right and wrong endings might be distinguishable using style features. We adopt style features that have been shown useful in the past in tasks such as detection of age (Schler et al., 2006; Rosenthal and McKeown, 2011; Nguyen et al., 2011), gender 2 www.nltk.org/api/nltk.tokenize.html www.tensorflow.org 4 We train on both the Spring 2016 and the Winter 2017 datasets, a total of roughly 100K stories. 3 1 Note that taking the logarithm of the expression in Equation 1 gives the pointwise mutual information between the story and the ending, under the language model. 53 Model DSSM (Mostafazadeh et al., 2016) LexVec (Salle et al., 2016) RNNLM features Stylistic features Combined (Style + RNNLM) Human judgment Acc. 0.585 0.599 0.677 0.724 0.752 1.000 style on the authors, which is expressed in the different style"
W17-0907,D13-1193,1,0.0470041,", we keep them. If not, the label whose posterior probability is lower is reversed. We describe the classification features below. 2.2 • Length. The number of words in the sentence. Features We use two types of features, designed to capture different aspects of the problem. We use neural language model features to leverage corpus level word distributions, specifically longer term sequence probabilities. We use stylistic features to capture differences in writing between coherent story endings and incoherent ones. • Word n-grams. We use sequences of 1– 5 words. Following Tsur et al. (2010) and Schwartz et al. (2013), we distinguish between high frequency and low frequency words. Specifically, we replace content words, which are often low frequency, with their part-of-speech tags (Nouns, Verbs, Adjectives, and Adverbs). Language model features. We experiment with state-of-the-art text comprehension models, specifically an LSTM (Hochreiter and Schmidhuber, 1997) recurrent neural network language model (RNNLM; Mikolov et al., 2010). Our RNNLM is used to generate two different probabilities: pθ (ending), which is the language model probability of the fifth sentence alone and pθ (ending |story), which is the"
W17-0907,K17-1004,1,0.671994,"Missing"
