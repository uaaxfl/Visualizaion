2020.coling-main.11,P17-1067,0,0.0217391,"ion that different fundamental emotions can occur together, for instance trust and joy, which is the case when love is experienced. Such emotion mixtures, as well as an opposition between anger and fear, joy and sadness, surprise and 1 The examples follow the results by Smith and Ellsworth (1985), an excerpt is shown in Table 1. 126 anticipation, trust and disgust, has been included in this model. In natural language processing, mostly a set of four to eight fundamental emotions is used, where anger, fear, joy, and sadness are shared by most approaches (an exception with 24 emotion classes is Abdul-Mageed and Ungar (2017)). A diametrically opposite view is held by the constructivist tradition (Averill, 1980; Oatley, 1993; Barrett and Russell, 2015), in which actions and physiological changes are the building blocks that construct emotions, rather than their direct effect (Feldman Barrett, 2006). Feeling an emotion means categorizing the fluctuations of an affect system along some components. For instance, the affect components valence (degree of polarity), arousal (degree of excitement), and dominance (degree of control over a situation) (Posner et al., 2005) are used as dimensions to describe affect experienc"
2020.coling-main.11,H05-1073,0,0.641675,"dimensions (according to a principle component analysis) as published by Smith and Ellsworth (1985), Table 6, filtered to those emotions which are available in the text corpus we use. (2018). Buechel et al. (2016) have developed a methodological framework to adapt existing affect lexicons to specific use cases. Other than dictionaries, emotion analysis relies on labeled corpora. Some of them include information relative to valence and arousal (Buechel and Hahn, 2017; Preot¸iuc-Pietro et al., 2016), but the majority of resources use discrete emotion classes, for instance to label fairy tales (Alm et al., 2005), blogs (Aman and Szpakowicz, 2007), tweets (Mohammad et al., 2017; Schuff et al., 2017; Mohammad, 2012; Mohammad and Bravo-Marquez, 2017; Klinger et al., 2018), Facebook posts (Preot¸iuc-Pietro et al., 2016), news headlines (Strapparava and Mihalcea, 2007), dialogues (Li et al., 2017), literary texts (Kim et al., 2017), or self reports on emotion events (Scherer and Wallbott, 1997; Troiano et al., 2019). We point the reader to the survey by Bostan and Klinger (2018) for a more an overview. Most automatic methods that assign labels to text rely on machine learning (Alm et al., 2005; Aman and S"
2020.coling-main.11,C18-1179,1,0.834495,"hn, 2017; Preot¸iuc-Pietro et al., 2016), but the majority of resources use discrete emotion classes, for instance to label fairy tales (Alm et al., 2005), blogs (Aman and Szpakowicz, 2007), tweets (Mohammad et al., 2017; Schuff et al., 2017; Mohammad, 2012; Mohammad and Bravo-Marquez, 2017; Klinger et al., 2018), Facebook posts (Preot¸iuc-Pietro et al., 2016), news headlines (Strapparava and Mihalcea, 2007), dialogues (Li et al., 2017), literary texts (Kim et al., 2017), or self reports on emotion events (Scherer and Wallbott, 1997; Troiano et al., 2019). We point the reader to the survey by Bostan and Klinger (2018) for a more an overview. Most automatic methods that assign labels to text rely on machine learning (Alm et al., 2005; Aman and Szpakowicz, 2007; Schuff et al., 2017, i.a.). Recent shared tasks showed an increase in transfer learning from generic representations (Klinger et al., 2018; Mohammad et al., 2018; Mohammad and Bravo-Marquez, 2017). Felbo et al. (2017) proposed to use emoji representations for pretraining, and Cevher et al. (2019) performed pretraining on existing emotion corpora followed by fine-tuning for a specific domain for which only little training data was available. We are on"
2020.coling-main.11,2020.lrec-1.194,1,0.798471,"and anticipation. Machine learning-based models need to figure out which words point to a particular emotion experienced by a reader, by the author of a text, or a character in it. Depending on the resource which has been annotated, the description of an emotion experience can vary. On Twitter, for instance, other than direct reports of an emotion state (“I feel depressed”), hashtags are used as emotion labels to enrich the description of events and stances (“I just got my exam result #sad”). In news articles, emotional events are sometimes explicitly mentioned (“couple infuriate officials” (Bostan et al., 2020)) and other times require world knowledge (“Tom Cruise and Katie Holmes set wedding date”, labeled as surprise (Strapparava and Mihalcea, 2007)). In literature, a sequence of events which forms the narrative leads to an emotion in the reader. In this paper, we focus on those texts which communicate emotions without an explicit emotion word, but rather describe events for which an emotion association is evident. Such textual examples became popular in natural language processing research with the use of the data generated in the ISEAR project (Scherer and Wallbott, 1997). The project led to a d"
2020.coling-main.11,E17-2092,0,0.436456,"rpretation needs to be accomplished. For instance, in the text “When a car is overtaking another and I am forced to drive off the road”, the model needs to associate the event with fear. To date, nearly all computational approaches that associate text with emotions are agnostic to the way in which emotions are communicated, they do “not know” how to interpret events, but, presumably, they purely learn word associations instead of actual event interpretations. One might argue that approaches predicting fine-grained dimensions of affect, namely arousal and valence, actually tackle this problem (Buechel and Hahn, 2017; Preot¸iuc-Pietro et al., 2016). However, these typically do not infer downstream emotion categories. Further, particularly regarding events, psychological theories offer more detailed information. As an example, the emotion component model (Scherer, 2005) advocates that cognitive appraisal dimensions underly discrete emotion classes (Smith and Ellsworth, 1985). These appraisal dimensions1 evaluate (1) how pleasant an event is (pleasantness, likely to be associated with joy, but unlikely to appear with disgust), (2) how much effort an event can be expected to cause (anticipated effort, likely"
2020.coling-main.11,W16-4008,0,0.02027,"ad 127 Emotion Happiness Sadness Anger Fear Disgust Shame Guilt Unpleasant Responsibility Uncertainty Attention Effort Control −1.46 0.87 0.85 0.44 0.38 0.73 0.60 0.09 −0.36 −0.94 −0.17 −0.50 1.31 1.31 −0.46 0.00 −0.29 0.73 −0.39 0.21 −0.15 0.15 −0.21 0.12 0.03 −0.96 −0.11 −0.36 −0.33 −0.14 0.53 0.63 0.06 0.07 0.00 −0.21 1.15 −0.96 0.59 −0.19 −0.07 −0.29 Table 1: The locations of emotions along appraisal dimensions (according to a principle component analysis) as published by Smith and Ellsworth (1985), Table 6, filtered to those emotions which are available in the text corpus we use. (2018). Buechel et al. (2016) have developed a methodological framework to adapt existing affect lexicons to specific use cases. Other than dictionaries, emotion analysis relies on labeled corpora. Some of them include information relative to valence and arousal (Buechel and Hahn, 2017; Preot¸iuc-Pietro et al., 2016), but the majority of resources use discrete emotion classes, for instance to label fairy tales (Alm et al., 2005), blogs (Aman and Szpakowicz, 2007), tweets (Mohammad et al., 2017; Schuff et al., 2017; Mohammad, 2012; Mohammad and Bravo-Marquez, 2017; Klinger et al., 2018), Facebook posts (Preot¸iuc-Pietro et"
2020.coling-main.11,D17-1169,0,0.0195814,"headlines (Strapparava and Mihalcea, 2007), dialogues (Li et al., 2017), literary texts (Kim et al., 2017), or self reports on emotion events (Scherer and Wallbott, 1997; Troiano et al., 2019). We point the reader to the survey by Bostan and Klinger (2018) for a more an overview. Most automatic methods that assign labels to text rely on machine learning (Alm et al., 2005; Aman and Szpakowicz, 2007; Schuff et al., 2017, i.a.). Recent shared tasks showed an increase in transfer learning from generic representations (Klinger et al., 2018; Mohammad et al., 2018; Mohammad and Bravo-Marquez, 2017). Felbo et al. (2017) proposed to use emoji representations for pretraining, and Cevher et al. (2019) performed pretraining on existing emotion corpora followed by fine-tuning for a specific domain for which only little training data was available. We are only aware of one preliminary study which considered appraisal dimensions to improve textbased emotion prediction, namely Campero et al. (2017). In their study, subjects labeled 200 stories with 38 appraisal features (which remain unmentioned), to evaluate if a text-based representation adds on top of an fMRI-based classification. Apart from this study, all previ"
2020.coling-main.11,W17-2203,1,0.849633,"Missing"
2020.coling-main.11,D14-1181,0,0.00305819,"iginal author meant it as a negative emotion, namely disgust). 4 Experiments We now move to our evaluation if automatic methods to recognize emotions can benefit from being informed about appraisal dimensions explicitly. We first describe our models and how we address our research questions and then turn to the results. 4.1 Model Configuration Figure 1 illustrates the four different tasks addressed by our models. Task T→E is the prediction of emotions from text, namely the standard setting in nearly all previous work of emotion analysis. We use a convolutional neural network (CNN) inspired by Kim (2014), with pretrained GloVe (Glove840B) as a 300-dimensional embedding layer (Pennington et al., 2014)3 with convolution filter sizes of 2, 3, and 4 with a ReLu activation function (Nair and Hinton, 2010), followed by a max pooling layer of length 2 and a dropout of 0.5 followed by another dense layer. As another model to predict emotions, we use a pipeline based on two steps, one to detect the appraisal from text, and the second to assign the appropriate emotion to the appraisal. We refer to Task T→A as the step of identifying appraisal from text. We use the model configuration of Task T→E, excep"
2020.coling-main.11,W18-6206,1,0.907668,"Missing"
2020.coling-main.11,I17-1099,0,0.0483409,"ons to specific use cases. Other than dictionaries, emotion analysis relies on labeled corpora. Some of them include information relative to valence and arousal (Buechel and Hahn, 2017; Preot¸iuc-Pietro et al., 2016), but the majority of resources use discrete emotion classes, for instance to label fairy tales (Alm et al., 2005), blogs (Aman and Szpakowicz, 2007), tweets (Mohammad et al., 2017; Schuff et al., 2017; Mohammad, 2012; Mohammad and Bravo-Marquez, 2017; Klinger et al., 2018), Facebook posts (Preot¸iuc-Pietro et al., 2016), news headlines (Strapparava and Mihalcea, 2007), dialogues (Li et al., 2017), literary texts (Kim et al., 2017), or self reports on emotion events (Scherer and Wallbott, 1997; Troiano et al., 2019). We point the reader to the survey by Bostan and Klinger (2018) for a more an overview. Most automatic methods that assign labels to text rely on machine learning (Alm et al., 2005; Aman and Szpakowicz, 2007; Schuff et al., 2017, i.a.). Recent shared tasks showed an increase in transfer learning from generic representations (Klinger et al., 2018; Mohammad et al., 2018; Mohammad and Bravo-Marquez, 2017). Felbo et al. (2017) proposed to use emoji representations for pretraini"
2020.coling-main.11,W17-5205,0,0.0246997,"those emotions which are available in the text corpus we use. (2018). Buechel et al. (2016) have developed a methodological framework to adapt existing affect lexicons to specific use cases. Other than dictionaries, emotion analysis relies on labeled corpora. Some of them include information relative to valence and arousal (Buechel and Hahn, 2017; Preot¸iuc-Pietro et al., 2016), but the majority of resources use discrete emotion classes, for instance to label fairy tales (Alm et al., 2005), blogs (Aman and Szpakowicz, 2007), tweets (Mohammad et al., 2017; Schuff et al., 2017; Mohammad, 2012; Mohammad and Bravo-Marquez, 2017; Klinger et al., 2018), Facebook posts (Preot¸iuc-Pietro et al., 2016), news headlines (Strapparava and Mihalcea, 2007), dialogues (Li et al., 2017), literary texts (Kim et al., 2017), or self reports on emotion events (Scherer and Wallbott, 1997; Troiano et al., 2019). We point the reader to the survey by Bostan and Klinger (2018) for a more an overview. Most automatic methods that assign labels to text rely on machine learning (Alm et al., 2005; Aman and Szpakowicz, 2007; Schuff et al., 2017, i.a.). Recent shared tasks showed an increase in transfer learning from generic representations (Kl"
2020.coling-main.11,S18-1001,0,0.0153747,"18), Facebook posts (Preot¸iuc-Pietro et al., 2016), news headlines (Strapparava and Mihalcea, 2007), dialogues (Li et al., 2017), literary texts (Kim et al., 2017), or self reports on emotion events (Scherer and Wallbott, 1997; Troiano et al., 2019). We point the reader to the survey by Bostan and Klinger (2018) for a more an overview. Most automatic methods that assign labels to text rely on machine learning (Alm et al., 2005; Aman and Szpakowicz, 2007; Schuff et al., 2017, i.a.). Recent shared tasks showed an increase in transfer learning from generic representations (Klinger et al., 2018; Mohammad et al., 2018; Mohammad and Bravo-Marquez, 2017). Felbo et al. (2017) proposed to use emoji representations for pretraining, and Cevher et al. (2019) performed pretraining on existing emotion corpora followed by fine-tuning for a specific domain for which only little training data was available. We are only aware of one preliminary study which considered appraisal dimensions to improve textbased emotion prediction, namely Campero et al. (2017). In their study, subjects labeled 200 stories with 38 appraisal features (which remain unmentioned), to evaluate if a text-based representation adds on top of an fMR"
2020.coling-main.11,S12-1033,0,0.0600938,"e 6, filtered to those emotions which are available in the text corpus we use. (2018). Buechel et al. (2016) have developed a methodological framework to adapt existing affect lexicons to specific use cases. Other than dictionaries, emotion analysis relies on labeled corpora. Some of them include information relative to valence and arousal (Buechel and Hahn, 2017; Preot¸iuc-Pietro et al., 2016), but the majority of resources use discrete emotion classes, for instance to label fairy tales (Alm et al., 2005), blogs (Aman and Szpakowicz, 2007), tweets (Mohammad et al., 2017; Schuff et al., 2017; Mohammad, 2012; Mohammad and Bravo-Marquez, 2017; Klinger et al., 2018), Facebook posts (Preot¸iuc-Pietro et al., 2016), news headlines (Strapparava and Mihalcea, 2007), dialogues (Li et al., 2017), literary texts (Kim et al., 2017), or self reports on emotion events (Scherer and Wallbott, 1997; Troiano et al., 2019). We point the reader to the survey by Bostan and Klinger (2018) for a more an overview. Most automatic methods that assign labels to text rely on machine learning (Alm et al., 2005; Aman and Szpakowicz, 2007; Schuff et al., 2017, i.a.). Recent shared tasks showed an increase in transfer learnin"
2020.coling-main.11,P18-1017,0,0.126816,"Missing"
2020.coling-main.11,D14-1162,0,0.0861277,"move to our evaluation if automatic methods to recognize emotions can benefit from being informed about appraisal dimensions explicitly. We first describe our models and how we address our research questions and then turn to the results. 4.1 Model Configuration Figure 1 illustrates the four different tasks addressed by our models. Task T→E is the prediction of emotions from text, namely the standard setting in nearly all previous work of emotion analysis. We use a convolutional neural network (CNN) inspired by Kim (2014), with pretrained GloVe (Glove840B) as a 300-dimensional embedding layer (Pennington et al., 2014)3 with convolution filter sizes of 2, 3, and 4 with a ReLu activation function (Nair and Hinton, 2010), followed by a max pooling layer of length 2 and a dropout of 0.5 followed by another dense layer. As another model to predict emotions, we use a pipeline based on two steps, one to detect the appraisal from text, and the second to assign the appropriate emotion to the appraisal. We refer to Task T→A as the step of identifying appraisal from text. We use the model configuration of Task T→E, except for the sigmoid activation function and binary cross entropy loss instead of softmax and cross-e"
2020.coling-main.11,W16-0404,0,0.0957022,"Missing"
2020.coling-main.11,W17-5203,1,0.908301,"Missing"
2020.coling-main.11,S07-1013,0,0.774537,"r, by the author of a text, or a character in it. Depending on the resource which has been annotated, the description of an emotion experience can vary. On Twitter, for instance, other than direct reports of an emotion state (“I feel depressed”), hashtags are used as emotion labels to enrich the description of events and stances (“I just got my exam result #sad”). In news articles, emotional events are sometimes explicitly mentioned (“couple infuriate officials” (Bostan et al., 2020)) and other times require world knowledge (“Tom Cruise and Katie Holmes set wedding date”, labeled as surprise (Strapparava and Mihalcea, 2007)). In literature, a sequence of events which forms the narrative leads to an emotion in the reader. In this paper, we focus on those texts which communicate emotions without an explicit emotion word, but rather describe events for which an emotion association is evident. Such textual examples became popular in natural language processing research with the use of the data generated in the ISEAR project (Scherer and Wallbott, 1997). The project led to a dataset of descriptions of events triggering specific affective states, which was originally collected to study event interpretations with a psy"
2020.coling-main.11,strapparava-valitutti-2004-wordnet,0,0.494742,"ing the cognitive components described by Smith and Ellsworth (1985). We show their main findings in Table 1, limited to the emotions that are available in the corpus we use. 2.3 Automatic Emotion Classification Previous work on emotion analysis in natural language processing focuses either on resource creation or on emotion classification for a specific task and domain. On the side of resource creation, the early and influential work of Pennebaker et al. (2001) is a dictionary of words being associated with different psychologically relevant categories, including a subset of emotions. Later, Strapparava and Valitutti (2004) made WordNet Affect available to target word classes and differences regarding their emotional connotation, Mohammad and Turney (2012) released the NRC dictionary with more than 14,000 words for a set of discrete emotion classes, and a valence-arousal-dominance dictionary was provided by Mohammad 127 Emotion Happiness Sadness Anger Fear Disgust Shame Guilt Unpleasant Responsibility Uncertainty Attention Effort Control −1.46 0.87 0.85 0.44 0.38 0.73 0.60 0.09 −0.36 −0.94 −0.17 −0.50 1.31 1.31 −0.46 0.00 −0.29 0.73 −0.39 0.21 −0.15 0.15 −0.21 0.12 0.03 −0.96 −0.11 −0.36 −0.33 −0.14 0.53 0.63 0."
2020.coling-main.11,P19-1391,1,0.750132,"Missing"
2020.coling-main.384,H05-1073,0,0.147361,"d). These realizations differ widely across domains and genres (Bostan and Klinger, 2018). To gain a representative picture and investigate the effect of translation on different emotion realizations, we compare four English corpora. ISEAR (Scherer and Wallbott, 1994) includes ≈7k descriptions of events. Each description is labeled with the emotion that it induced in the experiencers (anger, disgust, fear, guilt, joy, sadness and shame). TEC (Mohammad, 2012) contains ≈ 21k tweets associated to the six fundamental Ekman’s emotions (Ekman, 1992). The corpora by Aman and Szpakowicz (2007) and by Alm et al. (2005) are repertoires of ≈5k and ≈15k sentences from a number of Blogs and (fairy-)Tales, respectively (using Ekman+noemo). These corpora differ in labels (see Figure 3 vs. 4), topics, registers and communicative purposes: TEC collects short, spontaneous expressions, ISEAR provides statements that were produced in-lab. Micro F1 Em. ISEAR Blogs Tales TEC A D F G J No Sa Su Sh .51 .58 .70 .55 .72 — .61 — .46 .55 .64 .56 — .69 .88 .49 .41 — .39 .12 .33 — .45 .79 .37 .27 — .37 .26 .55 — .69 — .45 .49 — Table 2: Classification results (“—”: the emotion is not a label in the respective corpus). Emotion C"
2020.coling-main.384,W12-3709,0,0.171736,"al resource creation (Barnes and Klinger, 2019). With the increasing use of MT, however, expectations about output quality also grow, and now that the goal of adequacy with regard to propositional content is met more often than not, more subtle aspects start receiving attention. One such aspect is affective content. Establishing common ground is essential for successful MT-assisted communication (Yamashita et al., 2009), but it is still unclear how well MT promotes this, especially when handling the affective qualities of texts. On the one hand, it is able to mostly preserve author sentiment (Balahur and Turchi, 2012). On the other, it is known that translation obfuscates some socio-demographic characteristics of authors, like gender and personality traits (Mirkin et al., 2015; Rabinovich et al., 2017). In this paper, we investigate the question of how well emotions are preserved in MT. Answering this question and, if necessary, increasing the degree of emotion preservation, is important both theoretically (to inform cross-lingual studies that use translation as part of their experimental setup) and practically (to improve the usefulness of MT). The starting point of our research is a study by Rabinovich e"
2020.coling-main.384,banea-etal-2008-bootstrapping,0,0.0611729,") or colexification phenomena (i.e., naming related emotions with the same word, like grief and regret in Persian) which vary from language to language (Jackson et al., 2019). On the other hand, aesthetic considerations often call for making texts more readable or pleasant. These factors hamper methods that transfer affect or sentiment across languages, as they cause both translation errors (for human and machines alike) and stylistic choices which subvert the sentiment of words (Petrova and Rodionova, 2016). Thus, assessing the quality of sentimentannotated resources produced by translation (Banea et al., 2008; Chen and Skiena, 2014; Buechel et al., 2020, i.a.) is crucial. With this goal, Kajava et al. (2020) compare sentiment and emotion annotations of movie subtitles in English, Finnish, Italian, and French and find that the emotion preservation depends on the language pair. Validating resources for Romanian created from English ones, Mihalcea et al. (2007) notice that human translation can obscure the subjectivity of a lexicon. A comparable observation is drawn for polarity by Balahur and Turchi (2012) with SMT, and by Salameh et al. (2015) and Mohammad et al. (2016) who find that translation ca"
2020.coling-main.384,P05-1074,0,0.0825639,"g: this would require either manual annotation or highly comparable emotion classifiers for several languages. Manual annotations are costly, and emotion annotation is known to be tricky in terms of intersubjective replicability (Schuff et al., 2017). Neither are we aware of emotion classifiers with evidently similar behavior across languages. To circumvent this problem, our experimental setup uses a back-translation version of the method described above, shown in Figure 2. We compose two translation steps (S→T and T→S) such that the output is a paraphrase of the input in the same language S (Bannard and Callison-Burch, 2005) and the pitfalls of cross-lingual comparability can be avoided. Formally, given an input in S and a target 4343 Translation Input T S Forward Emotion Classification Backward S n S Emotion-Informed Selection Output S n Parameters: Decoding, Data, Target Language Figure 2: Instantiation of the method with back-translation. S is the source language; T is the target. emotion, we generate the best translation in T; this is then translated back into multiple hypotheses, providing a set of n paraphrases for the original input. We acknowledge that this type of setup is a conceptual simplification of"
2020.coling-main.384,C18-1179,1,0.910734,"sian models1 (Ng et al., 2019). These sentence-level models are based on transformers (Vaswani et al., 2017) and pretrained on bitext and back-translated news data, fine-tuned on in-domain data and used for decoding with a noisy channel approach to re-rank the n-best hypotheses. We use these models both with beamsearch and top-k sampling (cf. Section 3). Data Sets: Varying Emotion Realization. Emotions manifest themselves in various linguistic realizations, for instance with direct mentions (sad) or indirect associations (abandoned). These realizations differ widely across domains and genres (Bostan and Klinger, 2018). To gain a representative picture and investigate the effect of translation on different emotion realizations, we compare four English corpora. ISEAR (Scherer and Wallbott, 1994) includes ≈7k descriptions of events. Each description is labeled with the emotion that it induced in the experiencers (anger, disgust, fear, guilt, joy, sadness and shame). TEC (Mohammad, 2012) contains ≈ 21k tweets associated to the six fundamental Ekman’s emotions (Ekman, 1992). The corpora by Aman and Szpakowicz (2007) and by Alm et al. (2005) are repertoires of ≈5k and ≈15k sentences from a number of Blogs and (f"
2020.coling-main.384,2020.acl-main.112,0,0.0440317,"Missing"
2020.coling-main.384,P14-2063,0,0.0238516,"phenomena (i.e., naming related emotions with the same word, like grief and regret in Persian) which vary from language to language (Jackson et al., 2019). On the other hand, aesthetic considerations often call for making texts more readable or pleasant. These factors hamper methods that transfer affect or sentiment across languages, as they cause both translation errors (for human and machines alike) and stylistic choices which subvert the sentiment of words (Petrova and Rodionova, 2016). Thus, assessing the quality of sentimentannotated resources produced by translation (Banea et al., 2008; Chen and Skiena, 2014; Buechel et al., 2020, i.a.) is crucial. With this goal, Kajava et al. (2020) compare sentiment and emotion annotations of movie subtitles in English, Finnish, Italian, and French and find that the emotion preservation depends on the language pair. Validating resources for Romanian created from English ones, Mihalcea et al. (2007) notice that human translation can obscure the subjectivity of a lexicon. A comparable observation is drawn for polarity by Balahur and Turchi (2012) with SMT, and by Salameh et al. (2015) and Mohammad et al. (2016) who find that translation can corrupt textual senti"
2020.coling-main.384,P18-1082,0,0.0228633,"earch and top-k sampling, which differ in their ability to encourage diversity in the output. Beamsearch searches the space of hypotheses left-to-right, retaining at each time step a number of top-scoring candidates that equals the width of the beam, and expanding on those. Sequences decoded with beamsearch differ on minimal portions (Gimpel et al., 2013), while they are more varied when generated with sampling strategies. Top-k sampling, for instance, does not aim at maximizing the likelihood of text. Instead, it randomly samples words step-wise and outputs from the top-k most probable ones (Fan et al., 2018). Emotion Classification Model. To estimate the probability distribution over emotions for a given text, we use a biLSTM with a self-attention mechanism. This model architecture has been shown to perform close to state-of-the-art in emotion analysis (Baziotis et al., 2018). We treat the output of this emotion classifier as a scoring function emo(t, e) = p(e|t), i.e., the conditional probability of an emotion given a text t, and we assume that it is comparable across languages (see Section 4 for a discussion of this assumption). Translation Candidate Selection. Once the n translation candidates"
2020.coling-main.384,D13-1111,0,0.0293801,"ded. It shows state-of-the-art performance and it was developed with the goal to replicate other model architectures. Therefore, we assume that it is reasonably representative for other models. Importantly, FAIRSEQ supports different search algorithms, like beamsearch and top-k sampling, which differ in their ability to encourage diversity in the output. Beamsearch searches the space of hypotheses left-to-right, retaining at each time step a number of top-scoring candidates that equals the width of the beam, and expanding on those. Sequences decoded with beamsearch differ on minimal portions (Gimpel et al., 2013), while they are more varied when generated with sampling strategies. Top-k sampling, for instance, does not aim at maximizing the likelihood of text. Instead, it randomly samples words step-wise and outputs from the top-k most probable ones (Fan et al., 2018). Emotion Classification Model. To estimate the probability distribution over emotions for a given text, we use a biLSTM with a self-attention mechanism. This model architecture has been shown to perform close to state-of-the-art in emotion analysis (Baziotis et al., 2018). We treat the output of this emotion classifier as a scoring funct"
2020.coling-main.384,N19-1320,0,0.0164461,"towards a specific target attribute. The style transfer challenge is to create a fluent output that is semantically similar to the input, but differs systematically in style. Helbig et al. (2020) control for the balance between content, style and fluency with a dedicated component in their modular pipeline: after a text is re-written in many emotion variations, these are re-ranked by an objective function that measures their perplexity, preservation of content and expression of a target style. Evaluation metrics for these three desiderata are applied in the reinforcement learning approach of Gong et al. (2019) to impose constraints on output generation. Other attempts focus on the explicit separation between content and sentiment style (Li et al., 2018; Wen et al., 2020). Prabhumoye et al. (2018) do so using neural back-translation: in the latent representation of an input text, its stylistic properties are overwritten, which results in a style-specific paraphrase. Like them, we tap on back-translation as a paraphrasing strategy, but we transfer emotions, which we conceptualize as fine-grained styles. Using state-of-the-art off-the-shelf systems, we move from the problem of guaranteeing fluency and"
2020.coling-main.384,guerini-etal-2008-valentino,0,0.0113094,"at investigate specifically the preservation of emotions in MT. Style Transfer. Related to ours is the task of style transfer. This research direction leverages a variety of methods, from rule-based lexical substitution to sophisticated neural architectures, aiming at retaining the semantics of texts while modifying their linguistic properties, like genre (Lee et al., 2019; Jhamtani et al., 2017), romanticism (Li et al., 2018), politeness/offensiveness and formality (Sennrich et al., 2016; Nogueira dos Santos et al., 2018; Wang et al., 2019) and, importantly for us, affect-related attributes (Guerini et al., 2008; Whitehead and Cavedon, 2010; Shen et al., 2017; Fu et al., 2018; Xu et al., 2018; Smith et al., 2019; Helbig et al., 2020). Yet, only a handful of style transfer studies have considered emotions. Helbig et al. (2020), for instance, propose an interpretable framework based on lexical substitution which sequentially determines the portion of text to modify, performs the change, and filters out undesired output. Smith et al. (2019), instead, leverage a denoising auto-encoder and a back-translation objective to push the text generated during decoding towards a specific target attribute. The styl"
2020.coling-main.384,2020.socialnlp-1.6,1,0.878518,"nguages. Within this framework, we address three research questions (Table 1 shows motivating examples). We first ask if a state-of-the-art machine translation system, namely FAIRSEQ (Ott et al., 2019), loses emotional information during translation (RQ1). (Yes.) Next, we propose a post-processing step to re-rank n-best translation candidates and evaluate if this improves emotion preservation (RQ2). (It does.) Finally, we exploit the emotional variation in MT output to investigate whether this approach can actively change the input emotion (RQ3), essentially performing emotion style transfer (Helbig et al., 2020). (It can.) The implementation of the pipeline is available at http://www.ims.uni-stuttgart.de/ data/emotion-transfer. 2 Related Work Affect, Sentiment, and Emotion in Translation. Preserving affect in text is an issue for translation and other cross-linguistic studies (Wierzbicka, 2013; Wassmann, 2017; Hubscher-Davidson, 2017). On the one hand, there are linguistic constraints on translation, like the absence of terms for certain states (e.g., Sehnsucht is German for “a longing for some absent thing”) or colexification phenomena (i.e., naming related emotions with the same word, like grief an"
2020.coling-main.384,W17-4902,0,0.0150483,"negative/positive meanings of the ambiguous word as separate embeddings. While these studies gained some insight on translated polarity, subjectivity, valence, dominance and arousal, to our knowledge there are no studies that investigate specifically the preservation of emotions in MT. Style Transfer. Related to ours is the task of style transfer. This research direction leverages a variety of methods, from rule-based lexical substitution to sophisticated neural architectures, aiming at retaining the semantics of texts while modifying their linguistic properties, like genre (Lee et al., 2019; Jhamtani et al., 2017), romanticism (Li et al., 2018), politeness/offensiveness and formality (Sennrich et al., 2016; Nogueira dos Santos et al., 2018; Wang et al., 2019) and, importantly for us, affect-related attributes (Guerini et al., 2008; Whitehead and Cavedon, 2010; Shen et al., 2017; Fu et al., 2018; Xu et al., 2018; Smith et al., 2019; Helbig et al., 2020). Yet, only a handful of style transfer studies have considered emotions. Helbig et al. (2020), for instance, propose an interpretable framework based on lexical substitution which sequentially determines the portion of text to modify, performs the change"
2020.coling-main.384,W19-2309,0,0.0198761,"d by learning the negative/positive meanings of the ambiguous word as separate embeddings. While these studies gained some insight on translated polarity, subjectivity, valence, dominance and arousal, to our knowledge there are no studies that investigate specifically the preservation of emotions in MT. Style Transfer. Related to ours is the task of style transfer. This research direction leverages a variety of methods, from rule-based lexical substitution to sophisticated neural architectures, aiming at retaining the semantics of texts while modifying their linguistic properties, like genre (Lee et al., 2019; Jhamtani et al., 2017), romanticism (Li et al., 2018), politeness/offensiveness and formality (Sennrich et al., 2016; Nogueira dos Santos et al., 2018; Wang et al., 2019) and, importantly for us, affect-related attributes (Guerini et al., 2008; Whitehead and Cavedon, 2010; Shen et al., 2017; Fu et al., 2018; Xu et al., 2018; Smith et al., 2019; Helbig et al., 2020). Yet, only a handful of style transfer studies have considered emotions. Helbig et al. (2020), for instance, propose an interpretable framework based on lexical substitution which sequentially determines the portion of text to mod"
2020.coling-main.384,N18-1169,0,0.148648,"iguous word as separate embeddings. While these studies gained some insight on translated polarity, subjectivity, valence, dominance and arousal, to our knowledge there are no studies that investigate specifically the preservation of emotions in MT. Style Transfer. Related to ours is the task of style transfer. This research direction leverages a variety of methods, from rule-based lexical substitution to sophisticated neural architectures, aiming at retaining the semantics of texts while modifying their linguistic properties, like genre (Lee et al., 2019; Jhamtani et al., 2017), romanticism (Li et al., 2018), politeness/offensiveness and formality (Sennrich et al., 2016; Nogueira dos Santos et al., 2018; Wang et al., 2019) and, importantly for us, affect-related attributes (Guerini et al., 2008; Whitehead and Cavedon, 2010; Shen et al., 2017; Fu et al., 2018; Xu et al., 2018; Smith et al., 2019; Helbig et al., 2020). Yet, only a handful of style transfer studies have considered emotions. Helbig et al. (2020), for instance, propose an interpretable framework based on lexical substitution which sequentially determines the portion of text to modify, performs the change, and filters out undesired out"
2020.coling-main.384,P19-1194,0,0.0398724,"Missing"
2020.coling-main.384,E17-1083,0,0.144852,"sfer. An in-depth qualitative analysis reveals that there are recurring linguistic changes through which emotions are toned down or amplified, such as change of modality. 1 Introduction The quality of machine translation (MT) models in some areas follows close behind that of humans (Barrault et al., 2019). MT is deployed widely to support human-to-human communication across languages, e.g., in chat systems, customer support, or (social) media. It is also employed in downstream NLP tasks such as sentence simplification (Xu et al., 2016), error correction (Yuan and Briscoe, 2016), paraphrasing (Mallinson et al., 2017; Wieting and Gimpel, 2018), or cross-lingual resource creation (Barnes and Klinger, 2019). With the increasing use of MT, however, expectations about output quality also grow, and now that the goal of adequacy with regard to propositional content is met more often than not, more subtle aspects start receiving attention. One such aspect is affective content. Establishing common ground is essential for successful MT-assisted communication (Yamashita et al., 2009), but it is still unclear how well MT promotes this, especially when handling the affective qualities of texts. On the one hand, it is"
2020.coling-main.384,P07-1123,0,0.499469,"es, as they cause both translation errors (for human and machines alike) and stylistic choices which subvert the sentiment of words (Petrova and Rodionova, 2016). Thus, assessing the quality of sentimentannotated resources produced by translation (Banea et al., 2008; Chen and Skiena, 2014; Buechel et al., 2020, i.a.) is crucial. With this goal, Kajava et al. (2020) compare sentiment and emotion annotations of movie subtitles in English, Finnish, Italian, and French and find that the emotion preservation depends on the language pair. Validating resources for Romanian created from English ones, Mihalcea et al. (2007) notice that human translation can obscure the subjectivity of a lexicon. A comparable observation is drawn for polarity by Balahur and Turchi (2012) with SMT, and by Salameh et al. (2015) and Mohammad et al. (2016) who find that translation can corrupt textual sentiment, flattening positive and negative aspects down to neutrality. In MT research, some studies specifically try to incentivize the preservation of sentiment. Lohar et al. (2017) build separate translation models for data coming from each sentiment category. Si et al. (2019) 4341 Input Translation Emotion Classification S T Emotion"
2020.coling-main.384,N19-1049,0,0.0131783,"overgeneration to transfer a target emotion on a text? Having shown that MT prefers to output sentences with a toned-down emotion, and that it is possible to subselect instances with a similar emotional connotation as the input, we now turn to the question if diversity in MT output can be used for the task of emotion style transfer. In this setting, our backtranslation pipeline is used for paraphrasing with style transfer, following Xu et al. (2012) and Prabhumoye et al. (2018). Given an input text t and an emotion e, we want to produce a variation t0 which respects the following desiderata (Mir et al., 2019): it maximizes similarity with t; it is fluent and it expresses emotion e. Backtranslations provide us with a particularly easy setup: since MT systems are trained to maximize the fluency of their output and their faithfulness to the input, we assume that it is sufficient to pay attention to the presence of the target emotion (see Eq. (1)). Forward and backward translation steps alike are carried on through beamsearch or top-k sampling, with k=10, both producing n=50 paraphrases. Since this experiment tries to promote stylistic diversity, n-best lists could have been leveraged also in the targ"
2020.coling-main.384,D15-1130,0,0.0227321,"with regard to propositional content is met more often than not, more subtle aspects start receiving attention. One such aspect is affective content. Establishing common ground is essential for successful MT-assisted communication (Yamashita et al., 2009), but it is still unclear how well MT promotes this, especially when handling the affective qualities of texts. On the one hand, it is able to mostly preserve author sentiment (Balahur and Turchi, 2012). On the other, it is known that translation obfuscates some socio-demographic characteristics of authors, like gender and personality traits (Mirkin et al., 2015; Rabinovich et al., 2017). In this paper, we investigate the question of how well emotions are preserved in MT. Answering this question and, if necessary, increasing the degree of emotion preservation, is important both theoretically (to inform cross-lingual studies that use translation as part of their experimental setup) and practically (to improve the usefulness of MT). The starting point of our research is a study by Rabinovich et al. (2017), who show that some semantic nuances tend to vanish in translation. In fact, just like human translation, MT is not guaranteed to preserve any of the"
2020.coling-main.384,S12-1033,0,0.0268988,"Realization. Emotions manifest themselves in various linguistic realizations, for instance with direct mentions (sad) or indirect associations (abandoned). These realizations differ widely across domains and genres (Bostan and Klinger, 2018). To gain a representative picture and investigate the effect of translation on different emotion realizations, we compare four English corpora. ISEAR (Scherer and Wallbott, 1994) includes ≈7k descriptions of events. Each description is labeled with the emotion that it induced in the experiencers (anger, disgust, fear, guilt, joy, sadness and shame). TEC (Mohammad, 2012) contains ≈ 21k tweets associated to the six fundamental Ekman’s emotions (Ekman, 1992). The corpora by Aman and Szpakowicz (2007) and by Alm et al. (2005) are repertoires of ≈5k and ≈15k sentences from a number of Blogs and (fairy-)Tales, respectively (using Ekman+noemo). These corpora differ in labels (see Figure 3 vs. 4), topics, registers and communicative purposes: TEC collects short, spontaneous expressions, ISEAR provides statements that were produced in-lab. Micro F1 Em. ISEAR Blogs Tales TEC A D F G J No Sa Su Sh .51 .58 .70 .55 .72 — .61 — .46 .55 .64 .56 — .69 .88 .49 .41 — .39 .12"
2020.coling-main.384,W19-5333,0,0.0180596,"ent MT settings (which we do in the sections below). In addition, results that would indicate that we can improve emotion preservation in back-translation would conversely be stronger than such results obtained on a single translation step. 4.1 Experimental Setup Details Following the considerations of the previous paragraph, we do not run a single experiment, but instead carry out a series of comparisons, varying the different parameters of the emotion preservation method. NMT Model: Varying target language and sampling method. We use FAIRSEQ with English– German and English–Russian models1 (Ng et al., 2019). These sentence-level models are based on transformers (Vaswani et al., 2017) and pretrained on bitext and back-translated news data, fine-tuned on in-domain data and used for decoding with a noisy channel approach to re-rank the n-best hypotheses. We use these models both with beamsearch and top-k sampling (cf. Section 3). Data Sets: Varying Emotion Realization. Emotions manifest themselves in various linguistic realizations, for instance with direct mentions (sad) or indirect associations (abandoned). These realizations differ widely across domains and genres (Bostan and Klinger, 2018). To"
2020.coling-main.384,P18-2031,0,0.0248135,"rity, subjectivity, valence, dominance and arousal, to our knowledge there are no studies that investigate specifically the preservation of emotions in MT. Style Transfer. Related to ours is the task of style transfer. This research direction leverages a variety of methods, from rule-based lexical substitution to sophisticated neural architectures, aiming at retaining the semantics of texts while modifying their linguistic properties, like genre (Lee et al., 2019; Jhamtani et al., 2017), romanticism (Li et al., 2018), politeness/offensiveness and formality (Sennrich et al., 2016; Nogueira dos Santos et al., 2018; Wang et al., 2019) and, importantly for us, affect-related attributes (Guerini et al., 2008; Whitehead and Cavedon, 2010; Shen et al., 2017; Fu et al., 2018; Xu et al., 2018; Smith et al., 2019; Helbig et al., 2020). Yet, only a handful of style transfer studies have considered emotions. Helbig et al. (2020), for instance, propose an interpretable framework based on lexical substitution which sequentially determines the portion of text to modify, performs the change, and filters out undesired output. Smith et al. (2019), instead, leverage a denoising auto-encoder and a back-translation objec"
2020.coling-main.384,N19-4009,0,0.0390131,"e→target→source, which we can examine with only one emotion classifier for the source language. We acknowledge that this solution makes a simplifying assumption, namely that experimenting with back-translation can give a realistic picture of what would happen in a source→target setting. Still, adding a translation step seems a reasonable compromise in the absence of comparable emotion classifiers for different languages. Within this framework, we address three research questions (Table 1 shows motivating examples). We first ask if a state-of-the-art machine translation system, namely FAIRSEQ (Ott et al., 2019), loses emotional information during translation (RQ1). (Yes.) Next, we propose a post-processing step to re-rank n-best translation candidates and evaluate if this improves emotion preservation (RQ2). (It does.) Finally, we exploit the emotional variation in MT output to investigate whether this approach can actively change the input emotion (RQ3), essentially performing emotion style transfer (Helbig et al., 2020). (It can.) The implementation of the pipeline is available at http://www.ims.uni-stuttgart.de/ data/emotion-transfer. 2 Related Work Affect, Sentiment, and Emotion in Translation."
2020.coling-main.384,D14-1162,0,0.0847682,"Missing"
2020.coling-main.384,P18-1080,0,0.126853,"t al. (2020) control for the balance between content, style and fluency with a dedicated component in their modular pipeline: after a text is re-written in many emotion variations, these are re-ranked by an objective function that measures their perplexity, preservation of content and expression of a target style. Evaluation metrics for these three desiderata are applied in the reinforcement learning approach of Gong et al. (2019) to impose constraints on output generation. Other attempts focus on the explicit separation between content and sentiment style (Li et al., 2018; Wen et al., 2020). Prabhumoye et al. (2018) do so using neural back-translation: in the latent representation of an input text, its stylistic properties are overwritten, which results in a style-specific paraphrase. Like them, we tap on back-translation as a paraphrasing strategy, but we transfer emotions, which we conceptualize as fine-grained styles. Using state-of-the-art off-the-shelf systems, we move from the problem of guaranteeing fluency and similarity to an input. In line with ours, a few other works have attempted to generate emotionally loaded text for given emotion classes, for instance in dialogue systems (Song et al., 201"
2020.coling-main.384,E17-1101,0,0.0259886,"itional content is met more often than not, more subtle aspects start receiving attention. One such aspect is affective content. Establishing common ground is essential for successful MT-assisted communication (Yamashita et al., 2009), but it is still unclear how well MT promotes this, especially when handling the affective qualities of texts. On the one hand, it is able to mostly preserve author sentiment (Balahur and Turchi, 2012). On the other, it is known that translation obfuscates some socio-demographic characteristics of authors, like gender and personality traits (Mirkin et al., 2015; Rabinovich et al., 2017). In this paper, we investigate the question of how well emotions are preserved in MT. Answering this question and, if necessary, increasing the degree of emotion preservation, is important both theoretically (to inform cross-lingual studies that use translation as part of their experimental setup) and practically (to improve the usefulness of MT). The starting point of our research is a study by Rabinovich et al. (2017), who show that some semantic nuances tend to vanish in translation. In fact, just like human translation, MT is not guaranteed to preserve any of the linguistic properties of"
2020.coling-main.384,N15-1078,0,0.222246,"y of sentimentannotated resources produced by translation (Banea et al., 2008; Chen and Skiena, 2014; Buechel et al., 2020, i.a.) is crucial. With this goal, Kajava et al. (2020) compare sentiment and emotion annotations of movie subtitles in English, Finnish, Italian, and French and find that the emotion preservation depends on the language pair. Validating resources for Romanian created from English ones, Mihalcea et al. (2007) notice that human translation can obscure the subjectivity of a lexicon. A comparable observation is drawn for polarity by Balahur and Turchi (2012) with SMT, and by Salameh et al. (2015) and Mohammad et al. (2016) who find that translation can corrupt textual sentiment, flattening positive and negative aspects down to neutrality. In MT research, some studies specifically try to incentivize the preservation of sentiment. Lohar et al. (2017) build separate translation models for data coming from each sentiment category. Si et al. (2019) 4341 Input Translation Emotion Classification S T Emotion-Informed Selection n Output T Figure 1: Overview of emotion preservation and transfer (method). directly incorporate sentiment in their neural MT system, implementing a Seq2Seq English-to"
2020.coling-main.384,W17-5203,1,0.891722,"Missing"
2020.coling-main.384,N16-1005,0,0.02944,"ined some insight on translated polarity, subjectivity, valence, dominance and arousal, to our knowledge there are no studies that investigate specifically the preservation of emotions in MT. Style Transfer. Related to ours is the task of style transfer. This research direction leverages a variety of methods, from rule-based lexical substitution to sophisticated neural architectures, aiming at retaining the semantics of texts while modifying their linguistic properties, like genre (Lee et al., 2019; Jhamtani et al., 2017), romanticism (Li et al., 2018), politeness/offensiveness and formality (Sennrich et al., 2016; Nogueira dos Santos et al., 2018; Wang et al., 2019) and, importantly for us, affect-related attributes (Guerini et al., 2008; Whitehead and Cavedon, 2010; Shen et al., 2017; Fu et al., 2018; Xu et al., 2018; Smith et al., 2019; Helbig et al., 2020). Yet, only a handful of style transfer studies have considered emotions. Helbig et al. (2020), for instance, propose an interpretable framework based on lexical substitution which sequentially determines the portion of text to modify, performs the change, and filters out undesired output. Smith et al. (2019), instead, leverage a denoising auto-en"
2020.coling-main.384,D19-5227,0,0.0219631,"g resources for Romanian created from English ones, Mihalcea et al. (2007) notice that human translation can obscure the subjectivity of a lexicon. A comparable observation is drawn for polarity by Balahur and Turchi (2012) with SMT, and by Salameh et al. (2015) and Mohammad et al. (2016) who find that translation can corrupt textual sentiment, flattening positive and negative aspects down to neutrality. In MT research, some studies specifically try to incentivize the preservation of sentiment. Lohar et al. (2017) build separate translation models for data coming from each sentiment category. Si et al. (2019) 4341 Input Translation Emotion Classification S T Emotion-Informed Selection n Output T Figure 1: Overview of emotion preservation and transfer (method). directly incorporate sentiment in their neural MT system, implementing a Seq2Seq English-to-Chinese translation model that keeps not only the semantics but also the sentiment of input text, both by including the sentiment label in source sentences, and by learning the negative/positive meanings of the ambiguous word as separate embeddings. While these studies gained some insight on translated polarity, subjectivity, valence, dominance and ar"
2020.coling-main.384,P19-1359,0,0.0201472,"ye et al. (2018) do so using neural back-translation: in the latent representation of an input text, its stylistic properties are overwritten, which results in a style-specific paraphrase. Like them, we tap on back-translation as a paraphrasing strategy, but we transfer emotions, which we conceptualize as fine-grained styles. Using state-of-the-art off-the-shelf systems, we move from the problem of guaranteeing fluency and similarity to an input. In line with ours, a few other works have attempted to generate emotionally loaded text for given emotion classes, for instance in dialogue systems (Song et al., 2019; Zhou and Wang, 2018), but they create novel texts rather than re-styling existing ones. 3 A Method for Emotion Preservation in Neural Machine Translation We conceptualize emotion preservation in NMT as a post-processing re-ranking step. As shown in Figure 1, this involves three components: a translation model, an emotion classifier, and a candidate selection procedure. Starting from an input in source language S, we generate the n-best translation candidates in a target language T with an NMT system, which is presumably agnostic to emotion-specific considerations. Then, we re-rank these cand"
2020.coling-main.384,P19-1159,0,0.0129576,"fer, the input presents the action as one that the experiencer had to take. In d., sadness replaces disgust with the use of a softer expression, such as “loathe”. This example also highlights that removing a direct emotion word can determine a switch in connotation. Another reason why the backtranslation in b. is associated to fear could be that silence, in ISEAR, mostly occurs in the description of disruptive, frightening events, similarly to being “approached” by strangers (and hence, the joyful sentence in e. turns into fear). There are also signals that emotion changes show a gender bias (Sun et al., 2019): characterising the subject as a male moves anger to guilt or joy (a.), while we have found that female characters can elicit an association with shame. As for the transfer, it is possible that smaller lexical changes are sufficient to change emotions when the input label and the new emotions can co-occur. For instance, anger and guilt, being negative emotions, are more likely to co-occur than anger and joy, corresponding to output 1 and 3 for the first sentence. These examples also show that transfer can happen without disrupting grammaticality nor content – at least within the relatively sm"
2020.coling-main.384,D19-1365,0,0.0217347,"alence, dominance and arousal, to our knowledge there are no studies that investigate specifically the preservation of emotions in MT. Style Transfer. Related to ours is the task of style transfer. This research direction leverages a variety of methods, from rule-based lexical substitution to sophisticated neural architectures, aiming at retaining the semantics of texts while modifying their linguistic properties, like genre (Lee et al., 2019; Jhamtani et al., 2017), romanticism (Li et al., 2018), politeness/offensiveness and formality (Sennrich et al., 2016; Nogueira dos Santos et al., 2018; Wang et al., 2019) and, importantly for us, affect-related attributes (Guerini et al., 2008; Whitehead and Cavedon, 2010; Shen et al., 2017; Fu et al., 2018; Xu et al., 2018; Smith et al., 2019; Helbig et al., 2020). Yet, only a handful of style transfer studies have considered emotions. Helbig et al. (2020), for instance, propose an interpretable framework based on lexical substitution which sequentially determines the portion of text to modify, performs the change, and filters out undesired output. Smith et al. (2019), instead, leverage a denoising auto-encoder and a back-translation objective to push the tex"
2020.coling-main.384,2020.lrec-1.575,0,0.0295226,"in style. Helbig et al. (2020) control for the balance between content, style and fluency with a dedicated component in their modular pipeline: after a text is re-written in many emotion variations, these are re-ranked by an objective function that measures their perplexity, preservation of content and expression of a target style. Evaluation metrics for these three desiderata are applied in the reinforcement learning approach of Gong et al. (2019) to impose constraints on output generation. Other attempts focus on the explicit separation between content and sentiment style (Li et al., 2018; Wen et al., 2020). Prabhumoye et al. (2018) do so using neural back-translation: in the latent representation of an input text, its stylistic properties are overwritten, which results in a style-specific paraphrase. Like them, we tap on back-translation as a paraphrasing strategy, but we transfer emotions, which we conceptualize as fine-grained styles. Using state-of-the-art off-the-shelf systems, we move from the problem of guaranteeing fluency and similarity to an input. In line with ours, a few other works have attempted to generate emotionally loaded text for given emotion classes, for instance in dialogue"
2020.coling-main.384,W10-0211,0,0.0391602,"cally the preservation of emotions in MT. Style Transfer. Related to ours is the task of style transfer. This research direction leverages a variety of methods, from rule-based lexical substitution to sophisticated neural architectures, aiming at retaining the semantics of texts while modifying their linguistic properties, like genre (Lee et al., 2019; Jhamtani et al., 2017), romanticism (Li et al., 2018), politeness/offensiveness and formality (Sennrich et al., 2016; Nogueira dos Santos et al., 2018; Wang et al., 2019) and, importantly for us, affect-related attributes (Guerini et al., 2008; Whitehead and Cavedon, 2010; Shen et al., 2017; Fu et al., 2018; Xu et al., 2018; Smith et al., 2019; Helbig et al., 2020). Yet, only a handful of style transfer studies have considered emotions. Helbig et al. (2020), for instance, propose an interpretable framework based on lexical substitution which sequentially determines the portion of text to modify, performs the change, and filters out undesired output. Smith et al. (2019), instead, leverage a denoising auto-encoder and a back-translation objective to push the text generated during decoding towards a specific target attribute. The style transfer challenge is to cr"
2020.coling-main.384,P18-1042,0,0.016262,"ative analysis reveals that there are recurring linguistic changes through which emotions are toned down or amplified, such as change of modality. 1 Introduction The quality of machine translation (MT) models in some areas follows close behind that of humans (Barrault et al., 2019). MT is deployed widely to support human-to-human communication across languages, e.g., in chat systems, customer support, or (social) media. It is also employed in downstream NLP tasks such as sentence simplification (Xu et al., 2016), error correction (Yuan and Briscoe, 2016), paraphrasing (Mallinson et al., 2017; Wieting and Gimpel, 2018), or cross-lingual resource creation (Barnes and Klinger, 2019). With the increasing use of MT, however, expectations about output quality also grow, and now that the goal of adequacy with regard to propositional content is met more often than not, more subtle aspects start receiving attention. One such aspect is affective content. Establishing common ground is essential for successful MT-assisted communication (Yamashita et al., 2009), but it is still unclear how well MT promotes this, especially when handling the affective qualities of texts. On the one hand, it is able to mostly preserve au"
2020.coling-main.384,C12-1177,0,0.0264018,"eserving variation further down the space of candidate outputs (at least to a certain point) without sacrificing the system’s performance. 5.3 RQ3: Can we exploit overgeneration to transfer a target emotion on a text? Having shown that MT prefers to output sentences with a toned-down emotion, and that it is possible to subselect instances with a similar emotional connotation as the input, we now turn to the question if diversity in MT output can be used for the task of emotion style transfer. In this setting, our backtranslation pipeline is used for paraphrasing with style transfer, following Xu et al. (2012) and Prabhumoye et al. (2018). Given an input text t and an emotion e, we want to produce a variation t0 which respects the following desiderata (Mir et al., 2019): it maximizes similarity with t; it is fluent and it expresses emotion e. Backtranslations provide us with a particularly easy setup: since MT systems are trained to maximize the fluency of their output and their faithfulness to the input, we assume that it is sufficient to pay attention to the presence of the target emotion (see Eq. (1)). Forward and backward translation steps alike are carried on through beamsearch or top-k sampli"
2020.coling-main.384,Q16-1029,0,0.0182411,"lso be applied to change emotions, obtaining a model for emotion style transfer. An in-depth qualitative analysis reveals that there are recurring linguistic changes through which emotions are toned down or amplified, such as change of modality. 1 Introduction The quality of machine translation (MT) models in some areas follows close behind that of humans (Barrault et al., 2019). MT is deployed widely to support human-to-human communication across languages, e.g., in chat systems, customer support, or (social) media. It is also employed in downstream NLP tasks such as sentence simplification (Xu et al., 2016), error correction (Yuan and Briscoe, 2016), paraphrasing (Mallinson et al., 2017; Wieting and Gimpel, 2018), or cross-lingual resource creation (Barnes and Klinger, 2019). With the increasing use of MT, however, expectations about output quality also grow, and now that the goal of adequacy with regard to propositional content is met more often than not, more subtle aspects start receiving attention. One such aspect is affective content. Establishing common ground is essential for successful MT-assisted communication (Yamashita et al., 2009), but it is still unclear how well MT promotes this,"
2020.coling-main.384,P18-1090,0,0.0263812,"to ours is the task of style transfer. This research direction leverages a variety of methods, from rule-based lexical substitution to sophisticated neural architectures, aiming at retaining the semantics of texts while modifying their linguistic properties, like genre (Lee et al., 2019; Jhamtani et al., 2017), romanticism (Li et al., 2018), politeness/offensiveness and formality (Sennrich et al., 2016; Nogueira dos Santos et al., 2018; Wang et al., 2019) and, importantly for us, affect-related attributes (Guerini et al., 2008; Whitehead and Cavedon, 2010; Shen et al., 2017; Fu et al., 2018; Xu et al., 2018; Smith et al., 2019; Helbig et al., 2020). Yet, only a handful of style transfer studies have considered emotions. Helbig et al. (2020), for instance, propose an interpretable framework based on lexical substitution which sequentially determines the portion of text to modify, performs the change, and filters out undesired output. Smith et al. (2019), instead, leverage a denoising auto-encoder and a back-translation objective to push the text generated during decoding towards a specific target attribute. The style transfer challenge is to create a fluent output that is semantically similar to"
2020.coling-main.384,N16-1042,0,0.0247889,"btaining a model for emotion style transfer. An in-depth qualitative analysis reveals that there are recurring linguistic changes through which emotions are toned down or amplified, such as change of modality. 1 Introduction The quality of machine translation (MT) models in some areas follows close behind that of humans (Barrault et al., 2019). MT is deployed widely to support human-to-human communication across languages, e.g., in chat systems, customer support, or (social) media. It is also employed in downstream NLP tasks such as sentence simplification (Xu et al., 2016), error correction (Yuan and Briscoe, 2016), paraphrasing (Mallinson et al., 2017; Wieting and Gimpel, 2018), or cross-lingual resource creation (Barnes and Klinger, 2019). With the increasing use of MT, however, expectations about output quality also grow, and now that the goal of adequacy with regard to propositional content is met more often than not, more subtle aspects start receiving attention. One such aspect is affective content. Establishing common ground is essential for successful MT-assisted communication (Yamashita et al., 2009), but it is still unclear how well MT promotes this, especially when handling the affective qual"
2020.coling-main.384,P18-1104,0,0.0287804,"so using neural back-translation: in the latent representation of an input text, its stylistic properties are overwritten, which results in a style-specific paraphrase. Like them, we tap on back-translation as a paraphrasing strategy, but we transfer emotions, which we conceptualize as fine-grained styles. Using state-of-the-art off-the-shelf systems, we move from the problem of guaranteeing fluency and similarity to an input. In line with ours, a few other works have attempted to generate emotionally loaded text for given emotion classes, for instance in dialogue systems (Song et al., 2019; Zhou and Wang, 2018), but they create novel texts rather than re-styling existing ones. 3 A Method for Emotion Preservation in Neural Machine Translation We conceptualize emotion preservation in NMT as a post-processing re-ranking step. As shown in Figure 1, this involves three components: a translation model, an emotion classifier, and a candidate selection procedure. Starting from an input in source language S, we generate the n-best translation candidates in a target language T with an NMT system, which is presumably agnostic to emotion-specific considerations. Then, we re-rank these candidates based on probab"
2020.emnlp-main.396,C18-1139,0,0.0197969,"ts us to look more closely at results for individual span types, where we find that BERT+Feat+LSTM+CRF performs best on 16 of the 36 total span types, BERT+CRF on 7 span types, Feat+LSTM+CRF on 7 span types, and BERT+LSTM+CRF on 6 span types. Thus, ‘bespoke’ modeling of span types can evidently improve results. Even though our architectures are task-agnostic, and not tuned to particular tasks or datasets, our best architectures still perform quite competitively. For instance, on CoNLL’00, our BERT+Feat+LSTM+CRF model comes within 0.12 F1 points of the best published model’s F1 score of 97.62 (Akbik et al., 2018). For PARC, existing literature does not report micro-averaged F1 scores, but instead focuses only on F1 scores for content span detection. In this case, we find that our BERT+Feat+LSTM+CRF model beats the existing state of the art on this span type, achieving an F1 score of 78.1, compared to the score of 75 reported in Scheible et al. (2016). 5.3 Meta-learning Results The result of Step 2 is our performance prediction model. Table 3 shows both mean absolute error (MAE), which is directly interpretable as the mean 4885 BERT Feat Feat BERT Feat Feat LSTM LSTM BERT LSTM LSTM Feat BL BL CRF CRF L"
2020.emnlp-main.396,E14-1005,0,0.0264917,"ets. All of them have non-overlapping spans from a closed set of span types. In the following, we discuss (properties of) span types, assuming that each span type maps onto one span ID task. 2.1 Datasets Quotation Detection: PARC and R I Q UA. The Penn Attribution Relation Corpus (PARC) version 3.0 (Pareti, 2016) and the Rich Quotation Attribution Corpus (R I Q UA, Papay and Pad´o, 2020) are two datasets for quotation detection: models must identify direct and indirect quotation spans in text, which can be useful for social network construction (Elson et al., 2010) and coreference resolution (Almeida et al., 2014). The corpora cover articles from the Penn Treebank (PARC) and 19th century English novels (R I Q UA), respectively. Within each text, quotations are identified, along with each quotation’s speaker (or source), and its cue (an introducing word, usually a verb like “said”). We model detection of quotations as well as cues. As speaker and addressee identification are relation extraction tasks, we exclude these span types. Chunking: CoNLL’00. Chunking (shallow parsing) is an important preprocessing step in a number of NLP applications. We use the corpus from the 2000 CoNLL shared task on chunking"
2020.emnlp-main.396,S17-2091,0,0.0651276,"Missing"
2020.emnlp-main.396,J96-1002,0,0.226552,"hing (Pratapa et al., 2018). In terms of complexity, span ID tasks form a middle ground between simpler analysis tasks that predict labels for single linguistic units (such as lemmatization (Porter, 1980) or sentiment polarity classification (Liu, 2012)) and more complex analysis tasks such as relation extraction, which combines span ID with relation identification (Zelenko et al., 2002; Adel et al., 2018). Due to the rapid development of deep learning, an abundance of model architectures is available for the implementation of span ID tasks. These include isolated token classification models (Berger et al., 1996; Chieu and Ng, 2003), probabilistic models such as hidden Markov models (Rabiner, 1989), maximum entropy Markov models (McCallum et al., 2000), and conditional random fields (Lafferty et al., 2001), recurrent neural networks such as LSTMs (Hochreiter and Schmidhuber, 1997), and transformers such as BERT (Devlin et al., 2019). Though we have some understanding what each of these models can and cannot learn, there is, to our knowledge, little work on systematically understanding how different span ID tasks compare: are there model architectures that work well generally? Can we identify properti"
2020.emnlp-main.396,2020.lrec-1.104,1,0.816842,"Missing"
2020.emnlp-main.396,L16-1619,0,0.10946,"pan ID performance. We find, e.g., that span frequency is especially important for LSTMs, and that CRFs help when spans are infrequent and boundaries non-distinctive. 1 Introduction Span identification is a family of analysis tasks that make up a substantial portion of applied NLP. Span identification (or short, span ID) tasks have in common that they identify and classify contiguous spans of tokens within a running text. Examples are named entity recognition (Nadeau and Sekine, 2007), chunking (Tjong Kim Sang and Buchholz, 2000), entity extraction (Etzioni et al., 2005), quotation detection (Pareti, 2016), keyphrase detection (Augenstein et al., 2017), or code switching (Pratapa et al., 2018). In terms of complexity, span ID tasks form a middle ground between simpler analysis tasks that predict labels for single linguistic units (such as lemmatization (Porter, 1980) or sentiment polarity classification (Liu, 2012)) and more complex analysis tasks such as relation extraction, which combines span ID with relation identification (Zelenko et al., 2002; Adel et al., 2018). Due to the rapid development of deep learning, an abundance of model architectures is available for the implementation of span"
2020.emnlp-main.396,D14-1162,0,0.0844584,"Missing"
2020.emnlp-main.396,P18-1143,0,0.014699,"r LSTMs, and that CRFs help when spans are infrequent and boundaries non-distinctive. 1 Introduction Span identification is a family of analysis tasks that make up a substantial portion of applied NLP. Span identification (or short, span ID) tasks have in common that they identify and classify contiguous spans of tokens within a running text. Examples are named entity recognition (Nadeau and Sekine, 2007), chunking (Tjong Kim Sang and Buchholz, 2000), entity extraction (Etzioni et al., 2005), quotation detection (Pareti, 2016), keyphrase detection (Augenstein et al., 2017), or code switching (Pratapa et al., 2018). In terms of complexity, span ID tasks form a middle ground between simpler analysis tasks that predict labels for single linguistic units (such as lemmatization (Porter, 1980) or sentiment polarity classification (Liu, 2012)) and more complex analysis tasks such as relation extraction, which combines span ID with relation identification (Zelenko et al., 2002; Adel et al., 2018). Due to the rapid development of deep learning, an abundance of model architectures is available for the implementation of span ID tasks. These include isolated token classification models (Berger et al., 1996; Chieu"
2020.emnlp-main.396,P16-1164,1,0.892617,"Missing"
2020.emnlp-main.396,E17-1119,0,0.0210789,"TM+CRF. This architecture combines all components previously mentioned. It first uses a pre-trained BERT encoder to generate a sequence of contextualized embeddings. These embeddings are projected to 300 dimensions using a linear layer, yielding a sequence of vectors, which are then used as input for a LSTM+CRF network. As with BERT+CRF, we first train the non-BERT parameters to convergence while holding BERT’s parameters fixed, and subsequently fine-tune all parameters jointly. Handcrafted Features. Some studies have shown marked increases in performance by adding hand-crafted features (e.g. Shimaoka et al., 2017). We develop such features for our tasks and treat these to be an additional architecture component. For architectures with this component, a bag of features is extracted for each token (the exact features used for each dataset are enumerated in Table 5 in the Appendix). For each feature, we learn a 300-dimensional feature embedding which is averaged with the GloVe or BERT embedding to obtain a token embedding. Handcrafted features can be used with the Baseline, LSTM, LSTM+CRF, and BERT+LSTM+CRF architectures. BERT and BERT+CRF cannot utilize manual features, as they have no way of accepting t"
2020.emnlp-main.396,W00-0726,0,0.659983,"Missing"
2020.emnlp-main.396,2020.emnlp-demos.6,0,0.09424,"Missing"
2020.emnlp-main.396,C18-1182,0,0.0281447,"ocus on learning with very few training examples, while we focus on optimizing performance with normally sized corpora. Additionally, these models selectively train preselected model architectures, while we are concerned with comparisons between architectures. Model and Corpus Comparisons in Survey Papers. In a broad sense, our goal of comparison between existing corpora and modeling approaches is shared with many existing survey papers. Surveys include quantitative comparisons of existing systems’ performances on common tasks, producing a results matrix very similar to ours (Li et al., 2020; Yadav and Bethard, 2018; Bostan and Klinger, 2018, i.a.). However, most of these surveys limit themselves to collecting results across models and datasets without performing a detailed quantitative analysis of these results to identify recurring patterns, as we do with our performance prediction approach. 8 Conclusion In this work, we considered the class of span identification tasks. This class contains a number of widely used NLP tasks, but no comprehensive analysis beyond the level of individual tasks is available. We took a meta-learning perspective, predicting the performance of various architectures on various"
2020.fnp-1.31,N19-1423,0,0.0309707,"Missing"
2020.fnp-1.31,W19-6406,0,0.0430789,"es, which in turn serves as a disciplining tool (Li, 2010a; Kothari et al., 2009). As Kothari et al. (2009, p. 1643) notes, “the disciplining forces might be less operative when disclosures are qualitative and long-term in nature (e.g., discussion in MD&A section) rather than quantitative [...] and short-term”. Several studies have analysed the impact of corporate disclosures with respect to the financial information using the narrative information content: There is evidence for the disclosures affecting company’s risk (Kravet and Muslu, 2013; Kothari et al., 2009; Li, 2006), future earnings (Moreno-Sandoval et al., 2019; Athanasakou and Hussainey, 2014; Li, 2010a), and, ultimately, firm value (Campbell et al., 2014; Jegadeesh and Wu, 2013; Feldman et al., 2008).4 However, nearly all previous work relies purely on a word-count, word-phrase count or comparably, straightforward approaches. Thus, we hypothesise that the extraction of the information content will improve by using more advanced embedding methods or applying convolutional neural networks. While most studies tend to derive some kind of disclosure-score from the text and subsequently regress the score on financial performance or some other measure, w"
2020.fnp-1.31,D14-1162,0,0.0869402,"Missing"
2020.fnp-1.31,I17-1026,0,0.0863462,"Missing"
2020.lrec-1.102,Q14-1029,0,0.0263479,"n the second study they explore the New York Times Obituaries and find that the network of the second study differs from the first study in terms of network density, mean clustering coefficient and modularity. The last study is done on data from ObituaryData.com and the annotation with traits is performed in a semi-automatic manner. Han (2013) extract various facts about persons from obituaries. They use a feature scoring method that uses prior knowledge. Their method achieved high performance for the attributes person name, affiliation, position (occupation), age, gender, and cause of death. Bamman and Smith (2014) present an unsupervised model for learning life event classes from biographical texts in Wikipedia along with the structure that connects them. They discover evidence of systematic bias in the presentation of male and female biographies in which female biographies placed a significantly disproportionate emphasis on the perSource Location Daily Item Remembering CA The London Free Press US Canada UK All All # obituaries 9975 9984 99 20058 Table 2: Overview of the sources of obituary data. sonal events of marriage and divorce. This work is of interest here because it handled biographical informa"
2020.lrec-1.102,P09-1068,0,0.0366323,"They discover evidence of systematic bias in the presentation of male and female biographies in which female biographies placed a significantly disproportionate emphasis on the perSource Location Daily Item Remembering CA The London Free Press US Canada UK All All # obituaries 9975 9984 99 20058 Table 2: Overview of the sources of obituary data. sonal events of marriage and divorce. This work is of interest here because it handled biographical information (Wikipedia biographies), of which obituaries are also a part. Simonson and Davis (2016) investigate the distribution of narrative schemas (Chambers and Jurafsky, 2009) throughout different categories of documents and show that the structure of the narrative schemas are conditioned by the type of document. Their work uses the New York Times corpus, which makes the work relevant for us, because obituary data is part of the NYT library and a category of document the work focuses on. Their results show that obituaries are narratologically homogeneous and therefore more rigid in their wording and the events they describe. The stability of narrative schemas is explored in a follow up paper by Simonson and Davis (2018). Their goal was to test whether small changes"
2020.lrec-1.102,C12-1041,0,0.0215894,"ssify to 57 kinships each with 10 or more examples in 10-fold cross-validation experiment. 2.3. Zoning Many NLP tasks focus on the extraction and abstraction of specific types of information in documents. To make searching and retrieving information in documents accessible, the logical structure of documents in titles, headings, sections, arguments, and thematically related parts must be recognized (Paaß and Konya, 2011). A notable amount of work focuses on the argumentative zoning of scientific documents (Teufel et al., 1999; Teufel and Moens, 2002; Teufel et al., 2009; Liakata et al., 2010; Contractor et al., 2012; Ravenscroft et al., 2016; Neves et al., 2019). Guo et al. (2011) stated that readers of scientific work may be looking for “information about the objective of the study in question, the methods used in the study, the results obtained, or the conclusions drawn by authors”. The recognition of document structures generally makes use of two sources of information. On one side, text layout enables recognition of relationships between the various structural units such as headings, body text, references, fig819 ures, etc. On the other side, the wording and content itself can be used to recognize th"
2020.lrec-1.102,D11-1025,0,0.0160074,"dation experiment. 2.3. Zoning Many NLP tasks focus on the extraction and abstraction of specific types of information in documents. To make searching and retrieving information in documents accessible, the logical structure of documents in titles, headings, sections, arguments, and thematically related parts must be recognized (Paaß and Konya, 2011). A notable amount of work focuses on the argumentative zoning of scientific documents (Teufel et al., 1999; Teufel and Moens, 2002; Teufel et al., 2009; Liakata et al., 2010; Contractor et al., 2012; Ravenscroft et al., 2016; Neves et al., 2019). Guo et al. (2011) stated that readers of scientific work may be looking for “information about the objective of the study in question, the methods used in the study, the results obtained, or the conclusions drawn by authors”. The recognition of document structures generally makes use of two sources of information. On one side, text layout enables recognition of relationships between the various structural units such as headings, body text, references, fig819 ures, etc. On the other side, the wording and content itself can be used to recognize the connections and semantics of text passages. Most methods use sec"
2020.lrec-1.102,W19-3201,0,0.0464313,"erefore more rigid in their wording and the events they describe. The stability of narrative schemas is explored in a follow up paper by Simonson and Davis (2018). Their goal was to test whether small changes in the corpus would produce small changes in the induced schemas. The results confirm the distinction between the homogeneous and heterogeneous articles and show that homogeneous categories produced more stable batches of schemas than the heterogeneous ones. This is not surprising but supports that obituaries have a coherent structure which could be turned into a stable narrative schema. He et al. (2019) propose using online obituaries as a new data source for doing named entity recognition and relation extraction to capture kinship and family relation information. Their corpus consists of 1809 obituaries annotated with a novel tagging scheme. Using a joint neural model they classify to 57 kinships each with 10 or more examples in 10-fold cross-validation experiment. 2.3. Zoning Many NLP tasks focus on the extraction and abstraction of specific types of information in documents. To make searching and retrieving information in documents accessible, the logical structure of documents in titles,"
2020.lrec-1.102,D14-1181,0,0.00288179,"The Daily Item (USA), 445 obituaries are from Remembering.CA (Canada), and 88 obituaries are from The London Free Press (UK). Most sentences in the dataset are labeled as Biographical sketch (3041), followed by Funeral information (2831) and Family (2195). The least Methods To answer the question whether or not we can recognize the structure in obituaries we formulate the task as sentence classification, where each sentence will be assigned to one of the eight classes we defined previously. We evaluate four different models. CNN Convolutional Neural Networks (CNN) (Collobert and Weston, 2008; Kim, 2014) have been succesfully applied to practical NLP problems in the recent years. We use the sequential model in Keras5 where each sentence is represented as a sequence of one-hot embeddings of its words. We use three consecutive pairs of convolutional layers with 128 output channels, the ReLu activation function and max pooling followed by the output layer with softmax as activation function and with cross entropy as loss. This model does not have access to information of neighboring sentences. BiLSTM The BiLSTM models are structurally different from the CNN. The CNN predicts on the sentence-leve"
2020.lrec-1.102,liakata-etal-2010-corpora,0,0.0191569,"neural model they classify to 57 kinships each with 10 or more examples in 10-fold cross-validation experiment. 2.3. Zoning Many NLP tasks focus on the extraction and abstraction of specific types of information in documents. To make searching and retrieving information in documents accessible, the logical structure of documents in titles, headings, sections, arguments, and thematically related parts must be recognized (Paaß and Konya, 2011). A notable amount of work focuses on the argumentative zoning of scientific documents (Teufel et al., 1999; Teufel and Moens, 2002; Teufel et al., 2009; Liakata et al., 2010; Contractor et al., 2012; Ravenscroft et al., 2016; Neves et al., 2019). Guo et al. (2011) stated that readers of scientific work may be looking for “information about the objective of the study in question, the methods used in the study, the results obtained, or the conclusions drawn by authors”. The recognition of document structures generally makes use of two sources of information. On one side, text layout enables recognition of relationships between the various structural units such as headings, body text, references, fig819 ures, etc. On the other side, the wording and content itself ca"
2020.lrec-1.102,N13-1090,0,0.00811965,"Missing"
2020.lrec-1.102,W19-4515,0,0.0148108,"in 10-fold cross-validation experiment. 2.3. Zoning Many NLP tasks focus on the extraction and abstraction of specific types of information in documents. To make searching and retrieving information in documents accessible, the logical structure of documents in titles, headings, sections, arguments, and thematically related parts must be recognized (Paaß and Konya, 2011). A notable amount of work focuses on the argumentative zoning of scientific documents (Teufel et al., 1999; Teufel and Moens, 2002; Teufel et al., 2009; Liakata et al., 2010; Contractor et al., 2012; Ravenscroft et al., 2016; Neves et al., 2019). Guo et al. (2011) stated that readers of scientific work may be looking for “information about the objective of the study in question, the methods used in the study, the results obtained, or the conclusions drawn by authors”. The recognition of document structures generally makes use of two sources of information. On one side, text layout enables recognition of relationships between the various structural units such as headings, body text, references, fig819 ures, etc. On the other side, the wording and content itself can be used to recognize the connections and semantics of text passages. M"
2020.lrec-1.102,L16-1650,0,0.0158497,"with 10 or more examples in 10-fold cross-validation experiment. 2.3. Zoning Many NLP tasks focus on the extraction and abstraction of specific types of information in documents. To make searching and retrieving information in documents accessible, the logical structure of documents in titles, headings, sections, arguments, and thematically related parts must be recognized (Paaß and Konya, 2011). A notable amount of work focuses on the argumentative zoning of scientific documents (Teufel et al., 1999; Teufel and Moens, 2002; Teufel et al., 2009; Liakata et al., 2010; Contractor et al., 2012; Ravenscroft et al., 2016; Neves et al., 2019). Guo et al. (2011) stated that readers of scientific work may be looking for “information about the objective of the study in question, the methods used in the study, the results obtained, or the conclusions drawn by authors”. The recognition of document structures generally makes use of two sources of information. On one side, text layout enables recognition of relationships between the various structural units such as headings, body text, references, fig819 ures, etc. On the other side, the wording and content itself can be used to recognize the connections and semantic"
2020.lrec-1.102,W16-5707,0,0.0187477,"biographical texts in Wikipedia along with the structure that connects them. They discover evidence of systematic bias in the presentation of male and female biographies in which female biographies placed a significantly disproportionate emphasis on the perSource Location Daily Item Remembering CA The London Free Press US Canada UK All All # obituaries 9975 9984 99 20058 Table 2: Overview of the sources of obituary data. sonal events of marriage and divorce. This work is of interest here because it handled biographical information (Wikipedia biographies), of which obituaries are also a part. Simonson and Davis (2016) investigate the distribution of narrative schemas (Chambers and Jurafsky, 2009) throughout different categories of documents and show that the structure of the narrative schemas are conditioned by the type of document. Their work uses the New York Times corpus, which makes the work relevant for us, because obituary data is part of the NYT library and a category of document the work focuses on. Their results show that obituaries are narratologically homogeneous and therefore more rigid in their wording and the events they describe. The stability of narrative schemas is explored in a follow up"
2020.lrec-1.102,C18-1311,0,0.0136507,"ate the distribution of narrative schemas (Chambers and Jurafsky, 2009) throughout different categories of documents and show that the structure of the narrative schemas are conditioned by the type of document. Their work uses the New York Times corpus, which makes the work relevant for us, because obituary data is part of the NYT library and a category of document the work focuses on. Their results show that obituaries are narratologically homogeneous and therefore more rigid in their wording and the events they describe. The stability of narrative schemas is explored in a follow up paper by Simonson and Davis (2018). Their goal was to test whether small changes in the corpus would produce small changes in the induced schemas. The results confirm the distinction between the homogeneous and heterogeneous articles and show that homogeneous categories produced more stable batches of schemas than the heterogeneous ones. This is not surprising but supports that obituaries have a coherent structure which could be turned into a stable narrative schema. He et al. (2019) propose using online obituaries as a new data source for doing named entity recognition and relation extraction to capture kinship and family rel"
2020.lrec-1.102,J02-4002,0,0.0482178,"ed with a novel tagging scheme. Using a joint neural model they classify to 57 kinships each with 10 or more examples in 10-fold cross-validation experiment. 2.3. Zoning Many NLP tasks focus on the extraction and abstraction of specific types of information in documents. To make searching and retrieving information in documents accessible, the logical structure of documents in titles, headings, sections, arguments, and thematically related parts must be recognized (Paaß and Konya, 2011). A notable amount of work focuses on the argumentative zoning of scientific documents (Teufel et al., 1999; Teufel and Moens, 2002; Teufel et al., 2009; Liakata et al., 2010; Contractor et al., 2012; Ravenscroft et al., 2016; Neves et al., 2019). Guo et al. (2011) stated that readers of scientific work may be looking for “information about the objective of the study in question, the methods used in the study, the results obtained, or the conclusions drawn by authors”. The recognition of document structures generally makes use of two sources of information. On one side, text layout enables recognition of relationships between the various structural units such as headings, body text, references, fig819 ures, etc. On the ot"
2020.lrec-1.102,E99-1015,0,0.350382,"09 obituaries annotated with a novel tagging scheme. Using a joint neural model they classify to 57 kinships each with 10 or more examples in 10-fold cross-validation experiment. 2.3. Zoning Many NLP tasks focus on the extraction and abstraction of specific types of information in documents. To make searching and retrieving information in documents accessible, the logical structure of documents in titles, headings, sections, arguments, and thematically related parts must be recognized (Paaß and Konya, 2011). A notable amount of work focuses on the argumentative zoning of scientific documents (Teufel et al., 1999; Teufel and Moens, 2002; Teufel et al., 2009; Liakata et al., 2010; Contractor et al., 2012; Ravenscroft et al., 2016; Neves et al., 2019). Guo et al. (2011) stated that readers of scientific work may be looking for “information about the objective of the study in question, the methods used in the study, the results obtained, or the conclusions drawn by authors”. The recognition of document structures generally makes use of two sources of information. On one side, text layout enables recognition of relationships between the various structural units such as headings, body text, references, fig"
2020.lrec-1.102,D09-1155,0,0.0228709,"scheme. Using a joint neural model they classify to 57 kinships each with 10 or more examples in 10-fold cross-validation experiment. 2.3. Zoning Many NLP tasks focus on the extraction and abstraction of specific types of information in documents. To make searching and retrieving information in documents accessible, the logical structure of documents in titles, headings, sections, arguments, and thematically related parts must be recognized (Paaß and Konya, 2011). A notable amount of work focuses on the argumentative zoning of scientific documents (Teufel et al., 1999; Teufel and Moens, 2002; Teufel et al., 2009; Liakata et al., 2010; Contractor et al., 2012; Ravenscroft et al., 2016; Neves et al., 2019). Guo et al. (2011) stated that readers of scientific work may be looking for “information about the objective of the study in question, the methods used in the study, the results obtained, or the conclusions drawn by authors”. The recognition of document structures generally makes use of two sources of information. On one side, text layout enables recognition of relationships between the various structural units such as headings, body text, references, fig819 ures, etc. On the other side, the wording"
2020.lrec-1.194,H05-1073,0,0.660142,"Missing"
2020.lrec-1.194,J08-4004,0,0.0477438,"Missing"
2020.lrec-1.194,P98-1013,0,0.837791,"le addressing the issues raised above. Specifically, we introduce the corpus GoodNewsEveryone, a novel dataset of English news headlines collected from 82 different sources most of which are analyzed in the Media Bias Chart (Otero, 2018) annotated for emotion class, emotion intensity, semantic roles (experiencer, cause, target, cue), and reader perspective. We use semantic roles, since identifying who feels what and why is essentially a semantic role labeling task (Gildea and Jurafsky, 2000). The roles we consider are a subset of those defined for the semantic frame for “Emotion” in FrameNet (Baker et al., 1998). We focus on news headlines due to their brevity and density of contained information. Headlines often appeal to a reader’s emotions and hence are a potentially good source for emotion analysis. Besides, news headlines are easy-toobtain data across many languages, void of data privacy issues associated with social media and microblogging. Further, we opt for a crowdsourcing setting in contrast to an expert-based setting to obtain data annotated that is to a lesser extend influenced by individual opinions of a low number of annotators. Besides, our previous work showed that it is comparably ha"
2020.lrec-1.194,C18-1179,1,0.881877,"he idea of requesting self-reports (by experts in a lab setting) with the idea of using crowdsourcing. They extend their data to German reports (next to English) and validate each instance, again, via crowdsourcing. Lastly, social network platforms play a central role in data acquisition with distant supervision, because they provide a cheap way to obtain large amounts of noisy data (Mohammad, 2012; Mohammad et al., 2014; Mohammad and Kiritchenko, 2015; Liu et al., 2017). We show an overview of available resources in Table 1. Further, more details on previous work can for instance be found in Bostan and Klinger (2018). 1555 Emotion Annotation Int. Cue Exp. Cause Target Emotion & Intensity Classification Roles Dataset ISEAR Tales AffectiveText TEC fb-valence-arousal EmoBank DailyDialogs Grounded-Emotions SSEC EmoInt Multigenre The Affect in Tweets EmoContext MELD enISEAR Ekman + {shame, guilt} Ekman Ekman + {valence} Ekman + {±surprise} VA VAD Ekman Joy & Sadness Plutchik Ekman − {disgust, surprise} Plutchik Others Joy, Sadness, Anger & Others Ekman + Neutral Ekman + {shame, guilt} 7 7 7 7 7 7 7 7 7 3 7 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7"
2020.lrec-1.194,E17-2092,0,0.0646665,"questions that cannot yet be answered by these emotion analysis systems. Firstly, such systems do not often explicitly model the perspective of understanding the written discourse (reader, writer, or the text’s point of view). For example, the headline “Djokovic happy to carry on cruising” (Herman, 2019) contains an explicit mention of joy carried by the word “happy”. However, it may evoke different emotions in a reader (e. g., when the reader is a supporter of Roger Federer), and the same applies to the author of the headline. To the best of our knowledge, only one work considers this point (Buechel and Hahn, 2017c). Secondly, the structure that can be associated with the emotion description in text is not uncovered. Questions like “Who feels a particular emotion?” or “What causes that emotion?” remain unaddressed. There has been almost no work in this direction, with only a few exceptions in English (Kim and Klinger, 2018; Mohammad et al., 2014) and Mandarin (Xu et al., 2019; Ding et al., 2019). With this work, we argue that emotion analysis would benefit from a more fine-grained analysis that considers the full structure of an emotion, similar to the research in aspectbased sentiment analysis (Wang e"
2020.lrec-1.194,W17-0801,0,0.538827,"questions that cannot yet be answered by these emotion analysis systems. Firstly, such systems do not often explicitly model the perspective of understanding the written discourse (reader, writer, or the text’s point of view). For example, the headline “Djokovic happy to carry on cruising” (Herman, 2019) contains an explicit mention of joy carried by the word “happy”. However, it may evoke different emotions in a reader (e. g., when the reader is a supporter of Roger Federer), and the same applies to the author of the headline. To the best of our knowledge, only one work considers this point (Buechel and Hahn, 2017c). Secondly, the structure that can be associated with the emotion description in text is not uncovered. Questions like “Who feels a particular emotion?” or “What causes that emotion?” remain unaddressed. There has been almost no work in this direction, with only a few exceptions in English (Kim and Klinger, 2018; Mohammad et al., 2014) and Mandarin (Xu et al., 2019; Ding et al., 2019). With this work, we argue that emotion analysis would benefit from a more fine-grained analysis that considers the full structure of an emotion, similar to the research in aspectbased sentiment analysis (Wang e"
2020.lrec-1.194,S19-2005,0,0.0615173,"Missing"
2020.lrec-1.194,C10-1021,0,0.216125,"r, 2018; Gao et al., 2015). The task can be formulated in different ways. One is to define a closed set of potential causes after annotation. Then, cause detection is a classification task (Mohammad et al., 2014). Another setting is to find the cause in the text without sticking to clause boundaries. This is formulated as segmentation or clause classification on the token level (Ghazi et al., 2015; Kim and Klinger, 2018). Finding the cause of an emotion is widely researched on Mandarin in both resource creation and methods. Early works build on rule-based systems (Lee, 2010; Lee et al., 2010; Chen et al., 2010), which examine correlations between emotions and cause events in terms of linguistic cues. The works that follow up focus on both methods and corpus construction, showing large improvements over the early works (Li and Xu, 2014; Gui et al., 2014; Gao et al., 2015; Gui et al., 2016; Gui et al., 2017; Xu et al., 2017; Cheng et al., 2017; Chen et al., 2018; Ding et al., 2019). The most recent work on cause extraction is being done on Mandarin and formulates the task jointly with emotion detection (Xu et al., 2019; Xia and Ding, 2019; Xia et al., 2019). With the exception of Mohammad et al. (2014"
2020.lrec-1.194,D18-1066,0,0.05034,"tion on the token level (Ghazi et al., 2015; Kim and Klinger, 2018). Finding the cause of an emotion is widely researched on Mandarin in both resource creation and methods. Early works build on rule-based systems (Lee, 2010; Lee et al., 2010; Chen et al., 2010), which examine correlations between emotions and cause events in terms of linguistic cues. The works that follow up focus on both methods and corpus construction, showing large improvements over the early works (Li and Xu, 2014; Gui et al., 2014; Gao et al., 2015; Gui et al., 2016; Gui et al., 2017; Xu et al., 2017; Cheng et al., 2017; Chen et al., 2018; Ding et al., 2019). The most recent work on cause extraction is being done on Mandarin and formulates the task jointly with emotion detection (Xu et al., 2019; Xia and Ding, 2019; Xia et al., 2019). With the exception of Mohammad et al. (2014) who are annotating via crowdsourcing, all other datasets are manually labeled by experts, usually using the W3C Emotion Markup Language1 . 2.5. Semantic Role Labeling of Emotions Semantic role labeling in the context of emotion analysis deals with extracting who feels (experiencer) which emotion (cue, class), towards whom the emotion is directed (targe"
2020.lrec-1.194,P00-1065,0,0.358407,"tional experiences would remain hidden from us. In this study, we focus on an annotation task to develop a dataset that would enable addressing the issues raised above. Specifically, we introduce the corpus GoodNewsEveryone, a novel dataset of English news headlines collected from 82 different sources most of which are analyzed in the Media Bias Chart (Otero, 2018) annotated for emotion class, emotion intensity, semantic roles (experiencer, cause, target, cue), and reader perspective. We use semantic roles, since identifying who feels what and why is essentially a semantic role labeling task (Gildea and Jurafsky, 2000). The roles we consider are a subset of those defined for the semantic frame for “Emotion” in FrameNet (Baker et al., 1998). We focus on news headlines due to their brevity and density of contained information. Headlines often appeal to a reader’s emotions and hence are a potentially good source for emotion analysis. Besides, news headlines are easy-toobtain data across many languages, void of data privacy issues associated with social media and microblogging. Further, we opt for a crowdsourcing setting in contrast to an expert-based setting to obtain data annotated that is to a lesser extend"
2020.lrec-1.194,D16-1170,0,0.349306,"Missing"
2020.lrec-1.194,D17-1167,0,0.20429,"Missing"
2020.lrec-1.194,2020.lrec-1.205,1,0.74677,"sis which investigate the relation between sentiment of a blog post and the sentiment of their comments (Tang and Chen, 2012) or model the emotion of a news reader jointly with the emotion of a comment writer (Liu et al., 2013). Yang et al. (2009) deal with writer’s and reader’s emotions on online blogs and find that positive reader emotions tend to be linked to positive writer emotions. Buechel and Hahn (2017c) and Buechel and Hahn (2017b) look into the effects of different perspectives on annotation quality and find that the reader perspective yields better inter-annotator agreement values. Haider et al. (2020) create an annotated corpus of poetry, in which they make the task explicit that they care about the emotion perceived by the reader, and not an emotion that is expressed by the author or a character. They further propose that for the perception of art, the commonly used set of fundamental emotions is not appropriate but should be extended to a set of aesthetic emotions. 3. Data Collection & Annotation We gather the data in three steps: (1) collecting the news and the reactions they elicit in social media, (2) filtering the resulting set to retain relevant items, and (3) sampling the final sel"
2020.lrec-1.194,W19-2501,0,0.0223076,"ion analysis from text focuses on mapping words, sentences, or documents to emotion categories based on the models of Ekman (1992) or Plutchik (2001), which propose the emotion classes of joy, sadness, anger, fear, trust, disgust, anticipation and surprise. Emotion analysis has been applied to a variety of tasks including large scale social media mining (Stieglitz and Dang-Xuan, 2013), literature analysis (Reagan et al., 2016; Kim and Klinger, 2019), lyrics and music analysis (Mihalcea and Strapparava, 2012; Dodds and Danforth, 2010), and the analysis of the development of emotions over time (Hellrich et al., 2019). There are at least two types of questions that cannot yet be answered by these emotion analysis systems. Firstly, such systems do not often explicitly model the perspective of understanding the written discourse (reader, writer, or the text’s point of view). For example, the headline “Djokovic happy to carry on cruising” (Herman, 2019) contains an explicit mention of joy carried by the word “happy”. However, it may evoke different emotions in a reader (e. g., when the reader is a supporter of Roger Federer), and the same applies to the author of the headline. To the best of our knowledge, on"
2020.lrec-1.194,C18-1114,1,0.666929,"n explicit mention of joy carried by the word “happy”. However, it may evoke different emotions in a reader (e. g., when the reader is a supporter of Roger Federer), and the same applies to the author of the headline. To the best of our knowledge, only one work considers this point (Buechel and Hahn, 2017c). Secondly, the structure that can be associated with the emotion description in text is not uncovered. Questions like “Who feels a particular emotion?” or “What causes that emotion?” remain unaddressed. There has been almost no work in this direction, with only a few exceptions in English (Kim and Klinger, 2018; Mohammad et al., 2014) and Mandarin (Xu et al., 2019; Ding et al., 2019). With this work, we argue that emotion analysis would benefit from a more fine-grained analysis that considers the full structure of an emotion, similar to the research in aspectbased sentiment analysis (Wang et al., 2016; Ma et al., 2018; Xue and Li, 2018; Sun et al., 2019). Consider the headline: “A couple infuriated officials by landing their helicopter in the middle of a nature reserve” (Kenton, 2019) depicted in Figure 1. One could mark “officials” as the experiencer, “a couple” as the target, and “landing their he"
2020.lrec-1.194,N19-1067,1,0.671096,"ity prediction, emotion cause detection, and supports further qualitative studies. Keywords: emotion, structured learning, role labeling 1. Introduction Research in emotion analysis from text focuses on mapping words, sentences, or documents to emotion categories based on the models of Ekman (1992) or Plutchik (2001), which propose the emotion classes of joy, sadness, anger, fear, trust, disgust, anticipation and surprise. Emotion analysis has been applied to a variety of tasks including large scale social media mining (Stieglitz and Dang-Xuan, 2013), literature analysis (Reagan et al., 2016; Kim and Klinger, 2019), lyrics and music analysis (Mihalcea and Strapparava, 2012; Dodds and Danforth, 2010), and the analysis of the development of emotions over time (Hellrich et al., 2019). There are at least two types of questions that cannot yet be answered by these emotion analysis systems. Firstly, such systems do not often explicitly model the perspective of understanding the written discourse (reader, writer, or the text’s point of view). For example, the headline “Djokovic happy to carry on cruising” (Herman, 2019) contains an explicit mention of joy carried by the word “happy”. However, it may evoke diff"
2020.lrec-1.194,W10-0206,0,0.364624,"14; Kim and Klinger, 2018; Gao et al., 2015). The task can be formulated in different ways. One is to define a closed set of potential causes after annotation. Then, cause detection is a classification task (Mohammad et al., 2014). Another setting is to find the cause in the text without sticking to clause boundaries. This is formulated as segmentation or clause classification on the token level (Ghazi et al., 2015; Kim and Klinger, 2018). Finding the cause of an emotion is widely researched on Mandarin in both resource creation and methods. Early works build on rule-based systems (Lee, 2010; Lee et al., 2010; Chen et al., 2010), which examine correlations between emotions and cause events in terms of linguistic cues. The works that follow up focus on both methods and corpus construction, showing large improvements over the early works (Li and Xu, 2014; Gui et al., 2014; Gao et al., 2015; Gui et al., 2016; Gui et al., 2017; Xu et al., 2017; Cheng et al., 2017; Chen et al., 2018; Ding et al., 2019). The most recent work on cause extraction is being done on Mandarin and formulates the task jointly with emotion detection (Xu et al., 2019; Xia and Ding, 2019; Xia et al., 2019). With the exception of M"
2020.lrec-1.194,I17-1099,0,0.108104,"Missing"
2020.lrec-1.194,L16-1183,0,0.0497087,"Missing"
2020.lrec-1.194,P13-2091,0,0.0267912,"rget), and what is the event that caused the emotion (stimulus). The relations are defined akin to FrameNet’s Emotion frame (Baker et al., 1998). 1 https://www.w3.org/TR/emotionml/, cessed Nov 27 2019 1556 last ac2.6. Reader vs. Writer vs. Text Perspective Studying the impact of different annotation perspectives is another little explored area. There are few exceptions in sentiment analysis which investigate the relation between sentiment of a blog post and the sentiment of their comments (Tang and Chen, 2012) or model the emotion of a news reader jointly with the emotion of a comment writer (Liu et al., 2013). Yang et al. (2009) deal with writer’s and reader’s emotions on online blogs and find that positive reader emotions tend to be linked to positive writer emotions. Buechel and Hahn (2017c) and Buechel and Hahn (2017b) look into the effects of different perspectives on annotation quality and find that the reader perspective yields better inter-annotator agreement values. Haider et al. (2020) create an annotated corpus of poetry, in which they make the task explicit that they care about the emotion perceived by the reader, and not an emotion that is expressed by the author or a character. They f"
2020.lrec-1.194,D12-1054,0,0.163181,"ts further qualitative studies. Keywords: emotion, structured learning, role labeling 1. Introduction Research in emotion analysis from text focuses on mapping words, sentences, or documents to emotion categories based on the models of Ekman (1992) or Plutchik (2001), which propose the emotion classes of joy, sadness, anger, fear, trust, disgust, anticipation and surprise. Emotion analysis has been applied to a variety of tasks including large scale social media mining (Stieglitz and Dang-Xuan, 2013), literature analysis (Reagan et al., 2016; Kim and Klinger, 2019), lyrics and music analysis (Mihalcea and Strapparava, 2012; Dodds and Danforth, 2010), and the analysis of the development of emotions over time (Hellrich et al., 2019). There are at least two types of questions that cannot yet be answered by these emotion analysis systems. Firstly, such systems do not often explicitly model the perspective of understanding the written discourse (reader, writer, or the text’s point of view). For example, the headline “Djokovic happy to carry on cruising” (Herman, 2019) contains an explicit mention of joy carried by the word “happy”. However, it may evoke different emotions in a reader (e. g., when the reader is a sup"
2020.lrec-1.194,W17-5205,0,0.298551,"Missing"
2020.lrec-1.194,L18-1030,0,0.0730161,"Missing"
2020.lrec-1.194,W14-2607,0,0.16653,"oy carried by the word “happy”. However, it may evoke different emotions in a reader (e. g., when the reader is a supporter of Roger Federer), and the same applies to the author of the headline. To the best of our knowledge, only one work considers this point (Buechel and Hahn, 2017c). Secondly, the structure that can be associated with the emotion description in text is not uncovered. Questions like “Who feels a particular emotion?” or “What causes that emotion?” remain unaddressed. There has been almost no work in this direction, with only a few exceptions in English (Kim and Klinger, 2018; Mohammad et al., 2014) and Mandarin (Xu et al., 2019; Ding et al., 2019). With this work, we argue that emotion analysis would benefit from a more fine-grained analysis that considers the full structure of an emotion, similar to the research in aspectbased sentiment analysis (Wang et al., 2016; Ma et al., 2018; Xue and Li, 2018; Sun et al., 2019). Consider the headline: “A couple infuriated officials by landing their helicopter in the middle of a nature reserve” (Kenton, 2019) depicted in Figure 1. One could mark “officials” as the experiencer, “a couple” as the target, and “landing their helicopter in the middle o"
2020.lrec-1.194,S18-1001,0,0.286634,"ferent ways. One way to create annotated datasets is via expert annotation (Aman and Szpakowicz, 2007; Strapparava and Mihalcea, 2007; Ghazi et al., 2015; Schuff et al., 2017; Buechel and Hahn, 2017c). A special case of this procedure has been proposed by the creators of the ISEAR dataset who make use of self-reporting instead, where subjects are asked to describe situations associated with a specific emotion (Scherer and Wallbott, 1994). Crowdsourcing is another popular way to acquire human judgments (Mohammad, 2012; Mohammad et al., 2014; Mohammad et al., 2014; Abdul-Mageed and Ungar, 2017; Mohammad et al., 2018), for instance on Amazon Mechanical Turk or Figure Eight (previously known as Crowdflower). Troiano et al. (2019) recently published a data set which combines the idea of requesting self-reports (by experts in a lab setting) with the idea of using crowdsourcing. They extend their data to German reports (next to English) and validate each instance, again, via crowdsourcing. Lastly, social network platforms play a central role in data acquisition with distant supervision, because they provide a cheap way to obtain large amounts of noisy data (Mohammad, 2012; Mohammad et al., 2014; Mohammad and K"
2020.lrec-1.194,S12-1033,0,0.0721328,"lence, arousal and dominance (Russell, 1980). Datasets for those tasks can be created in different ways. One way to create annotated datasets is via expert annotation (Aman and Szpakowicz, 2007; Strapparava and Mihalcea, 2007; Ghazi et al., 2015; Schuff et al., 2017; Buechel and Hahn, 2017c). A special case of this procedure has been proposed by the creators of the ISEAR dataset who make use of self-reporting instead, where subjects are asked to describe situations associated with a specific emotion (Scherer and Wallbott, 1994). Crowdsourcing is another popular way to acquire human judgments (Mohammad, 2012; Mohammad et al., 2014; Mohammad et al., 2014; Abdul-Mageed and Ungar, 2017; Mohammad et al., 2018), for instance on Amazon Mechanical Turk or Figure Eight (previously known as Crowdflower). Troiano et al. (2019) recently published a data set which combines the idea of requesting self-reports (by experts in a lab setting) with the idea of using crowdsourcing. They extend their data to German reports (next to English) and validate each instance, again, via crowdsourcing. Lastly, social network platforms play a central role in data acquisition with distant supervision, because they provide a ch"
2020.lrec-1.194,passonneau-2004-computing,0,0.0605872,"Missing"
2020.lrec-1.194,N18-1202,0,0.0830407,"Missing"
2020.lrec-1.194,P19-1050,0,0.0623839,"Missing"
2020.lrec-1.194,W16-0404,0,0.144687,"Missing"
2020.lrec-1.194,W17-5203,1,0.91455,"Missing"
2020.lrec-1.194,S07-1013,0,0.618566,"for these tasks. We therefore look into each of these subtasks and explain how they are related to our new corpus. 2.1. Emotion Classification Emotion classification deals with mapping words, sentences, or documents to a set of emotions following psychological models such as those proposed by Ekman (1992) (anger, disgust, fear, joy, sadness, and surprise) or Plutchik (2001); or continuous values of valence, arousal and dominance (Russell, 1980). Datasets for those tasks can be created in different ways. One way to create annotated datasets is via expert annotation (Aman and Szpakowicz, 2007; Strapparava and Mihalcea, 2007; Ghazi et al., 2015; Schuff et al., 2017; Buechel and Hahn, 2017c). A special case of this procedure has been proposed by the creators of the ISEAR dataset who make use of self-reporting instead, where subjects are asked to describe situations associated with a specific emotion (Scherer and Wallbott, 1994). Crowdsourcing is another popular way to acquire human judgments (Mohammad, 2012; Mohammad et al., 2014; Mohammad et al., 2014; Abdul-Mageed and Ungar, 2017; Mohammad et al., 2018), for instance on Amazon Mechanical Turk or Figure Eight (previously known as Crowdflower). Troiano et al. (201"
2020.lrec-1.194,N19-1035,0,0.0206404,"ed with the emotion description in text is not uncovered. Questions like “Who feels a particular emotion?” or “What causes that emotion?” remain unaddressed. There has been almost no work in this direction, with only a few exceptions in English (Kim and Klinger, 2018; Mohammad et al., 2014) and Mandarin (Xu et al., 2019; Ding et al., 2019). With this work, we argue that emotion analysis would benefit from a more fine-grained analysis that considers the full structure of an emotion, similar to the research in aspectbased sentiment analysis (Wang et al., 2016; Ma et al., 2018; Xue and Li, 2018; Sun et al., 2019). Consider the headline: “A couple infuriated officials by landing their helicopter in the middle of a nature reserve” (Kenton, 2019) depicted in Figure 1. One could mark “officials” as the experiencer, “a couple” as the target, and “landing their helicopter in the middle of a nature reserve” as the cause of anger. Now let us imagine that the headline starts with “A cheerful couple” instead of “A couple”. A simple approach to emotion detection based on cue words will capture that this sentence contains descriptions of anger (“infuriated”) and joy (“cheerful”). It would, however, fail in attrib"
2020.lrec-1.194,L18-1199,0,0.126871,"Missing"
2020.lrec-1.194,tang-chen-2012-mining,0,0.0338061,"h extracting who feels (experiencer) which emotion (cue, class), towards whom the emotion is directed (target), and what is the event that caused the emotion (stimulus). The relations are defined akin to FrameNet’s Emotion frame (Baker et al., 1998). 1 https://www.w3.org/TR/emotionml/, cessed Nov 27 2019 1556 last ac2.6. Reader vs. Writer vs. Text Perspective Studying the impact of different annotation perspectives is another little explored area. There are few exceptions in sentiment analysis which investigate the relation between sentiment of a blog post and the sentiment of their comments (Tang and Chen, 2012) or model the emotion of a news reader jointly with the emotion of a comment writer (Liu et al., 2013). Yang et al. (2009) deal with writer’s and reader’s emotions on online blogs and find that positive reader emotions tend to be linked to positive writer emotions. Buechel and Hahn (2017c) and Buechel and Hahn (2017b) look into the effects of different perspectives on annotation quality and find that the reader perspective yields better inter-annotator agreement values. Haider et al. (2020) create an annotated corpus of poetry, in which they make the task explicit that they care about the emot"
2020.lrec-1.194,P19-1391,1,0.785712,"Missing"
2020.lrec-1.194,D16-1059,0,0.0408346,"Missing"
2020.lrec-1.194,P19-1096,0,0.129517,"works build on rule-based systems (Lee, 2010; Lee et al., 2010; Chen et al., 2010), which examine correlations between emotions and cause events in terms of linguistic cues. The works that follow up focus on both methods and corpus construction, showing large improvements over the early works (Li and Xu, 2014; Gui et al., 2014; Gao et al., 2015; Gui et al., 2016; Gui et al., 2017; Xu et al., 2017; Cheng et al., 2017; Chen et al., 2018; Ding et al., 2019). The most recent work on cause extraction is being done on Mandarin and formulates the task jointly with emotion detection (Xu et al., 2019; Xia and Ding, 2019; Xia et al., 2019). With the exception of Mohammad et al. (2014) who are annotating via crowdsourcing, all other datasets are manually labeled by experts, usually using the W3C Emotion Markup Language1 . 2.5. Semantic Role Labeling of Emotions Semantic role labeling in the context of emotion analysis deals with extracting who feels (experiencer) which emotion (cue, class), towards whom the emotion is directed (target), and what is the event that caused the emotion (stimulus). The relations are defined akin to FrameNet’s Emotion frame (Baker et al., 1998). 1 https://www.w3.org/TR/emotionml/, c"
2020.lrec-1.194,P18-1234,0,0.0138691,"at can be associated with the emotion description in text is not uncovered. Questions like “Who feels a particular emotion?” or “What causes that emotion?” remain unaddressed. There has been almost no work in this direction, with only a few exceptions in English (Kim and Klinger, 2018; Mohammad et al., 2014) and Mandarin (Xu et al., 2019; Ding et al., 2019). With this work, we argue that emotion analysis would benefit from a more fine-grained analysis that considers the full structure of an emotion, similar to the research in aspectbased sentiment analysis (Wang et al., 2016; Ma et al., 2018; Xue and Li, 2018; Sun et al., 2019). Consider the headline: “A couple infuriated officials by landing their helicopter in the middle of a nature reserve” (Kenton, 2019) depicted in Figure 1. One could mark “officials” as the experiencer, “a couple” as the target, and “landing their helicopter in the middle of a nature reserve” as the cause of anger. Now let us imagine that the headline starts with “A cheerful couple” instead of “A couple”. A simple approach to emotion detection based on cue words will capture that this sentence contains descriptions of anger (“infuriated”) and joy (“cheerful”). It would, howe"
2020.lrec-1.205,P17-1067,0,0.0268003,"relies on lexical resources of emotionally charged words (Strapparava and Valitutti, 2004; Esuli and Sebastiani, 2006; Mohammad and Turney, 2013) and offers a straightforward and transparent way to detect emotions in text. In contrast to rule-based approaches, current models for emotion classification are often based on neural networks and commonly use word embeddings as features. Schuff et al. (2017) applied models from the classes of CNN, BiLSTM, and LSTM and compare them to linear classifiers (SVM and MaxEnt), where the BiLSTM shows best results with the most balanced precision and recall. Abdul-Mageed and Ungar (2017) claim the highest F1 with gated recurrent unit networks (Chung et al., 2015) for Plutchik’s emotion model. More recently, shared tasks on emotion analysis (Mohammad et al., 2018; Klinger et al., 2018) triggered a set of more advanced deep learning approaches, including BERT (Devlin et al., 2019) and other transfer learning methods (Dankers et al., 2019). 3. Data Collection For our annotation and modeling studies, we build on top of two poetry corpora (in English and German), which we refer to as PO-EMO. This collection represents important contributions to the literary canon over the last 400"
2020.lrec-1.205,C16-1074,0,0.317111,"d cost overhead associated with in-house annotation process (that usually involve training and close supervision of the annotators). We provide the final datasets of German and English language poems annotated with reader emotions on verse level at https://github.com/tnhaider/ poetry-emotion. 2. 2.1. Related Work Poetry in Natural Language Processing Natural language understanding research on poetry has investigated stylistic variation (Kaplan and Blei, 2007; Kao and Jurafsky, 2015; Voigt and Jurafsky, 2013), with a focus on broadly accepted formal features such as meter (Greene et al., 2010; Agirrezabal et al., 2016; Estes and Hench, 2016) and rhyme (Reddy and Knight, 2011; Haider and Kuhn, 2018), as well as enjambement (Ruiz et al., 2017; Baumann et al., 2018) and metaphor (Kesarwani et al., 2017; Reinig and Rehbein, 2019). Recent work has also explored the relationship of poetry and prose, mainly on a syntactic level feelings of Beauty and Sublime have therefore come to be subsumed under the rubrique of aesthetic emotions in recent psychological research (Menninghaus et al., 2019). For this reason, we refer to the whole set of category labels as emotions throughout this paper. 2 to create emotion dicti"
2020.lrec-1.205,H05-1073,0,0.851899,"tion labels consists of Beauty/Joy, Sadness, Uneasiness, Vitality, Suspense, Awe/Sublime, Humor, Annoyance, and Nostalgia.1 1 The concepts Beauty and Awe/Sublime primarily define objectbased aesthetic virtues. Kant (2001) emphasized that such virtues are typically intuitively felt rather than rationally computed. Such 1652 In addition to selecting an adapted set of emotions, the annotation of poetry brings further challenges, one of which is the choice of the appropriate unit of annotation. Previous work considers words2 (Mohammad and Turney, 2013; Strapparava and Valitutti, 2004), sentences (Alm et al., 2005; Aman and Szpakowicz, 2007), utterances (Cevher et al., 2019), sentence triples (Kim and Klinger, 2018), or paragraphs (Liu et al., 2019) as the units of annotation. For poetry, reasonable units follow the logical document structure of poems, i.e., verse (line), stanza, and, owing to its relative shortness, the complete text. The more coarse-grained the unit, the more difficult the annotation is likely to be, but the more it may also enable the annotation of emotions in context. We find that annotating fine-grained units (lines) that are hierarchically ordered within a larger context (stanza,"
2020.lrec-1.205,C18-1164,0,0.117171,"al datasets of German and English language poems annotated with reader emotions on verse level at https://github.com/tnhaider/ poetry-emotion. 2. 2.1. Related Work Poetry in Natural Language Processing Natural language understanding research on poetry has investigated stylistic variation (Kaplan and Blei, 2007; Kao and Jurafsky, 2015; Voigt and Jurafsky, 2013), with a focus on broadly accepted formal features such as meter (Greene et al., 2010; Agirrezabal et al., 2016; Estes and Hench, 2016) and rhyme (Reddy and Knight, 2011; Haider and Kuhn, 2018), as well as enjambement (Ruiz et al., 2017; Baumann et al., 2018) and metaphor (Kesarwani et al., 2017; Reinig and Rehbein, 2019). Recent work has also explored the relationship of poetry and prose, mainly on a syntactic level feelings of Beauty and Sublime have therefore come to be subsumed under the rubrique of aesthetic emotions in recent psychological research (Menninghaus et al., 2019). For this reason, we refer to the whole set of category labels as emotions throughout this paper. 2 to create emotion dictionaries (Krishna et al., 2019; Gopidi and Alam, 2019). Furthermore, poetry also lends itself well to semantic (change) analysis (Haider, 2019; Haide"
2020.lrec-1.205,C18-1179,1,0.837616,". Emotion Annotation Emotion corpora have been created for different tasks and with different annotation strategies, with different units of analysis and different foci of emotion perspective (reader, writer, text). Examples include the ISEAR dataset (Scherer and Wallbott, 1994) (document-level); emotion annotation in children stories (Alm et al., 2005) and news headlines (Strapparava and Mihalcea, 2007) (sentence-level); and finegrained emotion annotation in literature by Kim and Klinger (2018) (phrase- and word-level). We refer the interested reader to an overview paper on existing corpora (Bostan and Klinger, 2018). We are only aware of a limited number of publications which look in more depth into the emotion perspective. Buechel and Hahn (2017a) report on an annotation study that focuses both on writer’s and reader’s emotions associated with English sentences. The results show that the reader perspective yields better inter-annotator agreement. Yang et al. (2009) also study the difference between writer and reader emotions, but not with a modeling perspective. The authors find that positive reader emotions tend to be linked to positive writer emotions in online blogs. 1653 3.1. ����� ������ ������ ���"
2020.lrec-1.205,E17-2092,0,0.574085,"emotion labels, the perspective of annotators plays a major role. Whether emotions are elicited in the reader, expressed in the text, or intended by the author largely changes the permissible labels. For example, feelings of Disgust or Love might be intended or expressed in the text, but the text might still fail to elicit corresponding feelings as these concepts presume a strong reaction in the reader. Our focus here was on the actual emotional experience of the readers rather than on hypothetical intentions of authors. We opted for this reader perspective based on previous research in NLP (Buechel and Hahn, 2017a; Buechel and Hahn, 2017b) and work in empirical aesthetics (Menninghaus et al., 2017), that specifically measured the reception of poetry. Our final set of emotion labels consists of Beauty/Joy, Sadness, Uneasiness, Vitality, Suspense, Awe/Sublime, Humor, Annoyance, and Nostalgia.1 1 The concepts Beauty and Awe/Sublime primarily define objectbased aesthetic virtues. Kant (2001) emphasized that such virtues are typically intuitively felt rather than rationally computed. Such 1652 In addition to selecting an adapted set of emotions, the annotation of poetry brings further challenges, one of wh"
2020.lrec-1.205,W17-0801,0,0.568203,"emotion labels, the perspective of annotators plays a major role. Whether emotions are elicited in the reader, expressed in the text, or intended by the author largely changes the permissible labels. For example, feelings of Disgust or Love might be intended or expressed in the text, but the text might still fail to elicit corresponding feelings as these concepts presume a strong reaction in the reader. Our focus here was on the actual emotional experience of the readers rather than on hypothetical intentions of authors. We opted for this reader perspective based on previous research in NLP (Buechel and Hahn, 2017a; Buechel and Hahn, 2017b) and work in empirical aesthetics (Menninghaus et al., 2017), that specifically measured the reception of poetry. Our final set of emotion labels consists of Beauty/Joy, Sadness, Uneasiness, Vitality, Suspense, Awe/Sublime, Humor, Annoyance, and Nostalgia.1 1 The concepts Beauty and Awe/Sublime primarily define objectbased aesthetic virtues. Kant (2001) emphasized that such virtues are typically intuitively felt rather than rationally computed. Such 1652 In addition to selecting an adapted set of emotions, the annotation of poetry brings further challenges, one of wh"
2020.lrec-1.205,D19-1227,0,0.0618039,"Missing"
2020.lrec-1.205,N19-1423,0,0.0881123,"often based on neural networks and commonly use word embeddings as features. Schuff et al. (2017) applied models from the classes of CNN, BiLSTM, and LSTM and compare them to linear classifiers (SVM and MaxEnt), where the BiLSTM shows best results with the most balanced precision and recall. Abdul-Mageed and Ungar (2017) claim the highest F1 with gated recurrent unit networks (Chung et al., 2015) for Plutchik’s emotion model. More recently, shared tasks on emotion analysis (Mohammad et al., 2018; Klinger et al., 2018) triggered a set of more advanced deep learning approaches, including BERT (Devlin et al., 2019) and other transfer learning methods (Dankers et al., 2019). 3. Data Collection For our annotation and modeling studies, we build on top of two poetry corpora (in English and German), which we refer to as PO-EMO. This collection represents important contributions to the literary canon over the last 400 years. We make this resource available in TEI P5 XML3 and an easy-to-use tab separated format. Table 1 shows a size overview of these data sets. Figure 1 shows the distribution of our data over time via density plots. Note that both corpora show a relative underrepresentation before the onset of"
2020.lrec-1.205,W16-0201,0,0.287208,"with in-house annotation process (that usually involve training and close supervision of the annotators). We provide the final datasets of German and English language poems annotated with reader emotions on verse level at https://github.com/tnhaider/ poetry-emotion. 2. 2.1. Related Work Poetry in Natural Language Processing Natural language understanding research on poetry has investigated stylistic variation (Kaplan and Blei, 2007; Kao and Jurafsky, 2015; Voigt and Jurafsky, 2013), with a focus on broadly accepted formal features such as meter (Greene et al., 2010; Agirrezabal et al., 2016; Estes and Hench, 2016) and rhyme (Reddy and Knight, 2011; Haider and Kuhn, 2018), as well as enjambement (Ruiz et al., 2017; Baumann et al., 2018) and metaphor (Kesarwani et al., 2017; Reinig and Rehbein, 2019). Recent work has also explored the relationship of poetry and prose, mainly on a syntactic level feelings of Beauty and Sublime have therefore come to be subsumed under the rubrique of aesthetic emotions in recent psychological research (Menninghaus et al., 2019). For this reason, we refer to the whole set of category labels as emotions throughout this paper. 2 to create emotion dictionaries (Krishna et al.,"
2020.lrec-1.205,esuli-sebastiani-2006-sentiwordnet,0,0.0387291,"thm, and rhyme in other studies (Haider and Kuhn, 2018; Haider et al., 2020). Figure 1: Temporal distribution of poetry corpora (Kernel Density Plots with bandwidth = 0.2). # tokens # lines # stanzas # poems # authors German English 20647 3651 731 158 51 3716 540 174 64 22 3.2. Table 1: Statistics on our poetry corpora PO-EMO. 2.3. Emotion Classification The task of emotion classification has been tackled before using rule-based and machine learning approaches. Rulebased emotion classification typically relies on lexical resources of emotionally charged words (Strapparava and Valitutti, 2004; Esuli and Sebastiani, 2006; Mohammad and Turney, 2013) and offers a straightforward and transparent way to detect emotions in text. In contrast to rule-based approaches, current models for emotion classification are often based on neural networks and commonly use word embeddings as features. Schuff et al. (2017) applied models from the classes of CNN, BiLSTM, and LSTM and compare them to linear classifiers (SVM and MaxEnt), where the BiLSTM shows best results with the most balanced precision and recall. Abdul-Mageed and Ungar (2017) claim the highest F1 with gated recurrent unit networks (Chung et al., 2015) for Plutch"
2020.lrec-1.205,W19-4702,0,0.297849,"yme (Reddy and Knight, 2011; Haider and Kuhn, 2018), as well as enjambement (Ruiz et al., 2017; Baumann et al., 2018) and metaphor (Kesarwani et al., 2017; Reinig and Rehbein, 2019). Recent work has also explored the relationship of poetry and prose, mainly on a syntactic level feelings of Beauty and Sublime have therefore come to be subsumed under the rubrique of aesthetic emotions in recent psychological research (Menninghaus et al., 2019). For this reason, we refer to the whole set of category labels as emotions throughout this paper. 2 to create emotion dictionaries (Krishna et al., 2019; Gopidi and Alam, 2019). Furthermore, poetry also lends itself well to semantic (change) analysis (Haider, 2019; Haider and Eger, 2019), as linguistic invention (Underwood and Sellers, 2012; Herbelot, 2014) and succinctness (Roberts, 2000) are at the core of poetic production. Corpus-based analysis of emotions in poetry has been considered, but there is no work on German, and little on English. Kao and Jurafsky (2015) analyze English poems with word associations from the Harvard Inquirer and LIWC, within the categories positive/negative outlook, positive/negative emotion and phys./psych. well-being. Hou and Frank (2"
2020.lrec-1.205,D10-1051,0,0.234926,"poetry, given time and cost overhead associated with in-house annotation process (that usually involve training and close supervision of the annotators). We provide the final datasets of German and English language poems annotated with reader emotions on verse level at https://github.com/tnhaider/ poetry-emotion. 2. 2.1. Related Work Poetry in Natural Language Processing Natural language understanding research on poetry has investigated stylistic variation (Kaplan and Blei, 2007; Kao and Jurafsky, 2015; Voigt and Jurafsky, 2013), with a focus on broadly accepted formal features such as meter (Greene et al., 2010; Agirrezabal et al., 2016; Estes and Hench, 2016) and rhyme (Reddy and Knight, 2011; Haider and Kuhn, 2018), as well as enjambement (Ruiz et al., 2017; Baumann et al., 2018) and metaphor (Kesarwani et al., 2017; Reinig and Rehbein, 2019). Recent work has also explored the relationship of poetry and prose, mainly on a syntactic level feelings of Beauty and Sublime have therefore come to be subsumed under the rubrique of aesthetic emotions in recent psychological research (Menninghaus et al., 2019). For this reason, we refer to the whole set of category labels as emotions throughout this paper."
2020.lrec-1.205,W19-4727,1,0.934327,"2018) and metaphor (Kesarwani et al., 2017; Reinig and Rehbein, 2019). Recent work has also explored the relationship of poetry and prose, mainly on a syntactic level feelings of Beauty and Sublime have therefore come to be subsumed under the rubrique of aesthetic emotions in recent psychological research (Menninghaus et al., 2019). For this reason, we refer to the whole set of category labels as emotions throughout this paper. 2 to create emotion dictionaries (Krishna et al., 2019; Gopidi and Alam, 2019). Furthermore, poetry also lends itself well to semantic (change) analysis (Haider, 2019; Haider and Eger, 2019), as linguistic invention (Underwood and Sellers, 2012; Herbelot, 2014) and succinctness (Roberts, 2000) are at the core of poetic production. Corpus-based analysis of emotions in poetry has been considered, but there is no work on German, and little on English. Kao and Jurafsky (2015) analyze English poems with word associations from the Harvard Inquirer and LIWC, within the categories positive/negative outlook, positive/negative emotion and phys./psych. well-being. Hou and Frank (2015) examine the binary sentiment polarity of Chinese poems with a weighted personalized PageRank algorithm. Bar"
2020.lrec-1.205,W18-4509,1,0.788717,"aining and close supervision of the annotators). We provide the final datasets of German and English language poems annotated with reader emotions on verse level at https://github.com/tnhaider/ poetry-emotion. 2. 2.1. Related Work Poetry in Natural Language Processing Natural language understanding research on poetry has investigated stylistic variation (Kaplan and Blei, 2007; Kao and Jurafsky, 2015; Voigt and Jurafsky, 2013), with a focus on broadly accepted formal features such as meter (Greene et al., 2010; Agirrezabal et al., 2016; Estes and Hench, 2016) and rhyme (Reddy and Knight, 2011; Haider and Kuhn, 2018), as well as enjambement (Ruiz et al., 2017; Baumann et al., 2018) and metaphor (Kesarwani et al., 2017; Reinig and Rehbein, 2019). Recent work has also explored the relationship of poetry and prose, mainly on a syntactic level feelings of Beauty and Sublime have therefore come to be subsumed under the rubrique of aesthetic emotions in recent psychological research (Menninghaus et al., 2019). For this reason, we refer to the whole set of category labels as emotions throughout this paper. 2 to create emotion dictionaries (Krishna et al., 2019; Gopidi and Alam, 2019). Furthermore, poetry also le"
2020.lrec-1.205,W15-3703,0,0.0256667,"and Alam, 2019). Furthermore, poetry also lends itself well to semantic (change) analysis (Haider, 2019; Haider and Eger, 2019), as linguistic invention (Underwood and Sellers, 2012; Herbelot, 2014) and succinctness (Roberts, 2000) are at the core of poetic production. Corpus-based analysis of emotions in poetry has been considered, but there is no work on German, and little on English. Kao and Jurafsky (2015) analyze English poems with word associations from the Harvard Inquirer and LIWC, within the categories positive/negative outlook, positive/negative emotion and phys./psych. well-being. Hou and Frank (2015) examine the binary sentiment polarity of Chinese poems with a weighted personalized PageRank algorithm. Barros et al. (2013) followed a tagging approach with a thesaurus to annotate words that are similar to the words ‘Joy’, ‘Anger’, ‘Fear’ and ‘Sadness’ (moreover translating these from English to Spanish). With these word lists, they distinguish the categories ‘Love’, ‘Songs to Lisi’, ‘Satire’ and ‘Philosophical-Moral-Religious’ in Quevedo’s poetry. Similarly, Alsharif et al. (2013) classify unique Arabic ‘emotional text forms’ based on word unigrams. Mohanty et al. (2018) create a corpus of"
2020.lrec-1.205,W17-2201,0,0.340627,"nguage poems annotated with reader emotions on verse level at https://github.com/tnhaider/ poetry-emotion. 2. 2.1. Related Work Poetry in Natural Language Processing Natural language understanding research on poetry has investigated stylistic variation (Kaplan and Blei, 2007; Kao and Jurafsky, 2015; Voigt and Jurafsky, 2013), with a focus on broadly accepted formal features such as meter (Greene et al., 2010; Agirrezabal et al., 2016; Estes and Hench, 2016) and rhyme (Reddy and Knight, 2011; Haider and Kuhn, 2018), as well as enjambement (Ruiz et al., 2017; Baumann et al., 2018) and metaphor (Kesarwani et al., 2017; Reinig and Rehbein, 2019). Recent work has also explored the relationship of poetry and prose, mainly on a syntactic level feelings of Beauty and Sublime have therefore come to be subsumed under the rubrique of aesthetic emotions in recent psychological research (Menninghaus et al., 2019). For this reason, we refer to the whole set of category labels as emotions throughout this paper. 2 to create emotion dictionaries (Krishna et al., 2019; Gopidi and Alam, 2019). Furthermore, poetry also lends itself well to semantic (change) analysis (Haider, 2019; Haider and Eger, 2019), as linguistic inve"
2020.lrec-1.205,C18-1114,1,0.868707,"nnoyance, and Nostalgia.1 1 The concepts Beauty and Awe/Sublime primarily define objectbased aesthetic virtues. Kant (2001) emphasized that such virtues are typically intuitively felt rather than rationally computed. Such 1652 In addition to selecting an adapted set of emotions, the annotation of poetry brings further challenges, one of which is the choice of the appropriate unit of annotation. Previous work considers words2 (Mohammad and Turney, 2013; Strapparava and Valitutti, 2004), sentences (Alm et al., 2005; Aman and Szpakowicz, 2007), utterances (Cevher et al., 2019), sentence triples (Kim and Klinger, 2018), or paragraphs (Liu et al., 2019) as the units of annotation. For poetry, reasonable units follow the logical document structure of poems, i.e., verse (line), stanza, and, owing to its relative shortness, the complete text. The more coarse-grained the unit, the more difficult the annotation is likely to be, but the more it may also enable the annotation of emotions in context. We find that annotating fine-grained units (lines) that are hierarchically ordered within a larger context (stanza, poem) caters to the specific structure of poems, where emotions are regularly mixed and are more interp"
2020.lrec-1.205,C18-2002,0,0.0660867,"Missing"
2020.lrec-1.205,W18-6206,1,0.903998,"Missing"
2020.lrec-1.205,P19-1111,0,0.0262129,"nd Hench, 2016) and rhyme (Reddy and Knight, 2011; Haider and Kuhn, 2018), as well as enjambement (Ruiz et al., 2017; Baumann et al., 2018) and metaphor (Kesarwani et al., 2017; Reinig and Rehbein, 2019). Recent work has also explored the relationship of poetry and prose, mainly on a syntactic level feelings of Beauty and Sublime have therefore come to be subsumed under the rubrique of aesthetic emotions in recent psychological research (Menninghaus et al., 2019). For this reason, we refer to the whole set of category labels as emotions throughout this paper. 2 to create emotion dictionaries (Krishna et al., 2019; Gopidi and Alam, 2019). Furthermore, poetry also lends itself well to semantic (change) analysis (Haider, 2019; Haider and Eger, 2019), as linguistic invention (Underwood and Sellers, 2012; Herbelot, 2014) and succinctness (Roberts, 2000) are at the core of poetic production. Corpus-based analysis of emotions in poetry has been considered, but there is no work on German, and little on English. Kao and Jurafsky (2015) analyze English poems with word associations from the Harvard Inquirer and LIWC, within the categories positive/negative outlook, positive/negative emotion and phys./psych. well"
2020.lrec-1.205,D19-1656,0,0.0362616,"Missing"
2020.lrec-1.205,S18-1001,0,0.0419404,"rent way to detect emotions in text. In contrast to rule-based approaches, current models for emotion classification are often based on neural networks and commonly use word embeddings as features. Schuff et al. (2017) applied models from the classes of CNN, BiLSTM, and LSTM and compare them to linear classifiers (SVM and MaxEnt), where the BiLSTM shows best results with the most balanced precision and recall. Abdul-Mageed and Ungar (2017) claim the highest F1 with gated recurrent unit networks (Chung et al., 2015) for Plutchik’s emotion model. More recently, shared tasks on emotion analysis (Mohammad et al., 2018; Klinger et al., 2018) triggered a set of more advanced deep learning approaches, including BERT (Devlin et al., 2019) and other transfer learning methods (Dankers et al., 2019). 3. Data Collection For our annotation and modeling studies, we build on top of two poetry corpora (in English and German), which we refer to as PO-EMO. This collection represents important contributions to the literary canon over the last 400 years. We make this resource available in TEI P5 XML3 and an easy-to-use tab separated format. Table 1 shows a size overview of these data sets. Figure 1 shows the distribution"
2020.lrec-1.205,P11-2014,0,0.428395,"(that usually involve training and close supervision of the annotators). We provide the final datasets of German and English language poems annotated with reader emotions on verse level at https://github.com/tnhaider/ poetry-emotion. 2. 2.1. Related Work Poetry in Natural Language Processing Natural language understanding research on poetry has investigated stylistic variation (Kaplan and Blei, 2007; Kao and Jurafsky, 2015; Voigt and Jurafsky, 2013), with a focus on broadly accepted formal features such as meter (Greene et al., 2010; Agirrezabal et al., 2016; Estes and Hench, 2016) and rhyme (Reddy and Knight, 2011; Haider and Kuhn, 2018), as well as enjambement (Ruiz et al., 2017; Baumann et al., 2018) and metaphor (Kesarwani et al., 2017; Reinig and Rehbein, 2019). Recent work has also explored the relationship of poetry and prose, mainly on a syntactic level feelings of Beauty and Sublime have therefore come to be subsumed under the rubrique of aesthetic emotions in recent psychological research (Menninghaus et al., 2019). For this reason, we refer to the whole set of category labels as emotions throughout this paper. 2 to create emotion dictionaries (Krishna et al., 2019; Gopidi and Alam, 2019). Fur"
2020.lrec-1.205,W17-2204,0,0.157637,"We provide the final datasets of German and English language poems annotated with reader emotions on verse level at https://github.com/tnhaider/ poetry-emotion. 2. 2.1. Related Work Poetry in Natural Language Processing Natural language understanding research on poetry has investigated stylistic variation (Kaplan and Blei, 2007; Kao and Jurafsky, 2015; Voigt and Jurafsky, 2013), with a focus on broadly accepted formal features such as meter (Greene et al., 2010; Agirrezabal et al., 2016; Estes and Hench, 2016) and rhyme (Reddy and Knight, 2011; Haider and Kuhn, 2018), as well as enjambement (Ruiz et al., 2017; Baumann et al., 2018) and metaphor (Kesarwani et al., 2017; Reinig and Rehbein, 2019). Recent work has also explored the relationship of poetry and prose, mainly on a syntactic level feelings of Beauty and Sublime have therefore come to be subsumed under the rubrique of aesthetic emotions in recent psychological research (Menninghaus et al., 2019). For this reason, we refer to the whole set of category labels as emotions throughout this paper. 2 to create emotion dictionaries (Krishna et al., 2019; Gopidi and Alam, 2019). Furthermore, poetry also lends itself well to semantic (change) analys"
2020.lrec-1.205,W17-5203,1,0.79635,"stics on our poetry corpora PO-EMO. 2.3. Emotion Classification The task of emotion classification has been tackled before using rule-based and machine learning approaches. Rulebased emotion classification typically relies on lexical resources of emotionally charged words (Strapparava and Valitutti, 2004; Esuli and Sebastiani, 2006; Mohammad and Turney, 2013) and offers a straightforward and transparent way to detect emotions in text. In contrast to rule-based approaches, current models for emotion classification are often based on neural networks and commonly use word embeddings as features. Schuff et al. (2017) applied models from the classes of CNN, BiLSTM, and LSTM and compare them to linear classifiers (SVM and MaxEnt), where the BiLSTM shows best results with the most balanced precision and recall. Abdul-Mageed and Ungar (2017) claim the highest F1 with gated recurrent unit networks (Chung et al., 2015) for Plutchik’s emotion model. More recently, shared tasks on emotion analysis (Mohammad et al., 2018; Klinger et al., 2018) triggered a set of more advanced deep learning approaches, including BERT (Devlin et al., 2019) and other transfer learning methods (Dankers et al., 2019). 3. Data Collectio"
2020.lrec-1.205,N18-2006,1,0.844183,"trategy that would suit the crowdsourcing environment. The dataset presented in this paper can be of use for different application scenarios, including multi-label emotion classification, style-conditioned poetry generation, investigating the influence of rhythm/prosodic features on emotion, or analysis of authors, genres and diachronic variation (e.g., how emotions are represented differently in certain periods). Further, though our modeling experiments are still rudimentary, we propose that this data set can be used to investigate the intra-poem relations either through multi-task learning (Schulz et al., 2018) and/or with the help of hierarchical sequence classification approaches. Acknowledgements A special thanks goes to Gesine Fuhrmann, who created the guidelines and tirelessly documented the annotation progress. Also thanks to Annika Palm and Debby Trzeciak who annotated and gave lively feedback. For help with the conceptualization of labels we thank Ines Schindler. This research has been partially conducted within the CRETA center (http://www.creta. uni-stuttgart.de/) which is funded by the German Ministry for Education and Research (BMBF) and partially funded by the German Research Council (D"
2020.lrec-1.205,S07-1013,0,0.350778,"hese studies focus on basic emotions and binary sentiment polarity only, rather than addressing aesthetic emotions. Moreover, they annotate on the level of complete poems (instead of fine-grained verse and stanza-level). 2.2. Emotion Annotation Emotion corpora have been created for different tasks and with different annotation strategies, with different units of analysis and different foci of emotion perspective (reader, writer, text). Examples include the ISEAR dataset (Scherer and Wallbott, 1994) (document-level); emotion annotation in children stories (Alm et al., 2005) and news headlines (Strapparava and Mihalcea, 2007) (sentence-level); and finegrained emotion annotation in literature by Kim and Klinger (2018) (phrase- and word-level). We refer the interested reader to an overview paper on existing corpora (Bostan and Klinger, 2018). We are only aware of a limited number of publications which look in more depth into the emotion perspective. Buechel and Hahn (2017a) report on an annotation study that focuses both on writer’s and reader’s emotions associated with English sentences. The results show that the reader perspective yields better inter-annotator agreement. Yang et al. (2009) also study the differenc"
2020.lrec-1.205,strapparava-valitutti-2004-wordnet,0,0.826287,"the reception of poetry. Our final set of emotion labels consists of Beauty/Joy, Sadness, Uneasiness, Vitality, Suspense, Awe/Sublime, Humor, Annoyance, and Nostalgia.1 1 The concepts Beauty and Awe/Sublime primarily define objectbased aesthetic virtues. Kant (2001) emphasized that such virtues are typically intuitively felt rather than rationally computed. Such 1652 In addition to selecting an adapted set of emotions, the annotation of poetry brings further challenges, one of which is the choice of the appropriate unit of annotation. Previous work considers words2 (Mohammad and Turney, 2013; Strapparava and Valitutti, 2004), sentences (Alm et al., 2005; Aman and Szpakowicz, 2007), utterances (Cevher et al., 2019), sentence triples (Kim and Klinger, 2018), or paragraphs (Liu et al., 2019) as the units of annotation. For poetry, reasonable units follow the logical document structure of poems, i.e., verse (line), stanza, and, owing to its relative shortness, the complete text. The more coarse-grained the unit, the more difficult the annotation is likely to be, but the more it may also enable the annotation of emotions in context. We find that annotating fine-grained units (lines) that are hierarchically ordered wit"
2020.lrec-1.205,W13-1403,0,0.0324413,"ential of a crowdsourcing environment for the task of self-perceived emotion annotation in poetry, given time and cost overhead associated with in-house annotation process (that usually involve training and close supervision of the annotators). We provide the final datasets of German and English language poems annotated with reader emotions on verse level at https://github.com/tnhaider/ poetry-emotion. 2. 2.1. Related Work Poetry in Natural Language Processing Natural language understanding research on poetry has investigated stylistic variation (Kaplan and Blei, 2007; Kao and Jurafsky, 2015; Voigt and Jurafsky, 2013), with a focus on broadly accepted formal features such as meter (Greene et al., 2010; Agirrezabal et al., 2016; Estes and Hench, 2016) and rhyme (Reddy and Knight, 2011; Haider and Kuhn, 2018), as well as enjambement (Ruiz et al., 2017; Baumann et al., 2018) and metaphor (Kesarwani et al., 2017; Reinig and Rehbein, 2019). Recent work has also explored the relationship of poetry and prose, mainly on a syntactic level feelings of Beauty and Sublime have therefore come to be subsumed under the rubrique of aesthetic emotions in recent psychological research (Menninghaus et al., 2019). For this re"
2020.peoples-1.12,H05-1073,0,0.0799053,"cer might be considered a confounder. Further, we analyze if informing the model about the position of the role improves the classification decision. Particularly on literature corpora we find that the role information improves the emotion classification. 1 Introduction Emotion analysis is now an established research area which finds application in a variety of different fields, including social media analysis (Purver and Battersby, 2012; Wang et al., 2012; Mohammad and Bravo-Marquez, 2017; Ying et al., 2019, i.a.), opinion mining (Choi et al., 2006, i.a.), and computational literary studies (Alm et al., 2005; Kim and Klinger, 2019a; Haider et al., 2020; Zehe et al., 2020, i.a.). The most prominent task in emotion analysis is emotion categorization, where text receives assignments from a predefined emotion inventory, such as the fundamental emotions of fear, anger, joy, anticipation, trust, surprise, disgust, and sadness which follow theories by Ekman (1999) or Plutchik (2001). Other tasks include the recognition of affect values, namely valence or arousal (Posner et al., 2005) or analyses of event appraisal (Hofmann et al., 2020; Scherer, 2005). More recently, categorization (or regression) tasks"
2020.peoples-1.12,C18-1179,1,0.848865,"op in performance compared to the As-Is setting. In such cases, the Only setting might show comparable performance, and the Position setting would show further improvements. When the role is a confounder, the performance in the Without setting is expected to be increased over the As-Is setting. The label set depends on each of the datasets. For ES, we use the emotion labels anger, disgust, fear, joy, no emotion, sadness, and surprise; for ECA, we use anger, sadness, disgust, joy, fear, surprise, and no emotion. For GNE and ET, we merge the categories according to the rules described for ET by Bostan and Klinger (2018) and keep the primary emotions described in Plutchik’s wheel. For REMAN, we group similarly and keep anger, disgust, fear, joy, anticipation, surprise, sadness, trust, and no emotion. ECA has a low number of instances annotated with multiple labels, which we ignore to keep all tasks as single-label classification. REMAN has emotion annotations only for the middle sentence in each triple. Thus we include only these middle segments in our experiments. The results are based on a random split of each dataset into train, validation, and test (0.8, 0.1, 0.1). We report macro-averages across 10 runs"
2020.peoples-1.12,2020.lrec-1.194,1,0.911877,"amely valence or arousal (Posner et al., 2005) or analyses of event appraisal (Hofmann et al., 2020; Scherer, 2005). More recently, categorization (or regression) tasks have been complemented by more fine-grained analyses, namely emotion stimulus detection and role labeling, to detect which words denote the experiencer of an emotion, the emotion cue description, or the target of an emotion. These efforts lead to computational approaches of detecting stimulus clauses (Xia and Ding, 2019; Wei et al., 2020; Gao et al., 2017) and emotion role labeling and sequence labeling (Mohammad et al., 2014; Bostan et al., 2020; Kim and Klinger, 2018; Ghazi et al., 2015; Zehe et al., 2020), with different advantages and disadvantages we discuss in Oberl¨ander and Klinger (2020). Further, this work led to a rich set of corpora with annotations of different subsets  of  roles.  An example of a sentence annotated with semantic role labels for emotion is “ John hates cars because they EXPERIENCER CUE TARGET   pollute the environment .” A number of English-language resources are available: Ghazi et al. (2015) STIMULUS This work is licensed under a Creative Commons Attribution 4.0 International License. License de"
2020.peoples-1.12,W06-1651,0,0.0759704,"muli and targets carry emotion information, while the experiencer might be considered a confounder. Further, we analyze if informing the model about the position of the role improves the classification decision. Particularly on literature corpora we find that the role information improves the emotion classification. 1 Introduction Emotion analysis is now an established research area which finds application in a variety of different fields, including social media analysis (Purver and Battersby, 2012; Wang et al., 2012; Mohammad and Bravo-Marquez, 2017; Ying et al., 2019, i.a.), opinion mining (Choi et al., 2006, i.a.), and computational literary studies (Alm et al., 2005; Kim and Klinger, 2019a; Haider et al., 2020; Zehe et al., 2020, i.a.). The most prominent task in emotion analysis is emotion categorization, where text receives assignments from a predefined emotion inventory, such as the fundamental emotions of fear, anger, joy, anticipation, trust, surprise, disgust, and sadness which follow theories by Ekman (1999) or Plutchik (2001). Other tasks include the recognition of affect values, namely valence or arousal (Posner et al., 2005) or analyses of event appraisal (Hofmann et al., 2020; Schere"
2020.peoples-1.12,N19-1423,0,0.0104607,"optimize with Adam (Kingma and Ba, 2015), with a base learning rate of 0.0003, L2 regularization, on a batch size of 32, with early stopping with patience of 3, and initialization with Kaiming (He et al., 2015). We train for up to 100 epochs for the bi-LSTM model and 10 for the transformer-based model. Both models fine-tune their input representations during training. The hyperparameters of the model are optimized for ECA. For the bi-LSTM, we use AllenNLP (Gardner et al., 2018) and for the transformer the Hugging Face library (Wolf et al., 2019) (following the training procedure described by Devlin et al. (2019)). The code of our project is available at http://www.ims.uni-stuttgart.de/data/emotion-classification-roles. 4 We experimented with adding two channels in the input embeddings which mark the tokens outside a role annotation with a 1 in one channel and the tokens which belong to the role annotation with a 1 in a second channel. The results were inferior to using positional indicators. 121 As-Is Dataset Role Without P R F1 P R Only F1 P Position R F1 P R F1 ECA Stimulus 41 39 39 48 48 48 30 25 23 52 51 51 ES Stimulus 93 89 90 94 89 90 65 23 18 95 90 92 REMAN Cue Stimulus Experiencer Target 61 4"
2020.peoples-1.12,W18-2501,0,0.0121307,"glove/ 3 The hyperparameters and details for the models are as follows. For the bi-LSTM, we set a dropout and recurrent dropout of 0.3 and optimize with Adam (Kingma and Ba, 2015), with a base learning rate of 0.0003, L2 regularization, on a batch size of 32, with early stopping with patience of 3, and initialization with Kaiming (He et al., 2015). We train for up to 100 epochs for the bi-LSTM model and 10 for the transformer-based model. Both models fine-tune their input representations during training. The hyperparameters of the model are optimized for ECA. For the bi-LSTM, we use AllenNLP (Gardner et al., 2018) and for the transformer the Hugging Face library (Wolf et al., 2019) (following the training procedure described by Devlin et al. (2019)). The code of our project is available at http://www.ims.uni-stuttgart.de/data/emotion-classification-roles. 4 We experimented with adding two channels in the input embeddings which mark the tokens outside a role annotation with a 1 in one channel and the tokens which belong to the role annotation with a 1 in a second channel. The results were inferior to using positional indicators. 121 As-Is Dataset Role Without P R F1 P R Only F1 P Position R F1 P R F1 EC"
2020.peoples-1.12,D16-1170,0,0.0240576,"th of each role filler in each dataset in the number of tokens. manually construct a dataset following FrameNet’s emotion predicate and annotate the stimulus as its core argument. Mohammad et al. (2014) annotate Tweets for emotion cue phrases, emotion targets, and the emotion stimulus. In our previous work (Bostan et al., 2020) we publish news headlines annotated with the roles of emotion experiencer, cue, target, and stimulus. Kim and Klinger (2018) annotate sentence triples taken from literature for the same roles. A popular benchmark for emotion stimulus detection is the Mandarin corpus by Gui et al. (2016). Gao et al. (2017) annotate English and Mandarin texts in a comparable way on the clause level (Emotion Cause Analysis, ECA). In this paper, we utilize role annotations to understand their influence on emotion classification. We evaluate which of the roles’ contents enable an emotion classifier to infer the emotions. It is reasonable to assume that the roles’ content carries different kinds of information regarding the emotion: One particular experiencer present in a corpus might always feel the same emotion; hence, be prone to a bias the model could pick up on. The target or stimulus might b"
2020.peoples-1.12,2020.lrec-1.205,1,0.833892,"her, we analyze if informing the model about the position of the role improves the classification decision. Particularly on literature corpora we find that the role information improves the emotion classification. 1 Introduction Emotion analysis is now an established research area which finds application in a variety of different fields, including social media analysis (Purver and Battersby, 2012; Wang et al., 2012; Mohammad and Bravo-Marquez, 2017; Ying et al., 2019, i.a.), opinion mining (Choi et al., 2006, i.a.), and computational literary studies (Alm et al., 2005; Kim and Klinger, 2019a; Haider et al., 2020; Zehe et al., 2020, i.a.). The most prominent task in emotion analysis is emotion categorization, where text receives assignments from a predefined emotion inventory, such as the fundamental emotions of fear, anger, joy, anticipation, trust, surprise, disgust, and sadness which follow theories by Ekman (1999) or Plutchik (2001). Other tasks include the recognition of affect values, namely valence or arousal (Posner et al., 2005) or analyses of event appraisal (Hofmann et al., 2020; Scherer, 2005). More recently, categorization (or regression) tasks have been complemented by more fine-grained"
2020.peoples-1.12,2020.coling-main.11,1,0.72188,"on mining (Choi et al., 2006, i.a.), and computational literary studies (Alm et al., 2005; Kim and Klinger, 2019a; Haider et al., 2020; Zehe et al., 2020, i.a.). The most prominent task in emotion analysis is emotion categorization, where text receives assignments from a predefined emotion inventory, such as the fundamental emotions of fear, anger, joy, anticipation, trust, surprise, disgust, and sadness which follow theories by Ekman (1999) or Plutchik (2001). Other tasks include the recognition of affect values, namely valence or arousal (Posner et al., 2005) or analyses of event appraisal (Hofmann et al., 2020; Scherer, 2005). More recently, categorization (or regression) tasks have been complemented by more fine-grained analyses, namely emotion stimulus detection and role labeling, to detect which words denote the experiencer of an emotion, the emotion cue description, or the target of an emotion. These efforts lead to computational approaches of detecting stimulus clauses (Xia and Ding, 2019; Wei et al., 2020; Gao et al., 2017) and emotion role labeling and sequence labeling (Mohammad et al., 2014; Bostan et al., 2020; Kim and Klinger, 2018; Ghazi et al., 2015; Zehe et al., 2020), with different"
2020.peoples-1.12,C18-1114,1,0.930732,"sal (Posner et al., 2005) or analyses of event appraisal (Hofmann et al., 2020; Scherer, 2005). More recently, categorization (or regression) tasks have been complemented by more fine-grained analyses, namely emotion stimulus detection and role labeling, to detect which words denote the experiencer of an emotion, the emotion cue description, or the target of an emotion. These efforts lead to computational approaches of detecting stimulus clauses (Xia and Ding, 2019; Wei et al., 2020; Gao et al., 2017) and emotion role labeling and sequence labeling (Mohammad et al., 2014; Bostan et al., 2020; Kim and Klinger, 2018; Ghazi et al., 2015; Zehe et al., 2020), with different advantages and disadvantages we discuss in Oberl¨ander and Klinger (2020). Further, this work led to a rich set of corpora with annotations of different subsets  of  roles.  An example of a sentence annotated with semantic role labels for emotion is “ John hates cars because they EXPERIENCER CUE TARGET   pollute the environment .” A number of English-language resources are available: Ghazi et al. (2015) STIMULUS This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creative"
2020.peoples-1.12,W19-3406,1,0.919461,"dered a confounder. Further, we analyze if informing the model about the position of the role improves the classification decision. Particularly on literature corpora we find that the role information improves the emotion classification. 1 Introduction Emotion analysis is now an established research area which finds application in a variety of different fields, including social media analysis (Purver and Battersby, 2012; Wang et al., 2012; Mohammad and Bravo-Marquez, 2017; Ying et al., 2019, i.a.), opinion mining (Choi et al., 2006, i.a.), and computational literary studies (Alm et al., 2005; Kim and Klinger, 2019a; Haider et al., 2020; Zehe et al., 2020, i.a.). The most prominent task in emotion analysis is emotion categorization, where text receives assignments from a predefined emotion inventory, such as the fundamental emotions of fear, anger, joy, anticipation, trust, surprise, disgust, and sadness which follow theories by Ekman (1999) or Plutchik (2001). Other tasks include the recognition of affect values, namely valence or arousal (Posner et al., 2005) or analyses of event appraisal (Hofmann et al., 2020; Scherer, 2005). More recently, categorization (or regression) tasks have been complemented"
2020.peoples-1.12,N19-1067,1,0.921267,"dered a confounder. Further, we analyze if informing the model about the position of the role improves the classification decision. Particularly on literature corpora we find that the role information improves the emotion classification. 1 Introduction Emotion analysis is now an established research area which finds application in a variety of different fields, including social media analysis (Purver and Battersby, 2012; Wang et al., 2012; Mohammad and Bravo-Marquez, 2017; Ying et al., 2019, i.a.), opinion mining (Choi et al., 2006, i.a.), and computational literary studies (Alm et al., 2005; Kim and Klinger, 2019a; Haider et al., 2020; Zehe et al., 2020, i.a.). The most prominent task in emotion analysis is emotion categorization, where text receives assignments from a predefined emotion inventory, such as the fundamental emotions of fear, anger, joy, anticipation, trust, surprise, disgust, and sadness which follow theories by Ekman (1999) or Plutchik (2001). Other tasks include the recognition of affect values, namely valence or arousal (Posner et al., 2005) or analyses of event appraisal (Hofmann et al., 2020; Scherer, 2005). More recently, categorization (or regression) tasks have been complemented"
2020.peoples-1.12,W17-5205,0,0.0187085,"n the text in a controlled manner and find that across multiple corpora, stimuli and targets carry emotion information, while the experiencer might be considered a confounder. Further, we analyze if informing the model about the position of the role improves the classification decision. Particularly on literature corpora we find that the role information improves the emotion classification. 1 Introduction Emotion analysis is now an established research area which finds application in a variety of different fields, including social media analysis (Purver and Battersby, 2012; Wang et al., 2012; Mohammad and Bravo-Marquez, 2017; Ying et al., 2019, i.a.), opinion mining (Choi et al., 2006, i.a.), and computational literary studies (Alm et al., 2005; Kim and Klinger, 2019a; Haider et al., 2020; Zehe et al., 2020, i.a.). The most prominent task in emotion analysis is emotion categorization, where text receives assignments from a predefined emotion inventory, such as the fundamental emotions of fear, anger, joy, anticipation, trust, surprise, disgust, and sadness which follow theories by Ekman (1999) or Plutchik (2001). Other tasks include the recognition of affect values, namely valence or arousal (Posner et al., 2005)"
2020.peoples-1.12,W14-2607,0,0.199819,"ion of affect values, namely valence or arousal (Posner et al., 2005) or analyses of event appraisal (Hofmann et al., 2020; Scherer, 2005). More recently, categorization (or regression) tasks have been complemented by more fine-grained analyses, namely emotion stimulus detection and role labeling, to detect which words denote the experiencer of an emotion, the emotion cue description, or the target of an emotion. These efforts lead to computational approaches of detecting stimulus clauses (Xia and Ding, 2019; Wei et al., 2020; Gao et al., 2017) and emotion role labeling and sequence labeling (Mohammad et al., 2014; Bostan et al., 2020; Kim and Klinger, 2018; Ghazi et al., 2015; Zehe et al., 2020), with different advantages and disadvantages we discuss in Oberl¨ander and Klinger (2020). Further, this work led to a rich set of corpora with annotations of different subsets  of  roles.  An example of a sentence annotated with semantic role labels for emotion is “ John hates cars because they EXPERIENCER CUE TARGET   pollute the environment .” A number of English-language resources are available: Ghazi et al. (2015) STIMULUS This work is licensed under a Creative Commons Attribution 4.0 Internationa"
2020.peoples-1.12,2020.starsem-1.7,1,0.821787,"Missing"
2020.peoples-1.12,D14-1162,0,0.090999,"sification. REMAN has emotion annotations only for the middle sentence in each triple. Thus we include only these middle segments in our experiments. The results are based on a random split of each dataset into train, validation, and test (0.8, 0.1, 0.1). We report macro-averages across 10 runs for the bi-LSTM and 5 runs for RoBERTa. 3 Results In the following, we discuss the results of the bi-LSTM model in detail and then point to differences to those of the transformer-based approach. Table 3 shows the results of our experiments for the bi-LSTM2 We use 42B tokens, pretrained on CommonCrawl (Pennington et al., 2014), https://nlp.stanford.edu/ projects/glove/ 3 The hyperparameters and details for the models are as follows. For the bi-LSTM, we set a dropout and recurrent dropout of 0.3 and optimize with Adam (Kingma and Ba, 2015), with a base learning rate of 0.0003, L2 regularization, on a batch size of 32, with early stopping with patience of 3, and initialization with Kaiming (He et al., 2015). We train for up to 100 epochs for the bi-LSTM model and 10 for the transformer-based model. Both models fine-tune their input representations during training. The hyperparameters of the model are optimized for EC"
2020.peoples-1.12,E12-1049,0,0.0382147,"ic role by masking the fillers of these roles in the text in a controlled manner and find that across multiple corpora, stimuli and targets carry emotion information, while the experiencer might be considered a confounder. Further, we analyze if informing the model about the position of the role improves the classification decision. Particularly on literature corpora we find that the role information improves the emotion classification. 1 Introduction Emotion analysis is now an established research area which finds application in a variety of different fields, including social media analysis (Purver and Battersby, 2012; Wang et al., 2012; Mohammad and Bravo-Marquez, 2017; Ying et al., 2019, i.a.), opinion mining (Choi et al., 2006, i.a.), and computational literary studies (Alm et al., 2005; Kim and Klinger, 2019a; Haider et al., 2020; Zehe et al., 2020, i.a.). The most prominent task in emotion analysis is emotion categorization, where text receives assignments from a predefined emotion inventory, such as the fundamental emotions of fear, anger, joy, anticipation, trust, surprise, disgust, and sadness which follow theories by Ekman (1999) or Plutchik (2001). Other tasks include the recognition of affect va"
2020.peoples-1.12,2020.acl-main.289,0,0.0126321,"follow theories by Ekman (1999) or Plutchik (2001). Other tasks include the recognition of affect values, namely valence or arousal (Posner et al., 2005) or analyses of event appraisal (Hofmann et al., 2020; Scherer, 2005). More recently, categorization (or regression) tasks have been complemented by more fine-grained analyses, namely emotion stimulus detection and role labeling, to detect which words denote the experiencer of an emotion, the emotion cue description, or the target of an emotion. These efforts lead to computational approaches of detecting stimulus clauses (Xia and Ding, 2019; Wei et al., 2020; Gao et al., 2017) and emotion role labeling and sequence labeling (Mohammad et al., 2014; Bostan et al., 2020; Kim and Klinger, 2018; Ghazi et al., 2015; Zehe et al., 2020), with different advantages and disadvantages we discuss in Oberl¨ander and Klinger (2020). Further, this work led to a rich set of corpora with annotations of different subsets  of  roles.  An example of a sentence annotated with semantic role labels for emotion is “ John hates cars because they EXPERIENCER CUE TARGET   pollute the environment .” A number of English-language resources are available: Ghazi et al. ("
2020.peoples-1.12,P19-1096,0,0.0174009,"t, and sadness which follow theories by Ekman (1999) or Plutchik (2001). Other tasks include the recognition of affect values, namely valence or arousal (Posner et al., 2005) or analyses of event appraisal (Hofmann et al., 2020; Scherer, 2005). More recently, categorization (or regression) tasks have been complemented by more fine-grained analyses, namely emotion stimulus detection and role labeling, to detect which words denote the experiencer of an emotion, the emotion cue description, or the target of an emotion. These efforts lead to computational approaches of detecting stimulus clauses (Xia and Ding, 2019; Wei et al., 2020; Gao et al., 2017) and emotion role labeling and sequence labeling (Mohammad et al., 2014; Bostan et al., 2020; Kim and Klinger, 2018; Ghazi et al., 2015; Zehe et al., 2020), with different advantages and disadvantages we discuss in Oberl¨ander and Klinger (2020). Further, this work led to a rich set of corpora with annotations of different subsets  of  roles.  An example of a sentence annotated with semantic role labels for emotion is “ John hates cars because they EXPERIENCER CUE TARGET   pollute the environment .” A number of English-language resources are availab"
2020.peoples-1.12,D19-5541,0,0.0228502,"and find that across multiple corpora, stimuli and targets carry emotion information, while the experiencer might be considered a confounder. Further, we analyze if informing the model about the position of the role improves the classification decision. Particularly on literature corpora we find that the role information improves the emotion classification. 1 Introduction Emotion analysis is now an established research area which finds application in a variety of different fields, including social media analysis (Purver and Battersby, 2012; Wang et al., 2012; Mohammad and Bravo-Marquez, 2017; Ying et al., 2019, i.a.), opinion mining (Choi et al., 2006, i.a.), and computational literary studies (Alm et al., 2005; Kim and Klinger, 2019a; Haider et al., 2020; Zehe et al., 2020, i.a.). The most prominent task in emotion analysis is emotion categorization, where text receives assignments from a predefined emotion inventory, such as the fundamental emotions of fear, anger, joy, anticipation, trust, surprise, disgust, and sadness which follow theories by Ekman (1999) or Plutchik (2001). Other tasks include the recognition of affect values, namely valence or arousal (Posner et al., 2005) or analyses of eve"
2020.socialnlp-1.6,H05-1073,0,0.623023,"at http://www. ims.uni-stuttgart.de/data/lexicalemotiontransfer. 2 2.1 et al. (2005), instead locates emotions along interval scales of affect components (valence, arousal, dominance). These studies have also influenced computational approaches to emotions, whose preliminary requirement is to follow a specific conceptualization coming from psychology, in order to determine the number and type of emotion classes to research in language. Emotion analysis in natural language processing has mainly established itself as a classification task, aimed at assigning a text to the emotion it expresses (Alm et al., 2005). It has been conducted on a variety of corpora that encompass different types of annotations1 , based on one of the established emotion models mentioned above. Such studies also differ with respect to the textual genres they consider, ranging from from tweets (Mohammad et al., 2017; Klinger et al., 2018) to literary texts (Kim et al., 2017). While emotion classification approaches have been used to guide controlled generation of text (Ghosh et al., 2017; Huang et al., 2018; Song et al., 2019), computationally modelling emotions has not yet been applied to style transfer. After describing a me"
2020.socialnlp-1.6,S18-1037,0,0.0428807,"Missing"
2020.socialnlp-1.6,W17-4902,0,0.109129,"ion datasets and annotation schemes can be found in Bostan and Klinger (2018). 42 Sentence s Target Emotion eˆ “He is young” Anger Selection “He is young” Substitution Objective “He is immature” 2. “He is immature” “He is youthful” 1. “He is youthful” Variation s0 with eˆ “He is immature” Figure 2: Pipeline model architecture. The selection module marks tokens to substitute, the substitution module retrieves candidates and perform substitution. The objective ranks and scores variations. ber of affect-related variables (Smith et al., 2019). Other examples include text genres (Lee et al., 2019; Jhamtani et al., 2017), romanticism (Li et al., 2018), politeness/offensiveness and formality (Sennrich et al., 2016; Nogueira dos Santos et al., 2018; Wang et al., 2019). One of the earliest methods that targets sentiment is proposed by Guerini et al. (2008), who change, add and delete sentiment-related words in a lexical substitution framework. Their strategy to retrieve candidate substitutes is informed by a thesaurus and an emotion dictionary: the first facilitates the extraction of substitutes standing in a specific semantic relation to the input words, the other allows to pick those words that have the desire"
2020.socialnlp-1.6,W17-2203,1,0.849372,"Missing"
2020.socialnlp-1.6,C18-1179,1,0.815271,"Xu et al., 2018) and a numRelated Work Emotion Analysis In the field of psychology, the two main emotion traditions are categorical models and the strand that focuses on the continuous nature of humans’ affect (Scherer, 2005). Emotions are grouped into categories corresponding to emotion terms, some of which are prototypical experiences shared across cultures. For Ekman (1992), they are anger, joy, surprise, disgust, fear and sadness; on top of these, Plutchik (2001) adds anticipation and trust. Posner 1 A comprehensive list of available emotion datasets and annotation schemes can be found in Bostan and Klinger (2018). 42 Sentence s Target Emotion eˆ “He is young” Anger Selection “He is young” Substitution Objective “He is immature” 2. “He is immature” “He is youthful” 1. “He is youthful” Variation s0 with eˆ “He is immature” Figure 2: Pipeline model architecture. The selection module marks tokens to substitute, the substitution module retrieves candidates and perform substitution. The objective ranks and scores variations. ber of affect-related variables (Smith et al., 2019). Other examples include text genres (Lee et al., 2019; Jhamtani et al., 2017), romanticism (Li et al., 2018), politeness/offensivene"
2020.socialnlp-1.6,P06-1057,0,0.0271691,"g candidate substitutes for the tokens that have been selected, producing paraphrases of the input sentence. Importantly, paraphrases are over-generated: at this stage of the pipeline, the output is likely to include sentences that do not express the target emotion. Paraphrases are then scored and re-ranked in the last, objective component, which picks up the “best” output. Paraphrase Generation through Lexical Substitution Lexical substitution received some attention independent of style transfer, as it is useful for a range of applications, like paraphrase generation and text summarisation (Dagan et al., 2006). This task, which was formulated by McCarthy and Navigli (2007) and implemented as part of the SemEval-2007 workshop, consists in finding lexical substitutes close in meaning to the original word, given its context within a sentence. The task has mainly been addressed using handcrafted and crowdsourced thesauri, such as WordNet, in order to retrieve lexical substitutes (Martinez et al., 2007; Sinha and Mihalcea, 2014; Kremer et al., 2014; 3.1 Selection This component identifies those tokens from a sentence s = t1 , . . . tn that will be substituted later, and groups them into selections S = {"
2020.socialnlp-1.6,E14-1057,0,0.0713226,"Missing"
2020.socialnlp-1.6,P19-1601,0,0.0234023,"Missing"
2020.socialnlp-1.6,W19-2309,0,0.164819,"of available emotion datasets and annotation schemes can be found in Bostan and Klinger (2018). 42 Sentence s Target Emotion eˆ “He is young” Anger Selection “He is young” Substitution Objective “He is immature” 2. “He is immature” “He is youthful” 1. “He is youthful” Variation s0 with eˆ “He is immature” Figure 2: Pipeline model architecture. The selection module marks tokens to substitute, the substitution module retrieves candidates and perform substitution. The objective ranks and scores variations. ber of affect-related variables (Smith et al., 2019). Other examples include text genres (Lee et al., 2019; Jhamtani et al., 2017), romanticism (Li et al., 2018), politeness/offensiveness and formality (Sennrich et al., 2016; Nogueira dos Santos et al., 2018; Wang et al., 2019). One of the earliest methods that targets sentiment is proposed by Guerini et al. (2008), who change, add and delete sentiment-related words in a lexical substitution framework. Their strategy to retrieve candidate substitutes is informed by a thesaurus and an emotion dictionary: the first facilitates the extraction of substitutes standing in a specific semantic relation to the input words, the other allows to pick those wo"
2020.socialnlp-1.6,N19-1423,0,0.0107308,"peline, we move on to the question whether our strategies for selection and substitution actually produce variations with the desired emotion (RQ1). In addition, we examine the interaction between the emotion connotation of the paraphrases and their similarity to the inputs (RQ2). These questions are answered in an automatic and a human evaluation. Similarity Score. To keep the semantic similarity as much as possible between the input sentence s and the candidate paraphrase s0 , we calculate the cosine similarity between the respective sentence embeddings, based on the pre-trained BERT model (Devlin et al., 2019), in the implementation provided by Wolf et al. (2019). We conceptualize BERT as a mapping function that takes a sequence of tokens s as input and produces a hidden vector representation for each token. The sentence embeddings r are obtained by averaging over all hidden vectors.4 Therefore, 4.1 Setting We instantiate and compare four model configurations for lexical substitution with different combinations of selection and substitution components. These are designed such that we can compare the selection procedure separately from the substitution component. • Bf+WN: We select isolated words in"
2020.socialnlp-1.6,N18-1169,0,0.224409,"er. 2.2 Style Transfer Most of the recently published approaches to style transfer make use of artificial neural network architectures, in which some latent semantic representation is the backbone of the system. For instance, Prabhumoye et al. (2018) use neural backtranslation to encode the content of text while reducing its stylistic properties, and later decoding it with a specific target style. Gong et al. (2019) evaluate paraphrases regarding their fluency, similarity to the input text and expression of a desired target style, and use this as feedback in a reinforcement learning approach. Li et al. (2018) combine rules with neural methods to explicitly encode attribute markers of the target style. Such transfer methods have been applied to a variety of styles, including sentiment (Shen et al., 2017; Fu et al., 2018; Xu et al., 2018) and a numRelated Work Emotion Analysis In the field of psychology, the two main emotion traditions are categorical models and the strand that focuses on the continuous nature of humans’ affect (Scherer, 2005). Emotions are grouped into categories corresponding to emotion terms, some of which are prototypical experiences shared across cultures. For Ekman (1992), the"
2020.socialnlp-1.6,S07-1050,0,0.0140195,"ation through Lexical Substitution Lexical substitution received some attention independent of style transfer, as it is useful for a range of applications, like paraphrase generation and text summarisation (Dagan et al., 2006). This task, which was formulated by McCarthy and Navigli (2007) and implemented as part of the SemEval-2007 workshop, consists in finding lexical substitutes close in meaning to the original word, given its context within a sentence. The task has mainly been addressed using handcrafted and crowdsourced thesauri, such as WordNet, in order to retrieve lexical substitutes (Martinez et al., 2007; Sinha and Mihalcea, 2014; Kremer et al., 2014; 3.1 Selection This component identifies those tokens from a sentence s = t1 , . . . tn that will be substituted later, and groups them into selections S = {Si }, where each Si consists of tokens, Si = {ti , . . . , tj } (1 ≥ i, j ≤ n). We experiment with two selection strategies, in which the maximal number of tokens 2 A comparison of different context-aware models for lexical substitution can be found in Soler et al. (2019). 43 in one selection is p and the maximal number of selections is q (p, q ∈ N). Distributional Retrieval – Uninformed. In"
2020.socialnlp-1.6,P17-1059,0,0.158601,"With this paper, we propose a non-binary style transfer setting, namely emotion style transfer, in which the target corresponds to one emotion (following Ekman’s fundamental emotions of anger, fear, joy, surprise, sadness, and disgust). Further, this setting is particularly challenging as emotions are on the fence between content and style. To the best of our knowledge, this type of attribute has been explored only to some degree by the unpublished work by Smith et al. (2019), who transfer text towards 20 affect-related styles. Emotions received more attention in conditioned text generation (Ghosh et al., 2017; Huang et al., 2018; Song et al., 2019). To explore the challenges of emotion style transfer (for which we depict an example in Figure 1), we develop a transparent pipeline based on lexical substitution (in contrast to a black-box neural encoder/decoder approach), in which we first (1) select those words that are promising to be changed to adapt the target style, (2) find candidates that may substitute these words, (3) select the best combination regarding content similarity to original We propose the task of emotion style transfer, which is particularly challenging, as emotions (here: anger,"
2020.socialnlp-1.6,S07-1009,0,0.0232223,"lected, producing paraphrases of the input sentence. Importantly, paraphrases are over-generated: at this stage of the pipeline, the output is likely to include sentences that do not express the target emotion. Paraphrases are then scored and re-ranked in the last, objective component, which picks up the “best” output. Paraphrase Generation through Lexical Substitution Lexical substitution received some attention independent of style transfer, as it is useful for a range of applications, like paraphrase generation and text summarisation (Dagan et al., 2006). This task, which was formulated by McCarthy and Navigli (2007) and implemented as part of the SemEval-2007 workshop, consists in finding lexical substitutes close in meaning to the original word, given its context within a sentence. The task has mainly been addressed using handcrafted and crowdsourced thesauri, such as WordNet, in order to retrieve lexical substitutes (Martinez et al., 2007; Sinha and Mihalcea, 2014; Kremer et al., 2014; 3.1 Selection This component identifies those tokens from a sentence s = t1 , . . . tn that will be substituted later, and groups them into selections S = {Si }, where each Si consists of tokens, Si = {ti , . . . , tj }"
2020.socialnlp-1.6,W15-1501,0,0.0197606,"the three objectives of fluency, similarity and the presence of the target style. Moreover, we opt for a more interpretable solution than neural strategies, as we aim at pointing out what leads to a successful transfer, and what, on the contrary, prevents it. 2.3 Biemann, 2013). Moreover, it has been approached with distributional spaces, where the embeddings of the candidate substitutes of a target word can be found, and they can be ranked according to their similarity to the target embedding (Zhao et al., 2007; Hassan et al., 2007), as well as the similarity of their contextual information (Melamud et al., 2015)2 . In the present paper, we follow a similar progression: we retrieve candidates for lexical substitution in WordNet; then, in our more advanced systems, we switch to embedding-based retrieval models. 3 Methods Emotion transfer can be seen as a task in which a sentence s is paraphrased, and the result of this operation exhibits a different emotion than s, specifically, a target emotion. We address emotion transfer with a pipeline in which each unit contributes to the creation of emotionally loaded paraphrases. The pipeline is shown in Figure 2. First is a selection component, which identifies"
2020.socialnlp-1.6,N19-1320,0,0.15293,"everaging Scherer’s component model: emotions are underlied by various dimensions of cognitive appraisal, which can be differently expressed in text and may pose different challenges for style transfer. 2.2 Style Transfer Most of the recently published approaches to style transfer make use of artificial neural network architectures, in which some latent semantic representation is the backbone of the system. For instance, Prabhumoye et al. (2018) use neural backtranslation to encode the content of text while reducing its stylistic properties, and later decoding it with a specific target style. Gong et al. (2019) evaluate paraphrases regarding their fluency, similarity to the input text and expression of a desired target style, and use this as feedback in a reinforcement learning approach. Li et al. (2018) combine rules with neural methods to explicitly encode attribute markers of the target style. Such transfer methods have been applied to a variety of styles, including sentiment (Shen et al., 2017; Fu et al., 2018; Xu et al., 2018) and a numRelated Work Emotion Analysis In the field of psychology, the two main emotion traditions are categorical models and the strand that focuses on the continuous na"
2020.socialnlp-1.6,S12-1033,0,0.208411,"ktheir informed score, leading to i=1 i v = 3 · 25 + 3 · 252 = 1950 variations (with k = 3, p = 2). To inform this method about emotion in the embedding space, we use the NRC emotion dictionary (Mohammad and Turney, 2013). Automatic Evaluation. The main goal of the automatic evaluation is to compare the potential of increasing the probability that the paraphrase contains the target emotion. To achieve that, we compare the four pipeline configurations, but only use the emotion score as the objective function to pick the best candidate. We use 1000 uniformly sampled Tweets from the corpus TEC (Mohammad, 2012). The emotion classification model used for scoring is trained on the same corpus using pretrained Twitter embeddings provided by Baziotis et al. (2018).5 . We use the attention scores obtained from this model for our attention-based selection method. As embedding space for the At+Un and At+In models, we use the same embeddings. As we transfer to the six emotions annotated in TEC, we obtain 6,000 paraphrases with At+Un and At+In 5 4.2 Results RQ1: Whats is the potential of emotion transfer with lexical substitution? We answer RQ1 by inspecting how likely the paraphrases are to contain the desi"
2020.socialnlp-1.6,guerini-etal-2008-valentino,0,0.580781,"is youthful” Variation s0 with eˆ “He is immature” Figure 2: Pipeline model architecture. The selection module marks tokens to substitute, the substitution module retrieves candidates and perform substitution. The objective ranks and scores variations. ber of affect-related variables (Smith et al., 2019). Other examples include text genres (Lee et al., 2019; Jhamtani et al., 2017), romanticism (Li et al., 2018), politeness/offensiveness and formality (Sennrich et al., 2016; Nogueira dos Santos et al., 2018; Wang et al., 2019). One of the earliest methods that targets sentiment is proposed by Guerini et al. (2008), who change, add and delete sentiment-related words in a lexical substitution framework. Their strategy to retrieve candidate substitutes is informed by a thesaurus and an emotion dictionary: the first facilitates the extraction of substitutes standing in a specific semantic relation to the input words, the other allows to pick those words that have the desired valence score. Following this approach, Whitehead and Cavedon (2010) filter out ungrammatical expressions resulting from lexical substitution. Like some works mentioned above, we adopt the view that emotions can be transfered by focusi"
2020.socialnlp-1.6,S18-1001,0,0.0202914,") for Bf+WN, At+WN, and At+Un and flu(s0 ) in addition for At+In. This evaluation is based on 100 randomly sampled Tweets for which we ensure that they are single sentences from TEC. The annotation of emotion connotation and similarity to the original text is then setup as a bestworst-scaling experiment (Louviere et al., 2015), in which each of our two annotators is presented with one paraphrase for each of the four configurations, all for the same emotion (randomly chosen as well). Note that in contrast to best-worst scaling used for annotation as, e.g., in emotion intensity corpus creation (Mohammad et al., 2018), where textual instances are scored, here the instances change from quadruple to quadruple, but the originating configurations remain the same and receive the score. The agreement calculated with Spearman correlation of both annotators is ρ = 1 for the emotion connotation and ρ = 0.8 for semantic similarity. ��� �� Figure 4: Results for the two human annotation trials, combined by model configuration. strategy. Specifically, u = 100 candidates are found based on their semantic similarity to the token to be substituted, and among those, v = 25 tokens are subselected based  i emotionPp on kthe"
2020.socialnlp-1.6,S07-1091,0,0.0253418,"ecific words, we use WordNet as a source of lexical substitutes, and we consider the three objectives of fluency, similarity and the presence of the target style. Moreover, we opt for a more interpretable solution than neural strategies, as we aim at pointing out what leads to a successful transfer, and what, on the contrary, prevents it. 2.3 Biemann, 2013). Moreover, it has been approached with distributional spaces, where the embeddings of the candidate substitutes of a target word can be found, and they can be ranked according to their similarity to the target embedding (Zhao et al., 2007; Hassan et al., 2007), as well as the similarity of their contextual information (Melamud et al., 2015)2 . In the present paper, we follow a similar progression: we retrieve candidates for lexical substitution in WordNet; then, in our more advanced systems, we switch to embedding-based retrieval models. 3 Methods Emotion transfer can be seen as a task in which a sentence s is paraphrased, and the result of this operation exhibits a different emotion than s, specifically, a target emotion. We address emotion transfer with a pipeline in which each unit contributes to the creation of emotionally loaded paraphrases. T"
2020.socialnlp-1.6,W10-0211,0,0.747612,"s/offensiveness and formality (Sennrich et al., 2016; Nogueira dos Santos et al., 2018; Wang et al., 2019). One of the earliest methods that targets sentiment is proposed by Guerini et al. (2008), who change, add and delete sentiment-related words in a lexical substitution framework. Their strategy to retrieve candidate substitutes is informed by a thesaurus and an emotion dictionary: the first facilitates the extraction of substitutes standing in a specific semantic relation to the input words, the other allows to pick those words that have the desired valence score. Following this approach, Whitehead and Cavedon (2010) filter out ungrammatical expressions resulting from lexical substitution. Like some works mentioned above, we adopt the view that emotions can be transfered by focusing on specific words, we use WordNet as a source of lexical substitutes, and we consider the three objectives of fluency, similarity and the presence of the target style. Moreover, we opt for a more interpretable solution than neural strategies, as we aim at pointing out what leads to a successful transfer, and what, on the contrary, prevents it. 2.3 Biemann, 2013). Moreover, it has been approached with distributional spaces, whe"
2020.socialnlp-1.6,P18-1080,0,0.281066,"oman.klinger}@ims.uni-stuttgart.de Abstract textual style transfer, where a stylistic variation is induced on an existing piece of text. The core idea is that texts have a content and a style, and that it is possible to keep the one while changing the other. Past work on style transfer has targeted attributes (or styles) like sentiment (Dai et al., 2019) and tense (Hu et al., 2017), producing a rich literature on deep generative models that disentangle the content and the style of an input text, and subsequently condition generation towards a desired style (Fu et al., 2018; Shen et al., 2017; Prabhumoye et al., 2018). With this paper, we propose a non-binary style transfer setting, namely emotion style transfer, in which the target corresponds to one emotion (following Ekman’s fundamental emotions of anger, fear, joy, surprise, sadness, and disgust). Further, this setting is particularly challenging as emotions are on the fence between content and style. To the best of our knowledge, this type of attribute has been explored only to some degree by the unpublished work by Smith et al. (2019), who transfer text towards 20 affect-related styles. Emotions received more attention in conditioned text generation"
2020.socialnlp-1.6,S07-1036,0,0.0138988,"d by focusing on specific words, we use WordNet as a source of lexical substitutes, and we consider the three objectives of fluency, similarity and the presence of the target style. Moreover, we opt for a more interpretable solution than neural strategies, as we aim at pointing out what leads to a successful transfer, and what, on the contrary, prevents it. 2.3 Biemann, 2013). Moreover, it has been approached with distributional spaces, where the embeddings of the candidate substitutes of a target word can be found, and they can be ranked according to their similarity to the target embedding (Zhao et al., 2007; Hassan et al., 2007), as well as the similarity of their contextual information (Melamud et al., 2015)2 . In the present paper, we follow a similar progression: we retrieve candidates for lexical substitution in WordNet; then, in our more advanced systems, we switch to embedding-based retrieval models. 3 Methods Emotion transfer can be seen as a task in which a sentence s is paraphrased, and the result of this operation exhibits a different emotion than s, specifically, a target emotion. We address emotion transfer with a pipeline in which each unit contributes to the creation of emotionally"
2020.socialnlp-1.6,P18-2031,0,0.0978025,"Selection “He is young” Substitution Objective “He is immature” 2. “He is immature” “He is youthful” 1. “He is youthful” Variation s0 with eˆ “He is immature” Figure 2: Pipeline model architecture. The selection module marks tokens to substitute, the substitution module retrieves candidates and perform substitution. The objective ranks and scores variations. ber of affect-related variables (Smith et al., 2019). Other examples include text genres (Lee et al., 2019; Jhamtani et al., 2017), romanticism (Li et al., 2018), politeness/offensiveness and formality (Sennrich et al., 2016; Nogueira dos Santos et al., 2018; Wang et al., 2019). One of the earliest methods that targets sentiment is proposed by Guerini et al. (2008), who change, add and delete sentiment-related words in a lexical substitution framework. Their strategy to retrieve candidate substitutes is informed by a thesaurus and an emotion dictionary: the first facilitates the extraction of substitutes standing in a specific semantic relation to the input words, the other allows to pick those words that have the desired valence score. Following this approach, Whitehead and Cavedon (2010) filter out ungrammatical expressions resulting from lexic"
2020.socialnlp-1.6,N16-1005,0,0.0309986,"rget Emotion eˆ “He is young” Anger Selection “He is young” Substitution Objective “He is immature” 2. “He is immature” “He is youthful” 1. “He is youthful” Variation s0 with eˆ “He is immature” Figure 2: Pipeline model architecture. The selection module marks tokens to substitute, the substitution module retrieves candidates and perform substitution. The objective ranks and scores variations. ber of affect-related variables (Smith et al., 2019). Other examples include text genres (Lee et al., 2019; Jhamtani et al., 2017), romanticism (Li et al., 2018), politeness/offensiveness and formality (Sennrich et al., 2016; Nogueira dos Santos et al., 2018; Wang et al., 2019). One of the earliest methods that targets sentiment is proposed by Guerini et al. (2008), who change, add and delete sentiment-related words in a lexical substitution framework. Their strategy to retrieve candidate substitutes is informed by a thesaurus and an emotion dictionary: the first facilitates the extraction of substitutes standing in a specific semantic relation to the input words, the other allows to pick those words that have the desired valence score. Following this approach, Whitehead and Cavedon (2010) filter out ungrammatica"
2020.socialnlp-1.6,W19-0423,0,0.0146853,"y been addressed using handcrafted and crowdsourced thesauri, such as WordNet, in order to retrieve lexical substitutes (Martinez et al., 2007; Sinha and Mihalcea, 2014; Kremer et al., 2014; 3.1 Selection This component identifies those tokens from a sentence s = t1 , . . . tn that will be substituted later, and groups them into selections S = {Si }, where each Si consists of tokens, Si = {ti , . . . , tj } (1 ≥ i, j ≤ n). We experiment with two selection strategies, in which the maximal number of tokens 2 A comparison of different context-aware models for lexical substitution can be found in Soler et al. (2019). 43 in one selection is p and the maximal number of selections is q (p, q ∈ N). Distributional Retrieval – Uninformed. In the “Distributional Retrieval – Uninformed” setting, we retrieve u substitution candidates based on the cosine similarity in a vector space. To build the vector space, we employ pre-trained word embeddings.3 They are the same that are used for training the emotion classifier responsible for retrieving attention scores in the selection stage. Brute-Force. This baseline selection strategy picks each token separately, therefore, we obtain n selections, one for each token, i.e"
2020.socialnlp-1.6,P19-1359,0,0.145767,"y style transfer setting, namely emotion style transfer, in which the target corresponds to one emotion (following Ekman’s fundamental emotions of anger, fear, joy, surprise, sadness, and disgust). Further, this setting is particularly challenging as emotions are on the fence between content and style. To the best of our knowledge, this type of attribute has been explored only to some degree by the unpublished work by Smith et al. (2019), who transfer text towards 20 affect-related styles. Emotions received more attention in conditioned text generation (Ghosh et al., 2017; Huang et al., 2018; Song et al., 2019). To explore the challenges of emotion style transfer (for which we depict an example in Figure 1), we develop a transparent pipeline based on lexical substitution (in contrast to a black-box neural encoder/decoder approach), in which we first (1) select those words that are promising to be changed to adapt the target style, (2) find candidates that may substitute these words, (3) select the best combination regarding content similarity to original We propose the task of emotion style transfer, which is particularly challenging, as emotions (here: anger, disgust, fear, joy, sadness, surprise)"
2020.socialnlp-1.6,D19-1365,0,0.132727,"g” Substitution Objective “He is immature” 2. “He is immature” “He is youthful” 1. “He is youthful” Variation s0 with eˆ “He is immature” Figure 2: Pipeline model architecture. The selection module marks tokens to substitute, the substitution module retrieves candidates and perform substitution. The objective ranks and scores variations. ber of affect-related variables (Smith et al., 2019). Other examples include text genres (Lee et al., 2019; Jhamtani et al., 2017), romanticism (Li et al., 2018), politeness/offensiveness and formality (Sennrich et al., 2016; Nogueira dos Santos et al., 2018; Wang et al., 2019). One of the earliest methods that targets sentiment is proposed by Guerini et al. (2008), who change, add and delete sentiment-related words in a lexical substitution framework. Their strategy to retrieve candidate substitutes is informed by a thesaurus and an emotion dictionary: the first facilitates the extraction of substitutes standing in a specific semantic relation to the input words, the other allows to pick those words that have the desired valence score. Following this approach, Whitehead and Cavedon (2010) filter out ungrammatical expressions resulting from lexical substitution. Lik"
2020.socialnlp-1.6,N18-2008,0,\N,Missing
2020.socialnlp-1.6,W18-6206,1,\N,Missing
2020.starsem-1.7,D14-1162,0,0.09115,"y a token labeled j). At prediction time, the most likely sequence is chosen with the Viterbi algorithm (Viterbi, 1967). With these components, we can now put together the actual models which we use for stimulus detection. We compare three different models, one for token sequence labeling (SL) and two for clause classification (CC). The model architectures are illustrated in Figure 4. Token Sequence Labeling (SL). In this model, we formulate emotion stimulus detection as token sequence labeling with the IOB alphabet (Ramshaw and Marcus, 1995). As embeddings, we use wordlevel GloVe embeddings (Pennington et al., 2014). The sequence-to-sequence architecture comprises a bidirectional LSTM, an attention layer and the CRF output layer. Independent Clause Classification (ICC). This labeled as clause-type (Bies et al., 1995). We then join the segments until convergence heuristically based on punctuation (Line 12). We illustrate the algorithm in the example in Figure 3. 2.2 N Stimulus Detection Our goal is to compare sequence labeling and clause classification. To attribute the performance of the model to the formulation of the task, we keep the differences between the models at a minimum. We therefore first disc"
2020.starsem-1.7,P18-1249,0,0.0169438,"e evaluation. We explain these steps in the following subsections. 2.1 Clause Extraction The clause classification methods rely on representing an instance as a sequence of clauses. Clauses in English grammar are defined as the smallest grammatical structures that contain a subject and a predicate, and can express a complete proposition (Kroeger, 2005). We show our algorithm to detect clauses in Algorithm 1. To mark the segments that would potentially approximate clauses, we rely on the constituency parse tree of the token sequence (Line 2). For that reason, we use the Berkeley Neural Parser (Kitaev and Klein, 2018). As illustrated by Feng et al. (2012) and Tafreshi and Diab (2018) we also do that by segmenting the constituency parse tree of the instance (Line 9) at the borders of constituents 59 S Algorithm 1: Clause Extraction 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 Input: text Output: Clauses c t ← tokenize(text) tree ← parse(t) // constituency parse gaps ← {0, |t|} // potential clause bounds segments ← ∅ // initial. set of segments foreach node n in tree do if label(n) ∈ S, SBAR, SBARQ, INV, SQ ` ← first token leaf that n governs r ← last token leaf that n governs gaps = gaps ∪ {idx` ,"
2020.starsem-1.7,P13-2147,1,0.747278,"Pfaffenwaldring 5b, 70569 Stuttgart, Germany {laura.oberlaender,roman.klinger}@ims.uni-stuttgart.de Abstract is expressed in a text, it does not detect the textual unit, which reveals why the emotion has been developed. For instance, in the example “Paul is angry because he lost his wallet.” it remains hidden that lost his wallet is the reason for experiencing the emotion of anger. This stimulus, e.g., an event description, a person, a state of affairs, or an object enables deeper insight, similar to targeted or aspectbased sentiment analysis (Jakob and Gurevych, 2010; Yang and Cardie, 2013; Klinger and Cimiano, 2013; Pontiki et al., 2015, 2016, i.a.). This situation is dissatisfying for (at least) two reasons. First, detecting the emotions expressed in social media and their stimuli might play a role in understanding why different social groups change their attitude towards specific events and could help recognize specific issues in society. Second, understanding the relationship between stimuli and emotions is also compelling from a psychological point of view, given that emotions are commonly considered responses to relevant situations (Scherer, 2005). Models which tackle the task of detecting the stim"
2020.starsem-1.7,S15-2082,0,0.203718,"y evaluated in two different ways and have not been compared before, to the best of our knowledge. Therefore, it remains unclear which task formulation is more appropriate for English. Further, the most recent approaches have been evaluated only on Mandarin Chinese, with the only exception being the EmotionCauseAnalysis dataset being considered by Fan et al. (2019), but not in comparison to token sequence labeling. No other English emotion stimulus data sets have been tackled with clause classification methods. We hypothesize that clauses are not appropriate units for English, as Ghazi et al. (2015) already noted that: “such granularity [is] too large to be considered an emotion stimulus in English”. A similar argument has been brought up during the development of semantic role labeling methods: Punyakanok et al. (2008) stated that “argument[s] may span over different parts of a sentence”. Our contributions are as follows: (1) we develop an integrated framework that represents different formulations for the emotion stimulus detection task and evaluate these on four available English datasets; (2) as part of this framework, we propose a clause detector for English which is required to per"
2020.starsem-1.7,lee-etal-2010-emotion,0,0.494613,"sed and compared in previous work for the available English and Chinese sets. We see that the methods applied on the Chinese sets are not evaluated on the English sets. 2019). Therefore, we use a comparable model that is grounded on the same concept of a hierarchical setup with LSTMs as encoders. Further, there is a strand of research which jointly predicts the clause that contains the emotion stimulus together with its emotion cue (Wei et al., 2020; Fan et al., 2020). However, the comparability of methods across data sets has been limited in previous work, as Table 8 in the appendices shows. Lee et al. (2010) firstly investigated the interactions between emotions and the corresponding stimuli from a linguistic perspective. They publish a list of linguistic cues that help in identifying emotion stimuli and develop a rule-based approach. Chen et al. (2010) build on top of their work to develop a machine learning method. Li and Xu (2014) implement a rule-based system to detect the stimuli in Weibo posts and further inform an emotion classifier with the output of this system. Other approaches to develop rules include manual strategies (Gao et al., 2015), bootstrapping (Yada et al., 2017) and the use o"
2020.starsem-1.7,J08-2005,0,0.0111976,"nt approaches have been evaluated only on Mandarin Chinese, with the only exception being the EmotionCauseAnalysis dataset being considered by Fan et al. (2019), but not in comparison to token sequence labeling. No other English emotion stimulus data sets have been tackled with clause classification methods. We hypothesize that clauses are not appropriate units for English, as Ghazi et al. (2015) already noted that: “such granularity [is] too large to be considered an emotion stimulus in English”. A similar argument has been brought up during the development of semantic role labeling methods: Punyakanok et al. (2008) stated that “argument[s] may span over different parts of a sentence”. Our contributions are as follows: (1) we develop an integrated framework that represents different formulations for the emotion stimulus detection task and evaluate these on four available English datasets; (2) as part of this framework, we propose a clause detector for English which is required to perform stimulus detection via clause classification in a real-world setting; (3) show that token 2 An Integrated Framework for Stimulus Detection The two approaches for open-domain stimulus detection, namely, clause classificat"
2020.starsem-1.7,W95-0107,0,0.609653,".e., T [i, j] represents the probability that a token labeled i is followed by a token labeled j). At prediction time, the most likely sequence is chosen with the Viterbi algorithm (Viterbi, 1967). With these components, we can now put together the actual models which we use for stimulus detection. We compare three different models, one for token sequence labeling (SL) and two for clause classification (CC). The model architectures are illustrated in Figure 4. Token Sequence Labeling (SL). In this model, we formulate emotion stimulus detection as token sequence labeling with the IOB alphabet (Ramshaw and Marcus, 1995). As embeddings, we use wordlevel GloVe embeddings (Pennington et al., 2014). The sequence-to-sequence architecture comprises a bidirectional LSTM, an attention layer and the CRF output layer. Independent Clause Classification (ICC). This labeled as clause-type (Bies et al., 1995). We then join the segments until convergence heuristically based on punctuation (Line 12). We illustrate the algorithm in the example in Figure 3. 2.2 N Stimulus Detection Our goal is to compare sequence labeling and clause classification. To attribute the performance of the model to the formulation of the task, we k"
2020.starsem-1.7,W11-1720,0,0.612933,"Missing"
2020.starsem-1.7,D18-1506,0,0.135693,"a stimulus mention. Another step is to investigate if the emotion stimulus and the emotion category classification benefit from joint modeling in English as it has been shown for Mandarin (Chen et al., 2018). 6 All recently published state-of-the-art methods for the task of emotion stimulus detection via clause classification are evaluated on the Mandarin data by Gui et al. (2016). They include multi-kernel learning (Gui et al., 2016) and long short-term memory networks (LSTM) (Cheng et al., 2017). Gui et al. (2017) propose a convolutional multipleslot deep memory network (ConvMS-Memnet), and Li et al. (2018) a co-attention neural network model, which encodes the clauses with a coattention based bi-directional long short-term memory into high-level input representations, which are further passed into a convolutional layer. Ding et al. (2019) proposed an architecture with components for “position augmented embedding” and “dynamic global label” which takes the relative position of the stimuli to the emotion keywords and use the predictions of previous clauses as features for predicting subsequent clauses. Xia et al. (2019) integrate the relative position of stimuli and evaluate a transformer-based m"
2020.starsem-1.7,2020.acl-main.289,0,0.562439,"t al. (2017) annotate Japanese sentences on newspaper articles, web news articles, and Q&A sites. Table 8 in Appendices shows which corpora and methods have been used and compared in previous work for the available English and Chinese sets. We see that the methods applied on the Chinese sets are not evaluated on the English sets. 2019). Therefore, we use a comparable model that is grounded on the same concept of a hierarchical setup with LSTMs as encoders. Further, there is a strand of research which jointly predicts the clause that contains the emotion stimulus together with its emotion cue (Wei et al., 2020; Fan et al., 2020). However, the comparability of methods across data sets has been limited in previous work, as Table 8 in the appendices shows. Lee et al. (2010) firstly investigated the interactions between emotions and the corresponding stimuli from a linguistic perspective. They publish a list of linguistic cues that help in identifying emotion stimuli and develop a rule-based approach. Chen et al. (2010) build on top of their work to develop a machine learning method. Li and Xu (2014) implement a rule-based system to detect the stimuli in Weibo posts and further inform an emotion classi"
2020.starsem-1.7,P19-1096,0,0.300064,"multiple clauses. After we obtain the final clause representation for each clause, we perform sequence labeling with a CRF on the clause level. The training objective is to minimize the negative log-likelihood loss across all clauses. This implementation follows the architecture by Xia et al. (2019), with the change of the upper layer, which is, in our case, an LSTM clause encoder and not a transformer, to keep the architecture comparable across our different formulations. Therefore, this is comparable to all other hierarchical models proposed for the task (Ding et al., 2019; Xu et al., 2019; Xia and Ding, 2019). 2.3 Experiments and Results 3.1 Data Sets We base our experiments on four data sets.1 For each data set, we report the size, the number of stimulus annotations and statistics for tokens and clauses in Table 1. EmotionStimulus. This data set proposed by Ghazi et al. (2015) is constructed based on FrameNet’s emotion-directed frame.2 The authors used FrameNet’s annotated data for 173 emotion lexical units, grouped the lexical units into seven basic emotions using their synonyms and built a dataset manually annotated with both the emotion stimulus and the emotion. The corpus consists of 820 sent"
2020.starsem-1.7,P13-1161,0,0.0181373,"University of Stuttgart Pfaffenwaldring 5b, 70569 Stuttgart, Germany {laura.oberlaender,roman.klinger}@ims.uni-stuttgart.de Abstract is expressed in a text, it does not detect the textual unit, which reveals why the emotion has been developed. For instance, in the example “Paul is angry because he lost his wallet.” it remains hidden that lost his wallet is the reason for experiencing the emotion of anger. This stimulus, e.g., an event description, a person, a state of affairs, or an object enables deeper insight, similar to targeted or aspectbased sentiment analysis (Jakob and Gurevych, 2010; Yang and Cardie, 2013; Klinger and Cimiano, 2013; Pontiki et al., 2015, 2016, i.a.). This situation is dissatisfying for (at least) two reasons. First, detecting the emotions expressed in social media and their stimuli might play a role in understanding why different social groups change their attitude towards specific events and could help recognize specific issues in society. Second, understanding the relationship between stimuli and emotions is also compelling from a psychological point of view, given that emotions are commonly considered responses to relevant situations (Scherer, 2005). Models which tackle the"
2021.bionlp-1.15,W16-2801,0,0.322441,", 2017). The setting as a sequence labeling task has been tackled on Wikipedia (Levy et al., 2014), on Twitter, and on news articles (Goudas et al., 2014; Sardianos et al., 2015). One common characteristic in most work on automatic claim detection is the focus on relatively formal text. Social media, like tweets, can be considered a more challenging text type, which despite this aspect, received considerable attention, also beyond classification or token sequence labeling. Bosc et al. (2016a) detect relations between arguments, Dusmanu et al. (2017) identify factual or opinionated tweets, and Addawood and Bashir (2016) further classify the type of premise which accompanies the claim. Ouertatani et al. (2020) combine aspects of sentiment detection, opinion, and argument mining in a pipeline to analyze argumentative tweets more comprehensively. Ma et al. (2018) specifically focus on the claim detection task in tweets, and present an approach to retrieve Twitter posts that contain argumentative claims about debatable political topics. To the best of our knowledge, detecting biomedical claims in tweets has not been approached yet. Biomedical argument mining, also for other text types, is generally still limited"
2021.bionlp-1.15,W14-2109,0,0.322169,"ty as well as in their conceptualization of claims, the claim element is widely considered the core component of an argument (Daxenberger et al., 2017). 3 https://go.drugbank.com/. At the time of creating the search term list, COVID-19 was not included in DrugBank. Instead, medications which were under investigation at the time of compiling this list as outlined on the WHO website were included for Sars-CoV-2 in this category: https://www. who.int/emergencies/diseases/novel-coronavirus-2019/ global-research-on-novel-coronavirus-2019-ncov/ solidarity-clinical-trial-for-covid-19-treatments. 133 Aharoni et al. (2014) suggest a framework in which an argument consists of two main components: a claim and premises. We follow Stab and Gurevych (2017) and define the claim as the argumentative component in which the speaker or writer expresses the central, controversial conclusion of their argument. This claim is presented as if it were true even though objectively it can be true or false (Mochales and Ieven, 2009). The premise which is considered the second part of an argument includes all elements that are used either to substantiate or disprove the claim. Arguments can contain multiple premises to justify the"
2021.bionlp-1.15,W15-3817,0,0.0269659,"resent an approach to retrieve Twitter posts that contain argumentative claims about debatable political topics. To the best of our knowledge, detecting biomedical claims in tweets has not been approached yet. Biomedical argument mining, also for other text types, is generally still limited. The work by Shi and Bei (2019) is one of the few exceptions that target this challenge and propose a pipeline to extract health-related claims from headlines of healththemed news articles. The majority of other argument mining approaches for the biomedical domain focus on research literature (Blake, 2010; Alamri and Stevenson, 2015; Alamri and Stevensony, 2015; Achakulvisut et al., 2019; Mayer et al., 2020). Argumentation mining covers a variety of different domains, text, and discourse types. This includes online content, for instance Wikipedia (Levy et al., 2014; Roitman et al., 2016; Lippi and Torroni, 2015), but also more interaction-driven platforms, like fora. As an example, Habernal and Gurevych (2017) extract argument structures from blogs and forum posts, including comments. Apart from that, Twitter is generally a popular text source (Bosc et al., 2016a; Dusmanu et al., 2017). Argument mining is also applied to"
2021.bionlp-1.15,Q17-1010,0,0.00624909,"n claim and non-claim. Multiclass Pipeline. A first classifier learns to discriminate between claims and non-claims (as in Binary). Each tweet that is classified as claim is further separated into implicit or explicit with another binary classifier. The secondary classifier is trained on gold data (not on predictions of the first model in the pipeline). 4.2 Model Architecture For each of the classification tasks (binary/multiclass, steps in the pipeline), we use a set of standard text classification methods which we compare. The first three models (NB, LG, BiLSTM) use 50-dimensional FastText (Bojanowski et al., 2017) embeddings trained on the Common Crawl corpus (600 billion tokens) as input6 . NB. We use a (Gaussian) naive Bayes with an average vector of the token embeddings as input. LG. We use a logistic regression classifier with the same features as in NB. BiLSTM. As a classifier which can consider contextual information and makes use of pretrained embeddings, we use a bidirectional long short-term memory network (Hochreiter and Schmidhuber, 1997) with 75 LSTM units followed by the output layer (sigmoid for binary classification, softmax for multiclass). BERT. We use the pretrained BERT (Devlin et al"
2021.bionlp-1.15,L16-1200,0,0.413052,"Missing"
2021.bionlp-1.15,D17-1218,0,0.119492,"a for the corpus was collected in June/July 2020 using Twitter’s API1 which offers a keywordbased retrieval for tweets. Table 1 provides a sample of the search terms we used.2 For each of the 1 https://developer.twitter.com/en/docs/twitter-api The full list of search terms (1771 queries in total) is available in the supplementary material. 2 3.2.1 Annotation Conceptual Definition While there are different schemes and models of argumentative structure varying in complexity as well as in their conceptualization of claims, the claim element is widely considered the core component of an argument (Daxenberger et al., 2017). 3 https://go.drugbank.com/. At the time of creating the search term list, COVID-19 was not included in DrugBank. Instead, medications which were under investigation at the time of compiling this list as outlined on the WHO website were included for Sars-CoV-2 in this category: https://www. who.int/emergencies/diseases/novel-coronavirus-2019/ global-research-on-novel-coronavirus-2019-ncov/ solidarity-clinical-trial-for-covid-19-treatments. 133 Aharoni et al. (2014) suggest a framework in which an argument consists of two main components: a claim and premises. We follow Stab and Gurevych (2017"
2021.bionlp-1.15,N19-1423,0,0.17103,"t al., 2017) embeddings trained on the Common Crawl corpus (600 billion tokens) as input6 . NB. We use a (Gaussian) naive Bayes with an average vector of the token embeddings as input. LG. We use a logistic regression classifier with the same features as in NB. BiLSTM. As a classifier which can consider contextual information and makes use of pretrained embeddings, we use a bidirectional long short-term memory network (Hochreiter and Schmidhuber, 1997) with 75 LSTM units followed by the output layer (sigmoid for binary classification, softmax for multiclass). BERT. We use the pretrained BERT (Devlin et al., 2019) base model7 and fine-tune it using the claim tweet corpus. 5 Experiments 5.1 Claim Detection With the first experiment we explore how reliably we can detect claim tweets in our corpus and how well the two different claim types (explicit vs. implicit claim tweets) can be distinguished. We use each model mentioned in Section 4.2 in each setting described in Section 4.1. We evaluate each classifier in a binary or (where applicable) in a multi-class setting, to understand if splitting the claim category into its subcomponents improves the claim prediction overall. Multiclass. A trained classifier"
2021.bionlp-1.15,D17-1245,0,0.298262,"s for tasks like factchecking or hypotheses generation. We show an example of a tweet with a claim in Figure 1. Claims are widely considered the conclusive and therefore central part of an argument (Lippi and Torroni, 2015; Stab and Gurevych, 2017), consequently making it the most valuable information to extract. Argument mining and claim detection has been explored for texts like legal documents, Wikipedia articles, essays (Moens et al., 2007; Levy et al., 2014; Stab and Gurevych, 2017, i.a.), social media and web content (Goudas et al., 2014; Habernal and Gurevych, 2017; Bosc et al., 2016a; Dusmanu et al., 2017, i.a.). It has also been applied to scientific biomedical publications (Achakulvisut et al., 2019; Mayer et al., 2020, i.a.), but biomedi1 Introduction cal arguments as they occur on social media, and particularly Twitter, have not been analyzed yet. Social media platforms like Twitter contain vast With this paper, we fill this gap and explore amounts of valuable and novel information, and biomedical aspects are no exception (Correia et al., claim detection for tweets discussing biomedical topics, particularly tweets about COVID-19, the 2020). Doctors share insights from their everyday life,"
2021.bionlp-1.15,P17-1002,0,0.0743711,"017) extract argument structures from blogs and forum posts, including comments. Apart from that, Twitter is generally a popular text source (Bosc et al., 2016a; Dusmanu et al., 2017). Argument mining is also applied to professionally generated content, for instance news (Goudas et al., 2014; Sardianos et al., 2015) and legal or political documents (Moens et al., 2007; Palau and Moens, 2009; Mochales and Moens, 2011; Florou et al., 2013). Another domain of interest are persuasive essays, which we also use in a cross-domain study in this paper (Lippi and Torroni, 2015; Stab and Gurevych, 2017; Eger et al., 2017). Existing approaches differ with regards to which tasks in the broader argument mining pipeline they address. While some focus on the detection of arguments (Moens et al., 2007; Florou et al., 2013; Levy et al., 2014; Bosc et al., 2016a; Dusmanu et al., 2017; Habernal and Gurevych, 2017), others analyze the relational aspects between argument components (Mochales and Moens, 2011; Stab and Gurevych, 2017; Eger et al., 2017). While most approaches cater to a specific domain or text genre, Stab et al. (2018) argue that domain-focused, specialized systems do not generalize to broader applications"
2021.bionlp-1.15,W13-2707,0,0.0543897,"stance Wikipedia (Levy et al., 2014; Roitman et al., 2016; Lippi and Torroni, 2015), but also more interaction-driven platforms, like fora. As an example, Habernal and Gurevych (2017) extract argument structures from blogs and forum posts, including comments. Apart from that, Twitter is generally a popular text source (Bosc et al., 2016a; Dusmanu et al., 2017). Argument mining is also applied to professionally generated content, for instance news (Goudas et al., 2014; Sardianos et al., 2015) and legal or political documents (Moens et al., 2007; Palau and Moens, 2009; Mochales and Moens, 2011; Florou et al., 2013). Another domain of interest are persuasive essays, which we also use in a cross-domain study in this paper (Lippi and Torroni, 2015; Stab and Gurevych, 2017; Eger et al., 2017). Existing approaches differ with regards to which tasks in the broader argument mining pipeline they address. While some focus on the detection of arguments (Moens et al., 2007; Florou et al., 2013; Levy et al., 2014; Bosc et al., 2016a; Dusmanu et al., 2017; Habernal and Gurevych, 2017), others analyze the relational aspects between argument components (Mochales and Moens, 2011; Stab and Gurevych, 2017; Eger et al., 2"
2021.bionlp-1.15,L18-1708,0,0.0242382,"cystic fibrosis AND treated, cystic fibrosis AND heal depression AND cure, depression AND treatment Drugs Hydroxychloroquine, Kaletra, Remdesivir M-M-R II, Priorix, ProQuad Orkambi, Trikafta, Tezacaftor Alprazolam, Buspirone, Xanax Table 1: Examples of the four categories of search terms used to retrieve tweets about COVID-19, the measles, cystic fibrosis, and depression via the Twitter API. ics community. One focus is on the automatic extraction of information from life science articles, including entity recognition, e.g., of diseases, drug names, protein and gene names (Habibi et al., 2017; Giorgi and Bader, 2018; Lee et al., 2019, i.a.) or relations between those (Lamurias et al., 2019; Sousa et al., 2021; Lin et al., 2019, i.a.). Biomedical text mining methods have also been applied to social media texts and web content (Wegrzyn-Wolska et al., 2011; Yang et al., 2016; Sullivan et al., 2016, i.a.). One focus is on the analysis of Twitter with regards to pharmacovigilance. Other topics include the extraction of adverse drug reactions (Nikfarjam et al., 2015; Cocos et al., 2017), monitoring public health (Paul and Dredze, 2012; Choudhury et al., 2013; Sarker et al., 2016), and detecting personal health"
2021.bionlp-1.15,J17-1004,0,0.332732,"ment mining and a prerequisite in further analysis for tasks like factchecking or hypotheses generation. We show an example of a tweet with a claim in Figure 1. Claims are widely considered the conclusive and therefore central part of an argument (Lippi and Torroni, 2015; Stab and Gurevych, 2017), consequently making it the most valuable information to extract. Argument mining and claim detection has been explored for texts like legal documents, Wikipedia articles, essays (Moens et al., 2007; Levy et al., 2014; Stab and Gurevych, 2017, i.a.), social media and web content (Goudas et al., 2014; Habernal and Gurevych, 2017; Bosc et al., 2016a; Dusmanu et al., 2017, i.a.). It has also been applied to scientific biomedical publications (Achakulvisut et al., 2019; Mayer et al., 2020, i.a.), but biomedi1 Introduction cal arguments as they occur on social media, and particularly Twitter, have not been analyzed yet. Social media platforms like Twitter contain vast With this paper, we fill this gap and explore amounts of valuable and novel information, and biomedical aspects are no exception (Correia et al., claim detection for tweets discussing biomedical topics, particularly tweets about COVID-19, the 2020). Doctors"
2021.bionlp-1.15,2020.nlpcovid19-2.11,0,0.0866328,"Missing"
2021.bionlp-1.15,C14-1141,0,0.317426,"reference to a reliable resource is important. The task of detecting such claims is essential in argument mining and a prerequisite in further analysis for tasks like factchecking or hypotheses generation. We show an example of a tweet with a claim in Figure 1. Claims are widely considered the conclusive and therefore central part of an argument (Lippi and Torroni, 2015; Stab and Gurevych, 2017), consequently making it the most valuable information to extract. Argument mining and claim detection has been explored for texts like legal documents, Wikipedia articles, essays (Moens et al., 2007; Levy et al., 2014; Stab and Gurevych, 2017, i.a.), social media and web content (Goudas et al., 2014; Habernal and Gurevych, 2017; Bosc et al., 2016a; Dusmanu et al., 2017, i.a.). It has also been applied to scientific biomedical publications (Achakulvisut et al., 2019; Mayer et al., 2020, i.a.), but biomedi1 Introduction cal arguments as they occur on social media, and particularly Twitter, have not been analyzed yet. Social media platforms like Twitter contain vast With this paper, we fill this gap and explore amounts of valuable and novel information, and biomedical aspects are no exception (Correia et al.,"
2021.bionlp-1.15,W17-5110,0,0.0288698,"argument mining field as well as the area of biomedical text mining. 2.1 Argumentation Mining 2.2 Claim Detection Claim detection is a central task in argumentation mining. It can be framed as a classification (Does a document/sentence contain a claim?) or as sequence labeling (Which tokens make up the claim?). The setting as classification has been explored, inter alia, as a retrieval task of online comments made by public stakeholders on pending governmental regulations (Kwon et al., 2007), for sentence detection in essays, (Lippi and Torroni, 2015), and for Wikipedia (Roitman et al., 2016; Levy et al., 2017). The setting as a sequence labeling task has been tackled on Wikipedia (Levy et al., 2014), on Twitter, and on news articles (Goudas et al., 2014; Sardianos et al., 2015). One common characteristic in most work on automatic claim detection is the focus on relatively formal text. Social media, like tweets, can be considered a more challenging text type, which despite this aspect, received considerable attention, also beyond classification or token sequence labeling. Bosc et al. (2016a) detect relations between arguments, Dusmanu et al. (2017) identify factual or opinionated tweets, and Addawoo"
2021.bionlp-1.15,W19-1908,0,0.0312656,"quine, Kaletra, Remdesivir M-M-R II, Priorix, ProQuad Orkambi, Trikafta, Tezacaftor Alprazolam, Buspirone, Xanax Table 1: Examples of the four categories of search terms used to retrieve tweets about COVID-19, the measles, cystic fibrosis, and depression via the Twitter API. ics community. One focus is on the automatic extraction of information from life science articles, including entity recognition, e.g., of diseases, drug names, protein and gene names (Habibi et al., 2017; Giorgi and Bader, 2018; Lee et al., 2019, i.a.) or relations between those (Lamurias et al., 2019; Sousa et al., 2021; Lin et al., 2019, i.a.). Biomedical text mining methods have also been applied to social media texts and web content (Wegrzyn-Wolska et al., 2011; Yang et al., 2016; Sullivan et al., 2016, i.a.). One focus is on the analysis of Twitter with regards to pharmacovigilance. Other topics include the extraction of adverse drug reactions (Nikfarjam et al., 2015; Cocos et al., 2017), monitoring public health (Paul and Dredze, 2012; Choudhury et al., 2013; Sarker et al., 2016), and detecting personal health mentions (Yin et al., 2015; Karisani and Agichtein, 2018). A small number of studies looked into the comparison"
2021.bionlp-1.15,C18-2010,0,0.119015,"focus on relatively formal text. Social media, like tweets, can be considered a more challenging text type, which despite this aspect, received considerable attention, also beyond classification or token sequence labeling. Bosc et al. (2016a) detect relations between arguments, Dusmanu et al. (2017) identify factual or opinionated tweets, and Addawood and Bashir (2016) further classify the type of premise which accompanies the claim. Ouertatani et al. (2020) combine aspects of sentiment detection, opinion, and argument mining in a pipeline to analyze argumentative tweets more comprehensively. Ma et al. (2018) specifically focus on the claim detection task in tweets, and present an approach to retrieve Twitter posts that contain argumentative claims about debatable political topics. To the best of our knowledge, detecting biomedical claims in tweets has not been approached yet. Biomedical argument mining, also for other text types, is generally still limited. The work by Shi and Bei (2019) is one of the few exceptions that target this challenge and propose a pipeline to extract health-related claims from headlines of healththemed news articles. The majority of other argument mining approaches for t"
2021.bionlp-1.15,2020.lrec-1.759,0,0.0234565,"016, i.a.). One focus is on the analysis of Twitter with regards to pharmacovigilance. Other topics include the extraction of adverse drug reactions (Nikfarjam et al., 2015; Cocos et al., 2017), monitoring public health (Paul and Dredze, 2012; Choudhury et al., 2013; Sarker et al., 2016), and detecting personal health mentions (Yin et al., 2015; Karisani and Agichtein, 2018). A small number of studies looked into the comparison of biomedical information in social media and scientific text: Thorne and Klinger (2018) analyze quantitatively how disease names are referred to across these domains. Seiffe et al. (2020) analyze laypersons’ medical vocabulary. medical topics, we sample English tweets from keywords and phrases from four different query categories. This includes (1) the name of the disease as well as the respective hashtag for each topic, e.g., depression and #depression, (2) topical hashtags like #vaccineswork, (3) combinations of the disease name with words like cure, treatment or therapy as well as their respective verb forms, and (4) a list of medications, products, and product brand names from the pharmaceutical database DrugBank3 . When querying the tweets, we exclude retweets by using th"
2021.bionlp-1.15,2020.emnlp-main.609,0,0.047608,"Missing"
2021.bionlp-1.15,J17-3005,0,0.254619,"e same time, unproven claims or even intentionally spread misinformation might also do great harm. Therefore, contextualizing a social media message and investigating if a statement is debated or can actually be proven with a reference to a reliable resource is important. The task of detecting such claims is essential in argument mining and a prerequisite in further analysis for tasks like factchecking or hypotheses generation. We show an example of a tweet with a claim in Figure 1. Claims are widely considered the conclusive and therefore central part of an argument (Lippi and Torroni, 2015; Stab and Gurevych, 2017), consequently making it the most valuable information to extract. Argument mining and claim detection has been explored for texts like legal documents, Wikipedia articles, essays (Moens et al., 2007; Levy et al., 2014; Stab and Gurevych, 2017, i.a.), social media and web content (Goudas et al., 2014; Habernal and Gurevych, 2017; Bosc et al., 2016a; Dusmanu et al., 2017, i.a.). It has also been applied to scientific biomedical publications (Achakulvisut et al., 2019; Mayer et al., 2020, i.a.), but biomedi1 Introduction cal arguments as they occur on social media, and particularly Twitter, have"
2021.bionlp-1.15,D18-1402,0,0.0118091,"in a cross-domain study in this paper (Lippi and Torroni, 2015; Stab and Gurevych, 2017; Eger et al., 2017). Existing approaches differ with regards to which tasks in the broader argument mining pipeline they address. While some focus on the detection of arguments (Moens et al., 2007; Florou et al., 2013; Levy et al., 2014; Bosc et al., 2016a; Dusmanu et al., 2017; Habernal and Gurevych, 2017), others analyze the relational aspects between argument components (Mochales and Moens, 2011; Stab and Gurevych, 2017; Eger et al., 2017). While most approaches cater to a specific domain or text genre, Stab et al. (2018) argue that domain-focused, specialized systems do not generalize to broader applications such as argument search in texts. In line with that, Daxenberger 2.3 Biomedical Text Mining et al. (2017) present a comparative study on crossdomain claim detection. They observe that diverse Biomedical natural language processing (BioNLP) training data leads to a more robust model perfor- is a field in computational linguistics which also mance in unknown domains. receives substantial attention from the bioinformat132 Query category Disease Names COVID-19, #COVID-19 measles, #measles cystic fibrosis, #cy"
2021.bionlp-1.15,2020.emnlp-demos.6,0,0.122674,"Missing"
2021.konvens-1.5,S12-1033,0,0.150195,"communication of emotions can either be an explicit mention of the emotion name (“I am angry”), focus on the motivational aspect (“He wanted to run away.”), describe the expression (“She smiled.”, “He shouted.”) or a physiological bodily reaction (“she was trembling”, “a tear was running down his face”), the subjective feeling (“I felt so bad.”), or, finally, describe a cognitive appraisal (“I wasn’t sure what was happening.”, “I am not responsible.”). With this paper, we study how emotions are communicated (following the component model) in Tweets (based on the Twitter Emotion Corpus TEC, by Mohammad (2012)) and literature (based on the REMAN corpus by Kim and Klinger (2018)). We post-annotate a subset of 3041 instances with the use of emotion component-based emotion communication categories, analyze this corpus, and perform joint modelling/multi-task learning experiments. Our research goals are (1) to understand if emotion components are distributed similarly across emotion categories and domains, and (2) to evaluate if informing an emotion classifier about emotion components improves their performance (and to evaluate various classification approaches). We find that emotion component and emoti"
2021.konvens-1.5,S18-1001,0,0.168979,"motion. 1 Introduction The task of emotion classification from written text is to map textual units, like documents, paragraphs, or sentences, to a predefined set of emotions. Common class inventories rely on psychological theories such as those proposed by Ekman (1992) (anger, disgust, fear, joy, sadness, surprise) or ∗ The first two authors contributed equally to this work. Plutchik (2001). Often, emotion classification is tackled as an end-to-end learning task, potentially informed by lexical resources (see the SemEval Shared Task 1 on Affect in Tweets for an overview of recent approaches (Mohammad et al., 2018)). While end-to-end learning and fine-tuning of pretrained models for classification have shown great performance improvements in contrast to purely feature-based methods, such approaches typically neglect the existing knowledge about emotions in psychology (which might help in classification and to better understand how emotions are communicated). There are only very few approaches that aim at combining psychological theories (beyond basic emotion categories) with emotion classification models: We are only aware of the work by Hofmann et al. (2020), who incorporate the cognitive appraisal of"
2021.konvens-1.5,D14-1162,0,0.0857742,"omponents occur in a text instance. Our Cpm-ME-Base baseline models (one for each component) only use bag-of-words features in the same configuration as Emo-ME-Base. In the model Cpm-ME-Adv, we add taskspecific features, namely features derived from manually crafted small dictionaries with words associated with the different components. Those dictionaries were developed without considering the corpora and with inspiration from Scherer (2005) and contain on average 26 items. Further, we add part-of-speech tags (calculated with spaCy4 , Honnibal et al. (2020)) and glove-twitter-100 embeddings5 (Pennington et al., 2014). Additionally, only for the cognitive appraisal component, we run the appraisal classifier developed by Hofmann et al. (2020) and use the predictions as features.6 For each component individually, the best-performing combination of these features is chosen. The Cpm-NN-Base is configured analogously to Emo-NN-Base. The primary reason for using an equivalent setup is to facilitate a multi-head architecture as joint model for both tasks in the next step. 3 We selected this architecture based on preliminary experiments on the validation data. We evaluated it against LSTM-Dense Layer and CNN-LSTM"
2021.konvens-1.5,P19-1391,1,0.883702,"Missing"
2021.konvens-1.7,2020.lrec-1.194,1,0.828482,", the fact that stimuli are essential in understanding the emotion evoked in a text is supported by research in psychology; Appraisal theorists of emotions seem to agree that emotions include a cognitive evaluative component of an event (Scherer, 2005). Therefore emotion stimulus detection brings the field of emotion analysis in NLP closer to the state of the art in psychology. To the best of our knowledge, there are mostly corpora published for Mandarin (Lee et al., 2010b; Gui et al., 2014, 2016; Gao et al., 2017) and English (Ghazi et al., 2015; Mohammad et al., 2014; Kim and Klinger, 2018; Bostan et al., 2020). We are not aware of any study that created resources or models for identifying emotion stimuli in German. We fill this gap and contribute the G ER S TI (GERman STImulus) corpus with 2006 German news headlines. The headlines have been annotated for emotion categories, for the mention of an experiencer or a cue phrase, and for stimuli on the token level (on which we focus in this paper). News headlines have been selected as the domain because they concisely provide concrete information and are easy to obtain. Additionally, unlike social media texts, this genre avoids potential privacy issues ("
2021.konvens-1.7,E17-2092,0,0.110945,"r respective CRF counterparts. 1 Introduction Emotions are a complex phenomenon that play a central role in our experiences and daily communications. Understanding them cannot be accounted by any single area of study since they can be represented and expressed in different ways, e.g., via facial expressions, voice, language, or gestures. In natural language processing, most models build on top of one out of three approaches to study and understand emotions, namely basic emotions (Ekman, 1992; Strapparava and Mihalcea, 2007; Aman and Szpakowicz, 2007), the valence-arousal model (Russell, 1980; Buechel and Hahn, 2017) or cognitive appraisal theory (Scherer, 2005; Hofmann et al., 2020, 2021). Emotion classification in text has received abundant attention in natural language processing research in the past few years. Hence, many studies have been conducted to investigate emotions on social media (Stieglitz and Dang-Xuan, 2013; Brynielsson et al., 2014; Tromp and Pechenizkiy, 2015), in literary and poetry texts (Kim and Klinger, 2019; Haider et al., 2020) or for analysing song lyrics (Mihalcea and Strapparava, 2012; Hijra Ferdinan et al., 2018; Edmonds and Sedoc, 2021). However, previous work mostly focused o"
2021.konvens-1.7,D18-1066,0,0.0173995,"loped a machine learning model that combines different sets of such rules (Chen et al., 2010). Gui et al. (2014) extended these rules and machine learning models on their Weibo corpus. Ghazi et al. (2015) formulated the task as structured learning. Most methods for stimulus detection have been evaluated on Mandarin. Gui et al. (2016) propose a convolution kernel-based learning method and train a classifier to extract emotion stimulus events on the clause level. Gui et al. (2017) treat emotion stimulus extraction as a question answering task. Li et al. (2018) use a co-attention neural network. Chen et al. (2018) explore a joint method for emotion classification and emotion stimulus detection in order to capture mutual benefits across these two tasks. Similarly, Xia et al. (2019) evaluate a hierarchical recurrent neural network transformer model to classify multiple clauses. They show that solving these subtasks jointly is beneficial for the model’s performance. Xia and Ding (2019) redefine the task as emotion/cause pair extraction and intend to detect potential emotions and corresponding causes in text. Xu et al. (2019) tackle the emotion/cause pair extraction task by adopting a learning-to-rank meth"
2021.konvens-1.7,C10-1021,0,0.0182672,"tention for Chinese Mandarin (Lee et al., 2010b; Li and Xu, 2014; Gui et al., 2014, 2016; Cheng et al., 2017, i.a.). Only few corpora have been created for English (Neviarouskaya and Aono, 2013; Mohammad et al., 2014; Kim and Klinger, 2018; Bostan et al., 2020). Russo et al. (2011) worked on a dataset for Italian news texts and Yada et al. (2017) annotated Japanese sentences from news articles and question/answer websites. Lee et al. (2010b,a) developed linguistic rules to extract emotion stimuli. A follow-up study developed a machine learning model that combines different sets of such rules (Chen et al., 2010). Gui et al. (2014) extended these rules and machine learning models on their Weibo corpus. Ghazi et al. (2015) formulated the task as structured learning. Most methods for stimulus detection have been evaluated on Mandarin. Gui et al. (2016) propose a convolution kernel-based learning method and train a classifier to extract emotion stimulus events on the clause level. Gui et al. (2017) treat emotion stimulus extraction as a question answering task. Li et al. (2018) use a co-attention neural network. Chen et al. (2018) explore a joint method for emotion classification and emotion stimulus det"
2021.konvens-1.7,2020.acl-main.747,0,0.0327329,"Missing"
2021.konvens-1.7,2021.wassa-1.24,0,0.0429927,"the valence-arousal model (Russell, 1980; Buechel and Hahn, 2017) or cognitive appraisal theory (Scherer, 2005; Hofmann et al., 2020, 2021). Emotion classification in text has received abundant attention in natural language processing research in the past few years. Hence, many studies have been conducted to investigate emotions on social media (Stieglitz and Dang-Xuan, 2013; Brynielsson et al., 2014; Tromp and Pechenizkiy, 2015), in literary and poetry texts (Kim and Klinger, 2019; Haider et al., 2020) or for analysing song lyrics (Mihalcea and Strapparava, 2012; Hijra Ferdinan et al., 2018; Edmonds and Sedoc, 2021). However, previous work mostly focused on assigning emotions to sentences or text passages. These approaches do not allow to identify which event, object, or person caused the emotion (which we refer to as the stimulus). Emotion stimulus detection is the subtask of emotion analysis which aims at extracting the stimulus of an expressed emotion. For instance, in the following example from FrameNet (Fillmore et al., 2003) “Holmes is happy having the freedom of the house when we are out” one could assume that happiness or joy is the emotion in the text. One could also highlight that the term “hap"
2021.konvens-1.7,2020.acl-main.342,0,0.0316518,"a et al. (2019) evaluate a hierarchical recurrent neural network transformer model to classify multiple clauses. They show that solving these subtasks jointly is beneficial for the model’s performance. Xia and Ding (2019) redefine the task as emotion/cause pair extraction and intend to detect potential emotions and corresponding causes in text. Xu et al. (2019) tackle the emotion/cause pair extraction task by adopting a learning-to-rank method. Wei et al. (2020) also argue for the use of a ranking approach. They rank each possible emotion/cause pair instead of solely ranking stimulus phrases. Fan et al. (2020) do not subdivide the emotion/cause pair detection task into two subtasks but propose a framework to detect emotions and their associated causes simultaneously. Oberl¨ander and Klinger (2020) studied whether sequence labeling or clause classification is appropriate for extracting English stimuli. As we assume that these findings also hold for German, we follow their finding that token sequence labeling is more appropriate. 3 Corpus Creation To tackle German emotion stimulus detection on the token-level, we select headlines from various online news portals, remove duplicates and irrelevant item"
2021.konvens-1.7,D17-1167,0,0.0155433,"icles and question/answer websites. Lee et al. (2010b,a) developed linguistic rules to extract emotion stimuli. A follow-up study developed a machine learning model that combines different sets of such rules (Chen et al., 2010). Gui et al. (2014) extended these rules and machine learning models on their Weibo corpus. Ghazi et al. (2015) formulated the task as structured learning. Most methods for stimulus detection have been evaluated on Mandarin. Gui et al. (2016) propose a convolution kernel-based learning method and train a classifier to extract emotion stimulus events on the clause level. Gui et al. (2017) treat emotion stimulus extraction as a question answering task. Li et al. (2018) use a co-attention neural network. Chen et al. (2018) explore a joint method for emotion classification and emotion stimulus detection in order to capture mutual benefits across these two tasks. Similarly, Xia et al. (2019) evaluate a hierarchical recurrent neural network transformer model to classify multiple clauses. They show that solving these subtasks jointly is beneficial for the model’s performance. Xia and Ding (2019) redefine the task as emotion/cause pair extraction and intend to detect potential emotio"
2021.konvens-1.7,D16-1170,0,0.0139915,"stan et al., 2020). Russo et al. (2011) worked on a dataset for Italian news texts and Yada et al. (2017) annotated Japanese sentences from news articles and question/answer websites. Lee et al. (2010b,a) developed linguistic rules to extract emotion stimuli. A follow-up study developed a machine learning model that combines different sets of such rules (Chen et al., 2010). Gui et al. (2014) extended these rules and machine learning models on their Weibo corpus. Ghazi et al. (2015) formulated the task as structured learning. Most methods for stimulus detection have been evaluated on Mandarin. Gui et al. (2016) propose a convolution kernel-based learning method and train a classifier to extract emotion stimulus events on the clause level. Gui et al. (2017) treat emotion stimulus extraction as a question answering task. Li et al. (2018) use a co-attention neural network. Chen et al. (2018) explore a joint method for emotion classification and emotion stimulus detection in order to capture mutual benefits across these two tasks. Similarly, Xia et al. (2019) evaluate a hierarchical recurrent neural network transformer model to classify multiple clauses. They show that solving these subtasks jointly is"
2021.konvens-1.7,2020.lrec-1.205,1,0.917657,"stand emotions, namely basic emotions (Ekman, 1992; Strapparava and Mihalcea, 2007; Aman and Szpakowicz, 2007), the valence-arousal model (Russell, 1980; Buechel and Hahn, 2017) or cognitive appraisal theory (Scherer, 2005; Hofmann et al., 2020, 2021). Emotion classification in text has received abundant attention in natural language processing research in the past few years. Hence, many studies have been conducted to investigate emotions on social media (Stieglitz and Dang-Xuan, 2013; Brynielsson et al., 2014; Tromp and Pechenizkiy, 2015), in literary and poetry texts (Kim and Klinger, 2019; Haider et al., 2020) or for analysing song lyrics (Mihalcea and Strapparava, 2012; Hijra Ferdinan et al., 2018; Edmonds and Sedoc, 2021). However, previous work mostly focused on assigning emotions to sentences or text passages. These approaches do not allow to identify which event, object, or person caused the emotion (which we refer to as the stimulus). Emotion stimulus detection is the subtask of emotion analysis which aims at extracting the stimulus of an expressed emotion. For instance, in the following example from FrameNet (Fillmore et al., 2003) “Holmes is happy having the freedom of the house when we are"
2021.konvens-1.7,2021.wassa-1.17,1,0.83486,"Missing"
2021.konvens-1.7,2020.coling-main.11,1,0.845063,"henomenon that play a central role in our experiences and daily communications. Understanding them cannot be accounted by any single area of study since they can be represented and expressed in different ways, e.g., via facial expressions, voice, language, or gestures. In natural language processing, most models build on top of one out of three approaches to study and understand emotions, namely basic emotions (Ekman, 1992; Strapparava and Mihalcea, 2007; Aman and Szpakowicz, 2007), the valence-arousal model (Russell, 1980; Buechel and Hahn, 2017) or cognitive appraisal theory (Scherer, 2005; Hofmann et al., 2020, 2021). Emotion classification in text has received abundant attention in natural language processing research in the past few years. Hence, many studies have been conducted to investigate emotions on social media (Stieglitz and Dang-Xuan, 2013; Brynielsson et al., 2014; Tromp and Pechenizkiy, 2015), in literary and poetry texts (Kim and Klinger, 2019; Haider et al., 2020) or for analysing song lyrics (Mihalcea and Strapparava, 2012; Hijra Ferdinan et al., 2018; Edmonds and Sedoc, 2021). However, previous work mostly focused on assigning emotions to sentences or text passages. These approache"
2021.konvens-1.7,C18-1114,1,0.907882,"otions). More than that, the fact that stimuli are essential in understanding the emotion evoked in a text is supported by research in psychology; Appraisal theorists of emotions seem to agree that emotions include a cognitive evaluative component of an event (Scherer, 2005). Therefore emotion stimulus detection brings the field of emotion analysis in NLP closer to the state of the art in psychology. To the best of our knowledge, there are mostly corpora published for Mandarin (Lee et al., 2010b; Gui et al., 2014, 2016; Gao et al., 2017) and English (Ghazi et al., 2015; Mohammad et al., 2014; Kim and Klinger, 2018; Bostan et al., 2020). We are not aware of any study that created resources or models for identifying emotion stimuli in German. We fill this gap and contribute the G ER S TI (GERman STImulus) corpus with 2006 German news headlines. The headlines have been annotated for emotion categories, for the mention of an experiencer or a cue phrase, and for stimuli on the token level (on which we focus in this paper). News headlines have been selected as the domain because they concisely provide concrete information and are easy to obtain. Additionally, unlike social media texts, this genre avoids pote"
2021.konvens-1.7,N19-1067,1,0.850348,"ches to study and understand emotions, namely basic emotions (Ekman, 1992; Strapparava and Mihalcea, 2007; Aman and Szpakowicz, 2007), the valence-arousal model (Russell, 1980; Buechel and Hahn, 2017) or cognitive appraisal theory (Scherer, 2005; Hofmann et al., 2020, 2021). Emotion classification in text has received abundant attention in natural language processing research in the past few years. Hence, many studies have been conducted to investigate emotions on social media (Stieglitz and Dang-Xuan, 2013; Brynielsson et al., 2014; Tromp and Pechenizkiy, 2015), in literary and poetry texts (Kim and Klinger, 2019; Haider et al., 2020) or for analysing song lyrics (Mihalcea and Strapparava, 2012; Hijra Ferdinan et al., 2018; Edmonds and Sedoc, 2021). However, previous work mostly focused on assigning emotions to sentences or text passages. These approaches do not allow to identify which event, object, or person caused the emotion (which we refer to as the stimulus). Emotion stimulus detection is the subtask of emotion analysis which aims at extracting the stimulus of an expressed emotion. For instance, in the following example from FrameNet (Fillmore et al., 2003) “Holmes is happy having the freedom of"
2021.konvens-1.7,W10-0206,0,0.333091,"additional information for a better understanding of the emotion structures (e.g., semantic frames associated with emotions). More than that, the fact that stimuli are essential in understanding the emotion evoked in a text is supported by research in psychology; Appraisal theorists of emotions seem to agree that emotions include a cognitive evaluative component of an event (Scherer, 2005). Therefore emotion stimulus detection brings the field of emotion analysis in NLP closer to the state of the art in psychology. To the best of our knowledge, there are mostly corpora published for Mandarin (Lee et al., 2010b; Gui et al., 2014, 2016; Gao et al., 2017) and English (Ghazi et al., 2015; Mohammad et al., 2014; Kim and Klinger, 2018; Bostan et al., 2020). We are not aware of any study that created resources or models for identifying emotion stimuli in German. We fill this gap and contribute the G ER S TI (GERman STImulus) corpus with 2006 German news headlines. The headlines have been annotated for emotion categories, for the mention of an experiencer or a cue phrase, and for stimuli on the token level (on which we focus in this paper). News headlines have been selected as the domain because they conc"
2021.konvens-1.7,lee-etal-2010-emotion,0,0.402835,"additional information for a better understanding of the emotion structures (e.g., semantic frames associated with emotions). More than that, the fact that stimuli are essential in understanding the emotion evoked in a text is supported by research in psychology; Appraisal theorists of emotions seem to agree that emotions include a cognitive evaluative component of an event (Scherer, 2005). Therefore emotion stimulus detection brings the field of emotion analysis in NLP closer to the state of the art in psychology. To the best of our knowledge, there are mostly corpora published for Mandarin (Lee et al., 2010b; Gui et al., 2014, 2016; Gao et al., 2017) and English (Ghazi et al., 2015; Mohammad et al., 2014; Kim and Klinger, 2018; Bostan et al., 2020). We are not aware of any study that created resources or models for identifying emotion stimuli in German. We fill this gap and contribute the G ER S TI (GERman STImulus) corpus with 2006 German news headlines. The headlines have been annotated for emotion categories, for the mention of an experiencer or a cue phrase, and for stimuli on the token level (on which we focus in this paper). News headlines have been selected as the domain because they conc"
2021.konvens-1.7,D18-1506,0,0.0226121,"s to extract emotion stimuli. A follow-up study developed a machine learning model that combines different sets of such rules (Chen et al., 2010). Gui et al. (2014) extended these rules and machine learning models on their Weibo corpus. Ghazi et al. (2015) formulated the task as structured learning. Most methods for stimulus detection have been evaluated on Mandarin. Gui et al. (2016) propose a convolution kernel-based learning method and train a classifier to extract emotion stimulus events on the clause level. Gui et al. (2017) treat emotion stimulus extraction as a question answering task. Li et al. (2018) use a co-attention neural network. Chen et al. (2018) explore a joint method for emotion classification and emotion stimulus detection in order to capture mutual benefits across these two tasks. Similarly, Xia et al. (2019) evaluate a hierarchical recurrent neural network transformer model to classify multiple clauses. They show that solving these subtasks jointly is beneficial for the model’s performance. Xia and Ding (2019) redefine the task as emotion/cause pair extraction and intend to detect potential emotions and corresponding causes in text. Xu et al. (2019) tackle the emotion/cause pa"
2021.konvens-1.7,I17-1099,0,0.0293206,"l language model with XLM-RoBERTa (Conneau et al., 2020). 2 Related Work We now introduce previous work on emotion analysis and for detecting emotion stimuli. 2.1 Emotion Analysis Emotion analysis is the task of understanding emotions in text, typically based on psychological theories of Ekman (1992), Plutchik (2001), Russell (1980) or Scherer (2005). Several corpora have been built for emotion classification such as Alm and Sproat (2005) with tales, Strapparava and Mihalcea (2007) with news headlines, Aman and Szpakowicz (2007) with blog posts, Buechel and Hahn (2017) with various domains or Li et al. (2017) with conversations. Some datasets were cre1 The data is available at https://www.ims. uni-stuttgart.de/data/emotion. ated using crowdsourcing, for instance Mohammad et al. (2014) , Mohammad and Kiritchenko (2015) or Bostan et al. (2020), that have been annotated with tweets, or news headlines, respectively. Some resources mix various annotation paradigms, for example Troiano et al. (2019) (self-reporting and crowd-sourcing) or Haider et al. (2020) (experts and crowdworkers). Emotion analysis also includes other aspects such as emotion intensities and emotion roles (Aman and Szpakowicz, 2007;"
2021.konvens-1.7,D12-1054,0,0.0285825,"Strapparava and Mihalcea, 2007; Aman and Szpakowicz, 2007), the valence-arousal model (Russell, 1980; Buechel and Hahn, 2017) or cognitive appraisal theory (Scherer, 2005; Hofmann et al., 2020, 2021). Emotion classification in text has received abundant attention in natural language processing research in the past few years. Hence, many studies have been conducted to investigate emotions on social media (Stieglitz and Dang-Xuan, 2013; Brynielsson et al., 2014; Tromp and Pechenizkiy, 2015), in literary and poetry texts (Kim and Klinger, 2019; Haider et al., 2020) or for analysing song lyrics (Mihalcea and Strapparava, 2012; Hijra Ferdinan et al., 2018; Edmonds and Sedoc, 2021). However, previous work mostly focused on assigning emotions to sentences or text passages. These approaches do not allow to identify which event, object, or person caused the emotion (which we refer to as the stimulus). Emotion stimulus detection is the subtask of emotion analysis which aims at extracting the stimulus of an expressed emotion. For instance, in the following example from FrameNet (Fillmore et al., 2003) “Holmes is happy having the freedom of the house when we are out” one could assume that happiness or joy is the emotion i"
2021.konvens-1.7,S17-1007,0,0.124066,"with a pre-trained cross-lingual language model with XLM-RoBERTa (Conneau et al., 2020). 2 Related Work We now introduce previous work on emotion analysis and for detecting emotion stimuli. 2.1 Emotion Analysis Emotion analysis is the task of understanding emotions in text, typically based on psychological theories of Ekman (1992), Plutchik (2001), Russell (1980) or Scherer (2005). Several corpora have been built for emotion classification such as Alm and Sproat (2005) with tales, Strapparava and Mihalcea (2007) with news headlines, Aman and Szpakowicz (2007) with blog posts, Buechel and Hahn (2017) with various domains or Li et al. (2017) with conversations. Some datasets were cre1 The data is available at https://www.ims. uni-stuttgart.de/data/emotion. ated using crowdsourcing, for instance Mohammad et al. (2014) , Mohammad and Kiritchenko (2015) or Bostan et al. (2020), that have been annotated with tweets, or news headlines, respectively. Some resources mix various annotation paradigms, for example Troiano et al. (2019) (self-reporting and crowd-sourcing) or Haider et al. (2020) (experts and crowdworkers). Emotion analysis also includes other aspects such as emotion intensities and e"
2021.konvens-1.7,W14-2607,0,0.0267412,"ames associated with emotions). More than that, the fact that stimuli are essential in understanding the emotion evoked in a text is supported by research in psychology; Appraisal theorists of emotions seem to agree that emotions include a cognitive evaluative component of an event (Scherer, 2005). Therefore emotion stimulus detection brings the field of emotion analysis in NLP closer to the state of the art in psychology. To the best of our knowledge, there are mostly corpora published for Mandarin (Lee et al., 2010b; Gui et al., 2014, 2016; Gao et al., 2017) and English (Ghazi et al., 2015; Mohammad et al., 2014; Kim and Klinger, 2018; Bostan et al., 2020). We are not aware of any study that created resources or models for identifying emotion stimuli in German. We fill this gap and contribute the G ER S TI (GERman STImulus) corpus with 2006 German news headlines. The headlines have been annotated for emotion categories, for the mention of an experiencer or a cue phrase, and for stimuli on the token level (on which we focus in this paper). News headlines have been selected as the domain because they concisely provide concrete information and are easy to obtain. Additionally, unlike social media texts,"
2021.konvens-1.7,I13-1121,0,0.0161133,"(2019) (self-reporting and crowd-sourcing) or Haider et al. (2020) (experts and crowdworkers). Emotion analysis also includes other aspects such as emotion intensities and emotion roles (Aman and Szpakowicz, 2007; Mohammad and Bravo-Marquez, 2017; Bostan et al., 2020) including experiencers, targets, and stimuli (Mohammad et al., 2014; Kim and Klinger, 2018). 2.2 Stimulus Detection Emotion stimulus detection received substantial attention for Chinese Mandarin (Lee et al., 2010b; Li and Xu, 2014; Gui et al., 2014, 2016; Cheng et al., 2017, i.a.). Only few corpora have been created for English (Neviarouskaya and Aono, 2013; Mohammad et al., 2014; Kim and Klinger, 2018; Bostan et al., 2020). Russo et al. (2011) worked on a dataset for Italian news texts and Yada et al. (2017) annotated Japanese sentences from news articles and question/answer websites. Lee et al. (2010b,a) developed linguistic rules to extract emotion stimuli. A follow-up study developed a machine learning model that combines different sets of such rules (Chen et al., 2010). Gui et al. (2014) extended these rules and machine learning models on their Weibo corpus. Ghazi et al. (2015) formulated the task as structured learning. Most methods for st"
2021.konvens-1.7,2020.starsem-1.7,1,0.860992,"Missing"
2021.konvens-1.7,W11-1720,0,0.0464633,"Missing"
2021.konvens-1.7,S07-1013,0,0.0996025,"th the German corpus achieves higher F1 scores than projection. Experiments with XLM-R outperform their respective CRF counterparts. 1 Introduction Emotions are a complex phenomenon that play a central role in our experiences and daily communications. Understanding them cannot be accounted by any single area of study since they can be represented and expressed in different ways, e.g., via facial expressions, voice, language, or gestures. In natural language processing, most models build on top of one out of three approaches to study and understand emotions, namely basic emotions (Ekman, 1992; Strapparava and Mihalcea, 2007; Aman and Szpakowicz, 2007), the valence-arousal model (Russell, 1980; Buechel and Hahn, 2017) or cognitive appraisal theory (Scherer, 2005; Hofmann et al., 2020, 2021). Emotion classification in text has received abundant attention in natural language processing research in the past few years. Hence, many studies have been conducted to investigate emotions on social media (Stieglitz and Dang-Xuan, 2013; Brynielsson et al., 2014; Tromp and Pechenizkiy, 2015), in literary and poetry texts (Kim and Klinger, 2019; Haider et al., 2020) or for analysing song lyrics (Mihalcea and Strapparava, 2012;"
2021.konvens-1.7,P19-1391,1,0.88427,"Missing"
2021.konvens-1.7,2020.acl-main.289,0,0.0251295,"lore a joint method for emotion classification and emotion stimulus detection in order to capture mutual benefits across these two tasks. Similarly, Xia et al. (2019) evaluate a hierarchical recurrent neural network transformer model to classify multiple clauses. They show that solving these subtasks jointly is beneficial for the model’s performance. Xia and Ding (2019) redefine the task as emotion/cause pair extraction and intend to detect potential emotions and corresponding causes in text. Xu et al. (2019) tackle the emotion/cause pair extraction task by adopting a learning-to-rank method. Wei et al. (2020) also argue for the use of a ranking approach. They rank each possible emotion/cause pair instead of solely ranking stimulus phrases. Fan et al. (2020) do not subdivide the emotion/cause pair detection task into two subtasks but propose a framework to detect emotions and their associated causes simultaneously. Oberl¨ander and Klinger (2020) studied whether sequence labeling or clause classification is appropriate for extracting English stimuli. As we assume that these findings also hold for German, we follow their finding that token sequence labeling is more appropriate. 3 Corpus Creation To t"
2021.konvens-1.7,P19-1096,0,0.02282,"ning method and train a classifier to extract emotion stimulus events on the clause level. Gui et al. (2017) treat emotion stimulus extraction as a question answering task. Li et al. (2018) use a co-attention neural network. Chen et al. (2018) explore a joint method for emotion classification and emotion stimulus detection in order to capture mutual benefits across these two tasks. Similarly, Xia et al. (2019) evaluate a hierarchical recurrent neural network transformer model to classify multiple clauses. They show that solving these subtasks jointly is beneficial for the model’s performance. Xia and Ding (2019) redefine the task as emotion/cause pair extraction and intend to detect potential emotions and corresponding causes in text. Xu et al. (2019) tackle the emotion/cause pair extraction task by adopting a learning-to-rank method. Wei et al. (2020) also argue for the use of a ranking approach. They rank each possible emotion/cause pair instead of solely ranking stimulus phrases. Fan et al. (2020) do not subdivide the emotion/cause pair detection task into two subtasks but propose a framework to detect emotions and their associated causes simultaneously. Oberl¨ander and Klinger (2020) studied whet"
2021.wassa-1.17,C18-1179,1,0.893714,"Missing"
2021.wassa-1.17,E17-2092,0,0.215439,"Barrett, 2009). The two prominent traditions which have dominated the field of emotion classification in natural language processing are discrete and dimen161 sional models (Kim and Klinger, 2019). Next to the creation of lexicons for emotion analysis (Pennebaker et al., 2001; Strapparava and Valitutti, 2004; Mohammad et al., 2013; Mohammad, 2018, i.a.), the annotation of text corpora received substantial attention (Bostan and Klinger, 2018). They vary across emotion categories and domains, with discrete classes being dominating – some exceptions focused on valence and arousal annotations are Buechel and Hahn (2017), Preot¸iuc-Pietro et al. (2016), and Yu et al. (2016). For instance, the ISEAR study by Scherer and Wallbott (1994) led to self-reports of emotionally connotated events. Its creators aimed at understanding what aspects of emotions are universal and which are relative to culture. It was built by asking students to recall an emotion-inducing event and to describe it. Other efforts focused more on creating corpora specifically for emotion analysis in NLP. Troiano et al. (2019) built enISEAR and deISEAR, whose 1001 event-descriptions were collected via crowdsourcing, with a questionnaire inspired"
2021.wassa-1.17,2020.acl-main.747,0,0.122924,"Missing"
2021.wassa-1.17,2020.acl-main.372,0,0.0121803,"For instance, Dittrich and Zepf (2019) argue that some of the basic emotions are too strong for measuring how people feel when driving a car and, based on that, Cevher et al. (2019) resort to joy, annoyance (instead of anger), insecurity (instead of fear), boredom, and relaxation to classify in-car utterances. Haider et al. (2020) model the emotional perception of poetry and opt for the categories beauty/joy, sadness, uneasiness, vitality, awe/sublime, suspense, humor, nostalgia, and annoyance, following the definition of aesthetic emotions (Schindler et al., 2017; Menninghaus et al., 2019). Demszky et al. (2020) define a taxonomy of emotions, reaching a high coverage while maintaining inter-class relations. An alternative to the use of categorical variables are the so-called “dimensional” approaches. The most popular of them models affective experiences along the variables of dominance, valence, and arousal (Russell and Mehrabian, 1977, VAD). Feldman Barrett (2006, 2017) theorizes that emotions are interpretations of continuous affective states experiencers find themselves in. Still, as Smith and Ellsworth (1985) note, not all emotions can be distinguished based on valence and arousal. One might argu"
2021.wassa-1.17,2020.lrec-1.205,1,0.649653,"e proposed by Ekman (1992) (anger, disgust, fear, joy, sadness, surprise) or Plutchik (2001). These theories are based on the assumption that there is a restricted number of emotions that have prototypical realizations. However, not all sets of emotions are appropriate for every domain. For instance, Dittrich and Zepf (2019) argue that some of the basic emotions are too strong for measuring how people feel when driving a car and, based on that, Cevher et al. (2019) resort to joy, annoyance (instead of anger), insecurity (instead of fear), boredom, and relaxation to classify in-car utterances. Haider et al. (2020) model the emotional perception of poetry and opt for the categories beauty/joy, sadness, uneasiness, vitality, awe/sublime, suspense, humor, nostalgia, and annoyance, following the definition of aesthetic emotions (Schindler et al., 2017; Menninghaus et al., 2019). Demszky et al. (2020) define a taxonomy of emotions, reaching a high coverage while maintaining inter-class relations. An alternative to the use of categorical variables are the so-called “dimensional” approaches. The most popular of them models affective experiences along the variables of dominance, valence, and arousal (Russell a"
2021.wassa-1.17,2020.coling-main.11,1,0.395354,"oy Attention, Certainty, Pleasant, Sit. Ctrl. Attention, Certainty, Effort, Sit. Ctrl. Attention, Effort, Sit. Ctrl. Attention, Certainty, Effort, Respons., Control Attention, Certainty, Sit. Ctrl. I felt . . . when I knew that I was going back to Florida a year earlier than I thought I would. Disgust Fear Guilt Sadness I felt . . . when my kitten was sick and I had to clean it up. I felt . . . when I was having a hard attach. I felt . . . when I went on holiday and left our cat behind. I felt . . . when I found out one of my favourite shops had shut down. Table 1: Examples from the corpus of Hofmann et al. (2020). is mostly made based on the effort that the emotion experiencer anticipates, but this is not represented by VAD. Therefore, Smith and Ellsworth (1985) propose a dimensional approach with the appraisal variables of how pleasant an event is (pleasantness, likely to be associated with joy, but unlikely to appear with disgust), how much effort an event can be expected to cause (anticipated effort, likely to be high when anger or fear is experienced), how certain the experiencer is in a specific situation (certainty, low, e.g., in the context of hope or surprise), how much attention is devoted to"
2021.wassa-1.17,2021.ccl-1.108,0,0.0373792,"Missing"
2021.wassa-1.17,S12-1033,0,0.164641,"(2016). For instance, the ISEAR study by Scherer and Wallbott (1994) led to self-reports of emotionally connotated events. Its creators aimed at understanding what aspects of emotions are universal and which are relative to culture. It was built by asking students to recall an emotion-inducing event and to describe it. Other efforts focused more on creating corpora specifically for emotion analysis in NLP. Troiano et al. (2019) built enISEAR and deISEAR, whose 1001 event-descriptions were collected via crowdsourcing, with a questionnaire inspired by ISEAR, both in English and in German. TEC (Mohammad, 2012), another popular resource, is bigger in size (≈21k instances), contains tweets and was automatically annotated with hashtags. The Blogs corpus by Aman and Szpakowicz (2007) has sentence-level annotations for 5205 texts, annotated by multiple raters. While ISEAR, enISEAR and deISEAR are focused on describing specific emotion-inducing events, the Blogs corpus and TEC are more general. This is also the set of corpora that we use in our study (a more comprehensive resource overview was made available by Hakak et al. (2017) and Bostan and Klinger (2018)). 2.2 Appraisal Theories A richer perspectiv"
2021.wassa-1.17,P18-1017,0,0.0165551,"s for Emotion Analysis There is a wealth of literature in psychology surrounding emotions, specifically regarding the way they are elicited, their universal validity, their number and stereotypical expressions, and their function (Scherer, 2000; Gendron and Feldman Barrett, 2009). The two prominent traditions which have dominated the field of emotion classification in natural language processing are discrete and dimen161 sional models (Kim and Klinger, 2019). Next to the creation of lexicons for emotion analysis (Pennebaker et al., 2001; Strapparava and Valitutti, 2004; Mohammad et al., 2013; Mohammad, 2018, i.a.), the annotation of text corpora received substantial attention (Bostan and Klinger, 2018). They vary across emotion categories and domains, with discrete classes being dominating – some exceptions focused on valence and arousal annotations are Buechel and Hahn (2017), Preot¸iuc-Pietro et al. (2016), and Yu et al. (2016). For instance, the ISEAR study by Scherer and Wallbott (1994) led to self-reports of emotionally connotated events. Its creators aimed at understanding what aspects of emotions are universal and which are relative to culture. It was built by asking students to recall an"
2021.wassa-1.17,S13-2053,0,0.0113249,"1 Related Work Resources for Emotion Analysis There is a wealth of literature in psychology surrounding emotions, specifically regarding the way they are elicited, their universal validity, their number and stereotypical expressions, and their function (Scherer, 2000; Gendron and Feldman Barrett, 2009). The two prominent traditions which have dominated the field of emotion classification in natural language processing are discrete and dimen161 sional models (Kim and Klinger, 2019). Next to the creation of lexicons for emotion analysis (Pennebaker et al., 2001; Strapparava and Valitutti, 2004; Mohammad et al., 2013; Mohammad, 2018, i.a.), the annotation of text corpora received substantial attention (Bostan and Klinger, 2018). They vary across emotion categories and domains, with discrete classes being dominating – some exceptions focused on valence and arousal annotations are Buechel and Hahn (2017), Preot¸iuc-Pietro et al. (2016), and Yu et al. (2016). For instance, the ISEAR study by Scherer and Wallbott (1994) led to self-reports of emotionally connotated events. Its creators aimed at understanding what aspects of emotions are universal and which are relative to culture. It was built by asking stude"
2021.wassa-1.17,W16-0404,0,0.0451718,"Missing"
2021.wassa-1.17,P19-1391,1,0.722347,"Missing"
2021.wassa-1.17,N16-1066,0,0.0304131,"ated the field of emotion classification in natural language processing are discrete and dimen161 sional models (Kim and Klinger, 2019). Next to the creation of lexicons for emotion analysis (Pennebaker et al., 2001; Strapparava and Valitutti, 2004; Mohammad et al., 2013; Mohammad, 2018, i.a.), the annotation of text corpora received substantial attention (Bostan and Klinger, 2018). They vary across emotion categories and domains, with discrete classes being dominating – some exceptions focused on valence and arousal annotations are Buechel and Hahn (2017), Preot¸iuc-Pietro et al. (2016), and Yu et al. (2016). For instance, the ISEAR study by Scherer and Wallbott (1994) led to self-reports of emotionally connotated events. Its creators aimed at understanding what aspects of emotions are universal and which are relative to culture. It was built by asking students to recall an emotion-inducing event and to describe it. Other efforts focused more on creating corpora specifically for emotion analysis in NLP. Troiano et al. (2019) built enISEAR and deISEAR, whose 1001 event-descriptions were collected via crowdsourcing, with a questionnaire inspired by ISEAR, both in English and in German. TEC (Mohamma"
2021.wassa-1.17,strapparava-valitutti-2004-wordnet,0,0.426566,"rt.de/data/appraisalemotion. 2 2.1 Related Work Resources for Emotion Analysis There is a wealth of literature in psychology surrounding emotions, specifically regarding the way they are elicited, their universal validity, their number and stereotypical expressions, and their function (Scherer, 2000; Gendron and Feldman Barrett, 2009). The two prominent traditions which have dominated the field of emotion classification in natural language processing are discrete and dimen161 sional models (Kim and Klinger, 2019). Next to the creation of lexicons for emotion analysis (Pennebaker et al., 2001; Strapparava and Valitutti, 2004; Mohammad et al., 2013; Mohammad, 2018, i.a.), the annotation of text corpora received substantial attention (Bostan and Klinger, 2018). They vary across emotion categories and domains, with discrete classes being dominating – some exceptions focused on valence and arousal annotations are Buechel and Hahn (2017), Preot¸iuc-Pietro et al. (2016), and Yu et al. (2016). For instance, the ISEAR study by Scherer and Wallbott (1994) led to self-reports of emotionally connotated events. Its creators aimed at understanding what aspects of emotions are universal and which are relative to culture. It wa"
2021.wassa-1.18,aker-etal-2017-simple,0,0.0158302,"ed task on offensive language identification organized by Zampieri et al. (2020) was featured in five languages. For a more detailed overview, we refer to the surveys by Mladenovi´c et al. (2021); Fortuna and Nunes (2018); Schmidt and Wiegand (2017). In contrast to this previous work, we provide data for a specific recent use case, and predefine two targets of interest to be analyzed. 2.2 Stance Detection Related work on stance detection includes stance detection on congressional debates (Thomas et al., 2006), online forums (Somasundaran and Wiebe, 2010), Twitter (Mohammad et al., 2016, 2017; Aker et al., 2017; K¨uc¸u¨ k and Can, 2018; Lozhnikov et al., 2020) and comments on news (Lozhnikov et al., 2020). Thomas et al. (2006) used a corpus of speeches from the US Congress and modeled their support/oppose towards a proposed legislation task. Somasundaran and Wiebe (2010) conducted experiments with sentiment and arguing expressions and used features based on modal verbs and sentiments for stance classification. For the SemEval 2016 Task 6 organized by Mohammad et al. (2016), stance was detected from tweets. The task contained two stance detection subtasks for supervised and weakly supervised settings"
2021.wassa-1.18,S19-2007,0,0.0289197,"016) used a feature-based approach to explore several feature types. Burnap and Williams (2014) collected hateful tweets related to the murder of Drummer Lee Rigby in 2013. The authors examined different classification methods with various features including n-grams, restricted n-grams, typed dependencies, and hateful terms. Schmidt and Wiegand (2017) outlined that the lack of a benchmark data set based on a commonly accepted definition of hate speech is challenging. Roß et al. (2016) found that there is low agreement among users when identifying hateful messages. For the SemEval 2019 Task 5, Basile et al. (2019) proposed two hate speech detection tasks on Spanish and English tweets which contained hateful messages against women and immigrants. Next to a binary classification, participating systems had to extract further features in harmful messages such as target identification. None of the submissions for the more fine-grained classification task in English could outperform the baseline of the task organizers. In case of Spanish, the best results were achieved by a linear-kernel SVM. The authors found that it was harder to detect further features than the presence of hate speech. The recent shared t"
2021.wassa-1.18,N19-1423,0,0.0233659,"Missing"
2021.wassa-1.18,gao-huang-2017-detecting,0,0.0606142,"ully in 2008 (Tumasjan et al., 2010). Twitter in particular is a much-frequented form of communication with monthly 330 million active users This paper contains offensive language. (Clement, 2019). The microblogging platform was credited to have played a key role in Donald Trump’s rise to power (Stolee and Caton, 2018). As Twitter enables users to express their opinions about topics and targets, the insights gained from detecting stance in political tweets can help monitor the voting base. In addition to the heated election of Trump in 2016, the world has also seen an increase of hate speech (Gao and Huang, 2017). Defined as “any communication that disparages a target group of people based on some characteristic such as race, colour, ethnicity, gender, sexual orientation, nationality, religion, or other characteristic” (Nockelby, 2000), hate speech is considered “a particular form of offensive language” (Warner and Hirschberg, 2012). However, some authors also conflate hateful and offensive speech and define hate speech as explicitly or implicitly degrading a person or group (Gao and Huang, 2017). Over the years, the use of hate speech in social media has increased (de Gibert et al., 2018). Consequent"
2021.wassa-1.18,W18-5102,0,0.0335359,"Missing"
2021.wassa-1.18,W12-2103,0,0.187371,"8). As Twitter enables users to express their opinions about topics and targets, the insights gained from detecting stance in political tweets can help monitor the voting base. In addition to the heated election of Trump in 2016, the world has also seen an increase of hate speech (Gao and Huang, 2017). Defined as “any communication that disparages a target group of people based on some characteristic such as race, colour, ethnicity, gender, sexual orientation, nationality, religion, or other characteristic” (Nockelby, 2000), hate speech is considered “a particular form of offensive language” (Warner and Hirschberg, 2012). However, some authors also conflate hateful and offensive speech and define hate speech as explicitly or implicitly degrading a person or group (Gao and Huang, 2017). Over the years, the use of hate speech in social media has increased (de Gibert et al., 2018). Consequently, there is a growing need for approaches that detect hate speech automatically (Gao and Huang, 2017). From the perspective of natural language processing (NLP), the combination of political stance and hate speech detection provides promising classification tasks, namely determining the attitude a text displays towards a pr"
2021.wassa-1.18,S16-1003,0,0.0631626,"Missing"
2021.wassa-1.18,W17-1101,0,0.0141793,"ither. Mandl et al. (2019) sampled their data from Twitter and partially from Facebook and experimented with binary as well as more fine-grained multi-class classifications. Their results suggest that systems based on deep neural networks performed best. Waseem and Hovy (2016) used a feature-based approach to explore several feature types. Burnap and Williams (2014) collected hateful tweets related to the murder of Drummer Lee Rigby in 2013. The authors examined different classification methods with various features including n-grams, restricted n-grams, typed dependencies, and hateful terms. Schmidt and Wiegand (2017) outlined that the lack of a benchmark data set based on a commonly accepted definition of hate speech is challenging. Roß et al. (2016) found that there is low agreement among users when identifying hateful messages. For the SemEval 2019 Task 5, Basile et al. (2019) proposed two hate speech detection tasks on Spanish and English tweets which contained hateful messages against women and immigrants. Next to a binary classification, participating systems had to extract further features in harmful messages such as target identification. None of the submissions for the more fine-grained classifica"
2021.wassa-1.18,W10-0214,0,0.0755623,"degrading a person or group (Gao and Huang, 2017). Over the years, the use of hate speech in social media has increased (de Gibert et al., 2018). Consequently, there is a growing need for approaches that detect hate speech automatically (Gao and Huang, 2017). From the perspective of natural language processing (NLP), the combination of political stance and hate speech detection provides promising classification tasks, namely determining the attitude a text displays towards a pre-determined target and the presence of hateful and offensive speech. In contrast to prior work on stance detection (Somasundaran and Wiebe, 2010; Mohammad et al., 2016, i.a.), we not only annotate if a text is favorable, against or does not mention the target at all (neither), but include whether the text of the tweet displays a mixed (both favorable and against) or neutral stance towards the targets. With this formulation we are also able to mark tweets that mention a target without taking a clear stance. To annotate hateful and offensive tweets, we follow the def171 Proceedings of the 11th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 171–180 April 19, 2021. ©2021 Association for Co"
2021.wassa-1.18,W06-1639,0,0.143365,"hors found that it was harder to detect further features than the presence of hate speech. The recent shared task on offensive language identification organized by Zampieri et al. (2020) was featured in five languages. For a more detailed overview, we refer to the surveys by Mladenovi´c et al. (2021); Fortuna and Nunes (2018); Schmidt and Wiegand (2017). In contrast to this previous work, we provide data for a specific recent use case, and predefine two targets of interest to be analyzed. 2.2 Stance Detection Related work on stance detection includes stance detection on congressional debates (Thomas et al., 2006), online forums (Somasundaran and Wiebe, 2010), Twitter (Mohammad et al., 2016, 2017; Aker et al., 2017; K¨uc¸u¨ k and Can, 2018; Lozhnikov et al., 2020) and comments on news (Lozhnikov et al., 2020). Thomas et al. (2006) used a corpus of speeches from the US Congress and modeled their support/oppose towards a proposed legislation task. Somasundaran and Wiebe (2010) conducted experiments with sentiment and arguing expressions and used features based on modal verbs and sentiments for stance classification. For the SemEval 2016 Task 6 organized by Mohammad et al. (2016), stance was detected from"
2021.wassa-1.18,2020.emnlp-demos.6,0,0.0184599,"Missing"
2021.wassa-1.18,2020.semeval-1.188,0,0.0236297,"nd English tweets which contained hateful messages against women and immigrants. Next to a binary classification, participating systems had to extract further features in harmful messages such as target identification. None of the submissions for the more fine-grained classification task in English could outperform the baseline of the task organizers. In case of Spanish, the best results were achieved by a linear-kernel SVM. The authors found that it was harder to detect further features than the presence of hate speech. The recent shared task on offensive language identification organized by Zampieri et al. (2020) was featured in five languages. For a more detailed overview, we refer to the surveys by Mladenovi´c et al. (2021); Fortuna and Nunes (2018); Schmidt and Wiegand (2017). In contrast to this previous work, we provide data for a specific recent use case, and predefine two targets of interest to be analyzed. 2.2 Stance Detection Related work on stance detection includes stance detection on congressional debates (Thomas et al., 2006), online forums (Somasundaran and Wiebe, 2010), Twitter (Mohammad et al., 2016, 2017; Aker et al., 2017; K¨uc¸u¨ k and Can, 2018; Lozhnikov et al., 2020) and comments"
2021.wassa-1.5,H05-1073,0,0.269204,"Missing"
2021.wassa-1.5,J11-4004,0,0.0237158,"o raters can be penalized more when the one choosing the emotion label does so by perceiving extreme confidence or intensity – even though we provided evidence that these cases are rare. Future work could explore this direction. In summary, we uphold that disagreements are not necessarily symptomatic of unreliability. This claim has so far not found much attention in emotion annotations, but is in line with a more general body of research dedicated to the reasons and the patterns underlying annotators’ disagreements and to the ways in which their intuitions should be aggregated and evaluated (Bayerl and Paul, 2011; Bhardwaj et al., 2010; Qing et al., 2014; Peldszus and Stede, 2013; Plank et al., 2014; Sommerauer et al., 2020, i.a.). The applicability of these ideas to emotions should not come as a surprise — their assessment can derive from perceptive and metaperceptual phenomena (intensity and confidence, for instance). Therefore, if emotion judgments alone might not be sufficient to measure the quality of annotations, they can be enriched and, eventually, explained by the knowledge of such phenomena. This annotation investigated if the perceived emotion (class), a perceived feature of emotion (intens"
2021.wassa-1.5,2020.coling-main.11,1,0.844011,"er automatic regressors or classifiers actually predict intensity, or rather human’s self-perceived confidence. 1 Introduction A plethora of theories exist on the matter of emotions: the intensity of affective states, their link to cognition, and their arrangement into categories are just a few of the angles from which psychology has tackled this complex phenomenon (Gendron and Feldman Barrett, 2009). Correspondingly, in computational emotion analysis, texts have been associated to values of intensity (Strapparava and Mihalcea, 2007; Mohammad and Bravo-Marquez, 2017), to cognitive components (Hofmann et al., 2020), and discrete classes (Zhang et al., 2018; Zhong et al., 2019, i.a.). In support of these tasks, substantial research effort has been directed to resource construction, which typically relies on the participation of human judges. Yet, emotions are a subjective experience. Their interpretation in text 40 Proceedings of the 11th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 40–49 April 19, 2021. ©2021 Association for Computational Linguistics In this paper, we experimentally investigate the relationship between three human judgments: about the"
2021.wassa-1.5,bobicev-sokolova-2017-inter,0,0.0432753,"Missing"
2021.wassa-1.5,I17-1099,0,0.0633886,"Missing"
2021.wassa-1.5,W13-2324,0,0.0340052,"label does so by perceiving extreme confidence or intensity – even though we provided evidence that these cases are rare. Future work could explore this direction. In summary, we uphold that disagreements are not necessarily symptomatic of unreliability. This claim has so far not found much attention in emotion annotations, but is in line with a more general body of research dedicated to the reasons and the patterns underlying annotators’ disagreements and to the ways in which their intuitions should be aggregated and evaluated (Bayerl and Paul, 2011; Bhardwaj et al., 2010; Qing et al., 2014; Peldszus and Stede, 2013; Plank et al., 2014; Sommerauer et al., 2020, i.a.). The applicability of these ideas to emotions should not come as a surprise — their assessment can derive from perceptive and metaperceptual phenomena (intensity and confidence, for instance). Therefore, if emotion judgments alone might not be sufficient to measure the quality of annotations, they can be enriched and, eventually, explained by the knowledge of such phenomena. This annotation investigated if the perceived emotion (class), a perceived feature of emotion (intensity), and self-perception (confidence) are tied together – and can h"
2021.wassa-1.5,P19-1391,1,0.890559,"Missing"
2021.wassa-1.5,P14-2083,0,0.0316962,"g extreme confidence or intensity – even though we provided evidence that these cases are rare. Future work could explore this direction. In summary, we uphold that disagreements are not necessarily symptomatic of unreliability. This claim has so far not found much attention in emotion annotations, but is in line with a more general body of research dedicated to the reasons and the patterns underlying annotators’ disagreements and to the ways in which their intuitions should be aggregated and evaluated (Bayerl and Paul, 2011; Bhardwaj et al., 2010; Qing et al., 2014; Peldszus and Stede, 2013; Plank et al., 2014; Sommerauer et al., 2020, i.a.). The applicability of these ideas to emotions should not come as a surprise — their assessment can derive from perceptive and metaperceptual phenomena (intensity and confidence, for instance). Therefore, if emotion judgments alone might not be sufficient to measure the quality of annotations, they can be enriched and, eventually, explained by the knowledge of such phenomena. This annotation investigated if the perceived emotion (class), a perceived feature of emotion (intensity), and self-perception (confidence) are tied together – and can help understand incon"
2021.wassa-1.5,2020.coling-main.422,0,0.0430629,"or intensity – even though we provided evidence that these cases are rare. Future work could explore this direction. In summary, we uphold that disagreements are not necessarily symptomatic of unreliability. This claim has so far not found much attention in emotion annotations, but is in line with a more general body of research dedicated to the reasons and the patterns underlying annotators’ disagreements and to the ways in which their intuitions should be aggregated and evaluated (Bayerl and Paul, 2011; Bhardwaj et al., 2010; Qing et al., 2014; Peldszus and Stede, 2013; Plank et al., 2014; Sommerauer et al., 2020, i.a.). The applicability of these ideas to emotions should not come as a surprise — their assessment can derive from perceptive and metaperceptual phenomena (intensity and confidence, for instance). Therefore, if emotion judgments alone might not be sufficient to measure the quality of annotations, they can be enriched and, eventually, explained by the knowledge of such phenomena. This annotation investigated if the perceived emotion (class), a perceived feature of emotion (intensity), and self-perception (confidence) are tied together – and can help understand inconsistent annotations. We f"
2021.wassa-1.5,S07-1013,0,0.105532,". This insight is relevant for modelling studies of intensity, as it opens the question wether automatic regressors or classifiers actually predict intensity, or rather human’s self-perceived confidence. 1 Introduction A plethora of theories exist on the matter of emotions: the intensity of affective states, their link to cognition, and their arrangement into categories are just a few of the angles from which psychology has tackled this complex phenomenon (Gendron and Feldman Barrett, 2009). Correspondingly, in computational emotion analysis, texts have been associated to values of intensity (Strapparava and Mihalcea, 2007; Mohammad and Bravo-Marquez, 2017), to cognitive components (Hofmann et al., 2020), and discrete classes (Zhang et al., 2018; Zhong et al., 2019, i.a.). In support of these tasks, substantial research effort has been directed to resource construction, which typically relies on the participation of human judges. Yet, emotions are a subjective experience. Their interpretation in text 40 Proceedings of the 11th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 40–49 April 19, 2021. ©2021 Association for Computational Linguistics In this paper, we ex"
C18-1070,D16-1250,0,0.0269853,"ss of information, as they must reduce the effect of discriminant features which are domain-dependent. We argue in this paper that this leads to a decreased performance especially in cases where the similarity between the domains is low. Unlike previous approaches, in this paper, we propose a domain adaptation approach based on lessons learned from cross-lingual sentiment analysis (Barnes et al., 2018). This approach maintains the domaindependent features, while adapting them to the target domain. Following state-of-the-art approaches to create bilingual word embeddings (Mikolov et al., 2013; Artetxe et al., 2016; Artetxe et al., 2017), we learn to project a mapping from a source domain vector space to the target domain space, while jointly training a sentiment classifier for the source domain. We show that our proposed model (1) performs comparably to state-of-the-art models when domains are similar and (2) outperforms state-of-the-art models significantly on divergent domains. We report novel state-of-the-art results on 11 domain pairs. We also contribute a detailed error analysis and compare the effect of different projection lexicons. Our code is available at https://github.com/ jbarnesspain/domai"
C18-1070,P17-1042,0,0.0289435,"they must reduce the effect of discriminant features which are domain-dependent. We argue in this paper that this leads to a decreased performance especially in cases where the similarity between the domains is low. Unlike previous approaches, in this paper, we propose a domain adaptation approach based on lessons learned from cross-lingual sentiment analysis (Barnes et al., 2018). This approach maintains the domaindependent features, while adapting them to the target domain. Following state-of-the-art approaches to create bilingual word embeddings (Mikolov et al., 2013; Artetxe et al., 2016; Artetxe et al., 2017), we learn to project a mapping from a source domain vector space to the target domain space, while jointly training a sentiment classifier for the source domain. We show that our proposed model (1) performs comparably to state-of-the-art models when domains are similar and (2) outperforms state-of-the-art models significantly on divergent domains. We report novel state-of-the-art results on 11 domain pairs. We also contribute a detailed error analysis and compare the effect of different projection lexicons. Our code is available at https://github.com/ jbarnesspain/domain_blse. 2 Related Work"
C18-1070,P18-1231,1,0.872925,"ce to a latent hidden space. While pivot-based domain adaptation methods are well-motivated, they are often outperformed by autoencoder methods. However, both approaches to domain adaptation effectively lead to a loss of information, as they must reduce the effect of discriminant features which are domain-dependent. We argue in this paper that this leads to a decreased performance especially in cases where the similarity between the domains is low. Unlike previous approaches, in this paper, we propose a domain adaptation approach based on lessons learned from cross-lingual sentiment analysis (Barnes et al., 2018). This approach maintains the domaindependent features, while adapting them to the target domain. Following state-of-the-art approaches to create bilingual word embeddings (Mikolov et al., 2013; Artetxe et al., 2016; Artetxe et al., 2017), we learn to project a mapping from a source domain vector space to the target domain space, while jointly training a sentiment classifier for the source domain. We show that our proposed model (1) performs comparably to state-of-the-art models when domains are similar and (2) outperforms state-of-the-art models significantly on divergent domains. We report n"
C18-1070,W06-1615,0,0.8895,". We show that our proposed model (1) performs comparably to state-of-the-art models when domains are similar and (2) outperforms state-of-the-art models significantly on divergent domains. We report novel state-of-the-art results on 11 domain pairs. We also contribute a detailed error analysis and compare the effect of different projection lexicons. Our code is available at https://github.com/ jbarnesspain/domain_blse. 2 Related Work and Motivation Domain adaptation is an omnipresent challenge in natural language processing. It has been applied for many tasks, such as part-of-speech tagging (Blitzer et al., 2006; Daume III, 2007), parsing (Blitzer et al., 2006; Finkel and Manning, 2009; McClosky et al., 2010), or named entity recognition (Daume III, 2007; Guo et al., 2009; Yu and Jiang, 2015). In the following, we limit our review to adaptation techniques which have been applied to sentiment analysis. 2.1 Pivot-based Approaches Blitzer et al. (2006) propose structural correspondence learning (S CL), which introduces the concept of pivots. These are features that behave in the same way for discriminative learning for both domains, e. g., good or terrible for sentiment analysis. The intuition is that n"
C18-1070,P07-1056,0,0.819641,"Licence details: http:// creativecommons.org/licenses/by/4.0/ 818 Proceedings of the 27th International Conference on Computational Linguistics, pages 818–830 Santa Fe, New Mexico, USA, August 20-26, 2018. 1 Introduction One of the main limitations of current approaches to sentiment analysis is that they are sensitive to differences in domain. This leads to classifiers that, after training, perform poorly on new domains (Pang and Lee, 2008; Deriu et al., 2017). Domain adaptation techniques provide a solution to reduce the discrepancy and enable models to perform well across multiple domains (Blitzer et al., 2007). The two main approaches to domain adaptation for sentiment analysis are pivot-based methods (Blitzer et al., 2007; Pan et al., 2010; Yu and Jiang, 2016), which augment the feature space with domain-independent features learned on unsupervised data, and autoencoder approaches (Glorot et al., 2011; Chen et al., 2012), which seek to create a good general mapping from a sentence to a latent hidden space. While pivot-based domain adaptation methods are well-motivated, they are often outperformed by autoencoder methods. However, both approaches to domain adaptation effectively lead to a loss of in"
C18-1070,P14-1058,0,0.0633112,"ges of being less interpretable, requiring long training times, and only utilizing a small amount of the original feature space. 2.3 Domain Specific Word Representations A third approach is to create word representations that provide useful features for multiple domains. He et al. (2011) propose a joint sentiment-topic model which uses pivots to change the topic-word Dirichlet priors. Bollegala et al. (2015) create domain-specific embeddings for pivots and non-pivots with the constraint that the pivot representations are similar across domains. The work that is most similar to ours is that of Bollegala et al. (2014). Their method learns to predict differences in word distributions across domains by learning to project lower-dimensional SVD representations of documents across domains. Unlike our work, however, they learn the projection step separately from the classification. They also only learn to project the features that the two domains have in common, which implies discarding information useful for classification. These approaches, however, perform worse than M SDA and N SCL. 3 Projecting Representations Our approach is motivated by previous success in learning to project embeddings across languages"
C18-1070,P15-1071,0,0.0257084,"significant gain in speed, as well as the ability to include more features from the original representations. Autoencoder models perform better than earlier S CL models (excluding N SCL), but have the disadvantages of being less interpretable, requiring long training times, and only utilizing a small amount of the original feature space. 2.3 Domain Specific Word Representations A third approach is to create word representations that provide useful features for multiple domains. He et al. (2011) propose a joint sentiment-topic model which uses pivots to change the topic-word Dirichlet priors. Bollegala et al. (2015) create domain-specific embeddings for pivots and non-pivots with the constraint that the pivot representations are similar across domains. The work that is most similar to ours is that of Bollegala et al. (2014). Their method learns to predict differences in word distributions across domains by learning to project lower-dimensional SVD representations of documents across domains. Unlike our work, however, they learn the projection step separately from the classification. They also only learn to project the features that the two domains have in common, which implies discarding information usef"
C18-1070,P07-1033,0,0.468904,"Missing"
C18-1070,W17-1103,0,0.033437,"Missing"
C18-1070,E14-1049,0,0.0350047,"CL or M SDA. In fact, M SDA performs very poorly on the minority negative class, with error rates reaching 98 percent. N SCL almost always favors a single class, with error rates as high as 60.4 on negative and 70.5 on positive. 5.3 Choice of Projection Lexicon Given that the choice of projection lexicon is one of the key parameters in the B LSE model, we experiment with three approaches to creating a projection lexicon and observe their effect on the books to SemEval 2013 setup. The Most Frequent Source Words are a common source of projection lexicon in the multilingual embedding literature (Faruqui and Dyer, 2014; Lazaridou et al., 2015). For our experiment, we take the 20,000 most frequent tokens from the Brown corpus (Francis and Kuˇcera, 1979). The hypothesis behind using a general corpus is that a large general lexicon will provide more supervision than a smaller task-specific lexicon. This should contribute to learning accurate projection matrices M and M 0 . Sentiment Lexicons often contain domain-independent words that convey sentiment. In our model, using a sentiment lexicon as a translation dictionary is equivalent to the use of pivots in other frameworks, as these are usually domain independ"
C18-1070,N09-1068,0,0.0333024,"he-art models when domains are similar and (2) outperforms state-of-the-art models significantly on divergent domains. We report novel state-of-the-art results on 11 domain pairs. We also contribute a detailed error analysis and compare the effect of different projection lexicons. Our code is available at https://github.com/ jbarnesspain/domain_blse. 2 Related Work and Motivation Domain adaptation is an omnipresent challenge in natural language processing. It has been applied for many tasks, such as part-of-speech tagging (Blitzer et al., 2006; Daume III, 2007), parsing (Blitzer et al., 2006; Finkel and Manning, 2009; McClosky et al., 2010), or named entity recognition (Daume III, 2007; Guo et al., 2009; Yu and Jiang, 2015). In the following, we limit our review to adaptation techniques which have been applied to sentiment analysis. 2.1 Pivot-based Approaches Blitzer et al. (2006) propose structural correspondence learning (S CL), which introduces the concept of pivots. These are features that behave in the same way for discriminative learning for both domains, e. g., good or terrible for sentiment analysis. The intuition is that non-pivot domain-dependent features, e. g., well-written for the book domain"
C18-1070,N09-1032,0,0.0710411,"on divergent domains. We report novel state-of-the-art results on 11 domain pairs. We also contribute a detailed error analysis and compare the effect of different projection lexicons. Our code is available at https://github.com/ jbarnesspain/domain_blse. 2 Related Work and Motivation Domain adaptation is an omnipresent challenge in natural language processing. It has been applied for many tasks, such as part-of-speech tagging (Blitzer et al., 2006; Daume III, 2007), parsing (Blitzer et al., 2006; Finkel and Manning, 2009; McClosky et al., 2010), or named entity recognition (Daume III, 2007; Guo et al., 2009; Yu and Jiang, 2015). In the following, we limit our review to adaptation techniques which have been applied to sentiment analysis. 2.1 Pivot-based Approaches Blitzer et al. (2006) propose structural correspondence learning (S CL), which introduces the concept of pivots. These are features that behave in the same way for discriminative learning for both domains, e. g., good or terrible for sentiment analysis. The intuition is that non-pivot domain-dependent features, e. g., well-written for the book domain or reliable for electronics, which are highly correlated to a pivot should be treated t"
C18-1070,P11-1013,0,0.0161035,"transformations which are performed in closed-form, with the non-linearity being applied afterwards. This leads to a significant gain in speed, as well as the ability to include more features from the original representations. Autoencoder models perform better than earlier S CL models (excluding N SCL), but have the disadvantages of being less interpretable, requiring long training times, and only utilizing a small amount of the original feature space. 2.3 Domain Specific Word Representations A third approach is to create word representations that provide useful features for multiple domains. He et al. (2011) propose a joint sentiment-topic model which uses pivots to change the topic-word Dirichlet priors. Bollegala et al. (2015) create domain-specific embeddings for pivots and non-pivots with the constraint that the pivot representations are similar across domains. The work that is most similar to ours is that of Bollegala et al. (2014). Their method learns to predict differences in word distributions across domains by learning to project lower-dimensional SVD representations of documents across domains. Unlike our work, however, they learn the projection step separately from the classification."
C18-1070,P15-1027,0,0.0315887,"SDA performs very poorly on the minority negative class, with error rates reaching 98 percent. N SCL almost always favors a single class, with error rates as high as 60.4 on negative and 70.5 on positive. 5.3 Choice of Projection Lexicon Given that the choice of projection lexicon is one of the key parameters in the B LSE model, we experiment with three approaches to creating a projection lexicon and observe their effect on the books to SemEval 2013 setup. The Most Frequent Source Words are a common source of projection lexicon in the multilingual embedding literature (Faruqui and Dyer, 2014; Lazaridou et al., 2015). For our experiment, we take the 20,000 most frequent tokens from the Brown corpus (Francis and Kuˇcera, 1979). The hypothesis behind using a general corpus is that a large general lexicon will provide more supervision than a smaller task-specific lexicon. This should contribute to learning accurate projection matrices M and M 0 . Sentiment Lexicons often contain domain-independent words that convey sentiment. In our model, using a sentiment lexicon as a translation dictionary is equivalent to the use of pivots in other frameworks, as these are usually domain independent words with are good p"
C18-1070,S13-2052,0,0.0444366,"constructing a lexicon that maps concepts from one domain to those of another, i. e. “read” in the books domain and “watch” for movies. In order to create a mapping from both original vector spaces S and T to shared sentiment-informed bi-domain spaces z and ˆ z, we employ two linear projection matrices, M and M 0 . During training, for each translation pair in L, we first look up their associated vectors, project them through their associated projection matrix and finally minimize the mean squared error of the two projected vectors. This is very similar to the approach taken by Mikolov et al. (2013), but includes an additional target projection matrix. The projection quality is ensured by minimizing the mean squared error1 n MSE = 1X (zi − ˆ zi )2 , n (1) i=1 where zi = Ssi · M is the dot product of the embedding for source word si and the source projection matrix and ˆ zi = Tti · M 0 is the same for the target word ti and target matrix M 0 . The intuition for including this second matrix is that a single projection matrix does not support the transfer of sentiment information from the source domain to the target domain. Although this term is degenerate by itself, when coupled with the s"
C18-1070,S16-1001,0,0.0337518,"arget domain (Ziser and Reichart, 2017). We take the unlabeled data from each domain to create the domain embeddings for our method, as well as to train the domain independent representations for the N SCL and M SDA methods. In order to create embeddings for the Amazon corpora, we concatenate all of the unlabeled data from all domains. The statistics for this corpus are given in Table 1. 822 4.1.2 SemEval Corpora Sentiment analysis of Twitter data is common nowadays, with several popular shared tasks organized on the topic (Nakov et al., 2013; Villena-Rom´an et al., 2013; Basile et al., 2014; Nakov et al., 2016, i. a.). In order to evaluate how well domain adaptation techniques perform on large domain gaps, we also use the message polarity classification corpora provided by the organizers of SemEval 2013 and 2016 (Nakov et al., 2013; Nakov et al., 2016). We will refer to these as S13 and S16, respectively. These contain tweets which have been annotated for positive, negative, and neutral sentiment. We remove neutral tweets, giving us a binary setup which allows compatibility with the Amazon corpora. The statistics for these corpora are given in Table 1. 4.2 Embeddings For B LSE, we create mono-domai"
C18-1070,P15-2028,0,0.0202908,"ins. We report novel state-of-the-art results on 11 domain pairs. We also contribute a detailed error analysis and compare the effect of different projection lexicons. Our code is available at https://github.com/ jbarnesspain/domain_blse. 2 Related Work and Motivation Domain adaptation is an omnipresent challenge in natural language processing. It has been applied for many tasks, such as part-of-speech tagging (Blitzer et al., 2006; Daume III, 2007), parsing (Blitzer et al., 2006; Finkel and Manning, 2009; McClosky et al., 2010), or named entity recognition (Daume III, 2007; Guo et al., 2009; Yu and Jiang, 2015). In the following, we limit our review to adaptation techniques which have been applied to sentiment analysis. 2.1 Pivot-based Approaches Blitzer et al. (2006) propose structural correspondence learning (S CL), which introduces the concept of pivots. These are features that behave in the same way for discriminative learning for both domains, e. g., good or terrible for sentiment analysis. The intuition is that non-pivot domain-dependent features, e. g., well-written for the book domain or reliable for electronics, which are highly correlated to a pivot should be treated the same by a sentimen"
C18-1070,D16-1023,0,0.148162,"830 Santa Fe, New Mexico, USA, August 20-26, 2018. 1 Introduction One of the main limitations of current approaches to sentiment analysis is that they are sensitive to differences in domain. This leads to classifiers that, after training, perform poorly on new domains (Pang and Lee, 2008; Deriu et al., 2017). Domain adaptation techniques provide a solution to reduce the discrepancy and enable models to perform well across multiple domains (Blitzer et al., 2007). The two main approaches to domain adaptation for sentiment analysis are pivot-based methods (Blitzer et al., 2007; Pan et al., 2010; Yu and Jiang, 2016), which augment the feature space with domain-independent features learned on unsupervised data, and autoencoder approaches (Glorot et al., 2011; Chen et al., 2012), which seek to create a good general mapping from a sentence to a latent hidden space. While pivot-based domain adaptation methods are well-motivated, they are often outperformed by autoencoder methods. However, both approaches to domain adaptation effectively lead to a loss of information, as they must reduce the effect of discriminant features which are domain-dependent. We argue in this paper that this leads to a decreased perfo"
C18-1070,K17-1040,0,0.210985,"Section 4.3. 4.1 Datasets 4.1.1 Amazon Corpora In order to evaluate our proposed method, we use the corpus collected by Blitzer et al. (2007), which consists of Amazon product reviews from four domains: books (B), DVD (D), electronics (E), and kitchen (K). Each subcorpus contains a balanced labeled subset, with 1000 positive and 1000 negative reviews, as well as a much larger set of unlabeled reviews. We use the standard split of 1600 reviews from each domain as training data and the remaining 400 reviews as validation data. For testing, we use all of the 2000 reviews from the target domain (Ziser and Reichart, 2017). We take the unlabeled data from each domain to create the domain embeddings for our method, as well as to train the domain independent representations for the N SCL and M SDA methods. In order to create embeddings for the Amazon corpora, we concatenate all of the unlabeled data from all domains. The statistics for this corpus are given in Table 1. 822 4.1.2 SemEval Corpora Sentiment analysis of Twitter data is common nowadays, with several popular shared tasks organized on the topic (Nakov et al., 2013; Villena-Rom´an et al., 2013; Basile et al., 2014; Nakov et al., 2016, i. a.). In order to"
C18-1070,N10-1004,0,\N,Missing
C18-1114,H05-1073,0,0.211868,"y and (4) show that the prediction performance of all subtasks benefits from joint prediction of experiencer, emotion words, and targets. 2 Related Work Emotions have strong linguistic markers that define the tone of the text (Johnson-Laird and Oatley, 1989). This allows for different granularities of emotion annotation. The corpus which originates from the ISEAR project (Scherer and Wallbott, 1994) is an example of document-level annotation that includes descriptions of situations in which respondents had experienced various emotions. Examples of sentencelevel annotations include the work by Alm et al. (2005), who annotate a corpus of children stories, and Strapparava and Mihalcea (2007), who label news headlines, but without specifying the textual markers of emotion. An early work which includes textual markers of emotions is Aman and Szpakowicz (2007), who annotate blogposts. Wiebe et al. (2005) annotate a corpus of news articles with emotions at a word and phrase level. Recent works have mainly diverged from plain emotion annotation, following the idea of emotion theorists (Russell, 2003, i.a.) that causes of emotions are inseparable from emotions: Russo et al. (2011) build a corpus of Italian"
C18-1114,Q17-1010,0,0.0230228,"Missing"
C18-1114,C10-1021,0,0.298444,"ich includes textual markers of emotions is Aman and Szpakowicz (2007), who annotate blogposts. Wiebe et al. (2005) annotate a corpus of news articles with emotions at a word and phrase level. Recent works have mainly diverged from plain emotion annotation, following the idea of emotion theorists (Russell, 2003, i.a.) that causes of emotions are inseparable from emotions: Russo et al. (2011) build a corpus of Italian newspaper articles annotated with emotion key words and emotion cause phrases. Both Mei et al. (2012) and Gui et al. (2016) construct emotion-cause-annotated corpora for Chinese. Chen et al. (2010) adopt a rule-based approach based on linguistic patterns to detect emotion causes in the annotated Chinese corpus. Gui et al. (2017) present a question-answering approach to emotion cause extraction, also for Chinese. 1346 cause experiencer When I mentioned the house, he seemed surprised. event character surprise Figure 1: Example annotation from Hugo (1885), with one character, an emotion word, and event and cause and experiencer annotations. experiencer target target experiencer All laughed at the mistake, and none louder than the forth member of the parliament . . . character disgust joy o"
C18-1114,D17-1167,0,0.120353,"of news articles with emotions at a word and phrase level. Recent works have mainly diverged from plain emotion annotation, following the idea of emotion theorists (Russell, 2003, i.a.) that causes of emotions are inseparable from emotions: Russo et al. (2011) build a corpus of Italian newspaper articles annotated with emotion key words and emotion cause phrases. Both Mei et al. (2012) and Gui et al. (2016) construct emotion-cause-annotated corpora for Chinese. Chen et al. (2010) adopt a rule-based approach based on linguistic patterns to detect emotion causes in the annotated Chinese corpus. Gui et al. (2017) present a question-answering approach to emotion cause extraction, also for Chinese. 1346 cause experiencer When I mentioned the house, he seemed surprised. event character surprise Figure 1: Example annotation from Hugo (1885), with one character, an emotion word, and event and cause and experiencer annotations. experiencer target target experiencer All laughed at the mistake, and none louder than the forth member of the parliament . . . character disgust joy other strong joy character Figure 2: Example annotation from Stimson (1943), with two characters who are experiencers of different emo"
C18-1114,W17-2203,1,0.738293,"y, 2009; Hogan, 2015). Not only do emotions help readers in literature comprehension (Barton, 1996; Robinson, 2005) but they also improve readers’ abilities of empathy and understanding of others’ lives (Mar et al., 2009; Kidd and Castano, 2013). This makes literature an interesting resource for the study of emotions, hence there is a growing interest in emotion-oriented text analysis among digital humanities scholars. Emotion analysis and classification is a challenging task which has mostly been tackled with comparably straight-forward approaches, at least in literary studies. For instance, Kim et al. (2017) and Reagan et al. (2016) show that emotions, recognized with dictionaries or bag-of-words models, serve as features for genre classification in fiction, however, only with limited performance. One reason is, presumably, that such approaches assume linearity of the story and ignore the semantic role structure of emotions: who feels the emotion and why, what caused the emotion, what is the target of it (Scarantino, 2016; Russell and Barrett, 1999). Consider the sentence “Jack is afraid of John because John has a knife”. Following structural approaches to defining emotional episodes, the sentenc"
C18-1114,W14-2607,0,0.152684,"on from Stimson (1943), with two characters who are experiencers of different emotions. Disgust and joy are annotated as a mixture of emotions. Both emotions have the same target. Fewer works exist for English. Neviarouskaya and Aono (2013) annotate 500 sentences from an online forum with experiencer, emotion, and emotion cause and present a method for extracting linguistic relations between an emotion and its cause. Ghazi et al. (2015) collect exemplary sentences from FrameNet that have cause annotation and implement a model that extracts the causes of emotions. Following a similar approach, Mohammad et al. (2014) annotate Tweets for semantic roles. Conceptually, our work partially overlaps with the FactBank corpus (Saurí and Pustejovsky, 2009), where “who thinks what” is taken into account as well. However, in contrast to FactBank, we do not predefine event-selecting predicates for emotion causes and targets, as those are defined by the annotators. In this sense, our work is also different from aspect-based sentiment analysis, where aspects of reviewed products are often predefined. 3 Annotation Task The goal of the REMAN annotation project is to create a dataset of excerpts from fictional texts that"
C18-1114,I13-1121,0,0.297447,"house, he seemed surprised. event character surprise Figure 1: Example annotation from Hugo (1885), with one character, an emotion word, and event and cause and experiencer annotations. experiencer target target experiencer All laughed at the mistake, and none louder than the forth member of the parliament . . . character disgust joy other strong joy character Figure 2: Example annotation from Stimson (1943), with two characters who are experiencers of different emotions. Disgust and joy are annotated as a mixture of emotions. Both emotions have the same target. Fewer works exist for English. Neviarouskaya and Aono (2013) annotate 500 sentences from an online forum with experiencer, emotion, and emotion cause and present a method for extracting linguistic relations between an emotion and its cause. Ghazi et al. (2015) collect exemplary sentences from FrameNet that have cause annotation and implement a model that extracts the causes of emotions. Following a similar approach, Mohammad et al. (2014) annotate Tweets for semantic roles. Conceptually, our work partially overlaps with the FactBank corpus (Saurí and Pustejovsky, 2009), where “who thinks what” is taken into account as well. However, in contrast to Fact"
C18-1114,W11-1720,0,0.775943,"otations include the work by Alm et al. (2005), who annotate a corpus of children stories, and Strapparava and Mihalcea (2007), who label news headlines, but without specifying the textual markers of emotion. An early work which includes textual markers of emotions is Aman and Szpakowicz (2007), who annotate blogposts. Wiebe et al. (2005) annotate a corpus of news articles with emotions at a word and phrase level. Recent works have mainly diverged from plain emotion annotation, following the idea of emotion theorists (Russell, 2003, i.a.) that causes of emotions are inseparable from emotions: Russo et al. (2011) build a corpus of Italian newspaper articles annotated with emotion key words and emotion cause phrases. Both Mei et al. (2012) and Gui et al. (2016) construct emotion-cause-annotated corpora for Chinese. Chen et al. (2010) adopt a rule-based approach based on linguistic patterns to detect emotion causes in the annotated Chinese corpus. Gui et al. (2017) present a question-answering approach to emotion cause extraction, also for Chinese. 1346 cause experiencer When I mentioned the house, he seemed surprised. event character surprise Figure 1: Example annotation from Hugo (1885), with one char"
C18-1114,W17-5203,1,0.832768,"provides interesting and valuable insights in the language of emotion expression and, therefore, is useful to the community of linguists who are interested in the study of linguistic properties of emotions. However, we also note that developing such a resource has its limitations: Due to the subjective nature of emotions, it is challenging, if not impossible, to come up with an annotation methodology that would lead to less disparate annotations, especially if in addition to emotion, other categories should be annotated together with roles. That is in line with previous research. For instance Schuff et al. (2017) and Russo et al. (2011) found that aggregating labels by multiple annotators without a majority vote procedure but by merging is easier to model computationally. We tackle this problem by employing a multi-step procedure that helps to improve the agreement of the relation annotation. This does not help in the emotion annotation itself, but helps in the role assignment. The introduction of our multi-step annotation procedure lead to an increased inter-annotator agreement for experiencer and cause annotations by 13 pp and 5 pp in strict evaluation. This indicates that the task seems easier to a"
C18-1114,S07-1013,0,0.264808,"efits from joint prediction of experiencer, emotion words, and targets. 2 Related Work Emotions have strong linguistic markers that define the tone of the text (Johnson-Laird and Oatley, 1989). This allows for different granularities of emotion annotation. The corpus which originates from the ISEAR project (Scherer and Wallbott, 1994) is an example of document-level annotation that includes descriptions of situations in which respondents had experienced various emotions. Examples of sentencelevel annotations include the work by Alm et al. (2005), who annotate a corpus of children stories, and Strapparava and Mihalcea (2007), who label news headlines, but without specifying the textual markers of emotion. An early work which includes textual markers of emotions is Aman and Szpakowicz (2007), who annotate blogposts. Wiebe et al. (2005) annotate a corpus of news articles with emotions at a word and phrase level. Recent works have mainly diverged from plain emotion annotation, following the idea of emotion theorists (Russell, 2003, i.a.) that causes of emotions are inseparable from emotions: Russo et al. (2011) build a corpus of Italian newspaper articles annotated with emotion key words and emotion cause phrases. B"
C18-1114,P13-4001,0,0.0637136,"Missing"
C18-1179,P17-1067,0,0.257848,", 2018; Cowen and Keltner, 2018). We do not contribute to these debates but focus on the main theories in psychology and natural language processing (NLP): discrete and finite sets of emotions (categorical models) and combinations of different continuous dimensions (dimensional models). Early work on emotion detection (Alm et al., 2005; Strapparava and Mihalcea, 2007) focused on conceptualizing emotions by following Ekman’s model which assumes the following six basic emotions: anger, disgust, fear, joy, sadness and surprise (Ekman, 1992). Suttles and Ide (2013), Meo and Sulis 2105 (2017), and Abdul-Mageed and Ungar (2017) follow the Wheel of Emotion (Plutchik, 1980; Plutchik, 2001) which also considers emotions as a discrete set of eight basic emotions in four opposing pairs: joy–sadness, anger–fear, trust–disgust, and anticipation–surprise, together with emotion mixtures. Dimensional models were more recently adopted in NLP (Preot¸iuc-Pietro et al., 2016; Buechel and Hahn, 2017a; Buechel and Hahn, 2017b): The circumplex model (Russell and Mehrabian, 1977) puts affective states into a vector space of valence (corresponding to sentiment/polarity), arousal (corresponding to a degree of calmness or excitement), a"
C18-1179,H05-1073,0,0.768961,"those proposed by Ekman (1992), Plutchik (1980) or Russell (1980), inter alia. The task has emerged from a purely research oriented topic to playing a role in a variety of applications, which include dialog systems (chatbots, tutoring systems), intelligent agents, clinical diagnoses of mental disorders (Calvo et al., 2017), or social media mining. As the variety of applications is large, the set of domains and differences in text is large. An early work, motivated by the goal to develop an empathic storyteller for children stories, is the corpus creation and modelling of emotions in tales by Alm et al. (2005). Afterwards, the idea has been transferred to the Web, namely blogs (Aman and Szpakowicz, 2007), and microblogs (Schuff et al., 2017; Mohammad, 2012; Wang et al., 2012). A different domain under consideration are news articles: Strapparava and Mihalcea (2007) focus on emotions in headlines. It can be doubted that emotions are expressed in a comparable way in these different domains: Journalists ideally tend to be objective when writing articles, authors of microblog posts need to focus on brevity, and one might assume that emotion expressions in tales are more subtle and implicit than, for in"
C18-1179,E06-2031,0,0.072221,"Missing"
C18-1179,W17-5202,1,0.867324,"sed feature-based learning. Common features include word n-grams, character n-grams, word embeddings, affect lexicons, negation, punctuation, emoticons, or hashtags (Mohammad, 2012) . This feature representation is then usually used as input to feed classifiers such as naive Bayes, SVM (Mohammad, 2012), MaxEnt and others to predict the relevant emotion category (Aman and Szpakowicz, 2007; Alm et al., 2005). Similarly to the paradigm shift in sentiment analysis, from feature-based modelling to deep learning, state-of-the-art models for emotion classification are often based on neural networks (Barnes et al., 2017). Schuff et al. (2017) applied models from the classes of CNN, BiLSTM (Schuster and Paliwal, 1997), and LSTM (Hochreiter and Schmidhuber, 1997) and compare them to linear classifiers (SVM and MaxEnt), where the 2107 Dataset Granularity Annotation Size AffectiveText Blogs CrowdFlower DailyDialogs Electoral-Tweets EmoBank EmoInt Emotion-Stimulus fb-valence-arousal Grounded-Emotions ISEAR Tales SSEC TEC headlines sentences tweets dialogues tweets sentences tweets sentences faceb. posts tweets descriptions sentences tweets tweets E+V E + ne + me E + CF E P V+A+D E − DS E + shame V+A HS E + SG E P"
C18-1179,E17-2092,0,0.430337,"focused on conceptualizing emotions by following Ekman’s model which assumes the following six basic emotions: anger, disgust, fear, joy, sadness and surprise (Ekman, 1992). Suttles and Ide (2013), Meo and Sulis 2105 (2017), and Abdul-Mageed and Ungar (2017) follow the Wheel of Emotion (Plutchik, 1980; Plutchik, 2001) which also considers emotions as a discrete set of eight basic emotions in four opposing pairs: joy–sadness, anger–fear, trust–disgust, and anticipation–surprise, together with emotion mixtures. Dimensional models were more recently adopted in NLP (Preot¸iuc-Pietro et al., 2016; Buechel and Hahn, 2017a; Buechel and Hahn, 2017b): The circumplex model (Russell and Mehrabian, 1977) puts affective states into a vector space of valence (corresponding to sentiment/polarity), arousal (corresponding to a degree of calmness or excitement), and dominance (perceived degree of control over a given situation). Any emotion is a linear combination of these. 2.2 Annotation Procedures A standard way to create annotated datasets is via expert annotation (Aman and Szpakowicz, 2007; Strapparava and Mihalcea, 2007; Ghazi et al., 2015; Li et al., 2017; Schuff et al., 2017; Li et al., 2017). However, having an e"
C18-1179,W17-0801,0,0.586091,"focused on conceptualizing emotions by following Ekman’s model which assumes the following six basic emotions: anger, disgust, fear, joy, sadness and surprise (Ekman, 1992). Suttles and Ide (2013), Meo and Sulis 2105 (2017), and Abdul-Mageed and Ungar (2017) follow the Wheel of Emotion (Plutchik, 1980; Plutchik, 2001) which also considers emotions as a discrete set of eight basic emotions in four opposing pairs: joy–sadness, anger–fear, trust–disgust, and anticipation–surprise, together with emotion mixtures. Dimensional models were more recently adopted in NLP (Preot¸iuc-Pietro et al., 2016; Buechel and Hahn, 2017a; Buechel and Hahn, 2017b): The circumplex model (Russell and Mehrabian, 1977) puts affective states into a vector space of valence (corresponding to sentiment/polarity), arousal (corresponding to a degree of calmness or excitement), and dominance (perceived degree of control over a given situation). Any emotion is a linear combination of these. 2.2 Annotation Procedures A standard way to create annotated datasets is via expert annotation (Aman and Szpakowicz, 2007; Strapparava and Mihalcea, 2007; Ghazi et al., 2015; Li et al., 2017; Schuff et al., 2017; Li et al., 2017). However, having an e"
C18-1179,W16-0423,0,0.0408552,"Missing"
C18-1179,D17-1169,0,0.114683,"download, research only, [D-U] Available to download, unknown licensing, [R] Available upon request, [GPLv3] GNU Public License version 3, [CC-by 4] Creative Commons Attribution version 4.0 BiLSTM show best results with the most balanced precision and recall. Abdul-Mageed and Ungar (2017) claim the highest F1 following Plutchik’s emotion model with gated recurrent unit networks (Chung et al., 2015). One approach to tackle sparsity of datasets is transfer learning; to make use of similar resources and then transfer the model to the actual task. A recent successful example for this procedure is Felbo et al. (2017) who present a neural network model trained on emoticons which is then transfered to different downstream tasks, namely the prediction of sentiment, sarcasm, and emotions. 3 Unified Dataset of Emotion Annotations In this section, we describe each dataset we aggregate in our unified corpus. We provide a brief description and then show how the different schemata are merged. Please note that our interpretation might differ from the author’s original description (though we aimed at avoiding that). 3.1 Datasets Overview AffectiveText The dataset AffectiveText published by Strapparava and Mihalcea ("
C18-1179,W17-2203,1,0.833489,"Missing"
C18-1179,L16-1413,0,0.0605024,"Missing"
C18-1179,W17-5206,1,0.905208,"Missing"
C18-1179,W16-3708,0,0.149506,", route making use of self reporting: subjects are asked to describe situations associated with a specific emotion (Scherer and Wallbott, 1994). This approach can be considered an annotation by experts in their own right. Crowdsourcing, for instance using the platforms Amazon’s Mechanical Turk1 or CrowdFlower2 , is another way to acquire human judgments. Crowdsourcing often lacks sufficient quality control but some popular datasets have been successfully acquired with this approach, e. g., the dataset released by Crowdflower for Cortana3 or the datasets constructed by Milnea et al. (2015) and Lapitan et al. (2016). Another example is the dataset by Mohammad (2012), who design two detailed online questionnaires and annotate tweets by crowdsourcing. Lastly, social networks play a central role in data acquisition with distant supervision (also called self-labeling in this context), because they provide a quick and cheap way to get large amounts of noisy data annotated by writers or readers (Mohammad and Kiritchenko, 2015; Abdul-Mageed and Ungar, 2017; De Choudhury et al., 2012; Liu et al., 2017). For example, on Twitter one could add a “#joy” hashtag to a happy tweet or on Facebook one could tag personal"
C18-1179,I17-1099,0,0.126938,"cently adopted in NLP (Preot¸iuc-Pietro et al., 2016; Buechel and Hahn, 2017a; Buechel and Hahn, 2017b): The circumplex model (Russell and Mehrabian, 1977) puts affective states into a vector space of valence (corresponding to sentiment/polarity), arousal (corresponding to a degree of calmness or excitement), and dominance (perceived degree of control over a given situation). Any emotion is a linear combination of these. 2.2 Annotation Procedures A standard way to create annotated datasets is via expert annotation (Aman and Szpakowicz, 2007; Strapparava and Mihalcea, 2007; Ghazi et al., 2015; Li et al., 2017; Schuff et al., 2017; Li et al., 2017). However, having an expert annotate a statement means that they must estimate the private state of the author. Therefore, the creators of the ISEAR dataset follow a different, but similar, route making use of self reporting: subjects are asked to describe situations associated with a specific emotion (Scherer and Wallbott, 1994). This approach can be considered an annotation by experts in their own right. Crowdsourcing, for instance using the platforms Amazon’s Mechanical Turk1 or CrowdFlower2 , is another way to acquire human judgments. Crowdsourcing of"
C18-1179,S17-1007,0,0.100005,"s, such as health, politics (Mohammad, 2012), and stock markets (Bollen et al., 2011). An early example and one of the first initiatives of emotion classification is the work by Aman and Szpakowicz (2007), who use blog posts, sampled without taking a specific topic into account. They identify the emotion, category, intensity and cue words and phrases. Mishne and de Rijke (2005), Balog et al. (2006) and Nguyen et al. (2014) works on LiveJournal4 data to develop predictive models for moods. Similarly, user-generated data in social media has been a subject of research. Mohammad et al. (2015) and Mohammad and Bravo-Marquez (2017b) annotate electoral tweets for sentiment, intensity, semantic 1 https://www.mturk.com/ https://www.crowdflower.com/ 3 https://www.crowdflower.com/data/sentiment-analysis-emotion-text/ 4 https://www.livejournal.com 2 2106 roles, style, purpose and emotions. De Choudhury et al. (2012) identify more than 200 moods frequent on Twitter. Mohammad (2012), Mohammad et al. (2015), Wang et al. (2012), Volkova and Bachrach (2016) make use of Twitter distantly labeled data. Recently, Liu et al. (2017) analyzed the role of context that grounds sentiment in tweets, and looked into whether the effect of we"
C18-1179,W17-5205,0,0.089173,"rrett et al., 2018; Cowen and Keltner, 2018). We do not contribute to these debates but focus on the main theories in psychology and natural language processing (NLP): discrete and finite sets of emotions (categorical models) and combinations of different continuous dimensions (dimensional models). Early work on emotion detection (Alm et al., 2005; Strapparava and Mihalcea, 2007) focused on conceptualizing emotions by following Ekman’s model which assumes the following six basic emotions: anger, disgust, fear, joy, sadness and surprise (Ekman, 1992). Suttles and Ide (2013), Meo and Sulis 2105 (2017), and Abdul-Mageed and Ungar (2017) follow the Wheel of Emotion (Plutchik, 1980; Plutchik, 2001) which also considers emotions as a discrete set of eight basic emotions in four opposing pairs: joy–sadness, anger–fear, trust–disgust, and anticipation–surprise, together with emotion mixtures. Dimensional models were more recently adopted in NLP (Preot¸iuc-Pietro et al., 2016; Buechel and Hahn, 2017a; Buechel and Hahn, 2017b): The circumplex model (Russell and Mehrabian, 1977) puts affective states into a vector space of valence (corresponding to sentiment/polarity), arousal (corresponding to a d"
C18-1179,S13-2053,0,0.0244435,"ssification in general, the array of methods seen for emotion classification can be divided into rule-based methods and machine learning, which we discuss in the following. 2.4.1 Rule-based Algorithms Rule-based text classification typically builds on top of lexical resources of emotionally charged words. These dictionaries can originate from crowdsourcing or expert curation. Examples include WordNetAffect (Strapparava et al., 2004) and SentiWordNet (Esuli and Sebastiani, 2007), both of which stem from expert annotation. Partly built on top of them is the NRC Word-Emotion Association Lexicon (Mohammad et al., 2013), which uses the eight basic emotions (Plutchik, 1980). Warriner et al. (2013) use crowdsourcing to assign values of valence, arousal, and dominance (Russell, 1980). Another related category of lexical resources which has been used for emotion analysis is concreteness and abstractness (K¨oper et al., 2017). Brysbaert et al. (2014) publish a lexicon based on crowdsourcing, where the task was to assign a rating from 1 to 5 of the concreteness of 40,000 words. Similarly, K¨oper and Schulte im Walde (2016) automatically generate affective norms of abstractness, arousal, imageability, and valence f"
C18-1179,S12-1033,0,0.774605,"role in a variety of applications, which include dialog systems (chatbots, tutoring systems), intelligent agents, clinical diagnoses of mental disorders (Calvo et al., 2017), or social media mining. As the variety of applications is large, the set of domains and differences in text is large. An early work, motivated by the goal to develop an empathic storyteller for children stories, is the corpus creation and modelling of emotions in tales by Alm et al. (2005). Afterwards, the idea has been transferred to the Web, namely blogs (Aman and Szpakowicz, 2007), and microblogs (Schuff et al., 2017; Mohammad, 2012; Wang et al., 2012). A different domain under consideration are news articles: Strapparava and Mihalcea (2007) focus on emotions in headlines. It can be doubted that emotions are expressed in a comparable way in these different domains: Journalists ideally tend to be objective when writing articles, authors of microblog posts need to focus on brevity, and one might assume that emotion expressions in tales are more subtle and implicit than, for instance, in blogs. Therefore, the transfer across emotion recognition models is, presumably, challenging. The most straight-forward alternative is, ho"
C18-1179,P11-1157,0,0.0296198,"Missing"
C18-1179,W16-4304,0,0.138184,"e of Twitter distantly labeled data. Recently, Liu et al. (2017) analyzed the role of context that grounds sentiment in tweets, and looked into whether the effect of weather and news events relate to the emotion expressed in a given tweet. EmoNet is claimed to be the largest dataset constructed of tweets (Abdul-Mageed and Ungar, 2017) . Twitter is often the preferred subject of research as it is easy to use and has a well-documented API. However, Facebook is also used, e. g., Preot¸iuc-Pietro et al. (2016) create a dataset of Facebook posts and train prediction models for valence and arousal. Pool and Nissim (2016) and Krebs et al. (2017) make use of the reaction feature in Facebook to collect labeled data for distant supervision of a classifier. A different approach within the same domain was used by Polignano et al. (2017) who labeled posts with emoticons mapped to Ekman’s model. Data in social media can be in the form of dialogues. Li et al. (2017) manually label a dataset of conversations. Wang et al. (2016) introduce EmotionPush, a system that automatically conveys the emotion of received text on mobile devices, deployed on Facebook’s messenger app. From the same domain, but on a different topic is"
C18-1179,W16-0404,0,0.348962,"Missing"
C18-1179,D17-1038,0,0.0226354,"any corpora, joy is the dominating emotion, followed by sadness, surprise, and anger. Exceptions are SSEC, Electoral-Tweets, and EmoInt, in which negative emotions are more frequent. In SSEC, this is because of its origin as a stance dataset. Similarly, Electoral-Tweets shows a polarizing nature of political debates with disgust and anger being more common. Figure 1 shows a quantitative similarity comparison of the data. We represent each dataset by its term distribution, taking the top 5,000 most common words from each dataset and calculating the cosine similarity across corpora (inspired by Ruder and Plank (2017) and Plank and Van Noord (2011)). Twitter corpora are more similar to each other (EmoInt and CrowdFlower are the most similar to TEC) than to other domains with the exception of SSEC, which is the most dissimilar to the other tweet datasets. DailyDialogs is more similar to the tweets than to ISEAR and Blogs. The column All stands for the union of all datasets except the one that is being compared to. In this context, the most dissimilar towards the respective aggregated set is AffectiveText. The reason is that this is a small dataset compared to the tweet-based corpora and that it covers a spe"
C18-1179,W17-5203,1,0.665792,"Missing"
C18-1179,S07-1013,0,0.654326,"intelligent agents, clinical diagnoses of mental disorders (Calvo et al., 2017), or social media mining. As the variety of applications is large, the set of domains and differences in text is large. An early work, motivated by the goal to develop an empathic storyteller for children stories, is the corpus creation and modelling of emotions in tales by Alm et al. (2005). Afterwards, the idea has been transferred to the Web, namely blogs (Aman and Szpakowicz, 2007), and microblogs (Schuff et al., 2017; Mohammad, 2012; Wang et al., 2012). A different domain under consideration are news articles: Strapparava and Mihalcea (2007) focus on emotions in headlines. It can be doubted that emotions are expressed in a comparable way in these different domains: Journalists ideally tend to be objective when writing articles, authors of microblog posts need to focus on brevity, and one might assume that emotion expressions in tales are more subtle and implicit than, for instance, in blogs. Therefore, the transfer across emotion recognition models is, presumably, challenging. The most straight-forward alternative is, however, to build resources from scratch, a costly process. Given this situation, it remains unclear, given a nov"
C18-1179,strapparava-valitutti-2004-wordnet,0,0.45858,"Missing"
C18-1179,P16-1148,0,0.0620576,"rks on LiveJournal4 data to develop predictive models for moods. Similarly, user-generated data in social media has been a subject of research. Mohammad et al. (2015) and Mohammad and Bravo-Marquez (2017b) annotate electoral tweets for sentiment, intensity, semantic 1 https://www.mturk.com/ https://www.crowdflower.com/ 3 https://www.crowdflower.com/data/sentiment-analysis-emotion-text/ 4 https://www.livejournal.com 2 2106 roles, style, purpose and emotions. De Choudhury et al. (2012) identify more than 200 moods frequent on Twitter. Mohammad (2012), Mohammad et al. (2015), Wang et al. (2012), Volkova and Bachrach (2016) make use of Twitter distantly labeled data. Recently, Liu et al. (2017) analyzed the role of context that grounds sentiment in tweets, and looked into whether the effect of weather and news events relate to the emotion expressed in a given tweet. EmoNet is claimed to be the largest dataset constructed of tweets (Abdul-Mageed and Ungar, 2017) . Twitter is often the preferred subject of research as it is easy to use and has a well-documented API. However, Facebook is also used, e. g., Preot¸iuc-Pietro et al. (2016) create a dataset of Facebook posts and train prediction models for valence and a"
C18-1179,C16-2030,0,0.0231532,"e and has a well-documented API. However, Facebook is also used, e. g., Preot¸iuc-Pietro et al. (2016) create a dataset of Facebook posts and train prediction models for valence and arousal. Pool and Nissim (2016) and Krebs et al. (2017) make use of the reaction feature in Facebook to collect labeled data for distant supervision of a classifier. A different approach within the same domain was used by Polignano et al. (2017) who labeled posts with emoticons mapped to Ekman’s model. Data in social media can be in the form of dialogues. Li et al. (2017) manually label a dataset of conversations. Wang et al. (2016) introduce EmotionPush, a system that automatically conveys the emotion of received text on mobile devices, deployed on Facebook’s messenger app. From the same domain, but on a different topic is the study of patients’ emotional states dynamics expressed by their Facebook posts (Lombardo et al., 2017). Motivated by the work of literary scholars is the creation of datasets to study emotion in literature. One of the first datasets is the tales corpus by Alm et al. (2005). Kim et al. (2017) investigate the relationship between literary genres and emotions. 2.4 Methods used in Emotion Identificati"
D13-1179,P07-2045,0,0.00343338,"Missing"
D13-1179,2005.mtsummit-papers.11,0,0.05364,"Missing"
D13-1179,P10-1116,0,0.0732658,"Missing"
D13-1179,D09-1092,0,0.153583,"Missing"
D13-1179,palmer-etal-1998-rapid,0,0.0865569,"citly defined topics, but then computes latent relations between these. Thus, the method combines the benefits of both explicit and latent topic modelling approaches. We show that on a crosslingual mate retrieval task, our model significantly outperforms LDA, LSI, and ESA, as well as a baseline that translates every word in a document into the target language. 1 Introduction Cross-lingual document matching is the task of, given a query document in some source language, estimating the similarity to a document in some target language. This task has important applications in machine translation (Palmer et al., 1998; Tam et al., 2007), word sense disambiguation (Li et al., 2010) and ontology alignment (Spiliopoulos et al., 2007). An approach that has become quite popular in recent years for cross-lingual document matching is Explicit Semantics Analysis (ESA, Gabrilovich and Markovitch (2007)) and its cross-lingual extension A key choice in Explicit Semantic Analysis is the document space that will act as the topic space. The standard choice is to regard all articles from a background document collection – Wikipedia articles are a typical choice – as the topic space. However, it is crucial to ensure that"
D18-2008,W09-1403,0,0.0134523,"f the proof-of-concept system for aspect based sentiment analysis (10-fold crossvalidation on USAGE corpus). ery. The evaluation calculates the F1 scores for the individual frames (events in the BioNLP task) using a soft matching for trigger boundaries and approximate recursive matching. Table 1 provides the results of our simple system on that task. Due to the restriction of our proof of concept to nonrecursive structures (cf. Section 4), we only report on the BioNLP event types where all slots are filled by spans. In comparison to the second-ranked system, which also reports results on dev (Buyko et al., 2009), our performance is slightly lower (1 percentage point less for protein catabolism, 13pp less for gene expression and phosphorylation, but 11pp more for localization). This confirms the general usability of our general method. Correspondingly, Table 2 provides the current results of the same model on the USAGE corpus for aspect based sentiment analysis (Klinger and Cimiano, 2014), with 10-fold crossvalidation on the English subset. In comparison to previous results, our numbers are very low. Previous work showed that this is tackled by joint inference, which we did not implement yet (Klinger"
D18-2008,W02-0109,0,0.194921,"for which we re-use the original evaluation machinWe generate negative examples automatically. 45 Event Class Precision Recall F1 Gene expression Transcription Protein catabolism Phosphorylation Localization 68.12 70.59 64.00 65.85 78.57 57.30 14.63 76.19 57.45 41.51 62.25 24.24 69.57 61.36 54.32 SVT-TOTAL 68.46 50.27 57.97 The choice of Python will also help with future integration of neural network models. For the proofof-concept backend, we use scikit-learn for feature extraction and training (Pedregosa et al., 2011) with crfsuite and liblinear. Tokenization and stemming is done with NLTK (Loper and Bird, 2002), dependency features are extracted with spacy (Honnibal and Johnson, 2015) and dependency graphs are stored and processed using NetworkX (Schult, 2008). The code is available under the Apache 2.0 License.2 Table 1: Performance of the proof-of-concept system for biomedical relation extraction (BioNLP’09 dev set) Sentiment Class Positive Negative Neutral Precision Recall F1 41.07 26.68 5.83 24.19 7.15 4.50 28.57 11.00 5.08 5 Conclusion and Future Work This paper presented D E RE, a general framework for declarative specification and compilation of template-based slot filling. It addresses the n"
D18-2008,clarke-etal-2012-nlp,0,0.0149051,"g (System Demonstrations), pages 42–47 c Brussels, Belgium, October 31–November 4, 2018. 2018 Association for Computational Linguistics IL-4 gene regulation in general involves the nuclear factor of activated T cells (NFAT) family of transcription factors, of which NFAT1 and NFAT2 are most prominent in peripheral T cells. tems we are aware of for solving these tasks are tailored to specific scenarios (Angeli et al., 2016; Adel et al., 2016, i.a.). As a result, it is not straightforward to apply them to other use cases. In contrast, our framework is designed to be task- and domain-independent. Clarke et al. (2012) develop an NLP component manager which combines several existing NLP tools in a pipeline. Similarly, Curran (2003) aims at a general NLP infrastructure but only reports implementations of non-relational sequence-tagging tasks. Examples of the few available toolkits which are intended to provide users with the possibility of automatically extracting information from text data are Jet (Java Extraction Toolkit), GATE (General Architecture for Text Engineering, Cunningham et al., 2013), UIMA (Unstructured Information Management Architecture, Ferrucci and Lally, 2004), FACTORIE (McCallum et al., 2"
D18-2008,N16-1030,0,0.0229713,"ble to apply D E RE to a large variety of natural language processing tasks, such as unary, binary and n-ary relation extraction, event extraction, semantic role labeling, aspect-based sentiment analysis, etc. As BRAT annotations are not as expressive as our task schema files, we plan to extend the frontend of D E RE by supporting a native, XML-based annotation format in the future. For the backend, our goal is to develop a variety of state-of-the-art models with joint span identification, slot classification, and frame decoding, e.g., neural networks with structured-prediction output layers (Lample et al., 2016; Adel and Sch¨utze, 2017, i.a.). Given a variety of different models and tasks, we will be able to address interesting research questions, such a transfer learning and joint learning across tasks and domains. We plan to further analyze the usage of D E RE and the possibilities it provides for integrating different model types and configurations in a multi-task oriented shared task. Table 2: Performance of the proof-of-concept system for aspect based sentiment analysis (10-fold crossvalidation on USAGE corpus). ery. The evaluation calculates the F1 scores for the individual frames (events in t"
D18-2008,P09-1113,0,0.0786889,"n be instantiated by different models, following different paradigms. The clear separation of frame specification and model backend will ease the implementation of new models and the evaluation of different models across different tasks. Furthermore, it simplifies transfer learning, joint learning across tasks and/or domains as well as the assessment of model generalizability. D E RE is available as open-source software. 1 Introduction A large number of tasks in natural language processing (NLP) are information extraction (IE) tasks, such as n-ary relation extraction (Doddington et al., 2004; Mintz et al., 2009; Hendrickx et al., 2010), semantic role labeling (Das et al., 2014) and event extraction (Kim et al., 2009; Doddington et al., 2004). Researchers address these tasks with a variety of different model paradigms, such as support vector machines (Rink and Harabagiu, 2010), convolutional neural networks (Collobert et al., 2011; Zeng et al., 2014) and recurrent neural networks (Tang et al., 2015; Nguyen et al., 2016). This landscape of different tasks and models gives rise to four challenges: (C1) Lack of gener∗ All authors contributed equally. 42 Proceedings of the 2018 Conference on Empirical Me"
D18-2008,N16-1034,0,0.0302282,"software. 1 Introduction A large number of tasks in natural language processing (NLP) are information extraction (IE) tasks, such as n-ary relation extraction (Doddington et al., 2004; Mintz et al., 2009; Hendrickx et al., 2010), semantic role labeling (Das et al., 2014) and event extraction (Kim et al., 2009; Doddington et al., 2004). Researchers address these tasks with a variety of different model paradigms, such as support vector machines (Rink and Harabagiu, 2010), convolutional neural networks (Collobert et al., 2011; Zeng et al., 2014) and recurrent neural networks (Tang et al., 2015; Nguyen et al., 2016). This landscape of different tasks and models gives rise to four challenges: (C1) Lack of gener∗ All authors contributed equally. 42 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (System Demonstrations), pages 42–47 c Brussels, Belgium, October 31–November 4, 2018. 2018 Association for Computational Linguistics IL-4 gene regulation in general involves the nuclear factor of activated T cells (NFAT) family of transcription factors, of which NFAT1 and NFAT2 are most prominent in peripheral T cells. tems we are aware of for solving these tasks are tailored"
D18-2008,S10-1057,0,0.0247273,"s transfer learning, joint learning across tasks and/or domains as well as the assessment of model generalizability. D E RE is available as open-source software. 1 Introduction A large number of tasks in natural language processing (NLP) are information extraction (IE) tasks, such as n-ary relation extraction (Doddington et al., 2004; Mintz et al., 2009; Hendrickx et al., 2010), semantic role labeling (Das et al., 2014) and event extraction (Kim et al., 2009; Doddington et al., 2004). Researchers address these tasks with a variety of different model paradigms, such as support vector machines (Rink and Harabagiu, 2010), convolutional neural networks (Collobert et al., 2011; Zeng et al., 2014) and recurrent neural networks (Tang et al., 2015; Nguyen et al., 2016). This landscape of different tasks and models gives rise to four challenges: (C1) Lack of gener∗ All authors contributed equally. 42 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (System Demonstrations), pages 42–47 c Brussels, Belgium, October 31–November 4, 2018. 2018 Association for Computational Linguistics IL-4 gene regulation in general involves the nuclear factor of activated T cells (NFAT) family of t"
D18-2008,doddington-etal-2004-automatic,0,0.146517,"hallenges. The backend can be instantiated by different models, following different paradigms. The clear separation of frame specification and model backend will ease the implementation of new models and the evaluation of different models across different tasks. Furthermore, it simplifies transfer learning, joint learning across tasks and/or domains as well as the assessment of model generalizability. D E RE is available as open-source software. 1 Introduction A large number of tasks in natural language processing (NLP) are information extraction (IE) tasks, such as n-ary relation extraction (Doddington et al., 2004; Mintz et al., 2009; Hendrickx et al., 2010), semantic role labeling (Das et al., 2014) and event extraction (Kim et al., 2009; Doddington et al., 2004). Researchers address these tasks with a variety of different model paradigms, such as support vector machines (Rink and Harabagiu, 2010), convolutional neural networks (Collobert et al., 2011; Zeng et al., 2014) and recurrent neural networks (Tang et al., 2015; Nguyen et al., 2016). This landscape of different tasks and models gives rise to four challenges: (C1) Lack of gener∗ All authors contributed equally. 42 Proceedings of the 2018 Confer"
D18-2008,E12-2021,0,0.032324,"Missing"
D18-2008,S10-1006,1,0.839272,"Missing"
D18-2008,D15-1167,0,0.0310034,"able as open-source software. 1 Introduction A large number of tasks in natural language processing (NLP) are information extraction (IE) tasks, such as n-ary relation extraction (Doddington et al., 2004; Mintz et al., 2009; Hendrickx et al., 2010), semantic role labeling (Das et al., 2014) and event extraction (Kim et al., 2009; Doddington et al., 2004). Researchers address these tasks with a variety of different model paradigms, such as support vector machines (Rink and Harabagiu, 2010), convolutional neural networks (Collobert et al., 2011; Zeng et al., 2014) and recurrent neural networks (Tang et al., 2015; Nguyen et al., 2016). This landscape of different tasks and models gives rise to four challenges: (C1) Lack of gener∗ All authors contributed equally. 42 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (System Demonstrations), pages 42–47 c Brussels, Belgium, October 31–November 4, 2018. 2018 Association for Computational Linguistics IL-4 gene regulation in general involves the nuclear factor of activated T cells (NFAT) family of transcription factors, of which NFAT1 and NFAT2 are most prominent in peripheral T cells. tems we are aware of for solving th"
D18-2008,D15-1162,0,0.0244088,"e examples automatically. 45 Event Class Precision Recall F1 Gene expression Transcription Protein catabolism Phosphorylation Localization 68.12 70.59 64.00 65.85 78.57 57.30 14.63 76.19 57.45 41.51 62.25 24.24 69.57 61.36 54.32 SVT-TOTAL 68.46 50.27 57.97 The choice of Python will also help with future integration of neural network models. For the proofof-concept backend, we use scikit-learn for feature extraction and training (Pedregosa et al., 2011) with crfsuite and liblinear. Tokenization and stemming is done with NLTK (Loper and Bird, 2002), dependency features are extracted with spacy (Honnibal and Johnson, 2015) and dependency graphs are stored and processed using NetworkX (Schult, 2008). The code is available under the Apache 2.0 License.2 Table 1: Performance of the proof-of-concept system for biomedical relation extraction (BioNLP’09 dev set) Sentiment Class Positive Negative Neutral Precision Recall F1 41.07 26.68 5.83 24.19 7.15 4.50 28.57 11.00 5.08 5 Conclusion and Future Work This paper presented D E RE, a general framework for declarative specification and compilation of template-based slot filling. It addresses the needs of three groups of users: backend model developers, developers of info"
D18-2008,P13-1161,0,0.0340886,"s slightly lower (1 percentage point less for protein catabolism, 13pp less for gene expression and phosphorylation, but 11pp more for localization). This confirms the general usability of our general method. Correspondingly, Table 2 provides the current results of the same model on the USAGE corpus for aspect based sentiment analysis (Klinger and Cimiano, 2014), with 10-fold crossvalidation on the English subset. In comparison to previous results, our numbers are very low. Previous work showed that this is tackled by joint inference, which we did not implement yet (Klinger and Cimiano, 2013; Yang and Cardie, 2013). However, this proof-of-concept implementation of the same model already shows the reusability of our framework by only changing the task schema specification. It motivates and enables further research on reusable models across tasks with different needs. Technical Details and Availability. The framework is implemented in Python, following an object-oriented design for frontend and backends to support easy interchangeability of components. Acknowledgments This work has been partially funded by the German Research Council (DFG), projects KL 2869/1-1 and PA 1956/4-1. 2 http://www.ims.uni-stuttg"
D18-2008,C14-1220,0,0.0130071,"essment of model generalizability. D E RE is available as open-source software. 1 Introduction A large number of tasks in natural language processing (NLP) are information extraction (IE) tasks, such as n-ary relation extraction (Doddington et al., 2004; Mintz et al., 2009; Hendrickx et al., 2010), semantic role labeling (Das et al., 2014) and event extraction (Kim et al., 2009; Doddington et al., 2004). Researchers address these tasks with a variety of different model paradigms, such as support vector machines (Rink and Harabagiu, 2010), convolutional neural networks (Collobert et al., 2011; Zeng et al., 2014) and recurrent neural networks (Tang et al., 2015; Nguyen et al., 2016). This landscape of different tasks and models gives rise to four challenges: (C1) Lack of gener∗ All authors contributed equally. 42 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (System Demonstrations), pages 42–47 c Brussels, Belgium, October 31–November 4, 2018. 2018 Association for Computational Linguistics IL-4 gene regulation in general involves the nuclear factor of activated T cells (NFAT) family of transcription factors, of which NFAT1 and NFAT2 are most prominent in periph"
D18-2008,W09-1401,0,0.454261,"tion and model backend will ease the implementation of new models and the evaluation of different models across different tasks. Furthermore, it simplifies transfer learning, joint learning across tasks and/or domains as well as the assessment of model generalizability. D E RE is available as open-source software. 1 Introduction A large number of tasks in natural language processing (NLP) are information extraction (IE) tasks, such as n-ary relation extraction (Doddington et al., 2004; Mintz et al., 2009; Hendrickx et al., 2010), semantic role labeling (Das et al., 2014) and event extraction (Kim et al., 2009; Doddington et al., 2004). Researchers address these tasks with a variety of different model paradigms, such as support vector machines (Rink and Harabagiu, 2010), convolutional neural networks (Collobert et al., 2011; Zeng et al., 2014) and recurrent neural networks (Tang et al., 2015; Nguyen et al., 2016). This landscape of different tasks and models gives rise to four challenges: (C1) Lack of gener∗ All authors contributed equally. 42 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (System Demonstrations), pages 42–47 c Brussels, Belgium, October 31–N"
K15-1016,W14-4203,0,0.0309241,"Missing"
K15-1016,W11-2104,0,0.0698541,"Missing"
K15-1016,D11-1033,0,0.0380362,"arallel resources are available containing pertinent annotations, models can be transfered after training. Early work includes a cross-lingual parser adaption (Zeman and Resnik, 2008). A recent example is the projection of a metaphor detection model using a bilingual dictionary (Tsvetkov et al., 2014). A combination of model transfer and annotation projection for dependency parsing has been proposed by Kozhevnikov and Titov (2014). To improve quality of the overall corpus of projected annotations, the selection of data points for dependency parsing has been studied (Søgaard, 2011). Similarly, Axelrod et al. (2011) improve the average quality of machine translation systems by selection of promising training examples and show that such a selection approach has a positive impact. Related to the latter, a generic instance weighting scheme has been proposed for domain adaptation (Jiang and Zhai, 2007). Other work has attempted to exploit information available in multiple languages to induce a model for a language for which sufficient training data is not available. For instance, universal tag sets take advantage of annotations that are aligned across languages (Snyder et al., 2008). Delexicalization allows"
K15-1016,P11-1022,0,0.0132309,"dictionaries for languages with scarce resources can be supported by bootstrapping approaches (Banea et al., 2008). Estimating the quality of machine translation can be understood as a ranking problem and thus be modeled as regression or classification. An important research focus is on investigating the impact of different features on predicting translation quality. For instance, sentence length, the output probability, number of unknown words of a target language as well as parsing-based features have been used (Avramidis et al., 2011). The alignment context can also be taken into account (Bach et al., 2011). An overview on confidence measures for machine translation is for instance provided by Ueffing et al. (2003). The impact of different features has been analyzed by Shah et al. (2013). A complete system and framework for quality estimation (including a list of possible features) is QuEst (Specia et al., 2013). For an overview of other cross-lingual applications and methods, we refer to Bikel and Zitouni (2012). 5 Conclusion and Future Work We have presented an approach that alleviates the need of training data for a target language when adapting a fine-grained sentiment analysis system to a n"
K15-1016,banea-etal-2008-bootstrapping,0,0.0326727,"models. Balahur and Turchi (2014) analyzed the impact of using different machine translation approaches in such settings. Differences in sentiment expressions have been analyzed between English and Dutch (Bal et al., 2011). Co-training with nonannotated corpora has been shown to yield good results for Chinese (Wan, 2009). Ghorbel (2012) analyzed the impact of automatic translation on sentiment analysis. Finally, SentiWordNet has been used for multilingual sentiment analysis (Denecke, 2008). Building dictionaries for languages with scarce resources can be supported by bootstrapping approaches (Banea et al., 2008). Estimating the quality of machine translation can be understood as a ranking problem and thus be modeled as regression or classification. An important research focus is on investigating the impact of different features on predicting translation quality. For instance, sentence length, the output probability, number of unknown words of a target language as well as parsing-based features have been used (Avramidis et al., 2011). The alignment context can also be taken into account (Bach et al., 2011). An overview on confidence measures for machine translation is for instance provided by Ueffing"
K15-1016,P11-2018,0,0.031699,"maller degree. This can as well be observed in the number of predictions the models based on different thresholds generate: While the number of true positive aspects for the coffee machine subdomain is 1100, only 221 are predicted with a threshold of the manual quality assignment of 0. However, a treshold of 9 leads to 560 predictions and a threshold of 10 to 1291. This effect can be observed for subjective phrases as well. It increases from 465 to 827 while the gold number is 676. These observations hold for all filtering methods analogously. 4 Related Work classification (Choi et al., 2010; Johansson and Moschitti, 2011; Yang and Cardie, 2012; Hu and Liu, 2004; Li et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010b). Such models are typically trained or optimized on manually annotated data (Klinger and Cimiano, 2013; Yang and Cardie, 2012; Jakob and Gurevych, 2010a; Zhang et al., 2011). The necessary data, at least containing fine-grained annotations for aspects and subjective phrases instead of only an overall polarity score, are mainly available for the English language to a sufficient extent. Popular corpora used for training are for instance the J.D. Power and Associates Sentiment Corpora"
K15-1016,C10-1004,0,0.0219846,"nte, 2012). The “Multilingual Subjectivity Analysis Gold Standard Data Set” focuses on subjectivity in the news domain (Balahur and Steinberger, 2009). A Chinese corpus annotated at the aspect and subjective phrase level is described by Zhao et al. (2014). There has not been too much work on approaches to transfer a model either directly or via annotation projection in the area of sentiment analysis. One example is based on sentence level annotations which are automatically translated to yield a resource in another language. This approach has been proven to work well across several languages (Banea et al., 2010; Mihalcea et al., 2007; Balahur and Turchi, 2014). Recent work approached multilingual opinion mining on the above-mentioned multi-lingual Youtube corpus with tree kernels predicting the polarity of a comment and whether it concerns the product or the video in which the product is featured. (Severyn et al., 2015). Brooke et al. (2009) compare dictionary and classification transfer from English to Spanish in a similar classification setting. In-target-language training approaches for finegrained sentiment analysis include those targeting the extraction of phrases or modelling it as text 159 Wh"
K15-1016,R09-1010,0,0.024722,"ctly or via annotation projection in the area of sentiment analysis. One example is based on sentence level annotations which are automatically translated to yield a resource in another language. This approach has been proven to work well across several languages (Banea et al., 2010; Mihalcea et al., 2007; Balahur and Turchi, 2014). Recent work approached multilingual opinion mining on the above-mentioned multi-lingual Youtube corpus with tree kernels predicting the polarity of a comment and whether it concerns the product or the video in which the product is featured. (Severyn et al., 2015). Brooke et al. (2009) compare dictionary and classification transfer from English to Spanish in a similar classification setting. In-target-language training approaches for finegrained sentiment analysis include those targeting the extraction of phrases or modelling it as text 159 While cross-lingual annotation projection has been investigated in the context of polarity computation, we are only aware of two approaches exploiting cross-lingual annotation projection on the task of identifying aspects specifically with an evaluation on manually annotated data in more than one language. The CLOpinionMiner (Zhou et al."
K15-1016,P13-2147,1,0.935166,"n training with the projected training data, there is no beneficial effect of the filtering. In the following, we describe our methodology in detail, including the description of the machine translation, annotation projection, and quality estimation methods (Section 2), and present the evaluation on manually annotated data (Section 3). Related work is discussed in Section 4. We conclude with Section 5 and mention promising future steps. 2 2.1 Methods Supervised Model for Aspect and Subjective Phrase Detection phrases and their relations. The structure follows the proposed pipeline approach by Klinger and Cimiano (2013).1 However, in contrast to their work, we focus on the detection of phrases only, and exploit the detection of relations only during inference, such that the detection of relations has an effect on the detection of phrases, but is not evaluated directly. The phrase detection follows the idea of semiMarkov conditional random fields (Sarawagi and Cohen, 2004; Yang and Cardie, 2012) and models phrases as spans over tokens as variables. Factor templates for spans of type aspect and subjective take into account token strings, prefixes, suffixes, the inclusion of digits, and part-of-speech tags, bot"
K15-1016,klinger-cimiano-2014-usage,1,0.94017,"jected data to train a supervised model crucially depends on the quality of the translations and alignments. In order to reduce the impact of spurious translations, we filter out low-quality sentence pairs. To estimate this quality, we take three measures into consideration (following approaches described by Shah and Specia (2014), in addition to a manual assessment of the translation quality as an upper baseline): 2 Note that the learning is independent from the actual value for all 0 &lt; α &lt; (maxg∈Corpus |g|)−1 . 3 www.statmt.org/moses/ 4 These example domains are taken from the USAGE corpus (Klinger and Cimiano, 2014), which is used in Section 3. 155 1. The probability of the sentence in the source language given a language model build on unannotated text in the source language (measuring if the language to be translated is typical, referred to as Source LM). 2. The probability of the machine translated sentence given a language model built on unanno5 https://cloud.google.com/translate/ # reviews en de coffee machine cutlery microwave toaster trash can vacuum cleaner washing machine dish washer 75 49 100 100 100 51 49 98 108 72 100 4 99 140 88 0 Table 1: Frequencies of the corpus used in our experiments (K"
K15-1016,2005.mtsummit-papers.11,0,0.073372,"require a corpus annotated for some source language and a translation from the source to a target language. As the availability of a parallel training corpus cannot be assumed in general, we use statistical machine translation (SMT) methods, relying on phrase-based translation models that use large amounts of parallel data for training (Koehn, 2010). While using an open-source system such as Moses3 would have been an option, we note that the quality would be limited by whether the system can be trained on a representative corpus. A standard dataset that SMT systems are trained on is EuroParl (Koehn, 2005). EuroParl covers 21 languages and contains 1.920.209 sentences for the pair German/English. The corpus includes only 4 sentences with the term “toaster”, 12 with “knives” (mostly in the context of violence), 6 with “dishwasher” (in the context of regulations) and 0 with “trash can”. The terms “camera” and “display” are more frequent, with 208 and 1186 mentions, respectively, but they never occur together.4 The corpus is thus not representative for product reviews as we consider in this paper. 2.3 Quality Estimation-based Instance Filtering The performance of an approach relying on projection"
K15-1016,J10-4005,0,0.0170986,"in and to a new language, corresponding training data is needed. In order to circumvent the need for additional training data when addressing a new language, we project training data automatically from a source to a target language. As input to our approach we require a corpus annotated for some source language and a translation from the source to a target language. As the availability of a parallel training corpus cannot be assumed in general, we use statistical machine translation (SMT) methods, relying on phrase-based translation models that use large amounts of parallel data for training (Koehn, 2010). While using an open-source system such as Moses3 would have been an option, we note that the quality would be limited by whether the system can be trained on a representative corpus. A standard dataset that SMT systems are trained on is EuroParl (Koehn, 2005). EuroParl covers 21 languages and contains 1.920.209 sentences for the pair German/English. The corpus includes only 4 sentences with the term “toaster”, 12 with “knives” (mostly in the context of violence), 6 with “dishwasher” (in the context of regulations) and 0 with “trash can”. The terms “camera” and “display” are more frequent, wi"
K15-1016,P14-2095,0,0.0156694,"is the work by Basili et al. (2009) who perform postprocessing of machine translated resources to improve the annotation for training semantic role labeling models). In cases in which no such parallel resources are available containing pertinent annotations, models can be transfered after training. Early work includes a cross-lingual parser adaption (Zeman and Resnik, 2008). A recent example is the projection of a metaphor detection model using a bilingual dictionary (Tsvetkov et al., 2014). A combination of model transfer and annotation projection for dependency parsing has been proposed by Kozhevnikov and Titov (2014). To improve quality of the overall corpus of projected annotations, the selection of data points for dependency parsing has been studied (Søgaard, 2011). Similarly, Axelrod et al. (2011) improve the average quality of machine translation systems by selection of promising training examples and show that such a selection approach has a positive impact. Related to the latter, a generic instance weighting scheme has been proposed for domain adaptation (Jiang and Zhai, 2007). Other work has attempted to exploit information available in multiple languages to induce a model for a language for which"
K15-1016,N13-1073,0,0.592047,"Proceedings of the 19th Conference on Computational Language Learning, pages 153–163, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics Es gibt mit Sicherheit bessere Maschinen , aber die bietet das beste Preis-Leistungs-Verh¨altnis . There are certainly better machines , but o↵ers the best price-performance ratio . Figure 1: Example for the projection of an annotation from the source language to the target language. The translation has been generated with the Google translate API (https://cloud.google.com/translate/). The alignment is induced with FastAlign (Dyer et al., 2013). • We propose to use a supervised approach to induce a fine-grained sentiment analysis model to predict aspect and subjective phrases on some target language, given training data in some source language. This approach relies on automatic translation of source training data and projection of annotations to the target language data. • We present an instance selection method that only selects sentences with a certain translation quality. For this, we incorporate different measures of translation and alignment confidence. We show that such an instance selection method leads to increased performan"
K15-1016,D11-1006,0,0.0280377,"ranslation systems by selection of promising training examples and show that such a selection approach has a positive impact. Related to the latter, a generic instance weighting scheme has been proposed for domain adaptation (Jiang and Zhai, 2007). Other work has attempted to exploit information available in multiple languages to induce a model for a language for which sufficient training data is not available. For instance, universal tag sets take advantage of annotations that are aligned across languages (Snyder et al., 2008). Delexicalization allows for applying a model to other languages (McDonald et al., 2011). Focusing on cross-lingual sentiment analysis, joint training of classification models on multiple languages shows an improvement over separated models. Balahur and Turchi (2014) analyzed the impact of using different machine translation approaches in such settings. Differences in sentiment expressions have been analyzed between English and Dutch (Bal et al., 2011). Co-training with nonannotated corpora has been shown to yield good results for Chinese (Wan, 2009). Ghorbel (2012) analyzed the impact of automatic translation on sentiment analysis. Finally, SentiWordNet has been used for multili"
K15-1016,D10-1101,0,0.0315454,"nerate: While the number of true positive aspects for the coffee machine subdomain is 1100, only 221 are predicted with a threshold of the manual quality assignment of 0. However, a treshold of 9 leads to 560 predictions and a threshold of 10 to 1291. This effect can be observed for subjective phrases as well. It increases from 465 to 827 while the gold number is 676. These observations hold for all filtering methods analogously. 4 Related Work classification (Choi et al., 2010; Johansson and Moschitti, 2011; Yang and Cardie, 2012; Hu and Liu, 2004; Li et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010b). Such models are typically trained or optimized on manually annotated data (Klinger and Cimiano, 2013; Yang and Cardie, 2012; Jakob and Gurevych, 2010a; Zhang et al., 2011). The necessary data, at least containing fine-grained annotations for aspects and subjective phrases instead of only an overall polarity score, are mainly available for the English language to a sufficient extent. Popular corpora used for training are for instance the J.D. Power and Associates Sentiment Corpora (Kessler et al., 2010) or the MPQA corpora (Wilson and Wiebe, 2005). Non-English resources are scarce. Examples"
K15-1016,P10-2049,0,0.0249686,"nerate: While the number of true positive aspects for the coffee machine subdomain is 1100, only 221 are predicted with a threshold of the manual quality assignment of 0. However, a treshold of 9 leads to 560 predictions and a threshold of 10 to 1291. This effect can be observed for subjective phrases as well. It increases from 465 to 827 while the gold number is 676. These observations hold for all filtering methods analogously. 4 Related Work classification (Choi et al., 2010; Johansson and Moschitti, 2011; Yang and Cardie, 2012; Hu and Liu, 2004; Li et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010b). Such models are typically trained or optimized on manually annotated data (Klinger and Cimiano, 2013; Yang and Cardie, 2012; Jakob and Gurevych, 2010a; Zhang et al., 2011). The necessary data, at least containing fine-grained annotations for aspects and subjective phrases instead of only an overall polarity score, are mainly available for the English language to a sufficient extent. Popular corpora used for training are for instance the J.D. Power and Associates Sentiment Corpora (Kessler et al., 2010) or the MPQA corpora (Wilson and Wiebe, 2005). Non-English resources are scarce. Examples"
K15-1016,P07-1034,0,0.0528929,"et al., 2014). A combination of model transfer and annotation projection for dependency parsing has been proposed by Kozhevnikov and Titov (2014). To improve quality of the overall corpus of projected annotations, the selection of data points for dependency parsing has been studied (Søgaard, 2011). Similarly, Axelrod et al. (2011) improve the average quality of machine translation systems by selection of promising training examples and show that such a selection approach has a positive impact. Related to the latter, a generic instance weighting scheme has been proposed for domain adaptation (Jiang and Zhai, 2007). Other work has attempted to exploit information available in multiple languages to induce a model for a language for which sufficient training data is not available. For instance, universal tag sets take advantage of annotations that are aligned across languages (Snyder et al., 2008). Delexicalization allows for applying a model to other languages (McDonald et al., 2011). Focusing on cross-lingual sentiment analysis, joint training of classification models on multiple languages shows an improvement over separated models. Balahur and Turchi (2014) analyzed the impact of using different machin"
K15-1016,P07-1123,0,0.103217,"tilingual Subjectivity Analysis Gold Standard Data Set” focuses on subjectivity in the news domain (Balahur and Steinberger, 2009). A Chinese corpus annotated at the aspect and subjective phrase level is described by Zhao et al. (2014). There has not been too much work on approaches to transfer a model either directly or via annotation projection in the area of sentiment analysis. One example is based on sentence level annotations which are automatically translated to yield a resource in another language. This approach has been proven to work well across several languages (Banea et al., 2010; Mihalcea et al., 2007; Balahur and Turchi, 2014). Recent work approached multilingual opinion mining on the above-mentioned multi-lingual Youtube corpus with tree kernels predicting the polarity of a comment and whether it concerns the product or the video in which the product is featured. (Severyn et al., 2015). Brooke et al. (2009) compare dictionary and classification transfer from English to Spanish in a similar classification setting. In-target-language training approaches for finegrained sentiment analysis include those targeting the extraction of phrases or modelling it as text 159 While cross-lingual annot"
K15-1016,H05-1043,0,0.0965831,"on different thresholds generate: While the number of true positive aspects for the coffee machine subdomain is 1100, only 221 are predicted with a threshold of the manual quality assignment of 0. However, a treshold of 9 leads to 560 predictions and a threshold of 10 to 1291. This effect can be observed for subjective phrases as well. It increases from 465 to 827 while the gold number is 676. These observations hold for all filtering methods analogously. 4 Related Work classification (Choi et al., 2010; Johansson and Moschitti, 2011; Yang and Cardie, 2012; Hu and Liu, 2004; Li et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010b). Such models are typically trained or optimized on manually annotated data (Klinger and Cimiano, 2013; Yang and Cardie, 2012; Jakob and Gurevych, 2010a; Zhang et al., 2011). The necessary data, at least containing fine-grained annotations for aspects and subjective phrases instead of only an overall polarity score, are mainly available for the English language to a sufficient extent. Popular corpora used for training are for instance the J.D. Power and Associates Sentiment Corpora (Kessler et al., 2010) or the MPQA corpora (Wilson and Wiebe, 2005). Non-English reso"
K15-1016,2014.eamt-1.22,0,0.022512,"occur together.4 The corpus is thus not representative for product reviews as we consider in this paper. 2.3 Quality Estimation-based Instance Filtering The performance of an approach relying on projection of training data from a source to a target language and using this automatically projected data to train a supervised model crucially depends on the quality of the translations and alignments. In order to reduce the impact of spurious translations, we filter out low-quality sentence pairs. To estimate this quality, we take three measures into consideration (following approaches described by Shah and Specia (2014), in addition to a manual assessment of the translation quality as an upper baseline): 2 Note that the learning is independent from the actual value for all 0 &lt; α &lt; (maxg∈Corpus |g|)−1 . 3 www.statmt.org/moses/ 4 These example domains are taken from the USAGE corpus (Klinger and Cimiano, 2014), which is used in Section 3. 155 1. The probability of the sentence in the source language given a language model build on unannotated text in the source language (measuring if the language to be translated is typical, referred to as Source LM). 2. The probability of the machine translated sentence given"
K15-1016,2013.mtsummit-papers.21,0,0.0151545,"a ranking problem and thus be modeled as regression or classification. An important research focus is on investigating the impact of different features on predicting translation quality. For instance, sentence length, the output probability, number of unknown words of a target language as well as parsing-based features have been used (Avramidis et al., 2011). The alignment context can also be taken into account (Bach et al., 2011). An overview on confidence measures for machine translation is for instance provided by Ueffing et al. (2003). The impact of different features has been analyzed by Shah et al. (2013). A complete system and framework for quality estimation (including a list of possible features) is QuEst (Specia et al., 2013). For an overview of other cross-lingual applications and methods, we refer to Bikel and Zitouni (2012). 5 Conclusion and Future Work We have presented an approach that alleviates the need of training data for a target language when adapting a fine-grained sentiment analysis system to a new language. Our approach relies on training data available for a source language and on automatic machine translation, in particular statistical methods, to project training data to t"
K15-1016,D08-1109,0,0.0198004,"gaard, 2011). Similarly, Axelrod et al. (2011) improve the average quality of machine translation systems by selection of promising training examples and show that such a selection approach has a positive impact. Related to the latter, a generic instance weighting scheme has been proposed for domain adaptation (Jiang and Zhai, 2007). Other work has attempted to exploit information available in multiple languages to induce a model for a language for which sufficient training data is not available. For instance, universal tag sets take advantage of annotations that are aligned across languages (Snyder et al., 2008). Delexicalization allows for applying a model to other languages (McDonald et al., 2011). Focusing on cross-lingual sentiment analysis, joint training of classification models on multiple languages shows an improvement over separated models. Balahur and Turchi (2014) analyzed the impact of using different machine translation approaches in such settings. Differences in sentiment expressions have been analyzed between English and Dutch (Bal et al., 2011). Co-training with nonannotated corpora has been shown to yield good results for Chinese (Wan, 2009). Ghorbel (2012) analyzed the impact of aut"
K15-1016,D12-1122,0,0.143617,"ude with Section 5 and mention promising future steps. 2 2.1 Methods Supervised Model for Aspect and Subjective Phrase Detection phrases and their relations. The structure follows the proposed pipeline approach by Klinger and Cimiano (2013).1 However, in contrast to their work, we focus on the detection of phrases only, and exploit the detection of relations only during inference, such that the detection of relations has an effect on the detection of phrases, but is not evaluated directly. The phrase detection follows the idea of semiMarkov conditional random fields (Sarawagi and Cohen, 2004; Yang and Cardie, 2012) and models phrases as spans over tokens as variables. Factor templates for spans of type aspect and subjective take into account token strings, prefixes, suffixes, the inclusion of digits, and part-of-speech tags, both as full string and as bigrams, for the spans and their vicinity. In addition, the length of the span is modeled by cumulative binning. The relation template indicates how close an aspect is to a subjective phrase based on token distance and on the length of the shortest path in the dependency tree. The edge names of the shortest path are also included as features. It is further"
K15-1016,N01-1026,0,0.101386,"transfered to Chinese. Models are further improved by cotraining. Xu et al. (2013) perform self-training based on a projected corpus from English to Chinese to detect opinion holders. Due to the lack of existing manually annotated resources, to our knowledge no cross-language projection approach for fine-grained annotation at the level of aspect and subjective phrases has been proposed before. The projection of annotated data sets has been investigated in a variety of applications. Early work includes an approach to the projection of part-ofspeech tags and noun phrases (Yarowsky et al., 2001; Yarowsky and Ngai, 2001) and parsing information (Hwa et al., 2005) on a parallel corpus. Especially in syntactic and semantic parsing, heuristics to remove or correct spuriously projected annotations have been developed (Pad´o and Lapata, 2009; Agi´c et al., 2014). It is typical for these approaches to be applied on existing parallel corpora (one counter example is the work by Basili et al. (2009) who perform postprocessing of machine translated resources to improve the annotation for training semantic role labeling models). In cases in which no such parallel resources are available containing pertinent annotations,"
K15-1016,H01-1035,0,0.0640615,"lish data set which is transfered to Chinese. Models are further improved by cotraining. Xu et al. (2013) perform self-training based on a projected corpus from English to Chinese to detect opinion holders. Due to the lack of existing manually annotated resources, to our knowledge no cross-language projection approach for fine-grained annotation at the level of aspect and subjective phrases has been proposed before. The projection of annotated data sets has been investigated in a variety of applications. Early work includes an approach to the projection of part-ofspeech tags and noun phrases (Yarowsky et al., 2001; Yarowsky and Ngai, 2001) and parsing information (Hwa et al., 2005) on a parallel corpus. Especially in syntactic and semantic parsing, heuristics to remove or correct spuriously projected annotations have been developed (Pad´o and Lapata, 2009; Agi´c et al., 2014). It is typical for these approaches to be applied on existing parallel corpora (one counter example is the work by Basili et al. (2009) who perform postprocessing of machine translated resources to improve the annotation for training semantic role labeling models). In cases in which no such parallel resources are available contain"
K15-1016,I08-3008,0,0.0335368,"emantic parsing, heuristics to remove or correct spuriously projected annotations have been developed (Pad´o and Lapata, 2009; Agi´c et al., 2014). It is typical for these approaches to be applied on existing parallel corpora (one counter example is the work by Basili et al. (2009) who perform postprocessing of machine translated resources to improve the annotation for training semantic role labeling models). In cases in which no such parallel resources are available containing pertinent annotations, models can be transfered after training. Early work includes a cross-lingual parser adaption (Zeman and Resnik, 2008). A recent example is the projection of a metaphor detection model using a bilingual dictionary (Tsvetkov et al., 2014). A combination of model transfer and annotation projection for dependency parsing has been proposed by Kozhevnikov and Titov (2014). To improve quality of the overall corpus of projected annotations, the selection of data points for dependency parsing has been studied (Søgaard, 2011). Similarly, Axelrod et al. (2011) improve the average quality of machine translation systems by selection of promising training examples and show that such a selection approach has a positive imp"
K15-1016,P11-2120,0,0.0250719,"In cases in which no such parallel resources are available containing pertinent annotations, models can be transfered after training. Early work includes a cross-lingual parser adaption (Zeman and Resnik, 2008). A recent example is the projection of a metaphor detection model using a bilingual dictionary (Tsvetkov et al., 2014). A combination of model transfer and annotation projection for dependency parsing has been proposed by Kozhevnikov and Titov (2014). To improve quality of the overall corpus of projected annotations, the selection of data points for dependency parsing has been studied (Søgaard, 2011). Similarly, Axelrod et al. (2011) improve the average quality of machine translation systems by selection of promising training examples and show that such a selection approach has a positive impact. Related to the latter, a generic instance weighting scheme has been proposed for domain adaptation (Jiang and Zhai, 2007). Other work has attempted to exploit information available in multiple languages to induce a model for a language for which sufficient training data is not available. For instance, universal tag sets take advantage of annotations that are aligned across languages (Snyder et al"
K15-1016,P13-4014,0,0.0293197,"Missing"
K15-1016,P14-1024,0,0.0222151,"2009; Agi´c et al., 2014). It is typical for these approaches to be applied on existing parallel corpora (one counter example is the work by Basili et al. (2009) who perform postprocessing of machine translated resources to improve the annotation for training semantic role labeling models). In cases in which no such parallel resources are available containing pertinent annotations, models can be transfered after training. Early work includes a cross-lingual parser adaption (Zeman and Resnik, 2008). A recent example is the projection of a metaphor detection model using a bilingual dictionary (Tsvetkov et al., 2014). A combination of model transfer and annotation projection for dependency parsing has been proposed by Kozhevnikov and Titov (2014). To improve quality of the overall corpus of projected annotations, the selection of data points for dependency parsing has been studied (Søgaard, 2011). Similarly, Axelrod et al. (2011) improve the average quality of machine translation systems by selection of promising training examples and show that such a selection approach has a positive impact. Related to the latter, a generic instance weighting scheme has been proposed for domain adaptation (Jiang and Zhai"
K15-1016,2003.mtsummit-papers.52,0,0.033574,"., 2008). Estimating the quality of machine translation can be understood as a ranking problem and thus be modeled as regression or classification. An important research focus is on investigating the impact of different features on predicting translation quality. For instance, sentence length, the output probability, number of unknown words of a target language as well as parsing-based features have been used (Avramidis et al., 2011). The alignment context can also be taken into account (Bach et al., 2011). An overview on confidence measures for machine translation is for instance provided by Ueffing et al. (2003). The impact of different features has been analyzed by Shah et al. (2013). A complete system and framework for quality estimation (including a list of possible features) is QuEst (Specia et al., 2013). For an overview of other cross-lingual applications and methods, we refer to Bikel and Zitouni (2012). 5 Conclusion and Future Work We have presented an approach that alleviates the need of training data for a target language when adapting a fine-grained sentiment analysis system to a new language. Our approach relies on training data available for a source language and on automatic machine tra"
K15-1016,uryupina-etal-2014-sentube,0,0.0243616,"otated data (Klinger and Cimiano, 2013; Yang and Cardie, 2012; Jakob and Gurevych, 2010a; Zhang et al., 2011). The necessary data, at least containing fine-grained annotations for aspects and subjective phrases instead of only an overall polarity score, are mainly available for the English language to a sufficient extent. Popular corpora used for training are for instance the J.D. Power and Associates Sentiment Corpora (Kessler et al., 2010) or the MPQA corpora (Wilson and Wiebe, 2005). Non-English resources are scarce. Examples are a YouTube corpus consisting of English and Italian comments (Uryupina et al., 2014), a not publicly available German Amazon review corpus of 270 sentences (Boland et al., 2013), in addition to the USAGE corpus (Klinger and Cimiano, 2014) we have used in this work, consisting of German and English reviews. The (non-fine-grained annotated) Spanish TASS corpus consists of Twitter messages (Saralegi and Vicente, 2012). The “Multilingual Subjectivity Analysis Gold Standard Data Set” focuses on subjectivity in the news domain (Balahur and Steinberger, 2009). A Chinese corpus annotated at the aspect and subjective phrase level is described by Zhao et al. (2014). There has not been"
K15-1016,P09-1027,0,0.0726727,"at are aligned across languages (Snyder et al., 2008). Delexicalization allows for applying a model to other languages (McDonald et al., 2011). Focusing on cross-lingual sentiment analysis, joint training of classification models on multiple languages shows an improvement over separated models. Balahur and Turchi (2014) analyzed the impact of using different machine translation approaches in such settings. Differences in sentiment expressions have been analyzed between English and Dutch (Bal et al., 2011). Co-training with nonannotated corpora has been shown to yield good results for Chinese (Wan, 2009). Ghorbel (2012) analyzed the impact of automatic translation on sentiment analysis. Finally, SentiWordNet has been used for multilingual sentiment analysis (Denecke, 2008). Building dictionaries for languages with scarce resources can be supported by bootstrapping approaches (Banea et al., 2008). Estimating the quality of machine translation can be understood as a ranking problem and thus be modeled as regression or classification. An important research focus is on investigating the impact of different features on predicting translation quality. For instance, sentence length, the output proba"
K15-1016,W05-0308,0,0.00961569,"i et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010b). Such models are typically trained or optimized on manually annotated data (Klinger and Cimiano, 2013; Yang and Cardie, 2012; Jakob and Gurevych, 2010a; Zhang et al., 2011). The necessary data, at least containing fine-grained annotations for aspects and subjective phrases instead of only an overall polarity score, are mainly available for the English language to a sufficient extent. Popular corpora used for training are for instance the J.D. Power and Associates Sentiment Corpora (Kessler et al., 2010) or the MPQA corpora (Wilson and Wiebe, 2005). Non-English resources are scarce. Examples are a YouTube corpus consisting of English and Italian comments (Uryupina et al., 2014), a not publicly available German Amazon review corpus of 270 sentences (Boland et al., 2013), in addition to the USAGE corpus (Klinger and Cimiano, 2014) we have used in this work, consisting of German and English reviews. The (non-fine-grained annotated) Spanish TASS corpus consists of Twitter messages (Saralegi and Vicente, 2012). The “Multilingual Subjectivity Analysis Gold Standard Data Set” focuses on subjectivity in the news domain (Balahur and Steinberger,"
K15-1016,H05-2017,0,\N,Missing
klinger-cimiano-2014-usage,N12-1085,0,\N,Missing
klinger-cimiano-2014-usage,N06-4006,0,\N,Missing
klinger-cimiano-2014-usage,ruppenhofer-etal-2008-finding,0,\N,Missing
klinger-cimiano-2014-usage,P11-2100,0,\N,Missing
klinger-cimiano-2014-usage,P04-1035,0,\N,Missing
klinger-cimiano-2014-usage,J09-3003,0,\N,Missing
klinger-cimiano-2014-usage,clematide-etal-2012-mlsa,0,\N,Missing
klinger-cimiano-2014-usage,P08-1036,0,\N,Missing
klinger-cimiano-2014-usage,P10-1059,0,\N,Missing
klinger-cimiano-2014-usage,P02-1053,0,\N,Missing
klinger-cimiano-2014-usage,P13-2147,1,\N,Missing
klinger-cimiano-2014-usage,S13-2052,0,\N,Missing
L16-1178,clematide-etal-2012-mlsa,0,0.0279877,"nd Liu (2004) constructed a corpus of Amazon reviews annotated with aspects, subjective phrases and a polarity score for each sentence. Spina et al. (2012) provide a data set containing 9,396 Tweets annotated with offsets for aspect mentions (of predefined categories) and evaluative phrases. Annotated blog posts about cars and cameras are the content of the JDPA sentiment corpus from Kessler et al. (2010). A few corpora exist for other languages than English. Examples are a corpus of sentences from German web texts with subjectivity and polarity annotations on sentence, phrase and word level (Clematide et al., 2012). Klinger and Cimiano (2014) published the USAGE corpus containing the annotation of product aspects and evaluative phrases in German and English Amazon reviews. 3. A Corpus for Fine-grained Sentiment Analysis of Mobile Application Reviews In the following, we present the Sentiment Corpus of App Reviews (SCARE) consisting of mobile application reviews annotated with aspects, subjective (evaluating) phrases, polarities and their relation. 3.1. Corpus Selection We select eleven application categories, which represent typical use-cases of mobile applications. The categories are instant messengers"
L16-1178,filatova-2012-irony,0,0.0709933,"Missing"
L16-1178,P13-2147,1,0.878147,"ifficult, because there is (to the best of our knowledge) no corpus of annotated application reviews available to the research community. With this paper, we contribute to this situation: We publish the first corpus with fine-grained sentiment information (i.e., annotations of subjective phrases, aspects, and their relations) of German mobile app reviews from the Google Play Store. The corpus is available for future research5 . 2. Previous Work A plethora of approaches for opinion mining has been proposed in the last decades. Many of them are based on statistically trained, supervised models (Klinger and Cimiano, 2013b; Li et al., 2010, for instance), incorporate weakly supervised machine learning techniques (Titov and McDonald, 2008; Täckström and McDonald, 2011) or employ rules (Reckman et al., 2013) or dictionaries (Waltinger, 2010b) to 5 The corpus and further information, including the complete app list and annotation guidelines, are available at http://www.romanklinger.de/scare/ 1114 Authors Store #Apps #Reviews Vasa et al. (2012) Harman et al. (2012) Iacob and Harrison (2013) Khalid (2013) Galvis Carreno et al. (2013) Fu et al. (2013) Pagano and Maalej (2013) Chen et al. (2014) Khalid et al. (2014)"
L16-1178,klinger-cimiano-2014-usage,1,0.899963,"8 — 137,000 6,390 327 13,286,706 1,100,000 241,656 6,390 32,210 2,106,605 2,729,103 1,303,182 Figure 1: Example review from Google Play Store for the app M APS .M E (personally identifiable information blurred for depiction). Table 1: Overview of existing work on app store review mining and analysis. For each approach the number applications and reviews used as well as the app store (Apple App Store (A), Google Play Store (G) or BlackBerry World (B)) they originate from are given. All approaches use English language reviews. detect sentiment in text. One focus is the study of product reviews (Klinger and Cimiano, 2014), Twitter messages (Liu et al., 2012; Tang et al., 2014) and blog posts (Klinger and Cimiano, 2013a; Kessler et al., 2010). Only a limited number of approaches focused on mobile applications and their user reviews. Early work found a strong correlation between the customer rating and the rank of app downloads by analyzing over 32,000 apps in the BlackBerry App Store (Harman et al., 2012). These results also show that there is no correlation between price and download as well as price and rating. Iacob and Harrison (2013) automatically detect feature requests. They use a corpus of 3,279 reviews"
L16-1178,S13-2052,0,0.0429615,"ify bug reports and feature requests (Maalej and Nabil, 2015), and coarse-grained sentiment analysis (Gu and Kim, 2015). Further research includes topic (Galvis Carreno and Winbladh, 2013) and feature detection (Guzman and Maalej, 2014), keyword extraction (Vu et al., 2015), and review impact analysis (Pagano and Maalej, 2013). Table 1 provides an overview of app store mining approaches and the review corpora used within them. For fine-grained sentiment analysis and opinion mining in other domains than app reviews, a plethora of manually annotated corpora is available (Lakkaraju et al., 2011; Nakov et al., 2013; Wiebe et al., 2005, for instance). Hu and Liu (2004) constructed a corpus of Amazon reviews annotated with aspects, subjective phrases and a polarity score for each sentence. Spina et al. (2012) provide a data set containing 9,396 Tweets annotated with offsets for aspect mentions (of predefined categories) and evaluative phrases. Annotated blog posts about cars and cameras are the content of the JDPA sentiment corpus from Kessler et al. (2010). A few corpora exist for other languages than English. Examples are a corpus of sentences from German web texts with subjectivity and polarity annotat"
L16-1178,S13-2085,0,0.0227508,"ublish the first corpus with fine-grained sentiment information (i.e., annotations of subjective phrases, aspects, and their relations) of German mobile app reviews from the Google Play Store. The corpus is available for future research5 . 2. Previous Work A plethora of approaches for opinion mining has been proposed in the last decades. Many of them are based on statistically trained, supervised models (Klinger and Cimiano, 2013b; Li et al., 2010, for instance), incorporate weakly supervised machine learning techniques (Titov and McDonald, 2008; Täckström and McDonald, 2011) or employ rules (Reckman et al., 2013) or dictionaries (Waltinger, 2010b) to 5 The corpus and further information, including the complete app list and annotation guidelines, are available at http://www.romanklinger.de/scare/ 1114 Authors Store #Apps #Reviews Vasa et al. (2012) Harman et al. (2012) Iacob and Harrison (2013) Khalid (2013) Galvis Carreno et al. (2013) Fu et al. (2013) Pagano and Maalej (2013) Chen et al. (2014) Khalid et al. (2014) Guzman and Maalej (2014) Vu et al. (2015) Martin et al. (2015) Maalej and Nabil (2015) A B B A G G A G A A,G G B A,G 17,330 32,108 270 20 3 171,493 1,100 4 20 7 95 15,095 1,140 8,701,198 —"
L16-1178,remus-etal-2010-sentiws,0,0.0745394,"Missing"
L16-1178,E12-2021,0,0.0504158,"Missing"
L16-1178,P11-2100,0,0.029505,"is paper, we contribute to this situation: We publish the first corpus with fine-grained sentiment information (i.e., annotations of subjective phrases, aspects, and their relations) of German mobile app reviews from the Google Play Store. The corpus is available for future research5 . 2. Previous Work A plethora of approaches for opinion mining has been proposed in the last decades. Many of them are based on statistically trained, supervised models (Klinger and Cimiano, 2013b; Li et al., 2010, for instance), incorporate weakly supervised machine learning techniques (Titov and McDonald, 2008; Täckström and McDonald, 2011) or employ rules (Reckman et al., 2013) or dictionaries (Waltinger, 2010b) to 5 The corpus and further information, including the complete app list and annotation guidelines, are available at http://www.romanklinger.de/scare/ 1114 Authors Store #Apps #Reviews Vasa et al. (2012) Harman et al. (2012) Iacob and Harrison (2013) Khalid (2013) Galvis Carreno et al. (2013) Fu et al. (2013) Pagano and Maalej (2013) Chen et al. (2014) Khalid et al. (2014) Guzman and Maalej (2014) Vu et al. (2015) Martin et al. (2015) Maalej and Nabil (2015) A B B A G G A G A A,G G B A,G 17,330 32,108 270 20 3 171,493 1"
L16-1178,P14-1146,0,0.0363984,"2,106,605 2,729,103 1,303,182 Figure 1: Example review from Google Play Store for the app M APS .M E (personally identifiable information blurred for depiction). Table 1: Overview of existing work on app store review mining and analysis. For each approach the number applications and reviews used as well as the app store (Apple App Store (A), Google Play Store (G) or BlackBerry World (B)) they originate from are given. All approaches use English language reviews. detect sentiment in text. One focus is the study of product reviews (Klinger and Cimiano, 2014), Twitter messages (Liu et al., 2012; Tang et al., 2014) and blog posts (Klinger and Cimiano, 2013a; Kessler et al., 2010). Only a limited number of approaches focused on mobile applications and their user reviews. Early work found a strong correlation between the customer rating and the rank of app downloads by analyzing over 32,000 apps in the BlackBerry App Store (Harman et al., 2012). These results also show that there is no correlation between price and download as well as price and rating. Iacob and Harrison (2013) automatically detect feature requests. They use a corpus of 3,279 reviews from different applications of the BlackBerry App Store"
L16-1178,P08-1036,0,0.0594924,"esearch community. With this paper, we contribute to this situation: We publish the first corpus with fine-grained sentiment information (i.e., annotations of subjective phrases, aspects, and their relations) of German mobile app reviews from the Google Play Store. The corpus is available for future research5 . 2. Previous Work A plethora of approaches for opinion mining has been proposed in the last decades. Many of them are based on statistically trained, supervised models (Klinger and Cimiano, 2013b; Li et al., 2010, for instance), incorporate weakly supervised machine learning techniques (Titov and McDonald, 2008; Täckström and McDonald, 2011) or employ rules (Reckman et al., 2013) or dictionaries (Waltinger, 2010b) to 5 The corpus and further information, including the complete app list and annotation guidelines, are available at http://www.romanklinger.de/scare/ 1114 Authors Store #Apps #Reviews Vasa et al. (2012) Harman et al. (2012) Iacob and Harrison (2013) Khalid (2013) Galvis Carreno et al. (2013) Fu et al. (2013) Pagano and Maalej (2013) Chen et al. (2014) Khalid et al. (2014) Guzman and Maalej (2014) Vu et al. (2015) Martin et al. (2015) Maalej and Nabil (2015) A B B A G G A G A A,G G B A,G 1"
L16-1178,waltinger-2010-germanpolarityclues,0,0.175897,"ed sentiment information (i.e., annotations of subjective phrases, aspects, and their relations) of German mobile app reviews from the Google Play Store. The corpus is available for future research5 . 2. Previous Work A plethora of approaches for opinion mining has been proposed in the last decades. Many of them are based on statistically trained, supervised models (Klinger and Cimiano, 2013b; Li et al., 2010, for instance), incorporate weakly supervised machine learning techniques (Titov and McDonald, 2008; Täckström and McDonald, 2011) or employ rules (Reckman et al., 2013) or dictionaries (Waltinger, 2010b) to 5 The corpus and further information, including the complete app list and annotation guidelines, are available at http://www.romanklinger.de/scare/ 1114 Authors Store #Apps #Reviews Vasa et al. (2012) Harman et al. (2012) Iacob and Harrison (2013) Khalid (2013) Galvis Carreno et al. (2013) Fu et al. (2013) Pagano and Maalej (2013) Chen et al. (2014) Khalid et al. (2014) Guzman and Maalej (2014) Vu et al. (2015) Martin et al. (2015) Maalej and Nabil (2015) A B B A G G A G A A,G G B A,G 17,330 32,108 270 20 3 171,493 1,100 4 20 7 95 15,095 1,140 8,701,198 — 137,000 6,390 327 13,286,706 1,1"
N19-1067,I13-1171,0,0.16235,"t fictional texts, in which, given a text, a network is to be generated, whose nodes correspond to characters and edges to emotions between characters. One of the characters is part of a trigger/cause for the emotion experienced by the other. Figure 1 depicts two examples for emotional character interactions at the text level. Such relation extraction is the basis for generating social networks of emotional interactions. Dynamic social networks of characters are analyzed in previous work with different goals, e.g., to test the differences in interactions between various adaptations of a book (Agarwal et al., 2013); to understand the correlation between dialogue and setting (Elson et al., 2010); to test whether social networks derived from Shakespeare’s plays can be explained by a general sociological model (Nalisnick and Baird, 2013); in the task of narrative generation (Sack, 2013); to better understand the nature of character interactions (Piper et al., 2017). Further, previous work analyses personality traits of characters (mostly) independently of each other (Massey et al., 2015; Barth et al., 2018; Bamman et al., 2014). Emotion analysis in literature has focused on the development of emotions over"
N19-1067,P14-1035,0,0.0859048,"to test the differences in interactions between various adaptations of a book (Agarwal et al., 2013); to understand the correlation between dialogue and setting (Elson et al., 2010); to test whether social networks derived from Shakespeare’s plays can be explained by a general sociological model (Nalisnick and Baird, 2013); in the task of narrative generation (Sack, 2013); to better understand the nature of character interactions (Piper et al., 2017). Further, previous work analyses personality traits of characters (mostly) independently of each other (Massey et al., 2015; Barth et al., 2018; Bamman et al., 2014). Emotion analysis in literature has focused on the development of emotions over time, abstracting away who experiences an emotion (Reagan et al., 2016; Elsner, 2015; Kim et al., 2017; Piper and Jean So, 2015, i.a.). Fewer works have adThe development of a fictional plot is centered around characters who closely interact with each other forming dynamic social networks. In literature analysis, such networks have mostly been analyzed without particular relation types or focusing on roles which the characters take with respect to each other. We argue that an important aspect for the analysis of s"
N19-1067,C18-1114,1,0.795918,"m the emotion phrase to the causing character (if available, i.e., Ccause can be empty). One character may be described as experiencing multiple emotions. We generate a “consensus” annotation by keeping all emotion labels by all annotators. This is motivated by the finding by Schuff et al. (2017) that such high-recall aggregation is better modelled in an emotion prediction task. The data is available at http://www.ims.uni-stuttgart.de/data/ relationalemotions. dressed the annotation of emotion causes, e.g., Neviarouskaya and Aono (2013), Ghazi et al. (2015), Saur´ı and Pustejovsky (2009), and Kim and Klinger (2018). To the best of our knowledge, there is no previous research that deals with emotional relationships of literary characters. The works that are conceptually the closest to our paper are Chaturvedi et al. (2017) and Massey et al. (2015), who use a more general set of relationship categories. Most approaches to emotion classification from text build on the classes proposed by Plutchik (2001) and Ekman (1992). Here, we use a discrete emotion categorization scheme based on fundamental emotions as proposed by Plutchik. This model has previously been used in computational analysis of literature (Mo"
N19-1067,W17-2203,1,0.842857,"Missing"
N19-1067,2015.lilt-12.5,0,0.022789,"., 2010); to test whether social networks derived from Shakespeare’s plays can be explained by a general sociological model (Nalisnick and Baird, 2013); in the task of narrative generation (Sack, 2013); to better understand the nature of character interactions (Piper et al., 2017). Further, previous work analyses personality traits of characters (mostly) independently of each other (Massey et al., 2015; Barth et al., 2018; Bamman et al., 2014). Emotion analysis in literature has focused on the development of emotions over time, abstracting away who experiences an emotion (Reagan et al., 2016; Elsner, 2015; Kim et al., 2017; Piper and Jean So, 2015, i.a.). Fewer works have adThe development of a fictional plot is centered around characters who closely interact with each other forming dynamic social networks. In literature analysis, such networks have mostly been analyzed without particular relation types or focusing on roles which the characters take with respect to each other. We argue that an important aspect for the analysis of stories and their development is the emotion between characters. In this paper, we combine these aspects into a unified framework to classify emotional relationships"
N19-1067,P10-1015,0,0.0593937,"s correspond to characters and edges to emotions between characters. One of the characters is part of a trigger/cause for the emotion experienced by the other. Figure 1 depicts two examples for emotional character interactions at the text level. Such relation extraction is the basis for generating social networks of emotional interactions. Dynamic social networks of characters are analyzed in previous work with different goals, e.g., to test the differences in interactions between various adaptations of a book (Agarwal et al., 2013); to understand the correlation between dialogue and setting (Elson et al., 2010); to test whether social networks derived from Shakespeare’s plays can be explained by a general sociological model (Nalisnick and Baird, 2013); in the task of narrative generation (Sack, 2013); to better understand the nature of character interactions (Piper et al., 2017). Further, previous work analyses personality traits of characters (mostly) independently of each other (Massey et al., 2015; Barth et al., 2018; Bamman et al., 2014). Emotion analysis in literature has focused on the development of emotions over time, abstracting away who experiences an emotion (Reagan et al., 2016; Elsner,"
N19-1067,I13-1121,0,0.11697,"hey marked two relations: from the emotion phrase to the experiencing character and from the emotion phrase to the causing character (if available, i.e., Ccause can be empty). One character may be described as experiencing multiple emotions. We generate a “consensus” annotation by keeping all emotion labels by all annotators. This is motivated by the finding by Schuff et al. (2017) that such high-recall aggregation is better modelled in an emotion prediction task. The data is available at http://www.ims.uni-stuttgart.de/data/ relationalemotions. dressed the annotation of emotion causes, e.g., Neviarouskaya and Aono (2013), Ghazi et al. (2015), Saur´ı and Pustejovsky (2009), and Kim and Klinger (2018). To the best of our knowledge, there is no previous research that deals with emotional relationships of literary characters. The works that are conceptually the closest to our paper are Chaturvedi et al. (2017) and Massey et al. (2015), who use a more general set of relationship categories. Most approaches to emotion classification from text build on the classes proposed by Plutchik (2001) and Ekman (1992). Here, we use a discrete emotion categorization scheme based on fundamental emotions as proposed by Plutchik."
N19-1067,D14-1162,0,0.0821778,"Chung et al., 2014) with max and averaged pooling. In the latter, we use different variations of encoding the character positions with indicators (inspired by Zhou et al. (2016), who propose the use of positional indicators for relation detection). Our variations are exemplified in Table 3. Note that the case of predicting directed relations is simplified in the “Role” and “MRole” cases in contrast to “Entity” and “MEntity”, as the model has access to gold information about the relation direction. We obtain word vectors for the embedding layer from GloVe (pre-trained on Common Crawl, d = 300, Pennington et al., 2014) and initialize outof-vocabulary terms with zeros (including the position indicators). 4 Results. Table 4 shows the results (precision and recall shown in supplementary material) on development data and independent test data for the best models. The GRU+MRole model achieves the highest performance with improvement over BOW-RF on the instance and story levels, and shows a clear improvement over the GRU+NoInd. model in the directed 8-class setting. GRU+Role achieves the highest performance on the graph level in the directed 8-class setting. In the undirected prediction setting, all models perfor"
N19-1067,W17-5203,1,0.890137,"Missing"
N19-1067,P13-4001,0,0.0246528,"Missing"
N19-1067,P16-2034,0,0.0320472,"positive the same way, but first calculate the result P/R/F1 for the whole story before averaging (similar to macro-averaging). On the graph-level, we accept a prediction for a character pair to be correct without considering the exact position. left and to the right of the character mentions. We compare an extremely randomized tree classifier with bag-of-words features (Geurts et al., 2006) (BOW-RF) with a two-layer GRU neural network (Chung et al., 2014) with max and averaged pooling. In the latter, we use different variations of encoding the character positions with indicators (inspired by Zhou et al. (2016), who propose the use of positional indicators for relation detection). Our variations are exemplified in Table 3. Note that the case of predicting directed relations is simplified in the “Role” and “MRole” cases in contrast to “Entity” and “MEntity”, as the model has access to gold information about the relation direction. We obtain word vectors for the embedding layer from GloVe (pre-trained on Common Crawl, d = 300, Pennington et al., 2014) and initialize outof-vocabulary terms with zeros (including the position indicators). 4 Results. Table 4 shows the results (precision and recall shown i"
N19-1069,P09-2041,0,0.345327,"including publication sources, enabling the experiments at hand, and c) the largest resource for satire detection so far.2 1 2 * Work was done at University of Stuttgart. https://www.theonion.com/, https://www.nytimes.com/ Data/code: www.ims.uni-stuttgart.de/data/germansatire. 660 Proceedings of NAACL-HLT 2019, pages 660–665 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics 2 Previous Work 3 Previous work tackled the task of automatic English satire detection with handcrafted features, for instance, the validity of the context of entity mentions (Burfoot and Baldwin, 2009), or the coherence of a story (Goldwasser and Zhang, 2016). Rubin et al. (2016) use distributions of parts-ofspeech, sentiment, and exaggerations. In contrast to these approaches, our model uses only word embeddings as input representations. Our work is therefore similar to Yang et al. (2017) and De Sarkar et al. (2018) who also use artificial neural networks to predict if a given text is satirical or regular news. They develop a hierarchical model of convolutional and recurrent layers with attention over paragraphs or sentences. We follow this line of work but our model is not hierarchical an"
N19-1069,C18-1285,0,0.29384,"Missing"
N19-1069,D18-1002,0,0.040086,"m with gradient reversal: The sign of the gradient of the domain discriminator is flipped when backpropagating to the feature extractor. Building upon the idea of eliminating domain-specific input representations, Wadsworth et al. (2018) debias input representations for recidivism prediction, or income prediction (Edwards and Storkey, 2016; Beutel et al., 2017; Madras et al., 2018; Zhang et al., 2018). Debiasing mainly focuses on word embeddings, e.g., to remove gender bias from embeddings (Bolukbasi et al., 2016). Despite previous positive results with adversarial training, a recent study by Elazar and Goldberg (2018) calls for being cautious and not blindly trusting adversarial training for debiasing. We therefore analyze whether it is possible at all to use adversarial training in another setting, namely to control for the confounding variable of publication sources in satire detection (see Section 3.1). Methods for Satire Classification 3.1 Limitations of Previous Methods The data set used by Yang et al. (2017) and De Sarkar et al. (2018) consists of text from 14 satirical and 6 regular news websites. Although the satire sources in train, validation, and test sets did not overlap, the sources of regular"
N19-1069,Q16-1038,0,0.141793,"at hand, and c) the largest resource for satire detection so far.2 1 2 * Work was done at University of Stuttgart. https://www.theonion.com/, https://www.nytimes.com/ Data/code: www.ims.uni-stuttgart.de/data/germansatire. 660 Proceedings of NAACL-HLT 2019, pages 660–665 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics 2 Previous Work 3 Previous work tackled the task of automatic English satire detection with handcrafted features, for instance, the validity of the context of entity mentions (Burfoot and Baldwin, 2009), or the coherence of a story (Goldwasser and Zhang, 2016). Rubin et al. (2016) use distributions of parts-ofspeech, sentiment, and exaggerations. In contrast to these approaches, our model uses only word embeddings as input representations. Our work is therefore similar to Yang et al. (2017) and De Sarkar et al. (2018) who also use artificial neural networks to predict if a given text is satirical or regular news. They develop a hierarchical model of convolutional and recurrent layers with attention over paragraphs or sentences. We follow this line of work but our model is not hierarchical and introduces less parameters. We apply attention to words"
N19-1069,W16-0802,0,0.232157,"esource for satire detection so far.2 1 2 * Work was done at University of Stuttgart. https://www.theonion.com/, https://www.nytimes.com/ Data/code: www.ims.uni-stuttgart.de/data/germansatire. 660 Proceedings of NAACL-HLT 2019, pages 660–665 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics 2 Previous Work 3 Previous work tackled the task of automatic English satire detection with handcrafted features, for instance, the validity of the context of entity mentions (Burfoot and Baldwin, 2009), or the coherence of a story (Goldwasser and Zhang, 2016). Rubin et al. (2016) use distributions of parts-ofspeech, sentiment, and exaggerations. In contrast to these approaches, our model uses only word embeddings as input representations. Our work is therefore similar to Yang et al. (2017) and De Sarkar et al. (2018) who also use artificial neural networks to predict if a given text is satirical or regular news. They develop a hierarchical model of convolutional and recurrent layers with attention over paragraphs or sentences. We follow this line of work but our model is not hierarchical and introduces less parameters. We apply attention to words instead of sentences"
N19-1069,C18-1283,0,0.0261556,"he adversarial component is crucial for the model to learn to pay attention to linguistic properties of satire. 1 Introduction Satire is a form of art used to criticize in an entertaining manner (cf. Sulzer, 1771, p. 995ff.). It makes use of different stylistic devices, e.g., humor, irony, sarcasm, exaggerations, parody or caricature (Knoche, 1982; Colletta, 2009). The occurrence of harsh, offensive or banal and funny words is typical (Golbert, 1962; Brummack, 1971). Satirical news are written with the aim of mimicking regular news in diction. In contrast to misinformation and disinformation (Thorne and Vlachos, 2018), it does not have the intention of fooling the readers into actually believing something wrong in order to manipulate their opinion. Previous work mostly builds on top of corpora of news articles which have been labeled automatically based on the publication source (e.g., “The New York Times” articles would be labeled as regular while “The Onion” articles as satire1 ). We hypothesize that such distant labeling approach leads to the model mostly representing characteristics of the publishers instead of actual satire. This has two main issues: First, interpretation of the model to obtain a bett"
P13-2147,D10-1101,0,0.618677,"cameras and cars), showing that our model outperforms state-ofthe-art models, as well as on a new dataset consisting of Twitter posts. 1 While the three key variables (subjective phrase, polarity and target) intuitively influence each other bidirectionally, most work in the area of opinion mining has concentrated on either predicting one of these variables in isolation (e. g. subjective expressions by Yang and Cardie (2012)) or modeling the dependencies uni-directionally in a pipeline architecture, e. g. predicting targets on the basis of perfect and complete knowledge about subjective terms (Jakob and Gurevych, 2010). However, such pipeline models do not allow for inclusion of bidirectional interactions between the key variables. In this paper, we propose a model that can include bidirectional dependencies, attempting to answer the following questions which so far have not been addressed but provide the basis for a joint model: Introduction Sentiment analysis or opinion mining is the task of identifying subjective statements about products, their polarity (e. g. positive, negative or neutral) in addition to the particular aspect or feature of the entity that is under discussion, i. e., the socalled target"
P13-2147,P11-2018,0,0.0536127,"al dependencies, attempting to answer the following questions which so far have not been addressed but provide the basis for a joint model: Introduction Sentiment analysis or opinion mining is the task of identifying subjective statements about products, their polarity (e. g. positive, negative or neutral) in addition to the particular aspect or feature of the entity that is under discussion, i. e., the socalled target. Opinion analysis is thus typically approached as a classification (T¨ackstr¨om and McDonald, 2011; Sayeed et al., 2012; Pang and Lee, 2004) or segmentation (Choi et al., 2010; Johansson and Moschitti, 2011; Yang and Cardie, 2012) task by which fragments of the input are classified or labelled as representing a subjective phrase (Yang and Cardie, 2012), a polarity or a target (Hu and Liu, 2004; Li et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010). As an example, the sentence “I like the low weight of the camera.” • What is the impact of the performance loss of a non-perfect subjective term extraction in comparison to perfect knowledge? • Further, how does perfect knowledge about targets influence the prediction of subjective terms? • How is the latter affected if the knowledge a"
P13-2147,N13-1039,0,0.0195395,"Missing"
P13-2147,P04-1035,0,0.00714512,"s paper, we propose a model that can include bidirectional dependencies, attempting to answer the following questions which so far have not been addressed but provide the basis for a joint model: Introduction Sentiment analysis or opinion mining is the task of identifying subjective statements about products, their polarity (e. g. positive, negative or neutral) in addition to the particular aspect or feature of the entity that is under discussion, i. e., the socalled target. Opinion analysis is thus typically approached as a classification (T¨ackstr¨om and McDonald, 2011; Sayeed et al., 2012; Pang and Lee, 2004) or segmentation (Choi et al., 2010; Johansson and Moschitti, 2011; Yang and Cardie, 2012) task by which fragments of the input are classified or labelled as representing a subjective phrase (Yang and Cardie, 2012), a polarity or a target (Hu and Liu, 2004; Li et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010). As an example, the sentence “I like the low weight of the camera.” • What is the impact of the performance loss of a non-perfect subjective term extraction in comparison to perfect knowledge? • Further, how does perfect knowledge about targets influence the prediction of"
P13-2147,H05-1043,0,0.267282,"about products, their polarity (e. g. positive, negative or neutral) in addition to the particular aspect or feature of the entity that is under discussion, i. e., the socalled target. Opinion analysis is thus typically approached as a classification (T¨ackstr¨om and McDonald, 2011; Sayeed et al., 2012; Pang and Lee, 2004) or segmentation (Choi et al., 2010; Johansson and Moschitti, 2011; Yang and Cardie, 2012) task by which fragments of the input are classified or labelled as representing a subjective phrase (Yang and Cardie, 2012), a polarity or a target (Hu and Liu, 2004; Li et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010). As an example, the sentence “I like the low weight of the camera.” • What is the impact of the performance loss of a non-perfect subjective term extraction in comparison to perfect knowledge? • Further, how does perfect knowledge about targets influence the prediction of subjective terms? • How is the latter affected if the knowledge about targets is imperfect, i. e. predicted by a learned model? We study these questions using imperatively defined factor graphs (IDFs, McCallum et al. (2008), McCallum et al. (2009)) to show how these bidirectional dependencies can b"
P13-2147,N12-1085,0,0.072235,"key variables. In this paper, we propose a model that can include bidirectional dependencies, attempting to answer the following questions which so far have not been addressed but provide the basis for a joint model: Introduction Sentiment analysis or opinion mining is the task of identifying subjective statements about products, their polarity (e. g. positive, negative or neutral) in addition to the particular aspect or feature of the entity that is under discussion, i. e., the socalled target. Opinion analysis is thus typically approached as a classification (T¨ackstr¨om and McDonald, 2011; Sayeed et al., 2012; Pang and Lee, 2004) or segmentation (Choi et al., 2010; Johansson and Moschitti, 2011; Yang and Cardie, 2012) task by which fragments of the input are classified or labelled as representing a subjective phrase (Yang and Cardie, 2012), a polarity or a target (Hu and Liu, 2004; Li et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010). As an example, the sentence “I like the low weight of the camera.” • What is the impact of the performance loss of a non-perfect subjective term extraction in comparison to perfect knowledge? • Further, how does perfect knowledge about targets influe"
P13-2147,P11-2100,0,0.296896,"Missing"
P13-2147,D12-1122,0,0.0832053,"on of target and subjective phrases in both directions, thus providing an upper bound for the impact of a joint model in comparison to a pipeline model. We report results on two public datasets (cameras and cars), showing that our model outperforms state-ofthe-art models, as well as on a new dataset consisting of Twitter posts. 1 While the three key variables (subjective phrase, polarity and target) intuitively influence each other bidirectionally, most work in the area of opinion mining has concentrated on either predicting one of these variables in isolation (e. g. subjective expressions by Yang and Cardie (2012)) or modeling the dependencies uni-directionally in a pipeline architecture, e. g. predicting targets on the basis of perfect and complete knowledge about subjective terms (Jakob and Gurevych, 2010). However, such pipeline models do not allow for inclusion of bidirectional interactions between the key variables. In this paper, we propose a model that can include bidirectional dependencies, attempting to answer the following questions which so far have not been addressed but provide the basis for a joint model: Introduction Sentiment analysis or opinion mining is the task of identifying subject"
P13-2147,H05-2017,0,\N,Missing
P16-1164,W12-2513,0,0.214933,"y is competitive with the state of the art; (b), a semi-Markov model significantly outperforms all others, by relaxing the Markov assumption. 1 Introduction Quotations are occurrences of reported speech, thought, and writing in text. They play an important role in computational linguistics and digital humanities, providing evidence for, e.g., speaker relationships (Elson et al., 2010), inter-speaker sentiment (Nalisnick and Baird, 2013) or politeness (Faruqui and Pado, 2012). Due to a lack of generalpurpose automatic systems, such information is often obtained through manual annotation (e.g., Agarwal et al. (2012)), which is labor-intensive and costly. Thus, models for automatic quotation detection form a growing research area (e.g., Pouliquen et al. (2007); Pareti et al. (2013)). Quotation detection looks deceptively simple, but is challenging, as the following example shows: [The pipeline], the company said, [would be built by a proposed joint venture . . . , and Trunkline . . . will “build and operate” the system . . . ].1 1 Penn Attributions Relation Corpus (PARC), wsj 0260 Note that quotations can (i) be signalled by lexical cues (e.g., communication verbs) without quotation marks, (ii) contain mi"
P16-1164,W02-1001,0,0.728494,"ing In this paper, we compare our new model architectures to the state-of-the-art approach by Pareti (2015), an extension of Pareti et al. (2013). Their system is a pipeline: Its first component is the cue model, a token-level k-NN classifier applied to the syntactic heads of all verb groups. After cues are detected, content spans are localized using the content model, a linear-chain conditional random field (CRF) which makes use of the location of cues in the document through features. As their system is not publicly available, we reimplement it. Our cue classifier is an averaged perceptron (Collins, 2002) which we describe in more detail in the following section. It uses the 3 http://www.ims.uni-stuttgart.de/data/qsample PARC, wsj 2418 Table 1: Cue detection features for a token ti at position i, mostly derived from Pareti (2015) S1. Is a direct or indirect dependency parent of ti classified as a cue, in the cue list, or the phrase “according to”? S2. Was any token in a window of ±5 classified as a cue? S3. Distance to the previous and next cue S4. Does the sentence containing ti have a cue? S5. Conjunction of S4 and all features from C14 Pareti et al. (2013) distinguish three types of quotati"
P16-1164,P10-1015,0,0.151266,"joint decisions about the begin, end, and internal context of a quotation. We perform an extensive analysis with two new model architectures. We find that (a), simple boundary classification combined with a greedy prediction strategy is competitive with the state of the art; (b), a semi-Markov model significantly outperforms all others, by relaxing the Markov assumption. 1 Introduction Quotations are occurrences of reported speech, thought, and writing in text. They play an important role in computational linguistics and digital humanities, providing evidence for, e.g., speaker relationships (Elson et al., 2010), inter-speaker sentiment (Nalisnick and Baird, 2013) or politeness (Faruqui and Pado, 2012). Due to a lack of generalpurpose automatic systems, such information is often obtained through manual annotation (e.g., Agarwal et al. (2012)), which is labor-intensive and costly. Thus, models for automatic quotation detection form a growing research area (e.g., Pouliquen et al. (2007); Pareti et al. (2013)). Quotation detection looks deceptively simple, but is challenging, as the following example shows: [The pipeline], the company said, [would be built by a proposed joint venture . . . , and Trunkli"
P16-1164,E12-1064,1,0.806361,"extensive analysis with two new model architectures. We find that (a), simple boundary classification combined with a greedy prediction strategy is competitive with the state of the art; (b), a semi-Markov model significantly outperforms all others, by relaxing the Markov assumption. 1 Introduction Quotations are occurrences of reported speech, thought, and writing in text. They play an important role in computational linguistics and digital humanities, providing evidence for, e.g., speaker relationships (Elson et al., 2010), inter-speaker sentiment (Nalisnick and Baird, 2013) or politeness (Faruqui and Pado, 2012). Due to a lack of generalpurpose automatic systems, such information is often obtained through manual annotation (e.g., Agarwal et al. (2012)), which is labor-intensive and costly. Thus, models for automatic quotation detection form a growing research area (e.g., Pouliquen et al. (2007); Pareti et al. (2013)). Quotation detection looks deceptively simple, but is challenging, as the following example shows: [The pipeline], the company said, [would be built by a proposed joint venture . . . , and Trunkline . . . will “build and operate” the system . . . ].1 1 Penn Attributions Relation Corpus ("
P16-1164,P13-1166,0,0.0578368,"Missing"
P16-1164,P13-2147,1,0.90986,"Missing"
P16-1164,krestel-etal-2008-minding,0,0.215633,"Missing"
P16-1164,P14-5010,0,0.00345942,"(henceforth PARC3), by Pareti (2015).5 It contains AR annotations on the Wall Street Journal part of the Penn Treebank (2,294 5 Note that the data and thus the results differ from those previously published in (Pareti et al., 2013). news documents). As in related work, we use sections 1–22 as training set, section 23 as test set, and section 24 as development set. We perform the same preprocessing as Pareti: We use gold tokenization, lemmatization, part-of-speech tags, constituency parses, gold named entity annotations (Weischedel and Brunstein, 2005), and Stanford parser dependency analyses (Manning et al., 2014). Evaluation We report precision, recall, and micro-averaged F1 , adopting the two metrics introduced by Pareti et al. (2013): Strict match considers cases as correct where the boundaries of the spans match exactly. Partial match measures correctness as the ratio of overlap of the predicted and true spans. In both cases, we report numbers for each of the three quotation types (direct, indirect, mixed) and their micro averages. Like Pareti (2015), we exclude single-token content spans from the evaluation. To test for statistical significance of differences, we use the approximate randomization"
P16-1164,P13-2085,0,0.123561,"rnal context of a quotation. We perform an extensive analysis with two new model architectures. We find that (a), simple boundary classification combined with a greedy prediction strategy is competitive with the state of the art; (b), a semi-Markov model significantly outperforms all others, by relaxing the Markov assumption. 1 Introduction Quotations are occurrences of reported speech, thought, and writing in text. They play an important role in computational linguistics and digital humanities, providing evidence for, e.g., speaker relationships (Elson et al., 2010), inter-speaker sentiment (Nalisnick and Baird, 2013) or politeness (Faruqui and Pado, 2012). Due to a lack of generalpurpose automatic systems, such information is often obtained through manual annotation (e.g., Agarwal et al. (2012)), which is labor-intensive and costly. Thus, models for automatic quotation detection form a growing research area (e.g., Pouliquen et al. (2007); Pareti et al. (2013)). Quotation detection looks deceptively simple, but is challenging, as the following example shows: [The pipeline], the company said, [would be built by a proposed joint venture . . . , and Trunkline . . . will “build and operate” the system . . . ]."
P16-1164,D08-1004,0,0.0688316,"Missing"
P16-1164,N15-1005,0,0.0257517,"ale to our application, since the maximum length of a span factors into the prediction runtime, and quotations can be arbitrarily long. As an alternative, we propose a sampling-based approach: we draw candidate spans (proposals) from an informed, non-uniform distribution of spans. We score these spans to decide whether they should be added to the document (accepted) or not (rejected). This way, we efficiently traverse the space of potential span assignments while still being able to make informed decisions (cf. Wick et al. (2011)). To obtain a distribution over spans, we adapt the approach by Zhang et al. (2015). We introduce two independent probability distributions: Pb is the distribution of probabilities of a token being a begin token; Pe is the distribution of probabilities of a token being an end token. We sample a single content span proposal (D RAW P ROPOSAL) by first sampling the order in which the boundaries are to be determined (begin token or end token first) by sampling a binary variable d ∼ Bernoulli(0.5). If the begin token is to be sampled first, we continue by drawing a begin token tb ∼ Pb and finally draw an end token te ∼ Pe within a window of up to `max tokens to the right of tb ."
P16-1164,D13-1101,0,0.441509,"Missing"
P16-1164,D12-1122,0,0.0767057,"Missing"
P18-1231,agerri-etal-2014-ixa,0,0.0281299,"d Basque (EU). ments, we take 70 percent of the data for training, 20 percent for testing and the remaining 10 percent are used as development data for tuning. 4.2 Monolingual Word Embeddings For B LSE, A RTETXE, and M T, we require monolingual vector spaces for each of our languages. For English, we use the publicly available GoogleNews vectors4 . For Spanish, Catalan, and Basque, we train skip-gram embeddings using the Word2Vec toolkit4 with 300 dimensions, subsampling of 10−4 , window of 5, negative sampling of 15 based on a 2016 Wikipedia corpus5 (sentence-split, tokenized with IXA pipes (Agerri et al., 2014) and lowercased). The statistics of the Wikipedia corpora are given in Table 2. 4.3 Bilingual Lexicon For B LSE, A RTETXE, and BARISTA, we also require a bilingual lexicon. We use the sentiment lexicon from Hu and Liu (2004) (to which we refer in the following as Bing Liu) and its translation into each target language. We translate the lexicon using Google Translate and exclude multi-word expressions.6 This leaves a dictionary of 5700 translations in Spanish, 5271 in Catalan, and 4577 in Basque. We set aside ten percent of the translation pairs as a development set in order to check that the d"
P18-1231,P15-1040,0,0.0175784,"l sentiment analysis typically exploit machine translation based methods or multilingual models. Machine translation (M T) can provide a way to transfer sentiment information from a resource-rich to resourcepoor languages (Mihalcea et al., 2007; Balahur and Turchi, 2014). However, M T-based methods require large parallel corpora to train the translation system, which are often not available for underresourced languages. Examples of multilingual methods that have been applied to cross-lingual sentiment analysis include domain adaptation methods (Prettenhofer and Stein, 2011), delexicalization (Almeida et al., 2015), and bilingual word embeddings (Mikolov et al., 2013; Hermann and Blunsom, 2014; Artetxe et al., 2016). These approaches however do not incorporate enough sentiment information to perform well cross-lingually, as we will show later. We propose a novel approach to incorporate sentiment information in a model, which does not have these disadvantages. Bilingual Sentiment Embeddings (B LSE) are embeddings that are jointly optimized to represent both (a) semantic information in the source and target languages, which are bound to each other through a small bilingual dictionary, and (b) sentiment in"
P18-1231,D16-1250,0,0.508022,"e translation (M T) can provide a way to transfer sentiment information from a resource-rich to resourcepoor languages (Mihalcea et al., 2007; Balahur and Turchi, 2014). However, M T-based methods require large parallel corpora to train the translation system, which are often not available for underresourced languages. Examples of multilingual methods that have been applied to cross-lingual sentiment analysis include domain adaptation methods (Prettenhofer and Stein, 2011), delexicalization (Almeida et al., 2015), and bilingual word embeddings (Mikolov et al., 2013; Hermann and Blunsom, 2014; Artetxe et al., 2016). These approaches however do not incorporate enough sentiment information to perform well cross-lingually, as we will show later. We propose a novel approach to incorporate sentiment information in a model, which does not have these disadvantages. Bilingual Sentiment Embeddings (B LSE) are embeddings that are jointly optimized to represent both (a) semantic information in the source and target languages, which are bound to each other through a small bilingual dictionary, and (b) sentiment information, which is annotated on the source language only. We only need three resources: (i) a comparab"
P18-1231,P17-1042,0,0.164081,"Missing"
P18-1231,C10-1004,0,0.0300373,"nal Linguistics 2 Related Work Machine Translation: Early work in cross-lingual sentiment analysis found that machine translation (M T) had reached a point of maturity that enabled the transfer of sentiment across languages. Researchers translated sentiment lexicons (Mihalcea et al., 2007; Meng et al., 2012) or annotated corpora and used word alignments to project sentiment annotation and create target-language annotated corpora (Banea et al., 2008; Duh et al., 2011; Demirtas and Pechenizkiy, 2013; Balahur and Turchi, 2014). Several approaches included a multi-view representation of the data (Banea et al., 2010; Xiao and Guo, 2012) or co-training (Wan, 2009; Demirtas and Pechenizkiy, 2013) to improve over a naive implementation of machine translation, where only the translated data is used. There are also approaches which only require parallel data (Meng et al., 2012; Zhou et al., 2016; Rasooli et al., 2017), instead of machine translation. All of these approaches, however, require large amounts of parallel data or an existing high quality translation tool, which are not always available. A notable exception is the approach proposed by Chen et al. (2016), an adversarial deep averaging network, which"
P18-1231,D08-1014,0,0.215816,"al Meeting of the Association for Computational Linguistics (Long Papers), pages 2483–2493 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2 Related Work Machine Translation: Early work in cross-lingual sentiment analysis found that machine translation (M T) had reached a point of maturity that enabled the transfer of sentiment across languages. Researchers translated sentiment lexicons (Mihalcea et al., 2007; Meng et al., 2012) or annotated corpora and used word alignments to project sentiment annotation and create target-language annotated corpora (Banea et al., 2008; Duh et al., 2011; Demirtas and Pechenizkiy, 2013; Balahur and Turchi, 2014). Several approaches included a multi-view representation of the data (Banea et al., 2010; Xiao and Guo, 2012) or co-training (Wan, 2009; Demirtas and Pechenizkiy, 2013) to improve over a naive implementation of machine translation, where only the translated data is used. There are also approaches which only require parallel data (Meng et al., 2012; Zhou et al., 2016; Rasooli et al., 2017), instead of machine translation. All of these approaches, however, require large amounts of parallel data or an existing high qual"
P18-1231,L18-1104,1,0.839911,"t . As in the training procedure, for each sentence, we take the word embeddings from the target embeddings T and average them to ai ∈ Rd . We then project this vector to the joint bilingual space ˆ zi = ai · M 0 . Finally, we pass ˆ zi through a softmax layer P to get our prediction yˆi = softmax(ˆ zi · P ). 4 4.1 Datasets and Resources OpeNER and MultiBooked To evaluate our proposed model, we conduct experiments using four benchmark datasets and three bilingual combinations. We use the OpeNER English and Spanish datasets (Agerri et al., 2013) and the MultiBooked Catalan and Basque datasets (Barnes et al., 2018). All datasets contain hotel reviews which are annotated for aspect-level sentiment analysis. The labels include Strong Negative (−−), Negative (−), Positive (+), and Strong Positive (++). We map the aspect-level annotations to sentence level by taking the most common label and remove instances of mixed polarity. We also create a binary setup by combining the strong and weak classes. This gives us a total of six experiments. The details of the sentence-level datasets are summarized in Table 1. For each of the experi2486 5 Experiments 5.1 Figure 2: Binary and four class macro F1 on Spanish (ES)"
P18-1231,Q18-1039,0,0.0618566,"Missing"
P18-1231,P11-2075,0,0.0254505,"sociation for Computational Linguistics (Long Papers), pages 2483–2493 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2 Related Work Machine Translation: Early work in cross-lingual sentiment analysis found that machine translation (M T) had reached a point of maturity that enabled the transfer of sentiment across languages. Researchers translated sentiment lexicons (Mihalcea et al., 2007; Meng et al., 2012) or annotated corpora and used word alignments to project sentiment annotation and create target-language annotated corpora (Banea et al., 2008; Duh et al., 2011; Demirtas and Pechenizkiy, 2013; Balahur and Turchi, 2014). Several approaches included a multi-view representation of the data (Banea et al., 2010; Xiao and Guo, 2012) or co-training (Wan, 2009; Demirtas and Pechenizkiy, 2013) to improve over a naive implementation of machine translation, where only the translated data is used. There are also approaches which only require parallel data (Meng et al., 2012; Zhou et al., 2016; Rasooli et al., 2017), instead of machine translation. All of these approaches, however, require large amounts of parallel data or an existing high quality translation to"
P18-1231,N15-1157,0,0.186226,"requiring the projection to be orthogonal, thereby preserving the monolingual quality of the original word vectors. Given source embeddings S, target embeddings T , and a bilingual lexicon L, Artetxe et al. (2016) learn a projection matrix W by minimizing the square of Euclidean distances X arg min ||S 0 W − T 0 ||2F , (1) W i where S 0 ∈ S and T 0 ∈ T are the word embedding matrices for the tokens in the bilingual lexicon L. This is solved using the Moore-Penrose pseudoinverse S 0+ = (S 0T S 0 )−1 S 0T as W = S 0+ T 0 , which can be computed using SVD. We refer to this approach as A RTETXE. Gouws and Søgaard (2015) propose a method to create a pseudo-bilingual corpus with a small taskspecific bilingual lexicon, which can then be used to train bilingual embeddings (BARISTA). This approach requires a monolingual corpus in both the source and target languages and a set of translation pairs. The source and target corpora are concatenated and then every word is randomly kept or replaced by its translation with a probability of 0.5. Any kind of word embedding algorithm can be trained with this pseudo-bilingual corpus to create bilingual word embeddings. These last techniques have the advantage of requiring re"
P18-1231,P14-1006,0,0.109653,"multilingual models. Machine translation (M T) can provide a way to transfer sentiment information from a resource-rich to resourcepoor languages (Mihalcea et al., 2007; Balahur and Turchi, 2014). However, M T-based methods require large parallel corpora to train the translation system, which are often not available for underresourced languages. Examples of multilingual methods that have been applied to cross-lingual sentiment analysis include domain adaptation methods (Prettenhofer and Stein, 2011), delexicalization (Almeida et al., 2015), and bilingual word embeddings (Mikolov et al., 2013; Hermann and Blunsom, 2014; Artetxe et al., 2016). These approaches however do not incorporate enough sentiment information to perform well cross-lingually, as we will show later. We propose a novel approach to incorporate sentiment information in a model, which does not have these disadvantages. Bilingual Sentiment Embeddings (B LSE) are embeddings that are jointly optimized to represent both (a) semantic information in the source and target languages, which are bound to each other through a small bilingual dictionary, and (b) sentiment information, which is annotated on the source language only. We only need three re"
P18-1231,P15-1162,0,0.0208872,"Classification We add a second training objective to optimize the projected source vectors to predict the sentiment of source phrases. This inevitably changes the projection characteristics of the matrix M , and consequently M 0 and encourages M 0 to learn to predict sentiment without any training examples in the target language. To train M to predict sentiment, we require a source-language corpus Csource = {(x1 , y1 ), (x2 , y2 ), . . . , (xi , yi )} where each sentence xi is associated with a label yi . For classification, we use a two-layer feedforward averaging network, loosely following Iyyer et al. (2015)3 . For a sentence xi we take the word embeddings from the source embedding S and average them to ai ∈ Rd . We then project this vector to the joint bilingual space zi = ai · M . Finally, we pass zi through a softmax layer P to get our prediction yˆi = softmax(zi · P ). To train our model to predict sentiment, we minimize the cross-entropy error of our predictions H=− n X yi log yˆi − (1 − yi ) log(1 − yˆi ) . (3) i=1 3.3 Joint Learning In order to jointly train both the projection component and the sentiment component, we combine the two loss functions to optimize the parameter 1 We omit para"
P18-1231,P15-1027,0,0.0494919,"using the most frequent words as translation pairs is an effective approach, for sentiment analysis, this does not seem to help. Using a translated sentiment lexicon, even if it is small, gives better results. 9 http://www.meta-share.org The translation took approximately one hour. We can extrapolate that hand translating a sentiment lexicon the size of the Bing Liu lexicon would take no more than 5 hours. 10 BLSE No M&apos; translation translation source F1 source F1 target F1 target F1 1.0 Cosine Similarity Research into projection techniques for bilingual word embeddings (Mikolov et al., 2013; Lazaridou et al., 2015; Artetxe et al., 2016) often uses a lexicon of the most frequent 8–10 thousand words in English and their translations as training data. We test this approach by taking the 10,000 wordto-word translations from the Apertium Englishto-Spanish dictionary9 . We also use the Google Translate API to translate the NRC hashtag sentiment lexicon (Mohammad et al., 2013) and keep the 22,984 word-to-word translations. We perform the same experiment as above and vary the amount of training data from 0, 100, 300, 600, 1000, 3000, 6000, 10,000 up to 20,000 training pairs. Finally, we compile a small hand tr"
P18-1231,P11-1015,0,0.126172,"proach requires a monolingual corpus in both the source and target languages and a set of translation pairs. The source and target corpora are concatenated and then every word is randomly kept or replaced by its translation with a probability of 0.5. Any kind of word embedding algorithm can be trained with this pseudo-bilingual corpus to create bilingual word embeddings. These last techniques have the advantage of requiring relatively little parallel training data while taking advantage of larger amounts of monolingual data. However, they are not optimized for sentiment. Sentiment Embeddings: Maas et al. (2011) first explored the idea of incorporating sentiment information into semantic word vectors. They proposed a topic modeling approach similar to latent Dirichlet allocation in order to collect the semantic information in their word vectors. To incorporate the sentiment information, they included a second objective whereby they maximize the probability of the sentiment label for each word in a labeled document. Tang et al. (2014) exploit distantly annotated tweets to create Twitter sentiment embeddings. To incorporate distributional information about tokens, they use a hinge loss and maximize the"
P18-1231,P12-1060,0,0.0609445,"bilingual sentiment space. Our implementation is publicly available at https://github.com/jbarnesspain/blse. 2483 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2483–2493 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2 Related Work Machine Translation: Early work in cross-lingual sentiment analysis found that machine translation (M T) had reached a point of maturity that enabled the transfer of sentiment across languages. Researchers translated sentiment lexicons (Mihalcea et al., 2007; Meng et al., 2012) or annotated corpora and used word alignments to project sentiment annotation and create target-language annotated corpora (Banea et al., 2008; Duh et al., 2011; Demirtas and Pechenizkiy, 2013; Balahur and Turchi, 2014). Several approaches included a multi-view representation of the data (Banea et al., 2010; Xiao and Guo, 2012) or co-training (Wan, 2009; Demirtas and Pechenizkiy, 2013) to improve over a naive implementation of machine translation, where only the translated data is used. There are also approaches which only require parallel data (Meng et al., 2012; Zhou et al., 2016; Rasooli e"
P18-1231,P07-1123,0,0.839981,"sentiment analysis are motivated by the lack of training data in the vast majority of languages. Even languages spoken by several million people, such as Catalan, often have few resources available to perform sentiment analysis in specific domains. We therefore aim to harness the knowledge previously collected in resource-rich languages. Previous approaches for cross-lingual sentiment analysis typically exploit machine translation based methods or multilingual models. Machine translation (M T) can provide a way to transfer sentiment information from a resource-rich to resourcepoor languages (Mihalcea et al., 2007; Balahur and Turchi, 2014). However, M T-based methods require large parallel corpora to train the translation system, which are often not available for underresourced languages. Examples of multilingual methods that have been applied to cross-lingual sentiment analysis include domain adaptation methods (Prettenhofer and Stein, 2011), delexicalization (Almeida et al., 2015), and bilingual word embeddings (Mikolov et al., 2013; Hermann and Blunsom, 2014; Artetxe et al., 2016). These approaches however do not incorporate enough sentiment information to perform well cross-lingually, as we will s"
P18-1231,P09-1027,0,0.280041,"y work in cross-lingual sentiment analysis found that machine translation (M T) had reached a point of maturity that enabled the transfer of sentiment across languages. Researchers translated sentiment lexicons (Mihalcea et al., 2007; Meng et al., 2012) or annotated corpora and used word alignments to project sentiment annotation and create target-language annotated corpora (Banea et al., 2008; Duh et al., 2011; Demirtas and Pechenizkiy, 2013; Balahur and Turchi, 2014). Several approaches included a multi-view representation of the data (Banea et al., 2010; Xiao and Guo, 2012) or co-training (Wan, 2009; Demirtas and Pechenizkiy, 2013) to improve over a naive implementation of machine translation, where only the translated data is used. There are also approaches which only require parallel data (Meng et al., 2012; Zhou et al., 2016; Rasooli et al., 2017), instead of machine translation. All of these approaches, however, require large amounts of parallel data or an existing high quality translation tool, which are not always available. A notable exception is the approach proposed by Chen et al. (2016), an adversarial deep averaging network, which trains a joint feature extractor for two langu"
P18-1231,W10-3111,0,0.038225,"number of mistakes (an average of 71 per model on binary and 167 on 4-class), it is mainly due to its prevalence. M T performed the best on the test examples which according to the annotation require a correct understanding of the vocabulary (81 F1 on binary /54 F1 on 4-class), with B LSE (79/48) slightly worse. A RTETXE (70/35) and BARISTA (67/41) perform significantly worse. This suggests that B LSE is better A RTETXE and BARISTA at transferring sentiment of the most important sentiment bearing words. Negation: Negation is a well-studied phenomenon in sentiment analysis (Pang et al., 2002; Wiegand et al., 2010; Zhu et al., 2014; Reitan et al., 2015). Therefore, we are interested in how these four models perform on phrases that include the negation of a key element, for example “In general, this hotel isn’t bad”. We would like our models to recognize that the combination of two negative elements “isn’t” and “bad” lead to a Positive label. Given the simple classification strategy, all models perform relatively well on phrases with negation (all reach nearly 60 F1 in the binary setting). However, while B LSE performs the best on negation in the binary setting (82.9 F1 ), it has more problems with nega"
P18-1231,C12-1174,0,0.683476,"lated Work Machine Translation: Early work in cross-lingual sentiment analysis found that machine translation (M T) had reached a point of maturity that enabled the transfer of sentiment across languages. Researchers translated sentiment lexicons (Mihalcea et al., 2007; Meng et al., 2012) or annotated corpora and used word alignments to project sentiment annotation and create target-language annotated corpora (Banea et al., 2008; Duh et al., 2011; Demirtas and Pechenizkiy, 2013; Balahur and Turchi, 2014). Several approaches included a multi-view representation of the data (Banea et al., 2010; Xiao and Guo, 2012) or co-training (Wan, 2009; Demirtas and Pechenizkiy, 2013) to improve over a naive implementation of machine translation, where only the translated data is used. There are also approaches which only require parallel data (Meng et al., 2012; Zhou et al., 2016; Rasooli et al., 2017), instead of machine translation. All of these approaches, however, require large amounts of parallel data or an existing high quality translation tool, which are not always available. A notable exception is the approach proposed by Chen et al. (2016), an adversarial deep averaging network, which trains a joint featu"
P18-1231,C00-2137,0,0.0403147,".5 **60.0 38.1 *42.5 R **80.1 **73.0 **72.7 *43.4 38.1 37.4 F1 **74.6 **72.9 **69.3 *41.2 35.9 30.0 In Figure 2, we report the results of all four methods. Our method outperforms the other projection methods (the baselines A RTETXE and BARISTA) on four of the six experiments substantially. It performs only slightly worse than the more resourcecostly upper bounds (M T and M ONO). This is especially noticeable for the binary classification task, where B LSE performs nearly as well as machine translation and significantly better than the other methods. We perform approximate randomization tests (Yeh, 2000) with 10,000 runs and highlight the results that are statistically significant (**p < 0.01, *p < 0.05) in Table 3. In more detail, we see that M T generally performs better than the projection methods (79–69 F1 on binary, 52–44 on 4-class). B LSE (75–69 on binary, 41–30 on 4-class) has the best performance of the projection methods and is comparable with M T on the binary setup, with no significant difference on binary Basque. A RTETXE (67–46 on binary, 35–21 on 4-class) and BARISTA (61– 55 on binary, 40–34 on 4-class) are significantly worse than B LSE on all experiments except Catalan and Ba"
P18-1231,S13-2053,0,0.0422246,"ng Liu lexicon would take no more than 5 hours. 10 BLSE No M&apos; translation translation source F1 source F1 target F1 target F1 1.0 Cosine Similarity Research into projection techniques for bilingual word embeddings (Mikolov et al., 2013; Lazaridou et al., 2015; Artetxe et al., 2016) often uses a lexicon of the most frequent 8–10 thousand words in English and their translations as training data. We test this approach by taking the 10,000 wordto-word translations from the Apertium Englishto-Spanish dictionary9 . We also use the Google Translate API to translate the NRC hashtag sentiment lexicon (Mohammad et al., 2013) and keep the 22,984 word-to-word translations. We perform the same experiment as above and vary the amount of training data from 0, 100, 300, 600, 1000, 3000, 6000, 10,000 up to 20,000 training pairs. Finally, we compile a small hand translated dictionary of 200 pairs, which we then expand using target language morphological information, finally giving us 657 translation pairs10 . The macro F1 score for the Bing Liu dictionary climbs constantly with the increasing translation pairs. Both the Apertium and NRC dictionaries perform worse than the translated lexicon by Bing Liu, while the expande"
P18-1231,W02-1011,0,0.0239736,"s the largest total number of mistakes (an average of 71 per model on binary and 167 on 4-class), it is mainly due to its prevalence. M T performed the best on the test examples which according to the annotation require a correct understanding of the vocabulary (81 F1 on binary /54 F1 on 4-class), with B LSE (79/48) slightly worse. A RTETXE (70/35) and BARISTA (67/41) perform significantly worse. This suggests that B LSE is better A RTETXE and BARISTA at transferring sentiment of the most important sentiment bearing words. Negation: Negation is a well-studied phenomenon in sentiment analysis (Pang et al., 2002; Wiegand et al., 2010; Zhu et al., 2014; Reitan et al., 2015). Therefore, we are interested in how these four models perform on phrases that include the negation of a key element, for example “In general, this hotel isn’t bad”. We would like our models to recognize that the combination of two negative elements “isn’t” and “bad” lead to a Positive label. Given the simple classification strategy, all models perform relatively well on phrases with negation (all reach nearly 60 F1 in the binary setting). However, while B LSE performs the best on negation in the binary setting (82.9 F1 ), it has m"
P18-1231,P15-1042,0,0.0548822,"sentiment information, they included a second objective whereby they maximize the probability of the sentiment label for each word in a labeled document. Tang et al. (2014) exploit distantly annotated tweets to create Twitter sentiment embeddings. To incorporate distributional information about tokens, they use a hinge loss and maximize the likelihood of a true n-gram over a corrupted n-gram. They include a second objective where they classify the polarity of the tweet given the true n-gram. While these techniques have proven useful, they are not easily transferred to a cross-lingual setting. Zhou et al. (2015) create bilingual sentiment embeddings by translating all source data to the 2484 target language and vice versa. This requires the existence of a machine translation system, which is a prohibitive assumption for many under-resourced languages, especially if it must be open and freely accessible. This motivates approaches which can use smaller amounts of parallel data to achieve similar results. 3 n 1X MSE = (zi − ˆ zi )2 , n (2) i=1 Model In order to project not only semantic similarity and relatedness but also sentiment information to our target language, we propose a new model, namely Bilin"
P18-1231,P14-1029,0,0.0235702,"n average of 71 per model on binary and 167 on 4-class), it is mainly due to its prevalence. M T performed the best on the test examples which according to the annotation require a correct understanding of the vocabulary (81 F1 on binary /54 F1 on 4-class), with B LSE (79/48) slightly worse. A RTETXE (70/35) and BARISTA (67/41) perform significantly worse. This suggests that B LSE is better A RTETXE and BARISTA at transferring sentiment of the most important sentiment bearing words. Negation: Negation is a well-studied phenomenon in sentiment analysis (Pang et al., 2002; Wiegand et al., 2010; Zhu et al., 2014; Reitan et al., 2015). Therefore, we are interested in how these four models perform on phrases that include the negation of a key element, for example “In general, this hotel isn’t bad”. We would like our models to recognize that the combination of two negative elements “isn’t” and “bad” lead to a Positive label. Given the simple classification strategy, all models perform relatively well on phrases with negation (all reach nearly 60 F1 in the binary setting). However, while B LSE performs the best on negation in the binary setting (82.9 F1 ), it has more problems with negation in the 4-clas"
P18-1231,W15-2914,0,0.0526238,"Missing"
P18-1231,P14-1146,0,0.0717978,"latively little parallel training data while taking advantage of larger amounts of monolingual data. However, they are not optimized for sentiment. Sentiment Embeddings: Maas et al. (2011) first explored the idea of incorporating sentiment information into semantic word vectors. They proposed a topic modeling approach similar to latent Dirichlet allocation in order to collect the semantic information in their word vectors. To incorporate the sentiment information, they included a second objective whereby they maximize the probability of the sentiment label for each word in a labeled document. Tang et al. (2014) exploit distantly annotated tweets to create Twitter sentiment embeddings. To incorporate distributional information about tokens, they use a hinge loss and maximize the likelihood of a true n-gram over a corrupted n-gram. They include a second objective where they classify the polarity of the tweet given the true n-gram. While these techniques have proven useful, they are not easily transferred to a cross-lingual setting. Zhou et al. (2015) create bilingual sentiment embeddings by translating all source data to the 2484 target language and vice versa. This requires the existence of a machine"
P18-4012,N16-1097,0,0.0391421,"Missing"
P18-4012,burchardt-etal-2006-salto,0,0.0243847,"f annotations (e. g., from multiple imports). It is implemented as a web service in order to support remote annotation workflows for multiple users in parallel. Figure 1: Example template following a schema derived from the Spinal Cord Injury Ontology. Availability and License. A demo installation is available at http://psink.techfak. uni-bielefeld.de/santo/. The source code of the application is publicly available under the Apache 2.0 License at https://github. com/ag-sc/SANTO. 2 Related Work Most annotation frameworks for text focus on the sentence level. Examples include syntactic parsing (Burchardt et al., 2006) or semantic role labeling (Kakkonen, 2006; Yimam et al., 2013). Other tools focus on segmentation annotation tasks, for instance Callisto (Day et al., 2004), WordFreak (Morton and LaCivita, 2003), MMax2 (M¨uller and Strube, 2006), or GATE Teamware (Bontcheva et al., 2013) (though the latter also supports more complex schemata). Brat (Stenetorp et al., 2012), WebAnno (Yimam et al., 2013), eHost (South et al., 2012) and CAT (Lenzi et al., 2012) support approaches for relational annotations. These tools are easy to use and highly flexible regarding the specification of annotation schemes. Projec"
P18-4012,day-etal-2004-callisto,0,0.0452109,"gure 1: Example template following a schema derived from the Spinal Cord Injury Ontology. Availability and License. A demo installation is available at http://psink.techfak. uni-bielefeld.de/santo/. The source code of the application is publicly available under the Apache 2.0 License at https://github. com/ag-sc/SANTO. 2 Related Work Most annotation frameworks for text focus on the sentence level. Examples include syntactic parsing (Burchardt et al., 2006) or semantic role labeling (Kakkonen, 2006; Yimam et al., 2013). Other tools focus on segmentation annotation tasks, for instance Callisto (Day et al., 2004), WordFreak (Morton and LaCivita, 2003), MMax2 (M¨uller and Strube, 2006), or GATE Teamware (Bontcheva et al., 2013) (though the latter also supports more complex schemata). Brat (Stenetorp et al., 2012), WebAnno (Yimam et al., 2013), eHost (South et al., 2012) and CAT (Lenzi et al., 2012) support approaches for relational annotations. These tools are easy to use and highly flexible regarding the specification of annotation schemes. Projects are easy to manage due to administration interfaces and remote annotation is supported. However, these approaches share the limitation that all relational"
P18-4012,W12-2416,0,0.0218138,"er the Apache 2.0 License at https://github. com/ag-sc/SANTO. 2 Related Work Most annotation frameworks for text focus on the sentence level. Examples include syntactic parsing (Burchardt et al., 2006) or semantic role labeling (Kakkonen, 2006; Yimam et al., 2013). Other tools focus on segmentation annotation tasks, for instance Callisto (Day et al., 2004), WordFreak (Morton and LaCivita, 2003), MMax2 (M¨uller and Strube, 2006), or GATE Teamware (Bontcheva et al., 2013) (though the latter also supports more complex schemata). Brat (Stenetorp et al., 2012), WebAnno (Yimam et al., 2013), eHost (South et al., 2012) and CAT (Lenzi et al., 2012) support approaches for relational annotations. These tools are easy to use and highly flexible regarding the specification of annotation schemes. Projects are easy to manage due to administration interfaces and remote annotation is supported. However, these approaches share the limitation that all relational structures need to be anchored at the textual surface. Thus, annotating complex templates as in Figure 1 becomes tedious and visually cumbersome, especially in cases of complex nestings within or across templates, or when fillers are widely dispersed across mu"
P18-4012,E12-2021,0,0.209166,"Missing"
P18-4012,bartalesi-lenzi-etal-2012-cat,0,0.0254089,"https://github. com/ag-sc/SANTO. 2 Related Work Most annotation frameworks for text focus on the sentence level. Examples include syntactic parsing (Burchardt et al., 2006) or semantic role labeling (Kakkonen, 2006; Yimam et al., 2013). Other tools focus on segmentation annotation tasks, for instance Callisto (Day et al., 2004), WordFreak (Morton and LaCivita, 2003), MMax2 (M¨uller and Strube, 2006), or GATE Teamware (Bontcheva et al., 2013) (though the latter also supports more complex schemata). Brat (Stenetorp et al., 2012), WebAnno (Yimam et al., 2013), eHost (South et al., 2012) and CAT (Lenzi et al., 2012) support approaches for relational annotations. These tools are easy to use and highly flexible regarding the specification of annotation schemes. Projects are easy to manage due to administration interfaces and remote annotation is supported. However, these approaches share the limitation that all relational structures need to be anchored at the textual surface. Thus, annotating complex templates as in Figure 1 becomes tedious and visually cumbersome, especially in cases of complex nestings within or across templates, or when fillers are widely dispersed across multiple sentences in a text. W"
P18-4012,P13-4001,0,0.101117,"Missing"
P18-4012,N03-4009,0,0.100058,"wing a schema derived from the Spinal Cord Injury Ontology. Availability and License. A demo installation is available at http://psink.techfak. uni-bielefeld.de/santo/. The source code of the application is publicly available under the Apache 2.0 License at https://github. com/ag-sc/SANTO. 2 Related Work Most annotation frameworks for text focus on the sentence level. Examples include syntactic parsing (Burchardt et al., 2006) or semantic role labeling (Kakkonen, 2006; Yimam et al., 2013). Other tools focus on segmentation annotation tasks, for instance Callisto (Day et al., 2004), WordFreak (Morton and LaCivita, 2003), MMax2 (M¨uller and Strube, 2006), or GATE Teamware (Bontcheva et al., 2013) (though the latter also supports more complex schemata). Brat (Stenetorp et al., 2012), WebAnno (Yimam et al., 2013), eHost (South et al., 2012) and CAT (Lenzi et al., 2012) support approaches for relational annotations. These tools are easy to use and highly flexible regarding the specification of annotation schemes. Projects are easy to manage due to administration interfaces and remote annotation is supported. However, these approaches share the limitation that all relational structures need to be anchored at the"
P18-4012,D17-1004,0,0.0491584,"Missing"
P18-4012,N06-4006,0,0.0395608,"administration interfaces and remote annotation is supported. However, these approaches share the limitation that all relational structures need to be anchored at the textual surface. Thus, annotating complex templates as in Figure 1 becomes tedious and visually cumbersome, especially in cases of complex nestings within or across templates, or when fillers are widely dispersed across multiple sentences in a text. We propose a tool to frame complex relational annotation problems as slot filling tasks. To our knowledge, the only existing tool for this purpose is the Prot´eg´e plugin Knowtator (Ogren, 2006), which is, however, not web-based, comparably difficult to use with multiple annotators, and no longer actively supported since 2009. Thus, our main contribution is an annotation tool which combines the advantages of (i) enabling complex relational slot filling with distant fillers, and (ii) ease of use in web-based environments in order to facilitate remote collaboration. SANTO is ontology-based, i. e., entity and relation types are derived from an underlying ontology. The same idea is prominent in several annotation tools within the Semantic Web community (cf. Oliveira and Rocha, 2013). Con"
P19-1391,C16-1152,0,0.0506035,"Missing"
P19-1391,C18-1179,1,0.893782,"Missing"
P19-1391,W18-1808,0,0.0207177,"sses to previous studies (Bostan and Klinger, 2018). Modeling performance and inter-annotator disagreement are correlated: emotions that are difficult to annotate are also difficult to predict (Spearman’s ρ between F1 and the diagonal in Figure 1 is 0.85 for German, p = .01, and 0.75 for English, p = .05). It is notable that results for German are on a level with English despite the translation step and the shorter length of the German descriptions. That goes against our expectations, as previous studies showed that translation is only sentimentpreserving to some degree (Salameh et al., 2015; Lohar et al., 2018). We take this outcome as evidence for the cross-lingual comparability of deISEAR and enISEAR, and our general method. 5 Conclusion We presented (a) deISEAR, a corpus of 1001 event descriptions in German, annotated with seven emotion classes; and (b) enISEAR, a companion English resource build analogously, to disentangle effects of annotation setup and English when comparing to the original ISEAR resource. Our two-phase annotation setup shows that perceived emotions can be different from expressed emotions in such eventfocused corpus, which also affects classification performance. Emotions var"
P19-1391,W11-1514,0,0.0311947,"t which consists of two steps. In step 1, participants create descriptions of emotional events for a given emotion. In step 2, five annotators assess the emotion expressed by the texts. We show that transferring an emotion classification model from the original English ISEAR to the German crowdsourced deISEAR via machine translation does not, on average, cause a performance drop. 1 Introduction Feeling emotions is a central part of the “human condition” (Russell, 1945). While existing studies on automatic recognition of emotions in text have achieved promising results (Pool and Nissim (2016); Mohammad (2011), i.a.), we see two main shortcomings. First, there is shortage of resources for non-English languages, with few exceptions, like Chinese (Li et al., 2017; Odbal and Wang, 2014; Yuan et al., 2002). This hampers the data-driven modeling of emotion recognition that has unfolded, e.g., for the related task of sentiment analysis. Second, emotions can be expressed in language with a wide variety of linguistic devices, from direct mentions (e.g., “I’m angry”) to evocative images (e.g.,“He was petrified”) or prosody. Computational emotion recognition on English has mostly focused on explicit emotion"
P19-1391,W14-6809,0,0.027761,"by the texts. We show that transferring an emotion classification model from the original English ISEAR to the German crowdsourced deISEAR via machine translation does not, on average, cause a performance drop. 1 Introduction Feeling emotions is a central part of the “human condition” (Russell, 1945). While existing studies on automatic recognition of emotions in text have achieved promising results (Pool and Nissim (2016); Mohammad (2011), i.a.), we see two main shortcomings. First, there is shortage of resources for non-English languages, with few exceptions, like Chinese (Li et al., 2017; Odbal and Wang, 2014; Yuan et al., 2002). This hampers the data-driven modeling of emotion recognition that has unfolded, e.g., for the related task of sentiment analysis. Second, emotions can be expressed in language with a wide variety of linguistic devices, from direct mentions (e.g., “I’m angry”) to evocative images (e.g.,“He was petrified”) or prosody. Computational emotion recognition on English has mostly focused on explicit emotion expressions. Often, however, emotions are merely inferable from world knowledge and experience. For instance, ”I finally found love” presumably depicts a joyful circumstance, w"
P19-1391,W16-4304,0,0.030813,"crowdsourcing experiment which consists of two steps. In step 1, participants create descriptions of emotional events for a given emotion. In step 2, five annotators assess the emotion expressed by the texts. We show that transferring an emotion classification model from the original English ISEAR to the German crowdsourced deISEAR via machine translation does not, on average, cause a performance drop. 1 Introduction Feeling emotions is a central part of the “human condition” (Russell, 1945). While existing studies on automatic recognition of emotions in text have achieved promising results (Pool and Nissim (2016); Mohammad (2011), i.a.), we see two main shortcomings. First, there is shortage of resources for non-English languages, with few exceptions, like Chinese (Li et al., 2017; Odbal and Wang, 2014; Yuan et al., 2002). This hampers the data-driven modeling of emotion recognition that has unfolded, e.g., for the related task of sentiment analysis. Second, emotions can be expressed in language with a wide variety of linguistic devices, from direct mentions (e.g., “I’m angry”) to evocative images (e.g.,“He was petrified”) or prosody. Computational emotion recognition on English has mostly focused on"
P19-1391,W17-0801,0,0.153474,"nal dataset, all the reports were translated to English, and accordingly, the responses of, e.g., German speakers who took part in the survey are not available in their original language. In this paper, we follow Scherer and Wallbott (1997) by re-using their set of seven basic emotions and recreating part of their questionnaire both in English and German. In contrast to ISEAR, we account for the fact that a description can be related to different emotions by its writer and its readers. Affective analyses have rendered evidence that emotional standpoints affect the quality of annotation tasks (Buechel and Hahn, 2017). For instance, annotation results vary depending on whether workers are asked if a text is associated with an emotion and if it evokes an emotion, with the first phrasing downplaying the reader’s perspective and inducing higher inter-annotator agreement (Mohammad and Turney, 2013). We take notice of these findings to design our annotation guidelines. 3 Crowdsourcing-based Corpus Creation We developed a two-phase crowdsourcing experiment: one for generating descriptions, the other for rating the emotions of the descriptions. Phase 1 can be understood as sampling from P (description|emotion), o"
P19-1391,klinger-cimiano-2014-usage,1,0.803168,"/data/emotion, supports the development of emotion classification models in German and English including multilingual aspects. 2 Previous Work For the related but structurally simpler task of sentiment analysis, resources have been created in many languages. For German, this includes dictionaries 4005 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4005–4011 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics (Ruppenhofer et al., 2017, i.a.), corpora of newspaper comments (Schabus et al., 2017) and reviews (Klinger and Cimiano, 2014; Ruppenhofer et al., 2014; Boland et al., 2013). Nevertheless, the resource situation leaves much to be desired. The situation is even more difficult for emotion analysis. Emotion annotation is slower and more subjective (Schuff et al., 2017). Further, there is less agreement on the set of classes to use, stemming from alternative psychological theories. These include, e.g., discrete classes vs. multiple continuous dimensions (Buechel and Hahn, 2016). Resources developed by one strand of research can be unusable for the other (Bostan and Klinger, 2018). In German, a few dictionaries have been"
P19-1391,W18-6206,1,0.878082,"Missing"
P19-1391,ruppenhofer-etal-2017-evaluating,0,0.0231514,"rences including a modelling experiment. Our corpus, available at https: //www.ims.uni-stuttgart.de/data/emotion, supports the development of emotion classification models in German and English including multilingual aspects. 2 Previous Work For the related but structurally simpler task of sentiment analysis, resources have been created in many languages. For German, this includes dictionaries 4005 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4005–4011 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics (Ruppenhofer et al., 2017, i.a.), corpora of newspaper comments (Schabus et al., 2017) and reviews (Klinger and Cimiano, 2014; Ruppenhofer et al., 2014; Boland et al., 2013). Nevertheless, the resource situation leaves much to be desired. The situation is even more difficult for emotion analysis. Emotion annotation is slower and more subjective (Schuff et al., 2017). Further, there is less agreement on the set of classes to use, stemming from alternative psychological theories. These include, e.g., discrete classes vs. multiple continuous dimensions (Buechel and Hahn, 2016). Resources developed by one strand of resear"
P19-1391,N15-1078,0,0.0250616,"es between emotion classes to previous studies (Bostan and Klinger, 2018). Modeling performance and inter-annotator disagreement are correlated: emotions that are difficult to annotate are also difficult to predict (Spearman’s ρ between F1 and the diagonal in Figure 1 is 0.85 for German, p = .01, and 0.75 for English, p = .05). It is notable that results for German are on a level with English despite the translation step and the shorter length of the German descriptions. That goes against our expectations, as previous studies showed that translation is only sentimentpreserving to some degree (Salameh et al., 2015; Lohar et al., 2018). We take this outcome as evidence for the cross-lingual comparability of deISEAR and enISEAR, and our general method. 5 Conclusion We presented (a) deISEAR, a corpus of 1001 event descriptions in German, annotated with seven emotion classes; and (b) enISEAR, a companion English resource build analogously, to disentangle effects of annotation setup and English when comparing to the original ISEAR resource. Our two-phase annotation setup shows that perceived emotions can be different from expressed emotions in such eventfocused corpus, which also affects classification perf"
P19-1391,W17-5203,1,0.763055,"Missing"
R09-1035,N04-1042,0,0.0338279,"eatures are generated. Then, all other tokens are tested for these features. This method is typically applied to determine the identity of words as well as for prefixes or suffixes of different length or for learning schemata of regular expression-like patterns [20]. Only a few approaches dealing with feature handling for CRFs are published. The work by McCallum [12] demonstrates a method for iteratively constructing feature conjunctions that would increase conditional log-likelihood if added to the model. An analysis of different penalty terms for regularization is shown by Peng and McCallum [15]. Goodman [3] presented a related analysis of exponential priors for Maximum Entropy models. The very recent work of Vail et al. [23, 22] shows feature selection in Conditional Random Fields by L1 -norm regularization in the robotics domain, a work which is related to the selection in Maximum Entropy models proposed by Koh et al. [7]. These methods incorporate the training procedure in the selection process. In contrast, we present different filter methods for feature selection to limit the complexity before starting the training. It is demonstrated how the sequential structure of text can be"
R09-1035,W03-0419,0,0.0798748,"the basis of two data sets with slightly different configurations of the CRF. Quantities of entities are given in Table 2. The BioCreative 2 Gene Mention Task data (BC2) contains entities of the class Gene/Protein with the specialty of acceptance of several boundaries for entities [24]. We incorporate the configuration of the CRF as described in a participating system using only the shortest possible annotation as exact true positive per entity [6, 21]. Name BC2 CoNLL Training Set BIO 18165 29466 15017 13180 382983 213499 Test Set BC2 CoNLL BIO 6290 5654 4801 2458 128915 38554 The CoNLL data [19] is an annotation of the Reuters corpus [10] containing the classes person, organization, locations and misc. We use an order-one CRF with offset conjunction combining features of one preceding and succeeding token for each position in the text sequence. The feature set is fairly standard with Word-As-Class, prefix and suffix generation of length two, three and four as well as regular expressions detecting capital letters, numbers, dashes and dots separately and as parts of tokens. The combination of the provided sets “train” and “testa” is used for training and “testb” for testing. For evalua"
R09-1035,J96-1002,0,0.0360627,"etworks [2] or decision trees [17]. The main advantages are an improvement of prediction performance, faster training and prediction as well as a better understanding of the models [4]. Methods can be distinguished between filters not using the learning algorithm and wrappers using the learning algorithm as a black box [8]. An overview of approaches for classification tasks is given by Liu and Motoda [11], more specifically for text classification by Yang and Pederson [25]. Such feature selection methods are not well established for Conditional Random Fields [9], a Maximum Entropybased method [1] for structured data. We propose methods coping with the demanding task of handling sequential data represented by a huge number of features. Reported numbers are for instance 1,686,456 for a gene name tagger [5]. Due to this high complexity, training and inference times can explode. These high numbers of features are generated by automated methods, i. e., for every token in the training set, features are generated. Then, all other tokens are tested for these features. This method is typically applied to determine the identity of words as well as for prefixes or suffixes of different length or"
R09-1035,N04-1039,0,0.0488946,"nerated. Then, all other tokens are tested for these features. This method is typically applied to determine the identity of words as well as for prefixes or suffixes of different length or for learning schemata of regular expression-like patterns [20]. Only a few approaches dealing with feature handling for CRFs are published. The work by McCallum [12] demonstrates a method for iteratively constructing feature conjunctions that would increase conditional log-likelihood if added to the model. An analysis of different penalty terms for regularization is shown by Peng and McCallum [15]. Goodman [3] presented a related analysis of exponential priors for Maximum Entropy models. The very recent work of Vail et al. [23, 22] shows feature selection in Conditional Random Fields by L1 -norm regularization in the robotics domain, a work which is related to the selection in Maximum Entropy models proposed by Koh et al. [7]. These methods incorporate the training procedure in the selection process. In contrast, we present different filter methods for feature selection to limit the complexity before starting the training. It is demonstrated how the sequential structure of text can be respected by"
R09-1036,H05-1087,0,0.0197917,"for Fβ evaluation is meaningful for information retrieval tasks often demanding for a high recall or information extraction with the need for a high precision. 2 Introduction In information retrieval, the Fβ measure, the weighted harmonic mean between recall and precision, is established as evaluation measure. The corresponding β value to be chosen is application-depend. Methods for selecting β at training time exist for Support Vector Machines [18], Logistic Regression as well as Conditional Random Fields (CRF) [11] all of which are classically optimized by means of accuracy-related measures [7, 8, 20]. A similar goal is known from the AmilCare system [4] with the main focus on user involvement. At inference time, a parameter to select between higher precision or recall can be introduced by changing the decision threshold for an adequate decision function d(·) ∈ R. In sequential segmentation tasks like named entity recognition (NER), precision can be increased with this approach without retraining. Increasing recall is possible with the allowance of overlaps 2.1 Methods Conditional Random Fields and Text Segmentation Conditional Random Fields (CRF) [11, 13] are a family of probabilistic, un"
R09-1036,R09-1035,1,0.777851,"locations and misc. We use an order-one CRF with offset conjunction combining features of one preceding and succeeding token for each position in the text sequence. The feature set is fairly standard with WordAs-Class, prefix and suffix generation of length two, three and four as well as several regular expressions detecting capital letters, numbers, dashes and dots separately and as parts of tokens. The combination of the provided sets “train” and “testa” is used for training and “testb” for testing. In both settings, a feature selection based on information gain is performed (namely IG-OAA [9]). For CoNLL, we use 38095 features and 22993 for BC2. 3.2 Results Figure 1 depicts the final population for both data sets. The estimated pareto-fronts for the training and test sets are shown, each individual forming one position in the plot on each front is connected with a line. The boxes show the results of the initial individual trained to maximize log-likelihood. The blue, green and red line show the individual with highest F2 , F1 and F0.5 measure respectively. The pareto-front on the training set is the one determined by MOCRF. The results shown as pareto-front on the test set are the"
R09-1036,J96-1002,0,0.0175172,"Missing"
R09-1036,W03-0419,0,0.0382221,". . , λ1,r , λ2,r+1 , . . . , λ2,n . (8) The objective functions are prec(~λ, D) and rec(~λ, D). 194 3 3.1 Experiments Data Sets The results and evaluations are shown on the basis of two data sets with slightly different configurations of the CRF. The BioCreative 2 Gene Mention Task data (BC2) contains entities of the class Gene/Protein with the specialty of acceptance of several boundaries for entities [21]. We incorporate the configuration of the CRF as described in a participating system using only the shortest possible annotation as exact true positive per entity [10, 19]. The ConLL data [17] is an annotation of the Reuters corpus [12] containing the classes person, organization, locations and misc. We use an order-one CRF with offset conjunction combining features of one preceding and succeeding token for each position in the text sequence. The feature set is fairly standard with WordAs-Class, prefix and suffix generation of length two, three and four as well as several regular expressions detecting capital letters, numbers, dashes and dots separately and as parts of tokens. The combination of the provided sets “train” and “testa” is used for training and “testb” for testing. In"
R09-1036,P06-1028,0,0.213674,"g. The output is an estimation of pareto-optimal solutions from which the user can select the best for the actual application. Evaluated on two publicly available data sets in the field of named entity recognition, nearly all Fβ values are superior to those resulting from log-likelihood training. Keywords Named Entity Recognition, Conditional Random Fields, MultiObjective Optimization, NSGA-II, Fβ measure, Recall, Precision 1 as demonstrated for gene and protein names [3]. This requires the computation of reliable confidences, which increasing runtime is a drawback especially during inference [5, 20]. In contrast to optimizing one special value or selecting the set of output entities in prediction phase, we propose to use an evolutionary optimization scheme to optimize recall and precision in a multi-objective way to yield different model configurations, which can be selected by an end-user depending on the respective task with higher recall or higher precision without retraining. Thereby, the non-intentional choice of precision and recall by optimization of accuracy (which is performed by maximizing the log-likelihood of the model given the training data in the case of CRFs) is avoided."
R09-1036,N04-4028,0,0.0294958,"g. The output is an estimation of pareto-optimal solutions from which the user can select the best for the actual application. Evaluated on two publicly available data sets in the field of named entity recognition, nearly all Fβ values are superior to those resulting from log-likelihood training. Keywords Named Entity Recognition, Conditional Random Fields, MultiObjective Optimization, NSGA-II, Fβ measure, Recall, Precision 1 as demonstrated for gene and protein names [3]. This requires the computation of reliable confidences, which increasing runtime is a drawback especially during inference [5, 20]. In contrast to optimizing one special value or selecting the set of output entities in prediction phase, we propose to use an evolutionary optimization scheme to optimize recall and precision in a multi-objective way to yield different model configurations, which can be selected by an end-user depending on the respective task with higher recall or higher precision without retraining. Thereby, the non-intentional choice of precision and recall by optimization of accuracy (which is performed by maximizing the log-likelihood of the model given the training data in the case of CRFs) is avoided."
R11-1082,W10-1902,0,0.0583274,"Missing"
R11-1082,W03-0419,0,\N,Missing
W11-3904,P10-1030,0,0.136949,"context of its interaction partners, e.g. other proteins or metabolites. Accordingly, getting a better understanding of protein-protein interactions (PPIs) is vital to understand biological processes within organisms. Several databases, such as IntAct, DIP, or MINT, contain detailed 25 Robust Unsupervised and Semi-Supervised Methods in Natural Language Processing, pages 25–32, Hissar, Bulgaria, 15 September 2011. to learn literally thousands of classifiers from relational databases like Freebase (Mintz et al., 2009; Yao et al., 2010), Yago (Nguyen and Moschitti, 2011), or Wikipedia infoboxes (Hoffmann et al., 2010). Pairs Corpus AIMed BioInfer HPRD50 IEPA LLL So far, approaches in the biomedical domain on distant supervision focused on pattern learning (Hakenberg et al., 2008; Abacha and Zweigenbaum, 2010; Thomas et al., 2011). This is surprising as statistical machine learning methods are most commonly used for relation extraction. For example, only one of the five best performing systems in the BioNLP 2011 shared task relied on patterns (Kim et al., 2011). Class ratio positive negative 1,000 2,534 163 335 164 4,834 7,132 270 482 166 positive negative 0.21 0.35 0.60 0.73 0.99 Table 1: Overview of the 5"
W11-3904,W11-1802,0,0.151848,"ese corpora, see Table 3. ing on the complexity of the corpus (Airola et al., 2008). The contribution of the work described herein is as follows: We present different variations of strategies to utilize distant supervision for PPI extraction in Section 2. The potential benefit for PPI extraction is evaluated. Parameters taken into account are the number of training instances as well as the ratio of positive to negative examples. Finally, we assess if an ensemble of classifiers can further improve classification performance. The approaches described by Hakenberg et al. (2008) and Thomas et al. (2011) are those most related to our work. Both approaches learn a set of initial patterns by extracting sentences from MEDLINE potentially describing protein-protein interactions. Both methods use a knowledge base (IntAct) as input and search sentences containing protein pairs known to interact according to the knowledge base. However, these approaches generate patterns only for positive training instances and ignore the information contained in the remaining presumably negative instances. 2 Methods In this section, the workflow to extract interaction pairs from the databases and to generate traini"
W11-3904,P09-1113,0,0.305296,"Missing"
W11-3904,P11-2048,0,0.0399267,"nction depends, to a large degree, on the functional context of its interaction partners, e.g. other proteins or metabolites. Accordingly, getting a better understanding of protein-protein interactions (PPIs) is vital to understand biological processes within organisms. Several databases, such as IntAct, DIP, or MINT, contain detailed 25 Robust Unsupervised and Semi-Supervised Methods in Natural Language Processing, pages 25–32, Hissar, Bulgaria, 15 September 2011. to learn literally thousands of classifiers from relational databases like Freebase (Mintz et al., 2009; Yao et al., 2010), Yago (Nguyen and Moschitti, 2011), or Wikipedia infoboxes (Hoffmann et al., 2010). Pairs Corpus AIMed BioInfer HPRD50 IEPA LLL So far, approaches in the biomedical domain on distant supervision focused on pattern learning (Hakenberg et al., 2008; Abacha and Zweigenbaum, 2010; Thomas et al., 2011). This is surprising as statistical machine learning methods are most commonly used for relation extraction. For example, only one of the five best performing systems in the BioNLP 2011 shared task relied on patterns (Kim et al., 2011). Class ratio positive negative 1,000 2,534 163 335 164 4,834 7,132 270 482 166 positive negative 0.2"
W11-3904,P07-1073,0,0.129729,"extraction is usually tackled by classifying the n2 undirected protein mention pairs within a sentence, where n is the number of protein mentions in the sentence. Classification of such pairs is often approached by machine learning (Airola et al., 2008; Tikk et al., 2010) or pattern-based methods (Fundel et al., 2007; Hakenberg et al., 2008) both requiring manually annotated corpora, which are costly to obtain and often biased to the annotation guidelines and corpus selection criteria. To overcome this issue, recent work has concentrated on distant supervision and multiple instance learning (Bunescu and Mooney, 2007; Mintz et al., 2009). Instead of manually annotated corpora, such approaches infer training instances from nonannotated texts using knowledge bases, thus allowing to increase the training set size by a few orders of magnitude. Corpora derived by distant supervision are inherently noisy, thus benefiting from robust classification methods. Most relation extraction methods, especially in the domain of biology, rely on machine learning methods to classify a cooccurring pair of entities in a sentence to be related or not. Such an approach requires a training corpus, which involves expert annotatio"
W11-3904,W11-0201,1,0.919223,"Several databases, such as IntAct, DIP, or MINT, contain detailed 25 Robust Unsupervised and Semi-Supervised Methods in Natural Language Processing, pages 25–32, Hissar, Bulgaria, 15 September 2011. to learn literally thousands of classifiers from relational databases like Freebase (Mintz et al., 2009; Yao et al., 2010), Yago (Nguyen and Moschitti, 2011), or Wikipedia infoboxes (Hoffmann et al., 2010). Pairs Corpus AIMed BioInfer HPRD50 IEPA LLL So far, approaches in the biomedical domain on distant supervision focused on pattern learning (Hakenberg et al., 2008; Abacha and Zweigenbaum, 2010; Thomas et al., 2011). This is surprising as statistical machine learning methods are most commonly used for relation extraction. For example, only one of the five best performing systems in the BioNLP 2011 shared task relied on patterns (Kim et al., 2011). Class ratio positive negative 1,000 2,534 163 335 164 4,834 7,132 270 482 166 positive negative 0.21 0.35 0.60 0.73 0.99 Table 1: Overview of the 5 corpora used for evaluation. For state-of-the-art results on these corpora, see Table 3. ing on the complexity of the corpus (Airola et al., 2008). The contribution of the work described herein is as follows: We pre"
W11-3904,E06-1051,0,0.189719,"all protein pairs not contained in the knowledge base according to the closed world assumption. However, reliable information about non-interaction is substantially more difficult to obtain and therefore the database contains far less entries than IntAct. From our 8 million protein pairs only 6,005 pairs could be labeled as negative. Additional negative training instances required for the training phase are therefore inferred using the closed world assumption. Classification and experimental settings For classification, we use a support vector machine with the shallow linguistic (SL) kernel (Giuliano et al., 2006) which has been previously shown to generate state-of-the-art results for PPI extraction (Tikk et al., 2010). This method uses syntactic features, e.g. word, stem, part-of-speech tag and morphologic properties of the surrounding words to train a classifier, but no parse tree information. Setting Feature: Interaction word count Condition: Applied to: ≥1 positive =0 negative Pairs in sentence =1 positive =1 negative baseline pos-iword neg-iword pos/neg-iword pos-pair neg-pair pos/neg-pair • • • • • • • • Table 2: Our experiment settings. Based on the number of interaction words and protein menti"
W11-3904,D10-1099,0,0.0523794,"nated in ideas Protein function depends, to a large degree, on the functional context of its interaction partners, e.g. other proteins or metabolites. Accordingly, getting a better understanding of protein-protein interactions (PPIs) is vital to understand biological processes within organisms. Several databases, such as IntAct, DIP, or MINT, contain detailed 25 Robust Unsupervised and Semi-Supervised Methods in Natural Language Processing, pages 25–32, Hissar, Bulgaria, 15 September 2011. to learn literally thousands of classifiers from relational databases like Freebase (Mintz et al., 2009; Yao et al., 2010), Yago (Nguyen and Moschitti, 2011), or Wikipedia infoboxes (Hoffmann et al., 2010). Pairs Corpus AIMed BioInfer HPRD50 IEPA LLL So far, approaches in the biomedical domain on distant supervision focused on pattern learning (Hakenberg et al., 2008; Abacha and Zweigenbaum, 2010; Thomas et al., 2011). This is surprising as statistical machine learning methods are most commonly used for relation extraction. For example, only one of the five best performing systems in the BioNLP 2011 shared task relied on patterns (Kim et al., 2011). Class ratio positive negative 1,000 2,534 163 335 164 4,834 7,13"
W12-0705,H05-1091,0,0.0468256,"57.28 61.19 Table 2: Comparison of fully supervised relations extraction systems for DDI. (lex denotes the use of lexical features, lex+dep the additional use of dependency parsing-based features.) grams based, with n ∈ {1, 2, 3, 4}. They encompass the local (window size 3) and global (window size 13) context left and right of the entity pair, along with the area between the entities (Li et al., 2010). Additionally, dictionary based domain specific trigger words are taken into account. The respective dependency parse tree is included through following the shortest dependency path hypothesis (Bunescu and Mooney, 2005), by using the syntactical and dependency information of edges (e) and vertices (v). So-called v-walks and e-walks of length 3 are created as well as n grams along the shortest path (Miwa et al., 2010). 3.2 Methods In this section, the relation extraction system used for classification of interacting pairs is presented. Furthermore, the process of generating an automatically labeled corpus is explained in more detail, along with specific characteristics of the PPI and DDI task. 3.1 P Interaction Classification We formulate the task of relation extraction as feature-based classification of co-o"
W12-0705,C10-2087,0,0.0351164,"Missing"
W12-0705,de-marneffe-etal-2006-generating,0,0.0121021,"Missing"
W12-0705,P09-1113,0,0.188542,"Missing"
W12-0705,D09-1013,0,0.0672155,"ects with limited precision. Extracting high quality interaction pairs from free text allows for ∗ These authors contributed equally. building networks, e. g. of proteins, which need less manual curation to serve as a model for further knowledge processing steps. Nevertheless, just assuming co-occurrence to model an interaction or relation is common, as the development of interaction extraction systems can be time-consuming and complex. Currently, a lot of relation extraction (RE) systems rely on machine learning, namely classifying pairs of entities to be related or not (Airola et al., 2008; Miwa et al., 2009; Kim et al., 2010). Despite the fact that machine learning has been most successful in identifying relevant relations in text, a drawback is the need for manually annotated training data. Domain experts have to dedicate time and effort to this tedious and labor-intensive process. Specific biomedical domains have been explored more extensively than others, thus creating an imbalance in the number of existing corpora for a specific RE task. Protein-protein interactions (PPI) have been investigated the most, which gave rise to a number of available corpora. Pyysalo et al. (2008) standardized fiv"
W12-0705,W11-3904,1,0.874805,"with the conference of the spanish society for natural language processing (SEPLN) in 2011, http: //labda.inf.uc3m.es/DDIExtraction2011/ 35 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 35–43, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics If two entities participate in a relation, all sentences that mention these two entities express that relation. Obviously, this assumption does not hold in general, and therefore exceptions need to be detected which are not used for training a model. Thomas et al. (2011b) successfully used simple filtering techniques in a distantly supervised setting to extract PPI. In contrast to their work, we introduce a more generic filter to detect frequent exceptions from the distant supervision assumption and make use of more data sources, by merging the interaction information from IntAct and KUPS databases (discussed in Section 2.1). In addition, we present the first system (to our knowledge), evaluating distant supervision for drug-drug interaction with promising results. 1.1 Related work Distant supervision approaches have received considerable attention in the pa"
W12-0705,W09-1405,0,0.0607388,"Missing"
W12-0705,D10-1099,0,0.04975,"Missing"
W14-2608,P06-1038,0,0.0113195,"ected excerpts containing the phrase “said sarcastically”, removed that phrase and performed a regression analysis on the remaining text, exploiting the number of words as well as the occurrence of adjectives, adverbs, interjections, exclamation and question marks as features. Tsur et al. (2010) present a system to identify sarcasm in Amazon product reviews exploiting features such as sentence length, punctuation marks, the total number of completely capitalized words and automatically generated patterns which are based on the occurrence frequency of different terms (following the approach by Davidov and Rappoport (2006)). Unfortunately, their corpus is not publicly available. Carvalho et al. (2009) use eight patterns to identify ironic utterances in comments on articles from a Portuguese online newspaper. These patterns contain positive predicates and utilize punctuation, interjections, positive words, emoticons, or onomatopoeia and acronyms for laughing as well as some Portuguese-specific patterns considering the verb-morphology. Gonz´alezIb´an˜ ez et al. (2011) differentiate between sarcastic and positive or negative Twitter messages. They Background Irony is an important and frequent device in human commu"
W14-2608,W10-2914,0,0.395054,"zed as being ironic or non-ironic. We investigate different classifiers and focus on the impact analysis of different features by investigating what effect their elimination has on the performance of the approach. In the following, we describe the features used and the set of classifiers compared. 2.1 Features 2.2 To estimate if a review is ironic or not, we measure a set of features. Following the idea that irony is expressing the opposite of its literal content, we take into account the imbalance between the overall (prior) polarity of words in the review and the star-rating (as proposed by Davidov et al. (2010)). We assume the imbalance to hold if the star-rating Classifiers In order to perform the classification based on the features mentioned above, we explore a set of standard classifiers typically used in text classification research. We employ the open source machine learning library scikit-learn (Pedregosa et al., 2011) for Python. 7 Note that examples can show that this is not always the case. Funny or odd products ironically receive a positive starrating. However, this feature may be a strong indicator for irony. 6 The system as implemented to perform the described experiments is made availa"
W14-2608,filatova-2012-irony,0,0.106996,"fiers of sentence complexity, structural, morphosyntactic and semantic ambiguity, polarity, unexpectedness, and emotional activation, imagery, and pleasantness of words. Tepperman et al. (2006) performed experiments to recognize sarcasm in spoken language, specifically in the expression “yeah right”, using spectral, contextual and prosodic cues. On the one hand, their results show that it is possible to identify sarcasm based on spectral and contextual features and, on the other hand, they confirm that prosody is insufficient to reliably detect sarcasm (Rockwell, 2005, p. 118). Very recently, Filatova (2012) published a product review corpus from Amazon, being annotated with Amazon Mechanical Turk. It contains 437 ironic and 817 non-ironic reviews. A more detailed description of this resource can be found in Section 3.1. To our knowledge, no automatic classification approach has been evaluated on this corpus. We therefore contribute a text classification system including the previously mentioned features. Our results serve as a strong baseline on this corpus as well as an “executable review” of previous work.6 2 is positive (i. e., 4 or 5 stars) but the majority of words is negative, and, vice ve"
W14-2608,P11-2102,0,0.388971,"ed words and automatically generated patterns which are based on the occurrence frequency of different terms (following the approach by Davidov and Rappoport (2006)). Unfortunately, their corpus is not publicly available. Carvalho et al. (2009) use eight patterns to identify ironic utterances in comments on articles from a Portuguese online newspaper. These patterns contain positive predicates and utilize punctuation, interjections, positive words, emoticons, or onomatopoeia and acronyms for laughing as well as some Portuguese-specific patterns considering the verb-morphology. Gonz´alezIb´an˜ ez et al. (2011) differentiate between sarcastic and positive or negative Twitter messages. They Background Irony is an important and frequent device in human communication that is used to convey an attitude or evaluation towards the propositional content of a message, typically in a humorous fashion (Abrams, 1957, p. 165–168). Between the age of six (Nakassis and Snedeker, 2002) and eight years (Creusere, 2007), children are able to recognize ironic utterances or at least notice that something in the situation is not common (Glenwright and Pexman, 2007). The principle of inferability (Kreuz, 1996) states tha"
W14-2608,W07-0101,0,0.759812,"erent theories such as the echoic account (Wilson and Sperber, 1992), the Pretense Theory (Clark and Gerrig, 1984) or the Allusional Pretense Theory (KumonNakamura et al., 1995) have challenged the understanding that an ironic utterance typically conveys the opposite of its literal propositional content. However, in spite of the fact that the attributive nature of irony is widely accepted (see Wilson and Sperber (2012)), no formal or operational definition of irony is available as of today. 1.2 Previous Work Corpora providing annotations as to whether expressions are ironic or not are scarce. Kreuz and Caucci (2007) have automatically generated such a corpus exploiting Google Book search5 . They collected excerpts containing the phrase “said sarcastically”, removed that phrase and performed a regression analysis on the remaining text, exploiting the number of words as well as the occurrence of adjectives, adverbs, interjections, exclamation and question marks as features. Tsur et al. (2010) present a system to identify sarcasm in Amazon product reviews exploiting features such as sentence length, punctuation marks, the total number of completely capitalized words and automatically generated patterns whic"
W14-2608,C96-2162,0,0.153989,"Missing"
W14-2608,W11-1715,0,\N,Missing
W14-2608,barbieri-saggion-2014-modelling-irony,0,\N,Missing
W14-2608,maynard-greenwood-2014-cares,0,\N,Missing
W14-2608,P13-2147,1,\N,Missing
W14-3418,H92-1045,0,0.0379033,"as an entity by the CRF. In addition, the cardinality of these entities in A is taken into account by cumulative binning. The feature long form holds if one of the long forms previously defined to correspond with the abbreviation occurs in the text (in arbitrary position). Besides using all features, we perform a greedy search for the best feature set by wrapping the best model configuration. A detailed discussion of the feature selection process follows in Section 5.3. 4.2.3 Feature Propagation Inspired by the “one sense per discourse” heuristic commonly adopted in word sense disambiguation (Gale et al., 1992), we apply two feature combination strategies. In the following, n denotes the number of occurrences of the abbreviation in an abstract. In the setting propagationall , n − 1 identical linked instances are added for each occurrence. Each new instance consists of the disjunction of the feature vectors of all occurrences. Based on the intuition that the first mention of an abbreviation might carry particularly valuable information, propagationfirst introduces one additional linked instance for each occurrence, in which the feature vector is joined with the first occurrence. 8 Using the stopword"
W14-3418,W03-1306,0,0.118085,"Missing"
W14-6204,doddington-etal-2004-automatic,0,0.0240142,"rmance and for specific entity classes such as organisms (Pafilis et al., 2013) or symptoms (Savova et al., 2010; Jimeno et al., 2008). A detailed overview on named entity recognition, covering other domains as well, can be found in Nadeau and Sekine (2007). The use case described in this paper, however, involves a highly relational problem structure in the sense that individual facts or relations have to be aggregated in order to yield accurate, holistic domain knowledge, which corresponds most closely to the problem structure encountered in event extraction, as triggered by the ACE program (Doddington et al., 2004; Ji and Grishman, 2008; Strassel et al., 2008), and the BioNLP shared task series (Nedellec et al., 2013; Tsujii et al., 2011; Tsujii, 2009). General semantic search engines in the biomedical domain mainly focus on isolated entities. Relations are typically only taken into account by co-occurrence on abstract or sentence level. Examples for such search engines include GoPubMed (Doms and Schroeder, 2005), SCAIView (Hofmann-Apitius et al., 2008), and GeneView (Thomas et al., 2012). With respect to the extraction methodology, our work is similar to Saggion et al. (2007) and Buitelaar et al. (200"
W14-6204,P08-1030,0,0.0139659,"ntity classes such as organisms (Pafilis et al., 2013) or symptoms (Savova et al., 2010; Jimeno et al., 2008). A detailed overview on named entity recognition, covering other domains as well, can be found in Nadeau and Sekine (2007). The use case described in this paper, however, involves a highly relational problem structure in the sense that individual facts or relations have to be aggregated in order to yield accurate, holistic domain knowledge, which corresponds most closely to the problem structure encountered in event extraction, as triggered by the ACE program (Doddington et al., 2004; Ji and Grishman, 2008; Strassel et al., 2008), and the BioNLP shared task series (Nedellec et al., 2013; Tsujii et al., 2011; Tsujii, 2009). General semantic search engines in the biomedical domain mainly focus on isolated entities. Relations are typically only taken into account by co-occurrence on abstract or sentence level. Examples for such search engines include GoPubMed (Doms and Schroeder, 2005), SCAIView (Hofmann-Apitius et al., 2008), and GeneView (Thomas et al., 2012). With respect to the extraction methodology, our work is similar to Saggion et al. (2007) and Buitelaar et al. (2008), in that a combinati"
W14-6204,N06-4006,0,0.0388148,"Missing"
W14-6204,strassel-etal-2008-linguistic,0,0.0190519,"rganisms (Pafilis et al., 2013) or symptoms (Savova et al., 2010; Jimeno et al., 2008). A detailed overview on named entity recognition, covering other domains as well, can be found in Nadeau and Sekine (2007). The use case described in this paper, however, involves a highly relational problem structure in the sense that individual facts or relations have to be aggregated in order to yield accurate, holistic domain knowledge, which corresponds most closely to the problem structure encountered in event extraction, as triggered by the ACE program (Doddington et al., 2004; Ji and Grishman, 2008; Strassel et al., 2008), and the BioNLP shared task series (Nedellec et al., 2013; Tsujii et al., 2011; Tsujii, 2009). General semantic search engines in the biomedical domain mainly focus on isolated entities. Relations are typically only taken into account by co-occurrence on abstract or sentence level. Examples for such search engines include GoPubMed (Doms and Schroeder, 2005), SCAIView (Hofmann-Apitius et al., 2008), and GeneView (Thomas et al., 2012). With respect to the extraction methodology, our work is similar to Saggion et al. (2007) and Buitelaar et al. (2008), in that a combination of gazetteers and ext"
W14-6204,W09-1400,0,\N,Missing
W15-2908,P10-2049,0,0.0159052,"oached as a text classification task in which snippets (like sentences, paragraphs, or phrases) are categorized into being objective or subjective and in the latter case positive, negative, or neutral (Liu, 2015; T¨ackstr¨om and McDonald, 2011; Sayeed et al., 2012; Pang and Lee, 2004). More differentiated results can be obtained by methods that additionally identify the target of the opinion, specific mentions of product characteristics usually called aspects (Choi et al., 2010; Johansson and Moschitti, 2011; Yang and Cardie, 2012; Hu and Liu, 2004; Li et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010; Klinger and Cimiano, 2013). It has been proposed to use the extracted information for summarizing specific information about a product (Hu and Liu, 2004). The main advantage of such result is that a star rating is not only associated to the whole product but separated for specific aspects. This is helpful when a user aims at getting an overview of the content of reviews but it might still be leading to an overwhelming amount of information. In this work, we propose to aim at generating a ranked list of products and hypothesize that such a ranking would be more helpful for the typical task of"
W15-2908,P11-2018,0,0.0204027,"views for products, for instance from Amazon.com, are a typical resource for opinions. Often, opinion mining is approached as a text classification task in which snippets (like sentences, paragraphs, or phrases) are categorized into being objective or subjective and in the latter case positive, negative, or neutral (Liu, 2015; T¨ackstr¨om and McDonald, 2011; Sayeed et al., 2012; Pang and Lee, 2004). More differentiated results can be obtained by methods that additionally identify the target of the opinion, specific mentions of product characteristics usually called aspects (Choi et al., 2010; Johansson and Moschitti, 2011; Yang and Cardie, 2012; Hu and Liu, 2004; Li et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010; Klinger and Cimiano, 2013). It has been proposed to use the extracted information for summarizing specific information about a product (Hu and Liu, 2004). The main advantage of such result is that a star rating is not only associated to the whole product but separated for specific aspects. This is helpful when a user aims at getting an overview of the content of reviews but it might still be leading to an overwhelming amount of information. In this work, we propose to aim at generat"
W15-2908,D13-1194,1,0.891587,"Missing"
W15-2908,kessler-kuhn-2014-corpus,1,0.852135,"lting list contains 920 products with a total of 71,409 reviews. Product names are extracted from the title of the page and shortened to the first six tokens to remove additional descriptions. As a second external gold ranking, we use the quality ranking provided by Snapsort. From the top 150 products in the Amazon sales ranking, 56 are found on Snapsort. We use the rank in the category “best overall” of “all digital cameras announced in the last 48 month” as retrieved on June 12th, 2015.3 J FSA is trained on the camera data set by Kessler et al. (2010). C SRL is trained on the camera data by Kessler and Kuhn (2014). For the methods D ICT and D ICT-N ORM, we try two different sources of opinion words, the general 2 The lists for aspect mention normalization are available as supplementary material. For instance, video contains “video”, “videos”, “film”, “films”, “movie”, “movies”, “record”, “records”, “recording”. 3 The full list of products with their names and the rankings are available in the supplementary material. 54 Aspect performance video size pictures battery price zoom shutter features autofocus screen lens flash # ρ σ 637 600 513 790 541 625 514 410 629 403 501 457 591 0.301 0.278 0.218 0.213 0"
W15-2908,P13-2147,1,0.745033,"ation task in which snippets (like sentences, paragraphs, or phrases) are categorized into being objective or subjective and in the latter case positive, negative, or neutral (Liu, 2015; T¨ackstr¨om and McDonald, 2011; Sayeed et al., 2012; Pang and Lee, 2004). More differentiated results can be obtained by methods that additionally identify the target of the opinion, specific mentions of product characteristics usually called aspects (Choi et al., 2010; Johansson and Moschitti, 2011; Yang and Cardie, 2012; Hu and Liu, 2004; Li et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010; Klinger and Cimiano, 2013). It has been proposed to use the extracted information for summarizing specific information about a product (Hu and Liu, 2004). The main advantage of such result is that a star rating is not only associated to the whole product but separated for specific aspects. This is helpful when a user aims at getting an overview of the content of reviews but it might still be leading to an overwhelming amount of information. In this work, we propose to aim at generating a ranked list of products and hypothesize that such a ranking would be more helpful for the typical task of a user to select a product"
W15-2908,P04-1035,0,0.00548492,"iltrud Kessler, Roman Klinger, and Jonas Kuhn Institute for Natural Language Processing University of Stuttgart 70569 Stuttgart, Germany {wiltrud.kessler,roman.klinger,jonas.kuhn}@ims.uni-stuttgart.de Abstract text. Reviews for products, for instance from Amazon.com, are a typical resource for opinions. Often, opinion mining is approached as a text classification task in which snippets (like sentences, paragraphs, or phrases) are categorized into being objective or subjective and in the latter case positive, negative, or neutral (Liu, 2015; T¨ackstr¨om and McDonald, 2011; Sayeed et al., 2012; Pang and Lee, 2004). More differentiated results can be obtained by methods that additionally identify the target of the opinion, specific mentions of product characteristics usually called aspects (Choi et al., 2010; Johansson and Moschitti, 2011; Yang and Cardie, 2012; Hu and Liu, 2004; Li et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010; Klinger and Cimiano, 2013). It has been proposed to use the extracted information for summarizing specific information about a product (Hu and Liu, 2004). The main advantage of such result is that a star rating is not only associated to the whole product but"
W15-2908,H05-1043,0,0.12577,"ten, opinion mining is approached as a text classification task in which snippets (like sentences, paragraphs, or phrases) are categorized into being objective or subjective and in the latter case positive, negative, or neutral (Liu, 2015; T¨ackstr¨om and McDonald, 2011; Sayeed et al., 2012; Pang and Lee, 2004). More differentiated results can be obtained by methods that additionally identify the target of the opinion, specific mentions of product characteristics usually called aspects (Choi et al., 2010; Johansson and Moschitti, 2011; Yang and Cardie, 2012; Hu and Liu, 2004; Li et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010; Klinger and Cimiano, 2013). It has been proposed to use the extracted information for summarizing specific information about a product (Hu and Liu, 2004). The main advantage of such result is that a star rating is not only associated to the whole product but separated for specific aspects. This is helpful when a user aims at getting an overview of the content of reviews but it might still be leading to an overwhelming amount of information. In this work, we propose to aim at generating a ranked list of products and hypothesize that such a ranking would be more helpf"
W15-2908,N12-1085,0,0.0134701,"of Product Rankings Wiltrud Kessler, Roman Klinger, and Jonas Kuhn Institute for Natural Language Processing University of Stuttgart 70569 Stuttgart, Germany {wiltrud.kessler,roman.klinger,jonas.kuhn}@ims.uni-stuttgart.de Abstract text. Reviews for products, for instance from Amazon.com, are a typical resource for opinions. Often, opinion mining is approached as a text classification task in which snippets (like sentences, paragraphs, or phrases) are categorized into being objective or subjective and in the latter case positive, negative, or neutral (Liu, 2015; T¨ackstr¨om and McDonald, 2011; Sayeed et al., 2012; Pang and Lee, 2004). More differentiated results can be obtained by methods that additionally identify the target of the opinion, specific mentions of product characteristics usually called aspects (Choi et al., 2010; Johansson and Moschitti, 2011; Yang and Cardie, 2012; Hu and Liu, 2004; Li et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010; Klinger and Cimiano, 2013). It has been proposed to use the extracted information for summarizing specific information about a product (Hu and Liu, 2004). The main advantage of such result is that a star rating is not only associated to t"
W15-2908,P11-2100,0,0.0579987,"Missing"
W15-2908,H05-1044,0,0.0123269,"SRL leads to the best result of all experiments with ρ = 0.51. In comparison to using all information extracted from reviews to generate a ranking, the aspectspecific results allow for an understanding of the Table 2: Results (Spearman’s ρ and standard deviation σ) of J FSA for predicting the Amazon sales ranking when only the subjective phrases are taken into account which refer to the specified target aspect. The number of products for which at least one evaluation of the target aspect is found is shown in column #. inquirer dictionary (Stone et al., 1996)4 and the MPQA subjectivity clues (Wilson et al., 2005)5 . To measure the correlation of the rankings generated by our different methods with the gold ranking, we calculate Spearman’s rank correlation coefficient ρ (Spearman, 1904). We test for significance with the Steiger test (Steiger, 1980). 4.2 Results As described in Section 2, we take into account two different rankings for evaluation: The Amazon.com sales ranking contains 920 products and is an example for a ranking that may be useful for sales managers or product designers. The second is the expert ranking by Snapsort.com which contains 56 products. These two rankings are conceptually dif"
W15-2908,D12-1122,0,0.0144485,"e from Amazon.com, are a typical resource for opinions. Often, opinion mining is approached as a text classification task in which snippets (like sentences, paragraphs, or phrases) are categorized into being objective or subjective and in the latter case positive, negative, or neutral (Liu, 2015; T¨ackstr¨om and McDonald, 2011; Sayeed et al., 2012; Pang and Lee, 2004). More differentiated results can be obtained by methods that additionally identify the target of the opinion, specific mentions of product characteristics usually called aspects (Choi et al., 2010; Johansson and Moschitti, 2011; Yang and Cardie, 2012; Hu and Liu, 2004; Li et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010; Klinger and Cimiano, 2013). It has been proposed to use the extracted information for summarizing specific information about a product (Hu and Liu, 2004). The main advantage of such result is that a star rating is not only associated to the whole product but separated for specific aspects. This is helpful when a user aims at getting an overview of the content of reviews but it might still be leading to an overwhelming amount of information. In this work, we propose to aim at generating a ranked list of pr"
W15-2908,H05-2017,0,\N,Missing
W15-2908,D08-1083,0,\N,Missing
W17-5202,N10-1000,0,0.136252,"Missing"
W17-5202,P15-1162,0,0.025946,"lead to good results on SemEval and to state-of-the-art results on SenTubeA and SenTube-T. Similarly to R ETROFIT, C NN does not outperform any of the other methods on any dataset. As said, this method does not beat the baseline on SST-fine, SenTube-A, and SenTube-T. However, it outperforms the AVE baseline on SST-binary and OpeNER. The best models are L STM and B I L STM. The best overall model is B I LSTM, which outperforms the other models on half of the tasks (SST-fine, 5 Discussion While approaches that average the word embeddings for a sentence are comparable to state-of-theart results (Iyyer et al., 2015), AVE and R ETROFIT do not perform particularly well. This is likely due to the fact that logistic regression lacks the nonlinearities which Iyyer et al. (2015) found helped, especially at deeper layers. Averaging all of the embeddings for longer phrases also seems to lead to representations that do not contain enough information for the classifier. We also experimented with using large sentiment lexicons as the semantic lexicon for retrofitting, but found that this hurt the representation more than it helped. We believe this is because there are not enough kinds of relationships to exploit th"
W17-5202,P16-1191,0,0.0230242,"e Stanford Sentiment Treebank (Socher et al., 2013). Since this dataset is annotated for sentiment at each node of a parse tree, they train and test on these annotated phrases. Both Socher et al. (2013) and Tai et al. (2015) also propose various RNNs which are able to take better advantage of the labeled nodes and which achieve better results than standard RNNs. However, these models require annotated parse trees, which are not necessarily available for other datasets. C ONVOLUTIONAL N EURAL N ETWORKS (C NN) have proven effective for text classification (dos Santos and Gatti, 2014; Kim, 2014; Flekova and Gurevych, 2016). Kim (2014) use skipgram vectors (Mikolov et al., 2013) as input to a variety of Convolutional Neural Networks and test on seven datasets, including the Stanford Sentiment Treebank (Socher et al., 2013). The best performing setup across datasets is a single layer CNN which updates the original skipgram vectors during training. Overall, these approaches currently achieve stateof-the-art results on many datasets, but they have not been compared to retrofitting or joint training approaches. (1) and backpropagate the error to the corresponding word embeddings. Here, t is the original n-gram, tr i"
W17-5202,D15-1242,0,0.0326042,"Missing"
W17-5202,Q16-1023,0,0.0895574,"alize the layer. These vectors then pass to an LSTM layer. We feed the final hidden state to a standard fully-connected 50-dimensional dense layer and then to a softmax layer, which gives us a probability distribution over our classes. As a regularizer, we use a dropout (Srivastava et al., 2014) of 0.5 before the LSTM layer. The B IDIRECTIONAL LSTM (B I L STM) has the same architecture as the normal LSTM, but includes an additional layer which runs from the end of the text to the front. This approach has led to state-of-the-art results for POS-tagging (Plank et al., 2016), dependency parsing (Kiperwasser and Goldberg, 2016) and text classification (Zhou et al., 2016), among others. We use the same parameters as the LSTM, but concatenate the two hidden layers before passing them to the dense layer6 . We also train a simple one-layer C NN with one convolutional layer on top of pre-trained word embeddings. The first layer is an embeddings layer that maps the input of length n (padded when needed) to an n × R dimensional matrix, where R is the dimensionality of the word embeddings. The embedding matrix is then convoluted with filter sizes of 2, 3, and 4, followed by a pooling layer of length 2. This is then fed to a"
W17-5202,S13-2052,0,0.310686,"ntiment of a sentence. δs (t) is 1 if the true sentiment is positive and −1 if it is negative. They then use a weighted sum of both scores to create their sentiment embeddings: losscombined (t, tr ) = (3) α · losscw (t, tr ) + (1 − α) · losss (t, tr ) . This requires sentiment-annotated data for training both the syntactic and sentiment losses, which they acquire by collecting tweets associated with certain emoticons. In this way, they are able to simultaneously incorporate sentiment and semantic information relevant to their task. They test their approach on the SemEval 2013 twitter dataset (Nakov et al., 2013), changing the task from three-class to binary classification, and find that they outperform other approaches. Overall, the joint approach shows promise for tasks with a large amount of distantly-labeled data. 2.2 Datasets We choose to evaluate the approaches presented in Section 2.1 on a number of different datasets from different domains, which also have differing levels of granularity of class labels. The Stanford Sentiment Treebank and SemEval 2013 shared-task dataset have already been used as benchmarks for some of the approaches mentioned in Section 2.1. Table 1 shows which approaches ha"
W17-5202,P15-2128,0,0.0146677,"(1.1) 64.2 (1.0) 64.0 (0.9) 64.4 (1.5) B OW Table 3: Accuracy on the test sets. For all neural models we perform 5 runs and show the mean and standard deviation. The best results for each dataset is given in bold and results that have been previously reported are highlighted. All results derive from our reimplementation of the methods. We describe significance values in the text and appendix. Footnotes refer to the work where a method was previously tested on a specific dataset, although not necessarily with the same results: [1] Tai et al. (2015) [2] Kim (2014) [3] Faruqui et al. (2015) [4] Lambert (2015) [5] Uryupina et al. (2014) [6] Tang et al. (2014). jargon. We performed a short analysis of datasets (shown in Table 4), where we take frequency of emoticons usage as an indirect indicator of informal speech and found that, indeed, the frequency of emoticons in the SemEval and SenTube datasets diverges significantly from the other datasets. The fact that J OINT is distantly trained on similar data gives it an advantage over other models on these datasets. This leads us to believe that this approach would transfer well to novel sentiment analysis tasks with similar properties. The fact that C"
W17-5202,P11-1015,0,0.0552599,"word while training a skip-gram model (Mikolov et al., 2013). They sample extra context words, taken either from a thesaurus or association data, and incorporate this into the context of the word for each update. The evaluation is both intrinsical, on word similarity and relatedness tasks, as well as extrinsical on TOEFL synonym and document classification tasks. The augmentation strategy improves the word vectors on all tasks. Faruqui et al. (2015) propose a method to refine word vectors by using relational information from semantic lexicons (we will refer to this method 2.1.2 Joint Training Maas et al. (2011) were the first to jointly train semantic and sentiment word vectors. In order to capture semantic similarities, they propose a probabilistic model using a continuous mixture model over words, similar to Latent Dirichlet Allocation (LDA, Blei et al., 2003). To capture sentiment information, they include a sentiment term which uses logistic regression to predict the sentiment of a document. The full objective function is a combination of the semantic and sentiment objectives. They test their model on several sentiment and subjectivity benchmarks. Their results indicate that including the sentim"
W17-5202,W02-1011,0,0.0266137,"Missing"
W17-5202,P16-2067,0,0.126759,"depending on the embeddings used to initialize the layer. These vectors then pass to an LSTM layer. We feed the final hidden state to a standard fully-connected 50-dimensional dense layer and then to a softmax layer, which gives us a probability distribution over our classes. As a regularizer, we use a dropout (Srivastava et al., 2014) of 0.5 before the LSTM layer. The B IDIRECTIONAL LSTM (B I L STM) has the same architecture as the normal LSTM, but includes an additional layer which runs from the end of the text to the front. This approach has led to state-of-the-art results for POS-tagging (Plank et al., 2016), dependency parsing (Kiperwasser and Goldberg, 2016) and text classification (Zhou et al., 2016), among others. We use the same parameters as the LSTM, but concatenate the two hidden layers before passing them to the dense layer6 . We also train a simple one-layer C NN with one convolutional layer on top of pre-trained word embeddings. The first layer is an embeddings layer that maps the input of length n (padded when needed) to an n × R dimensional matrix, where R is the dimensionality of the word embeddings. The embedding matrix is then convoluted with filter sizes of 2, 3, and 4, followed"
W17-5202,C00-2137,0,0.222252,"ly significant, except for the difference between B I L STM and C NN at 50 dimensions on the OpeNER dataset. Our analysis of different dimensionalities as input for the classification models reveals that, typically, the higher dimensional vectors (300 or 600) outperform lower dimensions. The only differences are in J OINT for SenTube-T and SemEval and L STM for SenTube-A and AVE on all datasets except OpeNER. Results Table 3 shows the results for the seven models across all datasets, as well as the macro-averaged results. We visualize them in Figure 3. We performed random approximation tests (Yeh, 2000) using the sigf package (Pad´o, 2006) with 10,000 iterations to determine the statistical significance of differences between models. Since the reported accuracies for the neural models are the means over five runs, we cannot use this technique in a straightforward manner. Therefore, we perform the random approximation tests between the runs7 and consider the models statistically different if a majority (at least 3) of the runs are statistically different (p < 0.01, which corresponds to p < 0.05 with Bonferroni correction for 5 hypotheses). The results of statistical testing are summarized in"
W17-5202,W03-1014,0,0.151015,"Missing"
W17-5202,P14-2089,0,0.029036,"paraphrases as the semantic lexicon, to improve the original vectors. This dataset includes 8 million lexical paraphrases collected from bilingual corpora, where words in language A are considered paraphrases if they are consistently translated to the same word in language B. They then test on the Stanford Sentiment Treebank (Socher et al., 2013). They train an L2regularized logistic regression classifier on the average of the word embeddings for a text and find improvements after retrofitting. All above approaches show improvements over previous word embedding approaches (Mnih and Teh, 2012; Yu and Dredze, 2014; Xu et al., 2014) on this data set. Retrofitting to Semantic Lexicons There have been several proposals to improve the quality of word embeddings using semantic lexicons. Yu and Dredze (2014) propose several methods which combine the CBOW architecture (Mikolov et al., 2013) and a second objective function which attempts to maximize the relations found within some semantic lexicon. They use both the Paraphrase Database (Ganitkevitch et al., 2013) and WordNet (Fellbaum, 1999) and test their models on language modeling and semantic similarity tasks. They report that their method leads to an impr"
W17-5202,C14-1008,0,0.034926,"ent neural networks and train on the Stanford Sentiment Treebank (Socher et al., 2013). Since this dataset is annotated for sentiment at each node of a parse tree, they train and test on these annotated phrases. Both Socher et al. (2013) and Tai et al. (2015) also propose various RNNs which are able to take better advantage of the labeled nodes and which achieve better results than standard RNNs. However, these models require annotated parse trees, which are not necessarily available for other datasets. C ONVOLUTIONAL N EURAL N ETWORKS (C NN) have proven effective for text classification (dos Santos and Gatti, 2014; Kim, 2014; Flekova and Gurevych, 2016). Kim (2014) use skipgram vectors (Mikolov et al., 2013) as input to a variety of Convolutional Neural Networks and test on seven datasets, including the Stanford Sentiment Treebank (Socher et al., 2013). The best performing setup across datasets is a single layer CNN which updates the original skipgram vectors during training. Overall, these approaches currently achieve stateof-the-art results on many datasets, but they have not been compared to retrofitting or joint training approaches. (1) and backpropagate the error to the corresponding word embeddin"
W17-5202,C16-1329,0,0.104973,". We feed the final hidden state to a standard fully-connected 50-dimensional dense layer and then to a softmax layer, which gives us a probability distribution over our classes. As a regularizer, we use a dropout (Srivastava et al., 2014) of 0.5 before the LSTM layer. The B IDIRECTIONAL LSTM (B I L STM) has the same architecture as the normal LSTM, but includes an additional layer which runs from the end of the text to the front. This approach has led to state-of-the-art results for POS-tagging (Plank et al., 2016), dependency parsing (Kiperwasser and Goldberg, 2016) and text classification (Zhou et al., 2016), among others. We use the same parameters as the LSTM, but concatenate the two hidden layers before passing them to the dense layer6 . We also train a simple one-layer C NN with one convolutional layer on top of pre-trained word embeddings. The first layer is an embeddings layer that maps the input of length n (padded when needed) to an n × R dimensional matrix, where R is the dimensionality of the word embeddings. The embedding matrix is then convoluted with filter sizes of 2, 3, and 4, followed by a pooling layer of length 2. This is then fed to a fully connected dense layer with ReLU activ"
W17-5202,D13-1170,0,0.020305,"tion into their word embeddings (we refer to this method as J OINT). They extend the word embedding approach of Collobert et al. (2011), who use a neural network to predict whether an n-gram is a true n-gram or a “corrupted” version. They use the hinge-loss losscw (t, tr ) = max(0, 1 − f cw (t) + f cw (tr )) R ECURRENT U NITS (GRUs) (Chung et al., 2014), are a variant of a feed-forward network which includes a memory state capable of learning long distance dependencies. In various forms, they have proven useful for text classification tasks (Tai et al., 2015; Tang et al., 2016). Socher et al. (2013) and Tai et al. (2015) use Glove vectors (Pennington et al., 2014) in combination with a recurrent neural networks and train on the Stanford Sentiment Treebank (Socher et al., 2013). Since this dataset is annotated for sentiment at each node of a parse tree, they train and test on these annotated phrases. Both Socher et al. (2013) and Tai et al. (2015) also propose various RNNs which are able to take better advantage of the labeled nodes and which achieve better results than standard RNNs. However, these models require annotated parse trees, which are not necessarily available for other datase"
W17-5202,W11-2207,0,0.0115737,"ng sentiment information into word embeddings during training gives good results for datasets that are lexically similar to the training data. With our experiments, we contribute to a better understanding of the performance of different model architectures on different data sets. Consequently, we detect novel state-of-the-art results on the SenTube datasets. 1 Introduction The task of analyzing private states expressed by an author in text, such as sentiment, emotion or affect, can give us access to a wealth of hidden information to analyze product reviews (Liu et al., 2005), political views (Speriosu et al., 2011), or to identify potentially dangerous activity on the 1 The code and embeddings for the best models are available at http://www.ims.uni-stuttgart.de/ data/sota_sentiment 2 Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 2–12 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics and sentiment perform well on datasets that are similar to the training data. 2 in this paper as R ETROFIT). They require a vocabulary V = {w1 , . . . , wn }, its word embedˆ = {ˆ dings matrix Q q1 , . . . , qˆ"
W17-5202,P15-1150,0,0.754357,"erminology here, but make no asand sentiment information into their word embeddings (we refer to this method as J OINT). They extend the word embedding approach of Collobert et al. (2011), who use a neural network to predict whether an n-gram is a true n-gram or a “corrupted” version. They use the hinge-loss losscw (t, tr ) = max(0, 1 − f cw (t) + f cw (tr )) R ECURRENT U NITS (GRUs) (Chung et al., 2014), are a variant of a feed-forward network which includes a memory state capable of learning long distance dependencies. In various forms, they have proven useful for text classification tasks (Tai et al., 2015; Tang et al., 2016). Socher et al. (2013) and Tai et al. (2015) use Glove vectors (Pennington et al., 2014) in combination with a recurrent neural networks and train on the Stanford Sentiment Treebank (Socher et al., 2013). Since this dataset is annotated for sentiment at each node of a parse tree, they train and test on these annotated phrases. Both Socher et al. (2013) and Tai et al. (2015) also propose various RNNs which are able to take better advantage of the labeled nodes and which achieve better results than standard RNNs. However, these models require annotated parse trees, which are"
W17-5202,C16-1311,0,0.0331778,"ut make no asand sentiment information into their word embeddings (we refer to this method as J OINT). They extend the word embedding approach of Collobert et al. (2011), who use a neural network to predict whether an n-gram is a true n-gram or a “corrupted” version. They use the hinge-loss losscw (t, tr ) = max(0, 1 − f cw (t) + f cw (tr )) R ECURRENT U NITS (GRUs) (Chung et al., 2014), are a variant of a feed-forward network which includes a memory state capable of learning long distance dependencies. In various forms, they have proven useful for text classification tasks (Tai et al., 2015; Tang et al., 2016). Socher et al. (2013) and Tai et al. (2015) use Glove vectors (Pennington et al., 2014) in combination with a recurrent neural networks and train on the Stanford Sentiment Treebank (Socher et al., 2013). Since this dataset is annotated for sentiment at each node of a parse tree, they train and test on these annotated phrases. Both Socher et al. (2013) and Tai et al. (2015) also propose various RNNs which are able to take better advantage of the labeled nodes and which achieve better results than standard RNNs. However, these models require annotated parse trees, which are not necessarily avai"
W17-5202,P14-1146,0,0.601608,"order to capture semantic similarities, they propose a probabilistic model using a continuous mixture model over words, similar to Latent Dirichlet Allocation (LDA, Blei et al., 2003). To capture sentiment information, they include a sentiment term which uses logistic regression to predict the sentiment of a document. The full objective function is a combination of the semantic and sentiment objectives. They test their model on several sentiment and subjectivity benchmarks. Their results indicate that including the sentiment information during training actually leads to decreased performance. Tang et al. (2014) take the joint training approach and simultaneously incorporate syntactic2 2 3 We use the authors’ terminology here, but make no asand sentiment information into their word embeddings (we refer to this method as J OINT). They extend the word embedding approach of Collobert et al. (2011), who use a neural network to predict whether an n-gram is a true n-gram or a “corrupted” version. They use the hinge-loss losscw (t, tr ) = max(0, 1 − f cw (t) + f cw (tr )) R ECURRENT U NITS (GRUs) (Chung et al., 2014), are a variant of a feed-forward network which includes a memory state capable of learning"
W17-5202,uryupina-etal-2014-sentube,0,0.39914,"a dataset. The number of labels corresponds to the annotation scheme, where: two is positive and negative; three is positive, neutral, negative; four is strong positive, positive, negative, strong negative; five is strong positive, positive, neutral, negative, strong negative. − − + + + − − + − − − − − + − − − − − − − − − + + + − − − − + + − − − − + + − − − − the sentiment phrase is obligatory. Additionally, sentiment phrases are annotated for four levels of sentiment: strong negative, negative, positive and strong positive. We use a split of 2780/186/734 examples. 2.2.3 The SenTube datasets (Uryupina et al., 2014) are texts that are taken from YouTube comments regarding automobiles and tablets. These comments are normally directed towards a commercial or a video that contains information about the product. We take only those comments that have some polarity towards the target product in the video. For the automobile dataset (SenTube-A), this gives a 3381/225/903 training, validation, and test split. For the tablets dataset (SenTube-T) the splits are 4997/333/1334. These are annotated for positive, negative, and neutral sentiment. Table 1: Mapping of previous state-of-the-art methods to previous evaluat"
W17-5203,S16-1063,0,0.0150074,"ractically feasible approximation of emotions, there is no publicly available, manually vetted data set for Twitter emotions that would support accurate and comparable evaluations. In addition, it has been shown that distant annotation is conceptually different from manual annotation for sentiment and emotion (Purver and Battersby, 2012). With this paper, we contribute manual emotion annotation for a publicly available Twitter data set. We annotate the SemEval 2016 Stance Data set (Mohammad et al., 2016) which provides sentiment and stance information and is popular in the research community (Augenstein et al., 2016; Wei et al., 2016; Dias and Becker, 2016; Ebrahimi et al., 2016). It therefore enables further research on the relations between sentiment, emotions, and stances. For instance, if the distribution of subclasses of positive or negative emotions is different for against and in-favor, emotion-based features could contribute to stance detection. An additional feature of our resource is that we do not only provide a “majority annotation” as is usual. We do define a well-performing aggregated annotation, but additionally provide the individual labels of each of our six annotators. This enables furt"
W17-5203,P82-1020,0,0.698097,"Missing"
W17-5203,S16-1061,0,0.0231627,"ns, there is no publicly available, manually vetted data set for Twitter emotions that would support accurate and comparable evaluations. In addition, it has been shown that distant annotation is conceptually different from manual annotation for sentiment and emotion (Purver and Battersby, 2012). With this paper, we contribute manual emotion annotation for a publicly available Twitter data set. We annotate the SemEval 2016 Stance Data set (Mohammad et al., 2016) which provides sentiment and stance information and is popular in the research community (Augenstein et al., 2016; Wei et al., 2016; Dias and Becker, 2016; Ebrahimi et al., 2016). It therefore enables further research on the relations between sentiment, emotions, and stances. For instance, if the distribution of subclasses of positive or negative emotions is different for against and in-favor, emotion-based features could contribute to stance detection. An additional feature of our resource is that we do not only provide a “majority annotation” as is usual. We do define a well-performing aggregated annotation, but additionally provide the individual labels of each of our six annotators. This enables further research on differences in the percep"
W17-5203,D14-1181,0,0.001523,"in the t=0.5 annotation: Bi-LSTM has the same architecture as the normal LSTM, but includes an additional layer with a reverse direction. This approach has produced stateof-the-art results for POS-tagging (Plank et al., 2016), dependency parsing (Kiperwasser and Goldberg, 2016) and text classification (Zhou et al., 2016), among others. We use the same parameters as the LSTM, but concatenate the two hidden layers before passing them to the dense layer. #WeNeedFeminism because #NoMeansNo it doesnt mean yes, it doesnt mean try harder! CNN has proven remarkably effective for text classification (Kim, 2014; dos Santos and Gatti, 2014; Flekova and Gurevych, 2016) . We train a simple one-layer CNN with one convolutional layer on top of pre-trained word embeddings, following Kim (2014). The first layer is an embeddings layer that maps the input of length n (padded when needed) to an n x 300 dimensional matrix. The embedding matrix is then convoluted with filter sizes of 2, 3, and 4, followed by a pooling layer of length 2. This is then fed to a fully connected dense layer with ReLu activations and finally to the 8 output neurons, which are gated with the sigmoid function. We again use dropout (0.5"
W17-5203,Q16-1023,0,0.00424888,"vely with negative sentiment. For the majority annotation (Table 5), the number of annotations is smaller. However, the average size of the odds ratios increase (from 1.96 for t=0.0 to 5.39 for t=0.5). A drastic example is disgust in combination with negative sentiment, the predominant combination. Disgust is only labeled once with positive sentiment in the t=0.5 annotation: Bi-LSTM has the same architecture as the normal LSTM, but includes an additional layer with a reverse direction. This approach has produced stateof-the-art results for POS-tagging (Plank et al., 2016), dependency parsing (Kiperwasser and Goldberg, 2016) and text classification (Zhou et al., 2016), among others. We use the same parameters as the LSTM, but concatenate the two hidden layers before passing them to the dense layer. #WeNeedFeminism because #NoMeansNo it doesnt mean yes, it doesnt mean try harder! CNN has proven remarkably effective for text classification (Kim, 2014; dos Santos and Gatti, 2014; Flekova and Gurevych, 2016) . We train a simple one-layer CNN with one convolutional layer on top of pre-trained word embeddings, following Kim (2014). The first layer is an embeddings layer that maps the input of length n (padded when need"
W17-5203,D16-1105,0,0.0427528,"Missing"
W17-5203,P16-1191,0,0.00670011,"he same architecture as the normal LSTM, but includes an additional layer with a reverse direction. This approach has produced stateof-the-art results for POS-tagging (Plank et al., 2016), dependency parsing (Kiperwasser and Goldberg, 2016) and text classification (Zhou et al., 2016), among others. We use the same parameters as the LSTM, but concatenate the two hidden layers before passing them to the dense layer. #WeNeedFeminism because #NoMeansNo it doesnt mean yes, it doesnt mean try harder! CNN has proven remarkably effective for text classification (Kim, 2014; dos Santos and Gatti, 2014; Flekova and Gurevych, 2016) . We train a simple one-layer CNN with one convolutional layer on top of pre-trained word embeddings, following Kim (2014). The first layer is an embeddings layer that maps the input of length n (padded when needed) to an n x 300 dimensional matrix. The embedding matrix is then convoluted with filter sizes of 2, 3, and 4, followed by a pooling layer of length 2. This is then fed to a fully connected dense layer with ReLu activations and finally to the 8 output neurons, which are gated with the sigmoid function. We again use dropout (0.5), this time before and after the convolutional layers. S"
W17-5203,W17-5205,0,0.0906458,"Missing"
W17-5203,S16-1003,0,0.0547157,"Missing"
W17-5203,C14-1008,0,0.0277193,"nnotation: Bi-LSTM has the same architecture as the normal LSTM, but includes an additional layer with a reverse direction. This approach has produced stateof-the-art results for POS-tagging (Plank et al., 2016), dependency parsing (Kiperwasser and Goldberg, 2016) and text classification (Zhou et al., 2016), among others. We use the same parameters as the LSTM, but concatenate the two hidden layers before passing them to the dense layer. #WeNeedFeminism because #NoMeansNo it doesnt mean yes, it doesnt mean try harder! CNN has proven remarkably effective for text classification (Kim, 2014; dos Santos and Gatti, 2014; Flekova and Gurevych, 2016) . We train a simple one-layer CNN with one convolutional layer on top of pre-trained word embeddings, following Kim (2014). The first layer is an embeddings layer that maps the input of length n (padded when needed) to an n x 300 dimensional matrix. The embedding matrix is then convoluted with filter sizes of 2, 3, and 4, followed by a pooling layer of length 2. This is then fed to a fully connected dense layer with ReLu activations and finally to the 8 output neurons, which are gated with the sigmoid function. We again use dropout (0.5), this time before and afte"
W17-5203,W11-2207,0,0.0147641,"xists. These include review texts from different domains, for instance from Amazon and other shopping sites (Hu and Liu, 2004; Ding et al., 2008; Toprak et al., 2010; Lakkaraju et al., 2011), restaurants (Ganu et al., 2009), news articles (Wiebe et al., 2005), blogs (Kessler et al., 2010), as well as microposts on Twitter. For the latter, shown in the upper half of Table 1, there are general corpora (Nakov et al., 2013; Spina et al., 2012; Thelwall et al., 2012) as well as ones focused on very specific subdomains, for instance on ObamaMcCain Debates (Shamma et al., 2009), Health Care Reforms (Speriosu et al., 2011). A popular example for a manually annotated corpus for sentiment, which includes stance annotation for a set of topics is the SemEval 2016 data set (Mohammad et al., 2016). For emotion analysis, the set of annotated resources is smaller (compare the lower half of Table 1). A very early resource is the ISEAR data set (Scherer and Wallbott, 1997) which contains descriptions of emotional events. While motivated by psychological research, it was later repurposed for computational research. The first data set developed specifically for computational research was the tales corpus by Alm et al. (200"
W17-5203,S16-1001,0,0.0379907,"3 78 489 177 156 33 984 520 487 213 Table 2: Corpus Statistics. The threshold t measures that a fraction of more than t annotators labeled the respective emotion (e. g., t=0.0: at least one annotator t=0.99: all annotators). Overall number of tweets: 4,868. Max Anger Anticipation Disgust Fear Joy Sadness Surprise Trust 0.28 0.11 0.06 0.08 0.30 0.04 0.09 0.29 0.49 0.39 0.30 0.25 0.52 0.30 0.33 0.57 ers and have college-level proficiency in English. To train the annotators on the task, we performed two training iterations based on 50 randomly selected tweets from the SemEval 2016 Task 4 corpus (Nakov et al., 2016). After each iteration, we discussed annotation differences (informally) in face-to-face meetings. For the final annotation, tweets were presented to the annotators in a web interface which paired a tweet with a set of binary check boxes, one for each emotion. Taggers could annotate any set of emotions. Each annotator was assigned with 5/7 of the corpus with equally-sized overlap of instances based on an offset shift. Not all annotators finished their task.2 Corpus Annotation and Analysis 3.1 Min Table 3: Kappa Statistics for all pairs of annotators. Mohammad et al. (2015) annotated electoral"
W17-5203,S13-2052,0,0.0171164,"fear, surprise, disgust, anger, anticipation, joy, roles, style, purpose (number denotes subset in corpus with emotion annotations) For sentiment analysis, a large number of annotated data sets exists. These include review texts from different domains, for instance from Amazon and other shopping sites (Hu and Liu, 2004; Ding et al., 2008; Toprak et al., 2010; Lakkaraju et al., 2011), restaurants (Ganu et al., 2009), news articles (Wiebe et al., 2005), blogs (Kessler et al., 2010), as well as microposts on Twitter. For the latter, shown in the upper half of Table 1, there are general corpora (Nakov et al., 2013; Spina et al., 2012; Thelwall et al., 2012) as well as ones focused on very specific subdomains, for instance on ObamaMcCain Debates (Shamma et al., 2009), Health Care Reforms (Speriosu et al., 2011). A popular example for a manually annotated corpus for sentiment, which includes stance annotation for a set of topics is the SemEval 2016 data set (Mohammad et al., 2016). For emotion analysis, the set of annotated resources is smaller (compare the lower half of Table 1). A very early resource is the ISEAR data set (Scherer and Wallbott, 1997) which contains descriptions of emotional events. Whi"
W17-5203,S07-1013,0,0.300528,"Electoral Tweets 8 9 10 11 12 13 descriptions sentences blogs headlines tweets tweets Size 498 15,196 2,516 3,238 4,490 8,850 12,770 2,205 4,870 4,242 Topic General General Politics Politics Weather Weather Gas prices General 5 topics General Source Go et al. (2009) Nakov et al. (2013) Speriosu et al. (2011) Shamma et al. (2009) Cavender-Bares (2011) Busch (2011) Busch (2012) Hassan Saif and Alani (2013) Mohammad et al. (2016) Thelwall et al. (2012) 7,666 Emotional Events Scherer and Wallbott (1997) 1,580 Grim’s Fairytales Alm et al. (2005) 173 General Aman and Szpakowicz (2007) 1,250 General Strapparava and Mihalcea (2007) 7,102 General Mohammad and Bravo-Marquez (2017) 965 Elections Mohammad et al. (2015) Table 1: A selection of resources for sentiment analysis (on Twitter, 1–7) and emotion analysis (in general, 8–12). Annotation refers to the following annotation schemes: [1] positive-negative, [2] positivenegative-neutral, [3] positive-negative-mixed-other, [4] positive-negative-netural-unrelated-can’t tell, [5] positive-negative-neutral-mixed-other, [6] for-against, [7] positive and negative strength (range), [8] joy, fear, anger, sadness, disgust, shame, guilt, [9] angry, disgusted, fearful, happy, sad, po"
W17-5203,P16-2067,0,0.018869,"an Victory. Disgust occurs almost exclusively with negative sentiment. For the majority annotation (Table 5), the number of annotations is smaller. However, the average size of the odds ratios increase (from 1.96 for t=0.0 to 5.39 for t=0.5). A drastic example is disgust in combination with negative sentiment, the predominant combination. Disgust is only labeled once with positive sentiment in the t=0.5 annotation: Bi-LSTM has the same architecture as the normal LSTM, but includes an additional layer with a reverse direction. This approach has produced stateof-the-art results for POS-tagging (Plank et al., 2016), dependency parsing (Kiperwasser and Goldberg, 2016) and text classification (Zhou et al., 2016), among others. We use the same parameters as the LSTM, but concatenate the two hidden layers before passing them to the dense layer. #WeNeedFeminism because #NoMeansNo it doesnt mean yes, it doesnt mean try harder! CNN has proven remarkably effective for text classification (Kim, 2014; dos Santos and Gatti, 2014; Flekova and Gurevych, 2016) . We train a simple one-layer CNN with one convolutional layer on top of pre-trained word embeddings, following Kim (2014). The first layer is an embeddings la"
W17-5203,E12-1049,0,0.00955703,"ormation (hashtags, emoticons, emojis) that can be used as weak supervision for training classifiers (Suttles and Ide, 2013). The classifier then learns the association of all other words in the message with the “self-labeled” emotion (Wang et al., 2012). While this approach provides a practically feasible approximation of emotions, there is no publicly available, manually vetted data set for Twitter emotions that would support accurate and comparable evaluations. In addition, it has been shown that distant annotation is conceptually different from manual annotation for sentiment and emotion (Purver and Battersby, 2012). With this paper, we contribute manual emotion annotation for a publicly available Twitter data set. We annotate the SemEval 2016 Stance Data set (Mohammad et al., 2016) which provides sentiment and stance information and is popular in the research community (Augenstein et al., 2016; Wei et al., 2016; Dias and Becker, 2016; Ebrahimi et al., 2016). It therefore enables further research on the relations between sentiment, emotions, and stances. For instance, if the distribution of subclasses of positive or negative emotions is different for against and in-favor, emotion-based features could con"
W17-5203,P15-1150,0,0.0587705,"Missing"
W17-5203,roberts-etal-2012-empatweet,0,0.0198922,"ines. A notable gap is the unavailability of a publicly available set of microposts (e. g., tweets) with emotion labels. To the best of our knowledge, there are only three previous approaches to labeling tweets with discrete emotion labels. One is the recent data set on for emotion intensity estimation, a shared task aiming at the development of a regression model. The goal is not to predict the emotion class, but a distribution over their intensities, and the set of emotions is limited to fear, sadness, anger, and joy (Mohammad and Bravo-Marquez, 2017). Most similar to our work is a study by Roberts et al. (2012) which annotated 7,000 tweets manually for 7 emotions (anger, disgust, fear, joy, love, sadness and surprise). They chose 14 topics which they believe should elicit emotional tweets and collect hashtags to help identify tweets that are on these topics. After several iterations, the annotators reached κ = 0.67 inter-annotator agreement on 500 tweets. Unfortunately, the data appear not to be available any more. An additional limitation of that dataset was that 5,000 of the 7,000 tweets were annotated by one annotator only. In contrast, we provide several annotations for each tweet. 14 Label coun"
W17-5203,C16-1311,0,0.00555306,"Missing"
W17-5203,P10-1059,0,0.0103963,"rful, happy, sad, positively surprised, negatively surprised, [10] happiness, sadness, anger, disgust, surprise, fear, mixed, [11] anger, disgust, fear, joy, sadness, surprise, [12] anger, fear, joy, sadness, [13] positive, negative, mixed, intensity, trust, fear, surprise, disgust, anger, anticipation, joy, roles, style, purpose (number denotes subset in corpus with emotion annotations) For sentiment analysis, a large number of annotated data sets exists. These include review texts from different domains, for instance from Amazon and other shopping sites (Hu and Liu, 2004; Ding et al., 2008; Toprak et al., 2010; Lakkaraju et al., 2011), restaurants (Ganu et al., 2009), news articles (Wiebe et al., 2005), blogs (Kessler et al., 2010), as well as microposts on Twitter. For the latter, shown in the upper half of Table 1, there are general corpora (Nakov et al., 2013; Spina et al., 2012; Thelwall et al., 2012) as well as ones focused on very specific subdomains, for instance on ObamaMcCain Debates (Shamma et al., 2009), Health Care Reforms (Speriosu et al., 2011). A popular example for a manually annotated corpus for sentiment, which includes stance annotation for a set of topics is the SemEval 2016 dat"
W17-5203,S16-1062,0,0.00582856,"ximation of emotions, there is no publicly available, manually vetted data set for Twitter emotions that would support accurate and comparable evaluations. In addition, it has been shown that distant annotation is conceptually different from manual annotation for sentiment and emotion (Purver and Battersby, 2012). With this paper, we contribute manual emotion annotation for a publicly available Twitter data set. We annotate the SemEval 2016 Stance Data set (Mohammad et al., 2016) which provides sentiment and stance information and is popular in the research community (Augenstein et al., 2016; Wei et al., 2016; Dias and Becker, 2016; Ebrahimi et al., 2016). It therefore enables further research on the relations between sentiment, emotions, and stances. For instance, if the distribution of subclasses of positive or negative emotions is different for against and in-favor, emotion-based features could contribute to stance detection. An additional feature of our resource is that we do not only provide a “majority annotation” as is usual. We do define a well-performing aggregated annotation, but additionally provide the individual labels of each of our six annotators. This enables further research on di"
W17-5203,C16-1329,0,0.00375071,"(Table 5), the number of annotations is smaller. However, the average size of the odds ratios increase (from 1.96 for t=0.0 to 5.39 for t=0.5). A drastic example is disgust in combination with negative sentiment, the predominant combination. Disgust is only labeled once with positive sentiment in the t=0.5 annotation: Bi-LSTM has the same architecture as the normal LSTM, but includes an additional layer with a reverse direction. This approach has produced stateof-the-art results for POS-tagging (Plank et al., 2016), dependency parsing (Kiperwasser and Goldberg, 2016) and text classification (Zhou et al., 2016), among others. We use the same parameters as the LSTM, but concatenate the two hidden layers before passing them to the dense layer. #WeNeedFeminism because #NoMeansNo it doesnt mean yes, it doesnt mean try harder! CNN has proven remarkably effective for text classification (Kim, 2014; dos Santos and Gatti, 2014; Flekova and Gurevych, 2016) . We train a simple one-layer CNN with one convolutional layer on top of pre-trained word embeddings, following Kim (2014). The first layer is an embeddings layer that maps the input of length n (padded when needed) to an n x 300 dimensional matrix. The em"
W17-5203,H05-1073,0,\N,Missing
W17-5204,I13-1013,0,0.0274489,"ics requiring that their changing saliency over time must be considered. Third, expressing some of these attitudes publicly in a particular manner can become relevant to criminal law. Thus, especially the glorification of national socialism is not suited to serve as a distinctive criterion, since its public expression in a non-subtle manner is avoided by Twitter users. Finally, research has repeatedly demonstrated that some of the attitudes mentioned above (e. g., xenophobia) are widespread among the German population (Best et al., 2016; Zick et al., 25 2016), violence or threat detection (C. Basave et al., 2013; Wester et al., 2016). 3 strongest race. Violence is legitimated as a basic law of society and any deviation from violence, e. g., by peaceful agreement, is considered to undermine the chances for survival and is thus illegitimate. The imagined homogeneity and purity of the own race needs to be defended; hence, political opponents and other people who are perceived as not fitting are considered as enemies who can be fought without any reservation. Indicative are thus words and semantic structures which aggressively offend the opponents as enemies refusing any agreement with them, e. g., “Absc"
W17-5204,W17-1101,0,0.0128083,"erefore, we propose a ranking approach which is capable of projecting user profiles to a continuous range spanning different degrees of similarity to known (groups of) right-wing extremist or non-extremist users. Extremism detection can also be seen as special case of profiling users of social network platforms in a more general way, e. g., classification of personality traits (Golbeck et al., 2011; Quercia et al., 2011). Such approaches can be seen as extensions to sentiment analysis in general (Liu, 2015). More recently, there is a growing interest in particular aspects such as hate speech (Schmidt and Wiegand, 2017; Waseem and Hovy, 2016), racism (Waseem, Background and Related Work Background. Right-wing extremism is an ideology of asymmetric quality of social groups, defined by race, ethnicity or nationality, and a related authoritarian concept of society. It encompasses aggressive behavior and the underlying attitudes of xenophobia, racism, anti-Semitism, social Darwinism, as well as national chauvinism, glorification of the historical national socialism and support for dictatorship (St¨oss, 2010). When transforming this concept into patterns used in Twitter communication, certain domainspecific cont"
W17-5204,W10-0217,0,0.0323759,"Missing"
W17-5204,W16-5618,0,0.0447263,"Missing"
W17-5204,N16-2013,0,0.0273039,"ng approach which is capable of projecting user profiles to a continuous range spanning different degrees of similarity to known (groups of) right-wing extremist or non-extremist users. Extremism detection can also be seen as special case of profiling users of social network platforms in a more general way, e. g., classification of personality traits (Golbeck et al., 2011; Quercia et al., 2011). Such approaches can be seen as extensions to sentiment analysis in general (Liu, 2015). More recently, there is a growing interest in particular aspects such as hate speech (Schmidt and Wiegand, 2017; Waseem and Hovy, 2016), racism (Waseem, Background and Related Work Background. Right-wing extremism is an ideology of asymmetric quality of social groups, defined by race, ethnicity or nationality, and a related authoritarian concept of society. It encompasses aggressive behavior and the underlying attitudes of xenophobia, racism, anti-Semitism, social Darwinism, as well as national chauvinism, glorification of the historical national socialism and support for dictatorship (St¨oss, 2010). When transforming this concept into patterns used in Twitter communication, certain domainspecific contextual opportunities and"
W17-5204,W16-0413,0,0.064594,"eir changing saliency over time must be considered. Third, expressing some of these attitudes publicly in a particular manner can become relevant to criminal law. Thus, especially the glorification of national socialism is not suited to serve as a distinctive criterion, since its public expression in a non-subtle manner is avoided by Twitter users. Finally, research has repeatedly demonstrated that some of the attitudes mentioned above (e. g., xenophobia) are widespread among the German population (Best et al., 2016; Zick et al., 25 2016), violence or threat detection (C. Basave et al., 2013; Wester et al., 2016). 3 strongest race. Violence is legitimated as a basic law of society and any deviation from violence, e. g., by peaceful agreement, is considered to undermine the chances for survival and is thus illegitimate. The imagined homogeneity and purity of the own race needs to be defended; hence, political opponents and other people who are perceived as not fitting are considered as enemies who can be fought without any reservation. Indicative are thus words and semantic structures which aggressively offend the opponents as enemies refusing any agreement with them, e. g., “Abschaumpresse” (“scum pre"
W17-5204,C00-2137,0,0.0425829,"ng group similarities over individual ones), while the best performance can be obtained by choosing the k and ` parameters independently of one another (k=4, `=5). As can be seen from the right-most column of Table 1, reducing the test set to a subsample of profiles with at least 100 Tweets each (62 profiles remaining) leads to an additional performance increase up to an F1 score of 0.81 in unbalanced discrete decoding. All differences of the ranking models as reported in Table 1 are statistically significant over the baseline and the classifier according to an approximate randomization test (Yeh, 2000) at significance levels of p < 0.05 or smaller. 4.2.2 Continuous Ranking In order to evaluate the plausibility of the ranking model scores in the absence of ground truth ranking annotations, we analyze the model predictions on the differential profiles for which no consensus regarding their category membership could be reached among the expert annotators (cf. Section 4.1). Being related to some New Right German political movements, which are notoriously hard to be delimited from right-wing extremist political Discussion. In Figure 1 we explore the parameter space for different values of k and"
W17-5206,H05-1073,0,0.455254,"Missing"
W17-5206,baccianella-etal-2010-sentiwordnet,0,0.0525166,"hammad and Kiritchenko, 2015) containing ratings for 17k words with associations to anger, anticipation, disgust, fear, joy, sadness, surprise and trust. Additionally, we use the 14k ratings for valence, arousal, and dominance collected by Warriner et al. (2013). For concreteness we rely on the collection of 40k ratings from Brysbaert et al. (2014). Finally, Extending and Adding Norms The baseline system builds on top of a variety of different lexical resources (Hu and Liu, 2004; Wilson et al., 2005; Svetlana Kiritchenko and Mohammad; Mohammad and Turney, 2013; Mohammad and Kiritchenko, 2015; Baccianella et al., 2010; Bravo-Marquez et al., 2016; Nielsen, 2011). Such 1 https://github.com/felipebravom/ AffectiveTweets 51 we use the 10k ratings for happiness from Dodds et al. (2011). These 13 ratings correspond to an automatic extension to 1.6 million word types with ≈ 21 million new word ratings. We map the ratings to an interval of [0, 10]. Table 1 shows the top words for eight ratings. For the emotion intensity prediction in our predictive model, we represent each rating with seven feature dimensions per tweet: 1. 2. 3. 4. 5. 6. 7. 2.3 I’m not really happy I’m quite sad ... Average rating score across all"
W17-5206,D14-1181,0,0.00386325,"motion intensity prediction tasks.4 Finally, for the full system IMS, we combine features in a random forest classifier using weka (Witten et al., 1999). We use 800 trees (called iterations in Weka). We estimate one model for each of the four target emotions. Tweet Regression The tweet regression feature relies on the annotated training samples. We train a neural network based on word embeddings to predict the emotion intensity for each tweet. Convolutional neural networks (CNNs), trained on top of pre-trained word vectors, have been shown to work well for sentence-level classification tasks (Kim, 2014). We apply a similar method here, combining CNNs and LSTMs (Hochreiter and Schmidhuber, 1997). The final architecture used by IMS is shown in Figure 1. Each tweet is represented by a matrix of size 50 × 300 (padded where necessary, embedding dimension is 300, the maximal token sequence in a tweet is set to 50). We apply dropout with a rate of 0.25. The matrix is then the input for a convolutional layer with a window size of 3, followed by a maxpooling layer (size 2) and an LSTM to predict a numerical output for each tweet. This architecture captures sequential information in a compact way. For"
W17-5206,L16-1413,1,0.908706,"Missing"
W17-5206,W17-1903,1,0.89356,"Missing"
W17-5206,W16-4008,0,0.0338857,"Missing"
W17-5206,N13-1039,0,0.0307932,"s. It consists of ≈50 million tweets and ≈800 million tokens. After removing words with less than 10 occurrences, the resource contains 1.6 million word types. The 300 dimensional word representations are obtained with word2vec2 (Mikolov et al., 2013). To study the impact of the training domain, we additionally conduct experiments with the public available GoogleNews-vectors that were trained on a 100b words corpus of news texts. Both word embeddings are used to extend the emotion lexicons (Section 2.2) as well as input embeddings in our tweet regression model (Section 2.3). We use TweetNLP3 (Owoputi et al., 2013) as tokenizer. In the case of observing only out-ofvocabulary words (no rating available) we set the score to the median value of the corresponding category. The regressor based on the tweet text is implemented with keras (Chollet et al., 2015). We train one model for each of the four emotions separately. Furthermore, we provide the output of all four emotion-specific regression models in all emotion intensity prediction tasks.4 Finally, for the full system IMS, we combine features in a random forest classifier using weka (Witten et al., 1999). We use 800 trees (called iterations in Weka). We"
W17-5206,H05-1044,0,0.0865343,"). We apply this procedure for 13 different lexicons using the following resources: NRC Hashtag Emotion Lexicon (Mohammad and Kiritchenko, 2015) containing ratings for 17k words with associations to anger, anticipation, disgust, fear, joy, sadness, surprise and trust. Additionally, we use the 14k ratings for valence, arousal, and dominance collected by Warriner et al. (2013). For concreteness we rely on the collection of 40k ratings from Brysbaert et al. (2014). Finally, Extending and Adding Norms The baseline system builds on top of a variety of different lexical resources (Hu and Liu, 2004; Wilson et al., 2005; Svetlana Kiritchenko and Mohammad; Mohammad and Turney, 2013; Mohammad and Kiritchenko, 2015; Baccianella et al., 2010; Bravo-Marquez et al., 2016; Nielsen, 2011). Such 1 https://github.com/felipebravom/ AffectiveTweets 51 we use the 10k ratings for happiness from Dodds et al. (2011). These 13 ratings correspond to an automatic extension to 1.6 million word types with ≈ 21 million new word ratings. We map the ratings to an interval of [0, 10]. Table 1 shows the top words for eight ratings. For the emotion intensity prediction in our predictive model, we represent each rating with seven featu"
W17-5206,E17-2090,0,0.0217273,"od to extend these lexicons to larger Twitter specific vocabulary ii), learning a new rating score for every word and not just highly associated terms and iii), including novel rating categories that provide complementary and potential useful information, such as valence, arousal, dominance and concreteness. Several approaches have been proposed to combine distributional word representations with supervised machine learning methods to extend affective norms (Turney et al., 2011; Tsvetkov et al., 2014; Recchia and Louwerse, 2015b; Vankrunkelsven et al., 2015; K¨oper and Schulte im Walde, 2016; Sedoc et al., 2017). K¨oper and Schulte im Walde (2017) compared various supervised methods and showed that a feed forward neural network together with low dimensional distributed word representations (embeddings) obtained the highest correlation with human annotated ratings for concreteness. Table 1: Top four words for eight different rating types based on our automatically generated ratings. system AffectiveTweets, explain how we extend resources to the domain of Twitter. Then, we explain our sentence regressor, which is based on deep learning and pre-trained word embeddings. Finally, we introduce two addition"
W17-5206,P14-1024,0,0.0204574,"of out-of-vocabulary words. We address this with three separate approaches, namely by i) applying a supervised method to extend these lexicons to larger Twitter specific vocabulary ii), learning a new rating score for every word and not just highly associated terms and iii), including novel rating categories that provide complementary and potential useful information, such as valence, arousal, dominance and concreteness. Several approaches have been proposed to combine distributional word representations with supervised machine learning methods to extend affective norms (Turney et al., 2011; Tsvetkov et al., 2014; Recchia and Louwerse, 2015b; Vankrunkelsven et al., 2015; K¨oper and Schulte im Walde, 2016; Sedoc et al., 2017). K¨oper and Schulte im Walde (2017) compared various supervised methods and showed that a feed forward neural network together with low dimensional distributed word representations (embeddings) obtained the highest correlation with human annotated ratings for concreteness. Table 1: Top four words for eight different rating types based on our automatically generated ratings. system AffectiveTweets, explain how we extend resources to the domain of Twitter. Then, we explain our sente"
W17-5206,D11-1063,0,0.0124655,"contain a great deal of out-of-vocabulary words. We address this with three separate approaches, namely by i) applying a supervised method to extend these lexicons to larger Twitter specific vocabulary ii), learning a new rating score for every word and not just highly associated terms and iii), including novel rating categories that provide complementary and potential useful information, such as valence, arousal, dominance and concreteness. Several approaches have been proposed to combine distributional word representations with supervised machine learning methods to extend affective norms (Turney et al., 2011; Tsvetkov et al., 2014; Recchia and Louwerse, 2015b; Vankrunkelsven et al., 2015; K¨oper and Schulte im Walde, 2016; Sedoc et al., 2017). K¨oper and Schulte im Walde (2017) compared various supervised methods and showed that a feed forward neural network together with low dimensional distributed word representations (embeddings) obtained the highest correlation with human annotated ratings for concreteness. Table 1: Top four words for eight different rating types based on our automatically generated ratings. system AffectiveTweets, explain how we extend resources to the domain of Twitter. The"
W18-6206,D17-1169,0,0.0692008,"X X X X X X X X X X X X X X X X X X X X X X X X X X X X 23 20 12 9 7 5 5 3 2 1 1 1 1 1 Table 7: Overview of methods employed by different teams (sorted by popularity from left to right). 4.4 9 X SemEval 26 X Sentence/Document X Emotion Emb. X X X X X X X X X X X X X X X X X X X X X X X X X X Unlabeled Corpora 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X 8 X X 7 6 4 X 4 3 Table 8: Overview of information sources employed by different teams (sorted by popularity from left to right). Top 3 Submissions (Felbo et al., 2017) and “Universal Sentence Encoder” (Cer et al., 2018) as features. In the following, we briefly summarize the approaches used by the top three teams: Amobee, IIIDYT, and NTUA-SLP. For more information on these approaches and those of the other teams, we refer the reader to the individual system description papers. The three best performing systems are all ensemble approaches. However, they make use of different underlying machine learning architectures and rely on different kinds of information. 4.4.1 Lexicons LDA X Emoji X X X X Characters X X MLP Autoencoder Random Forrest k-Means Bagging Att"
W18-6206,W18-6208,0,0.0526748,"ree best performing systems are all ensemble approaches. However, they make use of different underlying machine learning architectures and rely on different kinds of information. 4.4.1 Lexicons LDA X Emoji X X X X Characters X X MLP Autoencoder Random Forrest k-Means Bagging Attention Linear Classifier Transfer Learning Language model Ensemble CNN/Capsules X X X X X X X X X X X X X X X X X Words X X X X X X X X X X X Rank X X X X X X X X X X X X X X LSTM/RNN/GRU Rank Embeddings 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 4.4.2 IIIDYT The second-ranking system, IIIDYT (Balazs et al., 2018), preprocesses the dataset by tokenizing the sentences (including emojis), and normalizing the USERNAME, NEWLINE, URL and TRIGGERWORD indicators. Then, it feeds word-level representations returned by a pretrained ELMo layer into a Bi-LSTM with 1 layer of 2048 hidden units for each direction. The Bi-LSTM output word representations are max-pooled to generate sentencelevel representations, followed by a single hidden layer of 512 units and output size of 6. The team trains six models with different random initializations, obtains the probability distributions for each example, and then averages"
W18-6206,S07-1013,0,0.198525,"a.). This is motivated by the high availability of usergenerated text and by the challenge that manual annotation is typically tedious or expensive. This contrasts with the current data demand of machine learning, and especially, deep learning approaches. With our work in IEST, we combine the goal of the development of models which are able to recognize emotions from implicit descriptions without having access to explicit emotion words, with the paradigm of distant supervision. One of the first corpora annotated for emotions is that by Alm et al. (2005) who analyze sentences from fairy tales. Strapparava and Mihalcea (2007) annotate news headlines with emotions and valence, Mohammad et al. (2015) annotate tweets on elections, and Schuff et al. (2017) tweets of a stance dataset (Mohammad et al., 2017). The SemEval2018 Task 1: Affect in Tweets (Mohammad et al., 2018) includes several subtasks on inferring the affectual state of a person from their tweet: emotion intensity regression, emotion intensity ordinal classification, valence (sentiment) regression, valence ordinal classification, and multi-label emotion classification. In all of these prior shared tasks and datasets, no distinction is made between implicit"
W19-1304,P18-1002,0,0.0272504,"ions are equally common. Secondly, we perform a crowdsourcing experiment in which we collect scores for different combinations of degree adverbs and emotion adjectives. We use these data, which we make publicly available, as an additional challenging test set for the task of intensity prediction for English. Thirdly, we use a state-of-the-art intensity prediction model (Wu et al., 2018) on this test set and evaluate two methods to improve these predictions, namely the inclusion (Zhao et al., 2018) and n-gram embeddings ` La Carte of additional subword information via A with Bag-of-Substrings (Khodak et al., 2018). We evaluate based on Word2vec, GloVe and fastText embeddings and show that particularly the first two benefit from these changes, but to different extents. 2 from Early Modern English and shows the how the distributions of boosters change across time. Nevalainen (2008) study the social variation in intensifier use, with a focus on the suffix -ly. More recently, Napoli and Ravetto (2017) collect a volume of papers that explore the process of intensification following a corpus-based, cross-linguistic and contrastive approach. The volume contains various works on the variation in the distributi"
W19-1304,W16-0410,0,0.288,"on in terms of the methods and data used. Following this approach, one could build a distributional semantic model whose vocabulary includes the modified phrases. In practice, each occurrence of a modified adjective by a degree adverb could be treated as a single token (e. g. “not happy” would be represented as “not happy”). For a general overview of modality and negation in computational linguistics we refer the interested reader to the work by Morante and Sporleder (2012). Furthermore, Zhu et al. (2014) study the effect of negation words on sentiment and evaluate a neural composition model. Kiritchenko and Mohammad (2016a) create a sentiment lexicon of phrases that include modifiers such as negators, modals, and In this paper, we will use these terms interchangeably. 26 degree adverbs. The phrases and their constituent words are annotated manually with the same annotation procedure we will discuss in detail. We follow this work closely and apply the same procedures in the context of emotion analysis. Dragut and Fellbaum (2014) study the effect of intensifiers on the sentiment ratings and shows that the degree adverbs do not carry an inherent sentiment polarity but alter the degree of the polarity of the const"
W19-1304,N16-1095,0,0.194212,"Missing"
W19-1304,Q17-1010,0,0.240398,"llows to build vectors for misspelled words and concatenation of words. Particularly on Twitter data, we benefit from getting a representation for phrases like “sooooexcited:)”, “verrry cheerful”, “soo unhappy:(”. Relevant for our analysis is that BoS uses special characters to mark the start and the end of the word and thus helps the model to distinguish morphemes that occur at different word parts, like prefixes or suffixes. Through that we learn to distinguish morphemes like “un-”, “-er” and “-est” that are part of our focus phrases. Note that this method uses the same idea as in fastText (Bojanowski et al., 2017), but is for our case computationally more efficient, since the BoS model is trained directly on top of pre-trained vectors, instead of predicting over text corpora. disgust surprise sadness +.73 .43 .50 .52 +.41 +.02 +.53 .70 .67 .51 .16 .55 .10 .66 .11 +.17 .47 .76 so sad not sad kinda sad .50 +.66 +.04 +.13 +.55 .60 .57 .55 .41 +.62 .02 +.02 .16 +.03 .45 .52 .18 +.02 so angry not angry kinda angry .39 +.26 +.86 +.21 +.02 +.63 +.40 .36 .82 .17 .27 .45 .80 +.68 +.84 +.32 +.08 +.64 so scared not scared kinda scared .07 +.15 +.35 .35 +.03 +.10 so surprised +.34 not surprised +.60 kinda surprise"
W19-1304,W10-3110,0,0.0425061,"e also long history of research in English studies and more generally in Language Studies. Most English studies focus on the incidence and distribution of these adverbs in different corpora, e.g. Peters (1994) study letters 2 2.2 Modifiers in the context of Sentiment and Emotion Analysis In the context of sentiment analysis the discussion of intensifiers and negations has gained quite some attention, since those are primarily markers of subjectivity (Athanasiadou, 2007). Negations, and in particular negation cue detection (with the goal of scope recognition) have been the research interest of Councill et al. (2010) and Reitan et al. (2015), who use a lexicon for negation cue detection and a linear-chain conditional random field for scope recognition. In the area of distributional semantics, the investigation of word vectors with a focus on negated adjectives (Aina et al., 2018) is complementary to our work with regards to negation in terms of the methods and data used. Following this approach, one could build a distributional semantic model whose vocabulary includes the modified phrases. In practice, each occurrence of a modified adjective by a degree adverb could be treated as a single token (e. g. “no"
W19-1304,N16-1128,0,0.0455126,"Missing"
W19-1304,W14-3010,0,0.0294952,"nterested reader to the work by Morante and Sporleder (2012). Furthermore, Zhu et al. (2014) study the effect of negation words on sentiment and evaluate a neural composition model. Kiritchenko and Mohammad (2016a) create a sentiment lexicon of phrases that include modifiers such as negators, modals, and In this paper, we will use these terms interchangeably. 26 degree adverbs. The phrases and their constituent words are annotated manually with the same annotation procedure we will discuss in detail. We follow this work closely and apply the same procedures in the context of emotion analysis. Dragut and Fellbaum (2014) study the effect of intensifiers on the sentiment ratings and shows that the degree adverbs do not carry an inherent sentiment polarity but alter the degree of the polarity of the constituents they modify. We argue that there is not enough work on transferring the methods used in sentiment analysis to the more fine-grained analysis of emotions, except for Strohm and Klinger (2018), who limit themselves to analysis and do not apply state-of-the-art prediction models for handling degree adverbs, and Carrillo-de Albornoz and Plaza (2013) who consider modified emotions but predict sentiment. 3 (E"
W19-1304,W17-5205,0,0.1442,"Missing"
W19-1304,N15-1184,0,0.0695164,"Missing"
W19-1304,D17-1169,0,0.0988878,"Missing"
W19-1304,S18-1001,0,0.0439338,"ur extensions of the semantic spaces do not negatively affect the results on the EmoInt dataset. Unexpectedly, retrofitting does not help in all settings in our post-processing pipeline except for da kin f do kin t jus ve ry so rea lly 0.00 Modifier Figure 4: Most frequent three amplifiers and downtoners used across all emotions and their variation with respect to emotion. mon Crawl). Each embedding is then optionally augmented with phrase and subword embeddings and fed into a CNN-LSTM model as proposed by Wu et al. (2018), trained on the Affect in Tweets Dataset used at Sem Eval 2018 Task 1 (Mohammad et al., 2018). Their system achieved an average Pearson correlation score of 0.722, and ranked 12/48 in the emotion intensity regression task. Table 3 shows Spearman’s rank correlation between the predicted intensity scores and the emotion scores obtained in the annotation of our Twitter 31 average Lexicon EI regressor Attention CNN-LSTM (Wu et al., 2018) joy: 0.75 I am so happy!! #joy anger: 0.32 sadness: 0.27 fear: 0.13 Figure 6: Experimental Setup. The green arrow from Word2vec to the regressor unit shows the information flow in the baseline. The black solid arrows show the different experimental settin"
W19-1304,J12-2001,0,0.0309759,"emantics, the investigation of word vectors with a focus on negated adjectives (Aina et al., 2018) is complementary to our work with regards to negation in terms of the methods and data used. Following this approach, one could build a distributional semantic model whose vocabulary includes the modified phrases. In practice, each occurrence of a modified adjective by a degree adverb could be treated as a single token (e. g. “not happy” would be represented as “not happy”). For a general overview of modality and negation in computational linguistics we refer the interested reader to the work by Morante and Sporleder (2012). Furthermore, Zhu et al. (2014) study the effect of negation words on sentiment and evaluate a neural composition model. Kiritchenko and Mohammad (2016a) create a sentiment lexicon of phrases that include modifiers such as negators, modals, and In this paper, we will use these terms interchangeably. 26 degree adverbs. The phrases and their constituent words are annotated manually with the same annotation procedure we will discuss in detail. We follow this work closely and apply the same procedures in the context of emotion analysis. Dragut and Fellbaum (2014) study the effect of intensifiers"
W19-1304,N16-1018,0,0.0326417,"Missing"
W19-1304,W15-2914,0,0.0578291,"Missing"
W19-1304,S18-1028,0,0.239495,"such phrases and analyze which of the established embedding methods Word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and fastText embeddings (Bojanowski et al., 2017) performs well when predicting intensities for tweets containing such phrases. We will see that the performance of the popular and fast-to-train Word2vec method can be increased with a simple postprocessing pipeline which we present in this paper. As a motivating example, the DeepMoji model (Felbo et al., 2017) predicts anger for both the example sentences “I am not angry.” and “I am angry.”1 . Using the model by Wu et al. (2018) (one of the state-of-the-art intensity prediction models from Mohammad et al. (2018), building their model on top of Word2vec embeddings) we also obtain anger as having the highest intensity for both examples. We argue that the models should be more sensitive to the difference between negations, downtoners and amplifiers. With this paper, we contribute to alleviate this situation in three aspects. Firstly, we provide an analysis of the distribution of degree adverbs (inAdjective phrases like “a little bit surprised”, “completely shocked”, or “not stunned at all” are not handled properly by cu"
W19-1304,D14-1162,0,0.0841741,". The reliability drops most when context is not available for fear (by 11 percentage points). Figure 5 shows the distribution of the scores assigned through the annotation per emotion. We observe that disgust is mostly amplified and rarely negated (only once). The outliers in each boxplot mostly correspond to negated phrases. 4.3 Embedding Adaptations Figure 6 summarizes our experimental setup. We build on top of pretrained embeddings obtained with Word2vec (Mikolov et al., 2013) (300d, negative sampling, Google News corpus), fastText (Bojanowski et al., 2017) (300d, news corpora), or GloVe (Pennington et al., 2014) (300d, Com4.2 Annotation Analysis Table 1 shows examples of phrases annotated with real-valued scores following the annotation pro30 amplifier downtoner negation surprise Emotion sadness joy fear disgust t un no on ly pre tty qu ite slig htl y bu t jus t kin do f kin da littl e it ittle ab al tru ly utt er l y ve ry r ally tot so pe su de ep ly ex tre me ly fre ak ing fuc kin g inc red ibly mo re mo st pe rfe ctly rea lly ely let so ab co mp lut ely anger Modifier Figure 2: Relative frequencies of the most common 30 modifiers in the Twitter Corpus (from dark (infrequent) to yellow (frequent)."
W19-1304,D18-1059,0,0.0192459,"Association for Computational Linguistics cluding negations) with emotion words and show that not all such combinations are equally common. Secondly, we perform a crowdsourcing experiment in which we collect scores for different combinations of degree adverbs and emotion adjectives. We use these data, which we make publicly available, as an additional challenging test set for the task of intensity prediction for English. Thirdly, we use a state-of-the-art intensity prediction model (Wu et al., 2018) on this test set and evaluate two methods to improve these predictions, namely the inclusion (Zhao et al., 2018) and n-gram embeddings ` La Carte of additional subword information via A with Bag-of-Substrings (Khodak et al., 2018). We evaluate based on Word2vec, GloVe and fastText embeddings and show that particularly the first two benefit from these changes, but to different extents. 2 from Early Modern English and shows the how the distributions of boosters change across time. Nevalainen (2008) study the social variation in intensifier use, with a focus on the suffix -ly. More recently, Napoli and Ravetto (2017) collect a volume of papers that explore the process of intensification following a corpus-"
W19-1304,P14-1029,0,0.0186375,"with a focus on negated adjectives (Aina et al., 2018) is complementary to our work with regards to negation in terms of the methods and data used. Following this approach, one could build a distributional semantic model whose vocabulary includes the modified phrases. In practice, each occurrence of a modified adjective by a degree adverb could be treated as a single token (e. g. “not happy” would be represented as “not happy”). For a general overview of modality and negation in computational linguistics we refer the interested reader to the work by Morante and Sporleder (2012). Furthermore, Zhu et al. (2014) study the effect of negation words on sentiment and evaluate a neural composition model. Kiritchenko and Mohammad (2016a) create a sentiment lexicon of phrases that include modifiers such as negators, modals, and In this paper, we will use these terms interchangeably. 26 degree adverbs. The phrases and their constituent words are annotated manually with the same annotation procedure we will discuss in detail. We follow this work closely and apply the same procedures in the context of emotion analysis. Dragut and Fellbaum (2014) study the effect of intensifiers on the sentiment ratings and sho"
W19-1304,N18-1202,0,0.047333,"ion Word2vec joy Source embeddings (Mrkˇsi´c et al., 2016). Presumably, this will also generate additional insights into the aspect that we were only able to show a limited improvement based on retrofitting. Given the recent advances in representing contextualized word embeddings as functions computing dynamically the embeddings for words given their context, we hypothesize and intend to further verify that these embeddings would be a better choice for input to systems that predict intensity scores. It would be interesting to compare models such as word embeddings from language models (Elmo) (Peters et al., 2018), bidirectional encoder representations from transformers (BERT) (Devlin et al., 2018), and generative pre-training OpenAI (GPT) (Radford et al., 2019) to the ones we already discussed, since the contextualized embeddings assign a different vector for a word in each given context. These approaches presumably produce a different vector for “happy” in the context of “not” than in the content of “very” or “completely”. Lastly, we plan to also adjust the lexica created such that it covers more domains, sources, and languages. Conclusion & Future Work With this paper, we presented the first analysi"
W19-3406,D17-1169,0,0.0328058,"ly analyze the data, 3) we show that facial expressions, voice, eyes and body movements are the top three channels among which the emotion is expressed, 4) based on the data, we show that some emotions are more likely to be expressed via a certain channel, and this channel is also influenced by the presence or non-presence of a communication partner. Our corpus is available at https://www.ims. uni-stuttgart.de/data/emotion. 2 Related Work Emotion analysis has received great attention in natural language processing (Mohammad and Bravo-Marquez, 2017; Mohammad et al., 2018; Klinger et al., 2018; Felbo et al., 2017; AbdulMageed and Ungar, 2017; Zhou and Wang, 2018; Gui et al., 2017, i.a.). Most existing studies on the topic cast the problem of emotion analysis as a classification task, by classifying documents (e.g., social media posts) into a set of predefined emotion classes. Emotion classes used for the classification are usually based on discrete categories of Ekman (1970) or Plutchik (2001) (cf. 3 Corpus Creation We post-annotate our dataset of emotion relations between characters in fan fiction (Kim and 57 Appear. Look. Voice Gesture Sptrel. Sensations No channel Total Total Body anger anticipatio"
W19-3406,P17-1067,0,0.07055,"Missing"
W19-3406,D17-1167,0,0.0286835,"nd body movements are the top three channels among which the emotion is expressed, 4) based on the data, we show that some emotions are more likely to be expressed via a certain channel, and this channel is also influenced by the presence or non-presence of a communication partner. Our corpus is available at https://www.ims. uni-stuttgart.de/data/emotion. 2 Related Work Emotion analysis has received great attention in natural language processing (Mohammad and Bravo-Marquez, 2017; Mohammad et al., 2018; Klinger et al., 2018; Felbo et al., 2017; AbdulMageed and Ungar, 2017; Zhou and Wang, 2018; Gui et al., 2017, i.a.). Most existing studies on the topic cast the problem of emotion analysis as a classification task, by classifying documents (e.g., social media posts) into a set of predefined emotion classes. Emotion classes used for the classification are usually based on discrete categories of Ekman (1970) or Plutchik (2001) (cf. 3 Corpus Creation We post-annotate our dataset of emotion relations between characters in fan fiction (Kim and 57 Appear. Look. Voice Gesture Sptrel. Sensations No channel Total Total Body anger anticipation disgust fear joy sadness surprise trust Face Emotion 23 4 3 4 76 3"
W19-3406,H05-1073,0,0.574837,"Johnson-Laird and Oatley, 2016; Ingermanson and Economy, 2009), as well as story comCharacter Farrington Channel Physical sensations Emotion Anger His body ached to do something, to ... revel in violence. Figure 1: Example of the emotion expressed using non-verbal communication channel. The annotation of character and emotion are available in the dataset by Kim and Klinger (2019). Channel annotation (in blue) is an extension to the original dataset. 56 Proceedings of the Second Storytelling Workshop, pages 56–64 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics Alm et al. (2005), Suttles and Ide (2013), Mohammad (2012)). Fewer studies address emotion recognition using a dimensional emotion representation (cf. Buechel and Hahn (2017); Preot¸iucPietro et al. (2016)). Such representation is based on the valence-arousal emotion model (Russell, 1980), which can be helpful to account for subjective emotional states that do not fit into discrete categories. Early attempts to computationally model emotions in literary texts date back to the 1980s and are presented in the works by Anderson and McMaster (1982, 1986), who build a computational model of affect in text tracking h"
W19-3406,C18-1114,1,0.827034,"a wide range of goals. Some studies use emotions as feature input for genre classification (Samothrakis and Fasli, 2015; HennyKrahmer, 2018; Yu, 2008; Kim et al., 2017), story clustering (Reagan et al., 2016), mapping emotions to geographical locations in literature (Heuser et al., 2016), and construction of social networks of characters (Nalisnick and Baird, 2013; Jhavar and Mirza, 2018). Other studies use emotion analysis as a starting point for stylometry (Koolen, 2018), inferring psychological characters’ traits (Egloff et al., 2018), and analysis of the causes of emotions in literature (Kim and Klinger, 2018, 2019). To the best of our knowledge, there is no previous research that addresses the question of how emotions are expressed non-verbally. The only work that we are aware of is a literary study by van Meel (1995), who proposes several non-verbal communication channels for emotions and performs a manual analysis on a set of several books. He finds that voice is the most frequently used category, followed by facial expressions, arm and hand gestures and bodily postures. Van Meel explains the dominancy of voice by the predominant role that speech plays in novels. However, van Meel does not link"
W19-3406,W17-5205,0,0.129652,"Missing"
W19-3406,N19-1067,1,0.748006,"005) and a key component of every character is emotion, as “without emotion a character’s personal journey is pointless” (Ackerman and Puglisi, 2012, p. 1). Numerous works pinpoint the central role of emotions in storytelling (Hogan, 2015; Johnson-Laird and Oatley, 2016; Ingermanson and Economy, 2009), as well as story comCharacter Farrington Channel Physical sensations Emotion Anger His body ached to do something, to ... revel in violence. Figure 1: Example of the emotion expressed using non-verbal communication channel. The annotation of character and emotion are available in the dataset by Kim and Klinger (2019). Channel annotation (in blue) is an extension to the original dataset. 56 Proceedings of the Second Storytelling Workshop, pages 56–64 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics Alm et al. (2005), Suttles and Ide (2013), Mohammad (2012)). Fewer studies address emotion recognition using a dimensional emotion representation (cf. Buechel and Hahn (2017); Preot¸iucPietro et al. (2016)). Such representation is based on the valence-arousal emotion model (Russell, 1980), which can be helpful to account for subjective emotional states that do not fit into discre"
W19-3406,S18-1001,0,0.0347617,"Missing"
W19-3406,W17-2203,1,0.808454,"Missing"
W19-3406,W18-6206,1,0.902075,"Missing"
W19-3406,W18-1505,0,0.0614121,"Missing"
W19-3406,W16-0404,0,0.0725606,"Missing"
W19-3406,P18-1104,0,0.0231833,"ssions, voice, eyes and body movements are the top three channels among which the emotion is expressed, 4) based on the data, we show that some emotions are more likely to be expressed via a certain channel, and this channel is also influenced by the presence or non-presence of a communication partner. Our corpus is available at https://www.ims. uni-stuttgart.de/data/emotion. 2 Related Work Emotion analysis has received great attention in natural language processing (Mohammad and Bravo-Marquez, 2017; Mohammad et al., 2018; Klinger et al., 2018; Felbo et al., 2017; AbdulMageed and Ungar, 2017; Zhou and Wang, 2018; Gui et al., 2017, i.a.). Most existing studies on the topic cast the problem of emotion analysis as a classification task, by classifying documents (e.g., social media posts) into a set of predefined emotion classes. Emotion classes used for the classification are usually based on discrete categories of Ekman (1970) or Plutchik (2001) (cf. 3 Corpus Creation We post-annotate our dataset of emotion relations between characters in fan fiction (Kim and 57 Appear. Look. Voice Gesture Sptrel. Sensations No channel Total Total Body anger anticipation disgust fear joy sadness surprise trust Face Emo"
W19-3406,E17-2092,0,\N,Missing
