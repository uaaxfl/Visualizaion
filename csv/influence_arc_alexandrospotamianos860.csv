2021.naacl-main.203,D19-1371,0,0.0198913,"8) propose multi-source domain adversarial networks. Guo et al. (2018) propose a mixture-of-experts approach for multi-source UDA. Guo et al. (2020) explore distance measures as additional losses and use them to construct dynamic multi-armed bandit controller for the source domains. Shen et al. (2018) learn domain invariant features via Wasserstein distance. Bousmalis et al. (2016) introduce domain seperation networks with private and shared encoders. Unsupervised pretraining on domain-specific corpora can be an effective adaptation process. For example BioBERT (Lee et al., 2020) and SciBERT (Beltagy et al., 2019) are specialized BERT variants, where pretraining is extended on large amounts of biomedical and scientific corpora respectively. Sun et al. (2019) propose continuing the pretraining of BERT with target domain data and multitask learning using relevant tasks for BERT fine-tuning. Xu et al. (2019) introduce a review reading comprehension task and a post-training approach for BERT with an auxiliary loss on a question-answering task. Continuing pretraining on multiple phases, from general to domain specific (DAPT) and task specific data (TAPT), further improves performance of pretrained language"
2021.naacl-main.203,2020.tacl-1.33,0,0.0268131,"A research includes pivotbased methods, focusing on extracting crossdomain features. Structural Correspondence Learning (SCL) (Blitzer et al., 2006) and Spectral Feature Alignment (Pan et al., 2010) aim to find domaininvariant features (pivots) to learn a mapping between two domain distributions. Ziser and Reichart (2017, 2018, 2019) combine SCL with neural network architectures and language modeling. Miller (2019) propose to jointly learn the task and pivots. Li et al. (2018b) learn pivots with hierarchical attention networks. Pivot-based methods have also been used in conjunction with BERT (Ben-David et al., 2020). Domain adversarial training is a dominant approach for UDA (Ramponi and Plank, 2020), in1 https://github.com/ckarouzos/slp_daptmlm spired by the theory for learning from different domains introduced in Ben-David et al. (2007, 2010). Ganin et al. (2016); Ganin and Lempitsky (2015) propose to learn a task while not being able to distinguish if samples come from the source or the target distribution, through use of an adversarial cost. This approach has been adopted for a diverse set of problems, e.g. sentiment analysis, tweet classification and universal dependency parsing (Li et al., 2018a; A"
2021.naacl-main.203,P07-1056,0,0.479577,"Missing"
2021.naacl-main.203,W06-1615,0,0.146182,"ge Modeling Constantinos Karouzos1 , Georgios Paraskevopoulos1,4 , Alexandros Potamianos1,2,3 1 School of ECE, National Technical University of Athens, Athens, Greece Signal Analysis and Interpretation Laboratory (SAIL), USC, Los Angeles, CA, USA 3 Behavioral Signal Technologies, Los Angeles, CA, USA 4 Institute for Language and Speech Processing, Athena Research Center, Athens, Greece ckarouzos@gmail.com, geopar@central.ntua.gr, potam@central.ntua.gr 2 Abstract 2005), domain adversarial training (e.g. Ganin et al., 2016) and pivot-based approaches (e.g. In this work we explore Unsupervised DoBlitzer et al., 2006; Pan et al., 2010). Pseudomain Adaptation (UDA) of pretrained lanlabeling approaches use a model trained on the guage models for downstream tasks. We introsource labeled data to produce pseudo-labels for duce UDALM, a fine-tuning procedure, using unlabeled target data and then train a model for a mixed classification and Masked Language Model loss, that can adapt to the target dothe target domain in a supervised manner. Domain distribution in a robust and sample effimain adversarial training aims to learn a domaincient manner. Our experiments show that perindependent mapping for input samples"
2021.naacl-main.203,N19-1213,1,0.813145,"omprehension task and a post-training approach for BERT with an auxiliary loss on a question-answering task. Continuing pretraining on multiple phases, from general to domain specific (DAPT) and task specific data (TAPT), further improves performance of pretrained language models, as shown by Gururangan et al. (2020). Han and Eisenstein (2019) propose AdaptaBERT, which includes a second phase of unsupervised pretraining, in order to use BERT in a unsupervised domain adaptation context. Recent works have highlighted the merits of using Language Modeling as an auxiliary task during fine-tuning. Chronopoulou et al. (2019) use an auxiliary LM loss to avoid catastrophic forgetting in transfer learning and Jia et al. (2019) adopt this approach for cross-domain named-entity recognition. We draw inspiration from these approaches and utilize auxiliary Language Modeling for UDA. 2580 (a) (b) (c) Figure 1: (a) BERT (Devlin et al., 2019) is pretrained on English Wikipedia and BookCorpus with the Masked Language Modeling (MLM) and the Next Sentence Prediction (NSP) tasks. (b) We continue the pretraining of BERT on unlabeled target domain data using the MLM task. (c) We train a task classifier with source domain labeled"
2021.naacl-main.203,2020.acl-main.747,0,0.0410626,"seudo-labeling techniques are semi-supervised algorithms that either use the same model (self-training) (Yarowsky, 1995; McClosky et al., 2006; Abney, 2007) or multiple ensembles of models (tri-training) (Zhou and Li, 2005; Søgaard, 2010) in order to produce pseudo-labels for the target unlabeled data. Saito et al. (2017) proposed an asymmetric tri-training approach. Ruder and Plank (2018) introduced a multi-task tri-training method. Rotman and Reichart (2019) and Lim et al. (2020) study pseudolabeling with contextualized word representations. Ye et al. (2020) combine self-training with XLMR (Conneau et al., 2020) to reduce the produced label noise and propose CFd, class aware feature self-distillation. Another line of UDA research includes pivotbased methods, focusing on extracting crossdomain features. Structural Correspondence Learning (SCL) (Blitzer et al., 2006) and Spectral Feature Alignment (Pan et al., 2010) aim to find domaininvariant features (pivots) to learn a mapping between two domain distributions. Ziser and Reichart (2017, 2018, 2019) combine SCL with neural network architectures and language modeling. Miller (2019) propose to jointly learn the task and pivots. Li et al. (2018b) learn p"
2021.naacl-main.203,N19-1423,0,0.0126522,"Han and Eisenstein (2019) propose AdaptaBERT, which includes a second phase of unsupervised pretraining, in order to use BERT in a unsupervised domain adaptation context. Recent works have highlighted the merits of using Language Modeling as an auxiliary task during fine-tuning. Chronopoulou et al. (2019) use an auxiliary LM loss to avoid catastrophic forgetting in transfer learning and Jia et al. (2019) adopt this approach for cross-domain named-entity recognition. We draw inspiration from these approaches and utilize auxiliary Language Modeling for UDA. 2580 (a) (b) (c) Figure 1: (a) BERT (Devlin et al., 2019) is pretrained on English Wikipedia and BookCorpus with the Masked Language Modeling (MLM) and the Next Sentence Prediction (NSP) tasks. (b) We continue the pretraining of BERT on unlabeled target domain data using the MLM task. (c) We train a task classifier with source domain labeled data, while we keep the MLM objective on unlabeled target domain data. 3 Problem Definition Let X be the input space and Y the set of labels. For binary classification tasks Y = {0, 1}. In domain adaptation there are two different distributions over X × Y , called the source domain DS and the target domain DT ."
2021.naacl-main.203,2020.acl-main.370,0,0.0225161,"inant approach for UDA (Ramponi and Plank, 2020), in1 https://github.com/ckarouzos/slp_daptmlm spired by the theory for learning from different domains introduced in Ben-David et al. (2007, 2010). Ganin et al. (2016); Ganin and Lempitsky (2015) propose to learn a task while not being able to distinguish if samples come from the source or the target distribution, through use of an adversarial cost. This approach has been adopted for a diverse set of problems, e.g. sentiment analysis, tweet classification and universal dependency parsing (Li et al., 2018a; Alam et al., 2018; Sato et al., 2017). Du et al. (2020) pose domain adversarial training in the context of BERT models. Zhao et al. (2018) propose multi-source domain adversarial networks. Guo et al. (2018) propose a mixture-of-experts approach for multi-source UDA. Guo et al. (2020) explore distance measures as additional losses and use them to construct dynamic multi-armed bandit controller for the source domains. Shen et al. (2018) learn domain invariant features via Wasserstein distance. Bousmalis et al. (2016) introduce domain seperation networks with private and shared encoders. Unsupervised pretraining on domain-specific corpora can be an e"
2021.naacl-main.203,D18-1498,0,0.0361218,"Missing"
2021.naacl-main.203,2020.acl-main.740,0,0.035794,"Missing"
2021.naacl-main.203,D19-1433,0,0.0186779,"ing is extended on large amounts of biomedical and scientific corpora respectively. Sun et al. (2019) propose continuing the pretraining of BERT with target domain data and multitask learning using relevant tasks for BERT fine-tuning. Xu et al. (2019) introduce a review reading comprehension task and a post-training approach for BERT with an auxiliary loss on a question-answering task. Continuing pretraining on multiple phases, from general to domain specific (DAPT) and task specific data (TAPT), further improves performance of pretrained language models, as shown by Gururangan et al. (2020). Han and Eisenstein (2019) propose AdaptaBERT, which includes a second phase of unsupervised pretraining, in order to use BERT in a unsupervised domain adaptation context. Recent works have highlighted the merits of using Language Modeling as an auxiliary task during fine-tuning. Chronopoulou et al. (2019) use an auxiliary LM loss to avoid catastrophic forgetting in transfer learning and Jia et al. (2019) adopt this approach for cross-domain named-entity recognition. We draw inspiration from these approaches and utilize auxiliary Language Modeling for UDA. 2580 (a) (b) (c) Figure 1: (a) BERT (Devlin et al., 2019) is pr"
2021.naacl-main.203,P18-1031,0,0.0673523,"Missing"
2021.naacl-main.203,P19-1236,0,0.0282034,"ontinuing pretraining on multiple phases, from general to domain specific (DAPT) and task specific data (TAPT), further improves performance of pretrained language models, as shown by Gururangan et al. (2020). Han and Eisenstein (2019) propose AdaptaBERT, which includes a second phase of unsupervised pretraining, in order to use BERT in a unsupervised domain adaptation context. Recent works have highlighted the merits of using Language Modeling as an auxiliary task during fine-tuning. Chronopoulou et al. (2019) use an auxiliary LM loss to avoid catastrophic forgetting in transfer learning and Jia et al. (2019) adopt this approach for cross-domain named-entity recognition. We draw inspiration from these approaches and utilize auxiliary Language Modeling for UDA. 2580 (a) (b) (c) Figure 1: (a) BERT (Devlin et al., 2019) is pretrained on English Wikipedia and BookCorpus with the Masked Language Modeling (MLM) and the Next Sentence Prediction (NSP) tasks. (b) We continue the pretraining of BERT on unlabeled target domain data using the MLM task. (c) We train a task classifier with source domain labeled data, while we keep the MLM objective on unlabeled target domain data. 3 Problem Definition Let X be"
2021.naacl-main.203,N18-2076,0,0.0588091,"Missing"
2021.naacl-main.203,2021.ccl-1.108,0,0.0534461,"Missing"
2021.naacl-main.203,P06-1043,0,0.132578,"els based on multitask learning, (b) we achieve state-of-the-art results for the Amazon reviews benchmark dataset, surpassing more complicated approaches and (c) we explore how Adistance and the target error are related and conclude with some remarks on domain adversarial training, based on theoretical concepts and our empirical observations. Our code and models are publicly available1 . 2 Related Work Traditionally, UDA has been performed using pseudo-labeling approaches. Pseudo-labeling techniques are semi-supervised algorithms that either use the same model (self-training) (Yarowsky, 1995; McClosky et al., 2006; Abney, 2007) or multiple ensembles of models (tri-training) (Zhou and Li, 2005; Søgaard, 2010) in order to produce pseudo-labels for the target unlabeled data. Saito et al. (2017) proposed an asymmetric tri-training approach. Ruder and Plank (2018) introduced a multi-task tri-training method. Rotman and Reichart (2019) and Lim et al. (2020) study pseudolabeling with contextualized word representations. Ye et al. (2020) combine self-training with XLMR (Conneau et al., 2020) to reduce the produced label noise and propose CFd, class aware feature self-distillation. Another line of UDA research"
2021.naacl-main.203,N19-1039,0,0.0148468,"resentations. Ye et al. (2020) combine self-training with XLMR (Conneau et al., 2020) to reduce the produced label noise and propose CFd, class aware feature self-distillation. Another line of UDA research includes pivotbased methods, focusing on extracting crossdomain features. Structural Correspondence Learning (SCL) (Blitzer et al., 2006) and Spectral Feature Alignment (Pan et al., 2010) aim to find domaininvariant features (pivots) to learn a mapping between two domain distributions. Ziser and Reichart (2017, 2018, 2019) combine SCL with neural network architectures and language modeling. Miller (2019) propose to jointly learn the task and pivots. Li et al. (2018b) learn pivots with hierarchical attention networks. Pivot-based methods have also been used in conjunction with BERT (Ben-David et al., 2020). Domain adversarial training is a dominant approach for UDA (Ramponi and Plank, 2020), in1 https://github.com/ckarouzos/slp_daptmlm spired by the theory for learning from different domains introduced in Ben-David et al. (2007, 2010). Ganin et al. (2016); Ganin and Lempitsky (2015) propose to learn a task while not being able to distinguish if samples come from the source or the target distri"
2021.naacl-main.203,2020.coling-main.603,0,0.0219625,"Structural Correspondence Learning (SCL) (Blitzer et al., 2006) and Spectral Feature Alignment (Pan et al., 2010) aim to find domaininvariant features (pivots) to learn a mapping between two domain distributions. Ziser and Reichart (2017, 2018, 2019) combine SCL with neural network architectures and language modeling. Miller (2019) propose to jointly learn the task and pivots. Li et al. (2018b) learn pivots with hierarchical attention networks. Pivot-based methods have also been used in conjunction with BERT (Ben-David et al., 2020). Domain adversarial training is a dominant approach for UDA (Ramponi and Plank, 2020), in1 https://github.com/ckarouzos/slp_daptmlm spired by the theory for learning from different domains introduced in Ben-David et al. (2007, 2010). Ganin et al. (2016); Ganin and Lempitsky (2015) propose to learn a task while not being able to distinguish if samples come from the source or the target distribution, through use of an adversarial cost. This approach has been adopted for a diverse set of problems, e.g. sentiment analysis, tweet classification and universal dependency parsing (Li et al., 2018a; Alam et al., 2018; Sato et al., 2017). Du et al. (2020) pose domain adversarial trainin"
2021.naacl-main.203,Q19-1044,0,0.0130535,"our empirical observations. Our code and models are publicly available1 . 2 Related Work Traditionally, UDA has been performed using pseudo-labeling approaches. Pseudo-labeling techniques are semi-supervised algorithms that either use the same model (self-training) (Yarowsky, 1995; McClosky et al., 2006; Abney, 2007) or multiple ensembles of models (tri-training) (Zhou and Li, 2005; Søgaard, 2010) in order to produce pseudo-labels for the target unlabeled data. Saito et al. (2017) proposed an asymmetric tri-training approach. Ruder and Plank (2018) introduced a multi-task tri-training method. Rotman and Reichart (2019) and Lim et al. (2020) study pseudolabeling with contextualized word representations. Ye et al. (2020) combine self-training with XLMR (Conneau et al., 2020) to reduce the produced label noise and propose CFd, class aware feature self-distillation. Another line of UDA research includes pivotbased methods, focusing on extracting crossdomain features. Structural Correspondence Learning (SCL) (Blitzer et al., 2006) and Spectral Feature Alignment (Pan et al., 2010) aim to find domaininvariant features (pivots) to learn a mapping between two domain distributions. Ziser and Reichart (2017, 2018, 201"
2021.naacl-main.203,P18-1096,0,0.0920359,"s on domain adversarial training, based on theoretical concepts and our empirical observations. Our code and models are publicly available1 . 2 Related Work Traditionally, UDA has been performed using pseudo-labeling approaches. Pseudo-labeling techniques are semi-supervised algorithms that either use the same model (self-training) (Yarowsky, 1995; McClosky et al., 2006; Abney, 2007) or multiple ensembles of models (tri-training) (Zhou and Li, 2005; Søgaard, 2010) in order to produce pseudo-labels for the target unlabeled data. Saito et al. (2017) proposed an asymmetric tri-training approach. Ruder and Plank (2018) introduced a multi-task tri-training method. Rotman and Reichart (2019) and Lim et al. (2020) study pseudolabeling with contextualized word representations. Ye et al. (2020) combine self-training with XLMR (Conneau et al., 2020) to reduce the produced label noise and propose CFd, class aware feature self-distillation. Another line of UDA research includes pivotbased methods, focusing on extracting crossdomain features. Structural Correspondence Learning (SCL) (Blitzer et al., 2006) and Spectral Feature Alignment (Pan et al., 2010) aim to find domaininvariant features (pivots) to learn a mappi"
2021.naacl-main.203,K17-3007,0,0.0280412,"al training is a dominant approach for UDA (Ramponi and Plank, 2020), in1 https://github.com/ckarouzos/slp_daptmlm spired by the theory for learning from different domains introduced in Ben-David et al. (2007, 2010). Ganin et al. (2016); Ganin and Lempitsky (2015) propose to learn a task while not being able to distinguish if samples come from the source or the target distribution, through use of an adversarial cost. This approach has been adopted for a diverse set of problems, e.g. sentiment analysis, tweet classification and universal dependency parsing (Li et al., 2018a; Alam et al., 2018; Sato et al., 2017). Du et al. (2020) pose domain adversarial training in the context of BERT models. Zhao et al. (2018) propose multi-source domain adversarial networks. Guo et al. (2018) propose a mixture-of-experts approach for multi-source UDA. Guo et al. (2020) explore distance measures as additional losses and use them to construct dynamic multi-armed bandit controller for the source domains. Shen et al. (2018) learn domain invariant features via Wasserstein distance. Bousmalis et al. (2016) introduce domain seperation networks with private and shared encoders. Unsupervised pretraining on domain-specific c"
2021.naacl-main.203,D18-1131,0,0.0283782,"t towards synergistic learning of the supervised source task and the target domain distribution. 7.3 Limitations of Domain Adversarial Training Domain adversarial training (Ganin et al., 2016) faces some critical limitations that make the method difficult to be reproduced due to high hyperparameter sensitivity and instability during training. Such limitations have been highlighted by other authors in the UDA literature. For example, according to Shen et al. (2018) when a domain classifier can perfectly distinguish target from source representations, there will be a gradient vanishing problem. Shah et al. (2018) state that domain adversarial training is unstable and needs careful hyperparameter tuning for their experiments. Wang et al. (2020) report results over three multi-domain NLP datasets, where domain adversarial training in conjunction with BERT under-performs. Ruder and Plank (2018) found that the domain adversarial loss did not help for their experiments on the Amazon reviews dataset. In our experiments we note that domainadversarial training results to worse performance than naive source only training. Furthermore, we experienced the need for extensive tuning of the λd parameter from Eq. 4"
2021.naacl-main.203,P10-2038,0,0.0365338,"rk dataset, surpassing more complicated approaches and (c) we explore how Adistance and the target error are related and conclude with some remarks on domain adversarial training, based on theoretical concepts and our empirical observations. Our code and models are publicly available1 . 2 Related Work Traditionally, UDA has been performed using pseudo-labeling approaches. Pseudo-labeling techniques are semi-supervised algorithms that either use the same model (self-training) (Yarowsky, 1995; McClosky et al., 2006; Abney, 2007) or multiple ensembles of models (tri-training) (Zhou and Li, 2005; Søgaard, 2010) in order to produce pseudo-labels for the target unlabeled data. Saito et al. (2017) proposed an asymmetric tri-training approach. Ruder and Plank (2018) introduced a multi-task tri-training method. Rotman and Reichart (2019) and Lim et al. (2020) study pseudolabeling with contextualized word representations. Ye et al. (2020) combine self-training with XLMR (Conneau et al., 2020) to reduce the produced label noise and propose CFd, class aware feature self-distillation. Another line of UDA research includes pivotbased methods, focusing on extracting crossdomain features. Structural Corresponde"
2021.naacl-main.203,2020.emnlp-main.250,0,0.0184796,"l Training Domain adversarial training (Ganin et al., 2016) faces some critical limitations that make the method difficult to be reproduced due to high hyperparameter sensitivity and instability during training. Such limitations have been highlighted by other authors in the UDA literature. For example, according to Shen et al. (2018) when a domain classifier can perfectly distinguish target from source representations, there will be a gradient vanishing problem. Shah et al. (2018) state that domain adversarial training is unstable and needs careful hyperparameter tuning for their experiments. Wang et al. (2020) report results over three multi-domain NLP datasets, where domain adversarial training in conjunction with BERT under-performs. Ruder and Plank (2018) found that the domain adversarial loss did not help for their experiments on the Amazon reviews dataset. In our experiments we note that domainadversarial training results to worse performance than naive source only training. Furthermore, we experienced the need for extensive tuning of the λd parameter from Eq. 4 every time the experimental setting changed (e.g. when testing for different amounts of available target data as in Section 6.2). Thi"
2021.naacl-main.203,P95-1026,0,0.824585,"pact, because they do not rely on expensive and on simultaneously learning the task from labeled time-consuming annotation processes to collect la- data in the source distribution, while adapting to beled data for domain-specific supervised training, the language in the target distribution using multifurther streamlining the process. task learning. The key idea of our method is that UDA approaches in the literature can be grouped by simultaneously minimizing a task-specific loss in three major categories, namely pseudo-labeling on the source data and a language modeling loss techniques (e.g. Yarowsky, 1995; Zhou and Li, on the target data during fine-tuning, the model 2579 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2579–2590 June 6–11, 2021. ©2021 Association for Computational Linguistics will be able to adapt to the language of the target domain, while learning the supervised task from the available labeled data. Our key contributions are: (a) We introduce UDALM, a novel, simple and robust unsupervised domain adaptation procedure for downstream BERT models based on multitask learning, (b)"
2021.naacl-main.203,K17-1040,0,0.01892,"method. Rotman and Reichart (2019) and Lim et al. (2020) study pseudolabeling with contextualized word representations. Ye et al. (2020) combine self-training with XLMR (Conneau et al., 2020) to reduce the produced label noise and propose CFd, class aware feature self-distillation. Another line of UDA research includes pivotbased methods, focusing on extracting crossdomain features. Structural Correspondence Learning (SCL) (Blitzer et al., 2006) and Spectral Feature Alignment (Pan et al., 2010) aim to find domaininvariant features (pivots) to learn a mapping between two domain distributions. Ziser and Reichart (2017, 2018, 2019) combine SCL with neural network architectures and language modeling. Miller (2019) propose to jointly learn the task and pivots. Li et al. (2018b) learn pivots with hierarchical attention networks. Pivot-based methods have also been used in conjunction with BERT (Ben-David et al., 2020). Domain adversarial training is a dominant approach for UDA (Ramponi and Plank, 2020), in1 https://github.com/ckarouzos/slp_daptmlm spired by the theory for learning from different domains introduced in Ben-David et al. (2007, 2010). Ganin et al. (2016); Ganin and Lempitsky (2015) propose to learn"
2021.naacl-main.203,N18-1112,0,0.0467506,"Missing"
2021.naacl-main.203,P19-1591,0,0.0394767,"Missing"
2021.naacl-main.203,N19-1242,0,0.0247472,"learn domain invariant features via Wasserstein distance. Bousmalis et al. (2016) introduce domain seperation networks with private and shared encoders. Unsupervised pretraining on domain-specific corpora can be an effective adaptation process. For example BioBERT (Lee et al., 2020) and SciBERT (Beltagy et al., 2019) are specialized BERT variants, where pretraining is extended on large amounts of biomedical and scientific corpora respectively. Sun et al. (2019) propose continuing the pretraining of BERT with target domain data and multitask learning using relevant tasks for BERT fine-tuning. Xu et al. (2019) introduce a review reading comprehension task and a post-training approach for BERT with an auxiliary loss on a question-answering task. Continuing pretraining on multiple phases, from general to domain specific (DAPT) and task specific data (TAPT), further improves performance of pretrained language models, as shown by Gururangan et al. (2020). Han and Eisenstein (2019) propose AdaptaBERT, which includes a second phase of unsupervised pretraining, in order to use BERT in a unsupervised domain adaptation context. Recent works have highlighted the merits of using Language Modeling as an auxili"
C14-1069,N09-1003,0,0.0789359,"Missing"
C14-1069,J10-4006,0,0.584858,"one-dimensional manifold. Following the low-dimensional manifold hypothesis we propose to extend distributional semantic models (DSMs) into a hierarchical model of domains (or concepts) that contain semantically similar words. Global operations on the lexical space are decomposed into local operations on the low-dimensional domain sub-manifolds. Our goal is to exploit this hierarchical low-rank model to estimate relations between words, such as semantic similarity. There has been much research interest on devising data-driven approaches for estimating semantic similarity between words. DSMs (Baroni and Lenci, 2010) are based on the distributional hypothesis of meaning (Harris, 1954) assuming that semantic similarity between words is a function of the overlap of their linguistic contexts. DSMs are typically constructed from co-occurrence statistics of word tuples that are extracted on existing corpora or on corpora specifically harvested from the web. In (Iosif and Potamianos, 2013), general-purpose, language-agnostic algorithms were proposed for estimating semantic similarity using no linguistic resources other than a corpus created via web queries. The key idea of this work was the construction of sema"
C18-1243,J10-4006,0,0.0604865,"Missing"
C18-1243,P14-1023,0,0.109423,"Missing"
C18-1243,D15-1075,0,0.0809763,"Missing"
C18-1243,L16-1195,1,0.794019,"Missing"
C18-1243,W10-0603,0,0.0704754,"Missing"
C18-1243,S12-1019,0,0.0495509,"Missing"
C18-1243,D14-1162,0,0.0934053,"Missing"
C18-1243,D16-1064,0,0.0279947,"Missing"
C18-1243,D11-1014,0,0.0542037,"Missing"
C18-1243,D13-1170,0,0.00963444,"Missing"
C18-1243,D14-1160,0,0.0683539,"Missing"
C18-1243,P10-1040,0,0.0794945,"Missing"
E17-2093,D11-1014,0,0.0975242,"nit without the need of a separate memory cell. The activation hj of TreeGRU for node j is the interpolation of the previous calculated activation hjk of its kth child out of N total children and the candidate activation e hj . hj = zj ∗ N X hjk + (1 − zj ) ∗ e hj (1) k=1 where zj is the update function which decide the degree of update that will occur on the activation based on the input vector xj and previously calculated representation hjk : zj = σ(Uz ∗ xj + N X Wzi ∗ hjk ) (2) k=1 The candidate activation e hj for a node j is computed similarly to that of a Recursive Neural Network as in (Socher et al., 2011): e hj = f (Uh ∗ xj + N X Whk ∗ (hjk ∗ rj )) Bidirectional TreeGRU j (3) k=1 h↑j = zj↑ ∗ where rj is the reset gate which allows the network to forget effectively previous computed representations when the value is close to 0 and it is computed as follows: rj = σ(Ur ∗ xj + N X Wrk ∗ hjk ) N X k=1 h↑jk + (1 − zj↑ ) ∗ e h↑j (5) The update gate, rest gate and candidate activation are computed as follows: (4) zj↑ k=1 Every part of a gated recurrent unit xj , hj , rj , zj , e hj ∈ Rd where d is the input vector dimensionality. σ is the sigmoid function and f is the non-linear tanh function.The set"
E17-2093,D12-1110,0,0.424391,"Missing"
E17-2093,P14-1062,0,0.10376,"Missing"
E17-2093,D14-1181,0,0.0629902,"Missing"
E17-2093,D13-1170,0,0.148232,"unknown and the seed words. An example of sentence-level analysis was proposed in (Malandrakis et al., 2013). Other application areas include the detection of public opinion and prediction of election results (Singhal et al., 2015), correlation of mood states and stock market indices (Bollen et al., 2011). Recently, Recurrent Neural Network (RNN) with Long-Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) or Gated Recurrent Units (GRU) (Chung et al., 2014) have been We evaluate our approach on the sentence-level sentiment classification task using one standard movie review dataset (Socher et al., 2013). Experimental results show that the proposed model outperforms the state-of-the art methods. 586 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 586–591, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics 2 Tree-Structured GRUs 2.1 A natural extension of Tree-Structure GRU is the addition of a bidirectional approach. TreeGRUs calculate an activation for node j with the use of previously computed activations lying lower in the tree structure. The bidirectional approach for"
E17-2093,P15-1150,0,0.255453,"Missing"
E17-2093,D15-1166,0,0.0544941,"t the parent representation p(j). 2.2 Sentiment Classification For all of the aforementioned architectures at each node j we use a softmax classifier to predict the sentiment label yˆj . For example, the predicted label yˆj corresponds to the sentiment class of the spanned phrase produced from node j. The classifier for unidirectional TreeGRU architectures uses the hidden state hj produced from recursive computations till node j using a set xj of input nodes to predict the label as follows: Structural Attention We introduce Structural Attention, a generalization of sequential attention model (Luong et al., 2015) which extracts informative nodes out of a syntactic tree and aggregates the representation of those nodes in order to form the sentence vector. We feed representation hj of node through a one-layer Multilayer Perceptron with Ww ∈ Rdxd weight matrix to get the hidden representation uj . uj = tanh(Ww ∗ hj ) Experiments pˆθ (y|xj ) = sof tmax(Ws ∗ hj ) (16) where Ws ∈ Rdxc and c is the number of sentiment classes. The classifier for bidirectional TreeBiGRU architectures uses both the hidden state h↑j and h↓j produced from recursive computations till node j during Upward and Downward Phase using"
E17-2093,S13-2072,1,\N,Missing
iosif-etal-2012-associative,J90-1003,0,\N,Missing
iosif-etal-2012-associative,W06-2501,0,\N,Missing
iosif-etal-2012-associative,C92-2082,0,\N,Missing
iosif-etal-2012-associative,P99-1016,0,\N,Missing
iosif-etal-2012-associative,O97-1002,0,\N,Missing
iosif-etal-2012-associative,J92-4003,0,\N,Missing
iosif-etal-2012-associative,P94-1019,0,\N,Missing
iosif-etal-2012-associative,J06-1003,0,\N,Missing
iosif-etal-2012-associative,P08-1017,0,\N,Missing
iosif-etal-2012-associative,C08-1114,0,\N,Missing
iosif-etal-2012-associative,N03-1011,0,\N,Missing
iosif-potamianos-2012-semsim,J06-1003,0,\N,Missing
L16-1016,W15-4647,1,0.827742,"ous turns that were given a new value either by the speaker or the system. 4.3.2. Method Given that the main purpose of Spoken Dialogue Systems analytics is to provide tools to detect problems off-line, we will report the results of the offline model. This model was trained with all possible features available (including those derived from manual annotations) from each four turn snippet taken into account. The method developed was only applied for turns annotated either as PROBLEMATIC or NON-PROBLEMATIC. Given the skew of the distribution and to obtain comparable results to those reported in (Meena et al., 2015), we report the results in terms of UAR (Schmitt et al., 2011), for this the baseline majority will be 0.5 for all the datasets regardless of the distribution of the data. Several machine learning algorithms were explored both in Weka (Hall et al., 2009) and sklearn toolkit (Pedregosa et al., 2011). For validation purposes, instead of the 10-fold cross-validation scheme used in (Meena et al., 2015) leave-dialogue-out validation scheme was adopted to avoid that samples from the same dialogue could fall both in train and test sets for a given fold. The following sections report the results both"
L16-1016,N04-1006,0,0.0227089,"cess for spoken dialogue service development and speech service enhancement of deployed services. Our main goal was to provide a tool where incoming speech for the system is semi-automatically analyzed (human-inthe-loop) to identify sources of problems in the dialogue. The first step towards this goal was to build tools that automatically identify problematic dialogue situations or as we will call hereafter miscommunications. The automatic detection of miscommunications in SDSs has been extensively investigated in the literature (Walker et al., 2000; Swerts et al., 2000; Schmitt et al., 2010; Paek and Horvitz, 2004). This problem is vital in the development cycle of speech services. However, very little data is publicly available to perform research on this topic. One of the exceptions is (Swerts et al., 2000), but even in this case the dataset does not contain interactions with real users or annotations. In (Schmitt et al., 2012) a dataset collected with real users is described. This data was annotated for interaction quality (Schmitt et al., 2011), emotions and contains also a variety of automatically extracted features. This dataset was built with dialogues from CMU Let’s Go (Raux et al., 2005) system"
L16-1016,W11-2020,0,0.128198,"detection of miscommunications in SDSs has been extensively investigated in the literature (Walker et al., 2000; Swerts et al., 2000; Schmitt et al., 2010; Paek and Horvitz, 2004). This problem is vital in the development cycle of speech services. However, very little data is publicly available to perform research on this topic. One of the exceptions is (Swerts et al., 2000), but even in this case the dataset does not contain interactions with real users or annotations. In (Schmitt et al., 2012) a dataset collected with real users is described. This data was annotated for interaction quality (Schmitt et al., 2011), emotions and contains also a variety of automatically extracted features. This dataset was built with dialogues from CMU Let’s Go (Raux et al., 2005) system from 2006, which performance and architecture are substantially different than the current Let’s Go system. In addition, the interaction quality might not be the most suitable measure for identifying problematic dialogue situations, namely if severe problems occur in the very first exchange of the interaction. Recently, more Let’s Go data was made available for the Spoken Dialogue Challenge (Black et al., 2010). Although part of the data"
L16-1016,schmitt-etal-2012-parameterized,0,0.4603,"hat automatically identify problematic dialogue situations or as we will call hereafter miscommunications. The automatic detection of miscommunications in SDSs has been extensively investigated in the literature (Walker et al., 2000; Swerts et al., 2000; Schmitt et al., 2010; Paek and Horvitz, 2004). This problem is vital in the development cycle of speech services. However, very little data is publicly available to perform research on this topic. One of the exceptions is (Swerts et al., 2000), but even in this case the dataset does not contain interactions with real users or annotations. In (Schmitt et al., 2012) a dataset collected with real users is described. This data was annotated for interaction quality (Schmitt et al., 2011), emotions and contains also a variety of automatically extracted features. This dataset was built with dialogues from CMU Let’s Go (Raux et al., 2005) system from 2006, which performance and architecture are substantially different than the current Let’s Go system. In addition, the interaction quality might not be the most suitable measure for identifying problematic dialogue situations, namely if severe problems occur in the very first exchange of the interaction. Recently"
L16-1016,A00-2028,0,0.114449,"consortium (www.spedial.eu) worked to create a semiautomated process for spoken dialogue service development and speech service enhancement of deployed services. Our main goal was to provide a tool where incoming speech for the system is semi-automatically analyzed (human-inthe-loop) to identify sources of problems in the dialogue. The first step towards this goal was to build tools that automatically identify problematic dialogue situations or as we will call hereafter miscommunications. The automatic detection of miscommunications in SDSs has been extensively investigated in the literature (Walker et al., 2000; Swerts et al., 2000; Schmitt et al., 2010; Paek and Horvitz, 2004). This problem is vital in the development cycle of speech services. However, very little data is publicly available to perform research on this topic. One of the exceptions is (Swerts et al., 2000), but even in this case the dataset does not contain interactions with real users or annotations. In (Schmitt et al., 2012) a dataset collected with real users is described. This data was annotated for interaction quality (Schmitt et al., 2011), emotions and contains also a variety of automatically extracted features. This dataset w"
L16-1195,S12-1051,0,0.0784629,"Missing"
L16-1195,C14-1069,1,0.809796,"sented in (Dinu et al., 2013). In (Turney, 2012), the compositionality task is investigated in the space of similarities instead of features. Specifically, two distinct models are built, namely, domain and function spaces. Despite the fact that meaning is at the core of human cognition, incorporating findings from the area of semantic cognition into the design of DSMs is not straightforward. The relation between cognitive semantics and the information encoded by DSMs is discussed in (Lenci, 2008). Motivated by evidence related to the low-dimensional geometry of thought (G¨ardenfors, 2004) in (Athanasopoulou et al., 2014) word-level semantics are locally represented via a series of manifolds, while global operations (e.g., computation of word semantic similarity) are decomposed into local (i.e., manifold-level) operations. In (Georgiladakis et al., 2015) a two-layer model motivated by semantic priming (McNamara, 2005) and the dual cognitive processing theory (Kahneman, 2013) was adopted for representing the semantics of compositional phrases. Other related approaches include the mapping of DSMs to feature-normed spaces (F˘ag˘ar˘as¸an et al., 2015), the extension of word embeddings with cognitively motivated le"
L16-1195,D10-1115,0,0.0443876,"ile they are meant to address a number of properties that characterize the compositional aspects of meaning, namely, “linguistic creativity”, “order sensitivity”, “adaptive capacity”, and “information scalability” (Turney, 2012). The work presented in (Mitchell and Lapata, 2010) constitutes one 1226 of the earlier approaches in the area of compositional DSMs, where a series of algebraic operations are applied over the word-level feature vectors for representing the semantics of two-word phrases divided in three categories, namely, noun-noun (NN), adjective-noun (AN), and verb-object (VO). In (Baroni and Zamparelli, 2010), the compositional aspects of AN phrases were investigated where the semantic transformation triggered by adjectives was modeled via a function (implemented as matrix) operating over the vectorial representation of nouns. The phrase-level approach of (Baroni and Zamparelli, 2010) was generalized in (Socher et al., 2012) according to which sentence-level representations are constructed in a recursive bottom-up fashion. A comparison of the major compositional approaches reported in the literature is presented in (Dinu et al., 2013). In (Turney, 2012), the compositionality task is investigated i"
L16-1195,P14-1023,0,0.543402,"mance was achieved by unstructured DSMs. The multiple senses of words are typically not directly encoded in DSMs. For addressing this issue, exemplar models were proposed where the meaning of a word was represented by a set of stereotypical corpus sentences instead of a single feature vector (Erk and Pad´o, 2010). Recently, the computation of contextual features was posed in a learning-based framework where the goal is to estimate the context in which the words of interest are expected to occur (Bengio et al., 2003; Huang et al., 2012; Mikolov et al., 2013a). This advancement is discussed in (Baroni et al., 2014), where it is compared with traditional DSMs for various tasks of lexical semantics. A similar comparative study is also presented in (Huang et al., 2012). Word-level representations are the building blocks for phrase- and sentence-level models. Such models are motivated by the principle of semantic compositionality (Pelletier, 1994), while they are meant to address a number of properties that characterize the compositional aspects of meaning, namely, “linguistic creativity”, “order sensitivity”, “adaptive capacity”, and “information scalability” (Turney, 2012). The work presented in (Mitchell"
L16-1195,W13-3206,0,0.0182737,"(NN), adjective-noun (AN), and verb-object (VO). In (Baroni and Zamparelli, 2010), the compositional aspects of AN phrases were investigated where the semantic transformation triggered by adjectives was modeled via a function (implemented as matrix) operating over the vectorial representation of nouns. The phrase-level approach of (Baroni and Zamparelli, 2010) was generalized in (Socher et al., 2012) according to which sentence-level representations are constructed in a recursive bottom-up fashion. A comparison of the major compositional approaches reported in the literature is presented in (Dinu et al., 2013). In (Turney, 2012), the compositionality task is investigated in the space of similarities instead of features. Specifically, two distinct models are built, namely, domain and function spaces. Despite the fact that meaning is at the core of human cognition, incorporating findings from the area of semantic cognition into the design of DSMs is not straightforward. The relation between cognitive semantics and the information encoded by DSMs is discussed in (Lenci, 2008). Motivated by evidence related to the low-dimensional geometry of thought (G¨ardenfors, 2004) in (Athanasopoulou et al., 2014)"
L16-1195,P10-2017,0,0.0652362,"Missing"
L16-1195,W15-0107,0,0.0572076,"Missing"
L16-1195,W15-1105,1,0.921039,"t that meaning is at the core of human cognition, incorporating findings from the area of semantic cognition into the design of DSMs is not straightforward. The relation between cognitive semantics and the information encoded by DSMs is discussed in (Lenci, 2008). Motivated by evidence related to the low-dimensional geometry of thought (G¨ardenfors, 2004) in (Athanasopoulou et al., 2014) word-level semantics are locally represented via a series of manifolds, while global operations (e.g., computation of word semantic similarity) are decomposed into local (i.e., manifold-level) operations. In (Georgiladakis et al., 2015) a two-layer model motivated by semantic priming (McNamara, 2005) and the dual cognitive processing theory (Kahneman, 2013) was adopted for representing the semantics of compositional phrases. Other related approaches include the mapping of DSMs to feature-normed spaces (F˘ag˘ar˘as¸an et al., 2015), the extension of word embeddings with cognitively motivated lexico-semantic resources such as WordNet (Rothe and Sch¨utze, 2015). 3. Distributional Semantic Models Here, two types of DSMs are described, which rely on the distributional hypothesis of meaning (Harris, 1954). Following the terminology"
L16-1195,P12-1092,0,0.360781,"of semantic similarity estimation between words, where slightly higher performance was achieved by unstructured DSMs. The multiple senses of words are typically not directly encoded in DSMs. For addressing this issue, exemplar models were proposed where the meaning of a word was represented by a set of stereotypical corpus sentences instead of a single feature vector (Erk and Pad´o, 2010). Recently, the computation of contextual features was posed in a learning-based framework where the goal is to estimate the context in which the words of interest are expected to occur (Bengio et al., 2003; Huang et al., 2012; Mikolov et al., 2013a). This advancement is discussed in (Baroni et al., 2014), where it is compared with traditional DSMs for various tasks of lexical semantics. A similar comparative study is also presented in (Huang et al., 2012). Word-level representations are the building blocks for phrase- and sentence-level models. Such models are motivated by the principle of semantic compositionality (Pelletier, 1994), while they are meant to address a number of properties that characterize the compositional aspects of meaning, namely, “linguistic creativity”, “order sensitivity”, “adaptive capacity"
L16-1195,J07-2002,0,0.0637077,"of DSMs typically constructed from co-occurrence statistics of word tuples. Word-level DSMs can be broadly categorized into unstructured and structured with respect to the extraction of contextual features. The bag-ofwords model is the most widely used approach (e.g., see (Grefenstette, 1994)), lacking however some desirable characteristics such as “order sensitivity” (Turney, 2012). Unlike unstructured models, the order of extracted features is taken into account in the framework of structured DSMs via the exploitation of syntactic relationships (e.g., argument structures and modifications) (Pado and Lapata, 2007). A comparison of unstructured and structured DSMs was conducted in (Agirre et al., 2009) for the task of semantic similarity estimation between words, where slightly higher performance was achieved by unstructured DSMs. The multiple senses of words are typically not directly encoded in DSMs. For addressing this issue, exemplar models were proposed where the meaning of a word was represented by a set of stereotypical corpus sentences instead of a single feature vector (Erk and Pad´o, 2010). Recently, the computation of contextual features was posed in a learning-based framework where the goal"
L16-1195,P15-1173,0,0.0529284,"Missing"
L16-1195,D12-1110,0,0.0883686,"the area of compositional DSMs, where a series of algebraic operations are applied over the word-level feature vectors for representing the semantics of two-word phrases divided in three categories, namely, noun-noun (NN), adjective-noun (AN), and verb-object (VO). In (Baroni and Zamparelli, 2010), the compositional aspects of AN phrases were investigated where the semantic transformation triggered by adjectives was modeled via a function (implemented as matrix) operating over the vectorial representation of nouns. The phrase-level approach of (Baroni and Zamparelli, 2010) was generalized in (Socher et al., 2012) according to which sentence-level representations are constructed in a recursive bottom-up fashion. A comparison of the major compositional approaches reported in the literature is presented in (Dinu et al., 2013). In (Turney, 2012), the compositionality task is investigated in the space of similarities instead of features. Specifically, two distinct models are built, namely, domain and function spaces. Despite the fact that meaning is at the core of human cognition, incorporating findings from the area of semantic cognition into the design of DSMs is not straightforward. The relation between"
L16-1195,zervanou-etal-2014-word,1,0.898739,"Missing"
L16-1458,J90-1003,0,0.447647,"ve ratings estimation xi .xj ||xi |xj ||. (2) The elements of feature vectors can be weighted according to various schemes based on the corpus frequencies of wi and wj (Iosif and Potamianos, 2010). In this work, a binary and a PPMI based weighting scheme are used. According to the binary scheme the vector elements are set either to zero or to one. For the PPMI weighting scheme, vector elements are weighted using the positive point-wise mutual information (PPMI) metric. The point-wise mutual information (PMI) between the word wi and the n–th feature of its vector xi , fin , was computed as in (Church and Hanks, 1990): P M I(wi , fin ) = −log pˆ(wi , fin ) , pˆ(wi )ˆ p(fin ) (3) where pˆ(wi ) and pˆ(fin ) are the occurrence probabilities of wi and fin , respectively, while the probability of their co-occurrence (within the H window size) is denoted by pˆ(wi , fin ). The probabilities were computed according to maximum likelihood estimation using corpus-based word frequencies. PMI is unbounded, yielding scores that lie in the [−∞, +∞] interval. PPMI is equivalent to PMI with the modification that the negative scores are set to zero. This is based on the assumption that the contextual features that exhibit n"
L16-1458,esuli-sebastiani-2006-sentiwordnet,0,0.021029,"The disadvantage of manually created lexica is that they have low language coverage, since they contain only a few thousand words. Thus computational methods are used to create or expand an already existing lexicon. (Malandrakis et al., 2013) expands such affective lexica covering a sig1 For instance while both fear and anger are unpleasant emotions, anger is a dominant emotion, while fear is a submissive emotion 2867 nificant fraction of the vocabulary of a language. SentiWordNet and WordNetAffect are examples of large affective resources created through computational models. In the first, (Esuli and Sebastiani, 2006) annotated automatically all WordNet (Miller, 1995) synsets. The latter was developed by (Strapparava and Valitutti, 2004) who represented the affective meanings by selecting and labeling a subset of WordNet synsets. Computational models for affective text usually incorporate small manually created resources (Malandrakis et al., 2011), or larger automatically created resources (Chaumartin, 2007). 3. The Greek affective lexicon In this section, we describe the Greek affective lexicon creation process. We suggest that the words of an already existing affective lexicon can be transferred to the l"
L16-1458,strapparava-valitutti-2004-wordnet,0,0.216881,"housand words. Thus computational methods are used to create or expand an already existing lexicon. (Malandrakis et al., 2013) expands such affective lexica covering a sig1 For instance while both fear and anger are unpleasant emotions, anger is a dominant emotion, while fear is a submissive emotion 2867 nificant fraction of the vocabulary of a language. SentiWordNet and WordNetAffect are examples of large affective resources created through computational models. In the first, (Esuli and Sebastiani, 2006) annotated automatically all WordNet (Miller, 1995) synsets. The latter was developed by (Strapparava and Valitutti, 2004) who represented the affective meanings by selecting and labeling a subset of WordNet synsets. Computational models for affective text usually incorporate small manually created resources (Malandrakis et al., 2011), or larger automatically created resources (Chaumartin, 2007). 3. The Greek affective lexicon In this section, we describe the Greek affective lexicon creation process. We suggest that the words of an already existing affective lexicon can be transferred to the language of interest. Then, they can be shared to the target language’s native speakers in order to collect the affective r"
L16-1458,S07-1094,0,\N,Missing
L16-1627,J10-4006,0,0.0451015,"ncoded in the first layer, while the second layer is used for computing similarity between words. We exploit text- and image-derived features for performing computations at each layer, as well as various approaches for their crossmodal fusion. It is shown that the crossmodal model performs better (from 0.68 to 0.71 correlation coefficient) than the unimodal one for the task of similarity computation between words. Keywords: activation based models, crossmodal fusion, distributional semantic models, semantic similarity, bag-of-visual-words 1. Introduction Distributional semantic models (DSMs) (Baroni and Lenci, 2010) constitute a widely-used paradigm for extracting, representing and learning semantics from linguistic data. DSMs are based on the distributional hypothesis of meaning (Harris, 1954) assuming that semantic similarity between words is a function of the overlap of their linguistic contexts. Despite their success in various semantic tasks (e.g., semantic classification and computation of semantic similarity) these models have been criticized as “disembodied”, since they rely solely on linguistic information without being grounded in perception and action including other modalities, e.g., color (B"
L16-1627,W11-2503,0,0.390292,"rst approaches for the creation of multimodal DSMs was proposed in (Feng and Lapata, 2010) where a text corpus associated with images was exploited for building mixture models of latent topics based on Latent Dirichlet Allocation (LDA). Textual and visual features were jointly modeled using LDA via early fusion. The proposed method was evaluated on a semantic similarity task (WS353 dataset), where the multimodal model was found to yield higher performance compared to the textual one. However, the best performance was moderate (0.32 correlation coefficient). Another example of early fusion is (Bruni et al., 2011) where, unlike (Feng and Lapata, 2010), two modality-specific corpora were used. Vectorial representations were independently built for each modality and combined via vector concatenation. For the same task –although a different subset of WS353 was used– significantly higher correlation (0.52) 3973 was achieved demonstrating how the corpus used to estimate DSM features can significantly affect performance. A common technique for the late fusion of textual and visual models is the combination of similarity scores estimated via the unimodal models. For example, in (Leong and Mihalcea, 2011) the"
L16-1627,N10-1011,0,0.0287686,"behind network DSMs is a twotier system, motivated by cognitive considerations such as network activation and priming. The first layer, encodes the semantics of words via the creation of lexical neighborhoods. In the second layer, similarity metrics are defined that operate on these semantic neighborhoods. In this paper, we investigate the integration of visual and lexical features for network-based DSMs. The integration is achieved by alternating visual and lexical information in the two layers. 2. Related Work One of the first approaches for the creation of multimodal DSMs was proposed in (Feng and Lapata, 2010) where a text corpus associated with images was exploited for building mixture models of latent topics based on Latent Dirichlet Allocation (LDA). Textual and visual features were jointly modeled using LDA via early fusion. The proposed method was evaluated on a semantic similarity task (WS353 dataset), where the multimodal model was found to yield higher performance compared to the textual one. However, the best performance was moderate (0.32 correlation coefficient). Another example of early fusion is (Bruni et al., 2011) where, unlike (Feng and Lapata, 2010), two modality-specific corpora w"
L16-1627,W15-1105,1,0.853538,"red sets (according to similarity) denoted as Ti and Vi , respectively. The bimodal neighborhood of wi can be computed via: 1) union Fi∪ = Ti ∪ Vi , and 2) intersection Fi∩ = Ti ∩ Vi . Assume a bimodal neighborhood of fixed size n. For the case of union, half the neighbors are selected from Ti and the rest from Vi , i.e., |Ti |= |Vi |= n2 . Regarding intersection, the cardinality of Ti and Vi is not fixed since the goal is to have n neighbors within Fi∩ . For this purpose, we allow the gradual increment of |Ti |and |Vi |until the satisfaction of this criterion. This relaxation was adopted in (Georgiladakis et al., 2015) where it was applied for the computation of neighborhoods for short phrases based on the neighborhoods of the constituent words. The intersection-based fusion adheres to findings from the literature of psycholinguistics suggesting that the crossmodal neighborhoods should be more specific than the respective unimodal ones (Osherson and Smith, 1981). Global fusion. Given a target word wi , let Ti and Vi be the vectors containing the semantic similarities between wi and the words of lexicon L computed with respect to 3975 text and visual features, respectively. The fusion was implemented via the"
L16-1627,iosif-etal-2012-associative,1,0.844403,"of the vertices. The semantic neighborhood of a target wi ∈ L is a sub-graph of F , Fi = (Ai , Ei ), where the set of vertices Ai includes in total n members of L, which are linked with wi via edges Ei . This is motivated by theories of semantic priming (McNamara, 2005) according to which stimulus facilitates the cognitive processing of related entities present in the human semantic memory. The theory of priming applies to any perceptual entities regardless of modality. Thus, the use of lexical and visual features as priming cues is cognitively valid (Stenberg et al., 1995). For example, in (Iosif et al., 2012) lexical features and corpus statistics were used for the classification of lexical relations with respect to two broad types of priming. 4.2. Layer 2: Similarity Model In this section, we present two metrics of semantic similarity proposed in (Iosif and Potamianos, 2015) that are defined with respect to the activation areas computed in the first network layer. Maximum Similarity of Neighborhoods. This underlying hypothesis that the similarity of two words, wi and wj , can be computed as the maximum similarity of their respective activation areas: Mn (wi , wj ) = max{αij , αji }, (1) αij = max"
L16-1627,D15-1293,0,0.018908,"g refers to the mapping of the semantics of natural language to the physical world. This is supported by experimental findings indicating that real-world experiences also play a role for the acquisition of lexical semantics (Landau et al., 1998). For example, the naming of objects from pictures was found to be faster for color images vs grayscale (Therriault et al., 2009). Recently, focus has been given to the incorporation of features from modalities other than text in order to augment the textbased DSMs, e.g., see (Bruni et al., 2014) for imagederived features, and for audio-based features (Kiela and Clark, 2015). For additional insight into computation models that map data-derived low-level features to highlevel knowledge (including cognitive and social aspects) see (Potamianos, 2014). The proposed approach is an alternative framework for integrating textual and visual features for the task of semantic similarity computation between words. This is motivated by the cognitive evidence indicating the multimodal character of semantic representations utilized for various semantic tasks. This work, extends the unimodal (lexical-only) approach of network-based DSMs that has been successfully applied for the"
L16-1627,I11-1162,0,0.0217774,"fusion is (Bruni et al., 2011) where, unlike (Feng and Lapata, 2010), two modality-specific corpora were used. Vectorial representations were independently built for each modality and combined via vector concatenation. For the same task –although a different subset of WS353 was used– significantly higher correlation (0.52) 3973 was achieved demonstrating how the corpus used to estimate DSM features can significantly affect performance. A common technique for the late fusion of textual and visual models is the combination of similarity scores estimated via the unimodal models. For example, in (Leong and Mihalcea, 2011) the sum and the harmonic mean of similarities were used achieving 0.59 correlation for a small subset of WS353. In (Bruni et al., 2014), both early and late fusion schemes were applied. The early fusion was implemented as a linear weighted combination of the features vectors, while a similar combination was adopted for the late fusion based on the unimodal similarity scores. Both schemes were found to obtain high correlation (up to 0.78) for a subset of WS353, however, there is no clear winner since their relative performance vary with respect to the experimental parameters, e.g., the size of"
N19-1071,P82-1020,0,0.808165,"Missing"
N19-1071,K16-1002,0,0.104267,"Missing"
N19-1071,N16-1012,0,0.115087,"Missing"
N19-1071,D16-1140,0,0.0298938,"on of SEQ3 to unsupervised abstractive sentence compression, with additional task-specific loss functions; (3) state of the art performance in unsupervised abstractive sentence compression. This work is a step towards exploring the potential of SEQ3 in other tasks, such as machine translation. 2 N ∑ Wv oct + bv softmax(uct ) (3) Wo , bo , Wv , bv are learned. ct is also used when updating the state hct of the decoder, along with the embedding ect of yt and a countdown argument M − t (scaled by a learnable wd ) indicating the number of the remaining words of the summary (Fevry and Phang, 2018; Kikuchi et al., 2016). −−→ hct+1 = RNNc (hct , ect , ct , wd (M − t)) (4) For each input x = ⟨x1 , . . . , xN ⟩, we obtain a target length M for the summary y = ⟨y1 , . . . , yM ⟩ by sampling (and rounding) from a uniform distribution U (αN, βN); α, β are hyper-parameters (α < β < 1); we set M = 5, if the sampled M is smaller. Sampling M, instead of using a static compression ratio, allows us to train a model capable of producing summaries with varying (e.g., user-specified) compression ratios. Controlling the output length in encoder-decoder architectures has been explored in machine translation (Kikuchi et al.,"
N19-1071,W18-2706,0,0.0200439,"ect , ct , wd (M − t)) (4) For each input x = ⟨x1 , . . . , xN ⟩, we obtain a target length M for the summary y = ⟨y1 , . . . , yM ⟩ by sampling (and rounding) from a uniform distribution U (αN, βN); α, β are hyper-parameters (α < β < 1); we set M = 5, if the sampled M is smaller. Sampling M, instead of using a static compression ratio, allows us to train a model capable of producing summaries with varying (e.g., user-specified) compression ratios. Controlling the output length in encoder-decoder architectures has been explored in machine translation (Kikuchi et al., 2016) and summarization (Fan et al., 2018). Proposed Model 2.1 Compressor 2.2 Differentiable Word Sampling The bottom left part of Fig. 2 illustrates the internals of the compressor C. An embedding layer projects the source sequence x to the word embeddings es = ⟨es1 , . . . , esN ⟩, which are then enTo generate the summary, we need to sample its words yt from the categorical distributions p(yt |y<t , x), which is a non-differentiable process. 674 Soft-Argmax Instead of sampling yt , a simple workaround during training is to pass as input to the next timestep of C’s decoder and to the corresponding timestep of R’s encoder a weighted s"
N19-1071,K18-1040,0,0.62226,"equire parallel text-summary pairs, achieving promising results in unsupervised sentence compression on benchmark datasets. 1 Compressor (encoder-decoder) Topic Loss ?1 , ?2 , … , ?? ?ො1 , ?ො2 , … , ?ො? Reconstructor (encoder-decoder) LM Prior Loss Figure 1: Overview of the proposed SEQ3 autoencoder. posed (Artetxe et al., 2018; Lample et al., 2018b). Unsupervised (or semi-supervised) SEQ 2 SEQ models have also been proposed for summarization tasks with no (or small) parallel text-summary sets, including unsupervised sentence compression. Current models, however, barely reach leadN baselines (Fevry and Phang, 2018; Wang and Lee, 2018), and/or are non-differentiable (Wang and Lee, 2018; Miao and Blunsom, 2016), thus relying on reinforcement learning, which is unstable and inefficient. By contrast, we propose a sequence-to-sequence-to-sequence autoencoder, dubbed SEQ3 , that can be trained end-to-end via gradient-based optimization. SEQ3 employs differentiable approximations for sampling from categorical distributions (Maddison et al., 2017; Jang et al., 2017), which have been shown to outperform reinforcement learning (Havrylov and Titov, 2017). Therefore it is a generic framework which can be easily ex"
N19-1071,W17-3204,0,0.0356547,", . . . , xN ⟩ of N words, and generates a summary y = ⟨y1 , . . . , yM ⟩ of M words (M<N), y being a latent variable. R and C communicate only through the discrete words of the summary y (§2.2). R (§2.3) produces a seˆ = ⟨ˆ quence x x1 , . . . , x ˆN ⟩ of N words from y, tryIntroduction Neural sequence-to-sequence models (SEQ 2 SEQ) perform impressively well in several natural language processing tasks, such as machine translation (Sutskever et al., 2014; Bahdanau et al., 2015) or syntactic constituency parsing (Vinyals et al., 2015). However, they require massive parallel training datasets (Koehn and Knowles, 2017). Consequently there has been extensive work on utilizing non-parallel corpora to boost the performance of SEQ 2 SEQ models (Sennrich et al., 2016; G¨ulc¸ehre et al., 2015), mostly in neural machine translation where models that require absolutely no parallel corpora have also been pro673 Proceedings of NAACL-HLT 2019, pages 673–681 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics ?ො? ?ො2 ? ℎ?−1 ℎ1? … ℎ0? ℎ1? ℎ2? … ? ??−1 ?1? … ?0? ?1? ?2? … ?1 ?1 ?ො1 coded by a bidirectional RNN, producing hs = ⟨hs1 , . . . , hsN ⟩. Each hst is the concatenation"
N19-1071,N18-2081,0,0.0329404,"Missing"
N19-1071,D15-1044,0,0.821236,"s the input text. ai hsi i=1 The matrix Wa is learned. We obtain a probability distribution for yt over the vocabulary V by combining ct and the current state hct of the decoder. oct = tanh(Wo [ct ; hct ] + bo ) (1) uct (2) = p(yt |y<t , x) = ˆ) ing to minimize a reconstruction loss LR = (x, x (§2.5). A pretrained language model acts as a prior on y, introducing an additional loss LP (x, y) that encourages SEQ3 to produce human-readable summaries. A third loss LT (x, y) rewards summaries y with similar topic-indicating words as x. Experiments (§3) on the Gigaword sentence compression dataset (Rush et al., 2015) and the DUC -2003 and DUC -2004 shared tasks (Over et al., 2007) produce promising results. Our contributions are: (1) a fully differentiable sequence-to-sequence-to-sequence (SEQ3 ) autoencoder that can be trained without parallel data via gradient optimization; (2) an application of SEQ3 to unsupervised abstractive sentence compression, with additional task-specific loss functions; (3) state of the art performance in unsupervised abstractive sentence compression. This work is a step towards exploring the potential of SEQ3 in other tasks, such as machine translation. 2 N ∑ Wv oct + bv softma"
N19-1071,D15-1166,0,0.063871,"rallel corpora have also been pro673 Proceedings of NAACL-HLT 2019, pages 673–681 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics ?ො? ?ො2 ? ℎ?−1 ℎ1? … ℎ0? ℎ1? ℎ2? … ? ??−1 ?1? … ?0? ?1? ?2? … ?1 ?1 ?ො1 coded by a bidirectional RNN, producing hs = ⟨hs1 , . . . , hsN ⟩. Each hst is the concatenation of the corresponding left-to-right and right-to-left states (outputs in LSTMs) of the bi-RNN. ← − − → −−→ ←−− hst = [RNNs (est , h st−1 ); RNNs (est , h st+1 )] Reconstructor ? ℎ? ? ?? To generate the summary y, we employ the attentional RNN decoder of Luong et al. (2015), with their global attention and input feeding. Concretely, at each timestep (t ∈ {1, . . . , M}) we compute a probability distribution ai over all the states hs1 , . . . , hsN of the source encoder conditioned on the current state hct of the compressor’s decoder to produce a context vector ct . ?Μ LM prior Loss Compressor ℎ1? ℎ2? … ? ℎ? ℎ0? ℎ1? … ? ℎ?−1 ?1? ?2? … ??? ?0? ?1? … ? ??−1 ?1 ?2 ?? Topic Loss ai = softmax(hsi ⊺ Wa hct ), ct = 3 Figure 2: More detailed illustration of SEQ . The compressor (C) produces a summary from the input text, and the reconstructor (R) tries to reproduce the i"
N19-1071,P17-1099,0,0.260948,"Missing"
N19-1071,P16-1009,0,0.0512056,"gh the discrete words of the summary y (§2.2). R (§2.3) produces a seˆ = ⟨ˆ quence x x1 , . . . , x ˆN ⟩ of N words from y, tryIntroduction Neural sequence-to-sequence models (SEQ 2 SEQ) perform impressively well in several natural language processing tasks, such as machine translation (Sutskever et al., 2014; Bahdanau et al., 2015) or syntactic constituency parsing (Vinyals et al., 2015). However, they require massive parallel training datasets (Koehn and Knowles, 2017). Consequently there has been extensive work on utilizing non-parallel corpora to boost the performance of SEQ 2 SEQ models (Sennrich et al., 2016; G¨ulc¸ehre et al., 2015), mostly in neural machine translation where models that require absolutely no parallel corpora have also been pro673 Proceedings of NAACL-HLT 2019, pages 673–681 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics ?ො? ?ො2 ? ℎ?−1 ℎ1? … ℎ0? ℎ1? ℎ2? … ? ??−1 ?1? … ?0? ?1? ?2? … ?1 ?1 ?ො1 coded by a bidirectional RNN, producing hs = ⟨hs1 , . . . , hsN ⟩. Each hst is the concatenation of the corresponding left-to-right and right-to-left states (outputs in LSTMs) of the bi-RNN. ← − − → −−→ ←−− hst = [RNNs (est , h st−1 ); RNNs (e"
N19-1071,D18-1267,0,0.0184866,"of the sampled words yt in the forward pass, approximate differentiable embeddings in the backward pass). (5) i where uct is the unnormalized score in Eq. 2 (i.e., the logit) of each word wi and τ ∈ (0, ∞) is the temperature. As τ → 0 most of the probability mass in Eq. 5 goes to the most probable word, hence the operation approaches the arg max. 2.4 Decoder Initialization We initialize the hidden state of each decoder us−→ ← − ing a transformation of the concatenation [hsN ; hs1 ] of the last hidden states (from the two directions) of its bidirectional encoder and a length vector, following Mallinson et al. (2018). The length vector for the decoder of the compressor C consists of the target summary length M, scaled by a learnable parameter wv , and the compression ratio M N. Gumbel-Softmax We still want to be able to perform sampling, though, as it has the benefit of adding stochasticity and facilitating exploration of the parameter space. Hence, we use the GumbelSoftmax (GS) reparametrization trick (Maddison et al., 2017; Jang et al., 2017) as a low variance approximation of sampling from categorical distributions. Sampling a specific word yt from the softmax (Eq. 3) is equivalent to adding (element-w"
N19-1071,D16-1031,0,0.606807,"ssion on benchmark datasets. 1 Compressor (encoder-decoder) Topic Loss ?1 , ?2 , … , ?? ?ො1 , ?ො2 , … , ?ො? Reconstructor (encoder-decoder) LM Prior Loss Figure 1: Overview of the proposed SEQ3 autoencoder. posed (Artetxe et al., 2018; Lample et al., 2018b). Unsupervised (or semi-supervised) SEQ 2 SEQ models have also been proposed for summarization tasks with no (or small) parallel text-summary sets, including unsupervised sentence compression. Current models, however, barely reach leadN baselines (Fevry and Phang, 2018; Wang and Lee, 2018), and/or are non-differentiable (Wang and Lee, 2018; Miao and Blunsom, 2016), thus relying on reinforcement learning, which is unstable and inefficient. By contrast, we propose a sequence-to-sequence-to-sequence autoencoder, dubbed SEQ3 , that can be trained end-to-end via gradient-based optimization. SEQ3 employs differentiable approximations for sampling from categorical distributions (Maddison et al., 2017; Jang et al., 2017), which have been shown to outperform reinforcement learning (Havrylov and Titov, 2017). Therefore it is a generic framework which can be easily extended to other tasks, e.g., machine translation and semantic parsing via task-specific losses. I"
N19-1071,D18-1451,0,0.715263,"mmary pairs, achieving promising results in unsupervised sentence compression on benchmark datasets. 1 Compressor (encoder-decoder) Topic Loss ?1 , ?2 , … , ?? ?ො1 , ?ො2 , … , ?ො? Reconstructor (encoder-decoder) LM Prior Loss Figure 1: Overview of the proposed SEQ3 autoencoder. posed (Artetxe et al., 2018; Lample et al., 2018b). Unsupervised (or semi-supervised) SEQ 2 SEQ models have also been proposed for summarization tasks with no (or small) parallel text-summary sets, including unsupervised sentence compression. Current models, however, barely reach leadN baselines (Fevry and Phang, 2018; Wang and Lee, 2018), and/or are non-differentiable (Wang and Lee, 2018; Miao and Blunsom, 2016), thus relying on reinforcement learning, which is unstable and inefficient. By contrast, we propose a sequence-to-sequence-to-sequence autoencoder, dubbed SEQ3 , that can be trained end-to-end via gradient-based optimization. SEQ3 employs differentiable approximations for sampling from categorical distributions (Maddison et al., 2017; Jang et al., 2017), which have been shown to outperform reinforcement learning (Havrylov and Titov, 2017). Therefore it is a generic framework which can be easily extended to other tasks"
N19-1071,K16-1028,0,0.103221,"Missing"
N19-1071,D10-1050,0,0.0733195,"Missing"
N19-1071,D14-1162,0,0.0937538,"et. Length Penalty A fourth loss LL (not shown in Fig. 1) helps the (decoder of the) compressor to predict the end-of-sequence (EOS) token at the target summary length M. LL is the cross-entropy between the distributions p(yt |y<t , x) (Eq. 3) of the compressor at t = M + 1 and onward, with the one-hot distribution of the EOS token. 2.6 Modeling Details Parameter Sharing We tie the weights of layers encoding similar information, to reduce the number of trainable parameters. First, we use a shared embedding layer for the encoders and decoders, initialized with 100-dimensional GloVe embeddings (Pennington et al., 2014). Additionally, we tie the shared embedding layer with the output layers of both decoders (Press and Wolf, 2017; Inan et al., 2017). Finally, we tie the encoders of the compressor and reconstructor (see Appendix). OOVs Out-of-vocabulary words are handled as in Fevry and Phang (2018) (see Appendix). 3 Results Table 1 reports the Gigaword results. SEQ 3 outperforms the unsupervised Pretrained Generator across all metrics by a large margin. It also surpasses LEAD -8. If we remove the LM prior, performance drops, esp. in ROUGE -2 and ROUGE L . This makes sense, since the pretrained LM rewards corr"
N19-1071,E17-2025,0,0.0312917,"of-sequence (EOS) token at the target summary length M. LL is the cross-entropy between the distributions p(yt |y<t , x) (Eq. 3) of the compressor at t = M + 1 and onward, with the one-hot distribution of the EOS token. 2.6 Modeling Details Parameter Sharing We tie the weights of layers encoding similar information, to reduce the number of trainable parameters. First, we use a shared embedding layer for the encoders and decoders, initialized with 100-dimensional GloVe embeddings (Pennington et al., 2014). Additionally, we tie the shared embedding layer with the output layers of both decoders (Press and Wolf, 2017; Inan et al., 2017). Finally, we tie the encoders of the compressor and reconstructor (see Appendix). OOVs Out-of-vocabulary words are handled as in Fevry and Phang (2018) (see Appendix). 3 Results Table 1 reports the Gigaword results. SEQ 3 outperforms the unsupervised Pretrained Generator across all metrics by a large margin. It also surpasses LEAD -8. If we remove the LM prior, performance drops, esp. in ROUGE -2 and ROUGE L . This makes sense, since the pretrained LM rewards correct word order. We also tried removing the topic loss, but the model failed to converge and results were extrem"
N19-1071,P17-1101,0,0.0405545,"Missing"
N19-1110,P16-1177,0,0.0330006,"Missing"
N19-1110,D16-1250,0,0.0717032,"r with a specific sense in each topic, proposed a topic-based semantic mixture model that exploits a combination of similarities estimated on topic-based DSMs for the computation of semantic similarity between words. Their model performs well for a variety of semantic similarity tasks; however, it lacks a unified representation of multiple senses in a common semantic space. The problem of defining transformations between embeddings—trained independently under different corpora—has been previously examined in various works, such as machine translation (Mikolov et al., 2013b; Xing et al., 2015; Artetxe et al., 2016), induction of historical embeddings (Hamilton et al., 2016) and lexical resources enrichment (Prokhorov et al., 2017). Following this line of research, we induce the creation of multiple cross-topic word embeddings by projecting the semantic representations of topic-based DSMs to a unified semantic space. We investigate different ways to perform the mappings from the topic sub-spaces to the unified semantic space, and propose a completely unsupervised approach to extract semantic anchors that define those mappings. Furthermore, we claim that polysemous words change their meaning in different"
N19-1110,P18-1073,0,0.0216597,"overcome the above limitations, we propose a fully unsupervised method for semantic anchor induction. Although the embeddings of the topic and global semantic vector spaces are not aligned, their corresponding similarity matrices (once normalized) are. Based on this observation, we compute the similarity between a given word and every other word in the vocabulary (similarity distribution) for the different topic and global spaces. Then, we assume that good semantic anchors should have similar similarity distributions across the topic-specific and the global space, as illustrated in Figure 2. Artetxe et al. (2018) was based on a similar observation to align vector semantic spaces in bilingual machine translation context. 1054 professor crater view october Figure 2: Similarity distributions of four different words (corresponding to the smoothed density estimates of the similarity matrices) in topic domain space as defined in Equation 1 and global space sig . Selected anchors (“professor” and “october”) have more similar distributions in the global and topic spaces, when compared to unselected ones (“view” and “crater”). We observe that the selected anchors are less ambiguous, while the not selected ones"
N19-1110,P16-1141,0,0.0314372,"ed semantic mixture model that exploits a combination of similarities estimated on topic-based DSMs for the computation of semantic similarity between words. Their model performs well for a variety of semantic similarity tasks; however, it lacks a unified representation of multiple senses in a common semantic space. The problem of defining transformations between embeddings—trained independently under different corpora—has been previously examined in various works, such as machine translation (Mikolov et al., 2013b; Xing et al., 2015; Artetxe et al., 2016), induction of historical embeddings (Hamilton et al., 2016) and lexical resources enrichment (Prokhorov et al., 2017). Following this line of research, we induce the creation of multiple cross-topic word embeddings by projecting the semantic representations of topic-based DSMs to a unified semantic space. We investigate different ways to perform the mappings from the topic sub-spaces to the unified semantic space, and propose a completely unsupervised approach to extract semantic anchors that define those mappings. Furthermore, we claim that polysemous words change their meaning in different topic domains; this is reflected in rela1052 Proceedings of"
N19-1110,P12-1092,0,0.29296,"ns per word can be grouped into two broad categories.2 Unsupervised methods induce multiple word representations without leveraging semantic lexical resources. Reisinger and Mooney (2010) were the first to create a multi-prototype DSM with a fixed number of vectors assigned to each word. In their model, the centroids of context-dependent clusters were used to create a set of “sense-specific” vectors for each target word. Based on similar clustering approaches, follow-up works introduced neural network architectures that incorporated both local and global context in a joint training objective (Huang et al., 2012), as well as methods that jointly performed word sense clustering and embedding learning as in Neelakantan et al. (2014); Li and Jurafsky (2015). A probabilistic framework was introduced by Tian et al. (2014), where the Skip-Gram model of Word2Vec was modified to learn multiple embedding vectors. Furthermore, latent topics Unified Multi-Topic DSM (UTDSM) Our system follows a four-step approach: 1 https://github.com/Elbria/utdsm_ naacl2018 2 We limit our discussion to related works that use monolingual DSMs and corpora. 1053 1. Global Distributional Semantic Model. Given a large collection of t"
N19-1110,P15-1010,0,0.021838,"here a mixture of topic-based semantic models was extracted by topical adaptation of in-domain corpora. Other approaches used autoencoders (Amiri et al., 2016), convolutional neural networks designed to produce context representations that reflected the order of words in a context (Zheng et al., 2017) and reinforcement learning (Lee and Chen, 2017; Guo et al., 2018). Supervised approaches, based on prior knowledge acquired by sense inventories (e.g., WordNet) along with word sense disambiguation algorithms, were also introduced for sense-specific representations extraction (Chen et al., 2014; Iacobacci et al., 2015). In other works, pre-trained word embeddings have been extended to embeddings of lexemes and synsets (Rothe and Sch¨utze, 2015) or were de-conflated into their constituent sense representations (Pilehvar and Collier, 2016) by exploiting semantic lexical resources. 3 Methods that assign multiple distributed representations per word can be grouped into two broad categories.2 Unsupervised methods induce multiple word representations without leveraging semantic lexical resources. Reisinger and Mooney (2010) were the first to create a multi-prototype DSM with a fixed number of vectors assigned to"
N19-1110,D17-1034,0,0.0161013,"ted into the Skip-Gram model, resulting in topical word embeddings which modeled the semantics of a word under different contexts (Liu et al., 2015b,a; Nguyen et al., 2017). Another topic-related embedding creation approach was proposed in Christopoulou et al. (2018) where a mixture of topic-based semantic models was extracted by topical adaptation of in-domain corpora. Other approaches used autoencoders (Amiri et al., 2016), convolutional neural networks designed to produce context representations that reflected the order of words in a context (Zheng et al., 2017) and reinforcement learning (Lee and Chen, 2017; Guo et al., 2018). Supervised approaches, based on prior knowledge acquired by sense inventories (e.g., WordNet) along with word sense disambiguation algorithms, were also introduced for sense-specific representations extraction (Chen et al., 2014; Iacobacci et al., 2015). In other works, pre-trained word embeddings have been extended to embeddings of lexemes and synsets (Rothe and Sch¨utze, 2015) or were de-conflated into their constituent sense representations (Pilehvar and Collier, 2016) by exploiting semantic lexical resources. 3 Methods that assign multiple distributed representations p"
N19-1110,D15-1200,0,0.0187587,"lexical resources. Reisinger and Mooney (2010) were the first to create a multi-prototype DSM with a fixed number of vectors assigned to each word. In their model, the centroids of context-dependent clusters were used to create a set of “sense-specific” vectors for each target word. Based on similar clustering approaches, follow-up works introduced neural network architectures that incorporated both local and global context in a joint training objective (Huang et al., 2012), as well as methods that jointly performed word sense clustering and embedding learning as in Neelakantan et al. (2014); Li and Jurafsky (2015). A probabilistic framework was introduced by Tian et al. (2014), where the Skip-Gram model of Word2Vec was modified to learn multiple embedding vectors. Furthermore, latent topics Unified Multi-Topic DSM (UTDSM) Our system follows a four-step approach: 1 https://github.com/Elbria/utdsm_ naacl2018 2 We limit our discussion to related works that use monolingual DSMs and corpora. 1053 1. Global Distributional Semantic Model. Given a large collection of text data we train a DSM that encodes the contextual semantics of each word into a single representation, also referred to as Global-DSM. 2. Topi"
N19-1110,D14-1110,0,0.0191313,"lou et al. (2018) where a mixture of topic-based semantic models was extracted by topical adaptation of in-domain corpora. Other approaches used autoencoders (Amiri et al., 2016), convolutional neural networks designed to produce context representations that reflected the order of words in a context (Zheng et al., 2017) and reinforcement learning (Lee and Chen, 2017; Guo et al., 2018). Supervised approaches, based on prior knowledge acquired by sense inventories (e.g., WordNet) along with word sense disambiguation algorithms, were also introduced for sense-specific representations extraction (Chen et al., 2014; Iacobacci et al., 2015). In other works, pre-trained word embeddings have been extended to embeddings of lexemes and synsets (Rothe and Sch¨utze, 2015) or were de-conflated into their constituent sense representations (Pilehvar and Collier, 2016) by exploiting semantic lexical resources. 3 Methods that assign multiple distributed representations per word can be grouped into two broad categories.2 Unsupervised methods induce multiple word representations without leveraging semantic lexical resources. Reisinger and Mooney (2010) were the first to create a multi-prototype DSM with a fixed numbe"
N19-1110,C04-1051,0,0.188425,"t and sentence level downstream NLP tasks: text classification and paraphrase identification. We report weighted-averaging precision, recall, F1-measure and accuracy performance metrics. Text classification. We used the 20NewsGroup6 dataset, which consists of about 20 000 documents. Our goal is to classify each document into one of the 20 different newsgroups based on its content. 1056 6 http://qwone.com/ jason/20Newsgroups/ Paraphrase Identification. For this task we aimed at identifying whether two given sentences can be considered paraphrases or not, using the Microsoft Paraphrase dataset (Dolan et al., 2004). Document and Sentence level representations. Given a document or a sentence D, where wd corresponds to the d-th word in D, we extract its feature representation using three different ways: AvgCD = |D |K 1 XX p(k|D)x0k (wd ), |D| (7) supervised method instead of using randomly selected anchor words (UTDSM Random). We also observe that random anchoring performs slightly worse than UTDSM with respect to AvgSimC. This result validates our hypothesis that the representations of words, which share consistent similarity distributions across different topic domains, constitute informative semantic a"
N19-1110,N13-1090,0,0.286622,"the assumption that typically words appear with a specific sense in each topic, proposed a topic-based semantic mixture model that exploits a combination of similarities estimated on topic-based DSMs for the computation of semantic similarity between words. Their model performs well for a variety of semantic similarity tasks; however, it lacks a unified representation of multiple senses in a common semantic space. The problem of defining transformations between embeddings—trained independently under different corpora—has been previously examined in various works, such as machine translation (Mikolov et al., 2013b; Xing et al., 2015; Artetxe et al., 2016), induction of historical embeddings (Hamilton et al., 2016) and lexical resources enrichment (Prokhorov et al., 2017). Following this line of research, we induce the creation of multiple cross-topic word embeddings by projecting the semantic representations of topic-based DSMs to a unified semantic space. We investigate different ways to perform the mappings from the topic sub-spaces to the unified semantic space, and propose a completely unsupervised approach to extract semantic anchors that define those mappings. Furthermore, we claim that polysemo"
N19-1110,D14-1113,0,0.530115,"ach word is uniquely represented by one point in the vector space. From a ∗ The research was performed when the author was an undergraduate researcher at School of ECE, NTUA in Athens, Greece. linguistic perspective, these models cannot capture the distinct meanings of polysemous words (e.g., bank or cancer), resulting in conflated word representations of diverse contextual semantics. To alleviate this problem, DSMs with multiple representations per word have been proposed in the literature, based on clustering local contexts of individual words (Reisinger and Mooney, 2010; Tian et al., 2014; Neelakantan et al., 2014). An alternative way to train multiple representation DSMs is to utilize semantic lexical resources (Rothe and Sch¨utze, 2015; Pilehvar and Collier, 2016). Christopoulou et al. (2018), based on the assumption that typically words appear with a specific sense in each topic, proposed a topic-based semantic mixture model that exploits a combination of similarities estimated on topic-based DSMs for the computation of semantic similarity between words. Their model performs well for a variety of semantic similarity tasks; however, it lacks a unified representation of multiple senses in a common sema"
N19-1110,S17-1015,0,0.0145081,"task and yield state-of-the-art performance compared to other unsupervised multi-prototype word embedding approaches. We further perform experiments on two NLP downstream tasks: text classification and paraphrase identification and demonstrate that our learned word representations consistently provide higher performance than single-prototype word embedding models. The code of the present work is publicly available1 . 2 Related Work were integrated into the Skip-Gram model, resulting in topical word embeddings which modeled the semantics of a word under different contexts (Liu et al., 2015b,a; Nguyen et al., 2017). Another topic-related embedding creation approach was proposed in Christopoulou et al. (2018) where a mixture of topic-based semantic models was extracted by topical adaptation of in-domain corpora. Other approaches used autoencoders (Amiri et al., 2016), convolutional neural networks designed to produce context representations that reflected the order of words in a context (Zheng et al., 2017) and reinforcement learning (Lee and Chen, 2017; Guo et al., 2018). Supervised approaches, based on prior knowledge acquired by sense inventories (e.g., WordNet) along with word sense disambiguation al"
N19-1110,D16-1174,0,0.596212,"t School of ECE, NTUA in Athens, Greece. linguistic perspective, these models cannot capture the distinct meanings of polysemous words (e.g., bank or cancer), resulting in conflated word representations of diverse contextual semantics. To alleviate this problem, DSMs with multiple representations per word have been proposed in the literature, based on clustering local contexts of individual words (Reisinger and Mooney, 2010; Tian et al., 2014; Neelakantan et al., 2014). An alternative way to train multiple representation DSMs is to utilize semantic lexical resources (Rothe and Sch¨utze, 2015; Pilehvar and Collier, 2016). Christopoulou et al. (2018), based on the assumption that typically words appear with a specific sense in each topic, proposed a topic-based semantic mixture model that exploits a combination of similarities estimated on topic-based DSMs for the computation of semantic similarity between words. Their model performs well for a variety of semantic similarity tasks; however, it lacks a unified representation of multiple senses in a common semantic space. The problem of defining transformations between embeddings—trained independently under different corpora—has been previously examined in vario"
N19-1110,N15-1104,0,0.0598593,"Missing"
N19-1110,D10-1114,0,0.461449,"traditional DSMs rely solely on models where each word is uniquely represented by one point in the vector space. From a ∗ The research was performed when the author was an undergraduate researcher at School of ECE, NTUA in Athens, Greece. linguistic perspective, these models cannot capture the distinct meanings of polysemous words (e.g., bank or cancer), resulting in conflated word representations of diverse contextual semantics. To alleviate this problem, DSMs with multiple representations per word have been proposed in the literature, based on clustering local contexts of individual words (Reisinger and Mooney, 2010; Tian et al., 2014; Neelakantan et al., 2014). An alternative way to train multiple representation DSMs is to utilize semantic lexical resources (Rothe and Sch¨utze, 2015; Pilehvar and Collier, 2016). Christopoulou et al. (2018), based on the assumption that typically words appear with a specific sense in each topic, proposed a topic-based semantic mixture model that exploits a combination of similarities estimated on topic-based DSMs for the computation of semantic similarity between words. Their model performs well for a variety of semantic similarity tasks; however, it lacks a unified repr"
N19-1110,P15-1173,0,0.0425387,"Missing"
N19-1110,W17-4778,0,0.0247385,"Missing"
N19-1110,P15-1150,0,0.0443675,"Missing"
N19-1110,C14-1016,0,0.183171,"y on models where each word is uniquely represented by one point in the vector space. From a ∗ The research was performed when the author was an undergraduate researcher at School of ECE, NTUA in Athens, Greece. linguistic perspective, these models cannot capture the distinct meanings of polysemous words (e.g., bank or cancer), resulting in conflated word representations of diverse contextual semantics. To alleviate this problem, DSMs with multiple representations per word have been proposed in the literature, based on clustering local contexts of individual words (Reisinger and Mooney, 2010; Tian et al., 2014; Neelakantan et al., 2014). An alternative way to train multiple representation DSMs is to utilize semantic lexical resources (Rothe and Sch¨utze, 2015; Pilehvar and Collier, 2016). Christopoulou et al. (2018), based on the assumption that typically words appear with a specific sense in each topic, proposed a topic-based semantic mixture model that exploits a combination of similarities estimated on topic-based DSMs for the computation of semantic similarity between words. Their model performs well for a variety of semantic similarity tasks; however, it lacks a unified representation of multi"
N19-1213,S17-2126,1,0.855455,"pretrain the language model, we collect a dataset of 20 million English Twitter messages, including approximately 2M unique tokens. We use the 70K most frequent tokens as vocabulary. We evaluate our model on five datasets: Sent17 for sentiment analysis (Rosenthal et al., 2017), PsychExp for emotion recognition (Wallbott and Scherer, 1986), Irony18 for irony detection (Van Hee et al., 2018), SCv1 and SCv2 for sarcasm detection (Oraby et al., 2016; Lukin and Walker, 2013). More details about the datasets can be found in Table 1. 4.2 Experimental Setup To preprocess the tweets, we use Ekphrasis (Baziotis et al., 2017). For the generic datasets, we use NLTK (Loper and Bird, 2002). For the NBoW baseline, we use word2vec (Mikolov et al., 2013) 300-dimensional embeddings as features. 2091 Irony18 43.7 45.2 42.7 ± 0.6 41.8 ± 1.2 45.5 ± 0.9 47.0 ± 1.1 Sent17 61.0 63.0 61.2 ± 0.7 62.1 ± 0.8 65.1 ± 0.6 66.5 ± 0.2 SCv2 65.1 61.1 69.4 ± 0.4 69.9 ± 1.0 72.6 ± 0.7 75.0 ± 0.7 23.6 ± 1.6 41.6 ± 0.7 53.6 (Baziotis et al., 2018) 60.5 ± 0.5 65.6 ± 0.4 68.5 (Cliche, 2017) 68.7 ± 0.6 67.2 ± 0.9 76.0 (Ilic et al., 2018) BoW NBoW P-LM P-LM + su P-LM + aux SiATL (P-LM + aux + su) ULMFiT (Wiki-103) ULMFiT (Twitter) State of the"
N19-1213,W18-6209,1,0.886726,"Missing"
N19-1213,D17-1169,0,0.0738553,"Missing"
N19-1213,P18-1031,0,0.35149,"of challenging affective and text classification tasks, surpassing well established transfer learning methods with greater level of complexity. 1 Introduction Pretrained word representations captured by Language Models (LMs) have recently become popular in Natural Language Processing (NLP). Pretrained LMs encode contextual information and high-level features of language, modeling syntax and semantics, producing state-of-the-art results across a wide range of tasks, such as named entity recognition (Peters et al., 2017), machine translation (Ramachandran et al., 2017) and text classification (Howard and Ruder, 2018). However, in cases where contextual embeddings from language models are used as additional features (e.g. ELMo (Peters et al., 2018)), results come at a high computational cost and require task-specific architectures. At the same time, approaches that rely on fine-tuning a LM to the task at hand (e.g. ULMFiT (Howard and Ruder, 2018)) depend on pretraining the model on an extensive vocabulary and on employing a sophisticated slanted triangular learning rate scheme to adapt the parameters of the LM to the target dataset. We propose a simple and effective transfer learning approach, that leverag"
N19-1213,W18-6202,0,0.0532272,"he datasets can be found in Table 1. 4.2 Experimental Setup To preprocess the tweets, we use Ekphrasis (Baziotis et al., 2017). For the generic datasets, we use NLTK (Loper and Bird, 2002). For the NBoW baseline, we use word2vec (Mikolov et al., 2013) 300-dimensional embeddings as features. 2091 Irony18 43.7 45.2 42.7 ± 0.6 41.8 ± 1.2 45.5 ± 0.9 47.0 ± 1.1 Sent17 61.0 63.0 61.2 ± 0.7 62.1 ± 0.8 65.1 ± 0.6 66.5 ± 0.2 SCv2 65.1 61.1 69.4 ± 0.4 69.9 ± 1.0 72.6 ± 0.7 75.0 ± 0.7 23.6 ± 1.6 41.6 ± 0.7 53.6 (Baziotis et al., 2018) 60.5 ± 0.5 65.6 ± 0.4 68.5 (Cliche, 2017) 68.7 ± 0.6 67.2 ± 0.9 76.0 (Ilic et al., 2018) BoW NBoW P-LM P-LM + su P-LM + aux SiATL (P-LM + aux + su) ULMFiT (Wiki-103) ULMFiT (Twitter) State of the art SCv1 60.9 51.9 48.5 ± 1.5 48.4 ± 1.7 55.8 ± 1.0 56.8 ± 2.0 PsychExp 25.8 20.3 38.3 ± 0.3 38.7 ± 1.0 40.9 ± 0.5 45.8 ± 1.6 56.6 ± 0.5 21.8 ± 0.3 44.0 ± 0.7 40.2 ± 1.1 69.0 57.0 (Felbo et al., 2017) Table 2: Ablation study on various downstream datasets. Average over five runs with standard deviation. BoW stands for Bag of Words, NBoW for Neural Bag of Words. P-LM stands for a classifier initialized with our pretrained LM, su for sequential unfreezing and aux for the auxiliary LM loss."
N19-1213,S17-2094,0,0.0411676,"Missing"
N19-1213,W02-0109,0,0.0941765,"English Twitter messages, including approximately 2M unique tokens. We use the 70K most frequent tokens as vocabulary. We evaluate our model on five datasets: Sent17 for sentiment analysis (Rosenthal et al., 2017), PsychExp for emotion recognition (Wallbott and Scherer, 1986), Irony18 for irony detection (Van Hee et al., 2018), SCv1 and SCv2 for sarcasm detection (Oraby et al., 2016; Lukin and Walker, 2013). More details about the datasets can be found in Table 1. 4.2 Experimental Setup To preprocess the tweets, we use Ekphrasis (Baziotis et al., 2017). For the generic datasets, we use NLTK (Loper and Bird, 2002). For the NBoW baseline, we use word2vec (Mikolov et al., 2013) 300-dimensional embeddings as features. 2091 Irony18 43.7 45.2 42.7 ± 0.6 41.8 ± 1.2 45.5 ± 0.9 47.0 ± 1.1 Sent17 61.0 63.0 61.2 ± 0.7 62.1 ± 0.8 65.1 ± 0.6 66.5 ± 0.2 SCv2 65.1 61.1 69.4 ± 0.4 69.9 ± 1.0 72.6 ± 0.7 75.0 ± 0.7 23.6 ± 1.6 41.6 ± 0.7 53.6 (Baziotis et al., 2018) 60.5 ± 0.5 65.6 ± 0.4 68.5 (Cliche, 2017) 68.7 ± 0.6 67.2 ± 0.9 76.0 (Ilic et al., 2018) BoW NBoW P-LM P-LM + su P-LM + aux SiATL (P-LM + aux + su) ULMFiT (Wiki-103) ULMFiT (Twitter) State of the art SCv1 60.9 51.9 48.5 ± 1.5 48.4 ± 1.7 55.8 ± 1.0 56.8 ± 2.0"
N19-1213,W13-1104,0,0.0223947,"nential decay for γ over the training epochs. Domain Tweets Tweets Debate Forums Debate Forums Experiences 4 4.1 Experiments and Results Datasets To pretrain the language model, we collect a dataset of 20 million English Twitter messages, including approximately 2M unique tokens. We use the 70K most frequent tokens as vocabulary. We evaluate our model on five datasets: Sent17 for sentiment analysis (Rosenthal et al., 2017), PsychExp for emotion recognition (Wallbott and Scherer, 1986), Irony18 for irony detection (Van Hee et al., 2018), SCv1 and SCv2 for sarcasm detection (Oraby et al., 2016; Lukin and Walker, 2013). More details about the datasets can be found in Table 1. 4.2 Experimental Setup To preprocess the tweets, we use Ekphrasis (Baziotis et al., 2017). For the generic datasets, we use NLTK (Loper and Bird, 2002). For the NBoW baseline, we use word2vec (Mikolov et al., 2013) 300-dimensional embeddings as features. 2091 Irony18 43.7 45.2 42.7 ± 0.6 41.8 ± 1.2 45.5 ± 0.9 47.0 ± 1.1 Sent17 61.0 63.0 61.2 ± 0.7 62.1 ± 0.8 65.1 ± 0.6 66.5 ± 0.2 SCv2 65.1 61.1 69.4 ± 0.4 69.9 ± 1.0 72.6 ± 0.7 75.0 ± 0.7 23.6 ± 1.6 41.6 ± 0.7 53.6 (Baziotis et al., 2018) 60.5 ± 0.5 65.6 ± 0.4 68.5 (Cliche, 2017) 68.7 ±"
N19-1213,W16-3604,0,0.0560945,"Missing"
N19-1213,D14-1162,0,0.0857759,"achieve competitive results, while also being intuitively simple and computationally effective. 2) We address the problem of catastrophic forgetting, by adding an auxiliary LM objective and using an unfreezing method. 3) Our results show that our approach is competitive with more sophisticated transfer learning methods. We make our code widely available. 1 2 Related Work Unsupervised pretraining has played a key role in deep neural networks, building on the premise that representations learned for one task can be useful for another task. In NLP, pretrained word vectors (Mikolov et al., 2013; Pennington et al., 2014) are widely used, improving performance in various downstream tasks, such as part-of-speech tagging (Collobert et al., 2011) and question answering (Xiong et al., 2016). These pretrained word vectors serve as initialization of the embedding layer and remain frozen during training, while our pretrained language model also initializes the hidden layers of the model and is fine-tuned to each 1 /github.com/alexandra-chron/siatl 2089 Proceedings of NAACL-HLT 2019, pages 2089–2095 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics classification task. Aim"
N19-1213,P17-1161,0,0.0248209,"the network and we train our models end-toend in a single step. We present results on a variety of challenging affective and text classification tasks, surpassing well established transfer learning methods with greater level of complexity. 1 Introduction Pretrained word representations captured by Language Models (LMs) have recently become popular in Natural Language Processing (NLP). Pretrained LMs encode contextual information and high-level features of language, modeling syntax and semantics, producing state-of-the-art results across a wide range of tasks, such as named entity recognition (Peters et al., 2017), machine translation (Ramachandran et al., 2017) and text classification (Howard and Ruder, 2018). However, in cases where contextual embeddings from language models are used as additional features (e.g. ELMo (Peters et al., 2018)), results come at a high computational cost and require task-specific architectures. At the same time, approaches that rely on fine-tuning a LM to the task at hand (e.g. ULMFiT (Howard and Ruder, 2018)) depend on pretraining the model on an extensive vocabulary and on employing a sophisticated slanted triangular learning rate scheme to adapt the parameters of the LM"
N19-1213,N18-1202,0,0.43491,"plexity. 1 Introduction Pretrained word representations captured by Language Models (LMs) have recently become popular in Natural Language Processing (NLP). Pretrained LMs encode contextual information and high-level features of language, modeling syntax and semantics, producing state-of-the-art results across a wide range of tasks, such as named entity recognition (Peters et al., 2017), machine translation (Ramachandran et al., 2017) and text classification (Howard and Ruder, 2018). However, in cases where contextual embeddings from language models are used as additional features (e.g. ELMo (Peters et al., 2018)), results come at a high computational cost and require task-specific architectures. At the same time, approaches that rely on fine-tuning a LM to the task at hand (e.g. ULMFiT (Howard and Ruder, 2018)) depend on pretraining the model on an extensive vocabulary and on employing a sophisticated slanted triangular learning rate scheme to adapt the parameters of the LM to the target dataset. We propose a simple and effective transfer learning approach, that leverages LM contextual representations and does not require any elaborate scheduling schemes during training. We initially train a LM on a"
N19-1213,D17-1039,0,0.146617,"nd in a single step. We present results on a variety of challenging affective and text classification tasks, surpassing well established transfer learning methods with greater level of complexity. 1 Introduction Pretrained word representations captured by Language Models (LMs) have recently become popular in Natural Language Processing (NLP). Pretrained LMs encode contextual information and high-level features of language, modeling syntax and semantics, producing state-of-the-art results across a wide range of tasks, such as named entity recognition (Peters et al., 2017), machine translation (Ramachandran et al., 2017) and text classification (Howard and Ruder, 2018). However, in cases where contextual embeddings from language models are used as additional features (e.g. ELMo (Peters et al., 2018)), results come at a high computational cost and require task-specific architectures. At the same time, approaches that rely on fine-tuning a LM to the task at hand (e.g. ULMFiT (Howard and Ruder, 2018)) depend on pretraining the model on an extensive vocabulary and on employing a sophisticated slanted triangular learning rate scheme to adapt the parameters of the LM to the target dataset. We propose a simple and e"
N19-1213,S17-2088,0,0.0172585,"d noise to the pretrained representation. To avoid this issue, we choose to initially pay attention to the LM objective and gradually focus on the classification task. In this paper, we use an exponential decay for γ over the training epochs. Domain Tweets Tweets Debate Forums Debate Forums Experiences 4 4.1 Experiments and Results Datasets To pretrain the language model, we collect a dataset of 20 million English Twitter messages, including approximately 2M unique tokens. We use the 70K most frequent tokens as vocabulary. We evaluate our model on five datasets: Sent17 for sentiment analysis (Rosenthal et al., 2017), PsychExp for emotion recognition (Wallbott and Scherer, 1986), Irony18 for irony detection (Van Hee et al., 2018), SCv1 and SCv2 for sarcasm detection (Oraby et al., 2016; Lukin and Walker, 2013). More details about the datasets can be found in Table 1. 4.2 Experimental Setup To preprocess the tweets, we use Ekphrasis (Baziotis et al., 2017). For the generic datasets, we use NLTK (Loper and Bird, 2002). For the NBoW baseline, we use word2vec (Mikolov et al., 2013) 300-dimensional embeddings as features. 2091 Irony18 43.7 45.2 42.7 ± 0.6 41.8 ± 1.2 45.5 ± 0.9 47.0 ± 1.1 Sent17 61.0 63.0 61.2"
N19-1213,P16-2038,0,0.0314976,", via the addition of an auxiliary LM loss to the transferred model, to control the loss of the pretrained and the new task simultaneously. The intuition is that we should avoid catastrophic forgetting, but at the same time allow the LM to distill the knowledge of the prior data distribution and keep the most useful features. Multi-Task Learning (MTL) via hard parameter sharing (Caruana, 1993) in neural networks has proven to be effective in many NLP problems (Collobert and Weston, 2008). More recently, alternative approaches have been suggested that only share parameters across lower layers (Sogaard and Goldberg, 2016). By introducing partof-speech tags at the lower levels of the network, the proposed model achieves competitive results on chunking and CCG super tagging. Our auxiliary language model objective follows this line of thought and intends to boost the performance of the higher classification layer. 3 Our Model We introduce SiATL, which stands for Single-step Auxiliary loss Transfer Learning. In our proposed approach, we first train a LM. We then transfer its weights and add a task-specific recurrent layer to the final classifier. We also employ an auxiliary LM loss to avoid catastrophic forgetting"
N19-1213,S18-1005,0,0.0418782,"Missing"
P19-1385,D17-1169,0,0.0481673,"Missing"
P19-1385,C18-1161,0,0.0214217,".gr Abstract Modern deep learning algorithms often do away with feature engineering and learn latent representations directly from raw data that are given as input to Deep Neural Networks (DNNs) (Mikolov et al., 2013; McCann et al., 2017; Peters et al., 2018). However, it has been shown that linguistic knowledge (manually or semi-automatically encoded into lexicons and knowledge bases) can significantly improve DNN performance for Natural Language Processing (NLP) tasks, such as natural language inference (Mrkši´c et al., 2017), language modelling (Ahn et al., 2016), named entity recognition (Ghaddar and Langlais, 2018) and relation extraction (Vashishth et al., 2018). For NLP tasks, external sources of information are typically incorporated into deep neural architectures by processing the raw input in the context of such external linguistic knowledge. In machine learning, this contextual processing is known as conditioning; the computation carried out by a model is conditioned or modulated by information extracted from an auxiliary input. The most commonly-used method of conditioning is concatenating a representation of the external information to the input or hidden network layers. Attention mechanisms (Ba"
P19-1385,P82-1020,0,0.830684,"Missing"
P19-1385,W18-6202,0,0.0376414,"Missing"
P19-1385,C04-1200,0,0.135849,"omputational overhead and can be adapted to any deep neural architecture. 1 Introduction 3944 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3944–3951 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 2 Related Work In the traditional machine learning literature where statistical models are based on sparse features, affective lexicons have been shown to be highly effective for tasks such as sentiment analysis, as they provide additional information not captured in the raw training data (Hu and Liu, 2004; Kim and Hovy, 2004; Ding et al., 2008; Yu and Dredze, 2014; Taboada et al., 2011). After the emergence of pretrained word representations (Mikolov et al., 2013; Pennington et al., 2014), the use of lexicons is no longer common practice, since word embeddings can also capture some of the affective meaning of these words. Recently, there have been notable contributions towards integrating linguistic knowledge into DNNs for various NLP tasks. For sentiment analysis, Teng et al. (2016) integrate lexicon features to an RNN-based model with a custom weightedsum calculation of word features. Shin et al. (2017) propose"
P19-1385,N18-2041,0,0.151027,"al., 2013; Pennington et al., 2014), the use of lexicons is no longer common practice, since word embeddings can also capture some of the affective meaning of these words. Recently, there have been notable contributions towards integrating linguistic knowledge into DNNs for various NLP tasks. For sentiment analysis, Teng et al. (2016) integrate lexicon features to an RNN-based model with a custom weightedsum calculation of word features. Shin et al. (2017) propose three convolutional neural network specific methods of lexicon integration achieving state-of-the-art performance on two datasets. Kumar et al. (2018) concatenate features from a knowledge base to word representations in an attentive bidirectional LSTM architecture, also reporting state-of-the-art results. For sarcasm detection, Yang et al. (2017) incorporate psycholinguistic, stylistic, structural, and readability features by concatenating them to paragraph and documentlevel representations. Furthermore, there is limited literature regarding the development and evaluation of methods for combining representations in deep neural networks. Peters et al. (2017) claim that concatenation, non-linear mapping and attention-like mechanisms are unex"
P19-1385,P17-1168,0,0.0276508,"extracted from an auxiliary input. The most commonly-used method of conditioning is concatenating a representation of the external information to the input or hidden network layers. Attention mechanisms (Bahdanau et al., 2015; Vaswani et al., 2017; Lin et al., 2017) are a key ingredient for achieving state-of-the-art performance in tasks such as textual entailment (Rocktäschel et al., 2016), question answering (Xiong et al., 2017), and neural machine translation (Wu et al., 2016). Often task-specific attentional architectures are proposed in the literature to further improve DNN performance (Dhingra et al., 2017; Xu et al., 2015; Barrett et al., 2018). In this work, we propose a novel way of utilizing word-level prior information encoded in linguistic, sentiment, and emotion lexicons, to improve classification performance. Usually, lexicon features are concatenated to word-level representations (Wang et al., 2016; Yang et al., 2017; Trotzek et al., 2018), as additional features to the embedding of each word or the hidden states of the model. By contrast, we propose to incorporate them into the self-attention mechanism of RNNs. Our goal is to enable the self-attention mechanism to identify the most in"
P19-1385,W13-1104,0,0.0215233,"Missing"
P19-1385,N18-1202,0,0.0249538,"ziotis2 ∗, Alexandros Potamianos1,3,4 1 School of ECE, National Technical University of Athens, Athens, Greece 2 School of Informatics, University of Edinburgh, UK 3 Signal Analysis and Interpretation Laboratory (SAIL), USC, Los Angeles, USA 4 Behavioral Signal Technologies, Los Angeles, USA el12108@central.ntua.gr, c.baziotis@sms.ed.ac.uk, potam@central.ntua.gr Abstract Modern deep learning algorithms often do away with feature engineering and learn latent representations directly from raw data that are given as input to Deep Neural Networks (DNNs) (Mikolov et al., 2013; McCann et al., 2017; Peters et al., 2018). However, it has been shown that linguistic knowledge (manually or semi-automatically encoded into lexicons and knowledge bases) can significantly improve DNN performance for Natural Language Processing (NLP) tasks, such as natural language inference (Mrkši´c et al., 2017), language modelling (Ahn et al., 2016), named entity recognition (Ghaddar and Langlais, 2018) and relation extraction (Vashishth et al., 2018). For NLP tasks, external sources of information are typically incorporated into deep neural architectures by processing the raw input in the context of such external linguistic knowl"
P19-1385,P17-1161,0,0.0367217,"thods of lexicon integration achieving state-of-the-art performance on two datasets. Kumar et al. (2018) concatenate features from a knowledge base to word representations in an attentive bidirectional LSTM architecture, also reporting state-of-the-art results. For sarcasm detection, Yang et al. (2017) incorporate psycholinguistic, stylistic, structural, and readability features by concatenating them to paragraph and documentlevel representations. Furthermore, there is limited literature regarding the development and evaluation of methods for combining representations in deep neural networks. Peters et al. (2017) claim that concatenation, non-linear mapping and attention-like mechanisms are unexplored methods for including language model representations in their sequence model. They employ simple concatenation, leaving the exploration of other methods to future work. Dumoulin et al. (2018) provide an overview of feature-wise transformations such as concatenation-based conditioning, conditional biasing and gating mechanisms. They review the effectiveness of conditioning methods in tasks such as visual question answering (Strub et al., 2018), style transfer (Dumoulin et al., 2017) and language modeling"
P19-1385,S17-2088,0,0.0335129,"Missing"
P19-1385,Q17-1022,0,0.0356064,"Missing"
P19-1385,W16-3604,0,0.0263709,"Missing"
P19-1385,W17-5220,0,0.0357001,", 2004; Kim and Hovy, 2004; Ding et al., 2008; Yu and Dredze, 2014; Taboada et al., 2011). After the emergence of pretrained word representations (Mikolov et al., 2013; Pennington et al., 2014), the use of lexicons is no longer common practice, since word embeddings can also capture some of the affective meaning of these words. Recently, there have been notable contributions towards integrating linguistic knowledge into DNNs for various NLP tasks. For sentiment analysis, Teng et al. (2016) integrate lexicon features to an RNN-based model with a custom weightedsum calculation of word features. Shin et al. (2017) propose three convolutional neural network specific methods of lexicon integration achieving state-of-the-art performance on two datasets. Kumar et al. (2018) concatenate features from a knowledge base to word representations in an attentive bidirectional LSTM architecture, also reporting state-of-the-art results. For sarcasm detection, Yang et al. (2017) incorporate psycholinguistic, stylistic, structural, and readability features by concatenating them to paragraph and documentlevel representations. Furthermore, there is limited literature regarding the development and evaluation of methods"
P19-1385,D13-1170,0,0.0109593,"Missing"
P19-1385,J11-2001,0,0.127677,"architecture. 1 Introduction 3944 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3944–3951 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 2 Related Work In the traditional machine learning literature where statistical models are based on sparse features, affective lexicons have been shown to be highly effective for tasks such as sentiment analysis, as they provide additional information not captured in the raw training data (Hu and Liu, 2004; Kim and Hovy, 2004; Ding et al., 2008; Yu and Dredze, 2014; Taboada et al., 2011). After the emergence of pretrained word representations (Mikolov et al., 2013; Pennington et al., 2014), the use of lexicons is no longer common practice, since word embeddings can also capture some of the affective meaning of these words. Recently, there have been notable contributions towards integrating linguistic knowledge into DNNs for various NLP tasks. For sentiment analysis, Teng et al. (2016) integrate lexicon features to an RNN-based model with a custom weightedsum calculation of word features. Shin et al. (2017) propose three convolutional neural network specific methods of lexicon"
P19-1385,D14-1162,0,0.0840596,"tational Linguistics, pages 3944–3951 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 2 Related Work In the traditional machine learning literature where statistical models are based on sparse features, affective lexicons have been shown to be highly effective for tasks such as sentiment analysis, as they provide additional information not captured in the raw training data (Hu and Liu, 2004; Kim and Hovy, 2004; Ding et al., 2008; Yu and Dredze, 2014; Taboada et al., 2011). After the emergence of pretrained word representations (Mikolov et al., 2013; Pennington et al., 2014), the use of lexicons is no longer common practice, since word embeddings can also capture some of the affective meaning of these words. Recently, there have been notable contributions towards integrating linguistic knowledge into DNNs for various NLP tasks. For sentiment analysis, Teng et al. (2016) integrate lexicon features to an RNN-based model with a custom weightedsum calculation of word features. Shin et al. (2017) propose three convolutional neural network specific methods of lexicon integration achieving state-of-the-art performance on two datasets. Kumar et al. (2018) concatenate fea"
P19-1385,D16-1169,0,0.0611096,"Missing"
P19-1385,S18-1005,0,0.0291241,"Missing"
P19-1385,D18-1157,0,0.059275,"Missing"
P19-1385,D17-1211,0,0.0575571,"Missing"
P19-1385,P14-2089,0,0.0192208,"d to any deep neural architecture. 1 Introduction 3944 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3944–3951 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 2 Related Work In the traditional machine learning literature where statistical models are based on sparse features, affective lexicons have been shown to be highly effective for tasks such as sentiment analysis, as they provide additional information not captured in the raw training data (Hu and Liu, 2004; Kim and Hovy, 2004; Ding et al., 2008; Yu and Dredze, 2014; Taboada et al., 2011). After the emergence of pretrained word representations (Mikolov et al., 2013; Pennington et al., 2014), the use of lexicons is no longer common practice, since word embeddings can also capture some of the affective meaning of these words. Recently, there have been notable contributions towards integrating linguistic knowledge into DNNs for various NLP tasks. For sentiment analysis, Teng et al. (2016) integrate lexicon features to an RNN-based model with a custom weightedsum calculation of word features. Shin et al. (2017) propose three convolutional neural network spec"
P19-1385,H05-2018,0,0.051538,". All reported measures are F1 scores, apart from SST − 5 which is evaluated with Accuracy. tors to the word representations in the embedding layer. In Table 3 we use the abbreviations “baseline” and “emb. conc.” for the two baseline models respectively. 4 Experiments Lexicon Features. As prior knowledge, we leverage the lexicons presented in Table 1. We selected widely-used lexicons that represent different facets of affective and psycho-linguistic features, namely; LIWC (Tausczik and Pennebaker, 2010), Bing Liu Opinion Lexicon (Hu and Liu, 2004), AFINN (Nielsen, 2011), Subjectivity Lexicon (Wilson et al., 2005), SemEval 2015 English Twitter Lexicon (Svetlana Kiritchenko and Mohammad, 2014), and NRC Emotion Lexicon (EmoLex) (Mohammad and Turney, 2013). Datasets. The proposed framework can be applied to different domains and tasks. In this paper, we experiment with sentiment analysis, emotion recognition, irony, and sarcasm detection. Details of the benchmark datasets are shown in Table 2. Preprocessing. To preprocess the words, we use the tool Ekphrasis (Baziotis et al., 2017). After tokenization, we map each word to the corresponding pretrained word representation: Twitterspecific word2vec embedding"
poesio-etal-2010-babyexp,J07-2002,0,\N,Missing
poesio-etal-2010-babyexp,P98-2127,0,\N,Missing
poesio-etal-2010-babyexp,C98-2122,0,\N,Missing
S12-1082,J10-4006,0,0.0656142,"categorization (Malandrakis et al., 2011). In this work, we built on previous research on word-level semantic similarity estimation to design and implement a system for sentence-level STS for Task6 of the SemEval’12 campaign. Semantic similarity between words can be regarded as the graded semantic equivalence at the lexeme level and is tightly related with the tasks of word sense discovery and disambiguation (Agirre and Edmonds, 2007). Metrics of word semantic similarity can be divided into: (i) knowledge-based metrics (Miller, 1990; Budanitsky and Hirst, 2006) and (ii) corpus-based metrics (Baroni and Lenci, 2010; Iosif and Potamianos, 2010). When more complex structures, such as phrases and sentences, are considered, it is much harder to estimate semantic equivalence due to the noncompositional nature of sentence-level semantics and the exponential explosion of possible interpretations. STS is closely related to the problems of paraphrasing, which is bidirectional and based on semantic equivalence (Madnani and Dorr, 2010) and textual entailment, which is directional and based on relations between semantics (Dagan et al., 2006). Related methods incorporate measurements of similarity at various levels:"
S12-1082,H05-1079,0,0.135187,"ntic equivalence due to the noncompositional nature of sentence-level semantics and the exponential explosion of possible interpretations. STS is closely related to the problems of paraphrasing, which is bidirectional and based on semantic equivalence (Madnani and Dorr, 2010) and textual entailment, which is directional and based on relations between semantics (Dagan et al., 2006). Related methods incorporate measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). Measures from machine translation evaluation are often used to evaluate lexical level approaches (Finch et al., 2005; Perez and Alfonseca, 2005), including BLEU (Papineni et al., 2002), a metric based on word ngram hit rates. Motivated by BLEU, we use n-gram hit rates and word-level semantic similarity scores as features in 565 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 565–570, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics a linear regression model to estimate sentence level semantic similarity. We also propose sigmoid s"
S12-1082,J90-1003,0,0.1942,"o estimate sentence level semantic similarity. We also propose sigmoid scaling of similarity scores and sentence-length dependent modeling. The models are evaluated on the SemEval’12 sentence similarity task. 2 Semantic similarity between words In this section, two different metrics of word similarity are presented. The first is a language-agnostic, corpus-based metric requiring no knowledge resources, while the second metric relies on WordNet. Corpus-based metric: Given a corpus, the semantic similarity between two words, wi and wj , is estimated as their pointwise mutual information ˆ(i,j) (Church and Hanks, 1990): I(i, j) = log pˆp(i)ˆ p(j) , where pˆ(i) and pˆ(j) are the occurrence probabilities of wi and wj , respectively, while the probability of their co-occurrence is denoted by pˆ(i, j). These probabilities are computed according to maximum likelihood estimation. The assumption of this metric is that co-occurrence implies semantic similarity. During the past decade the web has been used for estimating the required probabilities (Turney, 2001; Bollegala et al., 2007), by querying web search engines and retrieving the number of hits required to estimate the frequency of individual words and their c"
S12-1082,I05-5003,0,0.127742,"interpretations. STS is closely related to the problems of paraphrasing, which is bidirectional and based on semantic equivalence (Madnani and Dorr, 2010) and textual entailment, which is directional and based on relations between semantics (Dagan et al., 2006). Related methods incorporate measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). Measures from machine translation evaluation are often used to evaluate lexical level approaches (Finch et al., 2005; Perez and Alfonseca, 2005), including BLEU (Papineni et al., 2002), a metric based on word ngram hit rates. Motivated by BLEU, we use n-gram hit rates and word-level semantic similarity scores as features in 565 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 565–570, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics a linear regression model to estimate sentence level semantic similarity. We also propose sigmoid scaling of similarity scores and sentence-length dependent modeling. The models are evaluated on the SemEval’12 sentenc"
S12-1082,P05-1045,0,0.0104081,"tion of (overlapping) linear regression models, each matching a range of sentence lengths. For example, the first model DL1 is trained with sentences with length up to l1 , i.e., l ≤ l1 , the second model DL2 up to length l2 etc. During testing, sentences with length l ∈ [1, l1 ] are decoded with DL1 , sentences with length l ∈ (l1 , l2 ] with model DL2 etc. Each of these partial models is a linear fusion model as shown in (2). In this work, we use four models with l1 = 10, l2 = 20, l3 = 30, l4 = ∞. 4 Experimental Procedure and Results Initially all sentences are pre-processed by the CoreNLP (Finkel et al., 2005; Toutanova et al., 2003) suite of tools, a process that includes named entity recognition, normalization, part of speech tagging, lemmatization and stemming. The exact type of pre-processing used depends on the metric used. For the plain lexical BLEU, we use lemmatization, stemming (of lemmas) and remove all non-content words, keeping only nouns, adjectives, verbs and adverbs. For computing semantic similarity scores, we don’t use stemming and keep only noun words, since we only have similarities between non-noun words. For the computation of semantic similarity we have created a dictionary c"
S12-1082,P06-1114,0,0.0739228,"oderate improvement on Task 6 of SemEval’12. Despite the simple features used, regression models provide good performance, especially for shorter sentences, reaching correlation of 0.62 on the SemEval test set. 1 Introduction Recently, there has been significant research activity on the area of semantic similarity estimation motivated both by abundance of relevant web data and linguistic resources for this task. Algorithms for computing semantic textual similarity (STS) are relevant for a variety of applications, including information extraction (Szpektor and Dagan, 2008), question answering (Harabagiu and Hickl, 2006) and machine translation (Mirkin et al., 2009). Wordor term-level STS (a special case of sentence level STS) has also been successfully applied to the problem of grammar induction (Meng and Siu, 2002) and affective text categorization (Malandrakis et al., 2011). In this work, we built on previous research on word-level semantic similarity estimation to design and implement a system for sentence-level STS for Task6 of the SemEval’12 campaign. Semantic similarity between words can be regarded as the graded semantic equivalence at the lexeme level and is tightly related with the tasks of word sen"
S12-1082,iosif-potamianos-2012-semsim,1,0.573424,"ities (Turney, 2001; Bollegala et al., 2007), by querying web search engines and retrieving the number of hits required to estimate the frequency of individual words and their co-occurrence. However, these approaches have failed to obtain state-of-the-art results (Bollegala et al., 2007), unless “expensive” conjunctive AND queries are used for harvesting a corpus and then using this corpus to estimate similarity scores (Iosif and Potamianos, 2010). Recently, a scalable approach1 for harvesting a corpus has been proposed where web snippets are downloaded using individual queries for each word (Iosif and Potamianos, 2012b). Semantic similarity can then be estimated using the I(i, j) metric and within-snippet word co-occurrence frequencies. Under the maximum sense similarity assumption (Resnik, 1995), it is relatively easy to show that a (more) lexically-balanced corpus2 (as the one cre1 The scalability of this approach has been demonstrated in (Iosif and Potamianos, 2012b) for a 10K vocabulary, here we extend it to the full 60K WordNet vocabulary. 2 According to this assumption the semantic similarity of two words can be estimated as the minimum pairwise similarity of their senses. The gist of the argument is"
S12-1082,J10-3003,0,0.0180427,"rre and Edmonds, 2007). Metrics of word semantic similarity can be divided into: (i) knowledge-based metrics (Miller, 1990; Budanitsky and Hirst, 2006) and (ii) corpus-based metrics (Baroni and Lenci, 2010; Iosif and Potamianos, 2010). When more complex structures, such as phrases and sentences, are considered, it is much harder to estimate semantic equivalence due to the noncompositional nature of sentence-level semantics and the exponential explosion of possible interpretations. STS is closely related to the problems of paraphrasing, which is bidirectional and based on semantic equivalence (Madnani and Dorr, 2010) and textual entailment, which is directional and based on relations between semantics (Dagan et al., 2006). Related methods incorporate measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). Measures from machine translation evaluation are often used to evaluate lexical level approaches (Finch et al., 2005; Perez and Alfonseca, 2005), including BLEU (Papineni et al., 2002), a metric based on word ngram hit rates. Motivated by BLEU, we use"
S12-1082,W07-1407,0,0.393324,"d Potamianos, 2010). When more complex structures, such as phrases and sentences, are considered, it is much harder to estimate semantic equivalence due to the noncompositional nature of sentence-level semantics and the exponential explosion of possible interpretations. STS is closely related to the problems of paraphrasing, which is bidirectional and based on semantic equivalence (Madnani and Dorr, 2010) and textual entailment, which is directional and based on relations between semantics (Dagan et al., 2006). Related methods incorporate measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). Measures from machine translation evaluation are often used to evaluate lexical level approaches (Finch et al., 2005; Perez and Alfonseca, 2005), including BLEU (Papineni et al., 2002), a metric based on word ngram hit rates. Motivated by BLEU, we use n-gram hit rates and word-level semantic similarity scores as features in 565 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 565–570, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computatio"
S12-1082,P09-3004,0,0.236078,"uch as phrases and sentences, are considered, it is much harder to estimate semantic equivalence due to the noncompositional nature of sentence-level semantics and the exponential explosion of possible interpretations. STS is closely related to the problems of paraphrasing, which is bidirectional and based on semantic equivalence (Madnani and Dorr, 2010) and textual entailment, which is directional and based on relations between semantics (Dagan et al., 2006). Related methods incorporate measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). Measures from machine translation evaluation are often used to evaluate lexical level approaches (Finch et al., 2005; Perez and Alfonseca, 2005), including BLEU (Papineni et al., 2002), a metric based on word ngram hit rates. Motivated by BLEU, we use n-gram hit rates and word-level semantic similarity scores as features in 565 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 565–570, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics a linear regres"
S12-1082,P09-1089,0,0.0810875,"the simple features used, regression models provide good performance, especially for shorter sentences, reaching correlation of 0.62 on the SemEval test set. 1 Introduction Recently, there has been significant research activity on the area of semantic similarity estimation motivated both by abundance of relevant web data and linguistic resources for this task. Algorithms for computing semantic textual similarity (STS) are relevant for a variety of applications, including information extraction (Szpektor and Dagan, 2008), question answering (Harabagiu and Hickl, 2006) and machine translation (Mirkin et al., 2009). Wordor term-level STS (a special case of sentence level STS) has also been successfully applied to the problem of grammar induction (Meng and Siu, 2002) and affective text categorization (Malandrakis et al., 2011). In this work, we built on previous research on word-level semantic similarity estimation to design and implement a system for sentence-level STS for Task6 of the SemEval’12 campaign. Semantic similarity between words can be regarded as the graded semantic equivalence at the lexeme level and is tightly related with the tasks of word sense discovery and disambiguation (Agirre and Ed"
S12-1082,P02-1040,0,0.0844732,"hrasing, which is bidirectional and based on semantic equivalence (Madnani and Dorr, 2010) and textual entailment, which is directional and based on relations between semantics (Dagan et al., 2006). Related methods incorporate measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). Measures from machine translation evaluation are often used to evaluate lexical level approaches (Finch et al., 2005; Perez and Alfonseca, 2005), including BLEU (Papineni et al., 2002), a metric based on word ngram hit rates. Motivated by BLEU, we use n-gram hit rates and word-level semantic similarity scores as features in 565 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 565–570, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics a linear regression model to estimate sentence level semantic similarity. We also propose sigmoid scaling of similarity scores and sentence-length dependent modeling. The models are evaluated on the SemEval’12 sentence similarity task. 2 Semantic similarity between words In this secti"
S12-1082,W06-2501,0,0.039545,"rage in the semantic neighborhood of each word. The Ia (i, j) metric using the estimated value of α = 0.8 was shown to significantly outperform I(i, j) and to achieve state-of-the-art results on standard semantic similarity datasets (Rubenstein and Goodenough, 1965; Miller and Charles, 1998; Finkelstein et al., 2002). For more details see (Iosif and Potamianos, 2012a). WordNet-based metrics: For comparison purposes, we evaluated various similarity metrics on the task of word similarity computation on three standard datasets (same as above). The best results were obtained by the Vector metric (Patwardhan and Pedersen, 2006), which exploits the lexical information that is included in the WordNet glosses. This metric was incorporated to our proposed approach. All metrics were computed using the WordNet::Similarity module (Pedersen, 2005). 3 N-gram Regression Models Inspired by BLEU (Papineni et al., 2002), we propose a simple regression model that combines evidence from two sources: number of n-gram matches and degree of similarity between non-matching words between two sentences. In order to incorporate a word semantic similarity metric into BLEU, we apply the following two-pass process: first lexical hits are id"
S12-1082,W03-1604,0,0.390243,"arder to estimate semantic equivalence due to the noncompositional nature of sentence-level semantics and the exponential explosion of possible interpretations. STS is closely related to the problems of paraphrasing, which is bidirectional and based on semantic equivalence (Madnani and Dorr, 2010) and textual entailment, which is directional and based on relations between semantics (Dagan et al., 2006). Related methods incorporate measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). Measures from machine translation evaluation are often used to evaluate lexical level approaches (Finch et al., 2005; Perez and Alfonseca, 2005), including BLEU (Papineni et al., 2002), a metric based on word ngram hit rates. Motivated by BLEU, we use n-gram hit rates and word-level semantic similarity scores as features in 565 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 565–570, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics a linear regression model to estimate sentence level semantic similarity. W"
S12-1082,C08-1107,0,0.0751943,"formation at the sentence level provides only moderate improvement on Task 6 of SemEval’12. Despite the simple features used, regression models provide good performance, especially for shorter sentences, reaching correlation of 0.62 on the SemEval test set. 1 Introduction Recently, there has been significant research activity on the area of semantic similarity estimation motivated both by abundance of relevant web data and linguistic resources for this task. Algorithms for computing semantic textual similarity (STS) are relevant for a variety of applications, including information extraction (Szpektor and Dagan, 2008), question answering (Harabagiu and Hickl, 2006) and machine translation (Mirkin et al., 2009). Wordor term-level STS (a special case of sentence level STS) has also been successfully applied to the problem of grammar induction (Meng and Siu, 2002) and affective text categorization (Malandrakis et al., 2011). In this work, we built on previous research on word-level semantic similarity estimation to design and implement a system for sentence-level STS for Task6 of the SemEval’12 campaign. Semantic similarity between words can be regarded as the graded semantic equivalence at the lexeme level a"
S12-1082,N03-1033,0,0.00798592,"linear regression models, each matching a range of sentence lengths. For example, the first model DL1 is trained with sentences with length up to l1 , i.e., l ≤ l1 , the second model DL2 up to length l2 etc. During testing, sentences with length l ∈ [1, l1 ] are decoded with DL1 , sentences with length l ∈ (l1 , l2 ] with model DL2 etc. Each of these partial models is a linear fusion model as shown in (2). In this work, we use four models with l1 = 10, l2 = 20, l3 = 30, l4 = ∞. 4 Experimental Procedure and Results Initially all sentences are pre-processed by the CoreNLP (Finkel et al., 2005; Toutanova et al., 2003) suite of tools, a process that includes named entity recognition, normalization, part of speech tagging, lemmatization and stemming. The exact type of pre-processing used depends on the metric used. For the plain lexical BLEU, we use lemmatization, stemming (of lemmas) and remove all non-content words, keeping only nouns, adjectives, verbs and adverbs. For computing semantic similarity scores, we don’t use stemming and keep only noun words, since we only have similarities between non-noun words. For the computation of semantic similarity we have created a dictionary containing all the single-"
S12-1082,W07-1401,0,\N,Missing
S12-1082,J06-1003,0,\N,Missing
S13-1014,S12-1051,0,0.0422271,"and Potamianos, 2010) metrics proposed. Moving to sentences increases the complexity exponentially and as a result has led to measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). Machine translation evaluation metrics can be used to estimate lexical level similarity (Finch et al., 2005; Perez and Alfonseca, 2005), including BLEU (Papineni et al., 2002), a metric using word n-gram hit rates. The pilot task of sentence STS in SemEval 2012 (Agirre et al., 2012) showed a similar trend towards multi-level similarity, with the top performing systems utilizing large amounts of partial similarity metrics and domain adaptation (the use of separate models for each ˇ c et al., 2012). input domain) (B¨ar et al., 2012; Sari´ Our approach is originally motivated by BLEU and primarily utilizes “hard” and “soft” n-gram hit rates to estimate similarity. Compared to last year, we utilize different alignment strategies (to decide which n-grams should be compared with which). We also include string similarities (at the token and character level) and similarity of af"
S13-1014,S13-1004,0,0.01237,"ability of data afforded by the world wide web. Semantic textual similarity (STS) estimates can be used for information extraction (Szpektor and Dagan, 2008), question answering (Harabagiu and Hickl, 2006) and machine translation (Mirkin et al., 2009). Term-level similarity has been successfully applied to problems like grammar induction (Meng and Siu, 2002) and affective text categorization (Malandrakis et al., 2011). In this work, we built on previous research and our submission to SemEval’2012 (Malandrakis et al., 2012) to create a sentence-level STS model for the shared task of *SEM 2013 (Agirre et al., 2013). Semantic similarity between words has been well researched, with a variety of knowledge-based (Miller, 1990; Budanitsky and Hirst, 2006) and corpus-based (Baroni and Lenci, 2010; Iosif and Potamianos, 2010) metrics proposed. Moving to sentences increases the complexity exponentially and as a result has led to measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). Machine translation evaluation metrics can be used to estimate lexical level"
S13-1014,S12-1059,0,0.063895,"Missing"
S13-1014,J10-4006,0,0.0334155,"(Harabagiu and Hickl, 2006) and machine translation (Mirkin et al., 2009). Term-level similarity has been successfully applied to problems like grammar induction (Meng and Siu, 2002) and affective text categorization (Malandrakis et al., 2011). In this work, we built on previous research and our submission to SemEval’2012 (Malandrakis et al., 2012) to create a sentence-level STS model for the shared task of *SEM 2013 (Agirre et al., 2013). Semantic similarity between words has been well researched, with a variety of knowledge-based (Miller, 1990; Budanitsky and Hirst, 2006) and corpus-based (Baroni and Lenci, 2010; Iosif and Potamianos, 2010) metrics proposed. Moving to sentences increases the complexity exponentially and as a result has led to measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). Machine translation evaluation metrics can be used to estimate lexical level similarity (Finch et al., 2005; Perez and Alfonseca, 2005), including BLEU (Papineni et al., 2002), a metric using word n-gram hit rates. The pilot task of sentence STS in SemEva"
S13-1014,H05-1079,0,0.0184601,"., 2012) to create a sentence-level STS model for the shared task of *SEM 2013 (Agirre et al., 2013). Semantic similarity between words has been well researched, with a variety of knowledge-based (Miller, 1990; Budanitsky and Hirst, 2006) and corpus-based (Baroni and Lenci, 2010; Iosif and Potamianos, 2010) metrics proposed. Moving to sentences increases the complexity exponentially and as a result has led to measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). Machine translation evaluation metrics can be used to estimate lexical level similarity (Finch et al., 2005; Perez and Alfonseca, 2005), including BLEU (Papineni et al., 2002), a metric using word n-gram hit rates. The pilot task of sentence STS in SemEval 2012 (Agirre et al., 2012) showed a similar trend towards multi-level similarity, with the top performing systems utilizing large amounts of partial similarity metrics and domain adaptation (the use of separate models for each ˇ c et al., 2012). input domain) (B¨ar et al., 2012; Sari´ Our approach is originally motivated by BLEU and primar"
S13-1014,J90-1003,0,0.452894,"Missing"
S13-1014,I05-5003,0,0.0328084,"milarity between words has been well researched, with a variety of knowledge-based (Miller, 1990; Budanitsky and Hirst, 2006) and corpus-based (Baroni and Lenci, 2010; Iosif and Potamianos, 2010) metrics proposed. Moving to sentences increases the complexity exponentially and as a result has led to measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). Machine translation evaluation metrics can be used to estimate lexical level similarity (Finch et al., 2005; Perez and Alfonseca, 2005), including BLEU (Papineni et al., 2002), a metric using word n-gram hit rates. The pilot task of sentence STS in SemEval 2012 (Agirre et al., 2012) showed a similar trend towards multi-level similarity, with the top performing systems utilizing large amounts of partial similarity metrics and domain adaptation (the use of separate models for each ˇ c et al., 2012). input domain) (B¨ar et al., 2012; Sari´ Our approach is originally motivated by BLEU and primarily utilizes “hard” and “soft” n-gram hit rates to estimate similarity. Compared to last year, we utilize dif"
S13-1014,P05-1045,0,0.00594181,"nces with length l ∈ [1, l1 ] are decoded with DL1 , sentences with length l ∈ (l1 , l2 ] with model DL2 etc. Each of these partial models is a linear fusion model as shown in (4). In this work, we use four models with l1 = 10, l2 = 20, l3 = 30, l4 = ∞. Domain adaptation is employed, by creating separate models per domain (training data source). Beyond that, we also create a unified model, trained on all data to be used as a fallback if an appropriate model can not be decided upon during evaluation. 3 Experimental Procedure and Results Initially all sentences are pre-processed by the CoreNLP (Finkel et al., 2005; Toutanova et al., 2003) suite of tools, a process that includes named entity recognition, normalization, part of speech tagging, lemmatization and stemming. We evaluated multiple types of preprocessing per unsupervised metric and chose different ones depending on the metric. Word-level semantic similarities, used for soft comparisons and affective feature extraction, were computed over a corpus of 116 million web snippets collected by posing one query for every word in the Aspell spellchecker (asp, ) vocabulary to the Yahoo! search engine. Word-level emotional ratings in continuous valence a"
S13-1014,P06-1114,0,0.0117605,"xical semantic similarity between non-matching words, 3) string similarity metrics, 4) affective content similarity and 5) sentence length. Domain adaptation is applied in the form of independent models and a model selection strategy achieving a mean correlation of 0.47. 1 Introduction Text semantic similarity estimation has been an active research area, thanks to a variety of potential applications and the wide availability of data afforded by the world wide web. Semantic textual similarity (STS) estimates can be used for information extraction (Szpektor and Dagan, 2008), question answering (Harabagiu and Hickl, 2006) and machine translation (Mirkin et al., 2009). Term-level similarity has been successfully applied to problems like grammar induction (Meng and Siu, 2002) and affective text categorization (Malandrakis et al., 2011). In this work, we built on previous research and our submission to SemEval’2012 (Malandrakis et al., 2012) to create a sentence-level STS model for the shared task of *SEM 2013 (Agirre et al., 2013). Semantic similarity between words has been well researched, with a variety of knowledge-based (Miller, 1990; Budanitsky and Hirst, 2006) and corpus-based (Baroni and Lenci, 2010; Iosi"
S13-1014,iosif-potamianos-2012-semsim,1,0.832742,"nce is denoted by pˆ(i, j). In our previous participation in SemEval12-STS task (Malandrakis et al., 2012) we employed a modification of the pointwise mutual information based on the maximum sense similarity assumption (Resnik, 1995) and the minimization of the respective error in similarity estimation. In particular, exponential weights α were introduced in order to reduce the overestimation of denominator probabilities. The modified metric Ia (i, j), is defined as:   pˆ(i, j) pˆ(i, j) 1 + log log α . (1) Ia (i, j) = 2 pˆ (i)ˆ p(j) pˆ(i)ˆ pα (j) The weight α was estimated on the corpus of (Iosif and Potamianos, 2012) in order to maximize word sense coverage in the semantic neighborhood of each word. The Ia (i, j) metric using the estimated value of α = 0.8 was shown to significantly outperform I(i, j) and to achieve state-of-the-art results on standard semantic similarity datasets (Rubenstein and Goodenough, 1965; Miller and Charles, 1998; Finkelstein et al., 2002). Context-based: The fundamental assumption behind context-based metrics is that similarity of context implies similarity of meaning (Harris, 1954). A contextual window of size 2H + 1 words is centered on the word of interest wi and lexical feat"
S13-1014,W13-0205,1,0.839775,"and lexical features are extracted. For every instance of wi in the corpus the H words left and right of wi formulate a feature vector vi . For a given value of H the context-based semantic similarity between two words, wi and wj , is computed as the cosine v .v of their feature vectors: QH (i, j) = ||vi ||i ||vj j ||. The elements of feature vectors can be weighted 104 according various schemes [(Iosif and Potamianos, 2010)], while, here we use a binary scheme. Network-based: The aforementioned similarity metrics were used for the definition of a semantic network (Iosif and Potamianos, 2013; Iosif et al., 2013). A number of similarity metrics were proposed under either the attributional similarity (Turney, 2006) or the maximum sense similarity (Resnik, 1995) assumptions of lexical semantics1 . 2.2 Sentence level similarities To utilize word-level semantic similarities in the sentence-level task we use a modified version of BLEU (Papineni et al., 2002). The model works in two passes: the first pass identifies exact matches (similar to baseline BLEU), the second pass compares non-matched terms using semantic similarity. Non-matched terms from the hypothesis sentence are compared with all terms of the"
S13-1014,P04-1077,0,0.0272266,"ree-of-match”. The semantic similarity scores from term pairs are summed (just like n-gram hits) to obtain a BLEU-like hit-rate. Alignment is performed via maximum similarity: we iterate on the hypothesis n-grams, left-to-right, and compare each with the most similar n-gram in the reference. The features produced by this process are “soft” hit-rates (for 1-, 2-, 3-, 4-grams)2 . We also use the “hard” hit rates produced by baseline BLEU as features of the final model. 2.3 String similarities We use the following string-based similarity features: 1) Longest Common Subsequence Similarity (LCSS) (Lin and Och, 2004) based on the Longest Common Subsequence (LCS) character-based dy1 The network-based metrics were applied only during the training phase of the shared task, due to time limitations. They exhibited almost identical performance as the metric defined by (1), which was used in the test runs. 2 Note that the features are computed twice on each sentence pair and then averaged. namic programming algorithm. LCSS represents the length of the longest string (or strings) that is a substring (or are substrings) of two or more strings. 2) Skip bigram co-occurrence measures the overlap of skip-bigrams betwe"
S13-1014,W07-1407,0,0.0337907,"zation (Malandrakis et al., 2011). In this work, we built on previous research and our submission to SemEval’2012 (Malandrakis et al., 2012) to create a sentence-level STS model for the shared task of *SEM 2013 (Agirre et al., 2013). Semantic similarity between words has been well researched, with a variety of knowledge-based (Miller, 1990; Budanitsky and Hirst, 2006) and corpus-based (Baroni and Lenci, 2010; Iosif and Potamianos, 2010) metrics proposed. Moving to sentences increases the complexity exponentially and as a result has led to measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). Machine translation evaluation metrics can be used to estimate lexical level similarity (Finch et al., 2005; Perez and Alfonseca, 2005), including BLEU (Papineni et al., 2002), a metric using word n-gram hit rates. The pilot task of sentence STS in SemEval 2012 (Agirre et al., 2012) showed a similar trend towards multi-level similarity, with the top performing systems utilizing large amounts of partial similarity metrics and domain adaptation (the use of separate models for each"
S13-1014,P09-3004,0,0.0153907,"built on previous research and our submission to SemEval’2012 (Malandrakis et al., 2012) to create a sentence-level STS model for the shared task of *SEM 2013 (Agirre et al., 2013). Semantic similarity between words has been well researched, with a variety of knowledge-based (Miller, 1990; Budanitsky and Hirst, 2006) and corpus-based (Baroni and Lenci, 2010; Iosif and Potamianos, 2010) metrics proposed. Moving to sentences increases the complexity exponentially and as a result has led to measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). Machine translation evaluation metrics can be used to estimate lexical level similarity (Finch et al., 2005; Perez and Alfonseca, 2005), including BLEU (Papineni et al., 2002), a metric using word n-gram hit rates. The pilot task of sentence STS in SemEval 2012 (Agirre et al., 2012) showed a similar trend towards multi-level similarity, with the top performing systems utilizing large amounts of partial similarity metrics and domain adaptation (the use of separate models for each ˇ c et al., 2012). input domai"
S13-1014,S12-1082,1,0.330273,"as been an active research area, thanks to a variety of potential applications and the wide availability of data afforded by the world wide web. Semantic textual similarity (STS) estimates can be used for information extraction (Szpektor and Dagan, 2008), question answering (Harabagiu and Hickl, 2006) and machine translation (Mirkin et al., 2009). Term-level similarity has been successfully applied to problems like grammar induction (Meng and Siu, 2002) and affective text categorization (Malandrakis et al., 2011). In this work, we built on previous research and our submission to SemEval’2012 (Malandrakis et al., 2012) to create a sentence-level STS model for the shared task of *SEM 2013 (Agirre et al., 2013). Semantic similarity between words has been well researched, with a variety of knowledge-based (Miller, 1990; Budanitsky and Hirst, 2006) and corpus-based (Baroni and Lenci, 2010; Iosif and Potamianos, 2010) metrics proposed. Moving to sentences increases the complexity exponentially and as a result has led to measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and"
S13-1014,P09-1089,0,0.0148156,", 3) string similarity metrics, 4) affective content similarity and 5) sentence length. Domain adaptation is applied in the form of independent models and a model selection strategy achieving a mean correlation of 0.47. 1 Introduction Text semantic similarity estimation has been an active research area, thanks to a variety of potential applications and the wide availability of data afforded by the world wide web. Semantic textual similarity (STS) estimates can be used for information extraction (Szpektor and Dagan, 2008), question answering (Harabagiu and Hickl, 2006) and machine translation (Mirkin et al., 2009). Term-level similarity has been successfully applied to problems like grammar induction (Meng and Siu, 2002) and affective text categorization (Malandrakis et al., 2011). In this work, we built on previous research and our submission to SemEval’2012 (Malandrakis et al., 2012) to create a sentence-level STS model for the shared task of *SEM 2013 (Agirre et al., 2013). Semantic similarity between words has been well researched, with a variety of knowledge-based (Miller, 1990; Budanitsky and Hirst, 2006) and corpus-based (Baroni and Lenci, 2010; Iosif and Potamianos, 2010) metrics proposed. Movi"
S13-1014,P02-1040,0,0.0970445,"of knowledge-based (Miller, 1990; Budanitsky and Hirst, 2006) and corpus-based (Baroni and Lenci, 2010; Iosif and Potamianos, 2010) metrics proposed. Moving to sentences increases the complexity exponentially and as a result has led to measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). Machine translation evaluation metrics can be used to estimate lexical level similarity (Finch et al., 2005; Perez and Alfonseca, 2005), including BLEU (Papineni et al., 2002), a metric using word n-gram hit rates. The pilot task of sentence STS in SemEval 2012 (Agirre et al., 2012) showed a similar trend towards multi-level similarity, with the top performing systems utilizing large amounts of partial similarity metrics and domain adaptation (the use of separate models for each ˇ c et al., 2012). input domain) (B¨ar et al., 2012; Sari´ Our approach is originally motivated by BLEU and primarily utilizes “hard” and “soft” n-gram hit rates to estimate similarity. Compared to last year, we utilize different alignment strategies (to decide which n-grams should be compa"
S13-1014,W03-1604,0,0.0419236,"012 (Malandrakis et al., 2012) to create a sentence-level STS model for the shared task of *SEM 2013 (Agirre et al., 2013). Semantic similarity between words has been well researched, with a variety of knowledge-based (Miller, 1990; Budanitsky and Hirst, 2006) and corpus-based (Baroni and Lenci, 2010; Iosif and Potamianos, 2010) metrics proposed. Moving to sentences increases the complexity exponentially and as a result has led to measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). Machine translation evaluation metrics can be used to estimate lexical level similarity (Finch et al., 2005; Perez and Alfonseca, 2005), including BLEU (Papineni et al., 2002), a metric using word n-gram hit rates. The pilot task of sentence STS in SemEval 2012 (Agirre et al., 2012) showed a similar trend towards multi-level similarity, with the top performing systems utilizing large amounts of partial similarity metrics and domain adaptation (the use of separate models for each ˇ c et al., 2012). input domain) (B¨ar et al., 2012; Sari´ Our approach is originally moti"
S13-1014,C08-1107,0,0.0185681,"ates (lexical matches) between sentences, 2) lexical semantic similarity between non-matching words, 3) string similarity metrics, 4) affective content similarity and 5) sentence length. Domain adaptation is applied in the form of independent models and a model selection strategy achieving a mean correlation of 0.47. 1 Introduction Text semantic similarity estimation has been an active research area, thanks to a variety of potential applications and the wide availability of data afforded by the world wide web. Semantic textual similarity (STS) estimates can be used for information extraction (Szpektor and Dagan, 2008), question answering (Harabagiu and Hickl, 2006) and machine translation (Mirkin et al., 2009). Term-level similarity has been successfully applied to problems like grammar induction (Meng and Siu, 2002) and affective text categorization (Malandrakis et al., 2011). In this work, we built on previous research and our submission to SemEval’2012 (Malandrakis et al., 2012) to create a sentence-level STS model for the shared task of *SEM 2013 (Agirre et al., 2013). Semantic similarity between words has been well researched, with a variety of knowledge-based (Miller, 1990; Budanitsky and Hirst, 2006"
S13-1014,N03-1033,0,0.00394169,"[1, l1 ] are decoded with DL1 , sentences with length l ∈ (l1 , l2 ] with model DL2 etc. Each of these partial models is a linear fusion model as shown in (4). In this work, we use four models with l1 = 10, l2 = 20, l3 = 30, l4 = ∞. Domain adaptation is employed, by creating separate models per domain (training data source). Beyond that, we also create a unified model, trained on all data to be used as a fallback if an appropriate model can not be decided upon during evaluation. 3 Experimental Procedure and Results Initially all sentences are pre-processed by the CoreNLP (Finkel et al., 2005; Toutanova et al., 2003) suite of tools, a process that includes named entity recognition, normalization, part of speech tagging, lemmatization and stemming. We evaluated multiple types of preprocessing per unsupervised metric and chose different ones depending on the metric. Word-level semantic similarities, used for soft comparisons and affective feature extraction, were computed over a corpus of 116 million web snippets collected by posing one query for every word in the Aspell spellchecker (asp, ) vocabulary to the Yahoo! search engine. Word-level emotional ratings in continuous valence and arousal scales were pr"
S13-1014,J06-3003,0,0.0445139,"ormulate a feature vector vi . For a given value of H the context-based semantic similarity between two words, wi and wj , is computed as the cosine v .v of their feature vectors: QH (i, j) = ||vi ||i ||vj j ||. The elements of feature vectors can be weighted 104 according various schemes [(Iosif and Potamianos, 2010)], while, here we use a binary scheme. Network-based: The aforementioned similarity metrics were used for the definition of a semantic network (Iosif and Potamianos, 2013; Iosif et al., 2013). A number of similarity metrics were proposed under either the attributional similarity (Turney, 2006) or the maximum sense similarity (Resnik, 1995) assumptions of lexical semantics1 . 2.2 Sentence level similarities To utilize word-level semantic similarities in the sentence-level task we use a modified version of BLEU (Papineni et al., 2002). The model works in two passes: the first pass identifies exact matches (similar to baseline BLEU), the second pass compares non-matched terms using semantic similarity. Non-matched terms from the hypothesis sentence are compared with all terms of the reference sentence (regardless of whether they were matched during the first pass). In the case of bigr"
S13-1014,S12-1060,0,0.0508661,"Missing"
S13-1014,J06-1003,0,\N,Missing
S13-2072,C10-2028,0,0.0531713,"out current events (Petulla, 2013). Analyzing emotion expressed in twitter borrows from other tasks related to affective analysis, but also presents unique challenges. One common issue is the breadth of content available in twitter: a more limited domain would make the task easier, however there are no such bounds. There is also a significant difference in the form of language used in tweets. The tone is informal and typographical and grammatical errors are very common, making even simple tasks, like Part-of-Speech tagging much harder. Features like hashtags and emoticons can also be helpful (Davidov et al., 2010). This paper describes our submissions for SemEval 2013 task 2, subtask B, which deals primarily with sentiment analysis in twitter. For the constrained condition (using only the organizerprovided twitter sentences) we implemented a system based on the use of an affective lexicon and partof-speech tag information, which has been shown relevant to the task (Pak and Paroubek, 2010). For the unconstrained condition (including external sources of twitter sentences) we combine the constrained model with a maximum entropy language 438 Second Joint Conference on Lexical and Computational Semantics (*"
S13-2072,esuli-sebastiani-2006-sentiwordnet,0,0.045149,"ach proved successful, reaching rankings of 9th and 4th in the twitter sentiment analysis constrained and unconstrained scenario respectively, despite using only lexical features. 1 Introduction The analysis of the emotional content of text, is relevant to numerous natural language processing (NLP), web and multi-modal dialogue applications. To that end there has been a significant scientific effort towards tasks like product review analysis (Wiebe and Mihalcea, 2006; Hu and Liu, 2004), speech emotion extraction (Lee and Narayanan, 2005; Lee et al., 2002; Ang et al., 2002) and pure text word (Esuli and Sebastiani, 2006; Strapparava and Valitutti, 2004) and sentence (Turney and Littman, 2002; Turney and Littman, 2003) level emotion extraction. The rise of social media in recent years has seen a shift in research focus towards them, particularly twitter. The large volume of text data available is particularly useful, since it allows the use of complex machine learning methods. Also important is the interest on the part of companies that are actively looking for ways to mine social media for opinions and attitudes towards them and their products. Similarly, in journalism there is interest in sentiment analysis"
S13-2072,pak-paroubek-2010-twitter,0,0.019042,"used in tweets. The tone is informal and typographical and grammatical errors are very common, making even simple tasks, like Part-of-Speech tagging much harder. Features like hashtags and emoticons can also be helpful (Davidov et al., 2010). This paper describes our submissions for SemEval 2013 task 2, subtask B, which deals primarily with sentiment analysis in twitter. For the constrained condition (using only the organizerprovided twitter sentences) we implemented a system based on the use of an affective lexicon and partof-speech tag information, which has been shown relevant to the task (Pak and Paroubek, 2010). For the unconstrained condition (including external sources of twitter sentences) we combine the constrained model with a maximum entropy language 438 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 438–442, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics model trained on external data. 2 Experimental procedure We use two separate models, one for the constrained condition and a combination for the unconstrained condition. Following are short descri"
S13-2072,strapparava-valitutti-2004-wordnet,0,0.209922,"ing rankings of 9th and 4th in the twitter sentiment analysis constrained and unconstrained scenario respectively, despite using only lexical features. 1 Introduction The analysis of the emotional content of text, is relevant to numerous natural language processing (NLP), web and multi-modal dialogue applications. To that end there has been a significant scientific effort towards tasks like product review analysis (Wiebe and Mihalcea, 2006; Hu and Liu, 2004), speech emotion extraction (Lee and Narayanan, 2005; Lee et al., 2002; Ang et al., 2002) and pure text word (Esuli and Sebastiani, 2006; Strapparava and Valitutti, 2004) and sentence (Turney and Littman, 2002; Turney and Littman, 2003) level emotion extraction. The rise of social media in recent years has seen a shift in research focus towards them, particularly twitter. The large volume of text data available is particularly useful, since it allows the use of complex machine learning methods. Also important is the interest on the part of companies that are actively looking for ways to mine social media for opinions and attitudes towards them and their products. Similarly, in journalism there is interest in sentiment analysis for a way to process and report o"
S13-2072,P12-3020,1,0.804054,"aive Bayes tree, a tree with Naive Bayes classifiers on each leaf. The motivation comes by considering this a two stage problem: subjectivity detection and polarity classification, making a hierarchical model a natural choice. NB trees proved superior to other types of trees during our testing, presumably due to the smoothing of observation distributions. 2.2 N-gram language model The method used for the unconstrained condition is based on a combination of the automatically expanded affective lexicon described in the previous section together with a bigram language model based on the work of (Wang et al., 2012), which uses a large set of twitter data from the U.S. 2012 Presidential election. As a part of the unconstrained system, we were able to leverage external annotated data apart from those provided by the SEMEVAL 2013 sentiment task dataset. Of the 315 million tweets we collected about the election, we annotated a subset of 40 thousand tweets using Amazon Mechanical Turk. The annotation labels that we used were “positive”, “negative”, “neutral”, and “unsure”, and additionally raters could mark tweets for sarcasm and humor. We excluded tweets marked as “unsure” as well as tweets that had disagre"
S13-2072,P06-1134,0,0.0332578,"mum entropy language models and trained on a large external dataset. The two models are fused at the posterior level to produce a final output. The approach proved successful, reaching rankings of 9th and 4th in the twitter sentiment analysis constrained and unconstrained scenario respectively, despite using only lexical features. 1 Introduction The analysis of the emotional content of text, is relevant to numerous natural language processing (NLP), web and multi-modal dialogue applications. To that end there has been a significant scientific effort towards tasks like product review analysis (Wiebe and Mihalcea, 2006; Hu and Liu, 2004), speech emotion extraction (Lee and Narayanan, 2005; Lee et al., 2002; Ang et al., 2002) and pure text word (Esuli and Sebastiani, 2006; Strapparava and Valitutti, 2004) and sentence (Turney and Littman, 2002; Turney and Littman, 2003) level emotion extraction. The rise of social media in recent years has seen a shift in research focus towards them, particularly twitter. The large volume of text data available is particularly useful, since it allows the use of complex machine learning methods. Also important is the interest on the part of companies that are actively looking"
S14-2002,P02-1040,0,0.0902511,"roposed in (Sarikaya, 2008), where from each indomain utterance a set of queries of varying length and complexity was generated. These approaches assume the availability of in-domain data (even if limited) for the successful formulation of queries; this dependency is also not eliminated when using a mildly lexicalized domain ontology to formulate the queries, as in (Misu and Kawahara, 2006). Selecting the most relevant sentences that get returned from web queries is typically done using statistical similarity metrics between in domain data and retrieved documents, for example the BLEU metric (Papineni et al., 2002) of n3 Task Description Next we describe in detail the candidate grammar fragment classification SemEval task. This task is part of a grammar rule induction scenario for high-level rules. The evaluation focuses in spoken dialogue system grammars for multiple domains and languages. 3.1 Task Design The goal of the task is to classify a number fragment to the rules available in the grammar. For each grammar we provide a training and development set, i.e., a set of rules with the associated fragments and the test set which is composed of plain fragments. An excerpt of the train set for the rule “&lt;"
S14-2002,P07-3008,0,0.0322761,"ontology. Harvesting web data can produce high-quality grammars while requiring up to 10 times less in-domain data (Sarikaya, 2008). Further, data-driven approaches induce syntactic grammars but do not learn their corresponding meanings, for this purpose an additional step is required of parsing the grammar fragments and attaching them to the domain ontology (Sarikaya, 2008). Also, in many cases it was observed that the fully automated bottom-up paradigm results to grammars of moderate quality (Wang and Acero, 2006), especially on corpora containing longer sentences and more lexical variety (Cramer, 2007). Finally, algorithms focusing on crosslingual grammar induction, like CLIoS (Kuhn, 2004), are often even more resourceintensive, as they require training corpora of parallel text and sometimes also a grammar for one of the languages. Grammar quality can be improved by introducing a human in the loop of grammar induction (Portdial, 2014a); an expert that validates the automatically created results (Meng and Siu, 2002). parse tree. In (Georgiladakis et al., 2014), the explicit extraction and selection of fragments is investigated following an example-driven approach where few rule seeds are pro"
S14-2002,P11-1108,0,0.0487071,"Missing"
S14-2002,P04-1060,0,0.0318172,"imes less in-domain data (Sarikaya, 2008). Further, data-driven approaches induce syntactic grammars but do not learn their corresponding meanings, for this purpose an additional step is required of parsing the grammar fragments and attaching them to the domain ontology (Sarikaya, 2008). Also, in many cases it was observed that the fully automated bottom-up paradigm results to grammars of moderate quality (Wang and Acero, 2006), especially on corpora containing longer sentences and more lexical variety (Cramer, 2007). Finally, algorithms focusing on crosslingual grammar induction, like CLIoS (Kuhn, 2004), are often even more resourceintensive, as they require training corpora of parallel text and sometimes also a grammar for one of the languages. Grammar quality can be improved by introducing a human in the loop of grammar induction (Portdial, 2014a); an expert that validates the automatically created results (Meng and Siu, 2002). parse tree. In (Georgiladakis et al., 2014), the explicit extraction and selection of fragments is investigated following an example-driven approach where few rule seeds are provided by the grammar developer. The second sub-problem of highlevel rule induction deals"
S14-2002,W14-1505,0,\N,Missing
S14-2002,S12-1051,0,\N,Missing
S14-2089,S12-1051,0,0.0174539,"ippets (up to 1000 for each word in the Aspell spellchecker) collected using the Yahoo! search engine. 513 2.3 Tweet-level similarity ratings Table 1: Performance and rank achieved by our submission for all datasets of subtasks A and B. Our lexicon was formed under the assumption that semantic similarity implies affective similarity, which should apply to larger lexical units like entire tweets. To estimate semantic similarity scores between tweets we used the publicly availˇ c et able TakeLab semantic similarity toolkit (Sari´ al., 2012) which is based on a submission to SemEval 2012 task 6 (Agirre et al., 2012). We used the data of SemEval 2012 task 6 to train three semantic similarity models corresponding to the three datasets of that task, plus an overall model. Using these models we created four similarity ratings between each tweet of interest and each tweet in the training set. These similarity ratings were used as features of the final model. 2.4 task A B Character features rank 16 16 14 13 15 15 24 10 7 2 Table 2: Selected features for subtask B. Features Lexicon-derived By lexicon Ewrd / S140 / SWNet / NRC By POS tag all (ignore tag) adj / verb / proper noun other tags By function avg / min"
S14-2089,C10-2028,0,0.0889222,"Similarity and Contrast Features Nikolaos Malandrakis, Michael Falcone, Colin Vaz, Jesse Bisogni, Alexandros Potamianos, Shrikanth Narayanan Signal Analysis and Interpretation Laboratory (SAIL), USC, Los Angeles, CA 90089, USA {malandra,mfalcone,cvaz,jbisogni}@usc.edu, potam@telecom.tuc.gr, shri@sipi.usc.edu Abstract grammar can be “unconventional” and there are unique artifacts like hashtags. Computation systems, like those submitted to SemEval 2013 task 2 (Nakov et al., 2013) mostly use bag-of-words models with specific features added to model emotion indicators like hashtags and emoticons (Davidov et al., 2010). This paper describes our submissions to SemEval 2014 task 9 (Rosenthal et al., 2014), which deals with sentiment analysis in twitter. The system is an expansion of our submission to the same task in 2013 (Malandrakis et al., 2013a), which used only token rating statistics as features. We expanded the system by using multiple lexica and more statistics, added steps to the pre-processing stage (including negation and multi-word expression handling), incorporated pairwise tweet-level semantic similarities as features and finally performed feature extraction on substrings and used the partial fe"
S14-2089,strapparava-valitutti-2004-wordnet,0,0.0798378,"h on the main set and 2nd when applied to sarcastic tweets. 1 Introduction The analysis of the emotional content of text is relevant to numerous natural language processing (NLP), web and multi-modal dialogue applications. In recent years the increased popularity of social media and increased availability of relevant data has led to a focus of scientific efforts on the emotion expressed through social media, with Twitter being the most common subject. Sentiment analysis in Twitter is usually performed by combining techniques used for related tasks, like word-level (Esuli and Sebastiani, 2006; Strapparava and Valitutti, 2004) and sentencelevel (Turney and Littman, 2002; Turney and Littman, 2003) emotion extraction. Twitter however does present specific challenges: the breadth of possible content is virtually unlimited, the writing style is informal, the use of orthography and 2 2.1 Model Description Preprocessing POS-tagging / Tokenization was performed using the ARK NLP tweeter tagger (Owoputi et al., 2013), a Twitter-specific tagger. Negations were detected using the list from Christopher Potts’ tutorial. All tokens up to the next punctuation were marked as negated. Hashtag expansion into word strings was perfor"
S14-2089,esuli-sebastiani-2006-sentiwordnet,0,0.0966358,"ng entire tweets, ranking 7th on the main set and 2nd when applied to sarcastic tweets. 1 Introduction The analysis of the emotional content of text is relevant to numerous natural language processing (NLP), web and multi-modal dialogue applications. In recent years the increased popularity of social media and increased availability of relevant data has led to a focus of scientific efforts on the emotion expressed through social media, with Twitter being the most common subject. Sentiment analysis in Twitter is usually performed by combining techniques used for related tasks, like word-level (Esuli and Sebastiani, 2006; Strapparava and Valitutti, 2004) and sentencelevel (Turney and Littman, 2002; Turney and Littman, 2003) emotion extraction. Twitter however does present specific challenges: the breadth of possible content is virtually unlimited, the writing style is informal, the use of orthography and 2 2.1 Model Description Preprocessing POS-tagging / Tokenization was performed using the ARK NLP tweeter tagger (Owoputi et al., 2013), a Twitter-specific tagger. Negations were detected using the list from Christopher Potts’ tutorial. All tokens up to the next punctuation were marked as negated. Hashtag expa"
S14-2089,W11-0818,0,0.0137311,"ng / Tokenization was performed using the ARK NLP tweeter tagger (Owoputi et al., 2013), a Twitter-specific tagger. Negations were detected using the list from Christopher Potts’ tutorial. All tokens up to the next punctuation were marked as negated. Hashtag expansion into word strings was performed using a combination of a word insertion Finite State Machine and a language model. A normalized perplexity threshold was used to detect if the output was a “proper” English string and expansion was not performed if it was not. Multi-word Expressions (MWEs) were detected using the MIT jMWE library (Kulkarni and Finlayson, 2011). MWEs are non-compositional expressions (Sag et al., 2002), which should be This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 512 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 512–516, Dublin, Ireland, August 23-24, 2014. Given the starting, manually annotated, lexicon Affective Norms for English Words (Bradley and Lang, 1999) we selected 600 out of the 1034 words contained in it to serve"
S14-2089,S12-1060,0,0.0497083,"Missing"
S14-2089,S13-2072,1,0.891732,"malandra,mfalcone,cvaz,jbisogni}@usc.edu, potam@telecom.tuc.gr, shri@sipi.usc.edu Abstract grammar can be “unconventional” and there are unique artifacts like hashtags. Computation systems, like those submitted to SemEval 2013 task 2 (Nakov et al., 2013) mostly use bag-of-words models with specific features added to model emotion indicators like hashtags and emoticons (Davidov et al., 2010). This paper describes our submissions to SemEval 2014 task 9 (Rosenthal et al., 2014), which deals with sentiment analysis in twitter. The system is an expansion of our submission to the same task in 2013 (Malandrakis et al., 2013a), which used only token rating statistics as features. We expanded the system by using multiple lexica and more statistics, added steps to the pre-processing stage (including negation and multi-word expression handling), incorporated pairwise tweet-level semantic similarities as features and finally performed feature extraction on substrings and used the partial features as indicators of irony, sarcasm or humor. This paper describes our submission to SemEval2014 Task 9: Sentiment Analysis in Twitter. Our model is primarily a lexicon based one, augmented by some preprocessing, including detec"
S14-2089,S13-2053,0,0.0472747,"The core of the system was formed by the lexiconbased features. We used a total of four lexica and some derivatives. 2.2.1 Third party lexica We used three third party affective lexica. SentiWordNet (Esuli and Sebastiani, 2006) provides continuous positive, negative and neutral ratings for each sense of every word in WordNet. We created two versions of SentiWordNet: one where ratings are averaged over all senses of a word (e.g., one ratings for “good”) and one where ratings are averaged over lexeme-pos pairs (e.g., one rating for the adjective “good” and one for the noun “good”). NRC Hashtag (Mohammad et al., 2013) Sentiment Lexicon provides continuous polarity ratings for tokens, generated from a collection of tweets that had a positive or a negative word hashtag. Sentiment140 (Mohammad et al., 2013) Lexicon provides continuous polarity ratings for tokens, generated from the sentiment140 corpus of 1.6 million tweets, with emoticons used as positive and negative labels. 2.2.2 Emotiword: expansion and adaptation To create our own lexicon we used an automated algorithm of affective lexicon expansion based on the one presented in (Malandrakis et al., 2011; Malandrakis et al., 2013b), which in turn is an ex"
S14-2089,S13-2052,0,0.192771,"Missing"
S14-2089,N13-1039,0,0.125746,"Missing"
S14-2089,S14-2009,0,\N,Missing
S14-2119,P09-1009,0,0.0735053,"Missing"
S14-2119,J00-4006,0,\N,Missing
S14-2119,W01-0713,0,\N,Missing
S14-2119,P99-1010,0,\N,Missing
S14-2119,P95-1031,0,\N,Missing
S14-2119,S12-1051,0,\N,Missing
S16-1023,S15-2109,0,0.0113922,"a.gr Abstract or health (Chew and Eysenbach, 2010). This task is especially challenging due to the terse and informal writing style, the semantic diversity of content, as well as the often “unconventional” grammar and orthography. Many computational systems like those submitted to SemEval 2015 Task 10 (Rosenthal et al., 2015) incorporate bag-of-words models with Twitter-specific features like hashtags and emoticons (Davidov et al., 2010; B¨uchner and Stein, 2015). Word embeddings obtained from large amounts of tweets are used under the scope of an unsupervised approach for sentiment analysis (Astudillo et al., 2015). Additionally, deep learning models have recently become very popular for Twitter sentiment analysis (Severyn and Moschitti, 2015). Topic modeling approaches for sentiment analysis can also be found in literature, e.g., (Mei et al., 2007; Lin and He, 2009; Lu et al., 2011; Alam et al., 2016; Rao, 2016) We describe our submission to SemEval2016 Task 4: Sentiment Analysis in Twitter. The proposed system ranked first for the subtask B. Our system comprises of multiple independent models such as neural networks, semantic-affective models and topic modeling that are combined in a probabilistic way"
S16-1023,S15-2097,0,0.179716,"Missing"
S16-1023,C10-2028,0,0.0322026,"uc.gr, akolovou@di.uoa.gr, el10136@mail.ntua.gr el11142@mail.ntua.gr, iosife@central.ntua.gr, malandra@usc.edu xaris@ilsp.athena-innovation.gr, shri@sipi.usc.edu, potam@central.ntua.gr Abstract or health (Chew and Eysenbach, 2010). This task is especially challenging due to the terse and informal writing style, the semantic diversity of content, as well as the often “unconventional” grammar and orthography. Many computational systems like those submitted to SemEval 2015 Task 10 (Rosenthal et al., 2015) incorporate bag-of-words models with Twitter-specific features like hashtags and emoticons (Davidov et al., 2010; B¨uchner and Stein, 2015). Word embeddings obtained from large amounts of tweets are used under the scope of an unsupervised approach for sentiment analysis (Astudillo et al., 2015). Additionally, deep learning models have recently become very popular for Twitter sentiment analysis (Severyn and Moschitti, 2015). Topic modeling approaches for sentiment analysis can also be found in literature, e.g., (Mei et al., 2007; Lin and He, 2009; Lu et al., 2011; Alam et al., 2016; Rao, 2016) We describe our submission to SemEval2016 Task 4: Sentiment Analysis in Twitter. The proposed system ranked firs"
S16-1023,S13-2054,0,0.0370393,"Missing"
S16-1023,L16-1195,1,0.77861,"Missing"
S16-1023,P14-1062,0,0.059109,"n belong to each revealed topic with a given probability. Then, clusters of sentences per topic were created by classifying each sentence to the most probable topic. A semantic model was built for each cluster, using word2vec (Mikolov et al., 2013). A mixture of the semantic models, weighted by the topic posteriors is used for the estimation of tweet’s semantic similarities, e.g., S(·) in (1). 2.3 2.5 Convolutional Neural Network A deep convolutional neural network (CNN) was also developed. The neural network’s architecture is inspired by sentence classification tasks (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014). Each tweet is represented by a “sentence” matrix S that is created as follows. First, each word is represented as a 300-D vector using word2vec, and then, the word vectors are concatenated as follows: S = W1 ⊕W2 ⊕W3 ⊕···⊕Wn ., S ∈ IRd×n , where ⊕ indicates the concatenation operation. Each column i of S is a vector W ∈ IRd that corresponds to the ith word of the tweet. This way the sequence of the words in the tweet is kept. In order to preserve the same length for all tweets, zero padding was applied concatenating zero word vectors until the length n of the longest tweet is reac"
S16-1023,D14-1181,0,0.00437748,"opic with a given probability. Then, clusters of sentences per topic were created by classifying each sentence to the most probable topic. A semantic model was built for each cluster, using word2vec (Mikolov et al., 2013). A mixture of the semantic models, weighted by the topic posteriors is used for the estimation of tweet’s semantic similarities, e.g., S(·) in (1). 2.3 2.5 Convolutional Neural Network A deep convolutional neural network (CNN) was also developed. The neural network’s architecture is inspired by sentence classification tasks (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014). Each tweet is represented by a “sentence” matrix S that is created as follows. First, each word is represented as a 300-D vector using word2vec, and then, the word vectors are concatenated as follows: S = W1 ⊕W2 ⊕W3 ⊕···⊕Wn ., S ∈ IRd×n , where ⊕ indicates the concatenation operation. Each column i of S is a vector W ∈ IRd that corresponds to the ith word of the tweet. This way the sequence of the words in the tweet is kept. In order to preserve the same length for all tweets, zero padding was applied concatenating zero word vectors until the length n of the longest tweet is reached. The siz"
S16-1023,S14-2089,1,0.874428,"Tasks such as opinion mining and sentiment analysis (Pang and Lee, 2008) have become very popular, since they can capture a large portion of the public opinion. The sentiment analysis of tweets was applied in various domains, such as commerce (Jansen et al., 2009), disaster management (Verma et al., 2011) In this paper, we present systems submitted to the SemEval 2016 Task 4 (Nakov et al., 2016) that deal with the sentiment analysis of tweets on the sentence level. The submitted systems are based on the fusion of the different classifiers. Specifically, 1) we enhanced the system submitted by Malandrakis et al. (2014) to the SemEval 2014 Task 9 (Rosenthal et al., 2014), 2) we used the open-source system submitted by B¨uchner and Stein (2015) to the SemEval 2015 Task 10 (Rosenthal et al., 2015), 3) we trained a convolutional neural network on a large amount of unlabelled Twitter data, 4) we developed a system 155 Proceedings of SemEval-2016, pages 155–163, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics based on topic modeling and 5) we trained a classifier using word embeddings as features. Our system was submitted on two subtasks, namely subtask A (message polarit"
S16-1023,S14-2111,0,0.0162743,"s tasks (Nakov et al., 2013; Rosenthal et al., 2014). The following systems were combined in a late fusion scheme, based on the mean posterior probabilities: 1) NRC-Canada (Mohammad et al., 2013) is based on morphological, linguistic and polarity features, 2) GU-MLT-LT (G¨unther and Furrer, 2013) trains a linear classifier by stochastic gradient descent which uses social media specific text preprocessing and linguistic features, 3) KLUE (Proisl et al., 2013) employs a Maximum Entropy classifier with bag-of-words models, sentiment, emoticons and internet slang abbreviations features, 4) TeamX (Miura et al., 2014) is similar to NRC-Canada but uses more lexicon-based features and handles the unbalance distribution of tweets’ sentiment by adopting a weighting scheme to bias the output. 2.6 Fusion of the systems The motivation behind the development of various systems for sentiment classification is that different systems may capture different aspects of the sentiment, and by combining them we can predict more accurately the sentiment of tweets. The system architecture including the fusion scheme is summarized in the figure below. Word2vec System Based on the assumption that similarity of meaning implies"
S16-1023,S13-2053,0,0.0440007,"11). Next, the output of the max-pooling layer is passed to a dropout layer (Srivastava et al., 2014). A softmax layer that classifies each test instance to one of the possible classes is the final step. 2.4 Webis The Webis open-source system (B¨uchner and Stein, 2015) that was submitted on (Rosenthal et al., 2015) is the ensemble of different subsystems that ranked at the top of Semeval 2013, Semeval 2014 Sentiment Analysis tasks (Nakov et al., 2013; Rosenthal et al., 2014). The following systems were combined in a late fusion scheme, based on the mean posterior probabilities: 1) NRC-Canada (Mohammad et al., 2013) is based on morphological, linguistic and polarity features, 2) GU-MLT-LT (G¨unther and Furrer, 2013) trains a linear classifier by stochastic gradient descent which uses social media specific text preprocessing and linguistic features, 3) KLUE (Proisl et al., 2013) employs a Maximum Entropy classifier with bag-of-words models, sentiment, emoticons and internet slang abbreviations features, 4) TeamX (Miura et al., 2014) is similar to NRC-Canada but uses more lexicon-based features and handles the unbalance distribution of tweets’ sentiment by adopting a weighting scheme to bias the output. 2."
S16-1023,S13-2052,0,0.0774341,"the inputs to the next layer which selects the maximum value of each feature by applying a max-over-time pooling operation (max-pooling layer) (Collobert et al., 2011). Next, the output of the max-pooling layer is passed to a dropout layer (Srivastava et al., 2014). A softmax layer that classifies each test instance to one of the possible classes is the final step. 2.4 Webis The Webis open-source system (B¨uchner and Stein, 2015) that was submitted on (Rosenthal et al., 2015) is the ensemble of different subsystems that ranked at the top of Semeval 2013, Semeval 2014 Sentiment Analysis tasks (Nakov et al., 2013; Rosenthal et al., 2014). The following systems were combined in a late fusion scheme, based on the mean posterior probabilities: 1) NRC-Canada (Mohammad et al., 2013) is based on morphological, linguistic and polarity features, 2) GU-MLT-LT (G¨unther and Furrer, 2013) trains a linear classifier by stochastic gradient descent which uses social media specific text preprocessing and linguistic features, 3) KLUE (Proisl et al., 2013) employs a Maximum Entropy classifier with bag-of-words models, sentiment, emoticons and internet slang abbreviations features, 4) TeamX (Miura et al., 2014) is simi"
S16-1023,S16-1001,0,0.034302,"f social networks such as Twitter dominates the daily communication of hundreds of millions of people around the world. People often share opinions and express their feelings about various topics through social networks. Tasks such as opinion mining and sentiment analysis (Pang and Lee, 2008) have become very popular, since they can capture a large portion of the public opinion. The sentiment analysis of tweets was applied in various domains, such as commerce (Jansen et al., 2009), disaster management (Verma et al., 2011) In this paper, we present systems submitted to the SemEval 2016 Task 4 (Nakov et al., 2016) that deal with the sentiment analysis of tweets on the sentence level. The submitted systems are based on the fusion of the different classifiers. Specifically, 1) we enhanced the system submitted by Malandrakis et al. (2014) to the SemEval 2014 Task 9 (Rosenthal et al., 2014), 2) we used the open-source system submitted by B¨uchner and Stein (2015) to the SemEval 2015 Task 10 (Rosenthal et al., 2015), 3) we trained a convolutional neural network on a large amount of unlabelled Twitter data, 4) we developed a system 155 Proceedings of SemEval-2016, pages 155–163, c San Diego, California, June"
S16-1023,N13-1039,0,0.0362346,"Missing"
S16-1023,S13-2065,0,0.022308,"that was submitted on (Rosenthal et al., 2015) is the ensemble of different subsystems that ranked at the top of Semeval 2013, Semeval 2014 Sentiment Analysis tasks (Nakov et al., 2013; Rosenthal et al., 2014). The following systems were combined in a late fusion scheme, based on the mean posterior probabilities: 1) NRC-Canada (Mohammad et al., 2013) is based on morphological, linguistic and polarity features, 2) GU-MLT-LT (G¨unther and Furrer, 2013) trains a linear classifier by stochastic gradient descent which uses social media specific text preprocessing and linguistic features, 3) KLUE (Proisl et al., 2013) employs a Maximum Entropy classifier with bag-of-words models, sentiment, emoticons and internet slang abbreviations features, 4) TeamX (Miura et al., 2014) is similar to NRC-Canada but uses more lexicon-based features and handles the unbalance distribution of tweets’ sentiment by adopting a weighting scheme to bias the output. 2.6 Fusion of the systems The motivation behind the development of various systems for sentiment classification is that different systems may capture different aspects of the sentiment, and by combining them we can predict more accurately the sentiment of tweets. The s"
S16-1023,S14-2009,0,0.316849,"Pang and Lee, 2008) have become very popular, since they can capture a large portion of the public opinion. The sentiment analysis of tweets was applied in various domains, such as commerce (Jansen et al., 2009), disaster management (Verma et al., 2011) In this paper, we present systems submitted to the SemEval 2016 Task 4 (Nakov et al., 2016) that deal with the sentiment analysis of tweets on the sentence level. The submitted systems are based on the fusion of the different classifiers. Specifically, 1) we enhanced the system submitted by Malandrakis et al. (2014) to the SemEval 2014 Task 9 (Rosenthal et al., 2014), 2) we used the open-source system submitted by B¨uchner and Stein (2015) to the SemEval 2015 Task 10 (Rosenthal et al., 2015), 3) we trained a convolutional neural network on a large amount of unlabelled Twitter data, 4) we developed a system 155 Proceedings of SemEval-2016, pages 155–163, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics based on topic modeling and 5) we trained a classifier using word embeddings as features. Our system was submitted on two subtasks, namely subtask A (message polarity classification) and subtask B (tweet classificatio"
S16-1023,S15-2078,0,0.100642,"Missing"
S16-1023,S15-2079,0,0.0287234,"style, the semantic diversity of content, as well as the often “unconventional” grammar and orthography. Many computational systems like those submitted to SemEval 2015 Task 10 (Rosenthal et al., 2015) incorporate bag-of-words models with Twitter-specific features like hashtags and emoticons (Davidov et al., 2010; B¨uchner and Stein, 2015). Word embeddings obtained from large amounts of tweets are used under the scope of an unsupervised approach for sentiment analysis (Astudillo et al., 2015). Additionally, deep learning models have recently become very popular for Twitter sentiment analysis (Severyn and Moschitti, 2015). Topic modeling approaches for sentiment analysis can also be found in literature, e.g., (Mei et al., 2007; Lin and He, 2009; Lu et al., 2011; Alam et al., 2016; Rao, 2016) We describe our submission to SemEval2016 Task 4: Sentiment Analysis in Twitter. The proposed system ranked first for the subtask B. Our system comprises of multiple independent models such as neural networks, semantic-affective models and topic modeling that are combined in a probabilistic way. The novelty of the system is the employment of a topic modeling approach in order to adapt the semantic-affective space for each"
S16-1023,H05-1044,0,0.0399468,"weetlevel features are required, for each tweet a 300-D vector is generated by averaging the corresponding vectors of the constituent words. 157 2.1.4 Additional features Additional features based on characters and subjectivity lexica were used. Character features include the absolute and relative frequencies of selected characters. The selected characters are the capitalized letters, the punctuation marks, the emoticons as well as characters repetitions , i.e., at least three same successive characters in a word. Subjectivity features were also extracted based on the subjectivity lexicon of (Wilson et al., 2005). Specifically, the absolute and the relative frequencies of the strong positive/negative and weak positive/negative words were used as features. 2.1.5 Statistics extraction The statistics of the token-level polarity features were estimated in order to extract tweet-level features. The following statistics were computed: length (cardinality), min, max, max amplitude, sum, average, range (max minus min), standard deviation and variance. Normalized versions of the same statistics were also calculated. 2.2 Topic Modeling Topic modeling is a method for discovering “topics” that occur in collection"
S17-2112,S13-2053,0,0.0576095,"Missing"
S17-2112,S15-2097,0,0.0250261,"Missing"
S17-2112,S13-2052,0,0.0984822,"del architecture which is trained on the re-annotated data. The output is predictions on neutral and “emotional” tweets. The next step involves the classification of the tweets that were found to belong to “emotional” category, into positive and negative. This step requires only the “emotional” tweets for training the CNN model. 2.4 S(tj , wi ) = Word2vec 2.7 Stacking (3) Webis We also incorporated the Webis system (B¨uchner and Stein, 2015), which is the ensemble of different subsystems (namely NRC, GUMLT, KlUE, TeamX) that ranked at the top of SemEval 2013 and 2014 Sentiment Analysis tasks (Nakov et al., 2013; Rosenthal et al., 2015b) The main idea of this technique is to reduce a multi-class problem into binary 2-class problems and train one separate classifier for each pair of classes (Savicky and F¨urnkranz, 2003). In the second step, the predictions of the binary classifiers are combined using a separate classifier. This process is referred to as stacking (F¨urnkranz, 2001). 2.6 n=1 p(n|s) · Sn (tj , wi ) , PT n=1 p(n|s) where s = {t0 , t1 , . . . , tj , . . . , tk } are the tweet’s tokens, wi is the ith seed word, T is the number of topics-clusters, p(n|s) is the posterior probability for s t"
S17-2112,N13-1039,0,0.0410333,"Missing"
S17-2112,L16-1195,1,0.834148,"stems using both general purpose and Twitter data. The training set is composed by the training, development and developmenttime testing data of SemEval-2013 and SemEval2016, as described in Table 1. We also add to the train set, the test data from SemEval-2015 and SemEval-2014. We omit the SemEval-2016 test data, which are kept for testing and experimenting with our models. For the procedure of adaptive lexica creation we used a general purpose corpus that contains 116M sentences that was created by posing queries on a web search engine and aggregating the resulting snippets of web documents(Iosif et al., 2016). In addition, a Twitter-specific dataset is created and consists of 300M tweets (T-300M). Finally the ANEW lexicon (Bradley and Lang, 1999) is used for selecting the initial set of seed words of (1). Subtask A Subtask B & D Subtask C & E Training Set 28,061 6,680 9,070 Table 1: Number of tweets used for training. 3.2 Systems The Semantic-Affective system (see Section 2.2) is trained using the SemEval datasets of Table 1 for each subtask. We perform feature selection on the massive set of candidate features. Specifically, we perform a forward stepwise feature selection using a correlation crit"
S17-2112,S16-1023,1,0.342818,"Missing"
S17-2112,P14-1062,0,0.308819,"First, a semantic model is built and then affective ratings are estimated for unknown tokens. This approach uses a set of words with known affective ratings, usually referred as seed words, as a starting point. The English manual annotated affective 676 2.2.2 2.3 Additional features Convolutional Neural Network In our framework we propose the combination of two neural networks. Specifically, we develop a deep Convolutional Neural Network (CNN) and a two-step Convolutional Neural Network. The neural network architecture is inspired by sentence classification tasks (Severyn and Moschitti, 2015; Kalchbrenner et al., 2014; Kim, 2014). Each tweet is represented by a sentence matrix D that is created as follows. First, each word is represented as a d-dimensional vector using word2vec (Mikolov et al., 2013b), and then, the word vectors are concatenated as follows: In addition to the affective features, we also incorporate morphology, character and word embedding based features. Character features include the frequencies of selected characters like capitalized letters, punctuation marks, emoticons and character repetition. Word embeddings are utilized for the semantic similarity estimation. They were derived using"
S17-2112,D14-1181,0,0.0220681,"built and then affective ratings are estimated for unknown tokens. This approach uses a set of words with known affective ratings, usually referred as seed words, as a starting point. The English manual annotated affective 676 2.2.2 2.3 Additional features Convolutional Neural Network In our framework we propose the combination of two neural networks. Specifically, we develop a deep Convolutional Neural Network (CNN) and a two-step Convolutional Neural Network. The neural network architecture is inspired by sentence classification tasks (Severyn and Moschitti, 2015; Kalchbrenner et al., 2014; Kim, 2014). Each tweet is represented by a sentence matrix D that is created as follows. First, each word is represented as a d-dimensional vector using word2vec (Mikolov et al., 2013b), and then, the word vectors are concatenated as follows: In addition to the affective features, we also incorporate morphology, character and word embedding based features. Character features include the frequencies of selected characters like capitalized letters, punctuation marks, emoticons and character repetition. Word embeddings are utilized for the semantic similarity estimation. They were derived using word2vec (M"
S17-2112,S14-2089,1,0.885496,"Missing"
S17-2112,S15-2078,0,0.329479,"ystem Description The submitted system is based on the fusion of several systems. Specifically the system consists of: 1) the semantic-affective system (submitted to the SemEval 2016 Task 4 (Palogiannidi et al., 2016)) that incorporates affective, semanticsimilarity, sarcasm/irony and topic modeling features, 2) a single and a two-step convolutional neural network, 3) a system based on word embeddings, 4) a “stacking” based system that transforms the 3-class polarity problem of Subtask A, into 2-class binary problems and finally 5) the open-source system submitted to the SemEval 2015 Task 10 (Rosenthal et al., 2015a). Introduction Tweets are short length pieces of text, usually written in informal style that contain abbreviations, misspellings and creative syntax (like emoticons, hashtags etc). The challenging nature of sentiment analysis in Twitter motivated the organization of numerous tasks within the Semantics Evaluation (SemEval) workshop. In this paper, we show how our sentiment analysis framework called “Tweester” (winner of Subtask B in SemEval-2016 (Palogiannidi et al., 2016)), can be applied to all subtasks, namely Subtask A (message polarity classification), Subtask B (tweet classification ac"
S17-2112,S14-2009,0,0.0912873,"ere is still room for improvement. Also, we aim to investigate in more depth the fusion of different systems. 4 Results In Table 2 the integrated systems’ performances are depicted along with the submitted combination for each subtask (the omitted systems are denoted with ×). For Subtasks A and B the evaluation metric is macro-averaged recall (AvgR), for Subtask C it is the macro-averaged mean absolute error (M AE M ), for Subtask D the normalized cross-entropy (KLD) is used and for Subtask E the metric is called the Earth Mover’s Distance (EMD). All the aforementioned metrics are defined in (Rosenthal et al., 2014). For Subtask A we combined all individual systems and achieved an AvgR of 0.659. CNN proved to be the most robust individual system, achieving the highest performance (0.621) among the others. The two-step CNN achieved a slightly lower score compared to the single-step model. Since the CNN model is quite robust in distinguishing positive vs negative tweets it seems that the 2-step model makes more errors on the first step, which is the distinction between neutral and emotional class. For Subtasks B and D the stepwise based systems are omitted (since they involve binary classification). The se"
S17-2112,S15-2079,0,0.0136801,"plies affective similarity”. First, a semantic model is built and then affective ratings are estimated for unknown tokens. This approach uses a set of words with known affective ratings, usually referred as seed words, as a starting point. The English manual annotated affective 676 2.2.2 2.3 Additional features Convolutional Neural Network In our framework we propose the combination of two neural networks. Specifically, we develop a deep Convolutional Neural Network (CNN) and a two-step Convolutional Neural Network. The neural network architecture is inspired by sentence classification tasks (Severyn and Moschitti, 2015; Kalchbrenner et al., 2014; Kim, 2014). Each tweet is represented by a sentence matrix D that is created as follows. First, each word is represented as a d-dimensional vector using word2vec (Mikolov et al., 2013b), and then, the word vectors are concatenated as follows: In addition to the affective features, we also incorporate morphology, character and word embedding based features. Character features include the frequencies of selected characters like capitalized letters, punctuation marks, emoticons and character repetition. Word embeddings are utilized for the semantic similarity estimati"
S17-2112,S17-2088,0,0.0447953,"Missing"
S17-2112,H05-1044,0,0.0626173,"aracters like capitalized letters, punctuation marks, emoticons and character repetition. Word embeddings are utilized for the semantic similarity estimation. They were derived using word2vec (Mikolov et al., 2013b), representing each word as a d-dimensional vector. For each tweet the corresponding vectors of its constituent words are averaged to get a sentence-level feature vector. D = W1 ⊕W2 ⊕W3 ⊕···⊕Wn ., D ∈ IRd×n (2) As subjectivity features we use the absolute and the relative frequencies of the strong positive/negative and weak positive/negative words taken from a subjectivity lexicon (Wilson et al., 2005). where ⊕ indicates the concatenation operation. Each column i of D is a vector W ∈ IRd that corresponds to the ith word of the tweet. This way the sequence of the words in the tweet is kept. In order to preserve the same length for all tweets, zero padding is applied by concatenating zero word vectors until the length n of the longest tweet is reached. The size of D is d × n, where d is the dimension of the word embedding and n is the maximum number of words. The matrix D is the input to the network, where a convolution operation is performed between D and a filter F ∈ IRd×m which is applied"
S18-1037,S17-2126,1,0.950647,"ffect-related dimensions, namely: valence, dominance, arousal, pleasantness, anger, sadness, fear, disgust, concreteness, familiarity. The method of generating word level norms is detailed in (Malandrakis et al., 2013) and relies on the assumption that given a similarity metric between two words, one may derive the similarity between their affective ratings. This approach uses a set of N words with known affective ratings (seed words), as a starting point. Concretely, we calculate the affective rating of a word w as follows: υˆ(w) = α0 + N X αi υ(ti )S(ti , w), 2.3 We utilized the ekphrasis2 (Baziotis et al., 2017) tool as a tweet preprocessor. The preprocessing steps included in ekphrasis are: Twitter-specific tokenization, spell correction, word normalization, word segmentation (for splitting hashtags) and word annotation. Tokenization. Tokenization is the first fundamental preprocessing step and since it is the basis for the other steps, it immediately affects the quality of the features learned by the network. Tokenization on Twitter is challenging, since there is large variation in the vocabulary and the expressions which are used. There are certain expressions which are better kept as one token (e"
S18-1037,S16-1173,0,0.0604328,"Missing"
S18-1037,D17-1169,0,0.0870203,"Missing"
S18-1037,W14-6905,0,0.0282962,"Word Embeddings Embedding Layer Pretraining Task Embeddings Pre-training Unlabeled Dataset Processed Input Data Figure 2: High-level overview of our approach recent advances in artificial neural networks for text classification have shown to outperform conventional approaches (Deriu et al., 2016; Rouvier and Favre, 2016; Rosenthal et al., 2017a). This can be attributed to their ability to learn features directly from data and also utilize hand-crafted features where needed. Most of aforementioned works focus on sentiment analysis, but similar approaches have been applied to emotion detection (Canales and Martínez-Barco, 2014) leading to similar conclusions. SemEval 2018 Task 1: “Affect in Tweets” (Mohammad et al., 2018) focuses on exploring emotional content of tweets for both classification and regression tasks concerning the four basic emotions (joy, sadness, anger, fear) and the presence of more fine-grained emotions such as disgust or optimism. In this paper, we present a deep-learning system that competed in SemEval 2018 Task 1: “Affect in Tweets”. We explore a transfer learning approach to compensate for limited training data that uses the sentiment analysis dataset of Semeval Task 4A (Rosenthal et al., 2017"
S18-1037,J00-4006,0,0.0112572,"07/11/2011, April 23rd), times (e.g. 4:30pm, 11:00 am), currencies (e.g. $10, 25mil, 50e), acronyms, censored words (e.g. s**t), words with emphasis (e.g. *very*) and more using an extensive list of regular expressions. Normalization. After tokenization, we apply a series of modifications on the extracted tokens, such as spell correction, word normalization and segmentation. Specifically for word normalization we use lowercase words, normalize URLs, emails, numbers, dates, times and user handles (@user). This helps reducing the vocabulary size without losing information. For spell correction (Jurafsky and James, 2000) and word segmentation (Segaran and Hammerbacher, 2009) we use the Viterbi algorithm. The prior probabilities are obtained from word statistics from the unlabeled dataset. The benefits of the aforementioned procedure are the reduction of the vocabulary size, without removing any words, and the preservation of information that is usually lost during tokenization. Table 1 shows an example text snippet and the resulting preprocessed tokens. (1) i=1 where t1 ...tN are the seed words, υ(ti ) is the affective rating for seed word ti , αi is a trainable weight corresponding to seed ti and S()˙ stands"
S18-1037,W17-4101,0,0.0257377,"of intensity of anger Figure 9: Examples of emotion recognition Figure 10: Attention heat-map visualization. The color intensity of each word corresponds to its weight (importance), given by the self-attention mechanism (Section 2.6). 3A (V-reg), 8/37 (tie with 6 and 7 place) in subtask 4A (V-oc) and 1/35 in subtask 5A (E-c). All of our models achieved competitive results. We used the same transfer learning approach in all subtasks (LSTM-TL-FT), utilizing the same pretrained model. 5 Secondly, we would like to introduce characterlevel information in our models, based on (Wieting et al., 2016; Labeau and Allauzen, 2017), in order to overcome the problem of out-of-vocabulary (OOV) words and learn syntactic and stylistic features (Peters et al., 2018), which are highly indicative of emotions and their intensity. Conclusion Finally, we make both our pretrained word embeddings and the source code of our models available to the community3 , in order to make our results easily reproducible and facilitate further experimentation in the field. In this paper we present a deep-learning system for short text emotion intensity, valence estimation for both regression and classification and multiclass emotion classificati"
S18-1037,N18-1202,0,0.0478796,"ord corresponds to its weight (importance), given by the self-attention mechanism (Section 2.6). 3A (V-reg), 8/37 (tie with 6 and 7 place) in subtask 4A (V-oc) and 1/35 in subtask 5A (E-c). All of our models achieved competitive results. We used the same transfer learning approach in all subtasks (LSTM-TL-FT), utilizing the same pretrained model. 5 Secondly, we would like to introduce characterlevel information in our models, based on (Wieting et al., 2016; Labeau and Allauzen, 2017), in order to overcome the problem of out-of-vocabulary (OOV) words and learn syntactic and stylistic features (Peters et al., 2018), which are highly indicative of emotions and their intensity. Conclusion Finally, we make both our pretrained word embeddings and the source code of our models available to the community3 , in order to make our results easily reproducible and facilitate further experimentation in the field. In this paper we present a deep-learning system for short text emotion intensity, valence estimation for both regression and classification and multiclass emotion classification. We used Bidirectional LSTMs, with a deep attention mechanism and took advantage of transfer learning in order to address the pro"
S18-1037,S18-1001,0,0.157641,"Missing"
S18-1037,C14-1019,0,0.123734,"Missing"
S18-1037,S13-2053,0,0.0394099,"en et al., 2011b) etc. The wide usage of figurative language, such as emojis and special language forms like abbreviations, hashtags, slang and other social media markers, which do not align with the conventional language structure, make natural language processing in Twitter even more challenging. Introduction In the past, sentiment analysis was tackled by extracting hand-crafted features or features from sentiment lexicons (Nielsen, 2011; Mohammad and Turney, 2010, 2013; Go et al., 2009) that were fed to classifiers such as Naive Bayes or Support Vector Machines (SVM) (Bollen et al., 2011a; Mohammad et al., 2013; Kiritchenko et al., 2014). The downside of such approaches is that they require extensive feature engineering from experts and thus they cannot keep up with rapid language evolution (Mudinas et al., 2012), especially in social media/micro-blogging context. However, Social media content has dominated online communication, enriching and changing language with new syntactic and semantic constructs that allow users to express facts, opinions and emotions in short amount of text. The analysis of such content has received great attention in NLP research due to the wide availability of data and the"
S18-1037,W10-0204,0,0.0754027,"to the weight given to each word by the self-attention mechanism. and Hurtado, 2014; Tumasjan et al., 2010), stock market monitoring (Si et al., 2013; Bollen et al., 2011b) etc. The wide usage of figurative language, such as emojis and special language forms like abbreviations, hashtags, slang and other social media markers, which do not align with the conventional language structure, make natural language processing in Twitter even more challenging. Introduction In the past, sentiment analysis was tackled by extracting hand-crafted features or features from sentiment lexicons (Nielsen, 2011; Mohammad and Turney, 2010, 2013; Go et al., 2009) that were fed to classifiers such as Naive Bayes or Support Vector Machines (SVM) (Bollen et al., 2011a; Mohammad et al., 2013; Kiritchenko et al., 2014). The downside of such approaches is that they require extensive feature engineering from experts and thus they cannot keep up with rapid language evolution (Mudinas et al., 2012), especially in social media/micro-blogging context. However, Social media content has dominated online communication, enriching and changing language with new syntactic and semantic constructs that allow users to express facts, opinions and e"
S18-1037,S17-2088,0,0.461679,"its valence intensity as a real-valued number between in the [0, 1] interval. Subtask E-c: determine the existence of none, one or more out of eleven emotions: anger, anticipation, disgust, fear, joy, love, optimism, pessimism, sadness, surprise, trust. Subtask X Final Layer Word Embeddings Embedding Layer Pretraining Task Embeddings Pre-training Unlabeled Dataset Processed Input Data Figure 2: High-level overview of our approach recent advances in artificial neural networks for text classification have shown to outperform conventional approaches (Deriu et al., 2016; Rouvier and Favre, 2016; Rosenthal et al., 2017a). This can be attributed to their ability to learn features directly from data and also utilize hand-crafted features where needed. Most of aforementioned works focus on sentiment analysis, but similar approaches have been applied to emotion detection (Canales and Martínez-Barco, 2014) leading to similar conclusions. SemEval 2018 Task 1: “Affect in Tweets” (Mohammad et al., 2018) focuses on exploring emotional content of tweets for both classification and regression tasks concerning the four basic emotions (joy, sadness, anger, fear) and the presence of more fine-grained emotions such as dis"
S18-1037,S16-1030,0,0.0203815,"Subtask V-reg: determine its valence intensity as a real-valued number between in the [0, 1] interval. Subtask E-c: determine the existence of none, one or more out of eleven emotions: anger, anticipation, disgust, fear, joy, love, optimism, pessimism, sadness, surprise, trust. Subtask X Final Layer Word Embeddings Embedding Layer Pretraining Task Embeddings Pre-training Unlabeled Dataset Processed Input Data Figure 2: High-level overview of our approach recent advances in artificial neural networks for text classification have shown to outperform conventional approaches (Deriu et al., 2016; Rouvier and Favre, 2016; Rosenthal et al., 2017a). This can be attributed to their ability to learn features directly from data and also utilize hand-crafted features where needed. Most of aforementioned works focus on sentiment analysis, but similar approaches have been applied to emotion detection (Canales and Martínez-Barco, 2014) leading to similar conclusions. SemEval 2018 Task 1: “Affect in Tweets” (Mohammad et al., 2018) focuses on exploring emotional content of tweets for both classification and regression tasks concerning the four basic emotions (joy, sadness, anger, fear) and the presence of more fine-grai"
S18-1037,P13-2005,0,0.0348132,"TMs on the dataset of Semeval 2017, Task 4A. The proposed approach ranked 1st in Subtask E “Multi-Label Emotion Classification”, 2nd in Subtask A “Emotion Intensity Regression” and achieved competitive results in other subtasks. 1 &lt;hashtag&gt; blessed &lt;/hashtag&gt; Emotions: joy, love, optimism seriously about to smack someone in the face &lt;hashtag&gt; arsehole &lt;/hashtag&gt; Emotions: anger, disgust Figure 1: Attention heat-map visualization. The color intensity corresponds to the weight given to each word by the self-attention mechanism. and Hurtado, 2014; Tumasjan et al., 2010), stock market monitoring (Si et al., 2013; Bollen et al., 2011b) etc. The wide usage of figurative language, such as emojis and special language forms like abbreviations, hashtags, slang and other social media markers, which do not align with the conventional language structure, make natural language processing in Twitter even more challenging. Introduction In the past, sentiment analysis was tackled by extracting hand-crafted features or features from sentiment lexicons (Nielsen, 2011; Mohammad and Turney, 2010, 2013; Go et al., 2009) that were fed to classifiers such as Naive Bayes or Support Vector Machines (SVM) (Bollen et al., 2"
S18-1037,D16-1157,0,0.0250544,"46 Figure 8: Examples of intensity of anger Figure 9: Examples of emotion recognition Figure 10: Attention heat-map visualization. The color intensity of each word corresponds to its weight (importance), given by the self-attention mechanism (Section 2.6). 3A (V-reg), 8/37 (tie with 6 and 7 place) in subtask 4A (V-oc) and 1/35 in subtask 5A (E-c). All of our models achieved competitive results. We used the same transfer learning approach in all subtasks (LSTM-TL-FT), utilizing the same pretrained model. 5 Secondly, we would like to introduce characterlevel information in our models, based on (Wieting et al., 2016; Labeau and Allauzen, 2017), in order to overcome the problem of out-of-vocabulary (OOV) words and learn syntactic and stylistic features (Peters et al., 2018), which are highly indicative of emotions and their intensity. Conclusion Finally, we make both our pretrained word embeddings and the source code of our models available to the community3 , in order to make our results easily reproducible and facilitate further experimentation in the field. In this paper we present a deep-learning system for short text emotion intensity, valence estimation for both regression and classification and mul"
S18-1069,W16-6208,0,0.0396658,"Missing"
S18-1069,E17-2017,0,0.165855,"yer of our model, with word2vec word embeddings, pretrained on a dataset of 550 million English tweets. Finally, our model does not rely on hand-crafted features or lexicons and is trained end-to-end with back-propagation. We ranked 2nd out of 48 teams. 1 Label: Top 5: i better get a bunch of fire type pokemon Label: Top 5: Figure 1: Attention heat-map visualization. The color intensity corresponds to the weight given to each word by the self-attention mechanism. In this work, we present a near state of the art approach for predicting emojis in tweets, which outperforms the best present work (Barbieri et al., 2017). For this purpose, we employ an LSTM network augmented with a context-aware selfattention mechanism, producing a feature representation used for classification. Moreover, the attention mechanism helps us make our model’s behavior more interpretable, by examining the distribution of the attention weights for a given tweet. To this end, we provide visualizations with the distributions of the attention weights. Introduction Emojis play an important role in textual communication, as they function as a substitute for nonverbal cues, that are taken for granted in face-toface communication, thus all"
S18-1069,S18-1003,0,0.0228593,"as they function as a substitute for nonverbal cues, that are taken for granted in face-toface communication, thus allowing users to convey emotions by means other than words. Despite their large appeal in text, they haven’t received much attention until recently. Former works, mostly consider their semantics (Aoki and Uchida, 2011; Espinosa-Anke et al., 2016; Barbieri et al., 2016b,a; Ljubeši´c and Fišer, 2016; Eisner et al., 2016) and only recently their role in social media was explored (Barbieri et al., 2017; Cappallo et al., 2018). In SemEval-2018 Task 2: “Multilingual Emoji Prediction” (Barbieri et al., 2018), given a tweet, we are asked to predict its most likely associated emoji. 2 Overview Figure 3 provides a high-level overview of our approach that consists of three main steps: (1) The text preprocessing step, which is common both for unlabeled data and the task’s dataset, (2) the word embeddings pre-training step, where we train custom word embeddings on a big collection of unlabeled Twitter messages and (3) the model training step where we train the deep learning model. 438 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 438–444 New Orleans, Louisi"
S18-1069,J00-4006,0,0.0125185,".g. 07/11/2011, April 23rd), times (e.g. 4:30pm, 11:00 am), currencies (e.g. $10, 25mil, 50e), acronyms, censored words (e.g. s**t), words with emphasis (e.g. *very*) and more using an extensive list of regular expressions. Data Normalization. After tokenization we apply a series of modifications on extracted tokens, such as spell correction, word normalization and segmentation. Specifically for word normalization we lowercase words, normalize URLs, emails, numbers, dates, times and user handles (@user). This helps reducing the vocabulary size without losing information. For spell correction (Jurafsky and James, 2000) and word segmentation (Segaran and Hammerbacher, 2009) we use the Viterbi algorithm. The prior probabilities are initialized using uni/bi-gram word statistics from the unlabeled dataset. Table 1 shows an example text snippet and the resulting preprocessed tokens. Preprocessing1 We utilized the ekphrasis2 tool (Baziotis et al., 2017) as a tweet preprocessor. The preprocessing 1 Significant portions of the systems submitted to SemEval 2018 in Tasks 1, 2 and 3, by the NTUA-SLP team are shared, specifically the preprocessing and portions of the DNN architecture. Their description is repeated here"
S18-1069,W16-2610,0,0.0561681,"Missing"
S18-1069,L16-1626,0,0.0325494,"able, by examining the distribution of the attention weights for a given tweet. To this end, we provide visualizations with the distributions of the attention weights. Introduction Emojis play an important role in textual communication, as they function as a substitute for nonverbal cues, that are taken for granted in face-toface communication, thus allowing users to convey emotions by means other than words. Despite their large appeal in text, they haven’t received much attention until recently. Former works, mostly consider their semantics (Aoki and Uchida, 2011; Espinosa-Anke et al., 2016; Barbieri et al., 2016b,a; Ljubeši´c and Fišer, 2016; Eisner et al., 2016) and only recently their role in social media was explored (Barbieri et al., 2017; Cappallo et al., 2018). In SemEval-2018 Task 2: “Multilingual Emoji Prediction” (Barbieri et al., 2018), given a tweet, we are asked to predict its most likely associated emoji. 2 Overview Figure 3 provides a high-level overview of our approach that consists of three main steps: (1) The text preprocessing step, which is common both for unlabeled data and the task’s dataset, (2) the word embeddings pre-training step, where we train custom word embeddings on a bi"
S18-1069,S17-2126,1,0.826665,"ction, word normalization and segmentation. Specifically for word normalization we lowercase words, normalize URLs, emails, numbers, dates, times and user handles (@user). This helps reducing the vocabulary size without losing information. For spell correction (Jurafsky and James, 2000) and word segmentation (Segaran and Hammerbacher, 2009) we use the Viterbi algorithm. The prior probabilities are initialized using uni/bi-gram word statistics from the unlabeled dataset. Table 1 shows an example text snippet and the resulting preprocessed tokens. Preprocessing1 We utilized the ekphrasis2 tool (Baziotis et al., 2017) as a tweet preprocessor. The preprocessing 1 Significant portions of the systems submitted to SemEval 2018 in Tasks 1, 2 and 3, by the NTUA-SLP team are shared, specifically the preprocessing and portions of the DNN architecture. Their description is repeated here for completeness. 2 github.com/cbaziotis/ekphrasis original processed Processed Input Data steps included in ekphrasis are: Twitter-specific tokenization, spell correction, word normalization, word segmentation (for splitting hashtags) and word annotation. Unlabeled Dataset. We collected a dataset of 550 million archived English Twi"
S18-1100,P11-2008,0,0.268886,"Missing"
S18-1100,S17-2126,1,0.840734,"This process also enables RNNs to handle inputs of variable length. RNNs are difficult to train (Pascanu et al., 2013), because gradients may grow or decay exponentially over long sequences (Bengio et al., 1994; Hochreiter et al., 2001). A way to overcome these problems is to use more sophisticated variants of regular RNNs, like Long Short-Term Memory (LSTM) networks (Hochreiter and Schmidhuber, 1997) or Gated Recurrent Units (GRU) (Cho et al., 2014), which introduce a gating mechanism to ensure proper gradient flow through the network. In this work, we use LSTMs. ekphrasis2 We utilized the (Baziotis et al., 2017) tool as a tweet preprocessor. The preprocessing steps included in ekphrasis are: Twitter-specific tokenization, spell correction, word normalization, word segmentation (for splitting hashtags) and word annotation. Tokenization. Tokenization is the first fundamental preprocessing step and since it is the basis for the other steps, it immediately affects the quality of the features learned by the network. Tokenization in Twitter is especially challenging, since there is large variation in the vocabulary and the used expressions. Part of the challenge is also the decision of whether to process a"
S18-1100,D16-1104,0,0.0230557,"f the proposed models. We ranked 2nd out of 42 teams in Subtask A and 2nd out of 31 teams in Subtask B. However, post-task-completion enhancements of our models achieve state-ofthe-art results ranking 1st for both subtasks. 1 magical i love everything Label: ironic by clash b e a u t i f u l w a y t o s t a r t m y m o r n i n g . Label: ironic by clash Figure 1: Attention heat-map visualization. The color intensity of each word / character, corresponds to its weight (importance), as given by the self-attention mechanism (Section 2.6). the words and the contrast between the sentiments. Also, (Joshi et al., 2016) recently added word embeddings statistics to the feature space and further boosted the performance in irony detection. Modeling irony, especially in Twitter, is a challenging task, since in ironic comments literal meaning can be misguiding; irony is expressed in “secondary” meaning and fine nuances that are hard to model explicitly in machine learning algorithms. Tracking irony in social media posses the additional challenge of dealing with special language, social media markers and abbreviations. Despite the accuracy achieved in this task by handcrafted features, a laborious feature-engineer"
S18-1100,J00-4006,0,0.0121351,"r messages, (2) the independent training of our models: word- and char-level, Data Unlabeled Dataset. We collected a dataset of 550 million archived English Twitter messages, from Apr. 2014 to Jun. 2017. This dataset is used for (1) calculating word statistics needed in our text preprocessing pipeline (Section 2.4) and (2) train614 ing word2vec word embeddings (Section 2.3). 2.3 such as spell correction, word normalization and segmentation. We also decide which tokens to omit, normalize and surround or replace with special tags (e.g. URLs, emails and @user). For the tasks of spell correction (Jurafsky and James, 2000) and word segmentation (Segaran and Hammerbacher, 2009) we use the Viterbi algorithm. The prior probabilities are initialized using uni/bigram word statistics from the unlabeled dataset. The benefits of the above procedure are the reduction of the vocabulary size, without removing any words, and the preservation of information that is usually lost during tokenization. Table 1 shows an example text snippet and the resulting preprocessed tokens. Word Embeddings Word embeddings are dense vector representations of words (Collobert and Weston, 2008; Mikolov et al., 2013), capturing semantic their a"
S18-1100,P15-2106,0,0.0213805,"rafted features, a laborious feature-engineering process and domain-specific knowledge are required; this type of prior knowledge must be continuously updated and investigated for each new domain. Moreover, the difficulty in parsing tweets (Gimpel et al., 2011) for feature extraction renders Introduction Irony is a form of figurative language, considered as “saying the opposite of what you mean”, where the opposition of literal and intended meanings is very clear (Barbieri and Saggion, 2014; Liebrecht et al., 2013). Traditional approaches in NLP (Tsur et al., 2010; Barbieri and Saggion, 2014; Karoui et al., 2015; Farías et al., 2016) model irony based on pattern-based features, such as the contrast between high and low frequent words, the punctuation used by the author, the level of ambiguity of 613 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 613–621 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics their precise semantic representation, which is key of determining their intended gist, much harder. In recent years, the successful utilization of deep learning architectures in NLP led to alternative approaches for tra"
S18-1100,P16-2044,0,0.0201366,"kshop on Semantic Evaluation (SemEval-2018), pages 613–621 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics their precise semantic representation, which is key of determining their intended gist, much harder. In recent years, the successful utilization of deep learning architectures in NLP led to alternative approaches for tracking irony in Twitter (Joshi et al., 2017; Ghosh and Veale, 2017). (Ghosh and Veale, 2016) proposed a Convolutional Neural Network (CNN) followed by a Long Short Term Memory (LSTM) architecture, outperforming the state-of-the-art. (Dhingra et al., 2016) utilized deep learning for representing tweets as a sequence of characters, instead of words and proved that such representations reveal information about the irony concealed in tweets. In this work, we propose the combination of word- and character-level representations in order to exploit both semantic and syntactic information of each tweet for successfully predicting irony. For this purpose, we employ a deep LSTM architecture which models words and characters separately. We predict whether a tweet is ironic or not, as well as the type of irony in the ironic ones by ensembling the two sepa"
S18-1100,W13-1605,0,0.0849595,"Missing"
S18-1100,W16-0425,0,0.0240236,"uch as the contrast between high and low frequent words, the punctuation used by the author, the level of ambiguity of 613 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 613–621 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics their precise semantic representation, which is key of determining their intended gist, much harder. In recent years, the successful utilization of deep learning architectures in NLP led to alternative approaches for tracking irony in Twitter (Joshi et al., 2017; Ghosh and Veale, 2017). (Ghosh and Veale, 2016) proposed a Convolutional Neural Network (CNN) followed by a Long Short Term Memory (LSTM) architecture, outperforming the state-of-the-art. (Dhingra et al., 2016) utilized deep learning for representing tweets as a sequence of characters, instead of words and proved that such representations reveal information about the irony concealed in tweets. In this work, we propose the combination of word- and character-level representations in order to exploit both semantic and syntactic information of each tweet for successfully predicting irony. For this purpose, we employ a deep LSTM architecture wh"
S18-1100,D17-1050,0,0.0145183,"pattern-based features, such as the contrast between high and low frequent words, the punctuation used by the author, the level of ambiguity of 613 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 613–621 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics their precise semantic representation, which is key of determining their intended gist, much harder. In recent years, the successful utilization of deep learning architectures in NLP led to alternative approaches for tracking irony in Twitter (Joshi et al., 2017; Ghosh and Veale, 2017). (Ghosh and Veale, 2016) proposed a Convolutional Neural Network (CNN) followed by a Long Short Term Memory (LSTM) architecture, outperforming the state-of-the-art. (Dhingra et al., 2016) utilized deep learning for representing tweets as a sequence of characters, instead of words and proved that such representations reveal information about the irony concealed in tweets. In this work, we propose the combination of word- and character-level representations in order to exploit both semantic and syntactic information of each tweet for successfully predicting irony. For this purpose, we employ a"
S18-1100,S18-1005,0,0.0624201,"Missing"
W12-1801,W10-4408,1,0.783837,"r Computational Linguistics formation is commonly captured in grammars, that are either hand-crafted or created by means of machine learning techniques. In order to be able to generate high-quality grammars with as little manual effort as possible, we aim at (semi) automating the knowledge-based generation of lexica and grammars. To achieve this, it is crucial to leverage Web resources for enriching ontologies with lexical and linguistic information, i.e. information about how ontological concepts are lexicalized in different languages, capturing in particular lexical and syntactic variation (Unger et al., 2010). This knowledge-centered grammar generation process may be merged with methods for automatically inferring structure from lightly annotated corpus, including data harvested from the Web, in a bottomup fashion (Tur and De Mori, 2011). For a dialog system to be able to exploit ontologies, lexica and grammars, these three resources need to be tightly aligned, i.e. they need to share domain-relevant vocabulary. For this alignment, we propose to build on Semantic Web standards, mainly in order to support the incorporation of already existing data, to share resources for SDS engineering, and facili"
W13-0205,N09-1003,0,0.0404911,"sumptions in network-based DSMs for abstract and concrete nouns (for both English and Greek). 2 Related Work Semantic similarity metrics can be divided into two broad categories: (i) metrics that rely on knowledge resources, and (ii) corpus-based metrics. A representative example of the first category are metrics that exploit the WordNet ontology [Miller (1990)]. Corpus-based metrics are formalized as DSM [Baroni and Lenci (2010)] and are based on the distributional hypothesis of meaning [Harris (1954)]. DSM can be categorized into unstructured (unsupervised) that employ a bag-of-words model [Agirre et al. (2009)] and structured that rely on syntactic relationships between words [Pado and Lapata (2007)]. Recently, motivated by the graph theory, several aspects of the human languages have been modeled using network-based methods. In [Mihalcea and Radev (2011)], an overview of network-based approaches is presented for a number of NLP problems. Different types of language units can be regarded as vertices of such networks, spanning from single words to sentences. Typically, network edges represent the relations of such units capturing phenomena such as co-occurrence, syntactic dependencies, and lexical s"
W13-0205,W06-1669,0,0.080581,"Missing"
W13-0205,J10-4006,0,0.398489,"ncreteness effect” for semantic similarity estimation. Network DSMs that implement the maximum sense similarity assumption perform best for concrete nouns, while attributional network DSMs perform best for abstract nouns. The performance of metrics is evaluated against human similarity ratings on an English and a Greek corpus. 1 Introduction Semantic similarity is the building block for numerous applications of natural language processing (NLP), such as grammar induction [Meng and Siu (2002)] and affective text categorization [Malandrakis et al. (2011)]. Distributional semantic models (DSMs) [Baroni and Lenci (2010)] are based on the distributional hypothesis of meaning [Harris (1954)] assuming that semantic similarity between words is a function of the overlap of their linguistic contexts. DSMs are typically constructed from co-occurrence statistics of word tuples that are extracted from a text corpus or from data harvested from the web. A wide range of contextual features are also used by DSM exploiting lexical, syntactic, semantic, and pragmatic information. DSMs have been successfully applied to the problem of semantic similarity computation. Recently [Iosif and Potamianos (2013)] proposed network-ba"
W13-0205,J07-2002,0,0.0372521,"ek). 2 Related Work Semantic similarity metrics can be divided into two broad categories: (i) metrics that rely on knowledge resources, and (ii) corpus-based metrics. A representative example of the first category are metrics that exploit the WordNet ontology [Miller (1990)]. Corpus-based metrics are formalized as DSM [Baroni and Lenci (2010)] and are based on the distributional hypothesis of meaning [Harris (1954)]. DSM can be categorized into unstructured (unsupervised) that employ a bag-of-words model [Agirre et al. (2009)] and structured that rely on syntactic relationships between words [Pado and Lapata (2007)]. Recently, motivated by the graph theory, several aspects of the human languages have been modeled using network-based methods. In [Mihalcea and Radev (2011)], an overview of network-based approaches is presented for a number of NLP problems. Different types of language units can be regarded as vertices of such networks, spanning from single words to sentences. Typically, network edges represent the relations of such units capturing phenomena such as co-occurrence, syntactic dependencies, and lexical similarity. An example of a large co-occurrence network is presented in [Widdows and Dorow ("
W13-0205,J06-3003,0,0.218401,"xt corpus or from data harvested from the web. A wide range of contextual features are also used by DSM exploiting lexical, syntactic, semantic, and pragmatic information. DSMs have been successfully applied to the problem of semantic similarity computation. Recently [Iosif and Potamianos (2013)] proposed network-based DSMs motivated by the organization of words, attributes and concepts in human cognition. The proposed semantic networks can operate under either the attributional similarity or the maximum sense similarity assumptions of lexical semantics. According to attributional similarity [Turney (2006)], semantic similarity between words is based on the commonality of their sense attributes. Following the maximum sense similarity hypothesis, the semantic similarity of two words can be estimated as the similarity of their two closest senses [Resnik (1995)]. Network-based DSMs have been shown to achieve state-of-the-art performance for semantic similarity tasks. Typically, the degree of semantic concreteness of a word is not taken into account in distributional models. However, evidence from neuro- and psycho-linguistics demonstrates significant differences in the cognitive organization of ab"
W13-0205,C02-1114,0,0.0339857,"o and Lapata (2007)]. Recently, motivated by the graph theory, several aspects of the human languages have been modeled using network-based methods. In [Mihalcea and Radev (2011)], an overview of network-based approaches is presented for a number of NLP problems. Different types of language units can be regarded as vertices of such networks, spanning from single words to sentences. Typically, network edges represent the relations of such units capturing phenomena such as co-occurrence, syntactic dependencies, and lexical similarity. An example of a large co-occurrence network is presented in [Widdows and Dorow (2002)] for the automatic creation of semantic classes. In [Iosif and Potamianos (2013)], a new paradigm for implementing DSMs is proposed: a two tier system in which corpus statistics are parsimoniously encoded in a network, while the task of similarity computation is shifted (from corpus-based techniques) to operations over network neighborhoods. 3 Corpus-Based Baseline Similarity Metrics Co-occurrence-based: The underlying assumption of co-occurrence-based metrics is that the co-existence of words in a specified contextual environment indicates semantic relatedness. In this work, we employ a wide"
W15-0121,N09-1003,0,0.0485893,"i.e., the Affective+Lexical approach. For the sake of completeness, the results when using textual features only (Lexical+Lexical) are presented for the respective best performing metrics and feature types (according to (Iosif and Potamianos, 2015)): CC/CT for Mn and CT/CC for Rn . Regarding the Affective+Lexical approach, the performance is reported only for Rn that was found to outperform the (omitted) Mn metric. It is notable2 that the Affective+Lexical combination performs very well being competitive3 against the best Lexical+Lexical approach, as well as other state-of-the-art approaches (Agirre et al., 2009). Specifically, the Affective+Lexical combination achieves higher (0.68 vs. 0.65) 2 This was experimentally verified using the affective word ratings given by human annotators (ANEW affective lexicon (Bradley and Lang, 1999)), instead of the automatically estimated ratings produced by (1). 3 The detailed comparison of the proposed affective models with other lexical DSMs is beyond the scope of this study. 168 0.8 0.8 0.7 0.7 0.6 0.6 Correlation Correlation and equal (0.91) correlation scores -compared to the Lexical+Lexical combination- for the WS353 and MC datasets, respectively. The Affectiv"
W15-0121,2014.lilt-9.5,0,0.031432,"co-occurrence statistics of word tuples that are extracted on existing corpora or on corpora specifically harvested from the web. In (Iosif and Potamianos, 2015), a language-agnostic DSM was proposed as a two-tier system motivated by cognitive considerations such as network activation and priming.. The first layer, encodes the semantics of words via the creation of lexical neighborhoods. In the second layer, similarity metrics are defined on these semantic neighborhoods. The extension of DSMs for representing the compositional aspects of lexical semantics constitutes an active research area (Baroni et al., 2014). Analysis of text to estimate affect or sentiment is a relatively recent research topic that has attracted great interest, as reflected by a series of shared evaluation tasks, e.g., analysis of tweets (Nakov et al., 2013). Relevant applications deal with numerous domains such as news stories (Lloyd et al., 2005) and product reviews (Hu and Liu, 2004). Affective analysis is also useful for other application domains such as dialogue systems (Lee and Narayanan, 2005). Several resources enable the development of these computational models, ranging from flat lexica (e.g., General Inquirer (Stone e"
W15-0121,J10-4006,0,0.0395189,"ge this is the first computational model investigating the role of affect in semantics. 162 Proceedings of the 11th International Conference on Computational Semantics, pages 162–172, c London, UK, April 15-17 2015. 2015 Association for Computational Linguistics 2 Related Work Semantic similarity is the building block for numerous applications of natural language processing, such as affective text analysis (Malandrakis et al., 2013). There has been much research interest on devising data-driven approaches for estimating semantic similarity between words. Distributional semantic models (DSMs) (Baroni and Lenci, 2010) are based on the distributional hypothesis of meaning (Harris, 1954) assuming that semantic similarity between words is a function of the overlap of their linguistic contexts. DSMs can be categorized into unstructured that employ a bag-of-words model and structured that employ syntactic relationships between words (Baroni and Lenci, 2010). DSMs are typically constructed from co-occurrence statistics of word tuples that are extracted on existing corpora or on corpora specifically harvested from the web. In (Iosif and Potamianos, 2015), a language-agnostic DSM was proposed as a two-tier system"
W15-0121,esuli-sebastiani-2006-sentiwordnet,0,0.0131539,"as reflected by a series of shared evaluation tasks, e.g., analysis of tweets (Nakov et al., 2013). Relevant applications deal with numerous domains such as news stories (Lloyd et al., 2005) and product reviews (Hu and Liu, 2004). Affective analysis is also useful for other application domains such as dialogue systems (Lee and Narayanan, 2005). Several resources enable the development of these computational models, ranging from flat lexica (e.g., General Inquirer (Stone et al., 1966) and Affective norms for English Words (Bradley and Lang, 1999)) to large lexical networks (e.g., SentiWordNet (Esuli and Sebastiani, 2006) and WordNet Affect (Strapparava and Valitutti, 2004)). Text can be analyzed for affect at different levels of granularity: from single words to entire sentences. In (Turney and Littman, 2003), the affective ratings of unknown words were predicted using the affective ratings for a small set of words (seeds) and the semantic relatedness between the unknown and the seed words. An example of sentence-level approach was proposed in (Malandrakis et al., 2013) applying techniques from n-gram language modeling. 3 Lexical Features and Metrics of Semantic Similarity Co-occurrence-based (CC). The underl"
W15-0121,J13-3004,0,0.0241034,"neighborhoods. Denoting vectors S(wi , Li ( |O |) and S(wi , Ai ( |O |) as ζiL and ζiA , respectively, two functions are used for the case of global fusion: ζiL ·ζiA and max{ζiL , ζiA }. The first stands for the product 166 of ζiL and ζiA . The second function gives the maximum element-wise value, i.e, for each lexicon entry and the target wi the respective maximum semantic or affective similarity score is selected. 6 Features of Semantic Semantic Opposition Here, we propose two feature sets that are relevant to the relations of synonymy and antonymy (also referred to as semantic opposition (Mohammad et al., 2013)). Antonymy constitutes a special lexical relation, since it embodies both the notion of (semantic) proximity and distance (Cruse, 1986). These features are based on the affective content of words and features of semantic similarity. Unlike people that can easily distinguish synonyms and antonyms, this is a challenging problem for the framework of DSMs. Both synonyms and antonyms exhibit strong associations which can be empirically verified via standard psycholinguistic experiments, as well as within the computational framework of DSMs. For example, in free association norms antonyms are frequ"
W15-0121,strapparava-valitutti-2004-wordnet,0,0.111751,"sks, e.g., analysis of tweets (Nakov et al., 2013). Relevant applications deal with numerous domains such as news stories (Lloyd et al., 2005) and product reviews (Hu and Liu, 2004). Affective analysis is also useful for other application domains such as dialogue systems (Lee and Narayanan, 2005). Several resources enable the development of these computational models, ranging from flat lexica (e.g., General Inquirer (Stone et al., 1966) and Affective norms for English Words (Bradley and Lang, 1999)) to large lexical networks (e.g., SentiWordNet (Esuli and Sebastiani, 2006) and WordNet Affect (Strapparava and Valitutti, 2004)). Text can be analyzed for affect at different levels of granularity: from single words to entire sentences. In (Turney and Littman, 2003), the affective ratings of unknown words were predicted using the affective ratings for a small set of words (seeds) and the semantic relatedness between the unknown and the seed words. An example of sentence-level approach was proposed in (Malandrakis et al., 2013) applying techniques from n-gram language modeling. 3 Lexical Features and Metrics of Semantic Similarity Co-occurrence-based (CC). The underlying assumption of co-occurrence-based metrics is tha"
W15-0121,S13-2052,0,\N,Missing
W15-1105,S12-1051,0,0.0598391,"zed two complementary models in an attempt to address a series of phenomena that apply to compositional semantics, namely, “linguistic creativity”, “order sensitivity”, “adaptive capacity”, and “information scalability”1 . Three types of phrases were investigated: noun-noun (NN), adjective-noun (AN), and verb-object (VO). In (Baroni and Zamparelli, 2010), particular focus was given to the AN type, where adjectives were represented as matrices acting as functions to the vectorial representation of head nouns. Recent research efforts have been expanded to longer text segments such as sentences (Agirre et al., 2012; Agirre et al., 2013; Polajnar et al., 2014). In (Socher et al., 2012), based on the functional space proposed in (Baroni and Zamparelli, 2010), phrase constituents were treated as both a continuous vector and a parameter matrix, where the representation of sentence semantics was constructed via a recursive bottom-up procedure. 3 Baseline Network-based Model In this section, we generalize the ideas regarding network-based DSMs presented in (Iosif and Potamianos, 2015), for the case of more complex structures. The network consists of two layers: 1) activation, and 2) similarity layer. Given a"
W15-1105,J10-4006,0,0.328925,"mbined with the lexical function method proposed by (Baroni and Zamparelli, 2010). We show that, by fusing a network-based with a lexical function model, performance gains can be achieved. 1 Introduction Vector Space Models (VSMs) have proven their efficiency at representing word semantics, which are vital components for numerous natural language applications, such as paraphrasing and textual entailment (Androutsopoulos and Malakasiotis, 2010), affective text analysis (Malandrakis et al., 2013), etc. VSMs constitute the most-widely used implementation of Distributional Semantic Models (DSMs) (Baroni and Lenci, 2010). A fundamental task addressed in the framework of DSMs is the computation of semantic similarity between words, adopting the distributional hypothesis of meaning, i.e., “similarity of context implies similarity of meaning” (Harris, 1954). DSMs have been successful when applied to the representation of word lexical semantics, enabling the computation of word semantic similarity (Turney and Pantel, 2010). However, the application of DSMs for representing the semantics of more complex structures, e.g., phrases or sentences, is not trivial since the meaning of such structures is the result of var"
W15-1105,D10-1115,0,0.797602,"ave been successful at modeling the meaning of individual words, with interest recently shifting to compositional structures, i.e., phrases and sentences. Network-based DSMs represent and handle semantics via operators applied on word neighborhoods, i.e., semantic graphs containing a target’s most similar words. We extend network-based DSMs to address compositionality using an activation model (motivated by psycholinguistics) that operates on the fused neighborhoods of variable size activation. The proposed method is evaluated against and combined with the lexical function method proposed by (Baroni and Zamparelli, 2010). We show that, by fusing a network-based with a lexical function model, performance gains can be achieved. 1 Introduction Vector Space Models (VSMs) have proven their efficiency at representing word semantics, which are vital components for numerous natural language applications, such as paraphrasing and textual entailment (Androutsopoulos and Malakasiotis, 2010), affective text analysis (Malandrakis et al., 2013), etc. VSMs constitute the most-widely used implementation of Distributional Semantic Models (DSMs) (Baroni and Lenci, 2010). A fundamental task addressed in the framework of DSMs is"
W15-1105,2014.lilt-9.5,0,0.0409644,"tween twoword phrases, however, their limitations were revealed for the case of longer structures (Polajnar et al., 2014), where the composition of meaning becomes more complex. Bengio and Mikolov (2003; 2013) proposed an approach based on deep learning for building language models that address the prob39 Proceedings of CMCL 2015, pages 39–47, c Denver, Colorado, June 4, 2015. 2015 Association for Computational Linguistics lem of language creativity. The models appear to constantly gain support in comparison with the traditional DSMs. A preliminary comparative analysis of them is provided in (Baroni et al., 2014b) with respect to a number of tasks related to lexical semantics. In this work, we extend a recent network-based implementation of DSMs (Iosif and Potamianos, 2015) in order to represent the semantics of compositional structures. The used framework consists of activation models motivated by semantic priming (McNamara, 2005). For each structure, an activation area (i.e, semantic neighborhood) is computed which is regarded as a sub-space within the network. The novelty of the present work is twofold. First, we propose various approaches for the creation of activation areas for compositional str"
W15-1105,P14-1023,0,0.106956,"tween twoword phrases, however, their limitations were revealed for the case of longer structures (Polajnar et al., 2014), where the composition of meaning becomes more complex. Bengio and Mikolov (2003; 2013) proposed an approach based on deep learning for building language models that address the prob39 Proceedings of CMCL 2015, pages 39–47, c Denver, Colorado, June 4, 2015. 2015 Association for Computational Linguistics lem of language creativity. The models appear to constantly gain support in comparison with the traditional DSMs. A preliminary comparative analysis of them is provided in (Baroni et al., 2014b) with respect to a number of tasks related to lexical semantics. In this work, we extend a recent network-based implementation of DSMs (Iosif and Potamianos, 2015) in order to represent the semantics of compositional structures. The used framework consists of activation models motivated by semantic priming (McNamara, 2005). For each structure, an activation area (i.e, semantic neighborhood) is computed which is regarded as a sub-space within the network. The novelty of the present work is twofold. First, we propose various approaches for the creation of activation areas for compositional str"
W15-1105,W13-0104,0,0.025331,"detect the effect that a word has to its linguistic context and the semantic changes on its meaning, e.g., a “nice” table is still a table but a “fake” or “broken” table is not. Depending on context, a modifier can affect the meaning of the encompassing phrase in different ways. For example, the modifier “normal” changes the meaning of “normal cat” much less than the modifier “dead” in “dead cat”. Moreover, the modifier effect may vary for each syntactic category. For example, verbs can be transitive or intransitive, nouns can be abstract or concrete, and adjectives can be intensional or not (Boleda et al., 2013). Words that act as functions on their linguistic context have attracted much interest, and have recently been successfully handled by computational models. 7.2 Estimating the Transformative Degree We categorise modifiers based on their regression performance, when training them for the lf model. Specifically, we acquire the MSE of their training as a measure for deciding the degree of their transformative effect on a given head noun. Taking the MSE is a sensible approach, since regression tries to derive a close approximation to observed vectorial representations of phrases and head nouns by"
W15-1105,P13-4006,0,0.0166612,"described in (5), we computed co-occurence counts for bigrams occurring at least 50 times in the corpus. Positive Pointwise Mutual Information (PPMI) was applied to reweigh them. We used a) Singular Value Decomposition (SVD), and b) Non-Negative Matrix Factorization (NMF) (Lee and Seung, 2001) to reduce the dimensionality of the space down to a) 300, and b) 500 dimensions. To train lf, we selected corpus bigrams comprising of a modifier and a noun. We used a) Least Squares (LSR), and b) Ridge (RR) (Hastie et al., 2009) regression. The DIStributional SEmantics Composition Toolkit (DISSECT 4 , (Dinu et al., 2013)) was used to implement lf, as well as the widely-used additive (add) and multiplicative (mult) models proposed in (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010). Fusion model. We combined the best performing model configurations on NNs (see Section 6.2) in order to implement the proposed fusion models. 6.2 For evaluation purposes, we used the widely-used Mitchell & Lapata (2010) datasets comprising of 108 noun-noun (NN), adjective-noun (AN), and verb-object (VO) phrase pairs, evaluated by human judgements and averaged per phrase pair. The models were evaluated using Spearman’s correla"
W15-1105,P08-1028,0,0.80209,"e representation of word lexical semantics, enabling the computation of word semantic similarity (Turney and Pantel, 2010). However, the application of DSMs for representing the semantics of more complex structures, e.g., phrases or sentences, is not trivial since the meaning of such structures is the result of various compositional phenomena (Pelletier, 1994) that are inherent properties of natural language creativity. The key idea behind current approaches in semantic composition (using DSMs) is the combination of word vectors using simple functions, e.g., vector addition or multiplication (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010), or other transformational functions. Regardless of the used function, the resulting representations adhere to the paradigm of VSMs, while the cosine between the (composed) vectors is used for estimating similarity. Such efforts proved to be effective when computing the similarity between twoword phrases, however, their limitations were revealed for the case of longer structures (Polajnar et al., 2014), where the composition of meaning becomes more complex. Bengio and Mikolov (2003; 2013) proposed an approach based on deep learning for building language models that"
W15-1105,polajnar-etal-2014-evaluation,0,0.0251634,"Missing"
W15-1105,D12-1110,0,0.13421,"omena that apply to compositional semantics, namely, “linguistic creativity”, “order sensitivity”, “adaptive capacity”, and “information scalability”1 . Three types of phrases were investigated: noun-noun (NN), adjective-noun (AN), and verb-object (VO). In (Baroni and Zamparelli, 2010), particular focus was given to the AN type, where adjectives were represented as matrices acting as functions to the vectorial representation of head nouns. Recent research efforts have been expanded to longer text segments such as sentences (Agirre et al., 2012; Agirre et al., 2013; Polajnar et al., 2014). In (Socher et al., 2012), based on the functional space proposed in (Baroni and Zamparelli, 2010), phrase constituents were treated as both a continuous vector and a parameter matrix, where the representation of sentence semantics was constructed via a recursive bottom-up procedure. 3 Baseline Network-based Model In this section, we generalize the ideas regarding network-based DSMs presented in (Iosif and Potamianos, 2015), for the case of more complex structures. The network consists of two layers: 1) activation, and 2) similarity layer. Given a lexical unit, the first layer represents an activation area that includ"
W16-0424,D10-1115,0,0.0919626,"Missing"
W16-0424,W02-1011,0,0.0157182,"; Celli, 2012; Rosenthal et al., 2014; Rosenthal et al., 2015). Word-level affective lexica can be created automatically with high accuracy (Turney and Littman, 2002; Strapparava and Valitutti, 2004; Esuli and Sebastiani, 2006; Malandrakis et al., 2013; Palogiannidi et al., 2015). Affective lexica are required in hierarchical models that combine words’ affective ratings for estimating affective ratings of larger lexical units, e.g., phrases (Turney and Littman, 2002; Wilson et al., 2005), sentences (Malandrakis et al., 2013; Rosenthal et al., 2014; Rosenthal et al., 2015) and whole documents (Pang et al., 2002; Pang and Lee, 2008). A sentiment classification approach on movies review documents was proposed in (Pang et al., 2002). Word-level semantic representations constitute the core aspect of DSMs typically constructed from cooccurrence statistics of word tuples. Such representations are the building block for models of larger lexical units, e.g., phrases and sentences, following the principle of semantic compositionality (Pelletier, 1994). They are meant to address a number of properties that are relevant to the compositional aspects of meaning, namely, “linguistic creativity”, “order sensitivit"
W16-0424,J90-1003,0,0.14038,"nd 357 NN pairs. 5.2 Semantic-affective models The proposed models estimate the affective ratings for each affective dimension in a continuous scale in [1,1], however we only report results for valence. The semantic-affective model shown in (1) was applied for the unigram (U) and bigram (B) models as defined in Section 2.1. The parameters of the semantic-affective model are set as follows: N = 600 seeds, for S(·) a contextbased metric of semantic similarity was applied with window size equal to one, while the extracted features were weighted according to positive pointwise mutual information (Church and Hanks, 1990). LSE was applied for estimating the weights α of (1). The parameters of the model are detailed in (Palogiannidi et al., 2015). The compositional model requires a large corpus2 for extracting the training pairs of each modifier (Iosif et al., 2016). For each modifier all word pairs with the same modifier are extracted creating hundreds of training pairs. 5.3 Fusion We investigate both weighted and unweighted average schemes while a compositionality criterion based on the 2 We use a web harvested corpus that was created posing one query that was formulated for each vocabulary word on search eng"
W16-0424,esuli-sebastiani-2006-sentiwordnet,0,0.0104747,"orted for NN and AN, respectively. 1 Introduction Affective analysis of text aims at eliciting emotion from linguistic information and it can be relevant for a wide range of applications such as sentiment analysis (Pang and Lee, 2008; Rosenthal et al., 2014; Rosenthal et al., 2015), news headlines analysis (Strapparava and Mihalcea, 2007) or affective analysis of social media (Quercia et al., 2011; Celli, 2012; Rosenthal et al., 2014; Rosenthal et al., 2015). Word-level affective lexica can be created automatically with high accuracy (Turney and Littman, 2002; Strapparava and Valitutti, 2004; Esuli and Sebastiani, 2006; Malandrakis et al., 2013; Palogiannidi et al., 2015). Affective lexica are required in hierarchical models that combine words’ affective ratings for estimating affective ratings of larger lexical units, e.g., phrases (Turney and Littman, 2002; Wilson et al., 2005), sentences (Malandrakis et al., 2013; Rosenthal et al., 2014; Rosenthal et al., 2015) and whole documents (Pang et al., 2002; Pang and Lee, 2008). A sentiment classification approach on movies review documents was proposed in (Pang et al., 2002). Word-level semantic representations constitute the core aspect of DSMs typically const"
W16-0424,W15-1105,1,0.927597,"presentations are the building block for models of larger lexical units, e.g., phrases and sentences, following the principle of semantic compositionality (Pelletier, 1994). They are meant to address a number of properties that are relevant to the compositional aspects of meaning, namely, “linguistic creativity”, “order sensitivity”, “adaptive capacity”, and “information scalability” (Turney, 2012). Compositional approaches have been reported for the estimation of compositional structures semantic similarity (Mitchell and Lapata., 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli., 2010; Georgiladakis et al., 2015). A combination of a symbolic and a distributional compositionality approach was investigated by (Clark and Pulman, 2007), while an approach for compositional neural networks was presented in (Hammer, 2003). A recursive neural network model that learns compositional vector representations for phrases and sentences at any length and syntactic type was proposed by (Socher et al., 2012) and showed that it can be used for the prediction of sentiment as well. In this work, we propose a compositional semanticaffective model, that is applicable to continuous affective spaces with one or more dimensio"
W16-0424,L16-1195,1,0.884503,"Missing"
W16-0424,S14-2009,0,0.0662939,"Missing"
W16-0424,S15-2078,0,0.116893,"Missing"
W16-0424,D12-1110,0,0.0485533,"y, 2012). Compositional approaches have been reported for the estimation of compositional structures semantic similarity (Mitchell and Lapata., 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli., 2010; Georgiladakis et al., 2015). A combination of a symbolic and a distributional compositionality approach was investigated by (Clark and Pulman, 2007), while an approach for compositional neural networks was presented in (Hammer, 2003). A recursive neural network model that learns compositional vector representations for phrases and sentences at any length and syntactic type was proposed by (Socher et al., 2012) and showed that it can be used for the prediction of sentiment as well. In this work, we propose a compositional semanticaffective model, that is applicable to continuous affective spaces with one or more dimensions. This model is applied for the affective estimation of AN and NN pairs. The semantic-affective models are motivated by the assumption that “semantic similarity implies affective sim154 Proceedings of NAACL-HLT 2016, pages 154–160, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics ilarity”, while the proposed compositional model is motivated"
W16-0424,P08-1028,0,0.295309,"Missing"
W16-0424,P13-1045,0,0.0239634,"− λ(p)(w3 υˆc1 (p) + w4 υˆc3 (p)), (8) where w1 , ...w4 are the weights that correspond to each P4 affective model and estimated through LSE and ˆ∗ (p) is the affective rating of each word i=1 wi = 1, υ pair derived from each affective model, and λ(p) is meant for weighting the contribution of compositional and noncompositional models. 5 5.1 Experimental Procedure and Results Dataset For evaluating the proposed semantic-affective models we use two word pairs datasets, one consisting of AN and one consisting of NN. The word pairs of the evaluation datasets were extracted from movie reviews by (Socher et al., 2013) as follows. Each movie review was first split into the constituent sentences and then into the constituent phrases. The derived sentences and phrases were annotated with respect to their polarity using crowdsourcing. We kept only the word pairs that have an adjective or noun as their first word, and their second word is a noun. The created dataset consists of 1009 AN and 357 NN pairs. 5.2 Semantic-affective models The proposed models estimate the affective ratings for each affective dimension in a continuous scale in [1,1], however we only report results for valence. The semantic-affective mo"
W16-0424,S07-1013,0,0.0897678,"Missing"
W16-0424,strapparava-valitutti-2004-wordnet,0,0.296889,"Missing"
W16-0424,H05-1044,0,0.0191342,"al., 2015), news headlines analysis (Strapparava and Mihalcea, 2007) or affective analysis of social media (Quercia et al., 2011; Celli, 2012; Rosenthal et al., 2014; Rosenthal et al., 2015). Word-level affective lexica can be created automatically with high accuracy (Turney and Littman, 2002; Strapparava and Valitutti, 2004; Esuli and Sebastiani, 2006; Malandrakis et al., 2013; Palogiannidi et al., 2015). Affective lexica are required in hierarchical models that combine words’ affective ratings for estimating affective ratings of larger lexical units, e.g., phrases (Turney and Littman, 2002; Wilson et al., 2005), sentences (Malandrakis et al., 2013; Rosenthal et al., 2014; Rosenthal et al., 2015) and whole documents (Pang et al., 2002; Pang and Lee, 2008). A sentiment classification approach on movies review documents was proposed in (Pang et al., 2002). Word-level semantic representations constitute the core aspect of DSMs typically constructed from cooccurrence statistics of word tuples. Such representations are the building block for models of larger lexical units, e.g., phrases and sentences, following the principle of semantic compositionality (Pelletier, 1994). They are meant to address a numbe"
W18-6209,W17-5207,0,0.0203876,"ke most NLP tasks, was tackled by traditional methods that included hand-crafted features or features from sentiment lexicons (Nielsen, 2011; Mohammad and Turney, 2010, 2013; Go et al., 2009) which were fed to classifiers such as Naive Bayes and SVMs (Bollen et al., 2011a; Mohammad et al., 2013; Kiritchenko et al., 2014). However, deep neural networks achieve increased performance compared to traditional methods, due to their ability to learn more abstract features from large amounts of data, producing state-of-the-art results in emotion recognition and sentiment analysis (Deriu et al., 2016; Goel et al., 2017; Baziotis et al., 2017). Introduction Social media, especially micro-blogging services like Twitter, have attracted lots of attention from the NLP community. The language used is constantly evolving by incorporating new syntactic and semantic constructs, such as emojis or hashtags, abbreviations and slang, making natural language processing in this domain even more demanding. Moreover, the analysis of such content leverages the high availability of datasets offered from Twitter, satisfying the need for large amounts of data for training. In this paper, we present our work submitted to the WAS"
W18-6209,S17-2126,1,0.939899,"was tackled by traditional methods that included hand-crafted features or features from sentiment lexicons (Nielsen, 2011; Mohammad and Turney, 2010, 2013; Go et al., 2009) which were fed to classifiers such as Naive Bayes and SVMs (Bollen et al., 2011a; Mohammad et al., 2013; Kiritchenko et al., 2014). However, deep neural networks achieve increased performance compared to traditional methods, due to their ability to learn more abstract features from large amounts of data, producing state-of-the-art results in emotion recognition and sentiment analysis (Deriu et al., 2016; Goel et al., 2017; Baziotis et al., 2017). Introduction Social media, especially micro-blogging services like Twitter, have attracted lots of attention from the NLP community. The language used is constantly evolving by incorporating new syntactic and semantic constructs, such as emojis or hashtags, abbreviations and slang, making natural language processing in this domain even more demanding. Moreover, the analysis of such content leverages the high availability of datasets offered from Twitter, satisfying the need for large amounts of data for training. In this paper, we present our work submitted to the WASSA 2018 IEST (Klinger et"
W18-6209,W18-6206,0,0.0646583,"Missing"
W18-6209,S13-2053,0,0.0207573,"se language models. Moreover, we utilize a sentiment analysis dataset for pretraining a model, which encodes emotion related information. The submitted model consists of an ensemble of the aforementioned TL models. Our team ranked 3rd out of 30 participants, achieving an F1 score of 0.703. 1 In the past, emotion analysis, like most NLP tasks, was tackled by traditional methods that included hand-crafted features or features from sentiment lexicons (Nielsen, 2011; Mohammad and Turney, 2010, 2013; Go et al., 2009) which were fed to classifiers such as Naive Bayes and SVMs (Bollen et al., 2011a; Mohammad et al., 2013; Kiritchenko et al., 2014). However, deep neural networks achieve increased performance compared to traditional methods, due to their ability to learn more abstract features from large amounts of data, producing state-of-the-art results in emotion recognition and sentiment analysis (Deriu et al., 2016; Goel et al., 2017; Baziotis et al., 2017). Introduction Social media, especially micro-blogging services like Twitter, have attracted lots of attention from the NLP community. The language used is constantly evolving by incorporating new syntactic and semantic constructs, such as emojis or hash"
W18-6209,W10-0204,0,0.0267831,"ur networks. We leverage a big collection of unlabeled Twitter messages, for pretraining word2vec word embeddings and a set of diverse language models. Moreover, we utilize a sentiment analysis dataset for pretraining a model, which encodes emotion related information. The submitted model consists of an ensemble of the aforementioned TL models. Our team ranked 3rd out of 30 participants, achieving an F1 score of 0.703. 1 In the past, emotion analysis, like most NLP tasks, was tackled by traditional methods that included hand-crafted features or features from sentiment lexicons (Nielsen, 2011; Mohammad and Turney, 2010, 2013; Go et al., 2009) which were fed to classifiers such as Naive Bayes and SVMs (Bollen et al., 2011a; Mohammad et al., 2013; Kiritchenko et al., 2014). However, deep neural networks achieve increased performance compared to traditional methods, due to their ability to learn more abstract features from large amounts of data, producing state-of-the-art results in emotion recognition and sentiment analysis (Deriu et al., 2016; Goel et al., 2017; Baziotis et al., 2017). Introduction Social media, especially micro-blogging services like Twitter, have attracted lots of attention from the NLP co"
W18-6209,P13-2005,0,0.0243324,"ol of ECE, National Technical University of Athens, Athens, Greece Department of Informatics, Athens University of Economics and Business, Athens, Greece 3 Signal Analysis and Interpretation Laboratory (SAIL), USC, Los Angeles, USA el12068@central.ntua.gr, el12108@central.ntua.gr cbaziotis@mail.ntua.gr, potam@central.ntua.gr Abstract Emotion recognition is particularly interesting in social media, as it has useful applications in numerous tasks, such as public opinion detection about political tendencies (Pla and Hurtado, 2014; Tumasjan et al., 2010; Li and Xu, 2014), stock market monitoring (Si et al., 2013; Bollen et al., 2011b), tracking product perception (Chamlertwat et al., 2012), even detection of suicide-related communication (Burnap et al., 2015). In this paper we present our approach to tackle the Implicit Emotion Shared Task (IEST) organized as part of WASSA 2018 at EMNLP 2018. Given a tweet, from which a certain word has been removed, we are asked to predict the emotion of the missing word. In this work, we experiment with neural Transfer Learning (TL) methods. Our models are based on LSTM networks, augmented with a selfattention mechanism. We use the weights of various pretrained mod"
W18-6209,D14-1162,0,0.0864735,"to improve the performance of 58 a related task by reducing the required training data (Torrey and Shavlik, 2010; Pan et al., 2010). In computer vision, transfer learning is employed in order to overcome the deficit of training samples for some categories by adapting classifiers trained for other categories (Oquab et al., 2014). With the power of deep supervised learning, learned knowledge can even be transferred to a totally different task (i.e. ImageNet (Krizhevsky et al., 2012)). Following this logic, TL methods have also been applied to NLP. Pretrained word vectors (Mikolov et al., 2013; Pennington et al., 2014) have become standard components of most architectures. Recently, approaches that leverage pretrained language models have emerged, which learn the compositionality of language, capture long-term dependencies and context-dependent features. For instance, ELMo contextual word representations (Peters et al., 2018) and ULMFiT (Howard and Ruder, 2018) achieve state-of-the-art results on a wide variety of NLP tasks. Our work is mainly inspired by ULMFiT, which we extend to the Twitter domain. 2.5 different models, the prediction for a specific instance is estimated as follows: vc = c pi ∈ IRC (2) p"
W18-6209,N18-1202,0,0.0221358,"et al., 2014). With the power of deep supervised learning, learned knowledge can even be transferred to a totally different task (i.e. ImageNet (Krizhevsky et al., 2012)). Following this logic, TL methods have also been applied to NLP. Pretrained word vectors (Mikolov et al., 2013; Pennington et al., 2014) have become standard components of most architectures. Recently, approaches that leverage pretrained language models have emerged, which learn the compositionality of language, capture long-term dependencies and context-dependent features. For instance, ELMo contextual word representations (Peters et al., 2018) and ULMFiT (Howard and Ruder, 2018) achieve state-of-the-art results on a wide variety of NLP tasks. Our work is mainly inspired by ULMFiT, which we extend to the Twitter domain. 2.5 different models, the prediction for a specific instance is estimated as follows: vc = c pi ∈ IRC (2) p = arg max vc c∈{1,...,C} where vc denotes the votes for class c from all different models, Fi is the decision of the ith model, which is either 1 or 0 with respect to whether the model has classified the instance in class c or not and p is the final prediction. 3 Network Architecture All of our TL schemes share"
W18-6209,C14-1019,0,0.019827,"poulou1∗, Aikaterini Margatina1∗ Christos Baziotis1,2 , Alexandros Potamianos1,3 1 2 School of ECE, National Technical University of Athens, Athens, Greece Department of Informatics, Athens University of Economics and Business, Athens, Greece 3 Signal Analysis and Interpretation Laboratory (SAIL), USC, Los Angeles, USA el12068@central.ntua.gr, el12108@central.ntua.gr cbaziotis@mail.ntua.gr, potam@central.ntua.gr Abstract Emotion recognition is particularly interesting in social media, as it has useful applications in numerous tasks, such as public opinion detection about political tendencies (Pla and Hurtado, 2014; Tumasjan et al., 2010; Li and Xu, 2014), stock market monitoring (Si et al., 2013; Bollen et al., 2011b), tracking product perception (Chamlertwat et al., 2012), even detection of suicide-related communication (Burnap et al., 2015). In this paper we present our approach to tackle the Implicit Emotion Shared Task (IEST) organized as part of WASSA 2018 at EMNLP 2018. Given a tweet, from which a certain word has been removed, we are asked to predict the emotion of the missing word. In this work, we experiment with neural Transfer Learning (TL) methods. Our models are based on LSTM networks, aug"
W18-6209,S17-2088,0,0.039794,"nsfer Embeddings + Encoder P-LM Twitter Dataset Language Model Finetuning IEST Language Model IEST Dataset Figure 1: High-level overview of our TL approaches. 2 Overview a general sub-corpus, so as to focus on the structural relationships of words, instead of their emotional content. The third chunk is composed of the two aforementioned corpora. We concatenated the 2M emotion dataset with 2M generic tweets, creating a final 4M dataset. We denote the three corpora as EmoCorpus (2M), EmoCorpus+ (4M) and GenCorpus (5M). Sentiment Analysis Dataset. We use the dataset of SemEval17 Task4A (Sent17) (Rosenthal et al., 2017) for training our sentiment classifier as described in Sec. 4.2. The dataset consists of Twitter messages annotated with their sentiment polarity (positive, negative, neutral). The training set contains 56K tweets and the validation set 6K tweets. Our approach is composed of the following three steps: (1) pretraining, in which we train word2vec word embeddings (P-Emb), a sentiment model (PSent) and Twitter-specific language models (PLM), (2) transfer learning, in which we transfer the weights of the aforementioned models to specific layers of our IEST classifier and (3) ensembling, in which we"
W18-6209,P16-1162,0,\N,Missing
zervanou-etal-2014-word,J13-1003,0,\N,Missing
zervanou-etal-2014-word,J10-4006,0,\N,Missing
zervanou-etal-2014-word,I05-1067,0,\N,Missing
