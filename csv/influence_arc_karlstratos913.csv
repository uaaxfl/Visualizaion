2020.acl-main.437,N19-1121,0,0.0148042,"ploring the viability of VQVAEs for text representation learning, an important part of this paper is a systematic comparison between different discretization techniques. GumbelSoftmax (Jang et al., 2017; Maddison et al., 2017) is a popular choice that has been considered for supervised text classification (Chen and Gimpel, 2018) and dialog generation (Zhao et al., 2018). In the binary latent variable setting, straight-through estimators are often used (Dong et al., 2019). Another choice is “continuous decoding” which takes a convex combination of latent values to make the loss differentiable (Al-Shedivat and Parikh, 2019). Yet a less considered choice is Hard EM (Brown et al., 1993; De Marcken, 1995; Spitkovsky et al., 2010). A main contribution of this work is a thorough empirical comparison between such different choices in a controlled setting. To demonstrate the usefulness of our models, we focus on improving low-resource classification performance by pretraining on unlabeled text. Previous best results are obtained with continuous latentvariable VAEs, e.g., VAMPIRE (Gururangan et al., 2019). We show that our discrete representations outperform these previous results while being significantly more lightwei"
2020.acl-main.437,K16-1002,0,0.0249487,"rformance of the learned representations as features for lowresource document and sentence classification. Our best models outperform the previous best reported results with continuous representations in these low-resource settings, while learning significantly more compressed representations. Interestingly, we find that an amortized variant of Hard EM performs particularly well in the lowest-resource regimes.1 1 Introduction Deep generative models with latent variables have become a major focus of NLP research over the past several years. These models have been used both for generating text (Bowman et al., 2016) and as a way of learning latent representations of text for downstream tasks (Yang et al., 2017; Gururangan et al., 2019). Most of this work has modeled the latent variables as being continuous, that is, as vectors in Rd , in part due to the simplicity of performing inference over (certain) continuous latents using variational autoencoders and the reparameterization trick (Kingma and Welling, 2014; Rezende et al., 2014). At the same time, deep generative models with discrete latent variables are attractive because the latents are arguably more interpretable, and because they lead to significa"
2020.acl-main.437,J93-2003,0,0.0895842,"Missing"
2020.acl-main.437,N18-2116,0,0.018213,"ut their work focuses on making inference faster, by decoding the target sequence from the discrete codes non-autoregressively. To our knowledge, we are the first that explores general text representations induced by VQ-VAEs for semi-supervised and transfer learning in NLP. In addition to exploring the viability of VQVAEs for text representation learning, an important part of this paper is a systematic comparison between different discretization techniques. GumbelSoftmax (Jang et al., 2017; Maddison et al., 2017) is a popular choice that has been considered for supervised text classification (Chen and Gimpel, 2018) and dialog generation (Zhao et al., 2018). In the binary latent variable setting, straight-through estimators are often used (Dong et al., 2019). Another choice is “continuous decoding” which takes a convex combination of latent values to make the loss differentiable (Al-Shedivat and Parikh, 2019). Yet a less considered choice is Hard EM (Brown et al., 1993; De Marcken, 1995; Spitkovsky et al., 2010). A main contribution of this work is a thorough empirical comparison between such different choices in a controlled setting. To demonstrate the usefulness of our models, we focus on improving low"
2020.acl-main.437,N19-1254,1,0.833376,"column highs in bold. 8.3 Compression We briefly discuss in what sense discrete latent representations reduce storage requirements. Given a vocabulary of size 30,000, storing a T -length sentence requires T log2 30000 ≈ 14.9T bits. Our While the above holds for storage, the space required to classify a sentence represented as M L integers using a parametric classifier may not be smaller than that required for classifying a sentence represented as a d-dimensional floating point vector. On the other hand, nearest neighbor-based methods, which are experiencing renewed interest (Guu et al., 2018; Chen et al., 2019; Wiseman and Stratos, 2019), should be significantly less expensive in terms of time and memory when sentences are encoded as M L integers rather than d-dimensional floating point vectors. In the next subsection we quantitatively evaluate our discrete representations in a nearest neighbor-based retrieval setting. 4838 M=4, K=256 Hard EM CatVAE VQ-VAE 76.1 77.5 69.1 L2 GloVe fastText Discrete Embedding M=8, K=128 M=16, K=256 79.6 73.7 73.5 78.8 78.5 71.2 Continuous Embedding (300d) C OSINE 76.4 72.8 76.6 74.1 Table 7: Unsupervised document retrieval on AG News dataset, measured by average labe"
2020.acl-main.437,W95-0102,0,0.56088,"Missing"
2020.acl-main.437,D19-1526,0,0.0284938,"are the first that explores general text representations induced by VQ-VAEs for semi-supervised and transfer learning in NLP. In addition to exploring the viability of VQVAEs for text representation learning, an important part of this paper is a systematic comparison between different discretization techniques. GumbelSoftmax (Jang et al., 2017; Maddison et al., 2017) is a popular choice that has been considered for supervised text classification (Chen and Gimpel, 2018) and dialog generation (Zhao et al., 2018). In the binary latent variable setting, straight-through estimators are often used (Dong et al., 2019). Another choice is “continuous decoding” which takes a convex combination of latent values to make the loss differentiable (Al-Shedivat and Parikh, 2019). Yet a less considered choice is Hard EM (Brown et al., 1993; De Marcken, 1995; Spitkovsky et al., 2010). A main contribution of this work is a thorough empirical comparison between such different choices in a controlled setting. To demonstrate the usefulness of our models, we focus on improving low-resource classification performance by pretraining on unlabeled text. Previous best results are obtained with continuous latentvariable VAEs, e."
2020.acl-main.437,P19-1590,0,0.286991,"dels outperform the previous best reported results with continuous representations in these low-resource settings, while learning significantly more compressed representations. Interestingly, we find that an amortized variant of Hard EM performs particularly well in the lowest-resource regimes.1 1 Introduction Deep generative models with latent variables have become a major focus of NLP research over the past several years. These models have been used both for generating text (Bowman et al., 2016) and as a way of learning latent representations of text for downstream tasks (Yang et al., 2017; Gururangan et al., 2019). Most of this work has modeled the latent variables as being continuous, that is, as vectors in Rd , in part due to the simplicity of performing inference over (certain) continuous latents using variational autoencoders and the reparameterization trick (Kingma and Welling, 2014; Rezende et al., 2014). At the same time, deep generative models with discrete latent variables are attractive because the latents are arguably more interpretable, and because they lead to significantly more compressed ∗ Work done as an intern at Toyota Technological Institute at Chicago. 1 Code available on GitHub: ht"
2020.acl-main.437,Q18-1031,0,0.0177222,"elopment set with column highs in bold. 8.3 Compression We briefly discuss in what sense discrete latent representations reduce storage requirements. Given a vocabulary of size 30,000, storing a T -length sentence requires T log2 30000 ≈ 14.9T bits. Our While the above holds for storage, the space required to classify a sentence represented as M L integers using a parametric classifier may not be smaller than that required for classifying a sentence represented as a d-dimensional floating point vector. On the other hand, nearest neighbor-based methods, which are experiencing renewed interest (Guu et al., 2018; Chen et al., 2019; Wiseman and Stratos, 2019), should be significantly less expensive in terms of time and memory when sentences are encoded as M L integers rather than d-dimensional floating point vectors. In the next subsection we quantitatively evaluate our discrete representations in a nearest neighbor-based retrieval setting. 4838 M=4, K=256 Hard EM CatVAE VQ-VAE 76.1 77.5 69.1 L2 GloVe fastText Discrete Embedding M=8, K=128 M=16, K=256 79.6 73.7 73.5 78.8 78.5 71.2 Continuous Embedding (300d) C OSINE 76.4 72.8 76.6 74.1 Table 7: Unsupervised document retrieval on AG News dataset, measu"
2020.acl-main.437,D14-1181,0,0.0109606,"lp Review Full (Zhang et al., 2015), which correspond to predicting news labels, Wikipedia ontology labels, and the number of Yelp stars, respectively. The data details are summarized in Table 1. For all datasets, we randomly sample 5,000 examples as development data. To evaluate the efficiency of the latent representation in lowresource settings, we train the classifier with varying numbers of labeled instances: 200, 500, 2500, and the full training set size (varies by dataset). We use accuracy as the evaluation metric. In preprocessing, we space tokenize, lowercase, and clean the text as in Kim (2014), and then truncate each sentence to a maximum sequence length of 400. For each dataset, we use a vocabulary of the 30,000 most common words. 4834 Dataset AG News DBPedia Yelp Review Full # Classes Train 4 14 5 115K 555K 645K Dev models on in-domain text for each dataset with 60 random hyperparameter search (with same ranges as specified in their Appendix A.1), and select best models based on validation accuracy in each setting. Test 5K 7.6K 5K 70K 5K 50K Table 1: The number of classes and the numbers of examples in each data subset, for the classification tasks. 5.2 Transfer Paradigm When tra"
2020.acl-main.437,D19-1370,0,0.0721935,"arning algorithm with exponential moving averages (EMA) to update the embedding vectors (van den Oord et al., 2017). We tune whether to use EMA updates or not. Also, we find small β for commitment loss to be beneficial, and search over {0.001, 0.01, 0.1}. 6.4 Mean-Field Categorical VAE We find that using the discrete analytic KL divergence term directly in the ELBO objective leads to posterior collapse. The KL term vanishes to 0 and the qml distributions converge to the uniform priors. To circumvent this, we modify the KL term to be max(KL, λ). This is known as Free Bits (Kingma et al., 2016; Li et al., 2019), which ensures that the latent variables encode a certain amount of information by not penalizing the KL divergence when it is less than λ. We set λ = γM L log K, where γ is a hyperparameter between 0 and 1. That is, we allocate a “KL budget” as a fraction of M L log K, which is the upper bound of KL divergence between M L independent categorical distributions and uniform prior distributions. Since in this case KL(qml (zml |x)||pml (zml )) = log K − H[qml (zml |x)], this is equivalent to thresholding H[qml (zml |x)] by (1 − γ) log K. We experiment with γ ∈ {0.2, 0.4, 0.6, 0.8, 1}.3 4835 3 Not"
2020.acl-main.437,L18-1008,0,0.0242431,"pment set of the AG News corpus as a query to retrieve 100 nearest neighbors in the training corpus, as measured by Hamming distance. We use average label precision, the fraction of retrieved documents that have the same label as the query document, to evaluate the retrieved neighbors. We compare with baselines that use averaged 300d pretrained word vectors (corresponding to each token in the document) as a representation, where neighbors are retrieved based on cosine or L2 distance. We use GloVe with a 2.2 million vocabulary (Pennington et al., 2014) and fastText with a 2 million vocabulary (Mikolov et al., 2018). The results are in Table 7. We see that CatVAE and Hard EM outperform these CBOW baselines (while being significantly more space efficient), while VQ-VAE does not. These results are in line with those of Figure 2, where VQ-VAE struggles when its code book vectors cannot be used (i.e., when reembedding from scratch). In Figure 4 we additionally experiment with a slightly different setting: Rather than retrieving a fixed number of nearest neighbors for a query document, we retrieve all the documents within a neighborhood of Hamming distance ≤ D, and calculate the average label precision. These"
2020.acl-main.437,D14-1162,0,0.104102,"classifier. In these experiments we use each document in the development set of the AG News corpus as a query to retrieve 100 nearest neighbors in the training corpus, as measured by Hamming distance. We use average label precision, the fraction of retrieved documents that have the same label as the query document, to evaluate the retrieved neighbors. We compare with baselines that use averaged 300d pretrained word vectors (corresponding to each token in the document) as a representation, where neighbors are retrieved based on cosine or L2 distance. We use GloVe with a 2.2 million vocabulary (Pennington et al., 2014) and fastText with a 2 million vocabulary (Mikolov et al., 2018). The results are in Table 7. We see that CatVAE and Hard EM outperform these CBOW baselines (while being significantly more space efficient), while VQ-VAE does not. These results are in line with those of Figure 2, where VQ-VAE struggles when its code book vectors cannot be used (i.e., when reembedding from scratch). In Figure 4 we additionally experiment with a slightly different setting: Rather than retrieving a fixed number of nearest neighbors for a query document, we retrieve all the documents within a neighborhood of Hammin"
2020.acl-main.437,P18-1101,0,0.0288126,"er, by decoding the target sequence from the discrete codes non-autoregressively. To our knowledge, we are the first that explores general text representations induced by VQ-VAEs for semi-supervised and transfer learning in NLP. In addition to exploring the viability of VQVAEs for text representation learning, an important part of this paper is a systematic comparison between different discretization techniques. GumbelSoftmax (Jang et al., 2017; Maddison et al., 2017) is a popular choice that has been considered for supervised text classification (Chen and Gimpel, 2018) and dialog generation (Zhao et al., 2018). In the binary latent variable setting, straight-through estimators are often used (Dong et al., 2019). Another choice is “continuous decoding” which takes a convex combination of latent values to make the loss differentiable (Al-Shedivat and Parikh, 2019). Yet a less considered choice is Hard EM (Brown et al., 1993; De Marcken, 1995; Spitkovsky et al., 2010). A main contribution of this work is a thorough empirical comparison between such different choices in a controlled setting. To demonstrate the usefulness of our models, we focus on improving low-resource classification performance by pr"
2020.acl-main.437,W10-2902,0,0.0247605,"tic comparison between different discretization techniques. GumbelSoftmax (Jang et al., 2017; Maddison et al., 2017) is a popular choice that has been considered for supervised text classification (Chen and Gimpel, 2018) and dialog generation (Zhao et al., 2018). In the binary latent variable setting, straight-through estimators are often used (Dong et al., 2019). Another choice is “continuous decoding” which takes a convex combination of latent values to make the loss differentiable (Al-Shedivat and Parikh, 2019). Yet a less considered choice is Hard EM (Brown et al., 1993; De Marcken, 1995; Spitkovsky et al., 2010). A main contribution of this work is a thorough empirical comparison between such different choices in a controlled setting. To demonstrate the usefulness of our models, we focus on improving low-resource classification performance by pretraining on unlabeled text. Previous best results are obtained with continuous latentvariable VAEs, e.g., VAMPIRE (Gururangan et al., 2019). We show that our discrete representations outperform these previous results while being significantly more lightweight. 3 Background We consider generative models of a sequence x = x1:T of T word tokens. We assume our la"
2020.acl-main.437,P19-1533,1,0.815841,"d. 8.3 Compression We briefly discuss in what sense discrete latent representations reduce storage requirements. Given a vocabulary of size 30,000, storing a T -length sentence requires T log2 30000 ≈ 14.9T bits. Our While the above holds for storage, the space required to classify a sentence represented as M L integers using a parametric classifier may not be smaller than that required for classifying a sentence represented as a d-dimensional floating point vector. On the other hand, nearest neighbor-based methods, which are experiencing renewed interest (Guu et al., 2018; Chen et al., 2019; Wiseman and Stratos, 2019), should be significantly less expensive in terms of time and memory when sentences are encoded as M L integers rather than d-dimensional floating point vectors. In the next subsection we quantitatively evaluate our discrete representations in a nearest neighbor-based retrieval setting. 4838 M=4, K=256 Hard EM CatVAE VQ-VAE 76.1 77.5 69.1 L2 GloVe fastText Discrete Embedding M=8, K=128 M=16, K=256 79.6 73.7 73.5 78.8 78.5 71.2 Continuous Embedding (300d) C OSINE 76.4 72.8 76.6 74.1 Table 7: Unsupervised document retrieval on AG News dataset, measured by average label precision of top 100 neare"
2020.findings-emnlp.313,W14-3000,0,0.0546905,"Missing"
2020.findings-emnlp.313,P14-1098,0,0.0492436,"Missing"
2020.findings-emnlp.313,E12-1004,0,0.0252224,"I dataset called “Break” using external knowledge bases such as WordNet. Since sentence pairs in the dataset only differ by one or two words, similar to a pair of adversarial examples, it has broken many NLI systems. Due to the fact that Break does not have a training split, we use the aforementioned subsampled MNLI training set as a training set for this dataset. We select the best performing model on the development set of MNLI and evaluate it on Break. 4.2.2 Lexical Entailment We use the lexical splits for 3 datasets from Levy et al. (2015), including K2010 (Kotlerman et al., 2009), B2012 (Baroni et al., 2012), and T2014 (Turney and Mohammad, 2015). These datasets all similarly formulate lexical entailment as a binary task, and they were constructed from diverse sources, including human annotations, WordNet, and Wikidata. 3503 BERT +WordNet +Wikidata +W IKI NLI RoBERTa +WordNet +Wikidata +W IKI NLI MNLI 75.0 75.8 75.7 76.4 82.5 83.8 84.0 84.4 Natural Language Inference RTE PPDB Break SciTail 69.9 66.7 80.2 92.3 71.3 71.1 83.5 90.8 71.3 75.0 81.3 91.5 70.9 70.7 85.7 91.8 78.8 65.9 81.3 93.6 82.2 72.0 82.3 93.9 82.3 72.5 83.2 92.9 83.1 71.7 83.8 93.0 Lexical Entailment K2010 B2012 T2014 85.2 79.4 63."
2020.findings-emnlp.313,J15-2003,0,0.0208077,"et al., 2013), but they are used in a particular form of distant supervision in which they are matched with an additional corpus to create noisy labels. In contrast, we use the category relations directly without requiring such additional steps. Onoe and Durrett (2020) use the direct parent categories of hyperlinks for training entity linking systems. Within this first approach, there have been many efforts aimed at harvesting inference rules from raw text (Lin and Pantel, 2001; Szpektor et al., 2004; Bhagat et al., 2007; Szpektor and Dagan, 2008; Yates and Etzioni, 2009; Bansal et al., 2014; Berant et al., 2015; Hosseini et al., 2018). Since W IKI NLI uses category pairs in which one is a hyponym of the other, it is more closely related to work in extracting hyponym-hypernym pairs from text (Hearst, 1992; Snow et al., 2005, 2006; Pasca and Durme, 2007; McNamee et al., 2008; Le et al., 2019). Pavlick et al. (2015) automatically generate a large-scale phrase pair dataset with several relationships by training classifiers on a relatively small amount of human-annotated data. However, … … … … Entertainment Events Days Day Calendar Day Firework Events Holidays Public Holiday Day New Year’s Eve New Year’s"
2020.findings-emnlp.313,D07-1017,0,0.13943,"Missing"
2020.findings-emnlp.313,D15-1075,0,0.0395989,"uage inference (NLI) is the task of classifying the relationship, such as entailment or contradiction, between sentences. It has been found useful in downstream tasks, such as summarization (Mehdad et al., 2013) and long-form text generation (Holtzman et al., 2018). NLI involves rich natural language understanding capabilities, many of which relate to world knowledge. To acquire such knowledge, researchers have found benefit from external knowledge bases like WordNet (Fellbaum, 1998), FrameNet (Baker, 2014), Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014), and large-scale human-annotated datasets (Bowman et al., 2015; Williams et al., 2018; Nie et al., 2020). Creating ∗ Equal contribution. Listed in alphabetical order. Code and data are available at https://github. com/ZeweiChu/WikiNLI. 1 these resources generally requires expensive human annotation. In this work, we are interested in automatically generating a large-scale dataset from Wikipedia categories that can improve performance on both NLI and lexical entailment (LE) tasks. One key component of NLI tasks is recognizing lexical and phrasal hypernym relationships. For example, vehicle is a hypernym of car. In this paper, we take advantage of the natu"
2020.findings-emnlp.313,D19-1040,1,0.833327,"hmark several resources on XNLI (Conneau et al., 2018), showing that W IKI NLI benefits performance on NLI tasks in the corresponding languages. 2 Related Work We build on a rich body of literature on leveraging specialized resources (such as knowledge bases) to enhance model performance. These works either (1) pretrain the model on datasets extracted from such resources, or (2) use the resources directly by changing the model itself. The first approach aims to improve performance at test time by designing useful signals for pretraining, for instance using hyperlinks (Logeswaran et al., 2019; Chen et al., 2019a) or document structure in Wikipedia (Chen et al., 2019b), knowledge bases (Logan et al., 2019), and discourse markers (Nie et al., 2019). Here, we focus on using category hierarchies in Wikipedia. There are some previous works that also use category relations derived from knowledge bases (Shwartz et al., 2016; Riedel et al., 2013), but they are used in a particular form of distant supervision in which they are matched with an additional corpus to create noisy labels. In contrast, we use the category relations directly without requiring such additional steps. Onoe and Durrett (2020) use the d"
2020.findings-emnlp.313,N19-1423,0,0.459826,"iversity, NJ, USA 2 University of Chicago, IL, USA 3 Toyota Technological Institute at Chicago, IL, USA {mchen,kgimpel}@ttic.edu,zeweichu@gmail.com,stratos@cs.rutgers.edu Abstract Accurate lexical entailment (LE) and natural language inference (NLI) often require large quantities of costly annotations. To alleviate the need for labeled data, we introduce W IKI NLI: a resource for improving model performance on NLI and LE tasks. It contains 428,899 pairs of phrases constructed from naturally annotated category hierarchies in Wikipedia. We show that we can improve strong baselines such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) by pretraining them on W IKI NLI and transferring the models on downstream tasks. We conduct systematic comparisons with phrases extracted from other knowledge bases such as WordNet and Wikidata to find that pretraining on W IKI NLI gives the best performance. In addition, we construct W IKI NLI in other languages, and show that pretraining on them improves performance on NLI tasks of corresponding languages.1 1 Introduction Natural language inference (NLI) is the task of classifying the relationship, such as entailment or contradiction, between sentences. It ha"
2020.findings-emnlp.313,W07-1401,0,0.191366,"Missing"
2020.findings-emnlp.313,P18-2103,0,0.0435565,"Missing"
2020.findings-emnlp.313,C92-2082,0,0.174568,"without requiring such additional steps. Onoe and Durrett (2020) use the direct parent categories of hyperlinks for training entity linking systems. Within this first approach, there have been many efforts aimed at harvesting inference rules from raw text (Lin and Pantel, 2001; Szpektor et al., 2004; Bhagat et al., 2007; Szpektor and Dagan, 2008; Yates and Etzioni, 2009; Bansal et al., 2014; Berant et al., 2015; Hosseini et al., 2018). Since W IKI NLI uses category pairs in which one is a hyponym of the other, it is more closely related to work in extracting hyponym-hypernym pairs from text (Hearst, 1992; Snow et al., 2005, 2006; Pasca and Durme, 2007; McNamee et al., 2008; Le et al., 2019). Pavlick et al. (2015) automatically generate a large-scale phrase pair dataset with several relationships by training classifiers on a relatively small amount of human-annotated data. However, … … … … Entertainment Events Days Day Calendar Day Firework Events Holidays Public Holiday Day New Year’s Eve New Year’s Eve New Year’s Eve Wikipedia categories Wikidata WordNet Figure 1: Example hierarchies obtained from Wikipedia categories, Wikidata, and WordNet. most of this prior work uses raw text or raw text"
2020.findings-emnlp.313,D19-1060,1,0.892523,"Missing"
2020.findings-emnlp.313,P18-1152,0,0.0119571,"conduct systematic comparisons with phrases extracted from other knowledge bases such as WordNet and Wikidata to find that pretraining on W IKI NLI gives the best performance. In addition, we construct W IKI NLI in other languages, and show that pretraining on them improves performance on NLI tasks of corresponding languages.1 1 Introduction Natural language inference (NLI) is the task of classifying the relationship, such as entailment or contradiction, between sentences. It has been found useful in downstream tasks, such as summarization (Mehdad et al., 2013) and long-form text generation (Holtzman et al., 2018). NLI involves rich natural language understanding capabilities, many of which relate to world knowledge. To acquire such knowledge, researchers have found benefit from external knowledge bases like WordNet (Fellbaum, 1998), FrameNet (Baker, 2014), Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014), and large-scale human-annotated datasets (Bowman et al., 2015; Williams et al., 2018; Nie et al., 2020). Creating ∗ Equal contribution. Listed in alphabetical order. Code and data are available at https://github. com/ZeweiChu/WikiNLI. 1 these resources generally requires expensive human annotation. In this"
2020.findings-emnlp.313,P18-1224,0,0.0626421,"ew Year’s Eve New Year’s Eve Wikipedia categories Wikidata WordNet Figure 1: Example hierarchies obtained from Wikipedia categories, Wikidata, and WordNet. most of this prior work uses raw text or raw text combined with either annotated data or curated resources like WordNet. W IKI NLI, on the other hand, seeks a middle road, striving to find largescale, naturally-annotated data that can improve performance on NLI tasks. The second approach aims to enable the model to leverage knowledge resources during prediction, for instance by computing attention weights over lexical relations in WordNet (Chen et al., 2018) or linking to reference entities in knowledge bases within the transformer block (Peters et al., 2019). While effective, this approach requires nontrivial and domain-specific modifications of the model itself. In contrast, we develop a simple pretraining method to leverage knowledge bases that can likewise improve the performance of already strong baselines such as BERT without requiring such complex model modifications. There are some additional related works that focus on the category information of Wikipedia. Ponzetto and Strube (2007) and Nastase and Strube (2008) extract knowledge of ent"
2020.findings-emnlp.313,Q18-1048,0,0.0130016,"ey are used in a particular form of distant supervision in which they are matched with an additional corpus to create noisy labels. In contrast, we use the category relations directly without requiring such additional steps. Onoe and Durrett (2020) use the direct parent categories of hyperlinks for training entity linking systems. Within this first approach, there have been many efforts aimed at harvesting inference rules from raw text (Lin and Pantel, 2001; Szpektor et al., 2004; Bhagat et al., 2007; Szpektor and Dagan, 2008; Yates and Etzioni, 2009; Bansal et al., 2014; Berant et al., 2015; Hosseini et al., 2018). Since W IKI NLI uses category pairs in which one is a hyponym of the other, it is more closely related to work in extracting hyponym-hypernym pairs from text (Hearst, 1992; Snow et al., 2005, 2006; Pasca and Durme, 2007; McNamee et al., 2008; Le et al., 2019). Pavlick et al. (2015) automatically generate a large-scale phrase pair dataset with several relationships by training classifiers on a relatively small amount of human-annotated data. However, … … … … Entertainment Events Days Day Calendar Day Firework Events Holidays Public Holiday Day New Year’s Eve New Year’s Eve New Year’s Eve Wiki"
2020.findings-emnlp.313,P09-2018,0,0.0511949,"Missing"
2020.findings-emnlp.313,P19-1313,0,0.0115826,"ent categories of hyperlinks for training entity linking systems. Within this first approach, there have been many efforts aimed at harvesting inference rules from raw text (Lin and Pantel, 2001; Szpektor et al., 2004; Bhagat et al., 2007; Szpektor and Dagan, 2008; Yates and Etzioni, 2009; Bansal et al., 2014; Berant et al., 2015; Hosseini et al., 2018). Since W IKI NLI uses category pairs in which one is a hyponym of the other, it is more closely related to work in extracting hyponym-hypernym pairs from text (Hearst, 1992; Snow et al., 2005, 2006; Pasca and Durme, 2007; McNamee et al., 2008; Le et al., 2019). Pavlick et al. (2015) automatically generate a large-scale phrase pair dataset with several relationships by training classifiers on a relatively small amount of human-annotated data. However, … … … … Entertainment Events Days Day Calendar Day Firework Events Holidays Public Holiday Day New Year’s Eve New Year’s Eve New Year’s Eve Wikipedia categories Wikidata WordNet Figure 1: Example hierarchies obtained from Wikipedia categories, Wikidata, and WordNet. most of this prior work uses raw text or raw text combined with either annotated data or curated resources like WordNet. W IKI NLI, on the"
2020.findings-emnlp.313,N15-1098,0,0.0242788,"test sets. Break. Glockner et al. (2018) constructed a challenging NLI dataset called “Break” using external knowledge bases such as WordNet. Since sentence pairs in the dataset only differ by one or two words, similar to a pair of adversarial examples, it has broken many NLI systems. Due to the fact that Break does not have a training split, we use the aforementioned subsampled MNLI training set as a training set for this dataset. We select the best performing model on the development set of MNLI and evaluate it on Break. 4.2.2 Lexical Entailment We use the lexical splits for 3 datasets from Levy et al. (2015), including K2010 (Kotlerman et al., 2009), B2012 (Baroni et al., 2012), and T2014 (Turney and Mohammad, 2015). These datasets all similarly formulate lexical entailment as a binary task, and they were constructed from diverse sources, including human annotations, WordNet, and Wikidata. 3503 BERT +WordNet +Wikidata +W IKI NLI RoBERTa +WordNet +Wikidata +W IKI NLI MNLI 75.0 75.8 75.7 76.4 82.5 83.8 84.0 84.4 Natural Language Inference RTE PPDB Break SciTail 69.9 66.7 80.2 92.3 71.3 71.1 83.5 90.8 71.3 75.0 81.3 91.5 70.9 70.7 85.7 91.8 78.8 65.9 81.3 93.6 82.2 72.0 82.3 93.9 82.3 72.5 83.2 92.9"
2020.findings-emnlp.313,2021.ccl-1.108,0,0.0511015,"Missing"
2020.findings-emnlp.313,P19-1598,0,0.028384,"mance on NLI tasks in the corresponding languages. 2 Related Work We build on a rich body of literature on leveraging specialized resources (such as knowledge bases) to enhance model performance. These works either (1) pretrain the model on datasets extracted from such resources, or (2) use the resources directly by changing the model itself. The first approach aims to improve performance at test time by designing useful signals for pretraining, for instance using hyperlinks (Logeswaran et al., 2019; Chen et al., 2019a) or document structure in Wikipedia (Chen et al., 2019b), knowledge bases (Logan et al., 2019), and discourse markers (Nie et al., 2019). Here, we focus on using category hierarchies in Wikipedia. There are some previous works that also use category relations derived from knowledge bases (Shwartz et al., 2016; Riedel et al., 2013), but they are used in a particular form of distant supervision in which they are matched with an additional corpus to create noisy labels. In contrast, we use the category relations directly without requiring such additional steps. Onoe and Durrett (2020) use the direct parent categories of hyperlinks for training entity linking systems. Within this first app"
2020.findings-emnlp.313,P19-1335,0,0.0145051,"other languages and benchmark several resources on XNLI (Conneau et al., 2018), showing that W IKI NLI benefits performance on NLI tasks in the corresponding languages. 2 Related Work We build on a rich body of literature on leveraging specialized resources (such as knowledge bases) to enhance model performance. These works either (1) pretrain the model on datasets extracted from such resources, or (2) use the resources directly by changing the model itself. The first approach aims to improve performance at test time by designing useful signals for pretraining, for instance using hyperlinks (Logeswaran et al., 2019; Chen et al., 2019a) or document structure in Wikipedia (Chen et al., 2019b), knowledge bases (Logan et al., 2019), and discourse markers (Nie et al., 2019). Here, we focus on using category hierarchies in Wikipedia. There are some previous works that also use category relations derived from knowledge bases (Shwartz et al., 2016; Riedel et al., 2013), but they are used in a particular form of distant supervision in which they are matched with an additional corpus to create noisy labels. In contrast, we use the category relations directly without requiring such additional steps. Onoe and Durre"
2020.findings-emnlp.313,I08-2112,0,0.0473055,"20) use the direct parent categories of hyperlinks for training entity linking systems. Within this first approach, there have been many efforts aimed at harvesting inference rules from raw text (Lin and Pantel, 2001; Szpektor et al., 2004; Bhagat et al., 2007; Szpektor and Dagan, 2008; Yates and Etzioni, 2009; Bansal et al., 2014; Berant et al., 2015; Hosseini et al., 2018). Since W IKI NLI uses category pairs in which one is a hyponym of the other, it is more closely related to work in extracting hyponym-hypernym pairs from text (Hearst, 1992; Snow et al., 2005, 2006; Pasca and Durme, 2007; McNamee et al., 2008; Le et al., 2019). Pavlick et al. (2015) automatically generate a large-scale phrase pair dataset with several relationships by training classifiers on a relatively small amount of human-annotated data. However, … … … … Entertainment Events Days Day Calendar Day Firework Events Holidays Public Holiday Day New Year’s Eve New Year’s Eve New Year’s Eve Wikipedia categories Wikidata WordNet Figure 1: Example hierarchies obtained from Wikipedia categories, Wikidata, and WordNet. most of this prior work uses raw text or raw text combined with either annotated data or curated resources like WordNet."
2020.findings-emnlp.313,W13-2117,0,0.0242457,"and transferring the models on downstream tasks. We conduct systematic comparisons with phrases extracted from other knowledge bases such as WordNet and Wikidata to find that pretraining on W IKI NLI gives the best performance. In addition, we construct W IKI NLI in other languages, and show that pretraining on them improves performance on NLI tasks of corresponding languages.1 1 Introduction Natural language inference (NLI) is the task of classifying the relationship, such as entailment or contradiction, between sentences. It has been found useful in downstream tasks, such as summarization (Mehdad et al., 2013) and long-form text generation (Holtzman et al., 2018). NLI involves rich natural language understanding capabilities, many of which relate to world knowledge. To acquire such knowledge, researchers have found benefit from external knowledge bases like WordNet (Fellbaum, 1998), FrameNet (Baker, 2014), Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014), and large-scale human-annotated datasets (Bowman et al., 2015; Williams et al., 2018; Nie et al., 2020). Creating ∗ Equal contribution. Listed in alphabetical order. Code and data are available at https://github. com/ZeweiChu/WikiNLI. 1 these resources"
2020.findings-emnlp.313,nastase-etal-2010-wikinet,0,0.0279548,"ransformer block (Peters et al., 2019). While effective, this approach requires nontrivial and domain-specific modifications of the model itself. In contrast, we develop a simple pretraining method to leverage knowledge bases that can likewise improve the performance of already strong baselines such as BERT without requiring such complex model modifications. There are some additional related works that focus on the category information of Wikipedia. Ponzetto and Strube (2007) and Nastase and Strube (2008) extract knowledge of entities from the Wikipedia category graphs using predefined rules. Nastase et al. (2010) build a dataset based on Wikipedia article or category titles as well as the relations between categories and pages (“WikiNet”), but they do not empirically validate the usefulness of the dataset. In a similarly non-empirical vein, Zesch and Gurevych (2007) analyze the differences between the graphs from WordNet and the ones from Wikipedia categories. Instead, we address the empirical benefits of leveraging the category information in the modern setting of pretrained text representations. 3 W IKI NLI We now describe how the W IKI NLI dataset is constructed from Wikipedia and its principal cha"
2020.findings-emnlp.313,P19-1442,0,0.0155073,"uages. 2 Related Work We build on a rich body of literature on leveraging specialized resources (such as knowledge bases) to enhance model performance. These works either (1) pretrain the model on datasets extracted from such resources, or (2) use the resources directly by changing the model itself. The first approach aims to improve performance at test time by designing useful signals for pretraining, for instance using hyperlinks (Logeswaran et al., 2019; Chen et al., 2019a) or document structure in Wikipedia (Chen et al., 2019b), knowledge bases (Logan et al., 2019), and discourse markers (Nie et al., 2019). Here, we focus on using category hierarchies in Wikipedia. There are some previous works that also use category relations derived from knowledge bases (Shwartz et al., 2016; Riedel et al., 2013), but they are used in a particular form of distant supervision in which they are matched with an additional corpus to create noisy labels. In contrast, we use the category relations directly without requiring such additional steps. Onoe and Durrett (2020) use the direct parent categories of hyperlinks for training entity linking systems. Within this first approach, there have been many efforts aimed"
2020.findings-emnlp.313,2020.acl-main.441,0,0.141866,"ing the relationship, such as entailment or contradiction, between sentences. It has been found useful in downstream tasks, such as summarization (Mehdad et al., 2013) and long-form text generation (Holtzman et al., 2018). NLI involves rich natural language understanding capabilities, many of which relate to world knowledge. To acquire such knowledge, researchers have found benefit from external knowledge bases like WordNet (Fellbaum, 1998), FrameNet (Baker, 2014), Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014), and large-scale human-annotated datasets (Bowman et al., 2015; Williams et al., 2018; Nie et al., 2020). Creating ∗ Equal contribution. Listed in alphabetical order. Code and data are available at https://github. com/ZeweiChu/WikiNLI. 1 these resources generally requires expensive human annotation. In this work, we are interested in automatically generating a large-scale dataset from Wikipedia categories that can improve performance on both NLI and lexical entailment (LE) tasks. One key component of NLI tasks is recognizing lexical and phrasal hypernym relationships. For example, vehicle is a hypernym of car. In this paper, we take advantage of the naturally-annotated Wikipedia category graph,"
2020.findings-emnlp.313,P15-1146,0,0.037768,"Missing"
2020.findings-emnlp.313,N18-1202,0,0.00983287,"constructing W IKI NLI are as follows. We use the tables “categorylinks” and “page”: these two pages provide category pairs in which one category is the parent of the other. We use all direct category relations. To eliminate trivial pairs, we remove pairs where either is a substring of the other. To construct neutral pairs, we randomly sample two categories where neither category is the ancestor of the other in the category graph. To make neutral pairs more “related” (so that they are harder to discriminate from direct relations), we encode both categories into continuous vectors using ELMo (Peters et al., 2018) (averaging its three layers over all positions) and compute the cosine similarities between pairs.2 We pick the topranked pairs as neutral pairs in W IKI NLI. After the above processing, we remove categories longer than 50 characters and those containing certain keywords3 (see supplementary material for more results and examples on filtering criteria). We ensure the dataset is balanced, and the final dataset has 428,899 unique pairs. For the following experiments, unless otherwise specified, we only use 100,000 samples from W IKI NLI as training data and 5,000 as the development set since we"
2020.findings-emnlp.313,D19-1005,0,0.0742555,"Missing"
2020.findings-emnlp.313,N13-1008,0,0.0192546,"on datasets extracted from such resources, or (2) use the resources directly by changing the model itself. The first approach aims to improve performance at test time by designing useful signals for pretraining, for instance using hyperlinks (Logeswaran et al., 2019; Chen et al., 2019a) or document structure in Wikipedia (Chen et al., 2019b), knowledge bases (Logan et al., 2019), and discourse markers (Nie et al., 2019). Here, we focus on using category hierarchies in Wikipedia. There are some previous works that also use category relations derived from knowledge bases (Shwartz et al., 2016; Riedel et al., 2013), but they are used in a particular form of distant supervision in which they are matched with an additional corpus to create noisy labels. In contrast, we use the category relations directly without requiring such additional steps. Onoe and Durrett (2020) use the direct parent categories of hyperlinks for training entity linking systems. Within this first approach, there have been many efforts aimed at harvesting inference rules from raw text (Lin and Pantel, 2001; Szpektor et al., 2004; Bhagat et al., 2007; Szpektor and Dagan, 2008; Yates and Etzioni, 2009; Bansal et al., 2014; Berant et al."
2020.findings-emnlp.313,P16-1226,0,0.0232149,"Missing"
2020.findings-emnlp.313,P06-1101,0,0.109827,"Missing"
2020.findings-emnlp.313,N18-1101,0,0.224903,"is the task of classifying the relationship, such as entailment or contradiction, between sentences. It has been found useful in downstream tasks, such as summarization (Mehdad et al., 2013) and long-form text generation (Holtzman et al., 2018). NLI involves rich natural language understanding capabilities, many of which relate to world knowledge. To acquire such knowledge, researchers have found benefit from external knowledge bases like WordNet (Fellbaum, 1998), FrameNet (Baker, 2014), Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014), and large-scale human-annotated datasets (Bowman et al., 2015; Williams et al., 2018; Nie et al., 2020). Creating ∗ Equal contribution. Listed in alphabetical order. Code and data are available at https://github. com/ZeweiChu/WikiNLI. 1 these resources generally requires expensive human annotation. In this work, we are interested in automatically generating a large-scale dataset from Wikipedia categories that can improve performance on both NLI and lexical entailment (LE) tasks. One key component of NLI tasks is recognizing lexical and phrasal hypernym relationships. For example, vehicle is a hypernym of car. In this paper, we take advantage of the naturally-annotated Wikiped"
2020.findings-emnlp.313,W07-0201,0,0.0119581,"e performance of already strong baselines such as BERT without requiring such complex model modifications. There are some additional related works that focus on the category information of Wikipedia. Ponzetto and Strube (2007) and Nastase and Strube (2008) extract knowledge of entities from the Wikipedia category graphs using predefined rules. Nastase et al. (2010) build a dataset based on Wikipedia article or category titles as well as the relations between categories and pages (“WikiNet”), but they do not empirically validate the usefulness of the dataset. In a similarly non-empirical vein, Zesch and Gurevych (2007) analyze the differences between the graphs from WordNet and the ones from Wikipedia categories. Instead, we address the empirical benefits of leveraging the category information in the modern setting of pretrained text representations. 3 W IKI NLI We now describe how the W IKI NLI dataset is constructed from Wikipedia and its principal characteristics. Each Wikipedia article is associated with crowd-sourced categories that correspond to topics or concepts covered by that article. Wikipedia or3501 ganizes these categories into a directed graph that models their hierarchical relations. For inst"
2020.findings-emnlp.313,C08-1107,0,0.0527167,"ry relations derived from knowledge bases (Shwartz et al., 2016; Riedel et al., 2013), but they are used in a particular form of distant supervision in which they are matched with an additional corpus to create noisy labels. In contrast, we use the category relations directly without requiring such additional steps. Onoe and Durrett (2020) use the direct parent categories of hyperlinks for training entity linking systems. Within this first approach, there have been many efforts aimed at harvesting inference rules from raw text (Lin and Pantel, 2001; Szpektor et al., 2004; Bhagat et al., 2007; Szpektor and Dagan, 2008; Yates and Etzioni, 2009; Bansal et al., 2014; Berant et al., 2015; Hosseini et al., 2018). Since W IKI NLI uses category pairs in which one is a hyponym of the other, it is more closely related to work in extracting hyponym-hypernym pairs from text (Hearst, 1992; Snow et al., 2005, 2006; Pasca and Durme, 2007; McNamee et al., 2008; Le et al., 2019). Pavlick et al. (2015) automatically generate a large-scale phrase pair dataset with several relationships by training classifiers on a relatively small amount of human-annotated data. However, … … … … Entertainment Events Days Day Calendar Day Fi"
2020.findings-emnlp.313,W04-3206,0,0.106866,"are some previous works that also use category relations derived from knowledge bases (Shwartz et al., 2016; Riedel et al., 2013), but they are used in a particular form of distant supervision in which they are matched with an additional corpus to create noisy labels. In contrast, we use the category relations directly without requiring such additional steps. Onoe and Durrett (2020) use the direct parent categories of hyperlinks for training entity linking systems. Within this first approach, there have been many efforts aimed at harvesting inference rules from raw text (Lin and Pantel, 2001; Szpektor et al., 2004; Bhagat et al., 2007; Szpektor and Dagan, 2008; Yates and Etzioni, 2009; Bansal et al., 2014; Berant et al., 2015; Hosseini et al., 2018). Since W IKI NLI uses category pairs in which one is a hyponym of the other, it is more closely related to work in extracting hyponym-hypernym pairs from text (Hearst, 1992; Snow et al., 2005, 2006; Pasca and Durme, 2007; McNamee et al., 2008; Le et al., 2019). Pavlick et al. (2015) automatically generate a large-scale phrase pair dataset with several relationships by training classifiers on a relatively small amount of human-annotated data. However, … … …"
2021.emnlp-main.352,1993.eamt-1.6,0,0.0507711,"Missing"
2021.emnlp-main.352,W96-0102,0,0.161464,"t al., 2019, inter alia), and, most closely, insertion-based (Stern et al., 2019; Gu et al., 2019b,a, inter alia) approaches. Our 6 Related Work work differs from this last category in several imNLP systems have incorporated neighbors for portant respects: first, we insert and replace (and decades. Early work focused on machine trans- model) full spans rather than tokens. Our policies lation (Sumita and Hitoshi, 1991), syntactic disam- are trained to minimize the number of insertion opbiguation (Cardie, 1994), and tagging (Daelemans, erations rather than to insert (centrally positioned) 1993; Daelemans et al., 1996). correct tokens in available slots, as is Insertion While some more recent work has made use of Transformer (Stern et al., 2019), or to mimic a retrieved neighbors for problems such as sequence Levenshtein distance-based oracle, as is LevT (Gu labeling (Wiseman and Stratos, 2019), auditing et al., 2019b). Our policies are also fundamentally multi-label text classification predictions (Schmaltz sequential, unlike these partially autoregressive aland Beam, 2020), and reasoning over knowledge ternatives, which can generate tokens in parallel. bases (Das et al., 2020, 2021), the majority of re- T"
2021.emnlp-main.352,P00-1037,0,0.508756,"ve models. Our work also relates to recent work on sentencelevel transduction tasks, like grammatical error correction (GEC), which allows for directly predicting certain span-level edits (Stahlberg and Kumar, 2020). These edits are different from our insertion operations, requiring token-level operations except when copying from the source sentence, and are obtained, following a long line of work in GEC (Swanson and Yamangil, 2012; Xue and Hwa, 2014; Felice et al., 2016; Bryant et al., 2017), by heuristically merging token-level alignments obtained with a Damerau-Levenshtein-style algorithm (Brill and Moore, 2000). 7 Conclusion In future work we hope to tackle more ambitious text generation tasks, which will likely require retrieving many more neighbors, perhaps dynamically, from larger data-stores, and with more sophisticated retrieval techniques, such as those currently being used in retrieval-based pretraining (Lewis et al., 2020a; Guu et al., 2020). We also hope to consider more sophisticated models, which explicitly capture the history of produced canvases, and more sophisticated training approaches, which search for optimal insertions while training, rather than as a preprocessing step (Daumé III"
2021.emnlp-main.352,P17-1074,0,0.0117945,", decoding serially with beam search will generally be slower than the iterated parallel decoding of partially autoregressive models. Our work also relates to recent work on sentencelevel transduction tasks, like grammatical error correction (GEC), which allows for directly predicting certain span-level edits (Stahlberg and Kumar, 2020). These edits are different from our insertion operations, requiring token-level operations except when copying from the source sentence, and are obtained, following a long line of work in GEC (Swanson and Yamangil, 2012; Xue and Hwa, 2014; Felice et al., 2016; Bryant et al., 2017), by heuristically merging token-level alignments obtained with a Damerau-Levenshtein-style algorithm (Brill and Moore, 2000). 7 Conclusion In future work we hope to tackle more ambitious text generation tasks, which will likely require retrieving many more neighbors, perhaps dynamically, from larger data-stores, and with more sophisticated retrieval techniques, such as those currently being used in retrieval-based pretraining (Lewis et al., 2020a; Guu et al., 2020). We also hope to consider more sophisticated models, which explicitly capture the history of produced canvases, and more sophisti"
2021.emnlp-main.352,2021.emnlp-main.755,0,0.0406905,"Missing"
2021.emnlp-main.352,D18-1340,0,0.0194372,"it, and then evaluate controllability. In Appendix C we describe, as a case study, attempting to control the number of sentences used in E2E dataset generations by controlling the neighbors; we find that F ULL significantly outperforms LRT policies in ensuring that generations have at least three sentences. cent NLP work involving neighbor-based methods has focused on conditioning neural text generation systems on retrieved neighbors. This conditioning is variously accomplished using a conventional encoder in an encoder-decoder setup (Song et al., 2016; Weston et al., 2018; Gu et al., 2018b; Cao and Xiong, 2018; Bapna and Firat, 2019), by allowing the parameters of the decoder to depend on the retrieved neighbor (Peng et al., 2019), or by viewing the unknown neighbor as a latent variable (Hashimoto et al., 2018; Guu et al., 2018; Chen et al., 2019; He et al., 2020). Recent work (Zhang et al., 2018; Khandelwal et al., 2019, 2020) has also used retrieved neighbors at decoding time to modify the next-token distribution of the decoder. Our work differs from these approaches in that we explicitly parameterize the splicing operations that form a generation from neighbors, rather than conditioning or other"
2021.emnlp-main.352,2021.eacl-main.64,0,0.0423505,"d neighbors, and of Li and Rush (2020), which produces interpretable segmentations, as well as with the model of Chen et al. (2020) (“KGPT” in tables), which is a fine-tuned, large pretrained model, and which we take to be close to the state of the art. We first note that our baselines are quite strong, largely outperforming previous work, including large pretrained models. In the case of E2E, we find that the F ULL model slightly outperforms these strong baselines and attains, we believe, state-ofthe-art performance in the setting where no pretrained models or data augmentation is used. (See Chang et al. (2021) for even better results without these restrictions). On WikiBio, however, F ULL slightly underperforms the strongest baselines. pendix B; we generally find that F ULL generations do not hallucinate more than S2S+Copy generations (the most faithful generations according to crowd-workers), but they do more frequently contradict information in the source table. This generally occurs when a span containing information that contradicts the table is copied to the canvas, and this information is not subsequently replaced; see Appendix B for more details and a discussion. 5.2 Interpretability Evaluat"
2021.emnlp-main.352,P19-1599,1,0.847399,"licies in ensuring that generations have at least three sentences. cent NLP work involving neighbor-based methods has focused on conditioning neural text generation systems on retrieved neighbors. This conditioning is variously accomplished using a conventional encoder in an encoder-decoder setup (Song et al., 2016; Weston et al., 2018; Gu et al., 2018b; Cao and Xiong, 2018; Bapna and Firat, 2019), by allowing the parameters of the decoder to depend on the retrieved neighbor (Peng et al., 2019), or by viewing the unknown neighbor as a latent variable (Hashimoto et al., 2018; Guu et al., 2018; Chen et al., 2019; He et al., 2020). Recent work (Zhang et al., 2018; Khandelwal et al., 2019, 2020) has also used retrieved neighbors at decoding time to modify the next-token distribution of the decoder. Our work differs from these approaches in that we explicitly parameterize the splicing operations that form a generation from neighbors, rather than conditioning or otherwise modifying a left-to-right token generation model using retrieved neighbors. Our parameterization is motivated by trying to increase the interpretability and controllability of the generation process, which also motivates recent work mak"
2021.emnlp-main.352,2020.emnlp-main.697,0,0.227498,"arg LRT 70.5 55.8 68.1 9.54 7.39 8.83 76.0 63.7 70.0 2.37 1.68 2.38 49.6 41.0 46.2 S2S+copy Li & Rush KGPT 64.7 67.1 68.1 8.26 8.52 - 69.1 68.7 70.9 2.22 2.24 - 43.7 45.4 45.8 BLEU NIST RG-4 F ULL LRT-no-marg LRT 43.5 45.4 45.7 9.59 10.16 9.99 41.4 39.6 44.0 S2S+copy Peng et al. Li & Rush KGPT 45.4 44.1 44.7 45.1 9.72 9.92 - 44.6 41.1 43.3 - WB Table 1: Standard automatic evaluation metrics for the E2E dataset (top) and WikiBio dataset (bottom). Baselines include our own transformer sequence-tosequence-with-copy model (“S2S+copy”), and the models of Li and Rush (2020), Peng et al. (2019), and Chen et al. (2020, “KGPT”). E2E F ULL LRT S2S+copy Natural Faithful Informative 3.87 3.75 3.87 3.97 3.94 3.94 3.89 3.94 3.81 3.69 3.83 3.75 3.52 3.74 3.75 3.27 3.37 3.40 WB F ULL LRT S2S+copy Table 2: Average rating (on 1-5 Likert scale) of generations’ naturalness, faithfulness, and informativeness, according to crowd-workers. No pairwise differences are significant under a Tukey HSD test. and Ba, 2015; Loshchilov and Hutter, 2018), using linear learning-rate warm-up and square-root decay as in Devlin et al. (2019), until validation loss stops decreasing. We generate with beam search (see Section 4.2), and ne"
2021.emnlp-main.352,N19-1423,0,0.00939178,"quence-with-copy model (“S2S+copy”), and the models of Li and Rush (2020), Peng et al. (2019), and Chen et al. (2020, “KGPT”). E2E F ULL LRT S2S+copy Natural Faithful Informative 3.87 3.75 3.87 3.97 3.94 3.94 3.89 3.94 3.81 3.69 3.83 3.75 3.52 3.74 3.75 3.27 3.37 3.40 WB F ULL LRT S2S+copy Table 2: Average rating (on 1-5 Likert scale) of generations’ naturalness, faithfulness, and informativeness, according to crowd-workers. No pairwise differences are significant under a Tukey HSD test. and Ba, 2015; Loshchilov and Hutter, 2018), using linear learning-rate warm-up and square-root decay as in Devlin et al. (2019), until validation loss stops decreasing. We generate with beam search (see Section 4.2), and neighbor-based models use 20 neighbors at test time, just as at training time. We discuss hyperparameters and tuning in Appendix D. We include sample generations from all systems in Appendix F, and additional visualizations of some F ULL generations in Figure 3. 5.1 Quality Evaluation We first evaluate our models and baselines using the standard automatic metrics associated with each dataset, including BLEU (Papineni et al., 2002), 4 We use the huggingface (Wolf et al., 2020) impleNIST, ROUGE (Lin, 20"
2021.emnlp-main.352,P19-1483,0,0.026969,"ted fol- WikiBio validation set, along with their derivations, lowing the methodology described in Reiter (2017). in Figure 3. However, it is difficult to precisely quantify the We ask crowd-workers on Amazon Mechanical Turk to score generations in terms of their natu- interpretability of a text generation model, and so we now quantify several aspects of our models’ preralness, their faithfulness to the source table, and their informativeness, on a 5-point Likert scale. dictions that presumably correlate with their interpretability. Table 3 shows the average length of the (Note that following Dhingra et al. (2019) we ask derivations (i.e., how many insert operations are reabout informativeness rather than usefulness). We quired to form a generation), the average number of score a total of 45 random examples from each neighbors used in forming a prediction, and the pertest dataset, with each generation being rated by 3 centage of generated tokens copied from a neighcrowd-workers, and each crowd-worker seeing a bor or the source x (rather than generated from the generation from each system. We ran multi-way model’s output vocabulary) for F ULL and LRT-noANOVAs with system-type (i.e., F ULL, LRT, or marg"
2021.emnlp-main.352,W18-2706,0,0.0257581,"true corresponding reference text y1:Tx ∈ V Tx consisting of Tx tokens. Since we are interested in nearest neighbor-based generation, we will also assume that along with each input x we have a set (n) N = {ν1:Tn }N n=1 of N neighbor sequences, with (n) each νt ∈ V. We will be interested in learning to form y1:Tx from its corresponding x and neighbor set N in a way that will be made more precise below. We note that finding an appropriate set of neighbor sequences to allow for successful generation with respect to an input x is an interesting and challenging problem (see, e.g., Hashimoto et al. (2018)), but for the purposes of our exposition we will assume these neighbor sequences are easy to obtain given only x (and without knowledge of y). We give the details of our simple retrieval approach in Section 5. icy π that consumes x and a canvas yˆ1:M ∈ V M representing a prefix, and produces either an action a ∈ A = V, or else a hstopi action and generation terminates. When an action a ∈ V is chosen, this leads to the formation of a new prefix canvas yˆ1:M · a, where · is the concatenation operator. Imitation learning of text generation policies conventionally proceeds by “rolling in” to a ca"
2021.emnlp-main.352,D19-1633,0,0.0169745,"llowed by the generation (Iyyer et al., 2018; Wiseman et al., 2018; Puduppully et al., 2019; Chen et al., 2019; Li and Rush, 2020, inter alia). This more structural or syntactic flavor of controllability differs slightly from foundational work on controlling content or stylistic attributes of text (Hu et al., 2017; Ficler and Goldberg, 2017; Fan et al., 2018). Our approach is also related to work in non-left-to-right text generation, including treebased (Welleck et al., 2019; Akoury et al., 2019), non-autoregressive (Gu et al., 2018a; Lee et al., 2018, inter alia), masked language modelbased (Ghazvininejad et al., 2019, inter alia), and, most closely, insertion-based (Stern et al., 2019; Gu et al., 2019b,a, inter alia) approaches. Our 6 Related Work work differs from this last category in several imNLP systems have incorporated neighbors for portant respects: first, we insert and replace (and decades. Early work focused on machine trans- model) full spans rather than tokens. Our policies lation (Sumita and Hitoshi, 1991), syntactic disam- are trained to minimize the number of insertion opbiguation (Cardie, 1994), and tagging (Daelemans, erations rather than to insert (centrally positioned) 1993; Daelemans e"
2021.emnlp-main.352,Q19-1042,0,0.0170176,"et al., 2019; Li and Rush, 2020, inter alia). This more structural or syntactic flavor of controllability differs slightly from foundational work on controlling content or stylistic attributes of text (Hu et al., 2017; Ficler and Goldberg, 2017; Fan et al., 2018). Our approach is also related to work in non-left-to-right text generation, including treebased (Welleck et al., 2019; Akoury et al., 2019), non-autoregressive (Gu et al., 2018a; Lee et al., 2018, inter alia), masked language modelbased (Ghazvininejad et al., 2019, inter alia), and, most closely, insertion-based (Stern et al., 2019; Gu et al., 2019b,a, inter alia) approaches. Our 6 Related Work work differs from this last category in several imNLP systems have incorporated neighbors for portant respects: first, we insert and replace (and decades. Early work focused on machine trans- model) full spans rather than tokens. Our policies lation (Sumita and Hitoshi, 1991), syntactic disam- are trained to minimize the number of insertion opbiguation (Cardie, 1994), and tagging (Daelemans, erations rather than to insert (centrally positioned) 1993; Daelemans et al., 1996). correct tokens in available slots, as is Insertion While some more recen"
2021.emnlp-main.352,P16-1154,0,0.0355829,"recompute neighbors for each training example, taking the topscoring 20 neighbors for each example in the training data (excluding itself) under a simple score s(·, ·) defined over pairs of inputs in X . For the E2E and WikiBio datasets, we define s(x, x0 ) = F1 (fields(x), fields(x0 )) + 0.1F1 (values(x), values(x0 )), where fields extracts the field-types (e.g., “name”) from the table x, values extracts the unigrams that appear as values in x, and F1 is the F1 -score. Baselines We compare F ULL policies to LRT policies, to a transformer-based sequence-tosequence model with a copy mechanism (Gu et al., 2016) that uses no retrieved neighbors (henceforth “S2S+copy”), and to recent models from the literature (see below). The S2S+copy model uses a generation vocabulary limited to the 30k most frequent target words. The neighbor-based policies, on the other hand, are limited to generating (rather than copying) only from a much smaller vocabulary consisting of target words that occur at least 50 times in the training set and which cannot be obtained from the target’s corresponding neighbors. Additional Details All models are implemented using 6-layer transformer encoders and decoders, with model dimens"
2021.emnlp-main.352,Q18-1031,0,0.0293298,"outperforms LRT policies in ensuring that generations have at least three sentences. cent NLP work involving neighbor-based methods has focused on conditioning neural text generation systems on retrieved neighbors. This conditioning is variously accomplished using a conventional encoder in an encoder-decoder setup (Song et al., 2016; Weston et al., 2018; Gu et al., 2018b; Cao and Xiong, 2018; Bapna and Firat, 2019), by allowing the parameters of the decoder to depend on the retrieved neighbor (Peng et al., 2019), or by viewing the unknown neighbor as a latent variable (Hashimoto et al., 2018; Guu et al., 2018; Chen et al., 2019; He et al., 2020). Recent work (Zhang et al., 2018; Khandelwal et al., 2019, 2020) has also used retrieved neighbors at decoding time to modify the next-token distribution of the decoder. Our work differs from these approaches in that we explicitly parameterize the splicing operations that form a generation from neighbors, rather than conditioning or otherwise modifying a left-to-right token generation model using retrieved neighbors. Our parameterization is motivated by trying to increase the interpretability and controllability of the generation process, which also motiva"
2021.emnlp-main.352,D19-1221,0,0.0208714,"plicing tially constructed generations. Standard techmay also increase our control over the generated niques for training such a policy require an text, and we suspect that approaches that make oracle derivation for each generation, and we clear the provenance of each piece of generated prove that finding the shortest such derivation text (as ours does) will be useful in preventing text can be reduced to parsing under a particular generation systems from emitting harmful or biweighted context-free grammar. We find that policies learned in this way perform on par ased text (Sheng et al., 2019; Wallace et al., 2019; with strong baselines in terms of automatic and Gehman et al., 2020, inter alia). That is, we might human evaluation, but allow for more interimagine preventing systems from emitting harmful pretable and controllable generation. or biased text by only allowing generation from approved neighbor examples. 1 Introduction Methodologically, we implement this generationThere has been recent interest in text generation sys- by-splicing approach by training a policy to directly tems that make use of retrieved “neighbors” — ex- insert or replace spans of neighbor text at arbitrary amples of good text"
2021.emnlp-main.352,W19-3620,0,0.048199,"ext, as we make more precise in the next section. Before doing so, we note that there has been much recent interest in generalizing the forms of policy used in generating text; see Section 6 for references and a discussion. 3 2.1 Imitation Learning for Text Generation Splicing Nearest Neighbors Given a canvas yˆ1:M ∈ V M and a set of neighbor Much recent work views conditional text gener(n) sequences N = {ν1:Tn }N n=1 , we define a generalation as implementing a policy π : X × V ∗ → ized insertion function, which forms a new canvas A ∪ {hstopi} (Bengio et al., 2015; Ranzato et al., 2016); see Welleck et al. (2019) for a recent review. from yˆ1:M . This generalized insertion function implements the following mapping That is, we view a generation algorithm as imple(n) menting a policy that consumes an input x ∈ X as insert(ˆ y1:M , i, j, n, k, l) 7→ yˆ1:i · νk:l · yˆj:M , well as a partially constructed output in the Kleene (1) closure of V, which we will refer to as a “canwhere · is again the concatenation operator, the vas” (Stern et al., 2019), and which outputs either 1 an action a ∈ A or a decision to stop. Taking ac- slice indexing is inclusive, 0 ≤ i < j ≤ M + 1, and 1 ≤ k ≤ l ≤ Tn . Note that thi"
2021.emnlp-main.352,P14-2098,0,0.0218997,"e, serial derivations. On the other hand, decoding serially with beam search will generally be slower than the iterated parallel decoding of partially autoregressive models. Our work also relates to recent work on sentencelevel transduction tasks, like grammatical error correction (GEC), which allows for directly predicting certain span-level edits (Stahlberg and Kumar, 2020). These edits are different from our insertion operations, requiring token-level operations except when copying from the source sentence, and are obtained, following a long line of work in GEC (Swanson and Yamangil, 2012; Xue and Hwa, 2014; Felice et al., 2016; Bryant et al., 2017), by heuristically merging token-level alignments obtained with a Damerau-Levenshtein-style algorithm (Brill and Moore, 2000). 7 Conclusion In future work we hope to tackle more ambitious text generation tasks, which will likely require retrieving many more neighbors, perhaps dynamically, from larger data-stores, and with more sophisticated retrieval techniques, such as those currently being used in retrieval-based pretraining (Lewis et al., 2020a; Guu et al., 2020). We also hope to consider more sophisticated models, which explicitly capture the hist"
2021.emnlp-main.352,N18-1120,0,0.109098,"eved from a database, per- positions within a partially constructed generation, haps paired with the source information on which and we define a generalized insert function capable these example texts condition — on the hope that of such manipulations in Section 3. We train this these neighbors might make a generation task eas- policy with “teacher forcing” (Williams and Zipser, ier, or the system more interpretable or control- 1989), which requires, for each training example, lable (Song et al., 2016; Weston et al., 2018; Guu an oracle sequence of insert actions that derive it. et al., 2018; Zhang et al., 2018; Peng et al., 2019, Accordingly, we define a shortest sequence of acinter alia). tions deriving a training generation from its neighWhereas most work along these lines has bors to be an oracle one, and we prove that, given adopted a conventional encoder-decoder approach, some neighbors, an oracle sequence of actions can conditioning on the retrieved neighbors and then au- be obtained by parsing under a particular weighted toregressively generating text token-by-token from context-free grammar, introduced in Section 3.1.1. left to right, we instead propose to generate text Empirically, we find"
2021.findings-acl.365,P18-1031,0,0.0642031,"Missing"
2021.findings-acl.365,C16-1252,0,0.0235649,"d makes the classifier more robust to the choice of label descriptions.1 1 Introduction Dataless text classification aims at classifying text into categories without using any annotated training data from the task of interest. Prior work (Chang et al., 2008; Song and Roth, 2014) has shown that with effective ways to represent texts and labels, dataless classifiers can perform text classification on unbounded label sets if suitable descriptions of the labels are provided. There have been many previous efforts in dataless or zero-shot text classification (Dauphin et al., 2013; Nam et al., 2016; Li et al., 2016; Ma et al., 2016; Shu et al., 2017; Fei and Liu, 2016; Zhang et al., 2019; Yogatama et al., 2017; Mullenbach et al., 2018; Rios and Kavuluru, 2018; Meng et al., 2019). Several settings have been considered across this prior work, and some have used slightly different definitions of dataless classifiers. In this paper, we use the term “dataless text classification” to refer to methods that: (1) can assign scores to any document-category pair, and (2) do not require any annotated training data from downstream tasks. A dataless classifier can therefore be immediately adapted to a particular labe"
2021.findings-acl.365,N09-1069,0,0.0660264,"labels with a modified k-means clustering algorithm. While dataless text classifiers are designed to handle an unbounded set of categories, they are used and evaluated on a particular set of documents with a set of labels. The idea of our approach is to leverage the assumption that the documents in a text classification dataset are separable according to the accompanying set of labels. That is, given a strong document encoder, the documents should be separable by label in the encoded space. This assumption is similarly made when performing clustering for unsupervised document classification (Liang and Klein, 2009). We use the set of unlabeled input texts to refine the predictions of our dataless classifiers via clustering. To better inform the algorithm, we initialize the clusters by using our dataless classifiers run on the provided label set for each task. The algorithm takes on different forms for the dual and single encoder models. Details are provided below. 3.1 ULR for Dual Encoder Architectures In the setting of a dual encoder model, we propose to perform k-means clustering among text representations, i.e., of vectors produced by the text encoder. The assumption is that texts falling under the s"
2021.findings-acl.365,2021.ccl-1.108,0,0.050426,"Missing"
2021.findings-acl.365,C16-1017,0,0.0299601,"ifier more robust to the choice of label descriptions.1 1 Introduction Dataless text classification aims at classifying text into categories without using any annotated training data from the task of interest. Prior work (Chang et al., 2008; Song and Roth, 2014) has shown that with effective ways to represent texts and labels, dataless classifiers can perform text classification on unbounded label sets if suitable descriptions of the labels are provided. There have been many previous efforts in dataless or zero-shot text classification (Dauphin et al., 2013; Nam et al., 2016; Li et al., 2016; Ma et al., 2016; Shu et al., 2017; Fei and Liu, 2016; Zhang et al., 2019; Yogatama et al., 2017; Mullenbach et al., 2018; Rios and Kavuluru, 2018; Meng et al., 2019). Several settings have been considered across this prior work, and some have used slightly different definitions of dataless classifiers. In this paper, we use the term “dataless text classification” to refer to methods that: (1) can assign scores to any document-category pair, and (2) do not require any annotated training data from downstream tasks. A dataless classifier can therefore be immediately adapted to a particular label set in a downst"
2021.findings-acl.365,D19-6116,0,0.0553819,"Missing"
2021.findings-acl.365,N19-1108,0,0.0225142,"1 1 Introduction Dataless text classification aims at classifying text into categories without using any annotated training data from the task of interest. Prior work (Chang et al., 2008; Song and Roth, 2014) has shown that with effective ways to represent texts and labels, dataless classifiers can perform text classification on unbounded label sets if suitable descriptions of the labels are provided. There have been many previous efforts in dataless or zero-shot text classification (Dauphin et al., 2013; Nam et al., 2016; Li et al., 2016; Ma et al., 2016; Shu et al., 2017; Fei and Liu, 2016; Zhang et al., 2019; Yogatama et al., 2017; Mullenbach et al., 2018; Rios and Kavuluru, 2018; Meng et al., 2019). Several settings have been considered across this prior work, and some have used slightly different definitions of dataless classifiers. In this paper, we use the term “dataless text classification” to refer to methods that: (1) can assign scores to any document-category pair, and (2) do not require any annotated training data from downstream tasks. A dataless classifier can therefore be immediately adapted to a particular label set in a downstream task dataset by scoring each possible label for a do"
2021.louhi-1.4,K18-1050,0,0.104962,"sing any semantic type information, our model significantly out-performs two recent biomedical entity linking models – MedType (Vashishth et al., 2020) and SciSpacy (Neumann et al., 2019) – on two benchmark datasets. 2 2.1 Biomedical Entity Linking 2.3 End-to-End Entity Linking End-to-end entity linking refers to the task of predicting mention spans and the corresponding target entities jointly using a single model. Traditionally, span detection and entity disambiguation tasks were done in a pipelined approach, making these approaches susceptible to error propagation. To alleviate this issue, Kolitsas et al. (2018) proposed a neural end-to-end model that performs the dual tasks of mention span detection and entity disambiguation. However, for span detection and disambiguation, their method relies on an empirical probabilistic entity mapping p(e|m) to select a candidate set C(m) for each mention m. Such mention–entity prior p(e|m) is not available in every domain, especially in the biomedical domain that we consider in this paper. In contrast, our method does not rely on any extrinsic sources of information. Recently, Furrer et al. (2020) proposed a parallel sequence tagging model that treats both span d"
2021.louhi-1.4,W03-0428,0,0.25291,"Missing"
2021.louhi-1.4,N19-1423,0,0.0390981,"task of identifying mentions of named entities (or other terms) in a text document and disambiguating them by mapping them to canonical entities (or concepts) listed in a reference knowledge graph (Hogan et al., 2020). This is an essential step in information extraction, and therefore has been studied extensively both in domainspecific and domain-agnostic settings. Recent stateof-the-art models (Logeswaran et al., 2019; Wu et al., 2019) attempt to learn better representations of mentions and candidates using the rich contextual information encoded in pre-trained language models such as BERT (Devlin et al., 2019). These models follow a retrieve and rerank paradigm, which consists of two separate steps: First, the can28 Proceedings of the 12th International Workshop on Health Text Mining and Information Analysis, pages 28–37 April 19, 2021. ©2021 Association for Computational Linguistics 2.2 (1) Biomedical text typically contains substantial domain-specific jargon and abbreviations. For example, CT could stand for Computed tomography or Copper Toxicosis. (2) The target concepts in the knowledge base often have very similar surface forms, making the disambiguation task difficult. For example, Pseudomona"
2021.louhi-1.4,2020.emnlp-main.522,0,0.0329518,"an be pre-computed and cached. The inference task is thus reduced to finding the maximum dot product between each mention representation and all entity representations. (3) e∈E 3.4 + we |hj q=i Training and Inference |e tˆk = arg max{(um k ) v } σ(ws |hi End-to-End Entity Linking Many of the state-of-the-art entity disambiguation models assume that gold mention spans are available during test time and thus have limited applicability in real-world entity linking tasks, where such gold mentions are typically not available. To avoid this, recent works (Kolitsas et al., 2018; F´evry et al., 2020; Li et al., 2020) have investigated end-to-end entity linking, where a model needs to perform both mention span detection and entity disambiguation. 4 Mention Span Detection We experiment with two different methods for mention span detection with different computational complexity. In our first method, following F´evry et al. (2020), we use a simple BIO tagging scheme to identify the mention spans. Every token in the input text is annotated with one of these three tags. Under this tagging scheme, any contiguous segment of tokens starting with a B tag and followed by I tags is treated as a mention. Although thi"
2021.louhi-1.4,K19-1049,0,0.19272,"r the candidate entities. A schematic diagram of the model is presented in Figure 1. Following the BERT model, the input sequences to these encoders start and end with the special tokens [CLS] and [SEP], respectively. 3.2 Candidate Selection Candidate Retrieval Since the entity disambiguation task is formulated as a learning to rank problem, we need to retrieve negative candidate entities for ranking during training. To this end, we randomly sample a set of negative candidates from the pool of all entities in the knowledge base. Additionally, we adopt the hard negative mining strategy used by Gillick et al. (2019) to retrieve negative candidates by performing nearest neighbor search using the dense representations of mentions and Mention Encoder Given an input text document [xd1 , . . . , xdT ] of T tokens with M mentions, the output of the final layer of the encoder, denoted by [h1 , . . . , hT ], is a contextualized representation of the input tokens. For each mention span (i, j), we concatenate the first and the last tokens of the span and pass it through a linear layer to obtain the 30 candidates described above. The hard negative candidates are the entities that are more similar to the mention tha"
2021.louhi-1.4,P19-1335,0,0.451836,"entity disambiguation and out-performs two recently proposed models. 1 Gerard de Melo Hasso Plattner Institute University of Potsdam Potsdam, Germany gdm@demelo.org Introduction Entity linking is the task of identifying mentions of named entities (or other terms) in a text document and disambiguating them by mapping them to canonical entities (or concepts) listed in a reference knowledge graph (Hogan et al., 2020). This is an essential step in information extraction, and therefore has been studied extensively both in domainspecific and domain-agnostic settings. Recent stateof-the-art models (Logeswaran et al., 2019; Wu et al., 2019) attempt to learn better representations of mentions and candidates using the rich contextual information encoded in pre-trained language models such as BERT (Devlin et al., 2019). These models follow a retrieve and rerank paradigm, which consists of two separate steps: First, the can28 Proceedings of the 12th International Workshop on Health Text Mining and Information Analysis, pages 28–37 April 19, 2021. ©2021 Association for Computational Linguistics 2.2 (1) Biomedical text typically contains substantial domain-specific jargon and abbreviations. For example, CT could stan"
2021.louhi-1.4,W19-5034,0,0.0875379,"rs that perform per-mention entity disambiguation. At inference time, our model is 3-25x faster than other comparable models. • At the same time, our model obtains favorable results on two biomedical datasets compared to state-of-the-art entity linking models. • Our model can also perform end-to-end entity linking when trained with the multi-task objective of mention span detection and entity disambiguation. We show that without using any semantic type information, our model significantly out-performs two recent biomedical entity linking models – MedType (Vashishth et al., 2020) and SciSpacy (Neumann et al., 2019) – on two benchmark datasets. 2 2.1 Biomedical Entity Linking 2.3 End-to-End Entity Linking End-to-end entity linking refers to the task of predicting mention spans and the corresponding target entities jointly using a single model. Traditionally, span detection and entity disambiguation tasks were done in a pipelined approach, making these approaches susceptible to error propagation. To alleviate this issue, Kolitsas et al. (2018) proposed a neural end-to-end model that performs the dual tasks of mention span detection and entity disambiguation. However, for span detection and disambiguation,"
2021.louhi-1.4,2020.acl-main.748,0,0.239295,"the density of mentions per document make the biomedical entity linking very challenging. In the biomedical domain, there are many existing tools, such as TaggerOne (Leaman and Lu, 2016), MetaMap (Aronson, 2006), cTAKES (Savova et al., 2010), QuickUMLS (Soldaini and Goharian, 2016), among others, for normalizing mentions of biomedical concepts to a biomedical thesaurus. Most of these methods rely on feature-based approaches. Recently, Zhu et al. (2019) proposed a model that utilizes the latent semantic information of mentions and entities to perform entity linking. Other recent models such as Xu et al. (2020) and Vashishth et al. (2020) also leverage semantic type information for improved entity disambiguation. Our work is different from these approaches, as our model does not use semantic type information, since such information may not always be available. Recent studies such as Xu et al. (2020) and Ji et al. (2020) deploy a BERT-based retrieve and re-rank model. In contrast, our model does not rely on a separate re-ranker model, which significantly improves its efficiency. Contributions The key contributions of our work are as follows. • Training our collective entity disambiguation model is 3x"
2021.naacl-main.86,N19-1423,0,0.0379176,"recent works to propose various architectures in search of a sweet spot (Humeau et al., 2020; Luan et al., 2020), but they are developed in isolation of one another and difficult to compare. In this section, we give a general algebraic form of the score function that subsumes many of the existing works as special cases. 4.1 General Form We focus on the standard setting in NLP in which 0 x ∈ V T and y ∈ V T are sequences of tokens in a vocabulary V. Let E(x) ∈ RH×T and F (y) ∈ 0 RH×T denote their encodings, typically obtained from the final layers of separate pretrained transformers like BERT (Devlin et al., 2019). We follow the convention popularized by BERT and assume the first token is a special symbol (i.e., [CLS]), so that E1 (x) and F1 (y) represent single-vector summaries of x and y. We have the following design choices: • Direction: If x → y, define the query Q = E(x) and key K = F (y). If y → x, define the query Q = F (y) and key K = E(x). • Reduction: Given integers m, m0 , reduce the number of columns in Q and K to obtain Qm ∈ 0 RH×m and Km0 ∈ RH×m . We can simply select leftmost columns, or introduce an additional layer to perform the reduction. • Attention: Choose a column-wise attention s"
2021.naacl-main.86,D17-1277,0,0.0250698,"Missing"
2021.naacl-main.86,K19-1049,0,0.0697125,"l form of hard-negative NCE with a realistic loss (5) using a the score function that unifies various architecgeneral conditional negative distribution, and view tures used in text retrieval. By combining hard negatives with appropriate score functions, we it as a biased estimator of the gradient of the crossobtain strong results on the challenging task of entropy loss. We give a simple analysis of the zero-shot entity linking. bias (Theorem 3.1). We then consider setting the negative distribution to be the model distribution, 1 Introduction which recovers the hard negative mining strategy of Gillick et al. (2019), and show that it yields an unbiNoise contrastive estimation (NCE) is a widely used approach to large-scale classification and re- ased gradient estimator when the model is optimal trieval. It estimates a score function of input- (Theorem 3.2). We complement the gradient-based perspective with an adversarial formulation (Theolabel pairs by a sampled softmax objective: given rem 3.3). a correct pair (x, y1 ), choose negative examples The choice of architecture to parametrize the y2 . . . yK and maximize the probability of (x, y1 ) score function is another key element in NCE. in a softmax over"
2021.naacl-main.86,D17-1284,0,0.0432281,"Missing"
2021.naacl-main.86,D11-1072,0,0.0404566,"connot explain the benefit of hard negatives in NCE vergence. However, their argument does not imply (Gillick et al., 2019; Wu et al., 2020; Karpukhin our theorems. They also assume a pairwise loss, et al., 2020; Févry et al., 2020). In contrast, we excluding non-pairwise losses such as (4). 1094 6 Experiments 6.2 We now study empirical aspects of the hardnegative NCE (Section 3) and the spectrum of score functions (Section 4). Our main testbed is Zeshel (Logeswaran et al., 2019), a challenging dataset for zero-shot entity linking. We also present complementary experiments on AIDA CoNLL-YAGO (Hoffart et al., 2011).2 6.1 Task Zeshel contains 16 domains (fictional worlds like Star Wars) partitioned to 8 training and 4 validation and test domains. Each domain has tens of thousands of entities along with their textual descriptions, which contain references to other entities in the domain and double as labeled mentions. The input x is a contextual mention and the label y is the description of the referenced entity. A score function sθ (x, y) is learned in the training domains and applied to a new domain for classification and retrieval. Thus the model must read descriptions of unseen entities and still make"
2021.naacl-main.86,2020.emnlp-main.550,0,0.029557,"Missing"
2021.naacl-main.86,P19-1612,0,0.0243283,"s the parameters of the encoders E, F and the optional reduction layer. 4.2 Examples Along with the choice of negatives, the choice of the score function sθ : X ×Y → R is a critical com- Dual encoder. Choose either direction x → y or ponent of NCE in practice. There is a clear trade- y → x. Select the leftmost m = m0 = 1 vectors in off between performance and efficiency in model- Q and K as the query and key. The choice of attening the cross interaction between the input-label tion has no effect. This recovers the standard dual 1093 encoder used in many retrieval problems (Gupta et al., 2017; Lee et al., 2019; Logeswaran et al., 2019; Wu et al., 2020; Karpukhin et al., 2020; Guu et al., 2020): sθ (x, y) = E1 (x)> F1 (y). Poly-encoder. Choose the direction y → x. Select the leftmost m = 1 vector in F (y) as the query. Choose an integer m0 and compute Km0 = 0 E(x)Soft(E(x)> O) where O ∈ RH×m is a learnable parameter (“code” embeddings). Choose soft attention. This recovers the poly-encoder (Humeau et al., 2020): sθ (x, y) = F1 (y)> Cm0 (x, y) where > F (y) . Similar archiCm0 (x, y) = Km0 Soft Km 0 1 tectures without length reduction have been used in previous works, for instance the neural attentio"
2021.naacl-main.86,P19-1335,0,0.114272,"f the encoders E, F and the optional reduction layer. 4.2 Examples Along with the choice of negatives, the choice of the score function sθ : X ×Y → R is a critical com- Dual encoder. Choose either direction x → y or ponent of NCE in practice. There is a clear trade- y → x. Select the leftmost m = m0 = 1 vectors in off between performance and efficiency in model- Q and K as the query and key. The choice of attening the cross interaction between the input-label tion has no effect. This recovers the standard dual 1093 encoder used in many retrieval problems (Gupta et al., 2017; Lee et al., 2019; Logeswaran et al., 2019; Wu et al., 2020; Karpukhin et al., 2020; Guu et al., 2020): sθ (x, y) = E1 (x)> F1 (y). Poly-encoder. Choose the direction y → x. Select the leftmost m = 1 vector in F (y) as the query. Choose an integer m0 and compute Km0 = 0 E(x)Soft(E(x)> O) where O ∈ RH×m is a learnable parameter (“code” embeddings). Choose soft attention. This recovers the poly-encoder (Humeau et al., 2020): sθ (x, y) = F1 (y)> Cm0 (x, y) where > F (y) . Similar archiCm0 (x, y) = Km0 Soft Km 0 1 tectures without length reduction have been used in previous works, for instance the neural attention model of Ganea and Hofm"
2021.naacl-main.86,D18-1405,0,0.141565,"labels y1:K = (y1 . . . yK ) ∈ Y K , define exp (sθ (x, yk )) πθ (k|x, y1:K ) = PK k0 =1 exp (sθ (x, yk0 )) (3) for all 1 ≤ k ≤ K. When K  |Y|, (3) is significantly cheaper to calculate than (1). Given K ≥ 2, we define JNCE (θ) = E (x,y1 )∼pop y2:K ∼q K−1 [− log πθ (1|x, y1:K )] (4) where y2:K ∈ Y K−1 are negative examples drawn iid from some “noise” distribution q over Y. Popular choices of q include the uniform distribution q(y) = 1/ |Y |and the population marginal q(y) = pop(y). The NCE loss (4) has been studied extensively. An optimal classifier can be extracted from a minimizer of JNCE (Ma and Collins, 2018); minimizing JNCE can be seen as maximizing a lower bound on the mutual information between (x, y) ∼ pop if q is the population marginal (Oord et al., 2018). We refer to Stratos (2019) for an overview. However, most of these results focus on unconditional negative examples and do not address hard negatives, which are clearly conditional. We now focus on conditional negative distributions, which are more suitable for describing hard negatives. 3 Hard Negatives in NCE Given K ≥ 2, we define JHARD (θ) = E (x,y1 )∼pop y2:K ∼h(·|x,y1 ) [− log πθ (1|x, y1:K )] (5) where y2:K ∈ Y K−1 are negative exa"
2021.naacl-main.86,2020.emnlp-main.519,0,0.12964,"he optional reduction layer. 4.2 Examples Along with the choice of negatives, the choice of the score function sθ : X ×Y → R is a critical com- Dual encoder. Choose either direction x → y or ponent of NCE in practice. There is a clear trade- y → x. Select the leftmost m = m0 = 1 vectors in off between performance and efficiency in model- Q and K as the query and key. The choice of attening the cross interaction between the input-label tion has no effect. This recovers the standard dual 1093 encoder used in many retrieval problems (Gupta et al., 2017; Lee et al., 2019; Logeswaran et al., 2019; Wu et al., 2020; Karpukhin et al., 2020; Guu et al., 2020): sθ (x, y) = E1 (x)> F1 (y). Poly-encoder. Choose the direction y → x. Select the leftmost m = 1 vector in F (y) as the query. Choose an integer m0 and compute Km0 = 0 E(x)Soft(E(x)> O) where O ∈ RH×m is a learnable parameter (“code” embeddings). Choose soft attention. This recovers the poly-encoder (Humeau et al., 2020): sθ (x, y) = F1 (y)> Cm0 (x, y) where > F (y) . Similar archiCm0 (x, y) = Km0 Soft Km 0 1 tectures without length reduction have been used in previous works, for instance the neural attention model of Ganea and Hofmann (2017). direc"
C16-1038,J92-4003,0,0.527356,"27 97.27 92.30 Union 89.24 82.09 81.79 89.91 58.74 84.54 77.76 79.21 79.87 63.21 89.48 85.71 87.46 84.25 74.45 79.22 91.27 81.07 Daum´e III (2009) 97.36 94.4 96.6 92.61 89.34 92.76 92.3 94.43 89.95 90.86 96.44 95.78 94.13 94.02 94.43 94.75 98.44 94.04 Table 2: F1 scores across seventeen personal assistant domains for Noadapt, Union and Daum´e III (2009) in a setting with sparse binary-valued features. 6.2 Results For non-neural models, we use conditional random fields (CRFs) (Lafferty et al., 2001) with a rich set of binary-valued features such as lexical features, gazetteers, Brown clusters (Brown et al., 1992) and context words. For parameter estimation, we used L-BFGS (Liu and Nocedal, 1989) with 100 as the maximum iteration count and 1.0 for the L2 regularization parameter. Their results are shown in Table 2. A few observations: the model without domain adaptation (Noadapt) is already very competitive because we have sufficient training data. However, simply training a single model with aggregated queries across all domains significantly degrades performance (Union). This is because in many cases the same query is labeled differently depending on the domain and the 393 context. This is also becau"
C16-1038,J81-4005,0,0.696695,"Missing"
C16-1038,P15-1033,0,0.00953814,"tially defines a feedforward network in which the same set of parameters are used multiple times and is well-suited for sequential data. But a simple RNN suffers from the vanishing gradient problem and does not model long-term dependency very well (Pascanu et al., 2013). An LSTM addresses this issue by introducing a memory cell in RNN architecture that can control how much past information to retain or to forget. This modification has been shown to be crucial in practice. Many recent works in NLP have achieved state-of-the-art results using variants of LSTM, for example in dependency parsing (Dyer et al., 2015) and machine translation (Bahdanau et al., 2014). 389 4 A Neural Extension of the Feature Augmentation Method We now describe a neural extension of the feature augmentation method for domain adaptation (Daum´e III, 2009). Our model consists of K + 1 LSTMs: one LSTM θ is used on all domains, and the remaining K LSTMs θ1 . . . θK are used only for the corresponding domains. More specifically, we predict one of L labels at the t-th time step in a given user query in domain k ∈ {1 . . . K} as follows. The common LSTM θ produces an output vector ht ∈ Rd and the domain-specific LSTM θk produces an o"
C16-1038,N15-1009,1,0.716868,"d across all domains and independent LSTMs used within individual domains, and then combining their outputs in the top layer. Second, we propose using the framework for learning predictive structures by Ando and Zhang (2005) for domain adaptation which has not previously been considered for this task (the original work only considers multi-tasking in the context of semi-supervised learning): we likewise consider a neural extension of this framework. We perform slot tagging experiments on 17 different personal digital assistant domains that Cortana handles (Tur, 2006; Anastasakos et al., 2014; Kim et al., 2015a; Kim et al., 2015c; Kim et al., 2015b; Kim et al., 2016a; Kim et al., 2016b). Our methods give clear performance improvement over naive baselines such as training K independent models on individual domains or training one model on the union of all domains. Our methods also significantly outperform the feature augmentation method of Daum´e III (2009) with standard sparse binary features implemented with conditional random fields (CRFs). The rest of the paper is organized as follows. In Section 2, we discuss related works. In Section 3, we provide background information on domain adaptation an"
C16-1038,P15-2132,1,0.899567,"d across all domains and independent LSTMs used within individual domains, and then combining their outputs in the top layer. Second, we propose using the framework for learning predictive structures by Ando and Zhang (2005) for domain adaptation which has not previously been considered for this task (the original work only considers multi-tasking in the context of semi-supervised learning): we likewise consider a neural extension of this framework. We perform slot tagging experiments on 17 different personal digital assistant domains that Cortana handles (Tur, 2006; Anastasakos et al., 2014; Kim et al., 2015a; Kim et al., 2015c; Kim et al., 2015b; Kim et al., 2016a; Kim et al., 2016b). Our methods give clear performance improvement over naive baselines such as training K independent models on individual domains or training one model on the union of all domains. Our methods also significantly outperform the feature augmentation method of Daum´e III (2009) with standard sparse binary features implemented with conditional random fields (CRFs). The rest of the paper is organized as follows. In Section 2, we discuss related works. In Section 3, we provide background information on domain adaptation an"
C16-1038,P15-2032,1,0.908968,"d across all domains and independent LSTMs used within individual domains, and then combining their outputs in the top layer. Second, we propose using the framework for learning predictive structures by Ando and Zhang (2005) for domain adaptation which has not previously been considered for this task (the original work only considers multi-tasking in the context of semi-supervised learning): we likewise consider a neural extension of this framework. We perform slot tagging experiments on 17 different personal digital assistant domains that Cortana handles (Tur, 2006; Anastasakos et al., 2014; Kim et al., 2015a; Kim et al., 2015c; Kim et al., 2015b; Kim et al., 2016a; Kim et al., 2016b). Our methods give clear performance improvement over naive baselines such as training K independent models on individual domains or training one model on the union of all domains. Our methods also significantly outperform the feature augmentation method of Daum´e III (2009) with standard sparse binary features implemented with conditional random fields (CRFs). The rest of the paper is organized as follows. In Section 2, we discuss related works. In Section 3, we provide background information on domain adaptation an"
C16-1038,D16-1222,1,0.815342,"dividual domains, and then combining their outputs in the top layer. Second, we propose using the framework for learning predictive structures by Ando and Zhang (2005) for domain adaptation which has not previously been considered for this task (the original work only considers multi-tasking in the context of semi-supervised learning): we likewise consider a neural extension of this framework. We perform slot tagging experiments on 17 different personal digital assistant domains that Cortana handles (Tur, 2006; Anastasakos et al., 2014; Kim et al., 2015a; Kim et al., 2015c; Kim et al., 2015b; Kim et al., 2016a; Kim et al., 2016b). Our methods give clear performance improvement over naive baselines such as training K independent models on individual domains or training one model on the union of all domains. Our methods also significantly outperform the feature augmentation method of Daum´e III (2009) with standard sparse binary features implemented with conditional random fields (CRFs). The rest of the paper is organized as follows. In Section 2, we discuss related works. In Section 3, we provide background information on domain adaptation and sequence modeling. First, we review the feature augment"
C16-1038,P16-2002,1,0.842343,"dividual domains, and then combining their outputs in the top layer. Second, we propose using the framework for learning predictive structures by Ando and Zhang (2005) for domain adaptation which has not previously been considered for this task (the original work only considers multi-tasking in the context of semi-supervised learning): we likewise consider a neural extension of this framework. We perform slot tagging experiments on 17 different personal digital assistant domains that Cortana handles (Tur, 2006; Anastasakos et al., 2014; Kim et al., 2015a; Kim et al., 2015c; Kim et al., 2015b; Kim et al., 2016a; Kim et al., 2016b). Our methods give clear performance improvement over naive baselines such as training K independent models on individual domains or training one model on the union of all domains. Our methods also significantly outperform the feature augmentation method of Daum´e III (2009) with standard sparse binary features implemented with conditional random fields (CRFs). The rest of the paper is organized as follows. In Section 2, we discuss related works. In Section 3, we provide background information on domain adaptation and sequence modeling. First, we review the feature augment"
C16-1038,D14-1162,0,0.106812,"cument Edit and create note Set up a phone Order food using app Find location and direction Remind appointment and to-do list Make a restaurant reservations Find and book a cab Ask weather Table 1: Data sets used in the experiments. For each domain, the number of unique labels, the number of quires in the training, development, and test sets, input vocabulary size of the training set, and short description about domain. the development set in preliminary experiments. To initialize word embedding, we used word embedding trained on 6 billion tokens (6B-200d) from Wikipedia 2014 plus Gigaword 5 (Pennington et al., 2014). These configurations were selected by observing the models performance on held-out development set. We compare the following methods for the slot tagging tasks: • NoAdapt: train a feature-rich CRF only on target training data. • Union: train a feature-rich CRF on the union of source and target training data. • Daume: train a feature-rich CRF with the discrete feature duplication method of Daum´e III (2009). • 1D&E: train a domain specific LSTM with a generic embedding on all domain training data, shown on the right in Figure 2. • 1D&L: train a domain specific LSTM with a generic LSTM on all"
C16-1193,W06-1615,0,0.254209,"n attempt to improve performance on any particular domain. There is a rich body of work in domain adaptation for natural language processing. A notable example is the feature augmentation method of Daum´e III (2009), who propose partitioning the model parameters to those that handle common patterns and those that handle domain-specific patterns. This way, the model is forced to learn from all domains yet preserve domain-specific knowledge. Another domain adaptation technique used in natural language processing utilizes unlabeled data in source and target distributions to find shared patterns (Blitzer et al., 2006; Blitzer et al., 2011). This is achieved by finding a shared subspace between the two domains through singular value decomposition (SVD). Unlike the feature augmentation method of Daum´e III (2009), however, it does not leverage labeled data in the target domain. This work is rather different from the conventional works in domain adaptation in that we remove the need to distinguish domains: we have a single model that can handle arbitrary (including unknown) domains. Among other benefits, this approach removes the error propagation due to domain misclassification. Most domain adaptation metho"
C16-1193,J92-4003,0,0.551064,"umbers of training, test and development queries across domains are 2964K, 217K and 153K, respectively. Note that we keep domain-specific slots such as alarm state, but there are enough shared labels across domains. To be specific, we have shared 62 labels among 131 labels. In ALARM domain, there are 6 shared slots among 8 slots. 4.2 Results In all our experiments, we follow same setting as in (Kim et al., 2015b; Kim et al., 2015c). We trained Conditional Random Fields (CRFs)(Lafferty et al., 2001) and used n-gram features up to n = 3, regular expression, lexicon features, and Brown Clusters (Brown et al., 1992). With these features, we compare the following methods for slot tagging1 : • In-domain: Train a domain-specific model using the domain-specific data covering the slots supported in that domain. • Binary: Train a binary classifier for each slot type, assuming prediction for each slot type is independent of one another. Then combine the classification result with the slots needed for a given schema. For each binary slot tagger targeting a specific slot type, the labeled data is programatically 1 For parameter estimation, we used L-BFGS (Liu and Nocedal, 1989) with 100 as the maximum number of i"
C16-1193,P15-1033,0,0.0374903,"Missing"
C16-1193,N15-1009,1,0.690831,"f slot sequences allowed under the schema. The proposed model achieves competitive and often superior performance over the conventional model trained separately per domain. 1 Introduction Recently, there has been much investment on the personal digital assistant (PDA) technology in industry (Sarikaya, 2015). Apple’s Siri, Google Now, Microsoft’s Cortana, and Amazon’s Alexa are some examples of personal digital assistants. Spoken language understanding is an important component of these examples that allows natural communication between the user and the agent (Tur, 2006; El-Kahky et al., 2014; Kim et al., 2015a; Kim et al., 2016b). PDAs support a number of scenarios including creating reminders, setting up alarms, note taking, scheduling meetings, finding and consuming entertainment (i.e. movie, music, games), finding places of interest and getting driving directions to them. The number of domains supported by these systems constantly increases, and whether there is a method that allows us to easily scale to a larger number of domains is an unsolved problem (Kim et al., 2015d; Kim et al., 2016a). The main reason behind the need for additional domains is that we require a new set of schema (i.e., qu"
C16-1193,P15-2132,1,0.872887,"f slot sequences allowed under the schema. The proposed model achieves competitive and often superior performance over the conventional model trained separately per domain. 1 Introduction Recently, there has been much investment on the personal digital assistant (PDA) technology in industry (Sarikaya, 2015). Apple’s Siri, Google Now, Microsoft’s Cortana, and Amazon’s Alexa are some examples of personal digital assistants. Spoken language understanding is an important component of these examples that allows natural communication between the user and the agent (Tur, 2006; El-Kahky et al., 2014; Kim et al., 2015a; Kim et al., 2016b). PDAs support a number of scenarios including creating reminders, setting up alarms, note taking, scheduling meetings, finding and consuming entertainment (i.e. movie, music, games), finding places of interest and getting driving directions to them. The number of domains supported by these systems constantly increases, and whether there is a method that allows us to easily scale to a larger number of domains is an unsolved problem (Kim et al., 2015d; Kim et al., 2016a). The main reason behind the need for additional domains is that we require a new set of schema (i.e., qu"
C16-1193,P15-2032,1,0.830449,"f slot sequences allowed under the schema. The proposed model achieves competitive and often superior performance over the conventional model trained separately per domain. 1 Introduction Recently, there has been much investment on the personal digital assistant (PDA) technology in industry (Sarikaya, 2015). Apple’s Siri, Google Now, Microsoft’s Cortana, and Amazon’s Alexa are some examples of personal digital assistants. Spoken language understanding is an important component of these examples that allows natural communication between the user and the agent (Tur, 2006; El-Kahky et al., 2014; Kim et al., 2015a; Kim et al., 2016b). PDAs support a number of scenarios including creating reminders, setting up alarms, note taking, scheduling meetings, finding and consuming entertainment (i.e. movie, music, games), finding places of interest and getting driving directions to them. The number of domains supported by these systems constantly increases, and whether there is a method that allows us to easily scale to a larger number of domains is an unsolved problem (Kim et al., 2015d; Kim et al., 2016a). The main reason behind the need for additional domains is that we require a new set of schema (i.e., qu"
C16-1193,D16-1222,1,0.817385,"lowed under the schema. The proposed model achieves competitive and often superior performance over the conventional model trained separately per domain. 1 Introduction Recently, there has been much investment on the personal digital assistant (PDA) technology in industry (Sarikaya, 2015). Apple’s Siri, Google Now, Microsoft’s Cortana, and Amazon’s Alexa are some examples of personal digital assistants. Spoken language understanding is an important component of these examples that allows natural communication between the user and the agent (Tur, 2006; El-Kahky et al., 2014; Kim et al., 2015a; Kim et al., 2016b). PDAs support a number of scenarios including creating reminders, setting up alarms, note taking, scheduling meetings, finding and consuming entertainment (i.e. movie, music, games), finding places of interest and getting driving directions to them. The number of domains supported by these systems constantly increases, and whether there is a method that allows us to easily scale to a larger number of domains is an unsolved problem (Kim et al., 2015d; Kim et al., 2016a). The main reason behind the need for additional domains is that we require a new set of schema (i.e., query topics), compos"
C16-1193,P16-2002,1,0.807908,"lowed under the schema. The proposed model achieves competitive and often superior performance over the conventional model trained separately per domain. 1 Introduction Recently, there has been much investment on the personal digital assistant (PDA) technology in industry (Sarikaya, 2015). Apple’s Siri, Google Now, Microsoft’s Cortana, and Amazon’s Alexa are some examples of personal digital assistants. Spoken language understanding is an important component of these examples that allows natural communication between the user and the agent (Tur, 2006; El-Kahky et al., 2014; Kim et al., 2015a; Kim et al., 2016b). PDAs support a number of scenarios including creating reminders, setting up alarms, note taking, scheduling meetings, finding and consuming entertainment (i.e. movie, music, games), finding places of interest and getting driving directions to them. The number of domains supported by these systems constantly increases, and whether there is a method that allows us to easily scale to a larger number of domains is an unsolved problem (Kim et al., 2015d; Kim et al., 2016a). The main reason behind the need for additional domains is that we require a new set of schema (i.e., query topics), compos"
C16-1193,D14-1162,0,0.0813171,"existing slots and intents. In cases where the new domain needs an a new intent or slot, the underlying generic models have to updated with the updated schema. 2052 3.1 Schema Prediction The first stage produces a set of label types that serve as constraints in the second stage. To this end, we use Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) (Figure 1). The LSTM processes the given query to produce a fixed-size vector where the input at each time step is the word embedding corresponding to the word used at the time. We initialize these word embeddings with GloVe vectors (Pennington et al., 2014). Then the network maps the query vector to a distribution over schema types. In more detail, we first map each word of the utterance into d-dimensional vector space using an embedding matrix of size V by d (which is trained along with other parameters in the network), where V is the size of the vocabulary. Then we map the sequence of the word vectors, {x1 , . . . xT }, to LSTM outputs {h1 , . . . hT } where we take the last output to be a d-dimensional summary vector of the utterance s = hT . We then use parameters W ∈ Rk×d and b ∈ Rk where k is the number of slot types and compute yˆ = softm"
C16-1193,Q13-1001,0,0.069055,"Missing"
D17-1075,D15-1041,0,0.125871,"posed of two characters, ᄀ ᆻ, ᄃ ᅡ ᅡ ∈ C. Each character is furthermore composed of three jamo letters as follows: • We release an implementation of our jamo architecture which can be plugged in any Korean processing network.1 2 Method Related Work We make a few additional remarks on related work to better situate our work. Our work follows the successful line of work on incorporating sub-lexical information to neural models. Various character-based architectures have been proposed. For instance, Ma and Hovy (2016) and Kim et al. (2016) use CNNs over characters whereas Lample et al. (2016) and Ballesteros et al. (2015) use bidirectional LSTMs (BiLSTMs). Both approaches have been shown to be profitable; we employ a BiLSTM-based approach. Many previous works have also considered morphemes to augment lexical models (Luong et al., 2013; Botha and Blunsom, 2014; Cotterell et al., 2016). Sub-character models are substantially rarer; an extreme case is considered by • ᅡ ᄀ ∈ C is composed of ㄱ ∈ Jh , ㅏ ∈ Jv , ᆻ and ㅆ ∈ Jt . • ᅡ ᄃ ∈ C is composed of ㄷ ∈ Jh , ㅏ ∈ Jv , and an empty letter ∅ ∈ Jt . The tail consonant can be empty; we assume a special symbol ∅ ∈ Jt to denote an empty letter. 1 https://github.com/karlstr"
D17-1075,P16-1156,0,0.060129,"Missing"
D17-1075,P15-1033,0,0.0149319,"oss-lingual parser originally reported in McDonald et al. (2013). • Yara: A beam-search transition-based parser of Rasooli and Tetreault (2015) based on the rich non-local features in Zhang and Nivre (2011). We use beam width 64. We use 5-fold jackknifing on the training portion to provide POS tag features. We also report on using gold POS tags. • K&G16: The basic BiLSTM parser of Kiperwasser and Goldberg (2016) without the sublexical architecture introduced in this work. • Stack LSTM: A greedy transition-based parser based on stack LSTM representations. Dyer15 denotes the word-level variant (Dyer et al., 2015). Ballesteros15 denotes the character-level variant (Ballesteros et al., 2015). Experiments For pre-trained word embeddings, we apply the spectral algorithm of Stratos et al. (2015) on a 2015 Korean Wikipedia dump to induce 285,933 embeddings of dimension 100. Data We use the publicly available Korean treebank in the universal treebank version 2.0 (McDonald et al., 2013).2 The dataset comes with a train/development/test split; data statistics are shown in Table 1. Since the test portion is significantly smaller than the dev portion, we report performance on both. As expected, we observe severe"
D17-1075,N16-1155,0,0.119198,"Missing"
D17-1075,P15-1124,1,0.855754,"Missing"
D17-1075,Q16-1023,0,0.26582,"atos/ koreannet 722 and induce a representation of w as Figure 1 illustrates the decomposition of a Korean sentence down to jamo letters. Note that the number of possible characters is combinatorial in the number of jamo letters, loosely upper bounded by 513 = 132, 651. This upper bound is loose because certain combinations are invalid. For instance, ㅁ ∈ Jh ∩ Jt but ㅁ 6∈ Jv whereas ㅏ ∈ Jv but ㅏ 6∈ Jh ∪ Jt . The combinatorial nature of Korean characters motivates the compositional architecture below. For completeness, we describe the entire forward pass of the transition-based BiLSTM parser of Kiperwasser and Goldberg (2016) that we use in our experiments. 3.2 h = tanh U • Feedforward for predicting transitions Given a sentence w1 . . . wn ∈ W, the final d∗ dimensional word representations are given by • U J , V J , W J ∈ Rd×d and bJ ∈ Rd  w   w  h 1 h n (z1 . . . zn ) = Φ . . . wn w 1 e e Given a Korean character c ∈ C, we perform Unicode decomposition (Section 3.3) to recover the underlying jamo letters ch , cv , ct ∈ J . We compose the letters to induce a representation of c as The parser then uses the feedforward network to greedily predict transitions based on words that are active in the system. The m"
D17-1075,N16-1030,0,0.0224038,"ᆻᄃ ᅡ ᅡ (went). It is composed of two characters, ᄀ ᆻ, ᄃ ᅡ ᅡ ∈ C. Each character is furthermore composed of three jamo letters as follows: • We release an implementation of our jamo architecture which can be plugged in any Korean processing network.1 2 Method Related Work We make a few additional remarks on related work to better situate our work. Our work follows the successful line of work on incorporating sub-lexical information to neural models. Various character-based architectures have been proposed. For instance, Ma and Hovy (2016) and Kim et al. (2016) use CNNs over characters whereas Lample et al. (2016) and Ballesteros et al. (2015) use bidirectional LSTMs (BiLSTMs). Both approaches have been shown to be profitable; we employ a BiLSTM-based approach. Many previous works have also considered morphemes to augment lexical models (Luong et al., 2013; Botha and Blunsom, 2014; Cotterell et al., 2016). Sub-character models are substantially rarer; an extreme case is considered by • ᅡ ᄀ ∈ C is composed of ㄱ ∈ Jh , ㅏ ∈ Jv , ᆻ and ㅆ ∈ Jt . • ᅡ ᄃ ∈ C is composed of ㄷ ∈ Jh , ㅏ ∈ Jv , and an empty letter ∅ ∈ Jt . The tail consonant can be empty; we assume a special symbol ∅ ∈ Jt to denote an empty letter"
D17-1075,D16-1100,0,0.118833,"Missing"
D17-1075,W13-3512,0,0.0178808,"1 2 Method Related Work We make a few additional remarks on related work to better situate our work. Our work follows the successful line of work on incorporating sub-lexical information to neural models. Various character-based architectures have been proposed. For instance, Ma and Hovy (2016) and Kim et al. (2016) use CNNs over characters whereas Lample et al. (2016) and Ballesteros et al. (2015) use bidirectional LSTMs (BiLSTMs). Both approaches have been shown to be profitable; we employ a BiLSTM-based approach. Many previous works have also considered morphemes to augment lexical models (Luong et al., 2013; Botha and Blunsom, 2014; Cotterell et al., 2016). Sub-character models are substantially rarer; an extreme case is considered by • ᅡ ᄀ ∈ C is composed of ㄱ ∈ Jh , ㅏ ∈ Jv , ᆻ and ㅆ ∈ Jt . • ᅡ ᄃ ∈ C is composed of ㄷ ∈ Jh , ㅏ ∈ Jv , and an empty letter ∅ ∈ Jt . The tail consonant can be empty; we assume a special symbol ∅ ∈ Jt to denote an empty letter. 1 https://github.com/karlstratos/ koreannet 722 and induce a representation of w as Figure 1 illustrates the decomposition of a Korean sentence down to jamo letters. Note that the number of possible characters is combinatorial in the number of j"
D17-1075,P11-2033,0,0.0189484,"the head consonant ㄱ is associated with the sound g, the vowel ㅏ with o, and the tail consonant ㅆ with t. This is clearly critical for speech recognition/synthesis and indeed has been investigated in the speech community (Lee et al., 1994; Sakti et al., 2010). While speech processing is not our focus, the phonetic signals can capture useful lexical correlation (e.g., for onomatopoeic words). 4 • McDonald13: A cross-lingual parser originally reported in McDonald et al. (2013). • Yara: A beam-search transition-based parser of Rasooli and Tetreault (2015) based on the rich non-local features in Zhang and Nivre (2011). We use beam width 64. We use 5-fold jackknifing on the training portion to provide POS tag features. We also report on using gold POS tags. • K&G16: The basic BiLSTM parser of Kiperwasser and Goldberg (2016) without the sublexical architecture introduced in this work. • Stack LSTM: A greedy transition-based parser based on stack LSTM representations. Dyer15 denotes the word-level variant (Dyer et al., 2015). Ballesteros15 denotes the character-level variant (Ballesteros et al., 2015). Experiments For pre-trained word embeddings, we apply the spectral algorithm of Stratos et al. (2015) on a 2"
D17-1075,P16-1101,0,0.0262076,"posed to yield a valid character c ∈ C. As an example, consider the word ᄀ ᆻᄃ ᅡ ᅡ (went). It is composed of two characters, ᄀ ᆻ, ᄃ ᅡ ᅡ ∈ C. Each character is furthermore composed of three jamo letters as follows: • We release an implementation of our jamo architecture which can be plugged in any Korean processing network.1 2 Method Related Work We make a few additional remarks on related work to better situate our work. Our work follows the successful line of work on incorporating sub-lexical information to neural models. Various character-based architectures have been proposed. For instance, Ma and Hovy (2016) and Kim et al. (2016) use CNNs over characters whereas Lample et al. (2016) and Ballesteros et al. (2015) use bidirectional LSTMs (BiLSTMs). Both approaches have been shown to be profitable; we employ a BiLSTM-based approach. Many previous works have also considered morphemes to augment lexical models (Luong et al., 2013; Botha and Blunsom, 2014; Cotterell et al., 2016). Sub-character models are substantially rarer; an extreme case is considered by • ᅡ ᄀ ∈ C is composed of ㄱ ∈ Jh , ㅏ ∈ Jv , ᆻ and ㅆ ∈ Jt . • ᅡ ᄃ ∈ C is composed of ㄷ ∈ Jh , ㅏ ∈ Jv , and an empty letter ∅ ∈ Jt . The tail consona"
D17-1075,P13-2017,0,0.0325879,"Missing"
D19-1040,D15-1075,0,0.0521416,"e author was at Toyota Technological Institute at Chicago. 1 Data processing and evaluation scripts are available at https://github.com/ZeweiChu/EntEval † 421 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 421–433, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work checking (Vlachos and Riedel, 2014; Wang, 2017; Thorne et al., 2018; Yin and Roth, 2018; Chen et al., 2019b), and commonsense learning (Angeli and Manning, 2014; Bowman et al., 2015; Li et al., 2016; Mihaylov et al., 2018; Zellers et al., 2018; Trinh and Le, 2018; Talmor et al., 2019; Zellers et al., 2019; Sap et al., 2019; Rajani et al., 2019). Another related line of work is to integrate entityrelated knowledge into the training of language models (Logan et al., 2019; Zhang et al., 2019; Sun et al., 2019). EntEval and the training objectives considered in this work are built on previous works that involve reasoning over entities. We give a brief overview of relevant works. Entity linking/disambiguation. Entity linking is a fundamental task in information extraction wit"
D19-1040,P18-1009,0,0.164166,"ddings directly; we find that their context and description encoders perform poorly on EntEval tasks. A closely related concurrent work by (Logeswaran et al., 2019) jointly encodes a mention in context and an entity description from Wikia to perform zero-shot entity linking. In contrast, here we seek to pretrain a general purpose entity representations that can function well either given or not given entity descriptions or mention contexts. Other entity-related tasks involve entity typing (Yaghoobzadeh and Sch¨utze, 2015; Murty et al., 2017; Del Corro et al., 2015; Rabinovich and Klein, 2017; Choi et al., 2018; Onoe and Durrett, 2019; Obeidat et al., 2019) and coreference resolution (Durrett and Klein, 2013; Wiseman et al., 2016; Lee et al., 2017; Webster et al., 2018; Kantor and Globerson, 2019). Contextualized word representations. Contextualized word representations and pretrained language representation models, such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018), are powerful pretrained models that have been shown to be effective for a variety of downstream tasks such as text classification, sentence relation prediction, named entity recognition, and question answering. Recent wo"
D19-1040,N18-1204,0,0.0267646,"es for incorporating the contextual information in hyperlinks into state-of-the-art pretrained language models (Peters et al., 2018a) and show that they improve strong baselines on multiple EntEval tasks.1 1 • The dynamically encoded entity representations show a strong improvement on the entity disambiguation task compared to prior work using static entity embeddings. Introduction Entity representations play a key role in numerous important problems including language modeling (Ji et al., 2017), dialogue generation (He et al., 2017), entity linking (Gupta et al., 2017), and story generation (Clark et al., 2018). One successful line of work on learning entity representations has been learning static embeddings: that is, assign a unique vector to each entity in the training data (Gupta et al., 2017; Yamada et al., 2016, 2017). While these embeddings are useful in many applications, they have the obvious drawback of not accommodating unknown entities. Another limiting factor is the lack of an evaluation benchmark: it is often difficult to know which entity representations are better for which tasks. • BERT-based entity representations require further supervised training to perform well on downstream ta"
D19-1040,L18-1269,0,0.391744,"er et al., 2018; Kantor and Globerson, 2019). Contextualized word representations. Contextualized word representations and pretrained language representation models, such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018), are powerful pretrained models that have been shown to be effective for a variety of downstream tasks such as text classification, sentence relation prediction, named entity recognition, and question answering. Recent work has sought to evaluate the knowledge acquired by such models (Shi et al., 2016; Adi et al., 2017; Belinkov et al., 2017; Conneau et al., 2018; Conneau and Kiela, 2018; Liu et al., 2019a). In this work, we focus on evaluating their capabilities in modeling entities. 3 EntEval We are interested in two approaches: contextualized entity representations (henceforth: CER) and descriptive entity representations (henceforth: DER), both encoding fixed-length vector representations for entities. The contextualized entity representations encodes an entity based on the context it appears regardless of whether the entity is seen before. The motivation behind contextualized entity representations is that we want an entity encoder that does not depend on entries in a kno"
D19-1040,D17-1284,0,0.554755,"n Wikipedia. We identify effective objectives for incorporating the contextual information in hyperlinks into state-of-the-art pretrained language models (Peters et al., 2018a) and show that they improve strong baselines on multiple EntEval tasks.1 1 • The dynamically encoded entity representations show a strong improvement on the entity disambiguation task compared to prior work using static entity embeddings. Introduction Entity representations play a key role in numerous important problems including language modeling (Ji et al., 2017), dialogue generation (He et al., 2017), entity linking (Gupta et al., 2017), and story generation (Clark et al., 2018). One successful line of work on learning entity representations has been learning static embeddings: that is, assign a unique vector to each entity in the training data (Gupta et al., 2017; Yamada et al., 2016, 2017). While these embeddings are useful in many applications, they have the obvious drawback of not accommodating unknown entities. Another limiting factor is the lack of an evaluation benchmark: it is often difficult to know which entity representations are better for which tasks. • BERT-based entity representations require further supervise"
D19-1040,P18-1198,0,0.0855121,"Missing"
D19-1040,P17-1162,0,0.0147654,"ng natural hyperlink annotations in Wikipedia. We identify effective objectives for incorporating the contextual information in hyperlinks into state-of-the-art pretrained language models (Peters et al., 2018a) and show that they improve strong baselines on multiple EntEval tasks.1 1 • The dynamically encoded entity representations show a strong improvement on the entity disambiguation task compared to prior work using static entity embeddings. Introduction Entity representations play a key role in numerous important problems including language modeling (Ji et al., 2017), dialogue generation (He et al., 2017), entity linking (Gupta et al., 2017), and story generation (Clark et al., 2018). One successful line of work on learning entity representations has been learning static embeddings: that is, assign a unique vector to each entity in the training data (Gupta et al., 2017; Yamada et al., 2016, 2017). While these embeddings are useful in many applications, they have the obvious drawback of not accommodating unknown entities. Another limiting factor is the lack of an evaluation benchmark: it is often difficult to know which entity representations are better for which tasks. • BERT-based entity repr"
D19-1040,P13-2006,0,0.0645503,"v et al., 2018; Zellers et al., 2018; Trinh and Le, 2018; Talmor et al., 2019; Zellers et al., 2019; Sap et al., 2019; Rajani et al., 2019). Another related line of work is to integrate entityrelated knowledge into the training of language models (Logan et al., 2019; Zhang et al., 2019; Sun et al., 2019). EntEval and the training objectives considered in this work are built on previous works that involve reasoning over entities. We give a brief overview of relevant works. Entity linking/disambiguation. Entity linking is a fundamental task in information extraction with a wealth of literature (He et al., 2013; Guo and Barbosa, 2014; Ling et al., 2015; Huang et al., 2015; Francis-Landau et al., 2016; Le and Titov, 2018; Martins et al., 2019). The goal of this task is to map a mention in context to the corresponding entity in a database. A natural approach is to learn entity representations that enable this mapping. Recent works focused on learning a fixed embedding for each entity using Wikipedia hyperlinks (Yamada et al., 2016; Ganea and Hofmann, 2017; Le and Titov, 2019). Gupta et al. (2017) additionally train context and description embeddings jointly, but this mainly aims to improve the quality"
D19-1040,D15-1103,0,0.017159,"ther than using the context and description embeddings directly; we find that their context and description encoders perform poorly on EntEval tasks. A closely related concurrent work by (Logeswaran et al., 2019) jointly encodes a mention in context and an entity description from Wikia to perform zero-shot entity linking. In contrast, here we seek to pretrain a general purpose entity representations that can function well either given or not given entity descriptions or mention contexts. Other entity-related tasks involve entity typing (Yaghoobzadeh and Sch¨utze, 2015; Murty et al., 2017; Del Corro et al., 2015; Rabinovich and Klein, 2017; Choi et al., 2018; Onoe and Durrett, 2019; Obeidat et al., 2019) and coreference resolution (Durrett and Klein, 2013; Wiseman et al., 2016; Lee et al., 2017; Webster et al., 2018; Kantor and Globerson, 2019). Contextualized word representations. Contextualized word representations and pretrained language representation models, such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018), are powerful pretrained models that have been shown to be effective for a variety of downstream tasks such as text classification, sentence relation prediction, named entity"
D19-1040,D13-1203,0,0.0254272,"l tasks. A closely related concurrent work by (Logeswaran et al., 2019) jointly encodes a mention in context and an entity description from Wikia to perform zero-shot entity linking. In contrast, here we seek to pretrain a general purpose entity representations that can function well either given or not given entity descriptions or mention contexts. Other entity-related tasks involve entity typing (Yaghoobzadeh and Sch¨utze, 2015; Murty et al., 2017; Del Corro et al., 2015; Rabinovich and Klein, 2017; Choi et al., 2018; Onoe and Durrett, 2019; Obeidat et al., 2019) and coreference resolution (Durrett and Klein, 2013; Wiseman et al., 2016; Lee et al., 2017; Webster et al., 2018; Kantor and Globerson, 2019). Contextualized word representations. Contextualized word representations and pretrained language representation models, such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018), are powerful pretrained models that have been shown to be effective for a variety of downstream tasks such as text classification, sentence relation prediction, named entity recognition, and question answering. Recent work has sought to evaluate the knowledge acquired by such models (Shi et al., 2016; Adi et al., 2017"
D19-1040,D11-1072,0,0.187417,"Missing"
D19-1040,Q14-1037,0,0.0821294,"d representations. Recent work has sought to evaluate the knowledge acquired by pretrained language models (Shi et al., 2016; Adi et al., 2017; Belinkov et al., 2017; Peters et al., 2018b; Conneau et al., 2018; Conneau and Kiela, 2018; Wang et al., 2018; Liu et al., 2019a; Chen et al., 2019a, inter alia). In this work, we focus on evaluating their capabilities in modeling entities. Part of EntEval involves evaluating world knowledge about entities, relating them to fact 422 Logic was established as a discipline by Aristotle, who established its fundamental place in philosophy. efit the model (Durrett and Klein, 2014). Unlike other tasks, coreference typically involves longer context. To restrict the effect of broad context, we only keep two groups of coreference arcs from smaller context. One includes mentions that are in the same sentence (“same”) for examining the model capability of encoding local context. The other includes mentions that are in consecutive sentences (“next”) for the broader context. We create this task from the PreCo dataset (Chen et al., 2018), which has mentions annotated even when they are not part of coreference chains. We filter out instances in which both mentions are pronouns."
D19-1040,N16-1150,0,0.0452198,"Zellers et al., 2019; Sap et al., 2019; Rajani et al., 2019). Another related line of work is to integrate entityrelated knowledge into the training of language models (Logan et al., 2019; Zhang et al., 2019; Sun et al., 2019). EntEval and the training objectives considered in this work are built on previous works that involve reasoning over entities. We give a brief overview of relevant works. Entity linking/disambiguation. Entity linking is a fundamental task in information extraction with a wealth of literature (He et al., 2013; Guo and Barbosa, 2014; Ling et al., 2015; Huang et al., 2015; Francis-Landau et al., 2016; Le and Titov, 2018; Martins et al., 2019). The goal of this task is to map a mention in context to the corresponding entity in a database. A natural approach is to learn entity representations that enable this mapping. Recent works focused on learning a fixed embedding for each entity using Wikipedia hyperlinks (Yamada et al., 2016; Ganea and Hofmann, 2017; Le and Titov, 2019). Gupta et al. (2017) additionally train context and description embeddings jointly, but this mainly aims to improve the quality of the fixed entity embeddings rather than using the context and description embeddings di"
D19-1040,2021.ccl-1.108,0,0.158509,"Missing"
D19-1040,P15-1001,0,0.010752,"defined by the ELMo parameters. In addition, we define the two bag-of-words reconstruction losses: X log q(xt |fELMo ([BOD]y1:Ty , 1, Ty )) lctx = − t ldesc = − X log q(yt |fELMo ([BOC]x1:Tx , i, j)) t where [BOD] and [BOC] are special symbols prepended to sentences to distinguish descriptions from contexts. The distribution q is parameterized by a linear layer that transforms the conditioning embedding into weights over the vocabulary. The final training loss is llang (x1:Tx ) + llang (y1:Ty ) + lctx + ldesc (1) Same as the original ELMo, each log loss is approximated with negative sampling (Jean et al., 2015). We write EntELMo to denote the model trained by Eq. (1). When using EntELMo for contextualized entity representations and descriptive entity representations, we use it analogously to ELMo. 5 5.1 5.2 Table 3 shows the performance of our models on the EntEval tasks. Our findings are detailed below: • Pretrained CWRs (ELMo, BERT) perform the best on EntEval overall, indicating that they capture knowledge about entities in contextual mentions or as entity descriptions. • BERT performs poorly on entity similarity and relatedness tasks. Since this task is zero-shot, it validates the recommended se"
D19-1040,P19-1598,0,0.059016,"atural Language Processing, pages 421–433, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work checking (Vlachos and Riedel, 2014; Wang, 2017; Thorne et al., 2018; Yin and Roth, 2018; Chen et al., 2019b), and commonsense learning (Angeli and Manning, 2014; Bowman et al., 2015; Li et al., 2016; Mihaylov et al., 2018; Zellers et al., 2018; Trinh and Le, 2018; Talmor et al., 2019; Zellers et al., 2019; Sap et al., 2019; Rajani et al., 2019). Another related line of work is to integrate entityrelated knowledge into the training of language models (Logan et al., 2019; Zhang et al., 2019; Sun et al., 2019). EntEval and the training objectives considered in this work are built on previous works that involve reasoning over entities. We give a brief overview of relevant works. Entity linking/disambiguation. Entity linking is a fundamental task in information extraction with a wealth of literature (He et al., 2013; Guo and Barbosa, 2014; Ling et al., 2015; Huang et al., 2015; Francis-Landau et al., 2016; Le and Titov, 2018; Martins et al., 2019). The goal of this task is to map a mention in context to the corresponding entity in a database. A natural approach"
D19-1040,D17-1195,0,0.0211293,"ng better entity representations by using natural hyperlink annotations in Wikipedia. We identify effective objectives for incorporating the contextual information in hyperlinks into state-of-the-art pretrained language models (Peters et al., 2018a) and show that they improve strong baselines on multiple EntEval tasks.1 1 • The dynamically encoded entity representations show a strong improvement on the entity disambiguation task compared to prior work using static entity embeddings. Introduction Entity representations play a key role in numerous important problems including language modeling (Ji et al., 2017), dialogue generation (He et al., 2017), entity linking (Gupta et al., 2017), and story generation (Clark et al., 2018). One successful line of work on learning entity representations has been learning static embeddings: that is, assign a unique vector to each entity in the training data (Gupta et al., 2017; Yamada et al., 2016, 2017). While these embeddings are useful in many applications, they have the obvious drawback of not accommodating unknown entities. Another limiting factor is the lack of an evaluation benchmark: it is often difficult to know which entity representations are better fo"
D19-1040,P19-1335,0,0.0538596,"database. A natural approach is to learn entity representations that enable this mapping. Recent works focused on learning a fixed embedding for each entity using Wikipedia hyperlinks (Yamada et al., 2016; Ganea and Hofmann, 2017; Le and Titov, 2019). Gupta et al. (2017) additionally train context and description embeddings jointly, but this mainly aims to improve the quality of the fixed entity embeddings rather than using the context and description embeddings directly; we find that their context and description encoders perform poorly on EntEval tasks. A closely related concurrent work by (Logeswaran et al., 2019) jointly encodes a mention in context and an entity description from Wikia to perform zero-shot entity linking. In contrast, here we seek to pretrain a general purpose entity representations that can function well either given or not given entity descriptions or mention contexts. Other entity-related tasks involve entity typing (Yaghoobzadeh and Sch¨utze, 2015; Murty et al., 2017; Del Corro et al., 2015; Rabinovich and Klein, 2017; Choi et al., 2018; Onoe and Durrett, 2019; Obeidat et al., 2019) and coreference resolution (Durrett and Klein, 2013; Wiseman et al., 2016; Lee et al., 2017; Webste"
D19-1040,P19-1066,0,0.0255656,"a mention in context and an entity description from Wikia to perform zero-shot entity linking. In contrast, here we seek to pretrain a general purpose entity representations that can function well either given or not given entity descriptions or mention contexts. Other entity-related tasks involve entity typing (Yaghoobzadeh and Sch¨utze, 2015; Murty et al., 2017; Del Corro et al., 2015; Rabinovich and Klein, 2017; Choi et al., 2018; Onoe and Durrett, 2019; Obeidat et al., 2019) and coreference resolution (Durrett and Klein, 2013; Wiseman et al., 2016; Lee et al., 2017; Webster et al., 2018; Kantor and Globerson, 2019). Contextualized word representations. Contextualized word representations and pretrained language representation models, such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018), are powerful pretrained models that have been shown to be effective for a variety of downstream tasks such as text classification, sentence relation prediction, named entity recognition, and question answering. Recent work has sought to evaluate the knowledge acquired by such models (Shi et al., 2016; Adi et al., 2017; Belinkov et al., 2017; Conneau et al., 2018; Conneau and Kiela, 2018; Liu et al., 2019a)."
D19-1040,D17-1086,0,0.0276967,"h relation type to form our training set and 10 instances per type the for validation and test sets. We use Wikipedia descriptions for each entity in the pair whose relation we are predicting and we use descriptive entity representations for each entity with supervised training. 3.7 China_men&apos;s_nationa l_basketball_team Named Entity Disambiguation (NED) Named entity disambiguation is the task of linking a named-entity mention to its corresponding instance in a knowledge base such as Wikipedia. In this task, we consider CoNLL-YAGO (CoNLL; Hoffart et al., 2011) and Rare Entity Prediction (Rare; Long et al., 2017). For CoNLL-YAGO, following Hoffart et al. (2011) and Yamada et al. (2016), we used the 425 Practically, we encode the context using CER to be x1 , and encode each entity description using DER to be x2 , and pass [x1 , x2 , x1 x2 , |x1 − x2 |] to a linear model to predict whether it is the correct entity to fill in. The model is trained with cross entropy loss. 4 France won the match 4–2 to The France national football team claim their second World Cup represents France in international foot ball title. Figure 5: An example of hyperlinks in Wikipedia. “France” is linked to the Wikipedia page o"
D19-1040,P18-1148,0,0.0183762,"19; Rajani et al., 2019). Another related line of work is to integrate entityrelated knowledge into the training of language models (Logan et al., 2019; Zhang et al., 2019; Sun et al., 2019). EntEval and the training objectives considered in this work are built on previous works that involve reasoning over entities. We give a brief overview of relevant works. Entity linking/disambiguation. Entity linking is a fundamental task in information extraction with a wealth of literature (He et al., 2013; Guo and Barbosa, 2014; Ling et al., 2015; Huang et al., 2015; Francis-Landau et al., 2016; Le and Titov, 2018; Martins et al., 2019). The goal of this task is to map a mention in context to the corresponding entity in a database. A natural approach is to learn entity representations that enable this mapping. Recent works focused on learning a fixed embedding for each entity using Wikipedia hyperlinks (Yamada et al., 2016; Ganea and Hofmann, 2017; Le and Titov, 2019). Gupta et al. (2017) additionally train context and description embeddings jointly, but this mainly aims to improve the quality of the fixed entity embeddings rather than using the context and description embeddings directly; we find that"
D19-1040,P19-2026,0,0.0153253,"al., 2019). Another related line of work is to integrate entityrelated knowledge into the training of language models (Logan et al., 2019; Zhang et al., 2019; Sun et al., 2019). EntEval and the training objectives considered in this work are built on previous works that involve reasoning over entities. We give a brief overview of relevant works. Entity linking/disambiguation. Entity linking is a fundamental task in information extraction with a wealth of literature (He et al., 2013; Guo and Barbosa, 2014; Ling et al., 2015; Huang et al., 2015; Francis-Landau et al., 2016; Le and Titov, 2018; Martins et al., 2019). The goal of this task is to map a mention in context to the corresponding entity in a database. A natural approach is to learn entity representations that enable this mapping. Recent works focused on learning a fixed embedding for each entity using Wikipedia hyperlinks (Yamada et al., 2016; Ganea and Hofmann, 2017; Le and Titov, 2019). Gupta et al. (2017) additionally train context and description embeddings jointly, but this mainly aims to improve the quality of the fixed entity embeddings rather than using the context and description embeddings directly; we find that their context and desc"
D19-1040,P19-1187,0,0.0237728,". Entity linking/disambiguation. Entity linking is a fundamental task in information extraction with a wealth of literature (He et al., 2013; Guo and Barbosa, 2014; Ling et al., 2015; Huang et al., 2015; Francis-Landau et al., 2016; Le and Titov, 2018; Martins et al., 2019). The goal of this task is to map a mention in context to the corresponding entity in a database. A natural approach is to learn entity representations that enable this mapping. Recent works focused on learning a fixed embedding for each entity using Wikipedia hyperlinks (Yamada et al., 2016; Ganea and Hofmann, 2017; Le and Titov, 2019). Gupta et al. (2017) additionally train context and description embeddings jointly, but this mainly aims to improve the quality of the fixed entity embeddings rather than using the context and description embeddings directly; we find that their context and description encoders perform poorly on EntEval tasks. A closely related concurrent work by (Logeswaran et al., 2019) jointly encodes a mention in context and an entity description from Wikia to perform zero-shot entity linking. In contrast, here we seek to pretrain a general purpose entity representations that can function well either given"
D19-1040,D17-1018,0,0.0176722,"Logeswaran et al., 2019) jointly encodes a mention in context and an entity description from Wikia to perform zero-shot entity linking. In contrast, here we seek to pretrain a general purpose entity representations that can function well either given or not given entity descriptions or mention contexts. Other entity-related tasks involve entity typing (Yaghoobzadeh and Sch¨utze, 2015; Murty et al., 2017; Del Corro et al., 2015; Rabinovich and Klein, 2017; Choi et al., 2018; Onoe and Durrett, 2019; Obeidat et al., 2019) and coreference resolution (Durrett and Klein, 2013; Wiseman et al., 2016; Lee et al., 2017; Webster et al., 2018; Kantor and Globerson, 2019). Contextualized word representations. Contextualized word representations and pretrained language representation models, such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018), are powerful pretrained models that have been shown to be effective for a variety of downstream tasks such as text classification, sentence relation prediction, named entity recognition, and question answering. Recent work has sought to evaluate the knowledge acquired by such models (Shi et al., 2016; Adi et al., 2017; Belinkov et al., 2017; Conneau et al.,"
D19-1040,P16-1137,1,0.849658,"a Technological Institute at Chicago. 1 Data processing and evaluation scripts are available at https://github.com/ZeweiChu/EntEval † 421 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 421–433, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work checking (Vlachos and Riedel, 2014; Wang, 2017; Thorne et al., 2018; Yin and Roth, 2018; Chen et al., 2019b), and commonsense learning (Angeli and Manning, 2014; Bowman et al., 2015; Li et al., 2016; Mihaylov et al., 2018; Zellers et al., 2018; Trinh and Le, 2018; Talmor et al., 2019; Zellers et al., 2019; Sap et al., 2019; Rajani et al., 2019). Another related line of work is to integrate entityrelated knowledge into the training of language models (Logan et al., 2019; Zhang et al., 2019; Sun et al., 2019). EntEval and the training objectives considered in this work are built on previous works that involve reasoning over entities. We give a brief overview of relevant works. Entity linking/disambiguation. Entity linking is a fundamental task in information extraction with a wealth of lit"
D19-1040,D18-1260,0,0.0138345,"nstitute at Chicago. 1 Data processing and evaluation scripts are available at https://github.com/ZeweiChu/EntEval † 421 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 421–433, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work checking (Vlachos and Riedel, 2014; Wang, 2017; Thorne et al., 2018; Yin and Roth, 2018; Chen et al., 2019b), and commonsense learning (Angeli and Manning, 2014; Bowman et al., 2015; Li et al., 2016; Mihaylov et al., 2018; Zellers et al., 2018; Trinh and Le, 2018; Talmor et al., 2019; Zellers et al., 2019; Sap et al., 2019; Rajani et al., 2019). Another related line of work is to integrate entityrelated knowledge into the training of language models (Logan et al., 2019; Zhang et al., 2019; Sun et al., 2019). EntEval and the training objectives considered in this work are built on previous works that involve reasoning over entities. We give a brief overview of relevant works. Entity linking/disambiguation. Entity linking is a fundamental task in information extraction with a wealth of literature (He et al., 201"
D19-1040,Q15-1023,0,0.0225531,"inh and Le, 2018; Talmor et al., 2019; Zellers et al., 2019; Sap et al., 2019; Rajani et al., 2019). Another related line of work is to integrate entityrelated knowledge into the training of language models (Logan et al., 2019; Zhang et al., 2019; Sun et al., 2019). EntEval and the training objectives considered in this work are built on previous works that involve reasoning over entities. We give a brief overview of relevant works. Entity linking/disambiguation. Entity linking is a fundamental task in information extraction with a wealth of literature (He et al., 2013; Guo and Barbosa, 2014; Ling et al., 2015; Huang et al., 2015; Francis-Landau et al., 2016; Le and Titov, 2018; Martins et al., 2019). The goal of this task is to map a mention in context to the corresponding entity in a database. A natural approach is to learn entity representations that enable this mapping. Recent works focused on learning a fixed embedding for each entity using Wikipedia hyperlinks (Yamada et al., 2016; Ganea and Hofmann, 2017; Le and Titov, 2019). Gupta et al. (2017) additionally train context and description embeddings jointly, but this mainly aims to improve the quality of the fixed entity embeddings rather tha"
D19-1040,N19-1112,0,0.345972,"rsity, NJ, USA {mchen,kgimpel}@ttic.edu,stratos@cs.rutgers.edu,zeweichu@uchicago.edu,chen.9279@osu.edu Abstract We introduce EntEval: a carefully designed benchmark for holistically evaluating entity representations. It is a test suite of diverse tasks that require nontrivial understanding of entities, including entity typing, entity similarity, entity relation prediction, and entity disambiguation. Motivated by the recent success of contextualized word representations (henceforth: CWRs) from pretrained models (McCann et al., 2017; Peters et al., 2018a; Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019b), we propose to encode the mention context or the description to dynamically represent an entity. In addition, we perform an in-depth comparison of ELMo and BERT-based embeddings and find that they show different characteristics on different tasks. We analyze each layer of the CWRs and make the following observations: Rich entity representations are useful for a wide class of problems involving entities. Despite their importance, there is no standardized benchmark that evaluates the overall quality of entity representations. In this work, we propose EntEval: a test suite of diverse tasks tha"
D19-1040,W18-3026,0,0.0316874,"alized entity representations in this task. 3.5 Entity Similarity and Relatedness (ESR) Given two entities with their descriptions from Wikipedia, the task is to determine their similarity or relatedness. After the entity descriptions are encoded into vector representations, we compute their cosine similarity as predictions. We use the KORE (Hoffart et al., 2012) and Wik1. For each relationship, we replace an entity with similar negative entities based on cosine similarity of averaged GloVe embeddings (Pennington et al., 2014). 424 SOCCER - JAPAN GET LUCKY WIN, CHINA IN SURPRISE DEFEAT. iSRS (Newman-Griffis et al., 2018) datasets in this task. Since the original datasets only provide entity names, we automatically add Wikipedia descriptions to each entity and manually ensure that every entity is matched to a Wikipedia description. We use Spearman’s rank correlation coefficient between our computed cosine similarity and the gold standard similarity/relatedness scores to measure the performance of entity representations. As KORE does not provide similarity scores of entity pairs, but simply ranks the candidate entities by their similarities to a target entity, we assign scores from 20 to 1 accordingly to each e"
D19-1040,N19-1087,0,0.0240893,"and description encoders perform poorly on EntEval tasks. A closely related concurrent work by (Logeswaran et al., 2019) jointly encodes a mention in context and an entity description from Wikia to perform zero-shot entity linking. In contrast, here we seek to pretrain a general purpose entity representations that can function well either given or not given entity descriptions or mention contexts. Other entity-related tasks involve entity typing (Yaghoobzadeh and Sch¨utze, 2015; Murty et al., 2017; Del Corro et al., 2015; Rabinovich and Klein, 2017; Choi et al., 2018; Onoe and Durrett, 2019; Obeidat et al., 2019) and coreference resolution (Durrett and Klein, 2013; Wiseman et al., 2016; Lee et al., 2017; Webster et al., 2018; Kantor and Globerson, 2019). Contextualized word representations. Contextualized word representations and pretrained language representation models, such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018), are powerful pretrained models that have been shown to be effective for a variety of downstream tasks such as text classification, sentence relation prediction, named entity recognition, and question answering. Recent work has sought to evaluate the knowledge acquire"
D19-1040,N19-1250,0,0.346171,"find that their context and description encoders perform poorly on EntEval tasks. A closely related concurrent work by (Logeswaran et al., 2019) jointly encodes a mention in context and an entity description from Wikia to perform zero-shot entity linking. In contrast, here we seek to pretrain a general purpose entity representations that can function well either given or not given entity descriptions or mention contexts. Other entity-related tasks involve entity typing (Yaghoobzadeh and Sch¨utze, 2015; Murty et al., 2017; Del Corro et al., 2015; Rabinovich and Klein, 2017; Choi et al., 2018; Onoe and Durrett, 2019; Obeidat et al., 2019) and coreference resolution (Durrett and Klein, 2013; Wiseman et al., 2016; Lee et al., 2017; Webster et al., 2018; Kantor and Globerson, 2019). Contextualized word representations. Contextualized word representations and pretrained language representation models, such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018), are powerful pretrained models that have been shown to be effective for a variety of downstream tasks such as text classification, sentence relation prediction, named entity recognition, and question answering. Recent work has sought to evaluat"
D19-1040,spitkovsky-chang-2012-cross,0,0.114517,"Missing"
D19-1040,D14-1162,0,0.0914542,"input representations to encode knowledge of entities based on the context. We use contextualized entity representations in this task. 3.5 Entity Similarity and Relatedness (ESR) Given two entities with their descriptions from Wikipedia, the task is to determine their similarity or relatedness. After the entity descriptions are encoded into vector representations, we compute their cosine similarity as predictions. We use the KORE (Hoffart et al., 2012) and Wik1. For each relationship, we replace an entity with similar negative entities based on cosine similarity of averaged GloVe embeddings (Pennington et al., 2014). 424 SOCCER - JAPAN GET LUCKY WIN, CHINA IN SURPRISE DEFEAT. iSRS (Newman-Griffis et al., 2018) datasets in this task. Since the original datasets only provide entity names, we automatically add Wikipedia descriptions to each entity and manually ensure that every entity is matched to a Wikipedia description. We use Spearman’s rank correlation coefficient between our computed cosine similarity and the gold standard similarity/relatedness scores to measure the performance of entity representations. As KORE does not provide similarity scores of entity pairs, but simply ranks the candidate entiti"
D19-1040,N18-1202,0,0.685304,"ta Technological Institute at Chicago, IL, USA 4 Rutgers University, NJ, USA {mchen,kgimpel}@ttic.edu,stratos@cs.rutgers.edu,zeweichu@uchicago.edu,chen.9279@osu.edu Abstract We introduce EntEval: a carefully designed benchmark for holistically evaluating entity representations. It is a test suite of diverse tasks that require nontrivial understanding of entities, including entity typing, entity similarity, entity relation prediction, and entity disambiguation. Motivated by the recent success of contextualized word representations (henceforth: CWRs) from pretrained models (McCann et al., 2017; Peters et al., 2018a; Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019b), we propose to encode the mention context or the description to dynamically represent an entity. In addition, we perform an in-depth comparison of ELMo and BERT-based embeddings and find that they show different characteristics on different tasks. We analyze each layer of the CWRs and make the following observations: Rich entity representations are useful for a wide class of problems involving entities. Despite their importance, there is no standardized benchmark that evaluates the overall quality of entity representations. In this"
D19-1040,N19-1421,0,0.0158562,"available at https://github.com/ZeweiChu/EntEval † 421 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 421–433, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work checking (Vlachos and Riedel, 2014; Wang, 2017; Thorne et al., 2018; Yin and Roth, 2018; Chen et al., 2019b), and commonsense learning (Angeli and Manning, 2014; Bowman et al., 2015; Li et al., 2016; Mihaylov et al., 2018; Zellers et al., 2018; Trinh and Le, 2018; Talmor et al., 2019; Zellers et al., 2019; Sap et al., 2019; Rajani et al., 2019). Another related line of work is to integrate entityrelated knowledge into the training of language models (Logan et al., 2019; Zhang et al., 2019; Sun et al., 2019). EntEval and the training objectives considered in this work are built on previous works that involve reasoning over entities. We give a brief overview of relevant works. Entity linking/disambiguation. Entity linking is a fundamental task in information extraction with a wealth of literature (He et al., 2013; Guo and Barbosa, 2014; Ling et al., 2015; Huang et al., 2015"
D19-1040,D18-1179,0,0.309682,"ta Technological Institute at Chicago, IL, USA 4 Rutgers University, NJ, USA {mchen,kgimpel}@ttic.edu,stratos@cs.rutgers.edu,zeweichu@uchicago.edu,chen.9279@osu.edu Abstract We introduce EntEval: a carefully designed benchmark for holistically evaluating entity representations. It is a test suite of diverse tasks that require nontrivial understanding of entities, including entity typing, entity similarity, entity relation prediction, and entity disambiguation. Motivated by the recent success of contextualized word representations (henceforth: CWRs) from pretrained models (McCann et al., 2017; Peters et al., 2018a; Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019b), we propose to encode the mention context or the description to dynamically represent an entity. In addition, we perform an in-depth comparison of ELMo and BERT-based embeddings and find that they show different characteristics on different tasks. We analyze each layer of the CWRs and make the following observations: Rich entity representations are useful for a wide class of problems involving entities. Despite their importance, there is no standardized benchmark that evaluates the overall quality of entity representations. In this"
D19-1040,N18-1074,0,0.0304412,"Missing"
D19-1040,P17-2052,0,0.0240077,"context and description embeddings directly; we find that their context and description encoders perform poorly on EntEval tasks. A closely related concurrent work by (Logeswaran et al., 2019) jointly encodes a mention in context and an entity description from Wikia to perform zero-shot entity linking. In contrast, here we seek to pretrain a general purpose entity representations that can function well either given or not given entity descriptions or mention contexts. Other entity-related tasks involve entity typing (Yaghoobzadeh and Sch¨utze, 2015; Murty et al., 2017; Del Corro et al., 2015; Rabinovich and Klein, 2017; Choi et al., 2018; Onoe and Durrett, 2019; Obeidat et al., 2019) and coreference resolution (Durrett and Klein, 2013; Wiseman et al., 2016; Lee et al., 2017; Webster et al., 2018; Kantor and Globerson, 2019). Contextualized word representations. Contextualized word representations and pretrained language representation models, such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018), are powerful pretrained models that have been shown to be effective for a variety of downstream tasks such as text classification, sentence relation prediction, named entity recognition, and question a"
D19-1040,W14-2508,0,0.023988,"n in hyperlinks and improve ELMo-based CWRs on a variety of entity related tasks. ∗ Equal contribution. Listed in alphabetical order. Work done while the author was at Toyota Technological Institute at Chicago. 1 Data processing and evaluation scripts are available at https://github.com/ZeweiChu/EntEval † 421 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 421–433, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work checking (Vlachos and Riedel, 2014; Wang, 2017; Thorne et al., 2018; Yin and Roth, 2018; Chen et al., 2019b), and commonsense learning (Angeli and Manning, 2014; Bowman et al., 2015; Li et al., 2016; Mihaylov et al., 2018; Zellers et al., 2018; Trinh and Le, 2018; Talmor et al., 2019; Zellers et al., 2019; Sap et al., 2019; Rajani et al., 2019). Another related line of work is to integrate entityrelated knowledge into the training of language models (Logan et al., 2019; Zhang et al., 2019; Sun et al., 2019). EntEval and the training objectives considered in this work are built on previous works that involve reasoning over enti"
D19-1040,P19-1487,0,0.0158614,"edings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 421–433, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work checking (Vlachos and Riedel, 2014; Wang, 2017; Thorne et al., 2018; Yin and Roth, 2018; Chen et al., 2019b), and commonsense learning (Angeli and Manning, 2014; Bowman et al., 2015; Li et al., 2016; Mihaylov et al., 2018; Zellers et al., 2018; Trinh and Le, 2018; Talmor et al., 2019; Zellers et al., 2019; Sap et al., 2019; Rajani et al., 2019). Another related line of work is to integrate entityrelated knowledge into the training of language models (Logan et al., 2019; Zhang et al., 2019; Sun et al., 2019). EntEval and the training objectives considered in this work are built on previous works that involve reasoning over entities. We give a brief overview of relevant works. Entity linking/disambiguation. Entity linking is a fundamental task in information extraction with a wealth of literature (He et al., 2013; Guo and Barbosa, 2014; Ling et al., 2015; Huang et al., 2015; Francis-Landau et al., 2016; Le and Titov, 2018; Martins et"
D19-1040,D19-1454,0,0.0212359,"ntEval † 421 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 421–433, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work checking (Vlachos and Riedel, 2014; Wang, 2017; Thorne et al., 2018; Yin and Roth, 2018; Chen et al., 2019b), and commonsense learning (Angeli and Manning, 2014; Bowman et al., 2015; Li et al., 2016; Mihaylov et al., 2018; Zellers et al., 2018; Trinh and Le, 2018; Talmor et al., 2019; Zellers et al., 2019; Sap et al., 2019; Rajani et al., 2019). Another related line of work is to integrate entityrelated knowledge into the training of language models (Logan et al., 2019; Zhang et al., 2019; Sun et al., 2019). EntEval and the training objectives considered in this work are built on previous works that involve reasoning over entities. We give a brief overview of relevant works. Entity linking/disambiguation. Entity linking is a fundamental task in information extraction with a wealth of literature (He et al., 2013; Guo and Barbosa, 2014; Ling et al., 2015; Huang et al., 2015; Francis-Landau et al., 2016; Le and Ti"
D19-1040,W18-5446,0,0.0793014,"Missing"
D19-1040,D16-1159,0,0.0574611,"esolution (Durrett and Klein, 2013; Wiseman et al., 2016; Lee et al., 2017; Webster et al., 2018; Kantor and Globerson, 2019). Contextualized word representations. Contextualized word representations and pretrained language representation models, such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018), are powerful pretrained models that have been shown to be effective for a variety of downstream tasks such as text classification, sentence relation prediction, named entity recognition, and question answering. Recent work has sought to evaluate the knowledge acquired by such models (Shi et al., 2016; Adi et al., 2017; Belinkov et al., 2017; Conneau et al., 2018; Conneau and Kiela, 2018; Liu et al., 2019a). In this work, we focus on evaluating their capabilities in modeling entities. 3 EntEval We are interested in two approaches: contextualized entity representations (henceforth: CER) and descriptive entity representations (henceforth: DER), both encoding fixed-length vector representations for entities. The contextualized entity representations encodes an entity based on the context it appears regardless of whether the entity is seen before. The motivation behind contextualized entity re"
D19-1040,P17-2067,0,0.0294836,"e ELMo-based CWRs on a variety of entity related tasks. ∗ Equal contribution. Listed in alphabetical order. Work done while the author was at Toyota Technological Institute at Chicago. 1 Data processing and evaluation scripts are available at https://github.com/ZeweiChu/EntEval † 421 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 421–433, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work checking (Vlachos and Riedel, 2014; Wang, 2017; Thorne et al., 2018; Yin and Roth, 2018; Chen et al., 2019b), and commonsense learning (Angeli and Manning, 2014; Bowman et al., 2015; Li et al., 2016; Mihaylov et al., 2018; Zellers et al., 2018; Trinh and Le, 2018; Talmor et al., 2019; Zellers et al., 2019; Sap et al., 2019; Rajani et al., 2019). Another related line of work is to integrate entityrelated knowledge into the training of language models (Logan et al., 2019; Zhang et al., 2019; Sun et al., 2019). EntEval and the training objectives considered in this work are built on previous works that involve reasoning over entities. We giv"
D19-1040,Q18-1042,0,0.0199368,"2019) jointly encodes a mention in context and an entity description from Wikia to perform zero-shot entity linking. In contrast, here we seek to pretrain a general purpose entity representations that can function well either given or not given entity descriptions or mention contexts. Other entity-related tasks involve entity typing (Yaghoobzadeh and Sch¨utze, 2015; Murty et al., 2017; Del Corro et al., 2015; Rabinovich and Klein, 2017; Choi et al., 2018; Onoe and Durrett, 2019; Obeidat et al., 2019) and coreference resolution (Durrett and Klein, 2013; Wiseman et al., 2016; Lee et al., 2017; Webster et al., 2018; Kantor and Globerson, 2019). Contextualized word representations. Contextualized word representations and pretrained language representation models, such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018), are powerful pretrained models that have been shown to be effective for a variety of downstream tasks such as text classification, sentence relation prediction, named entity recognition, and question answering. Recent work has sought to evaluate the knowledge acquired by such models (Shi et al., 2016; Adi et al., 2017; Belinkov et al., 2017; Conneau et al., 2018; Conneau and Kie"
D19-1040,N16-1114,0,0.0629429,"Missing"
D19-1040,D15-1083,0,0.059586,"Missing"
D19-1040,K16-1025,0,0.46592,"The dynamically encoded entity representations show a strong improvement on the entity disambiguation task compared to prior work using static entity embeddings. Introduction Entity representations play a key role in numerous important problems including language modeling (Ji et al., 2017), dialogue generation (He et al., 2017), entity linking (Gupta et al., 2017), and story generation (Clark et al., 2018). One successful line of work on learning entity representations has been learning static embeddings: that is, assign a unique vector to each entity in the training data (Gupta et al., 2017; Yamada et al., 2016, 2017). While these embeddings are useful in many applications, they have the obvious drawback of not accommodating unknown entities. Another limiting factor is the lack of an evaluation benchmark: it is often difficult to know which entity representations are better for which tasks. • BERT-based entity representations require further supervised training to perform well on downstream tasks, while ELMo-based representations are more capable of performing zeroshot tasks. • In general, higher layers of ELMo and BERTbased CWRs are more transferable to EntEval tasks. To further improve contextuali"
D19-1040,Q17-1028,0,0.0839573,"Missing"
D19-1040,D18-1010,0,0.0233129,"entity related tasks. ∗ Equal contribution. Listed in alphabetical order. Work done while the author was at Toyota Technological Institute at Chicago. 1 Data processing and evaluation scripts are available at https://github.com/ZeweiChu/EntEval † 421 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 421–433, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work checking (Vlachos and Riedel, 2014; Wang, 2017; Thorne et al., 2018; Yin and Roth, 2018; Chen et al., 2019b), and commonsense learning (Angeli and Manning, 2014; Bowman et al., 2015; Li et al., 2016; Mihaylov et al., 2018; Zellers et al., 2018; Trinh and Le, 2018; Talmor et al., 2019; Zellers et al., 2019; Sap et al., 2019; Rajani et al., 2019). Another related line of work is to integrate entityrelated knowledge into the training of language models (Logan et al., 2019; Zhang et al., 2019; Sun et al., 2019). EntEval and the training objectives considered in this work are built on previous works that involve reasoning over entities. We give a brief overview of relevant works. Ent"
D19-1040,D18-1009,0,0.0197179,"Data processing and evaluation scripts are available at https://github.com/ZeweiChu/EntEval † 421 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 421–433, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work checking (Vlachos and Riedel, 2014; Wang, 2017; Thorne et al., 2018; Yin and Roth, 2018; Chen et al., 2019b), and commonsense learning (Angeli and Manning, 2014; Bowman et al., 2015; Li et al., 2016; Mihaylov et al., 2018; Zellers et al., 2018; Trinh and Le, 2018; Talmor et al., 2019; Zellers et al., 2019; Sap et al., 2019; Rajani et al., 2019). Another related line of work is to integrate entityrelated knowledge into the training of language models (Logan et al., 2019; Zhang et al., 2019; Sun et al., 2019). EntEval and the training objectives considered in this work are built on previous works that involve reasoning over entities. We give a brief overview of relevant works. Entity linking/disambiguation. Entity linking is a fundamental task in information extraction with a wealth of literature (He et al., 2013; Guo and Barbosa, 20"
D19-1040,P19-1472,0,0.0164685,"/github.com/ZeweiChu/EntEval † 421 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 421–433, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work checking (Vlachos and Riedel, 2014; Wang, 2017; Thorne et al., 2018; Yin and Roth, 2018; Chen et al., 2019b), and commonsense learning (Angeli and Manning, 2014; Bowman et al., 2015; Li et al., 2016; Mihaylov et al., 2018; Zellers et al., 2018; Trinh and Le, 2018; Talmor et al., 2019; Zellers et al., 2019; Sap et al., 2019; Rajani et al., 2019). Another related line of work is to integrate entityrelated knowledge into the training of language models (Logan et al., 2019; Zhang et al., 2019; Sun et al., 2019). EntEval and the training objectives considered in this work are built on previous works that involve reasoning over entities. We give a brief overview of relevant works. Entity linking/disambiguation. Entity linking is a fundamental task in information extraction with a wealth of literature (He et al., 2013; Guo and Barbosa, 2014; Ling et al., 2015; Huang et al., 2015; Francis-Landau et al"
D19-1040,P19-1139,0,0.0279608,"essing, pages 421–433, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work checking (Vlachos and Riedel, 2014; Wang, 2017; Thorne et al., 2018; Yin and Roth, 2018; Chen et al., 2019b), and commonsense learning (Angeli and Manning, 2014; Bowman et al., 2015; Li et al., 2016; Mihaylov et al., 2018; Zellers et al., 2018; Trinh and Le, 2018; Talmor et al., 2019; Zellers et al., 2019; Sap et al., 2019; Rajani et al., 2019). Another related line of work is to integrate entityrelated knowledge into the training of language models (Logan et al., 2019; Zhang et al., 2019; Sun et al., 2019). EntEval and the training objectives considered in this work are built on previous works that involve reasoning over entities. We give a brief overview of relevant works. Entity linking/disambiguation. Entity linking is a fundamental task in information extraction with a wealth of literature (He et al., 2013; Guo and Barbosa, 2014; Ling et al., 2015; Huang et al., 2015; Francis-Landau et al., 2016; Le and Titov, 2018; Martins et al., 2019). The goal of this task is to map a mention in context to the corresponding entity in a database. A natural approach is to learn entity r"
E12-1076,W10-0701,0,0.00692588,"ace, blue clear sky becomes clear blue sky, wooden brown table becomes brown wooden table, etc. 5 Evaluation Each set of sentences is generated with α (likelihood cutoff) set to .01 and γ (observation count cutoff) set to 3. We compare the system against human-written descriptions and two state-of-theart vision-to-language systems, the Kulkarni et al. (2011) and Yang et al. (2011) systems. Human judgments were collected using Amazon’s Mechanical Turk (Amazon, 2011). We follow recommended practices for evaluating an NLG system (Reiter and Belz, 2009) and for running a study on Mechanical Turk (Callison-Burch and Dredze, 2010), using a balanced design with each subject rating 3 descriptions from each system. Subjects rated their level of agreement on a 5-point Likert scale including a neutral middle position, and since quality ratings are ordinal (points are not necessarily equidistant), we evaluate responses using a non-parametric test. Participants that took less than 3 minutes to answer all 60 questions and did not include a humanlike rating for at least 1 of the 3 human-written descriptions were removed and replaced. It is important to note that this evaluation compares full generation systems; many factors are"
E12-1076,2005.mtsummit-papers.11,0,0.0171464,"possible trees and removes mark-up to produce a final string. This is also the stage where punctuation may be added. Different strings may be generated depending on different specifications from the user, as discussed at the beginning of Section 4 and shown in the online demo. To evaluate the system against other systems, we specify that the system should (1) not hallucinate likely verbs; and (2) return the longest string possible. 4.4.1 Step 7: Get Final Tree, Clear Mark-Up We explored two methods for selecting a final string. In one method, a trigram language model built using the Europarl (Koehn, 2005) data with start/end symbols returns the highest-scoring description (normalizing for length). In the second method, we limit the generation system to select the most likely closed-class words (determiners, prepositions) while building the subtrees, overgenerating all possible adjective combinations. The final string is then the one with the most words. We find that the second method produces descriptions that seem more natural and varied than the n-gram ranking method for our development set, and so use the longest string method in evaluation. 4.4.2 Step 8: Prenominal Modifier Ordering To ord"
E12-1076,P08-1119,0,0.0302625,"Missing"
E12-1076,P98-1116,0,0.0450347,"structions between objects if specified at runtime. A similar approach was taken in Yang et al. (2011). Some examples are given in Figure 7. We follow a three-tiered generation process (Reiter and Dale, 2000), utilizing content determination to first cluster and order the object nouns, create their local subtrees, and filter incorrect detections; microplanning to construct full syntactic trees around the noun clusters, and surface realization to order selected modifiers, realize them as postnominal or prenominal, and select final outputs. The system follows an overgenerate-andselect approach (Langkilde and Knight, 1998), which allows different final trees to be selected with different settings. 4.1 Knowledge Base Midge uses a knowledge base that stores models for different tasks during generation. These models are primarily data-driven, but we also include a hand-built component to handle a small set of rules. The data-driven component provides the syntactically informed word co-occurrence statistics learned from the Flickr data, a model for ordering the selected nouns in a sentence, and a model to change computer vision attributes to attribute:value pairs. Below, we discuss the three main data-driven models"
E12-1076,W11-0326,1,0.644361,", the generator filters and constrains the noisy detections output from a vision system to generate syntactic trees that detail what the computer vision system sees. Results show that the generation system outperforms state-of-the-art systems, automatically generating some of the most natural image descriptions to date. 1 Introduction It is becoming a real possibility for intelligent systems to talk about the visual world. New ways of mapping computer vision to generated language have emerged in the past few years, with a focus on pairing detections in an image to words (Farhadi et al., 2010; Li et al., 2011; Kulkarni et al., 2011; Yang et al., 2011). The goal in connecting vision to language has varied: systems have started producing language that is descriptive and poetic (Li et al., 2011), summaries that add content where the computer vision system does not (Yang et al., 2011), and captions copied directly from other images that are globally (Farhadi et al., 2010) and locally similar (Ordonez et al., 2011). A commonality between all of these approaches is that they aim to produce naturalsounding descriptions from computer vision detections. This commonality is our starting point: We aim to des"
E12-1076,P11-2041,1,0.471394,"econd method, we limit the generation system to select the most likely closed-class words (determiners, prepositions) while building the subtrees, overgenerating all possible adjective combinations. The final string is then the one with the most words. We find that the second method produces descriptions that seem more natural and varied than the n-gram ranking method for our development set, and so use the longest string method in evaluation. 4.4.2 Step 8: Prenominal Modifier Ordering To order sets of selected adjectives, we use the top-scoring prenominal modifier ordering model discussed in Mitchell et al. (2011). This is an ngram model constructed over noun phrases that were extracted from an automatically parsed version of the New York Times portion of the Gigaword corpus (Graff and Cieri, 2003). With this in place, blue clear sky becomes clear blue sky, wooden brown table becomes brown wooden table, etc. 5 Evaluation Each set of sentences is generated with α (likelihood cutoff) set to .01 and γ (observation count cutoff) set to 3. We compare the system against human-written descriptions and two state-of-theart vision-to-language systems, the Kulkarni et al. (2011) and Yang et al. (2011) systems. Hu"
E12-1076,W11-1611,0,0.00779403,"Missing"
E12-1076,W10-0721,0,0.417713,"ed on object nouns, and this enables each noun head in a description to select for the kinds of structures it tends to appear in (syntactic constraints) and the other words it tends to occur with (semantic constraints). This is a data-driven way to generate likely adjectives, prepositions, determiners, etc., taking the intersection of what the vision system predicts and how the object noun tends to be described. 2 Background Our approach to describing images starts with a system from Kulkarni et al. (2011) that composes novel captions for images in the PASCAL sentence data set,2 introduced in Rashtchian et al. (2010). This provides multiple object detections based on Felzenszwalb’s mixtures of multiscale deformable parts models (Felzenszwalb et al., 2008), and stuff detections (roughly, mass nouns, things like sky and grass) based on linear SVMs for low level region features. Appearance characteristics are predicted using trained detectors for colors, shapes, textures, and materials, an idea originally introduced in Farhadi et al. (2009). Local texture, Histograms of Oriented Gradients (HOG) (Dalal and Triggs, 2005), edge, and color descriptors inside the bounding box of a recognized object are binned int"
E12-1076,J09-4008,0,0.01131,"ion of the Gigaword corpus (Graff and Cieri, 2003). With this in place, blue clear sky becomes clear blue sky, wooden brown table becomes brown wooden table, etc. 5 Evaluation Each set of sentences is generated with α (likelihood cutoff) set to .01 and γ (observation count cutoff) set to 3. We compare the system against human-written descriptions and two state-of-theart vision-to-language systems, the Kulkarni et al. (2011) and Yang et al. (2011) systems. Human judgments were collected using Amazon’s Mechanical Turk (Amazon, 2011). We follow recommended practices for evaluating an NLG system (Reiter and Belz, 2009) and for running a study on Mechanical Turk (Callison-Burch and Dredze, 2010), using a balanced design with each subject rating 3 descriptions from each system. Subjects rated their level of agreement on a 5-point Likert scale including a neutral middle position, and since quality ratings are ordinal (points are not necessarily equidistant), we evaluate responses using a non-parametric test. Participants that took less than 3 minutes to answer all 60 questions and did not include a humanlike rating for at least 1 of the 3 human-written descriptions were removed and replaced. It is important to"
E12-1076,D11-1041,1,\N,Missing
E12-1076,C04-1096,0,\N,Missing
E12-1076,C98-1112,0,\N,Missing
E12-1076,W10-0707,0,\N,Missing
E12-1076,P08-1000,0,\N,Missing
N12-1094,P89-1010,0,0.244848,"nouns and adjectives automatically based on bootstrapping techniques. First, we construct a graph between adjectives by computing distributional similarity (Turney and Pantel, 2010) between them. For computing distributional similarity between adjectives, each target adjective is defined as a vector of nouns which are modified by the target adjective. To be exact, we use only those adjectives as modifiers which appear adjacent to a noun (that is, in a JJ NN construction). For example, in “small red apple,” we consider only red as a modifier for noun. We use Pointwise Mutual Information (PMI) (Church and Hanks, 1989) to weight the contexts, and select the top 1000 PMI contexts for each adjective.3 Next, we apply cosine similarity to find the top 10 distributionally similar adjectives with respect to each target adjective based on our large generic corpus (Large-Data from Section 2.1). This creates a graph with adjectives as nodes and cosine similarity as weight on the edges. Analogously, we construct a graph with nouns as nodes (here, adjectives are used as contexts for nouns). We then apply bootstrapping (Kozareva et al., 2008) on the noun and adjective graphs by selecting 10 seeds for visual and non-vis"
N12-1094,P10-1126,0,0.071421,"ful. In general, features in the phrase were most useful (not surprisingly), and then features before the phrase (presumably to give context, for instance as in “out of the window”). Features from after the phrase were not useful. 4 Non-singleton features appear more than once in the data. + + + + + + + + + + C ATEGORY Words Image Bootstrap Spell Length Words Wordnet Spell Spell Wordnet Wordnet P OSITION Phrase Phrase Phrase Before Phrase After Before Before After AUC 74.7 74.4 74.3 75.3 74.7 76.2 76.1 76.0 76.8 77.0 75.6 Sadeghi, 2011), and automatic caption generation (Farhadi et al., 2010; Feng and Lapata, 2010; Ordonez et al., 2011; Kulkarni et al., 2011; Yang et al., 2011; Li et al., 2011; Mitchell et al., 2012), it becomes increasingly important to understand, and to be able to detect, text that actually refers to observed phenomena. Our results suggest that while this is a hard problem, it is possible to leverage large text resources and state-of-the-art computer vision algorithms to address it with high accuracy. Acknowledgments Table 6: Results of feature ablation on L ARGE data set. Corresponding results on the L ARGE data set are shown in Table 6. Note that the order of features selected is"
N12-1094,P08-1119,0,0.0257797,"Missing"
N12-1094,W11-0326,1,0.693528,"eatures before the phrase (presumably to give context, for instance as in “out of the window”). Features from after the phrase were not useful. 4 Non-singleton features appear more than once in the data. + + + + + + + + + + C ATEGORY Words Image Bootstrap Spell Length Words Wordnet Spell Spell Wordnet Wordnet P OSITION Phrase Phrase Phrase Before Phrase After Before Before After AUC 74.7 74.4 74.3 75.3 74.7 76.2 76.1 76.0 76.8 77.0 75.6 Sadeghi, 2011), and automatic caption generation (Farhadi et al., 2010; Feng and Lapata, 2010; Ordonez et al., 2011; Kulkarni et al., 2011; Yang et al., 2011; Li et al., 2011; Mitchell et al., 2012), it becomes increasingly important to understand, and to be able to detect, text that actually refers to observed phenomena. Our results suggest that while this is a hard problem, it is possible to leverage large text resources and state-of-the-art computer vision algorithms to address it with high accuracy. Acknowledgments Table 6: Results of feature ablation on L ARGE data set. Corresponding results on the L ARGE data set are shown in Table 6. Note that the order of features selected is different because the training data is different. Here, the most useful features"
N12-1094,U08-1013,0,0.0289441,"Missing"
N12-1094,E12-1076,1,0.90543,"Missing"
N12-1094,W10-0721,0,0.0518612,"n Flickr is almost always written by the photographer of that image. This means the descriptions often contain information that is not actually pictured in the image, or contain references that are only relevant to the photographer (referring to a person/pet by name). One might think that this is an artifact of this particular dataset, but it appears to be generic to all captions, even those written by a viewer (rather than the photographer). Figure 2 shows an image from the Pascal dataset (Everingham et al., 2010), together with captions written by random people collected via crowd-sourcing (Rashtchian et al., 2010). There is much in this caption that is clearly made-up by the author, presumably to make the caption more interesting (e.g., meta-references like “the camera” or “A photo” as well as “guesses” about the image, such as “garage” and “venison”). Second, there is a question of how much inference you are allowed to do when you say that you “see” something. For example, in the top image in Figure 1, the street is pictured, but does that mean that “Hanbury St.” is visual? What if there were a street sign that clearly read “Hanbury St.” in the image? This problem comes up all the time, when people sa"
N12-1094,W02-1028,0,0.0376964,"Missing"
N12-1094,N10-1119,0,0.0106617,"VP to 20 visually descriptive predicates shown in the top of Table 2, and VA to all nouns that appear in the object argument position with respect to the seed predicates. We approximate this by taking nouns on the right hand side of the predicates within a window of 4 words using the Web 1T Google N-gram data (Brants and Franz., 2006). For edge weights, we use conditional probabilities between predicates and arguments so that w(p → a) := pr(a|p) and w(a → p) := pr(p|a). In order to collectively induce the visually descriptive words from this graph, we apply the graph propagation algorithm of Velikovich et al. (2010), a variant of label propagation algorithms (Zhu and Ghahramani, 2002) that has been shown to be effective for inducing a web-scale polarity lexicon based on word co-occurrence statistics. This algoColor Material Shape Size Surface Direction Pattern Quality Beauty Age Ethnicity purple blue maroon beige green plastic cotton wooden metallic silver circular square round rectangular triangular small big tiny tall huge coarse smooth furry fluffy rough sideways north upward left down striped dotted checked plaid quilted shiny rusty dirty burned glittery beautiful cute pretty gorgeous lovely young ma"
N12-1094,J90-1003,0,\N,Missing
N12-1094,D11-1041,1,\N,Missing
N12-1094,W10-0707,0,\N,Missing
N12-1094,W05-1003,0,\N,Missing
N13-1015,H91-1060,0,0.284786,"e a coarse-to-fine algorithm for parsing with either the EM or spectral derived grammar: a PCFG without latent states is used to calculate marginals, and dynamic programming items are removed if their marginal probability is lower than some threshold (0.00005 in our experiments). For simplicity the parser takes part-of-speech tagged sentences as input. We use automatically tagged data from Turbo Tagger (Martins et al., 2010). The tagger is used to tag both the development data and the test data. The tagger was retrained on sections 2–21. We use the F1 measure according to the Parseval metric (Black et al., 1991). For the spectral algorithm, we tuned the smoothing parameters using section 0 of the treebank. 4.1 Comparison to EM: Accuracy We compare models trained using EM and the spectral algorithm using values for m in {8, 16, 24, 32}.5 For EM, we found that it was important to use development data to choose the number of iterations of training. We train the models for 100 iterations, then test accuracy of the model on section 22 (development data) at different iteration numbers. Table 1 shows that a peak level of accuracy is reached for all values of m, other than m = 8, at iteration 20–30, with som"
N13-1015,W08-2102,1,0.366224,"Missing"
N13-1015,P05-1022,0,0.193929,"Missing"
N13-1015,P12-1024,1,0.782914,"iments using the spectral algorithm. We show that the algorithm provides models with the same accuracy as EM, but is an order of magnitude more efficient. We describe a number of key steps used to obtain this level of performance; these should be relevant to other work on the application of spectral learning algorithms. We view our results as strong empirical evidence for the viability of spectral methods as an alternative to EM. 1 Introduction Latent-variable PCFGS (L-PCFGs) are a highly successful model for natural language parsing (Matsuzaki et al., 2005; Petrov et al., 2006). Recent work (Cohen et al., 2012) has introduced a spectral learning algorithm for L-PCFGs. A crucial property of the algorithm is that it is guaranteed to provide consistent parameter estimates—in fact it has PAC-style guarantees of sample complexity.1 This is in contrast to the EM algorithm, the usual method for parameter estimation in L-PCFGs, which has the weaker guarantee of reaching a local maximum of the likelihood function. The spectral algorithm is relatively simple and efficient, relying on a singular value decomposition of the training examples, followed by a single pass over the data where parameter values are cal"
N13-1015,J03-4003,1,0.155015,"Missing"
N13-1015,D12-1019,1,0.792489,"Missing"
N13-1015,P96-1024,0,0.481709,", 2, . . . n}. 149 The parsing problem is to take a sentence as input, and produce a skeletal tree as output. A standard method for parsing with L-PCFGs is as follows. First, for a given input sentence x1 . . . xn , for any triple (a, i, j) such that a ∈ N and 1 ≤ i ≤ j ≤ n, the marginal µ(a, i, j) is defined as X p(t) (1) µ(a, i, j) = t:(a,i,j)∈t where the sum is over all skeletal trees t for x1 . . . xn that include non-terminal a spanning words xi . . . xj . A variant of the inside-outside algorithm can be used to calculate marginals. Once marginals have been computed, Goodman’s algorithm (Goodman, 1996) is used to find P arg maxt (a,i,j)∈t µ(a, i, j).3 2.2 The Spectral Learning Algorithm We now give a sketch of the spectral learning algorithm. The training data for the algorithm is a set of skeletal trees. The output from the algorithm is a set of parameter estimates for t, q and π (more precisely, the estimates are estimates of linearly transformed parameters; see Cohen et al. (2012) and section 2.3.1 for more details). The algorithm takes two inputs in addition to the set of skeletal trees. The first is an integer m, specifying the number of latent state values in the model. Typically m is"
N13-1015,P12-1046,0,0.0332165,"Missing"
N13-1015,P03-1054,0,0.0369327,"D∗ N. • The two-level and three-level rule fragments above the foot node. In the above example these features would be VP V S NP D∗ NP N VP V NP D∗ N • The label of the foot node, together with the label of its parent. In the above example this is (D, NP). • The label of the foot node, together with the label of its parent and grandparent. In the above example this is (D, NP, VP). • The part of speech of the first head word along the path from the foot of the outside tree to the root of the tree which is different from the head node of 4 We use the English head rules from the Stanford parser (Klein and Manning, 2003). 152 the foot node. In the above example this is N. • The width of the span to the left of the foot node, paired with the label of the foot node. • The width of the span to the right of the foot node, paired with the label of the foot node. Scaling of features. The features defined above are almost all binary valued features. We scale the features in the following way. For each feature φi (t), define count(i) to be the number of times the feature is equal to 1, and M to be the number of training examples. The feature is then redefined to be s M φi (t) × count(i) + κ where κ is a smoothing ter"
N13-1015,E12-1042,0,0.573193,"Missing"
N13-1015,J93-2004,0,0.0490646,"ds to the lowest probability parse being output under the model). We suspect that this is because in some cases a dominant parameter has had its sign flipped due to sampling error; more theoretical and empirical work is required in fully understanding this issue. 4 Experiments In this section we describe parsing experiments using the L-PCFG estimation method. We give comparisons to the EM algorithm, considering both speed of training, and accuracy of the resulting model; we also give experiments investigating the various choices described in the previous section. We use the Penn WSJ treebank (Marcus et al., 1993) for our experiments. Sections 2–21 were used as training data, and sections 0 and 22 were used as development data. Section 23 is used as the final test set. We binarize the trees in training data using the same method as that described in Petrov et al. (2006). For example, the non-binary rule VP → V NP PP SBAR would be converted to the structure [VP [@VP [@VP V NP] PP] SBAR] where @VP is a new symbol in the grammar. Unary rules are removed by collapsing non-terminal chains: for example the unary rule S → VP would be replaced by a single non-terminal S|VP. For the EM algorithm we use the init"
N13-1015,D10-1004,0,0.0243085,"example the unary rule S → VP would be replaced by a single non-terminal S|VP. For the EM algorithm we use the initialization method described in Matsuzaki et al. (2005). For efficiency, we use a coarse-to-fine algorithm for parsing with either the EM or spectral derived grammar: a PCFG without latent states is used to calculate marginals, and dynamic programming items are removed if their marginal probability is lower than some threshold (0.00005 in our experiments). For simplicity the parser takes part-of-speech tagged sentences as input. We use automatically tagged data from Turbo Tagger (Martins et al., 2010). The tagger is used to tag both the development data and the test data. The tagger was retrained on sections 2–21. We use the F1 measure according to the Parseval metric (Black et al., 1991). For the spectral algorithm, we tuned the smoothing parameters using section 0 of the treebank. 4.1 Comparison to EM: Accuracy We compare models trained using EM and the spectral algorithm using values for m in {8, 16, 24, 32}.5 For EM, we found that it was important to use development data to choose the number of iterations of training. We train the models for 100 iterations, then test accuracy of the mo"
N13-1015,P05-1010,0,0.673932,"uarantees of sample complexity). This paper describes experiments using the spectral algorithm. We show that the algorithm provides models with the same accuracy as EM, but is an order of magnitude more efficient. We describe a number of key steps used to obtain this level of performance; these should be relevant to other work on the application of spectral learning algorithms. We view our results as strong empirical evidence for the viability of spectral methods as an alternative to EM. 1 Introduction Latent-variable PCFGS (L-PCFGs) are a highly successful model for natural language parsing (Matsuzaki et al., 2005; Petrov et al., 2006). Recent work (Cohen et al., 2012) has introduced a spectral learning algorithm for L-PCFGs. A crucial property of the algorithm is that it is guaranteed to provide consistent parameter estimates—in fact it has PAC-style guarantees of sample complexity.1 This is in contrast to the EM algorithm, the usual method for parameter estimation in L-PCFGs, which has the weaker guarantee of reaching a local maximum of the likelihood function. The spectral algorithm is relatively simple and efficient, relying on a singular value decomposition of the training examples, followed by a"
N13-1015,N07-1051,0,0.036876,"Missing"
N13-1015,P06-1055,0,0.331212,"lexity). This paper describes experiments using the spectral algorithm. We show that the algorithm provides models with the same accuracy as EM, but is an order of magnitude more efficient. We describe a number of key steps used to obtain this level of performance; these should be relevant to other work on the application of spectral learning algorithms. We view our results as strong empirical evidence for the viability of spectral methods as an alternative to EM. 1 Introduction Latent-variable PCFGS (L-PCFGs) are a highly successful model for natural language parsing (Matsuzaki et al., 2005; Petrov et al., 2006). Recent work (Cohen et al., 2012) has introduced a spectral learning algorithm for L-PCFGs. A crucial property of the algorithm is that it is guaranteed to provide consistent parameter estimates—in fact it has PAC-style guarantees of sample complexity.1 This is in contrast to the EM algorithm, the usual method for parameter estimation in L-PCFGs, which has the weaker guarantee of reaching a local maximum of the likelihood function. The spectral algorithm is relatively simple and efficient, relying on a singular value decomposition of the training examples, followed by a single pass over the d"
N13-4005,E12-1042,0,\N,Missing
N13-4005,J92-4003,0,\N,Missing
N13-4005,P06-1055,0,\N,Missing
N13-4005,P05-1010,0,\N,Missing
N13-4005,W99-0613,1,\N,Missing
N13-4005,D12-1019,1,\N,Missing
N13-4005,N13-1015,1,\N,Missing
N13-4005,W97-0309,0,\N,Missing
N13-4005,P96-1024,0,\N,Missing
N15-1009,P11-1061,0,0.0330234,"ion scheme that leverages unlabeled data, we show that our method gives significant improvement over strong supervised and weakly-supervised baselines. 1 Introduction A key problem in natural language processing (NLP) is to effectively utilize large amounts of unlabeled and partially labeled data in situations where little or no annotations are available for a task of interest. Many recent work tackled this problem mostly in the context of part-of-speech (POS) tagging by transferring POS tags from a supervised language via automatic alignment and/or constructing tag dictionaries from the web (Das and Petrov, 2011; Li et al., 2012; T¨ackstr¨om et al., 2013). In this work, we attack this problem in the context of slot tagging, where the goal is to find correct semantic segmentation of a given query, which is an important task for information extraction and natural language understanding. For instance, answering the question “when is the new bill murray movie release date?” requires recognizing and labeling key phrases: e.g., “bill murray” as actor and “movie” as media type. To remedy this limitation, we propose a weakly supervised framework that utilizes the information available in web click logs. A we"
N15-1009,N06-1041,0,0.024639,"4; Sarikaya et al., 2014; Marin et al., 2014). Distant supervision methods (Mintz et al., 2009; Riedel et al., 2010; Surdeanu et al., 2012; Agichtein and Gravano, 2000) learn to extract relations from text using weak supervision from related structured data sources such as Freebase or Wikipedia. These approaches rely on named entity recognition as a pre-processing step to identify text spans corresponding to candidate slot values. In contrast, our approach jointly segments and predicts slots. Works on weakly supervised POS tagging are also closely related to ours (Toutanova and Johnson, 2007; Haghighi and Klein, 2006). T¨ackstr¨om et al. (2013) investigate weakly supervised POS tagging in low-resource languages, combining dictionary constraints and labels projected across languages via parallel corpora and automatic alignment. Our work can be seen as an extension of their approach to the structured-data projection setup presented by Li et al. (2009). A notable component of our extension is that we introduce a training algorithm for learning a hidden unit CRF of Maaten et al. (2011) from partially labeled sequences. This model has a set of binary latent variables that introduce non-linearity by mediating be"
N15-1009,D12-1127,0,0.0437924,"Missing"
N15-1009,P09-1113,0,0.0325752,"red weakly supervised slot tagging using aligned labels from a database as constraints. Wu and Weld (2007) train a CRF on heuristically annotated Wikipedia articles with relations mentioned in their structured infobox data. Li et al. (2009) applied a similar strategy incorporating structured data projected through click-log data as both heuristic labels and additional features. Knowledge graphs and search logs have been also considered as extra resources (Liu et al., 2013; El-Kahky et al., 2014; Anastasakos et al., 2014; Sarikaya et al., 2014; Marin et al., 2014). Distant supervision methods (Mintz et al., 2009; Riedel et al., 2010; Surdeanu et al., 2012; Agichtein and Gravano, 2000) learn to extract relations from text using weak supervision from related structured data sources such as Freebase or Wikipedia. These approaches rely on named entity recognition as a pre-processing step to identify text spans corresponding to candidate slot values. In contrast, our approach jointly segments and predicts slots. Works on weakly supervised POS tagging are also closely related to ours (Toutanova and Johnson, 2007; Haghighi and Klein, 2006). T¨ackstr¨om et al. (2013) investigate weakly supervised POS tagging"
N15-1009,D12-1042,0,0.0198089,"aligned labels from a database as constraints. Wu and Weld (2007) train a CRF on heuristically annotated Wikipedia articles with relations mentioned in their structured infobox data. Li et al. (2009) applied a similar strategy incorporating structured data projected through click-log data as both heuristic labels and additional features. Knowledge graphs and search logs have been also considered as extra resources (Liu et al., 2013; El-Kahky et al., 2014; Anastasakos et al., 2014; Sarikaya et al., 2014; Marin et al., 2014). Distant supervision methods (Mintz et al., 2009; Riedel et al., 2010; Surdeanu et al., 2012; Agichtein and Gravano, 2000) learn to extract relations from text using weak supervision from related structured data sources such as Freebase or Wikipedia. These approaches rely on named entity recognition as a pre-processing step to identify text spans corresponding to candidate slot values. In contrast, our approach jointly segments and predicts slots. Works on weakly supervised POS tagging are also closely related to ours (Toutanova and Johnson, 2007; Haghighi and Klein, 2006). T¨ackstr¨om et al. (2013) investigate weakly supervised POS tagging in low-resource languages, combining dictio"
N15-1009,Q13-1001,0,0.0339637,"Missing"
N15-1009,P13-1164,0,0.028454,"e. To remedy this limitation, we propose a weakly supervised framework that utilizes the information available in web click logs. A web click log is a mapping from a user query to URL link. For example, users issuing queries about movies tend to click on links from the IMDB.com or rottentomatoes.com, which provide rich structured data for entities such as title of the movie (“The Matrix”), the director (“The Wachowski Brothers”), and the release date (“1999”). Web click logs present an opportunity to learn semantic tagging models from large-scale and naturally occurring user interaction data (Volkova et al., 2013). While some previous works (Li et al., 2009) have applied a similar strategy to incorporate click logs in slot tagging, they do not employ recent advances in machine learning to effectively leverage the incomplete annotations. In this paper, we pursue and extend learning from partially labeled sequences, in particular the approach of T¨ackstr¨om et al. (2013). 84 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 84–92, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics Instead of projecting labels fro"
N19-1113,N10-1083,0,0.0931075,"Missing"
N19-1113,J92-4003,0,0.259741,"lnerable. We obtain strong performance on a multitude of datasets and languages with a simple architecture that encodes morphology and context. 1 Introduction We consider information theoretic objectives for POS induction, an important unsupervised learning problem in computational linguistics (Christodoulopoulos et al., 2010). The idea is to make the induced label syntactically informative by maximizing its mutual information with respect to local context. Mutual information has long been a workhorse in the development of NLP techniques, for instance the classical Brown clustering algorithm (Brown et al., 1992). But its role in today’s deep learning paradigm is less clear and a subject of active investigation (Belghazi et al., 2018; Oord et al., 2018). We focus on fully differentiable objectives that can be plugged into an automatic differentiation system and efficiently optimized by SGD. Specifically, we investigate two training objectives. The first is a novel generalization of the Brown clustering objective obtained by relaxing the hard clustering constraint. The second is a recently proposed variational lower bound on mutual information (McAllester, 2018). A main challenge in optimizing these ob"
N19-1113,D10-1056,0,0.327337,"el generalization of the classical Brown clustering objective and a recently proposed variational lower bound. While both objectives are subject to noise in gradient updates, we show through analysis and experiments that the variational lower bound is robust whereas the generalized Brown objective is vulnerable. We obtain strong performance on a multitude of datasets and languages with a simple architecture that encodes morphology and context. 1 Introduction We consider information theoretic objectives for POS induction, an important unsupervised learning problem in computational linguistics (Christodoulopoulos et al., 2010). The idea is to make the induced label syntactically informative by maximizing its mutual information with respect to local context. Mutual information has long been a workhorse in the development of NLP techniques, for instance the classical Brown clustering algorithm (Brown et al., 1992). But its role in today’s deep learning paradigm is less clear and a subject of active investigation (Belghazi et al., 2018; Oord et al., 2018). We focus on fully differentiable objectives that can be plugged into an automatic differentiation system and efficiently optimized by SGD. Specifically, we investig"
N19-1113,J90-1003,0,0.351254,"riate representations. Comparison with CRF autoencoders. Table 5 shows a direct comparison with CRF autoencoders (Ammar et al., 2014; Lin et al., 2015) in manyto-one accuracy and the V-measure. We compare against their reported numbers by running our model once on the same datasets using the same setting in the experiments on the Penn WSJ dataset. The data consists of the training portion of CoNLL-X and CoNLL 2007 labeled with 12 universal tags. Our model is competitive with all baselines. 6 Related Work Information theory, in particular mutual information, has played a prominent role in NLP (Church and Hanks, 1990; Brown et al., 1992). It has intimate connections to the representation learning capabilities of neural networks (Tishby and Zaslavsky, 2015) and underlies many celebrated modern approaches to unsupervised learning such as generative adversarial networks (GANs) (Goodfellow et al., 2014). There is a recent burst of effort in learning continuous representations by optimizing various lower bounds on mutual information (Belghazi et al., 2018; Oord et al., 2018; Hjelm et al., 2018). These representations are typically eval1101 Method Variational Jbvar (7) Stratos et al. Berg-Kirkpatrick et al. Bro"
N19-1113,D18-1160,0,0.0136759,"he minibatch size is as large as 10,000 the gradient steps do not effectively increase the true data-wide mutual information (4). This supports our bias analysis in Section 4. While it may be possible to develop techniques to resolve the difficulty, for instance keeping a moving average of estimates to stabilize estimation, we leave this as future work and focus on the variational objective in the remainder of the paper. Table 2 shows ablation experiments on our best 4 We remark that Tran et al. (2016) report a single number 79.1 with a neuralized HMM. We also note that the concurrent work by He et al. (2018) obtains 80.8 by using word embeddings carefully pretrained on one billion words. 1100 Method Variational Jbvar (7) Generalized Brown Jbmi (4) Berg-Kirkpatrick et al. (2010) Stratos et al. (2016) Brown et al. (1992) Baum-Welch k- MEANS Cosine Similarity 0.7426 0.7113 0.6851 0.6613 0.6584 0.6574 0.6533 0.6521 0.6514 0.6494 0.6486 Accuracy 78.1 (±0.8) 48.8 (±0.9) 74.9 (±1.5) 67.7 65.6 62.6 (±1.1) 32.6 (±0.7) Table 1: Many-to-one accuracy on the 45-tag Penn WSJ with the best hyperparameter configurations. The average accuracy over 10 random restarts is reported and the standard deviation is given"
N19-1113,P08-1068,0,0.0705984,"it assumes a uniform distribution over consecutive word pairs (xi−1 , xi ) and optimizes the following empirical objective   X #(c, c0 ) #(c, c0 )N max log (3) N #(c)#(c0 ) C:V →[m] 0 c,c ∈[m] where #(c, c0 ) denotes the number of occurrences of the cluster pair (c, c0 ) under C. While this optimization is intractable, Brown et al. (1992) derive an effective heuristic that 1. initializes m most frequent words as singleton clusters and 2. repeatedly merges a pair of clusters that yields the smallest decrease in mutual information. The resulting clusters have been useful in many applications (Koo et al., 2008; Owoputi et al., 2013) and has remained a strong baseline for POS induction decades later (Christodoulopoulos et al., 2010). But the approach is tied to highly nontrivial combinatorial optimization tailored for the specific problem and difficult to scale/generalize. 3 Objectives In the remainder of the paper, we assume discrete random variables (X, Y ) ∈ X ×Y with a joint distribution D that represent naturally co-occurring observations. In POS induction experiments, we will set D to be a context-word distribution where Y is a random word and X is the surrounding context of Y (thus Y is the v"
N19-1113,N15-1144,0,0.0907505,"with the variational bound and the generalized Brown objectives. We only show the result with the gradient with respect to q, but the result with the gradient with respect to p is analogous and omitted for brevity. Theorem 4.1. Assume the setting in Section 4.1 and the gradient is taken with respect to the pa1098 rameters of q. For lN = −Jbvar defined in (7), = ground-truth POS tag in the annotated data and report the resulting accuracy. We also use the V-measure (Rosenberg and Hirschberg, 2007) when comparing with CRF autoencoders to be consistent with reported results (Ammar et al., 2014; Lin et al., 2015). K qˆ(z) 1 XX log ∇ˆ qk (z) K qˆk (z) z k=1 On the other hand, for lN = −Jbmi defined in (4), = • We use the number of ground-truth POS tags as the value of m (i.e., number of labels to induce). This is a data-dependent quantity, for instance 45 in the Penn WSJ and 12 in the universal treebank. Fixing the number of tags this way obviates many evaluation issues.  K 1 XX k (z, z 0 )∇ˆ qk (z 0 ) N 0 k=1 z,z + log pˆ(z)ˆ q (z 0 ) pˆk (z)ˆ qk (z 0 ) X p(z|x)∇q(z 0 |y)  (x,y)∈Bk where k (z, z 0 ) = 1 K − • Model-specific hyperparameters are tuned on the English Penn WSJ dataset. This configura"
N19-1113,P13-2017,0,0.0710685,"Missing"
N19-1113,J94-2001,0,0.114928,"ariational objective (7) or the generalized Brown objective (4) by taking gradient steps at random minibatches. This gives us conditional label distributions p(z|x) and q(z|y) for all contexts x, words y, and labels z. At test time, we use z ∗ = arg max q(z|y) z as the induced label of word y. We experimented with different inference methods such as taking arg maxz p(z|x)q(z|y) but did not find it helpful. 5.2 Experiments We demonstrate the effectiveness of our training objectives on the task of POS induction. The goal of this task is to induce the correct POS tag for a given word in context (Merialdo, 1994). As typical in unsupervised tasks, evaluating the quality of induced labels is challenging; see Christodoulopoulos et al. (2010) for an in-depth discussion. To avoid complications, we follow a standard practice (Berg-Kirkpatrick et al., 2010; Ammar et al., 2014; Lin et al., 2015; Stratos et al., 2016) and adopt the following setting for all compared methods. • We use many-to-one accuracy as a primary evaluation metric. That is, we map each induced label to the most frequently coinciding Setting Definition of (X, Y ) Let V denote the vocabulary. We assume an integer H ≥ 1 that specifies the wi"
N19-1113,N13-1039,0,0.0894145,"Missing"
N19-1113,D14-1162,0,0.0825477,"rds from a structured label sequence. Lin et al. (2015) extend Ammar et al. (2014) by switching a categorical reconstruction distribution with a Gaussian distribution. In addition to these 3 An implementation is available at: https:// github.com/karlstratos/mmi-tagger. had keys these k e in y my pocket s BiLSTM Figure 1: Architecture illustrated on the example text “had these keys in my” with target Y = “keys”. baselines, we also report results with Brown clustering (Brown et al., 1992), the Baum-Welch algorithm (Baum and Petrie, 1966), and k-means clustering of 300-dimensional GloVe vectors (Pennington et al., 2014). 5.5 (f1 . . . fT ) = LSTMf (ec1 . . . ecT ) 5.4 I Results The 45-tag Penn WSJ dataset. The 45-tag Penn WSJ dataset is a corpus of around one million words each tagged with one of m = 45 tags. It is used to optimize hyperparameter values for all compared methods. Table 1 shows the average accuracy over 10 random restarts with the best hyperparameter configurations; standard deviation is given in parentheses (except for deterministic methods Stratos et al. (2016) and Brown clustering). Our model trained with the variational objective (7) outperforms all baselines.4 We also observe that our mod"
N19-1113,D07-1043,0,0.0753637,"ic optimization will be effective. Result The following theorem precisely quantifies the bias for the empirical losses associated with the variational bound and the generalized Brown objectives. We only show the result with the gradient with respect to q, but the result with the gradient with respect to p is analogous and omitted for brevity. Theorem 4.1. Assume the setting in Section 4.1 and the gradient is taken with respect to the pa1098 rameters of q. For lN = −Jbvar defined in (7), = ground-truth POS tag in the annotated data and report the resulting accuracy. We also use the V-measure (Rosenberg and Hirschberg, 2007) when comparing with CRF autoencoders to be consistent with reported results (Ammar et al., 2014; Lin et al., 2015). K qˆ(z) 1 XX log ∇ˆ qk (z) K qˆk (z) z k=1 On the other hand, for lN = −Jbmi defined in (4), = • We use the number of ground-truth POS tags as the value of m (i.e., number of labels to induce). This is a data-dependent quantity, for instance 45 in the Penn WSJ and 12 in the universal treebank. Fixing the number of tags this way obviates many evaluation issues.  K 1 XX k (z, z 0 )∇ˆ qk (z 0 ) N 0 k=1 z,z + log pˆ(z)ˆ q (z 0 ) pˆk (z)ˆ qk (z 0 ) X p(z|x)∇q(z 0 |y)  (x,y)∈Bk wh"
N19-1113,Q16-1018,1,0.824706,"experimented with different inference methods such as taking arg maxz p(z|x)q(z|y) but did not find it helpful. 5.2 Experiments We demonstrate the effectiveness of our training objectives on the task of POS induction. The goal of this task is to induce the correct POS tag for a given word in context (Merialdo, 1994). As typical in unsupervised tasks, evaluating the quality of induced labels is challenging; see Christodoulopoulos et al. (2010) for an in-depth discussion. To avoid complications, we follow a standard practice (Berg-Kirkpatrick et al., 2010; Ammar et al., 2014; Lin et al., 2015; Stratos et al., 2016) and adopt the following setting for all compared methods. • We use many-to-one accuracy as a primary evaluation metric. That is, we map each induced label to the most frequently coinciding Setting Definition of (X, Y ) Let V denote the vocabulary. We assume an integer H ≥ 1 that specifies the width of local context. Given random word y ∈ V , we set x ∈ V 2H to be an ordered list of H left and H right words of y. For example, with H = 2, a typical context-target pair (x, y) ∼ D may look like x = (“had”, “these”, “in”, “my”) y = “keys” We find this simple fixed-window definition of observed var"
N19-1113,W16-5907,0,0.082282,"erve that our model trained with the generalized Brown objective (4) does not work. We have found that unless the minibatch size is as large as 10,000 the gradient steps do not effectively increase the true data-wide mutual information (4). This supports our bias analysis in Section 4. While it may be possible to develop techniques to resolve the difficulty, for instance keeping a moving average of estimates to stabilize estimation, we leave this as future work and focus on the variational objective in the remainder of the paper. Table 2 shows ablation experiments on our best 4 We remark that Tran et al. (2016) report a single number 79.1 with a neuralized HMM. We also note that the concurrent work by He et al. (2018) obtains 80.8 by using word embeddings carefully pretrained on one billion words. 1100 Method Variational Jbvar (7) Generalized Brown Jbmi (4) Berg-Kirkpatrick et al. (2010) Stratos et al. (2016) Brown et al. (1992) Baum-Welch k- MEANS Cosine Similarity 0.7426 0.7113 0.6851 0.6613 0.6584 0.6574 0.6533 0.6521 0.6514 0.6494 0.6486 Accuracy 78.1 (±0.8) 48.8 (±0.9) 74.9 (±1.5) 67.7 65.6 62.6 (±1.1) 32.6 (±0.7) Table 1: Many-to-one accuracy on the 45-tag Penn WSJ with the best hyperparameter"
P12-1024,P96-1024,0,0.812379,"rm for calculation of p(r1 . . . rN ). 1. For a given p(r1 . . . rN ). s-tree r1 . . . rN , calculate 2. For a given input sentence x = x1 . . . xN , calculate the marginal probabilities X µ(a, i, j) = p(τ ) τ ∈T (x):(a,i,j)∈τ for each non-terminal a ∈ N , for each (i, j) such that 1 ≤ i ≤ j ≤ N . Here T (x) denotes the set of all possible s-trees for the sentence x, and we write (a, i, j) ∈ τ if nonterminal a spans words xi . . . xj in the parse tree τ . The marginal probabilities have a number of uses. Perhaps most importantly, for a given sentence x = x1 . . . xN , the parsing algorithm of Goodman (1996) can be used to find X arg max µ(a, i, j) τ ∈T (x) (a,i,j)∈τ This is the parsing algorithm used by Petrov et al. (2006), for example. In addition, we can calculate the probability for anPinput sentence, p(x) = P τ ∈T (x) p(τ ), as p(x) = a∈I µ(a, 1, N ). Variants of the inside-outside algorithm can be used for problems 1 and 2. This section introduces a novel form of these algorithms, using tensors. This is the first step in deriving the spectral estimation method. The algorithms are shown in figures 2 and 3. Each algorithm takes the following inputs: 5 Tensor Form of the Inside-Outside Algori"
P12-1024,E12-1042,0,0.119777,"The tensor form of the insideoutside algorithm gives a new view of basic calculations in PCFGs, and may itself lead to new models. 2 Related Work For work on L-PCFGs using the EM algorithm, see Petrov et al. (2006), Matsuzaki et al. (2005), Pereira and Schabes (1992). Our work builds on methods for learning of HMMs (Hsu et al., 2009; Foster et al., 2012; Jaeger, 2000), but involves several extensions: in particular in the tensor form of the inside-outside algorithm, and observable representations for the tensor form. Balle et al. (2011) consider spectral learning of finite-state transducers; Lugue et al. (2012) considers spectral learning of head automata for dependency parsing. Parikh et al. (2011) consider spectral learning algorithms of treestructured directed bayes nets. 3 Notation 4 L-PCFGs: Basic Definitions This section gives a definition of the L-PCFG formalism used in this paper. An L-PCFG is a 5-tuple (N , I, P, m, n) where: • N is the set of non-terminal symbols in the grammar. I ⊂ N is a finite set of in-terminals. P ⊂ N is a finite set of pre-terminals. We assume that N = I ∪ P, and I ∩ P = ∅. Hence we have partitioned the set of non-terminals into two subsets. • [m] is the set of possi"
P12-1024,P05-1010,0,0.93,"ar value decomposition (SVD). In the general case, learning of HMMs or GMMs is intractable (e.g., see Terwijn, 2002). Spectral methods finesse the problem of intractibility by assuming separability conditions. For example, the algorithm of Hsu et al. (2009) has a sample complexity that is polynomial in 1/σ, where σ is the minimum singular value of an underlying decomposition. These methods are not susceptible to problems with local maxima, and give consistent parameter estimates. In this paper we derive a spectral algorithm for learning of latent-variable PCFGs (L-PCFGs) (Petrov et al., 2006; Matsuzaki et al., 2005). Our method involves a significant extension of the techniques from Hsu et al. (2009). L-PCFGs have been shown to be a very effective model for natural language parsing. Under a separation (singular value) condition, our algorithm provides consistent parameter estimates; this is in contrast with previous work, which has used the EM algorithm for parameter estimation, with the usual problems of local optima. The parameter estimation algorithm (see figure 4) is simple and efficient. The first step is to take an SVD of the training examples, followed by a projection of the training examples down"
P12-1024,P92-1017,0,0.508816,"tion for Computational Linguistics, pages 223–231, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics veloped in this paper are quite general, and should be relevant to the development of spectral methods for estimation in other models in NLP, for example alignment models for translation, synchronous PCFGs, and so on. The tensor form of the insideoutside algorithm gives a new view of basic calculations in PCFGs, and may itself lead to new models. 2 Related Work For work on L-PCFGs using the EM algorithm, see Petrov et al. (2006), Matsuzaki et al. (2005), Pereira and Schabes (1992). Our work builds on methods for learning of HMMs (Hsu et al., 2009; Foster et al., 2012; Jaeger, 2000), but involves several extensions: in particular in the tensor form of the inside-outside algorithm, and observable representations for the tensor form. Balle et al. (2011) consider spectral learning of finite-state transducers; Lugue et al. (2012) considers spectral learning of head automata for dependency parsing. Parikh et al. (2011) consider spectral learning algorithms of treestructured directed bayes nets. 3 Notation 4 L-PCFGs: Basic Definitions This section gives a definition of the L-"
P12-1024,P06-1055,0,0.878323,"in particular singular value decomposition (SVD). In the general case, learning of HMMs or GMMs is intractable (e.g., see Terwijn, 2002). Spectral methods finesse the problem of intractibility by assuming separability conditions. For example, the algorithm of Hsu et al. (2009) has a sample complexity that is polynomial in 1/σ, where σ is the minimum singular value of an underlying decomposition. These methods are not susceptible to problems with local maxima, and give consistent parameter estimates. In this paper we derive a spectral algorithm for learning of latent-variable PCFGs (L-PCFGs) (Petrov et al., 2006; Matsuzaki et al., 2005). Our method involves a significant extension of the techniques from Hsu et al. (2009). L-PCFGs have been shown to be a very effective model for natural language parsing. Under a separation (singular value) condition, our algorithm provides consistent parameter estimates; this is in contrast with previous work, which has used the EM algorithm for parameter estimation, with the usual problems of local optima. The parameter estimation algorithm (see figure 4) is simple and efficient. The first step is to take an SVD of the training examples, followed by a projection of t"
P12-1024,J98-4004,0,\N,Missing
P12-1024,J93-2004,0,\N,Missing
P12-1024,P97-1003,1,\N,Missing
P12-1024,P03-1054,0,\N,Missing
P15-1046,W06-1615,0,0.764894,"ions because they assume that the label set is invariant. We propose a solution based on label embeddings induced from canonical correlation analysis (CCA) that reduces the problem to a standard domain adaptation task and allows use of a number of transfer learning techniques. We also introduce a new transfer learning technique based on pretraining of hidden-unit CRFs (HUCRFs). We perform extensive experiments on slot tagging on eight personal digital assistant domains and demonstrate that the proposed methods are superior to strong baselines. 1 One might attempt to apply existing techniques (Blitzer et al., 2006; Daum´e III, 2007) in domain adaption to this problem, but a straightforward application is not possible because these techniques assume that the label set is invariant. In this work, we provide a simple and effective solution to this problem by abstracting the label types using the canonical correlation analysis (CCA) by Hotelling (Hotelling, 1936) a powerful and flexible statistical technique for dimensionality reduction. We derive a low dimensional representation for each label type that is maximally correlated to the average context of that label via CCA. These shared label representation"
P15-1046,N09-1068,0,0.0197997,"ion objective is to optimize the performance of semantic slot tagger on the target domain. Below, we review related work in domain adaption and natural language understanding (NLU). 2.1 Related Work Domain adaptation has been widely used in many natural language processing (NLP) applications including part-of-speech tagging (Schnabel and Sch¨utze, 2014), parsing (McClosky et al., 2010), and machine translation (Foster et al., 2010). Most of the work can be classified either supervised domain adaptation (Chelba and Acero, 2006; Blitzer et al., 2006; Daume III and Marcu, 2006; Daum´e III, 2007; Finkel and Manning, 2009; Chen et al., 2011) or semi-supervised adaptation (Ando and Zhang, 2005; Jiang and Zhai, 2007; Kumar et al., 2010; Huang and Yates, 2010). Our problem setting falls into the former. Multi-task learning has become popular in NLP. Sutton and McCallum (2005) showed that joint 3 Sequence Modeling Technique The proposed techniques in Section 4 and 5 are generic methodologies and not tied to any particular models such as any sequence models and instanced based models. However, because of superior performance over CRF, we use a hidden unit CRF (HUCRF) of Maaten et al. (2011). 1 Multilingual learning"
P15-1046,D10-1044,0,0.0119326,"ly but output spaces (slot tags) might. Multi-task learning differs from our task. In general multi-task learning aims to improve performance across all domains while our domain adaptation objective is to optimize the performance of semantic slot tagger on the target domain. Below, we review related work in domain adaption and natural language understanding (NLU). 2.1 Related Work Domain adaptation has been widely used in many natural language processing (NLP) applications including part-of-speech tagging (Schnabel and Sch¨utze, 2014), parsing (McClosky et al., 2010), and machine translation (Foster et al., 2010). Most of the work can be classified either supervised domain adaptation (Chelba and Acero, 2006; Blitzer et al., 2006; Daume III and Marcu, 2006; Daum´e III, 2007; Finkel and Manning, 2009; Chen et al., 2011) or semi-supervised adaptation (Ando and Zhang, 2005; Jiang and Zhai, 2007; Kumar et al., 2010; Huang and Yates, 2010). Our problem setting falls into the former. Multi-task learning has become popular in NLP. Sutton and McCallum (2005) showed that joint 3 Sequence Modeling Technique The proposed techniques in Section 4 and 5 are generic methodologies and not tied to any particular models"
P15-1046,W10-2604,0,0.00808131,"and natural language understanding (NLU). 2.1 Related Work Domain adaptation has been widely used in many natural language processing (NLP) applications including part-of-speech tagging (Schnabel and Sch¨utze, 2014), parsing (McClosky et al., 2010), and machine translation (Foster et al., 2010). Most of the work can be classified either supervised domain adaptation (Chelba and Acero, 2006; Blitzer et al., 2006; Daume III and Marcu, 2006; Daum´e III, 2007; Finkel and Manning, 2009; Chen et al., 2011) or semi-supervised adaptation (Ando and Zhang, 2005; Jiang and Zhai, 2007; Kumar et al., 2010; Huang and Yates, 2010). Our problem setting falls into the former. Multi-task learning has become popular in NLP. Sutton and McCallum (2005) showed that joint 3 Sequence Modeling Technique The proposed techniques in Section 4 and 5 are generic methodologies and not tied to any particular models such as any sequence models and instanced based models. However, because of superior performance over CRF, we use a hidden unit CRF (HUCRF) of Maaten et al. (2011). 1 Multilingual learning (Kim et al., 2011; Kim and Snyder, 2012; Kim and Snyder, 2013) has same setting. 2 Jeong and Lee (2009) pointed out that if the domain is"
P15-1046,N10-1004,0,0.0166861,"rvice request utterances) do not change drastically but output spaces (slot tags) might. Multi-task learning differs from our task. In general multi-task learning aims to improve performance across all domains while our domain adaptation objective is to optimize the performance of semantic slot tagger on the target domain. Below, we review related work in domain adaption and natural language understanding (NLU). 2.1 Related Work Domain adaptation has been widely used in many natural language processing (NLP) applications including part-of-speech tagging (Schnabel and Sch¨utze, 2014), parsing (McClosky et al., 2010), and machine translation (Foster et al., 2010). Most of the work can be classified either supervised domain adaptation (Chelba and Acero, 2006; Blitzer et al., 2006; Daume III and Marcu, 2006; Daum´e III, 2007; Finkel and Manning, 2009; Chen et al., 2011) or semi-supervised adaptation (Ando and Zhang, 2005; Jiang and Zhai, 2007; Kumar et al., 2010; Huang and Yates, 2010). Our problem setting falls into the former. Multi-task learning has become popular in NLP. Sutton and McCallum (2005) showed that joint 3 Sequence Modeling Technique The proposed techniques in Section 4 and 5 are generic meth"
P15-1046,P07-1034,0,0.0762092,"we review related work in domain adaption and natural language understanding (NLU). 2.1 Related Work Domain adaptation has been widely used in many natural language processing (NLP) applications including part-of-speech tagging (Schnabel and Sch¨utze, 2014), parsing (McClosky et al., 2010), and machine translation (Foster et al., 2010). Most of the work can be classified either supervised domain adaptation (Chelba and Acero, 2006; Blitzer et al., 2006; Daume III and Marcu, 2006; Daum´e III, 2007; Finkel and Manning, 2009; Chen et al., 2011) or semi-supervised adaptation (Ando and Zhang, 2005; Jiang and Zhai, 2007; Kumar et al., 2010; Huang and Yates, 2010). Our problem setting falls into the former. Multi-task learning has become popular in NLP. Sutton and McCallum (2005) showed that joint 3 Sequence Modeling Technique The proposed techniques in Section 4 and 5 are generic methodologies and not tied to any particular models such as any sequence models and instanced based models. However, because of superior performance over CRF, we use a hidden unit CRF (HUCRF) of Maaten et al. (2011). 1 Multilingual learning (Kim et al., 2011; Kim and Snyder, 2012; Kim and Snyder, 2013) has same setting. 2 Jeong and"
P15-1046,D12-1031,1,0.858892,"Missing"
P15-1046,P13-1150,1,0.853578,"Missing"
P15-1046,Q14-1002,0,0.0311458,"Missing"
P15-1046,D11-1030,1,0.907365,"Missing"
P15-1046,H05-1094,0,0.299522,"language processing (NLP) applications including part-of-speech tagging (Schnabel and Sch¨utze, 2014), parsing (McClosky et al., 2010), and machine translation (Foster et al., 2010). Most of the work can be classified either supervised domain adaptation (Chelba and Acero, 2006; Blitzer et al., 2006; Daume III and Marcu, 2006; Daum´e III, 2007; Finkel and Manning, 2009; Chen et al., 2011) or semi-supervised adaptation (Ando and Zhang, 2005; Jiang and Zhai, 2007; Kumar et al., 2010; Huang and Yates, 2010). Our problem setting falls into the former. Multi-task learning has become popular in NLP. Sutton and McCallum (2005) showed that joint 3 Sequence Modeling Technique The proposed techniques in Section 4 and 5 are generic methodologies and not tied to any particular models such as any sequence models and instanced based models. However, because of superior performance over CRF, we use a hidden unit CRF (HUCRF) of Maaten et al. (2011). 1 Multilingual learning (Kim et al., 2011; Kim and Snyder, 2012; Kim and Snyder, 2013) has same setting. 2 Jeong and Lee (2009) pointed out that if the domain is given, their method is the same as that of Daum´e III (2007). 474 As in restricted Boltzmann machines (Larochelle and"
P15-1046,N15-1009,1,0.310738,"s. First, we aim to optimize performance on the target domain by minimizing the gap between source and target domain while multi-task learning jointly learns the shared tasks. Second, in our problem the domains are different, but they are closely related. On the other hand, prior work focuses on multiple subtasks of the same data. Despite the increasing interest in NLU (De Mori et al., 2008; Xu and Sarikaya, 2013; Sarikaya et al., 2014; Xu and Sarikaya, 2014; Anastasakos et al., 2014; El-Kahky et al., 2014; Liu and Sarikaya, 2014; Marin et al., 2014; Celikyilmaz et al., 2015; Ma et al., 2015; Kim et al., 2015), transfer learning in the context of NLU has not been much explored. The most relevant previous work is Tur (2006) and Li et al. (2011), which described both the effectiveness of multi-task learning in the context of NLU. For multi-task learning, they used shared slots by associating each slot type with aggregate active feature weight vector based on an existing domain specific slot tagger. Our empirical results shows that these vector representation might be helpful to find shared slots across domain, but cannot find bijective mapping between domains. Also, Jeong and Lee (2009) presented a t"
P15-1046,P07-1033,0,\N,Missing
P15-1124,D14-1082,0,0.00594471,"beddings, but do not establish any explicit connection to learning HMM parameters or justify the squareroot transformation. Pennington et al. (2014) propose a weighted factorization of log-transformed co-occurrence counts, which is generally an intractable problem (Srebro et al., 2003). In contrast, our method requires only efficiently computable matrix decompositions. Finally, word embeddings have also been used as features to improve performance in a variety of supervised tasks such as sequence labeling (Dhillon et al., 2011; Collobert et al., 2011) and dependency parsing (Lei et al., 2014; Chen and Manning, 2014). Here, we focus on understanding word embeddings in the context of a generative word class model, as well as in empirical tasks that directly evaluate the word embeddings themselves. 7 7.1 Experiments Word similarity and analogy We first consider word similarity and analogy tasks for evaluating the quality of word embeddings. Word similarity measures the Spearman’s correlation coefficient between the human scores and the embeddings’ cosine similarities for word pairs. Word analogy measures the accuracy on syntactic and semantic analogy questions. We refer to Levy and Goldberg (2014a) for a de"
P15-1124,J92-4003,0,0.589011,"ce matrices E[XX > ] and E[Y Y > ] are diagonal. This follows from our definition of the word and context variables as one-hot encodings since E[Xw Xw0 ] = 0 for w 6= w0 and E[Yc Yc0 ] = 0 for c 6= c0 . With these observations and the binary definition of (X, Y ), each entry in Ω now has a simple closed-form solution: P (Xw = 1, Yc = 1) Ωw,c = p P (Xw = 1)P (Yc = 1) Using CCA for parameter estimation In a less well-known interpretation of Eq. (4), CCA is seen as a parameter estimation algorithm for a language model (Stratos et al., 2014). This model is a restricted class of HMMs introduced by Brown et al. (1992), henceforth called the Brown model. In this section, we extend the result of Stratos et al. (2014) and show that its correctness is preserved under certain element-wise data transformations. 4.1 Importantly, the model makes the following additional assumption: Assumption 4.1 (Brown assumption). For each word type w ∈ [n], there is a unique hidden state H(w) ∈ [m] such that o(w|H(w)) > 0 and o(w|h) = 0 for all h 6= H(w). In other words, this model is an HMM in which observation states are partitioned by hidden states. Thus a sequence of N words w1 . . . wN ∈ [n]N QN has probability π(H(w1 )) ×"
P15-1124,N13-1015,1,0.150368,"imensions Dev Test 90.04 84.40 92.49 88.75 92.27 88.87 92.51 88.08 92.25 89.27 92.88 89.28 91.49 87.16 92.44 88.34 92.63 88.78 50 dimensions Dev Test 90.04 84.40 92.49 88.75 92.91 89.67 92.73 88.88 92.53 89.37 92.94 89.01 91.58 86.80 92.83 89.21 93.11 89.32 Table 4: NER F1 scores when word embeddings are added as features to the baseline (—). Our proposed method gives the best result among spectral methods and is competitive to other popular word embedding techniques. This work suggests many directions for future work. Past spectral methods that involved CCA without data transformation (e.g., Cohen et al. (2013)) may be revisited with the square-root transformation. Using CCA to induce representations other than word embeddings is another important future work. It would also be interesting to formally investigate the theoretical merits and algorithmic possibility of solving the varianceweighted objective in Eq. (6). Even though the objective is hard to optimize in the worst case, it may be tractable under natural conditions. Acknowledgments We thank Omer Levy, Yoav Goldberg, and David Belanger for helpful discussions. This work was made possible by a research grant from Bloomberg’s Knowledge Engineer"
P15-1124,D14-1162,0,0.139314,"= 0.5. However, it has been found by many (including ourselves) that setting β = 1 yields substantially worse representations than setting β ∈ {0, 0.5} (Levy et al., 2015). Different combinations of these aspects reproduce various spectral embeddings explored in the literature. We enumerate some meaningful combinations: if t = — if t = log if t = two-thirds if t = sqrt 4. Define v(w) ∈ Rm to be the w-th row of U Σβ normalized to have unit 2-norm. Figure 2: A template for spectral word embedding methods.   No scaling t ∈ {—, log, sqrt}, s = — . This is a commonly considered setting (e.g., in Pennington et al. (2014)) where no scaling is applied to the co-occurrence counts. It is however typically accompanied with some kind of data transformation. Positive point-wise mutual information (PPMI)  t = —, s = ppmi . Mutual information is a popular metric in many natural language tasks (Brown et al., 1992; Pantel and Lin, 2002). In this setting, each term in the matrix for SVD is set as the pointwise mutual information between word w and context c: P #(w, c) c #(c)α pˆ(w, c) log = log pˆ(w)ˆ pα (c) #(w)#(c)α Typically negative values are thresholded to 0 to keep Ω sparse. Levy and Goldberg (2014b) observed th"
P15-1124,P14-1130,0,0.00968514,"to derive word embeddings, but do not establish any explicit connection to learning HMM parameters or justify the squareroot transformation. Pennington et al. (2014) propose a weighted factorization of log-transformed co-occurrence counts, which is generally an intractable problem (Srebro et al., 2003). In contrast, our method requires only efficiently computable matrix decompositions. Finally, word embeddings have also been used as features to improve performance in a variety of supervised tasks such as sequence labeling (Dhillon et al., 2011; Collobert et al., 2011) and dependency parsing (Lei et al., 2014; Chen and Manning, 2014). Here, we focus on understanding word embeddings in the context of a generative word class model, as well as in empirical tasks that directly evaluate the word embeddings themselves. 7 7.1 Experiments Word similarity and analogy We first consider word similarity and analogy tasks for evaluating the quality of word embeddings. Word similarity measures the Spearman’s correlation coefficient between the human scores and the embeddings’ cosine similarities for word pairs. Word analogy measures the accuracy on syntactic and semantic analogy questions. We refer to Levy and"
P15-1124,W14-1618,0,0.652131,"ethods such as WORD2VEC and GLOVE. 1 Introduction The recent spike of interest in dense, lowdimensional lexical representations—i.e., word embeddings—is largely due to their ability to capture subtle syntactic and semantic patterns that are useful in a variety of natural language tasks. A successful method for deriving such embeddings is the negative sampling training of the skip-gram model suggested by Mikolov et al. (2013b) and implemented in the popular software WORD2VEC. The form of its training objective was motivated by efficiency considerations, but has subsequently been interpreted by Levy and Goldberg (2014b) as seeking a low-rank factorization of a matrix whose entries are word-context co-occurrence counts, scaled and transformed in a certain way. This observation sheds new light on WORD2VEC, yet also raises several new questions about word embeddings based on decomposing count data. What is the right matrix to decompose? Are there rigorous justifications for the choice of matrix and count transformations? In this paper, we answer some of these questions by investigating the decomposition specified by CCA (Hotelling, 1936), a powerful technique for inducing generic representations whose computa"
P15-1124,Q15-1016,0,0.192846,"Note that the choice of α only affects methods that make use of the context distribution (s ∈ {ppmi, cca}). The parameter β controls the role of singular values in word embeddings. This is always 0 for CCA as it does not require singular values. But for other methods, one can consider setting β > 0 since the best-fit subspace for the rows of Ω is given by U Σ. For example, Deerwester et al. (1990) use β = 1 and Levy and Goldberg (2014b) use β = 0.5. However, it has been found by many (including ourselves) that setting β = 1 yields substantially worse representations than setting β ∈ {0, 0.5} (Levy et al., 2015). Different combinations of these aspects reproduce various spectral embeddings explored in the literature. We enumerate some meaningful combinations: if t = — if t = log if t = two-thirds if t = sqrt 4. Define v(w) ∈ Rm to be the w-th row of U Σβ normalized to have unit 2-norm. Figure 2: A template for spectral word embedding methods.   No scaling t ∈ {—, log, sqrt}, s = — . This is a commonly considered setting (e.g., in Pennington et al. (2014)) where no scaling is applied to the co-occurrence counts. It is however typically accompanied with some kind of data transformation. Positive poin"
P15-2032,P14-2104,1,0.612968,"Missing"
P15-2032,P13-1090,1,0.878428,"Missing"
P15-2032,N15-1009,1,0.34724,"Missing"
P15-2032,W02-1001,0,0.122276,"hnique. 1 Introduction Despite the recent accuracy gains of the deep learning techniques for sequence tagging problems (Collobert and Weston, 2008; Collobert et al., 2011; Mohamed et al., 2010; Deoras et al., 2012; Xu and Sarikaya, 2013; Yao et al., 2013; Mesnil et al., 2013; Wang and Manning, 2013; Devlin et al., 2014), conditional random fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) still have been widely used in many research and production systems for the problems due to the effectiveness and simplicity of training, which does not involve task specific parameter tuning (Collins, 2002; McCallum and Li, 2003; Sha and Pereira, 2003; Turian et al., 2010; Kim and Snyder, 2012; Celikyilmaz et al., 2013; Sarikaya et al., 2014; Anastasakos et al., 2014; 192 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 192–198, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Figure 2: Illustration of a pre-training scheme for HUCRFs. 0 θ ∈ Rd and γ ∈ Rd and defines a joint probability of y and z conditioned on x as follows: Fi"
P15-2032,P15-2132,1,0.543027,"Missing"
P15-2032,P15-1046,1,0.471341,"Missing"
P15-2032,P14-1129,0,0.077615,"Missing"
P15-2032,W03-0430,0,0.0482054,"duction Despite the recent accuracy gains of the deep learning techniques for sequence tagging problems (Collobert and Weston, 2008; Collobert et al., 2011; Mohamed et al., 2010; Deoras et al., 2012; Xu and Sarikaya, 2013; Yao et al., 2013; Mesnil et al., 2013; Wang and Manning, 2013; Devlin et al., 2014), conditional random fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) still have been widely used in many research and production systems for the problems due to the effectiveness and simplicity of training, which does not involve task specific parameter tuning (Collins, 2002; McCallum and Li, 2003; Sha and Pereira, 2003; Turian et al., 2010; Kim and Snyder, 2012; Celikyilmaz et al., 2013; Sarikaya et al., 2014; Anastasakos et al., 2014; 192 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 192–198, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Figure 2: Illustration of a pre-training scheme for HUCRFs. 0 θ ∈ Rd and γ ∈ Rd and defines a joint probability of y and z conditioned on x as follows: Figure 1: Graphical repre"
P15-2032,N04-1043,0,0.0547072,"bles are maximally correlated. 0 Let x(1) . . . x(n) ∈ Rd and y (1) . . . y (n) ∈ Rd be n samples of the two variables. For simplicity, assume that these variables have zero mean. Then CCA computes the following for i = 1 . . . k: Multi-Sense Clustering via CCA The proposed pre-training method requires assigning a cluster to each word in unlabeled text. Since it learns to associate the words to their clusters, the quality of clusters becomes important. A straightforward approach would be to perform Brown clustering (Brown et al., 1992), which has been very effective in a variety of NLP tasks (Miller et al., 2004; Koo et al., 2008). However, Brown clustering has some undesirable aspects for our purpose. First, it assigns a single cluster to each word type. Thus a word that can be used very differently depending on its context (e.g., “bank”) is treated the same across the corpus. Second, the Brown model uses only unigram and bigram statistics; this can be an issue if we wish to capture semantics in larger contexts. Finally, the algorithm is rather slow in practice for large vocabulary size. To mitigate these limitations, we propose multisense clustering via canonical correlation analysis (CCA). While t"
P15-2032,P12-1092,0,0.0375882,"ialization as well, we choose to only use θ1 since the label space is taskspecific. This process is illustrated in Figure 2. In summary, the first step is used to find generic parameters between observations and hidden states; the second step is used to specialize the parameters to a particular task. Note that the first step also generates additional feature types absent in the labeled data which can be useful at test time. 3 n X (l) [[yi = 1]] i=1 In the second step, we train a final model on the labeled data {(x(i) , y (i) )}N i=1 using θ1 as an initialization point: θ,γ: vi = Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014), our proposed method is simpler and is shown to perform better in experiments. 3.1 Review of CCA CCA is a general technique that operates on a pair of multi-dimensional variables. CCA finds k dimensions (k is a parameter to be specified) in which these variables are maximally correlated. 0 Let x(1) . . . x(n) ∈ Rd and y (1) . . . y (n) ∈ Rd be n samples of the two variables. For simplicity, assume that these variables have zero mean. Then CCA computes the following for i = 1 . . . k: Multi-Sense Clustering via CCA The proposed pre-training method requires assigning"
P15-2032,D14-1113,0,0.0141475,"we choose to only use θ1 since the label space is taskspecific. This process is illustrated in Figure 2. In summary, the first step is used to find generic parameters between observations and hidden states; the second step is used to specialize the parameters to a particular task. Note that the first step also generates additional feature types absent in the labeled data which can be useful at test time. 3 n X (l) [[yi = 1]] i=1 In the second step, we train a final model on the labeled data {(x(i) , y (i) )}N i=1 using θ1 as an initialization point: θ,γ: vi = Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014), our proposed method is simpler and is shown to perform better in experiments. 3.1 Review of CCA CCA is a general technique that operates on a pair of multi-dimensional variables. CCA finds k dimensions (k is a parameter to be specified) in which these variables are maximally correlated. 0 Let x(1) . . . x(n) ∈ Rd and y (1) . . . y (n) ∈ Rd be n samples of the two variables. For simplicity, assume that these variables have zero mean. Then CCA computes the following for i = 1 . . . k: Multi-Sense Clustering via CCA The proposed pre-training method requires assigning a cluster to each word in u"
P15-2032,D12-1031,1,0.776688,"niques for sequence tagging problems (Collobert and Weston, 2008; Collobert et al., 2011; Mohamed et al., 2010; Deoras et al., 2012; Xu and Sarikaya, 2013; Yao et al., 2013; Mesnil et al., 2013; Wang and Manning, 2013; Devlin et al., 2014), conditional random fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) still have been widely used in many research and production systems for the problems due to the effectiveness and simplicity of training, which does not involve task specific parameter tuning (Collins, 2002; McCallum and Li, 2003; Sha and Pereira, 2003; Turian et al., 2010; Kim and Snyder, 2012; Celikyilmaz et al., 2013; Sarikaya et al., 2014; Anastasakos et al., 2014; 192 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 192–198, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Figure 2: Illustration of a pre-training scheme for HUCRFs. 0 θ ∈ Rd and γ ∈ Rd and defines a joint probability of y and z conditioned on x as follows: Figure 1: Graphical representation of hidden unit CRFs. pθ,γ (y, z|x) = data. The intuition"
P15-2032,N10-1013,0,0.0969604,"Missing"
P15-2032,N03-1028,0,0.0324813,"ent accuracy gains of the deep learning techniques for sequence tagging problems (Collobert and Weston, 2008; Collobert et al., 2011; Mohamed et al., 2010; Deoras et al., 2012; Xu and Sarikaya, 2013; Yao et al., 2013; Mesnil et al., 2013; Wang and Manning, 2013; Devlin et al., 2014), conditional random fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) still have been widely used in many research and production systems for the problems due to the effectiveness and simplicity of training, which does not involve task specific parameter tuning (Collins, 2002; McCallum and Li, 2003; Sha and Pereira, 2003; Turian et al., 2010; Kim and Snyder, 2012; Celikyilmaz et al., 2013; Sarikaya et al., 2014; Anastasakos et al., 2014; 192 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 192–198, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Figure 2: Illustration of a pre-training scheme for HUCRFs. 0 θ ∈ Rd and γ ∈ Rd and defines a joint probability of y and z conditioned on x as follows: Figure 1: Graphical representation of hidden uni"
P15-2032,P10-1040,0,0.0629196,"he deep learning techniques for sequence tagging problems (Collobert and Weston, 2008; Collobert et al., 2011; Mohamed et al., 2010; Deoras et al., 2012; Xu and Sarikaya, 2013; Yao et al., 2013; Mesnil et al., 2013; Wang and Manning, 2013; Devlin et al., 2014), conditional random fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) still have been widely used in many research and production systems for the problems due to the effectiveness and simplicity of training, which does not involve task specific parameter tuning (Collins, 2002; McCallum and Li, 2003; Sha and Pereira, 2003; Turian et al., 2010; Kim and Snyder, 2012; Celikyilmaz et al., 2013; Sarikaya et al., 2014; Anastasakos et al., 2014; 192 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 192–198, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Figure 2: Illustration of a pre-training scheme for HUCRFs. 0 θ ∈ Rd and γ ∈ Rd and defines a joint probability of y and z conditioned on x as follows: Figure 1: Graphical representation of hidden unit CRFs. pθ,γ (y, z|x)"
P15-2032,I13-1183,0,0.0231971,"her improve the accuracy within the proposed framework. We report consistent gains over standard conditional random fields (CRFs) and HUCRFs without pre-training in semantic tagging, named entity recognition (NER), and part-of-speech (POS) tagging tasks, which could indicate the task independent nature of the proposed technique. 1 Introduction Despite the recent accuracy gains of the deep learning techniques for sequence tagging problems (Collobert and Weston, 2008; Collobert et al., 2011; Mohamed et al., 2010; Deoras et al., 2012; Xu and Sarikaya, 2013; Yao et al., 2013; Mesnil et al., 2013; Wang and Manning, 2013; Devlin et al., 2014), conditional random fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) still have been widely used in many research and production systems for the problems due to the effectiveness and simplicity of training, which does not involve task specific parameter tuning (Collins, 2002; McCallum and Li, 2003; Sha and Pereira, 2003; Turian et al., 2010; Kim and Snyder, 2012; Celikyilmaz et al., 2013; Sarikaya et al., 2014; Anastasakos et al., 2014; 192 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Jo"
P15-2032,J92-4003,0,\N,Missing
P15-2032,P08-1068,0,\N,Missing
P15-2132,P15-1046,1,0.609734,"t solution to this problem (Hotelling, 1936). The resulting solution 0 u1 . . . uk ∈ Rd and v1 . . . vk ∈ Rd can be used to project the variables from the original d- and d0 -dimensional spaces to a k-dimensional space: Search click log features: Large-scale search engines such as Bing and Google process millions of queries on a daily basis. Together with the search queries, user clicked URLs are also logged anonymously. These click logs have been 808 Subject Jason Statham Jason Statham Jason Statham Romeo & Juliet Romeo & Juliet used for extracting semantic information for various NLP tasks (Kim et al., 2015a; Tseng et al., 2009; Hakkani-T¨ur et al., 2011). We used the clicked URLs as features to determine the likelihood of an entity being a member of a dictionary. These features are useful because common URLs are shared across different names such as movie, business and music. Table 1 shows the top five most frequently clicked URLs for movies “Furious 7” and “The age of adaline”. Furious 7 imdb.com en.wikipedia.org furious7.com rottentomatoes.com www.msn.com Relation type type type type type Object film.actor tv.actor film.producer film.film music.album Table 2: Entities & relation in the knowle"
P15-2132,J92-4003,0,0.26351,"onaries are mined from the web and search logs automatically using basic pattern matching approaches (e.g. entities sharing the same or similar context in queries or documents) and consequently contain significant amount of noise. As the table indicates, the number of elements in total across all the gazetteers (#total gazet elements) in each domain are too large for models to consume. In all our experiments, we trained conditional random fields (CRFs) (Lafferty et al., 2001) with the following features: (1) n-gram features up to n = 3, (2) regular expression features, and (3) Brown clusters (Brown et al., 1992) induced from search logs. With these features, we compare the following methods to demonstrate the importance of adding appropriate gazetteers: The age of adaline imdb.com en.wikipedia.org youtube.com rottentomatoes.com movieinsider.com Table 1: Top clicked URLs of two movies. One issue with using only click logs is that some entities may not be covered in the query logs since logs are extracted from a limited time frame (e.g. six months). Even the big search engines employ a moving time window for processing and storing search logs. Consequently, click logs are not necessarily good evidence."
P15-2132,P13-1090,1,0.764586,"Missing"
P15-2132,D09-1055,0,0.0188485,"problem (Hotelling, 1936). The resulting solution 0 u1 . . . uk ∈ Rd and v1 . . . vk ∈ Rd can be used to project the variables from the original d- and d0 -dimensional spaces to a k-dimensional space: Search click log features: Large-scale search engines such as Bing and Google process millions of queries on a daily basis. Together with the search queries, user clicked URLs are also logged anonymously. These click logs have been 808 Subject Jason Statham Jason Statham Jason Statham Romeo & Juliet Romeo & Juliet used for extracting semantic information for various NLP tasks (Kim et al., 2015a; Tseng et al., 2009; Hakkani-T¨ur et al., 2011). We used the clicked URLs as features to determine the likelihood of an entity being a member of a dictionary. These features are useful because common URLs are shared across different names such as movie, business and music. Table 1 shows the top five most frequently clicked URLs for movies “Furious 7” and “The age of adaline”. Furious 7 imdb.com en.wikipedia.org furious7.com rottentomatoes.com www.msn.com Relation type type type type type Object film.actor tv.actor film.producer film.film music.album Table 2: Entities & relation in the knowledge graph. 4 Experime"
P15-2132,N13-1139,1,0.525734,"Missing"
P15-2132,N15-1009,1,0.309831,"t solution to this problem (Hotelling, 1936). The resulting solution 0 u1 . . . uk ∈ Rd and v1 . . . vk ∈ Rd can be used to project the variables from the original d- and d0 -dimensional spaces to a k-dimensional space: Search click log features: Large-scale search engines such as Bing and Google process millions of queries on a daily basis. Together with the search queries, user clicked URLs are also logged anonymously. These click logs have been 808 Subject Jason Statham Jason Statham Jason Statham Romeo & Juliet Romeo & Juliet used for extracting semantic information for various NLP tasks (Kim et al., 2015a; Tseng et al., 2009; Hakkani-T¨ur et al., 2011). We used the clicked URLs as features to determine the likelihood of an entity being a member of a dictionary. These features are useful because common URLs are shared across different names such as movie, business and music. Table 1 shows the top five most frequently clicked URLs for movies “Furious 7” and “The age of adaline”. Furious 7 imdb.com en.wikipedia.org furious7.com rottentomatoes.com www.msn.com Relation type type type type type Object film.actor tv.actor film.producer film.film music.album Table 2: Entities & relation in the knowle"
P15-2132,P15-2032,1,0.517976,"t solution to this problem (Hotelling, 1936). The resulting solution 0 u1 . . . uk ∈ Rd and v1 . . . vk ∈ Rd can be used to project the variables from the original d- and d0 -dimensional spaces to a k-dimensional space: Search click log features: Large-scale search engines such as Bing and Google process millions of queries on a daily basis. Together with the search queries, user clicked URLs are also logged anonymously. These click logs have been 808 Subject Jason Statham Jason Statham Jason Statham Romeo & Juliet Romeo & Juliet used for extracting semantic information for various NLP tasks (Kim et al., 2015a; Tseng et al., 2009; Hakkani-T¨ur et al., 2011). We used the clicked URLs as features to determine the likelihood of an entity being a member of a dictionary. These features are useful because common URLs are shared across different names such as movie, business and music. Table 1 shows the top five most frequently clicked URLs for movies “Furious 7” and “The age of adaline”. Furious 7 imdb.com en.wikipedia.org furious7.com rottentomatoes.com www.msn.com Relation type type type type type Object film.actor tv.actor film.producer film.film music.album Table 2: Entities & relation in the knowle"
P16-2002,N13-1139,1,0.845447,"chines, to build distributed memory systems achieving large memory capacity. However, building and maintaining these industry grade clusters is not trivial and thus not accessible to everyone. It is critical to have techniques that can process large data within a limited memory budget available in most typical enterprise servers. One solution is to approximate the matrix with some Y ∈ Rl×d where l  n. Many matrix approximation techniques have been proposed, such as random projection (Papadimitriou et al., 1998; Vempala, 2005), sampling (Drineas and Kannan, 2003; Rudelson and Vershynin, 2007; Kim and Snyder, 2013; Kim et al., 2015b), and hashing (Weinberger et al., 2009). Most of these techniques involve randomness, which can be undesirable in certain situations (e.g., when experiments need to be exactly reproducible). Moreover, many are not designed directly for the objective that we care about: namely, ensuring that the covariance matrices X > X and Y > Y remain “similar”. Σj,j = r   2 max Σ2j,j − σbl/2c ,0 iii. Set Y = ΣV > . Output:Y ∈ Rl×d s.t. X > X − Y > Y 2 ≤ 2 ||X||2F /l Figure 1: Matrix sketching algorithm by Liberty (2013). In the output, X ∈ Rn×d denotes the data matrix with rows x1 . ."
P16-2002,P15-2132,1,0.866296,"ibuted memory systems achieving large memory capacity. However, building and maintaining these industry grade clusters is not trivial and thus not accessible to everyone. It is critical to have techniques that can process large data within a limited memory budget available in most typical enterprise servers. One solution is to approximate the matrix with some Y ∈ Rl×d where l  n. Many matrix approximation techniques have been proposed, such as random projection (Papadimitriou et al., 1998; Vempala, 2005), sampling (Drineas and Kannan, 2003; Rudelson and Vershynin, 2007; Kim and Snyder, 2013; Kim et al., 2015b), and hashing (Weinberger et al., 2009). Most of these techniques involve randomness, which can be undesirable in certain situations (e.g., when experiments need to be exactly reproducible). Moreover, many are not designed directly for the objective that we care about: namely, ensuring that the covariance matrices X > X and Y > Y remain “similar”. Σj,j = r   2 max Σ2j,j − σbl/2c ,0 iii. Set Y = ΣV > . Output:Y ∈ Rl×d s.t. X > X − Y > Y 2 ≤ 2 ||X||2F /l Figure 1: Matrix sketching algorithm by Liberty (2013). In the output, X ∈ Rn×d denotes the data matrix with rows x1 . . . xn . A recent re"
P16-2002,P15-2032,1,0.498603,"ibuted memory systems achieving large memory capacity. However, building and maintaining these industry grade clusters is not trivial and thus not accessible to everyone. It is critical to have techniques that can process large data within a limited memory budget available in most typical enterprise servers. One solution is to approximate the matrix with some Y ∈ Rl×d where l  n. Many matrix approximation techniques have been proposed, such as random projection (Papadimitriou et al., 1998; Vempala, 2005), sampling (Drineas and Kannan, 2003; Rudelson and Vershynin, 2007; Kim and Snyder, 2013; Kim et al., 2015b), and hashing (Weinberger et al., 2009). Most of these techniques involve randomness, which can be undesirable in certain situations (e.g., when experiments need to be exactly reproducible). Moreover, many are not designed directly for the objective that we care about: namely, ensuring that the covariance matrices X > X and Y > Y remain “similar”. Σj,j = r   2 max Σ2j,j − σbl/2c ,0 iii. Set Y = ΣV > . Output:Y ∈ Rl×d s.t. X > X − Y > Y 2 ≤ 2 ||X||2F /l Figure 1: Matrix sketching algorithm by Liberty (2013). In the output, X ∈ Rn×d denotes the data matrix with rows x1 . . . xn . A recent re"
P16-2002,P15-1046,1,0.929507,"ibuted memory systems achieving large memory capacity. However, building and maintaining these industry grade clusters is not trivial and thus not accessible to everyone. It is critical to have techniques that can process large data within a limited memory budget available in most typical enterprise servers. One solution is to approximate the matrix with some Y ∈ Rl×d where l  n. Many matrix approximation techniques have been proposed, such as random projection (Papadimitriou et al., 1998; Vempala, 2005), sampling (Drineas and Kannan, 2003; Rudelson and Vershynin, 2007; Kim and Snyder, 2013; Kim et al., 2015b), and hashing (Weinberger et al., 2009). Most of these techniques involve randomness, which can be undesirable in certain situations (e.g., when experiments need to be exactly reproducible). Moreover, many are not designed directly for the objective that we care about: namely, ensuring that the covariance matrices X > X and Y > Y remain “similar”. Σj,j = r   2 max Σ2j,j − σbl/2c ,0 iii. Set Y = ΣV > . Output:Y ∈ Rl×d s.t. X > X − Y > Y 2 ≤ 2 ||X||2F /l Figure 1: Matrix sketching algorithm by Liberty (2013). In the output, X ∈ Rn×d denotes the data matrix with rows x1 . . . xn . A recent re"
P16-2002,D14-1162,0,0.0846512,"Missing"
P16-2002,W15-1511,1,0.843538,"ation. We do so by learning a PCA projection matrix Π from the unlabeled data and applying it on both training and test sentences. The matrix sketching algorithm in Figure 1 enables us to compute Π on arbitrarily large data. There are many design considerations for using the sketching algorithm for our task. feature vector of a bag-of-words sentence representation. Specifically, if x is the original bag-ofwords sentence vector, the new representation is given by 3.1 where ⊕ is the vector concatenation operation. This representational scheme is shown to be effective in previous work (e.g., see Stratos and Collins (2015)). xnew = Original sentence representations We use a bag-of-words vector to represent a sentence. Specifically, each sentence is a ddimensional vector x ∈ Rd where d is the size of the vocabulary and xi is the count of an n-gram i in the sentence (we use up to n = 3 in experiments); we denote this representation by SENT. In experiments, we also use a modification of this representation, denoted by SENT+, in which we explicitly define features over the first two words in a query and also use intent predictions made by a supervised model. 3.2 3.5 Random hashing Parallelization Experiment X ∈ R17"
P16-2002,N15-1009,1,\N,Missing
P17-1060,P07-1056,0,0.289109,"Missing"
P17-1060,W06-1615,0,0.442319,"g problems to easily scale to a larger number of domains: pretraining (Kim et al., 2015c), transfer learning (Kim et al., 2015d), constrained decoding with a single model (Kim et al., 2016a), multi-task learning (Jaech et al., 2016), neural domain adaptation (Kim et al., 2016c), domainless adaptation (Kim et al., 2016b), a sequence-to-sequence model (Hakkani-T¨ur et al., 2016), adversary domain training (Kim et al., 2017) and zero-shot learning(Chen et al., 2016; Ferreira et al., 2015). There is a venerable history of research on domain adaptation (Daume III and Marcu, 2006; Daum´e III, 2009; Blitzer et al., 2006, 2007; Pan et al., 2011) which is concerned with the shift in data distribution from one domain to another. In the context of NLP, a particularly successful approach is the feature augmentation method of Daum´e III (2009) whose key insight is that if we partition the model parameters to those that handle common patterns and those that handle domainspecific patterns, the model is forced to learn from all domains yet preserve domain-specific knowledge. The method is generalized to the neural paradigm by Kim et al. (2016c) who jointly use a domain-specific LSTM and also a global LSTM shared acro"
P17-1060,T75-2026,0,0.460678,"Missing"
P17-1060,N15-1009,1,0.739317,"es difficult to scale when there is a new domain coming in at high frequency. We address this problem by proposing a method that only calls K trained domain experts; we do not have to re-train these domain experts. This gives a clear computational advantage over the feature augmentation method. 2.2 There are also a line of prior works on enhancing model capability and features: jointly modeling intent and slot predictions (Jeong and Lee, 2008; Xu and Sarikaya, 2013; Guo et al., 2014; Zhang and Wang, 2016; Liu and Lane, 2016a,b), modeling SLU models with web search click logs (Li et al., 2009; Kim et al., 2015a) and enhancing features, including representations (Anastasakos et al., 2014; Sarikaya et al., 2014; Celikyilmaz et al., 2016, 2010; Kim et al., 2016d) and lexicon (Liu and Sarikaya, 2014; Kim et al., 2015b). 3 Method We use an LSTM simply as a mapping φ : Rd × 0 0 Rd → Rd that takes an input vector x and a state vector h to output a new state vector h0 = φ(x, h). See Hochreiter and Schmidhuber (1997) for a detailed description. At a high level, the individual model consists of builds on several ingredients shown in Figure 1: character and word embedding, a bidirectional LSTM (BiLSTM) at a c"
P17-1060,D16-1222,1,0.91248,"arch on domain adaptation (Daume III and Marcu, 2006; Daum´e III, 2009; Blitzer et al., 2006, 2007; Pan et al., 2011) which is concerned with the shift in data distribution from one domain to another. In the context of NLP, a particularly successful approach is the feature augmentation method of Daum´e III (2009) whose key insight is that if we partition the model parameters to those that handle common patterns and those that handle domainspecific patterns, the model is forced to learn from all domains yet preserve domain-specific knowledge. The method is generalized to the neural paradigm by Kim et al. (2016c) who jointly use a domain-specific LSTM and also a global LSTM shared across all domains. In the context of SLU, Jaech et al. (2016) proposed K domain-specific feedforward layers with a shared word-level LSTM layer across domains; Kim et al. (2016c) instead employed K + 1 LSTMs. Hakkani-T¨ur et al. (2016) proposed to employ a sequence-to-sequence model by introducing a fictitious symbol at the end of an utterance of which tag represents the corresponding domain and intent. All these methods require one to re-train a model from scratch to make it learn the correlation and invariance between d"
P17-1060,P17-1119,1,0.268559,"aining approach of Kim et al. (2016c), a neural analog of Daum´e III (2009). 2 2.1 Related Work Domain Adaptation Naturally, there has been an extensive line of prior studies for domain scaling problems to easily scale to a larger number of domains: pretraining (Kim et al., 2015c), transfer learning (Kim et al., 2015d), constrained decoding with a single model (Kim et al., 2016a), multi-task learning (Jaech et al., 2016), neural domain adaptation (Kim et al., 2016c), domainless adaptation (Kim et al., 2016b), a sequence-to-sequence model (Hakkani-T¨ur et al., 2016), adversary domain training (Kim et al., 2017) and zero-shot learning(Chen et al., 2016; Ferreira et al., 2015). There is a venerable history of research on domain adaptation (Daume III and Marcu, 2006; Daum´e III, 2009; Blitzer et al., 2006, 2007; Pan et al., 2011) which is concerned with the shift in data distribution from one domain to another. In the context of NLP, a particularly successful approach is the feature augmentation method of Daum´e III (2009) whose key insight is that if we partition the model parameters to those that handle common patterns and those that handle domainspecific patterns, the model is forced to learn from a"
P17-1060,P15-2132,1,0.857862,"es difficult to scale when there is a new domain coming in at high frequency. We address this problem by proposing a method that only calls K trained domain experts; we do not have to re-train these domain experts. This gives a clear computational advantage over the feature augmentation method. 2.2 There are also a line of prior works on enhancing model capability and features: jointly modeling intent and slot predictions (Jeong and Lee, 2008; Xu and Sarikaya, 2013; Guo et al., 2014; Zhang and Wang, 2016; Liu and Lane, 2016a,b), modeling SLU models with web search click logs (Li et al., 2009; Kim et al., 2015a) and enhancing features, including representations (Anastasakos et al., 2014; Sarikaya et al., 2014; Celikyilmaz et al., 2016, 2010; Kim et al., 2016d) and lexicon (Liu and Sarikaya, 2014; Kim et al., 2015b). 3 Method We use an LSTM simply as a mapping φ : Rd × 0 0 Rd → Rd that takes an input vector x and a state vector h to output a new state vector h0 = φ(x, h). See Hochreiter and Schmidhuber (1997) for a detailed description. At a high level, the individual model consists of builds on several ingredients shown in Figure 1: character and word embedding, a bidirectional LSTM (BiLSTM) at a c"
P17-1060,P15-2032,1,0.835715,"es difficult to scale when there is a new domain coming in at high frequency. We address this problem by proposing a method that only calls K trained domain experts; we do not have to re-train these domain experts. This gives a clear computational advantage over the feature augmentation method. 2.2 There are also a line of prior works on enhancing model capability and features: jointly modeling intent and slot predictions (Jeong and Lee, 2008; Xu and Sarikaya, 2013; Guo et al., 2014; Zhang and Wang, 2016; Liu and Lane, 2016a,b), modeling SLU models with web search click logs (Li et al., 2009; Kim et al., 2015a) and enhancing features, including representations (Anastasakos et al., 2014; Sarikaya et al., 2014; Celikyilmaz et al., 2016, 2010; Kim et al., 2016d) and lexicon (Liu and Sarikaya, 2014; Kim et al., 2015b). 3 Method We use an LSTM simply as a mapping φ : Rd × 0 0 Rd → Rd that takes an input vector x and a state vector h to output a new state vector h0 = φ(x, h). See Hochreiter and Schmidhuber (1997) for a detailed description. At a high level, the individual model consists of builds on several ingredients shown in Figure 1: character and word embedding, a bidirectional LSTM (BiLSTM) at a c"
P17-1060,C16-1193,1,0.943688,"arch on domain adaptation (Daume III and Marcu, 2006; Daum´e III, 2009; Blitzer et al., 2006, 2007; Pan et al., 2011) which is concerned with the shift in data distribution from one domain to another. In the context of NLP, a particularly successful approach is the feature augmentation method of Daum´e III (2009) whose key insight is that if we partition the model parameters to those that handle common patterns and those that handle domainspecific patterns, the model is forced to learn from all domains yet preserve domain-specific knowledge. The method is generalized to the neural paradigm by Kim et al. (2016c) who jointly use a domain-specific LSTM and also a global LSTM shared across all domains. In the context of SLU, Jaech et al. (2016) proposed K domain-specific feedforward layers with a shared word-level LSTM layer across domains; Kim et al. (2016c) instead employed K + 1 LSTMs. Hakkani-T¨ur et al. (2016) proposed to employ a sequence-to-sequence model by introducing a fictitious symbol at the end of an utterance of which tag represents the corresponding domain and intent. All these methods require one to re-train a model from scratch to make it learn the correlation and invariance between d"
P17-1060,C16-1038,1,0.647854,"arch on domain adaptation (Daume III and Marcu, 2006; Daum´e III, 2009; Blitzer et al., 2006, 2007; Pan et al., 2011) which is concerned with the shift in data distribution from one domain to another. In the context of NLP, a particularly successful approach is the feature augmentation method of Daum´e III (2009) whose key insight is that if we partition the model parameters to those that handle common patterns and those that handle domainspecific patterns, the model is forced to learn from all domains yet preserve domain-specific knowledge. The method is generalized to the neural paradigm by Kim et al. (2016c) who jointly use a domain-specific LSTM and also a global LSTM shared across all domains. In the context of SLU, Jaech et al. (2016) proposed K domain-specific feedforward layers with a shared word-level LSTM layer across domains; Kim et al. (2016c) instead employed K + 1 LSTMs. Hakkani-T¨ur et al. (2016) proposed to employ a sequence-to-sequence model by introducing a fictitious symbol at the end of an utterance of which tag represents the corresponding domain and intent. All these methods require one to re-train a model from scratch to make it learn the correlation and invariance between d"
P17-1060,P16-2002,1,0.914095,"arch on domain adaptation (Daume III and Marcu, 2006; Daum´e III, 2009; Blitzer et al., 2006, 2007; Pan et al., 2011) which is concerned with the shift in data distribution from one domain to another. In the context of NLP, a particularly successful approach is the feature augmentation method of Daum´e III (2009) whose key insight is that if we partition the model parameters to those that handle common patterns and those that handle domainspecific patterns, the model is forced to learn from all domains yet preserve domain-specific knowledge. The method is generalized to the neural paradigm by Kim et al. (2016c) who jointly use a domain-specific LSTM and also a global LSTM shared across all domains. In the context of SLU, Jaech et al. (2016) proposed K domain-specific feedforward layers with a shared word-level LSTM layer across domains; Kim et al. (2016c) instead employed K + 1 LSTMs. Hakkani-T¨ur et al. (2016) proposed to employ a sequence-to-sequence model by introducing a fictitious symbol at the end of an utterance of which tag represents the corresponding domain and intent. All these methods require one to re-train a model from scratch to make it learn the correlation and invariance between d"
P17-1060,N16-1030,0,0.0318111,"? Utterance Figure 1: The overall network architecture of the individual model. 3.1 for each i = 1 . . . n.2 Next, the model computes  W vi , fi−1 ∀i = 1 . . . n fiW = φW f  W bW vi , bW ∀i = n . . . 1 i = φb i+1 Individual Model Architecture Let C denote the set of character types and W the set of word types. Let ⊕ denote the vector concatenation operation. A wildly successful architecture for encoding a sentence (w1 . . . wn ) ∈ W n is given by bidirectional LSTMs (BiLSTMs) (Schuster and Paliwal, 1997; Graves, 2012). Our model first constructs a network over an utterance closely following Lample et al. (2016). The model parameters Θ associated with this BiLSTM layer are and induces a character- and context-sensitive word representation hi ∈ R200 as hi = fiW ⊕ bW i (1) for each i = 1 . . . n. These vectors can be used to perform intent classification or slot tagging on the utterance. • Character embedding ec ∈ R25 for each c ∈ C Intent Classification We can predict the intent of the utterance using (h1 . . . hn ) ∈ R200 in (1) as follows. Let I denote the set of intent types. We introduce a single-layer feedforward network g i : R200 → R|I |whose parameters are denoted by Θi . We compute a |I|-dime"
P17-1060,W16-3603,0,0.00624568,"from scratch to make it learn the correlation and invariance between domains. This becomes difficult to scale when there is a new domain coming in at high frequency. We address this problem by proposing a method that only calls K trained domain experts; we do not have to re-train these domain experts. This gives a clear computational advantage over the feature augmentation method. 2.2 There are also a line of prior works on enhancing model capability and features: jointly modeling intent and slot predictions (Jeong and Lee, 2008; Xu and Sarikaya, 2013; Guo et al., 2014; Zhang and Wang, 2016; Liu and Lane, 2016a,b), modeling SLU models with web search click logs (Li et al., 2009; Kim et al., 2015a) and enhancing features, including representations (Anastasakos et al., 2014; Sarikaya et al., 2014; Celikyilmaz et al., 2016, 2010; Kim et al., 2016d) and lexicon (Liu and Sarikaya, 2014; Kim et al., 2015b). 3 Method We use an LSTM simply as a mapping φ : Rd × 0 0 Rd → Rd that takes an input vector x and a state vector h to output a new state vector h0 = φ(x, h). See Hochreiter and Schmidhuber (1997) for a detailed description. At a high level, the individual model consists of builds on several ingredient"
P17-1119,T75-2026,0,0.648677,"Missing"
P17-1119,N15-1009,1,0.687347,"Spoken language understanding (SLU) is an important component of these examples that allows natural communication between the user and the agent (Tur, 2006; El-Kahky et al., 2014). PDAs support a number of scenarios including creating reminders, setting up alarms, note taking, scheduling meetings, finding and consuming entertainment (i.e. movie, music, games), finding places of interest and getting driving directions to them (Kim et al., 2016a). Naturally, there has been an extensive line of prior studies for domain scaling problems to easily scale to a larger number of domains: pretraining (Kim et al., 2015c), transfer learning (Kim et al., 2015d), constrained decoding with a single model (Kim et al., 2016a), multi-task learning (Jaech et al., 2016), neural domain adaptation (Kim et al., 2016c), domainless adaptation (Kim et al., 2016b), a sequence-to-sequence model (Hakkani-T¨ur et al., 2016), domain attention (Kim et al., 2017) and zero-shot learning(Chen et al., 2016; Ferreira et al., 2015). There are also a line of prior works on enhancing model capability and features: jointly modeling intent and slot predictions (Jeong and Lee, 2008; Xu and Sarikaya, 2013; Guo et al., 2014; Zhang and Wang,"
P17-1119,D16-1222,1,0.931685,"rvised DA. They partition the model parameters into two parts: one inducing domainspecific (or private) features and the other domaininvariant (or shared) features. The domaininvariant parameters are adversarially trained using a gradient reversal layer to be poor at domain classification; as a consequence, they produce representations that are domain agnostic. This approach is motivated by a rich literature on the theory of DA pioneered by Ben-David et al. (2007). We describe our use of adversarial training in Section 3.2.3. A special case of Ganin et al. (2016) is developed independently by Kim et al. (2016c) who motivate the method as a generalization of the feature augmentation method of Daum´e III (2009). Bousmalis et al. (2016) extend the framework of Ganin et al. (2016) by additionally encouraging the private and shared features to be mutually exclusive. This is achieved by minimizing the dot product between the two sets of parameters and simultaneously reconstructing the input (for all domains) from the features induced by these parameters. Both Ganin et al. (2016) and Bousmalis et al. (2016) discuss applications in computer vision. Zhang et al. (2017) apply the method of Bousmalis et al."
P17-1119,P17-1060,1,0.268354,"ainment (i.e. movie, music, games), finding places of interest and getting driving directions to them (Kim et al., 2016a). Naturally, there has been an extensive line of prior studies for domain scaling problems to easily scale to a larger number of domains: pretraining (Kim et al., 2015c), transfer learning (Kim et al., 2015d), constrained decoding with a single model (Kim et al., 2016a), multi-task learning (Jaech et al., 2016), neural domain adaptation (Kim et al., 2016c), domainless adaptation (Kim et al., 2016b), a sequence-to-sequence model (Hakkani-T¨ur et al., 2016), domain attention (Kim et al., 2017) and zero-shot learning(Chen et al., 2016; Ferreira et al., 2015). There are also a line of prior works on enhancing model capability and features: jointly modeling intent and slot predictions (Jeong and Lee, 2008; Xu and Sarikaya, 2013; Guo et al., 2014; Zhang and Wang, 2016; Liu and Lane, 2016a,b), modeling SLU models with web search click logs (Li et al., 2009; Kim et al., 2015a) and enhancing features, including representations (Anastasakos et al., 2014; Sarikaya et al., 2014; Celikyilmaz et al., 2016, 2010; Kim et al., 2016d) and lexicon (Liu and Sarikaya, 2014; Kim et al., 2015b). All th"
P17-1119,P15-2132,1,0.868211,"Spoken language understanding (SLU) is an important component of these examples that allows natural communication between the user and the agent (Tur, 2006; El-Kahky et al., 2014). PDAs support a number of scenarios including creating reminders, setting up alarms, note taking, scheduling meetings, finding and consuming entertainment (i.e. movie, music, games), finding places of interest and getting driving directions to them (Kim et al., 2016a). Naturally, there has been an extensive line of prior studies for domain scaling problems to easily scale to a larger number of domains: pretraining (Kim et al., 2015c), transfer learning (Kim et al., 2015d), constrained decoding with a single model (Kim et al., 2016a), multi-task learning (Jaech et al., 2016), neural domain adaptation (Kim et al., 2016c), domainless adaptation (Kim et al., 2016b), a sequence-to-sequence model (Hakkani-T¨ur et al., 2016), domain attention (Kim et al., 2017) and zero-shot learning(Chen et al., 2016; Ferreira et al., 2015). There are also a line of prior works on enhancing model capability and features: jointly modeling intent and slot predictions (Jeong and Lee, 2008; Xu and Sarikaya, 2013; Guo et al., 2014; Zhang and Wang,"
P17-1119,P15-2032,1,0.816611,"Spoken language understanding (SLU) is an important component of these examples that allows natural communication between the user and the agent (Tur, 2006; El-Kahky et al., 2014). PDAs support a number of scenarios including creating reminders, setting up alarms, note taking, scheduling meetings, finding and consuming entertainment (i.e. movie, music, games), finding places of interest and getting driving directions to them (Kim et al., 2016a). Naturally, there has been an extensive line of prior studies for domain scaling problems to easily scale to a larger number of domains: pretraining (Kim et al., 2015c), transfer learning (Kim et al., 2015d), constrained decoding with a single model (Kim et al., 2016a), multi-task learning (Jaech et al., 2016), neural domain adaptation (Kim et al., 2016c), domainless adaptation (Kim et al., 2016b), a sequence-to-sequence model (Hakkani-T¨ur et al., 2016), domain attention (Kim et al., 2017) and zero-shot learning(Chen et al., 2016; Ferreira et al., 2015). There are also a line of prior works on enhancing model capability and features: jointly modeling intent and slot predictions (Jeong and Lee, 2008; Xu and Sarikaya, 2013; Guo et al., 2014; Zhang and Wang,"
P17-1119,C16-1193,1,0.943775,"rvised DA. They partition the model parameters into two parts: one inducing domainspecific (or private) features and the other domaininvariant (or shared) features. The domaininvariant parameters are adversarially trained using a gradient reversal layer to be poor at domain classification; as a consequence, they produce representations that are domain agnostic. This approach is motivated by a rich literature on the theory of DA pioneered by Ben-David et al. (2007). We describe our use of adversarial training in Section 3.2.3. A special case of Ganin et al. (2016) is developed independently by Kim et al. (2016c) who motivate the method as a generalization of the feature augmentation method of Daum´e III (2009). Bousmalis et al. (2016) extend the framework of Ganin et al. (2016) by additionally encouraging the private and shared features to be mutually exclusive. This is achieved by minimizing the dot product between the two sets of parameters and simultaneously reconstructing the input (for all domains) from the features induced by these parameters. Both Ganin et al. (2016) and Bousmalis et al. (2016) discuss applications in computer vision. Zhang et al. (2017) apply the method of Bousmalis et al."
P17-1119,C16-1038,1,0.566274,"rvised DA. They partition the model parameters into two parts: one inducing domainspecific (or private) features and the other domaininvariant (or shared) features. The domaininvariant parameters are adversarially trained using a gradient reversal layer to be poor at domain classification; as a consequence, they produce representations that are domain agnostic. This approach is motivated by a rich literature on the theory of DA pioneered by Ben-David et al. (2007). We describe our use of adversarial training in Section 3.2.3. A special case of Ganin et al. (2016) is developed independently by Kim et al. (2016c) who motivate the method as a generalization of the feature augmentation method of Daum´e III (2009). Bousmalis et al. (2016) extend the framework of Ganin et al. (2016) by additionally encouraging the private and shared features to be mutually exclusive. This is achieved by minimizing the dot product between the two sets of parameters and simultaneously reconstructing the input (for all domains) from the features induced by these parameters. Both Ganin et al. (2016) and Bousmalis et al. (2016) discuss applications in computer vision. Zhang et al. (2017) apply the method of Bousmalis et al."
P17-1119,P16-2002,1,0.91564,"rvised DA. They partition the model parameters into two parts: one inducing domainspecific (or private) features and the other domaininvariant (or shared) features. The domaininvariant parameters are adversarially trained using a gradient reversal layer to be poor at domain classification; as a consequence, they produce representations that are domain agnostic. This approach is motivated by a rich literature on the theory of DA pioneered by Ben-David et al. (2007). We describe our use of adversarial training in Section 3.2.3. A special case of Ganin et al. (2016) is developed independently by Kim et al. (2016c) who motivate the method as a generalization of the feature augmentation method of Daum´e III (2009). Bousmalis et al. (2016) extend the framework of Ganin et al. (2016) by additionally encouraging the private and shared features to be mutually exclusive. This is achieved by minimizing the dot product between the two sets of parameters and simultaneously reconstructing the input (for all domains) from the features induced by these parameters. Both Ganin et al. (2016) and Bousmalis et al. (2016) discuss applications in computer vision. Zhang et al. (2017) apply the method of Bousmalis et al."
P17-1119,Q17-1036,0,0.0815866,"al. (2016) is developed independently by Kim et al. (2016c) who motivate the method as a generalization of the feature augmentation method of Daum´e III (2009). Bousmalis et al. (2016) extend the framework of Ganin et al. (2016) by additionally encouraging the private and shared features to be mutually exclusive. This is achieved by minimizing the dot product between the two sets of parameters and simultaneously reconstructing the input (for all domains) from the features induced by these parameters. Both Ganin et al. (2016) and Bousmalis et al. (2016) discuss applications in computer vision. Zhang et al. (2017) apply the method of Bousmalis et al. (2016) to tackle transfer learning in NLP. They focus on transfer learning between classification tasks over the same domain (“aspect transfer”). They assume a set of keywords associated with each aspect and use these keywords to inform the learner of the relevance of each sentence for that aspect. 2.2 Spoken Language Understanding Recently, there has been much investment on the personal digital assistant (PDA) technology in industry (Sarikaya, 2015; Sarikaya et al., 2016). Apples Siri, Google Now, Microsofts Cortana, and Amazons Alexa are some examples of"
P17-1119,W16-3603,0,0.0115873,"nsfer learning (Kim et al., 2015d), constrained decoding with a single model (Kim et al., 2016a), multi-task learning (Jaech et al., 2016), neural domain adaptation (Kim et al., 2016c), domainless adaptation (Kim et al., 2016b), a sequence-to-sequence model (Hakkani-T¨ur et al., 2016), domain attention (Kim et al., 2017) and zero-shot learning(Chen et al., 2016; Ferreira et al., 2015). There are also a line of prior works on enhancing model capability and features: jointly modeling intent and slot predictions (Jeong and Lee, 2008; Xu and Sarikaya, 2013; Guo et al., 2014; Zhang and Wang, 2016; Liu and Lane, 2016a,b), modeling SLU models with web search click logs (Li et al., 2009; Kim et al., 2015a) and enhancing features, including representations (Anastasakos et al., 2014; Sarikaya et al., 2014; Celikyilmaz et al., 2016, 2010; Kim et al., 2016d) and lexicon (Liu and Sarikaya, 2014; Kim et al., 2015b). All the above works assume that there are no any data shift issues which our work try to solve. 3 3.1 Method BiLSTM Encoder We use an LSTM simply as a mapping φ : Rd × 0 0 Rd → Rd that takes an input vector x and a state vector h to output a new state vector h0 = φ(x, h). See Hochreiter and Schmidhube"
P19-1533,W96-0102,0,0.627336,"l Meeting of the Association for Computational Linguistics, pages 5363–5369 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 3 DT NNP POS NNP NNP The SEC ‘s Mr. Lane DT NN VBD CD NNP The index fell 109.9 Monday NNP NNP POS NNP VBZ Mr. Ridley ‘s decision fires NNP NNP VBZ NNP Ms. Haag plays Elianti ... ... ... ... Figure 1: A visualization of POS tagging an input sentence x by copying token-labels from the label sequences y 0(m) of M = 3 retrieved sentences x0(m) . with early successes dating back at least to the taggers of Daelemans (Daelemans, 1993; Daelemans et al., 1996) and the syntactic disambiguation system of Cardie (1994). Similarly motivated approaches remain popular for computer vision tasks, especially when it is impractical to learn a parametric labeling function (Shakhnarovich et al., 2006; Schroff et al., 2015). More recently, there has been renewed interest in explicitly conditioning structured predictions on retrieved neighbors, especially in the context of language generation (Hashimoto et al., 2018; Guu et al., 2018; Gu et al., 2018; Weston et al., 2018), although much of this work uses neighbors as extra conditioning information within a seque"
P19-1533,P07-1033,0,0.181481,"Missing"
P19-1533,Q18-1031,0,0.173963,"shot sequencelabeling tasks. We additionally consider a dynamic programming approach to sequence labeling in the presence of retrieved neighbors, which allows for controlling the number of distinct (copied) segments used to form a prediction, and leads to both more interpretable and accurate predictions. 1 Introduction Retrieve-and-edit style structured prediction, where a model retrieves a set of labeled nearest neighbors from the training data and conditions on them to generate the target structure, is a promising approach that has recently received renewed interest (Hashimoto et al., 2018; Guu et al., 2018; Gu et al., 2018; Weston et al., 2018). This approach captures the intuition that while generating a highly complex structure from scratch may be difficult, editing a sufficiently similar structure or set of structures may be easier. Recent work in this area primarily uses the nearest neighbors and their labels simply as an additional context for a sequence-to-sequence style model to condition on. While effective, these models may not explicitly capture the discrete operations (like copying) that allow for the neighbors to be edited into the target structure, making interpreting the behavior"
P19-1533,N06-2015,0,0.0374086,"labeling performance of BERT (Devlin et al., 2018), which we take to be near state of the art. We use the standard datasetsplits and evaluations for all tasks, and BIO encoding for all segment-level tagging tasks. We evaluate zero-shot transfer performance by training on one dataset and evaluating on another, without any retraining. In particular, we consider three zero-shot transfer scenarios: training with Universal POS Tags on the Penn Treebank and then predicting the standard, fine-grained POS tags, training on the CoNLL 2003 NER data and predicting on the fine-grained OntoNotes NER data (Hovy et al., 2006) using the setup of Strubell et al. (2017), and finally training on the CoNLL 2003 chunking data and predicting on the CoNLL 2003 NER data. We again compare with a BERT baseline, where labels from the original task are deterministically mapped to the most frequent label on the new task with which they coincide.3 Our nearest-neighbor based models were finetuned by retrieving the 50 nearest neighbors of each sentence in a mini-batch of either size 16 or 20, and maximizing the objective (2) above. For training, nearest neighbors were determined based on cosine-similarity between the averaged topl"
P19-1533,N16-1030,0,0.0259999,"in D. Above, 0(m) xt and xk (both in RD ) represent the contextual word embeddings of the t’th token in x and the k’th token in x0(m) , respectively, as obtained by running a deep sequence-model over x and over x0(m) . In all experiments we use BERT (Devlin et al., 2018), a model based on the Transformer 1 More precisely, we will set yˆt to be an instance of the 0(m) label type of which yk is a label token; this distinction between label types and tokens can make the exposition unnecessarily obscure, and so we avoid it when possible. 2 While recent sequence labeling models (Ma and Hovy, 2016; Lample et al., 2016), often model inter-label dependence with a first-order CRF (Lafferty et al., 2001), Devlin et al. (2018) have recently shown that excellent performance can be obtained by modeling labels as conditionally independent given a sufficiently expressive representation of x. 5364 architecture (Vaswani et al., 2017), to obtain contextual word embeddings. We fine-tune these contextual word embeddings by maximizing a latent-variable style probabilistic objective T X t=1 ln M X 0(m) X m=1 k: y 0(m) = y k p(yt = yk |x, D), (2) t where we sum over all individual label tokens in D that match yt . At test t"
P19-1533,P16-1101,0,0.0280898,"ll label-sequences in D. Above, 0(m) xt and xk (both in RD ) represent the contextual word embeddings of the t’th token in x and the k’th token in x0(m) , respectively, as obtained by running a deep sequence-model over x and over x0(m) . In all experiments we use BERT (Devlin et al., 2018), a model based on the Transformer 1 More precisely, we will set yˆt to be an instance of the 0(m) label type of which yk is a label token; this distinction between label types and tokens can make the exposition unnecessarily obscure, and so we avoid it when possible. 2 While recent sequence labeling models (Ma and Hovy, 2016; Lample et al., 2016), often model inter-label dependence with a first-order CRF (Lafferty et al., 2001), Devlin et al. (2018) have recently shown that excellent performance can be obtained by modeling labels as conditionally independent given a sufficiently expressive representation of x. 5364 architecture (Vaswani et al., 2017), to obtain contextual word embeddings. We fine-tune these contextual word embeddings by maximizing a latent-variable style probabilistic objective T X t=1 ln M X 0(m) X m=1 k: y 0(m) = y k p(yt = yk |x, D), (2) t where we sum over all individual label tokens in D tha"
P19-1533,J93-2004,0,0.0652998,"for the prediction of any label type present in the database D used at test time, and so we can easily predict label types unseen at training time without any additional retraining. 4 Data and Methods Our main experiments seek to determine both whether the label-agnostic copy-based approach introduced above results in competitive sequencelabeling performance on standard metrics, as well as whether this approach gives rise to better zeroshot transfer. Accordingly, our first set of experiments consider several standard sequence-labeling tasks and datasets, namely, POS tagging the Penn Treebank (Marcus et al., 1993) with both the standard Penn Treebank POS tags and Universal POS tags (Petrov et al., 2012; Nivre et al., 2016), and the CoNLL 2003 NER task (Sang and Buchholz, 2000; Sang and De Meulder, 2003). We compare with the sequence-labeling performance of BERT (Devlin et al., 2018), which we take to be near state of the art. We use the standard datasetsplits and evaluations for all tasks, and BIO encoding for all segment-level tagging tasks. We evaluate zero-shot transfer performance by training on one dataset and evaluating on another, without any retraining. In particular, we consider three zero-sho"
P19-1533,P16-2025,0,0.017106,"predictions. Retrieval-based approaches to structured prediction appear particularly compelling now with the recent successes in contextualized word embedding (McCann et al., 2017; Peters et al., 2018; Radford et al.; Devlin et al., 2018), which should allow for expressive representations of sentences and phrases, which in turn allow for better retrieval of neighbors for structured prediction. Finally, we note that there is a long history of transfer-learning based approaches to sequence labeling (Ando and Zhang, 2005; Daume III, 2007; Schnabel and Sch¨utze, 2014; Zirikly and Hagiwara, 2015; Peng and Dredze, 2016; Yang et al., 2017; Rodriguez et al., 2018, inter alia), though it is generally not zero-shot. There has, however, been recent work in zero-shot transfer for sequence labeling problems with binary tokenlabels (Rei and Søgaard, 2018). Nearest Neighbor Based Labeling While nearest-neighbor style approaches are compelling for many structured prediction problems, we will limit ourselves here to sequence-labeling problems, such as part-of-speech (POS) tagging or named-entity recognition (NER), where we are given a T -length sequence x = x1:T (which we will assume to be a sentence), and we must pre"
P19-1533,N18-1202,0,0.01254,"d interest in explicitly conditioning structured predictions on retrieved neighbors, especially in the context of language generation (Hashimoto et al., 2018; Guu et al., 2018; Gu et al., 2018; Weston et al., 2018), although much of this work uses neighbors as extra conditioning information within a sequenceto-sequence framework (Sutskever et al., 2014), rather than making discrete edits to neighbors in forming new predictions. Retrieval-based approaches to structured prediction appear particularly compelling now with the recent successes in contextualized word embedding (McCann et al., 2017; Peters et al., 2018; Radford et al.; Devlin et al., 2018), which should allow for expressive representations of sentences and phrases, which in turn allow for better retrieval of neighbors for structured prediction. Finally, we note that there is a long history of transfer-learning based approaches to sequence labeling (Ando and Zhang, 2005; Daume III, 2007; Schnabel and Sch¨utze, 2014; Zirikly and Hagiwara, 2015; Peng and Dredze, 2016; Yang et al., 2017; Rodriguez et al., 2018, inter alia), though it is generally not zero-shot. There has, however, been recent work in zero-shot transfer for sequence labeling pro"
P19-1533,petrov-etal-2012-universal,0,0.0591632,"Missing"
P19-1533,N18-1027,0,0.0205876,", 2018), which should allow for expressive representations of sentences and phrases, which in turn allow for better retrieval of neighbors for structured prediction. Finally, we note that there is a long history of transfer-learning based approaches to sequence labeling (Ando and Zhang, 2005; Daume III, 2007; Schnabel and Sch¨utze, 2014; Zirikly and Hagiwara, 2015; Peng and Dredze, 2016; Yang et al., 2017; Rodriguez et al., 2018, inter alia), though it is generally not zero-shot. There has, however, been recent work in zero-shot transfer for sequence labeling problems with binary tokenlabels (Rei and Søgaard, 2018). Nearest Neighbor Based Labeling While nearest-neighbor style approaches are compelling for many structured prediction problems, we will limit ourselves here to sequence-labeling problems, such as part-of-speech (POS) tagging or named-entity recognition (NER), where we are given a T -length sequence x = x1:T (which we will assume to be a sentence), and we must predict a corresponding T -length sequence of labels yˆ = yˆ1:T for x. We will assume that for any given task there are Z distinct labels, and denote x’s true but unknown labeling as y = y1:T ∈ {1, . . . , Z}T . Sequence-labeling is par"
P19-1533,C18-1168,0,0.0175522,"o structured prediction appear particularly compelling now with the recent successes in contextualized word embedding (McCann et al., 2017; Peters et al., 2018; Radford et al.; Devlin et al., 2018), which should allow for expressive representations of sentences and phrases, which in turn allow for better retrieval of neighbors for structured prediction. Finally, we note that there is a long history of transfer-learning based approaches to sequence labeling (Ando and Zhang, 2005; Daume III, 2007; Schnabel and Sch¨utze, 2014; Zirikly and Hagiwara, 2015; Peng and Dredze, 2016; Yang et al., 2017; Rodriguez et al., 2018, inter alia), though it is generally not zero-shot. There has, however, been recent work in zero-shot transfer for sequence labeling problems with binary tokenlabels (Rei and Søgaard, 2018). Nearest Neighbor Based Labeling While nearest-neighbor style approaches are compelling for many structured prediction problems, we will limit ourselves here to sequence-labeling problems, such as part-of-speech (POS) tagging or named-entity recognition (NER), where we are given a T -length sequence x = x1:T (which we will assume to be a sentence), and we must predict a corresponding T -length sequence of"
P19-1533,W00-0726,0,0.312792,"Missing"
P19-1533,W03-0419,0,0.204362,"Missing"
P19-1533,Q14-1002,0,0.0587826,"Missing"
P19-1533,D17-1283,0,0.0355998,"t al., 2018), which we take to be near state of the art. We use the standard datasetsplits and evaluations for all tasks, and BIO encoding for all segment-level tagging tasks. We evaluate zero-shot transfer performance by training on one dataset and evaluating on another, without any retraining. In particular, we consider three zero-shot transfer scenarios: training with Universal POS Tags on the Penn Treebank and then predicting the standard, fine-grained POS tags, training on the CoNLL 2003 NER data and predicting on the fine-grained OntoNotes NER data (Hovy et al., 2006) using the setup of Strubell et al. (2017), and finally training on the CoNLL 2003 chunking data and predicting on the CoNLL 2003 NER data. We again compare with a BERT baseline, where labels from the original task are deterministically mapped to the most frequent label on the new task with which they coincide.3 Our nearest-neighbor based models were finetuned by retrieving the 50 nearest neighbors of each sentence in a mini-batch of either size 16 or 20, and maximizing the objective (2) above. For training, nearest neighbors were determined based on cosine-similarity between the averaged toplevel (non-fine-tuned) BERT token embedding"
P19-1533,W18-5713,0,0.12252,"itionally consider a dynamic programming approach to sequence labeling in the presence of retrieved neighbors, which allows for controlling the number of distinct (copied) segments used to form a prediction, and leads to both more interpretable and accurate predictions. 1 Introduction Retrieve-and-edit style structured prediction, where a model retrieves a set of labeled nearest neighbors from the training data and conditions on them to generate the target structure, is a promising approach that has recently received renewed interest (Hashimoto et al., 2018; Guu et al., 2018; Gu et al., 2018; Weston et al., 2018). This approach captures the intuition that while generating a highly complex structure from scratch may be difficult, editing a sufficiently similar structure or set of structures may be easier. Recent work in this area primarily uses the nearest neighbors and their labels simply as an additional context for a sequence-to-sequence style model to condition on. While effective, these models may not explicitly capture the discrete operations (like copying) that allow for the neighbors to be edited into the target structure, making interpreting the behavior of the model difficult. Moreover, since"
P19-1533,P15-2064,0,0.0163652,"to neighbors in forming new predictions. Retrieval-based approaches to structured prediction appear particularly compelling now with the recent successes in contextualized word embedding (McCann et al., 2017; Peters et al., 2018; Radford et al.; Devlin et al., 2018), which should allow for expressive representations of sentences and phrases, which in turn allow for better retrieval of neighbors for structured prediction. Finally, we note that there is a long history of transfer-learning based approaches to sequence labeling (Ando and Zhang, 2005; Daume III, 2007; Schnabel and Sch¨utze, 2014; Zirikly and Hagiwara, 2015; Peng and Dredze, 2016; Yang et al., 2017; Rodriguez et al., 2018, inter alia), though it is generally not zero-shot. There has, however, been recent work in zero-shot transfer for sequence labeling problems with binary tokenlabels (Rei and Søgaard, 2018). Nearest Neighbor Based Labeling While nearest-neighbor style approaches are compelling for many structured prediction problems, we will limit ourselves here to sequence-labeling problems, such as part-of-speech (POS) tagging or named-entity recognition (NER), where we are given a T -length sequence x = x1:T (which we will assume to be a sen"
Q16-1018,N10-1083,0,0.408007,"Missing"
Q16-1018,J92-4003,0,0.507952,"g by learning hidden Markov models (HMMs) that are particularly well-suited for the problem. These HMMs, which we call anchor HMMs, assume that each tag is associated with at least one word that can have no other tag, which is a relatively benign condition for POS tagging (e.g., “the” is a word that appears only under the determiner tag). We exploit this assumption and extend the non-negative matrix factorization framework of Arora et al. (2013) to design a consistent estimator for anchor HMMs. In experiments, our algorithm is competitive with strong baselines such as the clustering method of Brown et al. (1992) and the log-linear model of BergKirkpatrick et al. (2010). Furthermore, it produces an interpretable model in which hidden states are automatically lexicalized by words. 1 In this work, we tackle unsupervised POS tagging with HMMs whose structure is deliberately suitable for POS tagging. These HMMs impose an assumption that each hidden state is associated with an observation state (“anchor word”) that can appear under no other state. For this reason, we denote this class of restricted HMMs by anchor HMMs. Such an assumption is relatively benign for POS tagging; it is reasonable to assume that"
Q16-1018,D10-1056,0,0.734643,"sed POS tagging. The goal of this task is to induce the correct sequence of POS tags (hidden states) given a sequence of words (observation states). The anchor condition corresponds to assuming that each POS tag has at least one word that occurs only under that tag. 5.1 Background on Unsupervised POS Tagging Unsupervised POS tagging has long been an active area of research (Smith and Eisner, 2005a; Johnson, 2007; Toutanova and Johnson, 2007; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010), but results on this task are complicated by varying assumptions and unclear evaluation metrics (Christodoulopoulos et al., 2010). Rather than addressing multiple alternatives for evaluating unsupervised POS tagging, we focus on a simple and widely used metric: many-to-one accuracy (i.e., we map each hidden state to the most frequently coinciding POS tag in the labeled data and compute the resulting accuracy). 5.1.1 Better Model v.s. Better Learning Vanilla HMMs are notorious for their mediocre performance on this task, and it is well known that they perform poorly largely because of model misspecification, not because of suboptimal parameter estimation (e.g., because EM gets stuck in local optima). More generally, a la"
Q16-1018,P14-1099,1,0.833172,"In particular, we can multiply Ω by any rank-m projection matrix Π ∈ Rd×m on the right side: if Ω satisfies the properties in Proposition 4.1, then so does ΩΠ with m-dimensional rows (ΩΠ)x = E[YI Π|XI = x] Since rank(Ω) = m, a natural choice of Π is the projection onto the best-fit m-dimensional subspace of the row space of Ω. We mention that previous works on the NMFlearning framework have employed various projection methods, but they do not examine relative merits of their choices. For instance, Arora et al. (2013) simply use random projection, which is convenient for theoretical analysis. Cohen and Collins (2014) use a projection based on canonical correlation analysis (CCA) without further exploration. In contrast, we give a full comparison of valid construction methods and find that the choice of Ω is crucial in practice. 4.4.3 Construction of Ω for the Brown Model We can formulate an alternative way to construct a valid Ω when the model is further restricted to be a Brown model. Since every observation is an anchor, Ox ∈ Rm has a single nonzero entry for every x. Thus the rows defined by Ωx = Ox / ||Ox ||(an indicator vector for the unique hidden state of x) form Input: bigram probabilities B, unig"
Q16-1018,P11-1061,0,0.0316239,"universal tags are used (Table 2). Many past works on POS induction predate the introduction of the universal tagset by Petrov et al. (2012) and thus report results with fine-grained tags. More recent works adopt the universal tagset but 255 LOG - LINEAR Accuracy 62.6 (1.1) 65.6 67.2 67.7 74.9 (1.5) Table 7: Many-to-one accuracy on the English data with 45 original tags. We use the same setting as in Table 2. For BW and LOG - LINEAR, we report the mean and the standard deviation (in parentheses) of 10 random restarts run for 1,000 iterations. they leverage additional resources. For instance, Das and Petrov (2011) and T¨ackstr¨om et al. (2013) use parallel data to project POS tags from a supervised source language. Li et al. (2012) use tag dictionaries built from Wiktionary. Thus their results are not directly comparable to ours.4 7 Conclusion We have presented an exact estimation method for learning anchor HMMs from unlabeled data. There are several directions for future work. An important direction is to extend the method to a richer family of models such as log-linear models or neural networks. Another direction is to further generalize the method to handle a wider class of HMMs by relaxing the anch"
Q16-1018,N06-1041,0,0.564643,"find that these features can significantly boost the tagging performance. 5 Experiments We evaluate our A-HMM learning algorithm on the task of unsupervised POS tagging. The goal of this task is to induce the correct sequence of POS tags (hidden states) given a sequence of words (observation states). The anchor condition corresponds to assuming that each POS tag has at least one word that occurs only under that tag. 5.1 Background on Unsupervised POS Tagging Unsupervised POS tagging has long been an active area of research (Smith and Eisner, 2005a; Johnson, 2007; Toutanova and Johnson, 2007; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010), but results on this task are complicated by varying assumptions and unclear evaluation metrics (Christodoulopoulos et al., 2010). Rather than addressing multiple alternatives for evaluating unsupervised POS tagging, we focus on a simple and widely used metric: many-to-one accuracy (i.e., we map each hidden state to the most frequently coinciding POS tag in the labeled data and compute the resulting accuracy). 5.1.1 Better Model v.s. Better Learning Vanilla HMMs are notorious for their mediocre performance on this task, and it is well known that they perform po"
Q16-1018,D07-1031,0,0.459756,"e this is of course not true in practice, we find that these features can significantly boost the tagging performance. 5 Experiments We evaluate our A-HMM learning algorithm on the task of unsupervised POS tagging. The goal of this task is to induce the correct sequence of POS tags (hidden states) given a sequence of words (observation states). The anchor condition corresponds to assuming that each POS tag has at least one word that occurs only under that tag. 5.1 Background on Unsupervised POS Tagging Unsupervised POS tagging has long been an active area of research (Smith and Eisner, 2005a; Johnson, 2007; Toutanova and Johnson, 2007; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010), but results on this task are complicated by varying assumptions and unclear evaluation metrics (Christodoulopoulos et al., 2010). Rather than addressing multiple alternatives for evaluating unsupervised POS tagging, we focus on a simple and widely used metric: many-to-one accuracy (i.e., we map each hidden state to the most frequently coinciding POS tag in the labeled data and compute the resulting accuracy). 5.1.1 Better Model v.s. Better Learning Vanilla HMMs are notorious for their mediocre performance"
Q16-1018,D12-1127,0,0.0613514,"Missing"
Q16-1018,P08-1100,0,0.0230141,"idden state to the most frequently coinciding POS tag in the labeled data and compute the resulting accuracy). 5.1.1 Better Model v.s. Better Learning Vanilla HMMs are notorious for their mediocre performance on this task, and it is well known that they perform poorly largely because of model misspecification, not because of suboptimal parameter estimation (e.g., because EM gets stuck in local optima). More generally, a large body of work points to the inappropriateness of simple generative models for unsupervised induction of linguistic structure 251 (Merialdo, 1994; Smith and Eisner, 2005b; Liang and Klein, 2008). Consequently, many works focus on using more expressive models such as log-linear models (Smith and Eisner, 2005a; Berg-Kirkpatrick et al., 2010) and Markov random fields (MRF) (Haghighi and Klein, 2006). These models are shown to deliver good performance even though learning is approximate. Thus one may question the value of having a consistent estimator for A-HMMs and Brown models in this work: if the model is wrong, what is the point of learning it accurately? However, there is also ample evidence that HMMs are competitive for unsupervised POS induction when they incorporate domain-specif"
Q16-1018,P13-2017,0,0.0556415,"Missing"
Q16-1018,J94-2001,0,0.857781,"many-to-one accuracy (i.e., we map each hidden state to the most frequently coinciding POS tag in the labeled data and compute the resulting accuracy). 5.1.1 Better Model v.s. Better Learning Vanilla HMMs are notorious for their mediocre performance on this task, and it is well known that they perform poorly largely because of model misspecification, not because of suboptimal parameter estimation (e.g., because EM gets stuck in local optima). More generally, a large body of work points to the inappropriateness of simple generative models for unsupervised induction of linguistic structure 251 (Merialdo, 1994; Smith and Eisner, 2005b; Liang and Klein, 2008). Consequently, many works focus on using more expressive models such as log-linear models (Smith and Eisner, 2005a; Berg-Kirkpatrick et al., 2010) and Markov random fields (MRF) (Haghighi and Klein, 2006). These models are shown to deliver good performance even though learning is approximate. Thus one may question the value of having a consistent estimator for A-HMMs and Brown models in this work: if the model is wrong, what is the point of learning it accurately? However, there is also ample evidence that HMMs are competitive for unsupervised"
Q16-1018,petrov-etal-2012-universal,0,0.11375,"Missing"
Q16-1018,P05-1044,0,0.0970804,"word given its tag. While this is of course not true in practice, we find that these features can significantly boost the tagging performance. 5 Experiments We evaluate our A-HMM learning algorithm on the task of unsupervised POS tagging. The goal of this task is to induce the correct sequence of POS tags (hidden states) given a sequence of words (observation states). The anchor condition corresponds to assuming that each POS tag has at least one word that occurs only under that tag. 5.1 Background on Unsupervised POS Tagging Unsupervised POS tagging has long been an active area of research (Smith and Eisner, 2005a; Johnson, 2007; Toutanova and Johnson, 2007; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010), but results on this task are complicated by varying assumptions and unclear evaluation metrics (Christodoulopoulos et al., 2010). Rather than addressing multiple alternatives for evaluating unsupervised POS tagging, we focus on a simple and widely used metric: many-to-one accuracy (i.e., we map each hidden state to the most frequently coinciding POS tag in the labeled data and compute the resulting accuracy). 5.1.1 Better Model v.s. Better Learning Vanilla HMMs are notorious for their medio"
Q16-1018,P15-1124,1,0.905872,"Missing"
Q16-1018,Q13-1001,0,0.0551433,"Missing"
W13-3507,P12-1024,1,0.909167,"g statistically consistent parameter estimates: even with very large amounts of data, EM is not guaranteed to estimate parameters which are close to the “correct” model parameters. In this paper, we derive a spectral algorithm for learning the parameters of R-HMMs. Unlike EM, this technique is guaranteed to find the true parameters of the underlying model under mild conditions on the singular values of the model. The algorithm we derive is simple and efficient, relying on singular value decomposition followed by standard matrix operations. We also describe the connection of R-HMMs to L-PCFGs. Cohen et al. (2012) present a spectral algorithm for L-PCFG estimation, but the na¨ıve transformation of the L-PCFG model and its spectral algorithm to R-HMMs is awkward and opaque. We therefore work through the non-trivial derivation the spectral algorithm for R-HMMs. We note that much of the prior work on spectral algorithms for discrete structures in NLP has shown limited experimental success for this family of algorithms (see, for example, Luque et al., 2012). Our experiments demonstrate empirical Introduction Consider the task of supervised sequence labeling. We are given a training set where the j’th train"
W13-3507,N13-1015,1,0.903398,"hidden state as well as the label. Unfortunately, estimating the parameters of an R-HMM is complicated by the unobserved hidden variables. A standard approach is to use the expectation-maximization (EM) algorithm which 56 Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 56–64, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics 3.1 success for the R-HMM spectral algorithm. The spectral algorithm performs competitively with EM on a phoneme recognition task, and is more stable with respect to the number of hidden states. Cohen et al. (2013) present experiments with a parsing algorithm and also demonstrate it is competitive with EM. Our set of experiments comes as an additional piece of evidence that spectral algorithms can function as a viable, efficient and more principled alternative to the EM algorithm. 2 We distinguish row vectors from column vectors when such distinction is necessary. We use a superscript > to denote the transpose operation. We write [n] to denote the set {1, 2, . . . , n} for any integer n ≥ 1. For any vector v ∈ Rm , diag(v) ∈ Rm×m is a diagonal matrix with entries v1 . . . vm . For any statement S, we us"
W13-3507,E12-1042,0,0.0933967,"le and efficient, relying on singular value decomposition followed by standard matrix operations. We also describe the connection of R-HMMs to L-PCFGs. Cohen et al. (2012) present a spectral algorithm for L-PCFG estimation, but the na¨ıve transformation of the L-PCFG model and its spectral algorithm to R-HMMs is awkward and opaque. We therefore work through the non-trivial derivation the spectral algorithm for R-HMMs. We note that much of the prior work on spectral algorithms for discrete structures in NLP has shown limited experimental success for this family of algorithms (see, for example, Luque et al., 2012). Our experiments demonstrate empirical Introduction Consider the task of supervised sequence labeling. We are given a training set where the j’th training example consists of a sequence of ob(j) (j) servations x1 ...xN paired with a sequence of (j) (j) labels a1 ...aN and asked to predict the correct labels on a test set of observations. A common approach is to learn a joint distribution over sequences p(a1 . . . aN , x1 . . . xN ) as a hidden Markov model (HMM). The downside of HMMs is that they assume each label ai is independent of labels before the previous label ai−1 . This independence"
W13-3507,P05-1010,0,0.0362596,"or performance. Spectral learning has been applied to related models beyond HMMs including: head automata for dependency parsing (Luque et al., 2012), tree-structured directed Bayes nets (Parikh et al., 2011), finite-state transducers (Balle et al., 2011), and mixture models (Anandkumar et al., 2012a; Anandkumar et al., 2012b). Of special interest is Cohen et al. (2012), who describe a derivation for a spectral algorithm for L-PCFGs. This derivation is the main driving force behind the derivation of our R-HMM spectral algorithm. For work on L-PCFGs estimated with EM, see Petrov et al. (2006), Matsuzaki et al. (2005), and Pereira and Schabes (1992). Petrov et al. (2007) proposes a split-merge EM procedure for phoneme recognition analogous to that used in latent-variable parsing. 3 Notation 3.2 Definition of an R-HMM An R-HMM is a 7-tuple hl, m, n, π, o, t, f i for integers l, m, n ≥ 1 and functions π, o, t, f where • [l] is a set of labels. • [m] is a set of hidden states. • [n] is a set of observations. • π(a, h) is the probability of generating a ∈ [l] and h ∈ [m] in the first position in the labeled sequence. • o(x|a, h) is the probability of generating x ∈ [n], given a ∈ [l] and h ∈ [m]. • t(b, h0 |a,"
W13-3507,P92-1017,0,0.411969,"ning has been applied to related models beyond HMMs including: head automata for dependency parsing (Luque et al., 2012), tree-structured directed Bayes nets (Parikh et al., 2011), finite-state transducers (Balle et al., 2011), and mixture models (Anandkumar et al., 2012a; Anandkumar et al., 2012b). Of special interest is Cohen et al. (2012), who describe a derivation for a spectral algorithm for L-PCFGs. This derivation is the main driving force behind the derivation of our R-HMM spectral algorithm. For work on L-PCFGs estimated with EM, see Petrov et al. (2006), Matsuzaki et al. (2005), and Pereira and Schabes (1992). Petrov et al. (2007) proposes a split-merge EM procedure for phoneme recognition analogous to that used in latent-variable parsing. 3 Notation 3.2 Definition of an R-HMM An R-HMM is a 7-tuple hl, m, n, π, o, t, f i for integers l, m, n ≥ 1 and functions π, o, t, f where • [l] is a set of labels. • [m] is a set of hidden states. • [n] is a set of observations. • π(a, h) is the probability of generating a ∈ [l] and h ∈ [m] in the first position in the labeled sequence. • o(x|a, h) is the probability of generating x ∈ [n], given a ∈ [l] and h ∈ [m]. • t(b, h0 |a, h) is the probability of genera"
W13-3507,P06-1055,0,0.0566726,"his type are crucial for performance. Spectral learning has been applied to related models beyond HMMs including: head automata for dependency parsing (Luque et al., 2012), tree-structured directed Bayes nets (Parikh et al., 2011), finite-state transducers (Balle et al., 2011), and mixture models (Anandkumar et al., 2012a; Anandkumar et al., 2012b). Of special interest is Cohen et al. (2012), who describe a derivation for a spectral algorithm for L-PCFGs. This derivation is the main driving force behind the derivation of our R-HMM spectral algorithm. For work on L-PCFGs estimated with EM, see Petrov et al. (2006), Matsuzaki et al. (2005), and Pereira and Schabes (1992). Petrov et al. (2007) proposes a split-merge EM procedure for phoneme recognition analogous to that used in latent-variable parsing. 3 Notation 3.2 Definition of an R-HMM An R-HMM is a 7-tuple hl, m, n, π, o, t, f i for integers l, m, n ≥ 1 and functions π, o, t, f where • [l] is a set of labels. • [m] is a set of hidden states. • [n] is a set of observations. • π(a, h) is the probability of generating a ∈ [l] and h ∈ [m] in the first position in the labeled sequence. • o(x|a, h) is the probability of generating x ∈ [n], given a ∈ [l] a"
W13-3507,D07-1094,0,0.0373788,"ated models beyond HMMs including: head automata for dependency parsing (Luque et al., 2012), tree-structured directed Bayes nets (Parikh et al., 2011), finite-state transducers (Balle et al., 2011), and mixture models (Anandkumar et al., 2012a; Anandkumar et al., 2012b). Of special interest is Cohen et al. (2012), who describe a derivation for a spectral algorithm for L-PCFGs. This derivation is the main driving force behind the derivation of our R-HMM spectral algorithm. For work on L-PCFGs estimated with EM, see Petrov et al. (2006), Matsuzaki et al. (2005), and Pereira and Schabes (1992). Petrov et al. (2007) proposes a split-merge EM procedure for phoneme recognition analogous to that used in latent-variable parsing. 3 Notation 3.2 Definition of an R-HMM An R-HMM is a 7-tuple hl, m, n, π, o, t, f i for integers l, m, n ≥ 1 and functions π, o, t, f where • [l] is a set of labels. • [m] is a set of hidden states. • [n] is a set of observations. • π(a, h) is the probability of generating a ∈ [l] and h ∈ [m] in the first position in the labeled sequence. • o(x|a, h) is the probability of generating x ∈ [n], given a ∈ [l] and h ∈ [m]. • t(b, h0 |a, h) is the probability of generating b ∈ [l] and h0 ∈"
W15-1511,N10-1083,0,0.0348946,"Missing"
W15-1511,J92-4003,0,0.766904,"Missing"
W15-1511,D10-1056,0,0.0697824,"Missing"
W15-1511,A88-1019,0,0.337735,"Missing"
W15-1511,W99-0613,1,0.628841,"Missing"
W15-1511,P11-1061,0,0.0827887,"Missing"
W15-1511,N13-1014,0,0.169427,"Missing"
W15-1511,N06-1041,0,0.102163,"Missing"
W15-1511,D07-1031,0,0.129676,"Missing"
W15-1511,N13-1139,0,0.152127,"Missing"
W15-1511,D12-1127,0,0.0295142,"Missing"
W15-1511,P13-2017,0,0.036997,"Missing"
W15-1511,N04-1043,0,0.292628,"types partition word types while imposing a first-order sequence structure on tag types. 2.2 Brown et al. (1992) model This class of restricted HMMs is precisely the model proposed by Brown et al. (1992)—henceforth the Brown model. A popular use of this model is agglomerative word clustering: the result is a hierarchy over word types, such as the one shown in Figure 1(a). In practice, each word type is represented as a bit-string indicating the path from the root. These bit-strings have been used as discrete (binary) features in various natural language tasks such as named-entity recognition (Miller et al., 2004) and dependency parsing (Koo et al., 2008). Recently, Stratos et al. (2014) showed that a variant of canonical correlation analysis (CCA) (Hotelling, 1936) can be used to provably recover the clusters under the Brown model. Under this method, each word is represented as an mdimensional vector where m is the number of hidden states in the model: see Figure 1(b) for illustration. This can be used as m real-valued features in discrminative models. Note that real-valued representations can reflect ambiguity (e.g., set in the illustration) which can be seen as a benefit over discrete representation"
W15-1511,W07-1516,0,0.066709,"Missing"
W15-1511,P05-1044,0,0.127464,"Missing"
W15-1511,Q13-1001,0,0.0540522,"Missing"
W15-1511,P95-1026,0,0.596539,"Missing"
W15-1511,P08-1068,1,\N,Missing
W17-4119,P16-1156,0,0.0345433,"Missing"
W17-4119,P15-1033,0,0.0437899,"Missing"
W17-4119,N15-1184,0,0.0185183,"ogy transformation based on pretraining can be beneficial. Pre-trained word embeddings improve the performance of a neural model at the cost of increasing the model size. We propose to benefit from this resource without paying the cost by operating strictly at the sublexical level. Our approach is quite simple: before task-specific training, we first optimize sub-word parameters to reconstruct pre-trained word embeddings using various distance measures. We report interesting results on a variety of tasks: word similarity, word analogy, and part-of-speech tagging. 1 2 Introduction Related Work Faruqui et al. (2015) “retrofit” embeddings against semantic lexicons such as PPDB or WordNet. Cotterell et al. (2016) leverage existing morphological lexicons to incorporate sub-word components. The aim and scope of our work are clearly different: we are interested in training a strictly sublexical model that only operates over characters (which has the benefit of smaller model size) and yet somehow exploit pre-trained word embeddings in the process. Our work is also related to knowledge distillation which refers to training a smaller “student” network to perform better by learning from a larger “teacher” network"
W17-4119,D16-1139,0,0.0324067,"rs (which has the benefit of smaller model size) and yet somehow exploit pre-trained word embeddings in the process. Our work is also related to knowledge distillation which refers to training a smaller “student” network to perform better by learning from a larger “teacher” network. We adopt this terminology and refer to pre-trained word embeddings as the teacher and sub-lexical embeddings as the student. This problem has mostly been considered for classification and framed as matching the probabilities of the student to the probabilities of the teacher (Ba and Caruana, 2014; Li et al., 2014; Kim and Rush, 2016). In contrast, we work directly with representations in Euclidean space. Word embeddings trained from a large quantity of unlabled text are often important for a neural model to reach state-of-the-art performance. They are shown to improve the accuracy of partof-speech (POS) tagging from 97.13 to 97.55 (Ma and Hovy, 2016), the F1 score of named-entity recognition (NER) from 83.63 to 90.94 (Lample et al., 2016), and the UAS of dependency parsing from 93.1 to 93.9 (Kiperwasser and Goldberg, 2016). On the other hand, the benefit comes at the cost of a bigger model which now stores these embedding"
W17-4119,Q16-1023,0,0.0244815,"ing the probabilities of the student to the probabilities of the teacher (Ba and Caruana, 2014; Li et al., 2014; Kim and Rush, 2016). In contrast, we work directly with representations in Euclidean space. Word embeddings trained from a large quantity of unlabled text are often important for a neural model to reach state-of-the-art performance. They are shown to improve the accuracy of partof-speech (POS) tagging from 97.13 to 97.55 (Ma and Hovy, 2016), the F1 score of named-entity recognition (NER) from 83.63 to 90.94 (Lample et al., 2016), and the UAS of dependency parsing from 93.1 to 93.9 (Kiperwasser and Goldberg, 2016). On the other hand, the benefit comes at the cost of a bigger model which now stores these embeddings as additional parameters. In this study, we propose to benefit from this resource without paying the cost by operating strictly at the sub-lexical level. Specifically, we optimize the character-level parameters of the model to reconstruct the word embeddings prior to task-specific training. We frame the problem as distance minimization and consider various metrics suitable for different applications, for example Manhattan distance and negative cosine similarity. 130 Proceedings of the First W"
W17-4119,P15-1124,1,0.899574,"Missing"
W17-4119,N16-1030,0,0.0360194,"s problem has mostly been considered for classification and framed as matching the probabilities of the student to the probabilities of the teacher (Ba and Caruana, 2014; Li et al., 2014; Kim and Rush, 2016). In contrast, we work directly with representations in Euclidean space. Word embeddings trained from a large quantity of unlabled text are often important for a neural model to reach state-of-the-art performance. They are shown to improve the accuracy of partof-speech (POS) tagging from 97.13 to 97.55 (Ma and Hovy, 2016), the F1 score of named-entity recognition (NER) from 83.63 to 90.94 (Lample et al., 2016), and the UAS of dependency parsing from 93.1 to 93.9 (Kiperwasser and Goldberg, 2016). On the other hand, the benefit comes at the cost of a bigger model which now stores these embeddings as additional parameters. In this study, we propose to benefit from this resource without paying the cost by operating strictly at the sub-lexical level. Specifically, we optimize the character-level parameters of the model to reconstruct the word embeddings prior to task-specific training. We frame the problem as distance minimization and consider various metrics suitable for different applications, for exa"
W17-4119,W14-1618,0,0.0863032,"Missing"
W17-4119,D15-1161,0,0.0767105,"Missing"
W17-4119,P16-1101,0,0.0319888,"re-trained word embeddings as the teacher and sub-lexical embeddings as the student. This problem has mostly been considered for classification and framed as matching the probabilities of the student to the probabilities of the teacher (Ba and Caruana, 2014; Li et al., 2014; Kim and Rush, 2016). In contrast, we work directly with representations in Euclidean space. Word embeddings trained from a large quantity of unlabled text are often important for a neural model to reach state-of-the-art performance. They are shown to improve the accuracy of partof-speech (POS) tagging from 97.13 to 97.55 (Ma and Hovy, 2016), the F1 score of named-entity recognition (NER) from 83.63 to 90.94 (Lample et al., 2016), and the UAS of dependency parsing from 93.1 to 93.9 (Kiperwasser and Goldberg, 2016). On the other hand, the benefit comes at the cost of a bigger model which now stores these embeddings as additional parameters. In this study, we propose to benefit from this resource without paying the cost by operating strictly at the sub-lexical level. Specifically, we optimize the character-level parameters of the model to reconstruct the word embeddings prior to task-specific training. We frame the problem as dista"
W17-4119,P13-2017,0,0.0496315,"Missing"
W17-4119,N13-1090,0,0.0745363,"Missing"
W17-4302,W03-0419,0,0.11357,"Missing"
W17-4302,W03-3023,0,0.0090613,"on for Computational Linguistics 2 Related Work describe it here for completeness. The model parameters Θ associated with this base network are Our work is directly inspired by Lample et al. (2016) who demonstrate that a simple neural architecture based on BiLSTMs achieves state-ofthe-art performance on NER with no external features. They propose two models. The first makes structured prediction of NER labels with a CRF loss (LSTM-CRF) using the conventional BIO-label scheme. The second, which performs slightly worse, uses a shift-reduce framework mirroring tansition-based dependency parsing (Yamada and Matsumoto, 2003). While the latter also scales linearly in the number of types and produces embeddings of predicted mentions, our approach is quite different. We frame the problem as multitasking and do not need the stack/buffer data structure. Semi-Markov models (Kong et al., 2015; Sarawagi et al., 2004) explicitly incorporate the segment structure but are computationally intensive (quadratic in the sentence length). Multitasking has been shown to be effective in numerous previous works (Collobert et al., 2011; Yang et al., 2016; Kiperwasser and Goldberg, 2016). This is especially true with neural networks w"
W17-4302,Q16-1023,0,0.0173062,"ework mirroring tansition-based dependency parsing (Yamada and Matsumoto, 2003). While the latter also scales linearly in the number of types and produces embeddings of predicted mentions, our approach is quite different. We frame the problem as multitasking and do not need the stack/buffer data structure. Semi-Markov models (Kong et al., 2015; Sarawagi et al., 2004) explicitly incorporate the segment structure but are computationally intensive (quadratic in the sentence length). Multitasking has been shown to be effective in numerous previous works (Collobert et al., 2011; Yang et al., 2016; Kiperwasser and Goldberg, 2016). This is especially true with neural networks which greatly simplify joint optimization across multiple objectives. Most of these works consider multitasking across different problems. In contrast, we decompose a single problem (NER) into two natural subtasks and perform them jointly. Particularly relevant in this regard is the parsing model of Kiperwasser and Goldberg (2016) which multitasks edge prediction and classification. LSTMs (Hochreiter and Schmidhuber, 1997), and other variants of recurrent neural networks such as GRUs (Chung et al., 2014), have recently been wildly successful in va"
W17-4302,N16-1030,0,0.394649,"ly in the number of types. Furthermore, by construction, the model induces type-disambiguating embeddings of predicted mentions. 1 1. Boundaries of mentions in the sentence. 2. Entity types of the boundaries. Introduction Crucially, during training, the errors of these two predictions are minimized jointly. One might suspect that the separation could degrade performance; neither prediction accounts for the correlation between entity types. But we find that this is not the case due to joint optimization. In fact, our model performs competitively with fully structured models such as BiLSTMCRFs (Lample et al., 2016), implying that the model is able to capture the entity correlation indirectly by multitasking. On the other hand, the model scales linearly in the number of types and induces segment-level embeddings of predicted mentions that are type-disambiguating by construction. A popular convention in segmentation tasks such as named-entity recognition (NER) and chunking is the so-called “BIO”-label scheme. It hard-codes boundary detection and type prediction into labels using the indicators “B” (Beginning), “I” (Inside), and “O” (Outside). For instance, the sentence Where is John Smith is tagged as Whe"
W17-4302,W03-0430,0,0.536312,"Note that the runtime complexity of boundary detection is constant despite dynamic programming since the number of tags is fixed (three). # words/sec 3889 4825 # words/sec 495 4949 Table 1: Test F1 scores on CoNLL 2003 and OntoNotes newswire portion. Type classification loss Given a mention boundary 1 ≤ s ≤ t ≤ n, we predict its type using (1) as follows. We introduce an additional pair of LSTMs φEf , φEb : R200 × R200 → R200 and compute a corresponding mention representation µ ∈ R|E |as  E fjE = φEf hj , fj−1 ∀j = s . . . t  bEj = φEb hj , bEj+1 ∀j = t . . . s  E E (3) µ = q ft ⊕ bs Model McCallum and Li (2003) Collobert et al. (2011) Lample et al. (2016)–Greedy Lample et al. (2016)–Stack Lample et al. (2016)–CRF Mention2Vec F1 84.04 89.59 89.15 90.33 90.94 90.90 Table 2: Test F1 scores on CoNLL 2003. 4 where q : R400 → R|E |is again a feedforward network that adjusts the vector length to |E|.2 We write Θ2 to refer to the parameters in φEf , φEb , q. Now we can optimize the conditional probability of the correct type τ : p(τ |hs . . . ht ) ∝ exp (µτ ) F1 90.22 90.90 F1 90.77 89.37 Experiments Data We use two NER datasets: CoNLL 2003 which has four entity types PER, LOC, ORG and MISC (Tjong Kim Sang"
W18-2901,D10-1115,0,0.0489495,"tters through Unicode decomposition (Stratos, 2017). The present work also aims to incorporate subword information into word embeddings, and does so by modeling morphology. However, this work differs from those above in the means of composition, as our method is based principally on function application. Here, we take derivational morphemes (i.e. affixes) as functions, and stems as arguments. Broadly speaking, this work can be seen as an extension of Baroni et al. (2014)’s compositional distributional semantic framework to the sub-word level. At a more narrow level, our work is reminiscent of Baroni and Zamparelli (2010), who model adjectives as matrices and nouns as vectors, and work like Hartung et al. (2017), which seeks to learn composition functions in addition to vector representations. 3 ᅵᄌ ᄎ ᅳᄇ ᅥᄀ ᅥᄃ ᆯᄋ ᅳ ᅵ ᆯᄋ ᅳ ᅵ(=affixes) ᅵᄌ ᄎ ᅳᄇ ᅥᄀ ᅥ(=stems) ᄃ ᅥᄀ ᄇ ᅥ ᆯ ᅳ ᄃ ᅵ ᄋ CHEESE BURGER PL NOM Figure 1: Decomposition of ᄎ ᅵᄌ ᅳᄇ ᅥᄀ ᅥᄃ ᆯᄋ ᅳ ᅵ (=cheese.burger-PL . NOM) positional representation of a word from these constituent parts using a dynamic neural-network architecture. The architecture can be conceptually broken into three steps: (i) decomposing the word into its constituent stems and affixes, (ii) computi"
W18-2901,D17-1075,1,0.73406,"of Chicago danedmiston@uchicago.edu Karl Stratos Toyota Technical Institute at Chicago stratos@ttic.edu Abstract We test the viability of the linguistically motivated S TA FF N ET on a dependency parsing task in Korean—a language rich in derivational morphology. Here, we achieve promising results for infusing explicit linguistic analyses into NLP architectures. Specifically, the architecture achieves results which significantly outperform simple wordembedding baselines, and are competitive with other sub-word architectures which constitute the current state-of-the-art for this task in Korean (Stratos, 2017). We therefore submit the following as our contributions: This work introduces a linguistically motivated architecture, which we label S TA FF N ET, for composing morphemes to derive word embeddings. The principal novelty in the work is to treat stems as vectors and affixes as functions over vectors. In this way, our model’s architecture more closely resembles the compositionality of morphemes in natural language. Such a model stands in opposition to models which treat morphemes uniformly, making no distinction between stem and affix. We run this new architecture on a dependency parsing task i"
W18-2901,W17-4105,0,0.0909726,"ers (Ma and Hovy, 2016; Kim et al., 2016) or morphemes (Luong et al., 2013; Botha and Blunsom, 2014; Cotterell et al., 2016). For languages with complex orthography, sub-character models have also been proposed. Previous works consider modeling graphical components of Chinese characters called radicals (Sun et al., 2014; Yin et al., 2016) and syllable-blocks of Korean characters—either 1 Proceedings of the Workshop on the Relevance of Linguistic Structure in Neural Architectures for NLP, pages 1–5 c Melbourne, Australia, July 19, 2018. 2018 Association for Computational Linguistics as atomic (Choi et al., 2017) or as non-linear functions of underlying jamo letters through Unicode decomposition (Stratos, 2017). The present work also aims to incorporate subword information into word embeddings, and does so by modeling morphology. However, this work differs from those above in the means of composition, as our method is based principally on function application. Here, we take derivational morphemes (i.e. affixes) as functions, and stems as arguments. Broadly speaking, this work can be seen as an extension of Baroni et al. (2014)’s compositional distributional semantic framework to the sub-word level. At"
W18-2901,P16-1156,0,0.068293,"Missing"
