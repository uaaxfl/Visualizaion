2005.jeptalnrecital-court.12,W03-3002,1,0.828846,"Missing"
2005.jeptalnrecital-court.12,A94-1013,0,0.0922704,"Missing"
2006.jeptalnrecital-poster.13,2005.jeptalnrecital-court.12,1,0.734547,"Missing"
2006.jeptalnrecital-poster.13,W02-0506,0,0.0382457,"Missing"
2006.jeptalnrecital-poster.13,N04-4038,0,0.0469883,"Missing"
2006.jeptalnrecital-poster.13,2005.jeptalnrecital-recitalcourt.3,0,0.0855431,"Missing"
2008.jeptalnrecital-long.11,W04-1013,0,0.00768667,"Missing"
2008.jeptalnrecital-long.11,C04-1063,0,0.0619683,"Missing"
2008.jeptalnrecital-long.11,J98-3005,0,0.10348,"Missing"
2008.jeptalnrecital-long.11,H01-1054,0,0.0534356,"Missing"
2009.jeptalnrecital-court.17,2001.jeptalnrecital-poster.10,0,0.0112761,"Missing"
2010.jeptalnrecital-court.31,W03-0419,0,0.10891,"Missing"
2010.jeptalnrecital-court.31,farber-etal-2008-improving,0,0.0378029,"Missing"
2010.jeptalnrecital-court.31,A97-1030,0,0.0931199,"Missing"
2010.jeptalnrecital-court.31,zaghouani-etal-2010-adapting,0,0.037104,"Missing"
2010.jeptalnrecital-long.21,P92-1008,0,0.285809,"s avons répertorié les travaux existants selon ces deux niveaux de traitement à savoir, niveau reconnaissance et niveau compréhension. Traitement des disfluences dans le cadre de la compréhension automatique de l’oral arabe spontané 3.1 Niveau reconnaissance La reconnaissance automatique de la parole consiste à extraire la liste des mots contenue dans un signal vocal. À ce niveau de traitement et à notre connaissance, il y a une seule approche qui a été proposée pour le traitement des disfluences à savoir, l’approche de SRI proposée par Bear et al. au sein de Standford Research Instute (SRI) (Bear et al., 1992). Le travail de Bear et al. consiste, en première étape, à proposer un schème d’annotation des disfluences qui combine la simplicité à la finesse nécessaire pour la représentation des différentes formes de disfluences (Bear et al., 1992). Il s’agit, ensuite, de combiner l’analyse syntaxique et sémantique (afin de réduire la surgénération de patrons) avec la technique de la reconnaissance de patrons. Le but étant de détecter et de corriger les répétitions simples et les anomalies syntaxiques simples comme « a the » (Bear et al., 1992). L’inconvénient de cette combinaison est qu’elle est incompa"
2012.amta-caas14.10,W06-2810,0,0.0705746,"Missing"
2012.amta-caas14.10,W11-1212,0,0.0377799,"Missing"
2012.amta-caas14.10,J93-2003,0,0.0252116,"Missing"
2012.amta-caas14.10,declerck-etal-2006-multilingual,0,0.0712159,"Missing"
2012.amta-caas14.10,1998.amta-tutorials.5,0,0.0456587,"different languages that are not translations of each other” (Bowker and Pearson, 2002), but contains texts from the same domain. Comparable corpora have several obvious advantages over parallel corpora. They are available on the Web in large quantities for many languages and domains and many texts with similar content are produced every day (e.g. multilingual news feeds) (Skadiņa et al, 2010), but they are not organized. Also, bilingual lexicons are the key component of all cross-lingual NLP applications such as machine translation (Och and Ney, 2003) and crosslanguage information retrieval (Grefenstette, 1998). Parallel texts – as the most important resource in statistical machine translation (SMT) – appear to be limited in quantity, genre and language coverage. Providing more comparable corpora essentially boosts the coverage and the quality of machine translation system, especially for lesscovered languages and domains. In this paper we describe the extraction process of large comparable corpora and bilingual lexicons for Arabic and French language from a multilingual web-based encyclopedia, Wikipedia. We propose to build bilingual resources as follows: first comparable corpora from Wikipedia usi"
2012.amta-caas14.10,N10-1063,0,0.01734,"cordance with fast growth of Wikipedia, many works have been published in the last years focused on its use and exploitation for multilingual tasks in natural language processing: in this paper, our main concern is the use of Wikipedia as a source of comparable corpora and bilingual lexicon extraction. Li et al. (2010) consider Wikipedia as a comparable corpus, they align articles pairs based on inter-language links for the extraction of parallel sentences. Patry and Langlais (2011) also concentrate on documents pairs that are linked across language for extracting parallel documents. However, Smith et al. (2010) and Mohammadi and QasemAghaee (2010) use inter-language link to 74 identify aligned comparable Wikipedia documents. Sadat (2010) proposes an approach to build comparable corpora from Wikipedia encyclopedia. First, the author considers a preliminary query Q in a source language to input in Wikipedia search engine. The resulting document is used as a first document for the corpus in the source language. The usage of the inter-language link in the target language for this document leads to a corpus in a target language. Following this first step and exploiting the links in the same document as w"
2012.amta-caas14.10,N07-1025,0,0.0268377,"tions (e.g., DSM-IV redirects to Diagnostic and Statistical Manual of Mental Disorders), Alternative spellings or punctuation (e.g. Al-Jazeera redirects to Al Jazeera), etc. Link Texts This is a link to another page in Wikipedia. The link text can correspond to the title of the target article (the syntax will be: [[article title]]), or differ from the title of the target article (with the following syntax: [[article title |link text]]). As a rich and free resource, Wikipedia has been successfully used as an external resource in many natural language processing tasks (Buscaldi and Rosso, 2006; Mihalcea, 2007; Nakayama et al., 2007). 3 State of the Art In accordance with fast growth of Wikipedia, many works have been published in the last years focused on its use and exploitation for multilingual tasks in natural language processing: in this paper, our main concern is the use of Wikipedia as a source of comparable corpora and bilingual lexicon extraction. Li et al. (2010) consider Wikipedia as a comparable corpus, they align articles pairs based on inter-language links for the extraction of parallel sentences. Patry and Langlais (2011) also concentrate on documents pairs that are linked across lan"
2012.amta-caas14.10,J03-1002,0,0.0145644,"omparable corpora. Comparable corpora are “sets of texts in different languages that are not translations of each other” (Bowker and Pearson, 2002), but contains texts from the same domain. Comparable corpora have several obvious advantages over parallel corpora. They are available on the Web in large quantities for many languages and domains and many texts with similar content are produced every day (e.g. multilingual news feeds) (Skadiņa et al, 2010), but they are not organized. Also, bilingual lexicons are the key component of all cross-lingual NLP applications such as machine translation (Och and Ney, 2003) and crosslanguage information retrieval (Grefenstette, 1998). Parallel texts – as the most important resource in statistical machine translation (SMT) – appear to be limited in quantity, genre and language coverage. Providing more comparable corpora essentially boosts the coverage and the quality of machine translation system, especially for lesscovered languages and domains. In this paper we describe the extraction process of large comparable corpora and bilingual lexicons for Arabic and French language from a multilingual web-based encyclopedia, Wikipedia. We propose to build bilingual reso"
2012.amta-caas14.10,2010.jeptalnrecital-demonstration.6,1,0.821532,"Missing"
2012.amta-caas14.10,skadina-etal-2012-collecting,0,0.0505863,"Missing"
2012.amta-caas14.10,C96-2141,0,0.104461,"g them. The preprocessing step consists of removing all Arabic and French stop words. The step of word alignment presents several challenges. First, the alignments are not necessarily contiguous. Two consecutive words in the source sentence can be aligned with two words arbitrarily distant from the target sentence. This is called distortion. Second, a source language word can be aligned to many words in the target language; that is defined as fertility. The alignment of words of each title is based on IBM models [1, 2, 3, 4, 5] (Brown et al., 1993) in combination with the Hidden Markov Model (Vogel et al., 1996). These standard models have already proven their effectiveness in many researches. The five IBM models estimate the probability P(fr|ar) and P(ar|fr), for which fr is a French word and ar is an Arabic word. Each model is based on the parameters estimated by the previous model and incorporates new features such as distortion, fertility, etc. The Hidden Markov Model (HMM usually appointed) (Vogel et al., 1996) is an improvement of IBM2 model. It explicitly models the distance between the alignment of the current word and an alignment of the previous word. We used the open source toolkit GIZA++"
2013.mtsummit-wpt.5,W06-2810,0,0.08973,"Missing"
2013.mtsummit-wpt.5,2011.eamt-1.5,0,0.0210503,"s using the combined SMT with rule-based MT (Ehara, 2007; Wang, 2009; Jin, 2010). In order to obtain a large coverage without losing quality in the translation, Espana-Bonet et al. (2011) proposed a combination between a grammar-based multilingual translation system and a specialized SMT system. Enache et al. (2012) presented a hybrid translation system specifically designed to deal with patent translation. Indeed, the patent language follows a formal style adequate to be analysed with a grammar, but at the same time uses a rich and particular vocabulary adequate to be gathered statistically. Ceausu et al. (2011) presented a number of methods for adapting SMT to the patent domain. They proposed some patent-specific preprocessing to resolve the problem of long sentences and references to elements in figure. Ma et al. (2011) made changes to the SMT training procedure in order to better handle the special characteristics of patent data. He demonコンタクトホール 61 内に (i.e., here, in the contacth all 61 of the second, ) will have a proper segmentation as follows:|ここ |で|は|、 |第 2 |の |コンタク ト |ホール |6 1 |内 |に |. We used Mecab tool (Kudo, 2002) to segment the Japanese texts. English pre-processing simply included down-"
2013.mtsummit-wpt.5,2012.eamt-1.61,0,0.0208078,"Missing"
2013.mtsummit-wpt.5,J03-3002,0,0.04983,", a development data set of 2,000 pairs of bilingual sentences in Japanese and English and a test data set of 2,300 pairs of patent sentences in Japanese. Furthermore, a set of 2,300 patent sentences in English is released at the end of the evaluations, to be considered as a reference set of the Japanese test sentences. 5 Parallel Corpora Extraction from Wikipedia In most previous works on extraction of parallel sentences or phrases from comparable corpora, some coarse document-level similarity is used to determine which document pairs contain parallel data. For identifying similar web pages, Resnik and Smith (2003) compare the HTML structure. Munteanu and Marcu (2005) use publication date and vector-based similarity (after projecting words through a bilingual dictionary) to identify similar news articles. Wikipedia is an online collaborative encyclopaedia available for a wide variety of languages. There are 24 language editions with at least 100,000 articles. Currently (May 2013), the English Wikipedia is the largest one with over then 4 millions articles. Whereas, Japanese Wikipedia contains approximately 862,000 articles2. Wikipedia contains annotated article alignments. Indeed, articles on the same t"
2013.mtsummit-wpt.5,2011.mtsummit-wpt.7,0,0.0730463,"parts: an introductory phrase and the body of the claim usually linked by a conjunction. It is in the body of the claim where there is the 1 http://ntcir.nii.ac.jp/PatentMT-2/ 34 Proceedings of the 5th Workshop on Patent Translation, Nice, September 2, 2013. Yokoyama, S., ed. © 2013 Rahma Sellami, Fatiha Sadat, Lamia Hadrich Belguith. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. speciﬁc legal description of the exact invention. Therefore, claims are written in a specific style and use a very speciﬁc vocabulary of the patent domain (Espana-Bonet et al, 2011). Some of the patent document characteristics make MT easier, e.g., the presence of wellstructured sentences and less ambiguity of word meanings. On the other hand, some characteristics become challenges for MT, e.g., long and complicated sentence structures, technical terminology and new terms that are originally defined by patent applicants. Compared to the newswire Japanese text data, the Japanese patent text has some specific characteristics such as legalese, technical terminology and long sentences. Also, patent text includes significantly more special strings that are not written in Japa"
2013.mtsummit-wpt.5,J82-2005,0,0.764414,"Missing"
2013.mtsummit-wpt.5,N10-1063,0,0.0144965,"ied with a wide range of lexical and semantic information such as part of speech, word sense, gloss, etymology, pronunciation, declension, examples, sample quotations, translations, collocations, derived terms, and usage notes. In the current research, we have used the English Wiktionary since it contains more entries than the Japanese one; thus, we extracted all English terms having translations in Japanese. In total, we have extracted 1,528,475 pairs of English-Japanese terms. This process was based on the XML version of the English Wiktionary available online 5 and created by Sajous et al. (2010). Each entry in the XML Wiktionary contains an English term and its translations, gloss, POS, etc. In our experiments, we have considered the alternative translation of a term to be likely equal. We can envisage attributing a score for each alternative using a disambiguation technique based on a statistical probability, which will consider the context of a term in the training corpus and the semantic information proposed by the Wiktionary. 7 We re-implemented our system described in Sadat et al. (2013); we implemented a 5-gram language model instead of a 3-gram language model. Our results were"
2013.mtsummit-wpt.5,2011.mtsummit-wpt.8,0,0.0856555,"Missing"
2013.mtsummit-wpt.5,W02-2016,0,0.172305,"Missing"
2013.mtsummit-wpt.5,J05-4003,0,0.0331979,"sentences in Japanese and English and a test data set of 2,300 pairs of patent sentences in Japanese. Furthermore, a set of 2,300 patent sentences in English is released at the end of the evaluations, to be considered as a reference set of the Japanese test sentences. 5 Parallel Corpora Extraction from Wikipedia In most previous works on extraction of parallel sentences or phrases from comparable corpora, some coarse document-level similarity is used to determine which document pairs contain parallel data. For identifying similar web pages, Resnik and Smith (2003) compare the HTML structure. Munteanu and Marcu (2005) use publication date and vector-based similarity (after projecting words through a bilingual dictionary) to identify similar news articles. Wikipedia is an online collaborative encyclopaedia available for a wide variety of languages. There are 24 language editions with at least 100,000 articles. Currently (May 2013), the English Wikipedia is the largest one with over then 4 millions articles. Whereas, Japanese Wikipedia contains approximately 862,000 articles2. Wikipedia contains annotated article alignments. Indeed, articles on the same topic in different languages are connected via “Interla"
2013.mtsummit-wpt.5,2007.mtsummit-wpt.4,0,0.128235,"Missing"
2013.mtsummit-wpt.5,P03-1021,0,0.0255307,"Missing"
2013.mtsummit-wpt.5,J03-1002,0,0.00744663,"Missing"
2013.mtsummit-wpt.5,2001.mtsummit-papers.68,0,0.165165,"tructures, technical terminology and new terms that are originally defined by patent applicants. Compared to the newswire Japanese text data, the Japanese patent text has some specific characteristics such as legalese, technical terminology and long sentences. Also, patent text includes significantly more special strings that are not written in Japanese characters, such as English words, patent numbers, mathematical expressions and abbreviation names for materials. 3 strates that the re-training of the LM with patent text and the use of more features to the MT system improved the BLEU scores (Papineni et al., 2001) significantly. Komachi et al. (2008) proposed a semisupervised approach to acquire domain speciﬁc translation knowledge from the collection of Wikipedia. He has extracted a bilingual lexicon based on article tiles related by inter-language link and then applied the graph theoretic algorithm, regularized Laplacian, to ﬁnd the most relevant translation pairs to the Patent domain. 4 MT System Basic Description Our approach on statistical machine translation for Japanese and English pairs of languages is described as follows. First, a pre-processing step is performed on the source language, in or"
2015.jeptalnrecital-court.22,P11-2102,0,0.0580601,"Missing"
2015.jeptalnrecital-court.22,W07-0101,0,0.139745,"Missing"
2015.jeptalnrecital-court.22,W13-1605,0,0.0721841,"Missing"
2015.jeptalnrecital-court.22,W11-1715,0,0.0440982,"Missing"
2015.jeptalnrecital-court.22,D13-1066,0,0.0453047,"Missing"
2015.jeptalnrecital-court.22,C96-2162,0,0.208197,"Missing"
2016.jeptalnrecital-poster.20,boulaknadel-etal-2008-multi,0,0.045874,"Missing"
2016.jeptalnrecital-poster.20,E93-1011,0,0.603963,"Missing"
2016.jeptalnrecital-poster.20,W03-1802,0,0.0639815,"Missing"
2016.jeptalnrecital-poster.20,C00-1077,0,0.201914,"Missing"
2016.jeptalnrecital-poster.20,W14-4807,0,0.0541764,"Missing"
2019.jeptalnrecital-court.24,Y15-1023,1,0.887555,"Missing"
2019.jeptalnrecital-court.24,C16-1228,0,0.0395983,"Missing"
2019.jeptalnrecital-court.24,L18-1550,0,0.0293447,"Missing"
2019.jeptalnrecital-court.24,D14-1181,0,0.016591,"Missing"
2019.jeptalnrecital-court.24,D14-1162,0,0.0812704,"Missing"
2019.jeptalnrecital-court.24,N18-1202,0,0.105432,"Missing"
2019.jeptalnrecital-court.24,S16-1077,0,0.0568844,"Missing"
2019.jeptalnrecital-court.24,D17-1056,0,0.0375576,"Missing"
2020.lrec-1.610,W15-3202,0,0.0723262,"Missing"
2020.lrec-1.610,Q18-1008,0,0.0248364,"al male and female singular male plural male and female dual Lemma ÉJ Ôg. (pretty) I. k@ (love) Table 1: Examples showing reducing sparsity between word and lemma levels. In addition to lemmas, we decide to evaluate also words in order to see closely the gain obtained by lemmas. 4956 رواية جميلة استمتعت حقا بقراءتها Positive :) Negative :( n x k representation of review with non static channel Convolutional layer with multiple filters Max-pooling Output layer Figure 1: CNN architecture for an example review 3.3. Embedding sets Embedding models are trained with multiple parameters. (Antoniak and Mimno, 2018) found that nearest neighbors are highly sensitive to small changes in embedding training corpus. (Pierrejean and Tanguy, 2018) explored the impact of changing window size, embedding dimension and training corpora. In this work, we want to know if the type of training corpora affects SA task performance. In other words, is it better to train embeddings with task-specific (polar) corpora in SA framework? or generic corpora are more efficient? And what about corpora size? For rigorous study, we build embedding models trained with three types of corpora: polar, non polar and mixed at word and lem"
2020.lrec-1.610,W14-3623,0,0.0580902,"Missing"
2020.lrec-1.610,P14-1023,0,0.0525022,"cificity in embedding models. (Salama et al., 2018) studied the effect of incorporating morphological information to word embedding in 2 ways: (i) including POS tags with words before embedding and, (ii) performing lemma abstraction of morphological embeddings obtained in (i). (Barhoumi et al., 2019) proceeded differently and built embeddings for different Arabic lexical unit (word, token, tokenclitics, lemma, light stem and stem). 2.2. Embedding evaluation techniques Embedding evaluation techniques fall into two categories: intrinsic and extrinsic. On one hand, intrinsic evaluation methods (Baroni et al., 2014; Schnabel et al., 2015) consist in quantifying directly various linguistic regularities in embedding space. Syntactic and semantic analogies (Mikolov et al., 2013; Nayak et al., 2016) are the most used intrinsic methods. On the other hand, extrinsic evaluation methods assess the quality of embeddings for other NLP tasks such as part-of-speech tagging, chunking, named-entity recognition, sentiment analysis, etc. The majority of works dealing with Arabic word embedding evaluation use extrinsic technique. Many Arabic embeddings models have been evaluated in applications such as machine translati"
2020.lrec-1.610,C16-1228,0,0.226637,"r methodology to propose embeddings based on word and lemma in section 3. In section 4, we present our neural architecture used for Arabic SA task. We report, in section 5, the experimental framework and discuss obtained results in section 6 Finally, we conclude, in section 7, and give some outlooks to future works. 2. 2.1. Related works Sentiment analysis task Sentiment analysis research has benefited from scientific advances in deep learning techniques, and several recent works have been done with this type of learning for Arabic 1 . (Al Sallab et al., 2015) tested different deep networks. (Dahou et al., 2016; Barhoumi et al., 2018; Barhoumi et al., 2019) used a convolutional neural network (CNN) architecture. (Hassan, 2017; Heikal et al., 2018; Al-Smadi et al., 2018) used recurrent neural network (RNN) and its variants. 1 For an overview of Arabic SA field, (Al-Ayyoub et al., 2018; Al-Ayyoub et al., 2019; Badaro et al., 2019) build a complete survey. 4955 The majority of neural networks takes as input continuous vector representations of words (word embeddings). Word2vec (Mikolov et al., 2013) and fastText (Bojanowski et al., 2016) are the most common algorithms for learning pre-trained embedding"
2020.lrec-1.610,P17-2072,0,0.0227207,"er NLP tasks such as part-of-speech tagging, chunking, named-entity recognition, sentiment analysis, etc. The majority of works dealing with Arabic word embedding evaluation use extrinsic technique. Many Arabic embeddings models have been evaluated in applications such as machine translation (Shapiro and Duh, 2018; Lachraf et al., 2019), Sentiment analysis (Dahou et al., 2016; Soliman et al., 2017; Fouad et al., 2019), Information retrieval (El Mahdaouy et al., 2018), etc. Many downstream applications show the usefulness of word embeddings. For intrinsic evaluation of Arabic word embeddings, (Elrazzaz et al., 2017) is the only work up to our knowledge. It quantifies syntactic and semantic analogies in embedding spaces. In this work, we evaluate embeddings with both intrinsic and extrinsic methods within SA frame. We propose a new protocol for intrinsic evaluation showing the sentiment stability of neighbors in embedding spaces that will be detailed in section 6.1. 3. Methodology In this section, we explain specificity of Arabic language in subsection 3.1, and justify our choice of word and lemma as lexical units in subsection 3.2. Then, we present our intuitions for embedding construction in subsection"
2020.lrec-1.610,L18-1550,0,0.0295686,"55 The majority of neural networks takes as input continuous vector representations of words (word embeddings). Word2vec (Mikolov et al., 2013) and fastText (Bojanowski et al., 2016) are the most common algorithms for learning pre-trained embeddings. Contextualized word embeddings Elmo (Peters et al., 2018) recently appear to handle both linguistic contexts and word syntax/semantic. There are some embedding resources that are freely available for Arabic language: (Dahou et al., 2016; Soliman et al., 2017) built word embedding sets obtained by training skip-gram and CBOW versions of word2vec, (Grave et al., 2018) distribute pre-trained word vectors Arabic, trained on Common Crawl and Wikipedia using fastText. (Barhoumi et al., 2018) presents a rigorous comparison of these embedding resources and shows that their systems suffer from a low coverage of pre-trained embeddings at word level. To the best of our knowledge, (Salama et al., 2018; Barhoumi et al., 2019) are the only works dealing with Arabic specificity in embedding models. (Salama et al., 2018) studied the effect of incorporating morphological information to word embedding in 2 ways: (i) including POS tags with words before embedding and, (ii)"
2020.lrec-1.610,D14-1181,0,0.0064727,"rds, is it better to train embeddings with task-specific (polar) corpora in SA framework? or generic corpora are more efficient? And what about corpora size? For rigorous study, we build embedding models trained with three types of corpora: polar, non polar and mixed at word and lemma levels. This will be detailed in section 5.1. In addition to corpora type, we investigate the impact of epoch number used for model training on the neighborhood of polar units. 4. Description of our sentiment analysis system Several papers show that CNN architecture gives good performance for sentiment analysis (Kim, 2014; Dahou et al., 2016; Barhoumi et al., 2018). In the same way, we choose to consider a CNN architecture. We develop a CNN architecture similar to the one described in (Dahou et al., 2016) and train it according two modes: with or without adaptation of embeddings. Indeed, we test static and non static CNN learning ways (Kim, 2014) in order to evoke trainable and non trainable aspects of embeddings. Trainable embeddings obtained with non static CNN allow obtaining task-specific embeddings. They are updated while learning task system. Non trainable embeddings are obtained with static CNN, they ar"
2020.lrec-1.610,W19-4605,0,0.0243064,"nsist in quantifying directly various linguistic regularities in embedding space. Syntactic and semantic analogies (Mikolov et al., 2013; Nayak et al., 2016) are the most used intrinsic methods. On the other hand, extrinsic evaluation methods assess the quality of embeddings for other NLP tasks such as part-of-speech tagging, chunking, named-entity recognition, sentiment analysis, etc. The majority of works dealing with Arabic word embedding evaluation use extrinsic technique. Many Arabic embeddings models have been evaluated in applications such as machine translation (Shapiro and Duh, 2018; Lachraf et al., 2019), Sentiment analysis (Dahou et al., 2016; Soliman et al., 2017; Fouad et al., 2019), Information retrieval (El Mahdaouy et al., 2018), etc. Many downstream applications show the usefulness of word embeddings. For intrinsic evaluation of Arabic word embeddings, (Elrazzaz et al., 2017) is the only work up to our knowledge. It quantifies syntactic and semantic analogies in embedding spaces. In this work, we evaluate embeddings with both intrinsic and extrinsic methods within SA frame. We propose a new protocol for intrinsic evaluation showing the sentiment stability of neighbors in embedding spac"
2020.lrec-1.610,W10-0204,0,0.0400086,"timentanalysis/ 5 4958 phrase) and s: a sentiment score. In this work, we collected all available sentiment lexicons up to our knowledge (Badaro et al., 2014; ElSahar and ElBeltagy, 2015; Saif M. Mohammad and Kiritchenko, 2016; Al-Moslmi et al., 2018). This represents a set of 15 lexicons constructed with different methods. The first method consists in automatically translating English lexicons. Indeed, translated resources (Saif M. Mohammad and Kiritchenko, 2016) are obtained by translating the following four English lexicons: MPQA (Wilson et al., 2005), S140 (Kiritchenko et al., 2014), NRC (Mohammad and Turney, 2010; Mohammad and Yang, 2011; Mohammad et al., 2013) and Bing liu lexicon (Hu and Liu, 2004). The second method is based on Pointwise Mutual Information (PMI) between words and two labels (positive and negative). Indeed, the sentiment orientation (SO) of a word (Mohammad and Turney, 2013) represents the difference between PMI scores. The used lexicons have various size and different structures. In fact, each word is described with different features. These features vary from one lexicon to another. (Badaro et al., 2014) built 4 616 word annotated with their part-of-speech tags, positive and negat"
2020.lrec-1.610,W11-1709,0,0.0358473,"se) and s: a sentiment score. In this work, we collected all available sentiment lexicons up to our knowledge (Badaro et al., 2014; ElSahar and ElBeltagy, 2015; Saif M. Mohammad and Kiritchenko, 2016; Al-Moslmi et al., 2018). This represents a set of 15 lexicons constructed with different methods. The first method consists in automatically translating English lexicons. Indeed, translated resources (Saif M. Mohammad and Kiritchenko, 2016) are obtained by translating the following four English lexicons: MPQA (Wilson et al., 2005), S140 (Kiritchenko et al., 2014), NRC (Mohammad and Turney, 2010; Mohammad and Yang, 2011; Mohammad et al., 2013) and Bing liu lexicon (Hu and Liu, 2004). The second method is based on Pointwise Mutual Information (PMI) between words and two labels (positive and negative). Indeed, the sentiment orientation (SO) of a word (Mohammad and Turney, 2013) represents the difference between PMI scores. The used lexicons have various size and different structures. In fact, each word is described with different features. These features vary from one lexicon to another. (Badaro et al., 2014) built 4 616 word annotated with their part-of-speech tags, positive and negative scores, offsets in Ar"
2020.lrec-1.610,S13-2053,0,0.0675151,"Missing"
2020.lrec-1.610,W16-2504,0,0.0187056,"embedding and, (ii) performing lemma abstraction of morphological embeddings obtained in (i). (Barhoumi et al., 2019) proceeded differently and built embeddings for different Arabic lexical unit (word, token, tokenclitics, lemma, light stem and stem). 2.2. Embedding evaluation techniques Embedding evaluation techniques fall into two categories: intrinsic and extrinsic. On one hand, intrinsic evaluation methods (Baroni et al., 2014; Schnabel et al., 2015) consist in quantifying directly various linguistic regularities in embedding space. Syntactic and semantic analogies (Mikolov et al., 2013; Nayak et al., 2016) are the most used intrinsic methods. On the other hand, extrinsic evaluation methods assess the quality of embeddings for other NLP tasks such as part-of-speech tagging, chunking, named-entity recognition, sentiment analysis, etc. The majority of works dealing with Arabic word embedding evaluation use extrinsic technique. Many Arabic embeddings models have been evaluated in applications such as machine translation (Shapiro and Duh, 2018; Lachraf et al., 2019), Sentiment analysis (Dahou et al., 2016; Soliman et al., 2017; Fouad et al., 2019), Information retrieval (El Mahdaouy et al., 2018), e"
2020.lrec-1.610,N18-1202,0,0.0436749,", 2019) used a convolutional neural network (CNN) architecture. (Hassan, 2017; Heikal et al., 2018; Al-Smadi et al., 2018) used recurrent neural network (RNN) and its variants. 1 For an overview of Arabic SA field, (Al-Ayyoub et al., 2018; Al-Ayyoub et al., 2019; Badaro et al., 2019) build a complete survey. 4955 The majority of neural networks takes as input continuous vector representations of words (word embeddings). Word2vec (Mikolov et al., 2013) and fastText (Bojanowski et al., 2016) are the most common algorithms for learning pre-trained embeddings. Contextualized word embeddings Elmo (Peters et al., 2018) recently appear to handle both linguistic contexts and word syntax/semantic. There are some embedding resources that are freely available for Arabic language: (Dahou et al., 2016; Soliman et al., 2017) built word embedding sets obtained by training skip-gram and CBOW versions of word2vec, (Grave et al., 2018) distribute pre-trained word vectors Arabic, trained on Common Crawl and Wikipedia using fastText. (Barhoumi et al., 2018) presents a rigorous comparison of these embedding resources and shows that their systems suffer from a low coverage of pre-trained embeddings at word level. To the be"
2020.lrec-1.610,N18-4005,0,0.0183873,"educing sparsity between word and lemma levels. In addition to lemmas, we decide to evaluate also words in order to see closely the gain obtained by lemmas. 4956 رواية جميلة استمتعت حقا بقراءتها Positive :) Negative :( n x k representation of review with non static channel Convolutional layer with multiple filters Max-pooling Output layer Figure 1: CNN architecture for an example review 3.3. Embedding sets Embedding models are trained with multiple parameters. (Antoniak and Mimno, 2018) found that nearest neighbors are highly sensitive to small changes in embedding training corpus. (Pierrejean and Tanguy, 2018) explored the impact of changing window size, embedding dimension and training corpora. In this work, we want to know if the type of training corpora affects SA task performance. In other words, is it better to train embeddings with task-specific (polar) corpora in SA framework? or generic corpora are more efficient? And what about corpora size? For rigorous study, we build embedding models trained with three types of corpora: polar, non polar and mixed at word and lemma levels. This will be detailed in section 5.1. In addition to corpora type, we investigate the impact of epoch number used fo"
2020.lrec-1.610,L16-1006,0,0.0366674,"Missing"
2020.lrec-1.610,D15-1036,0,0.022541,"models. (Salama et al., 2018) studied the effect of incorporating morphological information to word embedding in 2 ways: (i) including POS tags with words before embedding and, (ii) performing lemma abstraction of morphological embeddings obtained in (i). (Barhoumi et al., 2019) proceeded differently and built embeddings for different Arabic lexical unit (word, token, tokenclitics, lemma, light stem and stem). 2.2. Embedding evaluation techniques Embedding evaluation techniques fall into two categories: intrinsic and extrinsic. On one hand, intrinsic evaluation methods (Baroni et al., 2014; Schnabel et al., 2015) consist in quantifying directly various linguistic regularities in embedding space. Syntactic and semantic analogies (Mikolov et al., 2013; Nayak et al., 2016) are the most used intrinsic methods. On the other hand, extrinsic evaluation methods assess the quality of embeddings for other NLP tasks such as part-of-speech tagging, chunking, named-entity recognition, sentiment analysis, etc. The majority of works dealing with Arabic word embedding evaluation use extrinsic technique. Many Arabic embeddings models have been evaluated in applications such as machine translation (Shapiro and Duh, 201"
2020.lrec-1.610,W18-1201,0,0.018954,"hnabel et al., 2015) consist in quantifying directly various linguistic regularities in embedding space. Syntactic and semantic analogies (Mikolov et al., 2013; Nayak et al., 2016) are the most used intrinsic methods. On the other hand, extrinsic evaluation methods assess the quality of embeddings for other NLP tasks such as part-of-speech tagging, chunking, named-entity recognition, sentiment analysis, etc. The majority of works dealing with Arabic word embedding evaluation use extrinsic technique. Many Arabic embeddings models have been evaluated in applications such as machine translation (Shapiro and Duh, 2018; Lachraf et al., 2019), Sentiment analysis (Dahou et al., 2016; Soliman et al., 2017; Fouad et al., 2019), Information retrieval (El Mahdaouy et al., 2018), etc. Many downstream applications show the usefulness of word embeddings. For intrinsic evaluation of Arabic word embeddings, (Elrazzaz et al., 2017) is the only work up to our knowledge. It quantifies syntactic and semantic analogies in embedding spaces. In this work, we evaluate embeddings with both intrinsic and extrinsic methods within SA frame. We propose a new protocol for intrinsic evaluation showing the sentiment stability of neig"
2020.lrec-1.610,H05-1044,0,0.164465,"o.cloudapp.net/farasa/ https://lium.univ-lemans.fr/en/ arsentimentanalysis/ 5 4958 phrase) and s: a sentiment score. In this work, we collected all available sentiment lexicons up to our knowledge (Badaro et al., 2014; ElSahar and ElBeltagy, 2015; Saif M. Mohammad and Kiritchenko, 2016; Al-Moslmi et al., 2018). This represents a set of 15 lexicons constructed with different methods. The first method consists in automatically translating English lexicons. Indeed, translated resources (Saif M. Mohammad and Kiritchenko, 2016) are obtained by translating the following four English lexicons: MPQA (Wilson et al., 2005), S140 (Kiritchenko et al., 2014), NRC (Mohammad and Turney, 2010; Mohammad and Yang, 2011; Mohammad et al., 2013) and Bing liu lexicon (Hu and Liu, 2004). The second method is based on Pointwise Mutual Information (PMI) between words and two labels (positive and negative). Indeed, the sentiment orientation (SO) of a word (Mohammad and Turney, 2013) represents the difference between PMI scores. The used lexicons have various size and different structures. In fact, each word is described with different features. These features vary from one lexicon to another. (Badaro et al., 2014) built 4 616"
F12-2017,2005.jeptalnrecital-court.12,1,0.597682,"Missing"
F12-2017,P09-2020,0,0.0702225,"Missing"
F12-2040,J94-4002,0,0.0940294,"Missing"
F12-2040,W05-0406,0,0.0461833,"Missing"
F12-2040,P07-1103,0,0.0402263,"Missing"
F12-2040,P02-1011,0,0.104384,"Missing"
F12-2040,2005.jeptalnrecital-court.12,1,0.715001,"Missing"
I13-1048,E06-1047,0,0.189687,"Missing"
I13-1048,P05-1071,0,0.147877,"Missing"
I13-1048,P06-1086,0,0.408431,"Missing"
I13-1048,maamouri-etal-2010-speech,0,0.135186,"Missing"
I13-1048,2008.jeptalnrecital-recital.8,0,0.105998,"Missing"
I13-1048,P12-2035,0,\N,Missing
I13-1133,W11-2602,0,0.106805,"Missing"
I13-1133,C10-1110,0,0.0409633,"Missing"
I13-1133,P11-1061,0,0.073421,"Missing"
I13-1133,W05-0708,0,0.0555347,"1 Introduction The Arabic Dialect (AD) is a collection of spoken varieties of Arabic. It is used in everyday communication. So, it is so important to consider it in the new technologies like dialogue systems, telephone applications, etc. (Zribi et al., 2013). The majority of these applications need a morphological analysis to segment words and to exploit their morphological features. Many important works have focused on the morphological analysis of the Arabic language, mainly on Modern Standard Arabic (MSA). AD has not received much attention due to the lack of dialectal tools and resources (Duh and Kirchhoff, 2005). However, there are differences between MSA and AD, they are considered as two related languages. Therefore, we suggest in this paper to exploit and adapt an MSA morphological analyzer (MA) to Tunisian Dialect (TD). The adaptation is done in two steps. The first step is to adapt an MSA lexicon to TD and to improve the resulting lexicon with TD specific roots and derivation patterns 1 . The second step is to integrate the 1 The Arabic derivation system consists to use a set of patterns and roots to generate words. To generate the word “  َي ْك ِت ُب, yaktibu, he write”, we replace the ri let"
I13-1133,W12-2301,0,0.186877,"Missing"
I13-1133,E06-1047,0,\N,Missing
I13-1133,W05-0703,0,\N,Missing
keskes-etal-2012-clause,P07-1062,0,\N,Missing
masmoudi-etal-2014-corpus,N09-1045,1,\N,Missing
masmoudi-etal-2014-corpus,habash-etal-2012-conventional,1,\N,Missing
masmoudi-etal-2014-corpus,zribi-etal-2014-conventional,1,\N,Missing
P15-2106,W14-6305,1,0.810208,"Missing"
P15-2106,P09-2041,0,0.111092,"n is quite a hot topic in the research community also due to its importance for efficient sentiment analysis (Ghosh et al., 2015). Several approaches have been proposed to detect irony casting the problem into a binary classification task relying on a variety of features. Most of them are gleaned from the utterance internal context going from n-grams models, stylistic (punctuation, emoticons, quotations, etc.), to dictionary-based features (sentiment and affect dictionaries, slang languages, etc.). These features have shown to be useful to learn whether a text span is ironic/sarcastic or not (Burfoot and Baldwin, 2009; Davidov et al., 2010; Tsur et al., 2010; GonzalezIbanez et al., 2011; Reyes et al., 2013; Barbieri and Saggion, 2014). However, many authors pointed out the necessity of additional pragmatic features: (Utsumi, 2004) showed that opposition, rhetorical questions and the politeness level are relevant. (Burfoot and Baldwin, 2009) focused on satire detection in newswire articles and introduced the notion of validity which models absurdity by identifying a conjunc644 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference o"
P15-2106,W14-2608,0,0.180302,"and testing on 430 tweets. CAll has been trained on 2,472 tweets (1432 contain negation –404 IR and 1028 NIR) and tested on 618 tweets (360 contain negation – 66 IR and 294 NIR). For each classifier, we represent each tweet with a vector composed of six groups of features. Most of them are state of the art features, others, in italic font are new. Surface features include tweet length in words (Tsur et al., 2010), the presence or absence of punctuation marks (Gonzalez-Ibanez et al., 2011), words in capital letters (Reyes et al., 2013), interjections (Gonzalez-Ibanez et al., 2011), emoticons (Buschmeier et al., 2014), quotations (Tsur et al., 2010), slang words (Burfoot and Baldwin, 2009), opposition words such as “but” and “although” (Utsumi, 2004), a sequence of exclamation or a sequence of question marks (Carvalho et al., 2009), a combination of both exclamation and question marks (Buschmeier et al., 2014) and finally, the presence of discourse connectives that do not convey opposition such as “hence, therefore, as a result” since we assume that non ironic tweets are likely to be more verbose. To implement these features, we rely on manually built French lexicons to deal with interjections, emoticons,"
P15-2106,W10-2914,0,0.181061,"he research community also due to its importance for efficient sentiment analysis (Ghosh et al., 2015). Several approaches have been proposed to detect irony casting the problem into a binary classification task relying on a variety of features. Most of them are gleaned from the utterance internal context going from n-grams models, stylistic (punctuation, emoticons, quotations, etc.), to dictionary-based features (sentiment and affect dictionaries, slang languages, etc.). These features have shown to be useful to learn whether a text span is ironic/sarcastic or not (Burfoot and Baldwin, 2009; Davidov et al., 2010; Tsur et al., 2010; GonzalezIbanez et al., 2011; Reyes et al., 2013; Barbieri and Saggion, 2014). However, many authors pointed out the necessity of additional pragmatic features: (Utsumi, 2004) showed that opposition, rhetorical questions and the politeness level are relevant. (Burfoot and Baldwin, 2009) focused on satire detection in newswire articles and introduced the notion of validity which models absurdity by identifying a conjunc644 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Pro"
P15-2106,S15-2080,0,0.0191979,"e the writer believes that his audience can detect the disparity between P and P 0 on the basis of contextual knowledge or common background shared with the writer. For example, in “#Hollande is really a good diplomat #Algeria.”, the writer critics the foreign policy of the French president Hollande in Algeria, whereas in ”The #NSA wiretapped a whole country. No worries for #Belgium: it is not a whole country.“, the irony occurs because the fact in bold font is not true. Irony detection is quite a hot topic in the research community also due to its importance for efficient sentiment analysis (Ghosh et al., 2015). Several approaches have been proposed to detect irony casting the problem into a binary classification task relying on a variety of features. Most of them are gleaned from the utterance internal context going from n-grams models, stylistic (punctuation, emoticons, quotations, etc.), to dictionary-based features (sentiment and affect dictionaries, slang languages, etc.). These features have shown to be useful to learn whether a text span is ironic/sarcastic or not (Burfoot and Baldwin, 2009; Davidov et al., 2010; Tsur et al., 2010; GonzalezIbanez et al., 2011; Reyes et al., 2013; Barbieri and"
P15-2106,P11-2102,0,0.658509,"estions and the politeness level are relevant. (Burfoot and Baldwin, 2009) focused on satire detection in newswire articles and introduced the notion of validity which models absurdity by identifying a conjunc644 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 644–650, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics tion of named entities present in a given document and queries the web for the conjunction of those entities. (Gonzalez-Ibanez et al., 2011) exploited the common ground between speaker and hearer by looking if a tweet is a reply to another tweet. (Reyes et al., 2013) employed opposition in time (adverbs of time such as now and suddenly) and context imbalance to estimate the semantic similarity of concepts in a text to each other. (Barbieri and Saggion, 2014) captured the gap between rare and common words as well as the use of common vs. rare synonyms. Finally, (Buschmeier et al., 2014) measured the imbalance between the overall polarity of words in a review and the star-rating. Most of these pragmatic features rely on linguistic a"
P15-2106,W13-1605,0,0.195893,"Missing"
P15-2106,D13-1066,0,0.236497,"Missing"
P15-2106,C96-2162,0,0.604809,"in tweets. We aim to test the validity of two main hypotheses: (1) the presence of negations, as an internal propriety of an utterance, can help to detect the disparity between the literal and the intended meaning of an utterance, (2) a tweet containing an asserted fact of the form N ot(P1 ) is ironic if and only if one can assess the absurdity of P1 . Our first results are encouraging and show that deriving a pragmatic contextual model is feasible. 1 Motivation Irony is a complex linguistic phenomenon widely studied in philosophy and linguistics (Grice et al., 1975; Sperber and Wilson, 1981; Utsumi, 1996). Despite theories differ on how to define irony, they all commonly agree that it involves an incongruity between the literal meaning of an utterance and what is expected about the speaker and/or the environment. For many researchers, irony overlaps with a variety of other figurative devices such as satire, parody, and sarcasm (Clark and Gerrig, 1984; Gibbs, 2000). In this paper, we use irony as an umbrella term that covers these devices focusing for the first time on the automatic detection of irony in French tweets. According to (Grice et al., 1975; Searle, 1979; Attardo, 2000), the search f"
P15-2106,Y13-1035,0,0.644036,"Missing"
P15-2106,W10-3111,0,0.0909516,"Missing"
P15-2106,barbieri-saggion-2014-modelling-irony,0,\N,Missing
R13-1032,C08-1019,0,0.191253,"mmary. Since the 2005 DUC, the PYRAMID metric (Nenkova and Passonneau, 2004) has been added as an optional manual evaluation metric. This metric, which is based on the identification of minimal semantic units called SCUs (Summary Content Units), has become one of the principal manual metrics for evaluating summaries in the TAC conference. use reference summaries have also been proposed by (Louis and Nenkova, 2009) and (Torres-Moreno et al., 2010). These metrics are used to compare each candidate summary to source documents using the Jensen-Shannon divergence measure. New metrics such as ROSE (Conroy and Dang, 2008) and Nouveau-ROUGE (Conroy et al., 2011) have involved a combination of ROUGE variants to predict PYRAMID or the Overall Responsiveness score. Other works have focused on metrics of linguistic quality evaluation. In this context, (Pilter et al., 2010) evaluated the five linguistic properties used in TAC by combining different types of features such as entity grid (Barzilay and Lapata, 2008), modeling language, etc. The most recent work, namely that of (Conroy et al, 2010), assessed content and linguistic quality using a combination of features. Concerning content features, (Conroy et al, 2010)"
R13-1032,D09-1032,0,0.189344,"uate a candidate summary. This metric is a combination of content and linguistic quality. It differs from other metrics of summary evaluation in that it doesn’t compare a candidate summary against a model summary. Since the 2005 DUC, the PYRAMID metric (Nenkova and Passonneau, 2004) has been added as an optional manual evaluation metric. This metric, which is based on the identification of minimal semantic units called SCUs (Summary Content Units), has become one of the principal manual metrics for evaluating summaries in the TAC conference. use reference summaries have also been proposed by (Louis and Nenkova, 2009) and (Torres-Moreno et al., 2010). These metrics are used to compare each candidate summary to source documents using the Jensen-Shannon divergence measure. New metrics such as ROSE (Conroy and Dang, 2008) and Nouveau-ROUGE (Conroy et al., 2011) have involved a combination of ROUGE variants to predict PYRAMID or the Overall Responsiveness score. Other works have focused on metrics of linguistic quality evaluation. In this context, (Pilter et al., 2010) evaluated the five linguistic properties used in TAC by combining different types of features such as entity grid (Barzilay and Lapata, 2008),"
R13-1032,C10-2032,0,0.304251,"by (Rankel et al., 2012) and which is equal to log (Number of sentences)). cohesion. For example, discourse connectives (e.g. “and”, “while”) are used to connect sentences. Since many functional words represent reference devices or discourse connectives, we decided to calculate the density of the four categories of function words: determinants (DET), conjunctions (CC), prepositions and subordinating conjunctions (PSC), and personal pronouns (PRP). In addition to the density of function words, we calculated the density of content words which is used in many works such as (To et al, 2013) and (Feng et al, 2010) to predict the readability of a text. So, we calculated the density of four categories of content words: adjectives (ADJ), nouns (N), verbs (V) and adverbs (ADV). The density of each of the above categories is the ratio between the number of words presenting one of the categories and the total number of words in the summary. To detect function words and content words, we used the morphological tagger ""Stanford Postagger4"", which provides the grammatical category of words. 5 Language modeling features Evaluation We used the corpus of the 2008 TAC conference to evaluate our metric. This corpus"
R13-1032,hovy-etal-2006-automated,0,0.693632,"d it can be of two types: extrinsic or intrinsic (Jing et al., 1998). Extrinsic evaluation measures the impact of using a summary in the place of the source document(s) on tasks such as document classification and indexing while intrinsic evaluation assesses the overall quality of the summary either manually or automatically. It should be noted that the manual evaluation is a difficult and expensive task because it requires a lot of time and expertise in the field of the source text topic. For this reason, several automatic evaluation metrics have been developed such as ROUGE (Lin, 2004), BE (Hovy et al., 2006), BEwTE (Tratz and Hovy, 2008), AutoSummENG (Giannakopoulos et al., 2008), etc. The advent of automatic evaluation metrics generates in its turn a new step: meta-evaluation i.e. the evaluation of evaluation metrics. We perform this meta-evaluation by making a comparison between these metrics and manual metrics. To achieve this comparison, the TAC1 conference proposed various metrics of correlations (i.e. Pearson, Spearman, Kandall). Most of the evaluation metrics assessed by the TAC conference are based on the evaluation of the relevance of a summary content. However, a summary with relevant c"
R13-1032,N04-1019,0,0.1594,"valuation metrics. Prior to 2005, the DUC2 conference evaluated summaries using the Summary Evaluation Environment (SEE) interface (Lin, 2001). This interface helps assessors in the evaluation of the content and the linguistic quality of a candidate summary. In 2006, DUC added the Overall Responsiveness metric (Dang and Owczarzak, 2008) to evaluate a candidate summary. This metric is a combination of content and linguistic quality. It differs from other metrics of summary evaluation in that it doesn’t compare a candidate summary against a model summary. Since the 2005 DUC, the PYRAMID metric (Nenkova and Passonneau, 2004) has been added as an optional manual evaluation metric. This metric, which is based on the identification of minimal semantic units called SCUs (Summary Content Units), has become one of the principal manual metrics for evaluating summaries in the TAC conference. use reference summaries have also been proposed by (Louis and Nenkova, 2009) and (Torres-Moreno et al., 2010). These metrics are used to compare each candidate summary to source documents using the Jensen-Shannon divergence measure. New metrics such as ROSE (Conroy and Dang, 2008) and Nouveau-ROUGE (Conroy et al., 2011) have involved"
R13-1032,P10-1056,0,0.0408076,"Missing"
R13-1032,W04-1013,0,0.0740988,"of a summary and it can be of two types: extrinsic or intrinsic (Jing et al., 1998). Extrinsic evaluation measures the impact of using a summary in the place of the source document(s) on tasks such as document classification and indexing while intrinsic evaluation assesses the overall quality of the summary either manually or automatically. It should be noted that the manual evaluation is a difficult and expensive task because it requires a lot of time and expertise in the field of the source text topic. For this reason, several automatic evaluation metrics have been developed such as ROUGE (Lin, 2004), BE (Hovy et al., 2006), BEwTE (Tratz and Hovy, 2008), AutoSummENG (Giannakopoulos et al., 2008), etc. The advent of automatic evaluation metrics generates in its turn a new step: meta-evaluation i.e. the evaluation of evaluation metrics. We perform this meta-evaluation by making a comparison between these metrics and manual metrics. To achieve this comparison, the TAC1 conference proposed various metrics of correlations (i.e. Pearson, Spearman, Kandall). Most of the evaluation metrics assessed by the TAC conference are based on the evaluation of the relevance of a summary content. However, a"
R13-1032,P12-1106,0,0.0210256,"inguistic properties used in TAC by combining different types of features such as entity grid (Barzilay and Lapata, 2008), modeling language, etc. The most recent work, namely that of (Conroy et al, 2010), assessed content and linguistic quality using a combination of features. Concerning content features, (Conroy et al, 2010) use ROUGE scores for initial summaries and Nouveau-ROUGE scores for update summaries. In a later work (Conroy et al., 2011) and (Rankel et al., 2012) combined features of content (six variations of bigram scores) and others of linguistic quality. In contrast to Conroy, (Lin et al., 2012) combined a machine translation metric adapted to summary evaluation with a coherence metric based on an entity grid to predict the Overall Responsiveness metric. 3 Because of the difficulties encountered during the manual evaluation, more research has focused on automatic evaluation. ROUGE (Lin, 2004) is one of the first automatic metrics for the intrinsic evaluation of automatic summaries. This metric is based on the overlap of N-grams between a candidate summary and one or more reference summaries. (Hovy et al., 2006) introduced the BE metric, which allows the correspondence between syntact"
R13-1032,P11-1100,0,0.059428,"Missing"
R13-1032,J11-1001,0,\N,Missing
R13-1032,J08-1001,0,\N,Missing
R19-1084,N03-2002,0,0.0508081,"Missing"
R19-1084,W05-0821,0,0.125676,"Missing"
R19-1084,masmoudi-etal-2014-corpus,1,0.892151,"Missing"
R19-1084,zribi-etal-2014-conventional,1,0.820433,"Missing"
R19-1084,P06-2113,0,0.13515,"Missing"
R19-1084,P02-1025,0,0.173962,"Missing"
R19-1085,L16-1681,0,0.0319373,"Missing"
R19-1085,N07-2014,0,0.109298,"Missing"
R19-1085,W14-3603,0,0.0605908,"Missing"
R19-1085,P07-2045,0,0.00893486,"Missing"
R19-1085,W17-1321,0,0.0312784,"Missing"
R19-1085,D15-1274,0,0.0383762,"Missing"
R19-1085,W15-3209,0,0.0355588,"Missing"
R19-1085,2007.mtsummit-papers.20,0,0.199248,"Missing"
R19-1085,zribi-etal-2014-conventional,1,0.895991,"Missing"
R19-1085,J03-1002,0,0.012741,"Missing"
R19-1085,W09-0804,0,0.0615396,"Missing"
R19-1085,L16-1577,0,0.0293399,"Missing"
R19-1085,P06-1073,0,0.0980455,"Missing"
W12-1310,W06-2810,0,0.0923427,"Missing"
W12-1310,J93-2003,0,0.0664698,"Missing"
W12-1310,declerck-etal-2006-multilingual,0,0.0360272,"Missing"
W12-1310,1998.amta-tutorials.5,0,0.103051,"Missing"
W12-1310,W11-1206,0,0.0670376,"Missing"
W12-1310,W11-1205,0,0.0642362,"Missing"
W12-1310,J03-1002,0,0.00991458,"Missing"
W12-1310,2007.mtsummit-papers.26,0,0.128071,"Missing"
W12-1310,P03-2025,1,0.742512,"Missing"
W12-1310,2010.jeptalnrecital-demonstration.6,1,0.791315,"Missing"
W12-1310,C96-2141,0,0.552025,"Missing"
W13-2813,maamouri-etal-2010-speech,0,0.0258046,"Missing"
W13-2813,P12-2035,0,0.0666587,"Missing"
W13-2813,2008.jeptalnrecital-recital.8,0,0.119489,"Missing"
W13-2813,W04-1602,0,\N,Missing
W17-1007,C08-1019,0,0.0698095,"NG and NPowER with parameters: minimum length of N-grams = maximum length of N-grams = window size=3 tent and the two other configurations to capture some grammatical phenomena from the well formation of reference sentences. We have assumed that also for those scores configurations which take into account large contexts may capture the linguistic qualities of the summary. features are: • ROUGE Scores: ROUGE scores are designed to evaluate the content of a text summary. They are based on the overlap of words N-grams between a candidate summary and one or more reference summaries. According to (Conroy and Dang, 2008), ROUGE variants which take into account large contexts may capture the linguistic qualities of the summary such as some grammatical phenomena. We mean that ROUGE variants that use bigrams, trigrams or more can capture some grammatical phenomena from the well formation of reference sentences. For this reason, we include ROUGE scores which take into account large contexts in the ROUGE feature class:ROUGE-1 (R1), ROUGE-2 (R2), ROUGE-3 (R3), ROUGE4 (R4) and ROUGE-5 (R5) which calculate respectively words overlaps of bigrams, trigrams, 4-grams and 5-grams. • SIMetrix scores: we have used the follo"
W17-1007,R13-1032,1,0.843046,"ms, we adopt the one that produces the best predictive model. The validation of each model is performed by two methods: cross-validation method with 10 folds and supplied test set method. 4 4.2 Experiments and results We have experimented our method in summary level evaluation (Micro-evaluation). At this level, we take, for each Summarizer system, each produced summary in a separate entry. It is worth mentioning that this evaluation level is more difficult than system level evaluation (i.e. where the average quality of a summarizing system is measured) even for MonoLingual summary evaluation (Ellouze et al., 2013), (Ellouze et al., 2016). For each language, we have tested several single and “ensemble learning” classifiers integrated on Weka environment and based on regression method like GaussianProcesses, linearRegression, vote, Bagging, etc. We validate our models using cross-validation with 10 folds and using supplied test set. For cross-validation method, we have calculated the features from ”MultiLing 2013” corpus. While, for supplied test set method we have used ”MultiLing 2013” corpus as training set and ”MultiLing Pilot TAC’2011” corpus as testing set. We have chosen to train our models on ”Mul"
W17-1007,C10-1062,0,0.025893,"ength of N-gram and 3 to window size. Finally, the third one attributs 4 to minimum length of N-gram, 4 to maximum length of N-gram and 3 to window size. In fact, because Overall responsivness scores evaluate the content and the linguistic quality of summary, we have chosen the first configuration to assess the con• Syntactic features: the syntactic structure of sentences is an important factor that can determine the linguistic quality of texts. (Schwarm and Ostendorf, 2005) and (Feng et al., 2010) used syntactic features to gauge the readability of text as assessment of reading level. While (Kate et al., 2010) used syntactic features to predict linguistic quality of natural-language documents. We implement some of these features using the Stanford parser (Klein and Manning, 2003). We calculate the number and the average number of noun phrases (NP), verbal phrases (VP) and prepositional phrases (PP). The average number of each of the previous phrases is calculated as the ratio between the number of one of the previous phrase type and the total number of sentences. 3.2 Combination scheme Before building a predictive model, we should first calculate the values of all the features. 49 articles related"
W17-1007,C10-2032,0,0.0117651,"ength of N-gram and 3 to window size. The second configuration assigns 3 to minimum length of Ngram, 3 to maximum length of N-gram and 3 to window size. Finally, the third one attributs 4 to minimum length of N-gram, 4 to maximum length of N-gram and 3 to window size. In fact, because Overall responsivness scores evaluate the content and the linguistic quality of summary, we have chosen the first configuration to assess the con• Syntactic features: the syntactic structure of sentences is an important factor that can determine the linguistic quality of texts. (Schwarm and Ostendorf, 2005) and (Feng et al., 2010) used syntactic features to gauge the readability of text as assessment of reading level. While (Kate et al., 2010) used syntactic features to predict linguistic quality of natural-language documents. We implement some of these features using the Stanford parser (Klein and Manning, 2003). We calculate the number and the average number of noun phrases (NP), verbal phrases (VP) and prepositional phrases (PP). The average number of each of the previous phrases is calculated as the ratio between the number of one of the previous phrase type and the total number of sentences. 3.2 Combination scheme"
W17-1007,P03-1054,0,0.00905264,"use Overall responsivness scores evaluate the content and the linguistic quality of summary, we have chosen the first configuration to assess the con• Syntactic features: the syntactic structure of sentences is an important factor that can determine the linguistic quality of texts. (Schwarm and Ostendorf, 2005) and (Feng et al., 2010) used syntactic features to gauge the readability of text as assessment of reading level. While (Kate et al., 2010) used syntactic features to predict linguistic quality of natural-language documents. We implement some of these features using the Stanford parser (Klein and Manning, 2003). We calculate the number and the average number of noun phrases (NP), verbal phrases (VP) and prepositional phrases (PP). The average number of each of the previous phrases is calculated as the ratio between the number of one of the previous phrase type and the total number of sentences. 3.2 Combination scheme Before building a predictive model, we should first calculate the values of all the features. 49 articles related to the same topic. Each summarization system is invited to generate a summary for each collection. Then, We select the relevant ones using ”wrapper method”(Kohavi and John,"
W17-1007,N03-1020,0,0.235073,"comparing each system with others. The evaluation of text summary covers its content, its linguistic quality or both. Whatever the type of evaluation (content and/or linguistic quality), the evaluation of system summary output is a difficult task given that in most times there is not a single good summary. In the extreme case, two summaries of the same documents set may have completely different words and/or sentences with different structures. Several metrics have been evaluated the content, the linguistic quality and the overall responsiveness of MonoLing text summaries. We can cite ROUGE (Lin and Hovy, 2003), BE (Hovy et al., 2006), AutoSummENG (Giannakopoulos et al., 2008), BEwTE (Tratz and Hovy, 2008) , etc. Some of those metircs can assess MultiLing text 47 Proceedings of the MultiLing 2017 Workshop on Summarization and Summary Evaluation Across Source Types and Genres, pages 47–54, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics dicts a manual (human) score. All the above metrics (ROUGE, AutoSummENG, NPowER and SIMetrix) are used in monolingual and multilingual summary evaluation. Same of those metrics are adapted to multilingual evaluation while others (i.e."
W17-1007,J13-2002,0,0.0948365,"edges represent the relations between them. The calculation of the similarity is performed by comparing the graph of the candidate summary with the graph of each reference summary. In a subsequent work, (Giannakopoulos and Karkaletsis, 2010) have presented Merge Model Graph (MeMoG) which is another variation of AutoSummENG based on n-gram graphs. This variation calculates the merged graph of all reference summaries. Then, it compares the candidate summary graph to the merged graph of reference summaries. Afterwards, the SIMetrix (Summary Input similarity Metrics) measurement was developed by (Louis and Nenkova, 2013); it assesses a candidate summary by comparing it with the source documents. The SIMetrix computes ten measures of similarity based on the comparison between the source documents and the candidate summary. Among the used similarity measures we cite the cosine similarity, the divergence of Jensen-Shannon, the divergence of Kullback-Leibler, etc. Recently, (Giannakopoulos and Karkaletsis, 2013) proposed NPowER (N-gram graph Powered Evaluation via Regression) metric, which presents a combination of AutoSummENG and MeMoG. They build a linear regression model that pre3 Proposed Method From Table 1,"
W17-1007,W15-4638,0,0.0243819,"ACL Laboratory, FSEG Sfax, University of Sfax, Sfax, Tunisia Samira.Ellouze@fsegs.rnu.tn, maher.jaoua@fsegs.rnu.tn, l.belguith@fsegs.rnu.tn Abstract summaries such as ROUGE and AutoSummENG. But, those features can only evaluate the content of MultiLing text summaries. To encourage research to develop automatic multilingual multi-documents summarization systems a new task, dubbed MultiLing Pilot (Giannakopoulos et al., 2011), has been introduced for the first time in TAC2011 conference. Later, the two workshops 2013 ACL MultiLing Pilot (Giannakopoulos, 2013) and MultiLing 2015 at SIGdial 2015 (Giannakopoulos et al., 2015) have been organised with the same purpose as MultiLing Pilot 2011. The participated summarization systems in the MultiLing task have been assessed using automatic content metrics such as ROUGE1, ROUGE-2 and MeMoG and a manual metric named Overall Respensiveness which covers the content and the linguistic quality of a text summary. However, the manual evaluation of both the content and the linguistic quality of multilingual multi-documents summarization systems is an arduous and costly process. In addition, the automatic evaluation of only the content of summary is not enough because a summary"
W17-1007,P05-1065,0,0.0557634,"m length of N-gram, 2 to maximum length of N-gram and 3 to window size. The second configuration assigns 3 to minimum length of Ngram, 3 to maximum length of N-gram and 3 to window size. Finally, the third one attributs 4 to minimum length of N-gram, 4 to maximum length of N-gram and 3 to window size. In fact, because Overall responsivness scores evaluate the content and the linguistic quality of summary, we have chosen the first configuration to assess the con• Syntactic features: the syntactic structure of sentences is an important factor that can determine the linguistic quality of texts. (Schwarm and Ostendorf, 2005) and (Feng et al., 2010) used syntactic features to gauge the readability of text as assessment of reading level. While (Kate et al., 2010) used syntactic features to predict linguistic quality of natural-language documents. We implement some of these features using the Stanford parser (Klein and Manning, 2003). We calculate the number and the average number of noun phrases (NP), verbal phrases (VP) and prepositional phrases (PP). The average number of each of the previous phrases is calculated as the ratio between the number of one of the previous phrase type and the total number of sentences"
W17-1007,W13-3103,0,0.133865,"ze and Maher Jaoua and Lamia Hadrich Belguith ANLP-RG, MIRACL Laboratory, FSEG Sfax, University of Sfax, Sfax, Tunisia Samira.Ellouze@fsegs.rnu.tn, maher.jaoua@fsegs.rnu.tn, l.belguith@fsegs.rnu.tn Abstract summaries such as ROUGE and AutoSummENG. But, those features can only evaluate the content of MultiLing text summaries. To encourage research to develop automatic multilingual multi-documents summarization systems a new task, dubbed MultiLing Pilot (Giannakopoulos et al., 2011), has been introduced for the first time in TAC2011 conference. Later, the two workshops 2013 ACL MultiLing Pilot (Giannakopoulos, 2013) and MultiLing 2015 at SIGdial 2015 (Giannakopoulos et al., 2015) have been organised with the same purpose as MultiLing Pilot 2011. The participated summarization systems in the MultiLing task have been assessed using automatic content metrics such as ROUGE1, ROUGE-2 and MeMoG and a manual metric named Overall Respensiveness which covers the content and the linguistic quality of a text summary. However, the manual evaluation of both the content and the linguistic quality of multilingual multi-documents summarization systems is an arduous and costly process. In addition, the automatic evaluati"
W17-1007,W05-0705,0,0.0139337,"have a big effect on its correlation with Overall Responsiveness when using supplied test set as validation method. Besides, we note that the correlation of the best model with Overall Responsiveness is low, while it is more important than the correlation of baselines. This may be due to the small set of the observations per Arabic language. We need a larger set of observations to determine the best combination of features and to have better correlation. Furthermore, perhaps, this is due to the complexity of the Arabic language structure which is an agglutinative language where agglutination (Grefenstette et al., 2005) occurs when articles, prepositions and conjunctions are attached to the beginning of words and pronouns are attached to the end of words. This phenomenon can greatly influence the operation of comparing the candidate summary with reference summaries. Especially when a word appears in the candidate summary without agglutination while it appears in a reference summary in an agglutinative form and vice versa. English Summary Evaluation We pass now to the different experiments performed with English language. The selected features for English models are NPowER123 , autosummeng443 , the number of"
W17-1007,hovy-etal-2006-automated,0,0.0335732,"th others. The evaluation of text summary covers its content, its linguistic quality or both. Whatever the type of evaluation (content and/or linguistic quality), the evaluation of system summary output is a difficult task given that in most times there is not a single good summary. In the extreme case, two summaries of the same documents set may have completely different words and/or sentences with different structures. Several metrics have been evaluated the content, the linguistic quality and the overall responsiveness of MonoLing text summaries. We can cite ROUGE (Lin and Hovy, 2003), BE (Hovy et al., 2006), AutoSummENG (Giannakopoulos et al., 2008), BEwTE (Tratz and Hovy, 2008) , etc. Some of those metircs can assess MultiLing text 47 Proceedings of the MultiLing 2017 Workshop on Summarization and Summary Evaluation Across Source Types and Genres, pages 47–54, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics dicts a manual (human) score. All the above metrics (ROUGE, AutoSummENG, NPowER and SIMetrix) are used in monolingual and multilingual summary evaluation. Same of those metrics are adapted to multilingual evaluation while others (i.e. AutoSummENG) can from th"
W98-1502,C94-2184,0,0.44107,"Missing"
W98-1502,P87-1022,0,0.118744,"Missing"
W98-1502,C88-1021,0,0.291474,"Missing"
W98-1502,P89-1032,0,0.0580421,"Missing"
W98-1502,C96-1021,0,0.0392262,"en proposed, making use of e.g. neural networks, a situation semantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et a!. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic processing of growing language resources. , Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or linguistic knowledge (Baldwin 1997; Dagan & llai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams eta!. 1996). Our work is a continuation of these latest trends in the search for inexpensive, rapid and reliable procedures for anaph~ ora resolution. It shows how pronouns in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing, benefiting instead from corpus-based NLP techniques such as sentence splitting and part-ofspeech tagging. On the other hand, none of the projects reported so far, has looked at the multilingual aspects of the approaches that have been developed, or, in particu~"
W98-1502,J90-4001,0,0.0389455,"Missing"
W98-1502,J94-4002,0,0.106155,"Missing"
W98-1502,C94-2191,1,0.886091,"Missing"
W98-1502,W97-1302,0,0.0221653,"Missing"
W98-1502,A92-1028,0,0.0377426,"Missing"
W98-1502,1995.tmi-1.7,0,0.0867686,"Missing"
W98-1502,C94-2189,0,0.111123,"networks, a situation semantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et a!. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic processing of growing language resources. , Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or linguistic knowledge (Baldwin 1997; Dagan & llai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams eta!. 1996). Our work is a continuation of these latest trends in the search for inexpensive, rapid and reliable procedures for anaph~ ora resolution. It shows how pronouns in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing, benefiting instead from corpus-based NLP techniques such as sentence splitting and part-ofspeech tagging. On the other hand, none of the projects reported so far, has looked at the multilingual aspects of the approaches that have been developed, or, in particu~ lar, how a specific approac"
W98-1502,W97-1314,0,0.0386543,"Missing"
W98-1502,A88-1003,0,0.251172,"Missing"
W98-1502,W97-1305,0,0.0107596,", knowledge poor anaphora resolution and multilingual NLP For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979). However, to represent and manipulate the various types of linguistic and domain 7 While anaphora resolution projects have been reported for French (Popescu-Belis & Robba 1997, Rolbert 1989), German (Dunker & Umbach 1993; Fischer eta!. 1996; Leass & Schwa!l1991; Stuckardt 1996; Stuckardt 1997), Japanese (Mori eta!. 1997; Nakaiwa & Ikehara 1992; Nakaiwa & Ikehara 1995; Nakaiwa et a!. 1995; Nakaiwa et a!. 1996; Wakao 1994 ), Portuguese (Abra9os & Lopes 1994 ), Swedish (Fraurud, 1988) and Turkish (Tin & Akman, 1994), the research on languages other than English constitutes only a small part of all the work in this field. In contrast to previous work in the field, our project has a tmly multilingual character. We have developed a knowledge-poor, robust approach which we propose as a platform for multilingual pronoun resolution in technical manuals. The approach was initially developed"
W98-1502,C94-2185,0,0.0281356,"methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979). However, to represent and manipulate the various types of linguistic and domain 7 While anaphora resolution projects have been reported for French (Popescu-Belis & Robba 1997, Rolbert 1989), German (Dunker & Umbach 1993; Fischer eta!. 1996; Leass & Schwa!l1991; Stuckardt 1996; Stuckardt 1997), Japanese (Mori eta!. 1997; Nakaiwa & Ikehara 1992; Nakaiwa & Ikehara 1995; Nakaiwa et a!. 1995; Nakaiwa et a!. 1996; Wakao 1994 ), Portuguese (Abra9os & Lopes 1994 ), Swedish (Fraurud, 1988) and Turkish (Tin & Akman, 1994), the research on languages other than English constitutes only a small part of all the work in this field. In contrast to previous work in the field, our project has a tmly multilingual character. We have developed a knowledge-poor, robust approach which we propose as a platform for multilingual pronoun resolution in technical manuals. The approach was initially developed and tested for English, but we have also adapted and tested it for Polish and Arabic. We found that the approach could be adapted"
W98-1502,C90-3063,0,\N,Missing
W98-1502,W97-1306,0,\N,Missing
Y15-1023,abdul-mageed-diab-2012-awatif,0,0.0744711,"Missing"
Y15-1023,esuli-sebastiani-2006-sentiwordnet,0,0.0322902,"Missing"
Y15-1023,al-saif-markert-2010-leeds,0,0.0696839,"Missing"
Y15-1023,P09-1078,0,0.114255,"itional features, the number of positive adjectives and adverbs and also the number of negative adjective and adverbs in each document. 3.4 Feature transformation Feature transformation step determines the numerical representation used in the classification process. It’s performed by applying a weighting scheme on the extracted textual data of the corpus. We distinguish three weighting schemes: binary, term frequency and TF-IDF representation. Binary schema takes into account presence or absence of a term in a document. Term frequency considers the number of times a term occurs in a document (Li et al., 2009). TF-IDF (Term Frequency - Inverse Document Frequency) considers not only term frequencies in a docu201 ment, but also the relevance of a term in the entire collection of documents (Manning et al., 2008). Many researches confirm that the most suitable representation for sentiment classification is binary since overall sentiment may not usually be highlighted through repeated use of the same terms. In fact, Pang et al. (Pang et al., 2002) showed in their experiments that better performance is obtained using presence rather than frequency, that is, binary-valued feature vectors in which the entr"
Y15-1023,P14-2034,0,0.0614108,"Missing"
Y15-1023,N10-1120,0,0.0275683,"r (KNN), as well as similarity scores methods (i.e. phrase pattern matching, distance vector, frequency counts and statistical weight measures). Nevertheless, to get a good accurate classifier, we need to select the most effective set of textual predictors (Liu and Motoda, 2008). In sentiment classification, n-grams (Pang et al., 2002) are the most used features, however, there are some researches where other semantic features are tested such us opinion words and phrases, opinion operators such as negation (Mejova et al., 2011), parts of speech (Wang et al., 2014), and syntactic dependencies (Nakagawa et al., 2010). Some other researches attempt to integrate discourse features and report a significant added value of rhetorical roles in sentiment classification (Chardon et al., 2013). For instance, Somasundarun et al. (Somasundarun et al., 2009) proposed a supervised and unsupervised methods employing Discourse relations to improve sentiment classification. This is performed by adopting relational feature that exploit discourse and neighbor opinion information. In general, most of adopted features tend to be domain specific (e.g., the term television has a negative polarity in a movie review, but may hav"
Y15-1023,W02-1011,0,0.0321442,"ervised learning, typically probabilistic methods such as Naïve Bayes (NB) and Maximum Entropy (MaxEnt), and linear discrimination methods such as Support Vector machine (SVM). As other possible classification schemes, we mention nonparametric classifiers such as k-Nearest Neighbor (KNN), as well as similarity scores methods (i.e. phrase pattern matching, distance vector, frequency counts and statistical weight measures). Nevertheless, to get a good accurate classifier, we need to select the most effective set of textual predictors (Liu and Motoda, 2008). In sentiment classification, n-grams (Pang et al., 2002) are the most used features, however, there are some researches where other semantic features are tested such us opinion words and phrases, opinion operators such as negation (Mejova et al., 2011), parts of speech (Wang et al., 2014), and syntactic dependencies (Nakagawa et al., 2010). Some other researches attempt to integrate discourse features and report a significant added value of rhetorical roles in sentiment classification (Chardon et al., 2013). For instance, Somasundarun et al. (Somasundarun et al., 2009) proposed a supervised and unsupervised methods employing Discourse relations to"
Y15-1023,pasha-etal-2014-madamira,0,0.0325653,"to normalize the spelling of some Arabic characters which can be written in different ways. Arabic text can be vowelized, non-vowelized, or even partially vowelized. To ensure the detection and extraction of all orthographic word forms, we decided 199 to eliminate discretization from the reviews. Normalization is also applied to some characters such as alef by transforming all his forms (Alef Hamza above &quot; &quot;أand Alef Hamza below &quot; )&quot;إinto bare Alef &quot;&quot;ا. This process is applied because many reviewers omit or confuse these similar letters and use them interchangeably. Stemming: MADAMIRA (Pasha et al., 2014) is used to apply a light stemming on the reviews. Light stemming aims, to transform nouns in singular and to conjugate verbs with the third personal pronoun. In fact, stemming, which reduces words to their roots, is not convenient in Arabic language, because it may affect the word sense. Light stemming will be helpful to detect all morphological variations of the word. Stop-word removal: To accelerate the detection process of the lexical cues, we have profited from the stop-word list of Khoja stemmer tool (Khoja and Garside, 1999) and revised it. In fact, this Stop-word list was established t"
Y15-1023,D09-1018,0,0.0618434,"Missing"
Y15-1023,J11-2001,0,0.0772424,"ds employing Discourse relations to improve sentiment classification. This is performed by adopting relational feature that exploit discourse and neighbor opinion information. In general, most of adopted features tend to be domain specific (e.g., the term television has a negative polarity in a movie review, but may have a positive one in a book review). This problem can be solved by the second approach: the lexicon based approach. 197 Lexicon-based approach relies on a sentiment lexicon to calculate orientation for a document from the semantic orientation of words or phrases in the document (Taboada et al., 2011). Sentiment lexicon is a collection of classified opinion terms that can be compiled according to three approaches: dictionary-based approach, corpus-based approach, or combined approach. In dictionary based approach, we attempt to find a set of opinion seed words and then enrich them by retrieving their synonyms and antonyms from dictionaries such WordNet and Thesaurus. For instance, Hu and Liu (Hu and Liu, 2004) and Esuli and Sebastiani (Esuli and Sebastiani, 2005) classify polarity using emotion words and semantic relations from WordNet, WordNet Gloss, WordNet-Affect and SentiWordNet respec"
zribi-etal-2014-conventional,habash-etal-2012-conventional,1,\N,Missing
zribi-etal-2014-conventional,I13-1133,1,\N,Missing
zribi-etal-2014-conventional,N13-1044,1,\N,Missing
zribi-etal-2014-conventional,pasha-etal-2014-madamira,1,\N,Missing
zribi-etal-2014-conventional,W13-2813,1,\N,Missing
zribi-etal-2014-conventional,N13-1066,1,\N,Missing
zribi-etal-2014-conventional,W12-2301,1,\N,Missing
