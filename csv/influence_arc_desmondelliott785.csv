2020.acl-main.679,J90-1003,0,\N,Missing
2020.acl-main.679,Q15-1016,0,\N,Missing
2020.acl-main.679,D12-1071,0,\N,Missing
2020.acl-main.679,L16-1680,0,\N,Missing
2020.acl-main.679,K17-1004,0,\N,Missing
2020.acl-main.679,S18-2023,0,\N,Missing
2020.acl-main.679,L18-1239,0,\N,Missing
2020.acl-main.679,W18-5446,0,\N,Missing
2020.acl-main.679,Q19-1004,1,\N,Missing
2020.acl-main.679,P19-1334,0,\N,Missing
2020.acl-main.679,P19-1478,0,\N,Missing
2020.acl-main.679,N19-1419,0,\N,Missing
2020.acl-main.679,W19-4825,0,\N,Missing
2020.acl-main.679,W19-4820,0,\N,Missing
2020.acl-main.679,P19-1084,1,\N,Missing
2020.acl-main.679,W19-4828,0,\N,Missing
2020.acl-main.679,N19-1423,0,\N,Missing
2020.acl-main.679,D19-1109,0,\N,Missing
2020.acl-main.679,D19-1593,1,\N,Missing
2020.acl-main.682,D16-1203,0,0.0262625,"e that is mapped to a scene graph composed of objects represented in terms of abstract and situated attributes. Introduction Several grounded language learning tasks have been proposed to capture perceptual aspects of language (Shekhar et al., 2017; Hudson and Manning, 2019; Suhr et al., 2019; Agrawal et al., 2018). However, the advances in this field have been primarily driven by the final performance measures and less on the grounding capability of the models. In fact, in some cases, high-performance models exploit dataset biases to achieve high scores on the final task (Zhang et al., 2016; Agrawal et al., 2016). In the literature, several methods have been proposed to analyse what kind of information is captured by neural network representations (K´ad´ar et al., 2017; Belinkov and Glass, 2019). Most of these works examine the hidden state representations learned by models trained on only textual data. However, many aspects of human semantic representations are grounded in perceptual experience (Andrews et al., 2009; Riordan and Jones, 2011). This paper explores the idea that visually grounded representations ought to be a result of systematic composition of grounded representations (Harnad, 1990). F"
2020.acl-main.682,J17-4003,0,0.0687773,"Missing"
2020.acl-main.682,W18-5446,0,0.0639334,"Missing"
2020.acl-main.699,W12-3202,0,0.021251,"ue ACL EACL EMNLP NAACL CL TACL 1250 1000 750 500 250 18 19 20 20 16 17 20 20 14 15 20 20 12 13 20 20 20 20 10 11 0 Figure 1: The distribution of the number of articles published between 2010–2019 in the ACL Anthology. “This paper does not cite any literature from before the neural network era.” Scientific progress benefits from researchers “standing on the shoulders of giants” and one way for researchers to recognise those shoulders is by citing articles that influence and inform their work. The nature of citations in NLP publications has previously been analysed with regards to topic areas (Anderson et al., 2012; Gollapalli and Li, 2015; Mariani et al., 2019b), semantic relations (G´abor et al., 2016), gender issues (Vogel and Jurafsky, 2012; Schluter, 2018), the role of sharing software (Wieling et al., 2018), and citation and collaboration networks (Radev et al., 2016; Mariani et al., 2019a). Mohammad (2019) provides the most recent analysis of the ACL Anthology, looking at demographics, topic areas, and research impact via citation analysis. In this paper, we conduct a corpus analysis of papers published in recent ACL venues to determine whether the community is collectively forgetting about older"
2020.acl-main.699,bird-etal-2008-acl,0,0.1294,"Missing"
2020.acl-main.699,councill-etal-2008-parscit,0,0.134409,"Missing"
2020.acl-main.699,L16-1586,0,0.0418575,"Missing"
2020.acl-main.699,W09-3607,0,0.105245,"Missing"
2020.acl-main.699,D18-1301,0,0.0283597,"r of articles published between 2010–2019 in the ACL Anthology. “This paper does not cite any literature from before the neural network era.” Scientific progress benefits from researchers “standing on the shoulders of giants” and one way for researchers to recognise those shoulders is by citing articles that influence and inform their work. The nature of citations in NLP publications has previously been analysed with regards to topic areas (Anderson et al., 2012; Gollapalli and Li, 2015; Mariani et al., 2019b), semantic relations (G´abor et al., 2016), gender issues (Vogel and Jurafsky, 2012; Schluter, 2018), the role of sharing software (Wieling et al., 2018), and citation and collaboration networks (Radev et al., 2016; Mariani et al., 2019a). Mohammad (2019) provides the most recent analysis of the ACL Anthology, looking at demographics, topic areas, and research impact via citation analysis. In this paper, we conduct a corpus analysis of papers published in recent ACL venues to determine whether the community is collectively forgetting about older papers as it experiences a period 1. Do recently published papers have a tendency to cite more recently published papers, and less older literature?"
2020.acl-main.699,W12-3204,0,0.0273524,"distribution of the number of articles published between 2010–2019 in the ACL Anthology. “This paper does not cite any literature from before the neural network era.” Scientific progress benefits from researchers “standing on the shoulders of giants” and one way for researchers to recognise those shoulders is by citing articles that influence and inform their work. The nature of citations in NLP publications has previously been analysed with regards to topic areas (Anderson et al., 2012; Gollapalli and Li, 2015; Mariani et al., 2019b), semantic relations (G´abor et al., 2016), gender issues (Vogel and Jurafsky, 2012; Schluter, 2018), the role of sharing software (Wieling et al., 2018), and citation and collaboration networks (Radev et al., 2016; Mariani et al., 2019a). Mohammad (2019) provides the most recent analysis of the ACL Anthology, looking at demographics, topic areas, and research impact via citation analysis. In this paper, we conduct a corpus analysis of papers published in recent ACL venues to determine whether the community is collectively forgetting about older papers as it experiences a period 1. Do recently published papers have a tendency to cite more recently published papers, and less"
2020.acl-main.699,J18-4003,0,0.0625409,"Missing"
2020.acl-main.699,D15-1235,0,0.0282743,"CL TACL 1250 1000 750 500 250 18 19 20 20 16 17 20 20 14 15 20 20 12 13 20 20 20 20 10 11 0 Figure 1: The distribution of the number of articles published between 2010–2019 in the ACL Anthology. “This paper does not cite any literature from before the neural network era.” Scientific progress benefits from researchers “standing on the shoulders of giants” and one way for researchers to recognise those shoulders is by citing articles that influence and inform their work. The nature of citations in NLP publications has previously been analysed with regards to topic areas (Anderson et al., 2012; Gollapalli and Li, 2015; Mariani et al., 2019b), semantic relations (G´abor et al., 2016), gender issues (Vogel and Jurafsky, 2012; Schluter, 2018), the role of sharing software (Wieling et al., 2018), and citation and collaboration networks (Radev et al., 2016; Mariani et al., 2019a). Mohammad (2019) provides the most recent analysis of the ACL Anthology, looking at demographics, topic areas, and research impact via citation analysis. In this paper, we conduct a corpus analysis of papers published in recent ACL venues to determine whether the community is collectively forgetting about older papers as it experiences"
2020.findings-emnlp.242,N19-1422,0,0.0964763,"ers have developed deep learning models that combine visual, linguistic, and auditory modalities for a variety of multimodal tasks, such as automatic image captioning (Vinyals et al., 2015), visual questionanswering (Antol et al., 2015), and image–speech retrieval (Harwath and Glass, 2015), inter-alia. In multimodal automatic speech recognition (ASR), there have been efforts to integrate visual context into acoustic models (Miao and Metze, 2016) and sequence-to-sequence models (Palaskar 1 The code is available at https://github.com/ tejas1995/MultimodalASR et al., 2018; Sanabria et al., 2018; Caglayan et al., 2019). However, it is not clear if the visual context actually improves ASR or if it helps to regularize the model (Caglayan et al., 2019). Srinivasan et al. (2020) recently showed that global visual context (a single feature vector representing the entire image) is useful when the visually depictable linguistic inputs are masked, i.e., masking the speech that refer to entities. This experimental methodology, inspired by Caglayan et al. (2019), creates a systematic gap in the speech signal that can be resolved by leveraging the visual context; for example, when the audio drops during online distanc"
2020.findings-emnlp.242,D18-1329,1,0.845767,"pes of multimodal representations such as imagescene representations and titles of instructional videos respectively. Although all these integration methods show improvements over unimodal baselines, it is not clear when such approaches perform better, and which representations are best. It has been argued that traditional multimodal architectures do not necessarily take advantage of image semantics in different tasks. Caglayan et al. (2019) showed that multimodal ASR models trained with shift adaptation (Miao and Metze, 2016)6 use the image as a regularization signal. In a similar direction, Elliott (2018) showed that misaligment between image and text representations do not affect multimodal MT models. Ramakrishnan et al. (2018) and Grand and Belinkov (2019) showed that traditional VQA neural architectures ignore the visual context and focus on linguistic biases of the dataset. More related to our work are the studies of Srinivasan et al. (2020) and Caglayan et al. (2019), which explore how multimodal models use image information under noisy scenarios. These studies conclude that when certain nouns 6 A linear transformation conditioned on the visual features is applied on the audio features. 2"
2020.findings-emnlp.242,W19-1801,0,0.0267086,"ion methods show improvements over unimodal baselines, it is not clear when such approaches perform better, and which representations are best. It has been argued that traditional multimodal architectures do not necessarily take advantage of image semantics in different tasks. Caglayan et al. (2019) showed that multimodal ASR models trained with shift adaptation (Miao and Metze, 2016)6 use the image as a regularization signal. In a similar direction, Elliott (2018) showed that misaligment between image and text representations do not affect multimodal MT models. Ramakrishnan et al. (2018) and Grand and Belinkov (2019) showed that traditional VQA neural architectures ignore the visual context and focus on linguistic biases of the dataset. More related to our work are the studies of Srinivasan et al. (2020) and Caglayan et al. (2019), which explore how multimodal models use image information under noisy scenarios. These studies conclude that when certain nouns 6 A linear transformation conditioned on the visual features is applied on the audio features. 2674 are dropped from the dominant language modality, multimodal models are capable of properly using the semantics provided by the image. However, unlike th"
2020.findings-emnlp.242,P19-1659,1,0.891332,"Missing"
2020.findings-emnlp.242,K19-1006,0,0.0584237,"ess and Wolf, 2017). The norm of the gradient is clipped with a threshold of 1 (Pascanu et al., 2012). UNIMODAL has 8.3M parameters, while MAG and MAOP have 9.1M parameters each. Models are trained using the nmtpytorch framework (Caglayan et al., 2017). We first pre-train our models on the SpeechCOCO dataset, which is also Augmented with masked speech. For every model described in Section 2, we train models on FACC using several checkpoints from the SpeechCOCO pre-training, and choose the model with the best development WER on the Augmented development set. This pre-training step, inspired by Ilharco et al. (2019), was crucial to ensure stable training of our models on the FACC dataset. Models take ≈ 5-6 hours to train on the FACC dataset. 3.6 Evaluation Metrics Our model development (and the associated results) is conducted on the development set of the Flickr8K Audio Captions Corpus; the rest of our analysis is conducted on the test set. We report Word Error Rate (WER) for all our models, and for datasets with masked audio, we compute Recovery Rate (RR) (Srinivasan et al., 2020), which measures the percentage of masked words in the dataset that are correctly recovered in the transcription: RR = Objec"
2020.findings-emnlp.242,N19-1357,0,0.0344213,"Missing"
2020.findings-emnlp.242,P17-2031,0,0.0566557,"Missing"
2020.findings-emnlp.242,E17-2025,0,0.0192899,"uses a single “global” feature vector extracted from each image. We extract visual features from ResNet-50 CNN (He et al., 2016) pre-trained on ImageNet. We extract 2048-dim average-pooled features, and project these to 256-dim through a learned linear layer: v = W · CNN(img) 3.4 Model Implementation All models are trained using Adam optimizer (Kingma and Ba, 2014), with a learning rate of 0.0004, decay of 0.5 and batch size of 36. The encoder and decoder GRU both have 256 hidden units. The embedding dimension for the decoder is also 256, and the input and output decoder embeddings are tied (Press and Wolf, 2017). The norm of the gradient is clipped with a threshold of 1 (Pascanu et al., 2012). UNIMODAL has 8.3M parameters, while MAG and MAOP have 9.1M parameters each. Models are trained using the nmtpytorch framework (Caglayan et al., 2017). We first pre-train our models on the SpeechCOCO dataset, which is also Augmented with masked speech. For every model described in Section 2, we train models on FACC using several checkpoints from the SpeechCOCO pre-training, and choose the model with the best development WER on the Augmented development set. This pre-training step, inspired by Ilharco et al. (201"
2020.findings-emnlp.242,D17-1017,0,0.0273071,"not expanded to other types of words. From an image representation perspective, previous works have studied the utility of using local representations, rather than global ones for multimodal language processing tasks. For instance, Xu et al. (2015) show that, by using attention, the model can use different regions of the image while performing image captioning. More recent work shows that bounding boxes (Ren et al., 2015), a discrete variant of attention over images, improve the representation and hence the performance of different tasks such as VQA (Anderson et al., 2018), image captioning (Yin and Ordonez, 2017) and machine translation (Specia et al., 2020). In this work, we apply this methodology to multimodal ASR (see Section 3.4). 6 Conclusions In this work, we introduce a new model for multimodal ASR that attends overs fine-grained object proposals and is capable of recovering words which are masked in the speech signal. We show that our model recovers masked words because it can accurately identify the relevant object proposal(s), and that this ability allows it to not only recover the object when it has been masked in the speech signal, but also the object’s attributes. In future work, we plan"
2020.findings-emnlp.242,P19-1282,0,0.0318616,"Missing"
2020.findings-emnlp.244,D17-1303,0,0.0213547,"pe-ind and pipe-seq) Image Encoder Shared encoder Speech Dedicated encoder Dedicated encoder Methodology The architecture and the training procedure used in this paper are inspired by the improved version of the visually-grounded spoken language understanding system of Merkx et al. (2019). Appendix A.1 provides details on the choice of hyperparameters. Decoder Text (d) mtl-transcribe/mtl-translate model Image Encoder Multilingual data The idea of using multilingual data is not new in the literature: existing work focuses on using the same modality for the two languages, either text or speech. Gella et al. (2017) and K´ad´ar et al. (2018) show that textual descriptions of images in different languages in the Multi30K dataset (Elliott et al., 2016) can be used in conjunction to improve the performance of a visually-grounded model. Harwath et al. (2018) focus on speech, exploring how spoken captions in two languages can be used simultaneously to improve performance in an English-Hindi parallel subset of the Places-205 dataset (Zhou et al., 2014). In contrast, our experiments concern the setting where speech data from a low-resource language is used in conjunction with corresponding translated written ca"
2020.findings-emnlp.244,Q14-1006,0,0.0288808,"vely a translated version of these transcripts. We obtain these elements from a set of related datasets: • Flickr8K (Hodosh et al., 2013) offers 8,000 images of everyday situations gathered from the website flickr.com together with English written captions (5 per image) that were obtained through crowd sourcing. • The Flickr Audio Caption Corpus (Harwath and Glass, 2015), augments Flickr8K with spoken captions read aloud by crowd workers. • F30kEnt-JP (Nakayama et al., 2020) provides Japanese translations of the captions (generated by humans). It covers the images and captions from Flickr30k (Young et al., 2014), a superset of Flickr8K, but only provides the translations of two captions per image.1 In all experiments, we use English as the source language for our models. While English is not a low-resource language, it is the only one for which we have spoken captions. The low-resource setting with translations is thus a simulated setting. To summarize, we have 8,000 images with 40,000 captions (five per image), in both English written and spoken form (amounting to ∼34 hours of speech). In addition, we have Japanese translations for two captions per image. Validation and test sets are composed of 1,0"
2020.findings-emnlp.244,2020.lrec-1.518,0,0.0317323,"ining. For our experiments on textual supervision, we additionally need the transcriptions corresponding to those spoken captions, or alternatively a translated version of these transcripts. We obtain these elements from a set of related datasets: • Flickr8K (Hodosh et al., 2013) offers 8,000 images of everyday situations gathered from the website flickr.com together with English written captions (5 per image) that were obtained through crowd sourcing. • The Flickr Audio Caption Corpus (Harwath and Glass, 2015), augments Flickr8K with spoken captions read aloud by crowd workers. • F30kEnt-JP (Nakayama et al., 2020) provides Japanese translations of the captions (generated by humans). It covers the images and captions from Flickr30k (Young et al., 2014), a superset of Flickr8K, but only provides the translations of two captions per image.1 In all experiments, we use English as the source language for our models. While English is not a low-resource language, it is the only one for which we have spoken captions. The low-resource setting with translations is thus a simulated setting. To summarize, we have 8,000 images with 40,000 captions (five per image), in both English written and spoken form (amounting"
2020.nlpbt-1.2,N19-1422,0,0.324701,"a Ramon Sanabria CSTR, ILCC University of Edinburgh r.sanabria@ed.ac.uk sliding Tejas Srinivasan Language Technologies Institute Carnegie Mellon University tsriniva@andrew.cmu.edu Visual signal Decoder the girl is sliding down a green slide Figure 1: We propose to train multimodal speech recognition models while randomly masking different types of words in the speech signal. The model learns to use the visual signal to correctly predict the masked words. tainties, there is a need to clarify the circumstances in which visual signals are useful. Previous work in multimodal machine translation (Caglayan et al., 2019) and ASR (Srinivasan et al., 2020) shows that the visual signal is useful when the linguistic signal is degraded by dropping the input. In this setting, multimodal models leverage the visual signals to recover the missing language information. The results in (Srinivasan et al., 2020) are a promising start towards verifiably useful multimodality for robust speech recognition. However, the experiments were conducted with structured noise that focused on a predetermined set of groundable entities (i.e., nouns and places). In real world scenarios, however, noise occurs in a more unstructured manne"
2020.nlpbt-1.2,D18-1329,1,0.791418,"ine translation (Sulubacak et al., 2019), visual question-answering (VQA) (Antol et al., 2015), summarization (Palaskar et al., 2019) and automatic speech recognition (ASR) (Palaskar et al., 2018; Sanabria et al., 2018). However, it is unclear exactly how the visual signals are useful for these tasks. For example, in VQA, it has been observed that models can ignore the visual context and instead rely on linguistic biases in the dataset (Ramakrishnan et al., 2018; Grand and Belinkov, 2019); in machine translation, it has been shown that some models are not affected by incorrect visual signals (Elliott, 2018); and in multimodal ASR, the visual signals were shown to act as a regularizer instead of useful disambiguating context (Caglayan et al., 2019). Given these uncer11 Proceedings of the First International Workshop on Natural Language Processing Beyond Text, pages 11–18 c Online, November 20, 2020. 2020 Association for Computational Linguistics http://www.aclweb.org/anthology/W23-20%2d 6-layer Pyramidal BiGRU English Speech ASR ShiftAdaptation Early-DF HierAttn-DF Att embeddings a dog GRU Att GRU with a collar ResNet50 jumping Visual features Figure 2: Our unimodal ASR model, along with several"
2020.nlpbt-1.2,W19-1801,0,0.0282758,"domly Introduction Jointly modelling linguistic and visual signals is beneficial for several language processing tasks, such as machine translation (Sulubacak et al., 2019), visual question-answering (VQA) (Antol et al., 2015), summarization (Palaskar et al., 2019) and automatic speech recognition (ASR) (Palaskar et al., 2018; Sanabria et al., 2018). However, it is unclear exactly how the visual signals are useful for these tasks. For example, in VQA, it has been observed that models can ignore the visual context and instead rely on linguistic biases in the dataset (Ramakrishnan et al., 2018; Grand and Belinkov, 2019); in machine translation, it has been shown that some models are not affected by incorrect visual signals (Elliott, 2018); and in multimodal ASR, the visual signals were shown to act as a regularizer instead of useful disambiguating context (Caglayan et al., 2019). Given these uncer11 Proceedings of the First International Workshop on Natural Language Processing Beyond Text, pages 11–18 c Online, November 20, 2020. 2020 Association for Computational Linguistics http://www.aclweb.org/anthology/W23-20%2d 6-layer Pyramidal BiGRU English Speech ASR ShiftAdaptation Early-DF HierAttn-DF Att embeddin"
2020.nlpbt-1.2,K19-1006,0,0.0614022,"Missing"
2020.nlpbt-1.2,P17-2031,0,0.061269,"Missing"
2020.nlpbt-1.2,P19-1659,1,0.894143,"Missing"
2021.eacl-main.48,W19-5203,0,0.315463,"re of sentences, making it less prone to overfitting the training data. Specifically, we explore three methods for integrating syntactic planning into captioning in our 593 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 593–607 April 19 - 23, 2021. ©2021 Association for Computational Linguistics experiments: (a) pre-generation of syntactic tags from the image, (b) interleaved generation of syntactic tags and words (N˘adejde et al., 2017), and (c) multi-task learning with a shared encoder that predicts syntactic tags or words (Currey and Heafield, 2019). We do so while also empirically investigating four different levels of syntactic granularity. The main findings of our experiments are that: • jointly modeling syntactic tags and tokens leads to improvements in Transformer-based (Cornia et al., 2020) and RNN-based (Anderson et al., 2018) image captioning models; • although the effectiveness of each syntactic tag set varies across our explored approaches, the widely-used chunking tag set never outperforms syntactic tags with finer granularity; • compositional generalization is affected by directly mapping from image representation to tokens b"
2021.eacl-main.48,E12-1076,0,0.0469469,"alization (Reiter and Dale, 1997). Within this framework, a three-stage pipeline has emerged (Reiter, 1994): • Text Planning: combining content determination and discourse planning. • Sentence Planning: combining sentence aggregation, lexicalization and referring expression generation to determine the structure of the selected input to be included in the output. • Linguistic Realization: this stage involves syntactic, morphological and orthographic processing to produce the final sentence. Early methods for image captioning drew inspiration from this framework; for example, the M IDGE system (Mitchell et al., 2012) features explicit steps for content determination, given detected objects, and sentence aggregation based on local and full phrase-structure tree construction, and T REETALK composes tree fragments using integer linear programming (Kuznetsova et al., 2014). More recently, Wang et al. (2017) propose a twostage algorithm where the skeleton sentence of the caption (main objects and their relationships) is first generated, and then the attributes for each object are generated if they are worth mentioning. In contrast, the majority of neural network models are based on the encoder-decoder framewor"
2021.eacl-main.48,W17-4707,0,0.0600285,"Missing"
2021.eacl-main.48,K19-1009,1,0.0516043,"lity, defined as the algebraic potential to understand and produce novel combinations from known components (Loula et al., 2018), has been questioned. Semantic compositionality of language in neural networks has attracted interest in the community (Irsoy and Cardie, 2014; Lake and Baroni, 2018; Baroni, 2019) as compositionality is conjectured to be a core feature not only of language but also of human thought (Fodor and Lepore, 2002). In image captioning, improving compositional generalization is a fundamental step towards generalizable systems that can be employed in daily life. To this end, Nikolaus et al. (2019) recently introduced a compositional generalization dataset where models need to describe images that depict unseen compositions of primitive concepts. For example, models are trained to describe images with “white” entities and all types of “dog” concepts but never the adjective–noun composition of “white dog.” In their dataset, models are evaluated on their ability to caption images depicting the unseen composition of held out concepts. Their study suggests that RNN-based captioning models do not compositionally generalize, and that this is primarily attributable to the language generation c"
2021.eacl-main.48,P02-1040,0,0.114661,"aptions for each of the M images in an evaluation set, M {hs11 , . . . , s1K i, . . . , hsM 1 , . . . , sK i}, the recall of the concept pairs is given by: Recall@K = m |{hsm k i|∃k : sk ∈ C}| , M (2) where sm k denotes the k-th generated caption for image m and C is the set of captions which contain the expected concept pair and in which the adjective or the verb is a dependent of the noun. In addition, we use pycocoeval to score models on the common image captioning metrics: METEOR (M; Denkowski and Lavie 2014), SPICE (S; Anderson et al. 2016), CIDE R (C; Vedantam et al. 2015), and BLEU (B; Papineni et al. 2002); and the recent multi-reference BERTS CORE (BS; Yi et al. 2020). In particular, we report the average recall across all concept pairs, the average across the four splits for each score in pycocoeval, and the average across all captions for BERTS CORE. Experimental Setup Data We use training and evaluation sets such that paradigmatic gaps exist in the training set. That is, for a concept pair {ci , cj }, the validation Dval and test Dtest sets only contain images in which at least one of the captions contains the pair of concepts, while the complementary set – where concepts ci and cj can only"
2021.eacl-main.48,N18-1202,0,0.0323313,"Missing"
2021.eacl-main.48,K18-2016,0,0.0152351,"r a concept pair {ci , cj }, the validation Dval and test Dtest sets only contain images in which at least one of the captions contains the pair of concepts, while the complementary set – where concepts ci and cj can only be observed independently – is used for training Dtrain . Following Nikolaus et al. (2019), we select the same 24 adjective–noun and verb–noun concept pairs, and split the English COCO dataset (Lin et al., 2014) into four sets, each containing six held out concept pairs. Pre-processing We first lower-case and strip away punctuation from the captions. We then use StanfordNLP (Qi et al., 2018) to tokenize and lemmatize the captions, and to extract universal POS tags and syntactic dependency relations. For IOBbased chunking, we train a classifier-based tagger on CoNLL2000 data (Tjong Kim Sang and Buchholz, 2000) using NLTK (Bird et al., 2009). Finally, we use the A* CCG parsing model by Yoshikawa et al. (2017) with ELMo embeddings (Peters et al., 596 Models We evaluate three models: • BUTD: Bottom-Up and Top-Down attention (Anderson et al., 2018), a strong and widelyemployed RNN-based captioning system. • BUTR: Bottom-Up and Top-Down attention with Ranking (Nikolaus et al., 2019), a"
2021.eacl-main.48,W00-0726,0,0.152863,"Missing"
2021.eacl-main.48,2020.acl-main.93,0,0.0262323,", s1K i, . . . , hsM 1 , . . . , sK i}, the recall of the concept pairs is given by: Recall@K = m |{hsm k i|∃k : sk ∈ C}| , M (2) where sm k denotes the k-th generated caption for image m and C is the set of captions which contain the expected concept pair and in which the adjective or the verb is a dependent of the noun. In addition, we use pycocoeval to score models on the common image captioning metrics: METEOR (M; Denkowski and Lavie 2014), SPICE (S; Anderson et al. 2016), CIDE R (C; Vedantam et al. 2015), and BLEU (B; Papineni et al. 2002); and the recent multi-reference BERTS CORE (BS; Yi et al. 2020). In particular, we report the average recall across all concept pairs, the average across the four splits for each score in pycocoeval, and the average across all captions for BERTS CORE. Experimental Setup Data We use training and evaluation sets such that paradigmatic gaps exist in the training set. That is, for a concept pair {ci , cj }, the validation Dval and test Dtest sets only contain images in which at least one of the captions contains the pair of concepts, while the complementary set – where concepts ci and cj can only be observed independently – is used for training Dtrain . Follo"
2021.eacl-main.48,P17-1026,0,0.0220629,"e select the same 24 adjective–noun and verb–noun concept pairs, and split the English COCO dataset (Lin et al., 2014) into four sets, each containing six held out concept pairs. Pre-processing We first lower-case and strip away punctuation from the captions. We then use StanfordNLP (Qi et al., 2018) to tokenize and lemmatize the captions, and to extract universal POS tags and syntactic dependency relations. For IOBbased chunking, we train a classifier-based tagger on CoNLL2000 data (Tjong Kim Sang and Buchholz, 2000) using NLTK (Bird et al., 2009). Finally, we use the A* CCG parsing model by Yoshikawa et al. (2017) with ELMo embeddings (Peters et al., 596 Models We evaluate three models: • BUTD: Bottom-Up and Top-Down attention (Anderson et al., 2018), a strong and widelyemployed RNN-based captioning system. • BUTR: Bottom-Up and Top-Down attention with Ranking (Nikolaus et al., 2019), an RNNbased, multi-task model trained for image captioning and image–sentence ranking that achieves state-of-the-art performance in the compositional generalization task.1 • M2 -TRM: Meshed-Memory Transformer (Cornia et al., 2020), a recently proposed Transformer-based architecture that achieves state-of-the-art performan"
2021.emnlp-main.775,Q19-1004,0,0.0244582,"ever, we find that the visual object annotations used in pretraining, which are automatically generated by an object detector, are much noisier than expected. We surmise that the ensuing models do not recruit text because it is not useful for predicting these noisy object features, and discuss implications for future V&L models. 2 Related Work Significant efforts have been devoted to understanding what is learned by text-only pretrained language models (Rogers et al., 2020), which can be categorised as probing based on diagnostic classifier, analysing attention weights, or direct evaluations (Belinkov and Glass, 2019). However, much less work has looked at V&L pretraining. Two studies have examined learned attention weights: Li et al. (2020) find that there are attention heads specialising in entity phrase grounding in VisualBERT, but do not investigate text-to-vision attention. Cao et al. (2020) find that the textual modality dominates in UNITER and LXMERT, both in terms of overall attention and more specifically during visual co-reference resolution tasks. In a direct evaluation analysis, Parcalabescu et al. (2021) evaluate VilBERT and LXMERT on a counting task and find more evidence of dataset bias (pre"
2021.emnlp-main.775,N19-1423,0,0.0742235,"Missing"
2021.emnlp-main.775,2021.acl-long.505,0,0.0352528,"al. (2021) evaluate VilBERT and LXMERT on a counting task and find more evidence of dataset bias (predicting frequent numbers) than accurate image grounding. These results all indicate that cross-modal interaction within these models may not be as strong or as symmetric as often assumed, which is further confirmed by the methods introduced in this paper. In addition, we evaluate a larger set of models, covering both dual-stream and single-stream models. The ablation method introduced here is related to concurrent work in language modelling (O’Connor and Andreas, 2021) and machine translation (Fernandes et al., 2021), in which the effect of removing context is measured. O’Connor and Andreas (2021) and Fernandes et al. (2021) use ablations at both training and evaluation, whereas we only study the effect of ablating inputs during evaluation. We envision cross-modal input ablation as a useful diagnostic check to be performed during model development, to test the effect of changes in architecture and optimisation. In this paper we perform a case study of crossmodal input ablation on existing models, to demonstrate its utility in understanding model behaviour. We test models that have different architectures"
2021.emnlp-main.775,2021.findings-acl.318,0,0.0325488,"Missing"
2021.emnlp-main.775,N19-1357,0,0.0269418,"al models have used diagnostic classifiers and attention analyses (Li et al., 2020; Cao et al., 2020; Parcalabescu et al., 2021). In comparison, crossmodal input ablation has the following advantages: • It is straightforward to perform, and easy to interpret, requiring no intervention in the model and only minimal intervention on the data. • As an intrinsic diagnostic, it examines the model directly, unlike methods that add learned parameters, such as classifier-based probing approaches (Hupkes et al., 2018). • It does not require interpreting activations or attention, which can be difficult (Jain and Wallace, 2019). vision, in order to identify possible avenues for improvement. Our experiments investigate different loss functions, initialisation and pretraining strategies, and visual co-masking procedures. None of these factors changes model behaviour significantly. However, we find that the visual object annotations used in pretraining, which are automatically generated by an object detector, are much noisier than expected. We surmise that the ensuing models do not recruit text because it is not useful for predicting these noisy object features, and discuss implications for future V&L models. 2 Related"
2021.emnlp-main.775,2020.acl-main.469,0,0.244385,"-target prediction tasks used during pretraining, in which a model must predict either a masked token given surrounding text and visual context (masked language modelling), or a masked visual object given the surrounding visual context and accompanying text (masked region classification). Cross-modal input ablation thus captures the degree to which a model depends on cross-modal inputs and activations when generating predictions. Our use of input ablation to assess cross-modal recruitment is novel. Previous analyses of multimodal models have used diagnostic classifiers and attention analyses (Li et al., 2020; Cao et al., 2020; Parcalabescu et al., 2021). In comparison, crossmodal input ablation has the following advantages: • It is straightforward to perform, and easy to interpret, requiring no intervention in the model and only minimal intervention on the data. • As an intrinsic diagnostic, it examines the model directly, unlike methods that add learned parameters, such as classifier-based probing approaches (Hupkes et al., 2018). • It does not require interpreting activations or attention, which can be difficult (Jain and Wallace, 2019). vision, in order to identify possible avenues for improve"
2021.emnlp-main.775,2021.acl-long.70,0,0.0926874,"Missing"
2021.emnlp-main.775,2020.acl-demos.14,0,0.0453779,"Missing"
2021.emnlp-main.775,2020.tacl-1.54,0,0.0197112,"initialisation and pretraining strategies, and visual co-masking procedures. None of these factors changes model behaviour significantly. However, we find that the visual object annotations used in pretraining, which are automatically generated by an object detector, are much noisier than expected. We surmise that the ensuing models do not recruit text because it is not useful for predicting these noisy object features, and discuss implications for future V&L models. 2 Related Work Significant efforts have been devoted to understanding what is learned by text-only pretrained language models (Rogers et al., 2020), which can be categorised as probing based on diagnostic classifier, analysing attention weights, or direct evaluations (Belinkov and Glass, 2019). However, much less work has looked at V&L pretraining. Two studies have examined learned attention weights: Li et al. (2020) find that there are attention heads specialising in entity phrase grounding in VisualBERT, but do not investigate text-to-vision attention. Cao et al. (2020) find that the textual modality dominates in UNITER and LXMERT, both in terms of overall attention and more specifically during visual co-reference resolution tasks. In"
2021.emnlp-main.775,P18-1238,0,0.0456388,"Missing"
2021.emnlp-main.775,D19-1514,0,0.0609422,"ne of the visual features are ablated, i.e. the model has access to the full image. This is the original multimodal setting and thus where a model that uses multimodal information effectively should perform best. Object: Here we remove only the image regions that correspond to the aligned textual phrase. This tests a model’s ability to ground text to specific regions of the image, by breaking any possible alignment. However, the model can 4.2 Models still use surrounding visual context features. We evaluate five different architectures: All: All the visual features are ablated and the LXMERT (Tan and Bansal, 2019) and ViLmodel needs to predict masked textual tokens 1 from its textual context only. Models that dehttps://github.com/e-bug/ pend on multimodal inputs should suffer. cross-modal-ablation. 9849 BERT (Lu et al., 2019) (dual-stream); VLBERT (Su et al., 2020), VisualBERT (Li et al., 2019) and UNITER (Chen et al., 2020) (single-stream). These models extend the BERT architecture to multimodal data and tasks by supplementing the text input with image features. Single-stream models process image and text jointly with a single Transformer encoder, while dual-stream ones first encode each modality sepa"
2021.emnlp-main.775,Q14-1006,0,0.0756029,"Missing"
2021.findings-emnlp.290,2020.findings-emnlp.151,1,0.754883,"utperform the general multilingual model, and performs close to its monolingual counterpart. This finding holds across two different pretraining methods, adapter-based pretraining and full model pretraining. is so important that better downstream task performance can be realized by pretraining models on more unique tokens, without repeating any examples, instead of iterating over smaller datasets (Raffel et al., 2020). When it is not possible to find vast amounts of unlabelled text, a better option is to continue pretraining a model on domainspecific unlabelled text (Han and Eisenstein, 2019; Dai et al., 2020), referred to as domain adaptive pretraining (Gururangan et al., 2020). This results in a better initialization for consequent fine-tuning for a downstream task in the specific domain, either on target domain data directly (Gururangan et al., 2020), or if unavailable on source domain data (Han and Eisenstein, 2019). The majority of domain-adapted models are trained on English domain-specific text, given the availability of English language data. However, many real-world applications, such as working with financial documents (Araci, 2019), biomedical text (Lee et al., 2019), and legal opinions"
2021.findings-emnlp.290,N19-1423,0,0.599063,"i) Data availabil1 Introduction ity: we cannot always find domain-specific text The unsupervised pretraining of language models in multiple languages so we should exploit the on unlabelled text has proven useful to many natu- available resources for effective transfer learning ral language processing tasks. The success of this (Zhang et al., 2020). (ii) Compute intensity: it is approach is a combination of deep neural networks environmentally unfriendly to domain-adaptive pre(Vaswani et al., 2017), the masked language model- train one model per language (Strubell et al., 2019), ing objective (Devlin et al., 2019), and large-scale and BioBERT was domain adaptive pretrained for corpora (Zhu et al., 2015). In fact, unlabelled data 23 days on 8×Nvidia V100 GPUs. (iii) Ease of ∗ use: a single multilingual model eases deployment The research was carried out while the author was employed at the University of Copenhagen. when an organization needs to work with multiple 3404 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3404–3418 November 7–11, 2021. ©2021 Association for Computational Linguistics languages on a regular basis (Johnson et al., 2017). Our method, multilingual domai"
2021.findings-emnlp.290,2020.findings-emnlp.387,0,0.0915063,"Missing"
2021.findings-emnlp.290,2020.acl-main.740,0,0.0394516,"Missing"
2021.findings-emnlp.290,D19-1433,0,0.0241162,"omain-specific model can outperform the general multilingual model, and performs close to its monolingual counterpart. This finding holds across two different pretraining methods, adapter-based pretraining and full model pretraining. is so important that better downstream task performance can be realized by pretraining models on more unique tokens, without repeating any examples, instead of iterating over smaller datasets (Raffel et al., 2020). When it is not possible to find vast amounts of unlabelled text, a better option is to continue pretraining a model on domainspecific unlabelled text (Han and Eisenstein, 2019; Dai et al., 2020), referred to as domain adaptive pretraining (Gururangan et al., 2020). This results in a better initialization for consequent fine-tuning for a downstream task in the specific domain, either on target domain data directly (Gururangan et al., 2020), or if unavailable on source domain data (Han and Eisenstein, 2019). The majority of domain-adapted models are trained on English domain-specific text, given the availability of English language data. However, many real-world applications, such as working with financial documents (Araci, 2019), biomedical text (Lee et al., 2019),"
2021.findings-emnlp.290,mitrofan-2017-bootstrapping,0,0.0214454,"iffer et al. (2020b) for more details of adapter-based training and also describe them in the Appendix D for self-containedness. 4 Domain-Specific Downstream Tasks To demonstrate the effectiveness of our multilingual domain-specific models, we conduct experiments on two downstream tasks—Named Entity Recognition (NER) and sentence classification— using datasets from biomedical and financial domains, respectively. 4.1 NER in the biomedical domain Datasets We evaluate on 5 biomedical NER datasets in different languages. The French QUAERO (Névéol et al., 2014) dataset, the Romanian BIORO dataset (Mitrofan, 2017), and the English NCBI DISEASE dataset (Do˘gan et al., 2014) comprise biomedical publications. The Spanish PHARMACONER (Agirre et al., 2019) dataset comprises publicly available clinical case studies, and the Portuguese CLINPT dataset is the publicly available subset of the data collected by Lopes et al. (2019), comprising texts about neurology from a clinical journal. The descriptive statistics of the Adapter-based training In contrast to fine- NER datasets are listed in Table 2, and more details tuning all weights of the base model, adapter-based about the datasets can be found in Appendix B"
2021.findings-emnlp.290,2020.emnlp-demos.2,0,0.0215146,"cial tax will have the great significance: Expert called it ""some hogwash"" Pretraining data for the mono DA -BERT includes Common Crawl texts and custom scraped data from two large debate forums. We believe this exposes the DA -BERT to the particular use of informal register. By contrast, the pretraining data we use are mainly sampled from publications. This could be an interesting direction of covering the variety of a language in sub-domains for a strong M DAPT model. 7 Related Work Fraction of continued words Recent studies on domain-specific BERT (Lee et al., 2019; Alsentzer et al., 2019; Nguyen et al., 2020), which mainly focus on English text, have 25 demonstrated that in-domain pretraining data can specific general 20 improve the effectiveness of pretrained models on downstream tasks. These works continue pretrain15 ing the whole base model—BERT or RO BERTA— 10 on domain-specific corpora, and the resulting mod5 els are supposed to capture both generic and domain-specific knowledge. By contrast, Belt0 fr ro es pt en en da de agy et al. (2019); Gu et al. (2020); Shin et al. Financial Biomedical (2020) train domain-specific models from scratch, tying an in-domain vocabulary. Despite its effecFigur"
2021.findings-emnlp.290,2020.emnlp-demos.7,0,0.135504,"ro train 5,424 8,137 # sents. dev 923 3,801 test 940 3,982 train 5,134 3,810 # mentions dev 787 1,926 test 960 1,876 # classes 1 4 1,540 1,481 1,413 4,516 4,123 4,086 10 1,192 336 973 7,600 2,047 6,315 13 1,886 631 629 5,180 1,864 1,768 4 Table 2: The descriptive statistics of the biomedical NER datasets. can be optimized using self-supervised pretraining or later downstream supervised objectives, are usually much lighter than the base model, enabling parameter efficient transfer learning (Houlsby et al., 2019). We train each adapter for 1.5M steps, taking only 2 GPU days. We refer readers to Pfeiffer et al. (2020b) for more details of adapter-based training and also describe them in the Appendix D for self-containedness. 4 Domain-Specific Downstream Tasks To demonstrate the effectiveness of our multilingual domain-specific models, we conduct experiments on two downstream tasks—Named Entity Recognition (NER) and sentence classification— using datasets from biomedical and financial domains, respectively. 4.1 NER in the biomedical domain Datasets We evaluate on 5 biomedical NER datasets in different languages. The French QUAERO (Névéol et al., 2014) dataset, the Romanian BIORO dataset (Mitrofan, 2017), a"
2021.findings-emnlp.290,2020.emnlp-main.617,0,0.0460796,"Missing"
2021.findings-emnlp.290,2020.aacl-main.56,0,0.0381106,"et al., 2020; Pfeiffer et al., 2020b). In • Markedet lukker: Medvind til bankaktier på contrast, we focus on the scenario that the NLP en rød C25-dag [P OSITIVE] model needs to process domain-specific text supporting a modest number of languages. English translation: Market closes: Tailwind Alternative approaches aim at adapting a model for bank shares on a red C25-day to a specific target task within the domain directly, • Nationalbanken tror ikke særskat får den store e.g. by an intermediate supervised fine-tuning step betydning: Ekspert kaldet det ""noget plad- (Pruksachatkun et al., 2020; Phang et al., 2020), der"" [N EGATIVE] resulting in a model specialized for a single task. 3411 Domain adaptive pretraining, on the other hand, aims at providing a good base model for different tasks within the specific domain. 8 Matthew McDermott. 2019. Publicly available clinical BERT embeddings. In Proceedings of the 2nd Clinical Natural Language Processing Workshop, pages 72–78, Minneapolis, Minnesota, USA. Association for Computational Linguistics. Conclusion We extend domain adaptive pretraining to a multilingual scenario that aims to train a single multilingual model better suited for the specific domain."
2021.findings-emnlp.290,2020.acl-main.467,0,0.0568034,"Missing"
2021.findings-emnlp.290,P19-1015,0,0.0466137,"Missing"
2021.findings-emnlp.290,2020.coling-main.603,0,0.0509471,"Missing"
2021.findings-emnlp.290,W19-6204,0,0.0192491,"ociation for Computational Linguistics languages on a regular basis (Johnson et al., 2017). Our method, multilingual domain adaptive pretraining (M DAPT), extends domain adaptive pretraining to a multilingual scenario, with the goal of training a single multilingual model that performs, as close as possible, to N language-specific models. M DAPT starts with a base model, i.e. a pretrained multilingual language model, such as mBERT (Devlin et al., 2019) or XLM-R (Conneau et al., 2020). As monolingual models have the advantage of language-specificity over multilingual models (Rust et al., 2020; Rönnqvist et al., 2019), we consider monolingual models as upper baseline to our approach. We assume the availability of English-language domain-specific unlabelled text, and, where possible, multilingual domain-specific text. However, given that multilingual domainspecific text can be a limited resource, we look to Wikipedia for general-domain multilingual text (Conneau and Lample, 2019). The base model is domain adaptive pretrained on the combination of the domain-specific text, and general-domain multilingual text. Combining these data sources should prevent the base model from forgetting how to represent multipl"
2021.findings-emnlp.290,2020.acl-main.368,0,0.023395,"difference between monolingual and multilingual tokenizer quality on specific text (the train splits of the downstream tasks), with their difference on general text sampled from Wikipedia. Figure 3 shows that the gap between monolingual and multilingual tokenization quality is indeed larger in the specific texts (green bars) compared to the general texts (brown bars), indicating that in a specific domain, it is even harder for a multilingual model to outperform a monolingual model. This suggests that methods for explicitly adding representations of domain-specific words (Poerner et al., 2020; Schick and Schütze, 2020) could be a promising direction for improving our approach. English translation: The Nationalbank does not think special tax will have the great significance: Expert called it ""some hogwash"" Pretraining data for the mono DA -BERT includes Common Crawl texts and custom scraped data from two large debate forums. We believe this exposes the DA -BERT to the particular use of informal register. By contrast, the pretraining data we use are mainly sampled from publications. This could be an interesting direction of covering the variety of a language in sub-domains for a strong M DAPT model. 7 Related"
2021.findings-emnlp.290,2020.emnlp-main.379,0,0.06205,"Missing"
2021.findings-emnlp.290,P19-1355,0,0.0155178,"for wanting to train a single model: (i) Data availabil1 Introduction ity: we cannot always find domain-specific text The unsupervised pretraining of language models in multiple languages so we should exploit the on unlabelled text has proven useful to many natu- available resources for effective transfer learning ral language processing tasks. The success of this (Zhang et al., 2020). (ii) Compute intensity: it is approach is a combination of deep neural networks environmentally unfriendly to domain-adaptive pre(Vaswani et al., 2017), the masked language model- train one model per language (Strubell et al., 2019), ing objective (Devlin et al., 2019), and large-scale and BioBERT was domain adaptive pretrained for corpora (Zhu et al., 2015). In fact, unlabelled data 23 days on 8×Nvidia V100 GPUs. (iii) Ease of ∗ use: a single multilingual model eases deployment The research was carried out while the author was employed at the University of Copenhagen. when an organization needs to work with multiple 3404 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3404–3418 November 7–11, 2021. ©2021 Association for Computational Linguistics languages on a regular basis (Johnson et al.,"
2021.findings-emnlp.290,2020.acl-main.148,0,0.0255127,"tasets. In this paper, we propose a method for domain adaptive pretraining of a single domain-specific multilingual language model that can be fine-tuned for tasks within that domain in multiple languages. There are several reasons for wanting to train a single model: (i) Data availabil1 Introduction ity: we cannot always find domain-specific text The unsupervised pretraining of language models in multiple languages so we should exploit the on unlabelled text has proven useful to many natu- available resources for effective transfer learning ral language processing tasks. The success of this (Zhang et al., 2020). (ii) Compute intensity: it is approach is a combination of deep neural networks environmentally unfriendly to domain-adaptive pre(Vaswani et al., 2017), the masked language model- train one model per language (Strubell et al., 2019), ing objective (Devlin et al., 2019), and large-scale and BioBERT was domain adaptive pretrained for corpora (Zhu et al., 2015). In fact, unlabelled data 23 days on 8×Nvidia V100 GPUs. (iii) Ease of ∗ use: a single multilingual model eases deployment The research was carried out while the author was employed at the University of Copenhagen. when an organization n"
2021.repl4nlp-1.16,Q19-1004,0,0.127102,"scene” (Figure 1). Such multi-step relational reasoning is at play in many real-life situations: e.g., the same exact pan may count as ‘big’ in all contexts except a restaurant kitchen. We experiment with two types of models to solve this task: a modular neural network (Hu et al., 2017) and LXMERT, a pre-trained multimodal transformer (Tan and Bansal, 2019). We probe the learned representations of LXMERT to assess whether, and to what extent, it has learned the underlying structure of the data. By means of two experiments with probing classifiers (Alain and Bengio, 2017; Hupkes et al., 2018; Belinkov and Glass, 2019), we first verify that it is able to perform the task at the image level (i.e., to compute the relative size of the target object at the image level); then, we test its ability to reason at the multi-image level and detect the image that stands out. The experiments show that LXMERT is able to solve the multi-step relational reasoning task 152 Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021), pages 152–162 Bangkok, Thailand (Online), August 6, 2021. ©2021 Association for Computational Linguistics T there is exactly one blue triangle that is small in its image i"
2021.repl4nlp-1.16,N18-2070,0,0.0173076,"derstanding (Jhamtani and Berg-Kirkpatrick, 2018); when it does, the changes often involve one object’s fixed attribute (color, shape, material, etc.) rather than a contextually-defined property whose applicability depends on the other objects in the image.2 A similar, partially overlapping task is discriminative captioning: systems are fed with a set of similar images and asked to provide a description that unequivocally refers to a target one. Many approaches have been proposed focusing on synthetic (Andreas and Klein, 2016; Achlioptas et al., 2019) or natural scenes (Vedantam et al., 2017; Cohn-Gordon et al., 2018; Vered et al., 2019), very often embedding pragmatic components based on the Rational Speech Acts framework (RSA; Goodman and Frank, 2016). Also in this case, however, differences among images mainly involve intrinsic attributes of the objects rather than relational properties defined at the level of the image. 4 4.1 a positive value < 0.5.3 Thus, an object with a certain area can count as big in one image and as small in another one. In total, the POS1 dataset contains 20K himage, statementi datapoints (16K train, 2K val, 2K test), where statements are about the size of a target object based"
2021.repl4nlp-1.16,N19-1423,0,0.0774388,"Missing"
2021.repl4nlp-1.16,D19-1065,0,0.0159132,"he single image and the multi-image context. 1 The code to generate the data, and to train and evaluate the models, is available at https://github.com/ jig-san/multi-step-size-reasoning. 153 3.2 Multi-Image Approaches Our approach is also related to other work in language and vision involving multiple images. One is the spot-the-difference task: in Jhamtani and Berg-Kirkpatrick (2018), models are fed with pairs of video-surveillance images that only differ in one detail, and asked to generate text which describes such difference. The same task—with different real-scene datasets—is explored by Forbes et al. (2019) and Su et al. (2017); others experiment with pairs of similar images drawn from CLEVR (Johnson et al., 2017) or similar synthetic 3D datasets (Park et al., 2019; Qiu et al., 2020). This task is akin to ours since it requires a higherlevel reasoning step: systems must reason over the two independent representations to describe what is different. However, in practice, it does not always require semantic understanding (Jhamtani and Berg-Kirkpatrick, 2018); when it does, the changes often involve one object’s fixed attribute (color, shape, material, etc.) rather than a contextually-defined proper"
2021.repl4nlp-1.16,D16-1125,0,0.0240677,"to describe what is different. However, in practice, it does not always require semantic understanding (Jhamtani and Berg-Kirkpatrick, 2018); when it does, the changes often involve one object’s fixed attribute (color, shape, material, etc.) rather than a contextually-defined property whose applicability depends on the other objects in the image.2 A similar, partially overlapping task is discriminative captioning: systems are fed with a set of similar images and asked to provide a description that unequivocally refers to a target one. Many approaches have been proposed focusing on synthetic (Andreas and Klein, 2016; Achlioptas et al., 2019) or natural scenes (Vedantam et al., 2017; Cohn-Gordon et al., 2018; Vered et al., 2019), very often embedding pragmatic components based on the Rational Speech Acts framework (RSA; Goodman and Frank, 2016). Also in this case, however, differences among images mainly involve intrinsic attributes of the objects rather than relational properties defined at the level of the image. 4 4.1 a positive value < 0.5.3 Thus, an object with a certain area can count as big in one image and as small in another one. In total, the POS1 dataset contains 20K himage, statementi datapoin"
2021.repl4nlp-1.16,D19-1275,0,0.0439159,"Missing"
2021.repl4nlp-1.16,D18-1436,0,0.0465296,"Missing"
2021.repl4nlp-1.16,P17-2034,0,0.112121,"any of these steps. In other words, the training data does not indicate which images contain an object that counts as big/small nor explicitly how many images contain a big/small target. 3 3.1 Related Work Visual Reasoning To evaluate reasoning abilities of multimodal models, several datasets of synthetic scenes and questions, such as CLEVR (Johnson et al., 2017), ShapeWorld (Kuhnle and Copestake, 2017), and MALeViC (Pezzelle and Fern´andez, 2019) have been proposed in recent years. Our work directly builds on them, and particularly on approaches adopting a multi-image setting, such as NLVR (Suhr et al., 2017) and NLVR2 (which, however, contains pairs of natural scenes; Suhr et al., 2019). In NLVR, in particular, a crowdsourced statement is coupled with a synthetic scene including 3 independent images, and models must verify whether the statement is true or false with respect to the entire visual input. This involves handling phenomena such as counting, negation or comparisons, that require perform relational reasoning over the entire scene, e.g.: There is a black item in every box, There is a tower with yellow base, etc. However, most hscene, statementi pairs do not challenge models to do the same"
2021.repl4nlp-1.16,P19-1644,0,0.231937,"mages contain an object that counts as big/small nor explicitly how many images contain a big/small target. 3 3.1 Related Work Visual Reasoning To evaluate reasoning abilities of multimodal models, several datasets of synthetic scenes and questions, such as CLEVR (Johnson et al., 2017), ShapeWorld (Kuhnle and Copestake, 2017), and MALeViC (Pezzelle and Fern´andez, 2019) have been proposed in recent years. Our work directly builds on them, and particularly on approaches adopting a multi-image setting, such as NLVR (Suhr et al., 2017) and NLVR2 (which, however, contains pairs of natural scenes; Suhr et al., 2019). In NLVR, in particular, a crowdsourced statement is coupled with a synthetic scene including 3 independent images, and models must verify whether the statement is true or false with respect to the entire visual input. This involves handling phenomena such as counting, negation or comparisons, that require perform relational reasoning over the entire scene, e.g.: There is a black item in every box, There is a tower with yellow base, etc. However, most hscene, statementi pairs do not challenge models to do the same at the level of the single image (or box), where a low-level understanding of t"
2021.repl4nlp-1.16,D19-1514,0,0.208115,"defined size and one image stands out in this regard. The task requires verifying whether a simple natural language statement standing for a first-order logical form describes a scene, e.g., “There is exactly one blue triangle that is small in its image in this scene” (Figure 1). Such multi-step relational reasoning is at play in many real-life situations: e.g., the same exact pan may count as ‘big’ in all contexts except a restaurant kitchen. We experiment with two types of models to solve this task: a modular neural network (Hu et al., 2017) and LXMERT, a pre-trained multimodal transformer (Tan and Bansal, 2019). We probe the learned representations of LXMERT to assess whether, and to what extent, it has learned the underlying structure of the data. By means of two experiments with probing classifiers (Alain and Bengio, 2017; Hupkes et al., 2018; Belinkov and Glass, 2019), we first verify that it is able to perform the task at the image level (i.e., to compute the relative size of the target object at the image level); then, we test its ability to reason at the multi-image level and detect the image that stands out. The experiments show that LXMERT is able to solve the multi-step relational reasoning"
2021.repl4nlp-1.16,D19-1285,1,0.787317,"Missing"
C14-1012,D13-1128,1,0.88814,"ood, such as people having fun at a party, or an action, such as using a computer. The bag-of-terms representation is limited to matching images based on the presence or absence of terms, and not the relation of the terms to each other. Figures 1(a) and (b) highlight the problem with using unstructured representations for image retrieval: there is a person and a computer in both images but only (a) depicts a person actually using the computer. To address this problem with unstructured representations we propose to represent the structure of an image using the Visual Dependency Representation (Elliott and Keller, 2013). The Visual Dependency Representation is a directed labelled graph over the regions of an image that captures the spatial relationships between regions. The representation is inspired by evidence from the psychology literature that people are better at recognising and searching for objects when the spatial relationships between the objects in the image are consistent with our expectations of the world.(Biederman, 1972; Bar and Ullman, 1996). In an automatic image description task, Elliott This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org"
C14-1012,H05-1066,0,0.135657,"Missing"
C14-1012,D13-1072,0,\N,Missing
C14-1012,D11-1041,0,\N,Missing
C18-1147,P16-1054,0,0.111699,"Missing"
C18-1147,W14-3348,0,0.367397,"Missing"
C18-1147,P15-2017,0,0.311606,"Missing"
C18-1147,P14-2074,1,0.840351,"example, van Miltenburg et al. (2016) provide a thorough overview of the uses of negations in human-generated image descriptions. Even though this is a low-frequent (or long-tail) phenomenon, studying a subset of the image descriptions informs us about the human image description process, and the cognitive requirements to produce a description containing a negation. It remains to be seen whether image description systems could produce similar descriptions. 5.2 Limitations and human validation Earlier work has shown that automated evaluation metrics do not correlate well with human judgments (Elliott and Keller, 2014; Kilickaya et al., 2017). For this reason, we should not blindly trust evaluation metrics in their assessment of system performance. Still, this paper only includes automatic, intrinsic metrics. This is by design: we want to gain insight into the descriptions, not to evaluate their quality. While you cannot evaluate a system using only automated metrics, they do tell us something about how a system behaves. Future researchers could try to improve the diversity metrics while maintaining or improving the quality of the descriptions (ideally measured by human judgments). At that point, we should"
C18-1147,D15-1021,0,0.0264482,"ed in future work. For reasons of space, we were also not able to cover metrics based on the frequency distribution of words in the training and validation data. We already mentioned Shetty et al.’s (2017) use of frequency ratios in the introduction. Their approach could be extended (perhaps also using log-likelihood (Rayson and Garside, 2000)) to produce a ranking of words that are over- or underused by a particular system. Overused words could be further analyzed by computing a ‘local precision’ metric, measuring how often a generated word is also used in at least one reference description. Ferraro et al. (2015) present other metrics in their survey of datasets for vision and language research, including: Yngve and Frazier measurements of syntactic complexity (Yngve, 1960; Frazier, 1985). Ferraro et al. (2015) found that the MS COCO and Flickr30K datasets have the most complex sentences, compared to other vision & language datasets. It is still an open question whether machine-generated descriptions are of equal complexity and, if not, what are the differences. Abstract-to-concrete ratio The authors also compare the proportion of abstract words that each corpus contains. They count abstract words by"
C18-1147,J84-3009,0,0.430681,"Missing"
C18-1147,N16-1014,0,0.144142,"Missing"
C18-1147,P02-1040,0,0.106323,"riptions generated for the MS COCO validation set. All these systems are listed in Table 1. With the exception of the two GAN-based systems (Dai et al., 2017; Shetty et al., 2017), the other systems are based on a conditioned recurrent neural network, trained using a Maximum Likelihood (MLE) objective. 2.2 Results Table 1 presents the results for the metrics discussed above. We discuss each of them in turn. Average sentence length. We observe that all models produce shorter sentences than humans, on average, perhaps also conveying less information. It also means that the BLEU brevity penalty (Papineni et al., 2002) and Meteor length penalty (Denkowski and Lavie, 2014) are affecting the metric scores. However, producing shorter sentences does not necessarily mean producing worse descriptions. Standard deviation of sentence length. We observe that the GAN-based systems vary more than most other systems, but the systems by Liu et al. (2017) and Vinyals et al. (2017) have more variation than other MLE-based systems. Humans vary much more than any model in the length of their descriptions. Number of types. The model by Liu et al. (2017) produces the fewest distinct word types (598), which severely limits the"
C18-1147,L16-1268,1,0.893335,"Missing"
C18-1147,W00-0901,0,0.0892584,"n ratio corresponds to the number of prepositional phrases per description. Types-1 refers to the number of PP types of depth 1. 1737 5 5.1 Discussion and Future Research Other metrics In addition to the metrics proposed in this paper, there are other options that could be explored in future work. For reasons of space, we were also not able to cover metrics based on the frequency distribution of words in the training and validation data. We already mentioned Shetty et al.’s (2017) use of frequency ratios in the introduction. Their approach could be extended (perhaps also using log-likelihood (Rayson and Garside, 2000)) to produce a ranking of words that are over- or underused by a particular system. Overused words could be further analyzed by computing a ‘local precision’ metric, measuring how often a generated word is also used in at least one reference description. Ferraro et al. (2015) present other metrics in their survey of datasets for vision and language research, including: Yngve and Frazier measurements of syntactic complexity (Yngve, 1960; Frazier, 1985). Ferraro et al. (2015) found that the MS COCO and Flickr30K datasets have the most complex sentences, compared to other vision & language datase"
C18-1147,W16-3207,1,0.779117,"Missing"
D13-1128,P12-1038,0,0.784503,"utperform approaches that rely on object proximity or corpus information to generate descriptions on both automatic measures and on human judgements. 1 Previous approaches to automatic description generation have typically tackled the problem using an object recognition system in conjunction with a natural language generation component based on language models or templates (Kulkarni et al., 2011; Li et al., 2011). Some approaches have utilised the visual attributes of objects (Farhadi et al., 2010), generated descriptions by retrieving the descriptions of similar images (Ordonez et al., 2011; Kuznetsova et al., 2012), relied on an external corpus to predict the relationships between objects (Yang et al., 2011), or combined sentence fragments using a treesubstitution grammar (Mitchell et al., 2012). Introduction Humans are readily able to produce a description of an image that correctly identifies the objects and actions depicted. Automating this process is useful for applications such as image retrieval, where users can go beyond keyword-search to describe their information needs, caption generation for improving the accessibility of existing image collections, story illustration, and in assistive technol"
D13-1128,W11-0326,0,0.328655,"gold-standard descriptions. We describe two template-based description generation models that operate over visual dependency representations. In an image description task, we find that these models outperform approaches that rely on object proximity or corpus information to generate descriptions on both automatic measures and on human judgements. 1 Previous approaches to automatic description generation have typically tackled the problem using an object recognition system in conjunction with a natural language generation component based on language models or templates (Kulkarni et al., 2011; Li et al., 2011). Some approaches have utilised the visual attributes of objects (Farhadi et al., 2010), generated descriptions by retrieving the descriptions of similar images (Ordonez et al., 2011; Kuznetsova et al., 2012), relied on an external corpus to predict the relationships between objects (Yang et al., 2011), or combined sentence fragments using a treesubstitution grammar (Mitchell et al., 2012). Introduction Humans are readily able to produce a description of an image that correctly identifies the objects and actions depicted. Automating this process is useful for applications such as image retriev"
D13-1128,P05-1012,0,0.0482571,"ike down the road. det aux det nsubj dobj det pobj advmod root (c) Figure 1: (a) Image with regions marked up: BIKE, CAR, MAN , ROAD , TREES ; (b) human-generated image description; (c) visual dependency representation expressing the relationships between MAN, BIKE, and ROAD aligned to the syntactic dependency parse of the first sentence in the human-generated description (b). Finally, we also show that the benefit of the visual dependency representation is maintained when image descriptions are generated from automatically parsed VDRs. We use a modified version of the edge-factored parser of McDonald et al. (2005) to predict VDRs over a set of annotated object regions. This result reaffirms the potential utility of this representation as a means to describe events in images. Note that throughout the paper, we work with goldstandard region annotations; this makes it possible to explore the effect of structured image representations independently of automatic object detection. 2 sentations would be able to correctly infer the action that is taking place, such as the distinction between repairing or riding a bike, which would greatly improve the descriptions it is able to generate. In this paper, we intro"
D13-1128,E12-1076,0,0.809976,"escription generation have typically tackled the problem using an object recognition system in conjunction with a natural language generation component based on language models or templates (Kulkarni et al., 2011; Li et al., 2011). Some approaches have utilised the visual attributes of objects (Farhadi et al., 2010), generated descriptions by retrieving the descriptions of similar images (Ordonez et al., 2011; Kuznetsova et al., 2012), relied on an external corpus to predict the relationships between objects (Yang et al., 2011), or combined sentence fragments using a treesubstitution grammar (Mitchell et al., 2012). Introduction Humans are readily able to produce a description of an image that correctly identifies the objects and actions depicted. Automating this process is useful for applications such as image retrieval, where users can go beyond keyword-search to describe their information needs, caption generation for improving the accessibility of existing image collections, story illustration, and in assistive technology for blind and A common aspect of existing work is that an image is represented as a bag of image regions. Bags of regions encode which objects co-occur in an image, but they are un"
D13-1128,W12-3018,0,0.0128969,"Missing"
D13-1128,D11-1041,0,\N,Missing
D13-1128,P04-1077,0,\N,Missing
D18-1329,W17-4746,0,0.219492,"Missing"
D18-1329,W14-3348,0,0.246149,"language word embeddings are modulated by an element-wise multiplication with a learned transformation of the visual data (Caglayan et al., 2017). The image awareness of a model M for a single instance aM (xi , yi , vi , v¯i ) is given by: E(xi , yi , v¯i ) (4) Systems Evaluation i (2) Model-external awareness ∆E aM = ∆E = T (xi , yi , vi ) − T (xi , yi , v¯i ) 1 X aM (xi , yi , vi , v¯i ) (1) |D| aM (xi , yi , vi , v¯i ) = E(xi , yi , vi ) − (3) A model-external awareness measure could be a text-similarity evaluation or human judgement. In this paper, we use the Meteor text-similarity score (Denkowski and Lavie, 2014) because it naturally decomposes to the sentence level, and it is already the de-facto evaluation metric for multimodal machine translation (Specia et al., 2016). Let E be any text-similarity scoring function T that decomposes to the sentence level. The difference in performance for a single instance is defined as: |D| ∆-Awareness = Model-internal awareness ∆I 1 This assumes that a higher score means better performance for the performance measure E. Swap the order of the operands if lower performance means better. 2975 I ∆E -Awareness 0.8 trgmul 57.3 57.3 ± 0.2 -0.001 ± 0.002 0.6 decinit 57.0"
D18-1329,W16-3210,1,0.855713,"Missing"
D18-1329,P16-1227,0,0.0729342,"Missing"
D18-1329,D17-1215,0,0.0240927,"8 Conference on Empirical Methods in Natural Language Processing, pages 2974–2978 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics to the foiled image captions evaluation, in which the performance of an image captioning system is measured when a single word is replaced with an incorrect, but similar word (Shekhar et al., 2017); the main difference is that we replace the visual data instead of manipulating the text. Our work is also related to a study of question-answering systems, in which additional text was appended to the end of a document (Jia and Liang, 2017). They found that these additional text segments distracted QA systems from producing the correct answer. In contrast, our evaluation does not manipulate the textual data, instead we replace the original visual input with a random distractor. We evaluate three publicly available multimodal translation systems with our adversarial evaluation. The main finding of this paper is that one publicly available multimodal translation system is not aware of the congruent image data. This finding raises doubts about whether state-of-theart multimodal translation systems actually use the visual context to"
D18-1329,P17-2031,0,0.281509,"Missing"
D18-1329,W17-4751,0,0.0292725,"6) needed at least one post-edit to reflect the joint meaning of the visual and linguistic context. However, the evidence that visual context helps computational models is less clear. Consider the three teams that submitted contrastive multimodal and text-only variants of their systems to the 2017 Multimodal Translation Shared Task (Elliott et al., 2017): the University of Le Mans’ multimodal system outperformed their text-only variant (Caglayan et al., ∗ Work carried out at the University of Edinburgh. 2017); the Oregon State University text-only system outperformed their multimodal variant (Ma et al., 2017); and the performance of the Charles University systems depended on the language pair (Libovick´y and Helcl, 2017). In light of these results, we need a better understanding of the role of visual context in multimodal translation systems. We propose an adversarial evaluation Method to determine whether multimodal translation systems are aware of the visual context. We introduce a measure of image awareness to quantify the difference in performance in two settings: (i) when a system is presented with congruent visual data; (ii) when it is presented with incongruent visual data. In both settings"
D18-1329,P17-1024,0,0.0298844,"he visual context, i.e. it is actually using the image for translation, then the system will perform better when it is presented with the congruent visual data than incongruent visual data. Our evaluation is related 2974 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2974–2978 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics to the foiled image captions evaluation, in which the performance of an image captioning system is measured when a single word is replaced with an incorrect, but similar word (Shekhar et al., 2017); the main difference is that we replace the visual data instead of manipulating the text. Our work is also related to a study of question-answering systems, in which additional text was appended to the end of a document (Jia and Liang, 2017). They found that these additional text segments distracted QA systems from producing the correct answer. In contrast, our evaluation does not manipulate the textual data, instead we replace the original visual input with a random distractor. We evaluate three publicly available multimodal translation systems with our adversarial evaluation. The main findi"
D18-1329,W16-2346,1,0.899937,"Missing"
D19-1662,D11-1120,0,0.0547334,"), with 5 a neutral middle value. The mean for sentences by male authors selected without adversarial training is 4.68 and for females 5.69. With adversarial training, this mean is 4.83. The mean of sentences extracted from the model trained with adversarial training is closest to the neutral value 5, but independent ttests show that the differences between all classes are insignificant (p > 0.05). Therefore, the results show that humans had difficulties determining the gender of the author. This is contrary to the findings of Flekova et al. (2016), but is similar by the unaveraged results of Burger et al. (2011). This indicates that the data did not exhibit (m)any obvious predictors of gender or age. In addition to this study, we also use this data to visualize what our diagnostic classifiers focus on. For this purpose, we use the models from Section 2, trained on the full PAN 16 T WIT dataset, and perform feature analysis using uptraining: Us6332 AGE (a) G ENDER −A DV (b) G ENDER +A DV G ENDER T EST SET -A DV +A DV -A DV +A DV PAN 16 T WIT 67.86* 53.45* 56.79* 53.74* PAN 16 R AND 50.13 50.32 49.50 50.00 100 AUTH T WIT – CF T WITTER – – – 50.92 51.63* 52.57 50.74* B LOGS R EVIEWS SOME 55.12* 50.26 50"
D19-1662,D18-1002,1,0.376033,"Elazar2 , Desmond Elliott1 , Anders Søgaard1 1 University of Copenhagen, 2 Bar-Ilan University {mjb,yova,de,soegaard}@di.ku.dk,yanaiela@gmail.com Abstract source languages in multi-lingual machine translation (Xie et al., 2017). Elazar and Goldberg (2018) argue, however, that adversarial learning does not fully remove sensitive demographic traits from the data representations. This conclusion is based on the observation that a diagnostic classifier trained over the supposedly debiased data representations could still predict gender, age and race above chance level in their experimental setup. Elazar and Goldberg (2018) showed that protected attributes can be extracted from the representations of a debiased neural network for mention detection at above-chance levels, by evaluating a diagnostic classifier on a heldout subsample of the data it was trained on. We revisit their experiments and conduct a series of follow-up experiments showing that, in fact, the diagnostic classifier generalizes poorly to both new in-domain samples and new domains, indicating that it relies on correlations specific to their particular data sample. We further show that a diagnostic classifier trained on the biased baseline neural"
D19-1662,P16-1080,0,0.0569942,"Missing"
D19-1662,P18-2005,0,0.0336493,"as in our models. 1 Introduction Several approaches have been proposed to learn classifiers that are invariant (unbiased with respect) to protected attributes: cost-sensitive (Agarwal et al., 2018), regularization-based (Bechavod and Ligett, 2017), and adversarial (Ganin and Lempitsky, 2015). In the adversarial approach, a model learns representations x that should be predictive for a main task y and oblivious to a protected attribute z. Adversarial training has been used to learn data representations that are invariant to demographic attributes (Raff and Sylvester, 2018; Beutel et al., 2017; Li et al., 2018), as well as representations invariant to domain differences (Ganin and Lempitsky, 2015), clean or noisy speech (Sriram et al., 2018), and invariant to differences between In general, diagnostic classifiers are trained on data representations to predict the protected demographic attributes in question as well as possible, i.e., the classifier picks up on any correlations, strong or weak, between data representations and the demographic classes. Correlations come in different flavours: P REVALENT: Certain features are indicative of gender in most contexts, e.g., the distribution of phrases like"
D19-1662,W19-4825,0,0.0272155,"), but we report results on exactly the same development split as well as on the new held-out test split. PAN 16 T WIT is balanced using undersampling with respect to main task and demographic attribute (gender and age respectively) which is why there are separate datasets for G ENDER and AGE. Our main observation here is that training, development and test splits and random subsamples of one sample of data. Using random subsamples this way is common in machine learning, including bias detection studies (Elazar and Goldberg, 2018; Zhao et al., 2019) and probing studies (Ravfogel et al., 2018; Lin et al., 2019), but is known to overestimate performance (Globerson and Roweis, 2016), in particular for highdimensional problems. Replication We start by replicating the experiment of Elazar and Goldberg (2018) using their code on PAN 16 T WIT with their data splits. The main task is predicting the mentions of other Twitter users after removing all user names in the tweets. The protected demographic attributes are age and gender, both with binary targets. Our development results (main and diagnostic classifier) which are comparable to Elazar and Goldberg (2018) are reported in Table 6 in the Appendix; test"
D19-1662,W18-5412,0,0.0189565,"azar and Goldberg (2018), but we report results on exactly the same development split as well as on the new held-out test split. PAN 16 T WIT is balanced using undersampling with respect to main task and demographic attribute (gender and age respectively) which is why there are separate datasets for G ENDER and AGE. Our main observation here is that training, development and test splits and random subsamples of one sample of data. Using random subsamples this way is common in machine learning, including bias detection studies (Elazar and Goldberg, 2018; Zhao et al., 2019) and probing studies (Ravfogel et al., 2018; Lin et al., 2019), but is known to overestimate performance (Globerson and Roweis, 2016), in particular for highdimensional problems. Replication We start by replicating the experiment of Elazar and Goldberg (2018) using their code on PAN 16 T WIT with their data splits. The main task is predicting the mentions of other Twitter users after removing all user names in the tweets. The protected demographic attributes are age and gender, both with binary targets. Our development results (main and diagnostic classifier) which are comparable to Elazar and Goldberg (2018) are reported in Table 6 in"
D19-1662,N19-1064,0,0.0305459,"wer sentences in our train split than Elazar and Goldberg (2018), but we report results on exactly the same development split as well as on the new held-out test split. PAN 16 T WIT is balanced using undersampling with respect to main task and demographic attribute (gender and age respectively) which is why there are separate datasets for G ENDER and AGE. Our main observation here is that training, development and test splits and random subsamples of one sample of data. Using random subsamples this way is common in machine learning, including bias detection studies (Elazar and Goldberg, 2018; Zhao et al., 2019) and probing studies (Ravfogel et al., 2018; Lin et al., 2019), but is known to overestimate performance (Globerson and Roweis, 2016), in particular for highdimensional problems. Replication We start by replicating the experiment of Elazar and Goldberg (2018) using their code on PAN 16 T WIT with their data splits. The main task is predicting the mentions of other Twitter users after removing all user names in the tweets. The protected demographic attributes are age and gender, both with binary targets. Our development results (main and diagnostic classifier) which are comparable to Elazar and"
D19-6406,D18-1316,0,0.0554931,"Missing"
D19-6406,D16-1025,0,0.0198685,"o et al., 2014), as implemented in nmtpytorch (Caglayan et al., 2017b). Visual Adversaries: Visual concepts and their relationships with the text are expected to provide rich supervision to multimodal translation systems. In addition to evaluating the robustness of these systems to textual adversaries, we also determine the interplay with visual adversaries. We pair each caption with a randomly sampled image from the test data to break the alignment between learned word semantics and visual concepts. 3 TTR has previously been used to estimate the quality of machine translation system outputs (Bentivogli et al., 2016). 37 Congruent Incongruent 0.1659 0.1703 0.1399 0.1655 0.1692 0.1352 decinit trgmul hierattn ther the sentence or the image. In N UM, pluralizing “A” to “Two” causes the model to generate an unknown word5 “Japan” instead of “Halloween”. The translation model is likely to have good representations of “A” and “two” because these words occur frequently in the training data, but it fails to distinguish between singulars and plurals, resulting in an incorrect translation. In P REP, swapping “in” for “up” causes the model to make an incorrect lexical choice “fische” (“fish”) instead of “waterfall”,"
D19-6406,W16-3203,0,0.0192158,"re θ > 0.6. We use WordNet (Miller, 1998) heuristic hypernymy rules to replace noun heads with terms that are semantically different. saries, they are substantially affected by the textual adversaries. 2 Generating Textual Adversaries (2) We define visual term as a word or phrase that can be expected to be clearly illustrated in an image. In our experiments, we evaluate the performance of multimodal translation systems by modifying a visual term in a sentence to create a textual adversary. We create four types of adversarial samples following the methodology introduced in Young et al. (2014); Hodosh and Hockenmaier (2016); Shi et al. (2018) 1 The adversaries are constructed from syntactic analyses of the sentences using POS tagging, chunking, and dependency parses from the SpaCy toolkit (Honnibal and Johnson, 2015). Figure 1 presents an overview and examples of each type of adversary. Replace Numeral (Num): Our simplest adversary is to replace the numeral in a sentence with a different quantity. We detect the tokens in a sentence that represent numbers (based on their part-of-speech tags) and replace them with alternative numerals. In addition, we treat the indefinite articles “a” and “an” as the numeral “one”"
D19-6406,W17-4746,0,0.035473,"Missing"
D19-6406,D15-1162,0,0.0139251,"s. 2 Generating Textual Adversaries (2) We define visual term as a word or phrase that can be expected to be clearly illustrated in an image. In our experiments, we evaluate the performance of multimodal translation systems by modifying a visual term in a sentence to create a textual adversary. We create four types of adversarial samples following the methodology introduced in Young et al. (2014); Hodosh and Hockenmaier (2016); Shi et al. (2018) 1 The adversaries are constructed from syntactic analyses of the sentences using POS tagging, chunking, and dependency parses from the SpaCy toolkit (Honnibal and Johnson, 2015). Figure 1 presents an overview and examples of each type of adversary. Replace Numeral (Num): Our simplest adversary is to replace the numeral in a sentence with a different quantity. We detect the tokens in a sentence that represent numbers (based on their part-of-speech tags) and replace them with alternative numerals. In addition, we treat the indefinite articles “a” and “an” as the numeral “one” because they are typically used as numerals in image captions. Furthermore, subsequent noun phrase chunks are either singularized or pluralized accordingly. We expect that this will have a small e"
D19-6406,D18-1150,0,0.0507646,"Missing"
D19-6406,N19-1422,0,0.0447215,"f visual and textual inputs produce better translations than systems trained using only textual inputs (Specia et al., 2016; Elliott et al., 2017). However, these claims have been the subject of debate in the literature: Elliott (2018) argued that the additional visual input is not necessarily used by demonstrating that the performance of a system did not change when it was evaluated with randomly selected images, and Gr¨onroos et al. (2018) observed that their models were insensitive to being evaluated with an “averaged” visual vector, as opposed to the expected visual vector. More recently, Caglayan et al. (2019) presented experiments in which the colour and entity tokens (e.g. 35 Proceedings of the Beyond Vision and LANguage: inTEgrating Real-world kNowledge (LANTERN), pages 35–40 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics Type Original Adversarial Num Noun NP Prep Two people walking on the beach. Two people walking on the beach. Two people walking on the beach. Two people walking on the beach. Four people walking on the beach. Two people walking on the grass. The beach walking on two people. Two people walking through the beach. Figure 1: Examples of adversa"
D19-6406,P17-2031,0,0.0485164,"Missing"
D19-6406,C18-1315,0,0.0157983,"ler, 1998) heuristic hypernymy rules to replace noun heads with terms that are semantically different. saries, they are substantially affected by the textual adversaries. 2 Generating Textual Adversaries (2) We define visual term as a word or phrase that can be expected to be clearly illustrated in an image. In our experiments, we evaluate the performance of multimodal translation systems by modifying a visual term in a sentence to create a textual adversary. We create four types of adversarial samples following the methodology introduced in Young et al. (2014); Hodosh and Hockenmaier (2016); Shi et al. (2018) 1 The adversaries are constructed from syntactic analyses of the sentences using POS tagging, chunking, and dependency parses from the SpaCy toolkit (Honnibal and Johnson, 2015). Figure 1 presents an overview and examples of each type of adversary. Replace Numeral (Num): Our simplest adversary is to replace the numeral in a sentence with a different quantity. We detect the tokens in a sentence that represent numbers (based on their part-of-speech tags) and replace them with alternative numerals. In addition, we treat the indefinite articles “a” and “an” as the numeral “one” because they are t"
D19-6406,W14-3348,0,0.0187678,"-5.4 Text-only 51.5 – -14.6 -10.4 -10.5 -6.3 Table 1: The differences in Corpus-level Meteor scores for the English–German Multi30K Test 2017 data for the different adversaries compared to the systems evaluated on the Original text and images. Visual: evaluation on the correct text but adversarial images. Textual: evaluation on the four different textual adversaries and the correct images. Text-only: performance of a text-only translation model with adversarial sentences. 3.1 2017 split (Elliott et al., 2017). The predicted translations are evaluated against human references using Meteor 1.5 (Denkowski and Lavie, 2014). The translations of the sentences with textual adversaries are evaluated against the gold standard, and not what the model should predict, given the adversarial input. Results In Table 1 we present the corpus-level Meteor scores for the text-only and multimodal systems when evaluated on the original data and the difference in performance when evaluating these models using the different adversaries. For visual adversaries, we confirm previously reported results of no substantial performance losses for the translations generated by the trgmul and decinit systems with visual features from unrel"
D19-6406,W16-2346,1,0.90671,"Missing"
D19-6406,D18-1329,1,0.892934,"o probe the visual awareness of these models by exposing them to randomly selected images. Our results show that although these systems are not greatly affected by the visual adverThere has been a surge of interest in tackling machine translation problems using additional information, such as a image or video context. It has been claimed that systems trained on a combination of visual and textual inputs produce better translations than systems trained using only textual inputs (Specia et al., 2016; Elliott et al., 2017). However, these claims have been the subject of debate in the literature: Elliott (2018) argued that the additional visual input is not necessarily used by demonstrating that the performance of a system did not change when it was evaluated with randomly selected images, and Gr¨onroos et al. (2018) observed that their models were insensitive to being evaluated with an “averaged” visual vector, as opposed to the expected visual vector. More recently, Caglayan et al. (2019) presented experiments in which the colour and entity tokens (e.g. 35 Proceedings of the Beyond Vision and LANguage: inTEgrating Real-world kNowledge (LANTERN), pages 35–40 c Hong Kong, China, November 3, 2019. 20"
D19-6406,D11-1063,0,0.0201587,"tt (2018) in order to make the evaluation of textual adversaries comparable to that of visual adversaries. Each system in this analysis is trained on the 29,000 English-German-image triplets in the translation data in the Multi30K dataset (Elliott et al., 2016). The analysis is performed on the Multi30K Test 1 The code to recreate these textual adversaries or new adversaries is available at https://github.com/ koeldc/Textual-adversaries-generation 2 The degree of concreteness in a word’s context is correlated with the likelihood that the word is used in a literal sense and not metaphorically (Turney et al., 2011). 36 Textual Original Visual Num Noun NP Prep decinit 51.5 +0.4 -14.0 -11.1 -11.0 -5.7 trgmul 52.1 +0.2 -14.8 -11.2 -11.2 -5.8 hierattn 48.2 -2.0 -13.2 -9.4 -11.2 -5.4 Text-only 51.5 – -14.6 -10.4 -10.5 -6.3 Table 1: The differences in Corpus-level Meteor scores for the English–German Multi30K Test 2017 data for the different adversaries compared to the systems evaluated on the Original text and images. Visual: evaluation on the correct text but adversarial images. Textual: evaluation on the four different textual adversaries and the correct images. Text-only: performance of a text-only transl"
D19-6406,W17-4718,1,0.907022,"ion 2. We evaluate the effect of these adversaries on three state-of-the-art systems, and we also probe the visual awareness of these models by exposing them to randomly selected images. Our results show that although these systems are not greatly affected by the visual adverThere has been a surge of interest in tackling machine translation problems using additional information, such as a image or video context. It has been claimed that systems trained on a combination of visual and textual inputs produce better translations than systems trained using only textual inputs (Specia et al., 2016; Elliott et al., 2017). However, these claims have been the subject of debate in the literature: Elliott (2018) argued that the additional visual input is not necessarily used by demonstrating that the performance of a system did not change when it was evaluated with randomly selected images, and Gr¨onroos et al. (2018) observed that their models were insensitive to being evaluated with an “averaged” visual vector, as opposed to the expected visual vector. More recently, Caglayan et al. (2019) presented experiments in which the colour and entity tokens (e.g. 35 Proceedings of the Beyond Vision and LANguage: inTEgra"
D19-6406,vilar-etal-2006-error,0,0.0555899,"ces that share some aspects of the correct caption. Our evaluation offers new insights on the limitations of these systems. The results indicate that the systems are primarily performing text-based translations, which is supported by the observation that the visual adversaries do not harm the systems as much as their textual counterparts. However, the textual adversaries sometimes resulted in ungrammatical sentences, which may be addressed by adopt4 The higher perplexities for the adversarial samples were, in part, due to incorrect grammatical conjugations. 5 38 We use the error taxonomy from Vilar et al. (2006). Original: Baseline: N UM: NMT: MMT: Reference : A group of young people dressed up for Halloween. Eine Gruppe junger Menschen verkleidet. Two groups of young people dressed up for halloween. Zwei Frauen vor einem Glasgeb¨aude. Zwei Gruppen von jungen Menschen in Japan. Eine Gruppe junger Leute verkleidet sich f¨ur Halloween. Original: Baseline: N OUN: NMT: MMT: Reference: A man paddles an inflatable canoe. Ein Mann paddelt in einem aufblasbaren Kanu. A city paddles an inflatable canoe. Ein Bew¨olkter kissen u¨ ber die Absperrung. Eine Stadt paddelt in einem aufblasbaren Kanu. Ein Mann paddel"
D19-6406,W16-3210,1,0.909651,"Missing"
D19-6406,Q14-1006,0,0.0161758,"sarial example. measure θ > 0.6. We use WordNet (Miller, 1998) heuristic hypernymy rules to replace noun heads with terms that are semantically different. saries, they are substantially affected by the textual adversaries. 2 Generating Textual Adversaries (2) We define visual term as a word or phrase that can be expected to be clearly illustrated in an image. In our experiments, we evaluate the performance of multimodal translation systems by modifying a visual term in a sentence to create a textual adversary. We create four types of adversarial samples following the methodology introduced in Young et al. (2014); Hodosh and Hockenmaier (2016); Shi et al. (2018) 1 The adversaries are constructed from syntactic analyses of the sentences using POS tagging, chunking, and dependency parses from the SpaCy toolkit (Honnibal and Johnson, 2015). Figure 1 presents an overview and examples of each type of adversary. Replace Numeral (Num): Our simplest adversary is to replace the numeral in a sentence with a different quantity. We detect the tokens in a sentence that represent numbers (based on their part-of-speech tags) and replace them with alternative numerals. In addition, we treat the indefinite articles “a"
I17-1014,W14-3348,0,0.00795483,"ws Commentary: external parallel text En De 240K 8.31M 8.95M 17K – – Table 1: The datasets used in our experiments. sentence with parameters Wvis : v ˆ = tanh(Wvis · N 1 X hi ) N 5 (14) We evaluate our multitasking approach with inand out-of-domain resources. We start by reporting results of models trained using only the Multi30K dataset. We also report the results of training the IMAGINET decoder with the COCO dataset. Finally, we report results on incorporating the external News Commentary parallel text into our model. Throughout, we report performance of the En→De translation using Meteor (Denkowski and Lavie, 2014) and BLEU (Papineni et al., 2002) against lowercased tokenized references. i This decoder is trained to predict the true image vector v with a margin-based objective, parameterised by the minimum margin α, and the cosine distance d(·, ·). A margin-based objective has previously been used in grounded representation learning (Vendrov et al., 2016; Chrupała et al., 2017). The contrastive examples v0 are drawn from the other instances in a minibatch: X JM AR (θ, φt ) = max{0, α − d(ˆ v, v) (15) v0 6=v + d(ˆ v, v0 )} 4 Experiments 5.1 Hyperparameters The encoder is a 1000D Gated Recurrent Unit bidi"
I17-1014,E17-2026,0,0.020187,"ranking loss between both pairs of examples. In this paper, we focus on enriching source language representations with visual information instead of zeroresource learning. 8 Multitask Learning improves the generalisability of a model by requiring it to be useful for more than one task (Caruana, 1997). This approach has recently been used to improve the performance of sentence compression using eye gaze as an auxiliary task (Klerke et al., 2016), and to improve shallow parsing accuracy through the auxiliary task of predicting keystrokes in an out-of-domain corpus (Plank, 2016). More recently, Bingel and Søgaard (2017) analysed the beneficial relationships between primary and auxiliary sequential prediction tasks. In the translation literature, multitask learning has been used to learn a one-to-many languages translation model (Dong et al., 2015), a multi-lingual translation model with a single attention mechanism shared across multiple languages (Firat et al., 2016), and in multitask sequence-tosequence learning without an attention-based decoder (Luong et al., 2016). We explore the benefits of grounded learning in the specific case of multimodal translation. We combine sequence prediction with continuous"
I17-1014,P15-1166,0,0.0321693,"Missing"
I17-1014,W16-2359,1,0.757918,"ns made by our model. Meteor Median Rank Inception-V3 56.0 ± 0.1 11.0 ± 0.0 Resnet-50 54.7 ± 0.4 11.7 ± 0.5 VGG-19 53.6 ± 1.8 13.0 ± 0.0 et al., 2016), the decoder (Libovick´y et al., 2016), or as features in a phrase-based translation model (Shah et al., 2016; Hitschler et al., 2016). Spatially-preserving image features are extracted from deeper inside a CNN, where the position of a feature is related to its position in the image. These features have been used in “double-attention models”, which calculate independent context vectors for the source language and a convolutional image features (Calixto et al., 2016; Caglayan et al., 2016; Calixto et al., 2017). We use an attentionbased translation model but our multitask model does not use images for translation. Table 7: The type of visual features predicted by the IMAGINET Decoder has a strong impact on the Multitask model performance. vectors (Inception-V3 and ResNet-50) compared to the 4096D vector from VGG-19). This difference is reflected in both the translation Meteor score and the Median rank of the images in the validation dataset. This is likely because it is easier to learn the parameters of the image prediction model that has fewer parameter"
I17-1014,W17-4718,1,0.798735,"Missing"
I17-1014,D17-1105,0,0.52544,"tion: 29,000 instances are reserved for training, 1,014 for development, and 1,000 for evaluation.1 The English and German sentences are preprocessed by normalising the punctuation, lowercasing and tokenizing the text using the Moses toolkit. We additionally decompound the German text using Zmorge (Sennrich and Kunz, 2014). 1 The Multi30K dataset also contains 155K independently collected descriptions in German and English. In order to make our experiments more comparable with previous work, we do not make use of this data. 133 Meteor BLEU 54.0 ± 0.6 35.5 ± 0.8 Calixto et al. (2017) 55.0 36.5 Calixto and Liu (2017) 55.1 37.3 55.8 ± 0.4 36.8 ± 0.8 Toyama et al. (2016) 56.0 36.5 Hitschler et al. (2016) 56.1 34.3 Moses 56.9 36.9 NMT Imagination Table 2: En→De translation results on the Multi30K dataset. Our Imagination model is competitive with the state of the art when it is trained on in-domain data. We report the mean and standard deviation of three random initialisations. Meteor BLEU Imagination 55.8 ± 0.4 36.8 ± 0.8 Imagination (COCO) 55.6 ± 0.5 36.4 ± 1.2 Table 3: Translation results when using out-ofdomain described images. Our approach is still effective when the image prediction model is trained o"
I17-1014,P17-1175,0,0.253567,"Missing"
I17-1014,W16-3210,1,0.419459,"Missing"
I17-1014,N16-1101,0,0.0107046,"rmance of sentence compression using eye gaze as an auxiliary task (Klerke et al., 2016), and to improve shallow parsing accuracy through the auxiliary task of predicting keystrokes in an out-of-domain corpus (Plank, 2016). More recently, Bingel and Søgaard (2017) analysed the beneficial relationships between primary and auxiliary sequential prediction tasks. In the translation literature, multitask learning has been used to learn a one-to-many languages translation model (Dong et al., 2015), a multi-lingual translation model with a single attention mechanism shared across multiple languages (Firat et al., 2016), and in multitask sequence-tosequence learning without an attention-based decoder (Luong et al., 2016). We explore the benefits of grounded learning in the specific case of multimodal translation. We combine sequence prediction with continuous (image) vector prediction, compared to previous work which multitasks different sequence prediction tasks. Conclusion We decompose multimodal translation into two sub-problems: learning to translate and learning visually grounded representations. In a multitask learning framework, we show how these subproblems can be addressed by sharing an encoder betw"
I17-1014,D14-1179,0,0.0226509,"Missing"
I17-1014,P17-1057,0,0.00694432,"ining the IMAGINET decoder with the COCO dataset. Finally, we report results on incorporating the external News Commentary parallel text into our model. Throughout, we report performance of the En→De translation using Meteor (Denkowski and Lavie, 2014) and BLEU (Papineni et al., 2002) against lowercased tokenized references. i This decoder is trained to predict the true image vector v with a margin-based objective, parameterised by the minimum margin α, and the cosine distance d(·, ·). A margin-based objective has previously been used in grounded representation learning (Vendrov et al., 2016; Chrupała et al., 2017). The contrastive examples v0 are drawn from the other instances in a minibatch: X JM AR (θ, φt ) = max{0, α − d(ˆ v, v) (15) v0 6=v + d(ˆ v, v0 )} 4 Experiments 5.1 Hyperparameters The encoder is a 1000D Gated Recurrent Unit bidirectional recurrent neural network (Cho et al., 2014, GRU) with 620D embeddings. We share all of the encoder parameters between the primary and auxiliary task. The translation decoder is a 1000D GRU recurrent neural network, with a 2000D context vector over the encoder states, and 620D word embeddings (Sennrich et al., 2017). The Imaginet decoder is a single-layer fee"
I17-1014,C16-1124,0,0.0114573,"perimenting with different image prediction architectures; multitasking different translation languages into a single shared encoder; and multitasking in both the encoder and decoder(s). Visual representation prediction has been studied using static images or videos. Lin and Parikh (2015) use a conditional random field to imagine the composition of a clip-art scene for visual paraphrasing and fill-in-the-blank tasks. Chrupała et al. (2015) predict the image vector associated with a sentence using an L2 loss; they found this improves multi-modal word similarity compared to text-only baselines. Gelderloos and Chrupała (2016) predict the image vector associated with a sequence of phonemes using a max-margin loss, similar to our image prediction objective. Collell et al. (2017) learn to predict the visual feature vector associated with a word for word similarity and relatedness tasks. As a video reconstruction problem, Srivastava et al. (2015) propose an LSTM Autoencoder to predict video frames as a Acknowledgments We are grateful to the anonymous reviewers for their feedback. We thank Joost Bastings for sharing his multitasking Nematus model, Wilker Aziz for discussions about formulating the problem, Stella Frank"
I17-1014,P15-2019,1,0.845813,"Missing"
I17-1014,W16-2361,0,0.073676,"Missing"
I17-1014,P16-1227,0,0.199345,"evaluation.1 The English and German sentences are preprocessed by normalising the punctuation, lowercasing and tokenizing the text using the Moses toolkit. We additionally decompound the German text using Zmorge (Sennrich and Kunz, 2014). 1 The Multi30K dataset also contains 155K independently collected descriptions in German and English. In order to make our experiments more comparable with previous work, we do not make use of this data. 133 Meteor BLEU 54.0 ± 0.6 35.5 ± 0.8 Calixto et al. (2017) 55.0 36.5 Calixto and Liu (2017) 55.1 37.3 55.8 ± 0.4 36.8 ± 0.8 Toyama et al. (2016) 56.0 36.5 Hitschler et al. (2016) 56.1 34.3 Moses 56.9 36.9 NMT Imagination Table 2: En→De translation results on the Multi30K dataset. Our Imagination model is competitive with the state of the art when it is trained on in-domain data. We report the mean and standard deviation of three random initialisations. Meteor BLEU Imagination 55.8 ± 0.4 36.8 ± 0.8 Imagination (COCO) 55.6 ± 0.5 36.4 ± 1.2 Table 3: Translation results when using out-ofdomain described images. Our approach is still effective when the image prediction model is trained over the COCO dataset. Meteor BLEU NMT 52.8 ± 0.6 33.4 ± 0.6 + NC 56.7 ± 0.3 37.2 ± 0.7"
I17-1014,W16-2360,0,0.404682,"Missing"
I17-1014,P02-1040,0,0.115994,"En De 240K 8.31M 8.95M 17K – – Table 1: The datasets used in our experiments. sentence with parameters Wvis : v ˆ = tanh(Wvis · N 1 X hi ) N 5 (14) We evaluate our multitasking approach with inand out-of-domain resources. We start by reporting results of models trained using only the Multi30K dataset. We also report the results of training the IMAGINET decoder with the COCO dataset. Finally, we report results on incorporating the external News Commentary parallel text into our model. Throughout, we report performance of the En→De translation using Meteor (Denkowski and Lavie, 2014) and BLEU (Papineni et al., 2002) against lowercased tokenized references. i This decoder is trained to predict the true image vector v with a margin-based objective, parameterised by the minimum margin α, and the cosine distance d(·, ·). A margin-based objective has previously been used in grounded representation learning (Vendrov et al., 2016; Chrupała et al., 2017). The contrastive examples v0 are drawn from the other instances in a minibatch: X JM AR (θ, φt ) = max{0, α − d(ˆ v, v) (15) v0 6=v + d(ˆ v, v0 )} 4 Experiments 5.1 Hyperparameters The encoder is a 1000D Gated Recurrent Unit bidirectional recurrent neural networ"
I17-1014,P17-1117,0,0.0295713,"Missing"
I17-1014,C16-1059,0,0.0238734,"ed by minimizing a multi-task ranking loss between both pairs of examples. In this paper, we focus on enriching source language representations with visual information instead of zeroresource learning. 8 Multitask Learning improves the generalisability of a model by requiring it to be useful for more than one task (Caruana, 1997). This approach has recently been used to improve the performance of sentence compression using eye gaze as an auxiliary task (Klerke et al., 2016), and to improve shallow parsing accuracy through the auxiliary task of predicting keystrokes in an out-of-domain corpus (Plank, 2016). More recently, Bingel and Søgaard (2017) analysed the beneficial relationships between primary and auxiliary sequential prediction tasks. In the translation literature, multitask learning has been used to learn a one-to-many languages translation model (Dong et al., 2015), a multi-lingual translation model with a single attention mechanism shared across multiple languages (Firat et al., 2016), and in multitask sequence-tosequence learning without an attention-based decoder (Luong et al., 2016). We explore the benefits of grounded learning in the specific case of multimodal translation. We co"
I17-1014,D16-1043,0,0.0613892,"Missing"
I17-1014,C16-1011,0,0.020202,"e related to our work is an extension of Variational Neural Machine Translation to infer latent variables to explicitly model the semantics of source sentences from visual and linguistic information (Toyama et al., 2016). They report improvements on the Multi30K data set but their model needs additional parameters in the “neural inferrer” modules. In our model, the grounded semantics are represented implicitly in the shared encoder. They assume Source-Target-Image training data, whereas our approach achieves equally good results if we train on separate Source-Image and Source-Target datasets. Saha et al. (2016) study cross-lingual image description where the task is to generate a sentence in language L1 given the image, using only Image-L2 and L1 -L2 training corpora. They propose a Correlational EncoderDecoder to model the Image-L2 and L1 -L2 data, which learns correlated representations for paired Image-L2 data and decodes L1 from the joint representation. Similar to our work, the encoder is trained by minimizing two loss functions: the Image-L2 correlation loss, and the L1 decoding Related work Initial work on multimodal translation used semantic or spatially-preserving image features as inputs t"
I17-1014,N16-1179,0,0.00986804,"o translate from L1 to L2 with only Image-L1 and Image-L2 corpora. Their model embeds the image, L1 , and L2 in a joint multimodal space learned by minimizing a multi-task ranking loss between both pairs of examples. In this paper, we focus on enriching source language representations with visual information instead of zeroresource learning. 8 Multitask Learning improves the generalisability of a model by requiring it to be useful for more than one task (Caruana, 1997). This approach has recently been used to improve the performance of sentence compression using eye gaze as an auxiliary task (Klerke et al., 2016), and to improve shallow parsing accuracy through the auxiliary task of predicting keystrokes in an out-of-domain corpus (Plank, 2016). More recently, Bingel and Søgaard (2017) analysed the beneficial relationships between primary and auxiliary sequential prediction tasks. In the translation literature, multitask learning has been used to learn a one-to-many languages translation model (Dong et al., 2015), a multi-lingual translation model with a single attention mechanism shared across multiple languages (Firat et al., 2016), and in multitask sequence-tosequence learning without an attention-"
I17-1014,E17-3017,0,0.0107449,"Missing"
I17-1014,P07-2045,0,0.00749072,"t is set to 0.2 for the embeddings and the recurrent connections in both tasks (Gal and Ghahramani, 2016). Translations are decoded using beam search with 12 hypotheses. 5.2 Table 4: Translation results with out-of-domain parallel text and described images. We find further improvements when we multitask with the News Commentary (NC) and COCO datasets. In-domain experiments We start by presenting the results of our multitask model trained using only the Multi30K dataset. We compare against state-of-the-art approaches and text-only baselines. Moses is the phrase-based machine translation model (Koehn et al., 2007) reported in (Specia et al., 2016). NMT is a text-only neural machine translation model. Calixto et al. (2017) is a double-attention model over the source language and the image. Calixto and Liu (2017) is a multimodal translation model that conditions the decoder on semantic image vector extracted from the VGG-19 CNN. Hitschler et al. (2016) uses visual features in a target-side retrieval model for translation. Toyama et al. (2016) is most comparable to our approach: it is a multimodal variational NMT model that infers latent variables to represent the source language semantics from the image"
I17-1014,P16-1009,0,0.0623979,"Missing"
I17-1014,sennrich-kunz-2014-zmorge,0,0.0133716,"when their norm exData We evaluate our model using the benchmark Multi30K dataset (Elliott et al., 2016), which is the largest collection of images paired with sentences in multiple languages. This dataset contains 31,014 images paired with an English language sentence and a German language translation: 29,000 instances are reserved for training, 1,014 for development, and 1,000 for evaluation.1 The English and German sentences are preprocessed by normalising the punctuation, lowercasing and tokenizing the text using the Moses toolkit. We additionally decompound the German text using Zmorge (Sennrich and Kunz, 2014). 1 The Multi30K dataset also contains 155K independently collected descriptions in German and English. In order to make our experiments more comparable with previous work, we do not make use of this data. 133 Meteor BLEU 54.0 ± 0.6 35.5 ± 0.8 Calixto et al. (2017) 55.0 36.5 Calixto and Liu (2017) 55.1 37.3 55.8 ± 0.4 36.8 ± 0.8 Toyama et al. (2016) 56.0 36.5 Hitschler et al. (2016) 56.1 34.3 Moses 56.9 36.9 NMT Imagination Table 2: En→De translation results on the Multi30K dataset. Our Imagination model is competitive with the state of the art when it is trained on in-domain data. We report t"
I17-1014,W16-2363,0,0.0492985,"3 The effect of visual feature vectors We used MT-ComparEval (Klejch et al., 2015) 136 (a) Nearest neighbours for “a native woman is working on a craft project .” (b) Nearest neighbours for “there is a cafe on the street corner with an oval painting on the side of the building .” Figure 2: We can interpret the IMAGINET Decoder by visualising the predictions made by our model. Meteor Median Rank Inception-V3 56.0 ± 0.1 11.0 ± 0.0 Resnet-50 54.7 ± 0.4 11.7 ± 0.5 VGG-19 53.6 ± 1.8 13.0 ± 0.0 et al., 2016), the decoder (Libovick´y et al., 2016), or as features in a phrase-based translation model (Shah et al., 2016; Hitschler et al., 2016). Spatially-preserving image features are extracted from deeper inside a CNN, where the position of a feature is related to its position in the image. These features have been used in “double-attention models”, which calculate independent context vectors for the source language and a convolutional image features (Calixto et al., 2016; Caglayan et al., 2016; Calixto et al., 2017). We use an attentionbased translation model but our multitask model does not use images for translation. Table 7: The type of visual features predicted by the IMAGINET Decoder has a strong impa"
I17-1014,W16-2346,1,0.443346,"Missing"
I17-1014,tiedemann-2012-parallel,0,0.0960759,"domain data. In the indomain experiments, we find that multitasking translation with image prediction is competitive with the state of the art. Our model achieves 55.8 Meteor as a single model trained on multimodal in-domain data, and 57.6 Meteor as an ensemble. In the experiments with out-of-domain resources, we find that the improvement in translation quality holds when training the IMAGINET decoder on the MS COCO dataset of described images (Chen et al., 2015). Furthermore, if we significantly improve our text-only baseline using out-of-domain parallel text from the News Commentary corpus (Tiedemann, 2012), we still find improvements in translation quality from the auxiliary image prediction task. Finally, we report a state-of-the-art result of 59.3 Meteor on the Multi30K corpus when ensembling models trained on in- and out-of-domain resources. The main contributions of this paper are: 2 Problem Formulation Multimodal translation is the task of producing target language translation y, given the source language sentence x and additional context, such as an image v (Specia et al., 2016). Let x be a source language sentence consisting of N tokens: x1 , x2 , . . ., xn and let y be a target language"
I17-1014,D16-1050,0,\N,Missing
I17-1014,P16-5005,0,\N,Missing
K18-1039,S15-2045,0,0.0356114,"Missing"
K18-1039,S14-2010,0,0.0354622,"ccessfully applied to many-languages-to-one character-level machine translation (Chung et al., 2016) and multilingual dependency parsing (Ammar et al., 2016). Recently, Gella et al. (2017) proposed to learn both bilingual and multimodal sentence representations using images paired with captions independently collected in English and German. Their results show that bilingual training improves imagesentence ranking performance over a monolingual ∗ Marc-Alexandre Côté Microsoft Research Montreal macote@microsoft.com baseline, and it improves performance on semantic textual similarity benchmarks (Agirre et al., 2014, 2015). These findings suggest that it may be beneficial to consider another language as another modality in a monolingual grounded language learning model. In the grounded learning scenario, descriptions of an image in multiple languages can be considered as multiple views of the same or closely related data. These additional views can help overcome the problems of data sparsity, and have practical implications for efficiently collecting imagetext datasets in different languages. In real-life applications, many tasks and domains can involve code switching (Barman et al., 2014), which is easi"
K18-1039,Q16-1031,0,0.152459,"epresentation (Barsalou et al., 2003). It has been shown that visually grounded word and sentence-representations (Kiela et al., 2014; Baroni, 2016; Elliott and Kádár, 2017; Kiela et al., 2017; Yoo et al., 2017) improve performance on the downstream tasks of paraphrase identification, semantic entailment, and multimodal machine translation (Dolan et al., 2004; Marelli et al., 2014; Specia et al., 2016). Multilingual sentence representations have also been successfully applied to many-languages-to-one character-level machine translation (Chung et al., 2016) and multilingual dependency parsing (Ammar et al., 2016). Recently, Gella et al. (2017) proposed to learn both bilingual and multimodal sentence representations using images paired with captions independently collected in English and German. Their results show that bilingual training improves imagesentence ranking performance over a monolingual ∗ Marc-Alexandre Côté Microsoft Research Montreal macote@microsoft.com baseline, and it improves performance on semantic textual similarity benchmarks (Agirre et al., 2014, 2015). These findings suggest that it may be beneficial to consider another language as another modality in a monolingual grounded langu"
K18-1039,P16-1160,0,0.0242586,"f perceptual grounding in human concept acquisition and representation (Barsalou et al., 2003). It has been shown that visually grounded word and sentence-representations (Kiela et al., 2014; Baroni, 2016; Elliott and Kádár, 2017; Kiela et al., 2017; Yoo et al., 2017) improve performance on the downstream tasks of paraphrase identification, semantic entailment, and multimodal machine translation (Dolan et al., 2004; Marelli et al., 2014; Specia et al., 2016). Multilingual sentence representations have also been successfully applied to many-languages-to-one character-level machine translation (Chung et al., 2016) and multilingual dependency parsing (Ammar et al., 2016). Recently, Gella et al. (2017) proposed to learn both bilingual and multimodal sentence representations using images paired with captions independently collected in English and German. Their results show that bilingual training improves imagesentence ranking performance over a monolingual ∗ Marc-Alexandre Côté Microsoft Research Montreal macote@microsoft.com baseline, and it improves performance on semantic textual similarity benchmarks (Agirre et al., 2014, 2015). These findings suggest that it may be beneficial to consider another lan"
K18-1039,C04-1051,0,0.116917,"et of images in multiple language enables further improvements via an additional caption-caption ranking objective. 1 Introduction Multimodal representation learning is largely motivated by evidence of perceptual grounding in human concept acquisition and representation (Barsalou et al., 2003). It has been shown that visually grounded word and sentence-representations (Kiela et al., 2014; Baroni, 2016; Elliott and Kádár, 2017; Kiela et al., 2017; Yoo et al., 2017) improve performance on the downstream tasks of paraphrase identification, semantic entailment, and multimodal machine translation (Dolan et al., 2004; Marelli et al., 2014; Specia et al., 2016). Multilingual sentence representations have also been successfully applied to many-languages-to-one character-level machine translation (Chung et al., 2016) and multilingual dependency parsing (Ammar et al., 2016). Recently, Gella et al. (2017) proposed to learn both bilingual and multimodal sentence representations using images paired with captions independently collected in English and German. Their results show that bilingual training improves imagesentence ranking performance over a monolingual ∗ Marc-Alexandre Côté Microsoft Research Montreal m"
K18-1039,W17-4718,1,0.89966,"Missing"
K18-1039,W16-3210,1,0.832014,"Missing"
K18-1039,D16-1044,0,0.0473835,"age–caption pairs further from each other, in a joint embedding space. while not stopping criterion do T ∼ Bern(p) if T = 1 then Dn ∼ Dc2i &lt; c, i >∼ Dn a ← φ(c, θφ ) b ← ψ(i, θψ ) else &lt; ca , cb >∼ Dc2c a ← φ(ca , θφ ) b ← φ(cb , θφ ) end if [θφ ; θψ ] ← SGD(∇[θφ ;θψ ] J (a, b)) end while In addition to learning grounded representations for image-sentence ranking, joint vision and language systems have been proposed to solve a wide range of tasks across modalities such as image captioning (Mao et al., 2014; Vinyals et al., 2015; Xu et al., 2015), visual question answering (Antol et al., 2015; Fukui et al., 2016; Jabri et al., 2016), text-toimage synthesis (Reed et al., 2016) and multimodal machine translation (Libovicky and Helcl, 2017; Elliott and Kádár, 2017). Our work is also closely related to multilingual joint representation learning. In this scenario, a single model is trained to solve a task across multiple languages. Ammar et al. (2016) train a multilingual dependency parser on the Universal Dependencies treebank (Nivre et al., 2015) and show that on average the single multilingual model outperforms the monolingual baselines. Johnson et al. (2016) present a zero-shot neural machine translat"
K18-1039,D15-1070,0,0.0234017,"ble pair Figure 1: An example taken from the Translation and Comparable portions of the Multi30K dataset. The translation portion (a) contains professional translations of the English captions into German, French, and Czech. The comparable portion (b) consists of five independently crowdsourced English and German descriptions, given only the image. Note that the sentences in (b) convey different information from the English–German translation pair in (a). captions are collected in different languages. Such disjoint settings have been explored in pivot-based multimodal representation learning (Funaki and Nakayama, 2015; Rajendran et al., 2015) or zero-shot multi-modal machine translation (Nakayama and Nishida, 2017). We compare translated vs. independently collected captions in Sections 5.2 and 6.1, and overlapping vs. disjoint images in Section 5.3. High-to-low resource transfer: In Section 6.2 we investigate whether low-resource languages benefit from jointly training on larger data sets from higher-resource languages. This type of transfer has previously been shown to be effective in machine translation (e.g., Zoph et al., 2016). Training objective: In addition to learning to map images to sentences, we"
K18-1039,D14-1005,0,0.0253874,"et al., 2001) or SimLex999 (Hill et al., 2015) compared to uni-modal representations (Kádár et al., 1 Gloss: Three men and two women with a South-East Asian appearance eat out of bowls at a black table, on which there are, among other things, paper cups and a bag; in the background there are other people and tables. 403 Require: p: task switching probability. Dc2i : datasets D1 . . . Dk of image-caption pairs &lt; c, i > for all k languages. Dc2c : data set of all possible caption pairs &lt; ca , cb > for all k languages. φ(c, θφ ): caption encoder ψ(i, θφ ): image encoder 2015; Bruni et al., 2014; Kiela and Bottou, 2014). Grounded representations of sentences that are learned from image–caption data sets also improve performance on a number of sentence-level tasks (Kiela et al., 2017; Yoo et al., 2017) when used as additional features to skip-thought vectors (Kiros et al., 2015). The model architectures used for these studies have the same overall structure as our model and coincide with image–sentence retrieval systems (Kiros et al., 2014; Karpathy and Fei-Fei, 2015): a pre-trained CNN is fixed or fine-tuned as image feature extractor, followed by a learned transformation, while sentence representations are"
K18-1039,P14-2135,0,0.0264095,"ow-resource languages benefit from training with higher-resource languages. We demonstrate that a multilingual model can be trained equally well on either translations or comparable sentence pairs, and that annotating the same set of images in multiple language enables further improvements via an additional caption-caption ranking objective. 1 Introduction Multimodal representation learning is largely motivated by evidence of perceptual grounding in human concept acquisition and representation (Barsalou et al., 2003). It has been shown that visually grounded word and sentence-representations (Kiela et al., 2014; Baroni, 2016; Elliott and Kádár, 2017; Kiela et al., 2017; Yoo et al., 2017) improve performance on the downstream tasks of paraphrase identification, semantic entailment, and multimodal machine translation (Dolan et al., 2004; Marelli et al., 2014; Specia et al., 2016). Multilingual sentence representations have also been successfully applied to many-languages-to-one character-level machine translation (Chung et al., 2016) and multilingual dependency parsing (Ammar et al., 2016). Recently, Gella et al. (2017) proposed to learn both bilingual and multimodal sentence representations using ima"
K18-1039,D17-1303,0,0.188901,"Missing"
K18-1039,Q17-1026,0,0.045102,"r work is also closely related to multilingual joint representation learning. In this scenario, a single model is trained to solve a task across multiple languages. Ammar et al. (2016) train a multilingual dependency parser on the Universal Dependencies treebank (Nivre et al., 2015) and show that on average the single multilingual model outperforms the monolingual baselines. Johnson et al. (2016) present a zero-shot neural machine translation model that is jointly trained on language pairs A ↔ B and B ↔ C and show that the model is capable of performing well on the unseen language pair A ↔ C. Lee et al. (2017) find that jointly training a many-languages-to-one translation model on unsegmented character sequences improves BLEU scores compared to monolingual training. They also show evidence that the model can handle intrasentence code-switching. Peters et al. (2017) train a multilingual sequence-to-sequence translation architecture on grapheme-to-phoneme conversion using more than 300 languages. They report better performance when adding multiple languages, even those which are not present in the test data. Finally, Figure 2: Pseudo-code of the training procedure used to train our multilingual multi"
K18-1039,J15-4004,0,0.0325193,"multilingual multimodal sentence embeddings. Finally, we recommend to collect captions for the same set of images in multiple languages, due to the benefits of the additional caption–caption ranking objective function. 2 Related work Learning visually grounded word-representations has been an active area of research in the fields of multi-modal semantics and cross-situational wordlearning. Such perceptually-grounded word representations have been shown to lead to higher correlation with human judgements on word-similarity benchmarks such as WordSim353 (Finkelstein et al., 2001) or SimLex999 (Hill et al., 2015) compared to uni-modal representations (Kádár et al., 1 Gloss: Three men and two women with a South-East Asian appearance eat out of bowls at a black table, on which there are, among other things, paper cups and a bag; in the background there are other people and tables. 403 Require: p: task switching probability. Dc2i : datasets D1 . . . Dk of image-caption pairs &lt; c, i > for all k languages. Dc2c : data set of all possible caption pairs &lt; ca , cb > for all k languages. φ(c, θφ ): caption encoder ψ(i, θφ ): image encoder 2015; Bruni et al., 2014; Kiela and Bottou, 2014). Grounded representati"
K18-1039,P17-2031,0,0.0182567,"= 1 then Dn ∼ Dc2i &lt; c, i >∼ Dn a ← φ(c, θφ ) b ← ψ(i, θψ ) else &lt; ca , cb >∼ Dc2c a ← φ(ca , θφ ) b ← φ(cb , θφ ) end if [θφ ; θψ ] ← SGD(∇[θφ ;θψ ] J (a, b)) end while In addition to learning grounded representations for image-sentence ranking, joint vision and language systems have been proposed to solve a wide range of tasks across modalities such as image captioning (Mao et al., 2014; Vinyals et al., 2015; Xu et al., 2015), visual question answering (Antol et al., 2015; Fukui et al., 2016; Jabri et al., 2016), text-toimage synthesis (Reed et al., 2016) and multimodal machine translation (Libovicky and Helcl, 2017; Elliott and Kádár, 2017). Our work is also closely related to multilingual joint representation learning. In this scenario, a single model is trained to solve a task across multiple languages. Ammar et al. (2016) train a multilingual dependency parser on the Universal Dependencies treebank (Nivre et al., 2015) and show that on average the single multilingual model outperforms the monolingual baselines. Johnson et al. (2016) present a zero-shot neural machine translation model that is jointly trained on language pairs A ↔ B and B ↔ C and show that the model is capable of performing well on th"
K18-1039,D17-1268,0,0.0278376,"de-switching. Peters et al. (2017) train a multilingual sequence-to-sequence translation architecture on grapheme-to-phoneme conversion using more than 300 languages. They report better performance when adding multiple languages, even those which are not present in the test data. Finally, Figure 2: Pseudo-code of the training procedure used to train our multilingual multi-task model. massively multilingual language representations trained on over 900 languages have been shown to resemble language families (Östling and Tiedemann, 2016) and can successfully predict linguistic typology features (Malaviya et al., 2017). In the vision and language domain, multilingualmultimodal sentence representation learning has been limited so far to two languages. The joint training of models on English and German data has been shown to outperform monolingual baselines on image-sentence ranking and semantic textual similarity tasks (Gella et al., 2017; Calixto et al., 2017). Recently Harwath et al. (2018) also showed the benefit of joint bilingual training in the domain of speech-to-image and image-to-speech retrieval using English and Hindi data. 3 Multilingual grounded learning We train a standard model of grounded lan"
K18-1039,marelli-etal-2014-sick,0,0.0251852,"iple language enables further improvements via an additional caption-caption ranking objective. 1 Introduction Multimodal representation learning is largely motivated by evidence of perceptual grounding in human concept acquisition and representation (Barsalou et al., 2003). It has been shown that visually grounded word and sentence-representations (Kiela et al., 2014; Baroni, 2016; Elliott and Kádár, 2017; Kiela et al., 2017; Yoo et al., 2017) improve performance on the downstream tasks of paraphrase identification, semantic entailment, and multimodal machine translation (Dolan et al., 2004; Marelli et al., 2014; Specia et al., 2016). Multilingual sentence representations have also been successfully applied to many-languages-to-one character-level machine translation (Chung et al., 2016) and multilingual dependency parsing (Ammar et al., 2016). Recently, Gella et al. (2017) proposed to learn both bilingual and multimodal sentence representations using images paired with captions independently collected in English and German. Their results show that bilingual training improves imagesentence ranking performance over a monolingual ∗ Marc-Alexandre Côté Microsoft Research Montreal macote@microsoft.com ba"
K18-1039,P16-1168,0,0.0209183,"dditional language, it is better to collect captions for the existing images because we can exploit the caption–caption objective. Our results lead to several directions for future work. We would like to pin down the mechanism via which multilingual training contributes to improved performance for image-sentence ranking. Additionally, we only consider four languages and show the gain of multilingual over bilingual training only for the English-German language pair. In future work we will incorporate more languages from data sets such as the Chinese Flickr8K (Li et al., 2016) or Japanese COCO (Miyazaki and Shimizu, 2016). 409 Acknowledgements Desmond Elliott was supported by an Amazon Research Award. 100 English 80 R@10 56.3 German 61 61.9 71.9 67.6 60 Image → Text Text → Image 56 65.1 49.1 40.1 40 52.6 39.5 20.9 20 0 Monolingual Bilingual Multilingual Monolingual Bilingual Multilingual Figure 3: Comparing models from the Monolingual, Bilingual and Multilingual settings. The Monolingual and Bilingual models are trained on the downsampled English and German comparable sets with additional c2c objective. The Multilingual model uses the French and Czech translation pairs as additional data. The results are repor"
K18-1039,W17-5403,0,0.0636155,"Missing"
K18-1039,W16-2346,1,0.912102,"Missing"
K18-1039,D16-1163,0,0.064418,"Missing"
K19-1009,D17-1098,0,0.0157086,"aptions into a joint visualsemantic embedding space RJ . We introduce a 8 For each image in the evaluation set, we construct a test set that consists of the 5 correct captions and the captions of 1,000 randomly selected images from the COCO validation set. We ensure that all captions in the test set contain exactly one of the constituent concept pairs, but not both (except for the 5 correct captions). We construct a ranking of the captions in this test set with respect to the image, and use the top-K ranked captions to calculate the concept pair recall (Eqn. 1). Exceptions: You et al. (2016); Anderson et al. (2017) 91 sitting Image Captioning Softmax Generation LSTM &lt;start&gt; Reweight Embedding LSTM Attention Embed Embed Ranking Embed a dog Figure 2: An overview of BUTR, which jointly learns image–sentence ranking and image captioning. language encoding LSTM with a hidden layer dimension of L. hlt = LSTM(W1 ot , hlt−1 ) We deﬁne the similarity between an image and a caption as the cosine similarity cos(v ∗ , s∗ ). (2) 5.2 For caption generation, we introduce a separate language generation LSTM that is stacked on top of the language encoding LSTM. At each timestep t, we ﬁrst calculate a weighted representa"
K19-1009,W14-3348,0,0.0826308,"Missing"
K19-1009,N19-1423,0,0.0167836,"whether it has seen examples of the evaluation concept pair at training time or not. In other words, the model achieves better compositional generalization than the captioning models. training dataset (FULL). In this setting, the model is trained on compositions of the type we seek to evaluate in this task, and thus does not need to generalize to new compositions. Pretrained Language Representations: The word embeddings of image captioning models are usually learned from scratch, without prePretrained word embeddings (e.g. training7 . GloVe (Pennington et al., 2014)) or language models (e.g. Devlin et al. (2019)) contain distributional information obtained from large-scale textual resources, which may improve generalization performance. However, we do use them for this task because the resulting model may not have the expected paradigmatic gaps. 4.2 5 Results In the previous section, we found that state-of-theart captioning models fail to generalize to unseen combinations of concepts, however, an imagesentence ranking model does generalize. We propose a multi-task model that is trained for image captioning and image–sentence ranking with shared parameters between the different tasks. The captioning c"
K19-1009,D10-1115,0,0.04842,"ormation (Frome et al., 2013; Karpathy and Fei-Fei, 2015; Vendrov et al., 2016; Faghri et al., 2018). 2.2 Investigations of compositionality in vector space models date back to early debates in the cognitive science (Fodor and Pylyshyn, 1988; Fodor and Lepore, 2002) and connectionist literature (McClelland et al., 1986; Smolensky, 1988) regarding the ability of connectionist systems to compose simple constituents into complex structures. In the NLP literature, numerous approaches that (loosely) follow the linguistic principle of compositionality2 have been proposed (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). More recently, it has become standard to employ representations which are learned using neural network architectures. The extent to which these models behave compositionally is an open topic of research (Lake and Baroni, 2017; Dasgupta et al., 2018; Ettinger et al., 2018; McCoy et al., 2018) that closely relates to the focus of the present paper. Related Work 2.1 Compositional Models of Language Caption Generation and Retrieval Image Caption Generation models are usually end-to-end differentiable encoder-decoder models trained with a maximum likelihood obje"
K19-1009,P15-2017,0,0.0412862,"Missing"
K19-1009,P14-2074,1,0.667634,"CIDEr (C; Vedantam et al., 2015), BLEU (B; Papineni et al., 2002). RR stands for re-ranking after decoding. Lrank (θ2 ) = max [α + cos(i, s� ) − cos(i, s)]+ � 5.4 + RR R the same image features and a decoder architecture as the BUTD model. Thus, when using the standard beam search decoding method, BUTR does not improve over BUTD. However, when using the improved decoding mechanism with re-ranking BUTR + RR , Recall@5 increases to 13.2. We also observe an improvement in METEOR and SPICE, and a drop in BLEU and CIDEr compared to the other models. We note that BLEU has the weakest correlations (Elliott and Keller, 2014), and SPICE and METEOR have the strongest correlations with human judgments (Kilickaya et al., 2017). The Recall@5 scores for different categories of held out pairs is presented in in Table 3, and Figure 3 presents examples of images and the generated captions from different models. We observe that all models are generally best at describing colors, especially of inanimate objects; they nearly never correctly describe held out size modiﬁers; and for held out noun–verb pairs, performance is slightly better for transitive verbs. Results We follow the experimental protocol deﬁned in Section 4 to"
K19-1009,C18-1152,0,0.0270345,", 1986; Smolensky, 1988) regarding the ability of connectionist systems to compose simple constituents into complex structures. In the NLP literature, numerous approaches that (loosely) follow the linguistic principle of compositionality2 have been proposed (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). More recently, it has become standard to employ representations which are learned using neural network architectures. The extent to which these models behave compositionally is an open topic of research (Lake and Baroni, 2017; Dasgupta et al., 2018; Ettinger et al., 2018; McCoy et al., 2018) that closely relates to the focus of the present paper. Related Work 2.1 Compositional Models of Language Caption Generation and Retrieval Image Caption Generation models are usually end-to-end differentiable encoder-decoder models trained with a maximum likelihood objective. Given an image encoding that is extracted from a convolutional neural network (CNN), an RNNbased decoder generates a sequence of words that form the corresponding caption (Vinyals et al., 2015, inter-alia). This approach has been improved by applying top-down (Xu et al., 2015) and bottom-up attention"
K19-1009,D11-1129,0,0.0351322,"Missing"
K19-1009,P02-1040,0,0.103587,"e generation LSTM (Eqn. 11) to predict the next token (Eqn. 12). The jointly-trained image–sentence ranking component can be used to re-rank the generated captions comparing the image embedding with a language encoder embedding of the captions (Eqn. 4). We expect the ranking model will produce a better ranking of the B captions than only beam search by considering their relevance and informativity with respect to the image. 6 M Table 2: Average results for Recall@5 (R; Eqn. 1), METEOR (M; Denkowski and Lavie, 2014), SPICE (S; Anderson et al., 2016) , CIDEr (C; Vedantam et al., 2015), BLEU (B; Papineni et al., 2002). RR stands for re-ranking after decoding. Lrank (θ2 ) = max [α + cos(i, s� ) − cos(i, s)]+ � 5.4 + RR R the same image features and a decoder architecture as the BUTD model. Thus, when using the standard beam search decoding method, BUTR does not improve over BUTD. However, when using the improved decoding mechanism with re-ranking BUTR + RR , Recall@5 increases to 13.2. We also observe an improvement in METEOR and SPICE, and a drop in BLEU and CIDEr compared to the other models. We note that BLEU has the weakest correlations (Elliott and Keller, 2014), and SPICE and METEOR have the strongest"
K19-1009,D14-1162,0,0.0825038,"el is 47.0, indicating that the model performs well whether it has seen examples of the evaluation concept pair at training time or not. In other words, the model achieves better compositional generalization than the captioning models. training dataset (FULL). In this setting, the model is trained on compositions of the type we seek to evaluate in this task, and thus does not need to generalize to new compositions. Pretrained Language Representations: The word embeddings of image captioning models are usually learned from scratch, without prePretrained word embeddings (e.g. training7 . GloVe (Pennington et al., 2014)) or language models (e.g. Devlin et al. (2019)) contain distributional information obtained from large-scale textual resources, which may improve generalization performance. However, we do use them for this task because the resulting model may not have the expected paradigmatic gaps. 4.2 5 Results In the previous section, we found that state-of-theart captioning models fail to generalize to unseen combinations of concepts, however, an imagesentence ranking model does generalize. We propose a multi-task model that is trained for image captioning and image–sentence ranking with shared parameter"
K19-1009,K18-2016,0,0.0261485,"n model. In particular, we identify adjectives, nouns, and verbs in the English COCO captions dataset (Chen et al., 2015) that are suitable for testing compositional generalization. We deﬁne concepts as sets of synonyms for each word, to account for the variation in how the concept can be expressed in a caption. For each noun, we use the synonyms deﬁned in Lu et al. (2018). For the verbs and adjectives, we use manually deﬁned synonyms (see Appendix D). From these concepts, we select adjective–noun and noun–verb pairs for the evaluation. To identify concept pair candidates, we use StanfordNLP (Qi et al., 2018) to label and lemmatize the nouns, adjectives, and verbs in the captions, and to check if the adjective or verb is connected to the respective noun in the dependency parse. Compositional Image Captioning Problem Deﬁnition In this section we deﬁne the compositional captioning task, which is designed to evaluate how well a model generalizes to captioning images that should be described using previously unseen combinations of concepts, when the individual concepts have been observed in the training data. We assume a dataset of captioned images D, in which N images are described by K captions: D N"
K19-1009,C18-1147,1,0.916216,"Missing"
K19-1009,P08-1028,0,\N,Missing
K19-1009,E17-1019,0,\N,Missing
K19-1009,P18-1082,0,\N,Missing
L16-1219,W10-0721,0,0.141119,"rate analysis or evaluation. Keywords: online news; image features; topic extraction 1. Introduction In recent years, several datasets have been released that include images and text (Ferraro et al., 2015; Bernardi et al., 2016), giving impulse to new methods that combine natural language processing and computer vision, such as automatic image description (Fang et al., 2014) and image– sentence matching (Hodosh et al., 2013). Current datasets typically consist of user-captioned images from Flickr1 (Ordonez et al., 2011; Chen et al., 2015) or images with descriptions produced by crowd workers (Rashtchian et al., 2010; Elliott and Keller, 2013; Zitnick and Parikh, 2013; Lin et al., 2014; Young et al., 2014). These datasets can be used to train systems to produce Flickr-like captions or crowdsourced descriptions, but we argue there is a need for more datasets for generating captions in context. This argument has previously been made by Feng and Lapata (2008), who released a dataset of 3,361 news articles from BBC News, including images and real-world captions; Tirilly et al. (2010) collected the texts, images and captions of 27,000 French newspaper articles, and used the real-world captions as a ground trut"
L16-1219,tirilly-etal-2010-news,0,0.590919,"aptioned images from Flickr1 (Ordonez et al., 2011; Chen et al., 2015) or images with descriptions produced by crowd workers (Rashtchian et al., 2010; Elliott and Keller, 2013; Zitnick and Parikh, 2013; Lin et al., 2014; Young et al., 2014). These datasets can be used to train systems to produce Flickr-like captions or crowdsourced descriptions, but we argue there is a need for more datasets for generating captions in context. This argument has previously been made by Feng and Lapata (2008), who released a dataset of 3,361 news articles from BBC News, including images and real-world captions; Tirilly et al. (2010) collected the texts, images and captions of 27,000 French newspaper articles, and used the real-world captions as a ground truth to evaluate an image annotation algorithm. As a continuation of these efforts, we present the Images in Online News (ION) corpus. The ION corpus contains news articles published between August 2014 - 2015 in five online newspapers from two different countries. It includes more than 323,707 articles, making it larger than the existing datasets of news images and text. The 1-year coverage over multiple publishers ensures a broad scope in terms of topics, image quality"
L16-1219,Q14-1006,0,0.0529594,"ction In recent years, several datasets have been released that include images and text (Ferraro et al., 2015; Bernardi et al., 2016), giving impulse to new methods that combine natural language processing and computer vision, such as automatic image description (Fang et al., 2014) and image– sentence matching (Hodosh et al., 2013). Current datasets typically consist of user-captioned images from Flickr1 (Ordonez et al., 2011; Chen et al., 2015) or images with descriptions produced by crowd workers (Rashtchian et al., 2010; Elliott and Keller, 2013; Zitnick and Parikh, 2013; Lin et al., 2014; Young et al., 2014). These datasets can be used to train systems to produce Flickr-like captions or crowdsourced descriptions, but we argue there is a need for more datasets for generating captions in context. This argument has previously been made by Feng and Lapata (2008), who released a dataset of 3,361 news articles from BBC News, including images and real-world captions; Tirilly et al. (2010) collected the texts, images and captions of 27,000 French newspaper articles, and used the real-world captions as a ground truth to evaluate an image annotation algorithm. As a continuation of these efforts, we present"
L16-1219,D15-1021,0,\N,Missing
L16-1219,W10-0707,0,\N,Missing
L16-1219,D13-1128,1,\N,Missing
L16-1219,P08-1032,0,\N,Missing
L16-1219,N15-1053,0,\N,Missing
L16-1488,P08-1032,0,0.419908,"words in the image, and it is easy to quickly collect multiple reference texts. However, crowdsourcing takes an image outside its original context, rendering the intentions of the photographer inaccessible. For example, does Figure 1 depict the world-famous Notre Dame Cathedral in Paris or automobiles waiting at a traffic light? It is difficult to be sure, but crowdsourcing affects our ability to situate the image in a wider context. An alternative to crowdsourced descriptions is sourcing images that naturally co-occur with text, such as on photo sharing sites, social media, or in newspapers (Feng and Lapata, 2008; Ordonez et al., 2011; Chen et al., 2015a). The captions in these datasets often refer to specific people (politicians, movie stars, etc.), emotions, or places (Paris, my back garden, etc.) that are difficult to ground against the images. In this paper we introduce the KBK-1M dataset of 1.6 million captioned newspaper images. Contrary to other visual datasets such as WikiMedia Commons, the KBK-1M dataset contains images and captions that have been created by professional photographers and journalists and they are situated in their original context in the newspaper. The dataset covers 72 years"
L16-1488,D15-1021,0,0.0200271,"nd Lapata, 2008) FiNGS (Kleppe, 2012) 1,603,396 3,361 5,344 Title and caption Title, caption, body Caption, descriptions Dutch newspapers BBC News Online Dutch Textbooks SBU Captioned Photos (Ordonez et al., 2011) Deja-Images (Chen et al., 2015a) 1,000,000 440,000 Caption Caption Photo uploader Photo uploader 30,000 164,062 Five descriptions Five descriptions Crowdsourced Crowdsourced Flickr30K (Hodosh et al., 2013b) MS COCO (Chen et al., 2015b) Table 1: Comparison of several news image datasets, user-captioned images, and crowd-sourced described images. For a more comprehensive overview, see Ferraro et al. (2015) and Bernardi et al. (2016). the newspaper issue, the date of publication, and the identifiers of the content and text blocks in the original repository metadata document. Each newspaper issue is stored with a unique identifier linking an image – caption pair directly back to the newspaper issue ID from the Newspaper Archive. The images are broken down by year of publication and are available as year-by-year ZIP files at the National Library of the Netherlands. A request for access can be submitted to dataservices@kb.nl in order to comply with Dutch copyright agreements. Therefore, it is not p"
L16-1488,P12-1039,0,0.0311138,"g et al., 2011; Li et al., 2011; Mitchell et al., 2012; Yatskar et al., 2014; Elliott and de Vries, 2015; Vinyals et al., 2015; Karpathy and FeiFei, 2015; Mao et al., 2015) and represents a strong challenge at the intersection of CV and NLP research. Most datasets for this task are based on crowdsourced descriptions, from which visual features and words can more easily co-reference. In the KBK-1M dataset there is no guarantee the captions will refer to anything depicted in the image. NLP: Data-to-Text Generation Data-to-Text Generation is a well-studied problem in Natural Language Generation (Konstas and Lapata, 2012, interalia). The task is framed as transforming database records into natural language, such as WeatherGov records with structured weather observation data. One possible use for the KBK-1M dataset is the development of a new method of Data-to-Text Generation, conditioned on weather-forecast sketches. The newspaper weather forecasts contain rich visual data, including isobar charts, temperatures, weather fronts, and major city names. Gold-standard weather data can be found in the formal records from the Royal Netherlands Meteorological Institute (KNMI) Data Centre. 3.3. Humanities: Image Reuse"
L16-1488,W11-0326,0,0.0371906,"aired with captions, making it suitable for large-scale multimodal matching experiments. 3.2. 3. Use Cases We present three use cases for the KBK-1M dataset, covering computer vision (CV), Natural Language Processing (NLP) and Humanities research. We welcome new usecases from these and other communities. 3.1. CV & NLP: Automatic Image Captioning and Multimodal Ranking Image-to-text generation is the task of automatically generating the description of an image using only the visual input. This task has received a great deal of attention in recent years (Farhadi et al., 2010; Yang et al., 2011; Li et al., 2011; Mitchell et al., 2012; Yatskar et al., 2014; Elliott and de Vries, 2015; Vinyals et al., 2015; Karpathy and FeiFei, 2015; Mao et al., 2015) and represents a strong challenge at the intersection of CV and NLP research. Most datasets for this task are based on crowdsourced descriptions, from which visual features and words can more easily co-reference. In the KBK-1M dataset there is no guarantee the captions will refer to anything depicted in the image. NLP: Data-to-Text Generation Data-to-Text Generation is a well-studied problem in Natural Language Generation (Konstas and Lapata, 2012, inter"
L16-1488,E12-1076,0,0.0190341,"ns, making it suitable for large-scale multimodal matching experiments. 3.2. 3. Use Cases We present three use cases for the KBK-1M dataset, covering computer vision (CV), Natural Language Processing (NLP) and Humanities research. We welcome new usecases from these and other communities. 3.1. CV & NLP: Automatic Image Captioning and Multimodal Ranking Image-to-text generation is the task of automatically generating the description of an image using only the visual input. This task has received a great deal of attention in recent years (Farhadi et al., 2010; Yang et al., 2011; Li et al., 2011; Mitchell et al., 2012; Yatskar et al., 2014; Elliott and de Vries, 2015; Vinyals et al., 2015; Karpathy and FeiFei, 2015; Mao et al., 2015) and represents a strong challenge at the intersection of CV and NLP research. Most datasets for this task are based on crowdsourced descriptions, from which visual features and words can more easily co-reference. In the KBK-1M dataset there is no guarantee the captions will refer to anything depicted in the image. NLP: Data-to-Text Generation Data-to-Text Generation is a well-studied problem in Natural Language Generation (Konstas and Lapata, 2012, interalia). The task is fram"
L16-1488,W10-0721,0,0.0284794,"ocks. The dataset is suitable for experiments in automatic image captioning, image–article matching, object recognition, and data-to-text generation for weather forecasting. It can also be used by humanities scholars to analyse photographic style changes, the representation of people and societal issues, and new tools for exploring photograph reuse via image-similarity-based search. Keywords: Digital Humanities, Digitised Newspapers, Language and Vision 1. Introduction Vision and language datasets typically consist of highquality born-digital photographs paired with crowdsourced descriptions (Rashtchian et al., 2010; Hodosh et al., 2013a; Young et al., 2014; Chen et al., 2015b). Descriptions obtained through crowdsourcing have several advantages: they usually describe what can be seen in the image, reducing the difficulty of visually grounding the words in the image, and it is easy to quickly collect multiple reference texts. However, crowdsourcing takes an image outside its original context, rendering the intentions of the photographer inaccessible. For example, does Figure 1 depict the world-famous Notre Dame Cathedral in Paris or automobiles waiting at a traffic light? It is difficult to be sure, but"
L16-1488,D11-1041,0,0.0402936,"t contains images paired with captions, making it suitable for large-scale multimodal matching experiments. 3.2. 3. Use Cases We present three use cases for the KBK-1M dataset, covering computer vision (CV), Natural Language Processing (NLP) and Humanities research. We welcome new usecases from these and other communities. 3.1. CV & NLP: Automatic Image Captioning and Multimodal Ranking Image-to-text generation is the task of automatically generating the description of an image using only the visual input. This task has received a great deal of attention in recent years (Farhadi et al., 2010; Yang et al., 2011; Li et al., 2011; Mitchell et al., 2012; Yatskar et al., 2014; Elliott and de Vries, 2015; Vinyals et al., 2015; Karpathy and FeiFei, 2015; Mao et al., 2015) and represents a strong challenge at the intersection of CV and NLP research. Most datasets for this task are based on crowdsourced descriptions, from which visual features and words can more easily co-reference. In the KBK-1M dataset there is no guarantee the captions will refer to anything depicted in the image. NLP: Data-to-Text Generation Data-to-Text Generation is a well-studied problem in Natural Language Generation (Konstas and La"
L16-1488,S14-1015,0,0.0135833,"for large-scale multimodal matching experiments. 3.2. 3. Use Cases We present three use cases for the KBK-1M dataset, covering computer vision (CV), Natural Language Processing (NLP) and Humanities research. We welcome new usecases from these and other communities. 3.1. CV & NLP: Automatic Image Captioning and Multimodal Ranking Image-to-text generation is the task of automatically generating the description of an image using only the visual input. This task has received a great deal of attention in recent years (Farhadi et al., 2010; Yang et al., 2011; Li et al., 2011; Mitchell et al., 2012; Yatskar et al., 2014; Elliott and de Vries, 2015; Vinyals et al., 2015; Karpathy and FeiFei, 2015; Mao et al., 2015) and represents a strong challenge at the intersection of CV and NLP research. Most datasets for this task are based on crowdsourced descriptions, from which visual features and words can more easily co-reference. In the KBK-1M dataset there is no guarantee the captions will refer to anything depicted in the image. NLP: Data-to-Text Generation Data-to-Text Generation is a well-studied problem in Natural Language Generation (Konstas and Lapata, 2012, interalia). The task is framed as transforming dat"
L16-1488,Q14-1006,0,0.0308193,"n automatic image captioning, image–article matching, object recognition, and data-to-text generation for weather forecasting. It can also be used by humanities scholars to analyse photographic style changes, the representation of people and societal issues, and new tools for exploring photograph reuse via image-similarity-based search. Keywords: Digital Humanities, Digitised Newspapers, Language and Vision 1. Introduction Vision and language datasets typically consist of highquality born-digital photographs paired with crowdsourced descriptions (Rashtchian et al., 2010; Hodosh et al., 2013a; Young et al., 2014; Chen et al., 2015b). Descriptions obtained through crowdsourcing have several advantages: they usually describe what can be seen in the image, reducing the difficulty of visually grounding the words in the image, and it is easy to quickly collect multiple reference texts. However, crowdsourcing takes an image outside its original context, rendering the intentions of the photographer inaccessible. For example, does Figure 1 depict the world-famous Notre Dame Cathedral in Paris or automobiles waiting at a traffic light? It is difficult to be sure, but crowdsourcing affects our ability to situa"
L16-1488,W10-0707,0,\N,Missing
L16-1488,N15-1053,0,\N,Missing
L16-1488,P15-1005,1,\N,Missing
N19-1200,W17-4746,0,0.0635256,"Missing"
N19-1200,D07-1007,0,0.0304704,"rrif, 1998). Word sense disambiguation is typically tackled using only textual context; however, in a multimodal setting, visual context is also available and can be used for disambiguation. Most prior work on visual word sense disambiguation has targeted noun senses (Barnard and Johnson, 2005; Loeff et al., 2006; Saenko and Darrell, 2008), but the task has recently been extended to verb senses (Gella et al., 2016, 2019). Resolving sense ambiguity is particularly crucial for translation tasks, as words can have more than one translation, and these translations often correspond to word senses (Carpuat and Wu, 2007; Navigli, 2009). As an example consider the verb ride, which can translate into German as fahren (ride a bike) or reiten (ride a horse). Recent work on multimodal machine translation has partly addressed lexical ambiguity by using visual information, but it still remains unresolved especially for the part-ofspeech categories such as verbs (Specia et al., 2016; Shah et al., 2016; Hitschler et al., 2016; Lala and Specia, 2018). Prior work on cross-lingual WSD has been limited in scale and has only employed textual context (Lefever and Hoste, 2013), even though the task should benefit from visua"
N19-1200,W14-3348,0,0.0113895,"multimodal) affects the accuracy of the verb prediction. We show the top verb predicted by our models for both German and Spanish. The top predicted verb using text-only visual features is incorrect. The unimodal visual features model predicts the correct Spanish verb but the incorrect We also evaluate our verb sense disambiguation model in the challenging downstream task of multimodal machine translation (Specia et al., 2016). We conduct this evaluation on the sentence-level translation subset of MultiSense. We evaluate model performance using BLEU (Papineni et al., 2002) and Meteor scores (Denkowski and Lavie, 2014) between the MultiSense reference description and the translation model output. We also evaluate the verb prediction accuracy of the output against the gold standard verb annotation. 2001 5.1 6 Models Our baseline is an attention-based neural machine translation model (Hieber et al., 2017) trained on the 29,000 English-German sentences in Multi30k (Elliott et al., 2016). We preprocessed the text with punctuation normalization, tokenization, and lowercasing. We then learned a joint byte-pairencoded vocabulary with 10,000 merge operations to reduce sparsity (Sennrich et al., 2016). Our approach"
N19-1200,W17-4718,1,0.872371,"has a clear application in machine translation. Determining the correct sense of a verb is important for high quality translation output, and sometimes text-only translation systems fail when the correct translation would be obvious from visual information (see Figure 1). To show that cross-lingual visual sense disambiguation can improve the performance of translation systems, we annotate a part of our MultiSense dataset with English image descriptions and their German translations. There are two existing multimodal translation evaluation sets with ambiguous words: the Ambiguous COCO dataset (Elliott et al., 2017) contains sentences that are “possibly ambiguous”, and the Multimodal Lexical Translation dataset is restricted to predicting single words instead of full sentences (Lala and Specia, 2018). This type of resource is important for multimodal translation because it is known that humans use visual context to resolve ambiguities for nouns and gender-neutral words (Frank et al., 2018). MultiSense contains sentences that are known to have ambiguities, and it allows for sentence-level and verb prediction evaluation. Here, we use the verbs predicted by our visual sense disambiguation model to constrain"
N19-1200,W16-3210,1,0.896034,"Missing"
N19-1200,N16-1022,1,0.688856,"allenging problems in natural language processing. It is often studied as a word sense disambiguation (WSD) problem, which is the task of assigning the correct sense to a word in a given context (Kilgarrif, 1998). Word sense disambiguation is typically tackled using only textual context; however, in a multimodal setting, visual context is also available and can be used for disambiguation. Most prior work on visual word sense disambiguation has targeted noun senses (Barnard and Johnson, 2005; Loeff et al., 2006; Saenko and Darrell, 2008), but the task has recently been extended to verb senses (Gella et al., 2016, 2019). Resolving sense ambiguity is particularly crucial for translation tasks, as words can have more than one translation, and these translations often correspond to word senses (Carpuat and Wu, 2007; Navigli, 2009). As an example consider the verb ride, which can translate into German as fahren (ride a bike) or reiten (ride a horse). Recent work on multimodal machine translation has partly addressed lexical ambiguity by using visual information, but it still remains unresolved especially for the part-ofspeech categories such as verbs (Specia et al., 2016; Shah et al., 2016; Hitschler et a"
N19-1200,D17-1303,1,0.795124,"just like monolingual WSD. Visual information has been shown to be useful to map words across languages for bilingual lexicon induction. For this, images are used as a pivot between languages or visual information is combined with cross-lingual vector spaces to learn word translations across languages (Bergsma and Van Durme, 2011; Kiela et al., 2015; Vulic et al., 2016). However, as with other grounding or word similarity tasks, bilingual lexicon induction has so far mainly targeted nouns and these approaches was shown to perform poorly for other word categories such as verbs. Recent work by Gella et al. (2017) and K´ad´ar et al. (2018) has shown using image as pivot between languages can lead to better multilingual multimodal representations and can have successful applications in crosslingual retrieval and 1998 Proceedings of NAACL-HLT 2019, pages 1998–2004 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics multilingual image retrieval. In this paper, we introduce the MultiSense dataset of 9,504 images annotated with English verbs and their translations in German and Spanish. For each image in MultiSense, the English verb is translation-ambiguous, i.e.,"
N19-1200,W17-4749,0,0.0360741,"Missing"
N19-1200,E17-3017,0,0.0224648,"evaluate our verb sense disambiguation model in the challenging downstream task of multimodal machine translation (Specia et al., 2016). We conduct this evaluation on the sentence-level translation subset of MultiSense. We evaluate model performance using BLEU (Papineni et al., 2002) and Meteor scores (Denkowski and Lavie, 2014) between the MultiSense reference description and the translation model output. We also evaluate the verb prediction accuracy of the output against the gold standard verb annotation. 2001 5.1 6 Models Our baseline is an attention-based neural machine translation model (Hieber et al., 2017) trained on the 29,000 English-German sentences in Multi30k (Elliott et al., 2016). We preprocessed the text with punctuation normalization, tokenization, and lowercasing. We then learned a joint byte-pairencoded vocabulary with 10,000 merge operations to reduce sparsity (Sennrich et al., 2016). Our approach uses the German verb predicted by the unimodal visual model (Section 3.1) to constrain the output of the translation decoder (Post and Vilar, 2018). This means that our approach does not directly use visual features, instead it uses the output of the visual verb sense disambiguation model"
N19-1200,P16-1227,0,0.0396333,"a et al., 2016, 2019). Resolving sense ambiguity is particularly crucial for translation tasks, as words can have more than one translation, and these translations often correspond to word senses (Carpuat and Wu, 2007; Navigli, 2009). As an example consider the verb ride, which can translate into German as fahren (ride a bike) or reiten (ride a horse). Recent work on multimodal machine translation has partly addressed lexical ambiguity by using visual information, but it still remains unresolved especially for the part-ofspeech categories such as verbs (Specia et al., 2016; Shah et al., 2016; Hitschler et al., 2016; Lala and Specia, 2018). Prior work on cross-lingual WSD has been limited in scale and has only employed textual context (Lefever and Hoste, 2013), even though the task should benefit from visual context, just like monolingual WSD. Visual information has been shown to be useful to map words across languages for bilingual lexicon induction. For this, images are used as a pivot between languages or visual information is combined with cross-lingual vector spaces to learn word translations across languages (Bergsma and Van Durme, 2011; Kiela et al., 2015; Vulic et al., 2016). However, as with oth"
N19-1200,K18-1039,1,0.85067,"rmation has been shown to be useful to map words across languages for bilingual lexicon induction. For this, images are used as a pivot between languages or visual information is combined with cross-lingual vector spaces to learn word translations across languages (Bergsma and Van Durme, 2011; Kiela et al., 2015; Vulic et al., 2016). However, as with other grounding or word similarity tasks, bilingual lexicon induction has so far mainly targeted nouns and these approaches was shown to perform poorly for other word categories such as verbs. Recent work by Gella et al. (2017) and K´ad´ar et al. (2018) has shown using image as pivot between languages can lead to better multilingual multimodal representations and can have successful applications in crosslingual retrieval and 1998 Proceedings of NAACL-HLT 2019, pages 1998–2004 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics multilingual image retrieval. In this paper, we introduce the MultiSense dataset of 9,504 images annotated with English verbs and their translations in German and Spanish. For each image in MultiSense, the English verb is translation-ambiguous, i.e., it has more than one poss"
N19-1200,D15-1015,0,0.0325299,"Missing"
N19-1200,L18-1602,0,0.315333,"esolving sense ambiguity is particularly crucial for translation tasks, as words can have more than one translation, and these translations often correspond to word senses (Carpuat and Wu, 2007; Navigli, 2009). As an example consider the verb ride, which can translate into German as fahren (ride a bike) or reiten (ride a horse). Recent work on multimodal machine translation has partly addressed lexical ambiguity by using visual information, but it still remains unresolved especially for the part-ofspeech categories such as verbs (Specia et al., 2016; Shah et al., 2016; Hitschler et al., 2016; Lala and Specia, 2018). Prior work on cross-lingual WSD has been limited in scale and has only employed textual context (Lefever and Hoste, 2013), even though the task should benefit from visual context, just like monolingual WSD. Visual information has been shown to be useful to map words across languages for bilingual lexicon induction. For this, images are used as a pivot between languages or visual information is combined with cross-lingual vector spaces to learn word translations across languages (Bergsma and Van Durme, 2011; Kiela et al., 2015; Vulic et al., 2016). However, as with other grounding or word sim"
N19-1200,N15-1016,0,0.0156832,"verb. Figure 1 shows an example of an image paired with its English description and German translation. 3 Verb Sense Disambiguation Modeling We propose three models for cross-lingual verb sense disambiguation, based on the visual input, the textual input, or using both inputs. Each model is trained to minimize the negative log probability of predicting the correct verb translation. 1999 Chance Majority Text Image MM German Spanish Spanish mandar German zublasen hinchar aufblasen Unimodal Visual Model Visual features have been shown to be useful for learning semantic representations of words (Lazaridou et al., 2015), bilingual lexicon learning (Kiela et al., 2015), and visual sense disambiguation (Gella et al., 2016), amongst others. We propose a model that learns to predict the verb translation using only visual input. Given an image I, we extract a fixed feature vector from a Convolutional Neural Network, and project it into a hidden layer hv with the learned matrix Wi ∈ Rh×512 (Eqn. 1). The hidden layer is projected into the output vocabulary of v verbs using the learned matrix Wo ∈ Rh×v , and normalized into a probability distribution using a softmax transformation (Eqn. 2). hv = Wi · CNN(I) + bi y ="
N19-1200,S13-2029,0,0.0483847,"these translations often correspond to word senses (Carpuat and Wu, 2007; Navigli, 2009). As an example consider the verb ride, which can translate into German as fahren (ride a bike) or reiten (ride a horse). Recent work on multimodal machine translation has partly addressed lexical ambiguity by using visual information, but it still remains unresolved especially for the part-ofspeech categories such as verbs (Specia et al., 2016; Shah et al., 2016; Hitschler et al., 2016; Lala and Specia, 2018). Prior work on cross-lingual WSD has been limited in scale and has only employed textual context (Lefever and Hoste, 2013), even though the task should benefit from visual context, just like monolingual WSD. Visual information has been shown to be useful to map words across languages for bilingual lexicon induction. For this, images are used as a pivot between languages or visual information is combined with cross-lingual vector spaces to learn word translations across languages (Bergsma and Van Durme, 2011; Kiela et al., 2015; Vulic et al., 2016). However, as with other grounding or word similarity tasks, bilingual lexicon induction has so far mainly targeted nouns and these approaches was shown to perform poorl"
N19-1200,P12-3029,0,0.0873912,"Missing"
N19-1200,P06-2071,0,0.205001,"Missing"
N19-1200,P02-1040,0,0.104812,"how varying the input (textual, visual, or multimodal) affects the accuracy of the verb prediction. We show the top verb predicted by our models for both German and Spanish. The top predicted verb using text-only visual features is incorrect. The unimodal visual features model predicts the correct Spanish verb but the incorrect We also evaluate our verb sense disambiguation model in the challenging downstream task of multimodal machine translation (Specia et al., 2016). We conduct this evaluation on the sentence-level translation subset of MultiSense. We evaluate model performance using BLEU (Papineni et al., 2002) and Meteor scores (Denkowski and Lavie, 2014) between the MultiSense reference description and the translation model output. We also evaluate the verb prediction accuracy of the output against the gold standard verb annotation. 2001 5.1 6 Models Our baseline is an attention-based neural machine translation model (Hieber et al., 2017) trained on the 29,000 English-German sentences in Multi30k (Elliott et al., 2016). We preprocessed the text with punctuation normalization, tokenization, and lowercasing. We then learned a joint byte-pairencoded vocabulary with 10,000 merge operations to reduce s"
N19-1200,N18-1119,0,0.013856,"of the output against the gold standard verb annotation. 2001 5.1 6 Models Our baseline is an attention-based neural machine translation model (Hieber et al., 2017) trained on the 29,000 English-German sentences in Multi30k (Elliott et al., 2016). We preprocessed the text with punctuation normalization, tokenization, and lowercasing. We then learned a joint byte-pairencoded vocabulary with 10,000 merge operations to reduce sparsity (Sennrich et al., 2016). Our approach uses the German verb predicted by the unimodal visual model (Section 3.1) to constrain the output of the translation decoder (Post and Vilar, 2018). This means that our approach does not directly use visual features, instead it uses the output of the visual verb sense disambiguation model to guide the translation process. We compare our approach against two state-ofthe-art multimodal translation systems: Caglayan et al. (2017) modulate the target language word embeddings by an element-wise multiplication with a learned transformation of the visual data; Helcl and Libovick´y (2017) use a double attention model that learns to selectively attend to a combination of the source language and the visual data. 5.2 Results Table 4 shows the resul"
N19-1200,W17-4739,0,0.0249021,"Missing"
N19-1200,P16-1162,0,0.0155016,"or scores (Denkowski and Lavie, 2014) between the MultiSense reference description and the translation model output. We also evaluate the verb prediction accuracy of the output against the gold standard verb annotation. 2001 5.1 6 Models Our baseline is an attention-based neural machine translation model (Hieber et al., 2017) trained on the 29,000 English-German sentences in Multi30k (Elliott et al., 2016). We preprocessed the text with punctuation normalization, tokenization, and lowercasing. We then learned a joint byte-pairencoded vocabulary with 10,000 merge operations to reduce sparsity (Sennrich et al., 2016). Our approach uses the German verb predicted by the unimodal visual model (Section 3.1) to constrain the output of the translation decoder (Post and Vilar, 2018). This means that our approach does not directly use visual features, instead it uses the output of the visual verb sense disambiguation model to guide the translation process. We compare our approach against two state-ofthe-art multimodal translation systems: Caglayan et al. (2017) modulate the target language word embeddings by an element-wise multiplication with a learned transformation of the visual data; Helcl and Libovick´y (201"
N19-1200,W16-2363,0,0.0218059,"o verb senses (Gella et al., 2016, 2019). Resolving sense ambiguity is particularly crucial for translation tasks, as words can have more than one translation, and these translations often correspond to word senses (Carpuat and Wu, 2007; Navigli, 2009). As an example consider the verb ride, which can translate into German as fahren (ride a bike) or reiten (ride a horse). Recent work on multimodal machine translation has partly addressed lexical ambiguity by using visual information, but it still remains unresolved especially for the part-ofspeech categories such as verbs (Specia et al., 2016; Shah et al., 2016; Hitschler et al., 2016; Lala and Specia, 2018). Prior work on cross-lingual WSD has been limited in scale and has only employed textual context (Lefever and Hoste, 2013), even though the task should benefit from visual context, just like monolingual WSD. Visual information has been shown to be useful to map words across languages for bilingual lexicon induction. For this, images are used as a pivot between languages or visual information is combined with cross-lingual vector spaces to learn word translations across languages (Bergsma and Van Durme, 2011; Kiela et al., 2015; Vulic et al., 201"
N19-1200,W16-2346,1,0.939576,"Missing"
N19-1200,P16-2031,0,0.0163985,"Shah et al., 2016; Hitschler et al., 2016; Lala and Specia, 2018). Prior work on cross-lingual WSD has been limited in scale and has only employed textual context (Lefever and Hoste, 2013), even though the task should benefit from visual context, just like monolingual WSD. Visual information has been shown to be useful to map words across languages for bilingual lexicon induction. For this, images are used as a pivot between languages or visual information is combined with cross-lingual vector spaces to learn word translations across languages (Bergsma and Van Durme, 2011; Kiela et al., 2015; Vulic et al., 2016). However, as with other grounding or word similarity tasks, bilingual lexicon induction has so far mainly targeted nouns and these approaches was shown to perform poorly for other word categories such as verbs. Recent work by Gella et al. (2017) and K´ad´ar et al. (2018) has shown using image as pivot between languages can lead to better multilingual multimodal representations and can have successful applications in crosslingual retrieval and 1998 Proceedings of NAACL-HLT 2019, pages 1998–2004 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics mult"
N19-1200,W09-2413,0,\N,Missing
P14-2074,P11-2031,0,0.289656,"pairings. In this analysis, we use only the first sentence of the description, which describes the event depicted in the image. 453 pn = c∈cand ∑ c∈cand  BP = N ∑ wn log pn n=1 ∑ ngram∈c ∑ countclip (ngram) ngram∈c 1 e(1−r/c) count(ngram) if c > r if c ≤ r Unigram BLEU without a brevity penalty has been reported by Kulkarni et al. (2011), Li et al. (2011), Ordonez et al. (2011), and Kuznetsova et al. (2012); to the best of our knowledge, the only image description work to use higher-order n-grams with BLEU is Elliott and Keller (2013). In this paper we use the smoothed BLEU implementation of Clark et al. (2011) to perform a sentence-level analysis, setting n = 1 and no brevity penalty to get the unigram BLEU measure, or n = 4 with the brevity penalty to get the Smoothed BLEU measure. We note that a higher BLEU score is better. ROUGE measures the longest common subsequence of tokens between a candidate Y and reference X. There is also a variant that measures the cooccurrence of pairs of tokens in both the candidate and reference (a skip-bigram): ROUGE - SU *. The skip-bigram calculation is parameterised with dskip , the maximum number of tokens between the words in the skip-bigram. Setting dskip to 0"
P14-2074,D13-1128,1,0.649925,"language processing have led to an upsurge of research on tasks involving both vision and language. State of the art visual detectors have made it possible to hypothesise what is in an image (Guillaumin et al., 2009; Felzenszwalb et al., 2010), paving the way for automatic image description systems. The aim of such systems is to extract and reason about visual aspects of images to generate a humanlike description. An example of the type of image and gold-standard descriptions available can be seen in Figure 1. Recent approaches to this task have been based on slot-filling (Yang et al., 2011; Elliott and Keller, 2013), combining web-scale ngrams (Li et al., 2011), syntactic tree substitution (Mitchell et al., 2012), and description-by-retrieval (Farhadi et al., 2010; Ordonez et al., 2011; Hodosh et al., 2013). Image description has been compared to translating an image into text (Li et al., 2011; Kulkarni et al., 2011) or summarising an image Figure 1: An image from the Flickr8K data set and five human-written descriptions. These descriptions vary in the adjectives or prepositional phrases that describe the woman (1, 3, 4, 5), incorrect or uncertain identification of the cat (1, 3), and include a sentence"
P14-2074,P12-1038,0,0.390137,"Missing"
P14-2074,W11-0326,0,0.512021,"on tasks involving both vision and language. State of the art visual detectors have made it possible to hypothesise what is in an image (Guillaumin et al., 2009; Felzenszwalb et al., 2010), paving the way for automatic image description systems. The aim of such systems is to extract and reason about visual aspects of images to generate a humanlike description. An example of the type of image and gold-standard descriptions available can be seen in Figure 1. Recent approaches to this task have been based on slot-filling (Yang et al., 2011; Elliott and Keller, 2013), combining web-scale ngrams (Li et al., 2011), syntactic tree substitution (Mitchell et al., 2012), and description-by-retrieval (Farhadi et al., 2010; Ordonez et al., 2011; Hodosh et al., 2013). Image description has been compared to translating an image into text (Li et al., 2011; Kulkarni et al., 2011) or summarising an image Figure 1: An image from the Flickr8K data set and five human-written descriptions. These descriptions vary in the adjectives or prepositional phrases that describe the woman (1, 3, 4, 5), incorrect or uncertain identification of the cat (1, 3), and include a sentence without a verb (5). (Yang et al., 2011), resul"
P14-2074,E12-1076,0,0.127084,"Missing"
P14-2074,J09-4008,0,0.0343498,"rieved from different images and show differences in how to describe an image. 5 some of which go beyond unigram matchings between references and candidates, whereas they only report unigram BLEU and unigram ROUGE. It is therefore difficult to directly compare the results of our correlation analysis against Hodosh et al.’s agreement analysis, but they also reach the conclusion that unigram BLEU is not an appropriate measure of image description performance. However, we do find stronger correlations with Smoothed BLEU , skip-bigram ROUGE , and Meteor. In contrast to the results presented here, Reiter and Belz (2009) found no significant correlations of automatic evaluation measures against human judgements of the accuracy of machine-generated weather forecasts. They did, however, find significant correlations of automatic measures against fluency judgements. There are no fluency judgements available for Flickr8K, but Elliott and Keller (2013) report grammaticality judgements for their data, which are comparable to fluency ratings. We failed to find significant correlations between grammatlicality judgements and any of the automatic measures on the Elliott and Keller (2013) data. This discrepancy could be"
P14-2074,D11-1041,0,\N,Missing
P14-2074,P02-1040,0,\N,Missing
P14-2074,P04-1077,0,\N,Missing
P14-2074,W11-2107,0,\N,Missing
P15-1005,P11-2031,0,0.0100733,"single recogniser for “persons”. The BRNN generates descriptions with “man” and “woman”, which leads to higher BLEU scores than our VDR model, but this is based on corpus statistics than the observed visual information. Me4.2 Evaluation Measures We evaluate the generated descriptions using sentence-level Meteor (Denkowski and Lavie, 2011) and BLEU 4 (Papineni et al., 2002), which have been shown to have moderate correlation with humans (Elliott and Keller, 2014). We adopt a jack-knifing evaluation methodology, which enables us to report human–human results (Lin and Och, 2004), using MultEval (Clark et al., 2011). 4.3 Data Sets We perform our experiments on two data sets: Pascal1K and VLT2K. The Pascal1K data set contains 1,000 images sampled from the PASCAL Object Detection Challenge data set (Everingham et al., 2010); each image is paired with five reference descriptions collected from Mechanical Turk. It contains a wide variety of subject matter drawn from the original 20 PASCAL Detection classes. The VLT2K data set contains 2,424 images taken from the trainval 2011 portion of the PASCAL Action Recognition Challenge; each image is paired with three reference descriptions, also collected from Mechan"
P15-1005,W11-2107,0,0.0115941,"l matching between sentences and thus penalises semantically equivalent descriptions. For example, identifying the gender of a person is important for generating a good description. However, object recognizers are not (yet) able to reliably achieve this distinction, and we only have a single recogniser for “persons”. The BRNN generates descriptions with “man” and “woman”, which leads to higher BLEU scores than our VDR model, but this is based on corpus statistics than the observed visual information. Me4.2 Evaluation Measures We evaluate the generated descriptions using sentence-level Meteor (Denkowski and Lavie, 2011) and BLEU 4 (Papineni et al., 2002), which have been shown to have moderate correlation with humans (Elliott and Keller, 2014). We adopt a jack-knifing evaluation methodology, which enables us to report human–human results (Lin and Och, 2004), using MultEval (Clark et al., 2011). 4.3 Data Sets We perform our experiments on two data sets: Pascal1K and VLT2K. The Pascal1K data set contains 1,000 images sampled from the PASCAL Object Detection Challenge data set (Everingham et al., 2010); each image is paired with five reference descriptions collected from Mechanical Turk. It contains a wide vari"
P15-1005,P12-1038,0,0.0819365,"Missing"
P15-1005,P14-2074,1,0.608488,"person is important for generating a good description. However, object recognizers are not (yet) able to reliably achieve this distinction, and we only have a single recogniser for “persons”. The BRNN generates descriptions with “man” and “woman”, which leads to higher BLEU scores than our VDR model, but this is based on corpus statistics than the observed visual information. Me4.2 Evaluation Measures We evaluate the generated descriptions using sentence-level Meteor (Denkowski and Lavie, 2011) and BLEU 4 (Papineni et al., 2002), which have been shown to have moderate correlation with humans (Elliott and Keller, 2014). We adopt a jack-knifing evaluation methodology, which enables us to report human–human results (Lin and Och, 2004), using MultEval (Clark et al., 2011). 4.3 Data Sets We perform our experiments on two data sets: Pascal1K and VLT2K. The Pascal1K data set contains 1,000 images sampled from the PASCAL Object Detection Challenge data set (Everingham et al., 2010); each image is paired with five reference descriptions collected from Mechanical Turk. It contains a wide variety of subject matter drawn from the original 20 PASCAL Detection classes. The VLT2K data set contains 2,424 images taken from"
P15-1005,W14-5403,0,0.0217522,"Missing"
P15-1005,W11-0326,0,0.128222,"Missing"
P15-1005,P08-1032,0,0.06691,"l detections to produce training data. The description of an unseen image is produced by first predicting its VDR over automatically detected objects, and then generating the text with a template-based generation model using the predicted VDR. The performance of our approach is comparable to a state-ofthe-art multimodal deep neural network in images depicting actions. 1 Introduction Humans typically write the text accompanying an image, which is a time-consuming and expensive activity. There are many circumstances in which people are well-suited to this task, such as captioning news articles (Feng and Lapata, 2008) where there are complex relationships between the modalities (Marsh and White, 2003). In this paper we focus on generating literal descriptions, which are rarely found alongside images because they describe what can easily be seen by others (Panofsky, 1939; Shatford, 1986; Hodosh et al., 2013). A computer that can automatically generate these literal descriptions, filling the gap left by humans, may improve access to existing image collections or increase information access for visually impaired users. There has been an upsurge of research in this area, including models that rely on spatial r"
P15-1005,Q14-1017,0,0.0301389,"and in a generative framework over densely-labelled data (Yatskar et al., 2014). The most recent developments have focused on deep learning the relationships between visual feature vectors and word-embeddings with language generation models based on recurrent neural networks or long-short term memory networks (Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Mao et al., 2015; Fang et al., 2015; Donahue et al., 2015; Lebret et al., 2015). An alternative thread of research has focused on directly pairing images with text, based on kCCA (Hodosh et al., 2013) or multimodal deep neural networks (Socher et al., 2014; Karpathy et al., 2014). We revisit the Visual Dependency Representation (Elliott and Keller, 2013, VDR), an intermediate structure that captures the spatial relationships between objects in an image. Spatial context has been shown to be useful in object recognition and naming tasks because humans benefit from the visual world conforming to their expectations (Biederman et al., 1982; Bar and Ullman, 1996). The spatial relationships defined in VDR are closely, but independently, related to cognitively plausible spatial templates (Logan and Sadler, 1996) and region connection calculus (Randell"
P15-1005,N03-1033,0,0.154438,"Missing"
P15-1005,E12-1076,0,0.156708,"h. At training time, we learn a VDR Parsing model from representations that are constructed by searching for the subject and object in the image. The description of an unseen image is generated using a templatebased generation model that leverages the VDR predicted over the top-N objects extracted from an object detector. We evaluate our method for inferring VDRs in an image description experiment on the Pascal1K (Rashtchian et al., 2010) and VL2K data sets (Elliott and Keller, 2013) against two models: the bi-directional recurrent neural network (Karpathy and Fei-Fei, 2015, BRNN) and M IDGE (Mitchell et al., 2012). The main finding is that the quality of the descriptions generated by our method depends on whether the images depict an action. In the VLT2K data set of people performing actions, the performance of our approach is comparable to the BRNN; in the more diverse Pascal1K dataset, the BRNN is substantially better than our method. In a second experiment, we transfer the VDR-based model from the VLT2K data set to the Pascal1K data set without re-training, which improves the descriptions generated in the Pascal1K data set. This suggests that refining how we extract training data may yield further i"
P15-1005,S14-1015,0,0.0349701,"rmatica Amsterdam, The Netherlands elliott@cwi.nl, arjen@acm.org Abstract tionships (Farhadi et al., 2010), corpus-based relationships (Yang et al., 2011), spatial and visual attributes (Kulkarni et al., 2011), n-gram phrase fusion from Web-scale corpora (Li et al., 2011), treesubstitution grammars (Mitchell et al., 2012), selecting and combining phrases from large imagedescription collections (Kuznetsova et al., 2012), using Visual Dependency Representations to capture spatial and corpus-based relationships (Elliott and Keller, 2013), and in a generative framework over densely-labelled data (Yatskar et al., 2014). The most recent developments have focused on deep learning the relationships between visual feature vectors and word-embeddings with language generation models based on recurrent neural networks or long-short term memory networks (Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Mao et al., 2015; Fang et al., 2015; Donahue et al., 2015; Lebret et al., 2015). An alternative thread of research has focused on directly pairing images with text, based on kCCA (Hodosh et al., 2013) or multimodal deep neural networks (Socher et al., 2014; Karpathy et al., 2014). We revisit the Visual Dependency Re"
P15-1005,Q14-1006,0,0.13773,"Missing"
P15-1005,N15-1174,0,0.0506945,"d to the model, we first extract N-candidate objects for the image. The detected objects are then parsed into a VDR structure, which is passed into a template-based language generator to produce a description of the image. alised as a “using”, “typing”, or “working” relationship between the objects. The main limitation of previous research on VDR has been the reliance on gold-standard training annotations, which requires trained annotators. We present the first approach to automatically inferring VDR training examples from natural scenes using only an object detector and an image description. Ortiz et al. (2015) have recently presented an alternative treatment of VDR within the context of abstract scenes and phrasebased machine translation. Figure 1 shows a detailed overview of our approach. At training time, we learn a VDR Parsing model from representations that are constructed by searching for the subject and object in the image. The description of an unseen image is generated using a templatebased generation model that leverages the VDR predicted over the top-N objects extracted from an object detector. We evaluate our method for inferring VDRs in an image description experiment on the Pascal1K (R"
P15-1005,P02-1040,0,0.0940523,"enalises semantically equivalent descriptions. For example, identifying the gender of a person is important for generating a good description. However, object recognizers are not (yet) able to reliably achieve this distinction, and we only have a single recogniser for “persons”. The BRNN generates descriptions with “man” and “woman”, which leads to higher BLEU scores than our VDR model, but this is based on corpus statistics than the observed visual information. Me4.2 Evaluation Measures We evaluate the generated descriptions using sentence-level Meteor (Denkowski and Lavie, 2011) and BLEU 4 (Papineni et al., 2002), which have been shown to have moderate correlation with humans (Elliott and Keller, 2014). We adopt a jack-knifing evaluation methodology, which enables us to report human–human results (Lin and Och, 2004), using MultEval (Clark et al., 2011). 4.3 Data Sets We perform our experiments on two data sets: Pascal1K and VLT2K. The Pascal1K data set contains 1,000 images sampled from the PASCAL Object Detection Challenge data set (Everingham et al., 2010); each image is paired with five reference descriptions collected from Mechanical Turk. It contains a wide variety of subject matter drawn from th"
P15-1005,W10-0721,0,0.0773681,") have recently presented an alternative treatment of VDR within the context of abstract scenes and phrasebased machine translation. Figure 1 shows a detailed overview of our approach. At training time, we learn a VDR Parsing model from representations that are constructed by searching for the subject and object in the image. The description of an unseen image is generated using a templatebased generation model that leverages the VDR predicted over the top-N objects extracted from an object detector. We evaluate our method for inferring VDRs in an image description experiment on the Pascal1K (Rashtchian et al., 2010) and VL2K data sets (Elliott and Keller, 2013) against two models: the bi-directional recurrent neural network (Karpathy and Fei-Fei, 2015, BRNN) and M IDGE (Mitchell et al., 2012). The main finding is that the quality of the descriptions generated by our method depends on whether the images depict an action. In the VLT2K data set of people performing actions, the performance of our approach is comparable to the BRNN; in the more diverse Pascal1K dataset, the BRNN is substantially better than our method. In a second experiment, we transfer the VDR-based model from the VLT2K data set to the Pas"
P15-1005,D11-1041,0,\N,Missing
P15-1005,C14-1012,1,\N,Missing
P15-1005,P04-1077,0,\N,Missing
P15-1005,W10-0707,0,\N,Missing
P15-1005,D13-1128,1,\N,Missing
W14-5417,N12-1094,0,0.0129983,"Attention in the literature ??? Quantity Be as informative as required Do not be more informative than required Do not say what you believe is false Quality Relevance Manner Do not say that for which you lack evidence ??? All models exploit some kind of corpus data to construct descriptions that are maximally probable (Yang et al., 2011; Li et al., 2011; Kuznetsova et al., 2012; Le et al., 2013). These approaches typically use language modelling to construct hypotheses based on the available evidence, but may eventually be false. Be relevant No models try to generate irrelevant descriptions. Dodge et al. (2012) explored the separation between what can be seen/not seen in an image/caption pair. Avoid obscure expressions No model has been deliberately obscure. Avoid ambiguity Kulkarni et al. (2011) introduced visual attributes to describe and distinguish objects. Be brief Be orderly ??? Mitchell et al. (2012) and Elliott and Keller (2013) explicitly try to predict the best ordering of objects in the final description. Table 1: An overview of Grice’s maxims and the relevant image description models. ??? means that we are unaware of any models that implicitly or explicitly claim to address this type of"
W14-5417,D13-1128,1,0.731923,"et al., 2011; Li et al., 2011; Kuznetsova et al., 2012; Le et al., 2013). These approaches typically use language modelling to construct hypotheses based on the available evidence, but may eventually be false. Be relevant No models try to generate irrelevant descriptions. Dodge et al. (2012) explored the separation between what can be seen/not seen in an image/caption pair. Avoid obscure expressions No model has been deliberately obscure. Avoid ambiguity Kulkarni et al. (2011) introduced visual attributes to describe and distinguish objects. Be brief Be orderly ??? Mitchell et al. (2012) and Elliott and Keller (2013) explicitly try to predict the best ordering of objects in the final description. Table 1: An overview of Grice’s maxims and the relevant image description models. ??? means that we are unaware of any models that implicitly or explicitly claim to address this type of maxim. adequate Quantity. It is not clear that current human judgements capture this distinction, yet the goldstandard crowdsourced descriptions almost certainly do conform to the maxim of sufficient Quantity. A further important consideration is how to obtain human judgements for multiple maxims without making the studies prohibi"
W14-5417,D13-1072,0,0.0124433,"r a discussion of the differences between descriptions and captions. 109 Proceedings of the 25th International Conference on Computational Linguistics, pages 109–111, Dublin, Ireland, August 23-29 2014. Category Maxim Attention in the literature ??? Quantity Be as informative as required Do not be more informative than required Do not say what you believe is false Quality Relevance Manner Do not say that for which you lack evidence ??? All models exploit some kind of corpus data to construct descriptions that are maximally probable (Yang et al., 2011; Li et al., 2011; Kuznetsova et al., 2012; Le et al., 2013). These approaches typically use language modelling to construct hypotheses based on the available evidence, but may eventually be false. Be relevant No models try to generate irrelevant descriptions. Dodge et al. (2012) explored the separation between what can be seen/not seen in an image/caption pair. Avoid obscure expressions No model has been deliberately obscure. Avoid ambiguity Kulkarni et al. (2011) introduced visual attributes to describe and distinguish objects. Be brief Be orderly ??? Mitchell et al. (2012) and Elliott and Keller (2013) explicitly try to predict the best ordering of"
W14-5417,W11-0326,0,0.0325876,"dosh et al., 2013) and (Panofsky, 1939) for a discussion of the differences between descriptions and captions. 109 Proceedings of the 25th International Conference on Computational Linguistics, pages 109–111, Dublin, Ireland, August 23-29 2014. Category Maxim Attention in the literature ??? Quantity Be as informative as required Do not be more informative than required Do not say what you believe is false Quality Relevance Manner Do not say that for which you lack evidence ??? All models exploit some kind of corpus data to construct descriptions that are maximally probable (Yang et al., 2011; Li et al., 2011; Kuznetsova et al., 2012; Le et al., 2013). These approaches typically use language modelling to construct hypotheses based on the available evidence, but may eventually be false. Be relevant No models try to generate irrelevant descriptions. Dodge et al. (2012) explored the separation between what can be seen/not seen in an image/caption pair. Avoid obscure expressions No model has been deliberately obscure. Avoid ambiguity Kulkarni et al. (2011) introduced visual attributes to describe and distinguish objects. Be brief Be orderly ??? Mitchell et al. (2012) and Elliott and Keller (2013) expl"
W14-5417,E12-1076,0,0.139632,"maxims of the Cooperative Principle? So far model evaulation has focused on automatic text-based measures, such as Unigram BLEU and human judgements of semantic correctness (see Hodosh et al. (2013) for discussion of framing image description as a ranking task, and Elliott and Keller (2014) for a correlation analysis of text-based measures against human judgements). The semantic correctness judgements task typically present a variant of “Rate the relevance of the description for this image”, which only evaluates the description visa` -vis the maxim of Relevance. One exception is the study of Mitchell et al. (2012), in which judgements about the ordering of noun phrases (the maxim of Manner) were also collected. The importance of being able to evaluate according to multiple maxims becomes clearer as computer vision becomes more accurate. It seems intuitive that a model that describes and relates every object in the image could be characterised as generating Relevant and Quality descriptions, but not necessarily descriptions of This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/ 1 This discussion primarily app"
W14-5417,D11-1041,0,\N,Missing
W14-5417,P14-2074,1,\N,Missing
W14-5417,P12-1038,0,\N,Missing
W16-2346,P14-2074,1,0.681895,"https://github.com/Z -TANG/re-scorer. 10 https://github.com/elliottd/Grounded Translation 11 548 https://github.com/jhclark/multeval 4.2 System submissions that preserved casing or had been tokenised were further processed for lowercasing and detokenisation.12 For all of these preprocessing steps, we used Moses scripts.13 Task 2: Crosslingual Image Description Table 4 presents the final results for the Crosslingual Image Description task. Meteor is the primary evaluation measure because it has been shown to have a much stronger correlation with human judgements than BLEU or TER for this task (Elliott and Keller, 2014). The data for this task was lowercased and had punctuation removed where necessary. The strongest performing constrained submission (LIUM 2 TextNMT C) does not use any visual features. Including multimodal features (i.e., LIUM 2 MultimodalNMT C) results in a 2.8 Meteor drop in performance for that model type. The baseline system 2 GroundedTranslation C outperformed all but these two systems. In general, there is a wide range of performances, and an intriguing discrepancy between Meteor and BLEU rankings. This discrepancy was much larger than the one observed in Task 1, where the overall ranki"
W16-2346,W16-3210,1,0.489205,"Missing"
W16-2346,W15-3001,1,0.437372,"Missing"
W16-2346,N10-1125,0,0.0165056,"he following main goals: • To push existing work on multimodal language processing towards multilingual multimodal language processing. Introduction • To investigate the effectiveness of information from images in machine translation. In recent years, significant research has been done to address problems that require joint modelling of language and vision. Examples of popular applications involving both Natural Language Processing (NLP) and Computer Vision (CV) include image description generation and video captioning (Bernardi et al., 2016), image retrieval based on textual and visual cues (Feng and Lapata, 2010), visual question answering (Yang et al., 2015), among many others (see (Ramisa et al., 2016) for more examples). With very few exceptions (Grubinger et al., 2006; Funaki and Nakayama, 2015; • To investigate the effectiveness of crosslingual textual information in image description generation. The challenge was organised in the framework of the well-established WMT series of shared tasks.1 Participants were called to submit systems focusing on either or both of these task variants. The tasks differ in the training data and in 1 http://www.statmt.org/wmt16/ 543 Proceedings of the First Conferen"
W16-2346,W16-2358,0,0.527348,"Missing"
W16-2346,D15-1070,0,0.0302355,"information from images in machine translation. In recent years, significant research has been done to address problems that require joint modelling of language and vision. Examples of popular applications involving both Natural Language Processing (NLP) and Computer Vision (CV) include image description generation and video captioning (Bernardi et al., 2016), image retrieval based on textual and visual cues (Feng and Lapata, 2010), visual question answering (Yang et al., 2015), among many others (see (Ramisa et al., 2016) for more examples). With very few exceptions (Grubinger et al., 2006; Funaki and Nakayama, 2015; • To investigate the effectiveness of crosslingual textual information in image description generation. The challenge was organised in the framework of the well-established WMT series of shared tasks.1 Participants were called to submit systems focusing on either or both of these task variants. The tasks differ in the training data and in 1 http://www.statmt.org/wmt16/ 543 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 543–553, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics translate Ein brauner Hund ..."
W16-2346,W16-2359,1,0.497561,"Missing"
W16-2346,P11-2031,0,0.0597769,"fter that, an SVM-based model decides which one is better according to the sentence’s score from a language model and the score from the model that generated the sentence. The only difference between the two submissions is that the unconstrained one used Task 1 dataset in the training of text translator. 4 Results Tables 3 and 4 present the official results for the Multimodal Machine Translation and Crosslingual Image Description tasks. We evaluated the submissions based on Meteor (Denkowski and Lavie, 2014) (primary), BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) using MultEval (Clark et al., 2011)11 with default parameters. 4.1 Baseline - GroundedTranslation (Tasks 1 & 2) This method follows (Elliott et al., 2015):10 A source language multimodal RNN model is initialised with a visual feature vector (i.e., a multimodal model for the source language). The final hidden state is then used to initialise a target Task 1 Table 4 shows the final results for the Multimodal Machine Translation task on the official test set, where systems are ranked by their Meteor scores. Meteor, BLEU and TER were computed based on the single reference (human translation) provided for the test set. For Meteor, w"
W16-2346,P16-1227,0,0.269311,"ken classes (Stewart et al., 2014). 4 http://github.com/BVLC/caffe/releases /tag/rc2 5 http://github.com/karpathy/neuraltalk /tree/master/matlab_features_reference 6 https://glosbe.com/en/de/ http://www.crowdflower.com http://illinois.edu/fb/sec/229675 545 ID CMU+NTU CUNI DCU DCU-UVA HUCL IBM-IITM-Montreal-NYU LIUM SHEF UPC UPCb Participating team Carnegie Melon University (Huang et al., 2016) Univerzita Karlova v Praze (Libovick´y et al., 2016) Dublin City University (Hokamp and Calixto, 2016) Dublin City University & Universiteit van Amsterdam (Calixto et al., 2016) Universit¨at Heidelberg (Hitschler et al., 2016) IBM Research India, IIT Madras, Universit´e de Montr´eal & New York University Laboratoire d’Informatique de l’Universit´e du Maine (Caglayan et al., 2016) University of Sheffield (Shah et al., 2016) Universitat Polit`ecnica de Catalunya (Rodr´ıguez Guasch and Costa-juss`a, 2016) Universitat Polit`ecnica de Catalunya Table 2: Participants in the WMT16 multimodal machine translation shared task. DCU (Task 1) Both submissions from DCU are neural MT systems with an attention mechanism on the source-side representation (Bahdanau et al., 2014). The first submission is text-only, and the second sub"
W16-2346,W14-3348,0,0.307313,", 2014). The other one is generated based on the image feature using method proposed in (Vinyals et al., 2015). After that, an SVM-based model decides which one is better according to the sentence’s score from a language model and the score from the model that generated the sentence. The only difference between the two submissions is that the unconstrained one used Task 1 dataset in the training of text translator. 4 Results Tables 3 and 4 present the official results for the Multimodal Machine Translation and Crosslingual Image Description tasks. We evaluated the submissions based on Meteor (Denkowski and Lavie, 2014) (primary), BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) using MultEval (Clark et al., 2011)11 with default parameters. 4.1 Baseline - GroundedTranslation (Tasks 1 & 2) This method follows (Elliott et al., 2015):10 A source language multimodal RNN model is initialised with a visual feature vector (i.e., a multimodal model for the source language). The final hidden state is then used to initialise a target Task 1 Table 4 shows the final results for the Multimodal Machine Translation task on the official test set, where systems are ranked by their Meteor scores. Meteor, BLEU and TE"
W16-2346,P16-1159,0,0.00608079,"Missing"
W16-2346,W16-2360,0,0.440474,"Missing"
W16-2346,2006.amta-papers.25,0,0.156429,"proposed in (Vinyals et al., 2015). After that, an SVM-based model decides which one is better according to the sentence’s score from a language model and the score from the model that generated the sentence. The only difference between the two submissions is that the unconstrained one used Task 1 dataset in the training of text translator. 4 Results Tables 3 and 4 present the official results for the Multimodal Machine Translation and Crosslingual Image Description tasks. We evaluated the submissions based on Meteor (Denkowski and Lavie, 2014) (primary), BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) using MultEval (Clark et al., 2011)11 with default parameters. 4.1 Baseline - GroundedTranslation (Tasks 1 & 2) This method follows (Elliott et al., 2015):10 A source language multimodal RNN model is initialised with a visual feature vector (i.e., a multimodal model for the source language). The final hidden state is then used to initialise a target Task 1 Table 4 shows the final results for the Multimodal Machine Translation task on the official test set, where systems are ranked by their Meteor scores. Meteor, BLEU and TER were computed based on the single reference (human translation) prov"
W16-2346,P07-2045,0,0.0201639,"five descriptions of the same image in the target language, created independently from the corresponding source description (image description variant). The data used for both tasks is an extended version of the Flickr30K dataset. Participants were also allowed to use external data and resources for unconstrained submissions. Participants were encouraged to make use of both the sentences and the images as part of their submissions but they were not required to do so. The baseline systems for the translation task were a text-only Moses phrase-based statistical machine translation (SMT) model (Koehn et al., 2007) and the GroundedTranslation multilingual image description model (Elliott et al., 2015) (in particular, the MLM→LM variant). The baseline system for the description generation task was also the GroundedTranslation model. In this paper we describe the data, image features and participants of the shared task (Sections 2 and 3), present its main findings (Section 4), and discuss interesting issues and directions for future research (Section 5). 2 courage participation we released two types of features extracted from the images. The use of such features was not mandatory, and participants could a"
W16-2346,2014.amta-researchers.3,0,0.0922404,"Missing"
W16-2346,W16-2361,0,0.218958,"Missing"
W16-2346,P02-1040,0,0.10153,"1) The approach integrates separate attention mechanisms over the source language and the CONV5,4 visual features in a single decoder. The source language was represented using a bidirectional RNN with Gated Recurrent Units (GRU); the images were represented as 196x512 matrix from the pre-trained VGG-19 convolutional network. A separate, time546 VGG16 deep convolutional model (Simonyan and Zisserman, 2015), supplied by the task organisers) for retrieval of matches. The retrieval model architecture was identical to that in Hitschler et al. (2016). Instead of TF-IDF, a modified version of BLEU (Papineni et al., 2002) was used in order to re-score hypotheses based on the target-language text of retrieved captions. Fixed settings were used for some parameters (d = 90, b = 0.01 and km = 20), while kr and λ were optimised on the validation set (parameters as defined in (Hitschler et al., 2016)). LIUM 1 MosesNMTRnnLMSent2Vec C and LIUM 1 MosesNMTRnnLMSent2VecVGGFC7 C are phrase-based systems based on Moses (14 standard features plus operation sequence models. They include re-scoring with several models and more particularly with a continuous space language model (CSLM) and a neural MT system (see TextNMT syste"
W16-2346,Q14-1006,0,0.409641,"a regionbased convolution neural network (RCNN) are designed to be appended in the head/tail of the textual feature or dissipated in parallel long short term memory (LSTM) threads to assist the LSTM reader in computing a representation. For rescoring, an additional bilingual dictionary is used to select the best sentence from candidates generated by five different models. The submission is thus unconstrained, with the German-English Dictionary from GLOSBE6 used as additional resource. Datasets and image features We created a new dataset for the shared task by extending the Flickr30K dataset (Young et al., 2014) into another language. The Multi30K dataset (Elliott et al., 2016) contains two types of multilingual data: a corpus of English sentences translated into German (used for Task 1), and a corpus of independently collected English and German sentences (used for Task 2). For the translation corpus, one sentence (of five) was chosen for professional translation such that the final dataset is a combination of short, medium, and long length sentences. The second corpus consists of crowdsourced descriptions gathered from Crowdflower,2 where each worker generated an independent description of the imag"
W16-2346,W16-2363,1,0.301031,"uilt using only the shared task data. This is a remarkable result, given the size of the dataset: 29,000 parallel segments. They all use additional features to re-rank the k-best output of a text-only phrase-based system, including visual features, although these seem to play a minor role and lead to only marginally better results. Submissions based on the output of a Moses translation model – like the main baseline (1 en-de-Moses C) – have very similar Meteor scores. In fact, SHEF 1 en-de-Moses-rerank C and CMU+NTU 1 MNMT+RERANK U are not considered significantly different from this baseline Shah et al. (2016) provide some analysis on the differences between SHEF 1 en-de-Mosesrerank C and 1 en-de-Moses C. They show that the output of these systems differ in 260 out of the 1,000 segments. However, despite differences in the actual translations, the Meteor scores for many of these cases may be the same/close. Disappointingly, truly multimodal systems, which in most cases use neural MT approaches (e.g. CUNI 1 MMS2S-1 C, DCU 1 min-riskmultimodal C) do not fare as well as the text-only SMT systems (or those followed by multimodalbased translation rescoring), except when additional resources are used for"
W16-2346,N12-1017,0,\N,Missing
W16-2346,P10-4002,0,\N,Missing
W16-2346,W16-2362,0,\N,Missing
W16-2359,W14-4012,0,0.170546,"Missing"
W16-2359,W14-3348,0,0.118285,"Throughout, we parameterise our models using 300D word embeddings, 1000D hidden states, and 1000D context vectors; the source and target languages are estimated over the entire vocabularies. Our non-recurrent matrices are initialised by sampling from a Gaussian distribution (µ = 0, σ = 0.01), recurrent matrices are orthogonal and bias vectors are all initialised to zero. We apply dropout on the source language words (encoder) and before the readout operation (decoder) with probability of 0.3 and apply no other regularisation. We apply early stopping for model selection based on Meteor scores (Denkowski and Lavie, 2014), and if it has not increased for 20 epochs on a validation set, training is halted. The models are trained using the Adadelta optimizer (Zeiler, 2012) with an initial learning rate of 0.005. In Table 1 we compare our models — CONV5,4 and FC7 -Multimodal NMT — against a text-only, attention-based NMT baseline, the Moses translation baseline (Koehn et al., 2007) and the multilingual image description baseline (Elliott et al., 2015). First, it is clear that the Moses SMT baseline is very strong, given that it is only trained over the parallel text without any visual information. Our models are u"
W16-2359,W16-3210,1,0.369703,"Missing"
W16-2359,P07-2045,0,0.0172284,"it captures the image patches that are relevant to the current state of the decoder and for generating the next word. 3 Experiments We report results for Task 1, which uses the translated data in the Multi30K corpus (Elliott et al., 2016). English and German descriptions in the Multi30K were normalised and tokenized, and compounds in German descriptions were further split in a pre-processing step1 . (6) i where fscore uses all image annotation vectors a, the decoder previous hidden state st−1 and the pre1 We use the scripts in the Moses SMT Toolkit to normalise, tokenize and split compounds (Koehn et al., 2007). 636 against a text-only NMT model (2.9 points). Our results indicate that incorporating image features in multimodal models helps, as compared to our text-only NMT baseline. Even though our neural models — both text-only and multimodal models — fall short of the SMT baseline performance, we believe that the use of neural architectures for this task is more principled, due to the ability to incorporate images and translations in one network that is trained end-to-end. Meteor Moses CONV5,4 -Multimodal NMT FC7 -Multimodal NMT Text-only Attention NMT Elliott et al. (2015) 52.3 46.4 44.1 43.5 24."
W16-3207,P98-1013,0,0.142307,"Missing"
W16-3207,P09-1068,0,0.0338899,"Missing"
W16-3207,W16-3210,1,0.896283,"Missing"
W16-3207,D08-1048,0,0.0244628,"Missing"
W16-3207,Q14-1006,0,0.6518,"e used our categorization to manually annotate sentences containing negations in the Flickr30k corpus, with an agreement score of κ=0.67. With this paper, we hope to open up a broader discussion of subjective language in image descriptions. 1 (1) a. Queen Elizabeth isn’t wearing a dress b. ?? Queen Elizabeth isn’t wearing jeans Introduction Descriptions of images are typically collected from untrained workers via crowdsourcing platforms, such as Mechanical Turk1 . The workers are explicitly instructed to describe only what they can see in the image, in an attempt to control content selection (Young et al., 2014; Chen et al., 2015). However, workers are still free to project their world view when writing the descriptions and they make linguistic choices, such as using negation structures (van Miltenburg, 2016). In this paper we study the use of negations in image descriptions. A negation is a word that communicates that something is not the case. Negations are often used when there is a mismatch between what speakers expect to be the case and what is actually the case (see e.g. (Leech, 1983; Beukeboom et al., 2010)). For example, if Queen Elizabeth of England were to appear in public wearing jeans in"
W16-3207,C98-1013,0,\N,Missing
W16-3210,P05-1033,0,0.0128193,"relationship between the sentences in different languages. In the translated corpus, we know there is a strong correspondence between the sentences in both languages. In the descriptions corpus, we only know that the sentences, regardless of the language, are supposed to describe the same image. A dataset of images paired with sentences in multiple languages broadens the scope of multimodal NLP research. Image description with multilingual data can also be seen as machine translation in a multimodal context. This opens up new avenues for researchers in machine translation (Koehn et al., 2003; Chiang, 2005; Sutskever et al., 2014; Bahdanau et al., 2015, inter-alia) to work with multilingual multimodal data. Image– sentence ranking using monolingual multimodal datasets (Hodosh et al., 2013, inter-alia) is also a natural task for multilingual modelling. The only existing datasets of images paired with multilingual sentences were created by professionally translating English into the target language: IAPR-TC12 with 20,000 English-German described images (Grubinger et al., 2006), and the Pascal Sentences Dataset of 1,000 JapaneseEnglish described images (Funaki and Nakayama, 2015). Multi30K dataset"
W16-3210,D13-1128,1,0.045395,"Missing"
W16-3210,D15-1070,0,0.217749,"ranslation (Koehn et al., 2003; Chiang, 2005; Sutskever et al., 2014; Bahdanau et al., 2015, inter-alia) to work with multilingual multimodal data. Image– sentence ranking using monolingual multimodal datasets (Hodosh et al., 2013, inter-alia) is also a natural task for multilingual modelling. The only existing datasets of images paired with multilingual sentences were created by professionally translating English into the target language: IAPR-TC12 with 20,000 English-German described images (Grubinger et al., 2006), and the Pascal Sentences Dataset of 1,000 JapaneseEnglish described images (Funaki and Nakayama, 2015). Multi30K dataset is larger than both of these and contains both independent and translated sentences. We hope this dataset will be of broad interest across NLP and CV research and anticipate that these communities will put the data to use in a broader range of tasks than we can foresee. 70 Proceedings of the 5th Workshop on Vision and Language, pages 70–74, c Berlin, Germany, August 12 2016. 2016 Association for Computational Linguistics 1. Brick layers constructing a wall. 1. The two men on the scaffolding are helping to build a red brick wall. 2. Maurer bauen eine Wand. 2. Zwei Mauerer mau"
W16-3210,P16-1227,0,0.103727,"slation Machine translation is typically performed using only textual data, for example news data, the Europarl corpora, or corpora harvested from the Web (CommonCrawl, Wikipedia, etc.). The Multi30K dataset makes it possible to further develop ma73 chine translation in a setting where multimodal data, such as images or video, are observed alongside text. The potential advantages of using multimodal information for machine translation include the ability to better deal with ambiguous source text and to avoid (untranslated) out-of-vocabulary words in the target language (Calixto et al., 2012). Hitschler and Riezler (2016) have demonstrated the potential of multimodal features in a targetside translation reranking model. Their approach is initially trained over large text-only translation copora and then fine-tuned with a small amount of in-domain data, such as our dataset. We expect a variety of translation models can be adapted to take advantage of multimodal data as features in a translation model or as feature vectors in neural machine translation models. 4 Conclusions We introduced Multi30K: a large-scale multilingual multimodal dataset for interdisciplinary machine learning research. Our dataset is an ext"
W16-3210,N03-1017,0,0.00435756,"hese corpora is the relationship between the sentences in different languages. In the translated corpus, we know there is a strong correspondence between the sentences in both languages. In the descriptions corpus, we only know that the sentences, regardless of the language, are supposed to describe the same image. A dataset of images paired with sentences in multiple languages broadens the scope of multimodal NLP research. Image description with multilingual data can also be seen as machine translation in a multimodal context. This opens up new avenues for researchers in machine translation (Koehn et al., 2003; Chiang, 2005; Sutskever et al., 2014; Bahdanau et al., 2015, inter-alia) to work with multilingual multimodal data. Image– sentence ranking using monolingual multimodal datasets (Hodosh et al., 2013, inter-alia) is also a natural task for multilingual modelling. The only existing datasets of images paired with multilingual sentences were created by professionally translating English into the target language: IAPR-TC12 with 20,000 English-German described images (Grubinger et al., 2006), and the Pascal Sentences Dataset of 1,000 JapaneseEnglish described images (Funaki and Nakayama, 2015). Mu"
W16-3210,W10-0721,0,0.0981994,"Missing"
W16-3210,W16-2346,1,0.477489,"Missing"
W16-3210,Q14-1006,0,0.793454,"s crowdsourced independently of the original English descriptions. We describe the data and outline how it can be used for multilingual image description and multimodal machine translation, but we anticipate the data will be useful for a broader range of tasks. 1 Introduction Image description is one of the core challenges at the intersection of Natural Language Processing (NLP) and Computer Vision (CV) (Bernardi et al., 2016). This task has only received attention in a monolingual English setting, helped by the availability of English datasets, e.g. Flickr8K (Hodosh et al., 2013), Flickr30K (Young et al., 2014), and MS COCO (Chen et al., 2015). However, the possible applications of image description are useful for all languages, such as searching for images using natural language, or providing alternativedescription text for visually impaired Web users. We introduce a large-scale dataset of images paired with sentences in English and German as an initial step towards studying the value and the characteristics of multilingual-multimodal data1 . 1 The dataset is freely available under the Creative Commons Attribution NonCommercial ShareAlike 4.0 International license from http://www.statmt.org/wmt 16/"
W16-3210,W10-0707,0,\N,Missing
W17-3503,W16-6615,0,0.0854802,"Missing"
W17-3503,W16-3210,1,0.897799,"Missing"
W17-3503,P16-1227,0,0.0306585,"of crowd workers with the subjects of the images has a noticeable influence on description specificity. 1 1. Task design effects: There are many possible approaches to collecting descriptions of images. Previous research has re-used the Flickr8K (Hodosh et al., 2013) template and methodology. Baltaretu and Castro Ferreira (2016) showed that task design may influence the form of crowd-sourced descriptions. Introduction Vision and language researchers have started to collect image description corpora for languages other than English, e.g. Chinese (Li et al., 2016), German (Elliott et al., 2016; Hitschler et al., 2016; Rajendran et al., 2016), Japanese (Miyazaki and Shimizu, 2016; Yoshikawa et al., 2017), French (Rajendran et al., 2016), and Turkish (Unal et al., 2016). The main aim of those efforts is to develop image description systems for non-English languages and to explore the related problems of cross-lingual image description (Elliott et al., 2015; Miyazaki and Shimizu, 2016) and machine translation in a visual context (Specia et al., 2016; Hitschler et al., 2016). We view these new corpora as sociological data that is in itself worth studying. Our research stems from the following question: To wha"
W17-3503,P14-2135,0,0.0310482,"e looking wood trailer is parked in a street in front of stores. c. An unusual looking vehicle parked in front of some stores. This example illustrates two strategies the crowd may use to provide descriptions for unfamiliar objects: (1) signal the unfamiliarity of the object using adjectives like strange and unusual looking. This is similar to the finding by Miyazaki and Shimizu (2016) that the Japanese crowd made frequent use of terms like foreign and overseas for the Western images from MS COCO. (2) use a more general cover term, like vehicle. Such terms may have a higher visual dispersion (Kiela et al., 2014), but they provide a safe back-off strategy. 27 Sports We found that unfamiliarity with different kinds of sports leads to the misclassification of those sports. We focus on three sports: American Football, Rugby, and Soccer. Looking at images for these sports, we compared how the three different groups referred to them. We found that the German and Dutch groups patterned together, deviating from the American crowd workers. As expected, the Dutch and German workers make the most mistakes categorizing American Football. For all seven pictures of American Football, there is at least one Dutch an"
W17-3503,P16-1168,0,0.463929,"ceable influence on description specificity. 1 1. Task design effects: There are many possible approaches to collecting descriptions of images. Previous research has re-used the Flickr8K (Hodosh et al., 2013) template and methodology. Baltaretu and Castro Ferreira (2016) showed that task design may influence the form of crowd-sourced descriptions. Introduction Vision and language researchers have started to collect image description corpora for languages other than English, e.g. Chinese (Li et al., 2016), German (Elliott et al., 2016; Hitschler et al., 2016; Rajendran et al., 2016), Japanese (Miyazaki and Shimizu, 2016; Yoshikawa et al., 2017), French (Rajendran et al., 2016), and Turkish (Unal et al., 2016). The main aim of those efforts is to develop image description systems for non-English languages and to explore the related problems of cross-lingual image description (Elliott et al., 2015; Miyazaki and Shimizu, 2016) and machine translation in a visual context (Specia et al., 2016; Hitschler et al., 2016). We view these new corpora as sociological data that is in itself worth studying. Our research stems from the following question: To what extent do speakers of different languages differ in their des"
W17-3503,N16-1021,0,0.0555208,"e subjects of the images has a noticeable influence on description specificity. 1 1. Task design effects: There are many possible approaches to collecting descriptions of images. Previous research has re-used the Flickr8K (Hodosh et al., 2013) template and methodology. Baltaretu and Castro Ferreira (2016) showed that task design may influence the form of crowd-sourced descriptions. Introduction Vision and language researchers have started to collect image description corpora for languages other than English, e.g. Chinese (Li et al., 2016), German (Elliott et al., 2016; Hitschler et al., 2016; Rajendran et al., 2016), Japanese (Miyazaki and Shimizu, 2016; Yoshikawa et al., 2017), French (Rajendran et al., 2016), and Turkish (Unal et al., 2016). The main aim of those efforts is to develop image description systems for non-English languages and to explore the related problems of cross-lingual image description (Elliott et al., 2015; Miyazaki and Shimizu, 2016) and machine translation in a visual context (Specia et al., 2016; Hitschler et al., 2016). We view these new corpora as sociological data that is in itself worth studying. Our research stems from the following question: To what extent do speakers of d"
W17-3503,W16-2346,1,0.903232,"Missing"
W17-3503,W16-3207,1,0.71946,"Missing"
W17-3503,P17-2066,0,0.135224,"ion specificity. 1 1. Task design effects: There are many possible approaches to collecting descriptions of images. Previous research has re-used the Flickr8K (Hodosh et al., 2013) template and methodology. Baltaretu and Castro Ferreira (2016) showed that task design may influence the form of crowd-sourced descriptions. Introduction Vision and language researchers have started to collect image description corpora for languages other than English, e.g. Chinese (Li et al., 2016), German (Elliott et al., 2016; Hitschler et al., 2016; Rajendran et al., 2016), Japanese (Miyazaki and Shimizu, 2016; Yoshikawa et al., 2017), French (Rajendran et al., 2016), and Turkish (Unal et al., 2016). The main aim of those efforts is to develop image description systems for non-English languages and to explore the related problems of cross-lingual image description (Elliott et al., 2015; Miyazaki and Shimizu, 2016) and machine translation in a visual context (Specia et al., 2016; Hitschler et al., 2016). We view these new corpora as sociological data that is in itself worth studying. Our research stems from the following question: To what extent do speakers of different languages differ in their descriptions of the same 2."
W17-3503,Q14-1006,0,0.706336,"claims about the last three factors based on the workers’ language and geolocations. We believe that studying differences between languages shows us which phenomena are robust across languages and thus important to consider when implementing and deploying models. Also, differences between languages can inform us about the feasibility of approaches to image description in different languages by translating existing English data (Li et al., 2016; Yoshikawa et al., 2017). Our analysis combines quantitative and qualitative studies of a trilingual corpus of described images. We use the Flickr30K (Young et al., 2014) for English, Multi30K for German (Elliott et al., 2016), and a new corpus of Dutch descriptions (Section 3). We build on earlier work that studies the semantic and pragmatic properties of English descriptions (van Miltenburg, 2016; van Miltenburg et al., 2016). Those works study ethnicity marking, negation marking, and unwarranted inferences about the roles of people. The main finding of our analysis is that all of these properties are stable across Dutch, US English, and German (Section 4). We also show how differences in background knowledge can affect description specificity (Section 5). W"
W17-4718,W17-4746,1,0.668714,"Missing"
W17-4718,D17-1105,0,0.401049,"Missing"
W17-4718,W17-4749,0,0.112115,"Missing"
W17-4718,W14-3348,0,0.128767,"yer from a ResNet-50 (He et al., 2016) convolutional neural network trained on the ImageNet dataset (Russakovsky et al., 2015). Those feature maps provide spatial information on which the model focuses through the attention mechanism. 4 Text-similarity Metric Results The submissions were evaluated against either professional or crowd-sourced references. All submissions and references were pre-processed to lowercase, normalise punctuation, and tokenise the sentences using the Moses scripts.3 The evaluation was performed using MultEval (Clark et al., 2011) with the primary metric of Meteor 1.5 (Denkowski and Lavie, 2014). We also report the results using BLEU (Papineni et al., 2002) and 220 3 https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/ TER (Snover et al., 2006) metrics. The winning submissions are indicated by •. These are the topscoring submissions and those that are not significantly different (based on Meteor scores) according the approximate randomisation test (with p-value ≤ 0.05) provided by MultEval. Submissions marked with * are not significantly different from the Baseline according to the same test. 4.1 4.1.1 Task 1: English → German Multi30K 2017 test data Table 4 shows the resu"
W17-4718,W17-4748,0,0.0838327,"Missing"
W17-4718,1983.tc-1.13,0,0.273609,"Missing"
W17-4718,P14-2074,1,0.832004,"Missing"
W17-4718,W16-3210,1,0.583063,"tion at test time. The training data, however, consists of images with independent descriptions in both source and target languages. Introduction The Shared Task on Multimodal Translation and Multilingual Image Description tackles the problem of generating descriptions of images for languages other than English. The vast majority of image description research has focused on Englishlanguage description due to the abundance of crowdsourced resources (Bernardi et al., 2016). However, there has been a significant amount of recent work on creating multilingual image description datasets in German (Elliott et al., 2016; Hitschler et al., 2016; Rajendran et al., 2016), Turkish (Unal et al., 2016), Chinese (Li et al., 2016), Japanese (Miyazaki and Shimizu, 2016; Yoshikawa et al., 2017), and Dutch (van Miltenburg et al., 2017). Progress on this problem will be useful for native-language image search, multilingual ecommerce, and audio-described video for visually impaired viewers. The first empirical results for multimodal translation showed the potential for visual context to The translation task has been extended to include a new language, French. This extension means the Multi30K dataset (Elliott et al., 201"
W17-4718,N16-1022,0,0.0283339,"authors selected 1,000 images from the collection to form the dataset for the Multimodal Translation task based on a manual inspection of the English descriptions. Professional German translations were collected for those 1,000 English-described images. The remaining 1,071 images were used for the Multilingual Image Description task. We collected five ad2 http://www.crowdflower.com As a secondary evaluation dataset for the Multimodal Translation task, we collected and translated a set of image descriptions that potentially contain ambiguous verbs. We based our selection on the VerSe dataset (Gella et al., 2016), which annotates a subset of the COCO (Lin et al., 2014) and TUHOI (Le et al., 2014) images with OntoNotes senses for 90 verbs which are ambiguous, e.g. play. Their goals were to test the feasibility of annotating images with the word sense of a given verb (rather than verbs themselves) and to provide a gold-labelled dataset for evaluating automatic visual sense disambiguation methods. Altogether, the VerSe dataset contains 3,518 images, but we limited ourselves to its COCO section, since for our purposes we also need the image descriptions, which are not available in TUHOI. The COCO portion"
W17-4718,P16-1227,0,0.0406008,"training data, however, consists of images with independent descriptions in both source and target languages. Introduction The Shared Task on Multimodal Translation and Multilingual Image Description tackles the problem of generating descriptions of images for languages other than English. The vast majority of image description research has focused on Englishlanguage description due to the abundance of crowdsourced resources (Bernardi et al., 2016). However, there has been a significant amount of recent work on creating multilingual image description datasets in German (Elliott et al., 2016; Hitschler et al., 2016; Rajendran et al., 2016), Turkish (Unal et al., 2016), Chinese (Li et al., 2016), Japanese (Miyazaki and Shimizu, 2016; Yoshikawa et al., 2017), and Dutch (van Miltenburg et al., 2017). Progress on this problem will be useful for native-language image search, multilingual ecommerce, and audio-described video for visually impaired viewers. The first empirical results for multimodal translation showed the potential for visual context to The translation task has been extended to include a new language, French. This extension means the Multi30K dataset (Elliott et al., 2016) is now triple aligned"
W17-4718,W17-4750,0,0.0641413,"Missing"
W17-4718,P16-4010,0,0.0212154,"Missing"
W17-4718,P07-2045,0,0.0170989,"etail in (Calixto et al., 2017b). They are model IMGW , in which image features are used as words in the source-language encoder; model IMGE , where image features are used to initialise the hidden states of the forward and backward encoder RNNs; and model IMGD , where the image features are used as additional signals to initialise the decoder hidden state. Each image has one corresponding feature vector, obtained from the activations of the NICT (Task 1) These are constrained submissions for both language pairs. First, a hierarchical phrase-based (HPB) translation system s built using Moses (Koehn et al., 2007) with standard features. Then, an attentional encoder-decoder network (Bahdanau et al., 2015) is trained and used as an additional feature to rerank the n-best output of the HPB system. A unimodal NMT model is also trained to integrate visual information. Instead of integrating visual features into the NMT model directly, image retrieval methods are employed to obtain target language descriptions of images that are similar to the image described by the source sentence, and this target description information is integrated into the NMT model. A multimodal 219 NMT model is also used to rerank th"
W17-4718,W14-5403,0,0.0178273,"l Translation task based on a manual inspection of the English descriptions. Professional German translations were collected for those 1,000 English-described images. The remaining 1,071 images were used for the Multilingual Image Description task. We collected five ad2 http://www.crowdflower.com As a secondary evaluation dataset for the Multimodal Translation task, we collected and translated a set of image descriptions that potentially contain ambiguous verbs. We based our selection on the VerSe dataset (Gella et al., 2016), which annotates a subset of the COCO (Lin et al., 2014) and TUHOI (Le et al., 2014) images with OntoNotes senses for 90 verbs which are ambiguous, e.g. play. Their goals were to test the feasibility of annotating images with the word sense of a given verb (rather than verbs themselves) and to provide a gold-labelled dataset for evaluating automatic visual sense disambiguation methods. Altogether, the VerSe dataset contains 3,518 images, but we limited ourselves to its COCO section, since for our purposes we also need the image descriptions, which are not available in TUHOI. The COCO portion covers 82 verbs; we further discarded verbs that are unambiguous in the dataset, i.e."
W17-4718,P17-2031,0,0.221406,"Missing"
W17-4718,D15-1166,0,0.00458431,"models - as measured by BLEU on the Multi30K development set - was used for both the constrained and unconstrained submissions. SHEF (Task 1) The SHEF systems utilize the predicted posterior probability distribution over the image object classes as image features. To do so, they make use of the pre-trained ResNet-152 (He et al., 2016), a deep CNN based image network that is trained over the 1,000 object categories on the Imagenet dataset (Deng et al., 2009) to obtain the posterior distribution. The model follows a standard encoder-decoder NMT approach using softdot attention as described in (Luong et al., 2015). It explores image information in three ways: a) to initialize the encoder; b) to initialize the decoder; c) to condition each source word with the image class posteriors. In all these three ways, non-linear affine transformations over the posteriors are used as image features. Baseline — Task 1 The baseline system for the multimodal translation task is a text-only neural machine translation system built with the Nematus toolkit (Sennrich et al., 2017). Most settings and hyperparameters were kept as default, with a few exceptions: batch size of 40 (instead of 80 due to memory constraints) and"
W17-4718,W17-4751,0,0.0653545,"Missing"
W17-4718,2006.amta-papers.25,0,0.119578,"ation on which the model focuses through the attention mechanism. 4 Text-similarity Metric Results The submissions were evaluated against either professional or crowd-sourced references. All submissions and references were pre-processed to lowercase, normalise punctuation, and tokenise the sentences using the Moses scripts.3 The evaluation was performed using MultEval (Clark et al., 2011) with the primary metric of Meteor 1.5 (Denkowski and Lavie, 2014). We also report the results using BLEU (Papineni et al., 2002) and 220 3 https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/ TER (Snover et al., 2006) metrics. The winning submissions are indicated by •. These are the topscoring submissions and those that are not significantly different (based on Meteor scores) according the approximate randomisation test (with p-value ≤ 0.05) provided by MultEval. Submissions marked with * are not significantly different from the Baseline according to the same test. 4.1 4.1.1 Task 1: English → German Multi30K 2017 test data Table 4 shows the results on the Multi30K 2017 test data with a German target language. It interesting to note that the metrics do not fully agree on the ranking of systems, although th"
W17-4718,W17-4752,1,0.893716,"Missing"
W17-4718,W16-2346,1,0.498931,"Missing"
W17-4718,P16-1168,0,0.0742711,"Missing"
W17-4718,P03-1021,0,0.033669,"re to rerank the n-best output of the HPB system. A unimodal NMT model is also trained to integrate visual information. Instead of integrating visual features into the NMT model directly, image retrieval methods are employed to obtain target language descriptions of images that are similar to the image described by the source sentence, and this target description information is integrated into the NMT model. A multimodal 219 NMT model is also used to rerank the HPB output. All feature weights (including the standard features, the NMT feature and the multimodal NMT feature) were tuned by MERT (Och, 2003). On the development set, the NMT feature improved the HPB system significantly. However, the multimodal NMT feature did not further improve the HPB system that had integrated the NMT feature. OREGONSTATE (Task 1) The OREGONSTATE system uses a very simple but effective model which feeds the image information to both encoder and decoder. On the encoder side, the image representation was used as an initialization information to generate the source words’ representations. This step strengthens the relatedness between image’s and source words’ representations. Additionally, the decoder uses alignm"
W17-4718,tiedemann-2012-parallel,0,0.032402,"and generates more accurate target side sentence. UvA-TiCC (Task 1) The submitted systems are Imagination models (Elliott and K´ad´ar, 2017), which are trained to perform two tasks in a multitask learning framework: a) produce the target sentence, and b) predict the visual feature vector of the corresponding image. The constrained models are trained over only the 29,000 training examples in the Multi30K dataset with a source-side vocabulary of 10,214 types and a target-side vocabulary of 16,022 types. The unconstrained models are trained over a concatenation of the Multi30K, News Commentary (Tiedemann, 2012) parallel texts, and MS COCO (Chen et al., 2015) dataset with a joint source-target vocabulary of 17,597 word pieces (Schuster and Nakajima, 2012). In both constrained and unconstrained submissions, the models were trained to predict the 2048D GoogleLeNetV3 feature vector (Szegedy et al., 2015) of an image associated with a source language sentence. The output of an ensemble of the three best randomly initialized models - as measured by BLEU on the Multi30K development set - was used for both the constrained and unconstrained submissions. SHEF (Task 1) The SHEF systems utilize the predicted po"
W17-4718,P02-1040,0,0.113412,"trained on the ImageNet dataset (Russakovsky et al., 2015). Those feature maps provide spatial information on which the model focuses through the attention mechanism. 4 Text-similarity Metric Results The submissions were evaluated against either professional or crowd-sourced references. All submissions and references were pre-processed to lowercase, normalise punctuation, and tokenise the sentences using the Moses scripts.3 The evaluation was performed using MultEval (Clark et al., 2011) with the primary metric of Meteor 1.5 (Denkowski and Lavie, 2014). We also report the results using BLEU (Papineni et al., 2002) and 220 3 https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/ TER (Snover et al., 2006) metrics. The winning submissions are indicated by •. These are the topscoring submissions and those that are not significantly different (based on Meteor scores) according the approximate randomisation test (with p-value ≤ 0.05) provided by MultEval. Submissions marked with * are not significantly different from the Baseline according to the same test. 4.1 4.1.1 Task 1: English → German Multi30K 2017 test data Table 4 shows the results on the Multi30K 2017 test data with a German target languag"
W17-4718,W17-3503,1,0.863632,"Missing"
W17-4718,P17-2066,0,0.0342394,"Multimodal Translation and Multilingual Image Description tackles the problem of generating descriptions of images for languages other than English. The vast majority of image description research has focused on Englishlanguage description due to the abundance of crowdsourced resources (Bernardi et al., 2016). However, there has been a significant amount of recent work on creating multilingual image description datasets in German (Elliott et al., 2016; Hitschler et al., 2016; Rajendran et al., 2016), Turkish (Unal et al., 2016), Chinese (Li et al., 2016), Japanese (Miyazaki and Shimizu, 2016; Yoshikawa et al., 2017), and Dutch (van Miltenburg et al., 2017). Progress on this problem will be useful for native-language image search, multilingual ecommerce, and audio-described video for visually impaired viewers. The first empirical results for multimodal translation showed the potential for visual context to The translation task has been extended to include a new language, French. This extension means the Multi30K dataset (Elliott et al., 2016) is now triple aligned, with English descriptions translated into both German and French. The description generation task has substantially changed since last year. T"
W17-4718,Q14-1006,0,0.339227,". In last year’s Crosslingual Image Description task, the aim was to produce a single target language description, given five source language descriptions and the image. In this year’s Multilingual Image Description task, participants received only an unseen image at test time, without source language descriptions. 2.2 En: A group of people are eating noddles. De: Eine Gruppe von Leuten isst Nudeln. Fr: Un groupe de gens mangent des nouilles. Datasets The Multi30K dataset (Elliott et al., 2016) is the primary dataset for the shared task. It contains 31K images originally described in English (Young et al., 2014) with two types of multilingual data: a collection of professionally translated German sentences, and a collection of independently crowdsourced German descriptions. This year the Multi30K dataset has been extended with new evaluation data for the Translation and Image Description tasks, and an additional language for the Translation task. In addition, we released a new evaluation dataset featuring ambiguities that we expected would benefit from visual context. Table 1 presents an overview of the new evaluation datasets. Figure 1 shows an example of an image with an aligned English-German-Fren"
W17-4718,W16-2323,0,0.0161023,"urce word with the image class posteriors. In all these three ways, non-linear affine transformations over the posteriors are used as image features. Baseline — Task 1 The baseline system for the multimodal translation task is a text-only neural machine translation system built with the Nematus toolkit (Sennrich et al., 2017). Most settings and hyperparameters were kept as default, with a few exceptions: batch size of 40 (instead of 80 due to memory constraints) and ADAM as optimizer. In order to handle rare and OOV words, we used the Byte Pair Encoding Compression Algorithm to segment words (Sennrich et al., 2016b). The merge operations for word segmentation were learned using training data in both source and target languages. These were then applied to all training, validation and test sets in both source and target languages. In post-processing, the original words were restored by concatenating the subwords. Baseline — Task 2 The baseline for the multilingual image description task is an attention-based image description system trained over only the German image descriptions (Caglayan et al., 2017b). The visual representation are extracted from the so-called res4f relu layer from a ResNet-50 (He et"
W17-4718,W17-4753,0,0.0938137,"sh component of the model is ignored and only German captions are ID AFRL-OHIOSTATE Participating team Air Force Research Laboratory & Ohio State University (Duselis et al., 2017) CMU Carnegie Melon University (Jaffe, 2017) CUNI Univerzita Karlova v Praze (Helcl and Libovick´y, 2017) DCU-ADAPT Dublin City University (Calixto et al., 2017a) LIUMCVC Laboratoire d’Informatique de l’Universit´e du Maine & Universitat Autonoma de Barcelona Computer Vision Center (Caglayan et al., 2017a) NICT National Institute of Information and Communications Technology & Nara Institute of Science and Technology (Zhang et al., 2017) OREGONSTATE SHEF UvA-TiCC Oregon State University (Ma et al., 2017) University of Sheffield (Madhyastha et al., 2017) Universiteit van Amsterdam & Tilburg University (Elliott and K´ad´ar, 2017) Table 3: Participants in the WMT17 multimodal machine translation shared task. generated. FC7 layer of the VGG19 network, and consist of a 4096D real-valued vector that encode information about the entire image. CUNI (Tasks 1 and 2) For Task 1, the submissions employ the standard neural MT (NMT) scheme enriched with another attentive encoder for the input image. It uses a hierarchical attention combina"
W17-4718,P16-1162,0,0.00319904,"urce word with the image class posteriors. In all these three ways, non-linear affine transformations over the posteriors are used as image features. Baseline — Task 1 The baseline system for the multimodal translation task is a text-only neural machine translation system built with the Nematus toolkit (Sennrich et al., 2017). Most settings and hyperparameters were kept as default, with a few exceptions: batch size of 40 (instead of 80 due to memory constraints) and ADAM as optimizer. In order to handle rare and OOV words, we used the Byte Pair Encoding Compression Algorithm to segment words (Sennrich et al., 2016b). The merge operations for word segmentation were learned using training data in both source and target languages. These were then applied to all training, validation and test sets in both source and target languages. In post-processing, the original words were restored by concatenating the subwords. Baseline — Task 2 The baseline for the multilingual image description task is an attention-based image description system trained over only the German image descriptions (Caglayan et al., 2017b). The visual representation are extracted from the so-called res4f relu layer from a ResNet-50 (He et"
W17-4718,E17-3017,0,0.0333329,"Missing"
W17-4718,P11-2031,0,\N,Missing
W17-4718,W17-4747,0,\N,Missing
W17-4718,I17-1014,1,\N,Missing
W17-4718,N16-1021,0,\N,Missing
W18-6402,W18-6438,1,0.861948,"Missing"
W18-6402,P11-2031,0,0.0245715,"5e−5 and a batch size of 64. The input embedding dimensionality was set to 128 and the remainder of the hyperparameters were kept as default. Bite-pair encoding with 10,000 merge operations was used for all language pairs. For Task 1b, only the English-Czech portion of the training corpus is used. 4 Automatic Metric Results The submissions were evaluated against either professional or crowd-sourced references. All submissions and references were pre-processed to lowercase, normalise punctuation, and tokenise the sentences using the Moses scripts.5 The evaluation was performed using MultEval (Clark et al., 2011) with the primary metric of Meteor 1.5 (Denkowski and Lavie, 2014). We also report the results using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) metrics. The winning submissions are indicated by •. These are the topscoring submissions and those that are not significantly different (based on Meteor scores) according the approximate randomisation test (with p-value ≤ 0.05) provided by MultEval. Submissions marked with * are not significantly different from the Baseline according to the same test. 4.1 Task 1: English → German Table 6 shows the results on the Test 2018 dataset with"
W18-6402,W18-6439,0,0.0587398,"Missing"
W18-6402,W14-3348,0,0.193691,"ality was set to 128 and the remainder of the hyperparameters were kept as default. Bite-pair encoding with 10,000 merge operations was used for all language pairs. For Task 1b, only the English-Czech portion of the training corpus is used. 4 Automatic Metric Results The submissions were evaluated against either professional or crowd-sourced references. All submissions and references were pre-processed to lowercase, normalise punctuation, and tokenise the sentences using the Moses scripts.5 The evaluation was performed using MultEval (Clark et al., 2011) with the primary metric of Meteor 1.5 (Denkowski and Lavie, 2014). We also report the results using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) metrics. The winning submissions are indicated by •. These are the topscoring submissions and those that are not significantly different (based on Meteor scores) according the approximate randomisation test (with p-value ≤ 0.05) provided by MultEval. Submissions marked with * are not significantly different from the Baseline according to the same test. 4.1 Task 1: English → German Table 6 shows the results on the Test 2018 dataset with a German target language. The first observation is that the best-p"
W18-6402,N13-1073,0,0.0242655,"t is the set of correct lexical translations of aw in the target language that conform to the context i. A word is said to be ambiguous in the source language if it has multiple translations (as given in the Multi30K corpus) with different meanings. We prepared the evaluation dataset following the procedure described in Lala and Specia (2018), with some additional steps. First, the parallel text in the Multi30K training and the validation sets are decompounded with SECOS (Riedl and Biemann, 2016) (for German only) and lemmatised3 . Second, we perform automatic word alignment using fast align (Dyer et al., 2013) to identify the English words that are aligned to two or more different words in the target language. This step results in a dictionary of {key : val} pairs, where key is a potentially ambiguous English word, and val is the set of words in the target language that align to key. This dictionary is then filtered by humans, students of translation studies who are fluent in both the source and target languages, to remove incorrect/noisy alignments and unambiguous instances, resulting in a cleaned dictionary containing {aw : lt} pairs, where aw is an ambiguous English word, and lt is the set of le"
W18-6402,W17-4718,1,0.509969,"using the image itself and its English description. This task can be addressed as either a pure translation task from the source English descriptions (ignoring the corresponding image), or as a multimodal translation task where the translation process is guided by the image in addition to the source description. Initial results in this area showed the potential for visual context to improve translation quality (Elliott et al., 2015; Hitschler et al., 2016). This was followed by a wide range of work in the first two editions of this shared task at the WMT in 2016 and 2017 (Specia et al., 2016; Elliott et al., 2017). This year we challenged participants to target the task of multimodal translation, with two variants: Task 1 is identical to previous editions of the shared task, however, it now includes an additional Czech target language. Therefore, participants can submit translations to any of the following languages: German, French and Czech. This extension means the Multi30K dataset (Elliott et al., 2016) is now 5-way aligned, with images described in English, which are translated into German, French and Czech.1 Task 1b is similar to Task 1; the main difference is that multiple source languages can be"
W18-6402,W16-3210,1,0.812877,"ation quality (Elliott et al., 2015; Hitschler et al., 2016). This was followed by a wide range of work in the first two editions of this shared task at the WMT in 2016 and 2017 (Specia et al., 2016; Elliott et al., 2017). This year we challenged participants to target the task of multimodal translation, with two variants: Task 1 is identical to previous editions of the shared task, however, it now includes an additional Czech target language. Therefore, participants can submit translations to any of the following languages: German, French and Czech. This extension means the Multi30K dataset (Elliott et al., 2016) is now 5-way aligned, with images described in English, which are translated into German, French and Czech.1 Task 1b is similar to Task 1; the main difference is that multiple source languages can be used (simultaneously) and Czech is the only target language. We introduce two new evaluation sets that extend the existing Multi30K dataset: a set of 1071 English sentences and their corresponding images and translations for Task 1, and 1,000 translations for the 2017 test set into Czech for Task 1b. Another new feature of this year’s shared task is the introduction of a new evaluation metric: Le"
W18-6402,I17-1014,1,0.823606,"Missing"
W18-6402,W18-6440,0,0.0330665,"Missing"
W18-6402,W18-6441,0,0.117332,"Missing"
W18-6402,P16-1227,0,0.120596,"ges, all parallel. Introduction The Shared Task on Multimodal Machine Translation tackles the problem of generating a description of an image in a target language using the image itself and its English description. This task can be addressed as either a pure translation task from the source English descriptions (ignoring the corresponding image), or as a multimodal translation task where the translation process is guided by the image in addition to the source description. Initial results in this area showed the potential for visual context to improve translation quality (Elliott et al., 2015; Hitschler et al., 2016). This was followed by a wide range of work in the first two editions of this shared task at the WMT in 2016 and 2017 (Specia et al., 2016; Elliott et al., 2017). This year we challenged participants to target the task of multimodal translation, with two variants: Task 1 is identical to previous editions of the shared task, however, it now includes an additional Czech target language. Therefore, participants can submit translations to any of the following languages: German, French and Czech. This extension means the Multi30K dataset (Elliott et al., 2016) is now 5-way aligned, with images desc"
W18-6402,P18-4020,0,0.0218896,"Missing"
W18-6402,P07-2045,0,0.00543567,"le 4: Statistics of dataset used for the LTA evaluation after human filtering. and their submission identifiers. AFRL-OHIO-STATE (Task 1) The AFRL-OHIO-STATE team builds on their previous year Visual Machine Translation (VMT) submission by combining it with text-only translation models. Two types of models were submitted: AFRL-OHIO-STATE 1 2IMPROVE U is a system combination of the VMT system and an instantiation of a Marian NMT model (Junczys-Dowmunt et al., 2018), and AFRL-OHIOSTATE 1 4COMBO U is a systems combination of the VMT system along with instantiations of Marian, OpenNMT, and Moses (Koehn et al., 2007). CUNI (Task 1) The CUNI submissions use two architectures based on the self-attentive Transformer model (Vaswani et al., 2017). For German and Czech, a language model is used to extract pseudo-in307 ID AFRL-OHIOSTATE CUNI LIUMCVC MeMAD OSU-BAIDU SHEF UMONS Participating team Air Force Research Laboratory & Ohio State University (Gwinnup et al., 2018) Univerzita Karlova v Praze (Helcl et al., 2018) Laboratoire d’Informatique de l’Universit´e du Maine & Universitat Autonoma de Barcelona Computer Vision Center (Caglayan et al., 2018) Aalto University, Helsinki University & EURECOM (Gr¨onroos et"
W18-6402,W18-6442,1,0.871581,"Missing"
W18-6402,L18-1602,1,0.908032,"ment and 2018 test datasets. The figures correspond to tuples with an image and parallel sentences in four languages: English, German, French and Czech. Dataset for LTA representing an instance in the test set, aw is an ambiguous word in English found in that instance i, and clt is the set of correct lexical translations of aw in the target language that conform to the context i. A word is said to be ambiguous in the source language if it has multiple translations (as given in the Multi30K corpus) with different meanings. We prepared the evaluation dataset following the procedure described in Lala and Specia (2018), with some additional steps. First, the parallel text in the Multi30K training and the validation sets are decompounded with SECOS (Riedl and Biemann, 2016) (for German only) and lemmatised3 . Second, we perform automatic word alignment using fast align (Dyer et al., 2013) to identify the English words that are aligned to two or more different words in the target language. This step results in a dictionary of {key : val} pairs, where key is a potentially ambiguous English word, and val is the set of words in the target language that align to key. This dictionary is then filtered by humans, st"
W18-6402,L16-1147,0,0.0341753,"nput image size for convolutional feature extraction process and found that multimodal attention without L2 normalisation performs significantly worse than baseline NMT. MeMAD (Task 1) The MeMAD team adapts the Transformer neural machine translation architecture to a multimodal setting. They use global image features extracted from Detectron (Girshick et al., 2018), a pre-trained object detection and localisation neural network, and two additional training corpora: MS-COCO (Lin et al., 2014) (an English multimodal dataset, which they extend with synthetic multilingual data) and OpenSubtitles (Lison and Tiedemann, 2016) (a multilingual, text-only dataset). Their experiments show that the effect of the visual features in the system is small; the largest differences in quality amongst the systems tested is attributed to the quality of the underlying text-only neural MT system. OSU-BAIDU (Tasks 1 and 1b) For Task 1, the OREGONSTATE system ensembles models including some neural machine translation models which only consider text information and multimodal machine translation models which also consider image information. Both types of models use global attention mechanism to align source to target words. For the"
W18-6402,P02-1040,0,0.10187,"kept as default. Bite-pair encoding with 10,000 merge operations was used for all language pairs. For Task 1b, only the English-Czech portion of the training corpus is used. 4 Automatic Metric Results The submissions were evaluated against either professional or crowd-sourced references. All submissions and references were pre-processed to lowercase, normalise punctuation, and tokenise the sentences using the Moses scripts.5 The evaluation was performed using MultEval (Clark et al., 2011) with the primary metric of Meteor 1.5 (Denkowski and Lavie, 2014). We also report the results using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) metrics. The winning submissions are indicated by •. These are the topscoring submissions and those that are not significantly different (based on Meteor scores) according the approximate randomisation test (with p-value ≤ 0.05) provided by MultEval. Submissions marked with * are not significantly different from the Baseline according to the same test. 4.1 Task 1: English → German Table 6 shows the results on the Test 2018 dataset with a German target language. The first observation is that the best-performing system, MeMAD 1 FLICKR DE MeMAD-OpenNMTmmod U, is sub"
W18-6402,D17-1120,0,0.0202937,"ates are re-ranked using word sense disambiguation (WSD) approaches: (i) most frequency sense (MFS), (ii) lexical translation (LT) and, (iii) multimodal lexical translation (MLT). Models (i) and (ii) are baselines, whilst MLT is a novel multimodal cross-lingual WSD model. The main idea is to have the cross-lingual WSD model select the translation candidate which correctly disambiguates ambiguous words in the source sentence and the intuition is that the image could help in the disambiguation process. The re-ranking cross-lingual WSD models are based on neural sequence learning models for WSD (Raganato et al., 2017; Yuan et al., 2016) trained on the Multimodal Lexical Translation Dataset (Lala and Specia, 2018). More specifically, they train LSTMs as taggers to disambiguate/translate every word in the source sentence. For Task 1b, the SHEF team explores three approaches. The first approach takes the concatenation of the 10-best translation candidates of German-Czech, French-Czech and English-Czech neural MT systems and then re-ranks them using the same multimodal cross-lingual WSD model as in Task 1. The second approach explores consensus between the different 10-best lists. The best hypothesis is selec"
W18-6402,N16-1075,0,0.024076,"taset for LTA representing an instance in the test set, aw is an ambiguous word in English found in that instance i, and clt is the set of correct lexical translations of aw in the target language that conform to the context i. A word is said to be ambiguous in the source language if it has multiple translations (as given in the Multi30K corpus) with different meanings. We prepared the evaluation dataset following the procedure described in Lala and Specia (2018), with some additional steps. First, the parallel text in the Multi30K training and the validation sets are decompounded with SECOS (Riedl and Biemann, 2016) (for German only) and lemmatised3 . Second, we perform automatic word alignment using fast align (Dyer et al., 2013) to identify the English words that are aligned to two or more different words in the target language. This step results in a dictionary of {key : val} pairs, where key is a potentially ambiguous English word, and val is the set of words in the target language that align to key. This dictionary is then filtered by humans, students of translation studies who are fluent in both the source and target languages, to remove incorrect/noisy alignments and unambiguous instances, resulti"
W18-6402,2006.amta-papers.25,0,0.106552,"ing with 10,000 merge operations was used for all language pairs. For Task 1b, only the English-Czech portion of the training corpus is used. 4 Automatic Metric Results The submissions were evaluated against either professional or crowd-sourced references. All submissions and references were pre-processed to lowercase, normalise punctuation, and tokenise the sentences using the Moses scripts.5 The evaluation was performed using MultEval (Clark et al., 2011) with the primary metric of Meteor 1.5 (Denkowski and Lavie, 2014). We also report the results using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) metrics. The winning submissions are indicated by •. These are the topscoring submissions and those that are not significantly different (based on Meteor scores) according the approximate randomisation test (with p-value ≤ 0.05) provided by MultEval. Submissions marked with * are not significantly different from the Baseline according to the same test. 4.1 Task 1: English → German Table 6 shows the results on the Test 2018 dataset with a German target language. The first observation is that the best-performing system, MeMAD 1 FLICKR DE MeMAD-OpenNMTmmod U, is substantially better than other s"
W18-6402,W16-2346,1,0.724118,"Missing"
W18-6402,P14-5003,0,0.0511421,"Missing"
W18-6402,Q14-1006,0,0.277424,"rs, pages 304–323 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64029 the accuracy of a system at translating correctly a subset of ambiguous source language words. Participants could submit both constrained (shared task data only) and unconstrained (any data) systems for both tasks, with a limit of two systems per task variant and language pair per team. 2 Datasets The Multi30K dataset (Elliott et al., 2016) is the primary resource for the shared task. It contains 31K images originally described in English (Young et al., 2014) with two types of multilingual data: a collection of professionally translated German sentences, and a collection of independently crowdsourced German descriptions. Over the two last years, we have extended the Multi30K dataset with 2,071 new images and two additional languages for the translation task: French and Czech. Table 1 presents an overview of the new evaluation datasets. Figure 1 shows an example of an image with an aligned EnglishGerman-French-Czech description. This year we also released a new version of the evaluation datasets featuring a subset of sentences that contain ambiguou"
W18-6402,W18-6443,0,0.0405294,"Missing"
W18-6550,P16-1054,0,0.0374173,"Missing"
W18-6550,W14-3348,0,0.0247587,"cription systems: Flickr30K and MS COCO (Young et al., 2014; Lin et al., 2014). Both of these data sets contain images with multiple crowd-sourced descriptions per image. These datasets are typically used to train data-driven natural language generation systems to automatically learn to associate visual features with natural language descriptions (Bernardi et al., 2016). Following the training phase, image description systems are evaluated by comparing their output with human generated descriptions for the same image (using textual similarity metrics like BLEU or METEOR, Papineni et al. 2002; Denkowski and Lavie 2014). The standard for what the image descriptions should look like is implicit in the corpus. The only point at which any explicit guidelines are provided is during the crowd-sourcing task, where annotators are given general instructions about what their description should look like. Here are the Flickr30K instructions (the MS COCO These guidelines leave much of the task open for interpretation by the annotator. For example, it is unclear how the descriptions will be used, or what the target audience is (as pointed out by van Miltenburg et al. 2017). Thus, the underspecified nature of the task in"
W18-6550,L18-1525,0,0.0389422,"Missing"
W18-6550,W17-3503,1,0.906527,"Missing"
W18-6550,C18-1147,1,0.888614,"Missing"
W18-6550,W16-3207,1,0.907843,"Missing"
W18-6550,P02-1040,0,0.10116,"evaluate automatic description systems: Flickr30K and MS COCO (Young et al., 2014; Lin et al., 2014). Both of these data sets contain images with multiple crowd-sourced descriptions per image. These datasets are typically used to train data-driven natural language generation systems to automatically learn to associate visual features with natural language descriptions (Bernardi et al., 2016). Following the training phase, image description systems are evaluated by comparing their output with human generated descriptions for the same image (using textual similarity metrics like BLEU or METEOR, Papineni et al. 2002; Denkowski and Lavie 2014). The standard for what the image descriptions should look like is implicit in the corpus. The only point at which any explicit guidelines are provided is during the crowd-sourcing task, where annotators are given general instructions about what their description should look like. Here are the Flickr30K instructions (the MS COCO These guidelines leave much of the task open for interpretation by the annotator. For example, it is unclear how the descriptions will be used, or what the target audience is (as pointed out by van Miltenburg et al. 2017). Thus, the underspec"
W18-6550,Q14-1006,0,0.168407,"Although this gives us a rich and diverse collection of data to work with, it also introduces uncertainty about how the world should be described. This paper shows the extent of this uncertainty in the PEOPLE domain. We present a taxonomy of different ways to talk about other people. This taxonomy serves as a reference point to think about how other people should be described, and can be used to classify and compute statistics about labels applied to people. 1 Introduction There are currently two major data sets used to train and evaluate automatic description systems: Flickr30K and MS COCO (Young et al., 2014; Lin et al., 2014). Both of these data sets contain images with multiple crowd-sourced descriptions per image. These datasets are typically used to train data-driven natural language generation systems to automatically learn to associate visual features with natural language descriptions (Bernardi et al., 2016). Following the training phase, image description systems are evaluated by comparing their output with human generated descriptions for the same image (using textual similarity metrics like BLEU or METEOR, Papineni et al. 2002; Denkowski and Lavie 2014). The standard for what the image"
