2020.acl-demos.19,D18-1547,0,0.234895,"Missing"
2020.acl-demos.19,P19-1360,0,0.06182,"b-2 provides a template-based method and SC-LSTM (Wen et al., 2015). 2.3.1 CamRest676 CamRest676 (Wen et al., 2017) is a Wizard-of-Oz dataset, consisting of 676 dialogues in a restaurant domain. ConvLab-2 offers an agenda-based user simulator and a complete set of models for building a traditional pipeline dialogue system on the CamRest676 dataset. 2.2.6 Word-level Policy Word-level policy directly generates a natural language response (rather than dialogue acts) according to the dialogue history and the belief state. ConvLab-2 integrates three models: MDRG (Budzianowski et al., 2018a), HDSA (Chen et al., 2019), and LaRL (Zhao et al., 2019). MDRG is the baseline model proposed by Budzianowski et al. (2018b) on MultiWOZ, while HDSA and LaRL achieve much stronger performance on this dataset. 2.2.7 User Policy User policy is the core of a user simulator. It takes a pre-set user goal and system dialogue acts as input and outputs user dialogue acts. ConvLab-2 provides an agenda-based (Schatzmann et al., 2007) model and neural network-based models including HUS and its variational variants (G¨ur et al., 2018). To perform end-to-end simulation, researchers can equip the user policy with NLU and NLG compone"
2020.acl-demos.19,N19-1423,0,0.0163653,"ntegrated models in ConvLab-2 are marked in bold. Researchers can easily add their models by implementing the interface of the corresponding component. We will keep adding state-of-the-art models to reflect the latest progress in task-oriented dialogue. 2.2.1 Natural Language Understanding The natural language understanding (NLU) component, which is used to parse the other agent’s intent, takes an utterance as input and outputs the corresponding dialogue acts. ConvLab-2 provides three models: Semantic Tuple Classifier (STC) (Mairesse et al., 2009), MILU (Lee et al., 2019b), and BERTNLU. BERT (Devlin et al., 2019) has shown strong performance in many NLP tasks. Thus, ConvLab-2 proposes a new BERTNLU model. BERTNLU adds two MLPs on top of BERT for intent classification and slot tagging, respectively, and fine-tunes all parameters on the specified tasks. BERTNLU achieves the best performance on MultiWOZ in comparison with other models. 2.2.2 Dialogue State Tracking The dialogue state tracking (DST) component updates the belief state, which contains the constraints and requirements of the other agent (such as a user). ConvLab-2 provides a rule-based tracker that takes dialogue acts parsed by the NLU as in"
2020.acl-demos.19,W14-4337,0,0.245946,"Missing"
2020.acl-demos.19,P19-1546,0,0.189107,"l Intelligence, † State Key Lab of Intelligent Technology and Systems, † Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China ‡ Microsoft Research, Redmond, USA † ‡ {zhu-q18,z-zhang15,fangy17,gxly19}@mails.tsinghua.edu.cn {jincli,bapeng,jfgao}@microsoft.com † {zxy-dcs,aihuang}@tsinghua.edu.cn Abstract We present ConvLab-2, an open-source toolkit that enables researchers to build task-oriented dialogue systems with state-of-the-art models, perform an end-to-end evaluation, and diagnose the weakness of systems. As the successor of ConvLab (Lee et al., 2019b), ConvLab2 inherits ConvLab’s framework but integrates more powerful dialogue models and supports more datasets. Besides, we have developed an analysis tool and an interactive tool to assist researchers in diagnosing dialogue systems. The analysis tool presents rich statistics and summarizes common mistakes from simulated dialogues, which facilitates error analysis and system improvement. The interactive tool provides a user interface that allows developers to diagnose an assembled dialogue system by interacting with the system and modifying the output of each system component. 1 Figure 1: F"
2020.acl-demos.19,P19-3011,1,0.903958,"Missing"
2020.acl-demos.19,P18-1133,0,0.311994,"improvement. The interactive tool provides a user interface that allows developers to diagnose an assembled dialogue system by interacting with the system and modifying the output of each system component. 1 Figure 1: Framework of ConvLab-2. The top block shows different approaches to build a dialogue system. Introduction Task-oriented dialogue systems are gaining increasing attention in recent years, resulting in a number of datasets (Henderson et al., 2014; Wen et al., 2017; Budzianowski et al., 2018b; Rastogi et al., 2019) and a wide variety of models (Wen et al., 2015; Peng et al., 2017; Lei et al., 2018; Wu et al., 2019; Gao et al., 2019). However, very few opensource toolkits provide full support to assembling an end-to-end dialogue system with state-of-the-art models, evaluating the performance in an end-toend fashion, and analyzing the bottleneck both qualitatively and quantitatively. To fill the gap, we have developed ConvLab-2 based on our previous dialogue system platform ConvLab (Lee et al., 2019b). ConvLab-2 inherits its predecessor’s framework and extend it by integrating many recently proposed state-of-the-art dialogue models. In addition, ∗ Corresponding author. two powerful tools"
2020.acl-demos.19,D17-1259,0,0.47465,"20. 2020 Association for Computational Linguistics have state-of-the-art models integrated. ConvLab (Lee et al., 2019b) is the first toolkit that provides various powerful models for all dialogue components and allows researchers to quickly assemble a complete dialogue system (using a set of recipes). ConvLab-2 inherits the flexible framework of ConvLab and imports recently proposed models that achieve state-of-the-art performance. In addition, ConvLab-2 supports several large-scale dialogue datasets including CamRest676 (Wen et al., 2017), MultiWOZ (Budzianowski et al., 2018b), DealOrNoDeal (Lewis et al., 2017), and CrossWOZ (Zhu et al., 2020). To support end-to-end evaluation, ConvLab-2 provides user simulators for automatic evaluation and integrates Amazon Mechanical Turk for human evaluation, similar to ConvLab. Moreover, it provides an analysis tool and a human-machine interactive tool for diagnosing a dialogue system. Researchers can perform quantitative analysis using the analysis tool. It presents useful statistics extracted from the conversations between the user simulator and the dialogue system. This information helps reveal the weakness of the system and signifies the direction for furthe"
2020.acl-demos.19,D17-2014,0,0.0314309,"rating many recently proposed state-of-the-art dialogue models. In addition, ∗ Corresponding author. two powerful tools, namely the analysis tool and the interactive tool, are provided for in-depth error analysis. ConvLab-2 will be the development platform for Multi-domain Task-oriented Dialog Challenge II track in the 9th Dialog System Technology Challenge (DSTC9)1 . As shown in Figure 1, there are many approaches to building a task-oriented dialogue system, ranging from pipeline methods with multiple components to fully end-to-end models. Previous toolkits focus on either end-to-end models (Miller et al., 2017) or one specific component such as dialogue policy (POL) (Ultes et al., 2017), while the others toolkits that are designed for developers (Bocklisch et al., 2017; Papangelis et al., 2020) do not 1 https://sites.google.com/dstc. community/dstc9/home 142 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 142–149 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics have state-of-the-art models integrated. ConvLab (Lee et al., 2019b) is the first toolkit that provides various powerful models for all dialogue components and allows rese"
2020.acl-demos.19,D17-1237,1,0.845901,"analysis and system improvement. The interactive tool provides a user interface that allows developers to diagnose an assembled dialogue system by interacting with the system and modifying the output of each system component. 1 Figure 1: Framework of ConvLab-2. The top block shows different approaches to build a dialogue system. Introduction Task-oriented dialogue systems are gaining increasing attention in recent years, resulting in a number of datasets (Henderson et al., 2014; Wen et al., 2017; Budzianowski et al., 2018b; Rastogi et al., 2019) and a wide variety of models (Wen et al., 2015; Peng et al., 2017; Lei et al., 2018; Wu et al., 2019; Gao et al., 2019). However, very few opensource toolkits provide full support to assembling an end-to-end dialogue system with state-of-the-art models, evaluating the performance in an end-toend fashion, and analyzing the bottleneck both qualitatively and quantitatively. To fill the gap, we have developed ConvLab-2 based on our previous dialogue system platform ConvLab (Lee et al., 2019b). ConvLab-2 inherits its predecessor’s framework and extend it by integrating many recently proposed state-of-the-art dialogue models. In addition, ∗ Corresponding author."
2020.acl-demos.19,P18-2069,0,0.077924,"Missing"
2020.acl-demos.19,N07-2038,0,0.404849,"rectly generates a natural language response (rather than dialogue acts) according to the dialogue history and the belief state. ConvLab-2 integrates three models: MDRG (Budzianowski et al., 2018a), HDSA (Chen et al., 2019), and LaRL (Zhao et al., 2019). MDRG is the baseline model proposed by Budzianowski et al. (2018b) on MultiWOZ, while HDSA and LaRL achieve much stronger performance on this dataset. 2.2.7 User Policy User policy is the core of a user simulator. It takes a pre-set user goal and system dialogue acts as input and outputs user dialogue acts. ConvLab-2 provides an agenda-based (Schatzmann et al., 2007) model and neural network-based models including HUS and its variational variants (G¨ur et al., 2018). To perform end-to-end simulation, researchers can equip the user policy with NLU and NLG components to assemble a complete user simulator. 2.2.8 End-to-end Model A fully end-to-end dialogue model receives the dialogue history and generates a response in natural language directly. ConvLab-2 extends Sequicity (Lei et al., 2018) to multi-domain scenarios: when the model senses that the current domain has switched, it resets the belief span, which records information of the current domain. ConvLa"
2020.acl-demos.19,D19-1010,1,0.86197,", 2019a), and TRADE (Wu et al., 2019). TRADE generates the belief state 143 from utterances using a copy mechanism and achieves state-of-the-art performance on MultiWOZ. the DealOrNoDeal dataset, we provide the ROLLOUTS RL policy proposed by Lewis et al. (2017). 2.2.4 Dialogue Policy Dialogue policy receives the belief state and outputs system dialogue acts. ConvLab-2 provides a rule-based policy, a simple neural policy that learns directly from the corpus using imitation learning, and reinforcement learning policies including REINFORCE (Williams, 1992), PPO (Schulman et al., 2017), and GDPL (Takanobu et al., 2019). GDPL achieves state-of-the-art performance on MultiWOZ. Compared with ConvLab, ConvLab-2 can integrate a new dataset more conveniently. For each dataset, ConvLab-2 provides a unified data loader that can be used by all the models, thus separating data processing from the model definition. Currently, ConvLab-2 supports four task-oriented dialogue datasets, including CamRest676 (Wen et al., 2017), MultiWOZ (Eric et al., 2019), DealOrNoDeal (Lewis et al., 2017), and CrossWOZ (Zhu et al., 2020). 2.2.5 Natural Language Generation The natural language generation (NLG) component transforms dialogue"
2020.acl-demos.19,P17-4013,0,0.0308827,"Corresponding author. two powerful tools, namely the analysis tool and the interactive tool, are provided for in-depth error analysis. ConvLab-2 will be the development platform for Multi-domain Task-oriented Dialog Challenge II track in the 9th Dialog System Technology Challenge (DSTC9)1 . As shown in Figure 1, there are many approaches to building a task-oriented dialogue system, ranging from pipeline methods with multiple components to fully end-to-end models. Previous toolkits focus on either end-to-end models (Miller et al., 2017) or one specific component such as dialogue policy (POL) (Ultes et al., 2017), while the others toolkits that are designed for developers (Bocklisch et al., 2017; Papangelis et al., 2020) do not 1 https://sites.google.com/dstc. community/dstc9/home 142 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 142–149 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics have state-of-the-art models integrated. ConvLab (Lee et al., 2019b) is the first toolkit that provides various powerful models for all dialogue components and allows researchers to quickly assemble a complete dialogue system (using a set of recipe"
2020.acl-demos.19,D15-1199,0,0.102409,"Missing"
2020.acl-demos.19,E17-1042,0,0.288565,"Missing"
2020.acl-demos.19,P19-1078,0,0.112045,"interactive tool provides a user interface that allows developers to diagnose an assembled dialogue system by interacting with the system and modifying the output of each system component. 1 Figure 1: Framework of ConvLab-2. The top block shows different approaches to build a dialogue system. Introduction Task-oriented dialogue systems are gaining increasing attention in recent years, resulting in a number of datasets (Henderson et al., 2014; Wen et al., 2017; Budzianowski et al., 2018b; Rastogi et al., 2019) and a wide variety of models (Wen et al., 2015; Peng et al., 2017; Lei et al., 2018; Wu et al., 2019; Gao et al., 2019). However, very few opensource toolkits provide full support to assembling an end-to-end dialogue system with state-of-the-art models, evaluating the performance in an end-toend fashion, and analyzing the bottleneck both qualitatively and quantitatively. To fill the gap, we have developed ConvLab-2 based on our previous dialogue system platform ConvLab (Lee et al., 2019b). ConvLab-2 inherits its predecessor’s framework and extend it by integrating many recently proposed state-of-the-art dialogue models. In addition, ∗ Corresponding author. two powerful tools, namely the anal"
2020.acl-demos.19,N19-1123,0,0.131839,"method and SC-LSTM (Wen et al., 2015). 2.3.1 CamRest676 CamRest676 (Wen et al., 2017) is a Wizard-of-Oz dataset, consisting of 676 dialogues in a restaurant domain. ConvLab-2 offers an agenda-based user simulator and a complete set of models for building a traditional pipeline dialogue system on the CamRest676 dataset. 2.2.6 Word-level Policy Word-level policy directly generates a natural language response (rather than dialogue acts) according to the dialogue history and the belief state. ConvLab-2 integrates three models: MDRG (Budzianowski et al., 2018a), HDSA (Chen et al., 2019), and LaRL (Zhao et al., 2019). MDRG is the baseline model proposed by Budzianowski et al. (2018b) on MultiWOZ, while HDSA and LaRL achieve much stronger performance on this dataset. 2.2.7 User Policy User policy is the core of a user simulator. It takes a pre-set user goal and system dialogue acts as input and outputs user dialogue acts. ConvLab-2 provides an agenda-based (Schatzmann et al., 2007) model and neural network-based models including HUS and its variational variants (G¨ur et al., 2018). To perform end-to-end simulation, researchers can equip the user policy with NLU and NLG components to assemble a complete use"
2020.acl-demos.19,2020.tacl-1.19,1,0.84554,"nal Linguistics have state-of-the-art models integrated. ConvLab (Lee et al., 2019b) is the first toolkit that provides various powerful models for all dialogue components and allows researchers to quickly assemble a complete dialogue system (using a set of recipes). ConvLab-2 inherits the flexible framework of ConvLab and imports recently proposed models that achieve state-of-the-art performance. In addition, ConvLab-2 supports several large-scale dialogue datasets including CamRest676 (Wen et al., 2017), MultiWOZ (Budzianowski et al., 2018b), DealOrNoDeal (Lewis et al., 2017), and CrossWOZ (Zhu et al., 2020). To support end-to-end evaluation, ConvLab-2 provides user simulators for automatic evaluation and integrates Amazon Mechanical Turk for human evaluation, similar to ConvLab. Moreover, it provides an analysis tool and a human-machine interactive tool for diagnosing a dialogue system. Researchers can perform quantitative analysis using the analysis tool. It presents useful statistics extracted from the conversations between the user simulator and the dialogue system. This information helps reveal the weakness of the system and signifies the direction for further improvement. With the interacti"
2020.acl-main.243,E06-1040,0,0.0668248,"latent tree variable in a variational autoencoding model with a CRF as the inference network, and on Yin et al. (2018) who use an encoder-decoder model as the inference network. 7 Experimental Setup Data and Metrics We consider two standard neural generation benchmarks: E2E (Novikova et al., 2017) and WikiBio (Lebret et al., 2016a) datasets, with examples shown in Figure 1. The E2E dataset contains approximately 50K examples with 8 distinct fields and 945 distinct word types; it contains multiple test references for one source table. We evaluate in terms of BLEU (Papineni et al., 2002), NIST (Belz and Reiter, 2006), ROUGE-L 2735 Table (x): name[Clowns] eatType[coffee shop] food[Chinese] customer-rating[1 out of 5] area[riverside] near[Clare Hall] Ref.1: Frederick ParkerRhodes (21 March 1914 Ref.1: Clowns is a coffee shop in the riverside area near Clare Hall that has a rating 1 out of 5 . They serve Chinese food . Ref.2: The Chinese coffee shop by the riverside near Clare Hall that only has a customer rating of 1 out of 5 is called Clowns . Ref.3: There is a Chinese coffee shop near Clare Hall in the riverside area called Clowns its not got a good rating though . – 21 November 1987) was an English lingu"
2020.acl-main.243,P19-1599,0,0.0202446,"al., 2018; Shen et al., 2017). Closest to this work is that of Wiseman et al. (2018) who control phrase-level content by using a neuralized hidden semi-Markov model for generation itself. Our work differs in that it makes no independence assumption on the decoder model, uses a faster training algorithm, and proposes a specific method for adding constraints. Finally, there is a line of work that manipulates the syntactic structure of generated texts, by using some labeled syntactic attribute (e.g., parses) or an exemplar (Deriu and Cieliebak, 2018; Colin and Gardent, 2018; Iyyer et al., 2018; Chen et al., 2019). While our work uses control states, there is no inherent assumption of compositional syntax or grammar. Posterior regularization (PR) is mostly used in standard EM settings to impose constraints on the posterior distribution that would otherwise be intractable (or computationally hard) in the prior. Ganchev et al. (2010) applies posterior regularization to word alignment, dependency parsing, and part-of-speech tagging. Combining powerful deep neural networks with structured knowledge has been a popular area of study: Xu et al. (2019) applies PR to multi-object generation to limit object over"
2020.acl-main.243,D18-1113,0,0.0215413,"Hu et al., 2017; Oraby et al., 2018; Zhang et al., 2018; Shen et al., 2017). Closest to this work is that of Wiseman et al. (2018) who control phrase-level content by using a neuralized hidden semi-Markov model for generation itself. Our work differs in that it makes no independence assumption on the decoder model, uses a faster training algorithm, and proposes a specific method for adding constraints. Finally, there is a line of work that manipulates the syntactic structure of generated texts, by using some labeled syntactic attribute (e.g., parses) or an exemplar (Deriu and Cieliebak, 2018; Colin and Gardent, 2018; Iyyer et al., 2018; Chen et al., 2019). While our work uses control states, there is no inherent assumption of compositional syntax or grammar. Posterior regularization (PR) is mostly used in standard EM settings to impose constraints on the posterior distribution that would otherwise be intractable (or computationally hard) in the prior. Ganchev et al. (2010) applies posterior regularization to word alignment, dependency parsing, and part-of-speech tagging. Combining powerful deep neural networks with structured knowledge has been a popular area of study: Xu et al. (2019) applies PR to mult"
2020.acl-main.243,W18-6503,0,0.012332,"generative neural models (Hu et al., 2017; Oraby et al., 2018; Zhang et al., 2018; Shen et al., 2017). Closest to this work is that of Wiseman et al. (2018) who control phrase-level content by using a neuralized hidden semi-Markov model for generation itself. Our work differs in that it makes no independence assumption on the decoder model, uses a faster training algorithm, and proposes a specific method for adding constraints. Finally, there is a line of work that manipulates the syntactic structure of generated texts, by using some labeled syntactic attribute (e.g., parses) or an exemplar (Deriu and Cieliebak, 2018; Colin and Gardent, 2018; Iyyer et al., 2018; Chen et al., 2019). While our work uses control states, there is no inherent assumption of compositional syntax or grammar. Posterior regularization (PR) is mostly used in standard EM settings to impose constraints on the posterior distribution that would otherwise be intractable (or computationally hard) in the prior. Ganchev et al. (2010) applies posterior regularization to word alignment, dependency parsing, and part-of-speech tagging. Combining powerful deep neural networks with structured knowledge has been a popular area of study: Xu et al."
2020.acl-main.243,P16-2008,0,0.0256064,"es over standard benchmarks, while also providing fine-grained control. 1 Introduction A core challenge in using deep learning for NLP is developing methods that allow for controlled output while maintaining the broad coverage of data-driven methods. While this issue is less problematic in classification tasks, it has hampered the deployment of systems for conditional natural language generation (NLG), where users often need to control output through task-specific knowledge or plans. While there have been significant improvements in generation quality from automatic systems (Mei et al., 2016; Dusek and Jurcicek, 2016; Lebret et al., 2016b), these methods are still far from being able to produce controlled output (Wiseman et al., 2017). Recent state-of-the-art system have even begun to utilize manual control through rulebased planning modules (Moryossef et al., 2019; Puduppully et al., 2019). Consider the case of encoder-decoder models for generation, built with RNNs or transformers. These models generate fluent output and provide flexible representations of their conditioning. Unfortunately, auto-regressive decoders are also globally dependent, which makes it challenging to incorporate domain constraints."
2020.acl-main.243,J99-4004,0,0.0249703,"en, 2005). We store two tables β and β , both of size T × |C|. βt (c) denotes the event that there is a transition at time t from state c. βt0 (c) denotes the event that there is a emission starting from time t at state c. Then we have the recursion for βt0 (c) by “summing” over different span length, and we have the recursion for βt (c) that sums over all different state transitions. The algorithm is generic in the sense that different (⊗, ⊕) operators allow us to compute different needed terms. For example, computing the parP tition function Z = φ(x, y, z 0 ) requires the z0 (+,×) semiring (Goodman, 1999; Li and Eisner, 2009), other distributional terms can be computed by using the same algorithm with alternative semirings and backpropagation 3 . 5 Posterior Constraints from Data Alignment To make the PR model concrete, we consider the problem of incorporating weak supervision from heuristic alignment in a data-to-text generation task. Assume that we are tasked with describing a table x consisting of global field names F each with a text value v, e.g. xf = v. Not all global fields may be used in a given x, we use f ∈ x to indicate an 3 We need four terms: (a) log-partition term P log z0 φ(x,"
2020.acl-main.243,P16-1154,0,0.197839,"ng15 (1, 2, name), (4, 6, eatType), (7, 9, near), (11, 15, rating) Table 2: Example of data alignment notation. Here x is a table of data, and f are its fields. For a given output y we enforce a soft alignment A. active field. We would like control states to indicate when each field is used in generation. Our alignment heuristic is that often these fields will be expressed using the identical text as in the table. While this heuristic obviously does not account for all cases, it is very common in natural language generation tasks as evidence by the wide use of copy attention based approaches (Gu et al., 2016; Gulcehre et al., 2016). To utilize these alignments, we use the notation (i, j, f ) ∈ A(x, y) to indicate that a span i : j in the training text y overlaps directly with a field f ∈ x. Table 2 gives an example of the notation. One-to-One Constraints We first consider oneto-one constraints where we assume that we have a static, mapping from fields to states σ : F 7→ C. Given this mapping, we need to add penalties to encourage the semi-Markov model to overlap with the given weak supervision. To enforce soft alignments, we define three posterior constraint types and their computation as shown i"
2020.acl-main.243,P16-1014,0,0.18045,", (4, 6, eatType), (7, 9, near), (11, 15, rating) Table 2: Example of data alignment notation. Here x is a table of data, and f are its fields. For a given output y we enforce a soft alignment A. active field. We would like control states to indicate when each field is used in generation. Our alignment heuristic is that often these fields will be expressed using the identical text as in the table. While this heuristic obviously does not account for all cases, it is very common in natural language generation tasks as evidence by the wide use of copy attention based approaches (Gu et al., 2016; Gulcehre et al., 2016). To utilize these alignments, we use the notation (i, j, f ) ∈ A(x, y) to indicate that a span i : j in the training text y overlaps directly with a field f ∈ x. Table 2 gives an example of the notation. One-to-One Constraints We first consider oneto-one constraints where we assume that we have a static, mapping from fields to states σ : F 7→ C. Given this mapping, we need to add penalties to encourage the semi-Markov model to overlap with the given weak supervision. To enforce soft alignments, we define three posterior constraint types and their computation as shown in Table 1 (Left). The th"
2020.acl-main.243,P16-1228,0,0.0357547,"Missing"
2020.acl-main.243,D16-1173,0,0.03259,"Missing"
2020.acl-main.243,N18-1170,0,0.0270139,"al., 2018; Zhang et al., 2018; Shen et al., 2017). Closest to this work is that of Wiseman et al. (2018) who control phrase-level content by using a neuralized hidden semi-Markov model for generation itself. Our work differs in that it makes no independence assumption on the decoder model, uses a faster training algorithm, and proposes a specific method for adding constraints. Finally, there is a line of work that manipulates the syntactic structure of generated texts, by using some labeled syntactic attribute (e.g., parses) or an exemplar (Deriu and Cieliebak, 2018; Colin and Gardent, 2018; Iyyer et al., 2018; Chen et al., 2019). While our work uses control states, there is no inherent assumption of compositional syntax or grammar. Posterior regularization (PR) is mostly used in standard EM settings to impose constraints on the posterior distribution that would otherwise be intractable (or computationally hard) in the prior. Ganchev et al. (2010) applies posterior regularization to word alignment, dependency parsing, and part-of-speech tagging. Combining powerful deep neural networks with structured knowledge has been a popular area of study: Xu et al. (2019) applies PR to multi-object generation"
2020.acl-main.243,N19-1114,1,0.902226,"θ (z |x, y), i.e. the probability of over state sequences for a known output. The decoder parameterization makes this distribution intractable to compute in general. We instead use variational inference to define a parameterized variational posterior distribution, qφ (z |x, y), from a preselected family of possible distributions Q.1 To fit the model parameters θ, we utilize the evidence lower bound (for any variational parameters φ), Several recent works have shown methods for effectively fitting neural models with structured variational inference (Johnson et al., 2016; Krishnan et al., 2017; Kim et al., 2019). We therefore use these techniques as a backbone for enforcing problem-specific control. See §4 for a full description of the variational family used. 3 Posterior Regularization of Control States Posterior regularization (PR) is an approach for enforcing soft constraints on the posterior distribution of generative models (Ganchev et al., 2010). Our goal is to utilize these soft constraints to enforce problem specific weak supervision. Traditionally PR uses linear constraints which in the special case of expectation maximization for exponential families leads to convenient closed-form training"
2020.acl-main.243,P18-1249,0,0.0158366,"erence for one source table. We follow the metrics from (Lebret et al., 2016a) and evaluate the BLEU, NIST, and ROUGE4 scores. Architecture and Hyperparameters For all tasks, we use an encoder-decoder LSTM for the generative model. We follow recent state-of-the-art works in parametrizing our encoder, and we use copy attention and dual attention (Gu et al., 2016; Gulcehre et al., 2016; Liu et al., 2018): full model architectures are given in the supplement. The inference network scores are computed using a BiLSTM. We compute the emission scores φ(e) using span embeddings (Wang and Chang, 2016; Kitaev and Klein, 2018; Stern et al., 2017); transition scores φ(t) by dot product between embedding vectors for the class labels; lengths φ(l) is kept uniform, as in Wiseman et al. (2018). Additional details are in the supplement. At training time, we use a rate for alleviating posterior collapse in the ELBO: warm-up the ELBO objective by annealing the coeffiPlinearly T cient on the term t=1 log pθ (zt |z&lt;t , y&lt;t ) and H[qφ (z |x, y)] from 0 to 1, as implemented in Kim et al. (2019). We use the REINFORCE algorithm to do Monte Carlo estimation of the stochastic gradient. We choose the control variate to be the mean"
2020.acl-main.243,W07-0734,0,0.0179128,"rside near Clare Hall that only has a customer rating of 1 out of 5 is called Clowns . Ref.3: There is a Chinese coffee shop near Clare Hall in the riverside area called Clowns its not got a good rating though . – 21 November 1987) was an English linguist, plant pathologist, computer scientist, mathematician, mystic, and mycologist. Figure 2: Generation benchmarks. Model is given a table x consisting of semantic fields and is tasked with generating a description y1:T of this data. Two example datasets are shown. Left: E2E, Right: WikiBio. (Lin, 2004), CIDEr (Vedantam et al., 2015) and METEOR (Lavie and Agarwal, 2007), using the official scoring scripts4 . The WikiBio dataset contains approximately 700K examples, 6K distinct table field types, and 400K word types approximately; it contains one reference for one source table. We follow the metrics from (Lebret et al., 2016a) and evaluate the BLEU, NIST, and ROUGE4 scores. Architecture and Hyperparameters For all tasks, we use an encoder-decoder LSTM for the generative model. We follow recent state-of-the-art works in parametrizing our encoder, and we use copy attention and dual attention (Gu et al., 2016; Gulcehre et al., 2016; Liu et al., 2018): full model"
2020.acl-main.243,D16-1128,0,0.0322208,"Missing"
2020.acl-main.243,D09-1005,0,0.0462944,"tore two tables β and β , both of size T × |C|. βt (c) denotes the event that there is a transition at time t from state c. βt0 (c) denotes the event that there is a emission starting from time t at state c. Then we have the recursion for βt0 (c) by “summing” over different span length, and we have the recursion for βt (c) that sums over all different state transitions. The algorithm is generic in the sense that different (⊗, ⊕) operators allow us to compute different needed terms. For example, computing the parP tition function Z = φ(x, y, z 0 ) requires the z0 (+,×) semiring (Goodman, 1999; Li and Eisner, 2009), other distributional terms can be computed by using the same algorithm with alternative semirings and backpropagation 3 . 5 Posterior Constraints from Data Alignment To make the PR model concrete, we consider the problem of incorporating weak supervision from heuristic alignment in a data-to-text generation task. Assume that we are tasked with describing a table x consisting of global field names F each with a text value v, e.g. xf = v. Not all global fields may be used in a given x, we use f ∈ x to indicate an 3 We need four terms: (a) log-partition term P log z0 φ(x, y, z 0 ) requires the"
2020.acl-main.243,W04-1013,0,0.010023,"nese food . Ref.2: The Chinese coffee shop by the riverside near Clare Hall that only has a customer rating of 1 out of 5 is called Clowns . Ref.3: There is a Chinese coffee shop near Clare Hall in the riverside area called Clowns its not got a good rating though . – 21 November 1987) was an English linguist, plant pathologist, computer scientist, mathematician, mystic, and mycologist. Figure 2: Generation benchmarks. Model is given a table x consisting of semantic fields and is tasked with generating a description y1:T of this data. Two example datasets are shown. Left: E2E, Right: WikiBio. (Lin, 2004), CIDEr (Vedantam et al., 2015) and METEOR (Lavie and Agarwal, 2007), using the official scoring scripts4 . The WikiBio dataset contains approximately 700K examples, 6K distinct table field types, and 400K word types approximately; it contains one reference for one source table. We follow the metrics from (Lebret et al., 2016a) and evaluate the BLEU, NIST, and ROUGE4 scores. Architecture and Hyperparameters For all tasks, we use an encoder-decoder LSTM for the generative model. We follow recent state-of-the-art works in parametrizing our encoder, and we use copy attention and dual attention (G"
2020.acl-main.243,P19-1603,0,0.0190473,"cabulary entry in σ should have low entropy; ii) Fit: The global σ should represent the class name distribution posterior of each table field by minimizing the cross entropy between types σ(c |f ) and tokens q(zi:j |x, y) for all (i, j, f ) ∈ A(x, y); iii) Diversity: the aggregate class label distribution over all the token in a sentence should have high entropy. 6 Related Work In addition to previously mentioned work, other researchers have noted the lack of control of deep neural networks and proposed methods at sentencelevel, word-level, and phrase-level. For example Peng et al. (2018) and Luo et al. (2019) control the sentiment in longer-form story generation. Others aim for sentence-level properties such as sentiment, style, tense, and specificity in generative neural models (Hu et al., 2017; Oraby et al., 2018; Zhang et al., 2018; Shen et al., 2017). Closest to this work is that of Wiseman et al. (2018) who control phrase-level content by using a neuralized hidden semi-Markov model for generation itself. Our work differs in that it makes no independence assumption on the decoder model, uses a faster training algorithm, and proposes a specific method for adding constraints. Finally, there is a"
2020.acl-main.243,N16-1086,0,0.0243437,"this method improves over standard benchmarks, while also providing fine-grained control. 1 Introduction A core challenge in using deep learning for NLP is developing methods that allow for controlled output while maintaining the broad coverage of data-driven methods. While this issue is less problematic in classification tasks, it has hampered the deployment of systems for conditional natural language generation (NLG), where users often need to control output through task-specific knowledge or plans. While there have been significant improvements in generation quality from automatic systems (Mei et al., 2016; Dusek and Jurcicek, 2016; Lebret et al., 2016b), these methods are still far from being able to produce controlled output (Wiseman et al., 2017). Recent state-of-the-art system have even begun to utilize manual control through rulebased planning modules (Moryossef et al., 2019; Puduppully et al., 2019). Consider the case of encoder-decoder models for generation, built with RNNs or transformers. These models generate fluent output and provide flexible representations of their conditioning. Unfortunately, auto-regressive decoders are also globally dependent, which makes it challenging to incor"
2020.acl-main.243,N19-1236,0,0.0330157,"Missing"
2020.acl-main.243,W17-5525,0,0.0700309,"blem-specific knowledge. At test time, the control states can be ignored or utilized as grounding for test-time constraints. Technically, the approach builds on recent advances in structured amortized variational inference to enforce additional constraints on the learned distribution. These constraints are enforced through efficient structured posterior calculations and do not hamper modeling power. We demonstrate that the method can improve accuracy and control, while utilizing a range of different posterior constraints. In particular on two large-scale data-to-text generation datasets, E2E (Novikova et al., 2017) and WikiBio (Lebret et al., 2016a), our method increases the performance of benchmark systems while also producing outputs that respect the grounded control states. Our code is available at https://github.com/XiangLi1999/ 2731 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2731–2743 c July 5 - 10, 2020. 2020 Association for Computational Linguistics PosteriorControl-NLG. 2 Control States for Blackbox Generation Consider a conditional generation setting where the input consists of an arbitrary context x and the output y1:T is a sequence of target"
2020.acl-main.243,W18-5019,0,0.0187267,"(zi:j |x, y) for all (i, j, f ) ∈ A(x, y); iii) Diversity: the aggregate class label distribution over all the token in a sentence should have high entropy. 6 Related Work In addition to previously mentioned work, other researchers have noted the lack of control of deep neural networks and proposed methods at sentencelevel, word-level, and phrase-level. For example Peng et al. (2018) and Luo et al. (2019) control the sentiment in longer-form story generation. Others aim for sentence-level properties such as sentiment, style, tense, and specificity in generative neural models (Hu et al., 2017; Oraby et al., 2018; Zhang et al., 2018; Shen et al., 2017). Closest to this work is that of Wiseman et al. (2018) who control phrase-level content by using a neuralized hidden semi-Markov model for generation itself. Our work differs in that it makes no independence assumption on the decoder model, uses a faster training algorithm, and proposes a specific method for adding constraints. Finally, there is a line of work that manipulates the syntactic structure of generated texts, by using some labeled syntactic attribute (e.g., parses) or an exemplar (Deriu and Cieliebak, 2018; Colin and Gardent, 2018; Iyyer et a"
2020.acl-main.243,P02-1040,0,0.106944,"t al. (2019), who introduce a latent tree variable in a variational autoencoding model with a CRF as the inference network, and on Yin et al. (2018) who use an encoder-decoder model as the inference network. 7 Experimental Setup Data and Metrics We consider two standard neural generation benchmarks: E2E (Novikova et al., 2017) and WikiBio (Lebret et al., 2016a) datasets, with examples shown in Figure 1. The E2E dataset contains approximately 50K examples with 8 distinct fields and 945 distinct word types; it contains multiple test references for one source table. We evaluate in terms of BLEU (Papineni et al., 2002), NIST (Belz and Reiter, 2006), ROUGE-L 2735 Table (x): name[Clowns] eatType[coffee shop] food[Chinese] customer-rating[1 out of 5] area[riverside] near[Clare Hall] Ref.1: Frederick ParkerRhodes (21 March 1914 Ref.1: Clowns is a coffee shop in the riverside area near Clare Hall that has a rating 1 out of 5 . They serve Chinese food . Ref.2: The Chinese coffee shop by the riverside near Clare Hall that only has a customer rating of 1 out of 5 is called Clowns . Ref.3: There is a Chinese coffee shop near Clare Hall in the riverside area called Clowns its not got a good rating though . – 21 Novem"
2020.acl-main.243,W18-1505,0,0.0235654,"σ: i) Sparsity: Each vocabulary entry in σ should have low entropy; ii) Fit: The global σ should represent the class name distribution posterior of each table field by minimizing the cross entropy between types σ(c |f ) and tokens q(zi:j |x, y) for all (i, j, f ) ∈ A(x, y); iii) Diversity: the aggregate class label distribution over all the token in a sentence should have high entropy. 6 Related Work In addition to previously mentioned work, other researchers have noted the lack of control of deep neural networks and proposed methods at sentencelevel, word-level, and phrase-level. For example Peng et al. (2018) and Luo et al. (2019) control the sentiment in longer-form story generation. Others aim for sentence-level properties such as sentiment, style, tense, and specificity in generative neural models (Hu et al., 2017; Oraby et al., 2018; Zhang et al., 2018; Shen et al., 2017). Closest to this work is that of Wiseman et al. (2018) who control phrase-level content by using a neuralized hidden semi-Markov model for generation itself. Our work differs in that it makes no independence assumption on the decoder model, uses a faster training algorithm, and proposes a specific method for adding constraint"
2020.acl-main.243,P19-1195,0,0.0374688,"Missing"
2020.acl-main.243,N19-1410,0,0.0748696,"m/tuetschek/e2e-metrics tion to jointly generate both the control states and the sentences. To obtain controlled generation, we observe the control states, and apply constrained beam search to p(y |x, z). Baselines For generation on E2E, we compare externally against 4 systems: E2E-B ENCHMARK (Duˇsek and Jurˇc´ıcˇ ek, 2016) is an encoder-decoder network followed by a reranker used as the shared task benchmark; NT EMP, a controllable neuralized hidden semi-Markov model; NT EMP +AR, the product of experts of both a NTemp model and an autoregressive LSTM network (Wiseman et al., 2018); S HEN 19 (Shen et al., 2019) is an pragmatically informed model, which is the current state-ofthe-art system on E2E dataset. We also compare internally with ablations of our system: E NC D EC is a conditional model p(y |x) trained without control states. PC0 is posterior control model with no constraints. It uses structured encoder with the PR coefficient set to 0. PC∞ is our model with hard constraints, which assumes fully-observed control states. These control states are obtained by mapping tokens with lexical overlap to their designated state; otherwise we map to a generic state. We train a seq2seq model p(y, z |x) wi"
2020.acl-main.243,P17-1076,0,0.0248875,"ble. We follow the metrics from (Lebret et al., 2016a) and evaluate the BLEU, NIST, and ROUGE4 scores. Architecture and Hyperparameters For all tasks, we use an encoder-decoder LSTM for the generative model. We follow recent state-of-the-art works in parametrizing our encoder, and we use copy attention and dual attention (Gu et al., 2016; Gulcehre et al., 2016; Liu et al., 2018): full model architectures are given in the supplement. The inference network scores are computed using a BiLSTM. We compute the emission scores φ(e) using span embeddings (Wang and Chang, 2016; Kitaev and Klein, 2018; Stern et al., 2017); transition scores φ(t) by dot product between embedding vectors for the class labels; lengths φ(l) is kept uniform, as in Wiseman et al. (2018). Additional details are in the supplement. At training time, we use a rate for alleviating posterior collapse in the ELBO: warm-up the ELBO objective by annealing the coeffiPlinearly T cient on the term t=1 log pθ (zt |z&lt;t , y&lt;t ) and H[qφ (z |x, y)] from 0 to 1, as implemented in Kim et al. (2019). We use the REINFORCE algorithm to do Monte Carlo estimation of the stochastic gradient. We choose the control variate to be the mean of the samples (Mnih"
2020.acl-main.243,P16-1218,0,0.025415,"y; it contains one reference for one source table. We follow the metrics from (Lebret et al., 2016a) and evaluate the BLEU, NIST, and ROUGE4 scores. Architecture and Hyperparameters For all tasks, we use an encoder-decoder LSTM for the generative model. We follow recent state-of-the-art works in parametrizing our encoder, and we use copy attention and dual attention (Gu et al., 2016; Gulcehre et al., 2016; Liu et al., 2018): full model architectures are given in the supplement. The inference network scores are computed using a BiLSTM. We compute the emission scores φ(e) using span embeddings (Wang and Chang, 2016; Kitaev and Klein, 2018; Stern et al., 2017); transition scores φ(t) by dot product between embedding vectors for the class labels; lengths φ(l) is kept uniform, as in Wiseman et al. (2018). Additional details are in the supplement. At training time, we use a rate for alleviating posterior collapse in the ELBO: warm-up the ELBO objective by annealing the coeffiPlinearly T cient on the term t=1 log pθ (zt |z&lt;t , y&lt;t ) and H[qφ (z |x, y)] from 0 to 1, as implemented in Kim et al. (2019). We use the REINFORCE algorithm to do Monte Carlo estimation of the stochastic gradient. We choose the contro"
2020.acl-main.243,D18-1356,1,0.686352,"corporate domain constraints. Research into controllable deep models aims to circumvent the all-or-nothing dependency tradeoff of encoder-decoder systems and expose explicit higher-level decisions. One line of research has looked at global control states that represent sentence-level properties for the full decoder. For example, Hu et al. (2017) uses generative adversarial networks where the attributes of the text (e.g., sentiment, tense) are exposed. Another line of research exposes fine-level properties, such as phrase type, but requires factoring the decoder to expose local decisions, e.g. Wiseman et al. (2018). This work proposes a method for augmenting any neural decoder architecture to incorporate finegrained control states. The approach first modifies training to incorporate structured latent control variables. Then, training constraints are added to anchor the state values to problem-specific knowledge. At test time, the control states can be ignored or utilized as grounding for test-time constraints. Technically, the approach builds on recent advances in structured amortized variational inference to enforce additional constraints on the learned distribution. These constraints are enforced thro"
2020.acl-main.243,P18-1070,0,0.0193307,"ove accuracy and interpretability. Finally, the core of this work is the use of amortized inference/variation autoencoder to approximate variational posterior (Kingma and Welling, 2014; Mnih and Gregor, 2014; Rezende et al., 2014). We rely heavily on a structure distribution, either linear chain or semi-Markov, which was introduced as a structured VAEs (Johnson et al., 2016; Krishnan et al., 2017; Ammar et al., 2014). Our setting and optimization are based on Kim et al. (2019), who introduce a latent tree variable in a variational autoencoding model with a CRF as the inference network, and on Yin et al. (2018) who use an encoder-decoder model as the inference network. 7 Experimental Setup Data and Metrics We consider two standard neural generation benchmarks: E2E (Novikova et al., 2017) and WikiBio (Lebret et al., 2016a) datasets, with examples shown in Figure 1. The E2E dataset contains approximately 50K examples with 8 distinct fields and 945 distinct word types; it contains multiple test references for one source table. We evaluate in terms of BLEU (Papineni et al., 2002), NIST (Belz and Reiter, 2006), ROUGE-L 2735 Table (x): name[Clowns] eatType[coffee shop] food[Chinese] customer-rating[1 out"
2020.acl-main.243,P18-1102,0,0.0186753,"(i, j, f ) ∈ A(x, y); iii) Diversity: the aggregate class label distribution over all the token in a sentence should have high entropy. 6 Related Work In addition to previously mentioned work, other researchers have noted the lack of control of deep neural networks and proposed methods at sentencelevel, word-level, and phrase-level. For example Peng et al. (2018) and Luo et al. (2019) control the sentiment in longer-form story generation. Others aim for sentence-level properties such as sentiment, style, tense, and specificity in generative neural models (Hu et al., 2017; Oraby et al., 2018; Zhang et al., 2018; Shen et al., 2017). Closest to this work is that of Wiseman et al. (2018) who control phrase-level content by using a neuralized hidden semi-Markov model for generation itself. Our work differs in that it makes no independence assumption on the decoder model, uses a faster training algorithm, and proposes a specific method for adding constraints. Finally, there is a line of work that manipulates the syntactic structure of generated texts, by using some labeled syntactic attribute (e.g., parses) or an exemplar (Deriu and Cieliebak, 2018; Colin and Gardent, 2018; Iyyer et al., 2018; Chen et al"
2020.autosimtrans-1.5,P18-1118,0,0.0638382,"s Figure 1: The architecture of our proposed encoder 2 Related Work the discourse structure information, which can be used to not only enrich word embedding but also guide the selection of relevant context for the current sentence. In the era of statistical machine translation, document-level machine translation has become one of the research focuses in the community of machine translation. (Xiao et al., 2011; Su et al., 2012; Xiao et al., 2012; Su et al., 2015). Recently, with the rapid development of NMT, documentlevel NMT has also gradually attracted people’s attention (Voita et al., 2018; Maruf and Haffari, 2018; Miculicich et al., 2018; Maruf et al., 2019; Yang et al., 2019). Typically, existing studies aim to improve document-level translation quality with the help of document context, which is usually extracted from neighboring sentences of the current sentence. For example,some researchers applied cache-based models to selectively remember the most relevant context information of the document (Voita et al., 2018; Maruf and Haffari, 2018; Kuang et al., 2018), while some researchers employed hierarchical context networks to catch document context information for Transformer (Miculicich et al., 2018"
2020.autosimtrans-1.5,D18-1325,0,0.679378,"016; Nejat et al., 2017). However, to the best of our knowledge, discourse structure has not been explicitly used in document-level NMT. Recently, document-level neural machine translation (NMT) has become a hot topic in the community of machine translation. Despite its success, most of existing studies ignored the discourse structure information of the input document to be translated, which has shown effective in other tasks. In this paper, we propose to improve document-level NMT with the aid of discourse structure information. Our encoder is based on a hierarchical attention network (HAN) (Miculicich et al., 2018). Specifically, we first parse the input document to obtain its discourse structure. Then, we introduce a Transformer-based path encoder to embed the discourse structure information of each word. Finally, we combine the discourse structure information with the word embedding before it is fed into the encoder. Experimental results on the Englishto-German dataset show that our model can significantly outperform both Transformer and Transformer+HAN. 1 To address the above problem, we propose to improve document-level NMT with the aid of discourse structure information. First, we represent each in"
2020.autosimtrans-1.5,C18-1203,0,0.0264746,"rd using its discourse path from root node to its corresponding leaf node. Each path is a mixed label sequence composed of the discourse relationship and the importance label (e.g., NUCLEUS ELABORATION, SATELLITE BACKGROUND). Please note that all tokens in the same EDU share the same discourse 3.3 HAN-based Context Modeling Following (2018), we apply hierarchical attention network (HAN) as our context encoder. Due to the advantage of accurately capturing different levels of contexts, HAN has been widely used in many tasks, such as document classification (Yang et al., 2016), stance detection (Sun et al., 2018), sentencelevel NMT (Su et al., 2018b). Using this encoder, we mainly focus on two levels of context modeling: Sentence-level Context Modeling For the i-th word of the current sentence, we employ muti-head 32 attention (Vaswani et al., 2017) to summarize the context from the k-th context sentence: csi,k = MultiHead(fs (hi ), Hk ), Training Development Test (2) Settings We use Transformer (Vaswani et al., 2017) as our context-agnostic baseline system and Transformer+HAN (Miculicich et al., 2018) as our context-aware baseline system. We conduct experiments using the same configuration as HAN. Sp"
2020.autosimtrans-1.5,W17-5535,0,0.0190985,"ina 2 Xiaomi AI Lab, Xiaomi Inc., Beijing, China {chenjx,zhangjiarui,clzhou}@stu.xmu.edu.cn jssu@xmu.edu.cn {lixiang21,cuijianwei,wangbin11}@xiaomi.com Abstract 2017a; Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018; Maruf and Haffari, 2018; Maruf et al., 2019; Yang et al., 2019). Discourse structure, as well as raw contextual sentences, is a major component of the document. And it has been proved to be effective in many other tasks, such as automatic document summarization (Yoshida et al., 2014; Isonuma et al., 2019) and sentiment classification (Schouten and Frasincar, 2016; Nejat et al., 2017). However, to the best of our knowledge, discourse structure has not been explicitly used in document-level NMT. Recently, document-level neural machine translation (NMT) has become a hot topic in the community of machine translation. Despite its success, most of existing studies ignored the discourse structure information of the input document to be translated, which has shown effective in other tasks. In this paper, we propose to improve document-level NMT with the aid of discourse structure information. Our encoder is based on a hierarchical attention network (HAN) (Miculicich et al., 2018)"
2020.autosimtrans-1.5,W17-4811,0,0.0446932,"ntext information in a hierarchical manner. Introduction Neural machine translation (NMT) has made great progress in the past decade. In practical applications, the need for NMT systems has expanded from individual sentences to complete documents. Therefore, document-level NMT has gradually drawn much more attention. Contextual information is particularly important for obtaining highquality document translation. To get better contextual information, researchers have proposed many methods (e.g., memory network and hierarchical attention network) for document-level translation (Sim Smith, 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017a; Tu et al., 2017; Wang et al., Our contributions are as follows: (i) We propose a novel and efficient approach to explicitly exploit discourse structure information for documentlevel NMT. Particularly, our approach is applicable for any other context encoder of document-level NMT; (ii) We carry out experiments on English-toGerman translation task and experimental results show that our model outperforms competitive baselines. ∗ This work is done when Junxuan Chen was interning at Xiaomi AI Lab, Xiaomi Inc., Beijing, China. † Corresponding author. 30 Proceedings of the 1st W"
2020.autosimtrans-1.5,P02-1040,0,0.112505,"Specifically, both sentence encoder and decoder are composed of 6 hidden layers, while path encoder is composed of 2 hidden layers. We use three previous sentences as contextual sentences for current sentence. The hidden size and pointwise FFN size are 512 and 2048 respectively. The dropout rates for all hidden states are set to 0.1. The source and target vocabulary sizes are both 30K. At the training phase, we use the Adam optimizer (Kingma and Ba, 2015) and the batch sizes of context-agnostic model and context-aware model are 4096 and 1024, respectively. Finally, we use case-sensitive BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) to measure the translation quality. Document-level Context Modeling Unlike the above modeling, here we mainly on capturing the context information from previous K sentences for the i-th word of the current sentence. (3) where fd is a linear transformation, and CSi = [csi,1 , csi,2 , · · ·, csi,K ] is the sentence-level context of K contextual sentences. Integrating Document-level Context into the Translation Encoder Finally, we integrate the above-mentioned document-level context into the translation encoder via a gating operation: λi = σ(Wh hi + Wcd cdi ) (4) he"
2020.autosimtrans-1.5,Q17-1007,0,0.0376646,"uction Neural machine translation (NMT) has made great progress in the past decade. In practical applications, the need for NMT systems has expanded from individual sentences to complete documents. Therefore, document-level NMT has gradually drawn much more attention. Contextual information is particularly important for obtaining highquality document translation. To get better contextual information, researchers have proposed many methods (e.g., memory network and hierarchical attention network) for document-level translation (Sim Smith, 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017a; Tu et al., 2017; Wang et al., Our contributions are as follows: (i) We propose a novel and efficient approach to explicitly exploit discourse structure information for documentlevel NMT. Particularly, our approach is applicable for any other context encoder of document-level NMT; (ii) We carry out experiments on English-toGerman translation task and experimental results show that our model outperforms competitive baselines. ∗ This work is done when Junxuan Chen was interning at Xiaomi AI Lab, Xiaomi Inc., Beijing, China. † Corresponding author. 30 Proceedings of the 1st Workshop on Automatic Simultaneous Tra"
2020.autosimtrans-1.5,S16-1057,0,0.020639,"Xiamen University, Xiamen, China 2 Xiaomi AI Lab, Xiaomi Inc., Beijing, China {chenjx,zhangjiarui,clzhou}@stu.xmu.edu.cn jssu@xmu.edu.cn {lixiang21,cuijianwei,wangbin11}@xiaomi.com Abstract 2017a; Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018; Maruf and Haffari, 2018; Maruf et al., 2019; Yang et al., 2019). Discourse structure, as well as raw contextual sentences, is a major component of the document. And it has been proved to be effective in many other tasks, such as automatic document summarization (Yoshida et al., 2014; Isonuma et al., 2019) and sentiment classification (Schouten and Frasincar, 2016; Nejat et al., 2017). However, to the best of our knowledge, discourse structure has not been explicitly used in document-level NMT. Recently, document-level neural machine translation (NMT) has become a hot topic in the community of machine translation. Despite its success, most of existing studies ignored the discourse structure information of the input document to be translated, which has shown effective in other tasks. In this paper, we propose to improve document-level NMT with the aid of discourse structure information. Our encoder is based on a hierarchical attention network (HAN) (Mic"
2020.autosimtrans-1.5,P18-1117,0,0.0696288,"utational Linguistics Figure 1: The architecture of our proposed encoder 2 Related Work the discourse structure information, which can be used to not only enrich word embedding but also guide the selection of relevant context for the current sentence. In the era of statistical machine translation, document-level machine translation has become one of the research focuses in the community of machine translation. (Xiao et al., 2011; Su et al., 2012; Xiao et al., 2012; Su et al., 2015). Recently, with the rapid development of NMT, documentlevel NMT has also gradually attracted people’s attention (Voita et al., 2018; Maruf and Haffari, 2018; Miculicich et al., 2018; Maruf et al., 2019; Yang et al., 2019). Typically, existing studies aim to improve document-level translation quality with the help of document context, which is usually extracted from neighboring sentences of the current sentence. For example,some researchers applied cache-based models to selectively remember the most relevant context information of the document (Voita et al., 2018; Maruf and Haffari, 2018; Kuang et al., 2018), while some researchers employed hierarchical context networks to catch document context information for Transformer"
2020.autosimtrans-1.5,W17-4814,0,0.0255062,"r to model context information in a hierarchical manner. Introduction Neural machine translation (NMT) has made great progress in the past decade. In practical applications, the need for NMT systems has expanded from individual sentences to complete documents. Therefore, document-level NMT has gradually drawn much more attention. Contextual information is particularly important for obtaining highquality document translation. To get better contextual information, researchers have proposed many methods (e.g., memory network and hierarchical attention network) for document-level translation (Sim Smith, 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017a; Tu et al., 2017; Wang et al., Our contributions are as follows: (i) We propose a novel and efficient approach to explicitly exploit discourse structure information for documentlevel NMT. Particularly, our approach is applicable for any other context encoder of document-level NMT; (ii) We carry out experiments on English-toGerman translation task and experimental results show that our model outperforms competitive baselines. ∗ This work is done when Junxuan Chen was interning at Xiaomi AI Lab, Xiaomi Inc., Beijing, China. † Corresponding autho"
2020.autosimtrans-1.5,2006.amta-papers.25,0,0.0432281,"der and decoder are composed of 6 hidden layers, while path encoder is composed of 2 hidden layers. We use three previous sentences as contextual sentences for current sentence. The hidden size and pointwise FFN size are 512 and 2048 respectively. The dropout rates for all hidden states are set to 0.1. The source and target vocabulary sizes are both 30K. At the training phase, we use the Adam optimizer (Kingma and Ba, 2015) and the batch sizes of context-agnostic model and context-aware model are 4096 and 1024, respectively. Finally, we use case-sensitive BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) to measure the translation quality. Document-level Context Modeling Unlike the above modeling, here we mainly on capturing the context information from previous K sentences for the i-th word of the current sentence. (3) where fd is a linear transformation, and CSi = [csi,1 , csi,2 , · · ·, csi,K ] is the sentence-level context of K contextual sentences. Integrating Document-level Context into the Translation Encoder Finally, we integrate the above-mentioned document-level context into the translation encoder via a gating operation: λi = σ(Wh hi + Wcd cdi ) (4) hei = λi hi + (1 − λi )cdi (5) D"
2020.autosimtrans-1.5,D17-1301,0,0.270329,"hical manner. Introduction Neural machine translation (NMT) has made great progress in the past decade. In practical applications, the need for NMT systems has expanded from individual sentences to complete documents. Therefore, document-level NMT has gradually drawn much more attention. Contextual information is particularly important for obtaining highquality document translation. To get better contextual information, researchers have proposed many methods (e.g., memory network and hierarchical attention network) for document-level translation (Sim Smith, 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017a; Tu et al., 2017; Wang et al., Our contributions are as follows: (i) We propose a novel and efficient approach to explicitly exploit discourse structure information for documentlevel NMT. Particularly, our approach is applicable for any other context encoder of document-level NMT; (ii) We carry out experiments on English-toGerman translation task and experimental results show that our model outperforms competitive baselines. ∗ This work is done when Junxuan Chen was interning at Xiaomi AI Lab, Xiaomi Inc., Beijing, China. † Corresponding author. 30 Proceedings of the 1st Workshop on Automati"
2020.autosimtrans-1.5,P12-1048,1,0.795182,"ijing, China. † Corresponding author. 30 Proceedings of the 1st Workshop on Automatic Simultaneous Translation, pages 30–36 c July 10, 2020. 2020 Association for Computational Linguistics Figure 1: The architecture of our proposed encoder 2 Related Work the discourse structure information, which can be used to not only enrich word embedding but also guide the selection of relevant context for the current sentence. In the era of statistical machine translation, document-level machine translation has become one of the research focuses in the community of machine translation. (Xiao et al., 2011; Su et al., 2012; Xiao et al., 2012; Su et al., 2015). Recently, with the rapid development of NMT, documentlevel NMT has also gradually attracted people’s attention (Voita et al., 2018; Maruf and Haffari, 2018; Miculicich et al., 2018; Maruf et al., 2019; Yang et al., 2019). Typically, existing studies aim to improve document-level translation quality with the help of document context, which is usually extracted from neighboring sentences of the current sentence. For example,some researchers applied cache-based models to selectively remember the most relevant context information of the document (Voita et al."
2020.autosimtrans-1.5,P17-2029,0,0.0985072,"hical manner. Introduction Neural machine translation (NMT) has made great progress in the past decade. In practical applications, the need for NMT systems has expanded from individual sentences to complete documents. Therefore, document-level NMT has gradually drawn much more attention. Contextual information is particularly important for obtaining highquality document translation. To get better contextual information, researchers have proposed many methods (e.g., memory network and hierarchical attention network) for document-level translation (Sim Smith, 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017a; Tu et al., 2017; Wang et al., Our contributions are as follows: (i) We propose a novel and efficient approach to explicitly exploit discourse structure information for documentlevel NMT. Particularly, our approach is applicable for any other context encoder of document-level NMT; (ii) We carry out experiments on English-toGerman translation task and experimental results show that our model outperforms competitive baselines. ∗ This work is done when Junxuan Chen was interning at Xiaomi AI Lab, Xiaomi Inc., Beijing, China. † Corresponding author. 30 Proceedings of the 1st Workshop on Automati"
2020.autosimtrans-1.5,2011.mtsummit-papers.13,0,0.0299431,"ab, Xiaomi Inc., Beijing, China. † Corresponding author. 30 Proceedings of the 1st Workshop on Automatic Simultaneous Translation, pages 30–36 c July 10, 2020. 2020 Association for Computational Linguistics Figure 1: The architecture of our proposed encoder 2 Related Work the discourse structure information, which can be used to not only enrich word embedding but also guide the selection of relevant context for the current sentence. In the era of statistical machine translation, document-level machine translation has become one of the research focuses in the community of machine translation. (Xiao et al., 2011; Su et al., 2012; Xiao et al., 2012; Su et al., 2015). Recently, with the rapid development of NMT, documentlevel NMT has also gradually attracted people’s attention (Voita et al., 2018; Maruf and Haffari, 2018; Miculicich et al., 2018; Maruf et al., 2019; Yang et al., 2019). Typically, existing studies aim to improve document-level translation quality with the help of document context, which is usually extracted from neighboring sentences of the current sentence. For example,some researchers applied cache-based models to selectively remember the most relevant context information of the docum"
2020.autosimtrans-1.5,P12-1079,0,0.0311157,"orresponding author. 30 Proceedings of the 1st Workshop on Automatic Simultaneous Translation, pages 30–36 c July 10, 2020. 2020 Association for Computational Linguistics Figure 1: The architecture of our proposed encoder 2 Related Work the discourse structure information, which can be used to not only enrich word embedding but also guide the selection of relevant context for the current sentence. In the era of statistical machine translation, document-level machine translation has become one of the research focuses in the community of machine translation. (Xiao et al., 2011; Su et al., 2012; Xiao et al., 2012; Su et al., 2015). Recently, with the rapid development of NMT, documentlevel NMT has also gradually attracted people’s attention (Voita et al., 2018; Maruf and Haffari, 2018; Miculicich et al., 2018; Maruf et al., 2019; Yang et al., 2019). Typically, existing studies aim to improve document-level translation quality with the help of document context, which is usually extracted from neighboring sentences of the current sentence. For example,some researchers applied cache-based models to selectively remember the most relevant context information of the document (Voita et al., 2018; Maruf and H"
2020.autosimtrans-1.5,D19-1164,0,0.0122307,"e discourse structure information, which can be used to not only enrich word embedding but also guide the selection of relevant context for the current sentence. In the era of statistical machine translation, document-level machine translation has become one of the research focuses in the community of machine translation. (Xiao et al., 2011; Su et al., 2012; Xiao et al., 2012; Su et al., 2015). Recently, with the rapid development of NMT, documentlevel NMT has also gradually attracted people’s attention (Voita et al., 2018; Maruf and Haffari, 2018; Miculicich et al., 2018; Maruf et al., 2019; Yang et al., 2019). Typically, existing studies aim to improve document-level translation quality with the help of document context, which is usually extracted from neighboring sentences of the current sentence. For example,some researchers applied cache-based models to selectively remember the most relevant context information of the document (Voita et al., 2018; Maruf and Haffari, 2018; Kuang et al., 2018), while some researchers employed hierarchical context networks to catch document context information for Transformer (Miculicich et al., 2018; Maruf et al., 2019; Yang et al., 2019). Specifically, Miculicic"
2020.autosimtrans-1.5,N16-1174,0,0.0850258,"ourse structure information of each word using its discourse path from root node to its corresponding leaf node. Each path is a mixed label sequence composed of the discourse relationship and the importance label (e.g., NUCLEUS ELABORATION, SATELLITE BACKGROUND). Please note that all tokens in the same EDU share the same discourse 3.3 HAN-based Context Modeling Following (2018), we apply hierarchical attention network (HAN) as our context encoder. Due to the advantage of accurately capturing different levels of contexts, HAN has been widely used in many tasks, such as document classification (Yang et al., 2016), stance detection (Sun et al., 2018), sentencelevel NMT (Su et al., 2018b). Using this encoder, we mainly focus on two levels of context modeling: Sentence-level Context Modeling For the i-th word of the current sentence, we employ muti-head 32 attention (Vaswani et al., 2017) to summarize the context from the k-th context sentence: csi,k = MultiHead(fs (hi ), Hk ), Training Development Test (2) Settings We use Transformer (Vaswani et al., 2017) as our context-agnostic baseline system and Transformer+HAN (Miculicich et al., 2018) as our context-aware baseline system. We conduct experiments us"
2020.autosimtrans-1.5,D14-1196,0,0.029133,", Jiarui Zhang1 , Chulun Zhou1 , Jianwei Cui2 , Bin Wang2 , Jinsong Su1† 1 Xiamen University, Xiamen, China 2 Xiaomi AI Lab, Xiaomi Inc., Beijing, China {chenjx,zhangjiarui,clzhou}@stu.xmu.edu.cn jssu@xmu.edu.cn {lixiang21,cuijianwei,wangbin11}@xiaomi.com Abstract 2017a; Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018; Maruf and Haffari, 2018; Maruf et al., 2019; Yang et al., 2019). Discourse structure, as well as raw contextual sentences, is a major component of the document. And it has been proved to be effective in many other tasks, such as automatic document summarization (Yoshida et al., 2014; Isonuma et al., 2019) and sentiment classification (Schouten and Frasincar, 2016; Nejat et al., 2017). However, to the best of our knowledge, discourse structure has not been explicitly used in document-level NMT. Recently, document-level neural machine translation (NMT) has become a hot topic in the community of machine translation. Despite its success, most of existing studies ignored the discourse structure information of the input document to be translated, which has shown effective in other tasks. In this paper, we propose to improve document-level NMT with the aid of discourse structur"
2020.autosimtrans-1.5,D16-1050,1,0.66979,"Missing"
2020.autosimtrans-1.5,D18-1049,0,0.109063,"Missing"
2020.emnlp-main.85,W10-1703,0,0.0150525,"so measuring the model’s ability to provide a diverse set of answers in order to match all the answer clusters. While such an approach can penalize a correct model prediction when it does not match an existing reference answer, we counter this issue by (a) gathering and clustering a large number of reference answers, and (b) utilizing methods of matching non-exact matches, such as WordNet (Miller, 1995) and contextual language models such as RoBERTa (Liu et al., 2019). Generative evaluation approaches are also used in other NLP tasks such as summarization (Radev et al., 2003) and translation (Callison-Burch et al., 2010). 2 We evaluate on a set of competitive baseline models — from QA models powered by large masked LMs such as B ERT, to the direct prediction of answers in a language-modeling paradigm using a large G PT-2 LM (Radford et al., 2018), as well as G PT-2 fine-tuned upon the training data. While most models perform quite poorly at this challenging task, when G PT-2 was fine-tuned using the FAMILY-F EUD training set its performance did improved drastically, although remaining significantly 1. We introduce a large-scale QA dataset of 9.7k questions regarding common sense knowledge of prototypical situ"
2020.emnlp-main.85,P09-1068,0,0.0431438,"ouse. 6 Related Work A wide variety of common sense reasoning datasets address related topics. Many datasets cover physical and spatial reasoning (Bisk et al., 2019), social common sense (Sap et al., 2019b), and common sense understanding of plausible sequences of events (Zellers et al., 2018, 2019; Huang et al., 2019; Bhagavatula et al., 2019; Sap et al., 2019a) or understanding of the entailments of a sentence (Zhang et al., 2017; Bowman et al., 2015; Roemmele et al., 2011; Levesque et al., 2012). There is also a long history of work in modeling scripts and frames (Schank and Abelson, 1977; Chambers and Jurafsky, 2009; Fillmore et al., 1976; Ferraro and Van Durme, 2016; Weber et al., 2020), which is related to the current focus on prototypical situations. Recent works have also sought to characterize the ability of pre-trained language models to understand common sense reasoning, showing such models perform well at common sense reasoning tasks even without fine-tuning, allowing one to explore the common sense reasoning inherent in those models (Tamborrino et al., 2020; Weir et al., 2020). Of particular relevance to the current work, Weir et al. (2020) explored the ability of pre-trained models to predict s"
2020.emnlp-main.85,2021.ccl-1.108,0,0.134017,"Missing"
2020.emnlp-main.85,P11-2057,0,0.350449,"coherent amount of agreement regarding the clustering of answers. 2.4 Analysis of the Dataset The data presented here involves a range of different types of common sense knowledge. To explore the distribution of different kinds of reasoning, and to test whether that distribution of reasoning varied between the publicly available data and the crowdsourced development and test set, we propose a small inventory of six types of common sense reasoning. We are not aware of an agreed-upon typology of all commonsense reasoning types. Categorizations of different types of commonsense reasoning exist (LoBue and Yates, 2011; Boratko et al., 2018), but since each provided categorizations needed for specific tasks (RTE and the ARC dataset, respectively), neither fully covered the range of commonsense types seen in the current work. After consulting both those prior works and a separate part of the training data, we characterize the data into the following six types. These types consist of (1) M ENTAL OR S O CIAL R EASONING , (2) K NOWLEDGE OF P RO TOTYPICAL S ITUATIONS which one is familiar with, (3) R EASONING ABOUT NOVEL , COMPLEX 6 The four total expert annotators annotated a random set of 10 questions together"
2020.emnlp-main.85,P14-2005,0,0.0251555,"lustering was agreed on.6 During this clustering phase answers could be marked as invalid as well — most commonly, either due to low-quality annotations or a clear misunderstanding of a question. In order to keep these clusters roughly similar to the granularity of answers used in the training data and to avoid low-quality evaluation we eliminated questions for which the 8 most popular clusters did not contain at least 85 of the 100 responses. Since each set of answers was clustered twice and adjudicated, we measure the agreement with a cluster agreement metric BLANC (Recasens and Hovy, 2011; Luo et al., 2014), an extension of the Rand index used to score coreference clustering. Using this, the similarity between the clusters produced by any two annotators averaged out to a BLANC score of 83.17, suggesting a coherent amount of agreement regarding the clustering of answers. 2.4 Analysis of the Dataset The data presented here involves a range of different types of common sense knowledge. To explore the distribution of different kinds of reasoning, and to test whether that distribution of reasoning varied between the publicly available data and the crowdsourced development and test set, we propose a s"
2020.emnlp-main.85,D19-1454,0,0.0653514,"Missing"
2020.emnlp-main.85,N19-1421,0,0.0777479,"are evaluated by providing a ranked list of answers in response to a question. These answers are then compared to the set of reference answers for that question and scored based upon how similar they are to the known answers. While one might instead convert questionanswer pairs into a multiple-choice paradigm by generating negatives, it is difficult to generate good negative examples, and the quality of a dataset can be compromised if such examples are either too easy or easily identified using biases in the negative example generation process (Mostafazadeh et al., 2016; Zellers et al., 2018; Talmor et al., 2019; Schwartz et al., 2017; Gururangan et al., 2018; Poliak et al., 2018). We outline here our proposed method for scoring these ranked lists of predicted answers. The dataset ground truth is a ranked list of clusters of answers, including weights(cluster sizes) associated with each cluster. A first component in such an evaluation is to match each answer to an existing cluster of answers, if any cluster is acceptable. We try both simple methods such as exact match as well as more flexible ways of matching to clusters, such as using synonyms from WordNet (Miller, 1995) or a vector-based similarity"
2020.emnlp-main.85,2020.acl-main.357,0,0.0457939,"h for each question constrained to Reddit. This resulted in a set of 85,781 Reddit posts total. Searches were constrained to Reddit in order to focus upon advice and personal narratives which 7 Note that since our scores are always calculated as a percentage of the max score one could receive, M AX A NSWERS is slightly different than hits@k in this setting. Language Model Baseline We also report a language model generation baseline, due to the improved representation power of modern language models and recent evidence of their power in modeling common sense reasoning tasks (Weir et al., 2020; Tamborrino et al., 2020). The baseline is performed using the AI2 G PT-2 large model (Radford et al., 2019) (specifically, the Hugging Face PyTorch implementation (Wolf et al., 2019)). We perform both a zero-shot evaluation and an evaluation after fine-tuning with using our training data. Because the original FAMILY-F EUD prompts are not structured as completion tasks, we transform the original question by hand-designed transformation rules in order for it to be compatible with the G PT-2 training data. E.g “Name something people do when they wake up.” → “One thing people do when they wake up is ...”. The hand-design"
2020.emnlp-main.85,2020.emnlp-main.612,0,0.0259265,"Missing"
2020.emnlp-main.85,2020.acl-main.515,0,0.0362727,"Missing"
2020.emnlp-main.85,D18-1009,0,0.325223,"wers. In each, models are evaluated by providing a ranked list of answers in response to a question. These answers are then compared to the set of reference answers for that question and scored based upon how similar they are to the known answers. While one might instead convert questionanswer pairs into a multiple-choice paradigm by generating negatives, it is difficult to generate good negative examples, and the quality of a dataset can be compromised if such examples are either too easy or easily identified using biases in the negative example generation process (Mostafazadeh et al., 2016; Zellers et al., 2018; Talmor et al., 2019; Schwartz et al., 2017; Gururangan et al., 2018; Poliak et al., 2018). We outline here our proposed method for scoring these ranked lists of predicted answers. The dataset ground truth is a ranked list of clusters of answers, including weights(cluster sizes) associated with each cluster. A first component in such an evaluation is to match each answer to an existing cluster of answers, if any cluster is acceptable. We try both simple methods such as exact match as well as more flexible ways of matching to clusters, such as using synonyms from WordNet (Miller, 1995) or a ve"
2020.emnlp-main.85,P19-1472,0,0.107105,"Missing"
2020.emnlp-main.85,2020.acl-main.184,0,0.0892005,"Missing"
2020.emnlp-main.85,Q17-1027,0,0.0683286,"Missing"
2020.emnlp-main.85,W05-0909,0,\N,Missing
2020.emnlp-main.85,P03-1048,0,\N,Missing
2020.emnlp-main.85,P18-2124,0,\N,Missing
2020.emnlp-main.85,W18-2607,1,\N,Missing
2020.emnlp-main.85,N19-1423,0,\N,Missing
2020.iwslt-1.18,P19-1309,0,0.0172207,"e corpus if its source side or target side fails to obey any of the following wild rules reflecting what ‘good data’ should look like. Some of the heuristic filtering methods can deal with aspects that can not be captured with models. • SBERT model: Recently, contextualized word embeddings derived from large-scale pre-trained language models (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019) have achieved new state-of-the-arts in various monolingual NLP tasks. The success has also been extended to cross-lingual scenarios (Schwenk, 2018; Conneau and Lample, 2019; Mulcaire et al., 2019; Artetxe and Schwenk, 2019). Recently, Reimers and Gurevych (2019) proposed sentence BERT (SBERT) to derive semantically meaningful sentence embeddings. According to the training framework of SBERT, we use the multilingual pre-train BERT model5 and finetune it on RFPD to yield useful Chinese and Japanese sentence embeddings in the same space. We reject sentence pairs with a cosine-similarity score below 0.2. • The token (i.e. character sequence between two spaces) length of every sentence is limited less than 50. • Sentence pairs with a length ratio greater than 4 are removed. • Chinese sentences with Chinese characters"
2020.iwslt-1.18,D18-1338,0,0.0221222,"synthetic data with the original data to train the baseline models from scratch in L2R, R2L, and T2S ways, respectively. Finally, we conduct the aforementioned approach based on ensemble models again to achieve better baseline systems. • Wider model: Dimension is an important factor to enhance the Transformer model capacity and performance. Based on the standard Transformer-Big model, we train a Transformer-Wide model with a inner dimension of position-wise feed-forward layers 8,192. • Deeper model: Building deeper networks via stacking more encoder and decoder layers has been a trend in NMT (Bapna et al., 2018; Wu et al., 2019; Zhang et al., 2019). We also exploit three deeper Transformer models by simply increasing the layer size of Transformer-Big, including TransformerDeep-12-12, Transformer-Deep-12-6, and Transformer-Deep-6-12 in which the first number represents the layer size of the encoder and the second number represents the layer size of the decoder. In addition to the standard Transformer in which the residual connection is applied between two adjacent 151 • T2S model: Back-translation has thus far been the most effective technique effective for NMT (Sennrich et al., 2016b). Instead of us"
2020.iwslt-1.18,W17-3209,0,0.0162021,"(Kudo, 2006) and then tokenized only for the non-Japanese part by the Moses script2 . 1 https://github.com/BYVoid/OpenCC https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/ tokenizer/tokenizer.perl 2 149 Proceedings of the 17th International Conference on Spoken Language Translation (IWSLT), pages 149–157 c July 9-10, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 2.2 Parallel Data Filtering • Duplicated sentence pairs are discarded. Though the NMT performance is highly correlated to the huge amounts of training data, a robust body of studies (Carpuat et al., 2017; Khayrallah and Koehn, 2018; Wang et al., 2018; Koehn et al., 2018) has shown the bad impact of noisy data on general NMT translation accuracy. In addition to a small amount of Japanese-Chinese parallel data3 from various public sources, the organizers also provide a large-scale but noisy parallel data4 extracted from a non-parallel web-crawled data through some similarity measures for parallel data mining. We apply a two-stage process consisting of rule-based filtering and model-based scoring to further filter harmful sentence pairs that are bound to negatively affect the quality of NMT syst"
2020.iwslt-1.18,W19-5206,0,0.0182554,"he layer size of the encoder and the second number represents the layer size of the decoder. In addition to the standard Transformer in which the residual connection is applied between two adjacent 151 • T2S model: Back-translation has thus far been the most effective technique effective for NMT (Sennrich et al., 2016b). Instead of using the synthetic training data produced by translating monolingual data in the target language into the source language conventionally, we prepend a special tag to all the source sentences from the synthetic data to distinguish synthetic data from original data (Caswell et al., 2019). • R2L model: Generally, most NMT systems produce translations in an L2R way, which suffers from the issue of exposure bias and consequent error propagation (Ranzato et al., 2016). It has been observed that the accuracy of the right part words in its translation results is usually worse than the left part words (Zhang et al., 2018; Zhou et al., 2019). We train all the baseline systems separately using L2R 6 https://github.com/tensorflow/ tensor2tensor/blob/master/tensor2tensor/ models/evolved_transformer.py and R2L decoding (Wang et al., 2017; Hassan et al., 2018). • L2R model: Knowledge dist"
2020.iwslt-1.18,P17-1176,0,0.0202212,"h suffers from the issue of exposure bias and consequent error propagation (Ranzato et al., 2016). It has been observed that the accuracy of the right part words in its translation results is usually worse than the left part words (Zhang et al., 2018; Zhou et al., 2019). We train all the baseline systems separately using L2R 6 https://github.com/tensorflow/ tensor2tensor/blob/master/tensor2tensor/ models/evolved_transformer.py and R2L decoding (Wang et al., 2017; Hassan et al., 2018). • L2R model: Knowledge distillation has been widely applied to NMT (Kim and Rush, 2016; Freitag et al., 2017; Chen et al., 2017; Gu et al., 2018; Tan et al., 2019). Recent work (Furlanello et al., 2018) demonstrates that the student model can surpass the accuracy of the teacher model, even if the student model is identical to their teacher model. Following this work, the teacher and student models in our experiments keep the same architecture. 3.3 Model Ensembling Ensemble decoding is an effective approach to boost the accuracy of NMT systems via averaging the word distributions output from multiple single models at each decoding step. We select the top 4 systems with the highest BLEU evaluated on the development data"
2020.iwslt-1.18,N19-1423,0,0.0085212,"llel corpus (RFPD). Afterward, we select better sentences according to these scores. 2.2.1 Rule-based Filtering During the first stage, we remove some illegal parallel sentences by applying several rule-based heuristics. A sentence pair is deleted from the corpus if its source side or target side fails to obey any of the following wild rules reflecting what ‘good data’ should look like. Some of the heuristic filtering methods can deal with aspects that can not be captured with models. • SBERT model: Recently, contextualized word embeddings derived from large-scale pre-trained language models (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019) have achieved new state-of-the-arts in various monolingual NLP tasks. The success has also been extended to cross-lingual scenarios (Schwenk, 2018; Conneau and Lample, 2019; Mulcaire et al., 2019; Artetxe and Schwenk, 2019). Recently, Reimers and Gurevych (2019) proposed sentence BERT (SBERT) to derive semantically meaningful sentence embeddings. According to the training framework of SBERT, we use the multilingual pre-train BERT model5 and finetune it on RFPD to yield useful Chinese and Japanese sentence embeddings in the same space. We reject sentence p"
2020.iwslt-1.18,N13-1073,0,0.0357366,"emoved. 3 https://iwslt.oss-cn-beijing.aliyuncs. com/existing_parallel.tgz 4 https://iwslt.oss-cn-beijing.aliyuncs. com/web_crawled_parallel_filtered_1.1. tgz 150 • Translation model: We construct parallel NMT systems based on the standard Transformer-big model in both directions using RFPD to obtain the target synthetic translation as the reference. BEER (Stanojevi´c and Sima’an, 2014) is used as a sentence-level metric of sentence similarity. We prune the sentence pairs with the BEER score of lower than 0.2. • Word alignment model: We perform a word alignment model on RFPD using fast align (Dyer et al., 2013) to check whether the sentence pair has the same meaning. Sentence pairs with the alignment probability of being each other translation less than 0.1 are discarded. • N-gram LM: It is beneficial to use fluent sentences for training NMT models. We train a 5-gram LM that is estimated with modified Kneser-Ney smoothing (Kneser and Ney, 5 https://storage.googleapis.com/bert_ models/2018_11_23/multi_cased_L-12_ H-768_A-12.zip 1995) using KenLM (Heafield, 2011) on each side of the parallel sentences to evaluate sentences’ naturalness. We normalize the LM perplexity (PPL) scores of all the sentences"
2020.iwslt-1.18,W11-2123,0,0.0586679,"Missing"
2020.iwslt-1.18,N16-1046,0,0.0227976,"tems via averaging the word distributions output from multiple single models at each decoding step. We select the top 4 systems with the highest BLEU evaluated on the development dataset from all the available baseline systems of each direction for models ensembling. 3.4 Reranking Reranking technique (Shen et al., 2004) has been applied in the recent years’ WMT tasks (Sennrich et al., 2016a; Wang et al., 2017; Ng et al., 2019) and have provided significant improvements. We first use the S2T-L2R and S2T-R2L ensemble systems to generate more diverse translation hypotheses for a source sentence (Liu et al., 2016). Then we use ensemble models of S2T-L2R, S2T-R2L and T2S-L2R to calculate 3 different likelihood scores for each sentence pair. We obtain the perplexity score for the translation candidates with a neural LM based on the Transformer encoder. We also employ SBERT to calculate the similarity score for each sentence pair. Each model’s score is treated as an individual feature. Considering the ranking problem as a classification problem, we employ the implementation of pairwise ranking in scikit-learn7 RankSVM (Joachims, 2006) to learn the weights of all the features on the development data for re"
2020.iwslt-1.18,2021.ccl-1.108,0,0.115026,"Missing"
2020.iwslt-1.18,N19-1392,0,0.0173608,"pair is deleted from the corpus if its source side or target side fails to obey any of the following wild rules reflecting what ‘good data’ should look like. Some of the heuristic filtering methods can deal with aspects that can not be captured with models. • SBERT model: Recently, contextualized word embeddings derived from large-scale pre-trained language models (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019) have achieved new state-of-the-arts in various monolingual NLP tasks. The success has also been extended to cross-lingual scenarios (Schwenk, 2018; Conneau and Lample, 2019; Mulcaire et al., 2019; Artetxe and Schwenk, 2019). Recently, Reimers and Gurevych (2019) proposed sentence BERT (SBERT) to derive semantically meaningful sentence embeddings. According to the training framework of SBERT, we use the multilingual pre-train BERT model5 and finetune it on RFPD to yield useful Chinese and Japanese sentence embeddings in the same space. We reject sentence pairs with a cosine-similarity score below 0.2. • The token (i.e. character sequence between two spaces) length of every sentence is limited less than 50. • Sentence pairs with a length ratio greater than 4 are removed. • Chinese sente"
2020.iwslt-1.18,W19-5333,0,0.0191262,"this work, the teacher and student models in our experiments keep the same architecture. 3.3 Model Ensembling Ensemble decoding is an effective approach to boost the accuracy of NMT systems via averaging the word distributions output from multiple single models at each decoding step. We select the top 4 systems with the highest BLEU evaluated on the development dataset from all the available baseline systems of each direction for models ensembling. 3.4 Reranking Reranking technique (Shen et al., 2004) has been applied in the recent years’ WMT tasks (Sennrich et al., 2016a; Wang et al., 2017; Ng et al., 2019) and have provided significant improvements. We first use the S2T-L2R and S2T-R2L ensemble systems to generate more diverse translation hypotheses for a source sentence (Liu et al., 2016). Then we use ensemble models of S2T-L2R, S2T-R2L and T2S-L2R to calculate 3 different likelihood scores for each sentence pair. We obtain the perplexity score for the translation candidates with a neural LM based on the Transformer encoder. We also employ SBERT to calculate the similarity score for each sentence pair. Each model’s score is treated as an individual feature. Considering the ranking problem as a"
2020.iwslt-1.18,P02-1040,0,0.112141,"U. All models are trained on one machine with 8 NVIDIA V100 GPUs each of which has 16GB memory for a total of 200K steps. We optimize all models against BLEU using the development set provided by the organizer, stopping early if BLEU does not improve for 16 checkpoints of 2,000 updates each. We set dropout 0.1 for Chinese→Japanese and 0.2 is for Japanese→Chinese. We average the top 10 checkpoints evaluated against the development set as the final model for decoding. During decoding, the beam size is set to 4 for the single model and 10 for ensemble models. We report the 4-gram character BLEU (Papineni et al., 2002) evaluated by the provided automatic evaluation script8 . The approach of two-stage parallel data filtering in Section 2.2 enables us to drastically reduce the training data from 19M to 12M. In order to enlarge the size of bilingual data, we also exploit to extract more high-quality sentence pairs from the provided pre-filtered parallel data9 . We first pre-process the data and use the rules in Section 2.2 to remove 7 https://scikit-learn.org/stable/ modules/generated/sklearn.svm.LinearSVC. html 152 8 https://github.com/didi/iwslt2020_ open_domain_translation/blob/master/ scripts/multi-bleu-de"
2020.iwslt-1.18,D19-1410,0,0.0140789,"side fails to obey any of the following wild rules reflecting what ‘good data’ should look like. Some of the heuristic filtering methods can deal with aspects that can not be captured with models. • SBERT model: Recently, contextualized word embeddings derived from large-scale pre-trained language models (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019) have achieved new state-of-the-arts in various monolingual NLP tasks. The success has also been extended to cross-lingual scenarios (Schwenk, 2018; Conneau and Lample, 2019; Mulcaire et al., 2019; Artetxe and Schwenk, 2019). Recently, Reimers and Gurevych (2019) proposed sentence BERT (SBERT) to derive semantically meaningful sentence embeddings. According to the training framework of SBERT, we use the multilingual pre-train BERT model5 and finetune it on RFPD to yield useful Chinese and Japanese sentence embeddings in the same space. We reject sentence pairs with a cosine-similarity score below 0.2. • The token (i.e. character sequence between two spaces) length of every sentence is limited less than 50. • Sentence pairs with a length ratio greater than 4 are removed. • Chinese sentences with Chinese characters ratio less than 0.15 or any character"
2020.iwslt-1.18,W18-2709,0,0.0130258,"tokenized only for the non-Japanese part by the Moses script2 . 1 https://github.com/BYVoid/OpenCC https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/ tokenizer/tokenizer.perl 2 149 Proceedings of the 17th International Conference on Spoken Language Translation (IWSLT), pages 149–157 c July 9-10, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 2.2 Parallel Data Filtering • Duplicated sentence pairs are discarded. Though the NMT performance is highly correlated to the huge amounts of training data, a robust body of studies (Carpuat et al., 2017; Khayrallah and Koehn, 2018; Wang et al., 2018; Koehn et al., 2018) has shown the bad impact of noisy data on general NMT translation accuracy. In addition to a small amount of Japanese-Chinese parallel data3 from various public sources, the organizers also provide a large-scale but noisy parallel data4 extracted from a non-parallel web-crawled data through some similarity measures for parallel data mining. We apply a two-stage process consisting of rule-based filtering and model-based scoring to further filter harmful sentence pairs that are bound to negatively affect the quality of NMT systems from the original parall"
2020.iwslt-1.18,D16-1139,0,0.0257798,"s produce translations in an L2R way, which suffers from the issue of exposure bias and consequent error propagation (Ranzato et al., 2016). It has been observed that the accuracy of the right part words in its translation results is usually worse than the left part words (Zhang et al., 2018; Zhou et al., 2019). We train all the baseline systems separately using L2R 6 https://github.com/tensorflow/ tensor2tensor/blob/master/tensor2tensor/ models/evolved_transformer.py and R2L decoding (Wang et al., 2017; Hassan et al., 2018). • L2R model: Knowledge distillation has been widely applied to NMT (Kim and Rush, 2016; Freitag et al., 2017; Chen et al., 2017; Gu et al., 2018; Tan et al., 2019). Recent work (Furlanello et al., 2018) demonstrates that the student model can surpass the accuracy of the teacher model, even if the student model is identical to their teacher model. Following this work, the teacher and student models in our experiments keep the same architecture. 3.3 Model Ensembling Ensemble decoding is an effective approach to boost the accuracy of NMT systems via averaging the word distributions output from multiple single models at each decoding step. We select the top 4 systems with the highe"
2020.iwslt-1.18,W18-6453,0,0.0220119,"Moses script2 . 1 https://github.com/BYVoid/OpenCC https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/ tokenizer/tokenizer.perl 2 149 Proceedings of the 17th International Conference on Spoken Language Translation (IWSLT), pages 149–157 c July 9-10, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 2.2 Parallel Data Filtering • Duplicated sentence pairs are discarded. Though the NMT performance is highly correlated to the huge amounts of training data, a robust body of studies (Carpuat et al., 2017; Khayrallah and Koehn, 2018; Wang et al., 2018; Koehn et al., 2018) has shown the bad impact of noisy data on general NMT translation accuracy. In addition to a small amount of Japanese-Chinese parallel data3 from various public sources, the organizers also provide a large-scale but noisy parallel data4 extracted from a non-parallel web-crawled data through some similarity measures for parallel data mining. We apply a two-stage process consisting of rule-based filtering and model-based scoring to further filter harmful sentence pairs that are bound to negatively affect the quality of NMT systems from the original parallel corpora as follows. 2.2.2 Model-based"
2020.iwslt-1.18,P18-2037,0,0.0212184,"everal rule-based heuristics. A sentence pair is deleted from the corpus if its source side or target side fails to obey any of the following wild rules reflecting what ‘good data’ should look like. Some of the heuristic filtering methods can deal with aspects that can not be captured with models. • SBERT model: Recently, contextualized word embeddings derived from large-scale pre-trained language models (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019) have achieved new state-of-the-arts in various monolingual NLP tasks. The success has also been extended to cross-lingual scenarios (Schwenk, 2018; Conneau and Lample, 2019; Mulcaire et al., 2019; Artetxe and Schwenk, 2019). Recently, Reimers and Gurevych (2019) proposed sentence BERT (SBERT) to derive semantically meaningful sentence embeddings. According to the training framework of SBERT, we use the multilingual pre-train BERT model5 and finetune it on RFPD to yield useful Chinese and Japanese sentence embeddings in the same space. We reject sentence pairs with a cosine-similarity score below 0.2. • The token (i.e. character sequence between two spaces) length of every sentence is limited less than 50. • Sentence pairs with a length"
2020.iwslt-1.18,W16-2323,0,0.415961,"ed filtering and model-based scoring. In the aspect of NMT architecture, we exploit some recent Transformer variants, including different Transformer models with deeper layers or wider inner dimension of feed-forward layers than the standard Transformer-Big model, Transformer with a dynamic linear combination of layers (DLCL) (Wang et al., 2019) and neural architecture search (NAS) based Transformer-Evolved (So et al., 2019), to increase the diversity of the system. We further strengthen our systems by diversifying the training data via some effective methods, including back-translation (BT) (Sennrich et al., 2016b), 2 Data 2.1 Pre-processing Our pre-processing pipeline begins by removing non-printable ASCII characters, lowercasing text, normalizing additional white-space, and control character and replacing any escaped characters with the corresponding symbol by our in-house script. All the data is further normalized so all full-width Roman characters and digits are normalized to half-width. All the traditional characters of Chinese data are converted to simplified characters using OpenCC1 . For all corpora, Chinese sentences are segmented by our in-house Chinese word segmenter, and Japanese sentences"
2020.iwslt-1.18,P16-1009,0,0.599685,"ed filtering and model-based scoring. In the aspect of NMT architecture, we exploit some recent Transformer variants, including different Transformer models with deeper layers or wider inner dimension of feed-forward layers than the standard Transformer-Big model, Transformer with a dynamic linear combination of layers (DLCL) (Wang et al., 2019) and neural architecture search (NAS) based Transformer-Evolved (So et al., 2019), to increase the diversity of the system. We further strengthen our systems by diversifying the training data via some effective methods, including back-translation (BT) (Sennrich et al., 2016b), 2 Data 2.1 Pre-processing Our pre-processing pipeline begins by removing non-printable ASCII characters, lowercasing text, normalizing additional white-space, and control character and replacing any escaped characters with the corresponding symbol by our in-house script. All the data is further normalized so all full-width Roman characters and digits are normalized to half-width. All the traditional characters of Chinese data are converted to simplified characters using OpenCC1 . For all corpora, Chinese sentences are segmented by our in-house Chinese word segmenter, and Japanese sentences"
2020.iwslt-1.18,P16-1162,0,0.772165,"ed filtering and model-based scoring. In the aspect of NMT architecture, we exploit some recent Transformer variants, including different Transformer models with deeper layers or wider inner dimension of feed-forward layers than the standard Transformer-Big model, Transformer with a dynamic linear combination of layers (DLCL) (Wang et al., 2019) and neural architecture search (NAS) based Transformer-Evolved (So et al., 2019), to increase the diversity of the system. We further strengthen our systems by diversifying the training data via some effective methods, including back-translation (BT) (Sennrich et al., 2016b), 2 Data 2.1 Pre-processing Our pre-processing pipeline begins by removing non-printable ASCII characters, lowercasing text, normalizing additional white-space, and control character and replacing any escaped characters with the corresponding symbol by our in-house script. All the data is further normalized so all full-width Roman characters and digits are normalized to half-width. All the traditional characters of Chinese data are converted to simplified characters using OpenCC1 . For all corpora, Chinese sentences are segmented by our in-house Chinese word segmenter, and Japanese sentences"
2020.iwslt-1.18,N04-1023,0,0.138253,"ass the accuracy of the teacher model, even if the student model is identical to their teacher model. Following this work, the teacher and student models in our experiments keep the same architecture. 3.3 Model Ensembling Ensemble decoding is an effective approach to boost the accuracy of NMT systems via averaging the word distributions output from multiple single models at each decoding step. We select the top 4 systems with the highest BLEU evaluated on the development dataset from all the available baseline systems of each direction for models ensembling. 3.4 Reranking Reranking technique (Shen et al., 2004) has been applied in the recent years’ WMT tasks (Sennrich et al., 2016a; Wang et al., 2017; Ng et al., 2019) and have provided significant improvements. We first use the S2T-L2R and S2T-R2L ensemble systems to generate more diverse translation hypotheses for a source sentence (Liu et al., 2016). Then we use ensemble models of S2T-L2R, S2T-R2L and T2S-L2R to calculate 3 different likelihood scores for each sentence pair. We obtain the perplexity score for the translation candidates with a neural LM based on the Transformer encoder. We also employ SBERT to calculate the similarity score for eac"
2020.iwslt-1.18,W14-3354,0,0.0684995,"Missing"
2020.iwslt-1.18,W18-6312,0,0.0159087,"verage BLEU score of 0.89. We attribute this finding to the quality gap between the provided Chinese and Japanese data. Table 2a shows that adding large-scale synthetic parallel data back-translated from external monolingual data further boost the performance in different degree. Both the best baseline systems obtain a significant improvement by 1.32 1.32 BLEU score for Zh→Ja. However, it is currently not clear to us how to interpret on the marginal improvement for Ja→Zh. There is a reason to conjecture that we might be suffering from reference bias towards translationese and non-native data (Toral et al., 2018). Unsurprisingly, utilizing diverse models with homogeneous architectures to the ensemble improves translation quality across both the tasks in different degrees. In constrained condition, the Zh→Ja ensemble models gain a substantial improvement compared to the baseline From the Table 2a, our reranking model finally achieves a significant improvement of about 1.4 BLEU score for Zh→Ja, even when applied on top of an ensemble of very strong KD+BT models. However, the improvement of reranking is relatively inconsiderable for Ja→Zh, and we also attribute this to the issue of translationese referen"
2020.iwslt-1.18,W17-4742,0,0.0234332,"distinguish synthetic data from original data (Caswell et al., 2019). • R2L model: Generally, most NMT systems produce translations in an L2R way, which suffers from the issue of exposure bias and consequent error propagation (Ranzato et al., 2016). It has been observed that the accuracy of the right part words in its translation results is usually worse than the left part words (Zhang et al., 2018; Zhou et al., 2019). We train all the baseline systems separately using L2R 6 https://github.com/tensorflow/ tensor2tensor/blob/master/tensor2tensor/ models/evolved_transformer.py and R2L decoding (Wang et al., 2017; Hassan et al., 2018). • L2R model: Knowledge distillation has been widely applied to NMT (Kim and Rush, 2016; Freitag et al., 2017; Chen et al., 2017; Gu et al., 2018; Tan et al., 2019). Recent work (Furlanello et al., 2018) demonstrates that the student model can surpass the accuracy of the teacher model, even if the student model is identical to their teacher model. Following this work, the teacher and student models in our experiments keep the same architecture. 3.3 Model Ensembling Ensemble decoding is an effective approach to boost the accuracy of NMT systems via averaging the word dist"
2020.iwslt-1.18,P19-1558,0,0.0153448,"the original data to train the baseline models from scratch in L2R, R2L, and T2S ways, respectively. Finally, we conduct the aforementioned approach based on ensemble models again to achieve better baseline systems. • Wider model: Dimension is an important factor to enhance the Transformer model capacity and performance. Based on the standard Transformer-Big model, we train a Transformer-Wide model with a inner dimension of position-wise feed-forward layers 8,192. • Deeper model: Building deeper networks via stacking more encoder and decoder layers has been a trend in NMT (Bapna et al., 2018; Wu et al., 2019; Zhang et al., 2019). We also exploit three deeper Transformer models by simply increasing the layer size of Transformer-Big, including TransformerDeep-12-12, Transformer-Deep-12-6, and Transformer-Deep-6-12 in which the first number represents the layer size of the encoder and the second number represents the layer size of the decoder. In addition to the standard Transformer in which the residual connection is applied between two adjacent 151 • T2S model: Back-translation has thus far been the most effective technique effective for NMT (Sennrich et al., 2016b). Instead of using the synthetic"
2020.iwslt-1.18,D19-1083,0,0.0138457,"to train the baseline models from scratch in L2R, R2L, and T2S ways, respectively. Finally, we conduct the aforementioned approach based on ensemble models again to achieve better baseline systems. • Wider model: Dimension is an important factor to enhance the Transformer model capacity and performance. Based on the standard Transformer-Big model, we train a Transformer-Wide model with a inner dimension of position-wise feed-forward layers 8,192. • Deeper model: Building deeper networks via stacking more encoder and decoder layers has been a trend in NMT (Bapna et al., 2018; Wu et al., 2019; Zhang et al., 2019). We also exploit three deeper Transformer models by simply increasing the layer size of Transformer-Big, including TransformerDeep-12-12, Transformer-Deep-12-6, and Transformer-Deep-6-12 in which the first number represents the layer size of the encoder and the second number represents the layer size of the decoder. In addition to the standard Transformer in which the residual connection is applied between two adjacent 151 • T2S model: Back-translation has thus far been the most effective technique effective for NMT (Sennrich et al., 2016b). Instead of using the synthetic training data produc"
2020.iwslt-1.18,Q19-1006,0,0.0141934,"aining data produced by translating monolingual data in the target language into the source language conventionally, we prepend a special tag to all the source sentences from the synthetic data to distinguish synthetic data from original data (Caswell et al., 2019). • R2L model: Generally, most NMT systems produce translations in an L2R way, which suffers from the issue of exposure bias and consequent error propagation (Ranzato et al., 2016). It has been observed that the accuracy of the right part words in its translation results is usually worse than the left part words (Zhang et al., 2018; Zhou et al., 2019). We train all the baseline systems separately using L2R 6 https://github.com/tensorflow/ tensor2tensor/blob/master/tensor2tensor/ models/evolved_transformer.py and R2L decoding (Wang et al., 2017; Hassan et al., 2018). • L2R model: Knowledge distillation has been widely applied to NMT (Kim and Rush, 2016; Freitag et al., 2017; Chen et al., 2017; Gu et al., 2018; Tan et al., 2019). Recent work (Furlanello et al., 2018) demonstrates that the student model can surpass the accuracy of the teacher model, even if the student model is identical to their teacher model. Following this work, the teache"
2020.starsem-1.2,2021.ccl-1.108,0,0.0705088,"Missing"
2020.starsem-1.2,K18-2016,0,0.0325535,"Missing"
2020.starsem-1.2,N19-1302,0,0.0472731,"Missing"
2020.starsem-1.2,D15-1075,0,0.0446301,"bdivided into “neutral” and “contradiction”. However, we only use the two-class version of the problem in this work. 12 Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics (*SEM), pages 12–19 Barcelona, Spain (Online), December 12–13, 2020 (a) QA sample (b) Converted NLI sample Figure 1: A RC sample with multiple answer choices converted to an NLI sample. questions. This establishes the usefulness of the converted datasets. Jin et al. (2019) show that despite the different form of NLI and QA tasks, performing coarse pretraining of models on NLI datasets like SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018) not only improves the performance of these models on downstream reading comprehension tasks, but also helps with faster convergence. We show that – for certain types of questions in reading comprehension datasets – simply transforming the task to NLI can show improvement in performance, even without pretraining on any NLI dataset. Trivedi et al. (2019) introduced a learnt weightand-combine architecture to effectively re-purpose pretrained entailment models (trained on SNLI and MultiNLI) to solve the task of multi-hop reading comprehension. They show that f"
2020.starsem-1.2,P19-1363,0,0.0148672,". We show that converting a RC task to an NLI task helps in answering certain types of Given two sentences, a premise and a hypothesis, the task of Natural Language Inference (NLI) is to determine whether the premise entails the hypothesis or not. † The concept of semantic entailment is central to natural language understanding (Van Benthem et al., 2008; MacCartney and Manning, 2009) and therefore, NLI models have been used to help with various downstream tasks like reading comprehension (Trivedi et al., 2019), summarization (Falke et al., 2019; Kry´sci´nski et al., 2019), and dialog systems (Welleck et al., 2019). However, the performance of an NLI system on these down∗ Equal contribution. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/. † The “not entailment” can further be subdivided into “neutral” and “contradiction”. However, we only use the two-class version of the problem in this work. 12 Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics (*SEM), pages 12–19 Barcelona, Spain (Online), December 12–13, 2020 (a) QA sample (b) Converted NLI sample Figure 1: A RC sample wi"
2021.acl-long.353,E06-1040,0,0.0750714,"Missing"
2021.acl-long.353,N19-1423,0,0.0913987,"Missing"
2021.acl-long.353,W17-3518,0,0.0321664,"e the matrix Pθ [i, :] = MLPθ (Pθ0 [i, :]) by a smaller matrix (Pθ0 ) composed with a large feedforward neural network (MLPθ ). Now, the trainable parameters include Pθ0 and the parameters of MLPθ . Note that Pθ and Pθ0 has the same number of rows (i.e., the prefix length), but different number of columns.4 Once training is complete, these reparametrization parameters can be dropped, and only the prefix (Pθ ) needs to be saved. 5 Experimental Setup 5.1 Datasets and Metrics We evaluate on three standard neural generation datasets for the table-to-text task: E2E (Novikova et al., 2017), WebNLG (Gardent et al., 2017), and DART (Radev et al., 2020), as shown in Table 1. The datasets are ordered by increasing complexity and size. E2E only has 1 domain (i.e. restaurant reviews); WebNLG has 14 domains, and DART is open-domain, using open-domain tables from Wikipedia. For evaluation, we report the metrics using the official evaluation scripts (see details in Appendix A.1). For the summarization task, we use the XSUM (Narayan et al., 2018) dataset, which is an abstractive summarization dataset on news articles. We report ROUGE-1, ROUGE-2 and ROUGE-L. 5.2 Methods For table-to-text generation, we compare prefixtu"
2021.acl-long.353,2021.acl-long.381,0,0.177252,"s no straightforward way to apply these controllable generation techniques to enforce fine-grained control over generated contents, as demanded by tasks like table-to-text and summarization. P*-tuning. Prefix tuning is an instance of a new class of methods that has emerged, which we call p*-tuning (since the other prominent instances, ptuning and prompt-tuning, also start with p), all based on the idea of optimizing a continuous prefix or prompt. Concurrent with our work, Qin and Eisner (2021) learn mixtures of soft fill-in-the-blank prompts to elicit knowledge from LMs such as BERT and BART. Hambardzumyan et al. (2021) learns task-specific embeddings that adapts BERT for sentiment classification. Both works show that tuning soft prompts outperforms previous work, which optimizes over discrete prompts. P-tuning (Liu et al., 2021) shows that jointly updating the prompt embeddings and LM parameters improves GPT-2’s performance on natural language understanding tasks, in both few-shot and full data settings. In a followup work, Prompt-tuning (Lester et al., 2021) simplifies our approach and applies it to T5 (Raffel et al., 2020), demonstrating that the performance gap between fine-tuning and p*tuning vanishes a"
2021.acl-long.353,2020.tacl-1.28,0,0.191809,"Missing"
2021.acl-long.353,2020.inlg-1.14,0,0.24164,"performance when trained on full datasets, prefix-tuning and fine-tuning are comparable for table-to-text (§6.1), while prefix-tuning suffers a small degradation for summarization (§6.2). In low-data settings, prefix-tuning outperforms finetuning on both tasks (§6.3). Prefix-tuning also extrapolates better to tables (for table-to-text) and articles (for summarization) with unseen topics (§6.4). 2 Related Work Fine-tuning for natural language generation. Current state-of-the-art systems for natural language generation (NLG) are based on fine-tuning pretrained LMs. For table-to-text generation, Kale (2020) fine-tunes a sequence-to-sequence model (T5; Raffel et al., 2020). For extractive and abstractive summarization, researchers fine-tune masked language models (e.g., BERT; Devlin et al., 2019) and encode-decoder models (e.g., BART; Lewis et al., 2020), respectively (Zhong et al., 2020; Liu and Lapata, 2019; Raffel et al., 2020). For other conditional NLG tasks such as machine translation and dialogue generation, fine-tuning is also the prevalent paradigm (Zhang et al., 2020c; Stickland et al., 2020; Zhu et al., 2020; Liu et al., 2020). In this paper, we focus on table-to-text using GPT-2 and s"
2021.acl-long.353,W07-0734,0,0.186271,"Missing"
2021.acl-long.353,2021.emnlp-main.243,0,0.336004,"rrent with our work, Qin and Eisner (2021) learn mixtures of soft fill-in-the-blank prompts to elicit knowledge from LMs such as BERT and BART. Hambardzumyan et al. (2021) learns task-specific embeddings that adapts BERT for sentiment classification. Both works show that tuning soft prompts outperforms previous work, which optimizes over discrete prompts. P-tuning (Liu et al., 2021) shows that jointly updating the prompt embeddings and LM parameters improves GPT-2’s performance on natural language understanding tasks, in both few-shot and full data settings. In a followup work, Prompt-tuning (Lester et al., 2021) simplifies our approach and applies it to T5 (Raffel et al., 2020), demonstrating that the performance gap between fine-tuning and p*tuning vanishes as the model size grows. 3 Problem Statement Consider a conditional generation task where the input x is a context and the output y is a sequence of tokens. We focus on two tasks, shown in Figure 2 (right): In table-to-text, x corresponds to a linearized data table and y is a textual description; in summarization, x is an article and y is a summary. 3.1 Autoregressive LM Assume we have an autoregressive neural language model pφ (y |x) parametrize"
2021.acl-long.353,2020.acl-main.703,0,0.528021,"ing on both tasks (§6.3). Prefix-tuning also extrapolates better to tables (for table-to-text) and articles (for summarization) with unseen topics (§6.4). 2 Related Work Fine-tuning for natural language generation. Current state-of-the-art systems for natural language generation (NLG) are based on fine-tuning pretrained LMs. For table-to-text generation, Kale (2020) fine-tunes a sequence-to-sequence model (T5; Raffel et al., 2020). For extractive and abstractive summarization, researchers fine-tune masked language models (e.g., BERT; Devlin et al., 2019) and encode-decoder models (e.g., BART; Lewis et al., 2020), respectively (Zhong et al., 2020; Liu and Lapata, 2019; Raffel et al., 2020). For other conditional NLG tasks such as machine translation and dialogue generation, fine-tuning is also the prevalent paradigm (Zhang et al., 2020c; Stickland et al., 2020; Zhu et al., 2020; Liu et al., 2020). In this paper, we focus on table-to-text using GPT-2 and summarization using BART, but prefix-tuning in principle can be applied to other generation tasks and pretrained models, such as masked LMs. Lightweight fine-tuning. Prefix-tuning falls under the broad class of lightweight fine-tuning methods, which fr"
2021.acl-long.353,W04-1013,0,0.0685606,"Missing"
2021.acl-long.353,D18-1206,0,0.0166297,"saved. 5 Experimental Setup 5.1 Datasets and Metrics We evaluate on three standard neural generation datasets for the table-to-text task: E2E (Novikova et al., 2017), WebNLG (Gardent et al., 2017), and DART (Radev et al., 2020), as shown in Table 1. The datasets are ordered by increasing complexity and size. E2E only has 1 domain (i.e. restaurant reviews); WebNLG has 14 domains, and DART is open-domain, using open-domain tables from Wikipedia. For evaluation, we report the metrics using the official evaluation scripts (see details in Appendix A.1). For the summarization task, we use the XSUM (Narayan et al., 2018) dataset, which is an abstractive summarization dataset on news articles. We report ROUGE-1, ROUGE-2 and ROUGE-L. 5.2 Methods For table-to-text generation, we compare prefixtuning with three other methods: full fine-tuning 3 We find in preliminary experiments that directly optimizing the prefix is very sensitive to initialization. 4 Pθ has dimensions |Pidx |× dim(hi ) while Pθ has dimensions |Pidx |× k. We choose k = 512 for table-to-text and 800 for summarization. MLPθ maps from k to dim(hi ). #examples input length output length E2E WebNLG DART 50K 22K 82K 28.5 49.6 38.8 27.8 30.7 27.3 XSUM"
2021.acl-long.353,W17-5525,0,0.0318707,"performance.3 So we reparametrize the matrix Pθ [i, :] = MLPθ (Pθ0 [i, :]) by a smaller matrix (Pθ0 ) composed with a large feedforward neural network (MLPθ ). Now, the trainable parameters include Pθ0 and the parameters of MLPθ . Note that Pθ and Pθ0 has the same number of rows (i.e., the prefix length), but different number of columns.4 Once training is complete, these reparametrization parameters can be dropped, and only the prefix (Pθ ) needs to be saved. 5 Experimental Setup 5.1 Datasets and Metrics We evaluate on three standard neural generation datasets for the table-to-text task: E2E (Novikova et al., 2017), WebNLG (Gardent et al., 2017), and DART (Radev et al., 2020), as shown in Table 1. The datasets are ordered by increasing complexity and size. E2E only has 1 domain (i.e. restaurant reviews); WebNLG has 14 domains, and DART is open-domain, using open-domain tables from Wikipedia. For evaluation, we report the metrics using the official evaluation scripts (see details in Appendix A.1). For the summarization task, we use the XSUM (Narayan et al., 2018) dataset, which is an abstractive summarization dataset on news articles. We report ROUGE-1, ROUGE-2 and ROUGE-L. 5.2 Methods For table-to-text"
2021.acl-long.353,2020.acl-main.704,0,0.0425012,"Missing"
2021.acl-long.353,P02-1040,0,0.110333,"Missing"
2021.acl-long.480,P17-2031,0,0.0447906,"Missing"
2021.acl-long.480,W18-6326,0,0.0608765,"Missing"
2021.acl-long.480,N19-4009,0,0.0220643,"swani et al., 2017) and (Wu et al., 2018). We perform 2 https://github.com/multi30k/dataset beam search with beam size set to 5. We report 4-gram BLEU and METEOR scores for all test sets. All models are trained and evaluated on one single machine with two Titan P100 GPUs. 4.3 Baselines Our baselines can be categorized into three types: • The text-only Transformer; • The conventional MMT models: Doubly-ATT and Imagination; • The retrieval-based MMT models: UVR-NMT. Details of these methods can be found in Section 2. For fairness, all the baselines are implemented by ourselves based on FairSeq (Ott et al., 2019). We use top-5 retrieved images for both UVR-NMT and our RMMT. We also consider two more recent state-of-the-art conventional methods for reference: GMNMT (Yin et al., 2020) and DCCN (Lin et al., 2020), whose results are reported as in their papers. Note that most MMT methods are difficult (or even impossible) to interpret. While there exist some interpretable methods (e.g., UVR-NMT) that contain gated fusion layers similar to ours, they perform sophisticated transformations on visual representation before fusion, which lowers the interpretability of the gating matrix. For example, in the gate"
2021.acl-long.480,W18-5713,0,0.0231068,"relies on the availability of images with bilingual annotations. This could restrict its wide applicability. To address this issue, Zhang et al. (2020) propose UVR-NMT that integrates a retrieval component into MMT. They use TF-IDF to build a token-to-image lookup table, based on which images sharing similar topics with a source sentence are retrieved as relevant images. This creates image-bilingual-annotation instances for training. Retrieval-based models have been shown to improve performance across a variety of NLP tasks besides MMT, such as question answering (Guu et al., 2020), dialogue (Weston et al., 2018), language modeling (Khandelwal et al., 2019), question generation (Lewis et al., 2020), and translation (Gu et al., 2018). 3 Method In this section we introduce two interpretable MMT models: (1) Gated Fusion for conventional MMT and (2) Dense-Retrieval-augmented MMT (RMMT) for retrieval-based MMT. Our design philosophy is that models should learn, in an interpretable manner, to which degree multimodal information is used. Following this principle, we focus on the component that integrates multimodal information. In particular, we use a gating matrix Λ (Yin et al., 2020; Zhang et al., 2020) to"
2021.acl-long.480,2020.acl-main.400,0,0.0280371,"embeddings as additional input tokens (Huang et al., 2016; Calixto and Liu, 2017). Recent works (Libovick´y et al., 2018; Zhou et al., 2018; Ive et al., 2019; Lin et al., 2020) employ attention mechanism to generate a visual-aware representation for the decoder. For instance, Doubly-ATT (Calixto et al., 2017; Helcl et al., 2018; Arslan et al., 2018) insert an extra visual attention sub-layer between the decoder’s source-target attention sub-layer and feed-forward sub-layer. While there are more works on engineering decoders, encoder-based approaches are relatively less explored. To this end, Yao and Wan (2020) and Yin et al. (2020) replace the vanilla Transformer encoder with a multi-modal encoder. Besides the exploration on network structure, researchers also propose to leverage the benefits of multi-tasking to improve MMT (Elliott and K´ad´ar, 2017; Zhou et al., 2018). The Imagination architecture (Elliott and K´ad´ar, 2017; Helcl et al., 2018) decomposes multimodal translation into two subtasks: translation task and an auxiliary visual reconstruction task, which encourages the model to learn a visually grounded source sentence representation. Retrieval-based MMT The effectiveness of conventional"
2021.acl-long.480,2020.acl-main.273,0,0.163201,"irical findings that highlight the importance of MMT models’ interpretability, and discuss how our findings will benefit future research. 1 Introduction Multimodal Machine Translation (MMT) aims at designing better translation systems by extending conventional text-only translation systems to take into account multimodal information, especially from visual modality (Specia et al., 2016; Wang et al., 2019). Despite many previous success in MMT that report improvements when models are equipped with visual information (Calixto et al., 2017; Helcl et al., 2018; Ive et al., 2019; Lin et al., 2020; Yin et al., 2020), there have been continuing debates on the need for visual context in MMT. In particular, Specia et al. (2016); Elliott et al. (2017); Barrault et al. (2018) argue that visual context does not seem to help translation reliably, at ∗ The majority of this work was done while the first author was interning at Tencent AI Lab. least as measured by automatic metrics. Elliott (2018); Gr¨onroos et al. (2018a) provide further evidence by showing that MMT models are, in fact, insensitive to visual input and can translate without significant performance losses even in the presence of features derived fr"
2021.ccl-1.63,P15-2122,0,0.0126907,"Missing"
2021.ccl-1.63,W17-5537,0,0.0370177,"Missing"
2021.ccl-1.63,2020.figlang-1.14,0,0.0910163,"Missing"
2021.gem-1.16,2020.emnlp-main.525,0,0.284941,"s in a language generation setting, they perform human evaluations using a convincingness metric on a short-form news generation task; long-form narrative generation is not bound by realism (and may actually benefit from less realistic output), and thus requires different metrics and evaluation setups. Guided generation is the middle ground of cloze and open-ended generation. The model is provided more context, such as characters, plot information, and potentially other information, and then generates a story based on all of the provided structural and semantic information (Peng et al., 2018; Akoury et al., 2020). Decoding methods for generation Decoding refers to the inference methods used in natural language generation; given input sequence S, how should we construct the output sequence T ? Since finding the exact most probable token at each time step often does not produce human-like or highquality results (Zhang et al., 2020a; Holtzman et al., 2020), search and sampling are used to overcome label bias and generate more human-like language. One popular search method is beam search, where at each time step, the algorithm keeps track of the top B most probable partial hypotheses. When B = 1, this met"
2021.gem-1.16,2020.acl-main.164,0,0.245706,"ar. However, as narrative generation is largely focused on coherence across long outputs, the strategies used in this subfield have evolved separately ∗ † Equal contribution. Work performed while at Johns Hopkins University. from those in chatbot response generation; the latter has been more concerned with generating interesting and diverse—and typically short—outputs. Thus, while many beneficial techniques may have arisen from one domain, they are not often employed in the other. One decoding method, nucleus sampling (Holtzman et al., 2020), has recently been applied to narrative generation (Ippolito et al., 2020), but a thorough evaluation of its various p thresholds has not been performed with human judgments using narrative-specific criteria, as this can be time- and labor-intensive. Also, recent advances in decoding methods for response generation—notably, the application of the maximum mutual information (MMI) objective (Li et al., 2016a)—have resulted in more interesting dialog according to human evaluators (Zhang et al., 2020b); nonetheless, this also has not been applied to narrative generation. Indeed, the MMI objective has been confined to short-form and less openended generation tasks thus f"
2021.gem-1.16,W19-2405,0,0.115083,"settings than those found for short-form or constrained generation. Our preprocessing, training, generation, and analysis scripts are available publicly.1 2 Related Work Narrative generation tasks Work on narrative generation is split between cloze tasks, open-ended generation, and guided generation. In a cloze task, a full story except for a final word, phrase, or sentence is given, and a model generates a completion. This could be cast as a short generation problem— or, more commonly in this domain, a multiplechoice problem (Mostafazadeh et al., 2016; Weston et al., 2015; Hill et al., 2015; Ippolito et al., 2019a). Open-ended generation is the task of generating long-form output conditioned on a prompt (Figure 1). Fan et al. (2018) create a paired prompt and response dataset from the subreddit r/WritingPrompts2 to train a sequence-tosequence “fusion model.” See et al. (2019) extend Fan et al. (2018), but use GPT-2 small and perform a top-k decoding parameter sweep. We focus on this open-ended narrative generation task in our investigation, but primarily focus on GPT-2 Medium and on the effect of nucleus sampling thresholds 1 https://github.com/AADeLucia/ gpt2-narrative-decoding 2 https://www.reddit.c"
2021.gem-1.16,P19-1365,1,0.911047,"settings than those found for short-form or constrained generation. Our preprocessing, training, generation, and analysis scripts are available publicly.1 2 Related Work Narrative generation tasks Work on narrative generation is split between cloze tasks, open-ended generation, and guided generation. In a cloze task, a full story except for a final word, phrase, or sentence is given, and a model generates a completion. This could be cast as a short generation problem— or, more commonly in this domain, a multiplechoice problem (Mostafazadeh et al., 2016; Weston et al., 2015; Hill et al., 2015; Ippolito et al., 2019a). Open-ended generation is the task of generating long-form output conditioned on a prompt (Figure 1). Fan et al. (2018) create a paired prompt and response dataset from the subreddit r/WritingPrompts2 to train a sequence-tosequence “fusion model.” See et al. (2019) extend Fan et al. (2018), but use GPT-2 small and perform a top-k decoding parameter sweep. We focus on this open-ended narrative generation task in our investigation, but primarily focus on GPT-2 Medium and on the effect of nucleus sampling thresholds 1 https://github.com/AADeLucia/ gpt2-narrative-decoding 2 https://www.reddit.c"
2021.gem-1.16,N16-1014,0,0.168148,"e—and typically short—outputs. Thus, while many beneficial techniques may have arisen from one domain, they are not often employed in the other. One decoding method, nucleus sampling (Holtzman et al., 2020), has recently been applied to narrative generation (Ippolito et al., 2020), but a thorough evaluation of its various p thresholds has not been performed with human judgments using narrative-specific criteria, as this can be time- and labor-intensive. Also, recent advances in decoding methods for response generation—notably, the application of the maximum mutual information (MMI) objective (Li et al., 2016a)—have resulted in more interesting dialog according to human evaluators (Zhang et al., 2020b); nonetheless, this also has not been applied to narrative generation. Indeed, the MMI objective has been confined to short-form and less openended generation tasks thus far. Thus, we apply techniques from neural response 166 Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 166–185 August 5–6, 2021. ©2021 Association for Computational Linguistics generation to neural narrative generation in order to investigate the potential benefits—and pitfal"
2021.gem-1.16,2020.emnlp-main.348,0,0.0389734,"Missing"
2021.gem-1.16,N16-1098,0,0.0234168,"language generation (NLG) tasks, we expect to find different ideal settings than those found for short-form or constrained generation. Our preprocessing, training, generation, and analysis scripts are available publicly.1 2 Related Work Narrative generation tasks Work on narrative generation is split between cloze tasks, open-ended generation, and guided generation. In a cloze task, a full story except for a final word, phrase, or sentence is given, and a model generates a completion. This could be cast as a short generation problem— or, more commonly in this domain, a multiplechoice problem (Mostafazadeh et al., 2016; Weston et al., 2015; Hill et al., 2015; Ippolito et al., 2019a). Open-ended generation is the task of generating long-form output conditioned on a prompt (Figure 1). Fan et al. (2018) create a paired prompt and response dataset from the subreddit r/WritingPrompts2 to train a sequence-tosequence “fusion model.” See et al. (2019) extend Fan et al. (2018), but use GPT-2 small and perform a top-k decoding parameter sweep. We focus on this open-ended narrative generation task in our investigation, but primarily focus on GPT-2 Medium and on the effect of nucleus sampling thresholds 1 https://githu"
2021.gem-1.16,2020.aacl-main.36,0,0.129294,"cia/ gpt2-narrative-decoding 2 https://www.reddit.com/r/WritingPrompts/ [ WP ] You live in a world where there has never been sickness , and you are the first to have ever experienced being sick . I open my eyes in a panic , sweat beading and then falling down my face . I look around and the sun in shining through the maroon curtains of my studio apartment . Everything seems to be as I left it the afternoon before , but there is a heavy , unfamiliar air in the room . Table 1: Example prompt and response excerpt from W RITING P ROMPTS. and diverse decoding strengths on narrative quality. While Nadeem et al. (2020) similarly perform a hyperparameter search over sampling algorithms in a language generation setting, they perform human evaluations using a convincingness metric on a short-form news generation task; long-form narrative generation is not bound by realism (and may actually benefit from less realistic output), and thus requires different metrics and evaluation setups. Guided generation is the middle ground of cloze and open-ended generation. The model is provided more context, such as characters, plot information, and potentially other information, and then generates a story based on all of the"
2021.gem-1.16,W18-1505,0,0.0207633,"sampling algorithms in a language generation setting, they perform human evaluations using a convincingness metric on a short-form news generation task; long-form narrative generation is not bound by realism (and may actually benefit from less realistic output), and thus requires different metrics and evaluation setups. Guided generation is the middle ground of cloze and open-ended generation. The model is provided more context, such as characters, plot information, and potentially other information, and then generates a story based on all of the provided structural and semantic information (Peng et al., 2018; Akoury et al., 2020). Decoding methods for generation Decoding refers to the inference methods used in natural language generation; given input sequence S, how should we construct the output sequence T ? Since finding the exact most probable token at each time step often does not produce human-like or highquality results (Zhang et al., 2020a; Holtzman et al., 2020), search and sampling are used to overcome label bias and generate more human-like language. One popular search method is beam search, where at each time step, the algorithm keeps track of the top B most probable partial hypotheses"
2021.gem-1.16,D19-1410,0,0.0126972,"lues, which may increase interestingness more than they decrease fluency. 3.4 Evaluation The qualities important for narrative generation are interestingness, coherence, fluency, and relevance to the prompt. These metrics are also evaluated in Akoury et al. (2020), though they measure “likeability&quot; instead of interestingness. A combination of automatic and human evaluation is used to assess the quality of generated narratives. For automatic evaluation, we employ test perplexity, lexical diversity (dist-n, Li et al. 2016b), and a BERT-based sentence similarity metric, Sentence-BERT (sent-BERT, Reimers and Gurevych 2019). Perplexity is used to evaluate language models and may correlate with fluency. The latter two may act as proxies for interestingness, since they measure n-gram diversity within an output and sentence embedding diversity across outputs, respectively. We use sent-BERT as an output diversity metric by using the cosine distance instead of cosine similarity. Our motivation in choosing these diversity metrics is from Tevet and Berant (2020), who identify dist-n and sent-BERT as the best metrics to evaluate two targeted types of diversity—diverse word choice and diverse content, respectively. For h"
2021.gem-1.16,K19-1079,0,0.0166279,"ation, and guided generation. In a cloze task, a full story except for a final word, phrase, or sentence is given, and a model generates a completion. This could be cast as a short generation problem— or, more commonly in this domain, a multiplechoice problem (Mostafazadeh et al., 2016; Weston et al., 2015; Hill et al., 2015; Ippolito et al., 2019a). Open-ended generation is the task of generating long-form output conditioned on a prompt (Figure 1). Fan et al. (2018) create a paired prompt and response dataset from the subreddit r/WritingPrompts2 to train a sequence-tosequence “fusion model.” See et al. (2019) extend Fan et al. (2018), but use GPT-2 small and perform a top-k decoding parameter sweep. We focus on this open-ended narrative generation task in our investigation, but primarily focus on GPT-2 Medium and on the effect of nucleus sampling thresholds 1 https://github.com/AADeLucia/ gpt2-narrative-decoding 2 https://www.reddit.com/r/WritingPrompts/ [ WP ] You live in a world where there has never been sickness , and you are the first to have ever experienced being sick . I open my eyes in a panic , sweat beading and then falling down my face . I look around and the sun in shining through the"
2021.gem-1.16,2020.findings-emnlp.291,0,0.0211342,"m a popular creative writing subreddit r/WritingPrompts. To the best of our knowledge, this dataset was not examined for hate speech or gender bias, and we did not perform such inspections here. Also, the released code has no post-generation filter to flag potentially offensive narratives. We did not pursue any of these filters or offensive text detection because our work was focused on evaluating generated narratives for stylistic measures of quality, and was not focused on contentbased sources of bias. However, one should look to relevant work in the field on bias and hate speech detection (Sheng et al., 2020; MacAvaney et al., 2019) before deploying such models as creative writing tools. Besides the clear ethical obligation to vet such a tool, a “creative” writing tool which propagates or amplifies the bias of its training set would potentially hinder the quality of output narratives. Normative and stereotypical narratives would likely be uninteresting. Acknowledgments We thank Daphne Ippolito, Nathaniel Weir, Carlos Aguirre, Rachel Wicks, Arya McCarthy, and the anonymous reviewers for their helpful feedback. We also wish to thank the anonymous mechanical Turkers who provided invaluable suggestio"
2021.gem-1.16,D18-1428,0,0.0199085,"s here. Decoding objective In chatbot response generation, top-k and nucleus sampling have been known to generate fluent, but uninteresting and simple high-probability responses which do not address the input (Li et al., 2016b). This issue is commonly referred to as the “I don’t know” problem, where the response to all inputs is often the highprobability phrase “I don’t know.” Proposed solutions to this response blandness issue involve altering the decoding objective. Some recent work in this domain includes Nakamura et al. (2018), who use Inverse Token Frequency to reweight generated tokens. Xu et al. (2018) and Zhang et al. (2018) use adversarial loss to optimize for diversity, informativeness, and fluency. Martins et al. (2020) propose entmax sampling to generate more effectively from sparse distributions and address the train-test mismatch in text generation. Another approach explores variants of the standard log-likelihood loss, applying different objectives during inference. An example of this is maximum mutual information (MMI, Li et al. 2016b), an objective that promotes more diverse responses in the neural response generation task. This mitigates the “I don’t know” problem in which all re"
2021.naacl-main.104,P16-1154,0,0.0254915,"the token overlap with the source is high. Besides, has been very limited success in translating this Demszky et al. (2018) derive an NLI dataset by performance to downstream NLP tasks. Work rele- converting subsets of various QA datasets. They vant to the use of these NLI models for downstream try two approaches for the conversion – rule-based 1323 and neural. For the rule-based approach, they extract POS tags from the question-answer pair and apply hand-crafted rules on them to convert the pair to a hypothesis sentence. Their neural approach uses a trained SEQ 2 SEQ BiLSTM-with-copy model (Gu et al., 2016) to convert each hquestion, answeri pair into a hypothesis sentence (the corresponding passage being the premise). While their approach looks promising, they do not show the utility of these converted datasets by training an NLI model on them. Thus, it remains unclear whether the NLI datasets generated by the conversion are beneficial for NLP tasks. We posit that this direction of research is promising and largely unexplored. In our work, we attempt to leverage the abundance of large and diverse MCRC datasets to generate long-premise NLI datasets, and show that such datasets are useful towards"
2021.naacl-main.68,E17-1068,0,0.157787,"random variables is captured by vector similarities. Without an explicit dependency structure it is difficult to enforce logical reasoning rules to maintain global consistency. In order to go beyond triple-level uncertainty modeling, we consider each entity as a binary random variable. However, representing such a probability distribution in an embedding space and reasoning over it is non-trivial. It is difficult to model marginal and joint probabilities for entities using simple geometric objects like vectors. In order to encode probability distributions in the embedding space, recent works (Lai and Hockenmaier, 2017; Vilnis et al., 2018; Li et al., 2019; Dasgupta et al., 2020) represent random variables as more complex geometric objects, such as cones and axis-aligned hyperrectangles (boxes), and use volume as the probability measure. Inspired by such advances of probability measures in embeddings, we present BEUrRE (Box Embedding for Uncertain RElational Data)2 . BEUrRE represents entities as boxes. Relations are modeled as two separate affine transforms on the head and tail entity boxes. Confidence of a triple is modeled by the intersection between the two transformed boxes. Fig. 1 shows how a fact abo"
2021.naacl-main.68,2020.emnlp-main.460,1,0.61838,"and tail entity boxes. We also incorporate logic constraints that capture the high-order dependency of facts and enhance global reasoning consistency. Extensive experiments show the promising capability of BEUrRE on confidence prediction and fact ranking for UKGs. The results are encouraging and suggest various extensions, including deeper transformation architectures as well as alternative geometries to allow for additional rules to be imposed. In this context, we are also interested in extending the use of the proposed technologies into more downstream tasks, such as knowledge association (Sun et al., 2020) and event hierarchy induction (Wang et al., 2020). Another direction is to use BEUrRE for ontology construction and population, since box embeddings are naturally capable of capturing granularities of concepts. Ethical Considerations posed in the paper aims to model uncertainty in knowledge graphs more accurately, and the effectiveness of the proposed model is supported by the empirical experiment results. Acknowledgment We appreciate the anonymous reviewers for their insightful comments and suggestions. This material is based upon work sponsored by the DARPA MCS program under Contract No. N6"
2021.naacl-main.68,P18-1025,1,0.889755,"d by vector similarities. Without an explicit dependency structure it is difficult to enforce logical reasoning rules to maintain global consistency. In order to go beyond triple-level uncertainty modeling, we consider each entity as a binary random variable. However, representing such a probability distribution in an embedding space and reasoning over it is non-trivial. It is difficult to model marginal and joint probabilities for entities using simple geometric objects like vectors. In order to encode probability distributions in the embedding space, recent works (Lai and Hockenmaier, 2017; Vilnis et al., 2018; Li et al., 2019; Dasgupta et al., 2020) represent random variables as more complex geometric objects, such as cones and axis-aligned hyperrectangles (boxes), and use volume as the probability measure. Inspired by such advances of probability measures in embeddings, we present BEUrRE (Box Embedding for Uncertain RElational Data)2 . BEUrRE represents entities as boxes. Relations are modeled as two separate affine transforms on the head and tail entity boxes. Confidence of a triple is modeled by the intersection between the two transformed boxes. Fig. 1 shows how a fact about the genre of the B"
2021.naacl-main.68,2020.emnlp-main.51,1,0.714844,"constraints that capture the high-order dependency of facts and enhance global reasoning consistency. Extensive experiments show the promising capability of BEUrRE on confidence prediction and fact ranking for UKGs. The results are encouraging and suggest various extensions, including deeper transformation architectures as well as alternative geometries to allow for additional rules to be imposed. In this context, we are also interested in extending the use of the proposed technologies into more downstream tasks, such as knowledge association (Sun et al., 2020) and event hierarchy induction (Wang et al., 2020). Another direction is to use BEUrRE for ontology construction and population, since box embeddings are naturally capable of capturing granularities of concepts. Ethical Considerations posed in the paper aims to model uncertainty in knowledge graphs more accurately, and the effectiveness of the proposed model is supported by the empirical experiment results. Acknowledgment We appreciate the anonymous reviewers for their insightful comments and suggestions. This material is based upon work sponsored by the DARPA MCS program under Contract No. N660011924033 with the United States Office Of Naval"
2021.repl4nlp-1.28,D19-1522,0,0.0620279,"Missing"
2021.repl4nlp-1.28,2020.acl-main.617,0,0.0632545,"Missing"
2021.repl4nlp-1.28,W09-1109,0,0.05137,"tional edges and transitive closure edges, both from an overfitting and generalization standpoint, identifying subsets where further improvement is needed. The source code for our model and the dataset can be found in https://github.com/ iesl/box-to-box-transform.git. 2 Related Work Recent advances in representing one single hierarchy mainly fall in two categories: 1) representing hierarchies in non-Euclidian space (eg. hyperbolic space, due to the curvature’s inductive bias to model tree-like structures) 2) using region-based representations instead of vectors for each node in the hierarchy (Erk, 2009). Hyperbolic space has been shown to be efficient in representing hierarchical relations, but also encounters difficulties in training (Nickel and Kiela, 2017; Ganea et al., 2018b; Chamberlain et al., 2017). Categorization models in psychology often represent a concept as a region (Nosofsky, 1986; Smith et al., 1988; Hampton, 1991). Vilnis and McCallum (2015) and Athiwaratkun and Wilson (2018) use Gaussian distributions to embed each word in the corpus, the latter of which uses thresholded divergences which amount to region representations. Vendrov et al. (2016) and Lai and Hockenmaier (2017)"
2021.repl4nlp-1.28,P18-1025,1,0.389962,"making generalization between hierarchies infeasible. In this work, we introduce a learned box-to-box transformation that respects the structure of each hierarchy. We demonstrate that this not only improves the capability of modeling cross-hierarchy compositional edges but is also capable of generalizing from a subset of the transitive reduction. 1 Introduction Representation learning for hierarchical relations is crucial in natural language processing because of the hierarchical nature of common knowledge, for example, <Bird I S A Animal> (Athiwaratkun and Wilson, 2018; Vendrov et al., 2016; Vilnis et al., 2018; Nickel and Kiela, 2017). The I S A relation represents meaningful hierarchical relationships between concepts and plays an essential role in generalization for other relations, such as the generalization of <organ PART O F person> based on <eye PART O F of person>, and <organ I S A eye>. The fundamental nature of the I S A relation means that it is inherently involved in a large amount of compositional reasoning involving other relations. Modeling hierarchies is essentially the problem of modeling a poset, or partially ordered set. The task of inferring missing edges that requires learning a"
2021.repl4nlp-1.28,E17-1068,0,0.0202635,"n the hierarchy (Erk, 2009). Hyperbolic space has been shown to be efficient in representing hierarchical relations, but also encounters difficulties in training (Nickel and Kiela, 2017; Ganea et al., 2018b; Chamberlain et al., 2017). Categorization models in psychology often represent a concept as a region (Nosofsky, 1986; Smith et al., 1988; Hampton, 1991). Vilnis and McCallum (2015) and Athiwaratkun and Wilson (2018) use Gaussian distributions to embed each word in the corpus, the latter of which uses thresholded divergences which amount to region representations. Vendrov et al. (2016) and Lai and Hockenmaier (2017) make use of the reverse product order on Rn+ , which effectively results in cone representations. Vilnis et al. (2018) further extend this cone representation to axis-aligned hyperRegion representations are also used for tasks which do not require modeling hierarchy. In Vilnis et al. (2018), the authors also model conditional probability distributions using box embeddings. Abboud et al. (2020) and Ren et al. (2020) take a different approach, using boxes for their capacity to contain many vectors to provide slack in the loss function when modeling knowledge base triples or representing logical"
2021.semeval-1.63,W18-4411,0,0.0208471,"Missing"
2021.semeval-1.63,N19-1423,0,0.0128512,"eri et al., 2020) further extends the dataset to 5 languages: Arabic, Danish, English, Greek, and Turkish. 3 where hk (yk ; x) is the score of the tag yk at the k time step. Then, the conditional probability is obtained by a normalization operation: exp(S(x, y)) . e )) e ∈Y exp(S(x, y y P (y|x) = P where Y contains all possible paths of tag sequences. During inference, the predicted tag seˆ is obtained by: quence y Methods In the section, we describe how toxic span detection is formalized and corresponding solutions in detail. 3.1 (2) Sequence Labeling ˆ = arg max P (y|x). y y∈Y We adopt BERT(Devlin et al., 2019) and BERT+LSTM(Hochreiter et al., 1997) as the language encoder respectively, resulting in two solutions: BERT+CRF and BERT+LSTM+CRF. The reason for adding LSTM is that we believe that the contextual representation refined by LSTM could be more sensitive to the position of tokens. 3.2 The BIO tag scheme is utilized to locating toxic spans, where B (Begin) corresponds to the first token in a toxic span, I (Inside) corresponds to the inside and end tokens in a toxic span, and O corresponds to those no-toxic tokens. Following most existing work (Lample et al., 2016; Ma and Hovy, 2016), we leverag"
2021.semeval-1.63,P19-1051,0,0.139317,"=1 2 https://www.kaggle.com/c/jigsaw-toxic-commentclassification-challenge 522 (3) Span Boundary Detection Different from SL formalization, SBD formalization utilizes the start and end positions tagging scheme to represent toxic spans. SBD formalization was originally applied in the machine reading comprehension task (Seo et al., 2016; Wang and Jiang, 2016). In these works, two n-classifiers are employed to predict the start position and end position separately, where n denotes the length of the input sentence. However, this strategy can only output a single span for an input sentence. Later, Hu et al. (2019b) extended the two n-classifiers strategy by a heuristic multi-span decoding algorithm. But this is not a concise and efficient solution for multi-span scenario, as the decoding algorithm relies on two hyper-parameters: (1) γ, the minimum score threshold, (2) K, the maximum number of spans. In addition to the two n-classifiers strategy, a more recent and popular strategy is to employ two binary classifiers to determine whether each token is the start (end) position or not (Li et al., 2020; Wei et al., 2020; Yu et al., 2019). In this paper, we adopt the binary classifiers strategy for SBD form"
2021.semeval-1.63,W18-4401,0,0.0214842,"ve high precision but rather low recall, and are strongly interpretable and flexible in practice. First, we mine a toxic lexicon from the training set by a simple statistical strategy. Next, WordNet (Fellbaum, 2010) and GloVe (Pennington et al., 2014) are utilized to extend this lexicon further. With a toxic lexicon, we extract toxic spans through word-level matching. 2 Related Work In recent years, cyber violence has become a widespread societal concern, and how to identify and filter hate speech has become an important topic in machine learning. TRAC proposes an aggression recognition task (Kumar et al., 2018) that provides a dataset of 15,000 annotated Facebook posts and comments in English and Hindi for 521 Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), pages 521–526 Bangkok, Thailand (online), August 5–6, 2021. ©2021 Association for Computational Linguistics O B I I O O O ?1 ?2 ?3 ?4 ?3 ?4 ?? S ?1 (a) ?2 E ?3 ?4 ?3 ?4 ?? (b) Figure 1: Comparison of SL and SBD, (a) denotes SL, (b) denotes SBD. training and validation. The task aims to classify comments into three categories: non-aggressive, covertly aggressive, and overly aggressive. The Toxic Comment Classi"
2021.semeval-1.63,N16-1030,0,0.037351,"x P (y|x). y y∈Y We adopt BERT(Devlin et al., 2019) and BERT+LSTM(Hochreiter et al., 1997) as the language encoder respectively, resulting in two solutions: BERT+CRF and BERT+LSTM+CRF. The reason for adding LSTM is that we believe that the contextual representation refined by LSTM could be more sensitive to the position of tokens. 3.2 The BIO tag scheme is utilized to locating toxic spans, where B (Begin) corresponds to the first token in a toxic span, I (Inside) corresponds to the inside and end tokens in a toxic span, and O corresponds to those no-toxic tokens. Following most existing work (Lample et al., 2016; Ma and Hovy, 2016), we leverage Conditional Random Fields (CRF) (Lafferty et al., 2001) for learning and inference. In addition to token-level classification, CRF models the dependencies between tags in a tag sequence by the transition matrix A ∈ RK×K , where K is the size of the tag space, i.e. K = 3. For the contextual representation x ∈ Rn×h , the score of a tag sequence y ∈ Rn in CRF is defined as: S(x, y) =h1 (y1 ; x)+ (1) n−1  X hk+1 (yk+1 ; x) + Ayk ,yk+1 . k=1 2 https://www.kaggle.com/c/jigsaw-toxic-commentclassification-challenge 522 (3) Span Boundary Detection Different from SL f"
2021.semeval-1.63,2020.acl-main.519,0,0.0316811,"of the input sentence. However, this strategy can only output a single span for an input sentence. Later, Hu et al. (2019b) extended the two n-classifiers strategy by a heuristic multi-span decoding algorithm. But this is not a concise and efficient solution for multi-span scenario, as the decoding algorithm relies on two hyper-parameters: (1) γ, the minimum score threshold, (2) K, the maximum number of spans. In addition to the two n-classifiers strategy, a more recent and popular strategy is to employ two binary classifiers to determine whether each token is the start (end) position or not (Li et al., 2020; Wei et al., 2020; Yu et al., 2019). In this paper, we adopt the binary classifiers strategy for SBD formalization and describe the details below. Split Num Train 6894 Dev 1723 Test 2000 BERT+LSTM+CRF BERT+CRF BERT+Span Ensemble Table 1: Data statistics. Given the contextual representation x = {x1 , x2 , · · · , xn } ∈ Rn×h , for the location i, we calculate the probability of whether it is a start position by Equation (4) and the probability of whether it is a end position by Equation (5). pstart (i) = σ(W1&gt; xi + b1 ), pend (i) = σ(W2&gt; [xi ; pstart (i)] + b2 ), 4.3 (7) 5 Ensemble Strategy Ex"
2021.semeval-1.63,P16-1101,0,0.0362417,"dopt BERT(Devlin et al., 2019) and BERT+LSTM(Hochreiter et al., 1997) as the language encoder respectively, resulting in two solutions: BERT+CRF and BERT+LSTM+CRF. The reason for adding LSTM is that we believe that the contextual representation refined by LSTM could be more sensitive to the position of tokens. 3.2 The BIO tag scheme is utilized to locating toxic spans, where B (Begin) corresponds to the first token in a toxic span, I (Inside) corresponds to the inside and end tokens in a toxic span, and O corresponds to those no-toxic tokens. Following most existing work (Lample et al., 2016; Ma and Hovy, 2016), we leverage Conditional Random Fields (CRF) (Lafferty et al., 2001) for learning and inference. In addition to token-level classification, CRF models the dependencies between tags in a tag sequence by the transition matrix A ∈ RK×K , where K is the size of the tag space, i.e. K = 3. For the contextual representation x ∈ Rn×h , the score of a tag sequence y ∈ Rn in CRF is defined as: S(x, y) =h1 (y1 ; x)+ (1) n−1  X hk+1 (yk+1 ; x) + Ayk ,yk+1 . k=1 2 https://www.kaggle.com/c/jigsaw-toxic-commentclassification-challenge 522 (3) Span Boundary Detection Different from SL formalization, SBD fo"
2021.semeval-1.63,W18-4423,0,0.0364524,"Missing"
2021.semeval-1.63,2021.semeval-1.6,0,0.0463561,"Missing"
2021.semeval-1.63,D14-1162,0,0.0869599,"the ensemble approach, and we can observe that our lexicon-based approaches obtain notable results in the F 1-score. In addition, we also calculate the average precision and average recall values of different methods on the test set, and our original lexicon-based approach even outperforms ensemble approaches in average precision, but there is still a significant gap in an average recall. Since the lexicon-based approaches can only identify the toxic words in the lexicon, the recall can be improved by expanding the toxic lexicon. To improve the recall, we use WordNet (Miller, 1995) and GloVe (Pennington et al., 2014) to expand the toxic lexicon. In detail, we collect synsets of each toxic from WordNet, and collect the nearest similar words by calculating cosine similarity of GloVe vectors. The performances of the two expanded approaches are shown in Table 3. Although the recall of two approaches improves over the original lexicon, the precision decreases significantly, which indicating that there are a considerable number of non-toxic words in the synonyms found through WordNet. Besides, we explore the impact of threshold θ when mining the original lexicon on performance. The performances with different t"
2021.semeval-1.63,2020.acl-main.136,0,0.0176988,"ence. However, this strategy can only output a single span for an input sentence. Later, Hu et al. (2019b) extended the two n-classifiers strategy by a heuristic multi-span decoding algorithm. But this is not a concise and efficient solution for multi-span scenario, as the decoding algorithm relies on two hyper-parameters: (1) γ, the minimum score threshold, (2) K, the maximum number of spans. In addition to the two n-classifiers strategy, a more recent and popular strategy is to employ two binary classifiers to determine whether each token is the start (end) position or not (Li et al., 2020; Wei et al., 2020; Yu et al., 2019). In this paper, we adopt the binary classifiers strategy for SBD formalization and describe the details below. Split Num Train 6894 Dev 1723 Test 2000 BERT+LSTM+CRF BERT+CRF BERT+Span Ensemble Table 1: Data statistics. Given the contextual representation x = {x1 , x2 , · · · , xn } ∈ Rn×h , for the location i, we calculate the probability of whether it is a start position by Equation (4) and the probability of whether it is a end position by Equation (5). pstart (i) = σ(W1&gt; xi + b1 ), pend (i) = σ(W2&gt; [xi ; pstart (i)] + b2 ), 4.3 (7) 5 Ensemble Strategy Experimental Setup D"
2021.semeval-1.63,N18-1095,0,0.0394392,"Missing"
2021.semeval-1.63,N12-1084,0,0.10001,"Missing"
2021.semeval-1.63,S19-2010,0,0.0185077,"ugust 5–6, 2021. ©2021 Association for Computational Linguistics O B I I O O O ?1 ?2 ?3 ?4 ?3 ?4 ?? S ?1 (a) ?2 E ?3 ?4 ?3 ?4 ?? (b) Figure 1: Comparison of SL and SBD, (a) denotes SL, (b) denotes SBD. training and validation. The task aims to classify comments into three categories: non-aggressive, covertly aggressive, and overly aggressive. The Toxic Comment Classification Challenge 5 2 is an open competition in Kaggle that provides participants with comments from Wikipedia and defines six toxic categories: toxic, severe toxic, obscene, threat, insult, identity hate. In SemEval 2019 task 6 (Zampieri et al., 2019), in addition to whether the comment is offensive, the type of the attack and the target of the attack are also included. Based on this, Semeval 2020 task 12 (Zampieri et al., 2020) further extends the dataset to 5 languages: Arabic, Danish, English, Greek, and Turkish. 3 where hk (yk ; x) is the score of the tag yk at the k time step. Then, the conditional probability is obtained by a normalization operation: exp(S(x, y)) . e )) e ∈Y exp(S(x, y y P (y|x) = P where Y contains all possible paths of tag sequences. During inference, the predicted tag seˆ is obtained by: quence y Methods In the se"
C18-1041,D14-1067,0,0.0156667,"res or manual labels which are hard to gather on a larger dataset. Whats more, the conventional models could not catch the subtle difference between similar crimes, thus they wouldnt perform well when the number of classes increases and more similar crimes appear. With the successful usage of neural network methods on speech (Mikolov et al., 2011; Hinton et al., 2012; Dahl et al., 2012; Sainath et al., 2013), computer vision (CV) (Krizhevsky et al., 2012; Farabet et al., 2013; Tompson et al., 2014; Szegedy et al., 2015) and natural language processing (NLP) (Collobert et al., 2011; Kim, 2014; Bordes et al., 2014; Sutskever et al., 2014; Jean et al., 2015; Yang et al., 2016), researchers propose to employ neural models for legal tasks. Luo et al. (2017) proposes a hierarchical attentional network to predict charges and extract relevant articles jointly. However, this work only focuses on high-frequency charges, without paying attention to few-shot and confusing ones. To address these issues, we propose an attention-based neural model by incorporating several discriminative legal attributes. 3 Method In this section, we propose a few-shot neural model which jointly models charge prediction task and leg"
C18-1041,J81-4005,0,0.745887,"Missing"
C18-1041,P15-1001,0,0.027774,"on a larger dataset. Whats more, the conventional models could not catch the subtle difference between similar crimes, thus they wouldnt perform well when the number of classes increases and more similar crimes appear. With the successful usage of neural network methods on speech (Mikolov et al., 2011; Hinton et al., 2012; Dahl et al., 2012; Sainath et al., 2013), computer vision (CV) (Krizhevsky et al., 2012; Farabet et al., 2013; Tompson et al., 2014; Szegedy et al., 2015) and natural language processing (NLP) (Collobert et al., 2011; Kim, 2014; Bordes et al., 2014; Sutskever et al., 2014; Jean et al., 2015; Yang et al., 2016), researchers propose to employ neural models for legal tasks. Luo et al. (2017) proposes a hierarchical attentional network to predict charges and extract relevant articles jointly. However, this work only focuses on high-frequency charges, without paying attention to few-shot and confusing ones. To address these issues, we propose an attention-based neural model by incorporating several discriminative legal attributes. 3 Method In this section, we propose a few-shot neural model which jointly models charge prediction task and legal attribute prediction task in a unified f"
C18-1041,D14-1181,0,0.121215,"text or case profiles. For example, some works (Liu et al., 2004; Liu and Hsieh, 2006) utilize shallow textual features, including characters, words, and phrases, to predict charges. Katz et al. (2017) predict the US Supreme Court’s decisions with efficient features extracted from case profiles (e.g., dates, locations, terms, and types). All these approaches require numerous human effort to design features and annotate training instances. Besides, these methods are hard to scale to other scenarios. Inspired by the successful usage of deep neural networks on natural language processing tasks (Kim, 2014; Baharudin et al., 2010; Tang et al., 2015), researchers propose to employ deep neural networks to model legal documents. For example, Luo et al. (2017) propose an attention-based neural network for charge prediction by incorporating the relevant law articles. However, charge prediction is still confronted with two major challenges which make it non-trivial: ∗ † Indicates equal contribution. Corresponding Author. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 487 Proceedings of the 27th Inter"
C18-1041,O12-5004,0,0.412552,"Missing"
C18-1041,D17-1289,0,0.600661,"ords, and phrases, to predict charges. Katz et al. (2017) predict the US Supreme Court’s decisions with efficient features extracted from case profiles (e.g., dates, locations, terms, and types). All these approaches require numerous human effort to design features and annotate training instances. Besides, these methods are hard to scale to other scenarios. Inspired by the successful usage of deep neural networks on natural language processing tasks (Kim, 2014; Baharudin et al., 2010; Tang et al., 2015), researchers propose to employ deep neural networks to model legal documents. For example, Luo et al. (2017) propose an attention-based neural network for charge prediction by incorporating the relevant law articles. However, charge prediction is still confronted with two major challenges which make it non-trivial: ∗ † Indicates equal contribution. Corresponding Author. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 487 Proceedings of the 27th International Conference on Computational Linguistics, pages 487–498 Santa Fe, New Mexico, USA, August 20-26, 2018. Fact On June 24, 2015, the defendant pry o"
C18-1041,D15-1167,0,0.0235342,", some works (Liu et al., 2004; Liu and Hsieh, 2006) utilize shallow textual features, including characters, words, and phrases, to predict charges. Katz et al. (2017) predict the US Supreme Court’s decisions with efficient features extracted from case profiles (e.g., dates, locations, terms, and types). All these approaches require numerous human effort to design features and annotate training instances. Besides, these methods are hard to scale to other scenarios. Inspired by the successful usage of deep neural networks on natural language processing tasks (Kim, 2014; Baharudin et al., 2010; Tang et al., 2015), researchers propose to employ deep neural networks to model legal documents. For example, Luo et al. (2017) propose an attention-based neural network for charge prediction by incorporating the relevant law articles. However, charge prediction is still confronted with two major challenges which make it non-trivial: ∗ † Indicates equal contribution. Corresponding Author. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 487 Proceedings of the 27th International Conference on Computational Linguis"
C18-1041,N16-1174,0,0.414897,"t. Whats more, the conventional models could not catch the subtle difference between similar crimes, thus they wouldnt perform well when the number of classes increases and more similar crimes appear. With the successful usage of neural network methods on speech (Mikolov et al., 2011; Hinton et al., 2012; Dahl et al., 2012; Sainath et al., 2013), computer vision (CV) (Krizhevsky et al., 2012; Farabet et al., 2013; Tompson et al., 2014; Szegedy et al., 2015) and natural language processing (NLP) (Collobert et al., 2011; Kim, 2014; Bordes et al., 2014; Sutskever et al., 2014; Jean et al., 2015; Yang et al., 2016), researchers propose to employ neural models for legal tasks. Luo et al. (2017) proposes a hierarchical attentional network to predict charges and extract relevant articles jointly. However, this work only focuses on high-frequency charges, without paying attention to few-shot and confusing ones. To address these issues, we propose an attention-based neural model by incorporating several discriminative legal attributes. 3 Method In this section, we propose a few-shot neural model which jointly models charge prediction task and legal attribute prediction task in a unified framework. In the fol"
C18-1041,D17-1099,0,0.0278547,"ttribute-based classification to the label-embedding task. Jayaraman and Grauman (2014) introduces a random forest method stressing the unreliability of attribute prediction for unseen classes. They also extend it to the few-shot scenario. Other than attributes, other external information can also be introduced to promote zero-shot classification. Elhoseiny et al. (2014) makes use of text description of the class label to transfer knowledge between text features and visual features. Zero-shot learning has also been used in applications besides object recognition, such as activity recognition (Zellers and Choi, 2017) and event recognition (Wu et al., 2014). 2.2 Charge Prediction Researchers in the legal area have been working on automatically making the legal judgment for a long time. Kort (1957) applies quantitative methods to predict judgment by calculation numerical values for factual elements. Nagel (1963) makes use of correlation analysis to make predictions for reapportioning cases. Keown (1980) introduced mathematical models used for legal prediction such as linear models and the scheme of nearest neighbors. These methods are usually mathematical or quantitative, and they are restricted to a small"
D19-1276,K18-2005,0,0.0306752,"ces with length  30. For each sentence, x is obtained by running the standard pre-trained ELMo on the UD token sequence (although UD’s tokenization may not perfectly match that of ELMo’s training data), and y is the labeled UD dependency parse without any part-of-speech (POS) tags. Thus, our tags t are tuned to predict only the dependency relations in UD, and not the gold POS tags a also in UD. Pretrained Word Embeddings For English, we used the pre-trained English ELMo model from the AllenNLP library (Gardner et al., 2017). For the 2748 other 8 languages, we used the pre-trained models from Che et al. (2018). Recall that ELMo has two layers of bidirectional LSTM (layer 1 and 2) built upon a context-independent character CNN (layer 0). We use either layer 1 or 2 as the input (xi ) to our token encoder p✓ . Layer 0 is the input ( xˆi ) to our type encoder s⇠ . Each encoder network (§§3.2– 3.3) has a single hidden layer with a tanh transfer function, which has 2d hidden units (typically 128 or 512) for continuous encodings and 512 hidden units for discrete encodings. Optimization We optimize with Adam (Kingma and Ba, 2014), a variant of stochastic gradient descent. We alternate between improving the"
D19-1276,P81-1022,0,0.704599,"Missing"
D19-1276,P03-1054,0,0.06139,"tion outperforms all three baselines in 8 of 9 languages, and is not significantly worse in the 9th language (Hindi). In short, our VIB joint training generalizes better to test data. This is because the training objective (2) includes terms that focus on the parsing task and also regularize the representations. In the discrete case, the VIB representation outperforms gold POS tags (at the same level of granularity) in 6 of 9 languages, and of the other 3, it is not significantly worse in 2. This suggests that our learned discrete tag set could be an improved alternative to gold POS tags (cf. Klein and Manning, 2003) when a discrete tag set is needed for speed. 8 Related Work Much recent NLP literature examines syntactic information encoded by deep models (Linzen et al., 2016) and more specifically, by powerful unsupervised word embeddings. Hewitt and Manning (2019) learn a linear projection from the embedding space to predict the distance between two words in a parse tree. Peters et al. (2018b) and Goldberg (2019) assess the ability of BERT and ELMo directly on syntactic NLP tasks. Tenney et al. (2019) extract information from the contextual embeddings by self-attention pooling within a span of word embe"
D19-1276,D07-1015,0,0.0347305,"Missing"
D19-1276,Q16-1037,0,0.0328628,"er to test data. This is because the training objective (2) includes terms that focus on the parsing task and also regularize the representations. In the discrete case, the VIB representation outperforms gold POS tags (at the same level of granularity) in 6 of 9 languages, and of the other 3, it is not significantly worse in 2. This suggests that our learned discrete tag set could be an improved alternative to gold POS tags (cf. Klein and Manning, 2003) when a discrete tag set is needed for speed. 8 Related Work Much recent NLP literature examines syntactic information encoded by deep models (Linzen et al., 2016) and more specifically, by powerful unsupervised word embeddings. Hewitt and Manning (2019) learn a linear projection from the embedding space to predict the distance between two words in a parse tree. Peters et al. (2018b) and Goldberg (2019) assess the ability of BERT and ELMo directly on syntactic NLP tasks. Tenney et al. (2019) extract information from the contextual embeddings by self-attention pooling within a span of word embeddings. The IB framework was first used in NLP to cluster distributionally similar words (Pereira et al., 1993). In cognitive science, it has been used to argue th"
D19-1276,W07-2216,0,0.043293,"ture We use the deep biaffine dependency parser (Dozat and Manning, 2016) as our variational distribution q (y |t), which functions as the decoder. This parser uses a Bi-LSTM to extract features from compressed tags or vectors and assign scores to each tree edge, setting q (y |t) proportional to the exp of the total score of all edges in y. During IB training, the code5 computes only an approximation to q (y|t) for the gold tree y (although in principle, it could have computed the exact normalizing constant in polytime with Tutte’s matrix-tree theorem (Smith and Smith, 2007; Koo et al., 2007; McDonald and Satta, 2007)). When we test the parser, the code does exactly find argmaxy q (y |t) via the directed spanning tree algorithm of Edmonds (1966). 4 Training and Inference With the approximations in §3, our final minimization objective is this upper bound on (2): E x,y h E [ log q (y|t)] + KL(p✓ (t|x)||r (t)) t⇠p✓ (t |x) n X + i=1 KL(p✓ (ti |x) ||s⇠ (ti |xˆi )) i (3) We apply stochastic gradient descent to optimize this objective. To get a stochastic estimate of the objective, we first sample some (x, y) from the treebank. We then have many expectations over t ⇠ p✓ (t |x), including the KL terms. We could es"
D19-1276,P93-1024,0,0.869073,"amines syntactic information encoded by deep models (Linzen et al., 2016) and more specifically, by powerful unsupervised word embeddings. Hewitt and Manning (2019) learn a linear projection from the embedding space to predict the distance between two words in a parse tree. Peters et al. (2018b) and Goldberg (2019) assess the ability of BERT and ELMo directly on syntactic NLP tasks. Tenney et al. (2019) extract information from the contextual embeddings by self-attention pooling within a span of word embeddings. The IB framework was first used in NLP to cluster distributionally similar words (Pereira et al., 1993). In cognitive science, it has been used to argue that color-naming systems across languages are nearly optimal (Zaslavsky et al., 2018). In machine learning, IB provides an information-theoretic perspective to explain the performance of deep neural networks (Tishby and Zaslavsky, 2015). The VIB method makes use of variational upper and lower bounds on mutual information. An alternative lower bound was proposed by Poole et al. (2019), who found it to work better empirically. 9 Conclusion and Future Work In this paper, we have proposed two ways to syntactically compress ELMo word token embeddin"
D19-1276,N18-1202,0,0.222873,"f the information bottleneck, with bottleneck variable T. A jagged arrow indicates a stochastic mapping, i.e. the jagged arrow points from the parameters of a distribution to a sample drawn from that distribution. Introduction Word embedding systems like BERT and ELMo use spelling and context to obtain contextual embeddings of word tokens. These systems are trained on large corpora in a task-independent way. The resulting embeddings have proved to then be useful for both syntactic and semantic tasks, with different layers of ELMo or BERT being somewhat specialized to different kinds of tasks (Peters et al., 2018b; Goldberg, 2019). State-of-the-art performance on many NLP tasks can be obtained by fine-tuning, i.e., back-propagating task loss all the way back into the embedding function (Peters et al., 2018a; Devlin et al., 2018). In this paper, we explore what task-specific information appears in the embeddings before finetuning takes place. We focus on the task of dependency parsing, but our method can be easily extended to other syntactic or semantic tasks. Our method compresses the embeddings by extracting just their syntactic properties—specifically, the information needed to reconstruct parse tre"
D19-1276,D18-1179,0,0.158156,"f the information bottleneck, with bottleneck variable T. A jagged arrow indicates a stochastic mapping, i.e. the jagged arrow points from the parameters of a distribution to a sample drawn from that distribution. Introduction Word embedding systems like BERT and ELMo use spelling and context to obtain contextual embeddings of word tokens. These systems are trained on large corpora in a task-independent way. The resulting embeddings have proved to then be useful for both syntactic and semantic tasks, with different layers of ELMo or BERT being somewhat specialized to different kinds of tasks (Peters et al., 2018b; Goldberg, 2019). State-of-the-art performance on many NLP tasks can be obtained by fine-tuning, i.e., back-propagating task loss all the way back into the embedding function (Peters et al., 2018a; Devlin et al., 2018). In this paper, we explore what task-specific information appears in the embeddings before finetuning takes place. We focus on the task of dependency parsing, but our method can be easily extended to other syntactic or semantic tasks. Our method compresses the embeddings by extracting just their syntactic properties—specifically, the information needed to reconstruct parse tre"
D19-1276,D07-1014,0,0.0343334,"s treebank parse y. 2747 Decoder Architecture We use the deep biaffine dependency parser (Dozat and Manning, 2016) as our variational distribution q (y |t), which functions as the decoder. This parser uses a Bi-LSTM to extract features from compressed tags or vectors and assign scores to each tree edge, setting q (y |t) proportional to the exp of the total score of all edges in y. During IB training, the code5 computes only an approximation to q (y|t) for the gold tree y (although in principle, it could have computed the exact normalizing constant in polytime with Tutte’s matrix-tree theorem (Smith and Smith, 2007; Koo et al., 2007; McDonald and Satta, 2007)). When we test the parser, the code does exactly find argmaxy q (y |t) via the directed spanning tree algorithm of Edmonds (1966). 4 Training and Inference With the approximations in §3, our final minimization objective is this upper bound on (2): E x,y h E [ log q (y|t)] + KL(p✓ (t|x)||r (t)) t⇠p✓ (t |x) n X + i=1 KL(p✓ (ti |x) ||s⇠ (ti |xˆi )) i (3) We apply stochastic gradient descent to optimize this objective. To get a stochastic estimate of the objective, we first sample some (x, y) from the treebank. We then have many expectations over t ⇠ p"
ji-etal-2010-annotating,N04-1043,0,\N,Missing
ji-etal-2010-annotating,J93-2004,0,\N,Missing
ji-etal-2010-annotating,P07-1034,0,\N,Missing
ji-etal-2010-annotating,W06-1615,0,\N,Missing
ji-etal-2010-annotating,J05-1004,0,\N,Missing
ji-etal-2010-annotating,N03-4011,0,\N,Missing
ji-etal-2010-annotating,P09-1116,0,\N,Missing
ji-etal-2010-annotating,R09-1032,1,\N,Missing
L18-1494,R15-1011,1,0.895589,"Missing"
L18-1494,N16-1087,0,0.0248279,"Missing"
L18-1494,W15-4502,1,0.891342,"Missing"
L18-1494,P15-1150,0,0.13833,"Missing"
P13-2105,P05-1022,0,0.354533,"ne parser trained on CTB only. 2 Automatic Annotation Transformation In this section, we present an effective approach that transforms the source treebank to another compatible with the target annotation guideline, then describe an optimization strategy of iterative training that conducts several rounds of bidirectional annotation transformation and improves the transformation performance gradually from a global view. 2.2 Learning the Annotation Transformer To capture the transformation information from the source treebank to the target treebank, we use the discriminative reranking technique (Charniak and Johnson, 2005; Collins and Koo, 2005) to train the annotation transformer and to score kbest parse trees with some heterogeneous features. In this paper, the averaged perceptron algorithm is used to train the treebank transformation model. It is an online training algorithm and has been successfully used in many NLP tasks, such as parsing (Collins and Roark, 2004) and word segmentation (Zhang and Clark, 2007; Zhang and Clark, 2010). In addition to the target features which closely follow Sun et al. (2010). We design the following quasi-synchronous features to model the annotation inconsistencies. • Bigram"
P13-2105,J05-1003,0,0.0308726,"y. 2 Automatic Annotation Transformation In this section, we present an effective approach that transforms the source treebank to another compatible with the target annotation guideline, then describe an optimization strategy of iterative training that conducts several rounds of bidirectional annotation transformation and improves the transformation performance gradually from a global view. 2.2 Learning the Annotation Transformer To capture the transformation information from the source treebank to the target treebank, we use the discriminative reranking technique (Charniak and Johnson, 2005; Collins and Koo, 2005) to train the annotation transformer and to score kbest parse trees with some heterogeneous features. In this paper, the averaged perceptron algorithm is used to train the treebank transformation model. It is an online training algorithm and has been successfully used in many NLP tasks, such as parsing (Collins and Roark, 2004) and word segmentation (Zhang and Clark, 2007; Zhang and Clark, 2010). In addition to the target features which closely follow Sun et al. (2010). We design the following quasi-synchronous features to model the annotation inconsistencies. • Bigram constituent relation For"
P13-2105,P04-1015,0,0.0270835,"roves the transformation performance gradually from a global view. 2.2 Learning the Annotation Transformer To capture the transformation information from the source treebank to the target treebank, we use the discriminative reranking technique (Charniak and Johnson, 2005; Collins and Koo, 2005) to train the annotation transformer and to score kbest parse trees with some heterogeneous features. In this paper, the averaged perceptron algorithm is used to train the treebank transformation model. It is an online training algorithm and has been successfully used in many NLP tasks, such as parsing (Collins and Roark, 2004) and word segmentation (Zhang and Clark, 2007; Zhang and Clark, 2010). In addition to the target features which closely follow Sun et al. (2010). We design the following quasi-synchronous features to model the annotation inconsistencies. • Bigram constituent relation For two consecutive fundamental constituents si and sj in the target parse tree, we find the minimum categories Ni and Nj of the spans of si and sj in the source parse tree respectively. Here 2.1 Principle for Annotation Transformation In training procedure, the source parser is used to parse the sentences in the target treebank s"
P13-2105,P99-1065,0,0.206222,"Missing"
P13-2105,W02-1001,0,0.224921,"Missing"
P13-2105,daum-etal-2004-automatic,0,0.0687766,"Missing"
P13-2105,D09-1087,0,0.0192773,"treebank annotation transformation used for Chinese constituent parsing. Other related work has been focused on semisupervised parsing methods which utilize labeled data to annotate unlabeled data, then use the additional annotated data to improve the original model (McClosky et al., 2006a; McClosky et 3.3 Iterative Transformation We use the iterative training method for annotation transformation. The CTB developing set is used to determine the optimal training iteration. After each iteration, we test the performance of a parser trained on the combined treebank. Fig594 References al., 2006b; Huang and Harper, 2009). The selftraining methodology enlightens us on getting annotated treebank compatibal with another annotation guideline. Our approach places extra emphasis on improving the transformation performance with the help of source annotation knowledge. Apart from constituency-to-constituency treebank transformation, there also exists some research on dependency-to-constituency treebank transformation. Collins et al. (1999) used transformed constituency treebank from Prague Dependency Treebank for constituent parsing on Czech. Xia and Palmer (2001) explored different algorithms that transform dependen"
P13-2105,P09-1059,1,0.612564,"used for training. We also can find that our approach further extends the advantage over the two baseline systems as the amount of CTB training data decreases in Figure 2. The figure confirms our approach is effective for improving parser performance, specially for the scenario where the target treebank is scarce. 4 Related Work Treebank transformation is an effective strategy to reuse existing annotated data. Wang et al. (1994) proposed an approach to transform a treebank into another with a different grammar using their matching metric based on the bracket information of original treebank. Jiang et al. (2009) proposed annotation adaptation in Chinese word segmentation, then, some work were done in parsing (Sun et al., 2010; Zhu et al., 2011; Sun and Wan, 2012). Recently, Jiang et al. (2012) proposed an advanced annotation transformation in Chinese word segmentation, and we extended it to the more complicated treebank annotation transformation used for Chinese constituent parsing. Other related work has been focused on semisupervised parsing methods which utilize labeled data to annotate unlabeled data, then use the additional annotated data to improve the original model (McClosky et al., 2006a; Mc"
P13-2105,N06-1020,0,0.0380705,"ebank. Jiang et al. (2009) proposed annotation adaptation in Chinese word segmentation, then, some work were done in parsing (Sun et al., 2010; Zhu et al., 2011; Sun and Wan, 2012). Recently, Jiang et al. (2012) proposed an advanced annotation transformation in Chinese word segmentation, and we extended it to the more complicated treebank annotation transformation used for Chinese constituent parsing. Other related work has been focused on semisupervised parsing methods which utilize labeled data to annotate unlabeled data, then use the additional annotated data to improve the original model (McClosky et al., 2006a; McClosky et 3.3 Iterative Transformation We use the iterative training method for annotation transformation. The CTB developing set is used to determine the optimal training iteration. After each iteration, we test the performance of a parser trained on the combined treebank. Fig594 References al., 2006b; Huang and Harper, 2009). The selftraining methodology enlightens us on getting annotated treebank compatibal with another annotation guideline. Our approach places extra emphasis on improving the transformation performance with the help of source annotation knowledge. Apart from constituen"
P13-2105,P06-1043,0,0.024697,"ebank. Jiang et al. (2009) proposed annotation adaptation in Chinese word segmentation, then, some work were done in parsing (Sun et al., 2010; Zhu et al., 2011; Sun and Wan, 2012). Recently, Jiang et al. (2012) proposed an advanced annotation transformation in Chinese word segmentation, and we extended it to the more complicated treebank annotation transformation used for Chinese constituent parsing. Other related work has been focused on semisupervised parsing methods which utilize labeled data to annotate unlabeled data, then use the additional annotated data to improve the original model (McClosky et al., 2006a; McClosky et 3.3 Iterative Transformation We use the iterative training method for annotation transformation. The CTB developing set is used to determine the optimal training iteration. After each iteration, we test the performance of a parser trained on the combined treebank. Fig594 References al., 2006b; Huang and Harper, 2009). The selftraining methodology enlightens us on getting annotated treebank compatibal with another annotation guideline. Our approach places extra emphasis on improving the transformation performance with the help of source annotation knowledge. Apart from constituen"
P13-2105,P09-1006,0,0.599005,"Missing"
P13-2105,N07-1051,0,0.0610134,"Missing"
P13-2105,P06-1055,0,0.156433,"Missing"
P13-2105,D09-1086,0,0.0528911,"Missing"
P13-2105,P12-1025,0,0.0189739,"ases in Figure 2. The figure confirms our approach is effective for improving parser performance, specially for the scenario where the target treebank is scarce. 4 Related Work Treebank transformation is an effective strategy to reuse existing annotated data. Wang et al. (1994) proposed an approach to transform a treebank into another with a different grammar using their matching metric based on the bracket information of original treebank. Jiang et al. (2009) proposed annotation adaptation in Chinese word segmentation, then, some work were done in parsing (Sun et al., 2010; Zhu et al., 2011; Sun and Wan, 2012). Recently, Jiang et al. (2012) proposed an advanced annotation transformation in Chinese word segmentation, and we extended it to the more complicated treebank annotation transformation used for Chinese constituent parsing. Other related work has been focused on semisupervised parsing methods which utilize labeled data to annotate unlabeled data, then use the additional annotated data to improve the original model (McClosky et al., 2006a; McClosky et 3.3 Iterative Transformation We use the iterative training method for annotation transformation. The CTB developing set is used to determine the"
P13-2105,W10-4144,0,0.0716351,"om the source treebank to the target treebank, we use the discriminative reranking technique (Charniak and Johnson, 2005; Collins and Koo, 2005) to train the annotation transformer and to score kbest parse trees with some heterogeneous features. In this paper, the averaged perceptron algorithm is used to train the treebank transformation model. It is an online training algorithm and has been successfully used in many NLP tasks, such as parsing (Collins and Roark, 2004) and word segmentation (Zhang and Clark, 2007; Zhang and Clark, 2010). In addition to the target features which closely follow Sun et al. (2010). We design the following quasi-synchronous features to model the annotation inconsistencies. • Bigram constituent relation For two consecutive fundamental constituents si and sj in the target parse tree, we find the minimum categories Ni and Nj of the spans of si and sj in the source parse tree respectively. Here 2.1 Principle for Annotation Transformation In training procedure, the source parser is used to parse the sentences in the target treebank so that there are k-best parse trees with the source annotation guideline and one gold tree with the target annotation guideline for each sentenc"
P13-2105,P94-1034,0,0.383953,"Missing"
P13-2105,H01-1014,0,0.12377,"Missing"
P13-2105,P07-1106,0,0.0218642,"rom a global view. 2.2 Learning the Annotation Transformer To capture the transformation information from the source treebank to the target treebank, we use the discriminative reranking technique (Charniak and Johnson, 2005; Collins and Koo, 2005) to train the annotation transformer and to score kbest parse trees with some heterogeneous features. In this paper, the averaged perceptron algorithm is used to train the treebank transformation model. It is an online training algorithm and has been successfully used in many NLP tasks, such as parsing (Collins and Roark, 2004) and word segmentation (Zhang and Clark, 2007; Zhang and Clark, 2010). In addition to the target features which closely follow Sun et al. (2010). We design the following quasi-synchronous features to model the annotation inconsistencies. • Bigram constituent relation For two consecutive fundamental constituents si and sj in the target parse tree, we find the minimum categories Ni and Nj of the spans of si and sj in the source parse tree respectively. Here 2.1 Principle for Annotation Transformation In training procedure, the source parser is used to parse the sentences in the target treebank so that there are k-best parse trees with the"
P13-2105,D10-1082,0,0.0157555,"Learning the Annotation Transformer To capture the transformation information from the source treebank to the target treebank, we use the discriminative reranking technique (Charniak and Johnson, 2005; Collins and Koo, 2005) to train the annotation transformer and to score kbest parse trees with some heterogeneous features. In this paper, the averaged perceptron algorithm is used to train the treebank transformation model. It is an online training algorithm and has been successfully used in many NLP tasks, such as parsing (Collins and Roark, 2004) and word segmentation (Zhang and Clark, 2007; Zhang and Clark, 2010). In addition to the target features which closely follow Sun et al. (2010). We design the following quasi-synchronous features to model the annotation inconsistencies. • Bigram constituent relation For two consecutive fundamental constituents si and sj in the target parse tree, we find the minimum categories Ni and Nj of the spans of si and sj in the source parse tree respectively. Here 2.1 Principle for Annotation Transformation In training procedure, the source parser is used to parse the sentences in the target treebank so that there are k-best parse trees with the source annotation guidel"
P13-2105,P11-2126,0,0.272128,"raining data decreases in Figure 2. The figure confirms our approach is effective for improving parser performance, specially for the scenario where the target treebank is scarce. 4 Related Work Treebank transformation is an effective strategy to reuse existing annotated data. Wang et al. (1994) proposed an approach to transform a treebank into another with a different grammar using their matching metric based on the bracket information of original treebank. Jiang et al. (2009) proposed annotation adaptation in Chinese word segmentation, then, some work were done in parsing (Sun et al., 2010; Zhu et al., 2011; Sun and Wan, 2012). Recently, Jiang et al. (2012) proposed an advanced annotation transformation in Chinese word segmentation, and we extended it to the more complicated treebank annotation transformation used for Chinese constituent parsing. Other related work has been focused on semisupervised parsing methods which utilize labeled data to annotate unlabeled data, then use the additional annotated data to improve the original model (McClosky et al., 2006a; McClosky et 3.3 Iterative Transformation We use the iterative training method for annotation transformation. The CTB developing set is u"
P13-2105,levy-andrew-2006-tregex,0,\N,Missing
P13-2105,P12-1071,0,\N,Missing
P13-2105,D12-1038,1,\N,Missing
P13-2105,W07-2416,0,\N,Missing
P15-2103,P14-2100,1,0.837656,"language model smoothing is another direction to investigate (Duan and Zhai, 2011). Mei et al. have proposed to smooth language model utilizing structural adjacency (2008). None of these methods incorporates social factors in language model smoothing. There is a study in (Lin et al., 2011) which smooths document language models of tweets for topic tracking in online text streams. Basically, it applies general smoothing strategies (e.g., JelinekMercer, Dirichlet, Absolute Discounting, etc.) on the specific tracking task. Social information is incorporated into a factor graph model as features (Huang et al., 2014; Yan et al., 2015). These factor graph model based methods are less efficient so as to better handle cold-start situations with little training data. In contrast with these works, we have proposed a language model smoothing framework which incorporates social factors as a regularizer. According to the experimental results, our method is effective with social information and as well much more efficient. Related Work Language models have been paid high attention to during recent years (Ponte and Croft, 1998). Many different ways of language modeling have been proposed to solve different tasks."
P15-2103,P10-1056,0,0.012181,"ag ‘#’ into clusters as different datasets to evaluate (Lin et al., 2011; Yan et al., 2015; Yan et al., 2011). We manually selected top-3 topics based on popularity (measured in the number of postings within the cluster) and to obtain broad coverage of different types: sports, technology, and general interests, as listed in Table 1. Pre-processing. Basically, the social network graph can be established from all posting documents and all users. However, the data is noisy. We first pre-filter the pointless babbles (Analytics, 2009) by applying the linguistic quality judgments (e.g., OOV ratio) (Pitler et al., 2010), and then remove inactive users that have less than one follower or followee and remove the users without any linkage to the remaining posting documents. We remove stopwords and URLs, perform stemming, and build the graph after filtering. We establish the 4.3 Evaluation Metric We apply language perplexity to evaluate the smoothed language models. The experimental procedure is as follows: given the topic clusters shown in Table 1, we remove the hashtags and compute its perplexity with respect to the current topic cluster, defined as a power function: [ ] 1 ∑ pow 2, − log P (wi ) N wi ∈V Perple"
P15-2103,N06-1052,0,0.431325,"g the posting document language model based on social regularization. We formulate an optimization framework with a social regularizer. Experimental results on the Twitter dataset validate the effectiveness and efficiency of our proposed model. 1 Figure 1: 2 different sources to smooth document language models: texts (colored in yellow) and social contacts (colored in blue). Each piece of texts is authored by a particular social network user. work on language model smoothing has been investigated based on textual characteristics (Lafferty and Zhai, 2001; Yan et al., 2013; Liu and Croft, 2004; Tao et al., 2006; Lavrenko and Croft, 2001; Song and Croft, 1999). However, for social networks, texts are actually associated with users (as illustrated in Figure 1). We propose that social factors should be utilized as an augmentation to better smooth language models. Here we propose an optimization framework with regularization for language model smoothing on social networks, using both textual information and the social structure. We believe the social factor is fundamental to smooth language models on social networks. Our framework optimizes the smoothed language model to be closer to social neighbors in"
P15-2103,P12-1054,1,0.858975,"s of node u, each of which shares an edge to u. Now we have finished modeling the language model smoothing with social factors as regularization, and have defined the context correlation between documents and user social relationships. By plugging in Equation (2) into Equation (1), we could compute the smoothed language model of P (w|d+ 0 ). All the definitions for π(.) result in a range which varies from 0 to 1. Particularly, the ego user similarity πu0 = 1, which would be a natural and intuitive answer. 4 Experiments and Evaluation 4.1 Datasets and Experimental Setups Utilizing the data in (Yan et al., 2012), we establish the dataset of microblogs and the corresponding users from 9/29/2012 to 11/30/2012. We use roughly one month as the training set and the rest as testing set. Based on this dataset, we group the posting documents with the same hashtag ‘#’ into clusters as different datasets to evaluate (Lin et al., 2011; Yan et al., 2015; Yan et al., 2011). We manually selected top-3 topics based on popularity (measured in the number of postings within the cluster) and to obtain broad coverage of different types: sports, technology, and general interests, as listed in Table 1. Pre-processing. Bas"
P15-2103,I13-1058,1,0.89178,"Missing"
P15-2103,P96-1041,0,\N,Missing
P16-1137,W13-3515,0,0.586494,"ould be used as terms. To expand frame relationships in FrameNet, tuples can draw relations from the frame relation types (e.g., “is causative of”) and terms can be frame lexical units or their definitions. Several researchers have used commonsense knowledge to improve language technologies, including sentiment analysis (Cambria et al., 2012; Agarwal et al., 2015), semantic similarity (Caro et al., 2015), and speech recognition (Lieberman et al., 2005). Our hope is that our models can enable many other NLP applications to benefit from commonsense knowledge. Our work is most similar to that of Angeli and Manning (2013). They also developed methods to assess the plausibility of new facts based on a training set of facts, considering commonsense data from ConceptNet in one of their settings. Like us, they can handle an unbounded set of terms by using (simple) composition functions for novel terms, which is rare among work in KBC. One key difference is that their best method requires iterating over the KB at test time, which can be computationally expensive with large KBs. Our models do not require iterating over the training set. We compare to several baselines inspired by their work, and we additionally eval"
P16-1137,D14-1059,0,0.146072,"z et al., 2009; Nickel et al., 2011; Riedel et al., 2013; West et al., 2014), including recent work using neural networks (Socher et al., 2013; Yang et al., 2014). Introduction Many ambiguities in natural language processing (NLP) can be resolved by using knowledge of various forms. Our focus is on the type of knowledge that is often referred to as “commonsense” or “background” knowledge. This knowledge is rarely expressed explicitly in textual corpora (Gordon and Van Durme, 2013). Some researchers have developed techniques for inferring this knowledge from patterns in raw text (Gordon, 2014; Angeli and Manning, 2014), while others have developed curated resources of commonsense knowledge via manual annotation (Lenat and Guha, 1989; Speer and Havasi, 2012) or games with a purpose (von Ahn et al., 2006). Curated resources typically have high precision but suffer from a lack of coverage. For cerWe improve the coverage of commonsense resources by formulating the problem as one of knowledge base completion. We focus on a particular curated commonsense resource called ConceptNet (Speer and Havasi, 2012). ConceptNet contains tuples consisting of a left term, a relation, and a right term. The relations come from"
P16-1137,P98-1013,0,0.0601106,"tml. ˜ ing, though our methods could be applied to the output of these or other extraction systems. Our goals are similar to those of the AnalogySpace method (Speer et al., 2008), which uses matrix factorization to improve coverage of ConceptNet. However, AnalogySpace can only return a confidence score for a pair of terms drawn from the training set. Our models can assign scores to tuples that contain novel terms (as long as they consist of words in our vocabulary). Though we use ConceptNet, similar techniques can be applied to other curated resources like WordNet (Miller, 1995) and FrameNet (Baker et al., 1998). For WordNet, tuples can contain lexical entries that are linked via synset relations (e.g., “hypernym”). WordNet contains many multiword entries (e.g., “cold sweat”), which can be modeled compositionally by our term models; alternatively, entire glosses could be used as terms. To expand frame relationships in FrameNet, tuples can draw relations from the frame relation types (e.g., “is causative of”) and terms can be frame lexical units or their definitions. Several researchers have used commonsense knowledge to improve language technologies, including sentiment analysis (Cambria et al., 2012"
P16-1137,P14-2131,1,0.284641,"rackets surround terms. We replace the bracketed portions with their corresponding terms and insert the relation between them: “The effect of soak in a hotspring C AUSES get pruny skin”. We do this for all training tuples.3 We used the word2vec (Mikolov et al., 2013) toolkit to train skip-gram word embeddings on this data. We trained for 20 iterations, using a dimensionality of 200 and a window size of 5. We refer to these as “CN-trained” embeddings for the remainder of this paper. Similar approaches have been used to learn embeddings for particular downstream tasks, e.g., dependency parsing (Bansal et al., 2014). We use our CN-trained embeddings within baseline methods and also provide the initial word embeddings of our models. For all of our models, we update the initial word embeddings during learning. In the baseline methods described below, we compare our CN-trained embeddings to pretrained word embeddings. We use the GloVe (Pennington et al., 2014) embeddings trained on 840 billion tokens of Common Crawl web text and the PARAGRAM-SimLex embeddings of Wieting et al. (2015), which were tuned to have strong performance on the SimLex-999 task (Hill et al., 2015). 3 For reversed relations, indicated"
P16-1137,D14-1067,0,0.0207423,"s medium-confidence tuples from ConceptNet. We release all of our resources, including our ConceptNet KBC task data, large sets of randomly-generated tuples scored with our model, training code, and pretrained models with code for calculating the confidence of novel tuples.1 2 Related Work Our methods are similar to past work on KBC (Mintz et al., 2009; Nickel et al., 2011; Lao et al., 2011; Nickel et al., 2012; Riedel et al., 2013; Gardner et al., 2014; West et al., 2014), particularly methods based on distributed representations and neural networks (Socher et al., 2013; Bordes et al., 2013; Bordes et al., 2014a; Bordes et al., 2014b; Yang et al., 2014; Neelakantan et al., 2015; Gu et al., 2015; Toutanova et al., 2015). Most prior work predicts new relational links between terms drawn from a fixed set. In a notable exception, Neelakantan and Chang (2015) add new entities to KBs using external resources along with properties of the KB itself. Relatedly, Yao et al. (2013) induce an unbounded set of entity categories and associate them with entities in KBs. Several researchers have developed techniques for discovering commonsense knowledge from text (Gordon et al., 2010; Gordon and Schubert, 2012; Gord"
P16-1137,D11-1142,0,0.015575,"utanova et al., 2015). Most prior work predicts new relational links between terms drawn from a fixed set. In a notable exception, Neelakantan and Chang (2015) add new entities to KBs using external resources along with properties of the KB itself. Relatedly, Yao et al. (2013) induce an unbounded set of entity categories and associate them with entities in KBs. Several researchers have developed techniques for discovering commonsense knowledge from text (Gordon et al., 2010; Gordon and Schubert, 2012; Gordon, 2014; Angeli and Manning, 2014). Open information extraction systems like R E V ERB (Fader et al., 2011) and NELL (Carlson et al., 2010) find tuples with arbitrary terms and relations from raw text. In contrast, we start with a set of commonsense facts to use for train1 Available at http://ttic.uchicago.edu/ kgimpel/commonsense.html. ˜ ing, though our methods could be applied to the output of these or other extraction systems. Our goals are similar to those of the AnalogySpace method (Speer et al., 2008), which uses matrix factorization to improve coverage of ConceptNet. However, AnalogySpace can only return a confidence score for a pair of terms drawn from the training set. Our models can assig"
P16-1137,D14-1044,0,0.023207,"fastest. We manually evaluate our trained model’s ability to assign quality scores to novel tuples, finding that it can propose tuples at the same quality level as medium-confidence tuples from ConceptNet. We release all of our resources, including our ConceptNet KBC task data, large sets of randomly-generated tuples scored with our model, training code, and pretrained models with code for calculating the confidence of novel tuples.1 2 Related Work Our methods are similar to past work on KBC (Mintz et al., 2009; Nickel et al., 2011; Lao et al., 2011; Nickel et al., 2012; Riedel et al., 2013; Gardner et al., 2014; West et al., 2014), particularly methods based on distributed representations and neural networks (Socher et al., 2013; Bordes et al., 2013; Bordes et al., 2014a; Bordes et al., 2014b; Yang et al., 2014; Neelakantan et al., 2015; Gu et al., 2015; Toutanova et al., 2015). Most prior work predicts new relational links between terms drawn from a fixed set. In a notable exception, Neelakantan and Chang (2015) add new entities to KBs using external resources along with properties of the KB itself. Relatedly, Yao et al. (2013) induce an unbounded set of entity categories and associate them with en"
P16-1137,W12-3023,0,0.060375,"al., 2013; Bordes et al., 2014a; Bordes et al., 2014b; Yang et al., 2014; Neelakantan et al., 2015; Gu et al., 2015; Toutanova et al., 2015). Most prior work predicts new relational links between terms drawn from a fixed set. In a notable exception, Neelakantan and Chang (2015) add new entities to KBs using external resources along with properties of the KB itself. Relatedly, Yao et al. (2013) induce an unbounded set of entity categories and associate them with entities in KBs. Several researchers have developed techniques for discovering commonsense knowledge from text (Gordon et al., 2010; Gordon and Schubert, 2012; Gordon, 2014; Angeli and Manning, 2014). Open information extraction systems like R E V ERB (Fader et al., 2011) and NELL (Carlson et al., 2010) find tuples with arbitrary terms and relations from raw text. In contrast, we start with a set of commonsense facts to use for train1 Available at http://ttic.uchicago.edu/ kgimpel/commonsense.html. ˜ ing, though our methods could be applied to the output of these or other extraction systems. Our goals are similar to those of the AnalogySpace method (Speer et al., 2008), which uses matrix factorization to improve coverage of ConceptNet. However, Ana"
P16-1137,D15-1038,0,0.0156161,"r ConceptNet KBC task data, large sets of randomly-generated tuples scored with our model, training code, and pretrained models with code for calculating the confidence of novel tuples.1 2 Related Work Our methods are similar to past work on KBC (Mintz et al., 2009; Nickel et al., 2011; Lao et al., 2011; Nickel et al., 2012; Riedel et al., 2013; Gardner et al., 2014; West et al., 2014), particularly methods based on distributed representations and neural networks (Socher et al., 2013; Bordes et al., 2013; Bordes et al., 2014a; Bordes et al., 2014b; Yang et al., 2014; Neelakantan et al., 2015; Gu et al., 2015; Toutanova et al., 2015). Most prior work predicts new relational links between terms drawn from a fixed set. In a notable exception, Neelakantan and Chang (2015) add new entities to KBs using external resources along with properties of the KB itself. Relatedly, Yao et al. (2013) induce an unbounded set of entity categories and associate them with entities in KBs. Several researchers have developed techniques for discovering commonsense knowledge from text (Gordon et al., 2010; Gordon and Schubert, 2012; Gordon, 2014; Angeli and Manning, 2014). Open information extraction systems like R E V E"
P16-1137,J15-4004,0,0.0181414,"am tasks, e.g., dependency parsing (Bansal et al., 2014). We use our CN-trained embeddings within baseline methods and also provide the initial word embeddings of our models. For all of our models, we update the initial word embeddings during learning. In the baseline methods described below, we compare our CN-trained embeddings to pretrained word embeddings. We use the GloVe (Pennington et al., 2014) embeddings trained on 840 billion tokens of Common Crawl web text and the PARAGRAM-SimLex embeddings of Wieting et al. (2015), which were tuned to have strong performance on the SimLex-999 task (Hill et al., 2015). 3 For reversed relations, indicated by an asterisk in the OMCS sentences, we swap t1 and t2 in the tuple. 1449 5.3 Baselines We consider three baselines inspired by those of Angeli and Manning (2013): • Similar Fact Count (Count): For each tuple τ = ht1 , R, t2 i in the evaluation set, we count the number of similar tuples in the training set. A training tuple τ 0 = ht01 , R0 , t02 i is considered “similar” to τ if R = R0 , one of the terms matches exactly, and the other term has the same head word. That is, (R = R0 ) ∧ (t1 = t01 ) ∧ (head (t2 ) = head (t02 )), or (R = R0 ) ∧ (t2 = t02 ) ∧ ("
P16-1137,P03-1054,0,0.0377736,"the tuple. 1449 5.3 Baselines We consider three baselines inspired by those of Angeli and Manning (2013): • Similar Fact Count (Count): For each tuple τ = ht1 , R, t2 i in the evaluation set, we count the number of similar tuples in the training set. A training tuple τ 0 = ht01 , R0 , t02 i is considered “similar” to τ if R = R0 , one of the terms matches exactly, and the other term has the same head word. That is, (R = R0 ) ∧ (t1 = t01 ) ∧ (head (t2 ) = head (t02 )), or (R = R0 ) ∧ (t2 = t02 ) ∧ (head (t1 ) = head (t01 )). The head word for a term was obtained by running the Stanford Parser (Klein and Manning, 2003) on the term. This baseline does not use word embeddings. • Argument Similarity (ArgSim): This baseline computes the cosine similarity of the vectors for t1 and t2 , ignoring the relation. Vectors for t1 and t2 are obtained by word averaging. • Max Similarity (MaxSim): For tuple τ in an evaluation set, this baseline outputs the maximum similarity between τ and any tuple in the training set. The similarity is computed by concatenating the vectors for t1 , R, and t2 , then computing cosine similarity. As in ArgSim, we obtain vectors for terms by averaging their words. We only consider R when usi"
P16-1137,D11-1049,0,0.0202937,"y-generated negative examples performs best while also being fastest. We manually evaluate our trained model’s ability to assign quality scores to novel tuples, finding that it can propose tuples at the same quality level as medium-confidence tuples from ConceptNet. We release all of our resources, including our ConceptNet KBC task data, large sets of randomly-generated tuples scored with our model, training code, and pretrained models with code for calculating the confidence of novel tuples.1 2 Related Work Our methods are similar to past work on KBC (Mintz et al., 2009; Nickel et al., 2011; Lao et al., 2011; Nickel et al., 2012; Riedel et al., 2013; Gardner et al., 2014; West et al., 2014), particularly methods based on distributed representations and neural networks (Socher et al., 2013; Bordes et al., 2013; Bordes et al., 2014a; Bordes et al., 2014b; Yang et al., 2014; Neelakantan et al., 2015; Gu et al., 2015; Toutanova et al., 2015). Most prior work predicts new relational links between terms drawn from a fixed set. In a notable exception, Neelakantan and Chang (2015) add new entities to KBs using external resources along with properties of the KB itself. Relatedly, Yao et al. (2013) induce"
P16-1137,D15-1176,0,0.027443,"train models that define a function score(t1 , R, t2 ) that provides a quality score for an arbitrary tuple ht1 , R, t2 i. These models will be evaluated by their ability to distinguish true heldout tuples from false ones. We describe two model families for scoring tuples. We assume that we have embeddings for words and define models that use these word embeddings to score tuples. So our models are limited to tuples in which terms consist of words in the word embedding vocabulary, though future work could consider character-based architectures for open-vocabulary modeling (Huang et al., 2013; Ling et al., 2015). 3.1 Bilinear Models We first consider bilinear models, since they have been found useful for KBC in past work (Nickel et al., 2011; Jenatton et al., 2012; Garc´ıa-Dur´an et al., 2014; Yang et al., 2014). A bilinear model has the following form for a tuple ht1 , R, t2 i: where a is a nonlinear activation function (tuned among ReLU, tanh, and logistic sigmoid) and where we have introduced additional parameters W (B) and b(B) . This gives us the following model: scorebilinear (t1 , R, t2 ) = u> 1 MR u2 When using the LSTM, we tune the decision about how to produce the final term vectors to pass"
P16-1137,P09-1113,0,0.345086,"tuples from ConceptNet. 1 right term relax relaxation your muscle be sore go to spa get pruny skin change into swim suit conf. 3.3 2.6 2.3 2.0 1.6 1.6 Table 1: ConceptNet tuples with left term “soak in hotspring”; final column is confidence score. tain resources, researchers have developed methods to automatically increase coverage by inferring missing entries. These methods are commonly categorized under the heading of knowledge base completion (KBC). KBC is widelystudied for knowledge bases like Freebase (Bollacker et al., 2008) which contain large sets of entities and relations among them (Mintz et al., 2009; Nickel et al., 2011; Riedel et al., 2013; West et al., 2014), including recent work using neural networks (Socher et al., 2013; Yang et al., 2014). Introduction Many ambiguities in natural language processing (NLP) can be resolved by using knowledge of various forms. Our focus is on the type of knowledge that is often referred to as “commonsense” or “background” knowledge. This knowledge is rarely expressed explicitly in textual corpora (Gordon and Van Durme, 2013). Some researchers have developed techniques for inferring this knowledge from patterns in raw text (Gordon, 2014; Angeli and Man"
P16-1137,N15-1054,0,0.0131238,"ing the confidence of novel tuples.1 2 Related Work Our methods are similar to past work on KBC (Mintz et al., 2009; Nickel et al., 2011; Lao et al., 2011; Nickel et al., 2012; Riedel et al., 2013; Gardner et al., 2014; West et al., 2014), particularly methods based on distributed representations and neural networks (Socher et al., 2013; Bordes et al., 2013; Bordes et al., 2014a; Bordes et al., 2014b; Yang et al., 2014; Neelakantan et al., 2015; Gu et al., 2015; Toutanova et al., 2015). Most prior work predicts new relational links between terms drawn from a fixed set. In a notable exception, Neelakantan and Chang (2015) add new entities to KBs using external resources along with properties of the KB itself. Relatedly, Yao et al. (2013) induce an unbounded set of entity categories and associate them with entities in KBs. Several researchers have developed techniques for discovering commonsense knowledge from text (Gordon et al., 2010; Gordon and Schubert, 2012; Gordon, 2014; Angeli and Manning, 2014). Open information extraction systems like R E V ERB (Fader et al., 2011) and NELL (Carlson et al., 2010) find tuples with arbitrary terms and relations from raw text. In contrast, we start with a set of commonsen"
P16-1137,P15-1016,0,0.0109296,"ur resources, including our ConceptNet KBC task data, large sets of randomly-generated tuples scored with our model, training code, and pretrained models with code for calculating the confidence of novel tuples.1 2 Related Work Our methods are similar to past work on KBC (Mintz et al., 2009; Nickel et al., 2011; Lao et al., 2011; Nickel et al., 2012; Riedel et al., 2013; Gardner et al., 2014; West et al., 2014), particularly methods based on distributed representations and neural networks (Socher et al., 2013; Bordes et al., 2013; Bordes et al., 2014a; Bordes et al., 2014b; Yang et al., 2014; Neelakantan et al., 2015; Gu et al., 2015; Toutanova et al., 2015). Most prior work predicts new relational links between terms drawn from a fixed set. In a notable exception, Neelakantan and Chang (2015) add new entities to KBs using external resources along with properties of the KB itself. Relatedly, Yao et al. (2013) induce an unbounded set of entity categories and associate them with entities in KBs. Several researchers have developed techniques for discovering commonsense knowledge from text (Gordon et al., 2010; Gordon and Schubert, 2012; Gordon, 2014; Angeli and Manning, 2014). Open information extraction sys"
P16-1137,N03-1033,0,0.0405368,"Missing"
P16-1137,D15-1174,0,0.00593326,"task data, large sets of randomly-generated tuples scored with our model, training code, and pretrained models with code for calculating the confidence of novel tuples.1 2 Related Work Our methods are similar to past work on KBC (Mintz et al., 2009; Nickel et al., 2011; Lao et al., 2011; Nickel et al., 2012; Riedel et al., 2013; Gardner et al., 2014; West et al., 2014), particularly methods based on distributed representations and neural networks (Socher et al., 2013; Bordes et al., 2013; Bordes et al., 2014a; Bordes et al., 2014b; Yang et al., 2014; Neelakantan et al., 2015; Gu et al., 2015; Toutanova et al., 2015). Most prior work predicts new relational links between terms drawn from a fixed set. In a notable exception, Neelakantan and Chang (2015) add new entities to KBs using external resources along with properties of the KB itself. Relatedly, Yao et al. (2013) induce an unbounded set of entity categories and associate them with entities in KBs. Several researchers have developed techniques for discovering commonsense knowledge from text (Gordon et al., 2010; Gordon and Schubert, 2012; Gordon, 2014; Angeli and Manning, 2014). Open information extraction systems like R E V ERB (Fader et al., 2011) a"
P16-1137,Q15-1025,1,0.776827,"f this paper. Similar approaches have been used to learn embeddings for particular downstream tasks, e.g., dependency parsing (Bansal et al., 2014). We use our CN-trained embeddings within baseline methods and also provide the initial word embeddings of our models. For all of our models, we update the initial word embeddings during learning. In the baseline methods described below, we compare our CN-trained embeddings to pretrained word embeddings. We use the GloVe (Pennington et al., 2014) embeddings trained on 840 billion tokens of Common Crawl web text and the PARAGRAM-SimLex embeddings of Wieting et al. (2015), which were tuned to have strong performance on the SimLex-999 task (Hill et al., 2015). 3 For reversed relations, indicated by an asterisk in the OMCS sentences, we swap t1 and t2 in the tuple. 1449 5.3 Baselines We consider three baselines inspired by those of Angeli and Manning (2013): • Similar Fact Count (Count): For each tuple τ = ht1 , R, t2 i in the evaluation set, we count the number of similar tuples in the training set. A training tuple τ 0 = ht01 , R0 , t02 i is considered “similar” to τ if R = R0 , one of the terms matches exactly, and the other term has the same head word. That"
P16-1137,D14-1162,0,0.115252,"terations, using a dimensionality of 200 and a window size of 5. We refer to these as “CN-trained” embeddings for the remainder of this paper. Similar approaches have been used to learn embeddings for particular downstream tasks, e.g., dependency parsing (Bansal et al., 2014). We use our CN-trained embeddings within baseline methods and also provide the initial word embeddings of our models. For all of our models, we update the initial word embeddings during learning. In the baseline methods described below, we compare our CN-trained embeddings to pretrained word embeddings. We use the GloVe (Pennington et al., 2014) embeddings trained on 840 billion tokens of Common Crawl web text and the PARAGRAM-SimLex embeddings of Wieting et al. (2015), which were tuned to have strong performance on the SimLex-999 task (Hill et al., 2015). 3 For reversed relations, indicated by an asterisk in the OMCS sentences, we swap t1 and t2 in the tuple. 1449 5.3 Baselines We consider three baselines inspired by those of Angeli and Manning (2013): • Similar Fact Count (Count): For each tuple τ = ht1 , R, t2 i in the evaluation set, we count the number of similar tuples in the training set. A training tuple τ 0 = ht01 , R0 , t02"
P16-1137,N13-1008,0,0.129197,"x relaxation your muscle be sore go to spa get pruny skin change into swim suit conf. 3.3 2.6 2.3 2.0 1.6 1.6 Table 1: ConceptNet tuples with left term “soak in hotspring”; final column is confidence score. tain resources, researchers have developed methods to automatically increase coverage by inferring missing entries. These methods are commonly categorized under the heading of knowledge base completion (KBC). KBC is widelystudied for knowledge bases like Freebase (Bollacker et al., 2008) which contain large sets of entities and relations among them (Mintz et al., 2009; Nickel et al., 2011; Riedel et al., 2013; West et al., 2014), including recent work using neural networks (Socher et al., 2013; Yang et al., 2014). Introduction Many ambiguities in natural language processing (NLP) can be resolved by using knowledge of various forms. Our focus is on the type of knowledge that is often referred to as “commonsense” or “background” knowledge. This knowledge is rarely expressed explicitly in textual corpora (Gordon and Van Durme, 2013). Some researchers have developed techniques for inferring this knowledge from patterns in raw text (Gordon, 2014; Angeli and Manning, 2014), while others have developed c"
P16-1137,speer-havasi-2012-representing,0,0.463962,"icago, Chicago, IL, 60637, USA University of Illinois at Chicago, Chicago, IL, 60607, USA ‡ Toyota Technological Institute at Chicago, Chicago, IL, 60637, USA † lix1@uchicago.edu, ataher2@uic.edu, {lifu,kgimpel}@ttic.edu relation M OTIVATED B Y G OAL U SED F OR M OTIVATED B Y G OAL H AS P REREQUISITE C AUSES H AS P REREQUISITE Abstract We enrich a curated resource of commonsense knowledge by formulating the problem as one of knowledge base completion (KBC). Most work in KBC focuses on knowledge bases like Freebase that relate entities drawn from a fixed set. However, the tuples in ConceptNet (Speer and Havasi, 2012) define relations between an unbounded set of phrases. We develop neural network models for scoring tuples on arbitrary phrases and evaluate them by their ability to distinguish true held-out tuples from false ones. We find strong performance from a bilinear model using a simple additive architecture to model phrases. We manually evaluate our trained model’s ability to assign quality scores to novel tuples, finding that it can propose tuples at the same quality level as mediumconfidence tuples from ConceptNet. 1 right term relax relaxation your muscle be sore go to spa get pruny skin change in"
P16-1137,C98-1013,0,\N,Missing
P18-1025,E17-1068,0,0.331942,"ns is a manifold of probability distributions, the model is not truly probabilistic in that it does not model asymmetries and relations in terms of probEmbedding methods which enforce a partial order or lattice structure over the concept space, such as Order Embeddings (OE) (Vendrov et al., 2016), are a natural way to model transitive relational data (e.g. entailment graphs). However, OE learns a deterministic knowledge base, limiting expressiveness of queries and the ability to use uncertainty for both prediction and learning (e.g. learning from expectations). Probabilistic extensions of OE (Lai and Hockenmaier, 2017) have provided the ability to somewhat calibrate these denotational probabilities while retaining the consistency and inductive bias of ordered models, but lack the ability to model the negative correlations found in real-world knowledge. In this work we show that a broad class of models that assign probability measures to OE can never capture negative correlation, which motivates our construction of a novel box lattice and accompanying probability measure to capture anticorrelation and even disjoint concepts, while still providing the benefits of probabilistic modeling, such as the ability to"
P18-1025,P14-1098,0,0.0720103,"Missing"
P18-1025,W09-1109,0,0.645253,"Missing"
P19-3011,D18-1547,0,0.0887227,"Missing"
P19-3011,P17-1163,0,0.058692,"Missing"
P19-3011,D17-1237,1,0.931587,"prior studies, it is been impractical to perform a rigorous comparative study under the same condition. ConvLab is the first dialog research platform that covers a full range of trainable statistical models with fully annotated datasets, differing from previous toolkits whose focus is largely concentrated on the system policy component while other components are mostly limited to pre-fixed baseline models (Ultes, 2017; Miller et al., 2017; Li et al., 2018). There is also an increasing interest in building bots that seamlessly intertwine multiple subdomains to accomplish high-level user goals (Peng et al., 2017; Budzianowski et al., 2018). The development of multi-domain dialog system adds additional complexities to both data collection and annotation, and the models for dialog system components. For the former, Budzianowski et al. (2018) collected the MultiWOZ dataset, a dialog corpus with dialogs ranging over multiple domains for the trip information setting, whereas there is no open-platform yet that is designed to handle multi-domain, multi-intent phenomena. To foster multi-domain dialog research, ConvLab features the MultiWOZ task and offers a complete set of reference models ranging from indiv"
P19-3011,P18-2069,0,0.0841428,"Missing"
P19-3011,P07-2045,0,0.00643389,"Missing"
P19-3011,N07-2038,0,0.54107,"Understanding For natural language understanding, ConvLab provides three reference models: Semantic Tuple Classifier (STC) (Mairesse et al., 2009), OneNet (Kim et al., 2017) and Multi-intent LU (MILU). STC can handle multi-domain, multi-intent dialog acts but cannot detect out-of-vocabulary (OOV) values. While OneNet can capture OOVs, it cannot handle multi-intent dialog acts. Thus, ConvLab offers a new MILU model which extends OneNet to process multi-intent dialog acts. For more details on MILU, please refer to the ConvLab site. User Policy For user policy, ConvLab provides an agenda-based (Schatzmann et al., 2007) user model and data-driven approaches such as HUS and its variational variants (Gur et al., 2018). Similar to the system side, each model works at the dialog act level, and can be pipelined with NLU and NLG modules to construct a whole user simulator. Dialog State Tracking The dialog state tracker is responsible for updating the belief state. ConvLab provides a rule-based tracker similar to the baselines in DSTCs (Williams et al., 2013) that are adapted to handle multi-domain interactions. End-to-end Model ConvLab makes available two end-to-end dialog system models: Mem2Seq (Madotto et al., 2"
P19-3011,W18-5007,0,0.0124046,"slight modifications in the configuration file. Some example configuration files are presented in Section 4. 2.2 Figure 3: An environment configuration view. As shown in Figure 3, there are also many different ways of combining components to build an environment. For example, the top layer corresponds to a user simulator operating at the dialog act level which is the typical setting of prior works focusing on developing reinforcement learning methods for dialog policy optimization. As with dialog agent, there are recent attempts on end-toend approaches to avoid requiring expensive annotation (Kreyssig et al., 2018). For human evaluation, ConvLab also provides an integration of crowd source platform such as Amazon Mechanical Turk3 as shown in the bottom layer. Dialog Agent Configuration 2.4 Reference Models This section describes a set of reference models for each component that are available in the initial release. As we will keep adding new state-of-theart models, the set of reference models available in ConvLab will be extended. Figure 2: A dialog system configuration view. In Figure 2, each layer represents a different way of constructing a dialog system. The top 2 Environment Configuration 3 ConvLab"
P19-3011,P18-1133,0,0.0586765,"-driven approaches such as HUS and its variational variants (Gur et al., 2018). Similar to the system side, each model works at the dialog act level, and can be pipelined with NLU and NLG modules to construct a whole user simulator. Dialog State Tracking The dialog state tracker is responsible for updating the belief state. ConvLab provides a rule-based tracker similar to the baselines in DSTCs (Williams et al., 2013) that are adapted to handle multi-domain interactions. End-to-end Model ConvLab makes available two end-to-end dialog system models: Mem2Seq (Madotto et al., 2018) and Sequicity (Lei et al., 2018). To support multi-domain intents, Sequicity resets the belief span when the model predicts a topic shift between domains. Word-level Dialog State Tracking Word-level DSTs directly take system and user natural language as inputs and update dialog state. ConvLab imports MDBT (Ramadan et al., 2018) model which jointly identifies the domain and tracks the belief states by utilizing the semantic similarity between dialog utterances and ontology terms. 3 Domains The initial release of ConvLab offers two domains of differing complexity: MultiWOZ and Movie. MultiWOZ The main task of the MultiWOZ doma"
P19-3011,P17-4013,0,0.21599,"Missing"
P19-3011,P18-1136,0,0.0410831,"ann et al., 2007) user model and data-driven approaches such as HUS and its variational variants (Gur et al., 2018). Similar to the system side, each model works at the dialog act level, and can be pipelined with NLU and NLG modules to construct a whole user simulator. Dialog State Tracking The dialog state tracker is responsible for updating the belief state. ConvLab provides a rule-based tracker similar to the baselines in DSTCs (Williams et al., 2013) that are adapted to handle multi-domain interactions. End-to-end Model ConvLab makes available two end-to-end dialog system models: Mem2Seq (Madotto et al., 2018) and Sequicity (Lei et al., 2018). To support multi-domain intents, Sequicity resets the belief span when the model predicts a topic shift between domains. Word-level Dialog State Tracking Word-level DSTs directly take system and user natural language as inputs and update dialog state. ConvLab imports MDBT (Ramadan et al., 2018) model which jointly identifies the domain and tracks the belief states by utilizing the semantic similarity between dialog utterances and ontology terms. 3 Domains The initial release of ConvLab offers two domains of differing complexity: MultiWOZ and Movie. MultiWOZ T"
P19-3011,D15-1199,0,0.116547,"Missing"
P19-3011,P14-5010,0,0.00241576,"m who is new to the area to quickly develop a reasonable baseline system for task-oriented dialog due to the lack of a well-structured, easy-to-use open-source system that allows researchers to build and evaluate dialog bots. ConvLab is aimed to fill the gap. ConvLab is an open-source multi-domain end-toend dialog system that allows researchers to automatically train dialog models, build and evaluate task-completion dialog bots. Such open-source systems have been instrumental in many AI research breakthroughs. For example, among many, Moses (Koehn, 2007), HTK (Young et al., 2002) and CoreNLP (Manning et al., 2014) have been widely used to facilitate subsequent research in machine translation, speech recognition and natural language processing, respectively. ConvLab consists of a rich set of modeling tools and runtime engines for building task-oriented bots of different types, and an end-to-end evaluation platform. There are roughly two architectures 64 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 64–69 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Agents-Environments-Bodies (AEB) desig"
P19-3011,W13-4065,0,0.021667,"process multi-intent dialog acts. For more details on MILU, please refer to the ConvLab site. User Policy For user policy, ConvLab provides an agenda-based (Schatzmann et al., 2007) user model and data-driven approaches such as HUS and its variational variants (Gur et al., 2018). Similar to the system side, each model works at the dialog act level, and can be pipelined with NLU and NLG modules to construct a whole user simulator. Dialog State Tracking The dialog state tracker is responsible for updating the belief state. ConvLab provides a rule-based tracker similar to the baselines in DSTCs (Williams et al., 2013) that are adapted to handle multi-domain interactions. End-to-end Model ConvLab makes available two end-to-end dialog system models: Mem2Seq (Madotto et al., 2018) and Sequicity (Lei et al., 2018). To support multi-domain intents, Sequicity resets the belief span when the model predicts a topic shift between domains. Word-level Dialog State Tracking Word-level DSTs directly take system and user natural language as inputs and update dialog state. ConvLab imports MDBT (Ramadan et al., 2018) model which jointly identifies the domain and tracks the belief states by utilizing the semantic similarit"
P19-3011,D17-2014,0,0.0321439,"n down the pipeline. There also have emerged some models in-between (Wen et al., 2016; Mrkˇsi´c et al., 2016). Due to the wide range of approaches and different metrics used in prior studies, it is been impractical to perform a rigorous comparative study under the same condition. ConvLab is the first dialog research platform that covers a full range of trainable statistical models with fully annotated datasets, differing from previous toolkits whose focus is largely concentrated on the system policy component while other components are mostly limited to pre-fixed baseline models (Ultes, 2017; Miller et al., 2017; Li et al., 2018). There is also an increasing interest in building bots that seamlessly intertwine multiple subdomains to accomplish high-level user goals (Peng et al., 2017; Budzianowski et al., 2018). The development of multi-domain dialog system adds additional complexities to both data collection and annotation, and the models for dialog system components. For the former, Budzianowski et al. (2018) collected the MultiWOZ dataset, a dialog corpus with dialogs ranging over multiple domains for the trip information setting, whereas there is no open-platform yet that is designed to handle mu"
Q19-1023,P14-2102,1,0.830857,"the chosen edit. At each step it selects the highest-priority local rewrite rule that can apply, and applies it as far left as possible. When no more rules can apply, the final state of the string is returned as x. Simplifying the formalism Markov algorithms are Turing complete. Fortunately, Johnson (1972) noted that in practice, phonological u 7! x maps described in this formalism can usually be implemented with finite-state transducers (FSTs). For computational simplicity, we will formulate our punctuation model as a probabilistic FST (PFST)—a locally normalized left-to-right rewrite model (Cotterell et al., 2014). The probabilities for each language must be learned, using gradient descent. Normally we expect most probabilities to be near 0 or 1, making the PFST nearly deterministic (i.e., close to a subsequential FST). However, permitting low-probability choices remains useful to account for typographical errors, dialectal differences, and free variation in the training corpus. Our PFST generates a surface string, but the invertibility of FSTs will allow us to work backwards when analyzing a surface string (§3). 2.3 Training Objective Building on equation (2), we train ✓, to locally maximize the regul"
Q19-1023,Q15-1031,1,0.841055,"d machine translation (Zens et al., 2002), punctuation marks are usually either removed or treated as ˇ uˇrek and Sojka, 2010). ordinary words (Reh˚ Yet to us, building a parse tree on a surface sentence seems as inappropriate as morphologically segmenting a surface word. In both cases, one should instead analyze the latent underlying form, jointly with recovering that form. For example, the proper segmentation of English hoping is not hop-ing but hope-ing (with underlying e), and the proper segmentation of stopping is neither stopp-ing nor stop-ping but stop-ing (with only one underlying p). Cotterell et al. (2015, 2016) get this right for morphology. We attempt to do the same for punctuation. Punctuation is helpful Surface punctuation remains correlated with syntactic phrase structure. NLP systems for generating or editing text must be able to deploy surface punctuation as human writers do. Parsers and grammar induction systems benefit from the presence of surface punctuation marks (Jones, 1994; Spitkovsky et al., 2011). It is plausible that they could do better with a linguistically informed model that explains exactly why the surface punctuation appears where it does. Patterns of punctuation usage c"
Q19-1023,W11-0705,0,0.0298466,"2 T , the ATTACH process stochastically attaches a left puncteme l and a right puncteme r, which may be empty. The resulting tree T 0 has underlying punctuation u. Each slot’s punctuation ui 2 u is rewritten to xi 2 x by N OISY C HANNEL. Punctuation is meaningful Pang et al. (2002) use question and exclamation marks as clues to sentiment. Similarly, quotation marks may be used to mark titles, quotations, reported speech, or dubious terminology (University of Chicago, 2010). Because of examples like this, methods for determining the similarity or meaning of syntax trees, such as a tree kernel (Agarwal et al., 2011) or a recursive neural network (Tai et al., 2015), should ideally be able to consider where the underlying punctuation marks attach. In tasks such as word embedding induction (Mikolov et al., 2013; Pennington et al., 2014) and machine translation (Zens et al., 2002), punctuation marks are usually either removed or treated as ˇ uˇrek and Sojka, 2010). ordinary words (Reh˚ Yet to us, building a parse tree on a surface sentence seems as inappropriate as morphologically segmenting a surface word. In both cases, one should instead analyze the latent underlying form, jointly with recovering that for"
Q19-1023,N16-1080,0,0.0583243,"Missing"
Q19-1023,D16-1111,0,0.0172702,"s to segment a sentence: this &quot;divide and rule&quot; strategy reduces ambiguity in parsing of long Chinese sentences. Punctuation can similarly be used to constrain syntactic structure during grammar induction (Spitkovsky et al., 2011). Punctuation restoration (§ 6.1) is useful for transcribing text from unpunctuated speech. The task is usually treated by tagging each slot with zero or more punctuation tokens, using a traditional sequence labeling method: conditional random fields (Lui and Wang, 2013; Lu and Ng, 2010), recurrent neural networks (Tilk and Alumäe, 2016), or transition-based systems (Ballesteros and Wanner, 2016). Full 553.4 78.7 75.4 91.8 239.3 Table 6: Perplexity (evaluated on the train split to avoid evaluating generalization) of a trigram language model trained (with add-0.001 smoothing) on different versions of rephrased training sentences. “Punctuation” only evaluates perplexity on the trigrams that have punctuation. “All” evaluates on all the trigrams. “Base” permutes all surface dependents including punctuation (Wang and Eisner, 2016). “Full” is our full approach: recover underlying punctuation, permute remaining dependents, regenerate surface punctuation. “Half” is like “Full” but it permutes"
Q19-1023,W06-1628,0,0.127554,"Missing"
Q19-1023,P04-1054,0,0.0123749,"berg’s (1990) formal grammar for English punctuation, but is probabilistic and trainable. We give exact algorithms for training and inference. We trained Nunberg-like models for 5 languages and L2 English. We compared the English model to Nunberg’s, and showed how the trained models can be used across languages for punctuation restoration, correction, and adjustment. In the future, we would like to study the usefulness of the recovered underlying trees on tasks such as syntactically sensitive sentiment analysis (Tai et al., 2015), machine translation (Cowan et al., 2006), relation extraction (Culotta and Sorensen, 2004), and coreference resolution (Kong et al., 2010). We would also like to investigate how underlying punctuation could aid parsing. For discriminative parsing, features for scoring the tree could refer to the underlying punctuation, not just the surface punctuation. For generative parsing (§3), we could follow the ˆthe caper failed ,If true ,. We ˆthe caper failed ,If true . leave the handling of capitalization to future work. We test the naturalness of the permuted sentences by asking how well a word trigram language model trained on them could predict the original sentences.23 As shown in Tabl"
Q19-1023,N16-1024,0,0.0706246,"Missing"
Q19-1023,C94-1069,0,0.472677,"r segmentation of English hoping is not hop-ing but hope-ing (with underlying e), and the proper segmentation of stopping is neither stopp-ing nor stop-ping but stop-ing (with only one underlying p). Cotterell et al. (2015, 2016) get this right for morphology. We attempt to do the same for punctuation. Punctuation is helpful Surface punctuation remains correlated with syntactic phrase structure. NLP systems for generating or editing text must be able to deploy surface punctuation as human writers do. Parsers and grammar induction systems benefit from the presence of surface punctuation marks (Jones, 1994; Spitkovsky et al., 2011). It is plausible that they could do better with a linguistically informed model that explains exactly why the surface punctuation appears where it does. Patterns of punctuation usage can also help identify the writer’s native language (Markov et al., 2018). 2 Formal Model We propose a probabilistic generative model of sentences (Figure 1): (1) P ¯ (T 0 )) p(¯ x) = T,T 0 psyn (T ) · p✓ (T 0 |T ) · p (¯ x|u Punctuation is neglected Work on syntax and parsing tends to treat punctuation as an afterthought rather than a phenomenon governed by its own linguistic principles"
Q19-1023,C96-1058,1,0.279961,"reebank annotation guidelines for punctuation tend to adopt simple heuristics like “attach to the highest possible node that preserves projectivity” (Bies et al., 1995; Nivre et al., 2018).1 Many dependency parsing works exclude punctuation from evaluation (Nivre et al., 2007b; Koo and Collins, 2010; Chen and Manning, 2014; Lei et al., 2014; Kiperwasser and Goldberg, 2016), although some others retain punctuation (Nivre et al., 2007a; Goldberg and Elhadad, 2010; Dozat and Manning, 2017). First, an unpunctuated dependency tree T is stochastically generated by some recursive process psyn (e.g., Eisner, 1996, Model C).2 Second, each constituent (i.e., dependency subtree) sprouts optional underlying punctuation at its left and right edges, according to a probability distribution p✓ that depends on the constituent’s syntactic role (e.g., dobj for “direct object”). This punctuated ¯ = u ¯ (T 0 ), tree T 0 yields the underlying string u which is edited by a finite-state noisy channel p ¯. to arrive at the surface sentence x 1 http://universaldependencies.org/u/ dep/punct.html 2 Our model could be easily adapted to work on constituency trees instead. 358 1. Point Absorption 3. Period Absorption „ 7! ,"
Q19-1023,W16-5901,1,0.9227,"erlying punctemes at w and all its descendants. These punctemes determine an s-to-final path at i, then initial-to-final paths at i + 1 through k 1, then an initial-to-t path at k. The weight of the extended path is the product of all the WFSA weights on these paths (which correspond to transition probabilities in p PFST) times the probability of the choice of punctemes (from p✓ ). This inside algorithm computes quantities needed for training (§ 2.3). Useful variants arise via well-known methods for weighted derivation forests (Berstel and Reutenauer, 1988; Goodman, 1999; Li and Eisner, 2009; Eisner, 2016). Specifically, to modify Algorithm 1 to maximize over T 0 values (§§ 6.2–6.3) instead of summing over them, we switch to the derivation semiring (Goodman, 1999), as follows. Whereas I N(w)st used to store the total weight of all extended paths from state s at slot i to state t at slot j, now it will store the weight of the best such extended path. It will also store that extended path’s choice of underlying punctemes, in the form of a punctemeannotated version of the subtree of T that is rooted at w. This is a potential subtree of T 0 . Thus, each element of I N(w) has the form (r, D) where r"
Q19-1023,Q16-1023,0,0.0187934,"sentences (Figure 1): (1) P ¯ (T 0 )) p(¯ x) = T,T 0 psyn (T ) · p✓ (T 0 |T ) · p (¯ x|u Punctuation is neglected Work on syntax and parsing tends to treat punctuation as an afterthought rather than a phenomenon governed by its own linguistic principles. Treebank annotation guidelines for punctuation tend to adopt simple heuristics like “attach to the highest possible node that preserves projectivity” (Bies et al., 1995; Nivre et al., 2018).1 Many dependency parsing works exclude punctuation from evaluation (Nivre et al., 2007b; Koo and Collins, 2010; Chen and Manning, 2014; Lei et al., 2014; Kiperwasser and Goldberg, 2016), although some others retain punctuation (Nivre et al., 2007a; Goldberg and Elhadad, 2010; Dozat and Manning, 2017). First, an unpunctuated dependency tree T is stochastically generated by some recursive process psyn (e.g., Eisner, 1996, Model C).2 Second, each constituent (i.e., dependency subtree) sprouts optional underlying punctuation at its left and right edges, according to a probability distribution p✓ that depends on the constituent’s syntactic role (e.g., dobj for “direct object”). This punctuated ¯ = u ¯ (T 0 ), tree T 0 yields the underlying string u which is edited by a finite-sta"
Q19-1023,C10-1068,0,0.0214641,"is probabilistic and trainable. We give exact algorithms for training and inference. We trained Nunberg-like models for 5 languages and L2 English. We compared the English model to Nunberg’s, and showed how the trained models can be used across languages for punctuation restoration, correction, and adjustment. In the future, we would like to study the usefulness of the recovered underlying trees on tasks such as syntactically sensitive sentiment analysis (Tai et al., 2015), machine translation (Cowan et al., 2006), relation extraction (Culotta and Sorensen, 2004), and coreference resolution (Kong et al., 2010). We would also like to investigate how underlying punctuation could aid parsing. For discriminative parsing, features for scoring the tree could refer to the underlying punctuation, not just the surface punctuation. For generative parsing (§3), we could follow the ˆthe caper failed ,If true ,. We ˆthe caper failed ,If true . leave the handling of capitalization to future work. We test the naturalness of the permuted sentences by asking how well a word trigram language model trained on them could predict the original sentences.23 As shown in Table 6, our permutation approach reduces the perple"
Q19-1023,P10-1001,0,0.0145622,"). 2 Formal Model We propose a probabilistic generative model of sentences (Figure 1): (1) P ¯ (T 0 )) p(¯ x) = T,T 0 psyn (T ) · p✓ (T 0 |T ) · p (¯ x|u Punctuation is neglected Work on syntax and parsing tends to treat punctuation as an afterthought rather than a phenomenon governed by its own linguistic principles. Treebank annotation guidelines for punctuation tend to adopt simple heuristics like “attach to the highest possible node that preserves projectivity” (Bies et al., 1995; Nivre et al., 2018).1 Many dependency parsing works exclude punctuation from evaluation (Nivre et al., 2007b; Koo and Collins, 2010; Chen and Manning, 2014; Lei et al., 2014; Kiperwasser and Goldberg, 2016), although some others retain punctuation (Nivre et al., 2007a; Goldberg and Elhadad, 2010; Dozat and Manning, 2017). First, an unpunctuated dependency tree T is stochastically generated by some recursive process psyn (e.g., Eisner, 1996, Model C).2 Second, each constituent (i.e., dependency subtree) sprouts optional underlying punctuation at its left and right edges, according to a probability distribution p✓ that depends on the constituent’s syntactic role (e.g., dobj for “direct object”). This punctuated ¯ = u ¯ (T 0"
Q19-1023,N10-1115,0,0.0217842,"ation is neglected Work on syntax and parsing tends to treat punctuation as an afterthought rather than a phenomenon governed by its own linguistic principles. Treebank annotation guidelines for punctuation tend to adopt simple heuristics like “attach to the highest possible node that preserves projectivity” (Bies et al., 1995; Nivre et al., 2018).1 Many dependency parsing works exclude punctuation from evaluation (Nivre et al., 2007b; Koo and Collins, 2010; Chen and Manning, 2014; Lei et al., 2014; Kiperwasser and Goldberg, 2016), although some others retain punctuation (Nivre et al., 2007a; Goldberg and Elhadad, 2010; Dozat and Manning, 2017). First, an unpunctuated dependency tree T is stochastically generated by some recursive process psyn (e.g., Eisner, 1996, Model C).2 Second, each constituent (i.e., dependency subtree) sprouts optional underlying punctuation at its left and right edges, according to a probability distribution p✓ that depends on the constituent’s syntactic role (e.g., dobj for “direct object”). This punctuated ¯ = u ¯ (T 0 ), tree T 0 yields the underlying string u which is edited by a finite-state noisy channel p ¯. to arrive at the surface sentence x 1 http://universaldependencies.o"
Q19-1023,P14-1130,0,0.0144485,"nerative model of sentences (Figure 1): (1) P ¯ (T 0 )) p(¯ x) = T,T 0 psyn (T ) · p✓ (T 0 |T ) · p (¯ x|u Punctuation is neglected Work on syntax and parsing tends to treat punctuation as an afterthought rather than a phenomenon governed by its own linguistic principles. Treebank annotation guidelines for punctuation tend to adopt simple heuristics like “attach to the highest possible node that preserves projectivity” (Bies et al., 1995; Nivre et al., 2018).1 Many dependency parsing works exclude punctuation from evaluation (Nivre et al., 2007b; Koo and Collins, 2010; Chen and Manning, 2014; Lei et al., 2014; Kiperwasser and Goldberg, 2016), although some others retain punctuation (Nivre et al., 2007a; Goldberg and Elhadad, 2010; Dozat and Manning, 2017). First, an unpunctuated dependency tree T is stochastically generated by some recursive process psyn (e.g., Eisner, 1996, Model C).2 Second, each constituent (i.e., dependency subtree) sprouts optional underlying punctuation at its left and right edges, according to a probability distribution p✓ that depends on the constituent’s syntactic role (e.g., dobj for “direct object”). This punctuated ¯ = u ¯ (T 0 ), tree T 0 yields the underlying string"
Q19-1023,J99-4004,0,0.0312395,"d path is defined by a choice of underlying punctemes at w and all its descendants. These punctemes determine an s-to-final path at i, then initial-to-final paths at i + 1 through k 1, then an initial-to-t path at k. The weight of the extended path is the product of all the WFSA weights on these paths (which correspond to transition probabilities in p PFST) times the probability of the choice of punctemes (from p✓ ). This inside algorithm computes quantities needed for training (§ 2.3). Useful variants arise via well-known methods for weighted derivation forests (Berstel and Reutenauer, 1988; Goodman, 1999; Li and Eisner, 2009; Eisner, 2016). Specifically, to modify Algorithm 1 to maximize over T 0 values (§§ 6.2–6.3) instead of summing over them, we switch to the derivation semiring (Goodman, 1999), as follows. Whereas I N(w)st used to store the total weight of all extended paths from state s at slot i to state t at slot j, now it will store the weight of the best such extended path. It will also store that extended path’s choice of underlying punctemes, in the form of a punctemeannotated version of the subtree of T that is rooted at w. This is a potential subtree of T 0 . Thus, each element o"
Q19-1023,D08-1025,0,0.0424338,"l)M0 Mk (r) 15: E E + p · l,r have unmatched punc Inference 17: In principle, working with the model (1) is straightforward, thanks to the closure properties of formal languages. Provided that psyn can be encoded as a weighted CFG, it can be composed with the weighted tree transducer p✓ and the weighted FST p to yield a new weighted CFG (similarly to Bar-Hillel et al., 1961; Nederhof and Satta, 2003). Under this new grammar, one can recover the opti¯ by dynamic programming, or sum mal T, T 0 for x over T, T 0 by the inside algorithm to get the likelihood p(¯ x). A similar approach was used by Levy (2008) with a different FST noisy channel. In this paper we assume that T is observed, allowing us to work with equation (2). This cuts the computation time from O(n3 ) to O(n).9 Whereas the inside algorithm for (1) must consider O(n2 ) ¯ and O(n) ways of buildpossible constituents of x ing each, our algorithm for (2) only needs to iterate over the O(n) true constituents of T and the 1 true way of building each. However, it must still consider the |Wd |puncteme pairs for each constituent. 18: 3.1 return M Mroot I N(root(T )) > return 0 Mroot ⇢n , E . RNi ⇥Nk . R, R Algorithms ¯ of length n, our job"
Q19-1023,W03-3016,0,0.10159,"headword 8: Mleft ( w0 2leftkids(w) I N(w0 ))⇢j 1 > Q 0 9: Mright j ( w0 2rightkids(w) I N (w )) 10: M0 Mleft · 1 · Mright . RNj ⇥1 , R1⇥Nj 11: M 0 . RNi ⇥Nk 12: for (l, r) 2 Wd(w) do 13: p p✓ (l, r |w) 14: M M + p · Mi (l)M0 Mk (r) 15: E E + p · l,r have unmatched punc Inference 17: In principle, working with the model (1) is straightforward, thanks to the closure properties of formal languages. Provided that psyn can be encoded as a weighted CFG, it can be composed with the weighted tree transducer p✓ and the weighted FST p to yield a new weighted CFG (similarly to Bar-Hillel et al., 1961; Nederhof and Satta, 2003). Under this new grammar, one can recover the opti¯ by dynamic programming, or sum mal T, T 0 for x over T, T 0 by the inside algorithm to get the likelihood p(¯ x). A similar approach was used by Levy (2008) with a different FST noisy channel. In this paper we assume that T is observed, allowing us to work with equation (2). This cuts the computation time from O(n3 ) to O(n).9 Whereas the inside algorithm for (1) must consider O(n2 ) ¯ and O(n) ways of buildpossible constituents of x ing each, our algorithm for (2) only needs to iterate over the O(n) true constituents of T and the 1 true way"
Q19-1023,I05-2002,0,0.0786985,"he surface punctuation from the distribution modeled by the trained PFST, yielding 367 Arabic Chinese English Hindi Spanish Punctuation Base Half Full 156.0 231.3 186.1 165.2 110.0 61.4 98.4 74.5 51.0 10.8 11.0 9.7 266.2 259.2 194.5 Base 540.8 205.0 140.9 118.4 346.3 All Half 590.3 174.4 131.4 118.8 343.4 The parser of Ma et al. (2014) takes a different approach and treats punctuation marks as features of their neighboring words. Zhang et al. (2013) use a generative model for punctuated sentences, leting them restore punctuation marks during transition-based parsing of unpunctuated sentences. Li et al. (2005) use punctuation marks to segment a sentence: this &quot;divide and rule&quot; strategy reduces ambiguity in parsing of long Chinese sentences. Punctuation can similarly be used to constrain syntactic structure during grammar induction (Spitkovsky et al., 2011). Punctuation restoration (§ 6.1) is useful for transcribing text from unpunctuated speech. The task is usually treated by tagging each slot with zero or more punctuation tokens, using a traditional sequence labeling method: conditional random fields (Lui and Wang, 2013; Lu and Ng, 2010), recurrent neural networks (Tilk and Alumäe, 2016), or trans"
Q19-1023,W14-1701,0,0.0282755,"ailored for the punctuation correction task. where we is the node in Te0 , and p(l, r |we ) is a similar log-linear model to equation (4) with additional features (Appendix C4 ) which look at we . Finally, we reconstruct xc based on the noisy channel p (xc |Tc0 ) in § 2.2. During training, is regularized to be close to the noisy channel parameters in the punctuation model trained on en_cesl. We use the same MBR decoder as in § 6.1 to choose the best action. We evaluate using AED as in § 6.1. As a second metric, we use the script from the CoNLL 2014 Shared Task on Grammatical Error Correction (Ng et al., 2014): it computes the F0.5 -measure of the set of edits found by the system, relative to the true set of edits. As shown in Table 5, our method achieves better performance than the punctuation restoration baselines (which ignore input punctuation). On the other hand, it is soundly beaten by a new BiLSTM-CRF that we trained specifically for the task of punctuation correction. This is the same as the BiLSTM-CRF in the previous section, except that the BiLSTM now reads a punctuated input sentence (with possibly erroneous punctuation). To be precise, at step 0  i  n, the BiLSTM reads a concatenation"
Q19-1023,D09-1005,1,0.650378,"ed by a choice of underlying punctemes at w and all its descendants. These punctemes determine an s-to-final path at i, then initial-to-final paths at i + 1 through k 1, then an initial-to-t path at k. The weight of the extended path is the product of all the WFSA weights on these paths (which correspond to transition probabilities in p PFST) times the probability of the choice of punctemes (from p✓ ). This inside algorithm computes quantities needed for training (§ 2.3). Useful variants arise via well-known methods for weighted derivation forests (Berstel and Reutenauer, 1988; Goodman, 1999; Li and Eisner, 2009; Eisner, 2016). Specifically, to modify Algorithm 1 to maximize over T 0 values (§§ 6.2–6.3) instead of summing over them, we switch to the derivation semiring (Goodman, 1999), as follows. Whereas I N(w)st used to store the total weight of all extended paths from state s at slot i to state t at slot j, now it will store the weight of the best such extended path. It will also store that extended path’s choice of underlying punctemes, in the form of a punctemeannotated version of the subtree of T that is rooted at w. This is a potential subtree of T 0 . Thus, each element of I N(w) has the form"
Q19-1023,U13-1020,0,0.0297786,"ore punctuation marks during transition-based parsing of unpunctuated sentences. Li et al. (2005) use punctuation marks to segment a sentence: this &quot;divide and rule&quot; strategy reduces ambiguity in parsing of long Chinese sentences. Punctuation can similarly be used to constrain syntactic structure during grammar induction (Spitkovsky et al., 2011). Punctuation restoration (§ 6.1) is useful for transcribing text from unpunctuated speech. The task is usually treated by tagging each slot with zero or more punctuation tokens, using a traditional sequence labeling method: conditional random fields (Lui and Wang, 2013; Lu and Ng, 2010), recurrent neural networks (Tilk and Alumäe, 2016), or transition-based systems (Ballesteros and Wanner, 2016). Full 553.4 78.7 75.4 91.8 239.3 Table 6: Perplexity (evaluated on the train split to avoid evaluating generalization) of a trigram language model trained (with add-0.001 smoothing) on different versions of rephrased training sentences. “Punctuation” only evaluates perplexity on the trigrams that have punctuation. “All” evaluates on all the trigrams. “Base” permutes all surface dependents including punctuation (Wang and Eisner, 2016). “Full” is our full approach: re"
Q19-1023,P14-2128,0,0.0229467,"a writing assistance tool (Heidorn, 2000), or subtree deletions in compressive summarization (Knight and Marcu, 2002). mark root. ,advcl, det nsubj ˆ, If true, the caper failed . by the maximizing variant of Algorithm 1 (§ 3.1). Then, we permute the underlying tree and sample the surface punctuation from the distribution modeled by the trained PFST, yielding 367 Arabic Chinese English Hindi Spanish Punctuation Base Half Full 156.0 231.3 186.1 165.2 110.0 61.4 98.4 74.5 51.0 10.8 11.0 9.7 266.2 259.2 194.5 Base 540.8 205.0 140.9 118.4 346.3 All Half 590.3 174.4 131.4 118.8 343.4 The parser of Ma et al. (2014) takes a different approach and treats punctuation marks as features of their neighboring words. Zhang et al. (2013) use a generative model for punctuated sentences, leting them restore punctuation marks during transition-based parsing of unpunctuated sentences. Li et al. (2005) use punctuation marks to segment a sentence: this &quot;divide and rule&quot; strategy reduces ambiguity in parsing of long Chinese sentences. Punctuation can similarly be used to constrain syntactic structure during grammar induction (Spitkovsky et al., 2011). Punctuation restoration (§ 6.1) is useful for transcribing text from"
Q19-1023,C18-1293,0,0.0311786,"do the same for punctuation. Punctuation is helpful Surface punctuation remains correlated with syntactic phrase structure. NLP systems for generating or editing text must be able to deploy surface punctuation as human writers do. Parsers and grammar induction systems benefit from the presence of surface punctuation marks (Jones, 1994; Spitkovsky et al., 2011). It is plausible that they could do better with a linguistically informed model that explains exactly why the surface punctuation appears where it does. Patterns of punctuation usage can also help identify the writer’s native language (Markov et al., 2018). 2 Formal Model We propose a probabilistic generative model of sentences (Figure 1): (1) P ¯ (T 0 )) p(¯ x) = T,T 0 psyn (T ) · p✓ (T 0 |T ) · p (¯ x|u Punctuation is neglected Work on syntax and parsing tends to treat punctuation as an afterthought rather than a phenomenon governed by its own linguistic principles. Treebank annotation guidelines for punctuation tend to adopt simple heuristics like “attach to the highest possible node that preserves projectivity” (Bies et al., 1995; Nivre et al., 2018).1 Many dependency parsing works exclude punctuation from evaluation (Nivre et al., 2007b; K"
Q19-1023,D14-1162,0,0.0877368,"y N OISY C HANNEL. Punctuation is meaningful Pang et al. (2002) use question and exclamation marks as clues to sentiment. Similarly, quotation marks may be used to mark titles, quotations, reported speech, or dubious terminology (University of Chicago, 2010). Because of examples like this, methods for determining the similarity or meaning of syntax trees, such as a tree kernel (Agarwal et al., 2011) or a recursive neural network (Tai et al., 2015), should ideally be able to consider where the underlying punctuation marks attach. In tasks such as word embedding induction (Mikolov et al., 2013; Pennington et al., 2014) and machine translation (Zens et al., 2002), punctuation marks are usually either removed or treated as ˇ uˇrek and Sojka, 2010). ordinary words (Reh˚ Yet to us, building a parse tree on a surface sentence seems as inappropriate as morphologically segmenting a surface word. In both cases, one should instead analyze the latent underlying form, jointly with recovering that form. For example, the proper segmentation of English hoping is not hop-ing but hope-ing (with underlying e), and the proper segmentation of stopping is neither stopp-ing nor stop-ping but stop-ing (with only one underlying p"
Q19-1023,W11-0303,0,0.130287,"n of English hoping is not hop-ing but hope-ing (with underlying e), and the proper segmentation of stopping is neither stopp-ing nor stop-ping but stop-ing (with only one underlying p). Cotterell et al. (2015, 2016) get this right for morphology. We attempt to do the same for punctuation. Punctuation is helpful Surface punctuation remains correlated with syntactic phrase structure. NLP systems for generating or editing text must be able to deploy surface punctuation as human writers do. Parsers and grammar induction systems benefit from the presence of surface punctuation marks (Jones, 1994; Spitkovsky et al., 2011). It is plausible that they could do better with a linguistically informed model that explains exactly why the surface punctuation appears where it does. Patterns of punctuation usage can also help identify the writer’s native language (Markov et al., 2018). 2 Formal Model We propose a probabilistic generative model of sentences (Figure 1): (1) P ¯ (T 0 )) p(¯ x) = T,T 0 psyn (T ) · p✓ (T 0 |T ) · p (¯ x|u Punctuation is neglected Work on syntax and parsing tends to treat punctuation as an afterthought rather than a phenomenon governed by its own linguistic principles. Treebank annotation guid"
Q19-1023,P15-1150,0,0.34039,"ft puncteme l and a right puncteme r, which may be empty. The resulting tree T 0 has underlying punctuation u. Each slot’s punctuation ui 2 u is rewritten to xi 2 x by N OISY C HANNEL. Punctuation is meaningful Pang et al. (2002) use question and exclamation marks as clues to sentiment. Similarly, quotation marks may be used to mark titles, quotations, reported speech, or dubious terminology (University of Chicago, 2010). Because of examples like this, methods for determining the similarity or meaning of syntax trees, such as a tree kernel (Agarwal et al., 2011) or a recursive neural network (Tai et al., 2015), should ideally be able to consider where the underlying punctuation marks attach. In tasks such as word embedding induction (Mikolov et al., 2013; Pennington et al., 2014) and machine translation (Zens et al., 2002), punctuation marks are usually either removed or treated as ˇ uˇrek and Sojka, 2010). ordinary words (Reh˚ Yet to us, building a parse tree on a surface sentence seems as inappropriate as morphologically segmenting a surface word. In both cases, one should instead analyze the latent underlying form, jointly with recovering that form. For example, the proper segmentation of Englis"
Q19-1023,W16-5907,0,0.0500477,"Missing"
Q19-1023,Q16-1035,1,0.864955,"as the BiLSTM-CRF in the previous section, except that the BiLSTM now reads a punctuated input sentence (with possibly erroneous punctuation). To be precise, at step 0  i  n, the BiLSTM reads a concatenation of the embedding of word i (or BOS if i = 0) with an embedding of the punctuation token sequence xi . The BiLSTMCRF wins because it is a discriminative model tailored for this task: the BiLSTM can extract arbitrary contextual features of slot i that are correlated with whether xi is correct in context. 6.3 For our experiment, we evaluate an interesting case of syntactic transformation. Wang and Eisner (2016) consider a systematic rephrasing procedure by rearranging the order of dependent subtrees within a UD treebank, in order to synthesize new languages with different word order that can then be used to help train multi-lingual systems (i.e., data augmentation with synthetic data). As Wang and Eisner acknowledge (2016, footnote 9), their permutations treat surface punctuation tokens like ordinary words, which can result in synthetic sentences whose punctuation is quite unlike that of real languages. In our experiment, we use Wang and Eisner’s (2016) “self-permutation” setting, where the dependen"
Q19-1023,W08-1703,0,0.0284705,"ive parsing (§3), we could follow the ˆthe caper failed ,If true ,. We ˆthe caper failed ,If true . leave the handling of capitalization to future work. We test the naturalness of the permuted sentences by asking how well a word trigram language model trained on them could predict the original sentences.23 As shown in Table 6, our permutation approach reduces the perplexity over the baseline on 4 of the 5 languages, often dramatically. 7 Conclusion and Future Work Related Work Punctuation can aid syntactic analysis, since it signals phrase boundaries and sentence structure. Briscoe (1994) and White and Rajkumar (2008) parse punctuated sentences using hand-crafted constraint-based grammars that implement Nunberg’s approach in a declarative way. These grammars treat surface punctuation symbols as ordinary words, but annotate the nonterminal categories so as to effectively keep track of the underlying punctuation. This is tantamount to crafting a grammar for underlyingly punctuated sentences and composing it with a finite-state noisy channel. 23 So the two approaches to permutation yield different training data, but are compared fairly on the same test data. 368 scheme in equation (1). For example, the psyn f"
Q19-1023,2002.tmi-tutorials.2,0,0.0772995,"et al. (2002) use question and exclamation marks as clues to sentiment. Similarly, quotation marks may be used to mark titles, quotations, reported speech, or dubious terminology (University of Chicago, 2010). Because of examples like this, methods for determining the similarity or meaning of syntax trees, such as a tree kernel (Agarwal et al., 2011) or a recursive neural network (Tai et al., 2015), should ideally be able to consider where the underlying punctuation marks attach. In tasks such as word embedding induction (Mikolov et al., 2013; Pennington et al., 2014) and machine translation (Zens et al., 2002), punctuation marks are usually either removed or treated as ˇ uˇrek and Sojka, 2010). ordinary words (Reh˚ Yet to us, building a parse tree on a surface sentence seems as inappropriate as morphologically segmenting a surface word. In both cases, one should instead analyze the latent underlying form, jointly with recovering that form. For example, the proper segmentation of English hoping is not hop-ing but hope-ing (with underlying e), and the proper segmentation of stopping is neither stopp-ing nor stop-ping but stop-ing (with only one underlying p). Cotterell et al. (2015, 2016) get this ri"
Q19-1023,P13-1074,0,0.0267007,"2002). mark root. ,advcl, det nsubj ˆ, If true, the caper failed . by the maximizing variant of Algorithm 1 (§ 3.1). Then, we permute the underlying tree and sample the surface punctuation from the distribution modeled by the trained PFST, yielding 367 Arabic Chinese English Hindi Spanish Punctuation Base Half Full 156.0 231.3 186.1 165.2 110.0 61.4 98.4 74.5 51.0 10.8 11.0 9.7 266.2 259.2 194.5 Base 540.8 205.0 140.9 118.4 346.3 All Half 590.3 174.4 131.4 118.8 343.4 The parser of Ma et al. (2014) takes a different approach and treats punctuation marks as features of their neighboring words. Zhang et al. (2013) use a generative model for punctuated sentences, leting them restore punctuation marks during transition-based parsing of unpunctuated sentences. Li et al. (2005) use punctuation marks to segment a sentence: this &quot;divide and rule&quot; strategy reduces ambiguity in parsing of long Chinese sentences. Punctuation can similarly be used to constrain syntactic structure during grammar induction (Spitkovsky et al., 2011). Punctuation restoration (§ 6.1) is useful for transcribing text from unpunctuated speech. The task is usually treated by tagging each slot with zero or more punctuation tokens, using a"
Q19-1023,P11-2033,0,0.0260543,"distribution given by the sample. This can be evaluated in O(m2 ) time. Performance on Extrinsic Tasks 19 To depunctuate a treebank sentence, we remove all tokens with POS-tag PUNCT or dependency relation punct. These are almost always leaves; else we omit the sentence. 20 Ideally, rather than maximize, one would integrate over possible trees T , in practice by sampling many Svalues Tk ¯ ) and replacing S(T ) in (10) with k S(Tk ). from psyn (· |u 21 Specifically, the Yara parser (Rasooli and Tetreault, 2015), a fast non-probabilistic transition-based parser that uses rich non-local features (Zhang and Nivre, 2011). We evaluate the trained punctuation model by using it in the following three tasks. 17 [en] Earlier, Kerry said, “Just because you get an honorable discharge does not, in fact, answer that question.” 18 [en] Sections 1, 2, 5, 6, 7, and 8 will survive any termination of this License. 365 We evaluate on Arabic, English, Chinese, Hindi, and Spanish. For each language, we train both the parser and the punctuation model on the training split of that UD treebank (§4), and evaluate on held-out data. We compare to the BiLSTM-CRF baseline in §4 (Xu et al., 2016).22 We also compare to a “trivial” dete"
Q19-1023,D10-1018,0,\N,Missing
Q19-1023,W02-1011,0,\N,Missing
Q19-1023,D14-1082,0,\N,Missing
Q19-1023,D07-1096,0,\N,Missing
R13-1051,J96-1002,0,0.21711,"Missing"
R13-1051,N04-4028,0,0.0297943,"n engine. Louis and Nenkova (2009) presented a study of predicting the confidence of automatic summarization outputs. Many approaches for confidence estimation have also been explored and implemented in other NLP research areas. There are also many previous confidence estimation studies in IE, and most of these have been in the Active Learning literature. Thompson et al. (1999) proposed a rule-based extraction method to compute confidence. Scheffer et al. (2001) utilized hidden Markov models to measure the confidence in an IE system, but they only estimated the confidence of singleton tokens. Culotta and McCallum (2004)’s work is the most relevant to our work, since they also utilized a machine learning model to estimate the confidence values for IE outputs. They estimated the confidence of both extracted fields and entire multi-field records mainly through a linear-chain Conditional Random Field (CRF) model, but their case studies are not as complicated and challenging as slot filling, since SF systems need to handle difficult crossdocument coreference resolution, sophisticated inference, and also other challenges (Min and Grishman, 2012). Furthermore, to the best of our knowledge, there is no previous work"
R13-1051,W03-0413,0,0.0360649,"et al., 2006). There is previous work in IE using probabilistic and heuristic methods to estimate confidence for extracting fields using a sequential model, but to the best of our knowledge, this work is the first probabilistic CE model for the multi-stage systems employed for the Knowledge Base Population (KBP) Slot Filling task (Section 2). 2 Related Work Confidence estimation is a generic machine learning approach for measuring confidence of a given output, and many different CE methods have been used extensively in various Natural Language Processing (NLP) fields (Gandrabur et al., 2006). Gandrabur and Foster (2003) and Nguyen et al. (2011) investigated the use of machine learning approaches for confidence estimation in machine translation. Agichtein (2006) showed 396 Proceedings of Recent Advances in Natural Language Processing, pages 396–401, Hissar, Bulgaria, 7-13 September 2013. Expectation-Maximization algorithms to estimate the confidence for partially supervised relation extraction. White et al. (2007) described how a maximum entropy model can be used to generate confidence scores for a speech recognition engine. Louis and Nenkova (2009) presented a study of predicting the confidence of automatic"
R13-1051,P11-1115,1,0.87933,"Missing"
R13-1051,E09-1062,0,0.0137873,"anguage Processing (NLP) fields (Gandrabur et al., 2006). Gandrabur and Foster (2003) and Nguyen et al. (2011) investigated the use of machine learning approaches for confidence estimation in machine translation. Agichtein (2006) showed 396 Proceedings of Recent Advances in Natural Language Processing, pages 396–401, Hissar, Bulgaria, 7-13 September 2013. Expectation-Maximization algorithms to estimate the confidence for partially supervised relation extraction. White et al. (2007) described how a maximum entropy model can be used to generate confidence scores for a speech recognition engine. Louis and Nenkova (2009) presented a study of predicting the confidence of automatic summarization outputs. Many approaches for confidence estimation have also been explored and implemented in other NLP research areas. There are also many previous confidence estimation studies in IE, and most of these have been in the Active Learning literature. Thompson et al. (1999) proposed a rule-based extraction method to compute confidence. Scheffer et al. (2001) utilized hidden Markov models to measure the confidence in an IE system, but they only estimated the confidence of singleton tokens. Culotta and McCallum (2004)’s work"
R13-1051,P11-1022,0,0.0323237,"work in IE using probabilistic and heuristic methods to estimate confidence for extracting fields using a sequential model, but to the best of our knowledge, this work is the first probabilistic CE model for the multi-stage systems employed for the Knowledge Base Population (KBP) Slot Filling task (Section 2). 2 Related Work Confidence estimation is a generic machine learning approach for measuring confidence of a given output, and many different CE methods have been used extensively in various Natural Language Processing (NLP) fields (Gandrabur et al., 2006). Gandrabur and Foster (2003) and Nguyen et al. (2011) investigated the use of machine learning approaches for confidence estimation in machine translation. Agichtein (2006) showed 396 Proceedings of Recent Advances in Natural Language Processing, pages 396–401, Hissar, Bulgaria, 7-13 September 2013. Expectation-Maximization algorithms to estimate the confidence for partially supervised relation extraction. White et al. (2007) described how a maximum entropy model can be used to generate confidence scores for a speech recognition engine. Louis and Nenkova (2009) presented a study of predicting the confidence of automatic summarization outputs. Ma"
R13-1052,W06-1613,0,0.324414,"t belonging to other functions The weakness of the cited work is discussed Table 1: Annotation Scheme for Citation Function: + represents POSITIVE sentiment, NEUTRAL sentiment, and − represents negative sentiment = represents citations. Dong and Sch¨afer (2011) proposed a four-category definition of citation functions following Moravcsik and Murugesan (1975) and a self-training-based classification model. Different from previous work that mainly classified citations into sentiment categories or coarse-grained functions, our scheme, we believe, is more finegrained. It is also worth noting that Teufel et al. (2006a), Athar (2011), and Dong and Sch¨afer (2011) all worked on citations in computational linguistics papers, but we investigate citations in biomedical articles. Researchers have introduced several annotation schemes for citation analysis. The work of Teufel et al. (2006b) is the most related to ours. They proposed an annotation scheme for citation functions based on why authors cite a particular paper, following Spiegel-R¨using (1977). This scheme provides clear definition for some of the basic citation functions, such as Contrast, but mainly concerns the citations that authors compare to or b"
R13-1052,W12-4303,0,0.0320289,"Missing"
R13-1052,W06-1312,0,0.0978912,"t belonging to other functions The weakness of the cited work is discussed Table 1: Annotation Scheme for Citation Function: + represents POSITIVE sentiment, NEUTRAL sentiment, and − represents negative sentiment = represents citations. Dong and Sch¨afer (2011) proposed a four-category definition of citation functions following Moravcsik and Murugesan (1975) and a self-training-based classification model. Different from previous work that mainly classified citations into sentiment categories or coarse-grained functions, our scheme, we believe, is more finegrained. It is also worth noting that Teufel et al. (2006a), Athar (2011), and Dong and Sch¨afer (2011) all worked on citations in computational linguistics papers, but we investigate citations in biomedical articles. Researchers have introduced several annotation schemes for citation analysis. The work of Teufel et al. (2006b) is the most related to ours. They proposed an annotation scheme for citation functions based on why authors cite a particular paper, following Spiegel-R¨using (1977). This scheme provides clear definition for some of the basic citation functions, such as Contrast, but mainly concerns the citations that authors compare to or b"
R13-1052,N12-1073,0,0.24777,"Missing"
R13-1052,J96-2004,0,0.272417,"Missing"
R13-1052,I11-1070,0,0.0483684,"Missing"
R13-1052,N09-1066,0,0.0180753,"sis systems are usually able to identify positive, neutral, or negative opinions, but if we want to better understand the exact function of a citation, we need to 2 Related Work The background for our work is in citation analysis. Applications of citation analysis include evaluating the impact of a published literature through a measurable bibliometric (Garfield, 1972; Luukkonen, 1992; Borgman and Furner, 2002), analyzing bibliometric networks (Radev et al., 2009), summarizing scientific papers (Qazvinian and Radev, 2008; Abu-Jbara and Radev, 2011), generating surveys of scientific paradigms (Mohammad et al., 2009), among others. Correctly and accurately recognizing citation functions is a cornerstone for these tasks. 402 Proceedings of Recent Advances in Natural Language Processing, pages 402–407, Hissar, Bulgaria, 7-13 September 2013. Citation Function Based on+ Corroboration+ Discover+ Positive+ Practical+ Significant+ Standard+ Supply+ Contrast= Co-citation= Neutral= Negative− Description A work is based on the cited work Two works corroborate each other Acknowledge the invention of a technique The cited work is successful The cited work has a practical use The cited work is important The cited work"
R13-1052,C08-1087,0,0.0202032,"ent, which has achieved good accuracy (see, e.g., (Teufel et al., 2006a)). Citation sentiment analysis systems are usually able to identify positive, neutral, or negative opinions, but if we want to better understand the exact function of a citation, we need to 2 Related Work The background for our work is in citation analysis. Applications of citation analysis include evaluating the impact of a published literature through a measurable bibliometric (Garfield, 1972; Luukkonen, 1992; Borgman and Furner, 2002), analyzing bibliometric networks (Radev et al., 2009), summarizing scientific papers (Qazvinian and Radev, 2008; Abu-Jbara and Radev, 2011), generating surveys of scientific paradigms (Mohammad et al., 2009), among others. Correctly and accurately recognizing citation functions is a cornerstone for these tasks. 402 Proceedings of Recent Advances in Natural Language Processing, pages 402–407, Hissar, Bulgaria, 7-13 September 2013. Citation Function Based on+ Corroboration+ Discover+ Positive+ Practical+ Significant+ Standard+ Supply+ Contrast= Co-citation= Neutral= Negative− Description A work is based on the cited work Two works corroborate each other Acknowledge the invention of a technique The cited"
R13-1052,P11-3015,0,\N,Missing
R13-1052,P11-1051,0,\N,Missing
R15-1010,P05-1045,0,0.0264124,"ect instances of events, where the patterns consist of a predicate, event trigger, and constraints on its local syntactic context. The constraints may involve specific lexical items or semantic classes. Efforts to improve event extraction performance have focused largely on either improving the pattern-matching kernel or adding new reasonable features. Most event extraction frameworks are feature-based systems. Some of the featurebased systems are based on phrase or sentence level extraction. Several recent studies use highlevel information to aid local event extraction systems. For example, (Finkel et al., 2005), (Maslennikov and Chua, 2007), (Ji and Grishman, 2008) and (Patwardhan and Riloff, 2007) tried to use discourse, document, or cross-document information to improve information extraction. Other research extends these approaches by introducing cross-event information to enhance the performance of multi-event-type extraction systems. (Liao and Grishman, 2010) use information about other types of events to make predictions or resolve ambiguities regarding a given event. (Li et 2. Sports Patterns Since ACE events are mainly about commercial and security-related news, patterns related to sports sh"
R15-1010,I13-1081,1,0.897092,"Missing"
R15-1010,E12-1029,0,0.0354177,"Missing"
R15-1010,P08-1030,1,0.957992,"classifiers for event arguments and argument roles.) We can see from Table 1 that the resulting system performance is competitive with other recent system results, such as the joint beam search described in (Li et al., 2013). 4 4.3 Discussion Experiments In this section, we will introduce the evaluation dataset, compare the performance of applying pattern expansion with other state-of-the-art systems, and discuss the contribution of pattern expansion. 4.1 Data We used the ACE 2005 corpus as our testbed. For comparison, we used the same test set with 40 newswire articles (672 sentences) as in (Ji and Grishman, 2008; Liao and Grishman, 2010) for the experiments, and randomly selected 30 other documents (863 sentences) from different genres as the development set. The remaining 529 documents (14,840 sentences) are used for training. Regarding the correctness criteria, following the previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Ji and Grishman, 2011; Li et al., 2013), a trigger candidate is counted as correct if its event subtype and offsets match those of a reference trigger. The ACE 2005 corpus has 33 event subtypes that, along with one class “None” for the non-trigger tokens, constitute"
R15-1010,P11-1113,0,0.489201,"Missing"
R15-1010,P13-1008,0,0.111191,"tion. At test time, to classify a candidate trigger (any word which has appeared at least once as a trigger in the training corpus) the tagger finds the best match between an event pattern and the input sentence and computes an event score. This score, along with other features, serves as input to the maximum entropy model to make the final ED prediction. (This brief description omits the classifiers for event arguments and argument roles.) We can see from Table 1 that the resulting system performance is competitive with other recent system results, such as the joint beam search described in (Li et al., 2013). 4 4.3 Discussion Experiments In this section, we will introduce the evaluation dataset, compare the performance of applying pattern expansion with other state-of-the-art systems, and discuss the contribution of pattern expansion. 4.1 Data We used the ACE 2005 corpus as our testbed. For comparison, we used the same test set with 40 newswire articles (672 sentences) as in (Ji and Grishman, 2008; Liao and Grishman, 2010) for the experiments, and randomly selected 30 other documents (863 sentences) from different genres as the development set. The remaining 529 documents (14,840 sentences) are u"
R15-1010,P10-1081,1,0.933065,"Missing"
R15-1010,W13-5711,0,0.0304095,"Missing"
R15-1010,P07-1075,0,0.0193811,", where the patterns consist of a predicate, event trigger, and constraints on its local syntactic context. The constraints may involve specific lexical items or semantic classes. Efforts to improve event extraction performance have focused largely on either improving the pattern-matching kernel or adding new reasonable features. Most event extraction frameworks are feature-based systems. Some of the featurebased systems are based on phrase or sentence level extraction. Several recent studies use highlevel information to aid local event extraction systems. For example, (Finkel et al., 2005), (Maslennikov and Chua, 2007), (Ji and Grishman, 2008) and (Patwardhan and Riloff, 2007) tried to use discourse, document, or cross-document information to improve information extraction. Other research extends these approaches by introducing cross-event information to enhance the performance of multi-event-type extraction systems. (Liao and Grishman, 2010) use information about other types of events to make predictions or resolve ambiguities regarding a given event. (Li et 2. Sports Patterns Since ACE events are mainly about commercial and security-related news, patterns related to sports should be removed. For example,"
R15-1010,D07-1075,0,0.0269549,", and constraints on its local syntactic context. The constraints may involve specific lexical items or semantic classes. Efforts to improve event extraction performance have focused largely on either improving the pattern-matching kernel or adding new reasonable features. Most event extraction frameworks are feature-based systems. Some of the featurebased systems are based on phrase or sentence level extraction. Several recent studies use highlevel information to aid local event extraction systems. For example, (Finkel et al., 2005), (Maslennikov and Chua, 2007), (Ji and Grishman, 2008) and (Patwardhan and Riloff, 2007) tried to use discourse, document, or cross-document information to improve information extraction. Other research extends these approaches by introducing cross-event information to enhance the performance of multi-event-type extraction systems. (Liao and Grishman, 2010) use information about other types of events to make predictions or resolve ambiguities regarding a given event. (Li et 2. Sports Patterns Since ACE events are mainly about commercial and security-related news, patterns related to sports should be removed. For example, “win a title” is one of the top 5 high-frequency dependency"
R15-1010,P05-1047,0,0.0849158,"Missing"
R15-1010,P03-1029,1,0.771002,"Missing"
R15-1010,C00-2136,1,0.548333,"Missing"
R15-1010,P03-1044,0,\N,Missing
R15-1011,P10-1081,1,0.928248,"of applying dependency regularization with other state-of-the-art systems, and discuss the contributions of these different dependency regularization rules. 4.1 3. With Nomlex Regularization, the sentence “The acquisition of Banco Zaragozano...” is detected as a TRANSFER - OWNERSHIP event, which was ignored in the original framework. This is because all the relevant sentences in the training data use the same trigger “acquire”. Data set We used the ACE 2005 corpus as our testbed. For comparison, we used the same test set with 40 newswire articles (672 sentences) as in (Ji and Grishman, 2008; Liao and Grishman, 2010) for the experiments, and randomly selected 30 other documents (863 sentences) from different genres as the development set. The remaining 529 documents (14,840 sentences) are used for training. Regarding the correctness criteria: following previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Ji and Grishman, 2011; Li et al., 2013), a trigger candidate is counted as correct if its event subtype and offsets match those of a reference trigger. The ACE 2005 corpus has 33 event subtypes that, along with one class “None” for the non-trigger tokens, constitutes a 34-class classification pr"
R15-1011,P07-1075,0,0.0770946,"Missing"
R15-1011,C14-1214,0,0.011351,"(2007) tried to use discourse, document, or cross-document information to improve information extraction. Other research extends these approaches by introducing cross-event information to enhance the performance of multievent-type extraction systems. Liao and Grishman (2010) use information about other types of events to make predictions or resolve ambiguities regarding a given event. Li et al. (2013) implements a joint model via structured prediction with crossevent features. Event extraction systems have used patterns and features based on a range of linguistic representations. For example, Miwa et al. (2014) used both a deep analysis and a dependency parse. The original NYU system for the 2005 ACE evaluation (Grishman et al., 2005) incorporated GLARF, a representation which captured both notions of transparency and verb-nominalization correspondences.4 However, assessment of the impact of individual regularizations has been limited; this prompted the investigation reported here. Contributions of different dependency regularizations Table 2 lists the system performance applying the different dependency regularization rules. The last line shows the performance with the combination of three types of"
R15-1011,D07-1075,0,0.026477,"setting, and also advances the current state-of-the-art systems. 4.2 5 Related Work Although there have been quite a few distinct designs for event extraction systems, most are loosely based on using patterns to detect instances of events, where the patterns consist of a predicate, event trigger, and constraints on its local syntactic context. The constraints may involve specific lexical items or semantic classes. Some recent studies use high-level information to aid local event extraction systems. For example, Finkel et al. (2005), Maslennikov and seng Chua (2007), Ji and Grishman (2008) and Patwardhan and Riloff (2007) tried to use discourse, document, or cross-document information to improve information extraction. Other research extends these approaches by introducing cross-event information to enhance the performance of multievent-type extraction systems. Liao and Grishman (2010) use information about other types of events to make predictions or resolve ambiguities regarding a given event. Li et al. (2013) implements a joint model via structured prediction with crossevent features. Event extraction systems have used patterns and features based on a range of linguistic representations. For example, Miwa e"
R15-1011,D11-1116,0,0.0523809,"ransparent word regularization Dependency Regularization 2.1 The ACE 2005 Event Guidelines specify a set of 33 types of events; these have been widely used for research on event extraction over the past decade. Some trigger words are unambiguous indicators of particular types of events. For example, the word murder indicates an event of type Die. However, most words have multiple senses and so may be associated with multiple types of events. Many of these cases can be disambiguated based on the semantic types of the trigger arguments: Verb Chain Regularization We use a fast dependency parser (Tratz and Hovy, 2011) that analyzes multi-word verb groups (with auxiliaries) into chains with the first word at the head of the chain. Verb Chain (vch) Regularization reverses the verb chains to place the main (final) verb at the top of the dependency parse tree. This reduces the variation in the dependency paths from trigger to arguments due to differences in tense, aspect, and modality. Here is an example sentence containing a verb chain: • fire can be either an ATTACK event (“fire a weapon”) or and END - POSITION event (“fire a person”), with the cases distinguishable by the semantic type of the direct object."
R15-1011,P05-1045,0,0.0478138,"stem with dependency regularizations can improve the performance over our baseline setting, and also advances the current state-of-the-art systems. 4.2 5 Related Work Although there have been quite a few distinct designs for event extraction systems, most are loosely based on using patterns to detect instances of events, where the patterns consist of a predicate, event trigger, and constraints on its local syntactic context. The constraints may involve specific lexical items or semantic classes. Some recent studies use high-level information to aid local event extraction systems. For example, Finkel et al. (2005), Maslennikov and seng Chua (2007), Ji and Grishman (2008) and Patwardhan and Riloff (2007) tried to use discourse, document, or cross-document information to improve information extraction. Other research extends these approaches by introducing cross-event information to enhance the performance of multievent-type extraction systems. Liao and Grishman (2010) use information about other types of events to make predictions or resolve ambiguities regarding a given event. Li et al. (2013) implements a joint model via structured prediction with crossevent features. Event extraction systems have use"
R15-1011,P08-1030,1,0.957347,"compare the performance of applying dependency regularization with other state-of-the-art systems, and discuss the contributions of these different dependency regularization rules. 4.1 3. With Nomlex Regularization, the sentence “The acquisition of Banco Zaragozano...” is detected as a TRANSFER - OWNERSHIP event, which was ignored in the original framework. This is because all the relevant sentences in the training data use the same trigger “acquire”. Data set We used the ACE 2005 corpus as our testbed. For comparison, we used the same test set with 40 newswire articles (672 sentences) as in (Ji and Grishman, 2008; Liao and Grishman, 2010) for the experiments, and randomly selected 30 other documents (863 sentences) from different genres as the development set. The remaining 529 documents (14,840 sentences) are used for training. Regarding the correctness criteria: following previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Ji and Grishman, 2011; Li et al., 2013), a trigger candidate is counted as correct if its event subtype and offsets match those of a reference trigger. The ACE 2005 corpus has 33 event subtypes that, along with one class “None” for the non-trigger tokens, constitutes a"
R15-1011,P11-1113,0,0.187324,"amework. This is because all the relevant sentences in the training data use the same trigger “acquire”. Data set We used the ACE 2005 corpus as our testbed. For comparison, we used the same test set with 40 newswire articles (672 sentences) as in (Ji and Grishman, 2008; Liao and Grishman, 2010) for the experiments, and randomly selected 30 other documents (863 sentences) from different genres as the development set. The remaining 529 documents (14,840 sentences) are used for training. Regarding the correctness criteria: following previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Ji and Grishman, 2011; Li et al., 2013), a trigger candidate is counted as correct if its event subtype and offsets match those of a reference trigger. The ACE 2005 corpus has 33 event subtypes that, along with one class “None” for the non-trigger tokens, constitutes a 34-class classification problem in this work. Finally we use Precision (P), Recall (R), and F-measure (F1) to evaluate the overall performance. Table 1 presents the overall performance of the systems with gold-standard entity mention and type information. We can see that our system with dependency regularizations can improve the performance over our"
R15-1011,P13-1008,0,0.377677,"e all the relevant sentences in the training data use the same trigger “acquire”. Data set We used the ACE 2005 corpus as our testbed. For comparison, we used the same test set with 40 newswire articles (672 sentences) as in (Ji and Grishman, 2008; Liao and Grishman, 2010) for the experiments, and randomly selected 30 other documents (863 sentences) from different genres as the development set. The remaining 529 documents (14,840 sentences) are used for training. Regarding the correctness criteria: following previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Ji and Grishman, 2011; Li et al., 2013), a trigger candidate is counted as correct if its event subtype and offsets match those of a reference trigger. The ACE 2005 corpus has 33 event subtypes that, along with one class “None” for the non-trigger tokens, constitutes a 34-class classification problem in this work. Finally we use Precision (P), Recall (R), and F-measure (F1) to evaluate the overall performance. Table 1 presents the overall performance of the systems with gold-standard entity mention and type information. We can see that our system with dependency regularizations can improve the performance over our baseline setting,"
W11-1215,P98-1069,0,0.238684,"6)) or logic reasoning (e.g. (Harabagiu et al., 2005)). However, all of these approaches made limited eﬀorts at disambiguating entities in queries and limited use of fact extraction in answer search and validation. Several recent IE studies have stressed the beneﬁts of using information redundancy on estimating the correctness of the IE output (Downey et al., 2005; Yangarber, 2006; Patwardhan and Riloﬀ, 2009; Ji and Grish111 man, 2008). Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009). To the best of our knowledge, this is the ﬁrst work on mining facts from comparable corpora for answer validation in a new crosslingual entity proﬁling task. 3 Experimental Setup 3.1 Task Deﬁnition The goal of the KBP slot ﬁlling task is to extract facts from a large source corpus regarding certain attributes (“slots”) of an entity, which may be a person or organization, and use these facts to augment an existing knowledge base (KB). Along with each slot answer, the system must provide the"
W11-1215,P08-1030,1,0.887201,"Missing"
W11-1215,Y09-1024,1,0.840324,"on each other, so we can improve the results by improving the “coherence” of the story (i.e. consistency among all generated answers - query proﬁles). We use feature f2 to check whether the same answer was generated for conﬂicting slots, such as per:parents and per:children. Compared to traditional QA tasks, slot ﬁlling is a more ﬁne-grained task in which diﬀerent slots are expected to obtain semantically diﬀerent answers. Therefore, we explored semantic constraints in both local and global contexts. For example, we utilized bilingual name gazetteers from ACE training corpora, Google n-grams (Ji and Lin, 2009) and the geonames website 3 to encode features f6, f8 and f9; The org:top members/employees slot requires a system to distinguish whether a person member/ employee is in the top position, thus we encoded f10 for this purpose. The knowledge used in our baseline pipelines is relatively static – it is not updated during the 3 http://www.geonames.org/statistics/ 115 extraction process. Achieving high performance for cross-lingual slot ﬁlling requires that we take a broader view, one that looks outside a single document or a single language in order to exploit global knowledge. Fortunately, as more"
W11-1215,W09-3107,1,0.883354,"disambiguating entities in queries and limited use of fact extraction in answer search and validation. Several recent IE studies have stressed the beneﬁts of using information redundancy on estimating the correctness of the IE output (Downey et al., 2005; Yangarber, 2006; Patwardhan and Riloﬀ, 2009; Ji and Grish111 man, 2008). Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009). To the best of our knowledge, this is the ﬁrst work on mining facts from comparable corpora for answer validation in a new crosslingual entity proﬁling task. 3 Experimental Setup 3.1 Task Deﬁnition The goal of the KBP slot ﬁlling task is to extract facts from a large source corpus regarding certain attributes (“slots”) of an entity, which may be a person or organization, and use these facts to augment an existing knowledge base (KB). Along with each slot answer, the system must provide the ID of a document which supports the correctness of this answer. KBP 2010 (Ji et al., 2010) deﬁnes 26 ty"
W11-1215,N06-1011,0,0.0324019,"knowledge from dependency parsing (e.g. (Shen et al., 2006)) or logic reasoning (e.g. (Harabagiu et al., 2005)). However, all of these approaches made limited eﬀorts at disambiguating entities in queries and limited use of fact extraction in answer search and validation. Several recent IE studies have stressed the beneﬁts of using information redundancy on estimating the correctness of the IE output (Downey et al., 2005; Yangarber, 2006; Patwardhan and Riloﬀ, 2009; Ji and Grish111 man, 2008). Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009). To the best of our knowledge, this is the ﬁrst work on mining facts from comparable corpora for answer validation in a new crosslingual entity proﬁling task. 3 Experimental Setup 3.1 Task Deﬁnition The goal of the KBP slot ﬁlling task is to extract facts from a large source corpus regarding certain attributes (“slots”) of an entity, which may be a person or organization, and use these facts to augment an existing knowledge base (KB). Along w"
W11-1215,O08-3001,0,0.0379743,"ms for improving crosslingual document retrieval. Many previous validation methods for crosslingual QA, such as those organized by Cross Language Evaluation Forum (Vallin et al., 2005), focused on local information which involves only the query and answer (e.g. (Kwork and Deng, 2006)), keyword translation (e.g. (Mitamura et al., 2006)) and surface patterns (e.g. (Soubbotin and Soubbotin, 2001)). Some global validation approaches considered information redundancy based on shallow statistics including cooccurrence, density score and mutual information (Clarke et al., 2001; Magnini et al., 2001; Lee et al., 2008), deeper knowledge from dependency parsing (e.g. (Shen et al., 2006)) or logic reasoning (e.g. (Harabagiu et al., 2005)). However, all of these approaches made limited eﬀorts at disambiguating entities in queries and limited use of fact extraction in answer search and validation. Several recent IE studies have stressed the beneﬁts of using information redundancy on estimating the correctness of the IE output (Downey et al., 2005; Yangarber, 2006; Patwardhan and Riloﬀ, 2009; Ji and Grish111 man, 2008). Some recent research used comparable corpora to re-score name transliterations (Sproat et al."
W11-1215,P09-1113,0,0.0373131,"ion 3.3.2 Monolingual Slot Filling We applied a state-of-the-art bilingual slot ﬁlling system (Chen et al., 2010) to process bilingual comparable corpora. This baseline system includes a supervised ACE IE pipeline and a bottom-up pattern matching pipeline. The IE pipeline includes relation extraction and event extraction based on maximum entropy models that incorporate diverse lexical, syntactic, semantic and ontological knowledge. The extracted ACE relations and events are then mapped to KBP slot ﬁlls. In pattern matching, we extract and rank patterns based on a distant supervision approach (Mintz et al., 2009) that uses entity-attribute pairs from Wikipedia Infoboxes and Freebase (Bollacker et al., 2008). We set a low threshold to include more answer candidates, and then a series of ﬁltering steps to reﬁne and improve the overall pipeline results. The ﬁltering steps include removing answers which have inappropriate entity types or have inappropriate dependency paths to the entities. 3.3.3 Document and Name Translation English Candidate Answers Figure 1: Overview of Baseline Crosslingual Slot Filling Pipelines 1 http://www.itl.nist.gov/iad/mig/tests/ace/ 112 We use a statistical, phrase-based MT sys"
W11-1215,W06-1905,0,0.0311885,"on we also demonstrate that these two approaches are complementary and can be used to boost each other’s results in a statistical rescoring model with global evidence from large comparable corpora. Hakkani-Tur et al. (2007) described a ﬁltering mechanism using two crosslingual IE systems for improving crosslingual document retrieval. Many previous validation methods for crosslingual QA, such as those organized by Cross Language Evaluation Forum (Vallin et al., 2005), focused on local information which involves only the query and answer (e.g. (Kwork and Deng, 2006)), keyword translation (e.g. (Mitamura et al., 2006)) and surface patterns (e.g. (Soubbotin and Soubbotin, 2001)). Some global validation approaches considered information redundancy based on shallow statistics including cooccurrence, density score and mutual information (Clarke et al., 2001; Magnini et al., 2001; Lee et al., 2008), deeper knowledge from dependency parsing (e.g. (Shen et al., 2006)) or logic reasoning (e.g. (Harabagiu et al., 2005)). However, all of these approaches made limited eﬀorts at disambiguating entities in queries and limited use of fact extraction in answer search and validation. Several recent IE studies have stresse"
W11-1215,P03-1021,0,0.0127194,"Missing"
W11-1215,C10-2109,0,0.154114,"Missing"
W11-1215,D09-1016,0,0.0253525,"ased on shallow statistics including cooccurrence, density score and mutual information (Clarke et al., 2001; Magnini et al., 2001; Lee et al., 2008), deeper knowledge from dependency parsing (e.g. (Shen et al., 2006)) or logic reasoning (e.g. (Harabagiu et al., 2005)). However, all of these approaches made limited eﬀorts at disambiguating entities in queries and limited use of fact extraction in answer search and validation. Several recent IE studies have stressed the beneﬁts of using information redundancy on estimating the correctness of the IE output (Downey et al., 2005; Yangarber, 2006; Patwardhan and Riloﬀ, 2009; Ji and Grish111 man, 2008). Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009). To the best of our knowledge, this is the ﬁrst work on mining facts from comparable corpora for answer validation in a new crosslingual entity proﬁling task. 3 Experimental Setup 3.1 Task Deﬁnition The goal of the KBP slot ﬁlling task is to extract facts from a large source corpus regard"
W11-1215,P99-1067,0,0.0820529,"ng (e.g. (Harabagiu et al., 2005)). However, all of these approaches made limited eﬀorts at disambiguating entities in queries and limited use of fact extraction in answer search and validation. Several recent IE studies have stressed the beneﬁts of using information redundancy on estimating the correctness of the IE output (Downey et al., 2005; Yangarber, 2006; Patwardhan and Riloﬀ, 2009; Ji and Grish111 man, 2008). Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009). To the best of our knowledge, this is the ﬁrst work on mining facts from comparable corpora for answer validation in a new crosslingual entity proﬁling task. 3 Experimental Setup 3.1 Task Deﬁnition The goal of the KBP slot ﬁlling task is to extract facts from a large source corpus regarding certain attributes (“slots”) of an entity, which may be a person or organization, and use these facts to augment an existing knowledge base (KB). Along with each slot answer, the system must provide the ID of a docu"
W11-1215,C04-1089,0,0.145328,"rabagiu et al., 2005)). However, all of these approaches made limited eﬀorts at disambiguating entities in queries and limited use of fact extraction in answer search and validation. Several recent IE studies have stressed the beneﬁts of using information redundancy on estimating the correctness of the IE output (Downey et al., 2005; Yangarber, 2006; Patwardhan and Riloﬀ, 2009; Ji and Grish111 man, 2008). Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009). To the best of our knowledge, this is the ﬁrst work on mining facts from comparable corpora for answer validation in a new crosslingual entity proﬁling task. 3 Experimental Setup 3.1 Task Deﬁnition The goal of the KBP slot ﬁlling task is to extract facts from a large source corpus regarding certain attributes (“slots”) of an entity, which may be a person or organization, and use these facts to augment an existing knowledge base (KB). Along with each slot answer, the system must provide the ID of a document which supports"
W11-1215,P06-1112,0,0.0206846,"idation methods for crosslingual QA, such as those organized by Cross Language Evaluation Forum (Vallin et al., 2005), focused on local information which involves only the query and answer (e.g. (Kwork and Deng, 2006)), keyword translation (e.g. (Mitamura et al., 2006)) and surface patterns (e.g. (Soubbotin and Soubbotin, 2001)). Some global validation approaches considered information redundancy based on shallow statistics including cooccurrence, density score and mutual information (Clarke et al., 2001; Magnini et al., 2001; Lee et al., 2008), deeper knowledge from dependency parsing (e.g. (Shen et al., 2006)) or logic reasoning (e.g. (Harabagiu et al., 2005)). However, all of these approaches made limited eﬀorts at disambiguating entities in queries and limited use of fact extraction in answer search and validation. Several recent IE studies have stressed the beneﬁts of using information redundancy on estimating the correctness of the IE output (Downey et al., 2005; Yangarber, 2006; Patwardhan and Riloﬀ, 2009; Ji and Grish111 man, 2008). Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fu"
W11-1215,P06-1010,0,0.0288872,"et al., 2008), deeper knowledge from dependency parsing (e.g. (Shen et al., 2006)) or logic reasoning (e.g. (Harabagiu et al., 2005)). However, all of these approaches made limited eﬀorts at disambiguating entities in queries and limited use of fact extraction in answer search and validation. Several recent IE studies have stressed the beneﬁts of using information redundancy on estimating the correctness of the IE output (Downey et al., 2005; Yangarber, 2006; Patwardhan and Riloﬀ, 2009; Ji and Grish111 man, 2008). Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009). To the best of our knowledge, this is the ﬁrst work on mining facts from comparable corpora for answer validation in a new crosslingual entity proﬁling task. 3 Experimental Setup 3.1 Task Deﬁnition The goal of the KBP slot ﬁlling task is to extract facts from a large source corpus regarding certain attributes (“slots”) of an entity, which may be a person or organization, and use these facts to augment an existing"
W11-1215,C04-1127,0,0.277434,"Workshop on Building and Using Comparable Corpora, pages 110–119, 49th Annual Meeting of the Association for Computational Linguistics, c Portland, Oregon, 24 June 2011. 2011 Association for Computational Linguistics provides substantial improvement over each individual baseline system and even state-of-theart monolingual slot ﬁlling systems. Compared to previous methods of exploiting comparable corpora, our approach is novel in multiple aspects because it exploits knowledge from: (1) both local and global statistics; (2) both languages; and (3) both shallow and deep analysis. 2 Related Work Sudo et al. (2004) found that for a crosslingual single-document IE task, source language extraction and fact translation performed notably better than machine translation and target language extraction. We observed the same results. In addition we also demonstrate that these two approaches are complementary and can be used to boost each other’s results in a statistical rescoring model with global evidence from large comparable corpora. Hakkani-Tur et al. (2007) described a ﬁltering mechanism using two crosslingual IE systems for improving crosslingual document retrieval. Many previous validation methods for cr"
W11-1215,E09-1091,0,0.0620664,"e limited eﬀorts at disambiguating entities in queries and limited use of fact extraction in answer search and validation. Several recent IE studies have stressed the beneﬁts of using information redundancy on estimating the correctness of the IE output (Downey et al., 2005; Yangarber, 2006; Patwardhan and Riloﬀ, 2009; Ji and Grish111 man, 2008). Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009). To the best of our knowledge, this is the ﬁrst work on mining facts from comparable corpora for answer validation in a new crosslingual entity proﬁling task. 3 Experimental Setup 3.1 Task Deﬁnition The goal of the KBP slot ﬁlling task is to extract facts from a large source corpus regarding certain attributes (“slots”) of an entity, which may be a person or organization, and use these facts to augment an existing knowledge base (KB). Along with each slot answer, the system must provide the ID of a document which supports the correctness of this answer. KBP 2010 (Ji et al., 2010) d"
W11-1215,N04-1033,0,0.0573646,"Missing"
W11-1215,C98-1066,0,\N,Missing
W11-1215,P02-1054,0,\N,Missing
W15-4502,P13-1008,0,0.172747,"ntext in ED. In this work, all AMR parse graphs are automatically generated from the first published AMR parser, JAMR (Flanigan et al., 2014). 3 Experiments 4.1 Dataset and Evaluation Metric We evaluate our system with above presented features over the ACE 2005 corpus. For comparison purposes, we utilize the same test set with 40 newswire articles (672 sentences), the same development set with 30 other documents (836 sentences) and the same training set with the remaining 529 documents (14, 849 sentences) as the previous studies on this dataset (Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013b). Following the previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013b), a trigger candidate is counted as correct if its event subtype and offsets match those of a reference trigger. The ACE 2005 corpus has 33 event subtypes that, along with one class “Other” for the non-trigger tokens, constitutes a 34-class classification problem in this work. Finally we use Precision (P), Recall (R), and F-measure (F1 ) to evaluate the performance. Table 2 presents the overall performance of the systems with gold-standard entity mention and Framework and Featur"
W15-4502,W06-0901,0,0.363076,"Missing"
W15-4502,P10-1081,1,0.890682,"only used to represent context in ED. In this work, all AMR parse graphs are automatically generated from the first published AMR parser, JAMR (Flanigan et al., 2014). 3 Experiments 4.1 Dataset and Evaluation Metric We evaluate our system with above presented features over the ACE 2005 corpus. For comparison purposes, we utilize the same test set with 40 newswire articles (672 sentences), the same development set with 30 other documents (836 sentences) and the same training set with the remaining 529 documents (14, 849 sentences) as the previous studies on this dataset (Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013b). Following the previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013b), a trigger candidate is counted as correct if its event subtype and offsets match those of a reference trigger. The ACE 2005 corpus has 33 event subtypes that, along with one class “Other” for the non-trigger tokens, constitutes a 34-class classification problem in this work. Finally we use Precision (P), Recall (R), and F-measure (F1 ) to evaluate the performance. Table 2 presents the overall performance of the systems with gold-standard entity mention and Fra"
W15-4502,W13-2322,0,0.0400666,"Missing"
W15-4502,R11-1002,1,0.927597,"Missing"
W15-4502,N15-1114,0,0.0103745,", including methods based on Markov Logic Networks (Riedel et al., 2009; Poon and Vanderwende, 2010; Venugopal et al., 2014), structured perceptrons (Li et al., 2013b), and dual decomposition (Riedel and McCallum (2009; 2011a; 2011b)). However, all of these methods as mentioned above have not exploited the knowledge captured in the AMR. A growing number of researchers are studying how to incorporate the knowledge encoded in the AMR parse and representations to help solve other NLP problems, such as entity linking (Pan et al., 2015), machine translation (Jones et al., 2015), and summarization (Liu et al., 2015). Especially the appearance of the first published AMR parser (Flanigan et al., 2014) will benefit and spur a lot of new research conducted using AMR. Discussion Applying the AMR features separately, we find that the features extracted from the sibling nodes are the best predictors of correctness, which indicates that the contexts of sibling nodes associated with the AMR tags can provide better evidence for word sense disambiguation of the trigger candidate as needed for event type classification. Features from the parent node and children nodes are also significant contributors. Performance o"
W15-4502,dorr-etal-1998-thematic,0,0.0693321,"Missing"
W15-4502,P11-1163,0,0.040647,"Missing"
W15-4502,P14-1134,0,0.243288,"be syntactic “sugar” and are not explicitly represented in AMR, except for the semantic relations they signal. Hence, it assigns the same AMR parse graph to sentences that have the same basic meaning.3 Compared to traditional dependency parsing and semantic role labeling, the nodes in AMR are entities instead of words, and the edge types are much more fine-grained. AMR thus captures deeper meaning compared with other representations which are more commonly used to represent context in ED. In this work, all AMR parse graphs are automatically generated from the first published AMR parser, JAMR (Flanigan et al., 2014). 3 Experiments 4.1 Dataset and Evaluation Metric We evaluate our system with above presented features over the ACE 2005 corpus. For comparison purposes, we utilize the same test set with 40 newswire articles (672 sentences), the same development set with 30 other documents (836 sentences) and the same training set with the remaining 529 documents (14, 849 sentences) as the previous studies on this dataset (Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013b). Following the previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013b), a trigg"
W15-4502,J05-1004,0,0.0338394,"ure 1: Two equivalent ways of representing the AMR parse graph, and the corresponding feature AMR parse for the example sentence, “The acquivalues, for trigger candidate “acquisition”, from sition of Edison GE will boost AIG’s annual life the above example AMR graph. insurance revenue.” boost-01 Improve Event Detection with Abstract Meaning Representation ARG0 ARG1 4 In this section, we will compare our MaxEnt classifiers using both baseline features and additional proposed AMR features with the state-of-the-art systems on the blind test set, and then discuss the results in more detail. 2002; Palmer et al., 2005). For example, a phrase like “bond investor” is represented using the frame “invest-01”, even though no verbs appear. In addition, many function words (determiners, prepositions) are considered to be syntactic “sugar” and are not explicitly represented in AMR, except for the semantic relations they signal. Hence, it assigns the same AMR parse graph to sentences that have the same basic meaning.3 Compared to traditional dependency parsing and semantic role labeling, the nodes in AMR are entities instead of words, and the edge types are much more fine-grained. AMR thus captures deeper meaning co"
W15-4502,N15-1119,0,0.0309298,") of baseline and AMR on a subset of event types. 4.2 has worked on joint models, including methods based on Markov Logic Networks (Riedel et al., 2009; Poon and Vanderwende, 2010; Venugopal et al., 2014), structured perceptrons (Li et al., 2013b), and dual decomposition (Riedel and McCallum (2009; 2011a; 2011b)). However, all of these methods as mentioned above have not exploited the knowledge captured in the AMR. A growing number of researchers are studying how to incorporate the knowledge encoded in the AMR parse and representations to help solve other NLP problems, such as entity linking (Pan et al., 2015), machine translation (Jones et al., 2015), and summarization (Liu et al., 2015). Especially the appearance of the first published AMR parser (Flanigan et al., 2014) will benefit and spur a lot of new research conducted using AMR. Discussion Applying the AMR features separately, we find that the features extracted from the sibling nodes are the best predictors of correctness, which indicates that the contexts of sibling nodes associated with the AMR tags can provide better evidence for word sense disambiguation of the trigger candidate as needed for event type classification. Features from the"
W15-4502,P09-2093,0,0.0487434,"Missing"
W15-4502,D09-1016,0,0.0224803,"Missing"
W15-4502,P11-1113,0,0.102381,"shed AMR parser, JAMR (Flanigan et al., 2014). 3 Experiments 4.1 Dataset and Evaluation Metric We evaluate our system with above presented features over the ACE 2005 corpus. For comparison purposes, we utilize the same test set with 40 newswire articles (672 sentences), the same development set with 30 other documents (836 sentences) and the same training set with the remaining 529 documents (14, 849 sentences) as the previous studies on this dataset (Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013b). Following the previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013b), a trigger candidate is counted as correct if its event subtype and offsets match those of a reference trigger. The ACE 2005 corpus has 33 event subtypes that, along with one class “Other” for the non-trigger tokens, constitutes a 34-class classification problem in this work. Finally we use Precision (P), Recall (R), and F-measure (F1 ) to evaluate the performance. Table 2 presents the overall performance of the systems with gold-standard entity mention and Framework and Features To compare our proposed AMR features with the previous approaches, we implemented a Maximum Ent"
W15-4502,N10-1123,0,0.0178759,"Missing"
W15-4502,D11-1001,0,0.024998,"Missing"
W15-4502,P08-1030,1,0.682387,"ons which are more commonly used to represent context in ED. In this work, all AMR parse graphs are automatically generated from the first published AMR parser, JAMR (Flanigan et al., 2014). 3 Experiments 4.1 Dataset and Evaluation Metric We evaluate our system with above presented features over the ACE 2005 corpus. For comparison purposes, we utilize the same test set with 40 newswire articles (672 sentences), the same development set with 30 other documents (836 sentences) and the same training set with the remaining 529 documents (14, 849 sentences) as the previous studies on this dataset (Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013b). Following the previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013b), a trigger candidate is counted as correct if its event subtype and offsets match those of a reference trigger. The ACE 2005 corpus has 33 event subtypes that, along with one class “Other” for the non-trigger tokens, constitutes a 34-class classification problem in this work. Finally we use Precision (P), Recall (R), and F-measure (F1 ) to evaluate the performance. Table 2 presents the overall performance of the systems with gold-standa"
W15-4502,W11-1807,0,0.0180918,"Missing"
W15-4502,W09-1406,0,0.0383609,"Missing"
W15-4502,kingsbury-palmer-2002-treebank,0,0.0917821,"Missing"
W15-4502,D12-1092,0,0.0248754,"Missing"
W15-4502,P13-1145,0,0.0316416,"Missing"
W15-4502,C12-1083,0,\N,Missing
Y10-1027,D08-1021,0,0.0363569,"Missing"
Y10-1027,P08-2017,0,0.025065,"can annotate event arguments efficiently and effectively by correcting SRL output. In order to evaluate the robustness of our approach, we experimented with both traditional news domain and a new domain of carbon sequestration. We expect that our method will enable developing annotated event corpora rapidly, and thus lead IE techniques to higher performance and broader applicability. In the future we will focus on filtering the remaining noise in trigger clustering by adding more distributional constraints. In addition, we plan to use more advanced cost-conscious active learning methods (e.g. Haertel et al., 2008) to further speed up our annotation procedures, because our task fits naturally into a hierarchical framework with multiple annotation sub-tasks. We are also interested in applying domain adaptation techniques on SRL in order to boost event annotation quality for non-news domains. Acknowledgement This work was supported by the U.S. Army Research Laboratory under Cooperative Agreement Number W911NF-09-2-0053, the U.S. NSF CAREER Award under Grant IIS-0953149, Google, Inc., DARPA GALE Program, CUNY Research Enhancement Program, PSC-CUNY Research Program, Faculty Publication Program and GRTI Prog"
Y10-1027,N06-2015,0,0.0176551,"DP as a 235 236 Regular Papers vector whose dimensions correspond to the words wi in the corpus vocabulary, and whose cell values are the log-likelihood ratio of w (or X) and wi, as in McDonald (2000). Then, each profile is biased toward each of the word senses s of w (or sense r of X), one sense at a time, and each of the biased profiles of w is compared with each of the biased profiles of X, using cosine of these vectors. The final similarity score of w and X is the score of the highest scoring sense-biased DPs. Words senses were determined using the technique and data in Mahammad and Hirst (2006). For each w we use the 50 top ranking candidates X above similarity score threshold 0.10, after filtering (shallowly approximated) textually entailing candidates: if all words of w also appear in same order in X, we filter X out. For example, for ``decided&apos;&apos;, the candidate ``decided quickly&apos;&apos; is filtered out. The semantic similarity between triggers u and v can be estimated by calculating the above similarity (vector distance) between their DPs. Then, u and v are grouped into the same cluster if the distance is shorter than a threshold. For example, an “abandon” cluster with similarity thresh"
Y10-1027,ji-etal-2010-annotating,1,0.813221,"new view of IE by considering it as a more fine-grained version of semantic role labeling (SRL). For each novel event type, we identify relevant and salient sentences involving new event triggers, and then correct errors based on uncertainty estimation, and finally map semantic roles into event argument roles based on semantic frame descriptions. We will demonstrate that our approach can make novel event discovery and annotation much more feasible (Section 5) by testing on three corpora from different domains: ACE, OntoNotes (Hovy et al., 2006) and a corpus of carbon sequestration literature (Ji et al., 2010). 2 2.1 Approach Overview Event Definition An event is a specific occurrence involving participants, and can be frequently described as a change of state (LDC, 2005). An event includes the following elements: Event type: a particular event class, such as “Life/Be-Born”, “Business-Transaction”, etc. Event trigger: the main word which most clearly expresses an event occurrence Event arguments: the mentions that are involved in an event (participants) For example, the sentence “the US-led coalition troops are reportedly thrusting into the second Iraqi city of Basra.” includes a “Movement_Transpor"
Y10-1027,W09-1704,1,0.922588,"ic similarity between triggers u and v can be estimated by calculating the above similarity (vector distance) between their DPs. Then, u and v are grouped into the same cluster if the distance is shorter than a threshold. For example, an “abandon” cluster with similarity threshold of 0.1 includes: {abandon, retire from, blight, quit, pull out, abducted, disuse, call off, end, withdraw, …} 3.3 Crosslingually derived Trigger Clustering In addition to the first clustering approach, we exploit a cross-lingual clustering algorithm based on sentence-aligned bilingual parallel texts, a.k.a. bitexts (Ji, 2009) to discover additional event trigger clusters. The general idea is that if two words w and u on the bitext’s source side are aligned with the same word on the target side with high confidence, then they should be grouped into the same cluster. In this paper we use Chinese-English bitexts from DARPA GALE program1. For each Chinese trigger, we search its aligned English words in order to construct a cluster including possible English trigger words. Then we acquire Chinese triggers from the other direction and continue the iterations. The word alignment was obtained by running Giza++ (Och and Ne"
Y10-1027,P09-1116,0,0.0738456,"Missing"
Y10-1027,D09-1040,1,0.821096,"09), together with the 1233 English triggers and 852 Chinese triggers in ACE05 training corpora as our ‘pivot’ event triggers. In order to minimize the impact of word alignment errors and some other noise, we conduct lemmatization based on WordNet (Fellbaum, 1998), and filter out stop-words (Fox, 1992), numbers and punctuations, time expressions and other function words that are not helpful in trigger clustering. 3.2 Monolingually derived Trigger Clustering The first trigger clustering approach is based on distributional semantic similarity measures over a monolingual, source-language corpus (Marton et al., 2009; Marton, 2010). We constructed a monolingual English corpus of about 500 million words, consisting of all English Gigaword documents from 2004 and 2008 (LDC2009T13). With this corpus, we used essentially the earlier technique described in Marton (2010). Its outline is as follows: For each word (or word sequence) of interest w, collect all contexts L w R in which w appear in our monolingual corpus, and then collect paraphrase candidates: all word sequences X up to 6 token long appearing in the same L X R contexts. Then, rank the candidates X by their semantic similarity to w, as estimated by a"
Y10-1027,W06-1605,0,0.0210812,"Missing"
Y10-1027,J03-1002,0,0.00247528,"(Ji, 2009) to discover additional event trigger clusters. The general idea is that if two words w and u on the bitext’s source side are aligned with the same word on the target side with high confidence, then they should be grouped into the same cluster. In this paper we use Chinese-English bitexts from DARPA GALE program1. For each Chinese trigger, we search its aligned English words in order to construct a cluster including possible English trigger words. Then we acquire Chinese triggers from the other direction and continue the iterations. The word alignment was obtained by running Giza++ (Och and Ney, 2003). From each cluster we filter out those trigger pairs with frequency (in bitexts) less than some threshold (we have tried frequency threshold of 1, 2, 3 and 4 separately). For example, “announce” is not an ACE-type event, but we can get its cluster as follows: {宣布, 通告，断言，宣告，声明，预报，传达，阐明，提出，显露，显示，陈述}  {announce, declare, herald, proclaim, set forth, set out, state, unveil, convey, affirm, assert} 3.4 Event Cluster Ranking Let E denote the unlabeled corpus from which we want to discover novel event types and annotate event arguments. We apply a high-performance entity extraction system (Grishman"
Y10-1027,J05-1004,0,0.0122398,"Correction & Role Mapping Novel Event Annotations Figure 1. Novel Event Discovery and Annotation Pipeline 3 Novel Event Type Discovery After pre-processing the text resources (Section 3.1), we continue with automatically detecting candidate event types based on trigger clustering (Sections 3.2 and 3.3), and then detecting novel event types based on cluster ranking (Section 3.4). 3.1 Pre-processing We apply two open-domain state-of-the-art automatic trigger clustering methods to discover event trigger clusters. We consider a collection of 3065 English verbs and 4865 Chinese verbs in PropBank (Palmer et al., 2005, Xue and Palmer, 2009), together with the 1233 English triggers and 852 Chinese triggers in ACE05 training corpora as our ‘pivot’ event triggers. In order to minimize the impact of word alignment errors and some other noise, we conduct lemmatization based on WordNet (Fellbaum, 1998), and filter out stop-words (Fox, 1992), numbers and punctuations, time expressions and other function words that are not helpful in trigger clustering. 3.2 Monolingually derived Trigger Clustering The first trigger clustering approach is based on distributional semantic similarity measures over a monolingual, sour"
Y10-1027,N03-1024,0,0.0625279,"r permeability. In these and many other cases, SRL failed to classify such arguments mainly because of some domain-specific features such as argument heads (e.g. “rates” appear very rarely as an “ARG0” in news domain). We expect to get further improvement after we incorporate some domainspecific knowledge such as high-frequency terminology lexicons into the SRL system. 6 Related Work A number of previous studies have described extensive techniques to cluster words or word sequences from large unlabeled corpora (e.g. , Lin and Wu, 2009), monolingual parallel corpora (e.g. Lin and Pantel, 2001; Pang et al., 2003) and bilingual parallel corpora (e.g. Callison-Burch et al., 2008; Ji, 2009). Stevenson and Joanis (2003) applied semi-supervised learning for verb class discovery. We chose the parallel corpora discovery method and hybrid distributional clustering method because our target trigger list is a relatively closed set. Parallel corpora are likely to yield higher quality due to the human linguistic knowledge implicit in sentence alignment, but it is limited in size and vocabulary. Monolingual corpora are not as limited, so can cover more out-of-vocabulary terms, and might equal or out-perform parall"
Y10-1027,J08-2006,0,0.0315091,") less than some threshold (we have tried frequency threshold of 1, 2, 3 and 4 separately). For example, “announce” is not an ACE-type event, but we can get its cluster as follows: {宣布, 通告，断言，宣告，声明，预报，传达，阐明，提出，显露，显示，陈述}  {announce, declare, herald, proclaim, set forth, set out, state, unveil, convey, affirm, assert} 3.4 Event Cluster Ranking Let E denote the unlabeled corpus from which we want to discover novel event types and annotate event arguments. We apply a high-performance entity extraction system (Grishman et al., 2005) and a state-of-the-art SRL system (including syntactic parsing) (Pradhan et al., 2008) to pre-process E. For each candidate trigger cluster C, we gather it together with the entities in E as a query, and then use information retrieval methods to obtain related sentences for this query. For any word/phrase v  C, if an entity e is identified by SRL as an argument of v in a sentence s, then s is the related sentence. If v is not tagged as a trigger for any existing event types, we consider s as a novel-event related sentence. Let nsk ( v , e ) be the kth novel-event related sentence, and si ( v , e ) be the ith related sentence for v and e; compute the salience of C as follows. s"
Y10-1027,W03-0410,0,0.0333845,"use of some domain-specific features such as argument heads (e.g. “rates” appear very rarely as an “ARG0” in news domain). We expect to get further improvement after we incorporate some domainspecific knowledge such as high-frequency terminology lexicons into the SRL system. 6 Related Work A number of previous studies have described extensive techniques to cluster words or word sequences from large unlabeled corpora (e.g. , Lin and Wu, 2009), monolingual parallel corpora (e.g. Lin and Pantel, 2001; Pang et al., 2003) and bilingual parallel corpora (e.g. Callison-Burch et al., 2008; Ji, 2009). Stevenson and Joanis (2003) applied semi-supervised learning for verb class discovery. We chose the parallel corpora discovery method and hybrid distributional clustering method because our target trigger list is a relatively closed set. Parallel corpora are likely to yield higher quality due to the human linguistic knowledge implicit in sentence alignment, but it is limited in size and vocabulary. Monolingual corpora are not as limited, so can cover more out-of-vocabulary terms, and might equal or out-perform parallel corpora methods if given a large enough monolingual corpus. Both methods do not require any supervisio"
Y10-1027,C00-2136,0,\N,Missing
Y10-1027,N07-1070,0,\N,Missing
Y10-1027,P06-2094,0,\N,Missing
