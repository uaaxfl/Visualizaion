2011.mtsummit-papers.21,C08-2005,0,0.207625,"Missing"
2011.mtsummit-papers.21,N10-1029,0,0.0117692,", 2010). Their model classiﬁes each word of an input sentence as beginning or ending. The predicted phrase boundary produced promising results in phrase-based translation. They did not, however, devise a probabilistic model which can be integrated into the translation model. There also have been some strategies which score each source phrase in a phrase table from various viewpoints. While the ﬁrst type is utilizing statistical collocation information (Ren et al., 2009; Liu et al., 2010), the second is based on the multiword expressions translation (Lambert and Banchs, 2005; Ren et al., 2009; Carpuat and Diab, 2010). These studies usually append additional features to a phrase table. It can be said that their methods indirectly give differential probabilities to possible segmentation results. However, they did not try to explicitly model the generation process of a phrase segmentation result. Most previous works did not describe the condition of good segmentation, i.e. segmentation to generate a high quality translation result. The good segmentation has some characteristics which are different from the bad segmentation in terms of translation. In this paper, we address the following questions. • What cha"
2011.mtsummit-papers.21,N03-1017,0,0.0212638,"have lexical cohesion and show more uniform translation for each phrase segment. Based on the observation, we propose a novel phrase segmentation model using collocation between two adjacent words and translation entropy of phrase segments. Experimental results show that the proposed model signiﬁcantly improves the translation quality in both English-to-Korean and English-to-Chinese translation tasks. 1 Introduction Phrase segmentation is to split a sentence into a sequence of multiple phrases. The phrase-based statistical machine translation (PBSMT) includes the phrase segmentation process (Koehn et al., 2003; Och and Ney, 2004; Zens and Ney, 2004). An input sentence is segmented into phrases at ﬁrst, (here, a phrase is not necessarily linguistically motivated) and then translated by way of phrase-to-phrase. Conventional PBSMT assumes that all of the possible phrase segmentation results are uniformly distributed.1 One sentence can have multiple ways of segmentation, because there is no assumption or constraint for a phrase. For example, a sentence including three words w1 w2 w3 possibly has four kinds 1 All imaginable phrase segmentation results do not actually have same probabilities because one"
2011.mtsummit-papers.21,P07-2045,0,0.0124805,"Missing"
2011.mtsummit-papers.21,2005.mtsummit-posters.11,0,0.0171555,"n boundary classiﬁer for PBSMT (Xiong et al., 2010). Their model classiﬁes each word of an input sentence as beginning or ending. The predicted phrase boundary produced promising results in phrase-based translation. They did not, however, devise a probabilistic model which can be integrated into the translation model. There also have been some strategies which score each source phrase in a phrase table from various viewpoints. While the ﬁrst type is utilizing statistical collocation information (Ren et al., 2009; Liu et al., 2010), the second is based on the multiword expressions translation (Lambert and Banchs, 2005; Ren et al., 2009; Carpuat and Diab, 2010). These studies usually append additional features to a phrase table. It can be said that their methods indirectly give differential probabilities to possible segmentation results. However, they did not try to explicitly model the generation process of a phrase segmentation result. Most previous works did not describe the condition of good segmentation, i.e. segmentation to generate a high quality translation result. The good segmentation has some characteristics which are different from the bad segmentation in terms of translation. In this paper, we"
2011.mtsummit-papers.21,P10-1085,0,0.0701835,"he translation quality. The second is the study for the discriminative translation boundary classiﬁer for PBSMT (Xiong et al., 2010). Their model classiﬁes each word of an input sentence as beginning or ending. The predicted phrase boundary produced promising results in phrase-based translation. They did not, however, devise a probabilistic model which can be integrated into the translation model. There also have been some strategies which score each source phrase in a phrase table from various viewpoints. While the ﬁrst type is utilizing statistical collocation information (Ren et al., 2009; Liu et al., 2010), the second is based on the multiword expressions translation (Lambert and Banchs, 2005; Ren et al., 2009; Carpuat and Diab, 2010). These studies usually append additional features to a phrase table. It can be said that their methods indirectly give differential probabilities to possible segmentation results. However, they did not try to explicitly model the generation process of a phrase segmentation result. Most previous works did not describe the condition of good segmentation, i.e. segmentation to generate a high quality translation result. The good segmentation has some characteristics w"
2011.mtsummit-papers.21,W97-0207,0,0.0831212,"n gives a constant K to the segment according to the equation 6, and K is optimized by experiments on the development set. ⎨ =  ⎩ K if |¯ ei |= 1 otherwise (6) where |¯ e |is the number of words contained in a segment and the function Col() means the collocation ∀ej ∈¯ ei Col(ej , ej+1 ) 1/(|¯ei |−1) Model using Translational Entropy 1 (TE) Based on the analysis of section 2, this model assumes that the segmentation in which a word sequence with low translational diversity is not split will generate a high quality translation. In order to reﬂect this idea, we use the translational entropy (Melamed, 1997) of each candidate segment. The following function gives a high score to the segment whose translations are not diverse. where Z is a normalization factor and Score(¯ e) denotes the scoring function of a candidate segment. Score(¯ ei ) ⎧ score of two words. The measurement of the score is presented in section 3.2. This model may produce similar effects to the approach using the collocation probability for improving the phrase table, which is proposed in (Liu et al., 2010). 3 We note that the direction of source and target was reversed by the assumption of the noisy channel. The segmentation mo"
2011.mtsummit-papers.21,J04-4002,0,0.170248,"on and show more uniform translation for each phrase segment. Based on the observation, we propose a novel phrase segmentation model using collocation between two adjacent words and translation entropy of phrase segments. Experimental results show that the proposed model signiﬁcantly improves the translation quality in both English-to-Korean and English-to-Chinese translation tasks. 1 Introduction Phrase segmentation is to split a sentence into a sequence of multiple phrases. The phrase-based statistical machine translation (PBSMT) includes the phrase segmentation process (Koehn et al., 2003; Och and Ney, 2004; Zens and Ney, 2004). An input sentence is segmented into phrases at ﬁrst, (here, a phrase is not necessarily linguistically motivated) and then translated by way of phrase-to-phrase. Conventional PBSMT assumes that all of the possible phrase segmentation results are uniformly distributed.1 One sentence can have multiple ways of segmentation, because there is no assumption or constraint for a phrase. For example, a sentence including three words w1 w2 w3 possibly has four kinds 1 All imaginable phrase segmentation results do not actually have same probabilities because one segment is constrai"
2011.mtsummit-papers.21,P02-1040,0,0.082243,"Experimental Setup We have experimented with our model in Englishto-Korean and English-to-Chinese translation tasks. Table 3 shows the statistics of English-Korean parallel corpus crawled from online newswires.5 We have also used 485K English-Chinese sentence pairs and 500 sentence pairs from LDC corpora (LDC2005T10, LDC2005T06, and part of LDC2004T08) as training and development set, respectively. The ofﬁcial evaluation set of NIST OpenMT 2008 Evaluation (MT08) has been used as a test set for English-to-Chinese translation task. Their statistics are reported in table 4. Both the BLEU score (Papineni et al., 2002) and the NIST score (NIST, 2001) are used as evaluation metrics of the translation quality. Performance on English-to-Korean task is measured with word-segmented translation sentences, while that on English-to-Chinese task is measured with charactersegmented translation sentences. We have used the open source SMT engine, Moses (Koehn et al., 2007) with default options as the baseline model which uses the uniform segmentation model. Our phrase segmentation models are trained by calculating scores for each source phrase of the phrase table in the model training step. We integrate our models to t"
2011.mtsummit-papers.21,W09-2907,0,0.0414022,"Missing"
2011.mtsummit-papers.21,N10-1016,0,0.14177,"ation result. It means that we should differentiate the probabilities of phrase segmentation candidates. There are two previous works closely related to the phrase segmentation model for PBSMT. The ﬁrst is the phrase segmentation model of Blackwood et al. (2008). They proposed a simple phrase bi-gram model which can be integrated into the translation model. They veriﬁed that estimating a phrase segmentation probability using a very large monolingual corpus is helpful for improving the translation quality. The second is the study for the discriminative translation boundary classiﬁer for PBSMT (Xiong et al., 2010). Their model classiﬁes each word of an input sentence as beginning or ending. The predicted phrase boundary produced promising results in phrase-based translation. They did not, however, devise a probabilistic model which can be integrated into the translation model. There also have been some strategies which score each source phrase in a phrase table from various viewpoints. While the ﬁrst type is utilizing statistical collocation information (Ren et al., 2009; Liu et al., 2010), the second is based on the multiword expressions translation (Lambert and Banchs, 2005; Ren et al., 2009; Carpuat"
2011.mtsummit-papers.21,N04-1033,0,0.0234509,"iform translation for each phrase segment. Based on the observation, we propose a novel phrase segmentation model using collocation between two adjacent words and translation entropy of phrase segments. Experimental results show that the proposed model signiﬁcantly improves the translation quality in both English-to-Korean and English-to-Chinese translation tasks. 1 Introduction Phrase segmentation is to split a sentence into a sequence of multiple phrases. The phrase-based statistical machine translation (PBSMT) includes the phrase segmentation process (Koehn et al., 2003; Och and Ney, 2004; Zens and Ney, 2004). An input sentence is segmented into phrases at ﬁrst, (here, a phrase is not necessarily linguistically motivated) and then translated by way of phrase-to-phrase. Conventional PBSMT assumes that all of the possible phrase segmentation results are uniformly distributed.1 One sentence can have multiple ways of segmentation, because there is no assumption or constraint for a phrase. For example, a sentence including three words w1 w2 w3 possibly has four kinds 1 All imaginable phrase segmentation results do not actually have same probabilities because one segment is constrained to use only a phr"
2011.mtsummit-papers.21,2004.tmi-1.9,0,0.0529667,"erent from those of the single models. However, one positive aspect is that CO+TE and CO+GT models give higher NIST scores than each single model of the combined model. It also can be said that the low performance of TE+GT is because of a high correlation between TE and GT model’s feature values. 4.3 Discussion In order to analyze the coverage and the practical effect of the proposed model, we ﬁrst evaluate only sentences whose source phrase segmentation is changed by the proposed model in the test set. The results are shown in table 7. Here, “#Sent” 6 We have used Zhang’s signiﬁcance tester (Zhang and Vogel, 2004). ∼ the number of abductees amounts to 82,959 . ∼ 납북자 의 수 가 82,959 명 에 이르 ㄴ다 input reference ( ∼ DPRK abductee baseline segmentation translation CO+TE+GT segmentation translation of number 82,959 amounts to ∼ the number of / abductees / amounts / to / 82,959 / . / ∼ 피랍 자 들 / 은 / 82,959 / 하 / 았 다 . / ( ∼ abductees / / 82,959 / do / was . /) ∼ the number of / abductees / amounts to / 82,959 / . / ∼ 피랍 자 들 / 82,959 / 에 이르 / ㄴ다 . / ( ∼ abductees / 82,959 / amount to / is . /) ∼ workers with shabby clothes and dirty faces . ∼ 허름하ㄴ 옷에 더럽ㄴ 얼굴 을 가지ㄴ input reference ( ∼ shabby baseline segmentation tra"
C00-1070,W99-0615,1,\N,Missing
C00-1070,W96-0213,0,\N,Missing
C00-2165,J95-2004,0,\N,Missing
C10-1054,W09-3109,0,0.0116525,"esearch Asia. and Smith, 2003; Yang and Li, 2003; Shi et al., 2006). The corpora thus obtained are generally of high quality and wide variety in domain, but the amount is still limited, as web pages that exhibit those structural cues are not abundant. Some other effort is to mine bilingual corpora by textual means only. That is, two pieces of text are decided to be parallel merely from the linguistic perspective, without considering any hint from HTML markup or website structure. These approaches (Zhao and Vogel, 2002; Utiyama and Isahara 2003; Fung and Cheung, 2004; Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009) share roughly the same framework: Phase 1: Document Pair Retrieval 1) documents in some target language (TL) are stored in some database; 2) each document in some source language (SL) is represented by some TL keywords; 3) the TL keywords in (2) are used to assign some TL documents to a particular SL document, using some information retrieval (IR) technique. For example, Munteanu and Marcu (2005) apply the Lemur IR toolkit, Utiyama and Isahara (2003) use the BM25 similarity measure, and Fung and Cheung (2004) use cosine similarity. Each TL document pairs up with the SL document to form a cand"
C10-1054,J93-2003,0,0.0386299,"ains some extra phrase or clause, or even conveys different meaning than the other. It is doubtful if the document pairs from Phase 1 are too noisy to be processed by the sentence pair classifier. An alternative way for sentence pair extraction is to further filter the document pairs and discard any pairs that do not look like parallel. It is hypothesized that the parallel relationship between two documents can be assimilated by the word alignment between them. The document pair filter produces the Viterbi alignment, with the associated probability, of each document pair based on IBM Model 1 (Brown et al., 1993). The word alignment model (i.e. the statistical dictionary used by IBM Model 1) is trained on the NIST SMT training dataset. The probability of the Viterbi alignment of a document pair is the sole basis on which we decide whether the pair is genuinely parallel. That is, an empirically determined threshold is used to distinguish parallel pairs from non-parallel ones. In our experiment, a very strict threshold is selected so as to boost up the precision at the expense of recall. There are a few important details that enable the document pair filter succeed in identifying parallel text: 1) Funct"
C10-1054,A00-1004,0,0.0204635,"has received much attention. Hansards, or parliamentary proceedings in more than one language, are obvious source of bilingual corpora, yet they are about a particular domain and therefore of limited use. Many researchers then explore the Web. Some approach attempts to locate bilingual text within a web page (Jiang et al., 2009); some others attempt to collect web pages in different languages and decide the parallel relationship between the web pages by means of structural cues, like existence of a common ancestor web page, similarity between URLs, and similarity between the HTML structures (Chen and Nie, 2000; Resnik 1 This work has been done while the first author was visiting Microsoft Research Asia. and Smith, 2003; Yang and Li, 2003; Shi et al., 2006). The corpora thus obtained are generally of high quality and wide variety in domain, but the amount is still limited, as web pages that exhibit those structural cues are not abundant. Some other effort is to mine bilingual corpora by textual means only. That is, two pieces of text are decided to be parallel merely from the linguistic perspective, without considering any hint from HTML markup or website structure. These approaches (Zhao and Vogel,"
C10-1054,J07-2003,0,0.333413,"e ranked by TF-IDF and the top-N words are selected. Each keyword is then translated into a few TL words by a statistically learned dictionary. In our experiments the dictionary is learned from NIST SMT training data. 2) Query of TF-IDF-ranked machine translated keywords (QTL-TFIDF). It is assumed that a machine translation (MT) system is better at handling lexical ambiguity than simple dictionary translation. Thus we propose to first translate the SL document into TL and extract the top-N TF-IDF-ranked words as query. In our experiments the MT system used is hierarchical phrase-based system (Chiang, 2007).2 3) Query of named entities (QNE). Another way to tackle the drawback of QSL-TFIDF is to focus on named entities (NEs) only, since NEs often provide strong clue for identifying correspondence between two languages. All NEs in a SL document are ranked by TF-IDF, and the top-N NEs are then translated (word by word) by dictionary. In our experiments we identify SL (Chinese) NEs 2 We also try online Google translation service, and the performance was roughly the same. 476 implicitly found by the word segmentation algorithm stated in Gao et al. (2003), and the dictionaries for translating NEs inc"
C10-1054,W04-3208,0,0.0253305,"e while the first author was visiting Microsoft Research Asia. and Smith, 2003; Yang and Li, 2003; Shi et al., 2006). The corpora thus obtained are generally of high quality and wide variety in domain, but the amount is still limited, as web pages that exhibit those structural cues are not abundant. Some other effort is to mine bilingual corpora by textual means only. That is, two pieces of text are decided to be parallel merely from the linguistic perspective, without considering any hint from HTML markup or website structure. These approaches (Zhao and Vogel, 2002; Utiyama and Isahara 2003; Fung and Cheung, 2004; Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009) share roughly the same framework: Phase 1: Document Pair Retrieval 1) documents in some target language (TL) are stored in some database; 2) each document in some source language (SL) is represented by some TL keywords; 3) the TL keywords in (2) are used to assign some TL documents to a particular SL document, using some information retrieval (IR) technique. For example, Munteanu and Marcu (2005) apply the Lemur IR toolkit, Utiyama and Isahara (2003) use the BM25 similarity measure, and Fung and Cheung (2004) use cosine similarity. Each"
C10-1054,P03-1035,0,0.0107406,"system used is hierarchical phrase-based system (Chiang, 2007).2 3) Query of named entities (QNE). Another way to tackle the drawback of QSL-TFIDF is to focus on named entities (NEs) only, since NEs often provide strong clue for identifying correspondence between two languages. All NEs in a SL document are ranked by TF-IDF, and the top-N NEs are then translated (word by word) by dictionary. In our experiments we identify SL (Chinese) NEs 2 We also try online Google translation service, and the performance was roughly the same. 476 implicitly found by the word segmentation algorithm stated in Gao et al. (2003), and the dictionaries for translating NEs include the same one used for QSL-TFIDF, and the LDC Chinese/English NE dictionary. For the NEs not covered by our dictionary, we use Google translation service as a back-up. A small-scale experiment is run to evaluate the merits of these queries. 300 Chinese news web pages in three different periods (each 100) are collected. For each Chinese text, each query (containing 10 keywords) is constructed and submitted to both Google and Yahoo Search, and top-40 returned English web pages for each search are kept. Note that the Chinese news articles are not"
C10-1054,P09-1098,1,0.83971,"y improves the performance of statistical machine translation. 1 Introduction Bilingual corpora are very valuable resources in NLP. They can be used in statistical machine translation (SMT), cross language information retrieval, and paraphrasing. Thus the acquisition of bilingual corpora has received much attention. Hansards, or parliamentary proceedings in more than one language, are obvious source of bilingual corpora, yet they are about a particular domain and therefore of limited use. Many researchers then explore the Web. Some approach attempts to locate bilingual text within a web page (Jiang et al., 2009); some others attempt to collect web pages in different languages and decide the parallel relationship between the web pages by means of structural cues, like existence of a common ancestor web page, similarity between URLs, and similarity between the HTML structures (Chen and Nie, 2000; Resnik 1 This work has been done while the first author was visiting Microsoft Research Asia. and Smith, 2003; Yang and Li, 2003; Shi et al., 2006). The corpora thus obtained are generally of high quality and wide variety in domain, but the amount is still limited, as web pages that exhibit those structural cu"
C10-1054,N03-1017,0,0.00862123,"ore and better training data for SMT, we evaluate the parallel and comparable corpora with respect to improvement in Bleu score (Papineni et al., 2002). 5.1 Experiment Setup Our experiment starts with the 11,000 Chinese documents as described in Section 2. We use various combinations of queries in document pair retrieval (Section 3). Based on the candidate document pairs, we produce both comparable corpora and parallel corpora using sentence pair extraction (Section 4). The corpora are then given to our SMT systems as training data. The SMT systems are our implementations of phrase-based SMT (Koehn et al., 2003) and hierarchical phrase-based SMT (Chiang, 2007). The two systems employ a 5-gram language model trained from the Xinhua section of the Gigaword corpus. There are many variations of the bilingual training dataset. The B1 section of the NIST SMT training set is selected as the baseline bilingual dataset; its size is of the same order of magnitude as most of the mined corpora so that the comparison is fair. Each of the mined bilingual corpora is compared to that baseline dataset, and we also evaluate the performance of the combination of each mined bilingual corpus with the baseline set. Phrase"
C10-1054,moore-2002-fast,0,0.0329541,"o work on data without common words. Therefore, all words on a comprehensive stopword list must be removed from a document pair before word alignment. 2) The alignment probability must be normalized with respect to sentence length, so that the threshold applies to all documents regardless of document length. Subjective evaluation on selected samples shows that most of the document pairs kept by the filter are genuinely parallel. Thus the document pairs can be broken down into sentence pairs simply by a sentence alignment method. For the sentence alignment, our experiments use the algorithm in Moore (2002). 5 Experiments It is a difficult task to evaluate the quality of automatically acquired bilingual corpora. As our ultimate purpose of mining bilingual corpora is to provide more and better training data for SMT, we evaluate the parallel and comparable corpora with respect to improvement in Bleu score (Papineni et al., 2002). 5.1 Experiment Setup Our experiment starts with the 11,000 Chinese documents as described in Section 2. We use various combinations of queries in document pair retrieval (Section 3). Based on the candidate document pairs, we produce both comparable corpora and parallel co"
C10-1054,J05-4003,0,0.366315,"r was visiting Microsoft Research Asia. and Smith, 2003; Yang and Li, 2003; Shi et al., 2006). The corpora thus obtained are generally of high quality and wide variety in domain, but the amount is still limited, as web pages that exhibit those structural cues are not abundant. Some other effort is to mine bilingual corpora by textual means only. That is, two pieces of text are decided to be parallel merely from the linguistic perspective, without considering any hint from HTML markup or website structure. These approaches (Zhao and Vogel, 2002; Utiyama and Isahara 2003; Fung and Cheung, 2004; Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009) share roughly the same framework: Phase 1: Document Pair Retrieval 1) documents in some target language (TL) are stored in some database; 2) each document in some source language (SL) is represented by some TL keywords; 3) the TL keywords in (2) are used to assign some TL documents to a particular SL document, using some information retrieval (IR) technique. For example, Munteanu and Marcu (2005) apply the Lemur IR toolkit, Utiyama and Isahara (2003) use the BM25 similarity measure, and Fung and Cheung (2004) use cosine similarity. Each TL document pairs up with"
C10-1054,P03-1021,0,0.0309187,".76(+1.68) 23.41(+1.75) 34.54(+1.69) 23.59(+2.41) B1+parallel(all query) 35.40(+2.32) 23.47(+1.81) 35.27(+2.42) 23.61(+2.43) Table 4: Evaluation of translation quality improvement by mined corpora. The figures inside brackets refer to the improvement over baseline. The bold figures indicate the highest Bleu score in each column for comparable corpora and parallel corpora, respectively. Bilingual Training Corpus The SMT systems learn translation knowledge (phrase table and rule table) in standard way. The parameters in the underlying log-linear model are trained by Minimum Error Rate Training (Och, 2003) on the development set of NIST 2003 test set. The quality of translation output is evaluated by case-insensitive BLEU4 on NIST 2005 and NIST 2008 test sets4. 5.2 Experimental result Table 3 lists the size of various mined parallel and comparable corpora against the baseline B1 bilingual dataset. It is obvious that for a specific type of query in document pair retrieval, the parallel corpus is significantly smaller than the corresponding comparable corpus. The apparent explanation is that a lot of document pairs are discarded due to the document Queries SP #SP #SL #TL extraction words words Ba"
C10-1054,P02-1040,0,0.0847863,"tive evaluation on selected samples shows that most of the document pairs kept by the filter are genuinely parallel. Thus the document pairs can be broken down into sentence pairs simply by a sentence alignment method. For the sentence alignment, our experiments use the algorithm in Moore (2002). 5 Experiments It is a difficult task to evaluate the quality of automatically acquired bilingual corpora. As our ultimate purpose of mining bilingual corpora is to provide more and better training data for SMT, we evaluate the parallel and comparable corpora with respect to improvement in Bleu score (Papineni et al., 2002). 5.1 Experiment Setup Our experiment starts with the 11,000 Chinese documents as described in Section 2. We use various combinations of queries in document pair retrieval (Section 3). Based on the candidate document pairs, we produce both comparable corpora and parallel corpora using sentence pair extraction (Section 4). The corpora are then given to our SMT systems as training data. The SMT systems are our implementations of phrase-based SMT (Koehn et al., 2003) and hierarchical phrase-based SMT (Chiang, 2007). The two systems employ a 5-gram language model trained from the Xinhua section of"
C10-1054,J03-3002,0,0.280125,"Missing"
C10-1054,P06-1062,1,0.890945,"about a particular domain and therefore of limited use. Many researchers then explore the Web. Some approach attempts to locate bilingual text within a web page (Jiang et al., 2009); some others attempt to collect web pages in different languages and decide the parallel relationship between the web pages by means of structural cues, like existence of a common ancestor web page, similarity between URLs, and similarity between the HTML structures (Chen and Nie, 2000; Resnik 1 This work has been done while the first author was visiting Microsoft Research Asia. and Smith, 2003; Yang and Li, 2003; Shi et al., 2006). The corpora thus obtained are generally of high quality and wide variety in domain, but the amount is still limited, as web pages that exhibit those structural cues are not abundant. Some other effort is to mine bilingual corpora by textual means only. That is, two pieces of text are decided to be parallel merely from the linguistic perspective, without considering any hint from HTML markup or website structure. These approaches (Zhao and Vogel, 2002; Utiyama and Isahara 2003; Fung and Cheung, 2004; Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009) share roughly the same framework: Pha"
C10-1054,P03-1010,0,0.239581,"k 1 This work has been done while the first author was visiting Microsoft Research Asia. and Smith, 2003; Yang and Li, 2003; Shi et al., 2006). The corpora thus obtained are generally of high quality and wide variety in domain, but the amount is still limited, as web pages that exhibit those structural cues are not abundant. Some other effort is to mine bilingual corpora by textual means only. That is, two pieces of text are decided to be parallel merely from the linguistic perspective, without considering any hint from HTML markup or website structure. These approaches (Zhao and Vogel, 2002; Utiyama and Isahara 2003; Fung and Cheung, 2004; Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009) share roughly the same framework: Phase 1: Document Pair Retrieval 1) documents in some target language (TL) are stored in some database; 2) each document in some source language (SL) is represented by some TL keywords; 3) the TL keywords in (2) are used to assign some TL documents to a particular SL document, using some information retrieval (IR) technique. For example, Munteanu and Marcu (2005) apply the Lemur IR toolkit, Utiyama and Isahara (2003) use the BM25 similarity measure, and Fung and Cheung (2004) use"
C10-1054,E03-1050,0,0.0481981,"Missing"
C10-2071,H05-1098,0,0.0399881,"Missing"
C10-2071,J03-1002,0,0.0207514,"Missing"
C10-2071,P08-1112,0,0.0155355,"end evaluation shows that the proposed method can improve not only the quality of statistical word alignment but the performance of statistical machine translation. 1 Introduction Word alignment is defined as mapping corresponding words in parallel text. A word aligned parallel corpora are very valuable resources in NLP. They can be used in various applications such as word sense disambiguation, automatic construction of bilingual lexicon, and statistical machine translation (SMT). In particular, the initial quality of statistical word alignment dominates the quality of SMT (Och and Ney 2000; Ganchev et al., 2008); almost all current SMT systems basically refer to the information inferred from word alignment result. One of the widely used approaches to statistical word alignment is based on the IBM models (Brown et al., 1993). IBM models are constructed based on words’ co-occurrence and positional information. If sufficient training data are given, IBM models can be successfully applied to any language pairs. However, for minority language pairs such as English-Korean and Swedish-Japanese, it is very difficult to obtain large amounts of parallel corpora. Without sufficient amount of parallel corpus, it"
C10-2071,P07-2045,0,0.0153695,"Missing"
C10-2071,J93-2003,0,\N,Missing
C10-2071,D07-1091,0,\N,Missing
C12-2060,C08-2005,0,0.0475901,"Missing"
C12-2060,D08-1024,0,0.0765585,"Missing"
C12-2060,C10-1054,1,0.908311,"Missing"
C12-2060,D11-1125,0,0.0288846,"Missing"
C12-2060,kang-kim-2004-sejong,0,0.0366214,"Missing"
C12-2060,C10-1064,0,0.0310258,"Missing"
C12-2060,P07-2045,0,0.00827121,"Missing"
C12-2060,2011.mtsummit-papers.21,1,0.850599,"Missing"
C12-2060,P03-1021,0,0.0821429,"Missing"
C12-2060,J04-4002,0,0.186298,"Missing"
C12-2060,P02-1040,0,0.0837115,"Missing"
C12-2060,N10-1016,0,0.0340385,"Missing"
C12-2060,C08-1136,0,0.0319156,"Missing"
C12-2060,I05-3027,0,\N,Missing
C12-3038,kang-kim-2004-sejong,0,0.014474,"in the previous section. These erroneous tuples cannot usually produce a fluent and comprehensible sentence when concatenating their entities and the relational phrase. Therefore, this problem can be solved by using a language model. We measure the perplexity of the word sequence generated by concatenating two entities and the relational phrase in order of the occurrence in its original sentence. The relation tuples that have a higher perplexity than a threshold are removed from the final set of relation tuples. To construct the Korean 5-gram language model, we use the refined Sejong corpus (Kang and Kim, 2004) consisted of 6,334,826 sentences. 3 Experiments 3.1 Experimental Environment We have experimented with our method on the Korean news corpus crawled in television program domain from August 13, 2011 to November 17, 2011. The corpus consists of 118K articles and 11.4M sentences. We have performed named entity recognition for pre-processing of this news corpus and have sampled 7,686 sentences containing one or more entities from the NE-recognized corpus. Among these annotated sentences, 4,893 sentences, 2,238 sentences, and 555 sentences are used as the training set for the entity-predicate pair"
C12-3038,P10-1013,0,0.0347291,"ocuments and internally accumulates a variety of valuable relational information. Therefore, extracting and utilizing the information from a large web corpus become a hot research issue. Banko et al. (2007) announced the first proposed system as a new paradigm called Open IE. The goal of Open IE system is to extract all possible correct relationships between entities without pre-defining the relationships. Open IE has shown successful result in some degree. Hereafter, a lot of research has been carried out on Open IE like TextRunner (Yates et al., 2007), REVERB (Etzioni et al., 2011) and WOE (Wu and Weld, 2010). However, most previous approaches have been proposed for English corpus. Although they are language independent approaches, when applied to another kind of language such as Korean, an unexpected problem occurs. English is a SVO(Subject-Verb-Object) word order language. In most cases, a relational phrase appears between subject(an entity) and object(another entity). Therefore, they naturally assume that the phrase is associated with the subject entity. However, in SOV language such as Korean, Japanese and Turkish, by default, the subject, object, and verb usually appear in that order. And the"
C12-3038,N07-4013,0,0.0280159,"oal. Recently, the World Wide Web provides vast amounts of documents and internally accumulates a variety of valuable relational information. Therefore, extracting and utilizing the information from a large web corpus become a hot research issue. Banko et al. (2007) announced the first proposed system as a new paradigm called Open IE. The goal of Open IE system is to extract all possible correct relationships between entities without pre-defining the relationships. Open IE has shown successful result in some degree. Hereafter, a lot of research has been carried out on Open IE like TextRunner (Yates et al., 2007), REVERB (Etzioni et al., 2011) and WOE (Wu and Weld, 2010). However, most previous approaches have been proposed for English corpus. Although they are language independent approaches, when applied to another kind of language such as Korean, an unexpected problem occurs. English is a SVO(Subject-Verb-Object) word order language. In most cases, a relational phrase appears between subject(an entity) and object(another entity). Therefore, they naturally assume that the phrase is associated with the subject entity. However, in SOV language such as Korean, Japanese and Turkish, by default, the subj"
C16-2027,C10-2005,0,0.147454,"Missing"
D08-1043,J93-2003,0,0.0265,"e query words not in a document are mapped to related words in the document. This implies that translation-based retrieval models would make positive contributions to retrieval performance only when the pre-constructed translation models have reliable translation probability distributions. 3 sim(Q, D) ≈ P (Q|MD ) Y P (Q|MD ) = The formulation of our retrieval model is basically equivalent to the approach of Jeon et al. (2005). 2.1 IBM Translation Model 1 Obviously, we need to build a translation model in advance. Usually the IBM Model 1, developed in the statistical machine translation field (Brown et al., 1993), is used to construct translation models for retrieval purposes in practice. Specifically, given a number of parallel strings, the IBM Model 1 learns the translation probability from a source word s to a target word t as: T (t|s) = λ−1 s N X c(t|s; Ji ) (5) i where λs is a normalization factor to make the sum of translation probabilities for the word s equal to 1, N is the number of parallel string pairs, and Ji is the ith parallel string pair. c(t|s; Ji ) is calculated as: µ c(t|s; Ji ) = P (t|s) P (t|s1 ) + · · · + P (t|sn ) ×f reqt,Ji × f reqs,Ji ¶ (6) where {s1 , . . . , sn } are words in"
D08-1043,W04-3252,0,0.0152581,"ht of word w in document D: tf -idfw,D = tfw,D × idfw tfw,D = f reqw,D , |D| idfw = log (7) |C| dfw where f reqw,D refers to the number of times w occurs in D, |D |refers to the size of D (in words), |C| refers to the size of the document collection, and dfw refers to the number of documents where w appears. Eventually, words with low tf-idf weights may be considered as unimportant. TextRank: The task of term weighting, in fact, has been often applied to the keyword extraction task in natural language processing studies. As 413 an alternative term weighting approach, we have used a variant of Mihalcea and Tarau (2004)’s TextRank, a graph-based ranking model for keyword extraction which achieves state-of-the-art accuracy without the need of deep linguistic knowledge or domain-specific corpora. Specifically, the ranking algorithm proceeds as follows. First, words in a given document are added as vertices in a graph G. Then, edges are added between words (vertices) if the words co-occur in a fixed-sized window. The number of co-occurrences becomes the weight of an edge. When the graph is constructed, the score of each vertex is initialized as 1, and the PageRank-based ranking algorithm is run on the graph ite"
D08-1043,J03-1002,0,0.00424537,"Missing"
D08-1043,P07-1059,0,0.147063,"’s work, which addresses the issue of word mismatch between queries and questions in large online Q&A collections by using translationbased methods. Apart from their work, there have been some related works on applying translationbased methods for retrieving FAQ data. Berger et al. (2000) report some of the earliest work on FAQ retrieval using statistical retrieval models, including translation-based approaches, with a small set of FAQ data. Soricut and Brill (2004) present an answer passage retrieval system that is trained from 1 million FAQs collected from the Web using translation methods. Riezler et al. (2007) demonstrate the advantages of translation-based approach to answer retrieval by utilizing a more complex translation model also trained from a large amount of data extracted from FAQs on the Web. Although all of these translation-based approaches are based on the statistical translation models, including the IBM Model 1, none of them focus on addressing the noise issues in translation models. 7 Conclusion and Future Work Bridging the query-question gap has been a major issue in retrieval models for large online Q&A collections. In this paper, we have shown that the performance of translation-"
D08-1043,N04-1008,0,0.0202143,"e translation models, a tolerant approach would yield better retrieval performance. 6 Related Works Our work is most closely related to Jeon et al. (2005)’s work, which addresses the issue of word mismatch between queries and questions in large online Q&A collections by using translationbased methods. Apart from their work, there have been some related works on applying translationbased methods for retrieving FAQ data. Berger et al. (2000) report some of the earliest work on FAQ retrieval using statistical retrieval models, including translation-based approaches, with a small set of FAQ data. Soricut and Brill (2004) present an answer passage retrieval system that is trained from 1 million FAQs collected from the Web using translation methods. Riezler et al. (2007) demonstrate the advantages of translation-based approach to answer retrieval by utilizing a more complex translation model also trained from a large amount of data extracted from FAQs on the Web. Although all of these translation-based approaches are based on the statistical translation models, including the IBM Model 1, none of them focus on addressing the noise issues in translation models. 7 Conclusion and Future Work Bridging the query-ques"
D14-1071,P14-1091,1,0.898601,"that our proposed method outperforms three KB-QA baseline methods on two publicly released QA data sets. 1 Introduction Knowledge-based question answering (KB-QA) involves answering questions posed in natural language (NL) using existing knowledge bases (KBs). As most KBs are structured databases, how to transform the input question into its corresponding structured query for KB (KB-query) as a logical form (LF), also known as semantic parsing, is the central task for KB-QA systems. Previous works (Mooney, 2007; Liang et al., 2011; Cai and Yates, 2013; Fader et al., 2013; Berant et al., 2013; Bao et al., 2014) usually leveraged mappings between NL phrases and logical predicates as lexical triggers to perform transformation tasks in semantic parsing, but they had to deal with two limitations: (i) as the meaning of a logical predicate often has different natural language expression (NLE) forms, the lexical triggers extracted for a predicate may at times are limited in size; (ii) entities detected by the named entity recognition (NER) component will be used to compose the 2 Related Work Supervised semantic parsers (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Mooney, 2007) heavily rely on th"
D14-1071,P11-1060,0,0.26728,"sentations and KBproperties in the latent space. Experimental results demonstrate that our proposed method outperforms three KB-QA baseline methods on two publicly released QA data sets. 1 Introduction Knowledge-based question answering (KB-QA) involves answering questions posed in natural language (NL) using existing knowledge bases (KBs). As most KBs are structured databases, how to transform the input question into its corresponding structured query for KB (KB-query) as a logical form (LF), also known as semantic parsing, is the central task for KB-QA systems. Previous works (Mooney, 2007; Liang et al., 2011; Cai and Yates, 2013; Fader et al., 2013; Berant et al., 2013; Bao et al., 2014) usually leveraged mappings between NL phrases and logical predicates as lexical triggers to perform transformation tasks in semantic parsing, but they had to deal with two limitations: (i) as the meaning of a logical predicate often has different natural language expression (NLE) forms, the lexical triggers extracted for a predicate may at times are limited in size; (ii) entities detected by the named entity recognition (NER) component will be used to compose the 2 Related Work Supervised semantic parsers (Zelle"
D14-1071,P14-1133,0,0.0783746,"(Weston et al., 2010). Their inputs are often statement sentences which include subject and object entities for a given predicate, whereas NLquestions lack either a subject or object entity that is the potential answer. Hence, we can only use the information of a subject or object entity, which leads to a different training instance generation procedure and a different training criterion. Recently, researchers have developed open domain systems based on large scale KBs such as F REEBASE1 (Cai and Yates, 2013; Fader et al., 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Bao et al., 2014; Berant and Liang, 2014; Yao and Van Durme, 2014). Their semantic parsers for Open QA are unified formal and scalable: they enable the NL-question to be mapped into the appropriate logical form. Our method obtains similar logical forms, but using only lowdimensional embeddings of n-grams, entity types, and predicates learned from texts and KB. 3 (s) that occurs in the input question; P denotes a set of logical predicates (p) in KB, each of which is the canonical form of different NLEs sharing an identical meaning (bag-of-words; c). Based on the components defined above, the paired relationships are described as foll"
D14-1071,D13-1160,0,0.426708,"results demonstrate that our proposed method outperforms three KB-QA baseline methods on two publicly released QA data sets. 1 Introduction Knowledge-based question answering (KB-QA) involves answering questions posed in natural language (NL) using existing knowledge bases (KBs). As most KBs are structured databases, how to transform the input question into its corresponding structured query for KB (KB-query) as a logical form (LF), also known as semantic parsing, is the central task for KB-QA systems. Previous works (Mooney, 2007; Liang et al., 2011; Cai and Yates, 2013; Fader et al., 2013; Berant et al., 2013; Bao et al., 2014) usually leveraged mappings between NL phrases and logical predicates as lexical triggers to perform transformation tasks in semantic parsing, but they had to deal with two limitations: (i) as the meaning of a logical predicate often has different natural language expression (NLE) forms, the lexical triggers extracted for a predicate may at times are limited in size; (ii) entities detected by the named entity recognition (NER) component will be used to compose the 2 Related Work Supervised semantic parsers (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Mooney, 2007)"
D14-1071,D12-1104,0,0.00652345,"y types to the given context, which may solve the entity disambiguation problem in KB-QA; C-P can leverage the semantic overlap between question contexts (n-gram features) and logical predicates, which is important for mapping NL-questions to their corresponding predicates. 3.2 This section describes how we extract the semantic associated pairs of NLE-entries and KB-triples to learn the relational embeddings (Section 4.1). &lt;Relation Mention, Predicate> Pair (MP) Each relation mention denotes a lexical phrase of an existing KB-predicate. Following information extraction methods, such as PATTY (Nakashole et al., 2012), we extracted the &lt;relation mention, logical predicate> pairs from English W IKIPEDIA3 , which is closely connected to our KB, as follows: Given a KB-triple &lt;entitysubj , logical predicate, entityobj >, we extracted NLEentries &lt;entitysubj , relation mention, entityobj > where relation mention is the shortest path between entitysubj and entityobj in the dependency tree of sentences. The assumption is that any relation mention (m) in the NLE-entry containing such entity pairs that occurred in the KB-triple is likely to express the predicate (p) of that triple. With obtaining high-quality MP pai"
D14-1071,P13-1042,0,0.520324,"perties in the latent space. Experimental results demonstrate that our proposed method outperforms three KB-QA baseline methods on two publicly released QA data sets. 1 Introduction Knowledge-based question answering (KB-QA) involves answering questions posed in natural language (NL) using existing knowledge bases (KBs). As most KBs are structured databases, how to transform the input question into its corresponding structured query for KB (KB-query) as a logical form (LF), also known as semantic parsing, is the central task for KB-QA systems. Previous works (Mooney, 2007; Liang et al., 2011; Cai and Yates, 2013; Fader et al., 2013; Berant et al., 2013; Bao et al., 2014) usually leveraged mappings between NL phrases and logical predicates as lexical triggers to perform transformation tasks in semantic parsing, but they had to deal with two limitations: (i) as the meaning of a logical predicate often has different natural language expression (NLE) forms, the lexical triggers extracted for a predicate may at times are limited in size; (ii) entities detected by the named entity recognition (NER) component will be used to compose the 2 Related Work Supervised semantic parsers (Zelle and Mooney, 1996; Zet"
D14-1071,D13-1136,0,0.100595,"igger extraction and model training. Due to the data annotation requirement, such methods are usually restricted to specific domains, and struggle with the coverage issue caused by the limited size of lexical triggers. Studies on weakly supervised semantic parsers have tried to reduce the amount of human supervision by using question-answer pairs (Liang et al., 2011) or distant supervision (Krishnamurthy and Mitchell, 2012) instead of full semantic annotations. Still, for KB-QA, the question of how to leverage KB-properties and analyze the question structures remains. Bordes et al. (2012) and Weston et al. (2013) designed embedding models that connect free texts with KBs using the relational learning method (Weston et al., 2010). Their inputs are often statement sentences which include subject and object entities for a given predicate, whereas NLquestions lack either a subject or object entity that is the potential answer. Hence, we can only use the information of a subject or object entity, which leads to a different training instance generation procedure and a different training criterion. Recently, researchers have developed open domain systems based on large scale KBs such as F REEBASE1 (Cai and Y"
D14-1071,J90-1003,0,0.164555,"Missing"
D14-1071,P13-1158,0,0.395373,"space. Experimental results demonstrate that our proposed method outperforms three KB-QA baseline methods on two publicly released QA data sets. 1 Introduction Knowledge-based question answering (KB-QA) involves answering questions posed in natural language (NL) using existing knowledge bases (KBs). As most KBs are structured databases, how to transform the input question into its corresponding structured query for KB (KB-query) as a logical form (LF), also known as semantic parsing, is the central task for KB-QA systems. Previous works (Mooney, 2007; Liang et al., 2011; Cai and Yates, 2013; Fader et al., 2013; Berant et al., 2013; Bao et al., 2014) usually leveraged mappings between NL phrases and logical predicates as lexical triggers to perform transformation tasks in semantic parsing, but they had to deal with two limitations: (i) as the meaning of a logical predicate often has different natural language expression (NLE) forms, the lexical triggers extracted for a predicate may at times are limited in size; (ii) entities detected by the named entity recognition (NER) component will be used to compose the 2 Related Work Supervised semantic parsers (Zelle and Mooney, 1996; Zettlemoyer and Collins"
D14-1071,P14-1090,0,0.205332,"Missing"
D14-1071,D12-1069,0,0.140714,"Missing"
D14-1071,D13-1161,0,0.0295005,"th KBs using the relational learning method (Weston et al., 2010). Their inputs are often statement sentences which include subject and object entities for a given predicate, whereas NLquestions lack either a subject or object entity that is the potential answer. Hence, we can only use the information of a subject or object entity, which leads to a different training instance generation procedure and a different training criterion. Recently, researchers have developed open domain systems based on large scale KBs such as F REEBASE1 (Cai and Yates, 2013; Fader et al., 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Bao et al., 2014; Berant and Liang, 2014; Yao and Van Durme, 2014). Their semantic parsers for Open QA are unified formal and scalable: they enable the NL-question to be mapped into the appropriate logical form. Our method obtains similar logical forms, but using only lowdimensional embeddings of n-grams, entity types, and predicates learned from texts and KB. 3 (s) that occurs in the input question; P denotes a set of logical predicates (p) in KB, each of which is the canonical form of different NLEs sharing an identical meaning (bag-of-words; c). Based on the components defined above, the"
I05-1057,A00-1031,0,0.0184698,"of distinguishing biomedical named entities from general terms and labeling the named entities with semantic classes that they belong to. They use support vector machines (SVM) for each phase. However, the SVM does not provide an easy way for labeling Markov sequence data like B following O and I following B in named entities. Furthermore, since this system is tested on the GENIA corpus rather than JNLPBA 2004 shared task, we cannot conﬁrm the eﬀectiveness of this approach on the ground of experiments for common resources. In this paper, we present a two-phase named entity recognition model: (1) boundary detection for NEs and (2) term classiﬁcation by semantic labeling. The advantage of dividing the recognition process into two phase is that we can 648 S. Kim et al. select separately a discriminative feature set for each subtask, and moreover can measure eﬀectiveness of models at each phase. We use two exponential models for this work, namely conditional random ﬁelds for boundary detection having Markov sequence, and the maximum entropy model for semantic labeling. In addition, results from the machine learning based model are reﬁned by a rule-based postprocessing, which is implement"
I05-1057,C00-1030,0,0.0928835,"Missing"
I05-1057,W04-1217,0,0.362279,"Missing"
I05-1057,W02-0301,0,0.207486,"m et al. – word normalization: This feature contributes to word normalization. We attempt to reduce a word to its stem or root form with a simple algorithm which has rules for words containing plural, hyphen, and alphanumeric letters. Speciﬁcally, the following patterns are considered. (1) “lymphocytes”, “cells” → “lymphocyte”, “cell” (2) “il-2”, “il-2a”, “il2a” → “il” (3) “5-lipoxygenase”, “v-Abl” → “lipoxygenase”, “abl” (4) “peri-kappa” or “t-cell” has two normalization forms of “peri”and“kappa” and “t” and “cell” respectively. (5) “Ca2+-independent” has two roots of “ca” and “independent”. (6) The root of digits is “D”. – informative suﬃx: This feature appears if a target word has a salient suﬃx for boundary detection. The list of salient suﬃxes is obtained by relative entropy [10]. – word construction form: This feature indicates how a target word is orthographically constructed. Word shapes refer to a mapping of each word on equivalence classes that encodes with dashes, numerals, capitalizations, lower letters, symbols, and so on. All spellings are represented with combinations of the attributes1 . For instance, the word construction form of “IL-2 ” would become “IDASH-ALPNUM”. –"
I05-1057,W04-1214,1,0.830406,"g plural, hyphen, and alphanumeric letters. Speciﬁcally, the following patterns are considered. (1) “lymphocytes”, “cells” → “lymphocyte”, “cell” (2) “il-2”, “il-2a”, “il2a” → “il” (3) “5-lipoxygenase”, “v-Abl” → “lipoxygenase”, “abl” (4) “peri-kappa” or “t-cell” has two normalization forms of “peri”and“kappa” and “t” and “cell” respectively. (5) “Ca2+-independent” has two roots of “ca” and “independent”. (6) The root of digits is “D”. – informative suﬃx: This feature appears if a target word has a salient suﬃx for boundary detection. The list of salient suﬃxes is obtained by relative entropy [10]. – word construction form: This feature indicates how a target word is orthographically constructed. Word shapes refer to a mapping of each word on equivalence classes that encodes with dashes, numerals, capitalizations, lower letters, symbols, and so on. All spellings are represented with combinations of the attributes1 . For instance, the word construction form of “IL-2 ” would become “IDASH-ALPNUM”. – word characteristics: This feature appears if a word represents a DNA sequence of “A”,“C”,“G”,“T” or Greek letter such as beta or alpha, ordinal index such as I, II or unit such as BU/ml, mic"
I05-1057,W04-1221,0,0.455508,"Missing"
I05-1057,W04-1219,0,0.119267,"etection (M EM arkov ) term detection (CRF) semantic classiﬁcation overall NER Recall Precision F-score 74.03 75.31 74.67 76.14 77.64 76.88 87.50 93.81 90.54 72.77 69.68 71.19 Table 8. Performance of NE recognition methods (one-phase vs. two-phase) method one-phase two-phase(baseline2) (only 5 classes) two-phase(baseline2) (5 classes+other class) Recall Precision F-score 64.23 63.13 63.68 66.24 64.54 65.38 68.51 67.58 68.04 Also, we compared our model with the one-phase model. The detailed results are presented in Table 8. Both of them have pros and cons. The best-reported system presented by [13] uses one-phase strategy. In our evaluation, the twophase method shows a better result than the one-phase method, although direct comparison is not possible since we tested with a maximum entropy based exponential models in all cases. The features for one-phase method are identical with the recognition features except that the local context of a word is extended as previous 4 words and next 4 words. In addition, we investigate whether the consideration of “other” class words is helpful in the recognition performance. Table 8 shows explicit annotations of other NE classes much improve the perfo"
I05-1080,J98-1004,0,0.202315,"Missing"
I05-1080,J98-1002,0,0.0790164,"Missing"
I05-1080,S01-1019,0,0.0302673,"Missing"
I05-1080,C96-1005,0,0.0538063,"Missing"
I05-1080,C92-2070,0,0.467738,"t to the target word, which prevent WSD systems from collecting correct WSD information. For example, an ambiguous word rail is a relative of a meaning bird of a target word crane at WordNet, but the word rail means railway for the most part, not the meaning related to bird. Therefore, most of the example sentences of rail are not helpful for WSD of crane. His method has another problem in disambiguating senses of a large number of target words because it requires a great amount of time and storage space to collect example sentences of relatives of the target words. [9] followed the method of [8], but tried to resolve the ambiguous relative problem by using just unambiguous relatives. That is, the ambiguous relative rail is not utilized to build a training data of the word crane because the word rail is ambiguous. Another diﬀerence from [8] is on a lexical database: they utilized WordNet as a lexical database for acquiring relatives of target words 1 Strictly speaking, our method utilizes bias of word senses at WordNet, which is acquired a sense tagged corpus. However, our method does not access a sense tagged corpus directly. Hence, our method is a kind of a weakly supervised approac"
I05-1080,J98-1006,0,0.313922,"example sentences irrelevant to the target word, which prevent WSD systems from collecting correct WSD information. For example, an ambiguous word rail is a relative of a meaning bird of a target word crane at WordNet, but the word rail means railway for the most part, not the meaning related to bird. Therefore, most of the example sentences of rail are not helpful for WSD of crane. His method has another problem in disambiguating senses of a large number of target words because it requires a great amount of time and storage space to collect example sentences of relatives of the target words. [9] followed the method of [8], but tried to resolve the ambiguous relative problem by using just unambiguous relatives. That is, the ambiguous relative rail is not utilized to build a training data of the word crane because the word rail is ambiguous. Another diﬀerence from [8] is on a lexical database: they utilized WordNet as a lexical database for acquiring relatives of target words 1 Strictly speaking, our method utilizes bias of word senses at WordNet, which is acquired a sense tagged corpus. However, our method does not access a sense tagged corpus directly. Hence, our method is a kind of"
I05-1080,W04-3204,0,0.0118291,"f two senses is, the weaker the relationship between them is. In other words, the unambiguous relatives in the long distance may provide irrelevant examples for WSD like ambiguous relatives. Hence, the method has diﬃculties in disambiguating senses of words that do not have unambiguous relatives near the target words in the WordNet. The problem becomes more serious when verbs, which most of the relatives are ambiguous, are disambiguated. Like [8], the method also has a diﬃculty in disambiguating senses of many words because the method collects the example sentences of relatives of many words. [10] reimplemented the method of [9] using a web, which may be a very large corpus, in order to collect example sentences. They built training datum of all noun words in WordNet whose size is larger than 7GB, but evaluated their method on a small number of nouns of lexical sample task of SENSEVAL-2 as [8] and [9]. 3 Word Sense Disambiguation by Relative Selection Our method disambiguates senses of a target word in a sentence by selecting only a relative among the relatives of the target word that most probably occurs in the sentence. A ﬂowchart of our method is presented in Figure 1 with an exampl"
I05-1080,H93-1061,0,0.0324892,"es of all ambiguous words eﬃciently by referring to only one CFM. The frequencies in Eq. 5 and 6 can be obtained through a CFM as follows: f req(wi ) = cf m(i, i) (8) f req(wi , wj ) = cf m(i, j) (9) where wi is a word, and cf m(i, j) represents the value in the i-th row and j-th column of the CFM, in other word, the frequency that the i-th word and j-th word co-occur in a raw corpus. 4 Experiments 4.1 Experimental Environment Experiments were carried out on several English sense tagged corpora: SemCor and corpora for both lexical sample task and all words task of both SENSEVAL2 & -3.5 SemCor [12]6 is a semantic concordance, where all content words (i.e. noun, verb, adjective, and adverb) are assigned to WordNet senses. SemCor consists of three parts: brown1, brown2 and brownv. We used all of the three parts of the SemCor for evaluation. In our method, raw corpora are utilized in order to build a CFM and to calculate similarities between words for the sake of the weights of relatives. We adopted Wall Street Journal corpus in Penn Treebank II [13] and LATIMES corpus in TREC as raw corpora, which contain about 37 million word occurrences. Our CFM contains frequencies of content words and"
I05-1080,S01-1018,0,0.0508982,"Missing"
I05-1080,W04-0856,0,0.0456293,"Missing"
I05-1080,W04-0829,0,0.0464999,"Missing"
I05-1080,W04-0820,0,0.0644412,"Missing"
I05-1080,J93-2004,0,\N,Missing
I05-1080,S01-1001,0,\N,Missing
I05-2034,C94-1087,0,\N,Missing
I05-2034,C94-1035,0,\N,Missing
I05-2041,P96-1024,0,0.0432268,"ose of evaluating the parsing performance given the correct segments, we classify the constituents in the syntactic structures into the constituents in the intra-structures of segments and the constituents in the inter-structures. Besides, we evaluate each classified constituents based on labeled precision, labeled recall, and distribution ratio. The labeled precision (LP) indicates the ratio of correct candidate constituents from candidate constituents generated by the parser, and the labeled recall (LR) indicates the ratio of correct candidate constituents from constituents in the treebank (Goodman, 1996). Also, the distribution ratio (Ratio) indicates the distribution ratio of constituents in the intra-structures from all of constituents in the original structure. Table 1 shows that the distribution ratio of the constituents in the intra-structures increases according to the longer segment length while the distribution ratio of the constituents in the interstructures decreases. Given the segment length 1, the constituents in the inter-structures of a sentence are the same as the constituents of the sen241 Table 1: Parsing Performance Intra-Structure Ratio LR LP Length 0.00 0.00 0.00 1 93.42 5"
I05-2041,P89-1015,0,0.140186,"yntactic information, and the structural analysis of each sentence is represented as a bracketed tree structure. This kind of corpus has served as an extremely valuable resource for computational linguistics applications such as machine translation and question answering (Lee et al., 1997; Choi, 2001), and has also proved useful in theoretical linguistics research (Marcus et al., 1993). However, for the purpose of building the treebank, an annotator spends a lot of time and manual effort. Furthermore, it is too difficult to maintain the consistency of the treebank based on only the annotator (Hindle, 1989; Chang et al., 1997). 238 Previous Works Up to data, several approaches have been developed in order to reduce manual effort for building a treebank. They can be classified into the approaches using the heuristics (Hindle, 1989; Chang et al., 1997) and the approaches using the rules extracted from an already built treebank (Kwak et al., 2001; Lim et al., 2004). The first approaches are used for Penn Treebank (Marcus et al., 1993) and the KAIST language resource (Lee et al., 1997; Choi, 2001). Given a sentence, the approaches try to assign an unambiguous partial syntactic structure to a segmen"
I05-2041,C90-3088,0,0.0606444,"k. Therefore, the extracted rules can be updated whenever the annotator wants (Kwak et al., 2001; Lim et al., 2004). Nevertheless, they place a limit on the manual effort reduction and the annotating efficiency improvement because the extracted rules are less credible than the heuristics. In this paper, we propose a tree annotation tool using a parser for the purpose of shifting the responsibility of extracting the reliable syntactic rules to the parser. It is always ready to change the parser into another parser. However, most parsers still tend to show low performance on the long sentences (Li et al., 1990; Doi et al., 1993; Kim et al., 2000). Besides, one of the reasons to decrease the parsing performance is that the initial syntactic errors of a word or a phrase propagates to the whole syntactic structure. In order to prevent the initial errors from propagating without any modification of the parser, the proposed tool requires the annotator to segment a sentence. And then, it performs two-phase parsing for the intra-structure of each segment and the inter-structure. The parsing methods using clause-based segmentation have been studied to improve the parsing performance and the parsing complex"
I05-2041,1997.iwpt-1.29,0,0.0230408,"et al., 2000). Besides, one of the reasons to decrease the parsing performance is that the initial syntactic errors of a word or a phrase propagates to the whole syntactic structure. In order to prevent the initial errors from propagating without any modification of the parser, the proposed tool requires the annotator to segment a sentence. And then, it performs two-phase parsing for the intra-structure of each segment and the inter-structure. The parsing methods using clause-based segmentation have been studied to improve the parsing performance and the parsing complexity (Kim et al., 2000; Lyon and Dickerson, 1997; Sang and Dejean, 2001). Nevertheless, the clause-based segmentation can permit a short sentence to be splitted into shorter segments unnecessarily although too short segments increase manual effort to build a treebank. For the sake of minimizing manual effort, the proposed tree annotation tool induces the annotator to segment a sentence according to few heuristics verified by experimentally analyzing the already built treebank. Therefore, the heuristics can prefer the specific length unit rather than the linguistic units such as phrases and clauses. Figure 1: tree annotation tool 3.1 Sentenc"
I05-2041,W01-0708,0,0.0223169,"ne of the reasons to decrease the parsing performance is that the initial syntactic errors of a word or a phrase propagates to the whole syntactic structure. In order to prevent the initial errors from propagating without any modification of the parser, the proposed tool requires the annotator to segment a sentence. And then, it performs two-phase parsing for the intra-structure of each segment and the inter-structure. The parsing methods using clause-based segmentation have been studied to improve the parsing performance and the parsing complexity (Kim et al., 2000; Lyon and Dickerson, 1997; Sang and Dejean, 2001). Nevertheless, the clause-based segmentation can permit a short sentence to be splitted into shorter segments unnecessarily although too short segments increase manual effort to build a treebank. For the sake of minimizing manual effort, the proposed tree annotation tool induces the annotator to segment a sentence according to few heuristics verified by experimentally analyzing the already built treebank. Therefore, the heuristics can prefer the specific length unit rather than the linguistic units such as phrases and clauses. Figure 1: tree annotation tool 3.1 Sentence Segmentation The sente"
P00-1034,O92-1001,0,\N,Missing
P00-1034,W96-0213,0,\N,Missing
P00-1034,C94-1027,0,\N,Missing
P03-1038,W99-0615,1,0.899042,"Missing"
P03-1038,P94-1025,0,0.146263,"Missing"
P03-1038,W96-0213,0,\N,Missing
P03-1038,A00-1031,0,\N,Missing
P03-1060,W02-1208,1,\N,Missing
P04-3010,P00-1048,1,\N,Missing
P09-1118,A97-1011,0,0.0113705,"pectively). Then, we have sampled at most r relevant documents and n non-relevant documents from each one and generated document pairs from them. In our experiments, m, r, and n is set to 100, 10, and 40, respectively. Phrase extraction and indexing: We evaluate our proposed method on two different types of phrases: syntactic head-modifier pairs (syntactic phrases) and simple bigram phrases (statistical phrases). To index the syntactic phrases, we use the method proposed in (Strzalkowski et al., 1994) with Connexor FDG parser3 , the syntactic parser based on the functional dependency grammar (Tapanainen and Jarvinen, 1997). All necessary information for feature values were indexed together for both syntactic and statistical phrases. To maintain indexes in a manageable size, phrases 3 Connexor FDG parser is a commercial parser; the demo is available at: http://www.connexor.com/demo 1052 Model Word (Baseline 1) One-parameter (Baseline 2) Multi-parameter (Proposed) Metric  Query MAP R-Prec P@10 MAP R-Prec P@10 MAP R-Prec P@10 Test set ← Training set 6 ← 7+8 7 ← 6+8 all partial all partial 0.2135 0.1433 0.1883 0.1876 0.2575 0.1894 0.2351 0.2319 0.3660 0.3333 0.4100 0.4324 0.2254 0.1633† 0.1988 0.2031 0.2738 0.2165"
P09-1118,A97-1046,0,0.0506273,"nally concludes the paper and discusses future work. 2 Previous Work To date, there have been numerous researches to utilize phrases in retrieval models. One of the most earliest work on phrase-based retrieval was done by (Fagan, 1987). In (Fagan, 1987), the effectiveness of proximity-based phrases (i.e. words occurring within a certain distance) in retrieval was investigated with varying criteria to extract phrases from text. Subsequently, various types of phrases, such as sequential n-grams (Mitra et al., 1997), head-modifier pairs extracted from syntactic structures (Lewis and Croft, 1990; Zhai, 1997; Dillon and Gray, 1983; Strzalkowski et al., 1994), proximity-based phrases (Turpin and Moffat, 1999), were examined with conventional retrieval models (e.g. vector space model). The benefit of using phrases for improving the retrieval performance over simple ‘bag-of-words’ models was far less than expected; the overall performance improvement was only marginal and sometimes even inconsistent, specifically when a reasonably good weighting scheme was used (Mitra et al., 1997). Many researchers argued that this was due to the use of improper retrieval models in the experiments. In many cases, t"
P09-2008,J94-2001,0,0.0106901,"l, confidence and threshold estimation, and output optimization. The following sections will explain the steps in detail. Confidence and Threshold Estimation T = f (C) 2.1 Baseline Word Segmentation Model We use the tri-gram Hidden Markov Model (HMM) of (Lee et al., 2007) as the baseline WS model; however, we adopt the Maximum Likelihood (ML) decoding strategy to independently find the best word spacing states. ML-decoding allows us to directly compare each output to the threshold. There is little discrepancy in accuracy when using ML-decoding, as compared to Viterbidecoding, as mentioned in (Merialdo, 1994).1 Let o1,n be a sequence of n-character user input without WBMs, xt be the best word spacing state for ot where 1 ≤ t ≤ n. Assume that xt is either 1 (space after ot ) or 0 (no space after ot ). Then each best word spacing state x ˆt for all t can be found by using Equation 1. xˆt = argmax P (xt = i|o1,n ) i∈(0,1) = argmax P (o1,n , xt = i) = argmax i∈(0,1) i∈(0,1) × X X Then, we define the confidence as is done in Equation 5. Because calculating such a variable is impossible, we estimate the value by substituting the word spacing states produced by the S baseline WS model, xW 1,n , with the"
P09-2008,C04-1067,0,0.0753508,"Missing"
P09-2059,P05-1066,0,0.126045,"Missing"
P09-2059,ma-2006-champollion,0,0.0272607,"riments 4.1 Experimental Setup The baseline of our approach is a statistical phrase-based system which is trained using MOSES (Koehn et al., 2007). We collect bilingual texts from the Web and combine them with the Sejong parallel corpora 2 . About 300K pair of sentences are collected from the major bilingual news broadcasting sites. We also collect around 1M monolingual sentences from the sites to train Korean language models. The best performing language model is 5-gram order with Kneser-Ney smoothing. For sentence level alignment, we modified the Champollion toolkit for English-Korean pair (Ma, 2006). We randomly selected 5,000 sentence pairs from Sejong corpora, of which 1,500 were used for a tuning set for minimum error rate training, and another 1,500 for development set for analysis experiment. We report testing results on the remaining 2,000 sentence pairs for the evaluation. Korean sentences are tokenized by the morphological analyzer (Lee and Rim, 2004). For English sentence preprocessing, we use the Stanford parser with output of typed dependency relations. We then applied the pseudo word insertion and four reordering rules described in the previous section to the parse tree of ea"
P09-2059,de-marneffe-etal-2006-generating,0,0.0412626,"Missing"
P09-2059,P07-2045,0,0.00810879,"irectly follow verbal head. we observe that inserting too many pseudo words can, on the contrary, increase null alignment of English sentence. Thus we filtered some pseudo words according to their respective null alignment probabilities. Figure 3 shows the top 9 selected dependency relations (actually used in the experiment) and the aligned Korean function words. 3 (can ’t) ((play) the guitar) (1) (can ’t) (the guitar (play)) (2) (the guitar (play)) (can ’t) (3) 4 Experiments 4.1 Experimental Setup The baseline of our approach is a statistical phrase-based system which is trained using MOSES (Koehn et al., 2007). We collect bilingual texts from the Web and combine them with the Sejong parallel corpora 2 . About 300K pair of sentences are collected from the major bilingual news broadcasting sites. We also collect around 1M monolingual sentences from the sites to train Korean language models. The best performing language model is 5-gram order with Kneser-Ney smoothing. For sentence level alignment, we modified the Champollion toolkit for English-Korean pair (Ma, 2006). We randomly selected 5,000 sentence pairs from Sejong corpora, of which 1,500 were used for a tuning set for minimum error rate trainin"
P09-2059,P04-3010,1,0.82656,"1M monolingual sentences from the sites to train Korean language models. The best performing language model is 5-gram order with Kneser-Ney smoothing. For sentence level alignment, we modified the Champollion toolkit for English-Korean pair (Ma, 2006). We randomly selected 5,000 sentence pairs from Sejong corpora, of which 1,500 were used for a tuning set for minimum error rate training, and another 1,500 for development set for analysis experiment. We report testing results on the remaining 2,000 sentence pairs for the evaluation. Korean sentences are tokenized by the morphological analyzer (Lee and Rim, 2004). For English sentence preprocessing, we use the Stanford parser with output of typed dependency relations. We then applied the pseudo word insertion and four reordering rules described in the previous section to the parse tree of each sentence. Syntactic Reordering Many approaches use syntactic reordering in the preprocessing step for SMT systems (Collins et al., 2005; Xia and McCord, 2004; Zwarts and Dras, 2007). Some reordering approaches have given significant improvements in performance for translation from French to English (Xia and McCord, 2004) and from German to English (Collins et al"
P09-2059,J03-1002,0,0.00646489,"Missing"
P09-2059,C04-1073,0,0.0787004,"and another 1,500 for development set for analysis experiment. We report testing results on the remaining 2,000 sentence pairs for the evaluation. Korean sentences are tokenized by the morphological analyzer (Lee and Rim, 2004). For English sentence preprocessing, we use the Stanford parser with output of typed dependency relations. We then applied the pseudo word insertion and four reordering rules described in the previous section to the parse tree of each sentence. Syntactic Reordering Many approaches use syntactic reordering in the preprocessing step for SMT systems (Collins et al., 2005; Xia and McCord, 2004; Zwarts and Dras, 2007). Some reordering approaches have given significant improvements in performance for translation from French to English (Xia and McCord, 2004) and from German to English (Collins et al., 2005). However, on the contrary, Lee et al. (2006) reported that the reordering of Korean for Korean-English translation degraded the performance. They presumed that the performance decrease might come from low parsing performance for conversational domain. We believe that it is very important to consider the structural properties of Korean for reordering English sentences. Though the wo"
P09-2059,2007.mtsummit-papers.74,0,0.0145971,"development set for analysis experiment. We report testing results on the remaining 2,000 sentence pairs for the evaluation. Korean sentences are tokenized by the morphological analyzer (Lee and Rim, 2004). For English sentence preprocessing, we use the Stanford parser with output of typed dependency relations. We then applied the pseudo word insertion and four reordering rules described in the previous section to the parse tree of each sentence. Syntactic Reordering Many approaches use syntactic reordering in the preprocessing step for SMT systems (Collins et al., 2005; Xia and McCord, 2004; Zwarts and Dras, 2007). Some reordering approaches have given significant improvements in performance for translation from French to English (Xia and McCord, 2004) and from German to English (Collins et al., 2005). However, on the contrary, Lee et al. (2006) reported that the reordering of Korean for Korean-English translation degraded the performance. They presumed that the performance decrease might come from low parsing performance for conversational domain. We believe that it is very important to consider the structural properties of Korean for reordering English sentences. Though the word order of a Korean sen"
P09-2081,C04-1088,0,\N,Missing
P09-2081,W02-2018,0,\N,Missing
P12-2057,P10-1088,0,0.0457426,"Missing"
P12-2057,J93-2003,0,0.0390093,"Missing"
P12-2057,P05-1033,0,0.0608879,"phrase pairs and hierarchical rules, and thus preserve the search space of SMT decoders as much as possible. Experimental results on Chinese-toEnglish machine translation tasks show that our method is able to reduce almost the half size of the translation model with very tiny degradation of translation performance. 1 Introduction Statistical Machine Translation (SMT) has gained considerable attention during last decades. From a bilingual corpus, all translation knowledge can be acquired automatically in SMT framework. Phrasebased model (Koehn et al., 2003) and hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) show state-of-the-art performance in various language pairs. This achievement is mainly benefit from huge size of translational knowledge extracted from sufficient parallel corpus. However, the errors of automatic word alignment and non-parallelized bilingual sentence pairs sometimes have caused the unreliable and unnecessary translation rule acquisition. According to Bloodgood and Callison-Burch Previous methods of reducing the size of SMT model try to identify infrequent entries (Zollmann et al., 2008; Huang and Xiang, 2010). Several statistical significance testing methods a"
P12-2057,J07-2003,0,0.0340551,"nd hierarchical rules, and thus preserve the search space of SMT decoders as much as possible. Experimental results on Chinese-toEnglish machine translation tasks show that our method is able to reduce almost the half size of the translation model with very tiny degradation of translation performance. 1 Introduction Statistical Machine Translation (SMT) has gained considerable attention during last decades. From a bilingual corpus, all translation knowledge can be acquired automatically in SMT framework. Phrasebased model (Koehn et al., 2003) and hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) show state-of-the-art performance in various language pairs. This achievement is mainly benefit from huge size of translational knowledge extracted from sufficient parallel corpus. However, the errors of automatic word alignment and non-parallelized bilingual sentence pairs sometimes have caused the unreliable and unnecessary translation rule acquisition. According to Bloodgood and Callison-Burch Previous methods of reducing the size of SMT model try to identify infrequent entries (Zollmann et al., 2008; Huang and Xiang, 2010). Several statistical significance testing methods are also examine"
P12-2057,C10-2025,1,0.846579,"duction is needed to be performed for both the phrase table and the hierarchical rule table simultaneously, namely joint reduction. Similar to phrase reduction and hierarchical rule reduction, it selects the best N entries of the mixture of phrase and hierarchical rules. This method results in safer pruning; once a phrase is determined to be pruned, the hierarchical rules, which are related to this phrase, are likely to be kept, and vice versa. 3 Experiment We investigate the effectiveness of our reduction method by conducting Chinese-to-English translation task. The training data, as same as Cui et al. (2010), consists of about 500K parallel sentence pairs which is a mixture of several datasets published by LDC. NIST 2003 set is used as a development set. NIST 2004, 2005, 2006, and 2008 sets are used for evaluation purpose. For word alignment, we use GIZA++1 , an implementation of IBM models (Brown et al., 1993). We have implemented a hierarchical phrase-based SMT model similar to Chiang (2005). The trigram target language model is trained from the Xinhua portion of English Gigaword corpus (Graff and Cieri, 2003). Sampled 10,000 sentences from Chinese Gigaword corpus (Graff, 2007) was used for sou"
P12-2057,C10-1056,0,0.258016,"del (Koehn et al., 2003) and hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) show state-of-the-art performance in various language pairs. This achievement is mainly benefit from huge size of translational knowledge extracted from sufficient parallel corpus. However, the errors of automatic word alignment and non-parallelized bilingual sentence pairs sometimes have caused the unreliable and unnecessary translation rule acquisition. According to Bloodgood and Callison-Burch Previous methods of reducing the size of SMT model try to identify infrequent entries (Zollmann et al., 2008; Huang and Xiang, 2010). Several statistical significance testing methods are also examined to detect unreliable noisy entries (Tomeh et al., 2009; Johnson et al., 2007; Yang and Zheng, 2009). These methods could harm the translation performance due to their side effect of algorithms; similar multiple entries can be pruned at the same time deteriorating potential coverage of translation. The proposed method, on the other hand, tries to measure the redundancy of phrase pairs and hierarchical rules. In this work, redundancy of an entry is defined as its translational ineffectiveness, and estimated by comparing scores"
P12-2057,D07-1103,0,0.114556,"rs. This achievement is mainly benefit from huge size of translational knowledge extracted from sufficient parallel corpus. However, the errors of automatic word alignment and non-parallelized bilingual sentence pairs sometimes have caused the unreliable and unnecessary translation rule acquisition. According to Bloodgood and Callison-Burch Previous methods of reducing the size of SMT model try to identify infrequent entries (Zollmann et al., 2008; Huang and Xiang, 2010). Several statistical significance testing methods are also examined to detect unreliable noisy entries (Tomeh et al., 2009; Johnson et al., 2007; Yang and Zheng, 2009). These methods could harm the translation performance due to their side effect of algorithms; similar multiple entries can be pruned at the same time deteriorating potential coverage of translation. The proposed method, on the other hand, tries to measure the redundancy of phrase pairs and hierarchical rules. In this work, redundancy of an entry is defined as its translational ineffectiveness, and estimated by comparing scores of entries and scores of their substituents. Suppose that the source phrase s1 s2 is always translated into t1 t2 with phrase entry &lt;s1 s2 →t1 t2"
P12-2057,N03-1017,0,0.0239505,"the estimation of the information redundancy encoded in phrase pairs and hierarchical rules, and thus preserve the search space of SMT decoders as much as possible. Experimental results on Chinese-toEnglish machine translation tasks show that our method is able to reduce almost the half size of the translation model with very tiny degradation of translation performance. 1 Introduction Statistical Machine Translation (SMT) has gained considerable attention during last decades. From a bilingual corpus, all translation knowledge can be acquired automatically in SMT framework. Phrasebased model (Koehn et al., 2003) and hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) show state-of-the-art performance in various language pairs. This achievement is mainly benefit from huge size of translational knowledge extracted from sufficient parallel corpus. However, the errors of automatic word alignment and non-parallelized bilingual sentence pairs sometimes have caused the unreliable and unnecessary translation rule acquisition. According to Bloodgood and Callison-Burch Previous methods of reducing the size of SMT model try to identify infrequent entries (Zollmann et al., 2008; Huang and Xiang, 2010)."
P12-2057,N03-2016,0,0.031157,"ranslation model, T M , our goal is to find the optimally reduced translation model, T M ∗ , which minimizes the degradation of translation performance. To measure the performance degradation, we introduce a new metric named consistency: C(T M, T M ∗ ) = BLEU (D(s; T M ), D(s; T M ∗ )) (1) where the function D produces the target sentence of the source sentence s, given the translation model T M . Consistency measures the similarity between the two groups of decoded target sentences produced by two different translation models. There are number of similarity metrics such as Dices coefficient (Kondrak et al., 2003), and Jaccard similarity coefficient. Instead, we use BLEU scores (Papineni et al., 2002) since it is one of the primary metrics for machine translation evaluation. Note that our consistency does not require the reference set while the original BLEU does. This means that only (abundant) source-side monolingual corpus is needed to predict performance degradation. Now, our goal can be rewritten with this metric; among all the possible reduced models, we want to find the set which can maximize the consistency: T M ∗ = argmax C(T M, T M ′ ) (2) T M ′ ⊂T M 292 where h is a feature function and λ is"
P12-2057,P03-1021,0,0.0219527,". The redundancy-based reduction can be performed to prune the phrase table, the hierarchical rule table, and both. Since the similar translation knowledge is accumulated at both of tables during the training stage, our reduction method performs effectively and safely. Unlike previous studies solely focus on either phrase table or hierarchical rule table, this work is the first attempt to reduce phrases and hierarchical rules simultaneously. In minimum error rate training (MERT) stages, a development set, which consists of bilingual sentences, is used to find out the best weights of features (Och, 2003). One characteristic of our method is that it isolates feature weights of the translation model from SMT log-linear model, trying to minimize the impact of search path during decoding. The reduction procedure consists of three stages: translation scoring, redundancy estimation, and redundancy-based reduction. Our reduction method starts with measuring the translation scores of the individual phrase and the hierarchical rule. Similar to the decoder, the scoring scheme is based on the log-linear framework: X P S(p) = λi hi (p) (3) i 2 Proposed Model Given an original translation model, T M , our"
P12-2057,P02-1040,0,0.0852558,"∗ , which minimizes the degradation of translation performance. To measure the performance degradation, we introduce a new metric named consistency: C(T M, T M ∗ ) = BLEU (D(s; T M ), D(s; T M ∗ )) (1) where the function D produces the target sentence of the source sentence s, given the translation model T M . Consistency measures the similarity between the two groups of decoded target sentences produced by two different translation models. There are number of similarity metrics such as Dices coefficient (Kondrak et al., 2003), and Jaccard similarity coefficient. Instead, we use BLEU scores (Papineni et al., 2002) since it is one of the primary metrics for machine translation evaluation. Note that our consistency does not require the reference set while the original BLEU does. This means that only (abundant) source-side monolingual corpus is needed to predict performance degradation. Now, our goal can be rewritten with this metric; among all the possible reduced models, we want to find the set which can maximize the consistency: T M ∗ = argmax C(T M, T M ′ ) (2) T M ′ ⊂T M 292 where h is a feature function and λ is its weight. As the conventional hierarchical phrase-based SMT model, our features are co"
P12-2057,2009.mtsummit-papers.17,0,0.505698,"various language pairs. This achievement is mainly benefit from huge size of translational knowledge extracted from sufficient parallel corpus. However, the errors of automatic word alignment and non-parallelized bilingual sentence pairs sometimes have caused the unreliable and unnecessary translation rule acquisition. According to Bloodgood and Callison-Burch Previous methods of reducing the size of SMT model try to identify infrequent entries (Zollmann et al., 2008; Huang and Xiang, 2010). Several statistical significance testing methods are also examined to detect unreliable noisy entries (Tomeh et al., 2009; Johnson et al., 2007; Yang and Zheng, 2009). These methods could harm the translation performance due to their side effect of algorithms; similar multiple entries can be pruned at the same time deteriorating potential coverage of translation. The proposed method, on the other hand, tries to measure the redundancy of phrase pairs and hierarchical rules. In this work, redundancy of an entry is defined as its translational ineffectiveness, and estimated by comparing scores of entries and scores of their substituents. Suppose that the source phrase s1 s2 is always translated into t1 t2 with phra"
P12-2057,P09-2060,0,0.449057,"s mainly benefit from huge size of translational knowledge extracted from sufficient parallel corpus. However, the errors of automatic word alignment and non-parallelized bilingual sentence pairs sometimes have caused the unreliable and unnecessary translation rule acquisition. According to Bloodgood and Callison-Burch Previous methods of reducing the size of SMT model try to identify infrequent entries (Zollmann et al., 2008; Huang and Xiang, 2010). Several statistical significance testing methods are also examined to detect unreliable noisy entries (Tomeh et al., 2009; Johnson et al., 2007; Yang and Zheng, 2009). These methods could harm the translation performance due to their side effect of algorithms; similar multiple entries can be pruned at the same time deteriorating potential coverage of translation. The proposed method, on the other hand, tries to measure the redundancy of phrase pairs and hierarchical rules. In this work, redundancy of an entry is defined as its translational ineffectiveness, and estimated by comparing scores of entries and scores of their substituents. Suppose that the source phrase s1 s2 is always translated into t1 t2 with phrase entry &lt;s1 s2 →t1 t2 &gt; where si and ti are"
P12-2057,C08-1144,0,0.100555,"amework. Phrasebased model (Koehn et al., 2003) and hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) show state-of-the-art performance in various language pairs. This achievement is mainly benefit from huge size of translational knowledge extracted from sufficient parallel corpus. However, the errors of automatic word alignment and non-parallelized bilingual sentence pairs sometimes have caused the unreliable and unnecessary translation rule acquisition. According to Bloodgood and Callison-Burch Previous methods of reducing the size of SMT model try to identify infrequent entries (Zollmann et al., 2008; Huang and Xiang, 2010). Several statistical significance testing methods are also examined to detect unreliable noisy entries (Tomeh et al., 2009; Johnson et al., 2007; Yang and Zheng, 2009). These methods could harm the translation performance due to their side effect of algorithms; similar multiple entries can be pruned at the same time deteriorating potential coverage of translation. The proposed method, on the other hand, tries to measure the redundancy of phrase pairs and hierarchical rules. In this work, redundancy of an entry is defined as its translational ineffectiveness, and estima"
P90-1007,P81-1022,0,0.0863485,"Missing"
P90-1007,J89-1002,1,0.885934,"Missing"
S01-1036,O97-1004,1,\N,Missing
W02-1208,J94-2001,0,0.0526615,"r example, spacing or concatenating individual nouns including a compound noun are both considered as right. As mentioned, word spacing is important for some reasons, but it is diÆcult for even man to space words correctly by spelling rules because of the characteristics of Korean and the inconsistent rules. Especially, it is much more confused in the case of having no in uence on understanding the meaning of a sentence. In this paper, we propose a word spacing model 1 using an HMM. HMM is a widely used statistical model to solve various NLP problems such as POS tagging(Charniak et al., 1993; Merialdo, 1994; Kim et al., 1998a; Lee, 1999). We regard the word spacing problem as a classi cation problem such as the POS tagging problem. When using an HMM for automatic word spacing task, raw texts can be used as training 1 Strictly speaking, our model described here is an Eojeol spacing model rather than a word spacing model because spacing unit of Korean is Eojeol. But we in this paper do not distinguish between Eojeol and word for convenience. Therefore, we use the term word&quot; as word, spacing unit in English.         pro n p oun ost p pos : st per er no ition ory un son nam : e"
W03-1305,C00-1030,0,0.099015,"Missing"
W03-1305,W02-0301,0,\N,Missing
W03-3013,J93-1002,0,0.124244,"Missing"
W03-3013,C00-2098,0,0.0262248,"Missing"
W04-0507,J92-1004,0,0.0118116,"e. Unstructured natural language sentences are indexed in the form of ternary expressions and stored in RDB. The START system covers much wider domain of questions than ours, however, it seems that the system returns more wrong answers than ours, because we extract the answer only from semi-structured documents. The Jupiter system (Zue et al., 2000) is a conversational system that provides weather information over the phone. Based on the Galaxy architecture (Goddeau et al., 1994), Jupiter recognizes user question over the phone, parses the question with the TINA language understanding system (Seneff, 1992) and generates SQL and natural language answer with the GENESIS system (Baptist and Seneff, 2000). The generated answer is synthesized with the ENVOICE system. Even the Jupiter system deals with the same domain, ours can process a bit wider-range of weather topics. Our QA system can cover the question which requires inferences such as When is the best day for washing my car in this week? Moreover, our system has an ability of inferring missing information from the user profile and the inferring algorithm. 6 Conclusion This paper describes the practical QA system for restricted domains. To be p"
W04-0854,J98-1006,0,\N,Missing
W04-0854,C92-2070,0,\N,Missing
W04-1214,A00-1031,0,\N,Missing
W04-1214,W03-1305,1,\N,Missing
W04-2419,J96-1002,0,0.0480833,"Missing"
W04-2419,W04-2412,0,0.0726359,"Missing"
W04-2419,J02-3001,0,0.39347,"Missing"
W04-2420,W04-2412,0,0.0799316,"Missing"
W04-2420,W03-3023,0,0.0363623,"Missing"
W05-0632,W05-0620,0,0.132475,"Missing"
W05-0632,J02-3001,0,\N,Missing
W05-0632,W04-2419,1,\N,Missing
W05-0632,N04-1030,0,\N,Missing
W08-2133,N04-1030,0,0.0427572,"redicate is auxiliary verb be and have. 2.3 Local Semantic Role Labeling Prediate identification is followed by argument labeling. For the given predicate, the system first eliminates inappropriate argument candidates. The argument identification uses different strategies for verbs, nouns, and other predicates. The argument classifier extracts features and labels semantic roles. None is used to indicate that a word is not a semantic argument. The classifier also uses different maximum entropy models for verbs, nouns, and other predicates 2.3.1 Argument Candidate Identification As mentioned by Pradhan et al. (2004), argument identification poses a significant bottleneck to improving performance of Semantic Role Labeling system. We tried an algorithm motivated from Hacioglu (2004) which defined a treestructured family membership of a predicate to identify more probable argument candidates and prune the others. However, we find that it works for verb and other predicate type, but does not work properly for noun predicate type. The main reason is due to the characteristics of arguments of noun predicates. First of all, a noun predicate can be an argument for itself, whereas a verb predicate cannot be. Seco"
W08-2133,W05-0620,0,0.0555143,"Missing"
W08-2133,W04-2412,0,0.0443716,"Missing"
W08-2133,P05-1073,0,0.0335121,"Missing"
W08-2133,W08-2121,0,0.032157,"Missing"
W08-2133,C04-1186,0,\N,Missing
W09-1415,C04-1186,0,0.0195182,"each trigger whether it has relations with participant candidates, and composites events with the extracted relations. In the last phase, multiple relations of the same trigger can be combined into an event for Binding event type. In addition, multiple relations can be combined and their participant types can be classified into not only theme but also cause for three Regulation event types. In this paper, we mainly use dependency parsing information of the analyzed data because several previous studies for SRL have improved their performance by using features extracted from this information (Hacioglu, 2004; Tsai et al., 2006). In the experimental results, the proposed system showed 68.46 f-score in TD phase, 85.20 accuracy in TC phase, 89.91 f-score in the initial step of RE phase and 81.24 f-score in the iterative step of RE phase, but officially achieved 61.65 precision, 9.40 recall and 16.31 f-score in approximate span matching. These figures were the lowest among twentyfour shared-task participants. However, we found that the threshold tuning for RE phase had caused a negative effect. It deteriorates the f-score of the 107 Proceedings of the Workshop on BioNLP: Shared Task, pages 107–110, c"
W09-1415,P03-1002,0,0.0630403,"Missing"
W09-1415,W06-0901,0,0.077832,"Missing"
W09-1415,W09-1401,0,\N,Missing
W09-3524,D07-1091,0,0.0236483,"ng can be performed in a totally monotonic way. The process of the general transliteration approach begins by matching the unit of a source 108 Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 108–111, c Suntec, Singapore, 7 August 2009. 2009 ACL and AFNLP Bilingual Corpus Eumjeol Decomposition Letter Alignment Factored Phrasebased Training Trained Model Input Word Eumjeol Decomposition MOSES Decoder Eumjeol Re-composition N-best Re-ranking Target Word Figure 2: Alignment example between ‘Knight’ and ‘ s à Ô [naiteu]’ Web 2.1 Dictionary Factored Phrase-based Training Koehn and Hoang (2007) introduces an integration of different information for phrase-based SMT model. We report on experiments with three factors: surface form, positional information, and the type of a letter. Surface form indicates a letter itself. For positional information, we add a BIO label to each input character in both the source words and the target words. The intuition is that certain character is differently pronounced depending on its position in a word. For example, ‘k’ in ‘Knight’ or ‘h’ in ‘Sarah’ are not pronounced. The type of a letter is used to classify whether a given letter is a vowel or a con"
W09-3524,P07-2045,0,0.0148544,"entence. Transliteration, a method of mapping phonemes or graphemes of source language into those of target language, can be used in this case in order to identify a possible translation of the word. The approaches to automatic transliteration between English and Korean can be performed through the following ways: First, in learning how to write the names of foreign origin, we can refer to a transliteration standard which is established by the government or some official linguistic organizations. No matter where the standard 2 Proposed Approach In order to build our base system, we use MOSES (Koehn et al., 2007), a well-known phrase-based system designed for SMT. MOSES offers a convenient framework which can be directly applied to machine transliteration experiments. In this framework, the transliteration can be performed in a very similar process of SMT task except the following changes. First, the unit of translation is changed from words to characters. Second, a phrase in transliteration refers to any contiguous block of character sequence which can be directly matched from a source word to a target word. Also, we do not have to worry about any distortion parameters because decoding can be perform"
W09-3524,W09-3502,0,0.0241209,"The internal code table represents mappings from each phonetic symbol to a single character within ASCII code table. Our pronouncing dictionary includes a list of words and their pronunciation information. For a given English word, if the word exists in the pronouncing dictionary, then its pronunciations are translated to Korean graphemes by a mapping table and transformation rules, which are defined by “Oeraeeo pyogibeop”. 3 3.1 Experimental Setup We participate in both standard and non-standard tracks for English-Korean name transliteration in NEWS 2009 Machine Transliteration Shared Task (Li et al., 2009). Experimenting on the development data, we determine the best performing parameters for MOSES as follows. • Maximum Phrase Length: 3 • Language Model N-gram Order: 3 • Language Model Smoothing: Kneser-Ney • Phrase Alignment Heuristic: grow-diag-final • Reordering: Monotone • Maximum Distortion Length: 0 With above parameter setup, the results are produced from the following five different systems. • Baseline System (BS): For the standard task, we use only given official training data 3 to construct translation model and language model for our base system. • Expanded Resource (ER): For all fou"
W09-3524,J03-1002,0,0.00312435,"proximity search by restricting two terms should occur within four-word distance. The frequency is adjusted as relative frequency form by dividing each frequency by total frequency of all n-best results. Also, we linearly interpolate the n-best score with the relative frequency of candidate output. To make fair interpolation, we adjust both scores to be between 0 and 1. Also, in this method, we decide to remove all the candidates whose frequencies are zero. Figure 1 shows the overall architecture of our system. The alignment between English letter and Korean letter is performed using GIZA++ (Och and Ney, 2003). We use MOSES decoder in order to search the best sequence of transliteration. In this paper we focus on describing factored phrase-based training and n-best re-ranking techniques including a Web-based method, a pronouncing dictionary-based method, and a phonicsbased method. 2.3 Pronouncing Dictionary-based Method According to “Oeraeeo pyogibeop1 ” (Korean orthography and writing method of borrowed for1 109 http://www.korean.go.kr/08 new/data/rule03.jsp Methods BS ER WR PD PB Acc.1 0.451 0.740 0.784 0.781 0.785 Mean F1 0.720 0.868 0.889 0.885 0.887 Mean Fdec 0.852 0.930 0.944 0.941 0.943 MRR"
W12-2029,W12-2006,0,0.0207765,"to model the types of errors that non-native speakers usually make. Recent studies demonstrate that it is possible to improve the performance of error correction systems by training the models on error-annotated non-native speaker texts (Han et al., 2010; Dahlmeier and Ng, 2011; Gamon, 2010). Most recently, a large collection of training data consisting of preposition and determiner errors made by non-native English speakers has been released in the HOO (Helping Our Own) 2012 Shared Task, which aims at promoting the research and development of automated tools for assisting authors in writing (Dale et al., 2012). In this paper, we introduce our error correction system that participated in the HOO 2012 Shared System Architecture The goal of our system is to detect and correct preposition and determiner errors in a given text. Our system consists of two types of classifiers, namely edit and insertion classifiers. Inputs for the two types of classifiers are noun phrases (NP), verb phrases (VP), and prepositional phrases (PP); we initially pre-process the text given for training/testing by using the Illinois Chunker1 and the Stanford Part-ofSpeech Tagger (Toutanova et al., 2003). For learning the classif"
W12-2029,han-etal-2010-using,0,0.216284,"e have been efforts aimed at developing grammar correction systems designed especially for non-native English speakers. A typical approach is to train statistical models on wellformed texts written by native English speakers and apply the learned models to non-native speaker texts to correct textual errors based on given context. This approach, however, fails to model the types of errors that non-native speakers usually make. Recent studies demonstrate that it is possible to improve the performance of error correction systems by training the models on error-annotated non-native speaker texts (Han et al., 2010; Dahlmeier and Ng, 2011; Gamon, 2010). Most recently, a large collection of training data consisting of preposition and determiner errors made by non-native English speakers has been released in the HOO (Helping Our Own) 2012 Shared Task, which aims at promoting the research and development of automated tools for assisting authors in writing (Dale et al., 2012). In this paper, we introduce our error correction system that participated in the HOO 2012 Shared System Architecture The goal of our system is to detect and correct preposition and determiner errors in a given text. Our system consist"
W12-2029,D10-1094,0,0.0179461,"ing data, and the candidates for determiner choice are the and a/an. In summary, we train a total of thirteen edit classifiers, one for each source preposition or determiner. For each edit classifier, the set of candidate outputs consists of the source preposition/determiner word itself, other confusable preposition/determiner words, and no preposition/determiner in case the source word should be deleted. Note that the number of confusable words for each source preposition is decided flexibly, depending on examples observed in the training data; a similar approach has been proposed earlier by Rozovskaya and Roth (2010a). For a particular source preposition/determiner word in the test data, the system decides whether to correct it or not based on the output of the classifier for that source word. Pattern s+NN s+*+NN s+VB s+*+VB VB+s Table 1: Patterns of candidates for insertion 2.3 252 Features Both edit and insertion classifiers can be trained using three types of features described below. • LEX/POS/HEAD This feature set refers to the contextual features from a window of n tokens to the right and left that are practically used in error correction studies (Rozovskaya and Roth, 2010b; Han et al., 2010; Gamon"
W12-2029,P11-1092,0,0.0259109,"s aimed at developing grammar correction systems designed especially for non-native English speakers. A typical approach is to train statistical models on wellformed texts written by native English speakers and apply the learned models to non-native speaker texts to correct textual errors based on given context. This approach, however, fails to model the types of errors that non-native speakers usually make. Recent studies demonstrate that it is possible to improve the performance of error correction systems by training the models on error-annotated non-native speaker texts (Han et al., 2010; Dahlmeier and Ng, 2011; Gamon, 2010). Most recently, a large collection of training data consisting of preposition and determiner errors made by non-native English speakers has been released in the HOO (Helping Our Own) 2012 Shared Task, which aims at promoting the research and development of automated tools for assisting authors in writing (Dale et al., 2012). In this paper, we introduce our error correction system that participated in the HOO 2012 Shared System Architecture The goal of our system is to detect and correct preposition and determiner errors in a given text. Our system consists of two types of classi"
W12-2029,N10-1018,0,0.0273819,"ing data, and the candidates for determiner choice are the and a/an. In summary, we train a total of thirteen edit classifiers, one for each source preposition or determiner. For each edit classifier, the set of candidate outputs consists of the source preposition/determiner word itself, other confusable preposition/determiner words, and no preposition/determiner in case the source word should be deleted. Note that the number of confusable words for each source preposition is decided flexibly, depending on examples observed in the training data; a similar approach has been proposed earlier by Rozovskaya and Roth (2010a). For a particular source preposition/determiner word in the test data, the system decides whether to correct it or not based on the output of the classifier for that source word. Pattern s+NN s+*+NN s+VB s+*+VB VB+s Table 1: Patterns of candidates for insertion 2.3 252 Features Both edit and insertion classifiers can be trained using three types of features described below. • LEX/POS/HEAD This feature set refers to the contextual features from a window of n tokens to the right and left that are practically used in error correction studies (Rozovskaya and Roth, 2010b; Han et al., 2010; Gamon"
W12-2029,P11-1093,0,0.0139519,"make an insertion or not based on the output of the insertion classifier. Example I’ll give you all information I need few days It may seem relaxing at beginning Buy new colored clothes I’m looking forward your reply • HAN This represents the set of features specifically used in the work of Han et al. (2010); they demonstrate that a model trained on non-native speaker texts can outperform one trained solely on well-formed texts. • L13 L1 refers to the first language of the author. There have been some efforts to leverage L1 information for improving error correction performance. For example, Rozovskaya and Roth (2011) propose an algorithm for adapting a learned model to the L1 of the author. There have been many studies leveraging writers’ L1. In this work, we propose to directly utilize L1 information of the authors as features. We also leverage additional features by combining L1 and individual head words that govern or are governed by VP or NP. 3 Additional Methods for Improvement The training data provided in the HOO 2012 Shared Task consists of exam scripts drawn from the publicly available FCE dataset (Yannakoudakis et al., 3 L1 information was provided in the training data but not in the test data."
W12-2029,N03-1033,0,0.0103012,"assisting authors in writing (Dale et al., 2012). In this paper, we introduce our error correction system that participated in the HOO 2012 Shared System Architecture The goal of our system is to detect and correct preposition and determiner errors in a given text. Our system consists of two types of classifiers, namely edit and insertion classifiers. Inputs for the two types of classifiers are noun phrases (NP), verb phrases (VP), and prepositional phrases (PP); we initially pre-process the text given for training/testing by using the Illinois Chunker1 and the Stanford Part-ofSpeech Tagger (Toutanova et al., 2003). For learning the classifiers, we use maximum entropy models, which have been successfully applied to many tasks in natural language processing. We particularly use Le Zhang’s Maximum Entropy Modeling Toolkit2 for implementation. 2.1 Edit Classifiers The role of an edit classifier is to check the source preposition/determiner word originally chosen by the author in a given text. If the source word is incorrect, the classifier replaces it with a better choice. For every preposition/determiner word, 1 2 Available at http://cogcomp.cs.illinois.edu Available at http://homepages.inf.ed.ac.uk/lzhan"
W12-2029,P11-1019,0,0.0819381,"Missing"
W13-3617,W13-1703,0,0.048493,"Missing"
W13-3617,C08-1022,0,0.0796287,"Missing"
W13-3617,han-etal-2010-using,0,\N,Missing
W99-0615,A88-1019,0,0.284497,"Missing"
W99-0615,J88-1003,0,0.105902,"Missing"
