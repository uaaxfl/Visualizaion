2021.jeptalnrecital-deft.6,Identification de profil clinique du patient: Une approche de classification de s{\\'e}quences utilisant des mod{\\`e}les de langage fran{\\c{c}}ais contextualis{\\'e}s (Identification of patient clinical profiles : A sequence classification approach using contextualised {F}rench language models ),2021,-1,-1,3,0,5692,aidan mannion,Actes de la 28e Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Atelier D{\\'E}fi Fouille de Textes (DEFT),0,"Cet article pr{\'e}sente un r{\'e}sum{\'e} de notre soumission pour T{\^a}che 1 de DEFT 2021. Cette t{\^a}che consiste {\`a} identifier le profil clinique d{'}un patient {\`a} partir d{'}une description textuelle de son cas clinique en identifiant les types de pathologie mentionn{\'e}s dans le texte. Ce travail {\'e}tudie des approches de classification de texte utilisant des plongements de mots contextualis{\'e}s en fran{\c{c}}ais. {\`A} partir d{'}une base de r{\'e}f{\'e}rence d{'}un mod{\`e}le constitu{\'e} pour la compr{\'e}hension g{\'e}n{\'e}rale de la langue fran{\c{c}}aise, nous utilisons des mod{\`e}les pr{\'e}-entra{\^\i}n{\'e}s avec masked language modelling et affin{\'e}s {\`a} la t{\^a}che d{'}identification, en utilisant un corpus externe de textes cliniques fourni par SOS M{\'e}decins, pour d{\'e}velopper des ensembles de classifieurs binaires associant les textes cliniques {\`a} des cat{\'e}gories de pathologies."
2021.iwslt-1.20,{ON}-{TRAC}{'} systems for the {IWSLT} 2021 low-resource speech translation and multilingual speech translation shared tasks,2021,-1,-1,8,1,5778,hang le,Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021),0,"This paper describes the ON-TRAC Consortium translation systems developed for two challenge tracks featured in the Evaluation Campaign of IWSLT 2021, low-resource speech translation and multilingual speech translation. The ON-TRAC Consortium is composed of researchers from three French academic laboratories and an industrial partner: LIA (Avignon Universit{\'e}), LIG (Universit{\'e} Grenoble Alpes), LIUM (Le Mans Universit{\'e}), and researchers from Airbus. A pipeline approach was explored for the low-resource speech translation task, using a hybrid HMM/TDNN automatic speech recognition system fed by wav2vec features, coupled to an NMT system. For the multilingual speech translation task, we investigated the us of a dual-decoder Transformer that jointly transcribes and translates an input speech. This model was trained in order to translate from multiple source languages to multiple target ones."
2021.findings-acl.250,Do Multilingual Neural Machine Translation Models Contain Language Pair Specific Attention Heads?,2021,-1,-1,4,0,8114,zae kim,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.codi-main.16,Visualizing {C}rossâ{L}ingual Discourse Relations in Multilingual {TED} Corpora,2021,-1,-1,4,0,8114,zae kim,Proceedings of the 2nd Workshop on Computational Approaches to Discourse,0,"This paper presents an interactive data dashboard that provides users with an overview of the preservation of discourse relations among 28 language pairs. We display a graph network depicting the cross-lingual discourse relations between a pair of languages for multilingual TED talks and provide a search function to look for sentences with specific keywords or relation types, facilitating ease of analysis on the cross-lingual discourse relations."
2021.acl-short.103,Lightweight Adapter Tuning for Multilingual Speech Translation,2021,-1,-1,5,1,5778,hang le,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Adapter modules were recently introduced as an efficient alternative to fine-tuning in NLP. Adapter tuning consists in freezing pre-trained parameters of a model and injecting lightweight modules between layers, resulting in the addition of only a small number of task-specific trainable parameters. While adapter tuning was investigated for multilingual neural machine translation, this paper proposes a comprehensive analysis of adapters for multilingual speech translation (ST). Starting from different pre-trained models (a multilingual ST trained on parallel data or a multilingual BART (mBART) trained on non parallel multilingual data), we show that adapters can be used to: (a) efficiently specialize ST to specific language pairs with a low extra cost in terms of parameters, and (b) transfer from an automatic speech recognition (ASR) task and an mBART pre-trained model to a multilingual ST task. Experiments show that adapter tuning offer competitive results to full fine-tuning, while being much more parameter-efficient."
2020.lrec-1.21,Providing Semantic Knowledge to a Set of Pictograms for People with Disabilities: a Set of Links between {W}ord{N}et and Arasaac: Arasaac-{WN},2020,-1,-1,1,1,5694,didier schwab,Proceedings of the 12th Language Resources and Evaluation Conference,0,"This article presents a resource that links WordNet, the widely known lexical and semantic database, and Arasaac, the largest freely available database of pictograms. Pictograms are a tool that is more and more used by people with cognitive or communication disabilities. However, they are mainly used manually via workbooks, whereas caregivers and families would like to use more automated tools (use speech to generate pictograms, for example). In order to make it possible to use pictograms automatically in NLP applications, we propose a database that links them to semantic knowledge. This resource is particularly interesting for the creation of applications that help people with cognitive disabilities, such as text-to-picto, speech-to-picto, picto-to-speech... In this article, we explain the needs for this database and the problems that have been identified. Currently, this resource combines approximately 800 pictograms with their corresponding WordNet synsets and it is accessible both through a digital collection and via an SQL database. Finally, we propose a method with associated tools to make our resource language-independent: this method was applied to create a first text-to-picto prototype for the French language. Our resource is distributed freely under a Creative Commons license at the following URL: https://github.com/getalp/Arasaac-WN."
2020.lrec-1.237,{WIKIR}: A Python Toolkit for Building a Large-scale {W}ikipedia-based {E}nglish Information Retrieval Dataset,2020,-1,-1,2,0,17086,jibril frej,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Over the past years, deep learning methods allowed for new state-of-the-art results in ad-hoc information retrieval. However such methods usually require large amounts of annotated data to be effective. Since most standard ad-hoc information retrieval datasets publicly available for academic research (e.g. Robust04, ClueWeb09) have at most 250 annotated queries, the recent deep learning models for information retrieval perform poorly on these datasets. These models (e.g. DUET, Conv-KNRM) are trained and evaluated on data collected from commercial search engines not publicly available for academic research which is a problem for reproducibility and the advancement of research. In this paper, we propose WIKIR: an open-source toolkit to automatically build large-scale English information retrieval datasets based on Wikipedia. WIKIR is publicly available on GitHub. We also provide wikIR59k: a large-scale publicly available dataset that contains 59,252 queries and 2,617,003 (query, relevant documents) pairs."
2020.lrec-1.302,{F}lau{BERT}: Unsupervised Language Model Pre-training for {F}rench,2020,-1,-1,10,1,5778,hang le,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Language models have become a key step to achieve state-of-the art results in many different Natural Language Processing (NLP) tasks. Leveraging the huge amount of unlabeled texts nowadays available, they provide an efficient way to pre-train continuous word representations that can be fine-tuned for a downstream task, along with their contextualization at the sentence level. This has been widely demonstrated for English using contextualized representations (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019b). In this paper, we introduce and share FlauBERT, a model learned on a very large and heterogeneous French corpus. Models of different sizes are trained using the new CNRS (French National Centre for Scientific Research) Jean Zay supercomputer. We apply our French language models to diverse NLP tasks (text classification, paraphrasing, natural language inference, parsing, word sense disambiguation) and show that most of the time they outperform other pre-training approaches. Different versions of FlauBERT as well as a unified evaluation protocol for the downstream tasks, called FLUE (French Language Understanding Evaluation), are shared to the research community for further reproducible experiments in French NLP."
2020.jeptalnrecital-taln.26,{F}lau{BERT} : des mod{\\`e}les de langue contextualis{\\'e}s pr{\\'e}-entra{\\^\\i}n{\\'e}s pour le fran{\\c{c}}ais ({F}lau{BERT} : Unsupervised Language Model Pre-training for {F}rench),2020,-1,-1,10,1,5778,hang le,"Actes de la 6e conf{\\'e}rence conjointe Journ{\\'e}es d'{\\'E}tudes sur la Parole (JEP, 33e {\\'e}dition), Traitement Automatique des Langues Naturelles (TALN, 27e {\\'e}dition), Rencontre des {\\'E}tudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (R{\\'E}CITAL, 22e {\\'e}dition). Volume 2 : Traitement Automatique des Langues Naturelles",0,"Les mod{\`e}les de langue pr{\'e}-entra{\^\i}n{\'e}s sont d{\'e}sormais indispensables pour obtenir des r{\'e}sultats {\`a} l{'}{\'e}tat-de-l{'}art dans de nombreuses t{\^a}ches du TALN. Tirant avantage de l{'}{\'e}norme quantit{\'e} de textes bruts disponibles, ils permettent d{'}extraire des repr{\'e}sentations continues des mots, contextualis{\'e}es au niveau de la phrase. L{'}efficacit{\'e} de ces repr{\'e}sentations pour r{\'e}soudre plusieurs t{\^a}ches de TALN a {\'e}t{\'e} d{\'e}montr{\'e}e r{\'e}cemment pour l{'}anglais. Dans cet article, nous pr{\'e}sentons et partageons FlauBERT, un ensemble de mod{\`e}les appris sur un corpus fran{\c{c}}ais h{\'e}t{\'e}rog{\`e}ne et de taille importante. Des mod{\`e}les de complexit{\'e} diff{\'e}rente sont entra{\^\i}n{\'e}s {\`a} l{'}aide du nouveau supercalculateur Jean Zay du CNRS. Nous {\'e}valuons nos mod{\`e}les de langue sur diverses t{\^a}ches en fran{\c{c}}ais (classification de textes, paraphrase, inf{\'e}rence en langage naturel, analyse syntaxique, d{\'e}sambigu{\""\i}sation automatique) et montrons qu{'}ils surpassent souvent les autres approches sur le r{\'e}f{\'e}rentiel d{'}{\'e}valuation FLUE {\'e}galement pr{\'e}sent{\'e} ici."
2020.jeptalnrecital-jep.24,Reconnaissance de parole beatbox{\\'e}e {\\`a} l{'}aide d{'}un syst{\\`e}me {HMM}-{GMM} inspir{\\'e} de la reconnaissance automatique de la parole ({BEATBOX} {SOUNDS} {RECOGNITION} {USING} A {SPEECH}-{DEDICATED} {HMM}-{GMM} {BASED} {SYSTEM} 1 Human beatboxing is a vocal art making use of speech organs to produce percussive sounds and imitate musical instruments),2020,-1,-1,4,0,18660,solene evain,"Actes de la 6e conf{\\'e}rence conjointe Journ{\\'e}es d'{\\'E}tudes sur la Parole (JEP, 33e {\\'e}dition), Traitement Automatique des Langues Naturelles (TALN, 27e {\\'e}dition), Rencontre des {\\'E}tudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (R{\\'E}CITAL, 22e {\\'e}dition). Volume 1 : Journ{\\'e}es d'{\\'E}tudes sur la Parole",0,"Le human-beatbox est un art vocal utilisant les organes de la parole pour produire des sons percussifs et imiter les instruments de musique. La classification des sons du beatbox repr{\'e}sente actuellement un d{\'e}fi. Nous proposons un syst{\`e}me de reconnaissance des sons de beatbox s{'}inspirant de la reconnaissance automatique de la parole. Nous nous appuyons sur la bo{\^\i}te {\`a} outils Kaldi, qui est tr{\`e}s utilis{\'e}e dans le cadre de la reconnaissance automatique de la parole (RAP). Notre corpus est compos{\'e} de sons isol{\'e}s produits par deux beatboxers et se compose de 80 sons diff{\'e}rents. Nous nous sommes concentr{\'e}s sur le d{\'e}codage avec des mod{\`e}les acoustiques monophones, {\`a} base de HMM-GMM. La transcription utilis{\'e}e s{'}appuie sur un syst{\`e}me d{'}{\'e}criture sp{\'e}cifique aux beatboxers, appel{\'e} Vocal Grammatics (VG). Ce syst{\`e}me d{'}{\'e}criture s{'}appuie sur les concepts de la phon{\'e}tique articulatoire."
2020.coling-main.314,Dual-decoder Transformer for Joint Automatic Speech Recognition and Multilingual Speech Translation,2020,-1,-1,5,1,5778,hang le,Proceedings of the 28th International Conference on Computational Linguistics,0,"We introduce dual-decoder Transformer, a new model architecture that jointly performs automatic speech recognition (ASR) and multilingual speech translation (ST). Our models are based on the original Transformer architecture (Vaswani et al., 2017) but consist of two decoders, each responsible for one task (ASR or ST). Our major contribution lies in how these decoders interact with each other: one decoder can attend to different information sources from the other via a dual-attention mechanism. We propose two variants of these architectures corresponding to two different levels of dependencies between the decoders, called the parallel and cross dual-decoder Transformers, respectively. Extensive experiments on the MuST-C dataset show that our models outperform the previously-reported highest translation performance in the multilingual settings, and outperform as well bilingual one-to-one results. Furthermore, our parallel models demonstrate no trade-off between ASR and ST compared to the vanilla multi-task architecture. Our code and pre-trained models are available at https://github.com/formiel/speech-translation."
W19-4605,{A}rb{E}ng{V}ec : {A}rabic-{E}nglish Cross-Lingual Word Embedding Model,2019,0,0,5,0,24075,raki lachraf,Proceedings of the Fourth Arabic Natural Language Processing Workshop,0,"Word Embeddings (WE) are getting increasingly popular and widely applied in many Natural Language Processing (NLP) applications due to their effectiveness in capturing semantic properties of words; Machine Translation (MT), Information Retrieval (IR) and Information Extraction (IE) are among such areas. In this paper, we propose an open source ArbEngVec which provides several Arabic-English cross-lingual word embedding models. To train our bilingual models, we use a large dataset with more than 93 million pairs of Arabic-English parallel sentences. In addition, we perform both extrinsic and intrinsic evaluations for the different word embedding model variants. The extrinsic evaluation assesses the performance of models on the cross-language Semantic Textual Similarity (STS), while the intrinsic evaluation is based on the Word Translation (WT) task."
2019.jeptalnrecital-long.4,"Compression de vocabulaire de sens gr{\\^a}ce aux relations s{\\'e}mantiques pour la d{\\'e}sambigu{\\\\\i}sation lexicale (Sense Vocabulary Compression through Semantic Knowledge for Word Sense Disambiguation)""",2019,-1,-1,3,1,16630,loic vial,Actes de la Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles (TALN) PFIA 2019. Volume I : Articles longs,0,"En D{\'e}sambigu{\""\i}sation Lexicale (DL), les syst{\`e}mes supervis{\'e}s dominent largement les campagnes d{'}{\'e}valuation. La performance et la couverture de ces syst{\`e}mes sont cependant rapidement limit{\'e}s par la faible quantit{\'e} de corpus annot{\'e}s en sens disponibles. Dans cet article, nous pr{\'e}sentons deux nouvelles m{\'e}thodes qui visent {\`a} r{\'e}soudre ce probl{\`e}me en exploitant les relations s{\'e}mantiques entre les sens tels que la synonymie, l{'}hyperonymie et l{'}hyponymie, afin de compresser le vocabulaire de sens de WordNet, et ainsi r{\'e}duire le nombre d{'}{\'e}tiquettes diff{\'e}rentes n{\'e}cessaires pour pouvoir d{\'e}sambigu{\""\i}ser tous les mots de la base lexicale. Nos m{\'e}thodes permettent de r{\'e}duire consid{\'e}rablement la taille des mod{\`e}les de DL neuronaux, avec l{'}avantage d{'}am{\'e}liorer leur couverture sans donn{\'e}es suppl{\'e}mentaires, et sans impacter leur pr{\'e}cision. En plus de nos m{\'e}thodes, nous pr{\'e}sentons un syst{\`e}me de DL qui tire parti des r{\'e}cents travaux sur les repr{\'e}sentations vectorielles de mots contextualis{\'e}es, afin d{'}obtenir des r{\'e}sultats qui surpassent largement l{'}{\'e}tat de l{'}art sur toutes les t{\^a}ches d{'}{\'e}valuation de la DL."
2019.jeptalnrecital-demo.1,"Apporter des connaissances s{\\'e}mantiques {\\`a} un jeu de pictogrammes destin{\\'e} {\\`a} des personnes en situation de handicap : Un ensemble de liens entre {P}rinceton {W}ord{N}et et Arasaac, Arasaac-{WN} (Giving semantic knowledge to a set of pictograms for people with disabilities : a set of links between {W}ord{N}et and Arasaac, Arasaac-{WN} )",2019,-1,-1,1,1,5694,didier schwab,Actes de la Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles (TALN) PFIA 2019. Volume IV : D{\\'e}monstrations,0,"Cet article pr{\'e}sente une ressource qui fait le lien entre WordNet et Arasaac, la plus grande base de pictogrammes librement disponible. Cette ressource est particuli{\`e}rement int{\'e}ressante pour la cr{\'e}ation d{'}applications visant l{'}aide aux personnes en situation de handicap cognitif."
2019.gwc-1.14,Sense Vocabulary Compression through the Semantic Knowledge of {W}ord{N}et for Neural Word Sense Disambiguation,2019,27,7,3,1,16630,loic vial,Proceedings of the 10th Global Wordnet Conference,0,"In this article, we tackle the issue of the limited quantity of manually sense annotated corpora for the task of word sense disambiguation, by exploiting the semantic relationships between senses such as synonymy, hypernymy and hyponymy, in order to compress the sense vocabulary of Princeton WordNet, and thus reduce the number of different sense tags that must be observed to disambiguate all words of the lexical database. We propose two different methods that greatly reduce the size of neural WSD models, with the benefit of improving their coverage without additional training data, and without impacting their precision. In addition to our methods, we present a WSD system which relies on pre-trained BERT word vectors in order to achieve results that significantly outperforms the state of the art on all WSD evaluation tasks."
L18-1166,{UFSAC}: Unification of Sense Annotated Corpora and Tools,2018,0,5,3,1,16630,loic vial,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"In Word Sense Disambiguation, sense annotated corpora are often essential for evaluating a system and also valuable in order to reach a good efficiency. Always created for a specific purpose, there are today a dozen of sense annotated English corpora, in various formats and using different versions of WordNet. The main hypothesis of this work is that it should be possible to build a disambiguation system by using any of these corpora during the training phase or during the testing phase regardless of their original purpose. In this article, we present UFSAC: a format of corpus that can be used for either training or testing a disambiguation system, and the process we followed for constructing this format. We give to the community the whole set of sense annotated English corpora that we know, in this unified format, when the copyright allows it, with sense keys converted to the last version of WordNet. We also provide the source code for building these corpora from their original data, and a complete Java API for manipulating corpora in this format. The whole resource is available at the following URL: https://github.com/getalp/UFSAC."
2018.jeptalnrecital-long.12,"Approche supervis{\\'e}e {\\`a} base de cellules {LSTM} bidirectionnelles pour la d{\\'e}sambigu{\\\\\i}sation lexicale ({LSTM} Based Supervised Approach for Word Sense Disambiguation)""",2018,-1,-1,3,1,16630,loic vial,"Actes de la Conf{\\'e}rence TALN. Volume 1 - Articles longs, articles courts de TALN",0,"En d{\'e}sambigu{\""\i}sation lexicale, l{'}utilisation des r{\'e}seaux de neurones est encore peu pr{\'e}sente et tr{\`e}s r{\'e}cente. Cette direction est pourtant tr{\`e}s prometteuse, tant les r{\'e}sultats obtenus par ces premiers syst{\`e}mes arrivent syst{\'e}matiquement en t{\^e}te des campagnes d{'}{\'e}valuation, malgr{\'e} une marge d{'}am{\'e}lioration qui semble encore importante. Nous pr{\'e}sentons dans cet article une nouvelle architecture {\`a} base de r{\'e}seaux de neurones pour la d{\'e}sambigu{\""\i}sation lexicale. Notre syst{\`e}me est {\`a} la fois moins complexe {\`a} entra{\^\i}ner que les syst{\`e}mes neuronaux existants et il obtient des r{\'e}sultats {\'e}tat de l{'}art sur la plupart des t{\^a}ches d{'}{\'e}valuation de la d{\'e}sambigu{\""\i}sation lexicale en anglais. L{'}accent est port{\'e} sur la reproductibilit{\'e} de notre syst{\`e}me et de nos r{\'e}sultats, par l{'}utilisation d{'}un mod{\`e}le de vecteurs de mots, de corpus d{'}apprentissage et d{'}{\'e}valuation librement accessibles."
2018.jeptalnrecital-demo.7,Un corpus en arabe annot{\\'e} manuellement avec des sens {W}ord{N}et ({A}rabic Manually Sense Annotated Corpus with {W}ord{N}et Senses),2018,-1,-1,4,1,30964,marwa salah,"Actes de la Conf{\\'e}rence TALN. Volume 2 - D{\\'e}monstrations, articles des Rencontres Jeunes Chercheurs, ateliers DeFT",0,"OntoNotes comprend le seul corpus manuellement annot{\'e} en sens librement disponible pour l{'}arabe. Elle reste peu connue et utilis{\'e}e certainement parce que le projet s{'}est achev{\'e} sans lier cet inventaire au Princeton WordNet qui lui aurait ouvert l{'}acc{\`e}s {\`a} son riche {\'e}cosyst{\`e}me. Dans cet article, nous pr{\'e}sentons une version {\'e}tendue de OntoNotes Release 5.0 que nous avons cr{\'e}{\'e}e en suivant une m{\'e}thodologie de construction semi-automatique. Il s{'}agit d{'}une mise {\`a} jour de la partie arabe annot{\'e}e en sens du corpus en ajoutant l{'}alignement vers le Princeton WordNet 3.0. Cette ressource qui comprend plus de 12 500 mots annot{\'e}s est librement disponible pour la communaut{\'e}. Nous esp{\'e}rons qu{'}elle deviendra un standard pour l{'}{\'e}valuation de la d{\'e}sambigu{\""\i}sation lexicale de l{'}arabe."
2018.jeptalnrecital-court.15,Traduction automatique de corpus en anglais annot{\\'e}s en sens pour la d{\\'e}sambigu{\\\\\i}sation lexicale d{'}une langue moins bien dot{\\'e}e, l{'}exemple de l{'}arabe (Automatic Translation of {E}nglish Sense Annotated Corpora for Word Sense Disambiguation of a Less Well-endowed Language," the Example of {A}rabic)""",2018,-1,-1,5,1,30964,marwa salah,"Actes de la Conf{\\'e}rence TALN. Volume 1 - Articles longs, articles courts de TALN",0,"Les corpus annot{\'e}s en sens sont des ressources cruciales pour la t{\^a}che de d{\'e}sambigu{\""\i}sation lexicale (Word Sense Disambiguation). La plupart des langues n{'}en poss{\`e}dent pas ou trop peu pour pouvoir construire des syst{\`e}mes robustes. Nous nous int{\'e}ressons ici {\`a} la langue arabe et pr{\'e}sentons 12 corpus annot{\'e}s en sens, fabriqu{\'e}s automatiquement {\`a} partir de 12 corpus en langue anglaise. Nous {\'e}valuons la qualit{\'e} de nos syst{\`e}mes de d{\'e}sambigu{\""\i}sation gr{\^a}ce {\`a} un corpus d{'}{\'e}valuation en arabe nouvellement disponible."
W17-6940,Sense Embeddings in Knowledge-Based Word Sense Disambiguation,2017,9,0,3,1,16630,loic vial,{IWCS} 2017 {---} 12th International Conference on Computational Semantics {---} Short papers,0,"In this paper, we develop a new way of creating sense vectors for any dictionary, by using an existing word embeddings model, and summing the vectors of the terms inside a sense's definition, weighted in function of their part of speech and their frequency. These vectors are then used for finding the closest senses to any other sense, thus creating a semantic network of related concepts, automatically generated. This network is hence evaluated against the existing semantic network found in WordNet, by comparing its contribution to a knowledge-based method for Word Sense Dis-ambiguation. This method can be applied to any other language which lacks such semantic network, as the creation of word vectors is totally unsupervised, and the creation of sense vectors only needs a traditional dictionary. The results show that our generated semantic network improves greatly the WSD system, almost as much as the manually created one."
W17-2502,Deep Investigation of Cross-Language Plagiarism Detection Methods,2017,12,1,3,1,31904,jeremy ferrero,Proceedings of the 10th Workshop on Building and Using Comparable Corpora,0,"This paper is a deep investigation of cross-language plagiarism detection methods on a new recently introduced open dataset, which contains parallel and comparable collections of documents with multiple characteristics (different genres, languages and sizes of texts). We investigate cross-language plagiarism detection methods for 6 language pairs on 2 granularities of text units in order to draw robust conclusions on the best methods while deeply analyzing correlations across document styles and languages."
W17-1303,Semantic Similarity of {A}rabic Sentences with Word Embeddings,2017,19,12,2,1,491,el nagoudi,Proceedings of the Third {A}rabic Natural Language Processing Workshop,0,"Semantic textual similarity is the basis of countless applications and plays an important role in diverse areas, such as information retrieval, plagiarism detection, information extraction and machine translation. This article proposes an innovative word embedding-based system devoted to calculate the semantic similarity in Arabic sentences. The main idea is to exploit vectors as word representations in a multidimensional space in order to capture the semantic and syntactic properties of words. IDF weighting and Part-of-Speech tagging are applied on the examined sentences to support the identification of words that are highly descriptive in each sentence. The performance of our proposed system is confirmed through the Pearson correlation between our assigned semantic similarity scores and human judgments."
S17-2012,{C}ompi{LIG} at {S}em{E}val-2017 Task 1: Cross-Language Plagiarism Detection Methods for Semantic Textual Similarity,2017,8,6,3,1,31904,jeremy ferrero,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,"We present our submitted systems for Semantic Textual Similarity (STS) Track 4 at SemEval-2017. Given a pair of Spanish-English sentences, each system must estimate their semantic similarity by a score between 0 and 5. In our submission, we use syntax-based, dictionary-based, context-based, and MT-based methods. We also combine these methods in unsupervised and supervised way. Our best run ranked 1st on track 4a with a correlation of 83.02{\%} with human annotations."
S17-2017,{LIM}-{LIG} at {S}em{E}val-2017 Task1: Enhancing the Semantic Similarity for {A}rabic Sentences with Vectors Weighting,2017,0,2,3,1,491,el nagoudi,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,"This article describes our proposed system named LIM-LIG. This system is designed for SemEval 2017 Task1: Semantic Textual Similarity (Track1). LIM-LIG proposes an innovative enhancement to word embedding-based model devoted to measure the semantic similarity in Arabic sentences. The main idea is to exploit the word representations as vectors in a multidimensional space to capture the semantic and syntactic properties of words. IDF weighting and Part-of-Speech tagging are applied on the examined sentences to support the identification of words that are highly descriptive in each sentence. LIM-LIG system achieves a Pearson{'}s correlation of 0.74633, ranking 2nd among all participants in the Arabic monolingual pairs STS task organized within the SemEval 2017 evaluation campaign"
E17-2066,Using Word Embedding for Cross-Language Plagiarism Detection,2017,15,8,3,1,31904,jeremy ferrero,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,This paper proposes to use distributed representation of words (word embeddings) in cross-language textual similarity detection. The main contributions of this paper are the following: (a) we introduce new cross-language similarity detection methods based on distributed representation of words; (b) we combine the different methods proposed to verify their complementarity and finally obtain an overall F1 score of 89.15{\%} for English-French similarity detection at chunk level (88.5{\%} at sentence level) on a very challenging corpus.
2017.jeptalnrecital-demo.9,Uniformisation de corpus anglais annot{\\'e}s en sens (Unification of sense annotated {E}nglish corpora for word sense disambiguation),2017,-1,-1,3,1,16630,loic vial,Actes des 24{\\`e}me Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 3 - D{\\'e}monstrations,0,"Pour la d{\'e}sambigu{\""\i}sation lexicale en anglais, on compte aujourd{'}hui une quinzaine de corpus annot{\'e}s en sens dans des formats souvent diff{\'e}rents et provenant de diff{\'e}rentes versions du Princeton WordNet. Nous pr{\'e}sentons un format pour uniformiser ces corpus, et nous fournissons {\`a} la communaut{\'e} l{'}ensemble des corpus annot{\'e}s en anglais port{\'e}s {\`a} notre connaissance avec des sens uniformis{\'e}s du Princeton WordNet 3.0, lorsque les droits le permettent et le code source pour construire l{'}ensemble des corpus {\`a} partir des donn{\'e}es originales."
2017.jeptalnrecital-court.14,Am{\\'e}lioration de la similarit{\\'e} s{\\'e}mantique vectorielle par m{\\'e}thodes non-supervis{\\'e}es (Improved the Semantic Similarity with Weighting Vectors),2017,-1,-1,3,0,33241,elmoatezbillah nagoudi,Actes des 24{\\`e}me Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 2 - Articles courts,0,
2017.jeptalnrecital-court.18,"Repr{\\'e}sentation vectorielle de sens pour la d{\\'e}sambigu{\\\\\i}sation lexicale {\\`a} base de connaissances (Sense Embeddings in Knowledge-Based Word Sense Disambiguation)""",2017,-1,-1,3,1,16630,loic vial,Actes des 24{\\`e}me Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 2 - Articles courts,0,"Dans cet article, nous proposons une nouvelle m{\'e}thode pour repr{\'e}senter sous forme vectorielle les sens d{'}un dictionnaire. Nous utilisons les termes employ{\'e}s dans leur d{\'e}finition en les projetant dans un espace vectoriel, puis en additionnant les vecteurs r{\'e}sultants, avec des pond{\'e}rations d{\'e}pendantes de leur partie du discours et de leur fr{\'e}quence. Le vecteur de sens r{\'e}sultant est alors utilis{\'e} pour trouver des sens reli{\'e}s, permettant de cr{\'e}er un r{\'e}seau lexical de mani{\`e}re automatique. Le r{\'e}seau obtenu est ensuite {\'e}valu{\'e} par rapport au r{\'e}seau lexical de WordNet, construit manuellement. Pour cela nous comparons l{'}impact des diff{\'e}rents r{\'e}seaux sur un syst{\`e}me de d{\'e}sambigu{\""\i}sation lexicale bas{\'e} sur la mesure de Lesk. L{'}avantage de notre m{\'e}thode est qu{'}elle peut {\^e}tre appliqu{\'e}e {\`a} n{'}importe quelle langue ne poss{\'e}dant pas un r{\'e}seau lexical comme celui de WordNet. Les r{\'e}sultats montrent que notre r{\'e}seau automatiquement g{\'e}n{\'e}r{\'e} permet d{'}am{\'e}liorer le score du syst{\`e}me de base, atteignant quasiment la qualit{\'e} du r{\'e}seau de WordNet."
L16-1657,"A Multilingual, Multi-style and Multi-granularity Dataset for Cross-language Textual Similarity Detection",2016,0,5,4,1,31904,jeremy ferrero,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"In this paper we describe our effort to create a dataset for the evaluation of cross-language textual similarity detection. We present preexisting corpora and their limits and we explain the various gathered resources to overcome these limits and build our enriched dataset. The proposed dataset is multilingual, includes cross-language alignment for different granularities (from chunk to document), is based on both parallel and comparable corpora and contains human and machine translated texts. Moreover, it includes texts written by multiple types of authors (from average to professionals). With the obtained dataset, we conduct a systematic and rigorous evaluation of several state-of-the-art cross-language textual similarity detection methods. The evaluation results are reviewed and discussed. Finally, dataset and scripts are made publicly available on GitHub: http://github.com/FerreroJeremy/Cross-Language-Dataset."
2016.jeptalnrecital-poster.1,Am{\\'e}lioration de la traduction automatique d{'}un corpus annot{\\'e} (Improvement of the automatic translation of an annotated corpus),2016,-1,-1,4,1,30964,marwa salah,Actes de la conf{\\'e}rence conjointe JEP-TALN-RECITAL 2016. volume 2 : TALN (Posters),0,"Dans cet article, nous pr{\'e}sentons une m{\'e}thode pour am{\'e}liorer la traduction automatique d{'}un corpus annot{\'e} et porter ses annotations de l{'}anglais vers une langue cible. Il s{'}agit d{'}am{\'e}liorer la m{\'e}thode de (Nasiruddin et al., 2015) qui donnait de nombreux segments non traduits, des duplications et des d{\'e}sordres. Nous proposons un processus de pr{\'e}-traitement du SemCor anglais, pour qu{'}il soit adapt{\'e} au syst{\`e}me de traduction automatique statistique utilis{\'e}, ainsi qu{'}un processus de post-traitement pour la sortie. Nous montrons une augmentation de 2,9 points en terme de score F1 sur une t{\^a}che de d{\'e}sambigu{\""\i}sation lexicale ce qui prouve l{'}efficacit{\'e} de notre m{\'e}thode."
2016.jeptalnrecital-long.13,Extension lexicale de d{\\'e}finitions gr{\\^a}ce {\\`a} des corpus annot{\\'e}s en sens (Lexical Expansion of definitions based on sense-annotated corpus ),2016,-1,-1,3,1,16630,loic vial,Actes de la conf{\\'e}rence conjointe JEP-TALN-RECITAL 2016. volume 2 : TALN (Articles longs),0,"Pour un certain nombre de t{\^a}ches ou d{'}applications du TALN, il est n{\'e}cessaire de d{\'e}terminer la proximit{\'e} s{\'e}mantique entre des sens, des mots ou des segments textuels. Dans cet article, nous nous int{\'e}ressons {\`a} une mesure bas{\'e}e sur des savoirs, la mesure de Lesk. La proximit{\'e} s{\'e}mantique de deux d{\'e}finitions est {\'e}valu{\'e}e en comptant le nombre de mots communs dans les d{\'e}finitions correspondantes dans un dictionnaire. Dans cet article, nous {\'e}tudions plus particuli{\`e}rement l{'}extension de d{\'e}finitions gr{\^a}ce {\`a} des corpus annot{\'e}s en sens. Il s{'}agit de prendre en compte les mots qui sont utilis{\'e}s dans le voisinage d{'}un certain sens et d{'}{\'e}tendre lexicalement la d{\'e}finition correspondante. Nous montrons une am{\'e}lioration certaine des performances obtenues en d{\'e}sambigu{\""\i}sation lexicale qui d{\'e}passent l{'}{\'e}tat de l{'}art."
2016.gwc-1.61,{W}ord{N}et and beyond: the case of lexical access,2016,-1,-1,2,0,33495,michael zock,Proceedings of the 8th Global WordNet Conference (GWC),0,"For humans the main functions of a dictionary is to store information concerning words and to reveal it when needed. While readers are interested in the meaning of words, writers look for answers concerning usage, spelling, grammar or word forms (lemma). We will focus here on this latter task: help authors to find the word they are looking for, word they may know but whose form is eluding them. Put differently, we try to build a resource helping authors to overcome the tip-of-the-tongue problem (ToT). Obviously, in order to access a word, it must be stored somewhere (brain, resource). Yet this is by no means sufficient. We will illustrate this here by comparing WordNet (WN) to an equivalent lexical resource bootstrapped from Wikipedia (WiPi). Both may contain a given word, but ease and success of access may be different depending on other factors like quality of the query, proximity, type of connections, etc. Next we will show under what conditions WN is suitable for word access, and finally we will present a roadmap showing the obstacles to be overcome to build a resource allowing the text producer to find the word s/he is looking for."
2015.jeptalnrecital-long.8,"Cr{\\'e}ation rapide et efficace d{'}un syst{\\`e}me de d{\\'e}sambigu{\\\\\i}sation lexicale pour une langue peu dot{\\'e}e""",2015,-1,-1,4,1,36410,mohammad nasiruddin,Actes de la 22e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Nous pr{\'e}sentons une m{\'e}thode pour cr{\'e}er rapidement un syst{\`e}me de d{\'e}sambigu{\""\i}sation lexicale (DL) pour une langue L peu dot{\'e}e pourvu que l{'}on dispose d{'}un syst{\`e}me de traduction automatique statistique (TAS) d{'}une langue riche en corpus annot{\'e}s en sens (ici l{'}anglais) vers L. Il est, en effet, plus facile de disposer des ressources n{\'e}cessaires {\`a} la cr{\'e}ation d{'}un syst{\`e}me de TAS que des ressources d{\'e}di{\'e}es n{\'e}cessaires {\`a} la cr{\'e}ation d{'}un syst{\`e}me de DL pour la langue L. Notre m{\'e}thode consiste {\`a} traduire automatiquement un corpus annot{\'e} en sens vers la langue L, puis de cr{\'e}er le syst{\`e}me de d{\'e}sambigu{\""\i}sation pour L par des m{\'e}thodes supervis{\'e}es classiques. Nous montrons la faisabilit{\'e} de la m{\'e}thode et sa g{\'e}n{\'e}ricit{\'e} en traduisant le SemCor, un corpus en anglais annot{\'e} gr{\^a}ce au Princeton WordNet, de l{'}anglais vers le bangla et de l{'}anglais vers le fran{\c{c}}ais. Nous montrons la validit{\'e} de l{'}approche en {\'e}valuant les r{\'e}sultats sur la t{\^a}che de d{\'e}sambigu{\""\i}sation lexicale multilingue de Semeval 2013."
W14-6706,Word storage does not guarantee accessibility (Stocker des Mots ne Garantit nullement leur Acc{\\`e}s) [in {F}rench],2014,0,0,2,0,33495,michael zock,TALN-RECITAL 2014 Workshop RLTLN 2014 : R{\\'e}seaux Lexicaux pour le TAL (RLTLN 2014 : Lexical Networks for NLP),0,None
F14-2035,Word Sense Induction for Lexical Resource Enrichment (Induction de sens pour enrichir des ressources lexicales) [in {F}rench],2014,0,0,2,1,36410,mohammad nasiruddin,Proceedings of TALN 2014 (Volume 2: Short Papers),0,None
S13-2041,{GETALP} System : Propagation of a {L}esk Measure through an Ant Colony Algorithm,2013,19,13,1,1,5694,didier schwab,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation ({S}em{E}val 2013)",0,"This article presents the GETALP system for the participation to SemEval-2013 Task 12, based on an adaptation of the Lesk measure propagated through an Ant Colony Algorithm, that yielded good results on the corpus of Semeval 2007 Task 7 (WordNet 2.1) as well as the trial data for Task 12 SemEval 2013 (BabelNet 1.0). We approach the parameter estimation to our algorithm from two perspectives: edogenous estimation where we maximised the sum the local Lesk scores; exogenous estimation where we maximised the F1 score on trial data. We proposed three runs of out system, exogenous estimation with BabelNet 1.1.1 synset id annotations, endogenous estimation with BabelNet 1.1.1 synset id annotations and endogenous estimation with WordNet 3.1 sense keys. A bug in our implementation led to incorrect results and here, we present an amended version thereof. Our system arrived third on this task and a more fine grained analysis of our results reveals that the algorithms performs best on general domain texts with as little named entities as possible. The presence of many named entities leads the performance of the system to plummet greatly."
W12-6108,Parameter estimation under uncertainty with Simulated Annealing applied to an ant colony based probabilistic {WSD} algorithm,2012,26,6,3,0.8,18982,andon tchechmedjiev,Proceedings of the First International Workshop on Optimization Techniques for Human Language Technology,0,"In this article we propose a method based on simulated annealing for the parameter estimation of probabilistic algorithms, where the solution provided by the algorithm can vary from execution to execution. Such algorithms are often very interesting to solve complex combinatorial problems, yet they involve many parameters that can be difficult to estimate manually due to their randomized output. We applied and evaluated a method for the parameter estimation of such algorithms and applied it for an Ant Colony Algorithm for WSD. For the evaluation, we used the Semeval 2007 Task 7 corpus. We split the corpus and took in turn one text as a training corpus and the four remaining texts as a test corpus. We tuned the parameters with an increasing number of sentences from the training text in order to estimate the quantity of data necessary to obtain an efficient and general set of parameters. We found that the results greatly depend on the nature of the text, even a very small amount of training sentences can lead to good results if the text has the right properties. RESUME (French) Estimation de parametres a base de Recuit Simule sous incertitude appliquee a un algorithmes a colonies de fourmis probabiliste. Nous proposons une methode basee sur un Recuit Simule pour lxe2x80x99estimation de parametres pour des algorithmes probabilistes ou les solutions generees varient. Ces algorithmes sont souvent tres interessant pour la resolution de problemes combinatoires complexes, mais ils requierent de nombreux parametres pouvant etre difficiles a estimer manuellement a cause de la nature aleatoire des solutions. Nous avons appliquesPlus specifiquement, nous appliquons et evaluons cette methode a pour estimer les parametres de tels algorimes et lxe2x80x99appliquons a un Algorithme a Colonies de Fourmis pour la desambiguisation lexicale. Pour lxe2x80x99evaluation, nous avons utilise le corpus de Semeval 2007 Txc3xa2che 7. Nous avons repectivement separe un texte comme corpus dxe2x80x99entrainement et les quatre autres comme corpus de test. Nous estimons les parametres pour un nombre croissant de phrases pour determiner combien de donnees sont necessaires pour obtenir un ensemble de valeurs de parametres generales et efficaces. Nous concluons que la qualite des resultats depends de la nature des textes. Meme des petites quantites de de phrases peuvent suffir a obtenir de bons resultats, du moment que le texte a les bonnes proprietes."
C12-1146,Ant Colony Algorithm for the Unsupervised Word Sense Disambiguation of Texts: Comparison and Evaluation,2012,30,16,1,1,5694,didier schwab,Proceedings of {COLING} 2012,0,"Brute-force word sense disambiguation (WSD) algorithms based on semantic relatedness are really time consuming. We study how to perform WSD faster and better on the span of a text. Several stochastic algorithms can be used to perform Global WSD. We focus here on an Ant Colony Algorithm and compare it to two other methods (Genetic and Simulated Annealing Algorithms) in order to evaluate them on the Semeval 2007 Task 7. A comparison of the algorithms shows that the Ant Colony Algorithm is faster than the two others, and yields better results. Furthermore, the Ant Colony Algorithm coupled with a majority vote strategy reaches the level of the first sense baseline and among other systems evaluated on the same task rivals the lower performing supervised algorithms."
2011.jeptalnrecital-long.11,"D{\\'e}sambigu{\\\\\i}sation lexicale par propagation de mesures s{\\'e}mantiques locales par algorithmes {\\`a} colonies de fourmis (Lexical disambiguation by propagation of local semantic measures using ant colony algorithms)""",2011,-1,-1,1,1,5694,didier schwab,Actes de la 18e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Effectuer une t{\^a}che de d{\'e}sambigu{\""\i}sation lexicale peut permettre d{'}am{\'e}liorer de nombreuses applications du traitement automatique des langues comme l{'}extraction d{'}informations multilingues, ou la traduction automatique. Sch{\'e}matiquement, il s{'}agit de choisir quel est le sens le plus appropri{\'e} pour chaque mot d{'}un texte. Une des approches classiques consiste {\`a} estimer la proximit{\'e} s{\'e}mantique qui existe entre deux sens de mots puis de l{'}{\'e}tendre {\`a} l{'}ensemble du texte. La m{\'e}thode la plus directe donne un score {\`a} toutes les paires de sens de mots puis choisit la cha{\^\i}ne de sens qui a le meilleur score. La complexit{\'e} de cet algorithme est exponentielle et le contexte qu{'}il est calculatoirement possible d{'}utiliser s{'}en trouve r{\'e}duit. Il ne s{'}agit donc pas d{'}une solution viable. Dans cet article, nous nous int{\'e}ressons {\`a} une autre m{\'e}thode, l{'}adaptation d{'}un algorithme {\`a} colonies de fourmis. Nous pr{\'e}sentons ses caract{\'e}ristiques et montrons qu{'}il permet de propager {\`a} un niveau global les r{\'e}sultats des algorithmes locaux et de tenir compte d{'}un contexte plus long et plus appropri{\'e} en un temps raisonnable."
2011.jeptalnrecital-long.20,{\\'E}valuation et consolidation d{'}un r{\\'e}seau lexical via un outil pour retrouver le mot sur le bout de la langue (Evaluation and consolidation of a lexical network via a tool to find the word on the tip of the tongue),2011,-1,-1,3,0.740741,32452,alain joubert,Actes de la 18e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Depuis septembre 2007, un r{\'e}seau lexical de grande taille pour le Fran{\c{c}}ais est en cours de construction {\`a} l{'}aide de m{\'e}thodes fond{\'e}es sur des formes de consensus populaire obtenu via des jeux (projet JeuxDeMots). L{'}intervention d{'}experts humains est marginale en ce qu{'}elle repr{\'e}sente moins de 0,5{\%} des relations du r{\'e}seau et se limite {\`a} des corrections, {\`a} des ajustements ainsi qu{'}{\`a} la validation des sens de termes. Pour {\'e}valuer la qualit{\'e} de cette ressource construite par des participants de jeu (utilisateurs non experts) nous adoptons une d{\'e}marche similaire {\`a} celle de sa construction, {\`a} savoir, la ressource doit {\^e}tre valid{\'e}e sur un vocabulaire de classe ouverte, par des non-experts, de fa{\c{c}}on stable (persistante dans le temps). Pour ce faire, nous proposons de v{\'e}rifier si notre ressource est capable de servir de support {\`a} la r{\'e}solution du probl{\`e}me nomm{\'e} {`}Mot sur le Bout de la Langue{'} (MBL). A l{'}instar de JeuxdeMots, l{'}outil d{\'e}velopp{\'e} peut {\^e}tre vu comme un jeu en ligne. Tout comme ce dernier, il permet d{'}acqu{\'e}rir de nouvelles relations, constituant ainsi un enrichissement de notre r{\'e}seau lexical."
W10-4009,Ontology driven content extraction using interlingual annotation of texts in the {OMNIA} project,2010,18,2,3,0,37968,achille falaise,Proceedings of the 4th Workshop on Cross Lingual Information Access,0,"OMNIA is an on-going project that aims to retrieve images accompanied with multilingual texts. In this paper, we propose a generic method (language and domain independent) to extract conceptual information from such texts and spontaneous user requests. First, texts are labelled with interlingual annotation, then a generic extractor taking a domain ontology as a parameter extract relevant conceptual information. Implementation is also presented with a first experiment and preliminary results."
W10-3411,"Lexical Access, a Search-Problem",2010,19,2,2,0.245459,33495,michael zock,Proceedings of the 2nd Workshop on Cognitive Aspects of the Lexicon,0,"Our work is confined to word access, that is, we present here our ideas of how to improve electronic dictionaries in order to help language producers (speaker/writer) to find the word they are looking for. Our approach is based on psychological findings (representation, storage and access of information in the human mind), observed search strategies and typical navigational behavior. If one agrees with the idea that lexical access (word finding) is basically a search problem, then one may still want to find out where and how to search. While the space, i.e. the semantic map in which search takes place is a resource problem,xe2x80x94 any of the following could be used: dictionary, corpus, thesauraus, etc. or a mix of them,xe2x80x94 its exploration is typically a search problem. Important as it may be, the building of a high quality resource is not the focus of this work, we rely on an existing one, and while we are concerned with its quality, we will be mostly concerned here with search methods, in order to determine the best."
W08-1902,Lexical access based on underspecified input,2008,38,15,2,0.245459,33495,michael zock,Coling 2008: Proceedings of the Workshop on Cognitive Aspects of the Lexicon ({COGALEX} 2008),0,"Words play a major role in language production, hence finding them is of vital importance, be it for speaking or writing. Words are stored in a dictionary, and the general belief holds, the bigger the better. Yet, to be truly useful the resource should contain not only many entries and a lot of information concerning each one of them, but also adequate means to reveal the stored information. Information access depends crucially on the organization of the data (words) and on the navigational tools. It also depends on the grouping, ranking and indexing of the data, a factor too often overlooked.n n We will present here some preliminary results, showing how an existing electronic dictionary could be enhanced to support language producers to find the word they are looking for. To this end we have started to build a corpus-based association matrix, composed of target words and access keys (meaning elements, related concepts/words), the two being connected at their intersection in terms of weight and type of link, information used subsequently for grouping, ranking and navigation."
2007.jeptalnrecital-long.27,"Les vecteurs conceptuels, un outil compl{\\'e}mentaire aux r{\\'e}seaux lexicaux",2007,6,2,1,1,5694,didier schwab,Actes de la 14{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Fr{\'e}quemment utilis{\'e}s dans le Traitement Automatique des Langues Naturelles, les r{\'e}seaux lexicaux font aujourd{'}hui l{'}objet de nombreuses recherches. La plupart d{'}entre eux, et en particulier le plus c{\'e}l{\`e}bre WordNet, souffrent du manque d{'}informations syntagmatiques mais aussi d{'}informations th{\'e}matiques (Â« probl{\`e}me du tennis Â»). Cet article pr{\'e}sente les vecteurs conceptuels qui permettent de repr{\'e}senter les id{\'e}es contenues dans un segment textuel quelconque et permettent d{'}obtenir une vision continue des th{\'e}matiques utilis{\'e}es gr{\^a}ce aux distances calculables entre eux. Nous montrons leurs caract{\'e}ristiques et en quoi ils sont compl{\'e}mentaires des r{\'e}seaux lexico-s{\'e}mantiques. Nous illustrons ce propos par l{'}enrichissement des donn{\'e}es de WordNet par des vecteurs conceptuels construits par {\'e}mergence."
2006.jeptalnrecital-poster.15,Approche {\\'e}volutive des notions de base pour une repr{\\'e}sentation th{\\'e}matique des connaissances g{\\'e}n{\\'e}rales,2006,-1,-1,3,0.740741,32452,alain joubert,Actes de la 13{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Posters,0,"Dans le domaine du Traitement Automatique du Langage Naturel, pour {\'e}laborer un syst{\`e}me de repr{\'e}sentation th{\'e}matique des connaissances g{\'e}n{\'e}rales, des m{\'e}thodes s{'}appuyant sur des th{\'e}saurus sont utilis{\'e}es depuis une quinzaine d{'}ann{\'e}es. Un th{\'e}saurus est constitu{\'e} d{'}un ensemble de concepts qui d{\'e}finissent un syst{\`e}me g{\'e}n{\'e}rateur d{'}un espace vectoriel mod{\'e}lisant les connaissances g{\'e}n{\'e}rales. Ces concepts, souvent organis{\'e}s en une hi{\'e}rarchie arborescente, constituent un instrument fondamental, mais totalement fig{\'e}. M{\^e}me si les notions {\'e}voluent (nous pensons par exemple aux domaines techniques), un th{\'e}saurus ne peut quant {\`a} lui {\^e}tre modifi{\'e} que lors d{'}un processus particuli{\`e}rement lourd, car n{\'e}cessitant la collaboration d{'}experts humains. C{'}est {\`a} ce probl{\`e}me que nous nous attaquons ici. Apr{\`e}s avoir d{\'e}taill{\'e} les caract{\'e}ristiques que doit poss{\'e}der un syst{\`e}me g{\'e}n{\'e}rateur de l{'}espace vectoriel de mod{\'e}lisation des connaissances, nous d{\'e}finissons les Â« notions de base Â». Celles-ci, dont la construction s{'}appuie initialement sur les concepts d{'}un th{\'e}saurus, constituent un autre syst{\`e}me g{\'e}n{\'e}rateur de cet espace vectoriel. Nous abordons la d{\'e}termination des acceptions exprimant les notions de base, ce qui nous am{\`e}ne naturellement {\`a} nous poser la question de leur nombre. Enfin, nous explicitons comment, s{'}affranchissant des concepts du th{\'e}saurus, ces notions de base {\'e}voluent par un processus it{\'e}ratif au fur et {\`a} mesure de l{'}analyse de nouveaux textes."
2005.jeptalnrecital-long.8,Extraction semi-supervis{\\'e}e de couples d{'}antonymes gr{\\^a}ce {\\`a} leur morphologie,2005,-1,-1,1,1,5694,didier schwab,Actes de la 12{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Dans le cadre de la recherche sur la repr{\'e}sentation du sens en Traitement Automatique des Langues Naturelles, nous nous concentrons sur la construction d{'}un syst{\`e}me capable d{'}acqu{\'e}rir le sens des mots, et les relations entre ces sens, {\`a} partir de dictionnaires {\`a} usage humain, du Web ou d{'}autres ressources lexicales. Pour l{'}antonymie, il n{'}existe pas de listes s{\'e}parant les antonymies compl{\'e}mentaire, scalaire et duale. Nous pr{\'e}sentons dans cet article une approche semi-supervis{\'e}e permettant de construire ces listes. Notre m{\'e}thode est bas{\'e}e sur les oppositions de nature morphologique qui peuvent exister entre les items lexicaux. {\`A} partir d{'}un premier ensemble de couples antonymes, elle permet non seulement de construire ces listes mais aussi de trouver des oppositions morphologiques. Nous {\'e}tudions les r{\'e}sultats obtenus par cette m{\'e}thode. En particulier, nous pr{\'e}sentons les oppositions de pr{\'e}fixes ainsi d{\'e}couvertes et leur validit{\'e} sur le corpus puis nous discutons de la r{\'e}partition des types d{'}antonymie en fonction des couples oppos{\'e}s de pr{\'e}fixes."
2003.jeptalnrecital-recital.8,Soci{\\'e}t{\\'e} d{'}agents apprenants et s{\\'e}mantique lexicale : comment construire des vecteurs conceptuels {\\`a} l{'}aide de la double boucle,2003,-1,-1,1,1,5694,didier schwab,Actes de la 10{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. REncontres jeunes Chercheurs en Informatique pour le Traitement Automatique des Langues,0,"Dans le cadre de la repr{\'e}sentation du sens en TALN, nous d{\'e}veloppons actuellement un syst{\`e}me d{'}analyse des aspects th{\'e}matiques des textes et de d{\'e}sambigu{\""\i}sation lexicale bas{\'e}e sur les vecteurs conceptuels. Ces vecteurs visent {\`a} repr{\'e}senter un ensemble d{'}id{\'e}es associ{\'e}es {\`a} tout segment textuel. {\`A} partir de ce mod{\`e}le, nous avons pos{\'e} des hypoth{\`e}ses sur la construction des vecteurs. Dans cet article, nous montrons comment ces hypoth{\`e}ses, ainsi que des consid{\'e}rations techniques comme la possibilit{\'e} de distribuer les t{\^a}ches {\`a} effectuer ou la modularit{\'e}, nous ont amen{\'e}es {\`a} adopter une architecture multi-agents. Chaque agent poss{\`e}de un certain nombre de comp{\'e}tences, une m{\'e}moire qui lui est propre et peut interragir avec son environnement (les autres agents). Pour finir, nous pr{\'e}sentons les agents d{\'e}j{\`a} impl{\'e}ment{\'e}s et un exemple de leur collaboration."
2003.jeptalnrecital-long.22,Am{\\'e}lioration de liens entre acceptions par fonctions lexicales vectorielles sym{\\'e}triques,2003,-1,-1,1,1,5694,didier schwab,Actes de la 10{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Dans le cadre du projet Papillon qui vise {\`a} la construction de bases lexicales multilingues par acceptions, nous avons d{\'e}fini des strat{\'e}gies pour peupler un dictionnaire pivot de liens interlingues {\`a} partir d{'}une base vectorielle monolingue. Il peut y avoir un nombre important de sens par entr{\'e}e et donc l{'}identification des acceptions correspondantes peut {\^e}tre erron{\'e}e. Nous am{\'e}liorons l{'}int{\'e}grit{\'e} de la base d{'}acception gr{\^a}ce {\`a} des agents experts dans les fonctions lexicales comme la synonymie, l{'}antonymie, l{'}hyp{\'e}ronymie ou l{'}holonymie. Ces agents sont capable de calculer la pertinence d{'}une relation s{\'e}mantique entre deux acceptions par les diverses informations lexicales r{\'e}colt{\'e}es et les vecteurs conceptuels. Si une certaine pertinence est au-dessus d{'}un seuil, ils cr{\'e}ent un lien s{\'e}mantique qui peut {\^e}tre utilis{\'e} par d{'}autres agents charg{\'e}s par exemple de la d{\'e}sambigu{\""\i}sation ou du transfert lexical. Les agents v{\'e}rifiant l{'}int{\'e}grit{\'e} de la base cherchent les incoh{\'e}rences de la base et en avertissent les lexicographes le cas {\'e}ch{\'e}ant."
C02-1061,Antonymy and Conceptual Vectors,2002,8,19,1,1,5694,didier schwab,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"For meaning representations in NLP, we focus our attention on thematic aspects and conceptual vectors. The learning strategy of conceptual vectors relies on a morphosyntaxic analysis of human usage dictionary definitions linked to vector propagation. This analysis currently doesn't take into account negation phenomena. This work aims at studying the antonymy aspects of negation, in the larger goal of its integration into the thematic analysis. We present a model based on the idea of symmetry compatible with conceptual vectors. Then, we define antonymy functions which allows the construction of an antonymous vector and the enumeration of its potentially antinomic lexical items. Finally, we introduce a measure which evaluates how a given word is an acceptable antonym for a term."
2002.jeptalnrecital-long.10,"Vers l{'}apprentissage automatique, pour et par les vecteurs conceptuels, de fonctions lexicales. L{'}exemple de l{'}antonymie",2002,3,4,1,1,5694,didier schwab,Actes de la 9{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Dans le cadre de recherches sur le sens en traitement automatique du langage, nous nous concentrons sur la repr{\'e}sentation de l{'}aspect th{\'e}matique des segments textuels {\`a} l{'}aide de vecteurs conceptuels. Les vecteurs conceptuels sont automatiquement appris {\`a} partir de d{\'e}finitions issues de dictionnaires {\`a} usage humain (Schwab, 2001). Un noyau de termes manuellement index{\'e}s est n{\'e}cessaire pour l{'}amor{\c{c}}age de cette analyse. Lorsque l{'}item d{\'e}fini s{'}y pr{\^e}te, ces d{\'e}finitions sont compl{\'e}t{\'e}es par des termes en relation avec lui. Ces relations sont des fonctions lexicales (Mel{'}cuk and al, 95) comme l{'}hyponymie, l{'}hyperonymie, la synonymie ou l{'}antonymie. Cet article propose d{'}am{\'e}liorer la fonction d{'}antonymie na{\""\i}ve expos{\'e}e dans (Schwab, 2001) et (Schwab and al, 2002) gr{\^a}ce {\`a} ces informations. La fonction s{'}auto-modifie, par r{\'e}vision de listes, en fonction des relations d{'}antonymie av{\'e}r{\'e}es entre deux items. Nous exposons la m{\'e}thode utilis{\'e}e, quelques r{\'e}sultats puis nous concluons sur les perspectives ouvertes."
