2021.nodalida-main.36,Grapheme-Based Cross-Language Forced Alignment: Results with Uralic Languages,2021,-1,-1,2,1,2695,juho leinonen,Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa),0,"Forced alignment is an effective process to speed up linguistic research. However, most forced aligners are language-dependent, and under-resourced languages rarely have enough resources to train an acoustic model for an aligner. We present a new Finnish grapheme-based forced aligner and demonstrate its performance by aligning multiple Uralic languages and English as an unrelated language. We show that even a simple non-expert created grapheme-to-phoneme mapping can result in useful word alignments."
2021.nodalida-main.37,Boosting Neural Machine Translation from {F}innish to {N}orthern {S}{\\'a}mi with Rule-Based Backtranslation,2021,-1,-1,2,1,2697,mikko aulamo,Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa),0,"We consider a low-resource translation task from Finnish into Northern S{\'a}mi. Collecting all available parallel data between the languages, we obtain around 30,000 sentence pairs. However, there exists a significantly larger monolingual Northern S{\'a}mi corpus, as well as a rule-based machine translation (RBMT) system between the languages. To make the best use of the monolingual data in a neural machine translation (NMT) system, we use the backtranslation approach to create synthetic parallel data from it using both NMT and RBMT systems. Evaluating the results on an in-domain test set and a small out-of-domain set, we find that the RBMT backtranslation outperforms NMT backtranslation clearly for the out-of-domain test set, but also slightly for the in-domain data, for which the NMT backtranslation model provided clearly better BLEU scores than the RBMT. In addition, combining both backtranslated data sets improves the RBMT approach only for the in-domain test set. This suggests that the RBMT system provides general-domain knowledge that cannot be found from the relative small parallel training data."
2021.americasnlp-1.29,The {H}elsinki submission to the {A}mericas{NLP} shared task,2021,-1,-1,3,0.390071,9977,raul vazquez,Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas,0,"The University of Helsinki participated in the AmericasNLP shared task for all ten language pairs. Our multilingual NMT models reached the first rank on all language pairs in track 1, and first rank on nine out of ten language pairs in track 2. We focused our efforts on three aspects: (1) the collection of additional data from various sources such as Bibles and political constitutions, (2) the cleaning and filtering of training data with the OpusFilter toolkit, and (3) different multilingual training techniques enabled by the latest version of the OpenNMT-py toolkit to make the most efficient use of the scarce data. This paper describes our efforts in detail."
2020.wmt-1.134,The {U}niversity of {H}elsinki and Aalto University submissions to the {WMT} 2020 news and low-resource translation tasks,2020,-1,-1,3,0,263,yves scherrer,Proceedings of the Fifth Conference on Machine Translation,0,"This paper describes the joint participation of University of Helsinki and Aalto University to two shared tasks of WMT 2020: the news translation between Inuktitut and English and the low-resource translation between German and Upper Sorbian. For both tasks, our efforts concentrate on efficient use of monolingual and related bilingual corpora with scheduled multi-task learning as well as an optimized subword segmentation with sampling. Our submission obtained the highest score for Upper Sorbian -{\textgreater} German and was ranked second for German -{\textgreater} Upper Sorbian according to BLEU scores. For English{--}Inuktitut, we reached ranks 8 and 10 out of 11 according to BLEU scores."
2020.sltu-1.6,Effects of Language Relatedness for Cross-lingual Transfer Learning in Character-Based Language Models,2020,-1,-1,3,0,13677,mittul singh,Proceedings of the 1st Joint Workshop on Spoken Language Technologies for Under-resourced languages (SLTU) and Collaboration and Computing for Under-Resourced Languages (CCURL),0,"Character-based Neural Network Language Models (NNLM) have the advantage of smaller vocabulary and thus faster training times in comparison to NNLMs based on multi-character units. However, in low-resource scenarios, both the character and multi-character NNLMs suffer from data sparsity. In such scenarios, cross-lingual transfer has improved multi-character NNLM performance by allowing information transfer from a source to the target language. In the same vein, we propose to use cross-lingual transfer for character NNLMs applied to low-resource Automatic Speech Recognition (ASR). However, applying cross-lingual transfer to character NNLMs is not as straightforward. We observe that relatedness of the source language plays an important role in cross-lingual pretraining of character NNLMs. We evaluate this aspect on ASR tasks for two target languages: Finnish (with English and Estonian as source) and Swedish (with Danish, Norwegian, and English as source). Prior work has observed no difference between using the related or unrelated language for multi-character NNLMs. We, however, show that for character-based NNLMs, only pretraining with a related language improves the ASR performance, and using an unrelated language may deteriorate it. We also observe that the benefits are larger when there is much lesser target data than source data."
2020.lrec-1.467,{O}pus{T}ools and Parallel Corpus Diagnostics,2020,-1,-1,3,1,2697,mikko aulamo,Proceedings of the 12th Language Resources and Evaluation Conference,0,"This paper introduces OpusTools, a package for downloading and processing parallel corpora included in the OPUS corpus collection. The package implements tools for accessing compressed data in their archived release format and make it possible to easily convert between common formats. OpusTools also includes tools for language identification and data filtering as well as tools for importing data from various sources into the OPUS format. We show the use of these tools in parallel corpus creation and data diagnostics. The latter is especially useful for the identification of potential problems and errors in the extensive data set. Using these tools, we can now monitor the validity of data sets and improve the overall quality and consitency of the data collection."
2020.lrec-1.486,{M}orfessor {EM}+{P}rune: Improved Subword Segmentation with Expectation Maximization and Pruning,2020,0,1,2,1,13980,stigarne gronroos,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Data-driven segmentation of words into subword units has been used in various natural language processing applications such as automatic speech recognition and statistical machine translation for almost 20 years. Recently it has became more widely adopted, as models based on deep neural networks often benefit from subword units even for morphologically simpler languages. In this paper, we discuss and compare training algorithms for a unigram subword model, based on the Expectation Maximization algorithm and lexicon pruning. Using English, Finnish, North Sami, and Turkish data sets, we show that this approach is able to find better solutions to the optimization problem defined by the Morfessor Baseline model than its original recursive training algorithm. The improved optimization also leads to higher morphological segmentation accuracy when compared to a linguistic gold standard. We publish implementations of the new algorithms in the widely-used Morfessor software package."
2020.blackboxnlp-1.13,Controlling the Imprint of Passivization and Negation in Contextualized Representations,2020,-1,-1,2,0,12485,hande celikkanat,Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,0,"Contextualized word representations encode rich information about syntax and semantics, alongside specificities of each context of use. While contextual variation does not always reflect actual meaning shifts, it can still reduce the similarity of embeddings for word instances having the same meaning. We explore the imprint of two specific linguistic alternations, namely passivization and negation, on the representations generated by neural models trained with two different objectives: masked language modeling and translation. Our exploration methodology is inspired by an approach previously proposed for removing societal biases from word vectors. We show that passivization and negation leave their traces on the representations, and that neutralizing this information leads to more similar embeddings for words that should preserve their meaning in the transformation. We also find clear differences in how the respective features generalize across datasets."
2020.acl-demos.20,{O}pus{F}ilter: A Configurable Parallel Corpus Filtering Toolbox,2020,-1,-1,2,1,2697,mikko aulamo,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"This paper introduces OpusFilter, a flexible and modular toolbox for filtering parallel corpora. It implements a number of components based on heuristic filters, language identification libraries, character-based language models, and word alignment tools, and it can easily be extended with custom filters. Bitext segments can be ranked according to their quality or domain match using single features or a logistic regression model that can be trained without manually labeled training data. We demonstrate the effectiveness of OpusFilter on the example of a Finnish-English news translation task based on noisy web-crawled training data. Applying our tool leads to improved translation quality while significantly reducing the size of the training data, also clearly outperforming an alternative ranking given in the crawled data set. Furthermore, we show the ability of OpusFilter to perform data selection for domain adaptation."
W19-5432,The {U}niversity of {H}elsinki Submissions to the {WMT}19 Similar Language Translation Task,2019,22,0,3,0,263,yves scherrer,"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)",0,"This paper describes the University of Helsinki Language Technology group{'}s participation in the WMT 2019 similar language translation task. We trained neural machine translation models for the language pairs Czech {\textless}-{\textgreater} Polish and Spanish {\textless}-{\textgreater} Portuguese. Our experiments focused on different subword segmentation methods, and in particular on the comparison of a cognate-aware segmentation method, Cognate Morfessor, with character segmentation and unsupervised segmentation methods for which the data from different languages were simply concatenated. We did not observe major benefits from cognate-aware segmentation methods, but further research may be needed to explore larger parts of the parameter space. Character-level models proved to be competitive for translation between Spanish and Portuguese, but they are slower in training and decoding."
W19-5347,The {U}niversity of {H}elsinki Submissions to the {WMT}19 News Translation Task,2019,22,0,5,0,2672,aarne talman,"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",0,"In this paper we present the University of Helsinki submissions to the WMT 2019 shared news translation task in three language pairs: English-German, English-Finnish and Finnish-English. This year we focused first on cleaning and filtering the training data using multiple data-filtering approaches, resulting in much smaller and cleaner training sets. For English-German we trained both sentence-level transformer models as well as compared different document-level translation approaches. For Finnish-English and English-Finnish we focused on different segmentation approaches and we also included a rule-based system for English-Finnish."
W19-0302,{N}orth {S}{\\'a}mi morphological segmentation with low-resource semi-supervised sequence labeling,2019,0,0,2,1,13980,stigarne gronroos,Proceedings of the Fifth International Workshop on Computational Linguistics for Uralic Languages,0,None
W18-6410,Cognate-aware morphological segmentation for multilingual neural translation,2018,0,0,2,1,13980,stigarne gronroos,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"This article describes the Aalto University entry to the WMT18 News Translation Shared Task. We participate in the multilingual subtrack with a system trained under the constrained condition to translate from English to both Finnish and Estonian. The system is based on the Transformer model. We focus on improving the consistency of morphological segmentation for words that are similar orthographically, semantically, and distributionally; such words include etymological cognates, loan words, and proper names. For this, we introduce Cognate Morfessor, a multilingual variant of the Morfessor method. We show that our approach improves the translation quality particularly for Estonian, which has less resources for training the translation model."
W18-0208,New Baseline in Automatic Speech Recognition for {N}orthern {S}{\\'a}mi,2018,0,0,3,1,2695,juho leinonen,Proceedings of the Fourth International Workshop on Computational Linguistics of Uralic Languages,0,None
W17-4727,Extending hybrid word-character neural machine translation with multi-task learning of morphological analysis,2017,0,3,2,1,13980,stigarne gronroos,Proceedings of the Second Conference on Machine Translation,0,None
W16-2312,Hybrid Morphological Segmentation for Phrase-Based Machine Translation,2016,0,2,2,1,13980,stigarne gronroos,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,None
J16-1003,A Comparative Study of Minimally Supervised Morphological Segmentation,2016,59,8,6,0.869565,35500,teemu ruokolainen,Computational Linguistics,0,"This article presents a comparative study of a subfield of morphology learning referred to as minimally supervised morphological segmentation. In morphological segmentation, word forms are segmented into morphs, the surface forms of morphemes. In the minimally supervised data-driven learning setting, segmentation models are learned from a small number of manually annotated word forms and a large set of unannotated word forms. In addition to providing a literature survey on published methods, we present an in-depth empirical comparison on three diverse model families, including a detailed error analysis. Based on the literature survey, we conclude that the existing methodology contains substantial work on generative morph lexicon-based approaches and methods based on discriminative boundary detection. As for which approach has been more successful, both the previous work and the empirical evaluation presented here strongly imply that the current state of the art is yielded by the discriminative boundary detection methodology."
W15-3010,Tuning Phrase-Based Segmented Translation for a Morphologically Complex Target Language,2015,11,2,2,1,13980,stigarne gronroos,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"This article describes the Aalto University entry to the English-to-Finnish shared translation task in WMT 2015. The system participates in the constrained condition, but in addition we impose some further constraints, using no language-specific resources beyond those provided in the task. We use a morphological segmenter, Morfessor FlatCat, but train and tune it in an unsupervised manner. The system could thus be used for another language pair with a morphologically complex target language, without needing modification or additional resources."
W15-3052,{L}e{BLEU}: N-gram-based Translation Evaluation Score for Morphologically Complex Languages,2015,15,5,1,1,2696,sami virpioja,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"This paper describes the LeBLEU evaluation score for machine translation, submitted to WMT15 Metrics Shared Task. LeBLEU extends the popular BLEU score to consider fuzzy matches between word n-grams. While there are several variants of BLEU that allow to non-exact matches between words either by character-based distance measures or morphological preprocessing, none of them use fuzzy comparison between longer chunks of text. The results on WMT data sets show that fuzzy n-gram matching improves correlations to human evaluation especially for highly compounding languages."
E14-4017,Painless Semi-Supervised Morphological Segmentation using Conditional Random Fields,2014,23,14,3,0.869565,35500,teemu ruokolainen,"Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics, volume 2: Short Papers",0,"We discuss data-driven morphological segmentation, in which word forms are segmented into morphs, that is the surface forms of morphemes. We extend a recent segmentation approach based on conditional random fields from purely supervised to semi-supervised learning by exploiting available unsupervised segmentation techniques. We integrate the unsupervised techniques into the conditional random field model via feature set augmentation. Experiments on three diverse languages show that this straightforward semi-supervised extension greatly improves the segmentation accuracy of the purely supervised CRFs in a computationally efficient manner."
E14-2006,{M}orfessor 2.0: Toolkit for statistical morphological segmentation,2014,13,22,2,1,14672,peter smit,Proceedings of the Demonstrations at the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Morfessor is a family of probabilistic machine learning methods for finding the morphological segmentation from raw text data. Recent developments include the development of semi-supervised methods for utilizing annotated data. Morfessor 2.0 is a rewrite of the original, widely-used Morfessor 1.0 software, with well documented command-line tools and library interface. It includes new features such as semi-supervised learning, online training, and integrated evaluation code."
C14-1111,{M}orfessor {F}lat{C}at: An {HMM}-Based Method for Unsupervised and Semi-Supervised Learning of Morphology,2014,18,25,2,1,13980,stigarne gronroos,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"Morfessor is a family of methods for learning morphological segmentations of words based on unannotated data. We introduce a new variant of Morfessor, FlatCat, that applies a hidden Markov model structure. It builds on previous work on Morfessor, sharing model components with the popular Morfessor Baseline and Categories-MAP variants. Our experiments show that while unsupervised FlatCat does not reach the accuracy of Categories-MAP, with semisupervised learning it provides state-of-the-art results in the Morpho Challenge 2010 tasks for English, Finnish, and Turkish."
W13-3504,Supervised Morphological Segmentation in a Low-Resource Learning Setting using Conditional Random Fields,2013,20,24,3,0.857143,35500,teemu ruokolainen,Proceedings of the Seventeenth Conference on Computational Natural Language Learning,0,"We discuss data-driven morphological segmentation, in which word forms are segmented into morphs, the surface forms of morphemes. Our focus is on a lowresource learning setting, in which only a small amount of annotated word forms are available for model training, while unannotated word forms are available in abundance. The current state-of-art methods 1) exploit both the annotated and unannotated data in a semi-supervised manner, and 2) learn morph lexicons and subsequently uncover segmentations by generating the most likely morph sequences. In contrast, we discuss 1) employing only the annotated data in a supervised manner, while entirely ignoring the unannotated data, and 2) directly learning to predict morph boundaries given their local sub-string contexts instead of learning the morph lexicons. Specifically, we employ conditional random fields, a popular discriminative log-linear model for segmentation. We present experiments on two data sets comprising five diverse languages. We show that the fully supervised boundary prediction approach outperforms the state-of-art semi-supervised morph lexicon approaches on all languages when using the same annotated data sets."
W11-4632,Evaluating the effect of word frequencies in a probabilistic generative model of morphology,2011,14,4,1,1,2696,sami virpioja,Proceedings of the 18th Nordic Conference of Computational Linguistics ({NODALIDA} 2011),0,"We consider generative probabilistic models for unsupervised learning of morphology. When training such a model, one has to decide what to include in the training data; e.g., should the frequencies of words affect the likelihood, and should words occurring only once be discarded. We show that for a certain type of models, the likelihood can be parameterized on a function of the word frequencies. Thorough experiments are carried out with Morfessor Baseline, evaluating the resulting quality of the morpheme analysis on English and Finnish test sets. Our results show that training on word types or with a logarithmic function of the word frequencies give similar scores, while a linear function, i.e., training on word tokens, is significantly worse."
W10-2210,Semi-Supervised Learning of Concatenative Morphology,2010,21,34,2,1,35501,oskar kohonen,Proceedings of the 11th Meeting of the {ACL} Special Interest Group on Computational Morphology and Phonology,0,"We consider morphology learning in a semi-supervised setting, where a small set of linguistic gold standard analyses is available. We extend Morfessor Baseline, which is a method for unsupervised morphological segmentation, to this task. We show that known linguistic segmentations can be exploited by adding them into the data likelihood function and optimizing separate weights for unlabeled and labeled data. Experiments on English and Finnish are presented with varying amount of labeled data. Results of the linguistic evaluation of Morpho Challenge improve rapidly already with small amounts of labeled data, surpassing the state-of-the-art unsupervised methods at 1000 labeled words for English and at 100 labeled words for Finnish."
W10-2211,Morpho Challenge 2005-2010: Evaluations and Results,2010,9,14,2,0.769231,2635,mikko kurimo,Proceedings of the 11th Meeting of the {ACL} Special Interest Group on Computational Morphology and Phonology,0,None
W10-1729,Applying Morphological Decompositions to Statistical Machine Translation,2010,13,3,1,1,2696,sami virpioja,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"This paper describes the Aalto submission for the German-to-English and the Czech-to-English translation tasks of the ACL 2010 Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR. Statistical machine translation has focused on using words, and longer phrases constructed from words, as tokens in the system. In contrast, we apply different morphological decompositions of words using the unsupervised Morfessor algorithms. While translation models trained using the morphological decompositions did not improve the BLEU scores, we show that the Minimum Bayes Risk combination with a word-based translation model produces significant improvements for the German-to-English translation. However, we did not see improvements for the Czech-to-English translations."
vatanen-etal-2010-language,Language Identification of Short Text Segments with N-gram Models,2010,21,56,3,0,45984,tommi vatanen,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"There are many accurate methods for language identification of long text samples, but identification of very short strings still presents a challenge. This paper studies a language identification task, in which the test samples have only 5-21 characters. We compare two distinct methods that are well suited for this task: a naive Bayes classifier based on character n-gram models, and the ranking method by Cavnar and Trenkle (1994). For the n-gram models, we test several standard smoothing techniques, including the current state-of-the-art, the modified Kneser-Ney interpolation. Experiments are conducted with 281 languages using the Universal Declaration of Human Rights. Advanced language model smoothing techniques improve the identification accuracy and the respective classifiers outperform the ranking method. The higher accuracy is obtained at the cost of larger models and slower classification speed. However, there are several methods to reduce the size of an n-gram model, and our experiments with model pruning show that it provides an easy way to balance the size and the identification accuracy. We also compare the results to the language identifier in Google AJAX Language API, using a subset of 50 languages."
N09-5004,Morpho Challenge - Evaluation of algorithms for unsupervised learning of morphology in various tasks and languages,2009,8,0,2,0.769231,2635,mikko kurimo,"Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics, Companion Volume: Demonstration Session",0,"After the release of the open source software implementation of Morfessor algorithm, a series of several open evaluations has been organized for unsupervised morpheme analysis and morpheme-based speech recognition and information retrieval. The unsupervised morpheme analysis is a particularly attractive approach for speech and language technology for the morphologically complex languages. When the amount of distinct word forms becomes prohibitive for the construction of a sufficient lexicon, it is important that the words can be segmented into smaller meaningful language modeling units. In this presentation we will demonstrate the results of the evaluations, the baseline systems built using the open source tools, and invite research groups to participate in the next evaluation where the task is to enhance statistical machine translation by morpheme analysis."
N09-2019,Minimum {B}ayes Risk Combination of Translation Hypotheses from Alternative Morphological Decompositions,2009,24,32,2,0,23877,adria gispert,"Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers",0,We describe a simple strategy to achieve translation performance improvements by combining output from identical statistical machine translation systems trained on alternative morphological decompositions of the source language. Combination is done by means of Minimum Bayes Risk decoding over a shared N-best list. When translating into English from two highly inflected languages such as Arabic and Finnish we obtain significant improvements over simply selecting the best morphological decomposition.
E09-1019,Web Augmentation of Language Models for Continuous Speech Recognition of {SMS} Text Messages,2009,18,14,2,0,203,mathias creutz,Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009),0,"In this paper, we present an efficient query selection algorithm for the retrieval of web text data to augment a statistical language model (LM). The number of retrieved relevant documents is optimized with respect to the number of queries submitted.n n The querying scheme is applied in the domain of SMS text messages. Continuous speech recognition experiments are conducted on three languages: English, Spanish, and French. The web data is utilized for augmenting in-domain LMs in general and for adapting the LMs to a user-specific vocabulary. Word error rate reductions of up to 6.6% (in LM augmentation) and 26.0% (in LM adaptation) are obtained in setups, where the size of the web mixture LM is limited to the size of the baseline in-domain LM."
2007.mtsummit-papers.65,Morphology-aware statistical machine translation based on morphs induced in an unsupervised manner,2007,24,67,1,1,2696,sami virpioja,Proceedings of Machine Translation Summit XI: Papers,0,"In this paper, we apply a method of unsupervised morphology learning to a state-of-the-art phrase-based statistical ma chine translation (SMT) system. In SMT, words are traditionally used as the smallest units of translation. Such a system generalizes poorl y to word forms that do not occur in the training data. In particular, this is problematic for languages that are highly compounding, highly inflecting, or both. An alternative way is to use sub-word units, such as morphemes. We use the Morfessor algorithm to find statistical mo rphemelike units (called morphs) that can be used to reduce the size of the lexicon and improve the ability to generalize. Transl ation and language models are trained directly on morphs instead of words. The approach is tested on three Nordic languages (Danish, Finnish, and Swedish) that are included in the Europarl corpus consisting of the Proceedings of the European Parliament. However, in our experiments we did not obtain higher BLEU scores for the morph model than for the standard word-based approach. Nonetheless, the proposed morph-based solution has clear benefits, as morpho logically well motivated structures (phrases) are learned , and the proportion of words left untranslated is clearly reduced."
