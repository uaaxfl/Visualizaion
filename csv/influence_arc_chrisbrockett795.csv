2020.acl-demos.30,D17-2014,0,0.0333427,"transferring and controllable generation. It includes reinforcement learning capabilities along with its sequence modelling tools. DeepPavlov (Burtsev et al., 2018) is a popular framework focusing on task-oriented dialogue. This public repository contains several demos and pre-trained models for question answering and sentiment classification. Icecaps (Shiv et al., 2019) is a response generation toolkit with techniques such as grounding on personalities or external knowledge and multi-task training. The ConvAI2 challenge (Dinan et al., 2019) has a focus on personalized conversations. ParlAI (Miller et al., 2017) is another library for developing task-oriented dialogue systems. It contains pre-trained models for knowledge-grounded chatbot trained with crowdsourced data. The Text-to-Text Transformer (Raffel et al., 2019) unifies multiple text modeling tasks, and achieves the state-of-the-art results in various natural language generation and understanding benchmarks. User Bot Table 5: An interactive example of multi-turn dialogue Role Response User Bot what is the meaning of life ? The meaning is to be with your family and friends . I’m going to guess : It means that your parents and friends have loved"
2020.acl-demos.30,P02-1040,0,0.106794,"of 50,257 entries, and was trained on 16 Nvidia V100 machines with 4 https://github.com/mgalley/ DSTC7-End-to-End-Conversation-Modeling/ tree/master/evaluation 3 https://github.com/huggingface/ pytorch-transformers 272 other filtering criteria such as turn length, this yields a 5-reference test set of size 2208. (For each instance, one of the 6 human responses is set aside to assess human performance on this task.) Note that our training data is collected from a different time span from the test set. We performed automatic evaluation using standard machine translation metrics, including BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007), and NIST (Doddington, 2002). NIST is a variant of BLEU that weights n-gram matches by their information gain, i.e., it indirectly penalizes uninformative n-grams. We also use Entropy (Zhang et al., 2018) and Dist-n (Li et al., 2016a) to evaluate lexical diversity. More details are provided in Galley et al. (2019). We compared D IALO GPT with our in-house competitive sequence-to-sequence model P ER SONALITY C HAT based on (Li et al., 2016a) and trained on Twitter data, which has been used in production as a Cognitive Service for Microsoft Azure.5 Table 2 summ"
2020.acl-demos.30,N18-1202,0,0.0181606,"rning repository (Wolf et al., 2019) contains the code for training conversational AI systems with transfer learning based on the GPT-2 transformer language model, which achieves the state-of-the-art performance on ConvAI-2 dialogue competition. DLGnet (Olabiyi and Mueller, 2019) is a large transformer model trained on dialogue dataset and achieves good performance in multi-turn dialogue generation. AllenNLP (Gardner et al., 2018) is developed as a toolkit for many natural language processing tasks, including the large-scale pre-trained bi-LSTM sentence representation learning framework ELMo (Peters et al., 2018). Texar (Hu et al., 2018) focuses on text generation including style transferring and controllable generation. It includes reinforcement learning capabilities along with its sequence modelling tools. DeepPavlov (Burtsev et al., 2018) is a popular framework focusing on task-oriented dialogue. This public repository contains several demos and pre-trained models for question answering and sentiment classification. Icecaps (Shiv et al., 2019) is a response generation toolkit with techniques such as grounding on personalities or external knowledge and multi-task training. The ConvAI2 challenge (Din"
2020.acl-demos.30,P19-1539,1,0.811156,"t thus poses a greater one-to-many problem than is typical in other text generation tasks such as neural machine translation, text summarization and paraphrasing. Human conversations are also generally more informal, noisy, and, when in the form of textual chat, often contain informal abbreviations or syntactic/lexical errors. Most open-domain neural response generation systems suffer from content or style inconsistency (Li et al., 2016b; Zhang et al., 2019; Gao et al., 2019c), lack of long-term contextual information (Serban et al., 2017), and blandness (Li et al., 2016a; Zhang et al., 2018; Qin et al., 2019). While these issues can be alleviated by modelling strategies specifically designed to boost information content, a transformer-based architecture like GPT-2 (Radford et al., 2018), which uses a multi-layer self-attentive mechanism to allow fully-connected cross-attention to the full context in a computationally efficient manner, seems like a natural choice for exploring a more general solution. Transformer models, for example, allow long-term dependency information to be better be preserved across time (Radford et al., 2018), thereby improving content consistency. They also have higher model"
2020.acl-demos.30,N19-1125,1,0.926516,"set aside to assess human performance on this task.) Note that our training data is collected from a different time span from the test set. We performed automatic evaluation using standard machine translation metrics, including BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007), and NIST (Doddington, 2002). NIST is a variant of BLEU that weights n-gram matches by their information gain, i.e., it indirectly penalizes uninformative n-grams. We also use Entropy (Zhang et al., 2018) and Dist-n (Li et al., 2016a) to evaluate lexical diversity. More details are provided in Galley et al. (2019). We compared D IALO GPT with our in-house competitive sequence-to-sequence model P ER SONALITY C HAT based on (Li et al., 2016a) and trained on Twitter data, which has been used in production as a Cognitive Service for Microsoft Azure.5 Table 2 summarizes the automatic evaluation results. D IALO GPT with 345M parameters and beam search achieved the highest automatic score across most metrics. Scores for D IALO GPT with 345M parameters are better across the board than with 117M parameters. Beam search (with beam width 10) dramatically improves BLEU and DIST scores, and marginally improves NIST"
2020.acl-demos.30,D19-1190,1,0.865771,"an Microsoft Corporation, Redmond, WA, USA ∗ {yizzhang,siqi.sun,mgalley,yenchen,chrisbkt,xiag,jfgao,jingjl,billdol}@microsoft.com Abstract tion. Neural response generation is a subcategory of text-generation that shares the objective of generating natural-looking text (distinct from any training instance) that is relevant to the prompt. Modelling conversations, however, presents distinct challenges in that human dialogue, which encapsulates the possibly competing goals of two participants, is intrinsically more diverse in the range of potential responses (Li et al., 2016a; Zhang et al., 2018; Gao et al., 2019a,b). It thus poses a greater one-to-many problem than is typical in other text generation tasks such as neural machine translation, text summarization and paraphrasing. Human conversations are also generally more informal, noisy, and, when in the form of textual chat, often contain informal abbreviations or syntactic/lexical errors. Most open-domain neural response generation systems suffer from content or style inconsistency (Li et al., 2016b; Zhang et al., 2019; Gao et al., 2019c), lack of long-term contextual information (Serban et al., 2017), and blandness (Li et al., 2016a; Zhang et al.,"
2020.acl-demos.30,W18-2501,0,0.0119024,"lionaire and happy . There is a reason the rich have a lot of money There are several open-sourced toolkits for largescale pre-trained transformer models. Huggingface Conv-AI transfer learning repository (Wolf et al., 2019) contains the code for training conversational AI systems with transfer learning based on the GPT-2 transformer language model, which achieves the state-of-the-art performance on ConvAI-2 dialogue competition. DLGnet (Olabiyi and Mueller, 2019) is a large transformer model trained on dialogue dataset and achieves good performance in multi-turn dialogue generation. AllenNLP (Gardner et al., 2018) is developed as a toolkit for many natural language processing tasks, including the large-scale pre-trained bi-LSTM sentence representation learning framework ELMo (Peters et al., 2018). Texar (Hu et al., 2018) focuses on text generation including style transferring and controllable generation. It includes reinforcement learning capabilities along with its sequence modelling tools. DeepPavlov (Burtsev et al., 2018) is a popular framework focusing on task-oriented dialogue. This public repository contains several demos and pre-trained models for question answering and sentiment classification."
2020.acl-demos.30,P16-1162,0,0.0541096,"of masked multi-head selfattention layers to train on massive web-text data. The text generated either from scratch or based on a user-specific prompt is realistic-looking. The success of GPT-2 demonstrates that a transformer language model is able to characterize human language data distributions at a fine-grained level, presumably due to large large model capacity and superior efficiency. Our model inherits from GPT-2 (Radford et al., 2018), a 12-to-48 layer transformer with layer normalization, a initialization scheme that accounts for model depth that we modified, and byte pair encodings (Sennrich et al., 2016) for the tokenizer. We follow the OpenAI GPT-2 to model a multiturn dialogue session as a long text and frame the generation task as language modeling. We first concatenate all dialog turns within a dialogue session into a long text x1 , · · · , xN (N is the sequence length), ended by the end-of-text token. We denote the source sentence (dialogue history) as S = x1 , · · · , xm and target sentence (ground truth response) as T = xm+1 , · · · , xN , the conditional probability of P (T |S) can be written as the product of a series of conditional probabilities: Dataset The dataset is extracted fro"
2020.acl-demos.30,W19-5944,0,0.0533119,"the “ground truth” references that will be tested on, while R4 is the “heldout” human response that serves to compute a “human” score. In semantic space, a generated response Rg from a well-trained model will presumably tend to lie in the vicinity the geometric center R3: I will s end some one right away R4: I . s the perp e trato Rg: W r still hen w R1: W insid as th e? as an is bre ythin ak-in g sto ? len? R2: I s any one hurt or in jured ? Source: I would like to report a break-in. Figure 1: A generated response can surpass a human response in automatic metrics. Example responses are from Gupta et al. (2019) of all possible responses, because the training objective seeks to generate the most likely response. This may be close to the geometric mean of all training instances, thus “averaging out” these instances. Consequently, a generated response Rg might have a lower “semantic distance” (manifested in higher automatic scores like BLEU) from R1-R3 than the targeted human response R4. 4.3 A New Reddit Multi-reference Dataset We further evaluate D IALO GPT on a multireference test set with 6K examples. The results are shown in Table 3. We test our method on two settings: training from scratch and fi"
2020.acl-demos.30,W18-2503,0,0.0199459,", 2019) contains the code for training conversational AI systems with transfer learning based on the GPT-2 transformer language model, which achieves the state-of-the-art performance on ConvAI-2 dialogue competition. DLGnet (Olabiyi and Mueller, 2019) is a large transformer model trained on dialogue dataset and achieves good performance in multi-turn dialogue generation. AllenNLP (Gardner et al., 2018) is developed as a toolkit for many natural language processing tasks, including the large-scale pre-trained bi-LSTM sentence representation learning framework ELMo (Peters et al., 2018). Texar (Hu et al., 2018) focuses on text generation including style transferring and controllable generation. It includes reinforcement learning capabilities along with its sequence modelling tools. DeepPavlov (Burtsev et al., 2018) is a popular framework focusing on task-oriented dialogue. This public repository contains several demos and pre-trained models for question answering and sentiment classification. Icecaps (Shiv et al., 2019) is a response generation toolkit with techniques such as grounding on personalities or external knowledge and multi-task training. The ConvAI2 challenge (Dinan et al., 2019) has a fo"
2020.acl-main.440,E09-1003,0,0.0173744,"et al., 2019) consists of 11,827 videos of 180 tasks in 12 daily life domains. YouMakeup (Wang et al., 2019) consists of 2,800 YouTube videos, annotated with natural language descriptions for instructional steps, grounded in temporal video range and spatial facial areas. Leveraging Document Level Alignments Our work relies on the assumption that text recipes and instructional cooking videos of the same dish are comparable. This idea has been used to extract parallel sentences from comparable corpora to increase the number of training examples for machine translation (Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010; Gr´egoire and Langlais, 2018). Likewise, TalkSumm (Lev et al., 2019) use the transcripts of scientific conference talks to automatically extract summaries. Zhu et al. (2015) use books and movie adaptations of the books to extract descriptive explanations of movie scenes. Related Tasks A related task is localizing and classifying steps in instructional videos (Alayrac et al., 2016; Zhukov et al., 2019) where they detect when an action is performed in the video whereas we focus on describing actions. Dense event captioning of instructional videos (Zhou et al., 2018b; Li et"
2020.acl-main.440,J93-2003,0,0.169746,"Missing"
2020.acl-main.440,2021.ccl-1.108,0,0.0414258,"Missing"
2020.acl-main.440,D15-1166,0,0.0881191,"Missing"
2020.acl-main.440,N15-1015,0,0.0466122,"tions (or video transcript sentences) in the target recipe. We make two modifications to the alignment algorithm described above: First, our recipe pairs, unlike the wet lab protocol data, does not follow the same temporal sequence. The alignment algorithm must thus learn to jump within a longer range. We set the window of jump probabilities at [−2, 2].9 Second, we use transcriptions to learn alignments rather than the objects detected in videos. We hypothesize that the richness of language used in instructional videos may facilitate better alignment with transcripts (as others have observed (Malmaud et al., 2015; Sener et al., 2015)). We use all words (except stop words) in video transcript sentences and all words in text instructions while learning the IBM1 word level probabilities. An instruction in one recipe can be aligned to multiple instructions in the other recipe. 3.2 Joint Alignment among Multiple Recipes We use the pairwise alignments to derive a joint alignment at the dish level between multiple text and video recipes. For each dish, we construct a graph where each node represents an instruction from a text recipe or a transcript sentence from a video recipe. We use the pairwise alignments"
2020.acl-main.440,J05-4003,0,0.106277,"e knowledge, we describe our approach for constructing the M ICROSOFT R ESEARCH M ULTI MODAL A LIGNED R ECIPE C ORPUS . We first extract a large number of text and video recipes from the web. Our goal is to find joint alignments between multiple text recipes and multiple video recipes for the same dish (see Figure 2). The task is challenging, as different recipes vary in their order of instructions and use of ingredients. Moreover, video instructions can be noisy, and text and video instructions include different levels of specificity in their descriptions. Most previous alignment approaches (Munteanu and Marcu, 2005) deal with pairwise alignments. Since our goal is to align multiple instruction sets, we introduce a novel twostage unsupervised algorithm. In the first stage, we learn pairwise alignments between two text recipes, two video recipes, and between a text and a video recipe using an unsupervised alignment algorithm (§ 3.1). In the second stage, we use the pairwise alignments between all recipes within a dish to construct a graph for each dish and find a maximum spanning tree of this graph to derive joint alignments across multiple recipes (§3.2). We train our unsupervised algorithm on 4,262 dishe"
2020.acl-main.440,D14-1162,0,0.0873914,"on, we compute its similarity score with every target instruction and align it to the target instruction with the highest score. a. Exact word match: Given two instructions, we define exact word match as the ratio of the number of common words between the two divided by the number of words in the longer of the two. This gives us a measure of word match that is comparable across instructions of different lengths. Methods Random Uniform alignment BM25 retrieval Textual Similarity Exact word match TF-IDF GloVe BERT RoBERTa HMM+IBM1 Nouns Nouns+Verbs All words c. GloVe: We train GloVe embeddings (Pennington et al., 2014) on an in-domain corpus of 3 million words put together by combining text recipes and video transcriptions. Given an instruction, we average the GloVe embeddings (Pennington Recall 14.00 31.85 55.27 F1 12.69 33.22 49.30 53.90 52.78 56.04 50.72 52.49 48.39 46.82 51.89 55.07 55.86 46.98 45.12 50.30 49.10 50.44 62.11 64.72 66.21 48.99 50.76 52.42 50.73 52.97 54.55 Table 3: Results for text-text recipe alignment on Common Crawl dataset. et al., 2014) of nouns and verbs12 to obtain its embedding vector. Given two instructions, we define their embedding similarity as the cosine similarity of their e"
2020.acl-main.440,D19-1410,0,0.0133423,"nstruction, we average the GloVe embeddings (Pennington Recall 14.00 31.85 55.27 F1 12.69 33.22 49.30 53.90 52.78 56.04 50.72 52.49 48.39 46.82 51.89 55.07 55.86 46.98 45.12 50.30 49.10 50.44 62.11 64.72 66.21 48.99 50.76 52.42 50.73 52.97 54.55 Table 3: Results for text-text recipe alignment on Common Crawl dataset. et al., 2014) of nouns and verbs12 to obtain its embedding vector. Given two instructions, we define their embedding similarity as the cosine similarity of their embedding vectors. d. BERT: Given an instruction, we compute its embedding vector using BERT-based sentence embedding (Reimers and Gurevych, 2019). We experiment with different variants and find that the BERT-base model trained on AllNLI, then on STS benchmark training set13 performed the best for us. Given two instructions, we define their BERT similarity as the cosine similarity between their sentence embedding vectors. e. RoBERTa: We also experiment with a variant of the above baseline where we use RoBERTa (Liu et al., 2019) instead of BERT to compute the sentence embeddings. We use RoBERTa-large trained on AllNLI, then on STS benchmark training set. 4.3 b. TF-IDF: We use all the recipes in our training set to create a term frequency"
2020.acl-main.440,D19-1517,0,0.0287429,"ch larger scale dataset. Multi-modal Instructional Datasets Marin et al. (2019) introduce a corpus of 1 million cooking recipes paired with 13 million food images for the task of retrieving a recipe given an image. YouCook2 dataset (Zhou et al., 2018a) consists of 2,000 recipe videos with human written descriptions for each video segment. The How2 dataset (Sanabria et al., 2018) consists of 79,114 instructional videos with English subtitles and crowdsourced Portuguese translations. The COIN dataset (Tang et al., 2019) consists of 11,827 videos of 180 tasks in 12 daily life domains. YouMakeup (Wang et al., 2019) consists of 2,800 YouTube videos, annotated with natural language descriptions for instructional steps, grounded in temporal video range and spatial facial areas. Leveraging Document Level Alignments Our work relies on the assumption that text recipes and instructional cooking videos of the same dish are comparable. This idea has been used to extract parallel sentences from comparable corpora to increase the number of training examples for machine translation (Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010; Gr´egoire and Langlais, 2018). Likewise, TalkSumm (Lev et"
2020.acl-main.440,N10-1063,0,0.0144346,"27 videos of 180 tasks in 12 daily life domains. YouMakeup (Wang et al., 2019) consists of 2,800 YouTube videos, annotated with natural language descriptions for instructional steps, grounded in temporal video range and spatial facial areas. Leveraging Document Level Alignments Our work relies on the assumption that text recipes and instructional cooking videos of the same dish are comparable. This idea has been used to extract parallel sentences from comparable corpora to increase the number of training examples for machine translation (Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010; Gr´egoire and Langlais, 2018). Likewise, TalkSumm (Lev et al., 2019) use the transcripts of scientific conference talks to automatically extract summaries. Zhu et al. (2015) use books and movie adaptations of the books to extract descriptive explanations of movie scenes. Related Tasks A related task is localizing and classifying steps in instructional videos (Alayrac et al., 2016; Zhukov et al., 2019) where they detect when an action is performed in the video whereas we focus on describing actions. Dense event captioning of instructional videos (Zhou et al., 2018b; Li et al., 2018; Hessel et"
2020.acl-main.440,C96-2141,0,0.21939,"-fry for another minute. Add onion, garlic, peas, and carrots. You can use peas or whatever else you want to add in there. Figure 4: A maximum span tree for fried rice dish with text instructions and transcript segments as nodes, alignments as edges, and alignment probabilities as edge weights. Nodes representing text instructions are labeled “T”. Nodes representing transcript segments are labeled “V”. Each color indicates a different recipe. The bounding box shows a magnified section of the tree with edge weights and the instruction/transcript associated with each node. (HMM) (Rabiner, 1989; Vogel et al., 1996) to generate each video segment f (m) from one of the text sentences e(n) . They then use IBM1 model (Brown et al., 1993) emission probabilities to gen(m) (m) erate the blobs {f1 , ..., fJ } in f (m) from the (n) (n) nouns {e1 , ..., eI } in e(n) as follows: P (f (m) |e (n) J J  Y X (m) (n) p(fj |ei ) )= (I)J (1) j=1 i=1 The hidden state in the HMM model corresponds to the alignment between video segment and text sentence, and the state transition probabilities correspond to the jump between adjacent alignments. For computational tractability, a video segment can be aligned to only one senten"
2020.emnlp-main.28,2020.acl-demos.26,1,0.861152,"Missing"
2020.emnlp-main.28,N19-1125,1,0.881167,"Missing"
2020.emnlp-main.28,D19-1190,1,0.886305,"Missing"
2020.emnlp-main.28,W19-4101,0,0.0613694,"Missing"
2020.emnlp-main.28,2020.emnlp-main.378,1,0.843421,"ning dataset for response quality prediction. cannot reliably distinguish between human- and machine-generated responses. Though surprisingly effective, the training objective for these models is conceptually simple: minimizing the perplexity of a reference response for a given context. Introduction Conversing freely in natural language is one of the greatest challenges of artificial intelligence. Endto-end open-domain dialog systems have become increasingly powerful, with advanced model architectures and large-scale training (Zhang et al., 2019b; Adiwardana et al., 2020; Roller et al., 2020; Li et al., 2020). In some settings, human annotators However, a meaningful evaluation of response generation must take into account more than whether a generated turn is relevant in context, or whether it “sounds human.” Conventional neural conversation models often generate trivial or bland responses (Li et al., 2016; Zhao et al., 2017) that are relevant to context but are not engaging. Even human responses can vary dramatically in terms of tonal appropriateness and whether they are interesting enough to prompt a rich listener reaction. A successful dialog turn must be proactive, engaging, and consistent wit"
2020.emnlp-main.28,P18-1205,0,0.075349,"Large-scale training data is necessary because of the one-to-many nature of dialog and the scope and complexity of human conversation. However, labeling conversations at scale is too expensive and time-consuming for this purpose. Labeling the “engagingness” of a response is not something a single annotator can do; the task requires something more like a large-scale, collective vote. And yet there is no obvious automated substitute for this kind of human labeling. Conventional quality measurements such as reference-based similarity (Papineni et al., 2002) or lexical diversity (Li et al., 2016; Zhang et al., 2018b) capture only limited aspects of response quality, and are not strongly predictive of human reactions: simply because a response is different from others does not necesarily mean that it will be perceived as “bad”. Figure 2: The long-tailed distribution of the raw scores of feedback of Reddit.com. pairwise classification. Using a dataset of 133M pairs of human comments and their associated number of replies or up-/downvotes, we train a set of large-scale transformer-based feedback ranking models which outperform several baselines. In particular, dialog perplexity shows little predictive powe"
2020.emnlp-main.28,N16-1014,1,0.939356,"versing freely in natural language is one of the greatest challenges of artificial intelligence. Endto-end open-domain dialog systems have become increasingly powerful, with advanced model architectures and large-scale training (Zhang et al., 2019b; Adiwardana et al., 2020; Roller et al., 2020; Li et al., 2020). In some settings, human annotators However, a meaningful evaluation of response generation must take into account more than whether a generated turn is relevant in context, or whether it “sounds human.” Conventional neural conversation models often generate trivial or bland responses (Li et al., 2016; Zhao et al., 2017) that are relevant to context but are not engaging. Even human responses can vary dramatically in terms of tonal appropriateness and whether they are interesting enough to prompt a rich listener reaction. A successful dialog turn must be proactive, engaging, and consistent with social norms (Grice, 1975, 1989). 1 Dataset and models open-sourced on https:// github.com/golsun/DialogRPT In this work, we move beyond simple prediction of response relevance, augmenting this with a prediction of how likely a response is to elicit a 386 Proceedings of the 2020 Conference on Empiric"
2020.emnlp-main.28,I17-1099,0,0.0369889,"ationship between these models. 4.2 Human-like Classification Human-vs-Rand We first evaluate performance on the task of selecting the gold response from a set of random distractor responses. For each context, we randomly select n distractors. Performance is evaluated using Hits@k, which is the ratio of the number of gold responses in the top-k ranked hypotheses. Here, k is equal to the number of gold responses. Although D IALOG RPT is trained solely on Human-vs-Rand Reddit data, we show in Table 7 that it performs well even when compared to baseline models on other data sources: DailyDialog (Li et al., 2017) and Twitter5 PersonaChat6 (Zhang et al., 2018a). Such zero-shot performance indicate that the model generalize reasonably well on unseen datasets. For the Reddit dataset, which has multiple gold replies, we also compare our method with reference-based similarity measurements, 7 including BLEU (Papineni et al., 2002), BERTScore (Zhang et al., 2019a), and BLEURT (Sellam et al., 2020). These metrics are not applicable on-thefly, since references are not available, but they are commonly used as offline measures of dialog system quality. As shown in Table 7, although BLEU, BERTScore, and BLEURT ta"
2020.emnlp-main.28,P02-1040,0,0.113728,"world human preferences or feedback in an end-to-end fashion. Large-scale training data is necessary because of the one-to-many nature of dialog and the scope and complexity of human conversation. However, labeling conversations at scale is too expensive and time-consuming for this purpose. Labeling the “engagingness” of a response is not something a single annotator can do; the task requires something more like a large-scale, collective vote. And yet there is no obvious automated substitute for this kind of human labeling. Conventional quality measurements such as reference-based similarity (Papineni et al., 2002) or lexical diversity (Li et al., 2016; Zhang et al., 2018b) capture only limited aspects of response quality, and are not strongly predictive of human reactions: simply because a response is different from others does not necesarily mean that it will be perceived as “bad”. Figure 2: The long-tailed distribution of the raw scores of feedback of Reddit.com. pairwise classification. Using a dataset of 133M pairs of human comments and their associated number of replies or up-/downvotes, we train a set of large-scale transformer-based feedback ranking models which outperform several baselines. In"
2020.emnlp-main.28,2020.acl-main.704,0,0.270005,"representative negative modes: retrieval and generative dialog model generation. For the former we simply construct negative examples by randomly sampling from the training data. For the latter we use DialoGPT with top-k decoding. Since DialoGPT is able to produce human-like responses in certain evaluation settings, we select only 5.3 M highly-rated human response as positive examples, instead of using all human responses. Note that our method can be extended to include other negative modes such as perturbations and excessive repetition, similar to the synthetic example creation using BLEURT (Sellam et al., 2020). 3.4 Baselines (MMI) between the response and context. We use DialoGPT and its reverse model to compute ppl. BM25 This classic metric measures keywords similarity (Robertson and Zaragoza, 2009). We use the inner product of the context BM25 vector and candidate response BM25 vector to rank candidates, similar to (Henderson et al., 2019a). ConveRT (Henderson et al., 2019b) is a transformer-based model pretrained on Reddit data. It encodes context and candidate as vectors and compute their inner product as similarity used for ranking, achieved the existing state-of-the-art performance on several"
2020.emnlp-main.28,P17-1061,0,0.0303204,"natural language is one of the greatest challenges of artificial intelligence. Endto-end open-domain dialog systems have become increasingly powerful, with advanced model architectures and large-scale training (Zhang et al., 2019b; Adiwardana et al., 2020; Roller et al., 2020; Li et al., 2020). In some settings, human annotators However, a meaningful evaluation of response generation must take into account more than whether a generated turn is relevant in context, or whether it “sounds human.” Conventional neural conversation models often generate trivial or bland responses (Li et al., 2016; Zhao et al., 2017) that are relevant to context but are not engaging. Even human responses can vary dramatically in terms of tonal appropriateness and whether they are interesting enough to prompt a rich listener reaction. A successful dialog turn must be proactive, engaging, and consistent with social norms (Grice, 1975, 1989). 1 Dataset and models open-sourced on https:// github.com/golsun/DialogRPT In this work, we move beyond simple prediction of response relevance, augmenting this with a prediction of how likely a response is to elicit a 386 Proceedings of the 2020 Conference on Empirical Methods in Natura"
2020.emnlp-main.698,C16-1316,0,0.0401239,"mic time complexity during inference time. Experimental results on both News and Yelp datasets demonstrate that P OINTER achieves state-of-the-art performance on constrained text generation. We released the pre-trained models and the source code to facilitate future research 2 . 1 Introduction Real-world editorial assistant applications must often generate text under specified lexical constraints, for example, convert a meeting note with key phrases into a concrete meeting summary, recast a user-input search query as a fluent sentence, generate a conversational response using grounding facts (Mou et al., 2016), or create a story using a pre-specified set of keywords (Fan et al., 2018; Yao et al., 2019; Donahue et al., 2020). ∗ These authors contributed equally to this work. Work was done while Guoyin was at Microsoft. 1 PrOgressive INsertion-based TransformER 2 https://github.com/dreasysnail/POINTER † Generating text under specific lexical constraints is challenging. Constrained text generation broadly falls into two categories, depending on whether inclusion of specified keywords in the output is mandatory. In soft-constrained generation (Qin et al., 2019; Tang et al., 2019), keyword-text pairs ar"
2020.emnlp-main.698,P02-1040,0,0.106104,"Missing"
2020.emnlp-main.698,N18-1119,0,0.0774977,"cted (sometimes along with other conditioning information), and a conditional text generation model is trained to capture their co-occurrence, so that the model learns to incorporate the constrained keywords into the generated text. While soft-constrained models are easy to design, even remedied by soft enforcing algorithms such as attention and copy mechanisms (Bahdanau et al., 2015; Gu et al., 2016; Chen et al., 2019a), keywords are still apt to be lost during generation, especially when multiple weakly correlated keywords must be included. Hard-constrained generation (Hokamp and Liu, 2017; Post and Vilar, 2018; Hu et al., 2019; Miao et al., 2019; Welleck et al., 2019), on the other hand, requires that all the lexical constraints be present in the output sentence. This approach typically involves sophisticated design of network architectures. Hokamp and Liu (2017) construct a lexical-constrained grid beam search decoding algorithm to incorporate constraints. However, Hu et al. (2019) observe that a naive implementation of this algorithm has a high running time complexity. Miao et al. (2019) introduces a sampling-based conditional generation method, where the constraints are first placed in a templat"
2020.emnlp-main.698,P19-1539,1,0.871309,"rsational response using grounding facts (Mou et al., 2016), or create a story using a pre-specified set of keywords (Fan et al., 2018; Yao et al., 2019; Donahue et al., 2020). ∗ These authors contributed equally to this work. Work was done while Guoyin was at Microsoft. 1 PrOgressive INsertion-based TransformER 2 https://github.com/dreasysnail/POINTER † Generating text under specific lexical constraints is challenging. Constrained text generation broadly falls into two categories, depending on whether inclusion of specified keywords in the output is mandatory. In soft-constrained generation (Qin et al., 2019; Tang et al., 2019), keyword-text pairs are typically first constructed (sometimes along with other conditioning information), and a conditional text generation model is trained to capture their co-occurrence, so that the model learns to incorporate the constrained keywords into the generated text. While soft-constrained models are easy to design, even remedied by soft enforcing algorithms such as attention and copy mechanisms (Bahdanau et al., 2015; Gu et al., 2016; Chen et al., 2019a), keywords are still apt to be lost during generation, especially when multiple weakly correlated keywords m"
2020.emnlp-main.698,2020.acl-demos.30,1,0.798531,"h as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), XLNet (Yang et al., 2019), Text-to-text Transformer (Raffel et al., 2019) and ELECTRA (Clark et al., 2020), have achieved great success on natural language understanding benchmarks. GPT2 (Radford et al., 2018) first demonstrates great potential for leveraging Transformer models in generating realistic text. MASS (Song et al., 2019) and BART (Lewis et al., 2019) propose methods for sequence-to-sequence pre-training. UniLM (Dong et al., 2019) unifies the generation and understanding tasks within a single pre-training scheme. DialoGPT (Zhang et al., 2020) and MEENA (Adiwardana et al., 2020) focus on opendomain conversations. CTRL (Keskar et al., 2019) and Grover (Zellers et al., 2019) guide text generation with pre-defined control codes. In addition, recent work has also investigated how to leverage BERT for conditional text generation (Chen et al., 2019b; Mansimov et al., 2019; Li et al., 2020). To the best of our knowledge, ours is the first largescale pre-training work for hard-constrained text generation. Non-autoregressive Generation Many attempts have been made to use non-autoregressive models for text generation tasks. For neural machin"
2020.emnlp-main.698,W19-3620,0,0.065774,"Missing"
2020.emnlp-main.698,W07-0734,0,\N,Missing
2020.emnlp-main.698,P16-1154,0,\N,Missing
2020.emnlp-main.698,N19-1090,0,\N,Missing
2020.emnlp-main.698,P19-1565,0,\N,Missing
2020.emnlp-main.698,N19-1423,0,\N,Missing
2020.emnlp-main.698,2020.findings-emnlp.17,1,\N,Missing
2020.emnlp-main.698,2020.emnlp-main.378,1,\N,Missing
2020.emnlp-main.698,N16-1014,1,\N,Missing
2020.emnlp-main.698,D18-1149,0,\N,Missing
2021.findings-acl.185,W19-4412,0,0.149057,". While automated document-level generation seems tantalizingly within reach, a high branching factor presents significant challenges in tailoring generated documents to the specific requirements of users. Topic drift and “hallucination” of information are endemic to these models (Wiseman et al., 2017). These risks have ensured that end-user applications involving text generation (e.g., Smart Compose, Smart Reply, Grammarly) still require a human to remain in control of content and are restricted to individual sentences or even smaller segments of text (Chen et al., 2019; Kannan et al., 2016; Alikaniotis and Raheja, 2019; Prabhumoye et al., 2019; Faltings et al., 2021). Can large generative language models be used to assist user writing at the document level while the user still controls the factual content? A possible answer lies in the observation that a substantial portion of day-to-day writing involves some form of reuse. Similar documents (e.g., monthly reports, sales letters, job descriptions) are effectively recycled by changing those segments that need to be modified (Fig. 1).1 Moreover, documents containing analogous texts are often found collocated in repositories, a common practice for organization"
2021.findings-acl.185,alonso-etal-2004-multiple,0,0.262173,"Missing"
2021.findings-acl.185,N03-1003,0,0.487908,"et) editing. Oya et al. (2014); Magooda and Litman (2019); Yang et al. (2020); Li et al. (2018) convert each single input text into a template with blanks using rule-based methods. Other work such as (Wiseman et al., 2018) and (Gangadharaiah and Narayanaswamy, 2020) relies on a knowledge base or a domain/task specific ontology to segment text sequences into templates. 2.3 Multi-Sequence Processing Multiple Sequence Alignment (MSA), widely used in the biological domain (Sauder et al., 2000) to align multiple biological sequences like proteins, has long been leveraged for text pattern matching (Barzilay and Lee, 2003; Alonso et al., 2004). We adopt this method to align input documents and create heuristic templates as weak supervision. Other tasks taking multiple text sequences as input include multi-document summarization, which seeks to generate an abstractive text summary of multiple input documents (Liu and Lapata, 2019; Chu and Liu, 2019), and multi-source machine translation (Nishimura et al., 2018; Garmash and Monz, 2016) that encodes input texts in multiple source languages and translates them into a target language. Cho et al. (2021) generate a question from input documents by applying a multi-en"
2021.findings-acl.185,D19-1195,0,0.0127106,"derived from one or two documents. 4 We release our data and source code for dataset construction and experiments at https://github.com/ ellenmellon/document_sketching. 2.2 Template-Based Generation Some existing work induces templates as an intermediate step for performing tasks like text summarization or response generation. Most use a retrieval-based method to extract similar references from the training corpus as prototypes (Cao et al., 2018; Yang et al., 2019; Wang et al., 2019; Gao et al., 2019; Peng et al., 2019), and learn to separate salient information and latent template structure. Cai et al. (2019) induce an intermediate template for response generation explicitly, but from a single retrieved relevant response. Similar prototype editing work (Guu et al., 2018; Hashimoto et al., 2018; Fabbri et al., 2020) focuses on short text (e.g., a question or a single sentence) or structured output (e.g., code snippet) editing. Oya et al. (2014); Magooda and Litman (2019); Yang et al. (2020); Li et al. (2018) convert each single input text into a template with blanks using rule-based methods. Other work such as (Wiseman et al., 2018) and (Gangadharaiah and Narayanaswamy, 2020) relies on a knowledge"
2021.findings-acl.185,P18-1015,0,0.0196927,"structed from a set of similar documents. 3 Our experiments in Section 6 also show that drafts derived from multiple analogous documents are more effective than those derived from one or two documents. 4 We release our data and source code for dataset construction and experiments at https://github.com/ ellenmellon/document_sketching. 2.2 Template-Based Generation Some existing work induces templates as an intermediate step for performing tasks like text summarization or response generation. Most use a retrieval-based method to extract similar references from the training corpus as prototypes (Cao et al., 2018; Yang et al., 2019; Wang et al., 2019; Gao et al., 2019; Peng et al., 2019), and learn to separate salient information and latent template structure. Cai et al. (2019) induce an intermediate template for response generation explicitly, but from a single retrieved relevant response. Similar prototype editing work (Guu et al., 2018; Hashimoto et al., 2018; Fabbri et al., 2020) focuses on short text (e.g., a question or a single sentence) or structured output (e.g., code snippet) editing. Oya et al. (2014); Magooda and Litman (2019); Yang et al. (2020); Li et al. (2018) convert each single input"
2021.findings-acl.185,N18-1150,0,0.14002,"ent learning (RL) to further improve generation quality. For each training example with input X, we generate a sequence sˆ, which is sampled from the probability distribution at each time step, p(ˆ st |ˆ s1 ...ˆ st−1 , X). We observe that directly optimizing the evaluation function proposed in Eq. (2) at the sequence level, using a vanilla policy gradient (PG) or a self-critical sequence training (TD-SCST) algorithm (Rennie et al., 2017; 2105 Paulus et al., 2018; Pasunuru and Bansal, 2018), can lead to instability during training as the reward cannot be calculated until the end of generation (Celikyilmaz et al., 2018). Therefore, we instead use a token-level incremental reward that is based on the change to the original reward function r(ˆ s, Y ) = score(ˆ s, Y ) from each sampled token sˆt , given references Y : rt (ˆ st , Y ) = r(ˆ s1...t , Y ) − r(ˆ s1...t−1 , Y ). (3) The training objective can be written as: LRL = T X −rt (sˆt , Y )p(ˆ st |ˆ s1 ...ˆ st−1 , X) (4) t=1 where T = |ˆ s|. Since optimizing RL loss alone runs the risk of compromising the language model (Paulus et al., 2018; Pasunuru and Bansal, 2017), we use a mixed loss as follows: LMIX = λLRL + (1 − λ)LMLE (5) where λ is a hyperparameter t"
2021.findings-acl.185,W19-2401,1,0.921705,"rzindar, 2010), we define an automatic evaluation metric based on Word Error Rate (Snover et al., 2006; Tom´as et al., 2003). We compare against strong baseline models including a mixture of experts model and a reinforcement learning approach designed to handle multi-source inputs and a weak supervision setting. Finally, we provide experimental analysis of these models, using automated and human evaluation studies. 2 2.1 Related Work Document Generation Recent work leverages the success of large pretrained language models to generate long texts such as stories (Rashkin et al., 2020), reviews (Cho et al., 2019a,b) and fake news (Zellers et al., 2019). Most end-user applications for assisting user writing, however, are confined to sentence-level generation (Chen et al., 2019; Kannan et al., 2016; Alikaniotis and Raheja, 2019; Prabhumoye et al., 2019; Faltings et al., 2021). Our work focuses on document-level writing assistance in which a document sketch is constructed from a set of similar documents. 3 Our experiments in Section 6 also show that drafts derived from multiple analogous documents are more effective than those derived from one or two documents. 4 We release our data and source code for"
2021.findings-acl.185,2021.eacl-main.2,1,0.72692,"oteins, has long been leveraged for text pattern matching (Barzilay and Lee, 2003; Alonso et al., 2004). We adopt this method to align input documents and create heuristic templates as weak supervision. Other tasks taking multiple text sequences as input include multi-document summarization, which seeks to generate an abstractive text summary of multiple input documents (Liu and Lapata, 2019; Chu and Liu, 2019), and multi-source machine translation (Nishimura et al., 2018; Garmash and Monz, 2016) that encodes input texts in multiple source languages and translates them into a target language. Cho et al. (2021) generate a question from input documents by applying a multi-encoder model with a transformer-based coordinator. 3 Problem Definition We introduce the task of document sketching, which aims to facilitate the authoring process by generating a template-like document draft, based on a collection of sampled similar documents. Formally, the task can be defined as follow: given a set of n documents X = {x1 , x2 , ..., xn }, generate 2103 a text sequence s that can be used as the sketch to reduce the human effort involved in composing a target document y. Evaluation Metrics As in most text generatio"
2021.findings-acl.185,2020.acl-main.413,0,0.0245581,"sting work induces templates as an intermediate step for performing tasks like text summarization or response generation. Most use a retrieval-based method to extract similar references from the training corpus as prototypes (Cao et al., 2018; Yang et al., 2019; Wang et al., 2019; Gao et al., 2019; Peng et al., 2019), and learn to separate salient information and latent template structure. Cai et al. (2019) induce an intermediate template for response generation explicitly, but from a single retrieved relevant response. Similar prototype editing work (Guu et al., 2018; Hashimoto et al., 2018; Fabbri et al., 2020) focuses on short text (e.g., a question or a single sentence) or structured output (e.g., code snippet) editing. Oya et al. (2014); Magooda and Litman (2019); Yang et al. (2020); Li et al. (2018) convert each single input text into a template with blanks using rule-based methods. Other work such as (Wiseman et al., 2018) and (Gangadharaiah and Narayanaswamy, 2020) relies on a knowledge base or a domain/task specific ontology to segment text sequences into templates. 2.3 Multi-Sequence Processing Multiple Sequence Alignment (MSA), widely used in the biological domain (Sauder et al., 2000) to a"
2021.findings-acl.185,2021.naacl-main.414,1,0.885569,"lizingly within reach, a high branching factor presents significant challenges in tailoring generated documents to the specific requirements of users. Topic drift and “hallucination” of information are endemic to these models (Wiseman et al., 2017). These risks have ensured that end-user applications involving text generation (e.g., Smart Compose, Smart Reply, Grammarly) still require a human to remain in control of content and are restricted to individual sentences or even smaller segments of text (Chen et al., 2019; Kannan et al., 2016; Alikaniotis and Raheja, 2019; Prabhumoye et al., 2019; Faltings et al., 2021). Can large generative language models be used to assist user writing at the document level while the user still controls the factual content? A possible answer lies in the observation that a substantial portion of day-to-day writing involves some form of reuse. Similar documents (e.g., monthly reports, sales letters, job descriptions) are effectively recycled by changing those segments that need to be modified (Fig. 1).1 Moreover, documents containing analogous texts are often found collocated in repositories, a common practice for organizations that manage professional documents.2 The high b"
2021.findings-acl.185,2020.acl-main.186,0,0.0167638,"atent template structure. Cai et al. (2019) induce an intermediate template for response generation explicitly, but from a single retrieved relevant response. Similar prototype editing work (Guu et al., 2018; Hashimoto et al., 2018; Fabbri et al., 2020) focuses on short text (e.g., a question or a single sentence) or structured output (e.g., code snippet) editing. Oya et al. (2014); Magooda and Litman (2019); Yang et al. (2020); Li et al. (2018) convert each single input text into a template with blanks using rule-based methods. Other work such as (Wiseman et al., 2018) and (Gangadharaiah and Narayanaswamy, 2020) relies on a knowledge base or a domain/task specific ontology to segment text sequences into templates. 2.3 Multi-Sequence Processing Multiple Sequence Alignment (MSA), widely used in the biological domain (Sauder et al., 2000) to align multiple biological sequences like proteins, has long been leveraged for text pattern matching (Barzilay and Lee, 2003; Alonso et al., 2004). We adopt this method to align input documents and create heuristic templates as weak supervision. Other tasks taking multiple text sequences as input include multi-document summarization, which seeks to generate an abstr"
2021.findings-acl.185,D19-1388,0,0.0143741,"nts in Section 6 also show that drafts derived from multiple analogous documents are more effective than those derived from one or two documents. 4 We release our data and source code for dataset construction and experiments at https://github.com/ ellenmellon/document_sketching. 2.2 Template-Based Generation Some existing work induces templates as an intermediate step for performing tasks like text summarization or response generation. Most use a retrieval-based method to extract similar references from the training corpus as prototypes (Cao et al., 2018; Yang et al., 2019; Wang et al., 2019; Gao et al., 2019; Peng et al., 2019), and learn to separate salient information and latent template structure. Cai et al. (2019) induce an intermediate template for response generation explicitly, but from a single retrieved relevant response. Similar prototype editing work (Guu et al., 2018; Hashimoto et al., 2018; Fabbri et al., 2020) focuses on short text (e.g., a question or a single sentence) or structured output (e.g., code snippet) editing. Oya et al. (2014); Magooda and Litman (2019); Yang et al. (2020); Li et al. (2018) convert each single input text into a template with blanks using rule-based metho"
2021.findings-acl.185,C16-1133,0,0.0117745,"Alignment (MSA), widely used in the biological domain (Sauder et al., 2000) to align multiple biological sequences like proteins, has long been leveraged for text pattern matching (Barzilay and Lee, 2003; Alonso et al., 2004). We adopt this method to align input documents and create heuristic templates as weak supervision. Other tasks taking multiple text sequences as input include multi-document summarization, which seeks to generate an abstractive text summary of multiple input documents (Liu and Lapata, 2019; Chu and Liu, 2019), and multi-source machine translation (Nishimura et al., 2018; Garmash and Monz, 2016) that encodes input texts in multiple source languages and translates them into a target language. Cho et al. (2021) generate a question from input documents by applying a multi-encoder model with a transformer-based coordinator. 3 Problem Definition We introduce the task of document sketching, which aims to facilitate the authoring process by generating a template-like document draft, based on a collection of sampled similar documents. Formally, the task can be defined as follow: given a set of n documents X = {x1 , x2 , ..., xn }, generate 2103 a text sequence s that can be used as the sketc"
2021.findings-acl.185,Q18-1031,0,0.0221217,"ng. 2.2 Template-Based Generation Some existing work induces templates as an intermediate step for performing tasks like text summarization or response generation. Most use a retrieval-based method to extract similar references from the training corpus as prototypes (Cao et al., 2018; Yang et al., 2019; Wang et al., 2019; Gao et al., 2019; Peng et al., 2019), and learn to separate salient information and latent template structure. Cai et al. (2019) induce an intermediate template for response generation explicitly, but from a single retrieved relevant response. Similar prototype editing work (Guu et al., 2018; Hashimoto et al., 2018; Fabbri et al., 2020) focuses on short text (e.g., a question or a single sentence) or structured output (e.g., code snippet) editing. Oya et al. (2014); Magooda and Litman (2019); Yang et al. (2020); Li et al. (2018) convert each single input text into a template with blanks using rule-based methods. Other work such as (Wiseman et al., 2018) and (Gangadharaiah and Narayanaswamy, 2020) relies on a knowledge base or a domain/task specific ontology to segment text sequences into templates. 2.3 Multi-Sequence Processing Multiple Sequence Alignment (MSA), widely used in th"
2021.findings-acl.185,K15-1007,0,0.035958,"Missing"
2021.findings-acl.185,2020.emnlp-main.622,0,0.0221967,"Missing"
2021.findings-acl.185,2020.acl-main.703,0,0.0117555,"irs: (x1 , s), (x2 , s), ..., (xn , s) from a data instance. This is followed by training a complete MoE model as described in Section 5. We use greedy beam search as the decoding strategy for all generative models with a beam size of 4 (the default value in T5base). We observe that consecutive ellipses and uninformative tokens hurt readability. To improve the readability of output sketches, we apply minor post-processing to all models in our experiments: we merge consecutive ellipses if all tokens between 5 Note that T5 can be easily replaced by Other pre-trained generative models like BART (Lewis et al., 2020) in our model framework. them are punctuation or among the 30 most frequent tokens.6 Sentence-terminating periods are excepted. Training and Parameters T5 has about 220 million parameters and MoE has about 310 million parameters in total. The average training time is about 20 hours for T5 and about 30 hours for MoE on a single Tesla V100 node (32GB) with 3 epochs. It takes an additional 10 hours to train a single expert in MoE. For MoE+RL, we use 4 V100 nodes and it takes about 2 days for training. During training, we tune batch size, learning rate and warm-up steps for T5. For MoE and MoE+RL,"
2021.findings-acl.185,N18-1169,0,0.0303707,"Missing"
2021.findings-acl.185,P19-1500,0,0.0250568,"ific ontology to segment text sequences into templates. 2.3 Multi-Sequence Processing Multiple Sequence Alignment (MSA), widely used in the biological domain (Sauder et al., 2000) to align multiple biological sequences like proteins, has long been leveraged for text pattern matching (Barzilay and Lee, 2003; Alonso et al., 2004). We adopt this method to align input documents and create heuristic templates as weak supervision. Other tasks taking multiple text sequences as input include multi-document summarization, which seeks to generate an abstractive text summary of multiple input documents (Liu and Lapata, 2019; Chu and Liu, 2019), and multi-source machine translation (Nishimura et al., 2018; Garmash and Monz, 2016) that encodes input texts in multiple source languages and translates them into a target language. Cho et al. (2021) generate a question from input documents by applying a multi-encoder model with a transformer-based coordinator. 3 Problem Definition We introduce the task of document sketching, which aims to facilitate the authoring process by generating a template-like document draft, based on a collection of sampled similar documents. Formally, the task can be defined as follow: given a"
2021.findings-acl.185,2020.acl-main.173,0,0.0345364,"Missing"
2021.findings-acl.185,W18-6322,0,0.0129935,"t documents. MoE based models are much more robust to low input similarity compared to the baselines. For the group with the most similar input documents, consensus-MSA gives the best score, while MoE based models yield much better performance in the other two groups. Human Evaluation In order to avoid bias in human judgments, we control possible confounding factors including sequence length and the number of ellipses in a sequence during decoding (Nakov et al., 2012; Guzm´an et al., 2015). We apply a tuneable penalty for each confounding factor (at inference time only) for each neural model (Murray and Chiang, 2018) to generate examples for human evaluation, such that the average output length and number of ellipses in each sequence from all neural systems compared in Tab. 4 are almost the 2108 same.8 We notice that such normalization does not affect the automatic evaluation score of each system much and the system ranks do not change. Human evaluation was conducted using crowdsourced workers. Judges were presented with paired randomized outputs and target documents, and were instructed to choose their preference for a starting point for writing the target document in order to save editing time.9 Judgmen"
2021.findings-acl.185,C12-1121,0,0.0203556,"ntain a lot of hallucinated content, an issue that is even more severe when there is little overlapping structure or factual content among the input documents. MoE based models are much more robust to low input similarity compared to the baselines. For the group with the most similar input documents, consensus-MSA gives the best score, while MoE based models yield much better performance in the other two groups. Human Evaluation In order to avoid bias in human judgments, we control possible confounding factors including sequence length and the number of ellipses in a sequence during decoding (Nakov et al., 2012; Guzm´an et al., 2015). We apply a tuneable penalty for each confounding factor (at inference time only) for each neural model (Murray and Chiang, 2018) to generate examples for human evaluation, such that the average output length and number of ellipses in each sequence from all neural systems compared in Tab. 4 are almost the 2108 same.8 We notice that such normalization does not affect the automatic evaluation score of each system much and the system ranks do not change. Human evaluation was conducted using crowdsourced workers. Judges were presented with paired randomized outputs and targ"
2021.findings-acl.185,W18-2711,0,0.0120534,"ssing Multiple Sequence Alignment (MSA), widely used in the biological domain (Sauder et al., 2000) to align multiple biological sequences like proteins, has long been leveraged for text pattern matching (Barzilay and Lee, 2003; Alonso et al., 2004). We adopt this method to align input documents and create heuristic templates as weak supervision. Other tasks taking multiple text sequences as input include multi-document summarization, which seeks to generate an abstractive text summary of multiple input documents (Liu and Lapata, 2019; Chu and Liu, 2019), and multi-source machine translation (Nishimura et al., 2018; Garmash and Monz, 2016) that encodes input texts in multiple source languages and translates them into a target language. Cho et al. (2021) generate a question from input documents by applying a multi-encoder model with a transformer-based coordinator. 3 Problem Definition We introduce the task of document sketching, which aims to facilitate the authoring process by generating a template-like document draft, based on a collection of sampled similar documents. Formally, the task can be defined as follow: given a set of n documents X = {x1 , x2 , ..., xn }, generate 2103 a text sequence s that"
2021.findings-acl.185,W14-4407,0,0.0165642,"etrieval-based method to extract similar references from the training corpus as prototypes (Cao et al., 2018; Yang et al., 2019; Wang et al., 2019; Gao et al., 2019; Peng et al., 2019), and learn to separate salient information and latent template structure. Cai et al. (2019) induce an intermediate template for response generation explicitly, but from a single retrieved relevant response. Similar prototype editing work (Guu et al., 2018; Hashimoto et al., 2018; Fabbri et al., 2020) focuses on short text (e.g., a question or a single sentence) or structured output (e.g., code snippet) editing. Oya et al. (2014); Magooda and Litman (2019); Yang et al. (2020); Li et al. (2018) convert each single input text into a template with blanks using rule-based methods. Other work such as (Wiseman et al., 2018) and (Gangadharaiah and Narayanaswamy, 2020) relies on a knowledge base or a domain/task specific ontology to segment text sequences into templates. 2.3 Multi-Sequence Processing Multiple Sequence Alignment (MSA), widely used in the biological domain (Sauder et al., 2000) to align multiple biological sequences like proteins, has long been leveraged for text pattern matching (Barzilay and Lee, 2003; Alonso"
2021.findings-acl.185,D17-1103,0,0.0139213,"o instability during training as the reward cannot be calculated until the end of generation (Celikyilmaz et al., 2018). Therefore, we instead use a token-level incremental reward that is based on the change to the original reward function r(ˆ s, Y ) = score(ˆ s, Y ) from each sampled token sˆt , given references Y : rt (ˆ st , Y ) = r(ˆ s1...t , Y ) − r(ˆ s1...t−1 , Y ). (3) The training objective can be written as: LRL = T X −rt (sˆt , Y )p(ˆ st |ˆ s1 ...ˆ st−1 , X) (4) t=1 where T = |ˆ s|. Since optimizing RL loss alone runs the risk of compromising the language model (Paulus et al., 2018; Pasunuru and Bansal, 2017), we use a mixed loss as follows: LMIX = λLRL + (1 − λ)LMLE (5) where λ is a hyperparameter to be tuned. 6 Experiments 6.1 Setup To leverage the recent success in such transformerbased generation models, neural generation models in our experiments are initialized with the base version of T5 (Raffel et al., 2019; Wolf et al., 2020), an encoder-decoder architecture pre-trained on a variety of text-to-text tasks.5 All hyperparameters are tuned on the validation set. For MoE, we first fine-tune T5-base to obtain a “single expert” model to initialize each individual component model of the MoE. The"
2021.findings-acl.185,N18-2102,0,0.0606828,"= ni=1 wit πit , where πt is the final distribution for generation at timestamp t. 5.2 Reinforcement Learning We leverage reinforcement learning (RL) to further improve generation quality. For each training example with input X, we generate a sequence sˆ, which is sampled from the probability distribution at each time step, p(ˆ st |ˆ s1 ...ˆ st−1 , X). We observe that directly optimizing the evaluation function proposed in Eq. (2) at the sequence level, using a vanilla policy gradient (PG) or a self-critical sequence training (TD-SCST) algorithm (Rennie et al., 2017; 2105 Paulus et al., 2018; Pasunuru and Bansal, 2018), can lead to instability during training as the reward cannot be calculated until the end of generation (Celikyilmaz et al., 2018). Therefore, we instead use a token-level incremental reward that is based on the change to the original reward function r(ˆ s, Y ) = score(ˆ s, Y ) from each sampled token sˆt , given references Y : rt (ˆ st , Y ) = r(ˆ s1...t , Y ) − r(ˆ s1...t−1 , Y ). (3) The training objective can be written as: LRL = T X −rt (sˆt , Y )p(ˆ st |ˆ s1 ...ˆ st−1 , X) (4) t=1 where T = |ˆ s|. Since optimizing RL loss alone runs the risk of compromising the language model (Paulus et"
2021.findings-acl.185,2020.acl-main.396,0,0.0471883,"Missing"
2021.findings-acl.185,N19-1263,0,0.0218549,"lso show that drafts derived from multiple analogous documents are more effective than those derived from one or two documents. 4 We release our data and source code for dataset construction and experiments at https://github.com/ ellenmellon/document_sketching. 2.2 Template-Based Generation Some existing work induces templates as an intermediate step for performing tasks like text summarization or response generation. Most use a retrieval-based method to extract similar references from the training corpus as prototypes (Cao et al., 2018; Yang et al., 2019; Wang et al., 2019; Gao et al., 2019; Peng et al., 2019), and learn to separate salient information and latent template structure. Cai et al. (2019) induce an intermediate template for response generation explicitly, but from a single retrieved relevant response. Similar prototype editing work (Guu et al., 2018; Hashimoto et al., 2018; Fabbri et al., 2020) focuses on short text (e.g., a question or a single sentence) or structured output (e.g., code snippet) editing. Oya et al. (2014); Magooda and Litman (2019); Yang et al. (2020); Li et al. (2018) convert each single input text into a template with blanks using rule-based methods. Other work such"
2021.findings-acl.185,N19-1269,1,0.928824,"el generation seems tantalizingly within reach, a high branching factor presents significant challenges in tailoring generated documents to the specific requirements of users. Topic drift and “hallucination” of information are endemic to these models (Wiseman et al., 2017). These risks have ensured that end-user applications involving text generation (e.g., Smart Compose, Smart Reply, Grammarly) still require a human to remain in control of content and are restricted to individual sentences or even smaller segments of text (Chen et al., 2019; Kannan et al., 2016; Alikaniotis and Raheja, 2019; Prabhumoye et al., 2019; Faltings et al., 2021). Can large generative language models be used to assist user writing at the document level while the user still controls the factual content? A possible answer lies in the observation that a substantial portion of day-to-day writing involves some form of reuse. Similar documents (e.g., monthly reports, sales letters, job descriptions) are effectively recycled by changing those segments that need to be modified (Fig. 1).1 Moreover, documents containing analogous texts are often found collocated in repositories, a common practice for organizations that manage professiona"
2021.findings-acl.185,2020.emnlp-main.349,0,0.054737,"Missing"
2021.findings-acl.185,2006.amta-papers.25,0,0.0826491,"er portions of the document to reflect modifications introduced by the user. In this work, we propose a new task, called DOC UMENT SKETCHING, in which initial template-like prototype documents are generated from collections of analogous documents. To support this task, we collected a dataset consisting of approximately 20K Wikipedia documents with similar textual characteristics.4 For this new task, inspired by previous work in measuring machine translation post-editing productivity (Tatsumi, 2009; Specia and Farzindar, 2010), we define an automatic evaluation metric based on Word Error Rate (Snover et al., 2006; Tom´as et al., 2003). We compare against strong baseline models including a mixture of experts model and a reinforcement learning approach designed to handle multi-source inputs and a weak supervision setting. Finally, we provide experimental analysis of these models, using automated and human evaluation studies. 2 2.1 Related Work Document Generation Recent work leverages the success of large pretrained language models to generate long texts such as stories (Rashkin et al., 2020), reviews (Cho et al., 2019a,b) and fake news (Zellers et al., 2019). Most end-user applications for assisting us"
2021.findings-acl.185,2010.jec-1.5,0,0.116675,"what is entailed in writing such documents.3 A fully-implemented dynamic system might update other portions of the document to reflect modifications introduced by the user. In this work, we propose a new task, called DOC UMENT SKETCHING, in which initial template-like prototype documents are generated from collections of analogous documents. To support this task, we collected a dataset consisting of approximately 20K Wikipedia documents with similar textual characteristics.4 For this new task, inspired by previous work in measuring machine translation post-editing productivity (Tatsumi, 2009; Specia and Farzindar, 2010), we define an automatic evaluation metric based on Word Error Rate (Snover et al., 2006; Tom´as et al., 2003). We compare against strong baseline models including a mixture of experts model and a reinforcement learning approach designed to handle multi-source inputs and a weak supervision setting. Finally, we provide experimental analysis of these models, using automated and human evaluation studies. 2 2.1 Related Work Document Generation Recent work leverages the success of large pretrained language models to generate long texts such as stories (Rashkin et al., 2020), reviews (Cho et al., 20"
2021.findings-acl.185,2009.mtsummit-posters.20,0,0.47678,"ull picture of what is entailed in writing such documents.3 A fully-implemented dynamic system might update other portions of the document to reflect modifications introduced by the user. In this work, we propose a new task, called DOC UMENT SKETCHING, in which initial template-like prototype documents are generated from collections of analogous documents. To support this task, we collected a dataset consisting of approximately 20K Wikipedia documents with similar textual characteristics.4 For this new task, inspired by previous work in measuring machine translation post-editing productivity (Tatsumi, 2009; Specia and Farzindar, 2010), we define an automatic evaluation metric based on Word Error Rate (Snover et al., 2006; Tom´as et al., 2003). We compare against strong baseline models including a mixture of experts model and a reinforcement learning approach designed to handle multi-source inputs and a weak supervision setting. Finally, we provide experimental analysis of these models, using automated and human evaluation studies. 2 2.1 Related Work Document Generation Recent work leverages the success of large pretrained language models to generate long texts such as stories (Rashkin et al., 2"
2021.findings-acl.185,W03-2804,0,0.121231,"Missing"
2021.findings-acl.185,2020.acl-main.450,0,0.0566789,"Missing"
2021.findings-acl.185,P19-1207,0,0.0512804,"Missing"
2021.findings-acl.185,D18-1356,0,0.0235525,"d learn to separate salient information and latent template structure. Cai et al. (2019) induce an intermediate template for response generation explicitly, but from a single retrieved relevant response. Similar prototype editing work (Guu et al., 2018; Hashimoto et al., 2018; Fabbri et al., 2020) focuses on short text (e.g., a question or a single sentence) or structured output (e.g., code snippet) editing. Oya et al. (2014); Magooda and Litman (2019); Yang et al. (2020); Li et al. (2018) convert each single input text into a template with blanks using rule-based methods. Other work such as (Wiseman et al., 2018) and (Gangadharaiah and Narayanaswamy, 2020) relies on a knowledge base or a domain/task specific ontology to segment text sequences into templates. 2.3 Multi-Sequence Processing Multiple Sequence Alignment (MSA), widely used in the biological domain (Sauder et al., 2000) to align multiple biological sequences like proteins, has long been leveraged for text pattern matching (Barzilay and Lee, 2003; Alonso et al., 2004). We adopt this method to align input documents and create heuristic templates as weak supervision. Other tasks taking multiple text sequences as input include multi-document sum"
2021.findings-acl.185,2020.emnlp-demos.6,0,0.0270809,"Missing"
2021.findings-acl.185,2020.acl-main.531,0,0.0128,"ences from the training corpus as prototypes (Cao et al., 2018; Yang et al., 2019; Wang et al., 2019; Gao et al., 2019; Peng et al., 2019), and learn to separate salient information and latent template structure. Cai et al. (2019) induce an intermediate template for response generation explicitly, but from a single retrieved relevant response. Similar prototype editing work (Guu et al., 2018; Hashimoto et al., 2018; Fabbri et al., 2020) focuses on short text (e.g., a question or a single sentence) or structured output (e.g., code snippet) editing. Oya et al. (2014); Magooda and Litman (2019); Yang et al. (2020); Li et al. (2018) convert each single input text into a template with blanks using rule-based methods. Other work such as (Wiseman et al., 2018) and (Gangadharaiah and Narayanaswamy, 2020) relies on a knowledge base or a domain/task specific ontology to segment text sequences into templates. 2.3 Multi-Sequence Processing Multiple Sequence Alignment (MSA), widely used in the biological domain (Sauder et al., 2000) to align multiple biological sequences like proteins, has long been leveraged for text pattern matching (Barzilay and Lee, 2003; Alonso et al., 2004). We adopt this method to align i"
2021.findings-acl.185,D19-1197,0,0.0149075,"t of similar documents. 3 Our experiments in Section 6 also show that drafts derived from multiple analogous documents are more effective than those derived from one or two documents. 4 We release our data and source code for dataset construction and experiments at https://github.com/ ellenmellon/document_sketching. 2.2 Template-Based Generation Some existing work induces templates as an intermediate step for performing tasks like text summarization or response generation. Most use a retrieval-based method to extract similar references from the training corpus as prototypes (Cao et al., 2018; Yang et al., 2019; Wang et al., 2019; Gao et al., 2019; Peng et al., 2019), and learn to separate salient information and latent template structure. Cai et al. (2019) induce an intermediate template for response generation explicitly, but from a single retrieved relevant response. Similar prototype editing work (Guu et al., 2018; Hashimoto et al., 2018; Fabbri et al., 2020) focuses on short text (e.g., a question or a single sentence) or structured output (e.g., code snippet) editing. Oya et al. (2014); Magooda and Litman (2019); Yang et al. (2020); Li et al. (2018) convert each single input text into a templa"
2021.naacl-main.400,D18-1316,0,0.367626,"ity. Original Text Super ant colony hits Australia . A giant 100km colony of ants could threaten local insect species. CLARE: Contextualized Perturbation Adversarial Text Super ant colony hits Australia {Coast, , }. A {gigantic, , } 100km colony of ants could threaten {insect, , } species. Figure 1: Illustration of CLARE. Through a mask-theninfill procedure, the model generates the adversarial text with three contextualized perturbations: Replace, Insert and Merge. A mask is indicated by “ ”. The degree of fade corresponds to the (decreasing) priority of the infill tokens. Liang et al., 2019; Alzantot et al., 2018; Ren et al., 2019; Jin et al., 2020, inter alia). Despite some empirical success, rule-based methods are agnostic to context, limiting their ability to produce natural, fluent, and grammatical outputs (Wang et al., 1 Introduction 2019b; Kurita et al., 2020, inter alia). Adversarial example generation for natural lanThis work presents CLARE, a ContextuaLized guage processing (NLP) tasks aims to perturb input AdversaRial Example generation model for text. text to trigger errors in machine learning models, CLARE perturbs the input with a mask-then-infill while keeping the output close to the ori"
2021.naacl-main.400,2020.emnlp-main.498,0,0.155017,"-infill perturbation. Compared with existing context-agnostic replacement approaches (Alzantot et al., 2018; Jin et al., 2020; Ren et al., 2019, inter alia), contextualized infilling produces more fluent and grammatical outputs. Generating adversarial examples with masked language models is also explored by concurrent work 4 3 A perturbation will not be considered if its candidate token set is empty. Insert and Merge actions change the text length. When any of them is applied, we accordingly change the text indices of affected actions remaining in A. 5055 BERTAttack (Li et al., 2020) and BAE (Garg and Ramakrishnan, 2020).5 • BERTAttack only replaces tokens and thus can only produce outputs of the same lengths as the inputs. This is analogous with a CLARE model with the Replace action only. BAE entangles replacing and inserting tokens: it inserts only at positions neighboring a replaced token, limiting its attacking capability. Departing from both, CLARE uses three different perturbations (Replace, Insert and Merge), each allowing efficient attacking against any position of the input, and can produce outputs of varied lengths. As we will show in the experiments (§3.3), CLARE outperforms both these methods. • W"
2021.naacl-main.400,D19-1633,0,0.0173364,"n rules to enhance semantic meaning preservation (Alzantot et al., 2018; Jin et al., 2020; Ren et al., 2019; Zhang et al., 2019; Zang et al., 2020, inter alia). Our work differs in that CLARE uses three contextualized perturbations that produces more fluent and grammatical outputs. Text generation with BERT. Generation with masked language models has been widely studied in various natural language tasks, ranging from lexical substitution (Wu et al., 2019a; Zhou et al., 2019a; Qiang et al., 2020; Wu et al., 2019b, inter alia) to non-autoregressive generation (Gu et al., 2018; Lee et al., 2018; Ghazvininejad et al., 2019; Wang and Cho, 2019; Ma et al., 2019; Sun et al., 2019; Ren et al., 2020; Zhang et al., 2020b, inter alia). 6 Conclusion We have presented CLARE, a contextualized adversarial example generation model for text. It uses contextualized knowledge from pretrained masked language models, and can generate ad9 In preliminary experiments, we found that it is more diffiversarial examples that are natural, fluent and cult to use other models to attack a victim model trained with grammatical. With three contextualized perturbathe adversarial examples generated by CLARE, than to use CLARE itself. tion pat"
2021.naacl-main.400,N18-1170,0,0.0481692,"Missing"
2021.naacl-main.400,D17-1215,0,0.0529769,"ng the output close to the original. Be- procedure: it first detects the vulnerabilities of sides exposing system vulnerabilities and helping a model and deploys masks to the inputs to inimprove their robustness and security (Zhao et al., dicate missing text, then plugs in an alternative 2018; Wallace et al., 2019; Cheng et al., 2019; Jia using a pretrained masked language model (e.g., et al., 2019, inter alia), adversarial examples are RoBERTa; Liu et al., 2019). CLARE features three also used to analyze and interpret the models’ deci- contextualized perturbations: Replace, Insert and sions (Jia and Liang, 2017; Ribeiro et al., 2018). Merge, which respectively replace a token, insert Generating adversarial examples for NLP tasks a new one, and merge a bigram (Figure 1). As can be challenging, in part due to the discrete nature a result, it can generate outputs of varied lengths, of natural language text. Most recent efforts have in contrast to token replacement based methods explored heuristic rules, such as replacing tokens that are limited to outputs of the same lengths as with their synonyms (Samanta and Mehta, 2017; the inputs (Alzantot et al., 2018; Ren et al., 2019; 5053 Proceedings of the 202"
2021.naacl-main.400,D19-1423,0,0.100651,"Missing"
2021.naacl-main.400,2020.acl-main.245,0,0.0692802,"Missing"
2021.naacl-main.400,K18-1031,0,0.0280548,"treebank (Socher et al., 2013), Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005), and Quora Question Pairs from the GLUE benchmark. The results on these datasets are summarized in Appendix A.2. Following previous practice (Alzantot et al., 2018), we fine-tune CLARE on training data, and evaluate with 1,000 randomly sampled test instances of lengths ≤ 100. In the sentence-pair tasks (e.g., MNLI, QNLI), we attack the longer sentence excluding the tokens that appear in both. (e.g., merging bigram ab into c). • Perplexity (PPL): a metric used to evaluate the fluency of adversaries (Kann et al., 2018; Zang et al., 2020). The perplexity is calculated using small sized GPT-2 with a 50K-sized vocabulary (Radford et al., 2019). • Grammar error (GErr): the absolute number of increased grammatical errors in the successful adversarial example, compared to the original text. Following (Zang et al., 2020; Morris et al., 2020b), we calculate this by the LanguageTool (Naber et al., 2003).7 • Textual similarity (Sim): the cosine similarity between the input and its adversary. Following (Jin et al., 2020; Morris et al., 2020b), we calculate this using the universal sentence encoder (USE; Cer et al., 2"
2021.naacl-main.400,D14-1181,0,0.0158655,"Missing"
2021.naacl-main.400,2020.acl-main.249,0,0.0303065,"threaten {insect, , } species. Figure 1: Illustration of CLARE. Through a mask-theninfill procedure, the model generates the adversarial text with three contextualized perturbations: Replace, Insert and Merge. A mask is indicated by “ ”. The degree of fade corresponds to the (decreasing) priority of the infill tokens. Liang et al., 2019; Alzantot et al., 2018; Ren et al., 2019; Jin et al., 2020, inter alia). Despite some empirical success, rule-based methods are agnostic to context, limiting their ability to produce natural, fluent, and grammatical outputs (Wang et al., 1 Introduction 2019b; Kurita et al., 2020, inter alia). Adversarial example generation for natural lanThis work presents CLARE, a ContextuaLized guage processing (NLP) tasks aims to perturb input AdversaRial Example generation model for text. text to trigger errors in machine learning models, CLARE perturbs the input with a mask-then-infill while keeping the output close to the original. Be- procedure: it first detects the vulnerabilities of sides exposing system vulnerabilities and helping a model and deploys masks to the inputs to inimprove their robustness and security (Zhao et al., dicate missing text, then plugs in an alternativ"
2021.naacl-main.400,D18-1149,0,0.0255533,"ynonym substitution rules to enhance semantic meaning preservation (Alzantot et al., 2018; Jin et al., 2020; Ren et al., 2019; Zhang et al., 2019; Zang et al., 2020, inter alia). Our work differs in that CLARE uses three contextualized perturbations that produces more fluent and grammatical outputs. Text generation with BERT. Generation with masked language models has been widely studied in various natural language tasks, ranging from lexical substitution (Wu et al., 2019a; Zhou et al., 2019a; Qiang et al., 2020; Wu et al., 2019b, inter alia) to non-autoregressive generation (Gu et al., 2018; Lee et al., 2018; Ghazvininejad et al., 2019; Wang and Cho, 2019; Ma et al., 2019; Sun et al., 2019; Ren et al., 2020; Zhang et al., 2020b, inter alia). 6 Conclusion We have presented CLARE, a contextualized adversarial example generation model for text. It uses contextualized knowledge from pretrained masked language models, and can generate ad9 In preliminary experiments, we found that it is more diffiversarial examples that are natural, fluent and cult to use other models to attack a victim model trained with grammatical. With three contextualized perturbathe adversarial examples generated by CLARE, than t"
2021.naacl-main.400,2020.emnlp-main.500,0,0.342645,"ARE is the local mask-then-infill perturbation. Compared with existing context-agnostic replacement approaches (Alzantot et al., 2018; Jin et al., 2020; Ren et al., 2019, inter alia), contextualized infilling produces more fluent and grammatical outputs. Generating adversarial examples with masked language models is also explored by concurrent work 4 3 A perturbation will not be considered if its candidate token set is empty. Insert and Merge actions change the text length. When any of them is applied, we accordingly change the text indices of affected actions remaining in A. 5055 BERTAttack (Li et al., 2020) and BAE (Garg and Ramakrishnan, 2020).5 • BERTAttack only replaces tokens and thus can only produce outputs of the same lengths as the inputs. This is analogous with a CLARE model with the Replace action only. BAE entangles replacing and inserting tokens: it inserts only at positions neighboring a replaced token, limiting its attacking capability. Departing from both, CLARE uses three different perturbations (Replace, Insert and Merge), each allowing efficient attacking against any position of the input, and can produce outputs of varied lengths. As we will show in the experiments (§3.3), CLA"
2021.naacl-main.400,2021.ccl-1.108,0,0.139462,"Missing"
2021.naacl-main.400,D19-1437,0,0.0222437,"(Alzantot et al., 2018; Jin et al., 2020; Ren et al., 2019; Zhang et al., 2019; Zang et al., 2020, inter alia). Our work differs in that CLARE uses three contextualized perturbations that produces more fluent and grammatical outputs. Text generation with BERT. Generation with masked language models has been widely studied in various natural language tasks, ranging from lexical substitution (Wu et al., 2019a; Zhou et al., 2019a; Qiang et al., 2020; Wu et al., 2019b, inter alia) to non-autoregressive generation (Gu et al., 2018; Lee et al., 2018; Ghazvininejad et al., 2019; Wang and Cho, 2019; Ma et al., 2019; Sun et al., 2019; Ren et al., 2020; Zhang et al., 2020b, inter alia). 6 Conclusion We have presented CLARE, a contextualized adversarial example generation model for text. It uses contextualized knowledge from pretrained masked language models, and can generate ad9 In preliminary experiments, we found that it is more diffiversarial examples that are natural, fluent and cult to use other models to attack a victim model trained with grammatical. With three contextualized perturbathe adversarial examples generated by CLARE, than to use CLARE itself. tion patterns, Replace, Insert and Merge in o"
2021.naacl-main.400,2020.emnlp-demos.16,0,0.117568,"Missing"
2021.naacl-main.400,2020.findings-emnlp.341,0,0.069201,"00 randomly sampled test instances of lengths ≤ 100. In the sentence-pair tasks (e.g., MNLI, QNLI), we attack the longer sentence excluding the tokens that appear in both. (e.g., merging bigram ab into c). • Perplexity (PPL): a metric used to evaluate the fluency of adversaries (Kann et al., 2018; Zang et al., 2020). The perplexity is calculated using small sized GPT-2 with a 50K-sized vocabulary (Radford et al., 2019). • Grammar error (GErr): the absolute number of increased grammatical errors in the successful adversarial example, compared to the original text. Following (Zang et al., 2020; Morris et al., 2020b), we calculate this by the LanguageTool (Naber et al., 2003).7 • Textual similarity (Sim): the cosine similarity between the input and its adversary. Following (Jin et al., 2020; Morris et al., 2020b), we calculate this using the universal sentence encoder (USE; Cer et al., 2018). The last four metrics are averaged across those adversarial examples that successfully attack the victim model. 3.3 Results Evaluation metrics. We follow previous works Table 2 summarizes the results. Overall CLARE (Jin et al., 2020; Morris et al., 2020a), and evaluate achieves the best performance on all metrics c"
2021.naacl-main.400,N16-1018,0,0.111363,"Missing"
2021.naacl-main.400,D13-1170,0,0.00302505,"the best performance for each metric. All numbers are reported on 1000 test instances. ↑ (↓) represents that the higher (lower) the better. • QNLI (Wang et al., 2019a): a binary classification dataset converted from the Stanford question answering dataset (Rajpurkar et al., 2016). The task is to determine whether the context contains the answer to a question. It is mainly based on English Wikipedia articles. Table 1 summarizes some statistics of the datasets. In addition to the above four datasets, we experiment with DBpedia ontology dataset (Zhang et al., 2015), Stanford sentiment treebank (Socher et al., 2013), Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005), and Quora Question Pairs from the GLUE benchmark. The results on these datasets are summarized in Appendix A.2. Following previous practice (Alzantot et al., 2018), we fine-tune CLARE on training data, and evaluate with 1,000 randomly sampled test instances of lengths ≤ 100. In the sentence-pair tasks (e.g., MNLI, QNLI), we attack the longer sentence excluding the tokens that appear in both. (e.g., merging bigram ab into c). • Perplexity (PPL): a metric used to evaluate the fluency of adversaries (Kann et al., 2018; Zang et al."
2021.naacl-main.400,D19-1221,0,0.0612854,"alia). Adversarial example generation for natural lanThis work presents CLARE, a ContextuaLized guage processing (NLP) tasks aims to perturb input AdversaRial Example generation model for text. text to trigger errors in machine learning models, CLARE perturbs the input with a mask-then-infill while keeping the output close to the original. Be- procedure: it first detects the vulnerabilities of sides exposing system vulnerabilities and helping a model and deploys masks to the inputs to inimprove their robustness and security (Zhao et al., dicate missing text, then plugs in an alternative 2018; Wallace et al., 2019; Cheng et al., 2019; Jia using a pretrained masked language model (e.g., et al., 2019, inter alia), adversarial examples are RoBERTa; Liu et al., 2019). CLARE features three also used to analyze and interpret the models’ deci- contextualized perturbations: Replace, Insert and sions (Jia and Liang, 2017; Ribeiro et al., 2018). Merge, which respectively replace a token, insert Generating adversarial examples for NLP tasks a new one, and merge a bigram (Figure 1). As can be challenging, in part due to the discrete nature a result, it can generate outputs of varied lengths, of natural language te"
2021.naacl-main.400,N18-1101,0,0.0734919,"Missing"
2021.naacl-main.414,P17-1171,0,0.0244406,"as it involves making nuet al. (2019) control higher level attributes of text, anced changes to text according to natural lansuch as style, tone, or topic. Our task instead guage commands. We also believe this task has 5266 uses natural language commands, which can flexibly express different types of constraints, ranging from low-level lexical ones, to high-level topical ones. In this sense, we can also draw the parallel to dialog response generation (Ghazvininejad et al., 2018; Dinan et al., 2018), task-oriented dialog (Gao et al., 2018), or open domain question answering (Min et al., 2019; Chen et al., 2017), that also involve user responses or queries, although these tasks are not concerned with text generation in the context of document creation. senting them. Related to Wikipedia data, Pryzant et al. (2020) also used Wikipedia revision histories to learn to debias text, whereas we considered general edits. Iso et al. (2020) propose a factbased text editing task, but they do not consider control or other types of edits. Another related task to text editing is text paraphrasing (Gupta et al., 2018), however paraphrasing usually conserves the meaning of a sentence. While the edits we consider inc"
2021.naacl-main.414,P18-1082,0,0.163969,"the office. add years in office Barack Obama was the 44th President of the United States from 2009 to 2017 and the first African-American to hold the office. Figure 1: An illustration of our interactive text generation setting. This is an example generated by our model. The blue panels represent the text being edited, taken from the document shown on the right. The orange panels represent user edit commands. The model grounds edits in query results from a commercial search engine. A long-standing goal of natural language processing research has been to generate long-form text (Lebowitz, 1985; Fan et al., 2018; Rashkin et al., 2020). Recent large generative language models such as GPT-2 (Radford et al., 2019), and GPT3 (Brown et al., 2020), demonstrate an impressive ability to generate fluent text, but their outputs are difficult to control beyond a prompt, and they manifest a tendency to hallucinate facts (Wiseman et al., 2017). Much recent work has thus focused on making such models more controllable (Keskar et al., 2019; Hu et al., 2017; Zhang et al., 2020; Dathathri et al., 2019), and factually grounded (Guu et al., 2020; Liu et al., 2018b). Most such work only considers a one-shot generation s"
2021.naacl-main.414,D18-1028,0,0.0813115,"of our full model are broken down by edit intention labels in Table 6. The columns report the same metrics as Ablations The middle rows of Table 5 show the in our main table of results, with the exception of results for three ablations of our model. The first S-BLEU, which reports the BLEU score between ablation removes everything but the source senthe source sentence and target, and the last coltence s. This is similar to the paraphrase setumn, which reports the number of test edits that ting (Gupta et al., 2018), and the editing setting were classified into each category. With the caveat in Faruqui et al. (2018) and Yin et al. (2018). that intention labels come from an automatic clasWe can see that including the context, grounding, sifier and not human annotation, we can observe and command as additional inputs yields signifithat our model has varying performance across cant improvements over only using the source sendifferent types of edits. The model performs very tence. We can also see from the second ablation well on fluency edits, but worse on content edits. that the commands are a crucial element in the This comes at no surprise given that fluency edmodel’s performance. This is not surprising s"
2021.naacl-main.414,P18-5002,1,0.889002,"Missing"
2021.naacl-main.414,Q18-1031,0,0.0603054,"h pretrained language model weights, yields encouraging results on both automatic and human evaluations. Additionally, our ablation studies showed the crucial role played by the user command and grounding. Breaking down our results by types of edits, we saw that our model not only performs well on easier fluency edits, but also on much harder content edits. Finally, we discussed future research directions for interactive document generation, as well as possible extensions to other domains such as images or code. Acknowledgments Text Editing Several previous works have focused on text editing. Guu et al. (2018) generate The authors would like to thank Thomas Hofsentences by editing prototypes taken from their mann, as well as Sudha Rao, Matt Richardtraining corpus, although they use editing only as a son, Zhang Li, Kosh Narayanan, and Chandra means for language modeling. Wu et al. (2019) exChikkareddy for their helpful suggestions. pand upon Guu et al. (2018)’s setting, but for dialog. More related to our own setting, Faruqui et al. (2018) propose WikiAtomicEdits, a dataset of edReferences its crawled from Wikipedia. However, they consider a much narrower definition of edits than our Giusepppe Attar"
2021.naacl-main.414,P17-1141,0,0.0461026,"Missing"
2021.naacl-main.414,W19-2405,0,0.0163809,"n generating long-form narratives (Jain et al., 2017). While earlier work in Story Generation focused more on plan-based architectures (Lebowitz, 1985), more recent work moved towards end-to-end approaches (Fan et al., 2018) allowing generation to be unconstrained and creative. As narratives are often aimed at particular goals expressed in terms of outlines and plans, much of the literature in Story Generation is framed as a form of controllable generation, using storylines (Peng et al., 2018), events (Martin et al., 2017; Harrison et al., 2017), plot words or word skeletons (Xu et al., 2018; Ippolito et al., 2019), plans (Yao et al., 2019), story ending (Tambwekar et al., 2019), and outlines (Rashkin et al., 2020) as various forms of constraints. Our work takes a significantly different approach, as we treat document or story generation as an iterative process that allows a human to generate a full document from scratch, but also allows constraints to be more dynamic (e.g., add nationality in Table 9 only if the system missed that the first time). 8 Conclusion In this work we argued that text generation should be interactive, and, as a means towards that end, we proposed a general text editing task, wh"
2021.naacl-main.414,2020.acl-main.17,0,0.0328833,"om low-level lexical ones, to high-level topical ones. In this sense, we can also draw the parallel to dialog response generation (Ghazvininejad et al., 2018; Dinan et al., 2018), task-oriented dialog (Gao et al., 2018), or open domain question answering (Min et al., 2019; Chen et al., 2017), that also involve user responses or queries, although these tasks are not concerned with text generation in the context of document creation. senting them. Related to Wikipedia data, Pryzant et al. (2020) also used Wikipedia revision histories to learn to debias text, whereas we considered general edits. Iso et al. (2020) propose a factbased text editing task, but they do not consider control or other types of edits. Another related task to text editing is text paraphrasing (Gupta et al., 2018), however paraphrasing usually conserves the meaning of a sentence. While the edits we consider include meaning-preserving edits, we are mostly interested in edits that affect meaning. Story Generation The task of Document Generation considered in our work bears similarity with work on generating long-form narratives (Jain et al., 2017). While earlier work in Story Generation focused more on plan-based architectures (Leb"
2021.naacl-main.414,J06-4003,0,0.0584681,"Missing"
2021.naacl-main.414,N19-1238,0,0.0634756,"Missing"
C00-1057,C94-1032,0,0.231487,"Missing"
C00-1057,P98-2180,0,0.138836,"d Segmentation ϼ Orthography Lexicon Ѐ Derivational Assembly ϼ Ѐ Syntactic Analysis Syntax Lexicon ϼ Ѐ Logical Form Figure 1: Block diagram of Japanese NL system syntax (or any downstream) component from arriving at a correct analysis (e.g., a missing record). A recoverable error is one that does not interfere with the operation of following components. An example of the latter is the inclusion of an extra record. This extra record does not (theoretically) prevent the parser from doing its job (although in practice it may since it consumes resources). extraction of semantic relationships (see [Richardson98]) and support other linguistic projects like information retrieval, NL interfaces and dialog systems, auto-summarization and machine translation. Using standard definitions of recall (R) and precision (P): The segmenter is the first level of processing. This is a finite-state morphological analyzer responsible for generating all possible word candidates into a word lattice. It has a custom lexicon (automatically derived from the main lexicon to ensure consistency) that is designed to facilitate the identification of orthographic variants. where Segcorrect and Segtotal are the number of “correc"
C00-1057,C00-2119,1,0.74795,"erived forms that are then used by the parser to create syntax trees and logical forms. Many of the techniques here are similar to what we use in our Chinese NL system (see [Wu98] for more details). The parser (described extensively in [Jensen93]) generates syntactic representations and logical forms. This is a bottom-up chart parser with binary rules within the Augmented Phrase Structure Grammar formalism. The grammar rules are language-specific while the core engine is shared among 7 languages (Chinese, Japanese, Korean, English, French, German, Spanish). The Japanese parser is described in [Suzuki00]. 2 Recall vs. Precision In this architecture, data is fed forward from one component to the next; hence, it is crucial that the base components (like the segmenter) generate a minimal number of omission errors. Since segmentation errors may affect subsequent components, it is convenient to divide these errors into two types: recoverable and non-recoverable. A non-recoverable error is one that prevents the R= Seg correct Tag total P= Seg correct Segtotal we can see that recall measures non-recoverable errors and precision measures recoverable errors. Since our goal is to create a robust NL sys"
C00-1057,C98-2175,0,\N,Missing
C00-1057,P98-1068,0,\N,Missing
C00-1057,C98-1065,0,\N,Missing
C00-2119,T75-2001,0,0.0252559,"Missing"
C00-2119,C00-1057,1,0.684753,"at later stages of processing using a large semantic knowledge base (Richardson 1997, Richardson et al. 1998). One of the goals of this paper is to show that the syntactic information alone can resolve the ambiguities in the word lattice sufficiently well to select the best breaking analysis in the absence of elaborate semantic information. Figure 1 (see Appendix) shows the default attachment of the relative clause to the closest NP. Though this structure may be semantically implausible, the word-breaking analysis is correct. The word-breaking component of our system is described in detail in Kacmarcik et al. (2000). For the purpose of robust parsing, the component is expected to solve the following two problems:  Lemmatization: Find possible words in the input text using a dictionary and its inflectional morphology, and return the dictionary entry forms (lemmas). Note that multiple lemmas are often possible for a given inflected form (e.g. surface form  (katte) could be an inflected form of the verbs  (kau &quot;buy&quot;),  (katu &quot;win&quot;) or  (karu &quot;trim&quot;), in which case all these forms must be returned. The dictionary the word-breaker uses has about 70,000 unique entries.  Orthography normalization:"
C00-2119,C94-1032,0,0.225092,"Missing"
C00-2119,P98-2180,0,0.0890997,"er, the grammar rules are elaborately conditioned on morphological and syntactic features, enabling much finer-grained parsing analyses than just relying on a small number of basic parts-of-speech (POS). This gives the grammar the power to disambiguate multiple word analyses in the input lattice. Because we do not utilize semantic information, we perform no semantically motivated attachment of phrases during parsing. Instead, we parse them into a default analysis, which can then be expanded and disambiguated at later stages of processing using a large semantic knowledge base (Richardson 1997, Richardson et al. 1998). One of the goals of this paper is to show that the syntactic information alone can resolve the ambiguities in the word lattice sufficiently well to select the best breaking analysis in the absence of elaborate semantic information. Figure 1 (see Appendix) shows the default attachment of the relative clause to the closest NP. Though this structure may be semantically implausible, the word-breaking analysis is correct. The word-breaking component of our system is described in detail in Kacmarcik et al. (2000). For the purpose of robust parsing, the component is expected to solve the following"
C00-2119,W97-0908,0,\N,Missing
C00-2119,C98-2175,0,\N,Missing
C00-2119,P98-1068,0,\N,Missing
C00-2119,C98-1065,0,\N,Missing
C04-1051,P01-1008,0,0.026132,"Missing"
C04-1051,N03-1003,0,0.0936505,"t the idea because of the noisy, comparable nature of their dataset. captured by edit distance techniques, which conflate semantic similarity with formal similarity. We conclude that paraphrase research would benefit by identifying richer data sources and developing appropriate learning techniques. 2 Data/Methodology Our two paraphrase datasets are distilled from a corpus of news articles gathered from thousands of news sources over an extended period. While the idea of exploiting multiple news reports for paraphrase acquisition is not new, previous efforts (for example, Shinyama et al. 2002; Barzilay and Lee 2003) have been restricted to at most two news sources. Our work represents what we believe to be the first attempt to exploit the explosion of news coverage on the Web, where a single event can generate scores or hundreds of different articles within a brief period of time. Some of these articles represent minor rewrites of an original AP or Reuters story, while others represent truly distinct descriptions of the same basic facts. The massive redundancy of information conveyed with widely varying surface strings is a resource begging to be exploited. Figure 1 shows the flow of our data collection"
C04-1051,W03-0301,0,0.0239292,"Missing"
C04-1051,P00-1056,0,0.0174201,"ndencies, and a variety of distributed paraphrases, with alignments spanning widely separated elements. 3.2 tagged corpus of alignments serving as the gold standard. Paraphrase data is of course monolingual, but otherwise the task is very similar to the MT alignment problem, posing the same issues with one-to-many, many-to-many, and one/many-tonull word mappings. Our a priori assumption was that the lower the AER for a corpus, the more likely it would be to yield learnable information about paraphrase alternations. We closely followed the evaluation standards established in Melamed (2001) and Och & Ney (2000, 2003). Following Och & Ney’s methodology, two annotators each created an initial annotation for each dataset, subcategorizing alignments as either SURE (necessary) or POSSIBLE (allowed, but not required). Differences were then highlighted and the annotators were asked to review these cases. Finally we combined the two annotations into a single gold standard in the following manner: if both annotators agreed that an alignment should be SURE, then the alignment was marked as sure in the gold-standard; otherwise the alignment was marked as POSSIBLE. To compute Precision, Recall, and Alignment E"
C04-1051,C96-2141,0,0.113546,"Missing"
C04-1051,J93-2003,0,\N,Missing
C04-1051,J03-1002,0,\N,Missing
D16-1033,W00-1401,0,0.151229,"al., 2012). A news-domain parallel sentence corpus containing 1,496 parallel examples has been culled from multireference Chinese-English translations by Ganitkevitch et al. (2011). The only publicly-available manually-created abstractive compression corpus is that described by Cohn and Lapata (2008), which comprises 575 single-reference sentence pairs. Automatic metrics: Early automatic metrics for evaluation of compressions include success rate (Jing, 2000), defined as accuracy of individual word or constituent deletion decisions; Simple String Accuracy (string edit distance), introduced by Bangalore et al. (2000) for natural language generation tasks; and Word Accuracy (Chiori and Furui, 2004), which generalizes Bangalore et al. (2000) to multiple references. Riezler et al. (2003) introduced the use of F-measure over grammatical relations. Word unigram and word-bigram F-measure have also been used (Unno et al., 2006; Filippova et al., 2015). Variants of ROUGE (Lin, 2004), used for summarization evaluation, have also been applied to sentence compressions (Rush et al., 2015). Riezler et al. (2003) show that F-measure over grammatical relations agrees with human ratings on the relative ranking of three s"
D16-1033,P11-1049,0,0.0139397,"urnals, and technical documents sampled from the Open American National Corpus (OANC1 ). • Each source text is accompanied by up to five crowd-sourced rewrites constrained to a preset compression ratio and annotated with quality judgments. Multiple rewrites permit study of the impact of operations on human compression quality and facilitate automatic evaluation. Introduction Automated sentence compression condenses a sentence or paragraph to its most important content in order to enhance writing quality, meet document length constraints, and build more accurate document summarization systems (Berg-Kirkpatrick et al., 2011; Vanderwende et al., 2007). Though word deletion is extensively used (e.g., (Clarke and Lapata, 2008)), state-of-the-art compression models (Cohn and Lapata, 2008; Rush et al., 2015) benefit crucially from data that can represent complex abstractive compression operations, including substitution of words and phrases and reordering. ∗ This research was conducted during the author’s internship at Microsoft Research. • This dataset is the first to provide compressions at the multi-sentence (two-sentence paragraph) level, which may present a stepping stone to whole document summarization. Many of"
D16-1033,briscoe-carroll-2002-robust,0,0.0155411,"metrics based on surface uni-grams (LR -1), bi-grams (LR -2), tri-grams (LR -3), and four-grams (LR -4), as well skip bi-grams (with a maximum of four intervening words as in ROUGE-S4) (SKIP -2), and dependency tree triples obtained from collapsed dependencies output from the Stanford parser (PARSE 2).7 The second criterion is the scoring measure used to evaluate the match between two sets of linguistic units corresponding to a system output and a reference compression. We compare Precision, Recall, F-measure, and Precision+Brevity penalty (as 7 Clarke and Lapata (2006) used the RASP parser (Briscoe and Carroll, 2002), but we expect that the Stanford parser is similarly robust and would lead to similar correlations. in BLEU). The third criterion is whether multiple references or a single reference is used, and in the case of multiple references, the method used to aggregate information from multiple references. We investigate two previously applied methods and introduce a novel approach that often outperforms the standard methods. To illustrate, we introduce some notation and use a simple example. Consider a sub-phrase of one of the sentences in Table 1, think about your household, as an input text to comp"
D16-1033,P06-1048,0,0.0963999,"matically-mined deletion corpora are single-reference and have varying (uncontrolled) compression rates. Knight and Marcu (2002) automatically mined a small parallel corpus (1,035 training and 32 test sentences) by aligning abstracts to sentences in articles. Filippova and Altun (2013) extracted deletion-based compressions by aligning news headlines to first sentences, yielding a corpus of 250,000 parallel sentences. The same approach was used by Filippova et al. (2015) to create a set of 2M sentence pairs. Only a subset of 10,000 parallel sentences from the latter has been publicly released. Clarke and Lapata (2006) and Clarke and Lapata (2008) provide two manually-created two-reference corpora for deletion-based compression:2 their sizes are 1,370 and 1,433 sentences, respectively. 2 http://jamesclarke.net/research/ resources 341 Abstractive compression corpora: Rush et al. (2015) have mined 4 million compression pairs from news articles and released their code to extract data from the Annotated Gigaword (Napoles et al., 2012). A news-domain parallel sentence corpus containing 1,496 parallel examples has been culled from multireference Chinese-English translations by Ganitkevitch et al. (2011). The only"
D16-1033,C08-1018,0,0.127462,"to a preset compression ratio and annotated with quality judgments. Multiple rewrites permit study of the impact of operations on human compression quality and facilitate automatic evaluation. Introduction Automated sentence compression condenses a sentence or paragraph to its most important content in order to enhance writing quality, meet document length constraints, and build more accurate document summarization systems (Berg-Kirkpatrick et al., 2011; Vanderwende et al., 2007). Though word deletion is extensively used (e.g., (Clarke and Lapata, 2008)), state-of-the-art compression models (Cohn and Lapata, 2008; Rush et al., 2015) benefit crucially from data that can represent complex abstractive compression operations, including substitution of words and phrases and reordering. ∗ This research was conducted during the author’s internship at Microsoft Research. • This dataset is the first to provide compressions at the multi-sentence (two-sentence paragraph) level, which may present a stepping stone to whole document summarization. Many of these two-sentence paragraphs are compressed both as paragraphs and separately sentence-bysentence, offering data that may yield insights into the impact of multi"
D16-1033,D13-1155,0,0.101382,"tituent type. Jing and McKeown (1999) identified abstractive operations (other than word deletion) employed by professional writers, including paraphrasing and re-ordering of phrases, and merging and reordering sentences, but did not quantify their impact on compression quality. Deletion-based compression corpora: Currently available automatically-mined deletion corpora are single-reference and have varying (uncontrolled) compression rates. Knight and Marcu (2002) automatically mined a small parallel corpus (1,035 training and 32 test sentences) by aligning abstracts to sentences in articles. Filippova and Altun (2013) extracted deletion-based compressions by aligning news headlines to first sentences, yielding a corpus of 250,000 parallel sentences. The same approach was used by Filippova et al. (2015) to create a set of 2M sentence pairs. Only a subset of 10,000 parallel sentences from the latter has been publicly released. Clarke and Lapata (2006) and Clarke and Lapata (2008) provide two manually-created two-reference corpora for deletion-based compression:2 their sizes are 1,370 and 1,433 sentences, respectively. 2 http://jamesclarke.net/research/ resources 341 Abstractive compression corpora: Rush et a"
D16-1033,P15-2073,1,0.283431,"Missing"
D16-1033,D11-1108,0,0.0317678,"Missing"
D16-1033,N15-1072,0,0.041093,"Missing"
D16-1033,D15-1013,0,0.189886,"r grammatical relations agrees with human ratings on the relative ranking of three systems at the corpus level. Clarke and Lapata (2006) evaluate two deletion-based automatic compression systems against a deletion-based gold-standard on sets of 20 sentences. Parse-based F-1 was shown to have high sentence-level Pearson’s ρ correlation with human judgments of overall quality, and to have higher ρ than Simple String Accuracy. Napoles et al. (2011) have pointed to the need of multiple references and studies of evaluation metrics. For the related tasks of document and multidocument summarization, Graham (2015) provides a fine-grained comparison of automated evaluation methods. However, to the best of our knowledge, no studies of automatic evaluation metrics exist for abstractive compression of shorter texts. Length 1-Sent Source Ref-1 2-Sent Ref-2 Source Text Think of all the ways everyone in your household will benefit from your membership in Audubon. Imagine how your household will benefit from your Audubon membership. Ref-1 Everyone in your household will benefit from membership in Audubon. Will the administration live up to its environmental promises? Can we save the last of our ancient forests"
D16-1033,ide-etal-2008-masc,0,0.0437419,"Missing"
D16-1033,P10-2013,0,0.0508029,"Missing"
D16-1033,A00-1043,0,0.0856246,"l. (2015) have mined 4 million compression pairs from news articles and released their code to extract data from the Annotated Gigaword (Napoles et al., 2012). A news-domain parallel sentence corpus containing 1,496 parallel examples has been culled from multireference Chinese-English translations by Ganitkevitch et al. (2011). The only publicly-available manually-created abstractive compression corpus is that described by Cohn and Lapata (2008), which comprises 575 single-reference sentence pairs. Automatic metrics: Early automatic metrics for evaluation of compressions include success rate (Jing, 2000), defined as accuracy of individual word or constituent deletion decisions; Simple String Accuracy (string edit distance), introduced by Bangalore et al. (2000) for natural language generation tasks; and Word Accuracy (Chiori and Furui, 2004), which generalizes Bangalore et al. (2000) to multiple references. Riezler et al. (2003) introduced the use of F-measure over grammatical relations. Word unigram and word-bigram F-measure have also been used (Unno et al., 2006; Filippova et al., 2015). Variants of ROUGE (Lin, 2004), used for summarization evaluation, have also been applied to sentence com"
D16-1033,P03-1054,0,0.0242237,"graph input/output pairs split into sentences by heuristically aligning source to target sentences in the paragraphs. T3: We use the authors’ implementation of the tree transducer system described in Cohn and Lapata (2008). T3 similarly requires sentence-level input/output pairs, but can also learn from abstractive compressions. We thus used a larger set of approximately 28,000 examples (single sentences with abstractive compressions taken directly from the data or as a result of heuristic sentence-level alignment of two-sentence paragraphs). We obtained parse trees using the Stanford parser (Klein and Manning, 2003), and used Jacana (Yao et al., 2013) for word alignment. The performance obtained by T3 in our experiments is substantially weaker (relative to ILP) than that reported in prior work (Cohn and Lapata, 2008). We therefore interpret this system output solely as data for evaluating automatic metrics. NAMAS: We run the publicly available implementation of NAMAS12 with the settings described by Rush et al. (2015). We modified the beam search algorithm to produce output with a compression ratio similar to that of the human references, since this ratio is a large factor in compression quality (Napoles"
D16-1033,W04-1013,0,0.043578,"ly automatic metrics for evaluation of compressions include success rate (Jing, 2000), defined as accuracy of individual word or constituent deletion decisions; Simple String Accuracy (string edit distance), introduced by Bangalore et al. (2000) for natural language generation tasks; and Word Accuracy (Chiori and Furui, 2004), which generalizes Bangalore et al. (2000) to multiple references. Riezler et al. (2003) introduced the use of F-measure over grammatical relations. Word unigram and word-bigram F-measure have also been used (Unno et al., 2006; Filippova et al., 2015). Variants of ROUGE (Lin, 2004), used for summarization evaluation, have also been applied to sentence compressions (Rush et al., 2015). Riezler et al. (2003) show that F-measure over grammatical relations agrees with human ratings on the relative ranking of three systems at the corpus level. Clarke and Lapata (2006) evaluate two deletion-based automatic compression systems against a deletion-based gold-standard on sets of 20 sentences. Parse-based F-1 was shown to have high sentence-level Pearson’s ρ correlation with human judgments of overall quality, and to have higher ρ than Simple String Accuracy. Napoles et al. (2011)"
D16-1033,W11-1611,0,0.209708,"Missing"
D16-1033,W12-3018,0,0.037565,"Missing"
D16-1033,Q16-1005,0,0.0328221,"Missing"
D16-1033,N03-1026,0,0.127073,"(2011). The only publicly-available manually-created abstractive compression corpus is that described by Cohn and Lapata (2008), which comprises 575 single-reference sentence pairs. Automatic metrics: Early automatic metrics for evaluation of compressions include success rate (Jing, 2000), defined as accuracy of individual word or constituent deletion decisions; Simple String Accuracy (string edit distance), introduced by Bangalore et al. (2000) for natural language generation tasks; and Word Accuracy (Chiori and Furui, 2004), which generalizes Bangalore et al. (2000) to multiple references. Riezler et al. (2003) introduced the use of F-measure over grammatical relations. Word unigram and word-bigram F-measure have also been used (Unno et al., 2006; Filippova et al., 2015). Variants of ROUGE (Lin, 2004), used for summarization evaluation, have also been applied to sentence compressions (Rush et al., 2015). Riezler et al. (2003) show that F-measure over grammatical relations agrees with human ratings on the relative ranking of three systems at the corpus level. Clarke and Lapata (2006) evaluate two deletion-based automatic compression systems against a deletion-based gold-standard on sets of 20 sentenc"
D16-1033,D15-1044,0,0.719526,"n ratio and annotated with quality judgments. Multiple rewrites permit study of the impact of operations on human compression quality and facilitate automatic evaluation. Introduction Automated sentence compression condenses a sentence or paragraph to its most important content in order to enhance writing quality, meet document length constraints, and build more accurate document summarization systems (Berg-Kirkpatrick et al., 2011; Vanderwende et al., 2007). Though word deletion is extensively used (e.g., (Clarke and Lapata, 2008)), state-of-the-art compression models (Cohn and Lapata, 2008; Rush et al., 2015) benefit crucially from data that can represent complex abstractive compression operations, including substitution of words and phrases and reordering. ∗ This research was conducted during the author’s internship at Microsoft Research. • This dataset is the first to provide compressions at the multi-sentence (two-sentence paragraph) level, which may present a stepping stone to whole document summarization. Many of these two-sentence paragraphs are compressed both as paragraphs and separately sentence-bysentence, offering data that may yield insights into the impact of multi-sentence operations"
D16-1033,P06-2109,0,0.0309074,"ses 575 single-reference sentence pairs. Automatic metrics: Early automatic metrics for evaluation of compressions include success rate (Jing, 2000), defined as accuracy of individual word or constituent deletion decisions; Simple String Accuracy (string edit distance), introduced by Bangalore et al. (2000) for natural language generation tasks; and Word Accuracy (Chiori and Furui, 2004), which generalizes Bangalore et al. (2000) to multiple references. Riezler et al. (2003) introduced the use of F-measure over grammatical relations. Word unigram and word-bigram F-measure have also been used (Unno et al., 2006; Filippova et al., 2015). Variants of ROUGE (Lin, 2004), used for summarization evaluation, have also been applied to sentence compressions (Rush et al., 2015). Riezler et al. (2003) show that F-measure over grammatical relations agrees with human ratings on the relative ranking of three systems at the corpus level. Clarke and Lapata (2006) evaluate two deletion-based automatic compression systems against a deletion-based gold-standard on sets of 20 sentences. Parse-based F-1 was shown to have high sentence-level Pearson’s ρ correlation with human judgments of overall quality, and to have hig"
D16-1033,Q16-1029,0,0.0351268,"ls are evaluated on the test set portion of our dataset. All models use the training portion of the data for training, and two models (Seq2Seq and NAMAS10 ) additionally use external training data. The external data is summarized in Table 7. The Gigaword set was extracted from the Annotated Gigaword (Napoles et al., 2012), using the implementation provided by Rush et al. (2015). The Headline data was extracted in similar fashion using an in-house news collection. 9 A similar insight was used in one of the component metrics of the SARI evaluation metric used for text simplification evaluation (Xu et al., 2016). 10 The original works introducing these models employed much larger training corpora, believed to be key to improving the accuracy of neutral network models with large parameter spaces. Data Abstractive Deletion-based Gigaword Headline Gigaword Headline #src tokens 114.1M 6.0M 1,353K 59K #trg tokens 30.0M 1.4M 329K 11K #sents 3.6M 0.2M 47K 2K Table 7: External data statistics. ILP: We use an open-source implementation11 of the semi-supervised ILP model described in (Clarke and Lapata, 2008). The model uses a trigram language model trained on a 9 million token subset of the OANC corpus. The I"
D16-1033,P11-1019,0,0.0125601,"ter if they compressed each sentence in a sequence independently. 15 This method has been standard for ROUGE, but has not for BLEU . We find that averaging sentence-level metrics is also advantageous for BLEU . 14 System T3 NAMAS Seq2Seq ILP Meaning 1.14 1.56 1.64 2.28 Grammar 1.40 1.30 1.51 2.22 Combined 1.26 1.43 1.57 2.25 4.3.2 Table 8: Average human ratings of system outputs for meaning and grammar separately and in combination. between dependent Pearson correlations with human judgments (Williams, 1959) as recommended for summarization evaluation (Graham, 2015) and other NLP tasks (e.g. (Yannakoudakis et al., 2011)). 4.3.1 Corpus-level metrics Table 8 shows the average human ratings of the four systems, separately in meaning and grammar, as well as the combined measure (an arithmetic mean of meaning and grammar judgments). Even though the performance of some systems is similar, the differences between all pairs of systems in meaning and grammar are significant p &lt; 0.0001 according to a paired t-test. It is interesting to note that ILP outperforms the more recently developed neural network systems Seq2Seq and NAMAS. This might seem to contradict recent results showing that the new models are superior to"
D16-1033,P13-2123,0,0.15085,"Missing"
D16-1033,D15-1042,0,\N,Missing
D17-1228,W11-0609,0,0.118393,"Missing"
D17-1228,P16-1154,0,0.0159742,"ot i’m building a wall, right now Kennedy bot today, i am asking the congress for a new program to make a new effort to increase the tax privileges and to stimulate Table 1: Example responses from our Star Wars, Hillary, Trump, and Kennedy bots with scented conversation models. Introduction Neural encoder-decoder models have demonstrated great promise in many sequence generation tasks, including neural machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Luong et al., 2015; Wu et al., 2016), image captioning (Xu et al., 2015), summarization (Rush et al., 2015; Gu et al., 2016; Kikuchi et al., 2016), and conversation generation (Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Shao et al., 2017; Li et al., 2015). These encouraging early successes have motivated research interest in training more natural-sounding conversational systems based on large volumes of open-domain human-to-human interactions. In order to create more human-like patterns of conversation, the agents need to have recognizable (and tunable) style, just as individual humans do, and also need to accept guidance from separate information processing modules in order to increase qualit"
D17-1228,D16-1140,0,0.0227497,"wall, right now Kennedy bot today, i am asking the congress for a new program to make a new effort to increase the tax privileges and to stimulate Table 1: Example responses from our Star Wars, Hillary, Trump, and Kennedy bots with scented conversation models. Introduction Neural encoder-decoder models have demonstrated great promise in many sequence generation tasks, including neural machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Luong et al., 2015; Wu et al., 2016), image captioning (Xu et al., 2015), summarization (Rush et al., 2015; Gu et al., 2016; Kikuchi et al., 2016), and conversation generation (Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Shao et al., 2017; Li et al., 2015). These encouraging early successes have motivated research interest in training more natural-sounding conversational systems based on large volumes of open-domain human-to-human interactions. In order to create more human-like patterns of conversation, the agents need to have recognizable (and tunable) style, just as individual humans do, and also need to accept guidance from separate information processing modules in order to increase quality of responses. In an e"
D17-1228,D16-1168,0,0.0250985,"with 64 chat contexts spanning a range of topics in politic, science, and technology: the sort of questions we might ask in an entertaining political debate.5 To test the model’s ability to control output topic in section 7.4.3, we also created one hint per question. 7.2 Network Setup and Implementation Our encoder and decoder RNNs contains twolayer stacked LSTMs. Each LSTM layer has a memory size of 500. The network weights are randomly initialized using a uniform distribution (−0.08, 0.08), and are trained with the ADAM optimizer (Kingma and Ba, 2014), with 3 http://www.presidency.ucsb.edu/ Koncel-Kedziorski et al. (2016) also uses Star Wars scripts to test theme rewriting of algebra word problems. 5 See the Supplementary material. 4 an initial learning rate of 0.002. Gradients were clipped so their norm does not exceed 5. Each mini-batch contains 200 answers and their questions. The words of input sentences were first converted to 300-dimensional vector representations learned from the RNN based language modeling tool word2vec (Mikolov et al., 2013). The beginning and end of each passage are also padded with a special boundary symbol. During decoding, our model generates 500 candidate samples in parallel, the"
D17-1228,P16-1094,1,0.881067,"he line of research initiated by (Ritter et al., 2011) and (Vinyals and Le, 2015) who treat generation of conversational dialog as a data-drive statistical machine translation (SMT) problem. Sordoni et al. (2015) extended (Ritter et al., 2011) by re-scoring SMT outputs using a neural encoder-decoder model conditioned on conversation history. Recently, researchers have used neural encoder-decoder models to directly generate responses in an end-to-end fashion without relying on SMT phrase tables(Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Shao et al., 2017; Li et al., 2015). Li et al. (2016) defined a “persona” as the character that an artificial agent, as actor, plays or performs during conversational interactions. Their dataset requires user identification for all speakers in the training set, while our methods treat the base data (millions of twitter conversations) as unlabeled, and the target persona is defined simply by a relatively small sample of their speech. In this sense, the persona can be any set of text data. In our experiments, for example, we used a generic Star Wars character that was based on the entire set of Star Wars scripts (in addition to 46 million base con"
D17-1228,D16-1230,0,0.166864,"Missing"
D17-1228,D15-1166,0,0.416556,"agic solo. Hillary bot i’m running for president, i’m going to be talking about some of these things Trump bot i’m building a wall, right now Kennedy bot today, i am asking the congress for a new program to make a new effort to increase the tax privileges and to stimulate Table 1: Example responses from our Star Wars, Hillary, Trump, and Kennedy bots with scented conversation models. Introduction Neural encoder-decoder models have demonstrated great promise in many sequence generation tasks, including neural machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Luong et al., 2015; Wu et al., 2016), image captioning (Xu et al., 2015), summarization (Rush et al., 2015; Gu et al., 2016; Kikuchi et al., 2016), and conversation generation (Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Shao et al., 2017; Li et al., 2015). These encouraging early successes have motivated research interest in training more natural-sounding conversational systems based on large volumes of open-domain human-to-human interactions. In order to create more human-like patterns of conversation, the agents need to have recognizable (and tunable) style, just as individual humans do,"
D17-1228,D11-1054,0,0.0762747,"ne translation and quickly achieved state-of-the-art results (Bahdanau et al., 2014; Luong et al., 2015). As an extension, the attention mechanism enables the decoder to revisit the input sequence’s hidden states and dynamically collects information needed for each decoding step. Specifically, our conversation model is established based on a combination of the models of (Bahdanau et al., 2014) and (Luong et al., 2015) that we found to be effective. In section 3, we describe the attention-based neural encoder-decoder model we used in detail. This work follows the line of research initiated by (Ritter et al., 2011) and (Vinyals and Le, 2015) who treat generation of conversational dialog as a data-drive statistical machine translation (SMT) problem. Sordoni et al. (2015) extended (Ritter et al., 2011) by re-scoring SMT outputs using a neural encoder-decoder model conditioned on conversation history. Recently, researchers have used neural encoder-decoder models to directly generate responses in an end-to-end fashion without relying on SMT phrase tables(Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Shao et al., 2017; Li et al., 2015). Li et al. (2016) defined a “persona” as the character"
D17-1228,D15-1044,0,0.036099,"hese things Trump bot i’m building a wall, right now Kennedy bot today, i am asking the congress for a new program to make a new effort to increase the tax privileges and to stimulate Table 1: Example responses from our Star Wars, Hillary, Trump, and Kennedy bots with scented conversation models. Introduction Neural encoder-decoder models have demonstrated great promise in many sequence generation tasks, including neural machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Luong et al., 2015; Wu et al., 2016), image captioning (Xu et al., 2015), summarization (Rush et al., 2015; Gu et al., 2016; Kikuchi et al., 2016), and conversation generation (Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Shao et al., 2017; Li et al., 2015). These encouraging early successes have motivated research interest in training more natural-sounding conversational systems based on large volumes of open-domain human-to-human interactions. In order to create more human-like patterns of conversation, the agents need to have recognizable (and tunable) style, just as individual humans do, and also need to accept guidance from separate information processing modules in order t"
D17-1228,P15-1152,0,0.255575,"Missing"
D17-1228,N15-1020,1,0.953863,"gram to make a new effort to increase the tax privileges and to stimulate Table 1: Example responses from our Star Wars, Hillary, Trump, and Kennedy bots with scented conversation models. Introduction Neural encoder-decoder models have demonstrated great promise in many sequence generation tasks, including neural machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Luong et al., 2015; Wu et al., 2016), image captioning (Xu et al., 2015), summarization (Rush et al., 2015; Gu et al., 2016; Kikuchi et al., 2016), and conversation generation (Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Shao et al., 2017; Li et al., 2015). These encouraging early successes have motivated research interest in training more natural-sounding conversational systems based on large volumes of open-domain human-to-human interactions. In order to create more human-like patterns of conversation, the agents need to have recognizable (and tunable) style, just as individual humans do, and also need to accept guidance from separate information processing modules in order to increase quality of responses. In an extreme case, an agent may be micro-managed by a human user who uses the n"
D17-1228,D15-1199,0,0.0284613,"p(S|T ) empirically perform much better than p(T |S) on the relevance ranking task. Therefore, we directly apply Bayes’ rule to Equation 1, as in statistical machine translation (Brown et al., 1993), and use:  Tˆ = arg max log p(S|T ) + log p(T )}. (2) T Since p(T |S) is empirically biased towards p(T ), in practice, this objective also resembles the Maximum Mutual Information (MMI) objective function in (Li et al., 2015). The challenge now is to develop an effective search algorithm for a target words sequence that maximize the product in Equation 2. Here, we follow a similar process as in (Wen et al., 2015) which generates multiple target hypotheses with stochastic sampling based on p(T |S), and then ranks them with the objective function 2 above. However, as also observed by (Shao et al., 2017), step-by-step naive sampling can accumulate errors as the sequence gets longer. To reduce language errors of stochastic sampling, we introduce a sample selector to choose the next token among N stochastically sampled tokens based on the predicted output word distributions. The sample selector, which is a multilayer perceptron in our experiments, takes the following features: 1) the log-probability of cur"
D17-1228,P15-2116,1,0.862309,"Missing"
D17-1228,J93-2003,0,\N,Missing
D19-1190,I17-2069,0,0.0689783,"different styles into a shared latent space where the ”content” information is preserved and ”style” information is discarded. An adversarial discriminator is used to align the latent spaces of two different styles. However, Yang et al. (2018) point out the difficulty of training an adversarial discriminator and proposed instead the use of language models as discriminator. Like Shen et al. (2017); Yang et al. (2018), we align latent spaces for different styles. However we also align latent spaces encoded by different models (S2S and AE). Stylized response generation is a relatively new task. Akama et al. (2017) use a stylized conversation corpus to fine-tune a conversation model pretrained on a background conversation dataset. However, stylized texts are usually in non-conversational format, as in the present setting. Niu and Bansal (2018) proposed a method that takes the weighted average of the token probability distribution predicted by a S2S trained on background conversational dataset and that predicted by a LM trained on style dataset as the token probability. They observed reduced relevance and attributed this to the fact that the LM was not trained to attend to conversation context and S2S wa"
D19-1190,W14-4012,0,0.0260137,"Missing"
D19-1190,N19-1125,1,0.816761,"oder to promote generation of response for that target speaker. However non-conversational data cannot be used. Luan et al. (2017) applied a multi-task learning approach to utilize non-conversational data. A S2S model, taking in conversational data, and an autoencoder (AE), taking in nonconversational data, share the decoder and are trained alternately. However, Gao et al. (2019b) observed that sharing the decoder may not truly allow S2S and AE to share the latent space, and thus S2S may not fully utilize what is learned by AE. Unlike Li et al. (2016b) using labelled persona IDs, Zhang et al. (2019) have proposed using a self-supervised method to extract persona features from conversation history. This allows modeling persona dynamically, which agrees with the fact that even the same person can speak in different style in different scenarios. Multi-task learning McCann et al. (2018); Liu et al. (2019); Luan et al. (2017); Gao et al. (2019b); Zhang et al. (2017) aggregates the strengths of each specific task, and induces regularization effects (Liu et al., 2019) as the model is trained to learn a more universal representation. However a simple multi-task approach (Luan et al., 2017) may l"
D19-1190,N19-1320,0,0.0371334,"ecific responses from conversational data and non-parallel non-conversation style data. 2) We generalize the S PACE F USION model of (Gao et al., 2019b) to non-parallel data by a new 2 Integrated into Microsoft Icecaps toolkit (Shiv et al., 2019) https://github.com/microsoft/ icecaps. regularization method. 3) We present a visualization analysis that provides intuitive insights into the drawbacks of alternative approaches. 2 Related Work Text style transfer is a related but distinct task. It usually preserves the content (Yang et al., 2018; Hu et al., 2017; Fu et al., 2018; Shen et al., 2017; Gong et al., 2019). In contrast, content of conversational responses in a given context can be semantically diverse. Various approaches have been proposed for non-parallel data setup. Fu et al. (2018) proposed to use separate decoders for different styles and a classifier to measure style strength. Shen et al. (2017) proposed to map texts of two different styles into a shared latent space where the ”content” information is preserved and ”style” information is discarded. An adversarial discriminator is used to align the latent spaces of two different styles. However, Yang et al. (2018) point out the difficulty o"
D19-1190,N16-1014,1,0.955002,"and continuously control the style level. We demonstrate this method using dialogues from Reddit data and two sets of sentences with distinct styles (arXiv and Sherlock Holmes novels). Automatic and human evaluation show that, without sacrificing appropriateness, the system generates responses of the targeted style and outperforms competitive baselines. 1 1 Introduction A social chatbot designed to establish long-term emotional connections with users must generate responses that not only match the content of user input and context, but also do so in a desired target style (Zhou et al., 2018; Li et al., 2016b; Luan et al., 2016; Gao et al., 2019a). A conversational agent that speaks in a polite, professional tone is likely to facilitate service in customer relationship scenarios; likewise, an agent that sounds like an cartoon character or a superhero can be more engaging in a theme park. The master of response style is also an important step towards humanlike chatbots. As highlighted in social psychology studies (Niederhoffer and Pennebaker, 2002a,b), when two people are talking, they tend to match ∗ Now at Alexa AI, Amazon. An implementation of our model and the scripts to generate the datasets"
D19-1190,P16-1094,1,0.952451,"and continuously control the style level. We demonstrate this method using dialogues from Reddit data and two sets of sentences with distinct styles (arXiv and Sherlock Holmes novels). Automatic and human evaluation show that, without sacrificing appropriateness, the system generates responses of the targeted style and outperforms competitive baselines. 1 1 Introduction A social chatbot designed to establish long-term emotional connections with users must generate responses that not only match the content of user input and context, but also do so in a desired target style (Zhou et al., 2018; Li et al., 2016b; Luan et al., 2016; Gao et al., 2019a). A conversational agent that speaks in a polite, professional tone is likely to facilitate service in customer relationship scenarios; likewise, an agent that sounds like an cartoon character or a superhero can be more engaging in a theme park. The master of response style is also an important step towards humanlike chatbots. As highlighted in social psychology studies (Niederhoffer and Pennebaker, 2002a,b), when two people are talking, they tend to match ∗ Now at Alexa AI, Amazon. An implementation of our model and the scripts to generate the datasets"
D19-1190,P19-1441,1,0.789342,"ional data, share the decoder and are trained alternately. However, Gao et al. (2019b) observed that sharing the decoder may not truly allow S2S and AE to share the latent space, and thus S2S may not fully utilize what is learned by AE. Unlike Li et al. (2016b) using labelled persona IDs, Zhang et al. (2019) have proposed using a self-supervised method to extract persona features from conversation history. This allows modeling persona dynamically, which agrees with the fact that even the same person can speak in different style in different scenarios. Multi-task learning McCann et al. (2018); Liu et al. (2019); Luan et al. (2017); Gao et al. (2019b); Zhang et al. (2017) aggregates the strengths of each specific task, and induces regularization effects (Liu et al., 2019) as the model is trained to learn a more universal representation. However a simple multi-task approach (Luan et al., 2017) may learn separate representations for each dataset (Gao et al., 2019b). To address this, in previous work (Gao et al., 2019b), we proposed the S PACE F USION model featuring a regularization technique that explicitly encourages alignment of latent spaces for a universal representation. S PACE F USION, however,"
D19-1190,I17-1061,1,0.723732,"ghly correspond to contents and style intensity, respectively, illustrated by examples taken from Table 2. linguistic style of each other, sometime even regardless of their intentions. Achieving this level of performance, however, is challenging. Lacking parallel data in different conversational styles, researchers often resort to what we will term style datasets that are in non-conversational format (e.g. news, novels, blogs). Since the contents and formats of these are quite different from conversation data, existing approaches tend to generate responses that are either less style-specific (Luan et al., 2017) or less context-relevant (Niu and Bansal, 2018). We suggest that this trade-off between appropriateness and style stems from profound differences 1814 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1814–1823, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics between conversation and style datasets in format, style and contents that impede joint learning. One approach has been to combine these only during decoding: Niu and Bansal (2018) t"
D19-1190,P16-2020,0,0.0288575,"control the style level. We demonstrate this method using dialogues from Reddit data and two sets of sentences with distinct styles (arXiv and Sherlock Holmes novels). Automatic and human evaluation show that, without sacrificing appropriateness, the system generates responses of the targeted style and outperforms competitive baselines. 1 1 Introduction A social chatbot designed to establish long-term emotional connections with users must generate responses that not only match the content of user input and context, but also do so in a desired target style (Zhou et al., 2018; Li et al., 2016b; Luan et al., 2016; Gao et al., 2019a). A conversational agent that speaks in a polite, professional tone is likely to facilitate service in customer relationship scenarios; likewise, an agent that sounds like an cartoon character or a superhero can be more engaging in a theme park. The master of response style is also an important step towards humanlike chatbots. As highlighted in social psychology studies (Niederhoffer and Pennebaker, 2002a,b), when two people are talking, they tend to match ∗ Now at Alexa AI, Amazon. An implementation of our model and the scripts to generate the datasets are available at htt"
D19-1190,Q18-1027,0,0.414647,"ty, respectively, illustrated by examples taken from Table 2. linguistic style of each other, sometime even regardless of their intentions. Achieving this level of performance, however, is challenging. Lacking parallel data in different conversational styles, researchers often resort to what we will term style datasets that are in non-conversational format (e.g. news, novels, blogs). Since the contents and formats of these are quite different from conversation data, existing approaches tend to generate responses that are either less style-specific (Luan et al., 2017) or less context-relevant (Niu and Bansal, 2018). We suggest that this trade-off between appropriateness and style stems from profound differences 1814 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1814–1823, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics between conversation and style datasets in format, style and contents that impede joint learning. One approach has been to combine these only during decoding: Niu and Bansal (2018) trained two models separately, a Sequence-to-Sequ"
D19-1190,P02-1040,0,0.104322,"Missing"
D19-1190,P19-1539,1,0.883551,"Missing"
D19-1190,P19-3021,1,0.834508,"m Reddit data and two sets of sentences with distinct styles (arXiv and Sherlock Holmes novels). Automatic and human evaluation show that, without sacrificing appropriateness, our system can generate responses in a targeted style and outperforms competitive baselines. Our contribution can be summarized thus: 1) We introduce an end-to-end approach that generates style-specific responses from conversational data and non-parallel non-conversation style data. 2) We generalize the S PACE F USION model of (Gao et al., 2019b) to non-parallel data by a new 2 Integrated into Microsoft Icecaps toolkit (Shiv et al., 2019) https://github.com/microsoft/ icecaps. regularization method. 3) We present a visualization analysis that provides intuitive insights into the drawbacks of alternative approaches. 2 Related Work Text style transfer is a related but distinct task. It usually preserves the content (Yang et al., 2018; Hu et al., 2017; Fu et al., 2018; Shen et al., 2017; Gong et al., 2019). In contrast, content of conversational responses in a given context can be semantically diverse. Various approaches have been proposed for non-parallel data setup. Fu et al. (2018) proposed to use separate decoders for differe"
D19-1325,D11-1033,0,0.0282286,"for text style transfer with a de-noising auto-encoding objective (Logeswaran et al., 2018; Subramanian et al., 2018). Our work differs in that we leverage domain adaptation to deal with limited target domain data, whereas previous methods require massive target domain style-labelled samples. Domain Adaptation. Domain adaptation has been studied in various natural language processing tasks, such as sentiment classification (Qu et al., 2019), dialogue system (Wen et al., 2016), abstractive summarization (Hua and Wang, 2017; Zhang et al., 2018b), machine translation (Koehn and Schroeder, 2007; Axelrod et al., 2011; Sennrich et al., 2016b; Michel and Neubig, 2018), etc. However, little or no work explores domain adaptation on text style transfer. To the best of our knowledge, we are the first to explore the adaptation of text style transfer models to a new domain with limited non-parallel data available. The task requires both style transfer and domain-specific generation on the target domain. To differentiate different domains, Sennrich et al. (2016a); Chu et al. (2017) appended domain tokens to the input sentences. Our model uses learnable domain vectors combining domain-specific style classifiers, wh"
D19-1325,P17-2061,0,0.0455215,"Missing"
D19-1325,N19-1320,0,0.129695,"s for style transfer tasks (sentiment and formality) on multiple target domains where only limited non-parallel data is available. Our implementation is available at https://github.com/ cookielee77/DAST. 2 Related Work Text Style Transfer. Text style transfer using neural networks has been widely studied in the past few years. A common paradigm is to first disentangle latent space as content and style features, and then generate stylistic sentences by tweaking the style-relevant features and passing through a decoder. Hu et al. (2017); Fu et al. (2018); Shen et al. (2017); Yang et al. (2018); Gong et al. (2019); Lin et al. (2017) explored this direction by assuming the disentanglement can be achieved in an auto-encoding procedure with a suitable style regularization, implemented by either adversarial discriminators or style classifiers. Li et al. (2018); Xu et al. (2018); Zhang et al. (2018c) achieved disentanglement by filtering the stylistic words of input sentences. Recently, Prabhumoye et al. (2018) has proposed to use back-translation for text style transfer with a de-noising auto-encoding objective (Logeswaran et al., 2018; Subramanian et al., 2018). Our work differs in that we leverage domain"
D19-1325,W18-2503,0,0.0980091,"Missing"
D19-1325,W17-4513,0,0.0215605,"f input sentences. Recently, Prabhumoye et al. (2018) has proposed to use back-translation for text style transfer with a de-noising auto-encoding objective (Logeswaran et al., 2018; Subramanian et al., 2018). Our work differs in that we leverage domain adaptation to deal with limited target domain data, whereas previous methods require massive target domain style-labelled samples. Domain Adaptation. Domain adaptation has been studied in various natural language processing tasks, such as sentiment classification (Qu et al., 2019), dialogue system (Wen et al., 2016), abstractive summarization (Hua and Wang, 2017; Zhang et al., 2018b), machine translation (Koehn and Schroeder, 2007; Axelrod et al., 2011; Sennrich et al., 2016b; Michel and Neubig, 2018), etc. However, little or no work explores domain adaptation on text style transfer. To the best of our knowledge, we are the first to explore the adaptation of text style transfer models to a new domain with limited non-parallel data available. The task requires both style transfer and domain-specific generation on the target domain. To differentiate different domains, Sennrich et al. (2016a); Chu et al. (2017) appended domain tokens to the input senten"
D19-1325,W17-4902,0,0.130872,"to edit an input sentence with the desired style while preserving style-irrelevant content, has received increasing attention in recent years. It has been applied successfully to stylized image captioning (Gan et al., 2017), personalized conversational response generation (Zhang et al., 2018a), formalized writing (Rao and Tetreault, 2018), offensive to nonoffensive language transfer (dos Santos et al., 2018), and other stylized text generation tasks (Akama et al., 2017; Zhang et al., 2019). Text style transfer has been explored as a sequence-to-sequence learning task using parallel datasets (Jhamtani et al., 2017). However, parallel datasets are often not available, and handannotating sentences in different styles is expensive. The recent surge of deep generative models (Kingma and Welling, 2013; Goodfellow et al., 2014) has spurred progress in text style transfer without parallel data by learning disentanglement (Hu et al., 2017; Shen et al., 2017; Fu et al., 2018; Li et al., 2018; Prabhumoye et al., 2018). These methods typically require massive amounts of data (Subramanian et al., 2018), and may perform poorly in limited data scenarios. A natural solution to the data-scarcity issue is to resort to m"
D19-1325,D14-1181,0,0.00292312,"ing in 2700 responses in total. 5.3 Experimental Setup The encoder E and the decoder D are implemented by one-layer GRU (Cho et al., 2014) with hidden dimensions 500 and 700, respectively. The domain-vector dimension is set to 50. The style labels are represented by learnable vectors with 150 dimensions. The decoder is initialized by a concatenation of representations of content, style, and domain vectors. If domain vectors are not used, the dimension of style labels is set to 200; accordingly, the initialization of the decoder is a concatenation of content and style representations. TextCNN (Kim, 2014) is employed for the domain-specific style classifiers pre-trained on corresponding domains. After pre-training, the parameters of the classifiers are fixed. We use the hard-sampling trick (Logeswaran et al., 2018) to back-propagate the loss through discrete tokens from the classifier to the encoder-decoder model. During training, we assign each mini-batch the same amount of source and target data to balance the training. We make an extensive comparison with five state-of-the-art text style transfer models: CrossAlign (Shen et al., 2017), Delete&Retrieve (Li et al., 2018), CycleRL (Xu et al.,"
D19-1325,W07-0733,0,0.0707096,"sed to use back-translation for text style transfer with a de-noising auto-encoding objective (Logeswaran et al., 2018; Subramanian et al., 2018). Our work differs in that we leverage domain adaptation to deal with limited target domain data, whereas previous methods require massive target domain style-labelled samples. Domain Adaptation. Domain adaptation has been studied in various natural language processing tasks, such as sentiment classification (Qu et al., 2019), dialogue system (Wen et al., 2016), abstractive summarization (Hua and Wang, 2017; Zhang et al., 2018b), machine translation (Koehn and Schroeder, 2007; Axelrod et al., 2011; Sennrich et al., 2016b; Michel and Neubig, 2018), etc. However, little or no work explores domain adaptation on text style transfer. To the best of our knowledge, we are the first to explore the adaptation of text style transfer models to a new domain with limited non-parallel data available. The task requires both style transfer and domain-specific generation on the target domain. To differentiate different domains, Sennrich et al. (2016a); Chu et al. (2017) appended domain tokens to the input sentences. Our model uses learnable domain vectors combining domain-specific"
D19-1325,N18-1169,0,0.380599,"sfer (dos Santos et al., 2018), and other stylized text generation tasks (Akama et al., 2017; Zhang et al., 2019). Text style transfer has been explored as a sequence-to-sequence learning task using parallel datasets (Jhamtani et al., 2017). However, parallel datasets are often not available, and handannotating sentences in different styles is expensive. The recent surge of deep generative models (Kingma and Welling, 2013; Goodfellow et al., 2014) has spurred progress in text style transfer without parallel data by learning disentanglement (Hu et al., 2017; Shen et al., 2017; Fu et al., 2018; Li et al., 2018; Prabhumoye et al., 2018). These methods typically require massive amounts of data (Subramanian et al., 2018), and may perform poorly in limited data scenarios. A natural solution to the data-scarcity issue is to resort to massive data from other domains. However, directly leveraging abundant data from other domains is problematic due to the discrepancies in data distribution on different domains. Different domains generally manifest themselves in domain-specific lexica. For example, sentiment adjectives such as “delicious”, “tasty”, and “disgusting” in restaurant reviews might be out of plac"
D19-1325,P18-2050,0,0.0184592,"o-encoding objective (Logeswaran et al., 2018; Subramanian et al., 2018). Our work differs in that we leverage domain adaptation to deal with limited target domain data, whereas previous methods require massive target domain style-labelled samples. Domain Adaptation. Domain adaptation has been studied in various natural language processing tasks, such as sentiment classification (Qu et al., 2019), dialogue system (Wen et al., 2016), abstractive summarization (Hua and Wang, 2017; Zhang et al., 2018b), machine translation (Koehn and Schroeder, 2007; Axelrod et al., 2011; Sennrich et al., 2016b; Michel and Neubig, 2018), etc. However, little or no work explores domain adaptation on text style transfer. To the best of our knowledge, we are the first to explore the adaptation of text style transfer models to a new domain with limited non-parallel data available. The task requires both style transfer and domain-specific generation on the target domain. To differentiate different domains, Sennrich et al. (2016a); Chu et al. (2017) appended domain tokens to the input sentences. Our model uses learnable domain vectors combining domain-specific style classifiers, which force the model to learn distinct stylized inf"
D19-1325,N19-1049,0,0.106949,"3 82.1 86.6 96.0 98.5 96.7 90.3 92.6 4.8 4.1 1.4 1.2 3.7 13.9 17.8 20.1 8.5 7.6 5.2 4.8 8.6 18.5 19.3 23.1 96.0 94.8 94.6 83.2 63.0 79.5 87.2 83.2 89.2 78.2 82.7 2.0 6.9 0.7 0.4 1.9 11.3 20.1 21.0 5.9 9.3 3.8 3.2 5.8 14.4 21.6 23.1 Table 2: Automatic evaluation results on Yelp and Amazon test sets. D-acc and S-acc denote domain accuracy and style accuracy, respectively. G-score is the geometric mean of S-acc and hBLEU. Human Evaluation. To assess the quality of transferred sentences, we conduct human evaluations based on the facets of content preservation, style control and fluency, following Mir et al. (2019). Previous works (Subramanian et al., 2018; Gong et al., 2019) ask workers to evaluate the quality via a numerical score, however, we found that this empirically leads to high-variance results. Instead, we pair transferred sentences from two different models, and ask workers to choose the sentence they prefer when compared to the input on each evaluation aspect. We provide a “No Preference” option to choose when the workers think the qualities of the two sentences are indistinguishable. Details of the human evaluation instruction are included in Appendix A.3. For each testing, we randomly samp"
D19-1325,P02-1040,0,0.104103,"hang et al., 2015). We split the 7k sentimental questions into 4k/2k/1k for train/dev/test sets, respectively. Note that the Yahoo sentiment dataset only consists of questions, which have different domain characteristics with the IMDB dataset. In all the sentiment experiments, we consider both transfer directions (positive-to-negative and negative-to-positive). Evaluation Automatic Metrics. We evaluate the effectiveness of our DAST models based on three automatic metrics: (i) Content Preservation. We assess the content preservation according to n-gram statistics, by measuring the BLEU scores (Papineni et al., 2002) between generated sentences and human references on the target domain, refered as human BLEU (hBLEU). When no human reference is available (e.g., Yahoo), we compute the BLEU scores with respect to the input sentences. (ii) Style Control. We generate samples from the model and measure the style accuracy with a style classifier that is pre-trained on the target domain. We refer the style accuracy as S-acc. (iii) Domain Control. To validate whether the generated sentences hold the characteristics of the target domain, we adopt a pre-trained domain classifier to measure the percentage of generate"
D19-1325,P18-1080,0,0.22033,"et al., 2018), and other stylized text generation tasks (Akama et al., 2017; Zhang et al., 2019). Text style transfer has been explored as a sequence-to-sequence learning task using parallel datasets (Jhamtani et al., 2017). However, parallel datasets are often not available, and handannotating sentences in different styles is expensive. The recent surge of deep generative models (Kingma and Welling, 2013; Goodfellow et al., 2014) has spurred progress in text style transfer without parallel data by learning disentanglement (Hu et al., 2017; Shen et al., 2017; Fu et al., 2018; Li et al., 2018; Prabhumoye et al., 2018). These methods typically require massive amounts of data (Subramanian et al., 2018), and may perform poorly in limited data scenarios. A natural solution to the data-scarcity issue is to resort to massive data from other domains. However, directly leveraging abundant data from other domains is problematic due to the discrepancies in data distribution on different domains. Different domains generally manifest themselves in domain-specific lexica. For example, sentiment adjectives such as “delicious”, “tasty”, and “disgusting” in restaurant reviews might be out of place in movie reviews, where"
D19-1325,N19-1258,1,0.833004,"Zhang et al. (2018c) achieved disentanglement by filtering the stylistic words of input sentences. Recently, Prabhumoye et al. (2018) has proposed to use back-translation for text style transfer with a de-noising auto-encoding objective (Logeswaran et al., 2018; Subramanian et al., 2018). Our work differs in that we leverage domain adaptation to deal with limited target domain data, whereas previous methods require massive target domain style-labelled samples. Domain Adaptation. Domain adaptation has been studied in various natural language processing tasks, such as sentiment classification (Qu et al., 2019), dialogue system (Wen et al., 2016), abstractive summarization (Hua and Wang, 2017; Zhang et al., 2018b), machine translation (Koehn and Schroeder, 2007; Axelrod et al., 2011; Sennrich et al., 2016b; Michel and Neubig, 2018), etc. However, little or no work explores domain adaptation on text style transfer. To the best of our knowledge, we are the first to explore the adaptation of text style transfer models to a new domain with limited non-parallel data available. The task requires both style transfer and domain-specific generation on the target domain. To differentiate different domains, Se"
D19-1325,N18-1012,0,0.16484,"s on two style transfer tasks (sentiment and formality) over multiple target domains where only limited non-parallel data is available. Extensive experiments demonstrate the effectiveness of the proposed model compared to the baselines. 1 Introduction Text style transfer, which aims to edit an input sentence with the desired style while preserving style-irrelevant content, has received increasing attention in recent years. It has been applied successfully to stylized image captioning (Gan et al., 2017), personalized conversational response generation (Zhang et al., 2018a), formalized writing (Rao and Tetreault, 2018), offensive to nonoffensive language transfer (dos Santos et al., 2018), and other stylized text generation tasks (Akama et al., 2017; Zhang et al., 2019). Text style transfer has been explored as a sequence-to-sequence learning task using parallel datasets (Jhamtani et al., 2017). However, parallel datasets are often not available, and handannotating sentences in different styles is expensive. The recent surge of deep generative models (Kingma and Welling, 2013; Goodfellow et al., 2014) has spurred progress in text style transfer without parallel data by learning disentanglement (Hu et al., 2"
D19-1325,N16-1015,0,0.0551628,"Missing"
D19-1325,P18-1205,0,0.171725,"are manner. We evaluate the proposed models on two style transfer tasks (sentiment and formality) over multiple target domains where only limited non-parallel data is available. Extensive experiments demonstrate the effectiveness of the proposed model compared to the baselines. 1 Introduction Text style transfer, which aims to edit an input sentence with the desired style while preserving style-irrelevant content, has received increasing attention in recent years. It has been applied successfully to stylized image captioning (Gan et al., 2017), personalized conversational response generation (Zhang et al., 2018a), formalized writing (Rao and Tetreault, 2018), offensive to nonoffensive language transfer (dos Santos et al., 2018), and other stylized text generation tasks (Akama et al., 2017; Zhang et al., 2019). Text style transfer has been explored as a sequence-to-sequence learning task using parallel datasets (Jhamtani et al., 2017). However, parallel datasets are often not available, and handannotating sentences in different styles is expensive. The recent surge of deep generative models (Kingma and Welling, 2013; Goodfellow et al., 2014) has spurred progress in text style transfer without paralle"
D19-1325,N18-1138,0,0.101211,"are manner. We evaluate the proposed models on two style transfer tasks (sentiment and formality) over multiple target domains where only limited non-parallel data is available. Extensive experiments demonstrate the effectiveness of the proposed model compared to the baselines. 1 Introduction Text style transfer, which aims to edit an input sentence with the desired style while preserving style-irrelevant content, has received increasing attention in recent years. It has been applied successfully to stylized image captioning (Gan et al., 2017), personalized conversational response generation (Zhang et al., 2018a), formalized writing (Rao and Tetreault, 2018), offensive to nonoffensive language transfer (dos Santos et al., 2018), and other stylized text generation tasks (Akama et al., 2017; Zhang et al., 2019). Text style transfer has been explored as a sequence-to-sequence learning task using parallel datasets (Jhamtani et al., 2017). However, parallel datasets are often not available, and handannotating sentences in different styles is expensive. The recent surge of deep generative models (Kingma and Welling, 2013; Goodfellow et al., 2014) has spurred progress in text style transfer without paralle"
D19-1325,D18-1138,0,0.12268,"are manner. We evaluate the proposed models on two style transfer tasks (sentiment and formality) over multiple target domains where only limited non-parallel data is available. Extensive experiments demonstrate the effectiveness of the proposed model compared to the baselines. 1 Introduction Text style transfer, which aims to edit an input sentence with the desired style while preserving style-irrelevant content, has received increasing attention in recent years. It has been applied successfully to stylized image captioning (Gan et al., 2017), personalized conversational response generation (Zhang et al., 2018a), formalized writing (Rao and Tetreault, 2018), offensive to nonoffensive language transfer (dos Santos et al., 2018), and other stylized text generation tasks (Akama et al., 2017; Zhang et al., 2019). Text style transfer has been explored as a sequence-to-sequence learning task using parallel datasets (Jhamtani et al., 2017). However, parallel datasets are often not available, and handannotating sentences in different styles is expensive. The recent surge of deep generative models (Kingma and Welling, 2013; Goodfellow et al., 2014) has spurred progress in text style transfer without paralle"
D19-1325,P18-2031,0,0.0226802,"t domains where only limited non-parallel data is available. Extensive experiments demonstrate the effectiveness of the proposed model compared to the baselines. 1 Introduction Text style transfer, which aims to edit an input sentence with the desired style while preserving style-irrelevant content, has received increasing attention in recent years. It has been applied successfully to stylized image captioning (Gan et al., 2017), personalized conversational response generation (Zhang et al., 2018a), formalized writing (Rao and Tetreault, 2018), offensive to nonoffensive language transfer (dos Santos et al., 2018), and other stylized text generation tasks (Akama et al., 2017; Zhang et al., 2019). Text style transfer has been explored as a sequence-to-sequence learning task using parallel datasets (Jhamtani et al., 2017). However, parallel datasets are often not available, and handannotating sentences in different styles is expensive. The recent surge of deep generative models (Kingma and Welling, 2013; Goodfellow et al., 2014) has spurred progress in text style transfer without parallel data by learning disentanglement (Hu et al., 2017; Shen et al., 2017; Fu et al., 2018; Li et al., 2018; Prabhumoye et"
D19-1325,N16-1005,0,0.0379604,"er with a de-noising auto-encoding objective (Logeswaran et al., 2018; Subramanian et al., 2018). Our work differs in that we leverage domain adaptation to deal with limited target domain data, whereas previous methods require massive target domain style-labelled samples. Domain Adaptation. Domain adaptation has been studied in various natural language processing tasks, such as sentiment classification (Qu et al., 2019), dialogue system (Wen et al., 2016), abstractive summarization (Hua and Wang, 2017; Zhang et al., 2018b), machine translation (Koehn and Schroeder, 2007; Axelrod et al., 2011; Sennrich et al., 2016b; Michel and Neubig, 2018), etc. However, little or no work explores domain adaptation on text style transfer. To the best of our knowledge, we are the first to explore the adaptation of text style transfer models to a new domain with limited non-parallel data available. The task requires both style transfer and domain-specific generation on the target domain. To differentiate different domains, Sennrich et al. (2016a); Chu et al. (2017) appended domain tokens to the input sentences. Our model uses learnable domain vectors combining domain-specific style classifiers, which force the model to"
D19-1325,P16-1009,0,0.0443277,"er with a de-noising auto-encoding objective (Logeswaran et al., 2018; Subramanian et al., 2018). Our work differs in that we leverage domain adaptation to deal with limited target domain data, whereas previous methods require massive target domain style-labelled samples. Domain Adaptation. Domain adaptation has been studied in various natural language processing tasks, such as sentiment classification (Qu et al., 2019), dialogue system (Wen et al., 2016), abstractive summarization (Hua and Wang, 2017; Zhang et al., 2018b), machine translation (Koehn and Schroeder, 2007; Axelrod et al., 2011; Sennrich et al., 2016b; Michel and Neubig, 2018), etc. However, little or no work explores domain adaptation on text style transfer. To the best of our knowledge, we are the first to explore the adaptation of text style transfer models to a new domain with limited non-parallel data available. The task requires both style transfer and domain-specific generation on the target domain. To differentiate different domains, Sennrich et al. (2016a); Chu et al. (2017) appended domain tokens to the input sentences. Our model uses learnable domain vectors combining domain-specific style classifiers, which force the model to"
D19-1325,D14-1179,0,\N,Missing
D19-1325,I17-2069,0,\N,Missing
D19-1325,P18-1090,0,\N,Missing
D19-1325,P19-3027,0,\N,Missing
D19-5604,W18-6536,0,0.0178141,"nt, but also the cross-attention between source and generation. our simple variant model for generating a common question. iii) an empirical evaluation framework based on automated metrics and human judgments on answerability, relevancy, and fluency to extensively evaluate our proposed MSQG model against the baselines. 2 Related Work The use of neural networks to generate natural language questions has mostly focused on question answering (Labutov et al., 2015; Serban et al., 2016; Rothe et al., 2016; Song et al., 2017; Duan et al., 2017; Du et al., 2017; Buck et al., 2017; Song et al., 2018; Harrison and Walker, 2018; Sun et al., 2018). A number of works process multiple passages by concatenating, adding, or attentionweight-summing among passage features into a single feature, and use it for downstream tasks (Zoph and Knight, 2016; Garmash and Monz, 2016; Libovick´y and Helcl, 2017; Wang et al., 2018; Yan et al., 2018; Lebanoff et al., 2018; Celikyilmaz et al., 2018; Nishimura et al., 2018; Libovick´y et al., 2018; Li et al., 2018b; Nishida et al., 2019). Our processing mechanisms are similar to Garmash and Monz (2016), Firat et al. (2016), and Dong and Smith (2018). The information retrieval literature i"
D19-5604,N18-1150,0,0.0198586,"works to generate natural language questions has mostly focused on question answering (Labutov et al., 2015; Serban et al., 2016; Rothe et al., 2016; Song et al., 2017; Duan et al., 2017; Du et al., 2017; Buck et al., 2017; Song et al., 2018; Harrison and Walker, 2018; Sun et al., 2018). A number of works process multiple passages by concatenating, adding, or attentionweight-summing among passage features into a single feature, and use it for downstream tasks (Zoph and Knight, 2016; Garmash and Monz, 2016; Libovick´y and Helcl, 2017; Wang et al., 2018; Yan et al., 2018; Lebanoff et al., 2018; Celikyilmaz et al., 2018; Nishimura et al., 2018; Libovick´y et al., 2018; Li et al., 2018b; Nishida et al., 2019). Our processing mechanisms are similar to Garmash and Monz (2016), Firat et al. (2016), and Dong and Smith (2018). The information retrieval literature is primarily concerned with reformulating queries, by either selecting expansion terms from candidates as in pseudo-relevance feedback (Salton, 1971; Zhai and Lafferty, 2001; Xu and Croft, 1996; Metzler and Croft, 2007; Cao et al., 2008; Bernhard, 2010; Nogueira and Cho, 2017; Li et al., 2018a). Our task differs because there is no supervision unlike mult"
D19-5604,P17-1123,0,0.0149318,"only encoded latent representation from a source document, but also the cross-attention between source and generation. our simple variant model for generating a common question. iii) an empirical evaluation framework based on automated metrics and human judgments on answerability, relevancy, and fluency to extensively evaluate our proposed MSQG model against the baselines. 2 Related Work The use of neural networks to generate natural language questions has mostly focused on question answering (Labutov et al., 2015; Serban et al., 2016; Rothe et al., 2016; Song et al., 2017; Duan et al., 2017; Du et al., 2017; Buck et al., 2017; Song et al., 2018; Harrison and Walker, 2018; Sun et al., 2018). A number of works process multiple passages by concatenating, adding, or attentionweight-summing among passage features into a single feature, and use it for downstream tasks (Zoph and Knight, 2016; Garmash and Monz, 2016; Libovick´y and Helcl, 2017; Wang et al., 2018; Yan et al., 2018; Lebanoff et al., 2018; Celikyilmaz et al., 2018; Nishimura et al., 2018; Libovick´y et al., 2018; Li et al., 2018b; Nishida et al., 2019). Our processing mechanisms are similar to Garmash and Monz (2016), Firat et al. (2016),"
D19-5604,D18-1478,0,0.0430011,"Missing"
D19-5604,D17-1090,0,0.022548,"ion comes from not only encoded latent representation from a source document, but also the cross-attention between source and generation. our simple variant model for generating a common question. iii) an empirical evaluation framework based on automated metrics and human judgments on answerability, relevancy, and fluency to extensively evaluate our proposed MSQG model against the baselines. 2 Related Work The use of neural networks to generate natural language questions has mostly focused on question answering (Labutov et al., 2015; Serban et al., 2016; Rothe et al., 2016; Song et al., 2017; Duan et al., 2017; Du et al., 2017; Buck et al., 2017; Song et al., 2018; Harrison and Walker, 2018; Sun et al., 2018). A number of works process multiple passages by concatenating, adding, or attentionweight-summing among passage features into a single feature, and use it for downstream tasks (Zoph and Knight, 2016; Garmash and Monz, 2016; Libovick´y and Helcl, 2017; Wang et al., 2018; Yan et al., 2018; Lebanoff et al., 2018; Celikyilmaz et al., 2018; Nishimura et al., 2018; Libovick´y et al., 2018; Li et al., 2018b; Nishida et al., 2019). Our processing mechanisms are similar to Garmash and Monz (2016), Fira"
D19-5604,P19-1220,0,0.0179477,"v et al., 2015; Serban et al., 2016; Rothe et al., 2016; Song et al., 2017; Duan et al., 2017; Du et al., 2017; Buck et al., 2017; Song et al., 2018; Harrison and Walker, 2018; Sun et al., 2018). A number of works process multiple passages by concatenating, adding, or attentionweight-summing among passage features into a single feature, and use it for downstream tasks (Zoph and Knight, 2016; Garmash and Monz, 2016; Libovick´y and Helcl, 2017; Wang et al., 2018; Yan et al., 2018; Lebanoff et al., 2018; Celikyilmaz et al., 2018; Nishimura et al., 2018; Libovick´y et al., 2018; Li et al., 2018b; Nishida et al., 2019). Our processing mechanisms are similar to Garmash and Monz (2016), Firat et al. (2016), and Dong and Smith (2018). The information retrieval literature is primarily concerned with reformulating queries, by either selecting expansion terms from candidates as in pseudo-relevance feedback (Salton, 1971; Zhai and Lafferty, 2001; Xu and Croft, 1996; Metzler and Croft, 2007; Cao et al., 2008; Bernhard, 2010; Nogueira and Cho, 2017; Li et al., 2018a). Our task differs because there is no supervision unlike multi-lingual translation tasks where a single target translation is available given sources f"
D19-5604,W18-2711,0,0.0130504,"language questions has mostly focused on question answering (Labutov et al., 2015; Serban et al., 2016; Rothe et al., 2016; Song et al., 2017; Duan et al., 2017; Du et al., 2017; Buck et al., 2017; Song et al., 2018; Harrison and Walker, 2018; Sun et al., 2018). A number of works process multiple passages by concatenating, adding, or attentionweight-summing among passage features into a single feature, and use it for downstream tasks (Zoph and Knight, 2016; Garmash and Monz, 2016; Libovick´y and Helcl, 2017; Wang et al., 2018; Yan et al., 2018; Lebanoff et al., 2018; Celikyilmaz et al., 2018; Nishimura et al., 2018; Libovick´y et al., 2018; Li et al., 2018b; Nishida et al., 2019). Our processing mechanisms are similar to Garmash and Monz (2016), Firat et al. (2016), and Dong and Smith (2018). The information retrieval literature is primarily concerned with reformulating queries, by either selecting expansion terms from candidates as in pseudo-relevance feedback (Salton, 1971; Zhai and Lafferty, 2001; Xu and Croft, 1996; Metzler and Croft, 2007; Cao et al., 2008; Bernhard, 2010; Nogueira and Cho, 2017; Li et al., 2018a). Our task differs because there is no supervision unlike multi-lingual translation ta"
D19-5604,P17-2031,0,0.0346284,"Missing"
D19-5604,D17-1061,0,0.0302653,"y and Helcl, 2017; Wang et al., 2018; Yan et al., 2018; Lebanoff et al., 2018; Celikyilmaz et al., 2018; Nishimura et al., 2018; Libovick´y et al., 2018; Li et al., 2018b; Nishida et al., 2019). Our processing mechanisms are similar to Garmash and Monz (2016), Firat et al. (2016), and Dong and Smith (2018). The information retrieval literature is primarily concerned with reformulating queries, by either selecting expansion terms from candidates as in pseudo-relevance feedback (Salton, 1971; Zhai and Lafferty, 2001; Xu and Croft, 1996; Metzler and Croft, 2007; Cao et al., 2008; Bernhard, 2010; Nogueira and Cho, 2017; Li et al., 2018a). Our task differs because there is no supervision unlike multi-lingual translation tasks where a single target translation is available given sources from multiple languages. 3 3.1 Aggregate Step During the Aggregate step, we aggregate the N different target distributions into one distribution by averaging them as below: 1 dec dec dec P˜tdec = (β1 P1,t + β2 P2,t + · · · + βN PN,t ) N where P˜tdec is the final decoding distribution at time t, and ΣN i βi = N . In our experiments, we weight all the decoding distributions equally (βi = 1) to smooth out features that are distin"
D19-5604,W18-6326,0,0.0412659,"Missing"
D19-5604,P16-2020,0,0.0605606,"Missing"
D19-5604,D14-1162,0,0.0855914,"MS-MARCO passages using the generated question. These 90 passages are retrieved via a different criterion: BM25 (Robertson and Zaragoza, 2009) using Lucene2 . Note that there are multiple 10-passage sets that generate the same question q˜. For each of these 10-passage sets, we construct a 100-passage evaluation set using the same 90 passages retrieved via the BM25 criterion. Results Experimental setup Our training method uses the standard LSTMbased (Hochreiter and Schmidhuber, 1997) S2S with bi-linear attention (Luong et al., 2015). An input to our encoder is a concatenation of 100-dim GloVe (Pennington et al., 2014) vector, 100-dim predicate location vector, and 1024-dim ELMo (Peters et al., 2018) vector. Targets are embedded into 100-dim vectors. The S2S is bi-directional with a 256-dim bi-linear attention in each direction with ReLU (Nair and Hinton, 2010). Our encoder has two layers and we use an Adam (Kingma and Ba, 2014) with a learning rate of 2 × 10−5 . 4.2 Dataset Baselines S2S We compare our model with a standard S2S baseline where we concatenate the N documents into a single document to generate a question. We provide detailed discussions about the effect of document order in supplementary mate"
D19-5604,D15-1166,0,0.00980868,"he question ranks higher than 90 other passages drawn from a pool of ∼8.8 million MS-MARCO passages using the generated question. These 90 passages are retrieved via a different criterion: BM25 (Robertson and Zaragoza, 2009) using Lucene2 . Note that there are multiple 10-passage sets that generate the same question q˜. For each of these 10-passage sets, we construct a 100-passage evaluation set using the same 90 passages retrieved via the BM25 criterion. Results Experimental setup Our training method uses the standard LSTMbased (Hochreiter and Schmidhuber, 1997) S2S with bi-linear attention (Luong et al., 2015). An input to our encoder is a concatenation of 100-dim GloVe (Pennington et al., 2014) vector, 100-dim predicate location vector, and 1024-dim ELMo (Peters et al., 2018) vector. Targets are embedded into 100-dim vectors. The S2S is bi-directional with a 256-dim bi-linear attention in each direction with ReLU (Nair and Hinton, 2010). Our encoder has two layers and we use an Adam (Kingma and Ba, 2014) with a learning rate of 2 × 10−5 . 4.2 Dataset Baselines S2S We compare our model with a standard S2S baseline where we concatenate the N documents into a single document to generate a question. W"
D19-5604,N18-1202,0,0.0225232,"ifferent criterion: BM25 (Robertson and Zaragoza, 2009) using Lucene2 . Note that there are multiple 10-passage sets that generate the same question q˜. For each of these 10-passage sets, we construct a 100-passage evaluation set using the same 90 passages retrieved via the BM25 criterion. Results Experimental setup Our training method uses the standard LSTMbased (Hochreiter and Schmidhuber, 1997) S2S with bi-linear attention (Luong et al., 2015). An input to our encoder is a concatenation of 100-dim GloVe (Pennington et al., 2014) vector, 100-dim predicate location vector, and 1024-dim ELMo (Peters et al., 2018) vector. Targets are embedded into 100-dim vectors. The S2S is bi-directional with a 256-dim bi-linear attention in each direction with ReLU (Nair and Hinton, 2010). Our encoder has two layers and we use an Adam (Kingma and Ba, 2014) with a learning rate of 2 × 10−5 . 4.2 Dataset Baselines S2S We compare our model with a standard S2S baseline where we concatenate the N documents into a single document to generate a question. We provide detailed discussions about the effect of document order in supplementary material (SM). Two variants are considered (S2S and S2Srmrep ). Beam size is set to 5."
D19-5604,radev-etal-2002-evaluating,0,0.0822478,"elines, based on the automated retrieval statistics. Discussion of the proportion of unique questions is dealt in supplementary material. 4.5 Evaluation Metrics MRR, MRR@10, nDCG An input to the reranker R is a concatenation of the generated question and one passage i.e. [˜ q , p]. For each pair, it returns a score ∈ (0, 1) where 1 denotes that the input passage is the most suitable for q˜. We score all 100 pairs in an evaluation set. For the source 10-passage set, we average the 10 scores into one score as one combined document and obtain the retrieval statistics MRR, MRR@10 (Voorhees, 2001; Radev et al., 2002), and nDCG (J¨arvelin and Kek¨al¨ainen, 2002) (see the SM for details). Human Judgments We also conduct human evaluation where we compare questions generated by MSQGsharedh,rmrep and the S2S baseline, and the reference question using three criteria: fluency, relevancy, and answerability to the original 10 passages. We randomly select 200 (10-passage, reference question) sets from which we generate questions, yielding 2,000 (passage, question) evaluation pairs for our model, baseline, and reference, respectively (see the SM for details). 4.6 5 Conclusion We present a new task of generating comm"
D19-5604,P17-1099,0,0.0413667,"m our pool of baselines. 3.2 Method Model Variants Avoiding repetitive generation We observed that naively averaging the target distributions at every decoding time continually emphasized the common topic, thereby decoding repetitive topic words. To increase the diversity of generated tokens, we mask those tokens that have already been decoded in subsequent decoding steps. This strategy is reasonable for our task since questions generally tend to be short and rarely have repeated words. This mechanism can be viewed as a hard counterpart of the coverage models developed in Tu et al. (2016) and See et al. (2017). We denote this feature by rmrep in subscript. Multi-Source Question Generator Our Multi-Source Question Generator (MSQG) model introduces a mechanism to generate a common question given multiple documents. At training time, it employs a standard sequence-tosequence (S2S) model using a large number of (single document, question) pairs. At test time, it generates a common question given multiple documents, similar to Garmash and Monz (2016) and Firat et al. (2016). Specifically, our MSQG model iterates over two interleaved steps, until an end-ofsentence (EOS) token is generated: 33 MESD We als"
D19-5604,P18-1178,0,0.0251825,"against the baselines. 2 Related Work The use of neural networks to generate natural language questions has mostly focused on question answering (Labutov et al., 2015; Serban et al., 2016; Rothe et al., 2016; Song et al., 2017; Duan et al., 2017; Du et al., 2017; Buck et al., 2017; Song et al., 2018; Harrison and Walker, 2018; Sun et al., 2018). A number of works process multiple passages by concatenating, adding, or attentionweight-summing among passage features into a single feature, and use it for downstream tasks (Zoph and Knight, 2016; Garmash and Monz, 2016; Libovick´y and Helcl, 2017; Wang et al., 2018; Yan et al., 2018; Lebanoff et al., 2018; Celikyilmaz et al., 2018; Nishimura et al., 2018; Libovick´y et al., 2018; Li et al., 2018b; Nishida et al., 2019). Our processing mechanisms are similar to Garmash and Monz (2016), Firat et al. (2016), and Dong and Smith (2018). The information retrieval literature is primarily concerned with reformulating queries, by either selecting expansion terms from candidates as in pseudo-relevance feedback (Salton, 1971; Zhai and Lafferty, 2001; Xu and Croft, 1996; Metzler and Croft, 2007; Cao et al., 2008; Bernhard, 2010; Nogueira and Cho, 2017; Li et al., 2"
D19-5604,C10-2007,0,\N,Missing
D19-5604,P15-1086,0,\N,Missing
D19-5604,P18-1220,0,\N,Missing
D19-5604,N18-2090,0,\N,Missing
D19-5604,D18-1427,0,\N,Missing
D19-5604,N19-1423,0,\N,Missing
D19-5604,C16-1133,0,\N,Missing
I05-5001,P00-1056,0,0.0824958,"Missing"
I05-5001,J03-1002,0,0.00151466,"uage Processing Group Microsoft Research One Microsoft Way, Redmond, WA 98502, U.S.A. {chrisbkt, billdol}@microsoft.com (2003), and Pang et al. (2003). One promising approach extends standard Statistical Machine Translation (SMT) techniques (e.g., Brown et al., 1993; Och & Ney, 2000, 2003) to the problems of monolingual paraphrase identification and generation. Finch et al. (2004) have described several MT based paraphrase systems within the context of improving machine translation output. Quirk et al. (2004) describe an end-to-end paraphrase identification and generation system using GIZA++ (Och & Ney, 2003) and a monotone decoder to generate informationpreserving paraphrases. As with conventional SMT systems, SMTbased paraphrase systems require extensive monolingual parallel training corpora. However, while translation is a common human activity, resulting in large corpora of human-translated bilingual sentence pairs being relatively easy to obtain across multiple domains and language pairs, this is not the case in monolingual paraphrase, where naturally-occurring parallel data are hard to come by. The paucity of readily available monolingual parallel training corpora poses a formidable obstacle"
I05-5001,N03-1024,0,0.25502,"2004. San Jose Medical Center has nounced that it will close doors by Dec. 1, 2004. First Two Sentences The genome of the fungal pathogen that causes Sudden Oak Death has been sequenced by US scientists Researchers announced Thursday they've completed the genetic blueprint of the blight-causing culprit responsible for Sudden Oak Death anits Table 1. Paraphrase Examples Identified by Two Heuristics approach utilizes multiple translations of a single source language text, where the source language text guarantees semantic equivalence in the target language texts (e.g., Barzilay & McKeown, 2001; Pang et al., 2003). Such corpora are of limited availability, however, since multiple translations of the same document are uncommon in non-literary domains. The second strain of corpora construction involves mining paraphrase strings or sentences from news articles, with document clustering typically providing the topical coherence necessary to boost the likelihood that any two arbitrary sentences in the cluster are paraphrases. In this vein, Shinyama et al. (2002) use named entity anchors to extract paraphrases within a narrow domain. Barzilay & Lee (2003) employ Multiple Sequence Alignment (MSA, e.g., Durbin"
I05-5001,W04-3219,1,0.631106,"es for Paraphrase Identification and Corpus Construction Chris Brockett and William B. Dolan Natural Language Processing Group Microsoft Research One Microsoft Way, Redmond, WA 98502, U.S.A. {chrisbkt, billdol}@microsoft.com (2003), and Pang et al. (2003). One promising approach extends standard Statistical Machine Translation (SMT) techniques (e.g., Brown et al., 1993; Och & Ney, 2000, 2003) to the problems of monolingual paraphrase identification and generation. Finch et al. (2004) have described several MT based paraphrase systems within the context of improving machine translation output. Quirk et al. (2004) describe an end-to-end paraphrase identification and generation system using GIZA++ (Och & Ney, 2003) and a monotone decoder to generate informationpreserving paraphrases. As with conventional SMT systems, SMTbased paraphrase systems require extensive monolingual parallel training corpora. However, while translation is a common human activity, resulting in large corpora of human-translated bilingual sentence pairs being relatively easy to obtain across multiple domains and language pairs, this is not the case in monolingual paraphrase, where naturally-occurring parallel data are hard to come"
I05-5001,P01-1008,0,0.185727,"uch as word-based edit distance. Use of this technique dramatically reduces the Alignment Error Rate of the extracted corpora over heuristic methods based on position of the sentences in the text. 1 Introduction Paraphrase detection—the ability to determine whether or not two formally distinct strings are similar in meaning—is increasingly recognized as crucial to future applications in multiple fields including Information Retrieval, Question Answering, and Summarization. A growing body of recent research has focused on the problems of identifying and generating paraphrases, e.g., Barzilay & McKeown (2001), Lin & Pantel (2002), Shinyama et al, (2002), Barzilay & Lee 2 Background Two broad approaches have dominated the literature on constructing paraphrase corpora. One 1 Edit Distance (e ≤ 12) San Jose Medical Center announced Wednesday that it would close its doors by Dec. 1, 2004. San Jose Medical Center has nounced that it will close doors by Dec. 1, 2004. First Two Sentences The genome of the fungal pathogen that causes Sudden Oak Death has been sequenced by US scientists Researchers announced Thursday they've completed the genetic blueprint of the blight-causing culprit responsible for Sudd"
I05-5001,N03-1003,0,0.250453,"e target language texts (e.g., Barzilay & McKeown, 2001; Pang et al., 2003). Such corpora are of limited availability, however, since multiple translations of the same document are uncommon in non-literary domains. The second strain of corpora construction involves mining paraphrase strings or sentences from news articles, with document clustering typically providing the topical coherence necessary to boost the likelihood that any two arbitrary sentences in the cluster are paraphrases. In this vein, Shinyama et al. (2002) use named entity anchors to extract paraphrases within a narrow domain. Barzilay & Lee (2003) employ Multiple Sequence Alignment (MSA, e.g., Durbin et al., 1998) to align strings extracted from closely related news articles. Although the MSA approach can produce dramatic results, it is chiefly effective in extracting highly templatic data, and appears to be of limited extensibility to broad domain application (Quirk et al. 2004). Recent work by Dolan, et al. (2004) describes the construction of broad-domain corpora of aligned paraphrase pairs extracted from newscluster data on the World Wide Web using two heuristic strategies: 1) pairing sentences based on a word-based edit distance h"
I05-5001,C04-1051,1,0.884328,"Missing"
I05-5001,I05-5002,1,0.350444,"Missing"
I05-5001,W01-1411,0,\N,Missing
I05-5001,J05-4004,0,\N,Missing
I05-5002,P01-1008,0,0.0648381,"Missing"
I05-5002,N03-1003,0,0.0559104,", or exhibit quality problems stemming from insufficient command of the target language by the translators of the documents in question, e.g. the Linguistic Data Consortium’s Multiple-Translation Chinese Corpus (Huang et al., 2002). Multiple translations of novels, such as those used in (Barzilay & McKeown, 2001) provide a relatively limited dataset to work with, and – since these usually involve works that are out of copyright – usually exhibit older styles of language that have little in common with modern language resources or application requirements. Likewise, the data made available by (Barzilay & Lee, 2003: http://www.cs.cornell.edu/ Info/Projects/NLP/statpar.html), while invaluable in understanding and evaluating their results, is too limited in size and domain coverage to serve as either training or test data. Attempting to evaluate models of paraphrase acquisition and generation under limitations can thus be an exercise in frustration. Accordingly, we have tried to create a reasonably large corpus of naturally-occurring, non-handcrafted sentence pairs, along with accompanying human judgments, that can be used as a resource for training or testing purposes. Since the search space for identify"
I05-5002,I05-5001,1,0.309503,"Missing"
I05-5002,W05-1209,0,0.0565233,"inherent in automated extraction, and allow better cross comparison across systems. 8 Future Directions For our part we plan to expand the MSRP, both by extending the number of sentence pairs, and also improving the balance of positive and negative examples. We anticipate using multiple classifiers to reduce inherent biases in candidate corpus selection, and with better author identification to ensure proper attribution, to be able to draw on a larger dataset for consideration by our judges. In future releases we expect to make available more information about individual evaluator judgments. Burger & Ferro (2005) have suggested that this data may allow researchers greater freedom to construct models based on the judgments of specific judges or combinations of judges, permitting more fine-grained use of the corpus. One further issue that we will also be attempting to address is the need to provide a better metric for corpus coverage and quality. Until reliable metrics can be established for end-toA Virtual Super Corpus? Although larger than any other non-translationbased labeled paraphrase corpus currently publicly available, MSRP is tiny compared with the huge bilingual parallel corpora publicly avail"
I05-5002,W05-1203,0,0.0170265,"Missing"
I05-5002,shirai-etal-2002-towards,0,0.0187865,"Missing"
I05-5002,W05-1202,0,0.0188447,"Missing"
I05-5002,W05-1205,0,0.0382597,"Missing"
I05-5002,C02-1056,0,0.0344878,"Missing"
I05-5002,C04-1051,1,0.695272,"r of insertions and deletions. Note that the number of sentence pairs collected in this first pass was relatively small compared with the overall size of the dataset; the requirement of author identification significantly circumscribed the available dataset. Source Data The Microsoft Research Paraphrase Corpus (MSRP) is distilled from a database of 13,127,938 sentence pairs, extracted from 9,516,684 sentences in 32,408 news clusters collected from the World Wide Web over a 2year period, The methods and assumptions used in building this initial data set are discussed in Quirk et al. (2004) and Dolan et al. (2004). Two heuristics based on shared lexical properties and sentence position in the document were employed to construct the initial database: 1 Author identification was performed on the basis of pattern matching datelines and other textual information. We made a strong effort to ensure correct attribution. 10 4 4.1 Encarta Thesaurus: 125,054 word synonym pairs were extracted from the Encarta Thesaurus (Rooney, 2001). Constructing a Classifier Sequential Minimal Optimization Composite Features: Additional, more abstract features summarized the frequency with which each feature or class of feature"
I05-5002,C04-1151,0,0.0138033,"Missing"
I05-5002,P00-1056,0,0.060349,"phrase corpus currently publicly available, MSRP is tiny compared with the huge bilingual parallel corpora publicly available within the Machine Translation community, for example, the Canadian Hansards, the Hong Kong Parliamentary corpus, or the United Nations documents. It is improbable that we will 14 paraphrase content quickly and in a cost effective manner. We hope that others will follow our example. end paraphrase tasks—these will probably need to be application specific—the Alignment Error Rate strategy that was successfully applied in early development of machine translation systems (Och & Ney, 2000, 2003) offers a useful intermediate representation of the coverage and precision of a corpus and extraction techniques. Though fullscale reliability studies have yet to be performed, the AER technique is already finding application in other fields such as summarization (Daumé & Marcu, forthcoming). We expect to be able to provide a reasonably large corpus of word-aligned paraphrase sentences in the near future that we hope will serve as some sort of standard by which corpus extraction techniques can be measured and compared in a uniform fashion. One other path that we are concurrently explori"
I05-5002,J03-1002,0,0.0525453,"Missing"
I05-5002,N03-1024,0,0.0638922,"Missing"
I05-5002,W04-3219,1,\N,Missing
I05-5002,J05-4004,0,\N,Missing
I08-1059,P01-1017,0,0.0447053,"Missing"
I08-1059,W07-1604,0,0.537721,"Felice and Pulman (2007) utilize a set of sophisticated syntactic and semantic analysis features to predict 5 common English prepositions. Obviously, this is impractical in a setting where noisy non-native text is subjected to proofing. Meanwhile, work on automated error detection on non-native text focuses primarily on detection of errors, rather than on the more difficult task of supplying viable corrections (e.g., Chodorow and Leacock, 2000). More recently, Han et al. (2004, 2006) use a maximum entropy classifier to propose article corrections in TESOL essays, while Izumi et al. (2003) and Chodorow et al. (2007) present techniques of automatic preposition choice modeling. These more recent efforts, nevertheless, do not attempt to integrate their methods into a more general proofing application designed to assist non-native speakers when writing English. Finally, Yi et al. (2008) designed a system that uses web counts to determine correct article usage for a given sentence, targeting ESL users. 4 System Description Our system consists of three major components: 1. Suggestion Provider (SP) 2. Language Model (LM) 3. Example Provider (EP) The Suggestion Provider contains modules for each error type discu"
I08-1059,W07-1607,0,0.562737,"Missing"
I08-1059,J93-1003,0,0.101425,"on sites are determined heuristically from the sequence of POS tags. Based on these features, we train a classifier for preposition choice and determiner choice. Currently we train decision tree classifiers with the WinMine toolkit (Chickering 2002). We also experimented with linear SVMs, but decision trees performed better overall and training and parameter optimization were considerably more efficient. Before training the classifiers, we perform feature ablation by imposing a count cutoff of 10, and by limiting the number of features to the top 75K features in terms of log likelihood ratio (Dunning 1993). We train two separate classifiers for both determiners and preposition:  decision whether or not a determiner/preposition should be present (presence/absence or pa classifier)  decision which determiner/preposition is the most likely choice, given that a determiner/preposition is present (choice or ch classifier) In the case of determiners, class values for the ch classifier are a/an and the. Preposition choice (equivalent to the ―confusion set‖ of a contextual speller) is limited to a set of 13 prepositions that figure prominently in the errors observed in the JLE corpus: about, as, at, b"
I08-1059,W02-2105,1,0.815053,"Missing"
I08-1059,han-etal-2004-detecting,0,0.212248,"Turner and Charniak (2007), for example, utilize a language model based on a statistical parser for Penn Tree Bank data. Similarly, De Felice and Pulman (2007) utilize a set of sophisticated syntactic and semantic analysis features to predict 5 common English prepositions. Obviously, this is impractical in a setting where noisy non-native text is subjected to proofing. Meanwhile, work on automated error detection on non-native text focuses primarily on detection of errors, rather than on the more difficult task of supplying viable corrections (e.g., Chodorow and Leacock, 2000). More recently, Han et al. (2004, 2006) use a maximum entropy classifier to propose article corrections in TESOL essays, while Izumi et al. (2003) and Chodorow et al. (2007) present techniques of automatic preposition choice modeling. These more recent efforts, nevertheless, do not attempt to integrate their methods into a more general proofing application designed to assist non-native speakers when writing English. Finally, Yi et al. (2008) designed a system that uses web counts to determine correct article usage for a given sentence, targeting ESL users. 4 System Description Our system consists of three major components: 1"
I08-1059,I08-2082,1,0.357362,"Missing"
I08-1059,N04-2006,0,0.131483,"Missing"
I08-1059,P00-1067,1,0.853874,"ot a significant problem for native speakers and hence remains unaddressed in proofing tools such as the grammar checker in Microsoft Word (Heidorn 2000). Plainly there is an Targeted Error Types Our system currently targets eight different error types: 1. Preposition presence and choice: In the other hand, ... (On the other hand ...) 2. Definite and indefinite determiner presence and choice: I am teacher... (am a teacher) 3. Gerund/infinitive confusion: I am interesting in this book. (interested in) 4. Auxiliary verb presence and choice: My teacher does is a good teacher (my teacher is...) 1 Liu et al. 2000 take a similar approach, retrieving example sentences from a large corpus. 449 5. Over-regularized verb inflection: I writed a letter (wrote) 6. Adjective/noun confusion: This is a China book (Chinese book) 7. Word order (adjective sequences and nominal compounds): I am a student of university (university student) 8. Noun pluralization: They have many knowledges (much knowledge) In this paper we will focus on the two most prominent and difficult errors: choice of determiner and prepositions. Empirical justification for targeting these errors comes from inspection of several corpora of non-nat"
I08-1059,W00-0708,0,0.590938,"Missing"
I08-1059,P06-1132,0,0.116761,"Missing"
I08-1059,N07-2045,0,0.363375,"Missing"
I08-1059,N07-1007,0,\N,Missing
I17-1047,N15-1053,0,0.036937,"Missing"
I17-1047,P04-1077,0,0.0230767,"ext C. The function V counts the number of verbs in the hypothesis and |h |denotes the number of tokens in the hypothesis. The function idf is the inverse document frequency, computing how common a hypothesis is across all the generated N-best lists. Here D is the set of all N-best lists and d is a specific N|D| best list. We define idf(h, D) = log |{d∈D:h∈d}| , where we set N =10 to cut short each N-best list. These parameters were selected following reranking experiments on the validation set. We optimize all the parameters of the scoring function towards maximizing the smoothed-BLEU score (Lin and Och, 2004) using the Pairwise Ranking Optimization algorithm (Hopkins and May, 2011). 5.2 Retrieval Models In addition to generation, we implemented two retrieval models customized for the tasks of question and response generation. Work in vision and language has demonstrated the effectiveness of retrieval models, where one uses the annotation (e.g., caption) of a nearest neighbor in the training image set to annotate a given test image (Mostafazadeh et al., 2016b; Devlin et al., 2015; 3 An example generic question is where is this? and a generic response is I don’t know. 467 Do you think this happened"
I17-1047,P15-2017,0,0.200084,"des answers. Figure 2 contrasts an example ICG conversation with the VisDial dataset. As this example shows, IGC involves natural conversations with the image as the grounding, where the literal objects (e.g., the pumpkins) may not even be mentioned in the conversation at all, whereas VisDial targets explicit image understanding. More recently, Das et al. (2017b) have explored the VisDial dataset with richer models that incorporate deep-reinforcement learning. Related Work Vision and Language Visual features combined with language modeling have shown good performance both in image captioning (Devlin et al., 2015; Xu et al., 2015; Fang et al., 2014; Donahue et al., 2015) and in question answering on images (Antol et al., 2015; Ray et al., 2016; Malinowski and Fritz, 2014), when trained on large datasets, such as the COCO dataset (Lin et al., 2014). In Visual Question Answering (VQA) (Antol et al., 2015), a system is tasked with answering a question about a given image, where the questions are constrained to be answerable directly from the image. In other words, the VQA task primarily serves to evaluate the extent to which the system has recognized the explicit content of the image. Mostafazadeh et al."
I17-1047,D16-1230,0,0.0151424,"Example question and response generations on IGCCrowd test set. All the generation models use beam search with reranking. In the textual context, <UTT> separates different utterances. The generations in bold are acceptable utterances given the underlying context. 468 evaluation. For ease of replicability, we use the standard Machine Translation metric, BLEU (Papineni et al., 2002), which captures n-gram overlap between hypotheses and multiple references. Results reported in Table 6 employ BLEU with equal weights up to 4-grams at corpus-level on the multi-reference IGCCrowd test set. Although Liu et al. (2016) suggest that BLEU fails to correlate with human judgment at the sentence level, correlation increases when BLEU is applied at the document or corpus level (Galley et al., 2015; Przybocki et al., 2008). Figure 7: The visual & textual context sensitive model with RNN encoding (V&T.RNN-Gen). Hodosh et al., 2013; Ordonez et al., 2011; Farhadi et al., 2010). 7 Visual Context Sensitive Model (V-Ret). This model uses only the provided image for retrieval. First, we find a set of K nearest training images for the given test image based on cosine similarity of the f c7 vision feature vectors. Then we"
I17-1047,W15-4640,0,0.0312834,"ce the task of visual question generation (VQG), in which the system itself outputs questions about a given image. Questions are required to be ‘natural and engaging’, i.e. a person would find them interesting to answer, but need not be answerable from the image alone. In this work, we introduce multimodal context, recognizing that images commonly come associated with a verbal commentary that can affect the interpretation. This is thus a broader, more complex task that involves implicit commonsense reasoning around both image and text. 463 2.2 Data-Driven Conversational Modeling logue corpus (Lowe et al., 2015) is the largest corpus of dialogues (almost 1 million mainly 3-turn dialogues) for the specific topic of troubleshooting Ubuntu problems. On the other hand, for openended conversation modeling (chitchat), now a high demand application in AI, shared datasets with which to track progress are severely lacking. The ICG task presented here lies nicely in the continuum between the two, where the visual grounding of event-centric images constrains the topic of conversation to contentful utterances. To enable benchmarking of progress in the IGC task, we constructed the IGCCrowd dataset for validation"
I17-1047,P15-2073,1,0.665818,"tterances. The generations in bold are acceptable utterances given the underlying context. 468 evaluation. For ease of replicability, we use the standard Machine Translation metric, BLEU (Papineni et al., 2002), which captures n-gram overlap between hypotheses and multiple references. Results reported in Table 6 employ BLEU with equal weights up to 4-grams at corpus-level on the multi-reference IGCCrowd test set. Although Liu et al. (2016) suggest that BLEU fails to correlate with human judgment at the sentence level, correlation increases when BLEU is applied at the document or corpus level (Galley et al., 2015; Przybocki et al., 2008). Figure 7: The visual & textual context sensitive model with RNN encoding (V&T.RNN-Gen). Hodosh et al., 2013; Ordonez et al., 2011; Farhadi et al., 2010). 7 Visual Context Sensitive Model (V-Ret). This model uses only the provided image for retrieval. First, we find a set of K nearest training images for the given test image based on cosine similarity of the f c7 vision feature vectors. Then we retrieve those K annotations as our pool of K candidates. Finally, we compute the textual similarity among the questions in the pool according to a Smoothed-BLEU (Lin and Och,"
I17-1047,W16-1007,1,0.460669,"Devlin et al., 2015; Xu et al., 2015; Fang et al., 2014; Donahue et al., 2015) and in question answering on images (Antol et al., 2015; Ray et al., 2016; Malinowski and Fritz, 2014), when trained on large datasets, such as the COCO dataset (Lin et al., 2014). In Visual Question Answering (VQA) (Antol et al., 2015), a system is tasked with answering a question about a given image, where the questions are constrained to be answerable directly from the image. In other words, the VQA task primarily serves to evaluate the extent to which the system has recognized the explicit content of the image. Mostafazadeh et al. (2016b) introduce the task of visual question generation (VQG), in which the system itself outputs questions about a given image. Questions are required to be ‘natural and engaging’, i.e. a person would find them interesting to answer, but need not be answerable from the image alone. In this work, we introduce multimodal context, recognizing that images commonly come associated with a verbal commentary that can affect the interpretation. This is thus a broader, more complex task that involves implicit commonsense reasoning around both image and text. 463 2.2 Data-Driven Conversational Modeling logu"
I17-1047,D11-1125,0,0.00844008,"|h |denotes the number of tokens in the hypothesis. The function idf is the inverse document frequency, computing how common a hypothesis is across all the generated N-best lists. Here D is the set of all N-best lists and d is a specific N|D| best list. We define idf(h, D) = log |{d∈D:h∈d}| , where we set N =10 to cut short each N-best list. These parameters were selected following reranking experiments on the validation set. We optimize all the parameters of the scoring function towards maximizing the smoothed-BLEU score (Lin and Och, 2004) using the Pairwise Ranking Optimization algorithm (Hopkins and May, 2011). 5.2 Retrieval Models In addition to generation, we implemented two retrieval models customized for the tasks of question and response generation. Work in vision and language has demonstrated the effectiveness of retrieval models, where one uses the annotation (e.g., caption) of a nearest neighbor in the training image set to annotate a given test image (Mostafazadeh et al., 2016b; Devlin et al., 2015; 3 An example generic question is where is this? and a generic response is I don’t know. 467 Do you think this happened on highway <EOS> <GO> day this not . . decoder This was not the way I imag"
I17-1047,P16-1170,1,0.732217,"Devlin et al., 2015; Xu et al., 2015; Fang et al., 2014; Donahue et al., 2015) and in question answering on images (Antol et al., 2015; Ray et al., 2016; Malinowski and Fritz, 2014), when trained on large datasets, such as the COCO dataset (Lin et al., 2014). In Visual Question Answering (VQA) (Antol et al., 2015), a system is tasked with answering a question about a given image, where the questions are constrained to be answerable directly from the image. In other words, the VQA task primarily serves to evaluate the extent to which the system has recognized the explicit content of the image. Mostafazadeh et al. (2016b) introduce the task of visual question generation (VQG), in which the system itself outputs questions about a given image. Questions are required to be ‘natural and engaging’, i.e. a person would find them interesting to answer, but need not be answerable from the image alone. In this work, we introduce multimodal context, recognizing that images commonly come associated with a verbal commentary that can affect the interpretation. This is thus a broader, more complex task that involves implicit commonsense reasoning around both image and text. 463 2.2 Data-Driven Conversational Modeling logu"
I17-1047,N16-1147,1,0.891951,"Missing"
I17-1047,P02-1040,0,0.122669,"at this baseball game. <UTT> Nice, which team won? My team won this game. 10 for me and 28 for my dad. ding ding ding! No it wasn’t too bad of a bang up. Yes. Nah, I’m at home now. lords cricket ground . beautiful. He’s not mine! Table 4: Example question and response generations on IGCCrowd test set. All the generation models use beam search with reranking. In the textual context, <UTT> separates different utterances. The generations in bold are acceptable utterances given the underlying context. 468 evaluation. For ease of replicability, we use the standard Machine Translation metric, BLEU (Papineni et al., 2002), which captures n-gram overlap between hypotheses and multiple references. Results reported in Table 6 employ BLEU with equal weights up to 4-grams at corpus-level on the multi-reference IGCCrowd test set. Although Liu et al. (2016) suggest that BLEU fails to correlate with human judgment at the sentence level, correlation increases when BLEU is applied at the document or corpus level (Galley et al., 2015; Przybocki et al., 2008). Figure 7: The visual & textual context sensitive model with RNN encoding (V&T.RNN-Gen). Hodosh et al., 2013; Ordonez et al., 2011; Farhadi et al., 2010). 7 Visual C"
I17-1047,N16-1014,1,0.87514,"tistics. The IGCCrowd dataset has been released as the Microsoft Research Image-Grounded Conversation dataset (https://www.microsoft.com/en-us/ download/details.aspx?id=55324& 751be11f-ede8). This work is also closely linked to research on data-driven conversation modeling. Ritter et al. (2011) posed response generation as a machine translation task, learning conversations from parallel message-response pairs found on social media. Their work has been successfully extended with the use of deep neural models (Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2015a; Vinyals and Le, 2015; Li et al., 2016a,b). Sordoni et al. (2015) introduce a context-sensitive neural language model that selects the most probable response conditioned on the conversation history (i.e., a text-only context). In this paper, we extend the contextual approach with multimodal features to build models that are capable of asking questions on topics of interest to a human that might allow a conversational agent to proactively drive a conversation forward. 3 3.1 Image-Grounded Conversations Task Definition We define the current scope of IGC as the following two consecutive conversational steps: • Question Generation: Gi"
I17-1047,P16-1094,1,0.153491,"tistics. The IGCCrowd dataset has been released as the Microsoft Research Image-Grounded Conversation dataset (https://www.microsoft.com/en-us/ download/details.aspx?id=55324& 751be11f-ede8). This work is also closely linked to research on data-driven conversation modeling. Ritter et al. (2011) posed response generation as a machine translation task, learning conversations from parallel message-response pairs found on social media. Their work has been successfully extended with the use of deep neural models (Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2015a; Vinyals and Le, 2015; Li et al., 2016a,b). Sordoni et al. (2015) introduce a context-sensitive neural language model that selects the most probable response conditioned on the conversation history (i.e., a text-only context). In this paper, we extend the contextual approach with multimodal features to build models that are capable of asking questions on topics of interest to a human that might allow a conversational agent to proactively drive a conversation forward. 3 3.1 Image-Grounded Conversations Task Definition We define the current scope of IGC as the following two consecutive conversational steps: • Question Generation: Gi"
I17-1047,D16-1090,0,0.027098,"sations with the image as the grounding, where the literal objects (e.g., the pumpkins) may not even be mentioned in the conversation at all, whereas VisDial targets explicit image understanding. More recently, Das et al. (2017b) have explored the VisDial dataset with richer models that incorporate deep-reinforcement learning. Related Work Vision and Language Visual features combined with language modeling have shown good performance both in image captioning (Devlin et al., 2015; Xu et al., 2015; Fang et al., 2014; Donahue et al., 2015) and in question answering on images (Antol et al., 2015; Ray et al., 2016; Malinowski and Fritz, 2014), when trained on large datasets, such as the COCO dataset (Lin et al., 2014). In Visual Question Answering (VQA) (Antol et al., 2015), a system is tasked with answering a question about a given image, where the questions are constrained to be answerable directly from the image. In other words, the VQA task primarily serves to evaluate the extent to which the system has recognized the explicit content of the image. Mostafazadeh et al. (2016b) introduce the task of visual question generation (VQG), in which the system itself outputs questions about a given image. Qu"
I17-1047,D11-1054,0,0.0139427,"ional questions and responses for the IGCCrowd contexts and initial questions. Table 1 shows three full conversations found in the IGCCrowd dataset. These examples show show that eventful images lead to conversations that are semantically rich and appear to involve commonsense reasoning. Table 2 summarizes basic dataset statistics. The IGCCrowd dataset has been released as the Microsoft Research Image-Grounded Conversation dataset (https://www.microsoft.com/en-us/ download/details.aspx?id=55324& 751be11f-ede8). This work is also closely linked to research on data-driven conversation modeling. Ritter et al. (2011) posed response generation as a machine translation task, learning conversations from parallel message-response pairs found on social media. Their work has been successfully extended with the use of deep neural models (Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2015a; Vinyals and Le, 2015; Li et al., 2016a,b). Sordoni et al. (2015) introduce a context-sensitive neural language model that selects the most probable response conditioned on the conversation history (i.e., a text-only context). In this paper, we extend the contextual approach with multimodal features to build models t"
I17-1047,P15-1152,0,0.0151711,"olve commonsense reasoning. Table 2 summarizes basic dataset statistics. The IGCCrowd dataset has been released as the Microsoft Research Image-Grounded Conversation dataset (https://www.microsoft.com/en-us/ download/details.aspx?id=55324& 751be11f-ede8). This work is also closely linked to research on data-driven conversation modeling. Ritter et al. (2011) posed response generation as a machine translation task, learning conversations from parallel message-response pairs found on social media. Their work has been successfully extended with the use of deep neural models (Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2015a; Vinyals and Le, 2015; Li et al., 2016a,b). Sordoni et al. (2015) introduce a context-sensitive neural language model that selects the most probable response conditioned on the conversation history (i.e., a text-only context). In this paper, we extend the contextual approach with multimodal features to build models that are capable of asking questions on topics of interest to a human that might allow a conversational agent to proactively drive a conversation forward. 3 3.1 Image-Grounded Conversations Task Definition We define the current scope of IGC as the following tw"
I17-1047,N15-1020,1,0.516832,"rich and appear to involve commonsense reasoning. Table 2 summarizes basic dataset statistics. The IGCCrowd dataset has been released as the Microsoft Research Image-Grounded Conversation dataset (https://www.microsoft.com/en-us/ download/details.aspx?id=55324& 751be11f-ede8). This work is also closely linked to research on data-driven conversation modeling. Ritter et al. (2011) posed response generation as a machine translation task, learning conversations from parallel message-response pairs found on social media. Their work has been successfully extended with the use of deep neural models (Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2015a; Vinyals and Le, 2015; Li et al., 2016a,b). Sordoni et al. (2015) introduce a context-sensitive neural language model that selects the most probable response conditioned on the conversation history (i.e., a text-only context). In this paper, we extend the contextual approach with multimodal features to build models that are capable of asking questions on topics of interest to a human that might allow a conversational agent to proactively drive a conversation forward. 3 3.1 Image-Grounded Conversations Task Definition We define the current scope of IGC"
I17-1047,N15-1173,0,0.0116461,"Missing"
I17-1047,H89-1033,0,0.533497,"Missing"
I17-1047,N10-1020,1,\N,Missing
I17-1047,P98-1013,0,\N,Missing
I17-1047,C98-1013,0,\N,Missing
I17-1061,P16-2020,1,0.793373,"Missing"
I17-1061,D17-1279,1,0.830225,"Missing"
I17-1061,P15-2073,1,0.174602,"Missing"
I17-1061,P03-1021,0,0.0260752,"maximum length of the generated candidates was set at 20 tokens. At each time step, we first examine all B × B possible next-word candidates, and add all hypotheses ending with an EOS token to the N-best list. We then preserve the top-B unfinished hypotheses and move to the next word position. We then use LSTM-MMI to rerank the N-best list and use the 1-best result of the re-ranked list in all evaluation. where p(R|M, v) is the probability of the generated response given message M and the respondents user ID. |R |is the length of the target and γ is the associated penalty weight. We use MERT (Och, 2003) to optimize γ and λ on BLEU using N-best lists of response candidates generated from the development set. To compute p(M |R), we train an inverse S EQ 2S EQ model by swapping messages and responses. The reverse S EQ 2S EQ models p(M |R) is trained with no user information considered. 6.4 MTASK -M Table 2: Performance on the Twitter dataset of 2-layer S EQ 2S EQ models and MMI models. Distinct-1 and distinct-2 are respectively the number of distinct unigrams and bigrams divided by total number of generated words. Baseline log p(R|M, v) + λ log p(M |R) + γ|R| MTASK -S Table 1: Perplexity for st"
I17-1061,N15-1124,0,0.0209474,"Missing"
I17-1061,P02-1040,0,0.120451,"wapping messages and responses. The reverse S EQ 2S EQ models p(M |R) is trained with no user information considered. 6.4 MTASK -M Table 2: Performance on the Twitter dataset of 2-layer S EQ 2S EQ models and MMI models. Distinct-1 and distinct-2 are respectively the number of distinct unigrams and bigrams divided by total number of generated words. Baseline log p(R|M, v) + λ log p(M |R) + γ|R| MTASK -S Table 1: Perplexity for standard S EQ 2S EQ and the user model on the Twitter Persona dev set. As in previous work (Sordoni et al., 2015), we use BLEU and human evaluation for evaluation. BLEU (Papineni et al., 2002) has been shown to correlate fairly well with human judgment at a document- and corpus-level, including on the response generation task.2 We also report perplexity as an indicator of model capability. We additionally report degree of diversity by calculating the number of distinct unigrams and bigrams in generated responses. The value is scaled by total number of generated tokens to avoid favoring long sentences (shown as distinct-1 and distinct-2). Finally, we present a human evaluation that validates our main findings. 6.3 Baseline 7 Training and Decoding Experimental Results The perplexity"
I17-1061,N16-1014,1,0.777511,"ovements over baseline model quality, generating responses that capture more precisely speakers’ traits and speaking styles. The model offers the benefits of being algorithmically simple and easy to implement, and not relying on large quantities of data representing specific individual speakers. 1 Figure 1: Existing neural conversational models (baseline) tend to produce generic responses. The system presented in this paper better represents the speaker role (support person), domain of expertise (technical), and speaking style (courteous). sponses that are often commonplace and uninteresting (Li et al., 2016a; Shao et al., 2017). This is illustrated in Fig. 1, where the output of a standard Sequence-to-Sequence conversation model is contrasted with that of the best system presented in this work. The baseline system generates a desultory answer that offers no useful information and is unlikely to inspire user confidence. The output of the second system, however, strongly reflects the agent’s role in providing technical support. It not only evidences domain knowledge, but also manifests the professional politeness associated with a speaker in that role. The challenge for neural conversation systems"
I17-1061,D11-1054,0,0.0941847,"to exploit directly, since, not being in conversational format, it does not mesh easily with existing source-target conversational models. Introduction Conversational engines are key components of intelligent “personal assistants” such as Apple’s Siri and Amazon’s Alexa. These assistants can perform simple tasks, answer questions, provide recommendations, and even engage in chitchats (De Mori et al., 2008; Chen et al., 2015, 2016). The emergence of these agents has been paralleled by burgeoning interest in training natural-sounding dialog systems from conversational exchanges between humans (Ritter et al., 2011; Sordoni et al., 2015; Luan et al., 2014, 2015; Vinyals and Le, 2015). A major challenge for data-driven systems is how to generate output that corresponds to specific traits that the agent needs to adopt, as they tend to generate “consensus” re* This work was performed at Microsoft. 605 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 605–614, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP In this paper we address the joint problems of blandness and data scarcity with multi-task learning (Caruana, 1998; Liu et al., 2015; Luan et al."
I17-1061,P16-1094,1,0.933706,"ovements over baseline model quality, generating responses that capture more precisely speakers’ traits and speaking styles. The model offers the benefits of being algorithmically simple and easy to implement, and not relying on large quantities of data representing specific individual speakers. 1 Figure 1: Existing neural conversational models (baseline) tend to produce generic responses. The system presented in this paper better represents the speaker role (support person), domain of expertise (technical), and speaking style (courteous). sponses that are often commonplace and uninteresting (Li et al., 2016a; Shao et al., 2017). This is illustrated in Fig. 1, where the output of a standard Sequence-to-Sequence conversation model is contrasted with that of the best system presented in this work. The baseline system generates a desultory answer that offers no useful information and is unlikely to inspire user confidence. The output of the second system, however, strongly reflects the agent’s role in providing technical support. It not only evidences domain knowledge, but also manifests the professional politeness associated with a speaker in that role. The challenge for neural conversation systems"
I17-1061,P16-1009,0,0.0899948,"to specific traits that the agent needs to adopt, as they tend to generate “consensus” re* This work was performed at Microsoft. 605 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 605–614, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP In this paper we address the joint problems of blandness and data scarcity with multi-task learning (Caruana, 1998; Liu et al., 2015; Luan et al., 2016a). This is a technique that has seen success in machine translation, where large monolingual data sets have been used to improve translation models (Sennrich et al., 2016). The intuition is that if two tasks are related, then joint training and parameter sharing can enable one task to benefit the other. In our case, this sharing is between two models: On one hand, a standard Sequence-to-Sequence conversational models is trained to predict the current response given the previous context. On the other hand, using the non-conversational data, we introduce an autoencoder multi-task learning strategy that predicts the response given the same sequence, but with the target parameters tied with the general conversational model. Our experiments with 4M conversation trip"
I17-1061,D17-1235,0,0.028605,"line model quality, generating responses that capture more precisely speakers’ traits and speaking styles. The model offers the benefits of being algorithmically simple and easy to implement, and not relying on large quantities of data representing specific individual speakers. 1 Figure 1: Existing neural conversational models (baseline) tend to produce generic responses. The system presented in this paper better represents the speaker role (support person), domain of expertise (technical), and speaking style (courteous). sponses that are often commonplace and uninteresting (Li et al., 2016a; Shao et al., 2017). This is illustrated in Fig. 1, where the output of a standard Sequence-to-Sequence conversation model is contrasted with that of the best system presented in this work. The baseline system generates a desultory answer that offers no useful information and is unlikely to inspire user confidence. The output of the second system, however, strongly reflects the agent’s role in providing technical support. It not only evidences domain knowledge, but also manifests the professional politeness associated with a speaker in that role. The challenge for neural conversation systems, then, is that an ag"
I17-1061,D16-1230,0,0.0284166,"Missing"
I17-1061,N15-1020,1,0.282437,"since, not being in conversational format, it does not mesh easily with existing source-target conversational models. Introduction Conversational engines are key components of intelligent “personal assistants” such as Apple’s Siri and Amazon’s Alexa. These assistants can perform simple tasks, answer questions, provide recommendations, and even engage in chitchats (De Mori et al., 2008; Chen et al., 2015, 2016). The emergence of these agents has been paralleled by burgeoning interest in training natural-sounding dialog systems from conversational exchanges between humans (Ritter et al., 2011; Sordoni et al., 2015; Luan et al., 2014, 2015; Vinyals and Le, 2015). A major challenge for data-driven systems is how to generate output that corresponds to specific traits that the agent needs to adopt, as they tend to generate “consensus” re* This work was performed at Microsoft. 605 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 605–614, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP In this paper we address the joint problems of blandness and data scarcity with multi-task learning (Caruana, 1998; Liu et al., 2015; Luan et al., 2016a). This is a te"
I17-1061,N15-1092,1,0.125072,"Missing"
I17-1061,D15-1199,0,0.0128467,"Missing"
N10-1017,P05-1074,0,0.890362,"Missing"
N10-1017,N03-1003,0,0.415595,"to be built into the graph, and incorporates truncation techniques to prevent the graph from growing too large for efficiency. Current approaches, by contrast, implicitly presuppose the graph to be bipartite, are limited to finding paraphrases that are of length two away from a phrase, and do not generally permit easy incorporation of domain knowledge. Manual evaluation of generated output shows that our approach outperforms the state-of-the-art system of Callison-Burch (2008). Early work on paraphrase acquisition has focused on using monolingual parallel corpora (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004). While effective, such methods are hampered by the scarcity of monolingual parallel corpora, an obstacle that limits both the quantity and quality of the paraphrases learned. To address this limitation, Bannard and CallisonBurch (2005) focused their attention on the abundance of bilingual parallel corpora. The crux of this system (referred to below as ”BCB”) is to align phrases in a bilingual parallel corpus and hypothesize English phrases as potential paraphrases if they are aligned to the same phrase in another language (the “pivot”). Callison-Burch ("
N10-1017,P01-1008,0,0.307139,"t represent domain knowledge to be built into the graph, and incorporates truncation techniques to prevent the graph from growing too large for efficiency. Current approaches, by contrast, implicitly presuppose the graph to be bipartite, are limited to finding paraphrases that are of length two away from a phrase, and do not generally permit easy incorporation of domain knowledge. Manual evaluation of generated output shows that our approach outperforms the state-of-the-art system of Callison-Burch (2008). Early work on paraphrase acquisition has focused on using monolingual parallel corpora (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004). While effective, such methods are hampered by the scarcity of monolingual parallel corpora, an obstacle that limits both the quantity and quality of the paraphrases learned. To address this limitation, Bannard and CallisonBurch (2005) focused their attention on the abundance of bilingual parallel corpora. The crux of this system (referred to below as ”BCB”) is to align phrases in a bilingual parallel corpus and hypothesize English phrases as potential paraphrases if they are aligned to the same phrase in another language (the “p"
N10-1017,P99-1071,0,0.0299695,"hypothesize English phrases as potential paraphrases if they are aligned to the same phrase in another language (the “pivot”). Callison-Burch (2008) further refines BCB with a system that constrains paraphrases to have the same syntactic structure (Syntactic Bilingual Phrases: SBP). Introduction Automatically learning paraphrases, or alternative ways of expressing the same meaning, is an active area of NLP research because of its usefulness in a variety of applications, e.g., question answering (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Reizler et al., 2007), document summarization (Barzilay et al., 1999; McKeown et al., 2002), natural language generation (Iordanskaja et al., 1991; Lenke, 1994; Stede, 1999), machine transWe take a graphical view of the state-of-the-art BCB and SBP approaches by representing the bilingual parallel corpora as a graph. A node corresponds to a phrase, and an edge exists between two nodes if their corresponding phrases are aligned. This graphical form makes the limitations of the BCB/SBP approaches more evident. The BCB/SBP graph is limited to be bipartite with English nodes on one side and foreign language nodes on the other, and an edge can only exist between no"
N10-1017,N06-1003,0,0.227109,"Missing"
N10-1017,D08-1021,0,0.663409,"f paraphrases with better ones being “closer” to a phrase of interest. This approach allows “feature” nodes that represent domain knowledge to be built into the graph, and incorporates truncation techniques to prevent the graph from growing too large for efficiency. Current approaches, by contrast, implicitly presuppose the graph to be bipartite, are limited to finding paraphrases that are of length two away from a phrase, and do not generally permit easy incorporation of domain knowledge. Manual evaluation of generated output shows that our approach outperforms the state-of-the-art system of Callison-Burch (2008). Early work on paraphrase acquisition has focused on using monolingual parallel corpora (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004). While effective, such methods are hampered by the scarcity of monolingual parallel corpora, an obstacle that limits both the quantity and quality of the paraphrases learned. To address this limitation, Bannard and CallisonBurch (2005) focused their attention on the abundance of bilingual parallel corpora. The crux of this system (referred to below as ”BCB”) is to align phrases in a bilingual parallel corpus and hyp"
N10-1017,N09-1015,0,0.0163573,"nguages. Callison-Burch (2008) aligned English phrases with phrases in each of the other languages using Giza++ (Och and Ney, 2004). We used his English-foreign phrasal alignments which are publicly available on the web at http://ironman.jhu.edu/emnlp08.tar. In addition, we paired sentences of different non-English languages that correspond to the same English sentence, and aligned the phrases using 5 iterations of IBM model 1 in each direction, followed by 5 iterations of HMM alignment with paired training using the algorithm described in Liang et al. (2006). We further used the technique of Chen et al. (2009) to remove a phrase alignment F -G (where F and G are phrases in different foreign languages) if it was always aligned to different phrases in a third “bridge” foreign language. As observed by Chen et al., this helped to remove spurious alignments. We used Finnish as the bridge language; when either F or G is Finnish, we used Spanish as the bridge language; when F and G were Finnish and Spanish, we used English as the bridge language. In our experiments, we used phrases of length 1 to 4 of the following six languages: English, Danish, German, Spanish, Finnish, and Dutch. All the phrasal alignm"
N10-1017,P09-1053,0,0.0178104,"aphrases of one another using their surrounding contexts. Lin and Pantel (2001) learn paraphrases using the distributional similarity of paths in dependency trees. Ibrahim et al. (2003) generalize syntactic paths in aligned monolingual sentence pairs in order to generate paraphrases. Pang et al. (2003) merge parse trees of monolingual sentence pairs, and then compress the merged tree into a word lattice that can subsequently be used to generate paraphrases. Recently, Zhao et al. (2008) used dependency parses to learn paraphrase patterns that include part-of-speech slots. In other recent work, Das and Smith (2009) use a generative model for paraphrase detection. Rather than outputing paraphrases of an input phrase, their system detects whether two input sentences are paraphrases of one another. 6 Conclusion and Future Work We have introduced HTP, a novel approach based on random walks and hitting times for the learning of paraphrases from bilingual parallel corpora. HTP works by converting aligned phrases into a graph, and finding paraphrases that are “close” to a phrase of interest. Compared to previous approaches, HTP is able to find more paraphrases by traversing paths of lengths longer than 2; util"
N10-1017,W03-1608,0,0.369045,"e the hitting time, which converges slowly on large graphs. In HTP, we have sought to obviate this issue by using truncated hitting time that can be computed efficiently by sampling random walks. Several approaches have been proposed to learn paraphrases. Barzilay and Mckeown (2001) acquire paraphrases from a monolingual parallel corpus using a co-training algorithm. Their co-trained classifier determines whether two phrases are paraphrases of one another using their surrounding contexts. Lin and Pantel (2001) learn paraphrases using the distributional similarity of paths in dependency trees. Ibrahim et al. (2003) generalize syntactic paths in aligned monolingual sentence pairs in order to generate paraphrases. Pang et al. (2003) merge parse trees of monolingual sentence pairs, and then compress the merged tree into a word lattice that can subsequently be used to generate paraphrases. Recently, Zhao et al. (2008) used dependency parses to learn paraphrase patterns that include part-of-speech slots. In other recent work, Das and Smith (2009) use a generative model for paraphrase detection. Rather than outputing paraphrases of an input phrase, their system detects whether two input sentences are paraphra"
N10-1017,N06-1058,0,0.292902,"Missing"
N10-1017,2005.mtsummit-papers.11,0,0.00330568,"149 f to n-gram nodes will have weight ngram . Likewise k for edges to the other two kinds of feature nodes. Each of the k outgoing edges from a feature node is simply set to have a weight of k1 . After adding the feature nodes, we again run M independent length-T random walks to estimate the truncated hitting times of the nodes, and return the English phrase nodes in order of increasing hitting times. 4 Experiments We conducted experiments to investigate how HTP compares with the state of the art, and to evaluate the contributions of its components. 4.1 Dataset We used the Europarl dataset (Koehn, 2005) for our experiments. This dataset contains English transcripts of the proceedings of the European Parliament, and their translations into 10 other European languages. In the dataset, there are about a million sentences per language, and English sentences are aligned with sentences in the other languages. Callison-Burch (2008) aligned English phrases with phrases in each of the other languages using Giza++ (Och and Ney, 2004). We used his English-foreign phrasal alignments which are publicly available on the web at http://ironman.jhu.edu/emnlp08.tar. In addition, we paired sentences of differe"
N10-1017,C94-1051,0,0.0782549,"er language (the “pivot”). Callison-Burch (2008) further refines BCB with a system that constrains paraphrases to have the same syntactic structure (Syntactic Bilingual Phrases: SBP). Introduction Automatically learning paraphrases, or alternative ways of expressing the same meaning, is an active area of NLP research because of its usefulness in a variety of applications, e.g., question answering (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Reizler et al., 2007), document summarization (Barzilay et al., 1999; McKeown et al., 2002), natural language generation (Iordanskaja et al., 1991; Lenke, 1994; Stede, 1999), machine transWe take a graphical view of the state-of-the-art BCB and SBP approaches by representing the bilingual parallel corpora as a graph. A node corresponds to a phrase, and an edge exists between two nodes if their corresponding phrases are aligned. This graphical form makes the limitations of the BCB/SBP approaches more evident. The BCB/SBP graph is limited to be bipartite with English nodes on one side and foreign language nodes on the other, and an edge can only exist between nodes on different sides. This neglects information between foreign language nodes that may a"
N10-1017,N06-1014,0,0.00885175,"h sentences are aligned with sentences in the other languages. Callison-Burch (2008) aligned English phrases with phrases in each of the other languages using Giza++ (Och and Ney, 2004). We used his English-foreign phrasal alignments which are publicly available on the web at http://ironman.jhu.edu/emnlp08.tar. In addition, we paired sentences of different non-English languages that correspond to the same English sentence, and aligned the phrases using 5 iterations of IBM model 1 in each direction, followed by 5 iterations of HMM alignment with paired training using the algorithm described in Liang et al. (2006). We further used the technique of Chen et al. (2009) to remove a phrase alignment F -G (where F and G are phrases in different foreign languages) if it was always aligned to different phrases in a third “bridge” foreign language. As observed by Chen et al., this helped to remove spurious alignments. We used Finnish as the bridge language; when either F or G is Finnish, we used Spanish as the bridge language; when F and G were Finnish and Spanish, we used English as the bridge language. In our experiments, we used phrases of length 1 to 4 of the following six languages: English, Danish, German"
N10-1017,W07-0716,0,0.103426,"Missing"
N10-1017,J04-4002,0,0.00647492,"conducted experiments to investigate how HTP compares with the state of the art, and to evaluate the contributions of its components. 4.1 Dataset We used the Europarl dataset (Koehn, 2005) for our experiments. This dataset contains English transcripts of the proceedings of the European Parliament, and their translations into 10 other European languages. In the dataset, there are about a million sentences per language, and English sentences are aligned with sentences in the other languages. Callison-Burch (2008) aligned English phrases with phrases in each of the other languages using Giza++ (Och and Ney, 2004). We used his English-foreign phrasal alignments which are publicly available on the web at http://ironman.jhu.edu/emnlp08.tar. In addition, we paired sentences of different non-English languages that correspond to the same English sentence, and aligned the phrases using 5 iterations of IBM model 1 in each direction, followed by 5 iterations of HMM alignment with paired training using the algorithm described in Liang et al. (2006). We further used the technique of Chen et al. (2009) to remove a phrase alignment F -G (where F and G are phrases in different foreign languages) if it was always al"
N10-1017,N03-1024,0,0.422313,"aph, and incorporates truncation techniques to prevent the graph from growing too large for efficiency. Current approaches, by contrast, implicitly presuppose the graph to be bipartite, are limited to finding paraphrases that are of length two away from a phrase, and do not generally permit easy incorporation of domain knowledge. Manual evaluation of generated output shows that our approach outperforms the state-of-the-art system of Callison-Burch (2008). Early work on paraphrase acquisition has focused on using monolingual parallel corpora (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004). While effective, such methods are hampered by the scarcity of monolingual parallel corpora, an obstacle that limits both the quantity and quality of the paraphrases learned. To address this limitation, Bannard and CallisonBurch (2005) focused their attention on the abundance of bilingual parallel corpora. The crux of this system (referred to below as ”BCB”) is to align phrases in a bilingual parallel corpus and hypothesize English phrases as potential paraphrases if they are aligned to the same phrase in another language (the “pivot”). Callison-Burch (2008) further refin"
N10-1017,W04-3219,1,0.662813,"es truncation techniques to prevent the graph from growing too large for efficiency. Current approaches, by contrast, implicitly presuppose the graph to be bipartite, are limited to finding paraphrases that are of length two away from a phrase, and do not generally permit easy incorporation of domain knowledge. Manual evaluation of generated output shows that our approach outperforms the state-of-the-art system of Callison-Burch (2008). Early work on paraphrase acquisition has focused on using monolingual parallel corpora (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004). While effective, such methods are hampered by the scarcity of monolingual parallel corpora, an obstacle that limits both the quantity and quality of the paraphrases learned. To address this limitation, Bannard and CallisonBurch (2005) focused their attention on the abundance of bilingual parallel corpora. The crux of this system (referred to below as ”BCB”) is to align phrases in a bilingual parallel corpus and hypothesize English phrases as potential paraphrases if they are aligned to the same phrase in another language (the “pivot”). Callison-Burch (2008) further refines BCB with a system"
N10-1017,P02-1006,0,0.0125909,"ed to below as ”BCB”) is to align phrases in a bilingual parallel corpus and hypothesize English phrases as potential paraphrases if they are aligned to the same phrase in another language (the “pivot”). Callison-Burch (2008) further refines BCB with a system that constrains paraphrases to have the same syntactic structure (Syntactic Bilingual Phrases: SBP). Introduction Automatically learning paraphrases, or alternative ways of expressing the same meaning, is an active area of NLP research because of its usefulness in a variety of applications, e.g., question answering (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Reizler et al., 2007), document summarization (Barzilay et al., 1999; McKeown et al., 2002), natural language generation (Iordanskaja et al., 1991; Lenke, 1994; Stede, 1999), machine transWe take a graphical view of the state-of-the-art BCB and SBP approaches by representing the bilingual parallel corpora as a graph. A node corresponds to a phrase, and an edge exists between two nodes if their corresponding phrases are aligned. This graphical form makes the limitations of the BCB/SBP approaches more evident. The BCB/SBP graph is limited to be bipartite with English nodes on one side and fore"
N10-1017,P07-1059,0,0.0478482,"lign phrases in a bilingual parallel corpus and hypothesize English phrases as potential paraphrases if they are aligned to the same phrase in another language (the “pivot”). Callison-Burch (2008) further refines BCB with a system that constrains paraphrases to have the same syntactic structure (Syntactic Bilingual Phrases: SBP). Introduction Automatically learning paraphrases, or alternative ways of expressing the same meaning, is an active area of NLP research because of its usefulness in a variety of applications, e.g., question answering (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Reizler et al., 2007), document summarization (Barzilay et al., 1999; McKeown et al., 2002), natural language generation (Iordanskaja et al., 1991; Lenke, 1994; Stede, 1999), machine transWe take a graphical view of the state-of-the-art BCB and SBP approaches by representing the bilingual parallel corpora as a graph. A node corresponds to a phrase, and an edge exists between two nodes if their corresponding phrases are aligned. This graphical form makes the limitations of the BCB/SBP approaches more evident. The BCB/SBP graph is limited to be bipartite with English nodes on one side and foreign language nodes on t"
N10-1017,P08-1089,0,0.0865398,"a monolingual parallel corpus using a co-training algorithm. Their co-trained classifier determines whether two phrases are paraphrases of one another using their surrounding contexts. Lin and Pantel (2001) learn paraphrases using the distributional similarity of paths in dependency trees. Ibrahim et al. (2003) generalize syntactic paths in aligned monolingual sentence pairs in order to generate paraphrases. Pang et al. (2003) merge parse trees of monolingual sentence pairs, and then compress the merged tree into a word lattice that can subsequently be used to generate paraphrases. Recently, Zhao et al. (2008) used dependency parses to learn paraphrase patterns that include part-of-speech slots. In other recent work, Das and Smith (2009) use a generative model for paraphrase detection. Rather than outputing paraphrases of an input phrase, their system detects whether two input sentences are paraphrases of one another. 6 Conclusion and Future Work We have introduced HTP, a novel approach based on random walks and hitting times for the learning of paraphrases from bilingual parallel corpora. HTP works by converting aligned phrases into a graph, and finding paraphrases that are “close” to a phrase of"
N15-1020,D13-1106,1,0.323212,"bels and attributes defining dialog states. In contrast, the dialog state in our neural network model is completely latent and directly optimized towards end-to-end performance. In this sense, 197 we believe the framework of this paper is a significant milestone towards more data-driven and less hand-coded dialog processing. Continuous representations of words and phrases estimated by neural network models have been applied on a variety of tasks ranging from Information Retrieval (IR) (Huang et al., 2013; Shen et al., 2014), Online Recommendation (Gao et al., 2014b), Machine Translation (MT) (Auli et al., 2013; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014), and Language Modeling (LM) (Bengio et al., 2003; Collobert and Weston, 2008). Gao et al. (2014a) successfully use an embedding model to refine the estimation of rare phrase-translation probabilities, which is traditionally affected by sparsity problems. Robustness to sparsity is a crucial property of our method, as it allows us to capture context information while avoiding unmanageable growth of model parameters. Our work extends the Recurrent Neural Network Language Model (RLM) of (Mikolov et al., 2010), which uses c"
N15-1020,W05-0909,0,0.0736125,"uced a corpus of 29M Twitter triples. Additionally, we hired crowdsourced raters to evaluate approximately 33K candidate triples. Judgments on a 5-point scale were obtained from 3 raters apiece. This yielded a set of 4232 triples with a mean score of 4 or better that was then randomly binned into a tuning set of 2118 triples and a test set of 2114 triples3 . The mean length of responses in these sets was approximately 11.5 tokens, after cleanup (e.g., stripping of emoticons), including punctuation. 5.2 Automatic Evaluation We evaluate all systems using BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), and supplement these results with more targeted human pairwise comparisons in Section 6.3. A major challenge in using these automated metrics for response generation is that the set of reasonable responses in our task is potentially vast and extremely diverse. The dataset construction method just described yields only a single reference for each status. Accordingly, we extend the set of references using an IR approach to mine potential responses, after which we have human judges rate their appropriateness. As we see in Section 6.3, it turns out that by optimizing systems towards BLEU using m"
N15-1020,D14-1179,0,0.0151747,"Missing"
N15-1020,P14-1129,0,0.0340037,"when generating long responses. 4.3 Dynamic-Context Generative Model II Because DCGM-I does not distinguish between c and m, that model has the propensity to underestimate the strong dependency that holds between m and r. Our third model (DCGM-II) addresses this issue by concatenating the two linear mappings of the bag-ofwords representations bc and bm in the input layer of the feed-forward network representing c and m (see Figure 3 right). Concatenating continuous representations prior to deep architectures is a common strategy to obtain order-sensitive representations (Bengio et al., 2003; Devlin et al., 2014). The forward equations for the context encoder are: 1 > 1 k1 = [b> c Wf , bm Wf ], > ` k` = σ(k`−1 Wf ) for ` = 2, · · · , L (8) where [x, y] denotes the concatenation of x and y vectors. In DCGM-II, the bias on the recurrent hidden state and the probability distribution over the next token are computed as described in Eq. 7. 5 Experimental Setting 5.1 Dataset Construction For computational efficiency and to alleviate the burden of human evaluators, we restrict the context sequence c to a single sentence. Hence, our dataset is composed of “triples” τ ≡ (cτ , mτ , rτ ) consisting of three sent"
N15-1020,P14-1066,1,0.560619,"stems remain hand-coded: in particular, the labels and attributes defining dialog states. In contrast, the dialog state in our neural network model is completely latent and directly optimized towards end-to-end performance. In this sense, 197 we believe the framework of this paper is a significant milestone towards more data-driven and less hand-coded dialog processing. Continuous representations of words and phrases estimated by neural network models have been applied on a variety of tasks ranging from Information Retrieval (IR) (Huang et al., 2013; Shen et al., 2014), Online Recommendation (Gao et al., 2014b), Machine Translation (MT) (Auli et al., 2013; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014), and Language Modeling (LM) (Bengio et al., 2003; Collobert and Weston, 2008). Gao et al. (2014a) successfully use an embedding model to refine the estimation of rare phrase-translation probabilities, which is traditionally affected by sparsity problems. Robustness to sparsity is a crucial property of our method, as it allows us to capture context information while avoiding unmanageable growth of model parameters. Our work extends the Recurrent Neural Network Language Mode"
N15-1020,D14-1002,1,0.788594,"stems remain hand-coded: in particular, the labels and attributes defining dialog states. In contrast, the dialog state in our neural network model is completely latent and directly optimized towards end-to-end performance. In this sense, 197 we believe the framework of this paper is a significant milestone towards more data-driven and less hand-coded dialog processing. Continuous representations of words and phrases estimated by neural network models have been applied on a variety of tasks ranging from Information Retrieval (IR) (Huang et al., 2013; Shen et al., 2014), Online Recommendation (Gao et al., 2014b), Machine Translation (MT) (Auli et al., 2013; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014), and Language Modeling (LM) (Bengio et al., 2003; Collobert and Weston, 2008). Gao et al. (2014a) successfully use an embedding model to refine the estimation of rare phrase-translation probabilities, which is traditionally affected by sparsity problems. Robustness to sparsity is a crucial property of our method, as it allows us to capture context information while avoiding unmanageable growth of model parameters. Our work extends the Recurrent Neural Network Language Mode"
N15-1020,D13-1176,0,0.046151,"tates. In contrast, the dialog state in our neural network model is completely latent and directly optimized towards end-to-end performance. In this sense, 197 we believe the framework of this paper is a significant milestone towards more data-driven and less hand-coded dialog processing. Continuous representations of words and phrases estimated by neural network models have been applied on a variety of tasks ranging from Information Retrieval (IR) (Huang et al., 2013; Shen et al., 2014), Online Recommendation (Gao et al., 2014b), Machine Translation (MT) (Auli et al., 2013; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014), and Language Modeling (LM) (Bengio et al., 2003; Collobert and Weston, 2008). Gao et al. (2014a) successfully use an embedding model to refine the estimation of rare phrase-translation probabilities, which is traditionally affected by sparsity problems. Robustness to sparsity is a crucial property of our method, as it allows us to capture context information while avoiding unmanageable growth of model parameters. Our work extends the Recurrent Neural Network Language Model (RLM) of (Mikolov et al., 2010), which uses continuous representations to estimate a probabilit"
N15-1020,P07-2045,0,0.0499714,"3.58 references per example on average (Table 1). The average lengths for the responses in the multi-reference tuning and test sets are 8.75 and 8.13 tokens respectively. 5.3 Feature Sets The response generation systems evaluated in this paper are parameterized as log-linear models in a framework typical of statistical machine translation (Och and Ney, 2004). These log-linear models comprise the following feature sets: MT MT features are derived from a large response generation system built along the lines of Ritter et al. (2011), which is based on a phrase-based MT decoder similar to Moses (Koehn et al., 2007). Our MT feature set includes the following features that are common in Moses: forward and backward maximum likelihood “translation” probabilities, word and phrase penalties, linear distortion, and a modified Kneser-Ney language model (Kneser and Ney, 1995) trained on Twitter responses. For the translation probabilities, we built a very large phrase table of 160.7 million entries by first filtering out Twitterisms (e.g., long sequences of vowels, hashtags), and then selecting candidate phrase pairs using Fisher’s exact test (Ritter et al., 2011). We also included MT decoder features specifical"
N15-1020,J04-4002,0,0.0179799,"a set of candidate triples {˜ τ }, human evaluators are asked to rate the quality of the response within the new triples {(cτ , mτ , rτ˜ )}. After human evaluation, we retain the references for which the score is 4 or better on a 5 point scale, resulting in 3.58 references per example on average (Table 1). The average lengths for the responses in the multi-reference tuning and test sets are 8.75 and 8.13 tokens respectively. 5.3 Feature Sets The response generation systems evaluated in this paper are parameterized as log-linear models in a framework typical of statistical machine translation (Och and Ney, 2004). These log-linear models comprise the following feature sets: MT MT features are derived from a large response generation system built along the lines of Ritter et al. (2011), which is based on a phrase-based MT decoder similar to Moses (Koehn et al., 2007). Our MT feature set includes the following features that are common in Moses: forward and backward maximum likelihood “translation” probabilities, word and phrase penalties, linear distortion, and a modified Kneser-Ney language model (Kneser and Ney, 1995) trained on Twitter responses. For the translation probabilities, we built a very lar"
N15-1020,P03-1021,0,0.041731,"r generalization. The last layer embeds the context vector into the hidden space of the decoder RLM. 5.5 Rescoring Setup We evaluate the proposed models by rescoring the n-best candidate responses obtained using the MT phrase-based decoder and the IR system. In contrast to MT, the candidate responses provided by IR have been created by humans and are less affected by fluency issues. The different n-best lists will provide a comprehensive testbed for our experiments. First, we augment the n-best list of the tuning set with the scores of the model of interest. Then, we run an iteration of MERT (Och, 2003) to estimate the log-linear weights of the new features. At test time, we rescore the test n-best list with the new weights. 6 Results 6.1 Lower and Upper Bounds Table 2 shows the expected upper and lower bounds for this task as suggested by BLEU scores for human responses and a random response baseline. The RAN DOM system comprises responses randomly extracted from the triples corpus. HUMAN is computed by choosing one reference amongst the multi-reference set for each context-status pair.4 Although the scores are lower than those usually reported in SMT tasks, the ranking of the three systems"
N15-1020,P02-1040,0,0.121444,"an 3 times in the corpus. This produced a corpus of 29M Twitter triples. Additionally, we hired crowdsourced raters to evaluate approximately 33K candidate triples. Judgments on a 5-point scale were obtained from 3 raters apiece. This yielded a set of 4232 triples with a mean score of 4 or better that was then randomly binned into a tuning set of 2118 triples and a test set of 2114 triples3 . The mean length of responses in these sets was approximately 11.5 tokens, after cleanup (e.g., stripping of emoticons), including punctuation. 5.2 Automatic Evaluation We evaluate all systems using BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), and supplement these results with more targeted human pairwise comparisons in Section 6.3. A major challenge in using these automated metrics for response generation is that the set of reasonable responses in our task is potentially vast and extremely diverse. The dataset construction method just described yields only a single reference for each status. Accordingly, we extend the set of references using an IR approach to mine potential responses, after which we have human judges rate their appropriateness. As we see in Section 6.3, it turns out that by o"
N15-1020,D11-1054,0,0.803346,"on-context-sensitive Machine Translation and Information Retrieval baselines. 1 message response yeah i’m on my way now ok good luck ! Figure 1: Example of three consecutive utterances occurring between two Twitter users A and B. Introduction Until recently, the goal of training open-domain conversational systems that emulate human conversation has seemed elusive. However, the vast quantities of conversational exchanges now available on social media websites such as Twitter and Reddit raise the prospect of building data-driven models that can begin to communicate conversationally. The work of Ritter et al. (2011), for example, demonstrates that a response generation system can be constructed from Twitter conversations using statistical machine translation techniques, where a status post by a Twitter user is “translated” into a plausible looking response. However, an approach such as that presented in Ritter et al. (2011) does not address the challenge of *The entirety of this work was conducted while at Microsoft Research. † Corresponding authors: Alessandro Sordoni (sordonia@iro.umontreal.ca) and Michel Galley (mgalley@microsoft.com). generating responses that are sensitive to the context of the conv"
N16-1014,P12-3007,0,0.0207071,"ilize a mutual information objective in the retrieval component of image caption retrieval. Below, we focus on the challenge of using MMI in response generation, comparing the performance of MMI models against maximum likelihood. 2 3 Related work The approach we take here is data-driven and end-toend. This stands in contrast to conventional dialog systems, which typically are template- or heuristicdriven even where there is a statistical component (Levin et al., 2000; Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Walker et al., 2003; Pieraccini et al., 2009; Young et al., 2010; Wang et al., 2011; Banchs and Li, 2012; Chen et al., 2013; Ameixa et al., 2014; Nio et al., 2014). We follow a newer line of investigation, originally introduced by Ritter et al. (2011), which frames response generation as a statistical machine translation (SMT) problem. Recent progress in SMT stemming from the use of neural language models (Sutskever et al., 2014; Gao et al., 2014; Bahdanau et 111 Sequence-to-Sequence Models Given a sequence of inputs X = {x1 , x2 , ..., xNx }, an LSTM associates each time step with an input gate, a memory gate and an output gate, respectively denoted as ik , fk and ok . We distinguish e and h wh"
N16-1014,P15-2073,1,0.256939,"ecutive message-response pairs spoken by different characters. We randomly selected two subsets as development and test datasets, each containing 2k pairs, with source and target length restricted to the range of [6,18]. 5.2 7 IMSDB (http://www.imsdb.com/) is a relatively small database of around 0.4 million sentences and thus not suitable for open domain dialogue training. 115 Model S EQ 2S EQ MMI-antiLM Evaluation For parameter tuning and final evaluation, we used B LEU (Papineni et al., 2002), which was shown to correlate reasonably well with human judgment on the response generation task (Galley et al., 2015). In the case of the Twitter models, we used multireference B LEU. As the IMSDB data is too limited to support extraction of multiple references, only single reference B LEU was used in training and evaluating the OSDb models. We did not follow Vinyals et al. (2015) in using perplexity as evaluation metric. Perplexity is unlikely to be a useful metric in our scenario, since our proposed model is designed to steer away from the standard S EQ 2S EQ model in order to diversify the outputs. We report degree of diversity by calculating the number of distinct unigrams and bigrams in generated respon"
N16-1014,P14-1066,1,0.170591,"systems, which typically are template- or heuristicdriven even where there is a statistical component (Levin et al., 2000; Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Walker et al., 2003; Pieraccini et al., 2009; Young et al., 2010; Wang et al., 2011; Banchs and Li, 2012; Chen et al., 2013; Ameixa et al., 2014; Nio et al., 2014). We follow a newer line of investigation, originally introduced by Ritter et al. (2011), which frames response generation as a statistical machine translation (SMT) problem. Recent progress in SMT stemming from the use of neural language models (Sutskever et al., 2014; Gao et al., 2014; Bahdanau et 111 Sequence-to-Sequence Models Given a sequence of inputs X = {x1 , x2 , ..., xNx }, an LSTM associates each time step with an input gate, a memory gate and an output gate, respectively denoted as ik , fk and ok . We distinguish e and h where ek denotes the vector for an individual text unit (for example, a word or sentence) at time step k while hk denotes the vector computed by LSTM model at time k by combining ek and hk−1 . ck is the cell state vector at time k, and σ denotes the sigmoid function. Then, the vector representation hk for each time step 2 Augmenting our technique"
N16-1014,D13-1111,0,0.0100372,"Missing"
N16-1014,P07-2045,0,0.0627244,"et We first report performance on Twitter datasets in Table 2, along with results for different models (i.e., Machine Translation and MT+neural reranking) reprinted from Sordoni et al. (2015) on the same dataset. The baseline is the S EQ 2S EQ model with its standard likelihood objective and a beam size of 200. We compare this baseline against greedy-search S EQ 2S EQ (Vinyals and Le, 2015), which can help achieve higher diversity by increasing search errors.8 Machine Translation is the phrase-based MT system described in (Ritter et al., 2011). MT features include commonly used ones in Moses (Koehn et al., 2007), e.g., forward and backward maximum likelihood “translation” probabilities, word and phrase penalties, linear distortion, etc. For more details, refer to Sordoni et al. (2015). MT+neural reranking is the phrase-based MT system, reranked using neural models. N-best lists are first generated from the MT system. Recurrent neural models generate scores for N-best list candidates given the input messages. These generated scores are re-incorporated to rerank all the candidates. Additional features to score [1, 2, 3, 4]-gram matches between context and response and between message and context (conte"
N16-1014,P15-1002,0,0.00622466,"200. The top examples are the responses with the highest average probability loglikelihoods in the N-best list. Lower-ranked, less-generic responses were manually chosen. speech recognition (Bahl et al., 1986; Brown, 1987), as an optimization objective that measures the mutual dependence between inputs and outputs. Below, we present practical strategies for neural generation models that use MMI as an objective function. We show that use of MMI results in a clear decrease in the proportion of generic response sequences, generating correspondingly more varied and interesting outputs. al., 2015; Luong et al., 2015) has inspired attempts to extend these neural techniques to response generation. Sordoni et al. (2015) improved upon Ritter et al. (2011) by rescoring the output of a phrasal SMT-based conversation system with a S EQ 2S EQ model that incorporates prior context. (Serban et al., 2015; Shang et al., 2015; Vinyals and Le, 2015; Wen et al., 2015) apply direct end-to-end S EQ 2S EQ models These S EQ 2S EQ models are Long Short-Term Memory (LSTM) neural networks (Hochreiter and Schmidhuber, 1997) that can implicitly capture compositionality and long-span dependencies. (Wen et al., 2015) attempt to le"
N16-1014,P03-1021,0,0.0536546,"sponses (T ) interchanged. 4.5 4.5.1 Decoding MMI-antiLM As described in Section 4.3.1, decoding using log p(T |S) − λU (T ) can be readily implemented by predicting tokens at each time-step. In addition, we found in our experiments that it is also important to take into account the length of responses in decod114 ing. We thus linearly combine the loss function with length penalization, leading to an ultimate score for a given target T as follows: Score(T ) = p(T |S) − λU (T ) + γNt (15) where Nt denotes the length of the target and γ denotes associated weight. We optimize γ and λ using MERT (Och, 2003) on N-best lists of response candidates. The N-best lists are generated using the decoder with beam size B = 200. We set a maximum length of 20 for generated candidates. At each time step of decoding, we are presented with B × B candidates. We first add all hypotheses with an EOS token being generated at current time step to the N-best list. Next we preserve the top B unfinished hypotheses and move to next time step. We therefore maintain beam size of 200 constant when some hypotheses are completed and taken down by adding in more unfinished hypotheses. This will lead the size of final N-best"
N16-1014,W00-0306,0,0.0299638,"oes not require identifying lexical overlap to foster diversity.2 On a somewhat different task, Mao et al. (2015, Section 6) utilize a mutual information objective in the retrieval component of image caption retrieval. Below, we focus on the challenge of using MMI in response generation, comparing the performance of MMI models against maximum likelihood. 2 3 Related work The approach we take here is data-driven and end-toend. This stands in contrast to conventional dialog systems, which typically are template- or heuristicdriven even where there is a statistical component (Levin et al., 2000; Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Walker et al., 2003; Pieraccini et al., 2009; Young et al., 2010; Wang et al., 2011; Banchs and Li, 2012; Chen et al., 2013; Ameixa et al., 2014; Nio et al., 2014). We follow a newer line of investigation, originally introduced by Ritter et al. (2011), which frames response generation as a statistical machine translation (SMT) problem. Recent progress in SMT stemming from the use of neural language models (Sutskever et al., 2014; Gao et al., 2014; Bahdanau et 111 Sequence-to-Sequence Models Given a sequence of inputs X = {x1 , x2 , ..., xNx }, an LSTM associates each time"
N16-1014,P02-1040,0,0.120743,"Missing"
N16-1014,D11-1054,0,0.770418,"tantive gains in B LEU scores on two conversational datasets and in human evaluations. 1 Introduction Conversational agents are of growing importance in facilitating smooth interaction between humans and their electronic devices, yet conventional dialog systems continue to face major challenges in the form of robustness, scalability and domain adaptation. Attention has thus turned to learning conversational patterns from data: researchers have begun to explore data-driven generation of conversational responses within the framework of statistical machine translation (SMT), either phrase-based (Ritter et al., 2011), or using neural networks to rerank, or directly in the form of sequence-to-sequence (S EQ 2S EQ) models (Sordoni et al., 2015; Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2015; Wen et al., 2015). S EQ 2S EQ models offer the promise of scalability and language-independence, together with the capacity * The entirety of this work was conducted at Microsoft. to implicitly learn semantic and syntactic relations between pairs, and to capture contextual dependencies (Sordoni et al., 2015) in a way not possible with conventional SMT approaches (Ritter et al., 2011). An engaging response"
N16-1014,P15-1152,0,0.523144,"h interaction between humans and their electronic devices, yet conventional dialog systems continue to face major challenges in the form of robustness, scalability and domain adaptation. Attention has thus turned to learning conversational patterns from data: researchers have begun to explore data-driven generation of conversational responses within the framework of statistical machine translation (SMT), either phrase-based (Ritter et al., 2011), or using neural networks to rerank, or directly in the form of sequence-to-sequence (S EQ 2S EQ) models (Sordoni et al., 2015; Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2015; Wen et al., 2015). S EQ 2S EQ models offer the promise of scalability and language-independence, together with the capacity * The entirety of this work was conducted at Microsoft. to implicitly learn semantic and syntactic relations between pairs, and to capture contextual dependencies (Sordoni et al., 2015) in a way not possible with conventional SMT approaches (Ritter et al., 2011). An engaging response generation system should be able to output grammatical, coherent responses that are diverse and interesting. In practice, however, neural conversation models tend to ge"
N16-1014,N15-1020,1,0.849031,"of growing importance in facilitating smooth interaction between humans and their electronic devices, yet conventional dialog systems continue to face major challenges in the form of robustness, scalability and domain adaptation. Attention has thus turned to learning conversational patterns from data: researchers have begun to explore data-driven generation of conversational responses within the framework of statistical machine translation (SMT), either phrase-based (Ritter et al., 2011), or using neural networks to rerank, or directly in the form of sequence-to-sequence (S EQ 2S EQ) models (Sordoni et al., 2015; Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2015; Wen et al., 2015). S EQ 2S EQ models offer the promise of scalability and language-independence, together with the capacity * The entirety of this work was conducted at Microsoft. to implicitly learn semantic and syntactic relations between pairs, and to capture contextual dependencies (Sordoni et al., 2015) in a way not possible with conventional SMT approaches (Ritter et al., 2011). An engaging response generation system should be able to output grammatical, coherent responses that are diverse and interesting. In practice, howe"
N16-1014,D15-1199,0,0.506634,"Missing"
N19-1125,K16-1002,0,0.544301,"versity and relevance of the responses, compared to strong baselines on two datasets with one-tomany context-response mapping. 2 Related Work Grounded conversation models utilize extra context inputs besides conversation history, such as persona (Li et al., 2016b), textual knowledge (Ghazvininejad et al., 2017; Galley et al., 2019), dialog act (Zhao et al., 2017) and emotion (Huber et al., 2018). Our approach does not depend on such extra input and thus is complementary to this line of studies. Variational autoencoder (VAE) models explicitly model the uncertainty of responses in latent space. Bowman et al. (2016) used VAE with Long-Short Term Memory (LSTM) cells to generate sentences. The basic idea of VAE is to encode the input x into a probability distribution (e.g. Gaussian) z instead of a point encoding. However, it suffers from the vanishing latent variable problem (Bowman et al., 2016; Zhao et al., 2017) when applied to text generation tasks. Bowman et al. (2016); Fu et al. (2019) proposed to tackle this problem with word dropping and specific KL annealing methods. Zhao et al. (2017) proposed to add a bag-of-word loss, complementary to KL annealing. Applying this to a CVAE conversation model, th"
N19-1125,W14-4012,0,0.196356,"Missing"
N19-1125,N19-1021,1,0.840516,"et al., 2018). Our approach does not depend on such extra input and thus is complementary to this line of studies. Variational autoencoder (VAE) models explicitly model the uncertainty of responses in latent space. Bowman et al. (2016) used VAE with Long-Short Term Memory (LSTM) cells to generate sentences. The basic idea of VAE is to encode the input x into a probability distribution (e.g. Gaussian) z instead of a point encoding. However, it suffers from the vanishing latent variable problem (Bowman et al., 2016; Zhao et al., 2017) when applied to text generation tasks. Bowman et al. (2016); Fu et al. (2019) proposed to tackle this problem with word dropping and specific KL annealing methods. Zhao et al. (2017) proposed to add a bag-of-word loss, complementary to KL annealing. Applying this to a CVAE conversation model, they showed that even greedy decoding can generate diverse responses. However, as VAE/CVAE conversation models can be limited to a simple latent representations such as standard Gaussian distribution, Gu et al. (2018) proposed to enrich the latent space by leveraging a Gaussian mixture prior. Our work takes a geometrical approach that is fundamentally different from probabilistic"
N19-1125,P16-1094,1,0.881173,"Yes I do. When? Figure 1: Illustration of one context and its multiple responses in the latent space induced by our model. Distance and direction from the predicted response vector given the context roughly match the relevance and diversity, respectively. Based on the example in Table 2.2 Introduction The field of neural response generation is advancing rapidly both in terms of research and commercial applications (Gao et al., 2019; Zhou et al., 2018; Yoshino et al., 2019; Zhang et al., 2019). Nevertheless, vanilla sequence-to-sequence (S2S) models often generate bland and generic responses (Li et al., 2016a). Li et al. (2016a) encourage diversity by re-ranking the beam search results according to their mutual information with the conversation context. However, as beam search itself often produces lists of nearly identical sequences, this method can require a large beam width (e.g. 200). As a result, re-ranking can be extremely 1 An implementation of our model is available at https: //github.com/golsun/SpaceFusion 2 For simplicity, we omitted the response at the center: ”I would love to play this game”. See Table 2 for more details. time-consuming, raising difficulties for real-time applications"
N19-1125,I17-1061,1,0.843614,"ted in Figure 1. To induce such a latent space, we leverage two different models: 1) a S2S model, producing the predicted response vector (the black dot at the center in Figure 1), and 2) an autoencoder (AE) model, yielding the vectors for potential responses (the colored dots). In order to make the S2S and AE share the same latent space (the cloud), we use the same decoder for both and train them jointly end-to-end with novel regularization terms. As this fuses the two latent spaces, we refer to our model as S PACE F USION. Regularization is necessary because only sharing the decoder, as in (Luan et al., 2017), does not necessarily align the latent spaces obtained by S2S and AE respectively or impose a disentangled structure onto the space. We introduce two regularization terms to tackle this issue. 1) interpolation term: we encourage a smooth semantic transition along the path between the predicted response vector and each target response vector (arrowed lines in Figure 1). This term effectively prevents semantically different responses from aligning in the same direction, essentially scattering them over different directions. 2) fusion term: we want the vectors from the two models to be distribut"
N19-1125,P02-1040,0,0.103428,"Missing"
N19-1125,P16-1009,0,0.0250463,"ibutions in representation and difficulties in training. Decoding and ranking encourage diversity during the decoding stage. As “vanilla” beam search often produces lists of nearly identical sequences, Vijayakumar et al. (2016) propose to include a dissimilarity term in the objective of beam search decoding. Li et al. (2016a) re-ranked the results 1230 obtained by beam search based on mutual information with the context using a separately trained response-to-context S2S model. ? S2S encoder +? ℒ fuse Multi-task learning is another line of studies related to the present work (see Section 3.2). Sennrich et al. (2016) use multi-task learning to improve neural machine translation by utilizing monolingual data, which usually far exceeds the amount of parallel data. A similar idea is applied by Luan et al. (2017) to conversational modeling, involving two tasks: 1) a S2S model that learns a context-to-response mapping using conversation data, and 2) an AE model that utilizes speakerspecific non-conversational data. The decoders of S2S and AE were shared, and the two tasks were trained alternately. 3 3.1 The S PACE F USION Model Problem statement Let D = [(x0 , y0 ), (x1 , y1 ), · · · , (xn , yn )] denote a con"
N19-1125,1983.tc-1.13,0,0.435323,"Missing"
N19-1125,N16-1014,1,0.836698,"Yes I do. When? Figure 1: Illustration of one context and its multiple responses in the latent space induced by our model. Distance and direction from the predicted response vector given the context roughly match the relevance and diversity, respectively. Based on the example in Table 2.2 Introduction The field of neural response generation is advancing rapidly both in terms of research and commercial applications (Gao et al., 2019; Zhou et al., 2018; Yoshino et al., 2019; Zhang et al., 2019). Nevertheless, vanilla sequence-to-sequence (S2S) models often generate bland and generic responses (Li et al., 2016a). Li et al. (2016a) encourage diversity by re-ranking the beam search results according to their mutual information with the conversation context. However, as beam search itself often produces lists of nearly identical sequences, this method can require a large beam width (e.g. 200). As a result, re-ranking can be extremely 1 An implementation of our model is available at https: //github.com/golsun/SpaceFusion 2 For simplicity, we omitted the response at the center: ”I would love to play this game”. See Table 2 for more details. time-consuming, raising difficulties for real-time applications"
N19-1125,P17-1061,0,0.528488,": //github.com/golsun/SpaceFusion 2 For simplicity, we omitted the response at the center: ”I would love to play this game”. See Table 2 for more details. time-consuming, raising difficulties for real-time applications. This highlights the need to improve the diversity of candidates before re-ranking, and the need to optimize for diversity during training rather than just at the decoding stage. While various approaches have been explored to diversify the output of conversation models, the improvement often comes at the cost of decreased response relevance along other dimensions. For instance, Zhao et al. (2017) present an approach to enhancing diversity by mapping diverse responses to a probability distribution using a conditional variational autoencoder (CVAE). Despite the improved response diversity, this approach reduces response relevance as measured against the baseline. One possible reason for this diversityrelevance trade-off is that such probabilistic approaches are not explicitly encouraged to induce a disentangled representation in latent space for 1229 Proceedings of NAACL-HLT 2019, pages 1229–1238 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguis"
P01-1020,P98-1006,0,0.0338465,"gned to facilitate the identification of areas for investigation and improvement. It focuses on evaluating the wellformedness of output and does not address issues of evaluating content transfer. Researchers are now applying automated evaluation in MT and natural language generation tasks, both as system-internal goodness metrics and for the assessment of output. Langkilde and Knight (1998), for example, employ n-gram metrics to select among candidate outputs in natural language generation, while Ringger et al. (2001) use ngram perplexity to compare the output of MT systems. Su et al. (1992), Alshawi et al. (1998) and Bangalore et al. (2000) employ string edit distance between reference and output sentences to gauge output quality for MT and generation. To be useful to researchers, however, assessment must provide linguistic information that can guide in identifying areas where work is required. (See Nyberg et al., 1994 for useful discussion of this issue.) The better the MT system, the more its output will resemble human-generated text. Indeed, MT might be considered a solved problem should it ever become impossible to distinguish automated output from human translation. We have observed that in gener"
P01-1020,W00-1401,0,0.0198838,"eas for investigation and improvement. It focuses on evaluating the wellformedness of output and does not address issues of evaluating content transfer. Researchers are now applying automated evaluation in MT and natural language generation tasks, both as system-internal goodness metrics and for the assessment of output. Langkilde and Knight (1998), for example, employ n-gram metrics to select among candidate outputs in natural language generation, while Ringger et al. (2001) use ngram perplexity to compare the output of MT systems. Su et al. (1992), Alshawi et al. (1998) and Bangalore et al. (2000) employ string edit distance between reference and output sentences to gauge output quality for MT and generation. To be useful to researchers, however, assessment must provide linguistic information that can guide in identifying areas where work is required. (See Nyberg et al., 1994 for useful discussion of this issue.) The better the MT system, the more its output will resemble human-generated text. Indeed, MT might be considered a solved problem should it ever become impossible to distinguish automated output from human translation. We have observed that in general humans can easily and rel"
P01-1020,P98-1116,0,0.0115683,"n of machine translation (MT) output is an expensive process, often prohibitively so when evaluations must be performed quickly and frequently in order to measure progress. This paper describes an approach to automated evaluation designed to facilitate the identification of areas for investigation and improvement. It focuses on evaluating the wellformedness of output and does not address issues of evaluating content transfer. Researchers are now applying automated evaluation in MT and natural language generation tasks, both as system-internal goodness metrics and for the assessment of output. Langkilde and Knight (1998), for example, employ n-gram metrics to select among candidate outputs in natural language generation, while Ringger et al. (2001) use ngram perplexity to compare the output of MT systems. Su et al. (1992), Alshawi et al. (1998) and Bangalore et al. (2000) employ string edit distance between reference and output sentences to gauge output quality for MT and generation. To be useful to researchers, however, assessment must provide linguistic information that can guide in identifying areas where work is required. (See Nyberg et al., 1994 for useful discussion of this issue.) The better the MT sys"
P01-1020,C94-1013,0,0.0234832,"goodness metrics and for the assessment of output. Langkilde and Knight (1998), for example, employ n-gram metrics to select among candidate outputs in natural language generation, while Ringger et al. (2001) use ngram perplexity to compare the output of MT systems. Su et al. (1992), Alshawi et al. (1998) and Bangalore et al. (2000) employ string edit distance between reference and output sentences to gauge output quality for MT and generation. To be useful to researchers, however, assessment must provide linguistic information that can guide in identifying areas where work is required. (See Nyberg et al., 1994 for useful discussion of this issue.) The better the MT system, the more its output will resemble human-generated text. Indeed, MT might be considered a solved problem should it ever become impossible to distinguish automated output from human translation. We have observed that in general humans can easily and reliably categorize a sentence as either machine- or human-generated. Moreover, they can usually justify their decision. This observation suggests that evaluation of the wellformedness of output sentences can be treated as a classification problem: given a sentence, how accurately can w"
P01-1020,2001.mtsummit-papers.53,0,0.103203,"sification task that targets both linguistic features and more abstract features such as ngram perplexity. 2 Data Our corpus consists of 350,000 aligned SpanishEnglish sentence pairs taken from published computer software manuals and online help documents. We extracted 200,000 English sentences for building language models to evaluate per-sentence perplexity. From the remainder of the corpus, we extracted 100,000 aligned sentence pairs. The Spanish sentences in this latter sample were then translated by the Microsoft machine translation system, which was trained on documents from this domain (Richardson et al., 2001). This yielded a set of 200,000 English sentences, one half of which were English reference sentences, and the other half of which were MT output. (The Spanish sentences were not used in building or evaluating the classifiers). We split the 200,000 English sentences 90/10, to yield 180,000 sentences for training classifiers and 20,000 sentences that we used as held-out test data. Training and test data were evenly divided between reference English sentences and Spanish-to-English translations. 3 Features The selection of features used in our classification task was motivated by failure analysi"
P01-1020,2001.mtsummit-papers.68,0,0.11958,"uently in order to measure progress. This paper describes an approach to automated evaluation designed to facilitate the identification of areas for investigation and improvement. It focuses on evaluating the wellformedness of output and does not address issues of evaluating content transfer. Researchers are now applying automated evaluation in MT and natural language generation tasks, both as system-internal goodness metrics and for the assessment of output. Langkilde and Knight (1998), for example, employ n-gram metrics to select among candidate outputs in natural language generation, while Ringger et al. (2001) use ngram perplexity to compare the output of MT systems. Su et al. (1992), Alshawi et al. (1998) and Bangalore et al. (2000) employ string edit distance between reference and output sentences to gauge output quality for MT and generation. To be useful to researchers, however, assessment must provide linguistic information that can guide in identifying areas where work is required. (See Nyberg et al., 1994 for useful discussion of this issue.) The better the MT system, the more its output will resemble human-generated text. Indeed, MT might be considered a solved problem should it ever become"
P01-1020,C92-2067,0,0.0259892,"ed evaluation designed to facilitate the identification of areas for investigation and improvement. It focuses on evaluating the wellformedness of output and does not address issues of evaluating content transfer. Researchers are now applying automated evaluation in MT and natural language generation tasks, both as system-internal goodness metrics and for the assessment of output. Langkilde and Knight (1998), for example, employ n-gram metrics to select among candidate outputs in natural language generation, while Ringger et al. (2001) use ngram perplexity to compare the output of MT systems. Su et al. (1992), Alshawi et al. (1998) and Bangalore et al. (2000) employ string edit distance between reference and output sentences to gauge output quality for MT and generation. To be useful to researchers, however, assessment must provide linguistic information that can guide in identifying areas where work is required. (See Nyberg et al., 1994 for useful discussion of this issue.) The better the MT system, the more its output will resemble human-generated text. Indeed, MT might be considered a solved problem should it ever become impossible to distinguish automated output from human translation. We have"
P01-1020,C98-1112,0,\N,Missing
P01-1020,C98-1006,0,\N,Missing
P06-1032,P05-1066,0,0.0169251,"information from TV Candidate 5: And we can learn a lot of knowledge or new information from TV Table 3. Multiple replacement candidates generated by 45K training set He has published thirty-two pieces of papers. In this equal world, lots of people are still concerned on the colors of them … The inability of our translation system to handle such discontinuities in a unitary manner reflects the limited ability of current SMT modeling techniques to capture long-distance effects. Similar alternations are rife in bilingual data, e.g., ne…pas in French (Fox, 2002) and separable prefixes in German (Collins et al. 2005). As SMT models become more adept at modeling long-distance effects in a principled manner, monolingual proofing will benefit as well. The Missed category is heterogeneous. The SMT system has an inherent bias against deletion, with the result that unwanted determiners tended not to be deleted, especially in the smaller training sets. Other errors related to coverage in the development data set. Several occurrences of greengrocer’s apostrophes (tea’s, equipment’s) caused correction failures: these were not anticipated when engineering the training data. Likewise, the test data presented several"
P06-1032,W03-0209,0,0.0154667,"little progress in this area over the last decade. Research into computer feedback for ESL writers remains largely focused on smallscale pedagogical systems implemented within the framework of CALL (Computer Aided Language Learning) (Reuer 2003; Vanderventer Faltin, 2003), while commercial ESL grammar checkers remain brittle and difficult to customize to meet the needs of ESL writers of different first-language (L1) backgrounds and skill levels. Some researchers have begun to apply statistical techniques to identify learner errors in the context of essay evaluation (Chodorow & Leacock, 2000; Lonsdale & Strong-Krause, 2003), to detect non-native text (Tomokiyo & Jones, 2001), and to support lexical selection by ESL learners through first-language translation (Liu et al., 2000). However, none of this work appears to directly address the more general problem of how to robustly provide feedback to ESL writers—and for that matter non-native writers in any second language—in a way that is easily tailored to different L1 backgrounds and secondlanguage (L2) skill levels. In this paper, we show that a noisy channel model instantiated within the paradigm of Statistical Machine Translation (SMT) (Brown et al., 1993) can s"
P06-1032,2005.iwslt-1.12,0,0.0192267,"errors produced by ESL writers of specific L1 backgrounds can be captured in the channel model as an emergent property of training data consisting ESL sentences aligned with their corrected edited counterparts. The highest frequency errors and infelicities should emerge as targets for replacement, while lesser frequency or idiosyncratic problems will in general not surface as false flags. 2.3 Implementation In this paper, we explore the use of a large-scale production statistical machine translation system to correct a class of ESL errors. A detailed description of the system can be found in (Menezes & Quirk 2005) and (Quirk et al., 2005). In keeping with current best practices in SMT, our system is a phrasal machine translation system that attempts to learn mappings between “phrases” (which may not correspond to linguistic units) rather than individual words. What distinguishes 250 this system from other phrasal SMT systems is that rather than aligning simple sequences of words, it maps small phrasal “treelets” generated by a dependency parse to corresponding strings in the target. This “Tree-To-String” model holds promise in that it allows us to potentially benefit from being able to access a certain"
P06-1032,P03-1021,0,0.00252946,"consideration when it is sought to handle ungrammatical or otherwise illformed ESL input, but also simultaneously to capture relationships not involving contiguous strings, for example determiner-noun relations. In our pilot study, this system was employed without modification to the system architecture. The sole adjustment made was to have both Source (erroneous) and Target (correct) sentences tokenized using an English language tokenizer. N-best results for phrasal alignment and ordering models in the decoder were optimized by lambda training via Maximum Bleu, along the lines described in (Och, 2003). This procedure yielded a list of 14 words: knowledge, food, homework, fruit, news, color, nutrition, equipment, paper, advice, haste, information, lunch, and tea. 3 Countability errors involving these words are scattered across 46 sentences in the CLEC corpus. For a baseline representing the level of writing assistance currently available to the average ESL writer, we submitted these sentences to the proofing tools in Microsoft Word 2003. The spelling and grammar checkers correctly identified 21 of the 46 relevant errors, proposed one incorrect substitution (a few advice a few advices), and"
P06-1032,P00-1056,0,0.0393123,"Missing"
P06-1032,P05-1034,0,0.0146714,"Missing"
P06-1032,N01-1031,0,0.0382049,"Missing"
P06-1032,A00-2019,0,\N,Missing
P06-1032,P00-1067,0,\N,Missing
P06-1032,J93-2003,0,\N,Missing
P06-1032,C94-1002,0,\N,Missing
P06-1032,W02-1039,0,\N,Missing
P15-2073,W05-0909,0,0.0517039,"irable. Tasks with intrinsically diverse targets range from machine translation, summarization, sentence compression, paraphrase generation, and image-to-text to generation of conversational interactions. A major hurdle for these tasks is automation of evaluation, since the space of plausible outputs can be enormous, and it is it impractical to run a new human evaluation every time a new model is built or parameters are modified. In Statistical Machine Translation (SMT), the automation problem has to a large extent been ameliorated by metrics such as B LEU (Papineni et al., 2002) and M ETEOR (Banerjee and Lavie, 2005) Although B LEU is not immune from criticism (e.g., Callison-Burch et al. (2006)), its properties are well understood, B LEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research. † Corresponding author: mgalley@microsoft.com 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed. B LEU has been less successfully applied to nonSMT generation tasks owing to the larger space of plausible outputs. As a result, attempts have been made to adapt the"
P15-2073,E06-1032,0,0.0476642,"on, summarization, sentence compression, paraphrase generation, and image-to-text to generation of conversational interactions. A major hurdle for these tasks is automation of evaluation, since the space of plausible outputs can be enormous, and it is it impractical to run a new human evaluation every time a new model is built or parameters are modified. In Statistical Machine Translation (SMT), the automation problem has to a large extent been ameliorated by metrics such as B LEU (Papineni et al., 2002) and M ETEOR (Banerjee and Lavie, 2005) Although B LEU is not immune from criticism (e.g., Callison-Burch et al. (2006)), its properties are well understood, B LEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research. † Corresponding author: mgalley@microsoft.com 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed. B LEU has been less successfully applied to nonSMT generation tasks owing to the larger space of plausible outputs. As a result, attempts have been made to adapt the metric. To foster diversity in paraphrase generation, Sun and Zhou (2012) propos"
P15-2073,2003.mtsummit-papers.9,0,0.031569,"run a new human evaluation every time a new model is built or parameters are modified. In Statistical Machine Translation (SMT), the automation problem has to a large extent been ameliorated by metrics such as B LEU (Papineni et al., 2002) and M ETEOR (Banerjee and Lavie, 2005) Although B LEU is not immune from criticism (e.g., Callison-Burch et al. (2006)), its properties are well understood, B LEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research. † Corresponding author: mgalley@microsoft.com 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed. B LEU has been less successfully applied to nonSMT generation tasks owing to the larger space of plausible outputs. As a result, attempts have been made to adapt the metric. To foster diversity in paraphrase generation, Sun and Zhou (2012) propose a metric called iB LEU in which the B LEU score is discounted by a B LEU score computed between the source and paraphrase. This solution, in addition to being dependent on a tunable parameter, is specific only to paraphrase. In image captioning tasks, Ve"
P15-2073,N12-1017,0,0.0244703,"ri,j )  PP i g ∈ n-grams(hi ) maxj wi,j ·#g (hi ) n 1 e(1−ρ/η) if η &gt; ρ otherwise Discriminative B LEU (2) where ρ and η are respectively hypothesis and reference lengths.2 Then corpus-level n-gram precision is defined as:  P P i g ∈ n-grams(hi ) maxj #g (hi , ri,j ) P P pn = i g ∈ n-grams(hi ) #g (hi ) where #g (·) is the number of occurrences of n-gram g in a given  sentence, and #g (u, v) is a shorthand for min #g (u), #g (v) . It has been demonstrated that metrics such as B LEU show increased correlation with human judgment as the number of references increases (Przybocki et al., 2008; Dreyer and Marcu, 2012). Unfortunately, gathering multiple references is difficult in the case of conversations. Data gathered from naturally occurring conversations offer only one response per message. One could search (c, m) pairs that occur multiple times in conversational data with the hope of finding distinct responses, but this solution is not feasible. Indeed, the larger 1 Unless mentioned otherwise, B LEU refers to the original IBM B LEU as first described in (Papineni et al., 2002). 2 In the case of multiple references, B LEU selects the reference whose length is closest to that of the hypothesis. In a nuts"
P15-2073,D14-1020,0,0.0691491,"evaluation every time a new model is built or parameters are modified. In Statistical Machine Translation (SMT), the automation problem has to a large extent been ameliorated by metrics such as B LEU (Papineni et al., 2002) and M ETEOR (Banerjee and Lavie, 2005) Although B LEU is not immune from criticism (e.g., Callison-Burch et al. (2006)), its properties are well understood, B LEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research. † Corresponding author: mgalley@microsoft.com 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed. B LEU has been less successfully applied to nonSMT generation tasks owing to the larger space of plausible outputs. As a result, attempts have been made to adapt the metric. To foster diversity in paraphrase generation, Sun and Zhou (2012) propose a metric called iB LEU in which the B LEU score is discounted by a B LEU score computed between the source and paraphrase. This solution, in addition to being dependent on a tunable parameter, is specific only to paraphrase. In image captioning tasks, Vendantam et al. (2015), emp"
P15-2073,N15-1124,0,0.0162952,"w model is built or parameters are modified. In Statistical Machine Translation (SMT), the automation problem has to a large extent been ameliorated by metrics such as B LEU (Papineni et al., 2002) and M ETEOR (Banerjee and Lavie, 2005) Although B LEU is not immune from criticism (e.g., Callison-Burch et al. (2006)), its properties are well understood, B LEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research. † Corresponding author: mgalley@microsoft.com 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed. B LEU has been less successfully applied to nonSMT generation tasks owing to the larger space of plausible outputs. As a result, attempts have been made to adapt the metric. To foster diversity in paraphrase generation, Sun and Zhou (2012) propose a metric called iB LEU in which the B LEU score is discounted by a B LEU score computed between the source and paraphrase. This solution, in addition to being dependent on a tunable parameter, is specific only to paraphrase. In image captioning tasks, Vendantam et al. (2015), employ a variant of B LEU"
P15-2073,W07-0734,0,0.0243517,"ool”, “well then! Why were the biscuits needed?”); others are a little more plausible, but irrelevant or possibly topic changing (“ohh I love that song”). Higher-valued positive-weighted mined responses are typically reasonably appropriate and relevant (even though 447 3 For this work, we sought 2 additional annotations of the seed responses for consistency with the mined responses. As a result, scores for some seed responses slipped below our initial threshold of 4. Nonetheless, these responses were retained. test set.4 While much prior work assesses automatic metrics for MT and other tasks (Lavie and Agarwal, 2007; Hodosh et al., 2013) by computing correlations on observations consisting of single-sentence system outputs, it has been shown (e.g., Przybocki et al. (2008)) that correlation coefficients significantly increase as observation units become larger. For instance, corpus-level or system-level correlations tend to be much higher than sentence-level correlations; Graham and Baldwin (2014) show that B LEU is competitive with more recent and advanced metrics when assessed at the system level.5 Therefore, we define our observation unit size to be M = 100 sentences (responses),6 unless stated otherwi"
P15-2073,C12-1121,0,0.0718467,"corpus-level or system-level correlations tend to be much higher than sentence-level correlations; Graham and Baldwin (2014) show that B LEU is competitive with more recent and advanced metrics when assessed at the system level.5 Therefore, we define our observation unit size to be M = 100 sentences (responses),6 unless stated otherwise. We evaluate qi by averaging human ratings on the M sentences, and mi by computing metric scores on the same set of sentences.7 We compare three different metrics: B LEU, ∆B LEU, and sentence-level B LEU (sB LEU). The last computes sentence-level B LEU scores (Nakov et al., 2012) and averages them on the M sentences (akin to macro-averaging). Finally, unless otherwise noted, all versions of B LEU use n-gram order up to 2 (B LEU-2), as this achieves better correlation for all metrics on this data. extracted from a completely unrelated conversation), and in some cases can outscore the original response, as can be seen in the third set of examples. 4.2 Human Evaluation of System Outputs Responses generated by the 7 systems used in this study on the 2114-triple test set were hand evaluated by 5 crowdsourced raters each on a 5-point Likert-type scale. From these 7 systems,"
P15-2073,P03-1021,0,0.0436342,"rced raters each on a 5-point Likert-type scale. From these 7 systems, 12 system pairs were evaluated, for a total of about pairwise 126K ratings (12 · 5 · 2114). Here too, raters were asked to evaluate responses in terms of their relevance to both context and message. Outputs from different systems were randomly interleaved for presentation to the raters. We obtained human ratings on the following systems: Phrase-based MT: A phrase-based MT system similar to (Ritter et al., 2011), whose weights have been manually tuned. We also included four variants of that system, which we tuned with MERT (Och, 2003). These variants differ in their number of features, and augment (Ritter et al., 2011) with the following phrase-level features: edit distance between source and target, cosine similarity, Jaccard index and distance, length ratio, and DSSM score (Huang et al., 2013). RNN-based MT: the log-probability according to the RNN model of (Sordoni et al., 2015). Baseline: a random baseline. While ∆B LEU relies on human qualitative judgments, it is important to note that human judgments on multi-references (§ 4.1) and those on system outputs were collected completely independently. We also note that the"
P15-2073,P02-1040,0,0.116946,"f outputs are acceptable or even desirable. Tasks with intrinsically diverse targets range from machine translation, summarization, sentence compression, paraphrase generation, and image-to-text to generation of conversational interactions. A major hurdle for these tasks is automation of evaluation, since the space of plausible outputs can be enormous, and it is it impractical to run a new human evaluation every time a new model is built or parameters are modified. In Statistical Machine Translation (SMT), the automation problem has to a large extent been ameliorated by metrics such as B LEU (Papineni et al., 2002) and M ETEOR (Banerjee and Lavie, 2005) Although B LEU is not immune from criticism (e.g., Callison-Burch et al. (2006)), its properties are well understood, B LEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research. † Corresponding author: mgalley@microsoft.com 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed. B LEU has been less successfully applied to nonSMT generation tasks owing to the larger space of plausible outputs. As a result"
P15-2073,D11-1054,0,0.592121,"f a corpus with which to compute tf ·idf. Both the above can be seen as attempting to capture a notion of target goodness that is not being captured in B LEU. In this paper, we introduce Discriminative B LEU (∆B LEU), a new metric that embeds human judgments concerning the quality of reference sentences directly into the computation of corpus-level multiple-reference B LEU. In effect, we push part of the burden of human evaluation into the automated metric, where it can be repeatedly utilized. Our testbed for this metric is data-driven conversation, a field that has begun to attract interest (Ritter et al., 2011; Sordoni et al., 2015) as an alternative to conventional rule-driven or scripted dialog systems. Intrinsic evaluation in this field is exceptionally challenging because the semantic space of possible responses resists definition and is only weakly constrained by conversational inputs. Below, we describe ∆B LEU and investigate its characteristics in comparison to standard B LEU in the context of conversational response generation. We demonstrate that ∆B LEU correlates well with human evaluation scores in this task and thus can 445 Proceedings of the 53rd Annual Meeting of the Association for C"
P15-2073,N15-1020,1,0.452044,"to compute tf ·idf. Both the above can be seen as attempting to capture a notion of target goodness that is not being captured in B LEU. In this paper, we introduce Discriminative B LEU (∆B LEU), a new metric that embeds human judgments concerning the quality of reference sentences directly into the computation of corpus-level multiple-reference B LEU. In effect, we push part of the burden of human evaluation into the automated metric, where it can be repeatedly utilized. Our testbed for this metric is data-driven conversation, a field that has begun to attract interest (Ritter et al., 2011; Sordoni et al., 2015) as an alternative to conventional rule-driven or scripted dialog systems. Intrinsic evaluation in this field is exceptionally challenging because the semantic space of possible responses resists definition and is only weakly constrained by conversational inputs. Below, we describe ∆B LEU and investigate its characteristics in comparison to standard B LEU in the context of conversational response generation. We demonstrate that ∆B LEU correlates well with human evaluation scores in this task and thus can 445 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistic"
P15-2073,P12-2008,0,0.0788347,"llison-Burch et al. (2006)), its properties are well understood, B LEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research. † Corresponding author: mgalley@microsoft.com 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed. B LEU has been less successfully applied to nonSMT generation tasks owing to the larger space of plausible outputs. As a result, attempts have been made to adapt the metric. To foster diversity in paraphrase generation, Sun and Zhou (2012) propose a metric called iB LEU in which the B LEU score is discounted by a B LEU score computed between the source and paraphrase. This solution, in addition to being dependent on a tunable parameter, is specific only to paraphrase. In image captioning tasks, Vendantam et al. (2015), employ a variant of B LEU in which n-grams are weighted by tf ·idf. This assumes the availability of a corpus with which to compute tf ·idf. Both the above can be seen as attempting to capture a notion of target goodness that is not being captured in B LEU. In this paper, we introduce Discriminative B LEU (∆B LEU"
P16-1094,P12-3007,0,0.0527842,"s the line of investigation initiated by Ritter et al. (2011) who treat generation of conversational dialog as a statistical machine translation (SMT) problem. Ritter et al. (2011) represents a break with previous and contemporaneous dialog work that relies extensively on hand-coded rules, typically either building statistical models on top of heuristic rules or templates (Levin et al., 2000; Young et al., 2010; Walker et al., 2003; Pieraccini et al., 2009; Wang et al., 2011) or learning generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014; Chen et al., 2013). More recently (Wen et al., 2015) have used a Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) to learn from unaligned data in order to reduce the heuristic space of sentence planning and surface realization. The SMT model proposed by Ritter et al., on the other hand, is end-to-end, purely data-driven, and contains no explicit model of dialog structure; the model learns to converse from human-to-human conversational corpora. Progress in SMT stemming from the use of neural language models (Sutskever et al., 2014; Gao et"
P16-1094,P15-2073,1,0.826361,"Missing"
P16-1094,P14-1066,1,0.260835,", 2012; Ameixa et al., 2014; Nio et al., 2014; Chen et al., 2013). More recently (Wen et al., 2015) have used a Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) to learn from unaligned data in order to reduce the heuristic space of sentence planning and surface realization. The SMT model proposed by Ritter et al., on the other hand, is end-to-end, purely data-driven, and contains no explicit model of dialog structure; the model learns to converse from human-to-human conversational corpora. Progress in SMT stemming from the use of neural language models (Sutskever et al., 2014; Gao et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) has inspired efforts to extend these neural techniques to SMT-based conversational response generation. Sordoni et al. (2015) augments Ritter et al. (2011) by rescoring outputs using a S EQ 2S EQ model conditioned on conversation history. Other researchers have recently used S EQ 2S EQ to directly generate responses in an end-to-end fashion without relying on SMT phrase tables (Serban et al., 2015; Shang et al., 2015; Vinyals and Le, 2015). Serban et al. (2015) propose a hierarchical neural model aimed at capturing dependencies over an extended conv"
P16-1094,N16-1014,1,0.261728,"? England, you? Where did you grow up? I grew up in Texas. How old are you? 16 and you? What’s your age? 18. What is your major? I’m majoring in psychology What did you study in college? English lit. Table 1: Inconsistent responses generated by a 4-layer S EQ 2S EQ model trained on 25 million Twitter conversation snippets. Introduction As conversational agents gain traction as user interfaces, there has been growing research interest in training naturalistic conversation systems from large volumes of human-to-human interactions (Ritter et al., 2011; Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016). One major issue for these data-driven systems is their propensity to select the response with greatest likelihood—in effect a consensus response of the humans represented in the training data. Outputs are frequently vague or non-committal (Li et al., 2016), and when not, they can be wildly inconsistent, as illustrated in Table 1. In this paper, we address the challenge of consistency and how to endow data-driven systems with the coherent “persona” needed to model humanlike behavior, whether as personal assistants, personalized avatar-like agents, or game characters.1 For present purposes, we"
P16-1094,P15-1002,0,0.023784,"2014; Chen et al., 2013). More recently (Wen et al., 2015) have used a Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) to learn from unaligned data in order to reduce the heuristic space of sentence planning and surface realization. The SMT model proposed by Ritter et al., on the other hand, is end-to-end, purely data-driven, and contains no explicit model of dialog structure; the model learns to converse from human-to-human conversational corpora. Progress in SMT stemming from the use of neural language models (Sutskever et al., 2014; Gao et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) has inspired efforts to extend these neural techniques to SMT-based conversational response generation. Sordoni et al. (2015) augments Ritter et al. (2011) by rescoring outputs using a S EQ 2S EQ model conditioned on conversation history. Other researchers have recently used S EQ 2S EQ to directly generate responses in an end-to-end fashion without relying on SMT phrase tables (Serban et al., 2015; Shang et al., 2015; Vinyals and Le, 2015). Serban et al. (2015) propose a hierarchical neural model aimed at capturing dependencies over an extended conversation history. Recent work by Li et al. ("
P16-1094,P03-1021,0,0.0474555,"rate generic and commonplace responses such as I don’t know, we follow Li et al. (2016) by reranking the generated N-best list using 997 a scoring function that linearly combines a length penalty and the log likelihood of the source given the target: log p(R|M, v) + λ log p(M |R) + γ|R| (11) where p(R|M, v) denotes the probability of the generated response given the message M and the respondent’s speaker ID. |R |denotes the length of the target and γ denotes the associated penalty weight. We optimize γ and λ on N-best lists of response candidates generated from the development set using MERT (Och, 2003) by optimizing B LEU. To compute p(M |R), we train an inverse S EQ 2S EQ model by swapping messages and responses. We trained standard S EQ 2S EQ models for p(M |R) with no speaker information considered. 5 Datasets 5.1 • Learning rate is set to 1.0. • Parameters are initialized by sampling from the uniform distribution [−0.1, 0.1]. • Gradients are clipped to avoid gradient explosion with a threshold of 5. • Vocabulary size is limited to 50,000. • Dropout rate is set to 0.2. Twitter Persona Dataset Data Collection Training data for the Speaker Model was extracted from the Twitter FireHose for"
P16-1094,W00-0306,0,0.39345,"nnotators. 2 Related Work This work follows the line of investigation initiated by Ritter et al. (2011) who treat generation of conversational dialog as a statistical machine translation (SMT) problem. Ritter et al. (2011) represents a break with previous and contemporaneous dialog work that relies extensively on hand-coded rules, typically either building statistical models on top of heuristic rules or templates (Levin et al., 2000; Young et al., 2010; Walker et al., 2003; Pieraccini et al., 2009; Wang et al., 2011) or learning generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014; Chen et al., 2013). More recently (Wen et al., 2015) have used a Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) to learn from unaligned data in order to reduce the heuristic space of sentence planning and surface realization. The SMT model proposed by Ritter et al., on the other hand, is end-to-end, purely data-driven, and contains no explicit model of dialog structure; the model learns to converse from human-to-human conversational corpora. Progress in SMT stemming from the use of neural languag"
P16-1094,P02-1040,0,0.111268,"Missing"
P16-1094,D11-1054,0,0.655954,"ou? Where were you born? I was born in Canada. Where are you from? England, you? Where did you grow up? I grew up in Texas. How old are you? 16 and you? What’s your age? 18. What is your major? I’m majoring in psychology What did you study in college? English lit. Table 1: Inconsistent responses generated by a 4-layer S EQ 2S EQ model trained on 25 million Twitter conversation snippets. Introduction As conversational agents gain traction as user interfaces, there has been growing research interest in training naturalistic conversation systems from large volumes of human-to-human interactions (Ritter et al., 2011; Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016). One major issue for these data-driven systems is their propensity to select the response with greatest likelihood—in effect a consensus response of the humans represented in the training data. Outputs are frequently vague or non-committal (Li et al., 2016), and when not, they can be wildly inconsistent, as illustrated in Table 1. In this paper, we address the challenge of consistency and how to endow data-driven systems with the coherent “persona” needed to model humanlike behavior, whether as personal assistants, personalized ava"
P16-1094,walker-etal-2012-annotated,0,0.0254272,"to reduce the proportion of generic responses typical of S EQ 2S EQ systems. Yao et al. (2015) employ an intention network to maintain the relevance of responses. Modeling of users and speakers has been extensively studied within the standard dialog modeling framework (e.g., (Wahlster and Kobsa, 1989; Kobsa, 1990; Schatztnann et al., 2005; Lin and Walker, 2011)). Since generating meaningful responses in an open-domain scenario is intrinsically difficult in conventional dialog systems, existing models often focus on generalizing character style on the basis of qualitative statistical analysis (Walker et al., 2012; Walker et al., 2011). The present work, by contrast, is in the vein of the S EQ 2S EQ models of Vinyals and Le (2015) and Li et al. (2016), enriching these models by training persona vectors directly from conversational data and relevant side-information, and incorporating these directly into the decoder. 995 3 Sequence-to-Sequence Models Given a sequence of inputs X = {x1 , x2 , ..., xnX }, an LSTM associates each time step with an input gate, a memory gate and an output gate, respectively denoted as it , ft and ot . We distinguish e and h where et denotes the vector for an individual text"
P16-1094,D15-1199,0,0.168657,"Missing"
P16-1094,P15-1152,0,0.72735,"Missing"
P16-1094,N15-1020,1,0.919869,"rn? I was born in Canada. Where are you from? England, you? Where did you grow up? I grew up in Texas. How old are you? 16 and you? What’s your age? 18. What is your major? I’m majoring in psychology What did you study in college? English lit. Table 1: Inconsistent responses generated by a 4-layer S EQ 2S EQ model trained on 25 million Twitter conversation snippets. Introduction As conversational agents gain traction as user interfaces, there has been growing research interest in training naturalistic conversation systems from large volumes of human-to-human interactions (Ritter et al., 2011; Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016). One major issue for these data-driven systems is their propensity to select the response with greatest likelihood—in effect a consensus response of the humans represented in the training data. Outputs are frequently vague or non-committal (Li et al., 2016), and when not, they can be wildly inconsistent, as illustrated in Table 1. In this paper, we address the challenge of consistency and how to endow data-driven systems with the coherent “persona” needed to model humanlike behavior, whether as personal assistants, personalized avatar-like agents, or ga"
P19-1539,W18-5709,0,0.13562,"round responses is distributed throughout the source document. Introduction While end-to-end neural conversation models (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017; Mostafazadeh et al., 2017). However, empirical results suggest that conditioning the decoder on rich and complex contexts, while helpful, does not on its own provide sufficient inductive bias for these systems to learn how to achieve deep and accurate integration between external knowledge and response generation. We posit that this ongoing challenge demands a more effective mechanism to support on-demand knowledge integration. We draw inspiration from how humans converse abo"
P19-1539,D18-1241,1,0.822774,"ent for a given question (Seo et al., 2017; Liu et al., 2018b; Yu et al., 2018). These models differ in how they fuse information between questions and documents. We chose SAN (Liu et al., 2018b) because of its representative architecture and competitive performance on existing MRC tasks. We note that other off-theshelf MRC models, such as BERT (Devlin et al., 2018), can also be plugged in. We leave the study of different MRC architectures for future work. Questions are treated as entirely independent in these “single-turn” MRC models, so recent work (e.g., CoQA (Reddy et al., 2019) and QuAC (Choi et al., 2018)) focuses on multi-turn MRC, modeling sequences of questions and answers in a conversation. While multi-turn MRC aims to answer complex questions, that body of work is restricted to factual questions, whereas our work—like much of the prior work in end-to-end dialogue—models free-form dialogue, which also encompasses chitchat and non-factual responses. 8 Conclusions We have demonstrated that the machine reading comprehension approach offers a promising step to generating, on the fly, contentful conversation exchanges that are grounded in extended text corpora. The functional combination of MRC"
P19-1539,D18-1045,0,0.0185929,"model training, with an initial learning rate of 0.0005. Batch size was set to 32. During training, all responses were truncated to have a maximum length of 30, and maximum query length and document length were set to 30, 500, respectively. we used regular teacher-forcing decoding during training. For inference, we found that top-k random sample decoding (Fan et al., 2018) provides the best results for all the systems. That is, at each decoding step, a token was drawn from the k most likely candidates according to the distribution over the vocabulary. Similar to recent work (Fan et al., 2018; Edunov et al., 2018), we set k = 20 (other common k values like 10 gave similar results). We selected key hyperparameter configurations on the validation set. 6.1 Evaluation Setup Table 2 shows automatic metrics for quantitative evaluation over three qualities of generated texts. We measure the overall relevance of the generated responses given the conversational history by using standard Machine Translation (MT) metrics, comparing generated outputs to ground-truth responses. These metrics include B LEU-4 (Papineni et al., 2002), M ETEOR (Lavie and Agarwal, 2007). and N IST (Doddington, 2002). The latter metric i"
P19-1539,P18-1082,0,0.0333813,"used the pretrained GloVe8 for initialization. We set hidden dimensions to 512 and dropout rate to 0.4. GRU cells are used for S EQ 2S EQ and M EM N ET (we also tested LSTM cells and obtained similar results). We used the Adam optimizer for model training, with an initial learning rate of 0.0005. Batch size was set to 32. During training, all responses were truncated to have a maximum length of 30, and maximum query length and document length were set to 30, 500, respectively. we used regular teacher-forcing decoding during training. For inference, we found that top-k random sample decoding (Fan et al., 2018) provides the best results for all the systems. That is, at each decoding step, a token was drawn from the k most likely candidates according to the distribution over the vocabulary. Similar to recent work (Fan et al., 2018; Edunov et al., 2018), we set k = 20 (other common k values like 10 gave similar results). We selected key hyperparameter configurations on the validation set. 6.1 Evaluation Setup Table 2 shows automatic metrics for quantitative evaluation over three qualities of generated texts. We measure the overall relevance of the generated responses given the conversational history b"
P19-1539,N19-1125,1,0.868164,"Missing"
P19-1539,W07-0734,0,0.0141127,"vocabulary. Similar to recent work (Fan et al., 2018; Edunov et al., 2018), we set k = 20 (other common k values like 10 gave similar results). We selected key hyperparameter configurations on the validation set. 6.1 Evaluation Setup Table 2 shows automatic metrics for quantitative evaluation over three qualities of generated texts. We measure the overall relevance of the generated responses given the conversational history by using standard Machine Translation (MT) metrics, comparing generated outputs to ground-truth responses. These metrics include B LEU-4 (Papineni et al., 2002), M ETEOR (Lavie and Agarwal, 2007). and N IST (Doddington, 2002). The latter metric is a variant of B LEU that weights n-gram matches by their information gain by effectively penalizing uninformative n-grams (such as “I don’t know”), which makes it a relevant metric for evaluating systems aiming diverse and informative responses. MT metrics may not be particularly adequate for our task (Liu et al., 2016), given its focus on the informativeness of responses, and for that reason we also use two other types of metrics to measure the level of grounding and diversity. As a diversity metric, we count all n-grams in the system output"
P19-1539,N16-1014,1,0.92711,"s recreated by the American television MythBusters. Four years later, […] two Praguebased journalists, claimed that Flight 367 had been mistaken for an enemy aircraft and shot down by the Czechoslovak Air Force at an altitude of 800 metres (2,600 ft). Figure 1: Users discussing a topic defined by a Wikipedia article. In this real-world example from our Reddit dataset, information needed to ground responses is distributed throughout the source document. Introduction While end-to-end neural conversation models (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017; Mostafazadeh et al., 2017). However, empirical results suggest that conditioning"
P19-1539,P16-1094,1,0.958274,"s recreated by the American television MythBusters. Four years later, […] two Praguebased journalists, claimed that Flight 367 had been mistaken for an enemy aircraft and shot down by the Czechoslovak Air Force at an altitude of 800 metres (2,600 ft). Figure 1: Users discussing a topic defined by a Wikipedia article. In this real-world example from our Reddit dataset, information needed to ground responses is distributed throughout the source document. Introduction While end-to-end neural conversation models (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017; Mostafazadeh et al., 2017). However, empirical results suggest that conditioning"
P19-1539,D16-1230,0,0.0886793,"Missing"
P19-1539,P18-1138,0,0.391323,"tributed throughout the source document. Introduction While end-to-end neural conversation models (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017; Mostafazadeh et al., 2017). However, empirical results suggest that conditioning the decoder on rich and complex contexts, while helpful, does not on its own provide sufficient inductive bias for these systems to learn how to achieve deep and accurate integration between external knowledge and response generation. We posit that this ongoing challenge demands a more effective mechanism to support on-demand knowledge integration. We draw inspiration from how humans converse about a topic, where"
P19-1539,P18-1157,1,0.938959,"tributed throughout the source document. Introduction While end-to-end neural conversation models (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017; Mostafazadeh et al., 2017). However, empirical results suggest that conditioning the decoder on rich and complex contexts, while helpful, does not on its own provide sufficient inductive bias for these systems to learn how to achieve deep and accurate integration between external knowledge and response generation. We posit that this ongoing challenge demands a more effective mechanism to support on-demand knowledge integration. We draw inspiration from how humans converse about a topic, where"
P19-1539,D15-1166,0,0.0725271,"n layer is applied to further ingest and capture the most salient information. The output memory, M ∈ Rd×n , is obtained by applying another BiLSTM layer for final information rearrangement. Note that d is the hidden size of the memory and n is the length of the document. 3.2 Response Generation Having read and processed both the conversation history and the extra knowledge in the document, the model then produces a free-form response y = (y1 , . . . , yT ) instead of generating a span or performing answer classification as in MRC tasks. We use an attentional recurrent neural network decoder (Luong et al., 2015) to generate response tokens while attending to the memory. At the beginning, the initial hidden state h0 is the weighted sum of the representation of the history X. For each decoding step t with a hidden state ht , we generate a token yt based on the distribution: p(yt ) = softmax((W1 ht + b)/τ ), (1) where τ > 0 is the softmax temperature. The hidden state ht is defined as follows: ht = W2 [zt ++fattention (zt , M )]. (2) Here, [· ++·] indicates a concatenation of two vectors; fattention is a dot-product attention (Vaswani et al., 2017); and zt is a state generated by GRU(et−1 , ht−1 ) with"
P19-1539,D18-1255,0,0.363735,"d-to-end neural conversation models (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017; Mostafazadeh et al., 2017). However, empirical results suggest that conditioning the decoder on rich and complex contexts, while helpful, does not on its own provide sufficient inductive bias for these systems to learn how to achieve deep and accurate integration between external knowledge and response generation. We posit that this ongoing challenge demands a more effective mechanism to support on-demand knowledge integration. We draw inspiration from how humans converse about a topic, where people often search and acquire external information as needed to"
P19-1539,I17-1047,1,0.933114,"Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017; Mostafazadeh et al., 2017). However, empirical results suggest that conditioning the decoder on rich and complex contexts, while helpful, does not on its own provide sufficient inductive bias for these systems to learn how to achieve deep and accurate integration between external knowledge and response generation. We posit that this ongoing challenge demands a more effective mechanism to support on-demand knowledge integration. We draw inspiration from how humans converse about a topic, where people often search and acquire external information as needed to continue a meaningful and informative conversation. Figure 1 i"
P19-1539,P02-1040,0,0.106889,"e decoding to draw yt from the above distribution p(yt ). Section 5 provides more details about the experimental configuration. 3.3 Data Weighting Scheme We further propose a simple data weighting scheme to encourage the generation of grounded responses. The idea is to bias the model training to fit better to those training instances where the ground-truth response is more closely relevant to the document. More specifically, given a training instance (X, D, y), we measure the closeness score c ∈ R between the document D and the gold response y (e.g., with the NIST (Doddington, 2002) or B LEU (Papineni et al., 2002) metrics). In each training data batch, we normalize the closeness scores of all the instances to have a sum of 1, and weight each of the instances with its corresponding normalized score when evaluating the 5429 # dialogues # utterances # documents # document sentences Train Valid Test 28.4k 2.36M 28.4k 15.18M 1.2k 0.12M 1.2k 0.58M 3.1k 0.34M 3.1k 1.68M 18.84 14.17 18.48 14.15 Average length (# words): utterances 18.74 document sentences 13.72 Table 1: Our grounded conversational dataset. training loss. This training regime promotes instances with grounded responses and thus encourages the mo"
P19-1539,D16-1264,0,0.230968,"ble at https://github.com/qkaren/ converse_reading_cmr. of turns X = (x1 , . . . , xM ) and a web document D = (s1 , . . . , sN ) as the knowledge source, where si is the ith sentence in the document. With the pair (X, D), the system needs to generate a natural language response y that is both conversationally appropriate and reflective of the contents of the web document. 3 Approach Our approach integrates conversation generation with on-demand MRC. Specifically, we use an MRC model to effectively encode the conversation history by treating it as a question in a typical QA task (e.g., SQuAD (Rajpurkar et al., 2016)), and encode the web document as the context. We then replace the output component of the MRC model (which is usually an answer classification module) with an attentional sequence generator that generates a free-form response. We refer to our approach as CMR (Conversation with on-demand Machine Reading). In general, any off-the-shelf MRC model could be applied here for knowledge comprehension. We use Stochastic Answer Networks (SAN)2 (Liu et al., 2018b), a performant machine reading model that until very recently held state-of-the-art performance on the SQuAD benchmark. We also employ a simpl"
P19-1539,Q19-1016,0,0.0224858,"ng meaning. from a given document for a given question (Seo et al., 2017; Liu et al., 2018b; Yu et al., 2018). These models differ in how they fuse information between questions and documents. We chose SAN (Liu et al., 2018b) because of its representative architecture and competitive performance on existing MRC tasks. We note that other off-theshelf MRC models, such as BERT (Devlin et al., 2018), can also be plugged in. We leave the study of different MRC architectures for future work. Questions are treated as entirely independent in these “single-turn” MRC models, so recent work (e.g., CoQA (Reddy et al., 2019) and QuAC (Choi et al., 2018)) focuses on multi-turn MRC, modeling sequences of questions and answers in a conversation. While multi-turn MRC aims to answer complex questions, that body of work is restricted to factual questions, whereas our work—like much of the prior work in end-to-end dialogue—models free-form dialogue, which also encompasses chitchat and non-factual responses. 8 Conclusions We have demonstrated that the machine reading comprehension approach offers a promising step to generating, on the fly, contentful conversation exchanges that are grounded in extended text corpora. The"
P19-1539,P15-1152,0,0.0338416,"fall without a parachute: 10,160 metres (33,330 ft). …… …… In 2005, Vulović‘s fall was recreated by the American television MythBusters. Four years later, […] two Praguebased journalists, claimed that Flight 367 had been mistaken for an enemy aircraft and shot down by the Czechoslovak Air Force at an altitude of 800 metres (2,600 ft). Figure 1: Users discussing a topic defined by a Wikipedia article. In this real-world example from our Reddit dataset, information needed to ground responses is distributed throughout the source document. Introduction While end-to-end neural conversation models (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017;"
P19-1539,P18-1205,0,0.0465991,"compared to machine translation, it is common for the generator to retain focus on the key information in the external document to produce semantically relevant responses. 7 Related Work Dialogue: Traditional dialogue systems (see (Jurafsky and Martin, 2009) for an historical perspective) are typically grounded, enabling these systems to be reflective of the user’s environment. The lack of grounding has been a stumbling block for the earliest end-to-end dialogue systems, as various researchers have noted that their outputs tend to be bland (Li et al., 2016a; Gao et al., 2019b), inconsistent (Zhang et al., 2018a; Li et al., Figure 3: Attention weights between words of the documents and words of the response. Dark (blue) cells represent probabilities closer to 1. 2016b; Zhang et al., 2019), and lacking in factual content (Ghazvininejad et al., 2018; Agarwal et al., 2018). Recently there has been growing interest in exploring different forms of grounding, including images, knowledge bases, and plain texts (Das et al., 2017; Mostafazadeh et al., 2017; Agarwal et al., 2018; Yang et al., 2019). A recent survey is included in Gao et al. (2019a). Prior work, e.g, (Ghazvininejad et al., 2018; Zhang et al.,"
P19-1539,N15-1020,1,0.77954,"hute: 10,160 metres (33,330 ft). …… …… In 2005, Vulović‘s fall was recreated by the American television MythBusters. Four years later, […] two Praguebased journalists, claimed that Flight 367 had been mistaken for an enemy aircraft and shot down by the Czechoslovak Air Force at an altitude of 800 metres (2,600 ft). Figure 1: Users discussing a topic defined by a Wikipedia article. In this real-world example from our Reddit dataset, information needed to ground responses is distributed throughout the source document. Introduction While end-to-end neural conversation models (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017; Mostafazadeh et al.,"
P19-1539,N19-1423,0,\N,Missing
P19-3021,P19-1441,1,0.838229,"nt. Figure from (Gao et al., 2019b). Figure 1: An example of a basic multi-task configuration. Two encoder-decoder chains share a common decoder, alternately trained on separate datasets. coders. This architecture consists of additional personality embeddings, which are provided to the decoder alongside token embeddings at each timestep of decoding. Grounding generated responses helps condition outputs based on a given personality embedding: for the same query, the system learns to generate responses in different styles, all while preserving the underlying context. tations (Gao et al., 2019b; Liu et al., 2019). By unifying a conversational sequence-to-sequence model and an autoencoder with a shared decoder, multi-task learning can personalize the conversational model (Luan et al., 2017). Multi-task learning has potentially many other powerful applications for inducing biases in conversational systems. I CECAPS allows users to build arrays of models with arbitrary sharing of components, and place them in a multi-task learning environment. Users can construct arbitrary multi-task training schedules, assigning different tasks or balances among tasks per training step. 3 3.2 SpaceFusion (Gao et al., 20"
P19-3021,P18-1157,1,0.844079,"general censor-list and a start-token censor-list. The general censor-list contains a list of tokens to disable during response generation; probabilities associated with these tokens are clamped to zero. The start-token censor-list is similar, but only masks the response’s first token. We also support infrequency filters; users may restrict the decoder from generating responses with rare words. provide informed responses with context about the real world, without needing comprehensive paired conversational data to embody that information. We provide an extension of stochastic answer networks (Liu et al., 2018), a machine reading comprehension system, that acts as a full knowledgegrounded conversation model (Qin et al., 2019), hybridizing machine reading comprehension with a response generation model. At a high level, this model consists of two deep biLSTMs in parallel that encode conversational context and knowledge, respectively. The information from these encoders is then combined using cross-attention, the output of which forms the basis of a memory cell that powers a response generator. 4 4.3 The standard beam-search implementation in TensorFlow works by iteratively generating tokens, generatin"
P19-3021,P18-4021,0,0.146101,"onversational systems have seen great improvements over the past several years, with current models able to generate surprisingly coherent dialogs (Gao et al., 2019a). Business applications, games, and potentially other settings can benefit from intelligent conversational agents, inviting users to interact intuitively with complex systems. Although a range of open-source tools is available to train neural network models for natural language processing (Vaswani et al., 2018; Gardner et al., 2018; Klein et al., 2017), only a few emphasize multi-turn conversational settings (Miller et al., 2017; Burtsev et al., 2018). Conversations present distinct challenges. They generally consist of many turns, and agents need to contextualize responses in these multi-turn contexts. Agents may also need to contextualize their responses in other cues, such as style, intent, and external knowledge, while retaining a conversational flow. 2.1 Component chaining Sequence-to-sequence models can be abstracted as chains of sequence encoders and sequence decoders. Our library implements various encoders and decoders, which can be chained together to form a single, end-to-end functional model. This chaining paradigm allows users"
P19-3021,I17-1061,1,0.82046,"atasets. coders. This architecture consists of additional personality embeddings, which are provided to the decoder alongside token embeddings at each timestep of decoding. Grounding generated responses helps condition outputs based on a given personality embedding: for the same query, the system learns to generate responses in different styles, all while preserving the underlying context. tations (Gao et al., 2019b; Liu et al., 2019). By unifying a conversational sequence-to-sequence model and an autoencoder with a shared decoder, multi-task learning can personalize the conversational model (Luan et al., 2017). Multi-task learning has potentially many other powerful applications for inducing biases in conversational systems. I CECAPS allows users to build arrays of models with arbitrary sharing of components, and place them in a multi-task learning environment. Users can construct arbitrary multi-task training schedules, assigning different tasks or balances among tasks per training step. 3 3.2 SpaceFusion (Gao et al., 2019b) is a learning paradigm that aligns latent spaces learned by different models trained over different datasets. Of particular interest is its application to neural conversation"
P19-3021,D15-1166,0,0.0461808,"me context to be placed nearby in latent space and aligning semantically related responses along straight lines in latent space. This induces a structure in the latent space such that distance and direction from a predicted response vector roughly correspond to relevance and diversity, respectively, as in Figure 2. Built-in modules and configurations I CECAPS provides several built-in modules and configurations. Most standard NLP architectures are available, including transformers (Vaswani et al., 2017), LSTM-based seq2seq models (Sutskever et al., 2014) with attention (Bahdanau et al., 2015; Luong et al., 2015), n-gram convolutional language models, and deep convolutional networks for baseline image grounding. Where applicable, these are implemented as chains of simpler components as per our design philosophy. We also provide features that target conversational scenarios, from individual chainable components to custom multi-task learning presets. 3.1 SpaceFusion Personality grounding 3.3 Inspired by recent work on modeling personality differences in conversational systems (Li et al., 2016b), I CECAPS provides implementations of personality-grounded seq2seq and transformer deKnowledge grounding A cri"
P19-3021,D17-2014,0,0.0618856,"Introduction Neural conversational systems have seen great improvements over the past several years, with current models able to generate surprisingly coherent dialogs (Gao et al., 2019a). Business applications, games, and potentially other settings can benefit from intelligent conversational agents, inviting users to interact intuitively with complex systems. Although a range of open-source tools is available to train neural network models for natural language processing (Vaswani et al., 2018; Gardner et al., 2018; Klein et al., 2017), only a few emphasize multi-turn conversational settings (Miller et al., 2017; Burtsev et al., 2018). Conversations present distinct challenges. They generally consist of many turns, and agents need to contextualize responses in these multi-turn contexts. Agents may also need to contextualize their responses in other cues, such as style, intent, and external knowledge, while retaining a conversational flow. 2.1 Component chaining Sequence-to-sequence models can be abstracted as chains of sequence encoders and sequence decoders. Our library implements various encoders and decoders, which can be chained together to form a single, end-to-end functional model. This chainin"
P19-3021,N18-1202,0,0.0294649,"ons with their agents and directly observe their responses. Response generation is powered by the custom decoder described in Section 4. While the commandline session is useful for quick testing, for conveTraining configurations I CECAPS training configurations follow a basic five-phase pattern. We include example training scripts that users may use as templates. 126 learning models. It places a strong emphasis on sequence modeling baselines. AllenNLP (Gardner et al., 2018) is a PyTorch library developed by AI2 for natural language processing tasks, notable for an open-source release of ELMo (Peters et al., 2018). OpenNMT (Klein et al., 2017) is a popular neural machine translation toolkit originally developed for LuaTorch that now has implementations in PyTorch and TensorFlow. MarianNMT (Junczys-Dowmunt et al., 2018) is another framework for neural machine translation developed between the Adam Mickiewicz University in Pozna and the University of Edinburgh. It is built in C++ and designed for fast training in multi-GPU systems. Texar (Hu et al., 2018) is a text generation toolkit affiliated with Carnegie Mellon University, featuring a similar emphasis on modularity to I CE CAPS . It includes reinforc"
P19-3021,N19-1125,1,0.84795,"sed directly or loaded for fine-tuning or bootstrapping other models; these models power an online demo of our framework. 1 2 Architecture I CECAPS is designed for modularity, flexibility, and ease of use. Modules are built on top of TensorFlow Estimators, making them easy for developers to use and extend flexibly. I CECAPS supports arbitrary architectures of modules chained together within versatile multi-task configurations. Introduction Neural conversational systems have seen great improvements over the past several years, with current models able to generate surprisingly coherent dialogs (Gao et al., 2019a). Business applications, games, and potentially other settings can benefit from intelligent conversational agents, inviting users to interact intuitively with complex systems. Although a range of open-source tools is available to train neural network models for natural language processing (Vaswani et al., 2018; Gardner et al., 2018; Klein et al., 2017), only a few emphasize multi-turn conversational settings (Miller et al., 2017; Burtsev et al., 2018). Conversations present distinct challenges. They generally consist of many turns, and agents need to contextualize responses in these multi-tu"
P19-3021,P19-1539,1,0.727317,"g response generation; probabilities associated with these tokens are clamped to zero. The start-token censor-list is similar, but only masks the response’s first token. We also support infrequency filters; users may restrict the decoder from generating responses with rare words. provide informed responses with context about the real world, without needing comprehensive paired conversational data to embody that information. We provide an extension of stochastic answer networks (Liu et al., 2018), a machine reading comprehension system, that acts as a full knowledgegrounded conversation model (Qin et al., 2019), hybridizing machine reading comprehension with a response generation model. At a high level, this model consists of two deep biLSTMs in parallel that encode conversational context and knowledge, respectively. The information from these encoders is then combined using cross-attention, the output of which forms the basis of a memory cell that powers a response generator. 4 4.3 The standard beam-search implementation in TensorFlow works by iteratively generating tokens, generating a constant number of hypotheses at the end of the decoding phase. I CECAPS implements a modified beam search decode"
P19-3021,W18-2501,0,0.128523,"rts arbitrary architectures of modules chained together within versatile multi-task configurations. Introduction Neural conversational systems have seen great improvements over the past several years, with current models able to generate surprisingly coherent dialogs (Gao et al., 2019a). Business applications, games, and potentially other settings can benefit from intelligent conversational agents, inviting users to interact intuitively with complex systems. Although a range of open-source tools is available to train neural network models for natural language processing (Vaswani et al., 2018; Gardner et al., 2018; Klein et al., 2017), only a few emphasize multi-turn conversational settings (Miller et al., 2017; Burtsev et al., 2018). Conversations present distinct challenges. They generally consist of many turns, and agents need to contextualize responses in these multi-turn contexts. Agents may also need to contextualize their responses in other cues, such as style, intent, and external knowledge, while retaining a conversational flow. 2.1 Component chaining Sequence-to-sequence models can be abstracted as chains of sequence encoders and sequence decoders. Our library implements various encoders and"
P19-3021,P16-1162,0,0.198835,"ke building complex dialogue systems intuitive for the end user. 5.1 Text data processing TensorFlow estimators expect to read data from TFRecord binary files for efficient processing. We provide a script TEXT DATA PROCESSING . PY for converting text data into TFRecords, equipped with several useful preprocessing transformations. Our script can sort data within local windows so that batches fed during training have minimal padding inefficiency. These batches can be shuffled amongst each other to mitigate any biases induced by sorting. We provide token preprocessing through byte pair encoding (Sennrich et al., 2016), which builds a token set at a level of abstraction between characters and words. This often allows for faster training and improved generalization. Another feature focused on conversational scenarios is fixed-length context extraction. Conversational data often contains large, potentially unwieldy multi-turn contexts; we can limit our data samples to a desired context length. We also provide an option for annotating datasets with topic grounding information, by analyzing the data for unique tokens to use as topic markers. 5.2 Training the system. The system is now ready to train: a single ca"
P19-3021,W18-2503,0,0.111568,"NLP (Gardner et al., 2018) is a PyTorch library developed by AI2 for natural language processing tasks, notable for an open-source release of ELMo (Peters et al., 2018). OpenNMT (Klein et al., 2017) is a popular neural machine translation toolkit originally developed for LuaTorch that now has implementations in PyTorch and TensorFlow. MarianNMT (Junczys-Dowmunt et al., 2018) is another framework for neural machine translation developed between the Adam Mickiewicz University in Pozna and the University of Edinburgh. It is built in C++ and designed for fast training in multi-GPU systems. Texar (Hu et al., 2018) is a text generation toolkit affiliated with Carnegie Mellon University, featuring a similar emphasis on modularity to I CE CAPS . It includes reinforcement learning capabilities alongside its sequence modelling tools. A few other toolkits have a dialog emphasis. DeepPavlov (Burtsev et al., 2018) is a deep learning library with a focus on task-oriented dialogue. It provides demos and pre-trained models for tasks such as question answering and sentiment classification. Affiliated with DeepPavlov is the ConvAI2 challenge (Dinan et al., 2019), a general dialogue competition featuring a synthetic"
P19-3021,P18-4020,0,0.0368469,"Missing"
P19-3021,W18-1819,0,0.0298992,"exibly. I CECAPS supports arbitrary architectures of modules chained together within versatile multi-task configurations. Introduction Neural conversational systems have seen great improvements over the past several years, with current models able to generate surprisingly coherent dialogs (Gao et al., 2019a). Business applications, games, and potentially other settings can benefit from intelligent conversational agents, inviting users to interact intuitively with complex systems. Although a range of open-source tools is available to train neural network models for natural language processing (Vaswani et al., 2018; Gardner et al., 2018; Klein et al., 2017), only a few emphasize multi-turn conversational settings (Miller et al., 2017; Burtsev et al., 2018). Conversations present distinct challenges. They generally consist of many turns, and agents need to contextualize responses in these multi-turn contexts. Agents may also need to contextualize their responses in other cues, such as style, intent, and external knowledge, while retaining a conversational flow. 2.1 Component chaining Sequence-to-sequence models can be abstracted as chains of sequence encoders and sequence decoders. Our library implements"
P19-3021,P17-4012,0,0.176011,"tures of modules chained together within versatile multi-task configurations. Introduction Neural conversational systems have seen great improvements over the past several years, with current models able to generate surprisingly coherent dialogs (Gao et al., 2019a). Business applications, games, and potentially other settings can benefit from intelligent conversational agents, inviting users to interact intuitively with complex systems. Although a range of open-source tools is available to train neural network models for natural language processing (Vaswani et al., 2018; Gardner et al., 2018; Klein et al., 2017), only a few emphasize multi-turn conversational settings (Miller et al., 2017; Burtsev et al., 2018). Conversations present distinct challenges. They generally consist of many turns, and agents need to contextualize responses in these multi-turn contexts. Agents may also need to contextualize their responses in other cues, such as style, intent, and external knowledge, while retaining a conversational flow. 2.1 Component chaining Sequence-to-sequence models can be abstracted as chains of sequence encoders and sequence decoders. Our library implements various encoders and decoders, which can b"
P19-3021,N16-1014,1,0.717728,"ani et al., 2017), LSTM-based seq2seq models (Sutskever et al., 2014) with attention (Bahdanau et al., 2015; Luong et al., 2015), n-gram convolutional language models, and deep convolutional networks for baseline image grounding. Where applicable, these are implemented as chains of simpler components as per our design philosophy. We also provide features that target conversational scenarios, from individual chainable components to custom multi-task learning presets. 3.1 SpaceFusion Personality grounding 3.3 Inspired by recent work on modeling personality differences in conversational systems (Li et al., 2016b), I CECAPS provides implementations of personality-grounded seq2seq and transformer deKnowledge grounding A critical task in building intelligent conversational agents is grounding their responses in an external knowledge base. This allows agents to 124 agent to avoid profanities or other offensive language. Likewise, the system should avoid obvious ungrammatical outputs, such as broken abbreviations or nonsensical punctuation marks. I CECAPS supports several filters, including a general censor-list and a start-token censor-list. The general censor-list contains a list of tokens to disable d"
P19-3021,P16-1094,1,0.920476,"ani et al., 2017), LSTM-based seq2seq models (Sutskever et al., 2014) with attention (Bahdanau et al., 2015; Luong et al., 2015), n-gram convolutional language models, and deep convolutional networks for baseline image grounding. Where applicable, these are implemented as chains of simpler components as per our design philosophy. We also provide features that target conversational scenarios, from individual chainable components to custom multi-task learning presets. 3.1 SpaceFusion Personality grounding 3.3 Inspired by recent work on modeling personality differences in conversational systems (Li et al., 2016b), I CECAPS provides implementations of personality-grounded seq2seq and transformer deKnowledge grounding A critical task in building intelligent conversational agents is grounding their responses in an external knowledge base. This allows agents to 124 agent to avoid profanities or other offensive language. Likewise, the system should avoid obvious ungrammatical outputs, such as broken abbreviations or nonsensical punctuation marks. I CECAPS supports several filters, including a general censor-list and a start-token censor-list. The general censor-list contains a list of tokens to disable d"
W02-1604,W01-0808,1,0.849298,"t string. The generation module is language-specific and used for both monolingual generation and MT. In the context of MT, generation takes as input the transferred LF and converts it into a basic syntactic tree. A small set of heuristic rules preprocesses the transferred LF to “nativize” some structural differences, such as pro-drop phenomena in Japanese. A series of core generation rules then applies to the LF tree, transforming it into a Japanese sentence string. Generation rules operate on a single tree only, are application-independent and are developed in a monolingual environment (see Aikawa et al. 2001a, 2001b for further details.) Generation of inflectional morphology is also handled in this component. The generation component has no explicit knowledge of the source language. 2 Acquisition of Complex Structural Mappings The generalization provided by LF makes it possible for MSR-MT to handle complex structural relations in cases where English and Japanese are systematically divergent. This is 2 MSR-MT resorts to lexical lookup only when a term is not found in the Mindnet. The handcrafted dictionary is slated for replacement by purely statistically generated data. Training Data Translation"
W02-1604,J90-2002,0,0.153813,"transfer patterns between languages, and generate output strings. Abstraction permits structural neutralizations that facilitate learning of translation examples across languages with radically different surface structure characteristics, and allows MT development to proceed within a largely languageindependent NLP architecture. Comparative evaluation indicates that after training in a domain the English-Japanese system is statistically indistinguishable from a non-customized commercially available MT system in the same domain. Introduction In the wake of the pioneering work of Nagao (1984), Brown et al. (1990) and Sato and Nagao (1990), Machine Translation (MT) research has increasingly focused on the issue of how to acquire translation knowledge from aligned parallel texts. While much of this research effort has focused on acquisition of correspondences between individual lexical items or between unstructured strings of words, closer attention has begun to be paid to the learning of structured phrasal units: Yamamoto and Matsumoto (2000), for example, describe a method for automatically extracting correspondences between dependency relations in Japanese and English. Similarly, Imamura (2001a, 2001"
W02-1604,C00-1057,1,0.870678,"Missing"
W02-1604,W01-1406,1,0.832484,"ociations,” that is, lexical mappings extracted from the training corpora using statistical techniques based on mutual information (Moore 2001). From these possible lexical correspondences, the algorithm uses a small grammar of (language-pair-independent) rules to align LF nodes on lexical and structural principles. The aligned LF pairs are then partitioned into smaller aligned LF segments, with individual node mappings captured in a relationship we call “sublinking.” Finally, the aligned LF segments are filtered on the basis of frequency, and compiled into a database known as a Mindnet. (See Menezes & Richardson 2001 for a detailed description of this process.) The Mindnet is a general-purpose database of semantic information (Richardson et al. 1998) that has been repurposed as the primary repository of translation information for MT applications. The process of building the Mindnet is entirely automated; there is no human vetting of candidate entries. At the end of a typical training session, 1,816,520 transfer In the Mindnet, LF segments from the source language are represented as linked to the corresponding LF segment from the target languages. These can be seen in Figs. 3 and 4, discussed below in Sec"
W02-1604,W01-1411,0,0.0129651,"deploys the general-purpose parsers to analyze the English and Japanese sentence pairs and generate LFs for each sentence. In the next step, an LF alignment algorithm is used to match source language and target language LFs at the sub-sentence level. The LF alignment algorithm first establishes tentative lexical correspondences between nodes in the source and target LFs on the basis of lexical matching over dictionary information and approximately 31,000 “word associations,” that is, lexical mappings extracted from the training corpora using statistical techniques based on mutual information (Moore 2001). From these possible lexical correspondences, the algorithm uses a small grammar of (language-pair-independent) rules to align LF nodes on lexical and structural principles. The aligned LF pairs are then partitioned into smaller aligned LF segments, with individual node mappings captured in a relationship we call “sublinking.” Finally, the aligned LF segments are filtered on the basis of frequency, and compiled into a database known as a Mindnet. (See Menezes & Richardson 2001 for a detailed description of this process.) The Mindnet is a general-purpose database of semantic information (Richa"
W02-1604,2001.mtsummit-papers.50,0,0.0341849,"Missing"
W02-1604,2002.tmi-papers.16,0,0.0221111,"Missing"
W02-1604,W01-1402,1,0.792933,"00). These parsers are robust in that if the analysis grammar fails to find an appropriate parse, it outputs a best-guess “fitted” parse. System development is not confined to English-Japanese: MSR-MT is part of a broader natural language processing project involving three Asian languages (Japanese, Chinese, and Korean) and four European languages (English, French, German, and Spanish). Development of the MSR-MT systems proceeds more or less simultaneously across these languages and in multiple directions, including Japanese-English. The Spanish-English version of MSR-MT has been described in Richardson et al. 2001a, Richardson et al 2001b, and the reader is referred to these papers for more information concerning algorithms employed during phrase alignment. A description of the French-Spanish MT system is found in Pinkham & Smets. 2002. 1.1 Training Data MSR-MT requires that a large corpus of aligned sentences be available as examples for training. For English-Japanese MT, the system currently trains on a corpus of approximately 596,000 pre-aligned sentence pairs. About 274,000 of these are sentence pairs extracted from Microsoft technical documentation that had been professionally translated from Engl"
W02-1604,2001.mtsummit-papers.53,1,0.770798,"00). These parsers are robust in that if the analysis grammar fails to find an appropriate parse, it outputs a best-guess “fitted” parse. System development is not confined to English-Japanese: MSR-MT is part of a broader natural language processing project involving three Asian languages (Japanese, Chinese, and Korean) and four European languages (English, French, German, and Spanish). Development of the MSR-MT systems proceeds more or less simultaneously across these languages and in multiple directions, including Japanese-English. The Spanish-English version of MSR-MT has been described in Richardson et al. 2001a, Richardson et al 2001b, and the reader is referred to these papers for more information concerning algorithms employed during phrase alignment. A description of the French-Spanish MT system is found in Pinkham & Smets. 2002. 1.1 Training Data MSR-MT requires that a large corpus of aligned sentences be available as examples for training. For English-Japanese MT, the system currently trains on a corpus of approximately 596,000 pre-aligned sentence pairs. About 274,000 of these are sentence pairs extracted from Microsoft technical documentation that had been professionally translated from Engl"
W02-1604,P98-2180,0,0.0252536,"2001). From these possible lexical correspondences, the algorithm uses a small grammar of (language-pair-independent) rules to align LF nodes on lexical and structural principles. The aligned LF pairs are then partitioned into smaller aligned LF segments, with individual node mappings captured in a relationship we call “sublinking.” Finally, the aligned LF segments are filtered on the basis of frequency, and compiled into a database known as a Mindnet. (See Menezes & Richardson 2001 for a detailed description of this process.) The Mindnet is a general-purpose database of semantic information (Richardson et al. 1998) that has been repurposed as the primary repository of translation information for MT applications. The process of building the Mindnet is entirely automated; there is no human vetting of candidate entries. At the end of a typical training session, 1,816,520 transfer In the Mindnet, LF segments from the source language are represented as linked to the corresponding LF segment from the target languages. These can be seen in Figs. 3 and 4, discussed below in Section 2. 1.4 Transfer and Generation At translation time, the broad-coverage source language parser processes the English input sentence,"
W02-1604,C90-3044,0,0.0695244,"en languages, and generate output strings. Abstraction permits structural neutralizations that facilitate learning of translation examples across languages with radically different surface structure characteristics, and allows MT development to proceed within a largely languageindependent NLP architecture. Comparative evaluation indicates that after training in a domain the English-Japanese system is statistically indistinguishable from a non-customized commercially available MT system in the same domain. Introduction In the wake of the pioneering work of Nagao (1984), Brown et al. (1990) and Sato and Nagao (1990), Machine Translation (MT) research has increasingly focused on the issue of how to acquire translation knowledge from aligned parallel texts. While much of this research effort has focused on acquisition of correspondences between individual lexical items or between unstructured strings of words, closer attention has begun to be paid to the learning of structured phrasal units: Yamamoto and Matsumoto (2000), for example, describe a method for automatically extracting correspondences between dependency relations in Japanese and English. Similarly, Imamura (2001a, 2001b) seeks to match correspo"
W02-1604,C00-2119,1,0.871246,"Missing"
W02-1604,C00-2135,0,0.0709887,"Missing"
W02-1604,2001.mtsummit-ebmt.4,1,\N,Missing
W02-1604,C98-2175,0,\N,Missing
W04-3219,P01-1008,0,0.257977,"of the resulting corpus. A monotone phrasal decoder generates contextual replacements. Human evaluation shows that this system outperforms baseline paraphrase generation techniques and, in a departure from previous work, offers better coverage and scalability than the current best-of-breed paraphrasing approaches. 1 Introduction The ability to categorize distinct word sequences as “meaning the same thing” is vital to applications as diverse as search, summarization, dialog, and question answering. Recent research has treated paraphrase acquisition and generation as a machine learning problem (Barzilay & McKeown, 2001; Lin & Pantel, 2002; Shinyama et al, 2002, Barzilay & Lee, 2003, Pang et al., 2003). We approach this problem as one of statistical machine translation (SMT), within the noisy channel model of Brown et al. (1993). That is, we seek to identify the optimal paraphrase T* of a sentence S by finding: T * = arg max{P(T |S )} T = arg max{P( S |T ) P(T )} T T and S being sentences in the same language. We describe and evaluate an SMT-based paraphrase generation system that utilizes a monotone phrasal decoder to generate meaning-preserving paraphrases across multiple domains. By adopting at the outset"
W04-3219,N03-1003,0,0.650357,"lternate WordNet baseline using a target language model. 8 In combination with the language model described in section 3.4, we used a very simple replacement model: each appropriately inflected member of the most frequent synset was proposed as a possible replacement with uniform probability. This was intended to isolate the contribution of the language model from the replacement model. Given that our alignments, while aggregated into phrases, are fundamentally word-aligned, one question that arises is whether the information we learn is different in character than that learned 8 In contrast, Barzilay and Lee (2003) avoided using a language model for essentially the same reason: their MSA approach did not take advantage of such a resource. Evaluation We have experimented with several methods for extracting a parallel sentence-aligned corpus from news clusters using word alignment error rate, or AER, (Och & Ney 2003) as an evaluation metric. A brief summary of these experiments is provided in Table 1. To evaluate the quality of generation, we followed the lead of Barzilay & Lee (2003). We started with the 59 sentences and corresponding paraphrases from MSA and WordNet (designated as WN below). Since the s"
W04-3219,C04-1051,1,0.664678,"edit distance to identify likely paraphrases has the unfortunate result of excluding interesting sentence pairs that are similar in meaning though different in form. For example: The Cassini spacecraft, which is en route to Saturn, is about to make a close pass of the ringed planet's mysterious moon Phoebe On its way to an extended mission at Saturn, the Cassini probe on Friday makes its closest rendezvous with Saturn's dark moon Phoebe. We are currently experimenting with data extracted from the first two sentences in each article, which by journalistic convention tend to summarize content (Dolan et al. 2004). While noisier than the edit distance data, initial results suggest that these can be a rich source of information about larger phrasal substitutions and syntactic reordering. Although we have not attempted to address the issue of paraphrase identification here, we are currently exploring machine learning techniques, based in part on features of document structure and other linguistic features that should allow us to bootstrap initial alignments to develop more data. This will we hope, eventually allow us to address such issues as paraphrase identification for IR. To exploit richer data sets,"
W04-3219,W03-1608,0,0.32129,"cient monolingual parallel data.1 We show that a huge corpus of comparable and alignable sentence pairs can be culled from ready-made topical/temporal clusters of news articles gathered on a daily basis from thousands of sources on the World Wide Web, thereby permitting the system to operate outside the narrow domains typical of existing systems. 2 Related work Until recently, efforts in paraphrase were not strongly focused on generation and relied primarily on narrow data sources. One data source has been multiple translations of classic literary works (Barzilay & McKeown 2001; Ibrahim 2002; Ibrahim et al. 2003). Pang et al. (2003) obtain parallel monolingual texts from a set of 100 multiply-translated news articles. While translation-based approaches to obtaining data do address the problem of how to identify two strings as meaning the same thing, they are limited in scalability owing to the difficulty (and expense) of obtaining large quantities of multiply-translated source documents. Other researchers have sought to identify patterns in large unannotated monolingual corpora. Lin & Pantel (2002) derive inference rules by parsing text fragments and extracting semantically similar paths. Shinyama et"
W04-3219,N03-1017,0,0.0189416,"Missing"
W04-3219,W03-0301,0,0.0106332,"Missing"
W04-3219,P00-1056,0,0.167624,"Missing"
W04-3219,J03-1002,0,0.00962553,"isolate the contribution of the language model from the replacement model. Given that our alignments, while aggregated into phrases, are fundamentally word-aligned, one question that arises is whether the information we learn is different in character than that learned 8 In contrast, Barzilay and Lee (2003) avoided using a language model for essentially the same reason: their MSA approach did not take advantage of such a resource. Evaluation We have experimented with several methods for extracting a parallel sentence-aligned corpus from news clusters using word alignment error rate, or AER, (Och & Ney 2003) as an evaluation metric. A brief summary of these experiments is provided in Table 1. To evaluate the quality of generation, we followed the lead of Barzilay & Lee (2003). We started with the 59 sentences and corresponding paraphrases from MSA and WordNet (designated as WN below). Since the size of this data set made it difficult to obtain statistically significant results, we also included 141 randomly selected sentences from held-out clusters. We then produced paraphrases with each of the following systems and compared them with MSA and WN: • • • WN+LM: WordNet with a trigram LM CL: Statist"
W04-3219,N03-1024,0,0.186586,"an evaluation shows that this system outperforms baseline paraphrase generation techniques and, in a departure from previous work, offers better coverage and scalability than the current best-of-breed paraphrasing approaches. 1 Introduction The ability to categorize distinct word sequences as “meaning the same thing” is vital to applications as diverse as search, summarization, dialog, and question answering. Recent research has treated paraphrase acquisition and generation as a machine learning problem (Barzilay & McKeown, 2001; Lin & Pantel, 2002; Shinyama et al, 2002, Barzilay & Lee, 2003, Pang et al., 2003). We approach this problem as one of statistical machine translation (SMT), within the noisy channel model of Brown et al. (1993). That is, we seek to identify the optimal paraphrase T* of a sentence S by finding: T * = arg max{P(T |S )} T = arg max{P( S |T ) P(T )} T T and S being sentences in the same language. We describe and evaluate an SMT-based paraphrase generation system that utilizes a monotone phrasal decoder to generate meaning-preserving paraphrases across multiple domains. By adopting at the outset a paradigm geared toward generating sentences, this approach overcomes many problem"
W04-3219,W01-1401,0,0.0210743,"Missing"
W04-3219,P97-1037,0,0.0043073,"hrasal replacement probability with MLE (Vogel et al. 2003). Secondly, it appears to boost translation quality in more sophisticated translation systems by inducing lexical triggering (Och et al. 2004). Collocations and other non-compositional phrases receive a higher probability as a whole than they would as independent single word replacements. One further simplification was made. Given that our domain is restricted to the generation of monolingual paraphrase, interesting output can be produced without tackling the difficult problem of inter-phrase reordering.7 Therefore, along the lines of Tillmann et al. (1997), we rely on only monotone phrasal alignments, although we do allow intra-phrasal reordering. While this means certain common structural alternations (e.g., active/passive) cannot be generated, we are still able to express a broad range of phenomena: pings to be both unwieldy in practice and very often indicative of poor a word alignment. 7 Even in the realm of MT, such an assumption can produce competitive results (Vogel et al. 2003). In addition, we were hesitant to incur the exponential increase in running time associated with those movement models in the tradition of Brown el al (1993), es"
W04-3219,P03-1020,0,0.011373,"t case O(kn), where n is the maximal target length and k is the maximal number of replacements for any word). In addition, fast algorithms exist for computing the n-best lists over a lattice (Soong & Huang 1991). Figure 2. A simplified generation lattice: 44 top ranked edges from a total 4,140 Finally the resultant paraphrases were cleaned up in a post-processing phase to ensure output was not trivially distinguishable from other systems during human evaluation. All generic named entity tokens were re-instantiated with their source values, and case was restored using a model like that used in Vita et al. (2003). from much simpler techniques. To explore this hypothesis, we introduced an additional baseline that used statistical clustering to produce an automated, unsupervised synonym list, again with a trigram language model. We used standard bigram clustering techniques (Goodman 2002) to produce 4,096 clusters of our 65,225 vocabulary items. 3.5 4 Alternate approaches Barzilay & Lee (2003) have released a common dataset that provides a basis for comparing different paraphrase generation systems. It consists of 59 sentences regarding acts of violence in the Middle East. These are accompanied by parap"
W04-3219,C96-2141,0,0.0903917,"ces were identical or differed only in punctuation; Duplicate sentence pairs; Sentence pairs with significantly different lengths (the shorter is less than two-thirds the length of the longer); Sentence pairs where the Levenshtein distance was greater than 12.0.3 A total of 139K non-identical sentence pairs were obtained. Mean Levenshtein distance was 5.17; mean sentence length was 18.6 words. 3.2 Word alignment To this corpus we applied the word alignment algorithms available in Giza++ (Och & Ney, 2000), a freely available implementation of IBM Models 1-5 (Brown, 1993) and the HMM alignment (Vogel et al, 1996), along with various improvements and modifications motivated by experimentation by Och & Ney (2000). In order to capture the many-to-many alignments that identify correspondences between idioms and other phrasal chunks, we align in the forward direction and again in the backward direction, heuristically recombining each unidirectional word alignment into a single bidirectional alignment (Och & Ney 2000). Figure 1 shows an example of a monolingual alignment produced by Giza++. Each line represents a uni-directional link; directionality is indicated by a tick mark on the target side of the link"
W04-3219,2003.mtsummit-papers.53,0,0.0440398,"y running five iterations of Model 1 on the training corpus. This allows for computing the probability of a sequence of source words S given a sequence of target words T as the sum over all possible alignments of the Model 1 probabilities: P (S |T ) = ∑ P (S , A |T ) A = ∏ ∑ P (s |t ) t∈T s∈S (Brown et al. (1993) provides a more detailed derivation of this identity.) Although simple, this approach has proven effective in SMT for several reasons. First and foremost, phrasal scoring by Model 1 avoids the sparsity problems associated with estimating each phrasal replacement probability with MLE (Vogel et al. 2003). Secondly, it appears to boost translation quality in more sophisticated translation systems by inducing lexical triggering (Och et al. 2004). Collocations and other non-compositional phrases receive a higher probability as a whole than they would as independent single word replacements. One further simplification was made. Given that our domain is restricted to the generation of monolingual paraphrase, interesting output can be produced without tackling the difficult problem of inter-phrase reordering.7 Therefore, along the lines of Tillmann et al. (1997), we rely on only monotone phrasal al"
W09-2111,C94-1002,0,0.0194874,"m or examination conditions, leaving unresolved the more practical question as to whether the ESL Assistant can actually help a perRelated Work Language learner error correction techniques typically fall into either of two categories: rule-based or data-driven. Eeg-Olofsson and Knutsson (2003) report on a rule-based system that detects and corrects preposition errors in non-native Swedish text. Rule-based approaches have also been used to predict definiteness and indefiniteness of Japanese noun phrases as a preprocessing step for Japanese to English machine translation (Murata and Nagao 1993; Bond et al, 1994; Heine, 1998), a task that is similar to the prediction of English articles. More recently, data-driven approaches have gained popularity and been applied to article prediction in English (Knight and Chander 1994; Minnen et al, 2000; Turner and Charniak 2007), to an array of Japanese learners’ errors in English (Izumi et al, 2003), to verb errors (Lee and Seneff, 2008), and to article and preposition correction in texts written by non-native ELLs (Han et al, 2004, 2006; Nagata et al, 2005; Nagata et al, 2006; De Felice and Pulman, 2007; Chodorow et al, 2007; Gamon et al, 2008, 2009; Tetreault"
W09-2111,W07-1604,0,0.094797,"Missing"
W09-2111,W07-1607,0,0.122066,"Missing"
W09-2111,I08-1059,1,0.849467,"interactions, like article and preposition errors. Rule-based approaches handle those error types that are amenable to simpler solutions. For example, a regular expression is sufficient for identifying when a modal is (incorrectly) followed by a tensed verb. The output of all modules, both machine-learned and rule-based, is filtered through a very large language model. Only when the language model finds that the likelihood of the suggested rewrite is suffi74 ciently larger than the original text is a suggestion shown to the user. For a detailed description of ESL Assistant’s architecture, see Gamon et al (2008, 2009). Although this and the systems cited in section 2 are designed to be used by non-native writers, system performance is typically reported in relation to native text – the prediction of a preposition, for example, will ideally be consistent with usage in native, edited text. An error is counted each time the system predicts a token that differs from the observed usage and a correct prediction is counted each time the system predicts the usage that occurs in the text. Although somewhat artificial, this approach to evaluation offers the advantages of being fully automatable and having abu"
W09-2111,han-etal-2004-detecting,1,0.931498,"Missing"
W09-2111,P98-1085,0,0.0371609,"onditions, leaving unresolved the more practical question as to whether the ESL Assistant can actually help a perRelated Work Language learner error correction techniques typically fall into either of two categories: rule-based or data-driven. Eeg-Olofsson and Knutsson (2003) report on a rule-based system that detects and corrects preposition errors in non-native Swedish text. Rule-based approaches have also been used to predict definiteness and indefiniteness of Japanese noun phrases as a preprocessing step for Japanese to English machine translation (Murata and Nagao 1993; Bond et al, 1994; Heine, 1998), a task that is similar to the prediction of English articles. More recently, data-driven approaches have gained popularity and been applied to article prediction in English (Knight and Chander 1994; Minnen et al, 2000; Turner and Charniak 2007), to an array of Japanese learners’ errors in English (Izumi et al, 2003), to verb errors (Lee and Seneff, 2008), and to article and preposition correction in texts written by non-native ELLs (Han et al, 2004, 2006; Nagata et al, 2005; Nagata et al, 2006; De Felice and Pulman, 2007; Chodorow et al, 2007; Gamon et al, 2008, 2009; Tetreault and Chodorow,"
W09-2111,P03-2026,0,0.0762794,"Missing"
W09-2111,N04-2006,0,0.320835,"Missing"
W09-2111,P08-1021,0,0.0272888,"Missing"
W09-2111,W00-0708,0,0.143988,"es: rule-based or data-driven. Eeg-Olofsson and Knutsson (2003) report on a rule-based system that detects and corrects preposition errors in non-native Swedish text. Rule-based approaches have also been used to predict definiteness and indefiniteness of Japanese noun phrases as a preprocessing step for Japanese to English machine translation (Murata and Nagao 1993; Bond et al, 1994; Heine, 1998), a task that is similar to the prediction of English articles. More recently, data-driven approaches have gained popularity and been applied to article prediction in English (Knight and Chander 1994; Minnen et al, 2000; Turner and Charniak 2007), to an array of Japanese learners’ errors in English (Izumi et al, 2003), to verb errors (Lee and Seneff, 2008), and to article and preposition correction in texts written by non-native ELLs (Han et al, 2004, 2006; Nagata et al, 2005; Nagata et al, 2006; De Felice and Pulman, 2007; Chodorow et al, 2007; Gamon et al, 2008, 2009; Tetreault and Chodorow, 2008a). 1 http://www.eslassistant.com 73 Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 73–81, c Boulder, Colorado, June 2009. 2009 Association for Computati"
W09-2111,1993.tmi-1.18,0,0.0102476,"bly artificial classroom or examination conditions, leaving unresolved the more practical question as to whether the ESL Assistant can actually help a perRelated Work Language learner error correction techniques typically fall into either of two categories: rule-based or data-driven. Eeg-Olofsson and Knutsson (2003) report on a rule-based system that detects and corrects preposition errors in non-native Swedish text. Rule-based approaches have also been used to predict definiteness and indefiniteness of Japanese noun phrases as a preprocessing step for Japanese to English machine translation (Murata and Nagao 1993; Bond et al, 1994; Heine, 1998), a task that is similar to the prediction of English articles. More recently, data-driven approaches have gained popularity and been applied to article prediction in English (Knight and Chander 1994; Minnen et al, 2000; Turner and Charniak 2007), to an array of Japanese learners’ errors in English (Izumi et al, 2003), to verb errors (Lee and Seneff, 2008), and to article and preposition correction in texts written by non-native ELLs (Han et al, 2004, 2006; Nagata et al, 2005; Nagata et al, 2006; De Felice and Pulman, 2007; Chodorow et al, 2007; Gamon et al, 200"
W09-2111,P06-1031,0,0.0327211,"Missing"
W09-2111,C08-1109,0,0.0606258,"Missing"
W09-2111,W08-1205,0,0.0720649,"Missing"
W09-2111,N07-2045,0,0.192999,"ta-driven. Eeg-Olofsson and Knutsson (2003) report on a rule-based system that detects and corrects preposition errors in non-native Swedish text. Rule-based approaches have also been used to predict definiteness and indefiniteness of Japanese noun phrases as a preprocessing step for Japanese to English machine translation (Murata and Nagao 1993; Bond et al, 1994; Heine, 1998), a task that is similar to the prediction of English articles. More recently, data-driven approaches have gained popularity and been applied to article prediction in English (Knight and Chander 1994; Minnen et al, 2000; Turner and Charniak 2007), to an array of Japanese learners’ errors in English (Izumi et al, 2003), to verb errors (Lee and Seneff, 2008), and to article and preposition correction in texts written by non-native ELLs (Han et al, 2004, 2006; Nagata et al, 2005; Nagata et al, 2006; De Felice and Pulman, 2007; Chodorow et al, 2007; Gamon et al, 2008, 2009; Tetreault and Chodorow, 2008a). 1 http://www.eslassistant.com 73 Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 73–81, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics Noun Relat"
W09-2111,I05-1071,0,\N,Missing
W09-2111,C98-1082,0,\N,Missing
W19-2401,P18-5002,1,0.877282,"Missing"
W19-2401,D14-1002,1,0.771046,"at local connections between any two neighboring sentences can be overlooked. One can easily distinguish a generated sentence from a real one by judging whether it is semantically cohesive with its neighboring sentences. We strive to embody these two different yet important concepts by developing coherence and cohesion discriminators, operating on the sentence level and word level, respectively. Our design of these two discriminators is inspired by the Deep Structured Semantic Model (DSSM) which was originally developed to measure the semantic similarity between two texts (Huang et al., 2013; Gao et al., 2014; Palangi et al., 2016; Xu et al., 2017). In this study, we extend ‘semantic similarity’ to coherence and cohesion in a long-form text. 3.1 Figure 1: Illustration of coherence and cohesion discriminators. Dcoherence takes in bag-of-words sentence embeddings as inputs, and Dcohesion takes in the raw word embeddings of consecutive sentences as inputs. The source encoder f (or u) is different from the target encoder g (or v). (CNN)1 or RNN2 , denoted as f , takes as input the BOW vectors of the source text chunk S and encodes it into a single vector f (S). Similarly, g encodes the target text chu"
W19-2401,J08-1001,0,0.0220925,"ohesion. Coherence and cohesion have been extensively studied in the computational linguistics community, particularly in the ‘pre-deep-learning’ era. Lack of formal specifications for coherence and cohesion (Mani et al., 1998), resulted in many different formalisms, such as Rhetorical Structure Theory (Mann and Thompson, 1988), and other forms of coherence and cohesion relations and their quantification (Mani et al., 1998; Hobbs, 1985; Hovy, 1988; McKeown, 1985; Cohen and Levesque, 1985; Hovy, 1991; Cristea et al., 1998; Halliday and Hasan, 1996; Liddy, 1991; Van Dijk, 2013; Edmundson, 1969; Barzilay and Lapata, 2008). This list is not exhaustive. However, prior work jointly exploring coherence and cohesion using neural models in the context of long-form text generation has not come to our attention. Reinforcement learning for text generation. The text generation task can be framed as a reinforcement learning (RL) problem (Daum´e et al., 2009), in which the generator G is acting as a policy π, with parameters θπ , and each generated word at time t, wt , can be viewed as an action to be chosen by the policy from a large discrete space, or vocabulary, conditioned on state st−1 = w≤t−1 . Let rt be the reward"
W19-2401,P85-1007,0,0.24846,"nerations. Furthermore, we model cohesion between consecutive sentence pairs using word-level features. Related work Coherence and cohesion. Coherence and cohesion have been extensively studied in the computational linguistics community, particularly in the ‘pre-deep-learning’ era. Lack of formal specifications for coherence and cohesion (Mani et al., 1998), resulted in many different formalisms, such as Rhetorical Structure Theory (Mann and Thompson, 1988), and other forms of coherence and cohesion relations and their quantification (Mani et al., 1998; Hobbs, 1985; Hovy, 1988; McKeown, 1985; Cohen and Levesque, 1985; Hovy, 1991; Cristea et al., 1998; Halliday and Hasan, 1996; Liddy, 1991; Van Dijk, 2013; Edmundson, 1969; Barzilay and Lapata, 2008). This list is not exhaustive. However, prior work jointly exploring coherence and cohesion using neural models in the context of long-form text generation has not come to our attention. Reinforcement learning for text generation. The text generation task can be framed as a reinforcement learning (RL) problem (Daum´e et al., 2009), in which the generator G is acting as a policy π, with parameters θπ , and each generated word at time t, wt , can be viewed as an a"
W19-2401,P18-1152,0,0.020626,"anguage model that generates more coherent and cohesive long-form texts, and empirically validate its effectiveness using the TripAdvisor and Yelp English reviews datasets. 2 et al. (2015) and Paulus et al. (2017). These works directly optimize for specific metrics, such as BLEU (Papineni et al., 2002) or ROUGE (Lin and Hovy, 2003), using REINFORCE (Williams, 1992). However, these metrics do not give a complete picture of the text generation quality. Only recently have there been efforts to provide more relevant objectives, such as consistency and repetition in a text (Li et al., 2015, 2016a; Holtzman et al., 2018). But these works use the objectives to re-rank candidate outputs, not to reward or penalize them. Li et al. (2016b) constructed a set of reward models for the dialogue task, such as information flow and semantic coherence, to tune the generator, yet they do not provide an ablation study on the relative contribution of these reward models individually. It is not clear that these reward models can be generalized to other tasks, in particular, long-form text generation tasks. The most relevant to our work is Bosselut et al. (2018), which promotes text generation in the correct order, and discour"
W19-2401,P98-1044,0,0.0287386,"ion between consecutive sentence pairs using word-level features. Related work Coherence and cohesion. Coherence and cohesion have been extensively studied in the computational linguistics community, particularly in the ‘pre-deep-learning’ era. Lack of formal specifications for coherence and cohesion (Mani et al., 1998), resulted in many different formalisms, such as Rhetorical Structure Theory (Mann and Thompson, 1988), and other forms of coherence and cohesion relations and their quantification (Mani et al., 1998; Hobbs, 1985; Hovy, 1988; McKeown, 1985; Cohen and Levesque, 1985; Hovy, 1991; Cristea et al., 1998; Halliday and Hasan, 1996; Liddy, 1991; Van Dijk, 2013; Edmundson, 1969; Barzilay and Lapata, 2008). This list is not exhaustive. However, prior work jointly exploring coherence and cohesion using neural models in the context of long-form text generation has not come to our attention. Reinforcement learning for text generation. The text generation task can be framed as a reinforcement learning (RL) problem (Daum´e et al., 2009), in which the generator G is acting as a policy π, with parameters θπ , and each generated word at time t, wt , can be viewed as an action to be chosen by the policy f"
W19-2401,P88-1020,0,0.329802,"the relative quality of generations. Furthermore, we model cohesion between consecutive sentence pairs using word-level features. Related work Coherence and cohesion. Coherence and cohesion have been extensively studied in the computational linguistics community, particularly in the ‘pre-deep-learning’ era. Lack of formal specifications for coherence and cohesion (Mani et al., 1998), resulted in many different formalisms, such as Rhetorical Structure Theory (Mann and Thompson, 1988), and other forms of coherence and cohesion relations and their quantification (Mani et al., 1998; Hobbs, 1985; Hovy, 1988; McKeown, 1985; Cohen and Levesque, 1985; Hovy, 1991; Cristea et al., 1998; Halliday and Hasan, 1996; Liddy, 1991; Van Dijk, 2013; Edmundson, 1969; Barzilay and Lapata, 2008). This list is not exhaustive. However, prior work jointly exploring coherence and cohesion using neural models in the context of long-form text generation has not come to our attention. Reinforcement learning for text generation. The text generation task can be framed as a reinforcement learning (RL) problem (Daum´e et al., 2009), in which the generator G is acting as a policy π, with parameters θπ , and each generated w"
W19-2401,D17-1153,0,0.0304854,"Missing"
W19-2401,D14-1181,0,0.00294367,"l encoder. Given source text chunk S and target text chunk T , the coherence discriminator Dcoherence computes the coherence score in three steps, as illustrated in Figure 1 (upper). First, each sentence is encoded by the bag-of-words (BOW) embedding, i.e., the average of its word vectors from a pre-trained word embedding (Pennington et al., 2014). Secondly, an encoder which can be implemented using a convolutional neural network 1 We explored with deeper networks. However, the performance difference was marginal. For simplicity, we decided to use a 1-layer convolutional network architecture (Kim, 2014; Collobert et al., 2011). 2 For clarity in our model description, we omit RNN hereafter. We present results using both CNN and RNN encoders in Table 2. 3 ing data, negative (incoherent) pairs need to be artificially constructed. The next section describes the way these negative pairs are generated. form acohesive pairof consecutive sentences. Let sk := s1k , s2k , ..., snk be the k th sentence that con- sists of n words, sk+1 := s1k+1 , s2k+1 , ..., sm k+1 be the real next sentence of m  1 that2 consistsm e words, and sek+1 := sek+1 , sek+1 , ..., sek+1 be the artificially constructed i"
W19-2401,D17-1259,0,0.0347691,"Missing"
W19-2401,P02-1040,0,0.10418,"hat uses negative samples to estimate its reward baseline and therefore eliminates the need for a sepa1 Proceedings of the First Workshop on Narrative Understanding, pages 1–11 c Minneapolis, Minnesota, June 7, 2019. 2019 Association for Computational Linguistics rate critic function; and (4) we develop a new neural language model that generates more coherent and cohesive long-form texts, and empirically validate its effectiveness using the TripAdvisor and Yelp English reviews datasets. 2 et al. (2015) and Paulus et al. (2017). These works directly optimize for specific metrics, such as BLEU (Papineni et al., 2002) or ROUGE (Lin and Hovy, 2003), using REINFORCE (Williams, 1992). However, these metrics do not give a complete picture of the text generation quality. Only recently have there been efforts to provide more relevant objectives, such as consistency and repetition in a text (Li et al., 2015, 2016a; Holtzman et al., 2018). But these works use the objectives to re-rank candidate outputs, not to reward or penalize them. Li et al. (2016b) constructed a set of reward models for the dialogue task, such as information flow and semantic coherence, to tune the generator, yet they do not provide an ablatio"
W19-2401,P16-1094,1,0.872387,"Missing"
W19-2401,D14-1162,0,0.0821075,"that consists of m e sentences. Dcoherence is designed to distinguish a positive (coherent) pair (S, T ) from a negative (incoherent) pair (S, Te) by assigning different scores, i.e., Dcoherence (S, T ) &gt; Dcoherence (S, Te). Model architecture. The model takes a form of dual encoder. Given source text chunk S and target text chunk T , the coherence discriminator Dcoherence computes the coherence score in three steps, as illustrated in Figure 1 (upper). First, each sentence is encoded by the bag-of-words (BOW) embedding, i.e., the average of its word vectors from a pre-trained word embedding (Pennington et al., 2014). Secondly, an encoder which can be implemented using a convolutional neural network 1 We explored with deeper networks. However, the performance difference was marginal. For simplicity, we decided to use a 1-layer convolutional network architecture (Kim, 2014; Collobert et al., 2011). 2 For clarity in our model description, we omit RNN hereafter. We present results using both CNN and RNN encoders in Table 2. 3 ing data, negative (incoherent) pairs need to be artificially constructed. The next section describes the way these negative pairs are generated. form acohesive pairof consecutive sen"
W19-2401,D16-1127,1,0.823404,"the TripAdvisor and Yelp English reviews datasets. 2 et al. (2015) and Paulus et al. (2017). These works directly optimize for specific metrics, such as BLEU (Papineni et al., 2002) or ROUGE (Lin and Hovy, 2003), using REINFORCE (Williams, 1992). However, these metrics do not give a complete picture of the text generation quality. Only recently have there been efforts to provide more relevant objectives, such as consistency and repetition in a text (Li et al., 2015, 2016a; Holtzman et al., 2018). But these works use the objectives to re-rank candidate outputs, not to reward or penalize them. Li et al. (2016b) constructed a set of reward models for the dialogue task, such as information flow and semantic coherence, to tune the generator, yet they do not provide an ablation study on the relative contribution of these reward models individually. It is not clear that these reward models can be generalized to other tasks, in particular, long-form text generation tasks. The most relevant to our work is Bosselut et al. (2018), which promotes text generation in the correct order, and discourages in its reverse order using rewards. However, this may not be sufficient in capturing coherence since there ar"
W19-2401,D17-1230,0,0.0520846,"Missing"
W19-2401,P15-1152,0,0.0590307,"Missing"
W19-2401,N03-1020,0,0.21645,"Missing"
W19-2401,N15-1020,1,0.830371,"al approaches to natural language generation rely on a large amount of human-generated text to train language models (Cho et al., 2014; Graves, 2013; Sutskever et al., 2014). Although these models can generate sentences that, if judged individually, are similar to human-generated ones, they often fail to capture the local and global dependencies among sentences, resulting in a text that is neither coherent nor cohesive. For example, neural language models based on Recurrent Neural Networks (RNNs) are widely applied to response generation for dialogue (Vinyals and Le, 2015; Shang et al., 2015; Sordoni et al., 2015; Li et al., 2015). Although the responses by themselves look reasonable, they are detached from the whole dialogue session. See Gao et al. (2018) for a comprehensive survey. In this paper, we address the challenge in a principled manner, employing a pair of discriminators to score whether and to what extent a text is coherent or cohesive. The coherence discriminator measures the compatibility among all sentences in a paragraph. The cohesion discriminator measures the compatibility of each pair of consecutive sentences. These models, given a conditional input text and multiple candidate output"
W19-2401,1983.tc-1.13,0,0.528181,"Missing"
W19-2401,C98-1044,0,\N,Missing
W19-2401,N18-1016,1,\N,Missing
W19-2401,N16-1014,1,\N,Missing
