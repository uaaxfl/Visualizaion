2014.iwslt-papers.1,W12-2301,0,0.0369333,"Missing"
2014.iwslt-papers.1,elmahdy-etal-2014-development,0,0.0830499,"Missing"
2014.iwslt-papers.1,J14-1006,0,0.0123809,"n excess of 15 million per day (private communication). To build a dialectal tweet corpus a multi-step procedure was used: 1) Arabic tweets were extracted by issuing the query lang:ar against the Twitter API3 . 2) Each tweet was classified as dialectal or not dialectal. 3) Dialectal tweets were mapped, if possible, to a country. If such a mapping was possible, the tweet was classified as being written in the dialect associated with that country according to Figure 2. In more detail: To perform step 2, dialectal words were extracted from the Arabic Online Commentary Dataset (AOCD) described in [20]. Examples of words used in di «  E$An, J ë hyk,  @ Ay$,ñ» @ Ako, ñJ  alects: ø X dy, àA  $nw, @ð wA$ etc. As shown in [14], many of these dialectal words are used in more than one dialect. I.e. these words do not map a tweet uniquely to a dialect. For example the word Figure 3: Dialectal Tweets Distribution Percentages. 5. Speech Recognition This section describes the details of the speech recognition system, esp. the acoustic model training and the language models used in the experiments. 5.1. Language Modeling Following [7] we wanted to test the impact of using tweets when buildin"
2014.iwslt-papers.1,N12-1006,0,0.144049,"Missing"
2014.iwslt-papers.1,W14-3601,1,0.833102,"tweets were extracted by issuing the query lang:ar against the Twitter API3 . 2) Each tweet was classified as dialectal or not dialectal. 3) Dialectal tweets were mapped, if possible, to a country. If such a mapping was possible, the tweet was classified as being written in the dialect associated with that country according to Figure 2. In more detail: To perform step 2, dialectal words were extracted from the Arabic Online Commentary Dataset (AOCD) described in [20]. Examples of words used in di «  E$An, J ë hyk,  @ Ay$,ñ» @ Ako, ñJ  alects: ø X dy, àA  $nw, @ð wA$ etc. As shown in [14], many of these dialectal words are used in more than one dialect. I.e. these words do not map a tweet uniquely to a dialect. For example the word Figure 3: Dialectal Tweets Distribution Percentages. 5. Speech Recognition This section describes the details of the speech recognition system, esp. the acoustic model training and the language models used in the experiments. 5.1. Language Modeling Following [7] we wanted to test the impact of using tweets when building the language model for the speech recognition system. This leads to a number of questions: Is it better to use all dialectal tweets"
2020.bucc-1.3,N16-3003,1,0.824794,"ng Statistical Machine Translation (SMT) pipeline. For evaluation purposes, Jehl et al. (2012) use crowdsourcing to create a parallel corpus of 1000 Arabic tweets and 3 manual English translations for each Arabic tweet and reports improvement for SMT pipeline. Abidi and Smaili (2017) used topics related to Syria to crawl Twitter and collect 58,000 Arabic tweets and 60,000 English tweets. The tweets are then preprocessed heavily, which requires knowledge of Arabic. Then, the tweets are aligned to produce a corpus of comparable Arabic-English tweets aimed at improving MT systems. Vicente et al. (2016) present a parallel corpus that covers 5 languages from the Iberian Peninsula, created by automatic collection and crowdsourcing. To align parallel content, Vicente et al. (2016) use measures such as publication date, string length similarity, hashtag and user mention overlap, and Longest Common Subsequence ratio (LCSR). LCSR exploits the similarity of the languages within the Iberian peninsula. The aim of the corpus is to aid in the development of microtext translation systems. Vicente et al. (2016) 3. Methodology and Corpus Construction Before diving further into the methodology, it’s import"
2020.bucc-1.3,W19-4622,1,0.839243,"t easily adaptable for less-resourced languages. Ling et al. (2013) collect parallel content of different languages from single tweets (compare Table 1 and Table 2 for difference). They reported a significant improvement in MT systems. In this work, we will not focus on extracting parallel content from single tweets. However, our methods can be adapted to do so in the future. Our work also augments existing work in Twitter account annotation. Specifically for Arabic Twitter users, there is a scarcity of resources. Inspired by Mubarak and Darwish (2014), who annotate tweets for their dialects, Bouamor et al. (2019) presented a dataset of 3000 Twitter accounts annotated with their countries of origin. Alhozaimi and Almishari (2018) categorize 80 Twitter accounts into 4 categories of topics the accounts are interested in. It suffices to say that there is a need for such resources and our annotation of Twitter accounts for country and topic, although not our primary goal, is a step forward. Related Work Although the amount of data on social media is growing at an incredible speed and can be a valuable resource for NLP tasks, the utilization of data on social media has been underwhelming. Efforts to use the"
2020.bucc-1.3,L16-1170,1,0.828424,"template for posting tweets and are likely to be bots. Table 4 shows an example of such accounts. These accounts are not very useful for the purpose of creating a corpus for machine translation. To identify these accounts, we plot number of words in all the tweets posted by the account against the number of unique words among them. If the ratio of unique words versus total words is below a threshold, we exclude the account. To increase the quality of the collected Arabic-English tweets, we can use complex Arabic word segmenter to split prefixes and suffixes, for example Farasa word segmenter (Darwish and Mubarak, 2016; Abdelali et al., 2016), or lemmatizer (Mubarak, 2018), and for English we can use Porter stemmer (Porter, 1980). We leave this for future work. 3.3. Arabic-English Parallel Tweets Corpus Using the method described in Section 3.2., we collect a corpus of 166K Arabic-English parallel tweets and 1,389 accounts who regularly post them. For our collection of Arabic-English parallel tweets, first, we collect 175M Arabic tweets in March 2014 using Twitter API with language Then, we calculate ratio of unique words and total number of words in tweets posted by each account. If this ratio falls below"
2020.bucc-1.3,W12-3153,0,0.00897186,"is a step forward. Related Work Although the amount of data on social media is growing at an incredible speed and can be a valuable resource for NLP tasks, the utilization of data on social media has been underwhelming. Efforts to use these platforms as a resource for translation are still relatively small. Sluyter Gäthje et al. (2018) built a parallel resource for English-German using 4000 English tweets that were manually translated into German with a special focus on the informal nature of the tweets. The objective was to provide a resource tailored for translating user generated-content. Jehl et al. (2012) and Abidi and Smaili (2017) extract parallel phrases by using CLIR techniques. The major difference is that these methods are extracting comparable data, whereas, we want to extract parallel tweets, which we can expect to be closer to true translation. Jehl et al. use a probabilistic translation-based retrieval (Xu et al., 2001) in the context of Twitter for the purpose of training Statistical Machine Translation (SMT) pipeline. For evaluation purposes, Jehl et al. (2012) use crowdsourcing to create a parallel corpus of 1000 Arabic tweets and 3 manual English translations for each Arabic twee"
2020.bucc-1.3,P13-1018,0,0.0360142,"out to as large of an audience as possible. Often the audience consists of individuals who use different languages. To build a connection with this diverse audience, organizations, celebrities, and public figures post tweets in multiple languages to ensure max reach out. Twitter, with traditionally 140 (Now, 280) character limit on the tweets, prompts the users to reach out to their audiences across multiple tweets containing the same message in different languages. In our paper, we propose a method to collect such tweets. These parallel tweets can be a great resource for machine translation. Ling et al., (2013) show that parallel texts from Twitter can significantly improve MT systems. As opposed to crowdsourcing translations that cost money or complex mechanisms of cross-language information retrieval, we provide a free and generic method of obtaining a large amount of translations that cover highly sought after new vocabulary and terminology. For example, in Table 1, we can see  QºË@ éÓY  that, éJ Kð g is translated to ""e-Service"" by the user. 14 In addition to collecting parallel tweets and Twitter accounts, we also annotate a subset of Twitter accounts for their countries and topics the acco"
2020.bucc-1.3,W14-3601,1,0.815595,"ernal resources. The generic and simple nature of our method makes it easily adaptable for less-resourced languages. Ling et al. (2013) collect parallel content of different languages from single tweets (compare Table 1 and Table 2 for difference). They reported a significant improvement in MT systems. In this work, we will not focus on extracting parallel content from single tweets. However, our methods can be adapted to do so in the future. Our work also augments existing work in Twitter account annotation. Specifically for Arabic Twitter users, there is a scarcity of resources. Inspired by Mubarak and Darwish (2014), who annotate tweets for their dialects, Bouamor et al. (2019) presented a dataset of 3000 Twitter accounts annotated with their countries of origin. Alhozaimi and Almishari (2018) categorize 80 Twitter accounts into 4 categories of topics the accounts are interested in. It suffices to say that there is a need for such resources and our annotation of Twitter accounts for country and topic, although not our primary goal, is a step forward. Related Work Although the amount of data on social media is growing at an incredible speed and can be a valuable resource for NLP tasks, the utilization of"
2020.bucc-1.3,L18-1181,1,0.785138,"ws an example of such accounts. These accounts are not very useful for the purpose of creating a corpus for machine translation. To identify these accounts, we plot number of words in all the tweets posted by the account against the number of unique words among them. If the ratio of unique words versus total words is below a threshold, we exclude the account. To increase the quality of the collected Arabic-English tweets, we can use complex Arabic word segmenter to split prefixes and suffixes, for example Farasa word segmenter (Darwish and Mubarak, 2016; Abdelali et al., 2016), or lemmatizer (Mubarak, 2018), and for English we can use Porter stemmer (Porter, 1980). We leave this for future work. 3.3. Arabic-English Parallel Tweets Corpus Using the method described in Section 3.2., we collect a corpus of 166K Arabic-English parallel tweets and 1,389 accounts who regularly post them. For our collection of Arabic-English parallel tweets, first, we collect 175M Arabic tweets in March 2014 using Twitter API with language Then, we calculate ratio of unique words and total number of words in tweets posted by each account. If this ratio falls below the threshold of 0.1, we exclude the account and all th"
2020.bucc-1.3,L18-1422,0,0.0373591,"Missing"
2020.bucc-1.3,L16-1469,0,0.0163787,"rpose of training Statistical Machine Translation (SMT) pipeline. For evaluation purposes, Jehl et al. (2012) use crowdsourcing to create a parallel corpus of 1000 Arabic tweets and 3 manual English translations for each Arabic tweet and reports improvement for SMT pipeline. Abidi and Smaili (2017) used topics related to Syria to crawl Twitter and collect 58,000 Arabic tweets and 60,000 English tweets. The tweets are then preprocessed heavily, which requires knowledge of Arabic. Then, the tweets are aligned to produce a corpus of comparable Arabic-English tweets aimed at improving MT systems. Vicente et al. (2016) present a parallel corpus that covers 5 languages from the Iberian Peninsula, created by automatic collection and crowdsourcing. To align parallel content, Vicente et al. (2016) use measures such as publication date, string length similarity, hashtag and user mention overlap, and Longest Common Subsequence ratio (LCSR). LCSR exploits the similarity of the languages within the Iberian peninsula. The aim of the corpus is to aid in the development of microtext translation systems. Vicente et al. (2016) 3. Methodology and Corpus Construction Before diving further into the methodology, it’s import"
2020.coling-demos.15,N16-3003,1,0.796987,"rade 6) 1 Buckwalter transliteration and translation are provided. We thank The World Organization for Renaissance of Arabic Language (WORAL) for data collection and preparation. 3 We use ISO 3166-1 alpha-2 for country codes. 2 81 4 System Description System Architecture: An overview of the system functionalities is illustrated in Figure 1, and the system can be publicly accessed using the following URL: curriculum.qcri.org. After the acquisition of the textbooks collection, we used the publicly available Farasa Arabic NLP toolkit to process the text. This includes morphological segmentation (Abdelali et al., 2016), diacritization (Darwish et al., 2017); and lemmatization (Mubarak, 2018). These steps are crucial to enhance the analysis given the complexities of Arabic. Next, language experts classified lemmas into 50 categories (ex: Function Words, Human, Animal, Food, History, Politics, Travel, Religious Acts, etc.) The system provides the following functions: Term Usage, Category, Statistics, Differences, and Text Grading. It also uses Text to Speech (TTS) , Machine Translation (MT), and Farasa Tools to pronounce, translate, and provide morphological analysis of lexical items respectively. Design: To"
2020.coling-demos.15,L18-1366,0,0.0587851,"orm that analyzes curricula can help identify shortcomings and whether they are tailored to desired outcomes. Natural Language Processing (NLP) can provide automated methods to perform such analysis and provide feedback to curricula developers. A wealth of research devoted to build, curate, and assess educational materials has been published for English and other Latin languages (Tyler, 1950; Oliva, 2005; Braun et al., 2006; Soto, 2015). Though some recent NLP work on Arabic has addressed language learning, readability and textbook assessments (Zaghouani et al., 2014; Zalmout et al., 2016; Al Khalil et al., 2018), the work is limited with rather scarce resources and tools. This paper aims to contribute to curricula assessment, and fill some of the gaps in the literature. We focus on analyzing Arabic curricula taught in Gulf countries at elementary school level. We built a tool that analyzes curricula by providing: statistics about word usage and morphological forms in different grades; words belonging to specific categories, such as food or animals; comparison with other curricula; and complexity levels of words in a text according to selected grades. The tool provides insights into the strengths and"
2020.coling-demos.15,W17-1302,1,0.819228,"translation are provided. We thank The World Organization for Renaissance of Arabic Language (WORAL) for data collection and preparation. 3 We use ISO 3166-1 alpha-2 for country codes. 2 81 4 System Description System Architecture: An overview of the system functionalities is illustrated in Figure 1, and the system can be publicly accessed using the following URL: curriculum.qcri.org. After the acquisition of the textbooks collection, we used the publicly available Farasa Arabic NLP toolkit to process the text. This includes morphological segmentation (Abdelali et al., 2016), diacritization (Darwish et al., 2017); and lemmatization (Mubarak, 2018). These steps are crucial to enhance the analysis given the complexities of Arabic. Next, language experts classified lemmas into 50 categories (ex: Function Words, Human, Animal, Food, History, Politics, Travel, Religious Acts, etc.) The system provides the following functions: Term Usage, Category, Statistics, Differences, and Text Grading. It also uses Text to Speech (TTS) , Machine Translation (MT), and Farasa Tools to pronounce, translate, and provide morphological analysis of lexical items respectively. Design: To implement our tool, we used Django4 , a"
2020.coling-demos.15,L18-1039,0,0.0127727,"predict the reading difficulty of texts. Al-Khalifa and Al-Ajlan (2010) proposed a tool for readability analysis and applied this to curricula in Saudi Arabia. Zalmout et al. (2016) described a process to analyze the textbooks of two different English teaching methods for English as a Second Language (ESL) by using readability scoring technique. Al Khalil et al. (2018) presented an Arabic reading corpus that was collected from textbooks from first to twelfth grade from United Arab Emirates and works of fiction to enhance the inadequate resources that effected educational applications. Garc´ıa Salido et al. (2018) proposed a lexical tool for academic writing in Spanish and described the data extraction from a corpus of academic texts. This tool basically provides insight into how to use typical vocabulary for academic genre in order to build an entire text. Arabic is a complex language with rich morphology. Stems are typically derived from a set of roots using predefined stem templates. Affixes can be attached to stems to generate words (surface forms). For . JºJ ð (“wsyktbwnhA” – “and they will write it”)1 has two prefixes (and and will) example, the word AîEñJ and two suffixes (they and it). Furthe"
2020.coling-demos.15,L18-1181,1,0.737646,"rld Organization for Renaissance of Arabic Language (WORAL) for data collection and preparation. 3 We use ISO 3166-1 alpha-2 for country codes. 2 81 4 System Description System Architecture: An overview of the system functionalities is illustrated in Figure 1, and the system can be publicly accessed using the following URL: curriculum.qcri.org. After the acquisition of the textbooks collection, we used the publicly available Farasa Arabic NLP toolkit to process the text. This includes morphological segmentation (Abdelali et al., 2016), diacritization (Darwish et al., 2017); and lemmatization (Mubarak, 2018). These steps are crucial to enhance the analysis given the complexities of Arabic. Next, language experts classified lemmas into 50 categories (ex: Function Words, Human, Animal, Food, History, Politics, Travel, Religious Acts, etc.) The system provides the following functions: Term Usage, Category, Statistics, Differences, and Text Grading. It also uses Text to Speech (TTS) , Machine Translation (MT), and Farasa Tools to pronounce, translate, and provide morphological analysis of lexical items respectively. Design: To implement our tool, we used Django4 , a Python web framework for the rapid"
2020.coling-demos.15,zaghouani-etal-2014-large,0,0.031376,"r example by vocabulary level. Developing a platform that analyzes curricula can help identify shortcomings and whether they are tailored to desired outcomes. Natural Language Processing (NLP) can provide automated methods to perform such analysis and provide feedback to curricula developers. A wealth of research devoted to build, curate, and assess educational materials has been published for English and other Latin languages (Tyler, 1950; Oliva, 2005; Braun et al., 2006; Soto, 2015). Though some recent NLP work on Arabic has addressed language learning, readability and textbook assessments (Zaghouani et al., 2014; Zalmout et al., 2016; Al Khalil et al., 2018), the work is limited with rather scarce resources and tools. This paper aims to contribute to curricula assessment, and fill some of the gaps in the literature. We focus on analyzing Arabic curricula taught in Gulf countries at elementary school level. We built a tool that analyzes curricula by providing: statistics about word usage and morphological forms in different grades; words belonging to specific categories, such as food or animals; comparison with other curricula; and complexity levels of words in a text according to selected grades. The"
2020.coling-demos.15,W16-4916,0,0.154605,"level. Developing a platform that analyzes curricula can help identify shortcomings and whether they are tailored to desired outcomes. Natural Language Processing (NLP) can provide automated methods to perform such analysis and provide feedback to curricula developers. A wealth of research devoted to build, curate, and assess educational materials has been published for English and other Latin languages (Tyler, 1950; Oliva, 2005; Braun et al., 2006; Soto, 2015). Though some recent NLP work on Arabic has addressed language learning, readability and textbook assessments (Zaghouani et al., 2014; Zalmout et al., 2016; Al Khalil et al., 2018), the work is limited with rather scarce resources and tools. This paper aims to contribute to curricula assessment, and fill some of the gaps in the literature. We focus on analyzing Arabic curricula taught in Gulf countries at elementary school level. We built a tool that analyzes curricula by providing: statistics about word usage and morphological forms in different grades; words belonging to specific categories, such as food or animals; comparison with other curricula; and complexity levels of words in a text according to selected grades. The tool provides insight"
2020.lrec-1.761,W17-3008,1,0.905605,"Modeling techniques such as keyword-based search, traditional machine learning to deep learning have been explored. However, most of the previous studies are limited to IndoEuropean languages due to the availability of resources in these languages. Unlike these resource-rich languages, studies and resources for detecting offensive language in dialectal or Modern Standard Arabic (MSA) are still very limited. Similar to the Indo-European languages, most of the publicly available datasets for Arabic (Mubarak and Darwish, 2019; Mulki et al., 2019; Alakrot et al., 2018; Al-Ajlan and Ykhlef, 2018; Mubarak et al., 2017) originate mainly from one social media platform: either Twitter (TW) or YouTube (YT). However, the challenges of detecting offensive language are not constrained to one or two platforms but have a crossplatform nature (Salminen et al., 2020). Therefore, research efforts are needed to develop rich resources that can be used to design and evaluate cross-platform offensive language classifiers. In this study, we introduce one of the first Dialectal Arabic (DA) offensive language datasets, extracted from three different social media platforms: TW, YT, and Facebook (FB). Our annotated dataset comp"
2020.lrec-1.761,W19-3512,0,0.196572,"einmardi et al., 2015), aggression (Kumar et al., 2018) among others. Modeling techniques such as keyword-based search, traditional machine learning to deep learning have been explored. However, most of the previous studies are limited to IndoEuropean languages due to the availability of resources in these languages. Unlike these resource-rich languages, studies and resources for detecting offensive language in dialectal or Modern Standard Arabic (MSA) are still very limited. Similar to the Indo-European languages, most of the publicly available datasets for Arabic (Mubarak and Darwish, 2019; Mulki et al., 2019; Alakrot et al., 2018; Al-Ajlan and Ykhlef, 2018; Mubarak et al., 2017) originate mainly from one social media platform: either Twitter (TW) or YouTube (YT). However, the challenges of detecting offensive language are not constrained to one or two platforms but have a crossplatform nature (Salminen et al., 2020). Therefore, research efforts are needed to develop rich resources that can be used to design and evaluate cross-platform offensive language classifiers. In this study, we introduce one of the first Dialectal Arabic (DA) offensive language datasets, extracted from three different socia"
2020.lrec-1.761,D17-1117,0,0.0625589,"Missing"
2020.lrec-1.761,W18-5110,0,0.0233224,"Missing"
2020.semeval-1.188,2020.semeval-1.206,0,0.0947199,"Missing"
2020.semeval-1.188,C18-1139,0,0.018965,"ømberg-Derczynski et al., 2020), etc. 4 1428 Many teams also used context-independent embeddings from word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014), including language-specific embeddings such as Mazajak (Farha and Magdy, 2019) for Arabic. Some teams used other techniques: word n-grams, character n-grams, lexicons for sentiment analysis, and lexicon of offensive words. Other representations included emoji priors extracted from the weakly supervised SOLID dataset for English, and sentiment analysis using NLTK (Bird et al., 2009), Vader (Hutto and Gilbert, 2014), and FLAIR (Akbik et al., 2018). Machine learning models In terms of machine learning models, most teams used some kind of pretrained Transformers: typically BERT, but RoBERTa, XLM-RoBERTa (Conneau et al., 2020), ALBERT (Lan et al., 2019), and GPT-2 (Radford et al., 2019) were also popular. Other popular models included CNNs (Fukushima, 1980), RNNs (Rumelhart et al., 1986), and GRUs (Cho et al., 2014). Older models such as SVMs (Cortes and Vapnik, 1995) were also used, typically as part of ensembles. 5 English Track A total of 87 teams made submissions for the English track (23 of them participated in the 2019 edition of th"
2021.acl-long.177,N16-3003,1,0.821743,"(Valk and Alum¨ae, 2020; Wang et al., 2021) transcription. This work enables to either reduce Word Error Rate (WER) considerably or extract metadata from speech: dialect-identification (Shon et al., 2020); speaker-identification (Shon et al., 2019); and codeswitching (Chowdhury et al., 2020b, 2021). Natural Language Processing (NLP), on the other hand values large amount of textual information for designing experiments. NLP research for Arabic has achieved a milestone in the last few years in morphological disambiguation, Named Entity Recognition (NER) and diacritization (Pasha et al., 2014; Abdelali et al., 2016; Mubarak et al., 2019). The NLP stack for Modern Standard Arabic (MSA) has reached very high performance in many tasks. With the rise of Dialectal Arabic (DA) content online, more resources and models have been built to study DA textual dialect identification (Abdul-Mageed et al., 2020; Samih et al., 2017). Our objective is to release the first Arabic speech and NLP corpus to study spoken MSA and DA. This is to enable empirical evaluation of learning more than the word sequence from the speech. In our view, existing speech and NLP corpora are missing the link between the two different modalit"
2021.acl-long.177,2020.wanlp-1.9,0,0.039754,"al., 2020b, 2021). Natural Language Processing (NLP), on the other hand values large amount of textual information for designing experiments. NLP research for Arabic has achieved a milestone in the last few years in morphological disambiguation, Named Entity Recognition (NER) and diacritization (Pasha et al., 2014; Abdelali et al., 2016; Mubarak et al., 2019). The NLP stack for Modern Standard Arabic (MSA) has reached very high performance in many tasks. With the rise of Dialectal Arabic (DA) content online, more resources and models have been built to study DA textual dialect identification (Abdul-Mageed et al., 2020; Samih et al., 2017). Our objective is to release the first Arabic speech and NLP corpus to study spoken MSA and DA. This is to enable empirical evaluation of learning more than the word sequence from the speech. In our view, existing speech and NLP corpora are missing the link between the two different modalities. Speech poses unique challenges such as disfluency (Pravin and Palanivelan, 2021), overlap speech (Tripathi et al., 2020; Chowdhury et al., 2019), hesitation (Wottawa et al., 2020; Chowdhury et al., 2017), and code-switching (Du et al., 2021; Chowdhury et al., 2021). These challenge"
2021.acl-long.177,2020.wnut-1.18,0,0.0983321,"Missing"
2021.acl-long.177,2020.osact-1.2,0,0.0426031,"Missing"
2021.acl-long.177,W17-4601,1,0.83481,"urces and models have been built to study DA textual dialect identification (Abdul-Mageed et al., 2020; Samih et al., 2017). Our objective is to release the first Arabic speech and NLP corpus to study spoken MSA and DA. This is to enable empirical evaluation of learning more than the word sequence from the speech. In our view, existing speech and NLP corpora are missing the link between the two different modalities. Speech poses unique challenges such as disfluency (Pravin and Palanivelan, 2021), overlap speech (Tripathi et al., 2020; Chowdhury et al., 2019), hesitation (Wottawa et al., 2020; Chowdhury et al., 2017), and code-switching (Du et al., 2021; Chowdhury et al., 2021). These challenges are often overlooked when it comes to NLP tasks, since they are not present in typical text data. In this paper, we create and release2 the largest corpus for transcribed Arabic speech. It comprises of 2, 000 hours of speech data with lightly supervised transcriptions. Our contributions are: (i) 2 Data can be obtained from: https://arabicspeech.org/qasr 2274 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Proces"
2021.acl-long.177,cieri-etal-2004-fisher,0,0.244138,"tion task. To prepare the training data, we first segment the utterances from the same speaker with a maximum window of 120 tokens. We then remove utterances with ≤ 6 words and no punctuation in the segment. We pre-process the lexical utterances, removing diacritics, brackets, among others. For the task, we only keep the top 3 punctuation classes (‘, ’, ‘? ’ and ‘.’) and rest are mapped to class ‘O’ representing no punctuation. The distribution of punctuation in QASR are highly imbalanced (as shown in Table 6), which is expected of a spoken corpus. However, in comparison to the Fisher corpus (Cieri et al., 2004) and other language datasets (see (Li and Lin, 2020)), the distribution is more skewed. This is because in Arabic, punctuation marks are rarely used, e.g., Segment1 in Figure 1, can be logically divided into two segments separated by a full stop. We adapt a simple transformer-biLSTM architecture (Alam et al., 2020) as our baseline model using lexical information. Given an input token sequence (x1 , x2 ..., xm ), we extract the subwords (s1 , s2 ..., sn ) using wordpiece tokenizer. These subwords are fed into the pre-trained BERT model, which outputs a vector of d dimension for each time step."
2021.acl-long.177,P13-1153,0,0.0625336,"Missing"
2021.acl-long.177,L16-1292,0,0.0388686,"Missing"
2021.acl-long.177,2021.eacl-demos.14,1,0.781218,"Missing"
2021.acl-long.177,N19-1248,1,0.704002,"0; Wang et al., 2021) transcription. This work enables to either reduce Word Error Rate (WER) considerably or extract metadata from speech: dialect-identification (Shon et al., 2020); speaker-identification (Shon et al., 2019); and codeswitching (Chowdhury et al., 2020b, 2021). Natural Language Processing (NLP), on the other hand values large amount of textual information for designing experiments. NLP research for Arabic has achieved a milestone in the last few years in morphological disambiguation, Named Entity Recognition (NER) and diacritization (Pasha et al., 2014; Abdelali et al., 2016; Mubarak et al., 2019). The NLP stack for Modern Standard Arabic (MSA) has reached very high performance in many tasks. With the rise of Dialectal Arabic (DA) content online, more resources and models have been built to study DA textual dialect identification (Abdul-Mageed et al., 2020; Samih et al., 2017). Our objective is to release the first Arabic speech and NLP corpus to study spoken MSA and DA. This is to enable empirical evaluation of learning more than the word sequence from the speech. In our view, existing speech and NLP corpora are missing the link between the two different modalities. Speech poses uniqu"
2021.acl-long.177,W14-3617,1,0.755499,"er self attention layers with each hidden layer of 768. These token embeddings are then passed onto a BiLSTM with hidden dimension of 768. The baseline model is trained using Adam optimizer with a learning rate of 1e − 5 and 32 batch size for 10 epochs. Despite the fact that Arabic has a skewed distribution in punctuation, the baseline results reported in Table 7 for the 3 punctuation and ‘O’ labels show that the prediction results of the full stop and the question mark are better than the comma. This again reconfirms that in Arabic, the use of comma is highly debatable (Mubarak et al., 2015; Mubarak and Darwish, 2014) and can easily be substituted by the full stop or other punctuation. In the future, we will explore better architectures with information from different modalities, such as acoustics. 10 The maximum length of the subwords is set to 256. In cases, if the sequence exceeds the maximum length, it is then divided into two separate sequences. 2280 Speakers – Male – Female Segments Countries 40 (Anchor (A): 20, Guest (G): 20) 33 (A: 14, G:19) 7 (A:6, G:1) 4, 000 (100 / speaker) 11 unique countries (DZ, EG, IQ, LB, LY, MA, PS, SA, SY, TN, YE) Table 8: QASR subset used for speaker verification (SV) an"
2021.acl-long.177,W15-3218,1,0.838772,"containing 3 transformer self attention layers with each hidden layer of 768. These token embeddings are then passed onto a BiLSTM with hidden dimension of 768. The baseline model is trained using Adam optimizer with a learning rate of 1e − 5 and 32 batch size for 10 epochs. Despite the fact that Arabic has a skewed distribution in punctuation, the baseline results reported in Table 7 for the 3 punctuation and ‘O’ labels show that the prediction results of the full stop and the question mark are better than the comma. This again reconfirms that in Arabic, the use of comma is highly debatable (Mubarak et al., 2015; Mubarak and Darwish, 2014) and can easily be substituted by the full stop or other punctuation. In the future, we will explore better architectures with information from different modalities, such as acoustics. 10 The maximum length of the subwords is set to 256. In cases, if the sequence exceeds the maximum length, it is then divided into two separate sequences. 2280 Speakers – Male – Female Segments Countries 40 (Anchor (A): 20, Guest (G): 20) 33 (A: 14, G:19) 7 (A:6, G:1) 4, 000 (100 / speaker) 11 unique countries (DZ, EG, IQ, LB, LY, MA, PS, SA, SY, TN, YE) Table 8: QASR subset used for"
2021.acl-long.177,pasha-etal-2014-madamira,0,0.0930427,"Missing"
2021.acl-long.177,W17-1306,1,0.849129,"Language Processing (NLP), on the other hand values large amount of textual information for designing experiments. NLP research for Arabic has achieved a milestone in the last few years in morphological disambiguation, Named Entity Recognition (NER) and diacritization (Pasha et al., 2014; Abdelali et al., 2016; Mubarak et al., 2019). The NLP stack for Modern Standard Arabic (MSA) has reached very high performance in many tasks. With the rise of Dialectal Arabic (DA) content online, more resources and models have been built to study DA textual dialect identification (Abdul-Mageed et al., 2020; Samih et al., 2017). Our objective is to release the first Arabic speech and NLP corpus to study spoken MSA and DA. This is to enable empirical evaluation of learning more than the word sequence from the speech. In our view, existing speech and NLP corpora are missing the link between the two different modalities. Speech poses unique challenges such as disfluency (Pravin and Palanivelan, 2021), overlap speech (Tripathi et al., 2020; Chowdhury et al., 2019), hesitation (Wottawa et al., 2020; Chowdhury et al., 2017), and code-switching (Du et al., 2021; Chowdhury et al., 2021). These challenges are often overlooke"
2021.acl-long.177,2020.lrec-1.190,0,0.0283667,"tent online, more resources and models have been built to study DA textual dialect identification (Abdul-Mageed et al., 2020; Samih et al., 2017). Our objective is to release the first Arabic speech and NLP corpus to study spoken MSA and DA. This is to enable empirical evaluation of learning more than the word sequence from the speech. In our view, existing speech and NLP corpora are missing the link between the two different modalities. Speech poses unique challenges such as disfluency (Pravin and Palanivelan, 2021), overlap speech (Tripathi et al., 2020; Chowdhury et al., 2019), hesitation (Wottawa et al., 2020; Chowdhury et al., 2017), and code-switching (Du et al., 2021; Chowdhury et al., 2021). These challenges are often overlooked when it comes to NLP tasks, since they are not present in typical text data. In this paper, we create and release2 the largest corpus for transcribed Arabic speech. It comprises of 2, 000 hours of speech data with lightly supervised transcriptions. Our contributions are: (i) 2 Data can be obtained from: https://arabicspeech.org/qasr 2274 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference o"
2021.acl-long.177,2021.acl-long.80,0,0.0949936,"Missing"
2021.findings-emnlp.56,2021.nlp4if-1.9,1,0.705503,"tions. Some of the larger datasets include the Liar, Liar dataset of 12.8K claims from PolitiFact (Wang, 2017), the ClaimsKG dataset and system (Tchechmedjiev et al., 2019) of 28K claims from eight factchecking organizations, the MultiFC dataset of 38K claims from 26 fact-checking organizations (Augenstein et al., 2019), and the 10K claims Truth of Various Shades dataset (Rashkin et al., 2017). There have been also datasets for other languages, • We develop a large manually annotated created in a similar fashion, e.g., for Arabic (Baly dataset of 16K tweets related to the COVID- et al., 2018; Alhindi et al., 2021). 19 infodemic in four languages (Arabic, BulA number of datasets were created as part of garian, Dutch, and English), using a schema shared tasks. In most cases, they performed their that combines the perspective of journalists, own annotation, either (a) manually, e.g., the Sefact-checkers, social media platforms, policymEval tasks on determining the veracity of rumakers, and the society. mors (Derczynski et al., 2017; Gorrell et al., 2019), propaganda detection in news articles and memes • We demonstrate sizable performance gains over popular deep contextualized text repre- (Da San Martino"
2021.findings-emnlp.56,N19-1216,1,0.795082,"tweets in Arabic, Bulgarian, Dutch, and English, and we are making it freely available to the research community. We further reported a number of evaluation results for all languages using various transformer architectures. Moreover, we performed advanced experiments, including multilingual training, modeling the Twitter context, the use of propagandistic language, and whether the user is likely to be a bot, as well as multitask learning. In future work, we plan to explore multimodality and explainability (Yu et al., 2021). We further want to model the task as a multitask ordinal regression (Baly et al., 2019), as Q2–Q5 are defined on an ordinal scale. Moreover, we would like to put the data and the system in some practical use; in fact, we have already used them to analyze disinformation about COVID-19 in Bulgaria (Nakov et al., 2021a) and Qatar (Nakov et al., 2021b). Finally, the data will be used in a shared task at the CLEF2022 CheckThat! lab; part of it was used for the NLP4IF-2021 shared task (Shaar et al., 2021a). Acknowledgments We thank Akter Fatema, Al-Awthan Ahmed, AlDobashi Hussein, El Messelmani Jana, Fayoumi 6.3 Multitask Learning Sereen, Mohamed Esraa, Ragab Saleh, and Shurafa For th"
2021.findings-emnlp.56,N18-2004,1,0.90491,"Missing"
2021.findings-emnlp.56,2020.acl-main.747,0,0.0346481,"a URL, and the factuality of the website it points to.4 Models Large-scale pretrained Transformer models have achieved state-of-the-art performance for several NLP tasks. We experimented with several such models to evaluate their efficacy under various training scenarios such as, binary vs. multiclass classification, multilingual setup, etc. We used BERT (Devlin et al., 2019) and RoBERTa for English, AraBERT (Antoun et al., 2020) for Arabic, and BERTje (de Vries et al., 2019) for Dutch. We further used multilingual transformers such as (Liu et al., 2019), multilingual BERT (mBERT) and XLM-r (Conneau et al., 2020). Finally, we used static embeddings from FastText (Joulin et al., 2017). 616 4 From http://mediabiasfactcheck.com English Q. Cls. Arabic Maj. FT BT RT Bulgarian Maj. FT ArBT XLM-r Dutch Maj. FT mBT XLM-r Maj. FT BTje XLM-r Binary (Coarse-grained) Q1 Q2 Q3 Q4 Q5 Q6 Q7 2 2 2 2 2 2 2 Avg. 48.7 91.6 96.3 66.7 67.7 86.7 78.3 77.7 89.0 69.3 96.3 83.8 92.1 80.6 76.5 92.1 96.4 85.6 80.6 88.9 85.5 78.6 92.7 96.9 89.0 84.4 90.5 86.1 76.6 84.1 86.5 88.3 83.8 84.0 96.0 90.3 65.9 88.9 77.4 84.2 83.1 96.3 89.0 66.7 89.8 77.4 58.3 95.0 96.5 86.8 70.5 83.2 80.1 84.0 94.7 96.0 87.7 80.5 84.5 81.6 87.6 95.0 96"
2021.findings-emnlp.56,2020.semeval-1.186,1,0.850189,"., 2021). 19 infodemic in four languages (Arabic, BulA number of datasets were created as part of garian, Dutch, and English), using a schema shared tasks. In most cases, they performed their that combines the perspective of journalists, own annotation, either (a) manually, e.g., the Sefact-checkers, social media platforms, policymEval tasks on determining the veracity of rumakers, and the society. mors (Derczynski et al., 2017; Gorrell et al., 2019), propaganda detection in news articles and memes • We demonstrate sizable performance gains over popular deep contextualized text repre- (Da San Martino et al., 2020a; Dimitrov et al., sentations (such as BERT), when using mul- 2021a,b), fact-checking in community question answering forums (Mihaylova et al., 2019), the CLEF titask learning, cross-language learning, and when modeling the social context of the tweet, CheckThat! lab on identification and verification of claims (Nakov et al., 2018; Elsayed et al., 2019; as well as the propagandistic nature of the Barrón-Cedeño et al., 2020; Shaar et al., 2020; language used. Nakov et al., 2021c; Shaar et al., 2021b,c), or (b) us• We make our data and code freely available.1 ing crowdsourcing, e.g., the FEVER"
2021.findings-emnlp.56,2020.acl-demos.32,1,0.926725,"., 2021). 19 infodemic in four languages (Arabic, BulA number of datasets were created as part of garian, Dutch, and English), using a schema shared tasks. In most cases, they performed their that combines the perspective of journalists, own annotation, either (a) manually, e.g., the Sefact-checkers, social media platforms, policymEval tasks on determining the veracity of rumakers, and the society. mors (Derczynski et al., 2017; Gorrell et al., 2019), propaganda detection in news articles and memes • We demonstrate sizable performance gains over popular deep contextualized text repre- (Da San Martino et al., 2020a; Dimitrov et al., sentations (such as BERT), when using mul- 2021a,b), fact-checking in community question answering forums (Mihaylova et al., 2019), the CLEF titask learning, cross-language learning, and when modeling the social context of the tweet, CheckThat! lab on identification and verification of claims (Nakov et al., 2018; Elsayed et al., 2019; as well as the propagandistic nature of the Barrón-Cedeño et al., 2020; Shaar et al., 2020; language used. Nakov et al., 2021c; Shaar et al., 2021b,c), or (b) us• We make our data and code freely available.1 ing crowdsourcing, e.g., the FEVER"
2021.findings-emnlp.56,S19-2147,0,0.0285024,"s, • We develop a large manually annotated created in a similar fashion, e.g., for Arabic (Baly dataset of 16K tweets related to the COVID- et al., 2018; Alhindi et al., 2021). 19 infodemic in four languages (Arabic, BulA number of datasets were created as part of garian, Dutch, and English), using a schema shared tasks. In most cases, they performed their that combines the perspective of journalists, own annotation, either (a) manually, e.g., the Sefact-checkers, social media platforms, policymEval tasks on determining the veracity of rumakers, and the society. mors (Derczynski et al., 2017; Gorrell et al., 2019), propaganda detection in news articles and memes • We demonstrate sizable performance gains over popular deep contextualized text repre- (Da San Martino et al., 2020a; Dimitrov et al., sentations (such as BERT), when using mul- 2021a,b), fact-checking in community question answering forums (Mihaylova et al., 2019), the CLEF titask learning, cross-language learning, and when modeling the social context of the tweet, CheckThat! lab on identification and verification of claims (Nakov et al., 2018; Elsayed et al., 2019; as well as the propagandistic nature of the Barrón-Cedeño et al., 2020; Shaar"
2021.findings-emnlp.56,2021.wanlp-1.9,0,0.0367297,"llected tweets by specifying a target language (English, Arabic, Bulgarian, or Dutch), a set of COVID-19 related keywords, as shown in Figure 2, and different time frames: from January 2020 till March 2021. We collected original tweets (no retweets or replies), we removed duplicates using a similarity-based approach (Alam et al., 2021b), and we filtered out tweets with less than five words. Finally, we selected the most frequently liked and retweeted tweets for annotation. COVID-19 Research There are a number of COVID-19 Twitter datasets: some unlabeled (Chen et al., 2020; Banda et al., 2021; Haouari et al., 2021), some automatically labeled with location information (Abdul-Mageed et al., 2021; Qazi et al., 2020), some labeled using distant supervision (Cinelli et al., 2020; Zhou et al., 2020), and some manually annotated (Song et al., 2020; Vidgen et al., 2020; Shahi and Nandini, 2020; Pulido et al., 2020; Dharawat et al., 2020). There is also work on credibility (Cinelli et al., 2020; Pulido et al., 2020; Zhou et al., 2020), racial prejudices and fear (Medford et al., 2020; Vidgen et al., 2020), as well as situational information, e.g., caution and advice (Li et al., 2020), as well as on detecting me"
2021.findings-emnlp.56,2020.nlpcovid19-2.11,0,0.043054,"Abdul-Mageed et al., 2021; Qazi et al., 2020), some labeled using distant supervision (Cinelli et al., 2020; Zhou et al., 2020), and some manually annotated (Song et al., 2020; Vidgen et al., 2020; Shahi and Nandini, 2020; Pulido et al., 2020; Dharawat et al., 2020). There is also work on credibility (Cinelli et al., 2020; Pulido et al., 2020; Zhou et al., 2020), racial prejudices and fear (Medford et al., 2020; Vidgen et al., 2020), as well as situational information, e.g., caution and advice (Li et al., 2020), as well as on detecting mentions and stance with respect to known misconceptions (Hossain et al., 2020). The closest work to ours is that of Song et al. (2020), who collected false and misleading claims about COVID-19 from IFCN Poynter, and annotated them as (1) Public authority, (2) Community spread and impact, (3) Medical advice, selftreatments, and virus effects, (4) Prominent actors, (5) Conspiracies, (6) Virus transmission, (7) Virus Figure 2: The keywords used to collect the tweets. origins and properties, (8) Public reaction, and (9) Vaccines, medical treatments, and tests. These categories partially overlap with ours, but account 3.2 Annotation Task for less perspectives. Moreover, we c"
2021.findings-emnlp.56,N18-5006,1,0.802211,"tweets (they used claims from news, speeches, political debates, community question answering fora, or were just made up by human annotators; RumourEval is a notable exception), targeted factuality only (we cover a number of other issues), were limited to a single language (typically English; except for CLEF), and did not focus on COVID-19. Check-Worthiness Estimation Another relevant research line is on detecting check-worthy claims in political debates using manual annotations (Hassan et al., 2015) or by observing the selection of fact-checkers (Gencheva et al., 2017; Patwari et al., 2017; Jaradat et al., 2018; Vasileva et al., 2019). 3 3.1 Dataset Data Collection We collected tweets by specifying a target language (English, Arabic, Bulgarian, or Dutch), a set of COVID-19 related keywords, as shown in Figure 2, and different time frames: from January 2020 till March 2021. We collected original tweets (no retweets or replies), we removed duplicates using a similarity-based approach (Alam et al., 2021b), and we filtered out tweets with less than five words. Finally, we selected the most frequently liked and retweeted tweets for annotation. COVID-19 Research There are a number of COVID-19 Twitter data"
2021.findings-emnlp.56,E17-2068,0,0.0257945,"le pretrained Transformer models have achieved state-of-the-art performance for several NLP tasks. We experimented with several such models to evaluate their efficacy under various training scenarios such as, binary vs. multiclass classification, multilingual setup, etc. We used BERT (Devlin et al., 2019) and RoBERTa for English, AraBERT (Antoun et al., 2020) for Arabic, and BERTje (de Vries et al., 2019) for Dutch. We further used multilingual transformers such as (Liu et al., 2019), multilingual BERT (mBERT) and XLM-r (Conneau et al., 2020). Finally, we used static embeddings from FastText (Joulin et al., 2017). 616 4 From http://mediabiasfactcheck.com English Q. Cls. Arabic Maj. FT BT RT Bulgarian Maj. FT ArBT XLM-r Dutch Maj. FT mBT XLM-r Maj. FT BTje XLM-r Binary (Coarse-grained) Q1 Q2 Q3 Q4 Q5 Q6 Q7 2 2 2 2 2 2 2 Avg. 48.7 91.6 96.3 66.7 67.7 86.7 78.3 77.7 89.0 69.3 96.3 83.8 92.1 80.6 76.5 92.1 96.4 85.6 80.6 88.9 85.5 78.6 92.7 96.9 89.0 84.4 90.5 86.1 76.6 84.1 86.5 88.3 83.8 84.0 96.0 90.3 65.9 88.9 77.4 84.2 83.1 96.3 89.0 66.7 89.8 77.4 58.3 95.0 96.5 86.8 70.5 83.2 80.1 84.0 94.7 96.0 87.7 80.5 84.5 81.6 87.6 95.0 96.5 88.4 82.9 85.1 81.7 36.5 64.9 62.3 63.9 44.4 84.7 65.6 75.4 75.1 76.9"
2021.findings-emnlp.56,2020.emnlp-demos.2,0,0.0153692,"80.2 69.2 68.3 Finally, we should note the strong performance Avg. 73.3 73.1 60.7 59.8 71.4 71.5 55.3 54.9 of context-free models such as FastText. We believe that it is suitable for the noisy text of Table 6: Multilingual experiments using mBERT. tweets due to its ability to model not only words Shown are results for monolingual vs. multilingual models (weighted F1 ). Mul is trained on the combined but also character n-grams. In future work, we English, Arabic, Bulgarian, and Dutch data. plan to try transformers specifically trained on tweets and/or on COVID-19 related data such as BERTweet (Nguyen et al., 2020) and COVID5 Twitter-BERT (Müller et al., 2020). We also tried XLM-r, but it performed worse. 618 6.2 Twitter/Propagandistic/Botometer We conducted experiments with Twitter, propaganda, and botness features alongside the posteriors from the BERT classifier, which we combined using XGBoost (Chen and Guestrin, 2016). The results are shown in Table 7. We can see that many of the combinations yielded improvements, with botness being the most useful, followed by propaganda, and finally by the Twitter object features. Binary (Coarse-grained) Q. Cls BERT B+TF B+Prop B+Bot B+All Q1 Q2 Q3 Q4 Q5 Q6 Q7 2"
2021.findings-emnlp.56,D17-1317,0,0.0286494,"onversations with a Ministry of Public Health. Our contributions can be summarized as follows: 2 Related Work Fact-Checking Research on fact-checking claims is largely based on datasets mined from major fact-checking organizations. Some of the larger datasets include the Liar, Liar dataset of 12.8K claims from PolitiFact (Wang, 2017), the ClaimsKG dataset and system (Tchechmedjiev et al., 2019) of 28K claims from eight factchecking organizations, the MultiFC dataset of 38K claims from 26 fact-checking organizations (Augenstein et al., 2019), and the 10K claims Truth of Various Shades dataset (Rashkin et al., 2017). There have been also datasets for other languages, • We develop a large manually annotated created in a similar fashion, e.g., for Arabic (Baly dataset of 16K tweets related to the COVID- et al., 2018; Alhindi et al., 2021). 19 infodemic in four languages (Arabic, BulA number of datasets were created as part of garian, Dutch, and English), using a schema shared tasks. In most cases, they performed their that combines the perspective of journalists, own annotation, either (a) manually, e.g., the Sefact-checkers, social media platforms, policymEval tasks on determining the veracity of rumakers"
2021.findings-emnlp.56,2021.nlp4if-1.12,1,0.887854,"We demonstrate sizable performance gains over popular deep contextualized text repre- (Da San Martino et al., 2020a; Dimitrov et al., sentations (such as BERT), when using mul- 2021a,b), fact-checking in community question answering forums (Mihaylova et al., 2019), the CLEF titask learning, cross-language learning, and when modeling the social context of the tweet, CheckThat! lab on identification and verification of claims (Nakov et al., 2018; Elsayed et al., 2019; as well as the propagandistic nature of the Barrón-Cedeño et al., 2020; Shaar et al., 2020; language used. Nakov et al., 2021c; Shaar et al., 2021b,c), or (b) us• We make our data and code freely available.1 ing crowdsourcing, e.g., the FEVER task on fact ex1 traction and verification, focusing on claims about https://github.com/firojalam/ COVID-19-disinformation Wikipedia content (Thorne et al., 2018, 2019). 612 Unlike our work, the above datasets did not focus on tweets (they used claims from news, speeches, political debates, community question answering fora, or were just made up by human annotators; RumourEval is a notable exception), targeted factuality only (we cover a number of other issues), were limited to a single language (t"
2021.wanlp-1.1,L18-1577,0,0.0352448,"Missing"
2021.wanlp-1.1,D14-1154,1,0.786389,"nd feminine forms with and without the definite article È@ (Al – the) such as ú ¯@ Q« (ErAqy - Iraqi (m.)), éJ ¯@ Q« (ErAqyp - Iraqi (f.)), and ú¯@ QªË@ (AlErAqy - the Iraqi (m.)). Arabic Variant Identification The second filter checks if the account mainly tweets in either dialectal Arabic or MSA. Since Arabic users commonly switch between MSA and dialectal Arabic, and we were interested in strictly dialectal tweets, we sought to filter out MSA tweets. There are multiple ways to distinguish between dialectal and MSA text. One such method involves using a list of strictly dialectal words (Darwish et al., 2014). However, constructing such lists across multiple dialects can be challenging. Thus, we opted to train a text classifier using a heuristically labeled tweets. Specifically, given 50 million tweets that we collected between March and September 2018, we assumed that tweets strictly containing the MSA ,úæË@  ,úæË@  YË@ relative pronouns áK ,ø YË@ ,ø YË@ (“Al*y, Al*Y, Alty, AltY, Al*yn” - who/that in masculine, feminine, and plural forms) were MSA, and those strictly containing the dialectal relative pronoun úÎË@ , ú ÎË@ (“Ally, AllY” – who/that) were dialectal. The major advantage of the diale"
2021.wanlp-1.1,2020.wanlp-1.9,0,0.0856536,"Missing"
2021.wanlp-1.1,R15-1015,0,0.0266832,"r tweets contained vulgar words. Removing the tweets of such users was motivated by the fact that their tweets contain strong genre specific signals, which may adversely affect the generalization of dialect identification. 99.5 100 Accuracy 92 90 99.5100 99 97 96.5 96 95 95 94 92 91.5 89.5 87.5 89 89 86.5 84.5 85 84.5 82.582.5 80 Normalization Tweets often contain tokens that are specific to the Twitter platform such as hashtags and user mentions. To improve generalization of the trained models (hopefully beyond tweets), we split hashtags into their semantic constituents (Bansal et al., 2015; Declerck and Lendvai, 2015) and replaced user mentions and URLs with “@USER” and “URL” respectively. 77 75 IQ BH KW SA AE OM QA YE SY JO PL LB EG SD LY TN DZ MA Country Figure 2: Annotation accuracy per country. Second annotators are colored in “Red”. The manually rejected tweets that the annotators classified as not from their dialects were mostly cases where the users interacted with or responded to users from different countries. In such cases, users tend to code-switch or adopt to other users’ dialects. For example, a user identified as Tunisian  ¢Ë@ Im&apos; AÓñÔ« AK@ (Ana EmwmA bHb tweeted ø ð@ éÒÊ . . AlZlmp Awy – I"
2021.wanlp-1.1,N19-1423,0,0.0195153,"SY ا ردن ا ك LB دا ة ز ّٰ ء ي ا وي ة ش ا را ل ا ي ء ا ا ا ا او ل وى د ادا ك اك ره ى ذو وة د ا ا ن ن خ PL م ا ول ن ت رض ف ي وي ا ازاي د ل إ ا د ك JO زول دم ا ّٰ ا ذ ر د ه ا SD زى دا را ن ن EG داك را ا TN ش را ا ا LY د ل ر أ ه MA ا رو د ة ا ن ا ى ا ر Also in other dialects Figure 3: Highest valence words for each country. contextualized language models such as BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019), UMLFIT (Howard and Ruder, 2018), and OpenAI GPT (Radford et al., 2018), to name but a few, have achieved ground-breaking results in many NLP classification and language understanding tasks. Both mBERT and AraBERT are pre-trained on identical architectures, namely an encoder with 12 Transformer blocks, hidden size of 768, and 12 selfattention heads. However, they differ in one major way. While mBERT is pre-trained on Wikipedia text for 104 languages,AraBERT is trained on a large Arabic news corpus containing 8.5M articles composed of roughly 2.5B tokens. For consistency with mBERT, we used Ar"
2021.wanlp-1.1,W19-4621,0,0.0272532,". 5.1 Representations Surface Features: We used two different surfacelevel features, namely word and character n-grams. Specifically, we represented tweets using: i) character n-grams, where we used 2 to 6-grams (C{2-6}); ii) word n-grams, where we used unigrams (W{1}) and unigrams to 6-grams (W{1-6}); and iii) a combination of word and character n-grams. For our dataset and MADAR , we normalized URLs, numbers, and user mentions to URL, NUM, and MENTION respectively. We used tf-idf weighting for character and word n-grams. Static Embeddings: We used Mazajak wordlevel skip-gram embeddings (Abu Farha and Magdy, 2019) that were trained on 250M Arabic tweets with 300-dimensional vectors. Deep Contextualized Embeddings: We also experimented with two pre-trained contextualized embeddings with fine-tuning for down-stream tasks, namely BERTbase-multilingual (mBERT) and AraBERT (Antoun et al., 2020). Recently, deep 5.2 Classification Models For classification, we used an SVM classifier and fine-tuned mBERT and AraBERT. We utilized the SVM classifier when using surface features and static pre-trained Mazajak embeddings. We used the Scikit Learn libsvm implementations of the SVM classifier with a linear kernel. Wh"
2021.wanlp-1.1,L18-1579,0,0.0346953,"Missing"
2021.wanlp-1.1,L18-1573,0,0.0439378,"Missing"
2021.wanlp-1.1,2020.osact-1.2,0,0.0924101,"8). Multiple approaches have been used for dialect ID that exploit a variety of features, such as character or word n-grams (Darwish et al., 2014; Zaidan and Callison-Burch, 2014; Malmasi et al., 2016; Sadat et al., 2014), and techniques such as multiple kernel learning (Ionescu and Popescu, 2016) and distributed representation of dialects (Abdul-Mageed et al., 2018; Zhang and Abdul-Mageed, 2019) to name a few. Zhang and Abdul-Mageed (2019) used semi-supervised learning using multilingual BERT for user-level dialect identification on the MADAR Shared Task. Arabic Tranformers-based approaches (Antoun et al., 2020; Safaya et al., 2020) showed competitive results in NADI (Abdul-Mageed et al., 2020) Shared Task. 3 • All Arab country names written in either Arabic, English, or French,4 such as H. QªÖ Ï @ (Almgrb – Morocco), Morocco, and Maroc respectively. • The names of major cities in these countries in both Arabic and English as specified in  (Alqds – Jerusalem) Wikipedia,5 such as Y®Ë@ and à@ Qëð (whrAn – Oran, Algeria). • Arabic adjectives specifying all nationalities in both masculine and feminine forms with and without the definite article È@ (Al – the) such as ú ¯@ Q« (ErAqy - Iraqi (m.)), éJ"
2021.wanlp-1.1,P13-2081,0,0.0694626,"Missing"
2021.wanlp-1.1,P18-1031,0,0.0182863,"ة ز ّٰ ء ي ا وي ة ش ا را ل ا ي ء ا ا ا ا او ل وى د ادا ك اك ره ى ذو وة د ا ا ن ن خ PL م ا ول ن ت رض ف ي وي ا ازاي د ل إ ا د ك JO زول دم ا ّٰ ا ذ ر د ه ا SD زى دا را ن ن EG داك را ا TN ش را ا ا LY د ل ر أ ه MA ا رو د ة ا ن ا ى ا ر Also in other dialects Figure 3: Highest valence words for each country. contextualized language models such as BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019), UMLFIT (Howard and Ruder, 2018), and OpenAI GPT (Radford et al., 2018), to name but a few, have achieved ground-breaking results in many NLP classification and language understanding tasks. Both mBERT and AraBERT are pre-trained on identical architectures, namely an encoder with 12 Transformer blocks, hidden size of 768, and 12 selfattention heads. However, they differ in one major way. While mBERT is pre-trained on Wikipedia text for 104 languages,AraBERT is trained on a large Arabic news corpus containing 8.5M articles composed of roughly 2.5B tokens. For consistency with mBERT, we used AraBERT with BP. Following Devlin e"
2021.wanlp-1.1,W16-4818,0,0.0376284,"Missing"
2021.wanlp-1.1,W19-4622,1,0.872945,"transcends geographical regions and borders. Automatically distinguishing between the different dialectal variations is valuable for many downstream applications such as machine translations (Diab et al., 2014), POS tagging (Darwish et al., 2020), geo-locating users, and author profiling (Sadat et al., 2014). Though there has been prior work on performing Arabic Dialect Identification (ADI), much of the work was conducted on datasets with significant limitations in terms of genre (Bouamor et al., 2018; Zaidan and Callison-Burch, 2011), number of dialects (Abdul-Mageed et al., 2018), or focus (Bouamor et al., 2019; Zaghouani and Charfi, 2018a), where often the focus was on geo-locating and profiling users as opposed to dialect identification. In this work, we expand beyond these efforts by utilizing tweets from across the MENA region to build a large, non-genre specific, fine-grained, and balanced country-level dialectal Arabic dataset that we use to build effective Arabic Dialect Identification. We rely on two main features to build the dataset. The first feature is the Twitter user profile description, where we identify users who self-declare themselves as belonging to a specific country in different"
2021.wanlp-1.1,W16-4801,0,0.0614661,"Missing"
2021.wanlp-1.1,J14-1006,0,0.0502226,"Missing"
2021.wanlp-1.1,W19-4637,0,0.0331781,"Missing"
2021.wanlp-1.1,W14-3601,1,0.826287,"Missing"
2021.wanlp-1.1,W17-3008,1,0.716641,"Missing"
2021.wanlp-1.1,W14-5904,0,0.0630664,"Missing"
2021.wanlp-1.1,2020.semeval-1.271,0,0.0745377,"Missing"
2021.wanlp-1.1,C18-1113,0,0.0161785,"gorithm treats each dialect as a singleton cluster at the outset and then successively merges (or agglomerates) clusters until all clusters have been merged into a single cluster that contains all dialects. Figure 4 shows the results of hierarchical clustering. The figure reflects the similarity and the geographical proximity of various dialects. At higher levels, dialects are grouped per region, where we can identify the major dialectal groups, namely Gulf, Maghrebi, Egyptian, and Levantine. This is aligned with geographical distribution of the dialects as well as the findings of prior work (Salameh et al., 2018). Corpus Statistics and Analysis Upon constructing the dataset, we attempted to explore its characteristics. First, we extracted features that are distinctive for each dialect. To do so, we computed the so-called valence score for each word in each dialect (Conover et al., 2011). The score helps determine the distinctiveness of a given word in a specific dialect in reference to other dialects. Given N (t, Di ), which is the frequency of the term t in Dialect Di , valence is computed as follows: V (t)i = N (t,Di ) N (Di ) 2 P N (t,D n) n N (Dn ) −1 (1) Where N (Di ) is the total number of occur"
2021.wanlp-1.1,K17-1043,1,0.906165,"Missing"
2021.wanlp-1.1,L18-1111,0,0.119407,"l regions and borders. Automatically distinguishing between the different dialectal variations is valuable for many downstream applications such as machine translations (Diab et al., 2014), POS tagging (Darwish et al., 2020), geo-locating users, and author profiling (Sadat et al., 2014). Though there has been prior work on performing Arabic Dialect Identification (ADI), much of the work was conducted on datasets with significant limitations in terms of genre (Bouamor et al., 2018; Zaidan and Callison-Burch, 2011), number of dialects (Abdul-Mageed et al., 2018), or focus (Bouamor et al., 2019; Zaghouani and Charfi, 2018a), where often the focus was on geo-locating and profiling users as opposed to dialect identification. In this work, we expand beyond these efforts by utilizing tweets from across the MENA region to build a large, non-genre specific, fine-grained, and balanced country-level dialectal Arabic dataset that we use to build effective Arabic Dialect Identification. We rely on two main features to build the dataset. The first feature is the Twitter user profile description, where we identify users who self-declare themselves as belonging to a specific country in different forms such as showing signs"
2021.wanlp-1.1,P11-2007,0,0.0826773,"Missing"
2021.wanlp-1.13,N16-3003,1,0.802265,"“prosti“doorman”) or tute”), and Q« (“ErS” – “pimp”). Figure 5 shows the top words with the highest valance scores for individual words in the offensive tweets. Larger fonts are used to highlight words with highest scores and align as well with the categories mentioned in the breakdown for the offensive languages. We slightly modified the valence score described by (Conover et al., 2011) to magnify its value by multiplying valence with frequency of occurrence. Data Pre-processing We performed several text pre-processing steps. First, we tokenized the text using the Farasa Arabic NLP toolkit (Abdelali et al., 2016). Second, we removed URLs, numbers, and all tweet specific tokens, namely mentions, retweets, and hashtags as they are not part of the language semantic structure, and therefore, not usable in pre-trained embeddings. Third, we performed basic Arabic letter normalization, namely variants of the letter alef to bare alef, ta marbouta to ha, and alef maqsoura to ya. We also separated words that are commonly incorrectly attached such as I . Ê¿ AK (“yAklb” – “O dog”), is split to I . Ê¿ AK (“yA klb”). Lastly, we normalized letter repetitions to allow for a maximum of 2 repeated letters. For example,"
2021.wanlp-1.13,W19-4621,0,0.0609864,"tive polarity and terms with negative polarity in tweets as features. 131 Static Embeddings We experimented with various static embeddings that were pre-trained on different corpora with different vector dimensionality. We compared pre-trained embeddings to embeddings that were trained on our dataset. For pre-trained embeddings, we used: fastText Egyptian Arabic pre-trained embeddings (Bojanowski et al., 2017) with vector dimensionality of 300; AraVec skip-gram embeddings (Mohammad et al., 2017), trained on 66.9M Arabic tweets with 100dimensional vectors; and Mazajak skip-gram embeddings (Abu Farha and Magdy, 2019), trained on 250M Arabic tweets with 300-dimensional vectors. Sentence embeddings were calculated by taking the mean of the embeddings of their tokens. The importance of testing a character level n-gram model like fastText lies in the agglutinative nature of the Arabic language. We trained a new fastText text classification model (Joulin et al., 2017) on our dataset with vectors of 40 dimensions, 0.5 learning rate, 2−10 character n-grams as features, for 30 epochs. These hyper-parameters were tuned using a 5-fold cross-validated grid-search. Deep Contextualized Embeddings We also experimented"
2021.wanlp-1.13,2020.osact-1.2,0,0.0282173,"to establish strong Arabic offensive language classification results. Though offensive tweets have finer-grained labels where offensive tweet could also be vulgar and/or hate speech, we conducted coarser-grained classification to determine if a tweet was offensive or not. For classification, we experimented with several tweet representation and classification models. For tweet representations, we used: the count of positive and negative terms, based on a polarity lexicon; static embeddings, namely fastText and SkipGram; and deep contextual embeddings, namely BERTbase-multilingual and AraBERT (Antoun et al., 2020). and 4.1 éJ ë@X ú ¯ hðP (“rwH fy dAhyp” – equivalent to “go to hell”). Name alteration: One common way to insult others is to change a letter or two in their names to produce new offensive words that rhyme with the original names. Some such examples include chang ing èQK Qm .Ì &apos;@ (“Aljzyrp” – “Aljazeera (channel)”) to èQK Q  m Ì &apos;@ (“Alxnzyrp” – “the pig”) and àA ®Ê g (“xl ¯Q k (“xrfAn” fAn” – “Khalfan (person name)”) to àA – “crazed”). Societal stratification: Some insults are associated with: certain jobs such as H . @ñK. (“bwAb” – ÐXAg (“xAdm” – “servant”); and specific societal compon"
2021.wanlp-1.13,Q17-1010,0,0.0340608,"used NileULex (El-Beltagy, 2016), which is an Arabic polarity lexicon containing 3,279 MSA and 2,674 Egyptian terms, out of which 4,256 are negative and 1,697 are positive. We used the counts of terms with positive polarity and terms with negative polarity in tweets as features. 131 Static Embeddings We experimented with various static embeddings that were pre-trained on different corpora with different vector dimensionality. We compared pre-trained embeddings to embeddings that were trained on our dataset. For pre-trained embeddings, we used: fastText Egyptian Arabic pre-trained embeddings (Bojanowski et al., 2017) with vector dimensionality of 300; AraVec skip-gram embeddings (Mohammad et al., 2017), trained on 66.9M Arabic tweets with 100dimensional vectors; and Mazajak skip-gram embeddings (Abu Farha and Magdy, 2019), trained on 250M Arabic tweets with 300-dimensional vectors. Sentence embeddings were calculated by taking the mean of the embeddings of their tokens. The importance of testing a character level n-gram model like fastText lies in the agglutinative nature of the Arabic language. We trained a new fastText text classification model (Joulin et al., 2017) on our dataset with vectors of 40 dim"
2021.wanlp-1.13,N19-1423,0,0.0375937,"m model like fastText lies in the agglutinative nature of the Arabic language. We trained a new fastText text classification model (Joulin et al., 2017) on our dataset with vectors of 40 dimensions, 0.5 learning rate, 2−10 character n-grams as features, for 30 epochs. These hyper-parameters were tuned using a 5-fold cross-validated grid-search. Deep Contextualized Embeddings We also experimented with pre-trained contextualized embeddings with fine-tuning for down-stream tasks. Recently, deep contextualized language models such as BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019), UMLFIT (Howard and Ruder, 2018), and OpenAI GPT (Radford et al., 2018), have achieved ground-breaking results in many NLP classification and language understanding tasks. In this paper, we fine-tuned BERTbase-multilingual (or simply BERT) and AraBERT embeddings to classify Arabic offensive language on Twitter as it eliminates the need for feature engineering. Although Robustly Optimized BERT (RoBERTa) embeddings perform better than (BERTlarge ) on GLUE (Wang et al., 2018), RACE (Lai et al., 2017), and SQuAD (Rajpurkar et al., 2016) tasks, pre-trained multilingual RoBERTa models are not avail"
2021.wanlp-1.13,L16-1463,0,0.0294886,"ya. We also separated words that are commonly incorrectly attached such as I . Ê¿ AK (“yAklb” – “O dog”), is split to I . Ê¿ AK (“yA klb”). Lastly, we normalized letter repetitions to allow for a maximum of 2 repeated letters. For example, the token éêêêë (“hhhhh” – “hahahahaha”) is normalized to éë (“hh”). We also removed Arabic diacritics and word elongations (kashida). 4.2 Representations Lexical Features Since offensive words typically have a negative polarity, we wanted to test the effectiveness of using a polarity lexicon in detecting offensive tweets. For the lexicon, we used NileULex (El-Beltagy, 2016), which is an Arabic polarity lexicon containing 3,279 MSA and 2,674 Egyptian terms, out of which 4,256 are negative and 1,697 are positive. We used the counts of terms with positive polarity and terms with negative polarity in tweets as features. 131 Static Embeddings We experimented with various static embeddings that were pre-trained on different corpora with different vector dimensionality. We compared pre-trained embeddings to embeddings that were trained on our dataset. For pre-trained embeddings, we used: fastText Egyptian Arabic pre-trained embeddings (Bojanowski et al., 2017) with vec"
2021.wanlp-1.13,P18-1031,0,0.0286332,"the agglutinative nature of the Arabic language. We trained a new fastText text classification model (Joulin et al., 2017) on our dataset with vectors of 40 dimensions, 0.5 learning rate, 2−10 character n-grams as features, for 30 epochs. These hyper-parameters were tuned using a 5-fold cross-validated grid-search. Deep Contextualized Embeddings We also experimented with pre-trained contextualized embeddings with fine-tuning for down-stream tasks. Recently, deep contextualized language models such as BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019), UMLFIT (Howard and Ruder, 2018), and OpenAI GPT (Radford et al., 2018), have achieved ground-breaking results in many NLP classification and language understanding tasks. In this paper, we fine-tuned BERTbase-multilingual (or simply BERT) and AraBERT embeddings to classify Arabic offensive language on Twitter as it eliminates the need for feature engineering. Although Robustly Optimized BERT (RoBERTa) embeddings perform better than (BERTlarge ) on GLUE (Wang et al., 2018), RACE (Lai et al., 2017), and SQuAD (Rajpurkar et al., 2016) tasks, pre-trained multilingual RoBERTa models are not available. BERT is pre-trained on Wiki"
2021.wanlp-1.13,E17-2068,0,0.0822972,"an Arabic pre-trained embeddings (Bojanowski et al., 2017) with vector dimensionality of 300; AraVec skip-gram embeddings (Mohammad et al., 2017), trained on 66.9M Arabic tweets with 100dimensional vectors; and Mazajak skip-gram embeddings (Abu Farha and Magdy, 2019), trained on 250M Arabic tweets with 300-dimensional vectors. Sentence embeddings were calculated by taking the mean of the embeddings of their tokens. The importance of testing a character level n-gram model like fastText lies in the agglutinative nature of the Arabic language. We trained a new fastText text classification model (Joulin et al., 2017) on our dataset with vectors of 40 dimensions, 0.5 learning rate, 2−10 character n-grams as features, for 30 epochs. These hyper-parameters were tuned using a 5-fold cross-validated grid-search. Deep Contextualized Embeddings We also experimented with pre-trained contextualized embeddings with fine-tuning for down-stream tasks. Recently, deep contextualized language models such as BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019), UMLFIT (Howard and Ruder, 2018), and OpenAI GPT (Radford et al., 2018), have achieved ground-breaking results in many NLP classifi"
2021.wanlp-1.13,malmasi-zampieri-2017-detecting,0,0.0532717,"Missing"
2021.wanlp-1.13,W17-3008,1,0.85101,"Missing"
2021.wanlp-1.13,D16-1264,0,0.0502861,"BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019), UMLFIT (Howard and Ruder, 2018), and OpenAI GPT (Radford et al., 2018), have achieved ground-breaking results in many NLP classification and language understanding tasks. In this paper, we fine-tuned BERTbase-multilingual (or simply BERT) and AraBERT embeddings to classify Arabic offensive language on Twitter as it eliminates the need for feature engineering. Although Robustly Optimized BERT (RoBERTa) embeddings perform better than (BERTlarge ) on GLUE (Wang et al., 2018), RACE (Lai et al., 2017), and SQuAD (Rajpurkar et al., 2016) tasks, pre-trained multilingual RoBERTa models are not available. BERT is pre-trained on Wikipedia text from 104 languages, and AraBERT is trained on a large Arabic news corpus containing 8.5M articles composed of roughly 2.5B tokens. Both use identical architectures and come with hundreds of millions of parameters. Both contain an encoder with 12 Transformer blocks, hidden size of 768, and 12 self-attention heads. These embedding use BP sub-word segments. Following Devlin et al. (2019), the classification consists of introducing a dense layer over the final hidden state h corresponding to fi"
2021.wanlp-1.13,K17-1043,1,0.897502,"Missing"
2021.wanlp-1.13,W18-5446,0,0.0297172,". Recently, deep contextualized language models such as BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019), UMLFIT (Howard and Ruder, 2018), and OpenAI GPT (Radford et al., 2018), have achieved ground-breaking results in many NLP classification and language understanding tasks. In this paper, we fine-tuned BERTbase-multilingual (or simply BERT) and AraBERT embeddings to classify Arabic offensive language on Twitter as it eliminates the need for feature engineering. Although Robustly Optimized BERT (RoBERTa) embeddings perform better than (BERTlarge ) on GLUE (Wang et al., 2018), RACE (Lai et al., 2017), and SQuAD (Rajpurkar et al., 2016) tasks, pre-trained multilingual RoBERTa models are not available. BERT is pre-trained on Wikipedia text from 104 languages, and AraBERT is trained on a large Arabic news corpus containing 8.5M articles composed of roughly 2.5B tokens. Both use identical architectures and come with hundreds of millions of parameters. Both contain an encoder with 12 Transformer blocks, hidden size of 768, and 12 self-attention heads. These embedding use BP sub-word segments. Following Devlin et al. (2019), the classification consists of introducing a"
2021.wanlp-1.13,N16-2013,0,0.0892398,"Missing"
2021.wanlp-1.13,S19-2010,0,0.10723,"Missing"
D14-1154,cotterell-callison-burch-2014-multi,0,0.4921,"Missing"
D14-1154,N12-1006,0,0.042694,"rb suffixes such as “yn” instead of “wn” and “wA” instead of “wn” respectively. Also, so-called “five nouns”, are used in only one form (ex. “&gt;bw” (father of) instead of “&gt;bA” or “&gt;by”). 4 Detecting Dialectal Peculiarities ARZ is different from MSA lexically, morphologically, phonetically, and syntactically. Here, we present methods to handle such peculiarities. We chose not to handle syntactic differences, because they may be captured using word n-gram models. To capture lexical variations, we extracted and sorted by frequency all the unigrams from the Egyptian side of the LDC2012T09 corpus (Zbib et al., 2012), which has ≈ 38k Egyptian-English parallel sentences. A linguist was tasked with manually reviewing the words from the top until 1,300 dialectal words were found. Some of the words on the list included dialectal words, commonly used foreign words, words that exhibit morphological variations, and others with letter substitution. 1466 For morphological phenomenon, we employed three methods, namely: • Unsupervised Morphology Induction: We employed the unsupervised morpheme segmentation tool, Morfessor (Virpioja et al., 2013). It is a data driven tool that automatically learns morphemes from data"
D14-1154,I11-1062,0,0.0283516,"Missing"
D14-1154,C12-1160,0,0.0521899,"Missing"
D14-1154,P11-2007,0,\N,Missing
D14-1154,J14-1006,0,\N,Missing
D14-1154,darwish-etal-2014-using,1,\N,Missing
D14-1154,P13-2081,0,\N,Missing
D19-3037,W12-2301,0,0.0839234,"Missing"
D19-3037,N07-2014,0,0.106619,"Missing"
D19-3037,N16-3003,1,0.913237,"two varieties of Dialectal Arabic (DA), namely Moroccan (MA) and Tunisian (TN). Our system beats all previously reported SOTA results for the aforementioned varieties of Arabic. The underlying approach treats diacritic recovery as a translation problem, where a sequential encoder and a sequential decoder are employed with undiacritized characters as input and diacritized characters as output. The system is composed of four main componenets, namely: 1) a web application that efficiently handles concurrent user diacritization requests; 2) a text tokenization and cleaning module based on Farasa (Abdelali et al., 2016), a SOTA Arabic NLP toolkit; 3) Arabic variety identifier based on a fastText (Joulin et al., 2016), a deep learning classification toolkit, to properly ascertain the appropriate diacritization model; and 4) a Neural Machine Translation (NMT) based architecture, based on OpenNMT (Klein et al., 2017), to translate sequences of undiacritized characters to diacritized sequences. The contributions in this paper are: Short vowels, aka diacritics, are more often omitted when writing different varieties of Arabic including Modern Standard Arabic (MSA), Classical Arabic (CA), and Dialectal Arabic (DA)"
D19-3037,W17-1305,0,0.0467909,"Missing"
D19-3037,P17-4012,0,0.0277426,"er are employed with undiacritized characters as input and diacritized characters as output. The system is composed of four main componenets, namely: 1) a web application that efficiently handles concurrent user diacritization requests; 2) a text tokenization and cleaning module based on Farasa (Abdelali et al., 2016), a SOTA Arabic NLP toolkit; 3) Arabic variety identifier based on a fastText (Joulin et al., 2016), a deep learning classification toolkit, to properly ascertain the appropriate diacritization model; and 4) a Neural Machine Translation (NMT) based architecture, based on OpenNMT (Klein et al., 2017), to translate sequences of undiacritized characters to diacritized sequences. The contributions in this paper are: Short vowels, aka diacritics, are more often omitted when writing different varieties of Arabic including Modern Standard Arabic (MSA), Classical Arabic (CA), and Dialectal Arabic (DA). However, diacritics are required to properly pronounce words, which makes diacritic restoration (a.k.a. diacritization) essential for language learning and text-to-speech applications. In this paper, we present a system for diacritizing MSA, CA, and two varieties of DA, namely Moroccan and Tunisia"
D19-3037,W18-2507,0,0.0275433,"Missing"
D19-3037,D15-1274,0,0.0528553,"Missing"
D19-3037,D17-1151,0,0.0215157,"Missing"
D19-3037,N19-1248,1,0.599941,"ieties of Arabic Hamdy Mubarak Ahmed Abdelali Kareem Darwish Mohamed Eldesouki Younes Samih Hassan Sajjad {hmubarak,aabdelali}@qf.org.qa Qatar Computing Research Institute, HBKU Research Complex, Doha 5825, Qatar Abstract the diacritics, a prerequisite for Language Learning (Asadi, 2017) and Text to Speech (Sherif, 2018) among other applications. In this paper, we present a system that employs a character-based sequence-to-sequence model (seq2seq) (Britz et al., 2017; Cho et al., 2014; Kuchaiev et al., 2018) for diacritizing four different varieties of Arabic. We use the approach described by Mubarak et al. (2019), which they applied to MSA only, to build a system that effectively diacritizes MSA, CA, and and two varieties of Dialectal Arabic (DA), namely Moroccan (MA) and Tunisian (TN). Our system beats all previously reported SOTA results for the aforementioned varieties of Arabic. The underlying approach treats diacritic recovery as a translation problem, where a sequential encoder and a sequential decoder are employed with undiacritized characters as input and diacritized characters as output. The system is composed of four main componenets, namely: 1) a web application that efficiently handles con"
D19-3037,pasha-etal-2014-madamira,0,0.0626661,"Missing"
D19-3037,W17-1302,1,0.833503,"Missing"
D19-3037,W04-1612,0,0.189762,"Missing"
D19-3037,W02-0504,0,0.274384,"Missing"
darwish-etal-2014-using,N03-1033,0,\N,Missing
darwish-etal-2014-using,P03-1051,0,\N,Missing
darwish-etal-2014-using,E12-1069,0,\N,Missing
E17-3016,E14-4029,1,0.83174,"f the systems.3 The results shown in Figure 3 depict the significant time gain we achieved using the pruned phrase based system. However, with a 5 BLEU point difference in translation quality, we decided to compromise and use the slower NMTCPU in our final demo. We also allow the user to switch to the phrase-based system, if translation speed is more important. We did not use NMTGPU since it is very costly to put into production with its requirement for a dedicated GPU card. Finally, we added a customized dictionary and translated unknown words by transliterating them in a post-decoding step (Durrani et al., 2014). PB-Pruned: The PB-best system is not suitable for real time translation and has high memory requirements. To increase the efficiency, we dropped the OSM and NNJM features, heavily pruned the language model and used MML-filtering to select a subset of training data. The resulting system was trained on 1.2 M sentences, 10 times less the original data. 2.4 NMT-GPU: This is our best system2 that we submitted to the IWSLT’16 campaign (Durrani et al., 2016). The advantage of Neural models is that their size does not scale linearly with the data, and hence we were able to train using all available"
E17-3016,W16-2323,0,0.0666475,"Missing"
E17-3016,P14-1129,0,0.018282,"eamlessly switch between them. We had four systems to choose from for our demo, two of which were Phrase-based systems, and the two were Neural MT systems trained using Nematus (Sennrich et al., 2016). Figure 3: Performance and Translation speed of various MT systems PB-Best: This is a competition-grade phrasebased system, also used for our participation at the IWSLT’16 campaign (Durrani et al., 2016). It was trained using all the freely available ArabicEnglish data with state-of-the-art features such as a large language model, lexical reordering, OSM (Durrani et al., 2011) and NNJM features (Devlin et al., 2014). We also computed the translation speed of each of the systems.3 The results shown in Figure 3 depict the significant time gain we achieved using the pruned phrase based system. However, with a 5 BLEU point difference in translation quality, we decided to compromise and use the slower NMTCPU in our final demo. We also allow the user to switch to the phrase-based system, if translation speed is more important. We did not use NMTGPU since it is very costly to put into production with its requirement for a dedicated GPU card. Finally, we added a customized dictionary and translated unknown words"
E17-3016,P11-1105,1,\N,Missing
K17-1043,N16-3003,1,0.913316,"suffixes for each dialect in comparison to MSA. As the tables show, MGR has the most number of prefixes, while GLF has the most number of suffixes. Further, there are certain prefixes and suffixes that are unique to dialects. While the prefix “Al” (the) leads the list of prefixes for all dialects, the prefix H . “b” in LEV and EGY, where it is either a progressive particle or a preposition, is used more frequently than in MSA, where it is used strictly as a preposition. Similarly, the suffix “kn” (your) is more frequent in LEV than any á» 5.1 We used the SVM-based ranking approach proposed by Abdelali et al. (2016), in which they used SVM based ranking to ascertain the best segmentation for Modern Standard Arabic (MSA), which they show to be fast and of high accuracy. The approach involves generating all possible segmentations of a word and then ranking them. The possible segmentations are generated based on possible prefixes and suffixes that are observed during training. For example, if hypothetically we only had the prefixes ð “w” (and) and È “l” (to) other dialect. The Negation suffix  “$” (not) and feminine suffix marker No. 8 11 11 14 19 Top 5 Al,w,l,b,f Al,b,w,m,h Al,b,w,l,E Al,w,b,l,mA Al,w,l,"
K17-1043,habash-etal-2012-conventional,0,0.0821494,"ained for each dialect and the number of words they contain. Dialect Egyptian Levantine Gulf Maghrebi No of Tokens 6,721 6,648 6,844 5,495 Table 1: Dataset size for the different dialects We manually segmented each word in the corpus while preserving the original characters. This decision was made to allow processing real dialectal words in their original form. Table 2 shows segmented examples from the different dialects. 3.1 Segmentation Convention In some research projects, segmentation of DA is done on a CODA’fied version of the text, where CODA is a standardized writing convention for DA (Habash et al., 2012). CODA guidelines provide directions on to how to normalize words, correct spelling and unify writing. Nonetheless, these guidelines are not available for all dialects. In the absence of such guidelines as well as the dynamic nature of the language, we choose to operate directly on the raw text. As in contrast to MSA, where guidelines for spelling are common and standardized, written DA seems to exhibit a lot of diversity, and hence, segmentation systems need to be robust enough to handle all the variants that might be encountered in such texts. Our segmentation convention is closer to stemmin"
K17-1043,W11-4417,1,0.839746,"Missing"
K17-1043,N13-1044,0,0.142459,"val. Though much work has focused on segmenting Modern Standard Arabic (MSA), recent work began to examine dialectal segmentation in some Arabic dialects. Dialectal segmentation is becoming increasingly important due to the ubiquity of social media, where users typically write in their own dialects as opposed to MSA. Dialectal text poses interesting challenges such as lack of spelling standards, pervasiveness of word merging, letter substitution or deletion, and foreign word borrowing. Existing work on dialectal segmentation focused on building resources and tools for each dialect separately (Habash et al., 2013; 2 Background Work on dialectal Arabic is fairly recent compared to MSA. A number of research projects were devoted to dialect identification (Biadsy et al., 2009; Zbib et al., 2012; Zaidan and Callison-Burch, 2014; Eldesouki et al., 2016). There are five major dialects including Egyptian, Gulf, Iraqi, Levantine and Maghrebi. Few resources for these dialects 432 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 432–441, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics are available such as the CALLHO"
K17-1043,W09-0807,0,0.0760766,"Missing"
K17-1043,bouamor-etal-2014-multidialectal,0,0.0679135,"Missing"
K17-1043,N16-1030,0,0.011125,"A+MSA). 5.2 Figure 2: Architecture of our proposed neural network Arabic segmentation model applied to the  word éJ.Ê¯ “qlbh” and output “qlb+h”. o and c are respectively the input gate, forget gate, output gate and cell activation vectors. More interpretation about this architecture can be found in (Graves and Schmidhuber, 2005) and(Lipton et al., 2015). Bi-LSTM-CRF Approach In this subsection we describe the different components of our Arabic segmentation bi-LSTMCRF based model, shown in Figure 2. It is a slight variant of the bi-LSTM-CRF architecture first proposed by Huang et al. (2015), Lample et al. (2016), and Ma and Hovy (2016) 5.2.1 Bi-LSTMs Another extension to the single LSTM networks are the bi-LSTMs (Schuster and Paliwal, 1997). They are also capable of learning long-term dependencies and maintain contextual features from both past and future states. As shown in Figure 2, they are comprised of two separate hidden layers that feed forwards to the same output layer. Recurrent Neural Networks A recurrent neural network (RNN) together with its variants, i.e. LSTM, bi-LSTM, GRU, belong to a family of powerful neural networks that are well suited for modeling sequential data. Over the last sev"
K17-1043,D14-1154,1,0.904598,"Missing"
K17-1043,W16-4828,1,0.82771,"social media, where users typically write in their own dialects as opposed to MSA. Dialectal text poses interesting challenges such as lack of spelling standards, pervasiveness of word merging, letter substitution or deletion, and foreign word borrowing. Existing work on dialectal segmentation focused on building resources and tools for each dialect separately (Habash et al., 2013; 2 Background Work on dialectal Arabic is fairly recent compared to MSA. A number of research projects were devoted to dialect identification (Biadsy et al., 2009; Zbib et al., 2012; Zaidan and Callison-Burch, 2014; Eldesouki et al., 2016). There are five major dialects including Egyptian, Gulf, Iraqi, Levantine and Maghrebi. Few resources for these dialects 432 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 432–441, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics are available such as the CALLHOME Egyptian Arabic Transcripts (LDC97T19), which was made available for research as early as 1997. Newly developed resources include the corpus developed by Bouamor et al. (2014), which contains 2,000 parallel sentences in multiple dialects"
K17-1043,P16-1101,0,0.0180724,"itecture of our proposed neural network Arabic segmentation model applied to the  word éJ.Ê¯ “qlbh” and output “qlb+h”. o and c are respectively the input gate, forget gate, output gate and cell activation vectors. More interpretation about this architecture can be found in (Graves and Schmidhuber, 2005) and(Lipton et al., 2015). Bi-LSTM-CRF Approach In this subsection we describe the different components of our Arabic segmentation bi-LSTMCRF based model, shown in Figure 2. It is a slight variant of the bi-LSTM-CRF architecture first proposed by Huang et al. (2015), Lample et al. (2016), and Ma and Hovy (2016) 5.2.1 Bi-LSTMs Another extension to the single LSTM networks are the bi-LSTMs (Schuster and Paliwal, 1997). They are also capable of learning long-term dependencies and maintain contextual features from both past and future states. As shown in Figure 2, they are comprised of two separate hidden layers that feed forwards to the same output layer. Recurrent Neural Networks A recurrent neural network (RNN) together with its variants, i.e. LSTM, bi-LSTM, GRU, belong to a family of powerful neural networks that are well suited for modeling sequential data. Over the last several years, they have ac"
K17-1043,maamouri-etal-2014-developing,0,0.0662827,"Missing"
K17-1043,mohamed-etal-2012-annotating,0,0.12055,"Missing"
K17-1043,P14-2034,0,0.0769493,"Missing"
K17-1043,W14-3601,1,0.936065,"Missing"
K17-1043,pasha-etal-2014-madamira,0,0.10814,"Missing"
K17-1043,W17-1306,1,0.586168,"Missing"
K17-1043,J14-1006,0,0.0520662,"important due to the ubiquity of social media, where users typically write in their own dialects as opposed to MSA. Dialectal text poses interesting challenges such as lack of spelling standards, pervasiveness of word merging, letter substitution or deletion, and foreign word borrowing. Existing work on dialectal segmentation focused on building resources and tools for each dialect separately (Habash et al., 2013; 2 Background Work on dialectal Arabic is fairly recent compared to MSA. A number of research projects were devoted to dialect identification (Biadsy et al., 2009; Zbib et al., 2012; Zaidan and Callison-Burch, 2014; Eldesouki et al., 2016). There are five major dialects including Egyptian, Gulf, Iraqi, Levantine and Maghrebi. Few resources for these dialects 432 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 432–441, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics are available such as the CALLHOME Egyptian Arabic Transcripts (LDC97T19), which was made available for research as early as 1997. Newly developed resources include the corpus developed by Bouamor et al. (2014), which contains 2,000 parallel sente"
K17-1043,N12-1006,0,0.0492725,"oming increasingly important due to the ubiquity of social media, where users typically write in their own dialects as opposed to MSA. Dialectal text poses interesting challenges such as lack of spelling standards, pervasiveness of word merging, letter substitution or deletion, and foreign word borrowing. Existing work on dialectal segmentation focused on building resources and tools for each dialect separately (Habash et al., 2013; 2 Background Work on dialectal Arabic is fairly recent compared to MSA. A number of research projects were devoted to dialect identification (Biadsy et al., 2009; Zbib et al., 2012; Zaidan and Callison-Burch, 2014; Eldesouki et al., 2016). There are five major dialects including Egyptian, Gulf, Iraqi, Levantine and Maghrebi. Few resources for these dialects 432 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 432–441, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics are available such as the CALLHOME Egyptian Arabic Transcripts (LDC97T19), which was made available for research as early as 1997. Newly developed resources include the corpus developed by Bouamor et al. (2014), wh"
L16-1054,W11-0705,0,0.156848,"Missing"
L16-1054,kirschenbaum-wintner-2010-general,0,0.0604887,"Missing"
L16-1054,P07-2045,0,0.00412632,"Missing"
L16-1054,W10-2404,0,0.0620325,"Missing"
L16-1054,W14-3601,1,0.875557,"Missing"
L16-1054,W15-3201,1,0.845594,"s available at http://alt. qcri.org/resources/TwitterAr2EnTranslit.tgz 9 Regions: Gulf (GLF), Egypt (EG), Levant (LEV), and Maghreb (MGR) Q»@ X (ZAkr)  @ (A$rf) ¬Qå ZAJ  (DyA’) ù®¢Ó (mSTfY) J ¯P (rfyq) ú G. QmÌ '@ (AlHrby) éJ Jë (hnyp) LEV Jamal Zaker Thaker Ashraf Achraf Diyaa Dhiyaa Mostafa Mostapha Rafik, Rafiq Rafig Rafic El Harby Al Harby Haniyya Haniyyeh Table 5: Samples of Arabic names that are transliterated differently according to regional dialectal variations. graphically, i.e. inferring a country or a region given only the full username written in Arabic on Latin characters (Mubarak and Darwish, 2015). 3.4. Transliteration Similarity Score Our hypothesis for name transliteration between N amearb and N ametrans needed a gauge to measure and quantify the similarity between them. Given a N amearb is transliterated using elaborate mapping scheme similar to Buckwalter transliteration. We took into consideration removing of name title, informal writings and dialectal variations, some characters are considered equivalent (ex: k=q, 353 gh=g, dh=d, sh= ch), vowels are removed from N amearb and N ametrans , and than similarity score is calculated using Levenshtein edit distance. For example, names"
L16-1054,P11-1044,0,0.0189049,"iNER), a large multilingual resource that is used for NE disambiguation, translation and transliteration. The resource contains lists of NEs with various sizes in 15 languages. They used triangulation cross languages to expand the initial lists. The size of the English list was 1.74 million entries. The numbers decrease sharply for nonWestern languages. Similarly, H´alek et, al. (2011) built a bilingual lexicons for English-Czech that was used to improve transliteration in a Statistical Machine Translation (SMT) task. Using the new mined resource improved the score with about 0.5 BLEU points. Sajjad et al. (2011; 2012) mined transliteration from parallel corpora to improve SMT system. Their unsupervised transliteration mining system uses a parallel corpus to generate a list of word pairs and filters transliteration pairs from that. The system will be retrained on the filtered dataset and this process is iterated several times until all transliteration word pairs were detected. The approach proved fruitful with a BLEU improvement of up to 0.4 points. Yoon et al. (2007) proposed a phonetic method for multilingual transliteration. The approach exploits the string alignment and linear classifiers that we"
L16-1054,P12-1049,0,0.0439081,"Missing"
L16-1054,W06-1630,0,0.032856,"o generate a list of word pairs and filters transliteration pairs from that. The system will be retrained on the filtered dataset and this process is iterated several times until all transliteration word pairs were detected. The approach proved fruitful with a BLEU improvement of up to 0.4 points. Yoon et al. (2007) proposed a phonetic method for multilingual transliteration. The approach exploits the string alignment and linear classifiers that were trained using the Winnow algorithm to learn transliteration characteristics. The results achieved were improved over earlier results reported by Tao et al. (2006). methods built using pure linguistic knowledge. Yoon et al. (2007) used Mean Reciprocal Rank (MRR) to measure the performance of the transliteration system tested on Arabic, Chinese, Hindi and Korean. The main challenges with former approaches is both unrobustness or dependability on scares resources that are not easy to find. Data collected from Twitter can expand rapidly and complement the resources in WK. 3. Collecting Names from Twitter When creating a new account on Twitter, user fills full name (in any characters; less than 20 characters), and an email. Twitter might suggest some user n"
L16-1054,wentland-etal-2008-building,0,0.0219146,"8 599.6 578.3 439.8 154.1 fr(k) de(k) es(k) ar(k) 907.2 469.6 397.1 133.1 857.4 340.6 120.9 699.1 136.8 233.2 his/her name. Hence, for our case-study, we proceed to collect full names written in Arabic with their transliterations using Twitter user ID (username field). Table 1: Statistics from WK using interwiki links for Named Entities translation/transliteration. 2. Related Work WK as a free multilingual encyclopedia, provides a valuable resource for parallel information that can be easily processed and deployed in cross-language Named Entity (NE) disambiguation, resolution and translation. Wentland et al. (2008) used WK to build Heidelberg NE Resource (HeiNER), a large multilingual resource that is used for NE disambiguation, translation and transliteration. The resource contains lists of NEs with various sizes in 15 languages. They used triangulation cross languages to expand the initial lists. The size of the English list was 1.74 million entries. The numbers decrease sharply for nonWestern languages. Similarly, H´alek et, al. (2011) built a bilingual lexicons for English-Czech that was used to improve transliteration in a Statistical Machine Translation (SMT) task. Using the new mined resource imp"
L16-1054,P07-1015,0,0.0283703,"teration in a Statistical Machine Translation (SMT) task. Using the new mined resource improved the score with about 0.5 BLEU points. Sajjad et al. (2011; 2012) mined transliteration from parallel corpora to improve SMT system. Their unsupervised transliteration mining system uses a parallel corpus to generate a list of word pairs and filters transliteration pairs from that. The system will be retrained on the filtered dataset and this process is iterated several times until all transliteration word pairs were detected. The approach proved fruitful with a BLEU improvement of up to 0.4 points. Yoon et al. (2007) proposed a phonetic method for multilingual transliteration. The approach exploits the string alignment and linear classifiers that were trained using the Winnow algorithm to learn transliteration characteristics. The results achieved were improved over earlier results reported by Tao et al. (2006). methods built using pure linguistic knowledge. Yoon et al. (2007) used Mean Reciprocal Rank (MRR) to measure the performance of the transliteration system tested on Arabic, Chinese, Hindi and Korean. The main challenges with former approaches is both unrobustness or dependability on scares resourc"
L16-1170,W11-4417,0,0.0705104,"Missing"
L16-1170,C96-1017,0,0.587632,"more accurate and much faster than the current stateof-the-art segmenters. • the introduction of a new test set to evaluate word segmentation. We plan to make this tool and data freely available. A demo of the segmenter is available online at: http://qatsdemo.cloudapp.net/farasa/ 2. Related Work Due to the morphological complexity of the Arabic language, morphological processing such as word segmentation helps recover the units of meaning or their proxies, such as stems (or perhaps roots). Most early Arabic morphological analyzers generally used finite state transducers (Beesley et al., 1989; Beesley, 1996; Kiraz, 1998). Their use is problematic for two reasons. First, they were designed to produce as many analyses as possible without indicating which analysis is most likely. This property of the analyzers complicate subsequent NLP as many applications require the most likely solution. Second, the use of finite state transducers inherently limits coverage, which is the number of words that the analyzer can analyze, to the cases programmed into the transducers. Other similar approaches attempt to find all possible prefix and suffix combinations in a word and then try to match the remaining 1070"
L16-1170,darwish-etal-2014-using,1,0.791867,"Missing"
L16-1170,W02-0506,1,0.22553,"Missing"
L16-1170,N13-1044,0,0.0208869,"Missing"
L16-1170,W98-1009,0,0.167865,"nd much faster than the current stateof-the-art segmenters. • the introduction of a new test set to evaluate word segmentation. We plan to make this tool and data freely available. A demo of the segmenter is available online at: http://qatsdemo.cloudapp.net/farasa/ 2. Related Work Due to the morphological complexity of the Arabic language, morphological processing such as word segmentation helps recover the units of meaning or their proxies, such as stems (or perhaps roots). Most early Arabic morphological analyzers generally used finite state transducers (Beesley et al., 1989; Beesley, 1996; Kiraz, 1998). Their use is problematic for two reasons. First, they were designed to produce as many analyses as possible without indicating which analysis is most likely. This property of the analyzers complicate subsequent NLP as many applications require the most likely solution. Second, the use of finite state transducers inherently limits coverage, which is the number of words that the analyzer can analyze, to the cases programmed into the transducers. Other similar approaches attempt to find all possible prefix and suffix combinations in a word and then try to match the remaining 1070 stem to a list"
L16-1170,P03-1051,0,0.0708448,"Missing"
L16-1170,P14-2034,0,0.0121578,"plifying assumption that word-context is not necessary to find the correct segmentation of a word. Though a word may have multiple valid segmentations, our results show that this assumption minimally impacts our results. We also report evaluation results on a new test set that we developed to measure segmentation accuracy. We also compare to other state-of-the-art segmenters, namely MADAMIRA (Pasha et al., 2014) and QCRI Advanced Tools For ARAbic (QATARA) (Darwish et al., 2014). 1 Buckwalter encoding is used exclusively in the paper. We also wanted to compare to the Stanford Arabic segmenter (Monroe et al., 2014), but its segmentation scheme was different from the one we used in Farasa. Many of the current results reported in the literature are done on subsets of the Penn Arabic Treebank (ATB) (Maamouri et al., 2005). Since the aforementioned tools are trained on the ATB, testing on a subset of the ATB is problematic due to its limited lexical diversity and the similarity between the training and test sets. This generally leads to results that are often artificially high. Our new dataset is composed of recent WikiNews articles from the years 2013 and 2014. The contributions of this paper are: • the de"
L16-1170,pasha-etal-2014-madamira,0,0.262305,"Missing"
L16-1170,2013.iwslt-evaluation.8,0,0.0183226,"Missing"
L18-1015,N16-3003,1,0.699404,"weet-specific POS tags Data Description Dialect Egyptian (EGY) Levantine (LEV) Gulf (GLF) Maghrebi (MGR) POS PROG PART https://catalog.ldc.upenn.edu/LDC2017T07 Buckwalter transliteration is used in the paper 94 tion is that MSA has more noun suffixes and grammatical case endings, while dialects have more progressive particles and negation suffixes. This variance is related more to the linguistic nature of the language rather than the genre. 4. 4.1. would be effective for dialects also, particularly given the overlap between MSA and dialectal Arabic. We used Farasa to determine stem templates (Abdelali et al., 2016). For all the experiments, we trained on the training and dev parts and tested on the test part. As mentioned earlier, we also randomly selected 350 MSA sentences from Arabic Penn Treebank (ATB) and treated MSA as a language variety. Doing so would allow us to observe the divergence of dialects from MSA and the relative effectiveness of using a small dataset compared to much more data. Experiments and Evaluation Experimental Setup For the experiments that we conducted, we used the CRF++ implementation of a CRF sequence labeler with L2 regularization and default value of 10 for the generalizati"
L18-1015,al-sabbagh-girju-2010-mining,0,0.0566271,"Missing"
L18-1015,bouamor-etal-2014-multidialectal,0,0.0463357,"Missing"
L18-1015,J92-4003,0,0.113623,"Missing"
L18-1015,cotterell-callison-burch-2014-multi,0,0.0466115,"Missing"
L18-1015,D14-1154,1,0.889503,"Missing"
L18-1015,W17-1316,1,0.769801,"ur justification for this noticeable disparity is that the POS distribution is affected by the genre. The MSA text is from the formal news domain with a special focus on facts and entities, while the dialects are informal expressions with a focus on events, attitudes, and conversations. Another observaThe words in the dataset were segmented in place without any modification or standardization attempts (ex. CODA (Habash et al., 2012)), and the segmentation guidelines aimed to generate a number of segments that match the correct number of POS tags for a word. We used the POS tagset described by Darwish et al. (2017) which has 18 tags for MSA POS tagging, and we added 2 dialect-specific tags (namely PROG PART, and NEG PART), and 4 tweet-specific tags (namely HASH, EMOT, MENTION, and URL). Table 1 contains description of the newly added tags5 . 4 Example I.JºJK . (bnktb) Segmentation and POS tagging were applied on the original raw text without any correction as suggested by Eldesouki et al. (2017) to overcome the need for standardization of different dialectal writings proposed in CODA by  ®J J.Óð Habash et al. (2012). For example the word ñËñ We used the dialectal Arabic dataset described by Eldesouk"
L18-1015,W05-0708,0,0.819563,"Missing"
L18-1015,P06-1086,0,0.314234,"Missing"
L18-1015,habash-etal-2012-conventional,0,0.027441,"prepositions, numbers, and definite articles appear more frequently in MSA than in dialects, while on the other hand dialects show higher frequency of verbs, pronouns and particles. Our justification for this noticeable disparity is that the POS distribution is affected by the genre. The MSA text is from the formal news domain with a special focus on facts and entities, while the dialects are informal expressions with a focus on events, attitudes, and conversations. Another observaThe words in the dataset were segmented in place without any modification or standardization attempts (ex. CODA (Habash et al., 2012)), and the segmentation guidelines aimed to generate a number of segments that match the correct number of POS tags for a word. We used the POS tagset described by Darwish et al. (2017) which has 18 tags for MSA POS tagging, and we added 2 dialect-specific tags (namely PROG PART, and NEG PART), and 4 tweet-specific tags (namely HASH, EMOT, MENTION, and URL). Table 1 contains description of the newly added tags5 . 4 Example I.JºJK . (bnktb) Segmentation and POS tagging were applied on the original raw text without any correction as suggested by Eldesouki et al. (2017) to overcome the need for"
L18-1015,N13-1044,0,0.508721,"Missing"
L18-1015,N13-1039,0,0.0374619,"Missing"
L18-1015,P08-2030,0,0.65644,"Missing"
L18-1015,W17-1306,1,0.871249,"or MSA POS tagging, and we added 2 dialect-specific tags (namely PROG PART, and NEG PART), and 4 tweet-specific tags (namely HASH, EMOT, MENTION, and URL). Table 1 contains description of the newly added tags5 . 4 Example I.JºJK . (bnktb) Segmentation and POS tagging were applied on the original raw text without any correction as suggested by Eldesouki et al. (2017) to overcome the need for standardization of different dialectal writings proposed in CODA by  ®J J.Óð Habash et al. (2012). For example the word ñËñ We used the dialectal Arabic dataset described by Eldesouki et al. (2017) and Samih et al. (2017b), which includes a set of 350 tweets for four major Arabic dialects that were manually segmented. The size of the dataset is as follows: No of Tweets 350 350 350 350 Description Progressive Part. Table 1: Dialect-specific and tweet-specific POS tags Data Description Dialect Egyptian (EGY) Levantine (LEV) Gulf (GLF) Maghrebi (MGR) POS PROG PART https://catalog.ldc.upenn.edu/LDC2017T07 Buckwalter transliteration is used in the paper 94 tion is that MSA has more noun suffixes and grammatical case endings, while dialects have more progressive particles and negation suffixes. This variance is rel"
L18-1015,K17-1043,1,0.86893,"or MSA POS tagging, and we added 2 dialect-specific tags (namely PROG PART, and NEG PART), and 4 tweet-specific tags (namely HASH, EMOT, MENTION, and URL). Table 1 contains description of the newly added tags5 . 4 Example I.JºJK . (bnktb) Segmentation and POS tagging were applied on the original raw text without any correction as suggested by Eldesouki et al. (2017) to overcome the need for standardization of different dialectal writings proposed in CODA by  ®J J.Óð Habash et al. (2012). For example the word ñËñ We used the dialectal Arabic dataset described by Eldesouki et al. (2017) and Samih et al. (2017b), which includes a set of 350 tweets for four major Arabic dialects that were manually segmented. The size of the dataset is as follows: No of Tweets 350 350 350 350 Description Progressive Part. Table 1: Dialect-specific and tweet-specific POS tags Data Description Dialect Egyptian (EGY) Levantine (LEV) Gulf (GLF) Maghrebi (MGR) POS PROG PART https://catalog.ldc.upenn.edu/LDC2017T07 Buckwalter transliteration is used in the paper 94 tion is that MSA has more noun suffixes and grammatical case endings, while dialects have more progressive particles and negation suffixes. This variance is rel"
L18-1015,W15-1511,0,0.189752,"Missing"
L18-1015,P11-2007,0,0.0736066,"Missing"
L18-1181,N16-3003,1,0.878192,"Missing"
L18-1181,C96-1017,0,0.631101,"Missing"
L18-1181,L16-1170,1,0.916274,"Missing"
L18-1181,W17-1302,1,0.897981,"Missing"
L18-1181,2003.mtsummit-semit.5,0,0.187077,"s having the same meaning. Arabic is the largest Semitic language spoken by almost 300 million people. It’s one of the six official languages in the United Nations, and the fifth most widely spoken language after Chinese, Spanish, English, and Hindi1 . Lemmatization is an important preprocessing step for many applications of text mining and question-answering systems. Researches in Arabic Information Retrieval (IR) systems show the need for representing Arabic words at lemma level for many applications, including keyphrase extraction (El-Shishtawy and Al-Sammak, 2009) and Machine Translation (Dichy and Fargaly, 2003). In addition, lemmatization provides a productive way to generate generic keywords for search engines (SE) or labels for concept maps (Plisson et al., 2004). Arabic has a very rich morphology, both derivational and inflectional. Generally, Arabic words are derived from a root that uses three or more consonants to define a broad meaning or concept, and they follow some templatic  P@ ñÖÏ @). By adding morphological patterns ( éJ ¯QåË@ áK vowels, prefixes and suffixes to the root, word inflections J®J ð (wsyftHare generated. For instance, the word AîEñj wnhA)2 “and they will open it” has the"
L18-1181,pasha-etal-2014-madamira,0,0.0876612,"Missing"
L18-1620,N16-3003,1,0.859153,"was considered as a class, and a set of features mentioned at the end of the section were extracted for each clitic and used to train the SVM classifier. In this work we use a combination of features that includes probabilistic, binary, and Arabic-specific features. For probabilistic features we used a combination of bigrams, trigrams, and 4grams of tags and clitics. For binary features we used some features including meta-types of clitics, which indicate if a clitic is a number, a foreign word, a user mention or a URL. For Arabic specific features, we used stem template feature introduced by Abdelali et al. (2016). Where stem template represents the word pattern applied to the root mentioned in section 2.1 . The template for each clitic has been extracted and concatenated to word representation. The set of used features for SVM are: 1. Clitic features: each unique clitic in our training set acted as a feature, and an additional feature is added to represent out-of-vocabulary (OOV) clitics. We experimented with three different values for clitic features. The first value is binary (whether it exists or not). The second is the log of clitic counts in training data. The 3927 third is the Term Frequency-Inv"
L18-1620,W17-1316,1,0.896696,"Approach, Maximum Entropy Approach, Support Vector Machine(SVM) Approach and Neural Network Approach (Wilks, 1996). In this section we present our POS tagging approach; first we describe the set of features we extracted, then we discuss the two machine learning approaches we used, which are SVM and Bi-LSTM. It is worth mentioning that our taggers operate at clitic level instead of word level where a clitic is a word segment that has single POS tag. 3.1. SVM Based POS Tagger SVM is used in many NLP classification tasks including POS tagging and proves to achieve high accuracy results with MSA (Darwish et al., 2017; Gim´enez and M`arquez, 2003). For this work, we used an SVM multi-class, specifically the SVMmulticlass tool developed by Thorsten Joachims (Joachims, 2008). SVMmulticlass uses regularization parameter C to prevent overfitting (Manning et al., 2009). Each tag of POS tags was considered as a class, and a set of features mentioned at the end of the section were extracted for each clitic and used to train the SVM classifier. In this work we use a combination of features that includes probabilistic, binary, and Arabic-specific features. For probabilistic features we used a combination of bigrams"
L18-1620,N07-5003,0,0.0268657,"(Khalifa et al., 2016). • Morphologically: In most cases, there is no case inflection on GA words. Also, the prefix H . [ba] and • DA words are written as they are pronounced since there is no orthographic standards for dialects. This fact causes inconsistency in writing some words for  example the word Y [sQ Idq], which means ’truth’ 2 IPA is used to present Arabic words phonetically  [q] has different pronunciations e.g. ÈAg. [dZa:l] and ÈA¿ [ka:l] ] which means over, the sound • Word order: in dialects it is usually Subject-VerbObject (SVO) while it is Verb-Subject-Object (VSO) in MSA(Diab and Habash, 2007). is written as l .  [sQ IdZ] in some Gulf dialects variants. Another result of writing words as they are pronounced is that some letters are dropped when pro nounced. For example the word Y«A¯[qa:QId], which ©ËA£ [tQ a:lIQ], which means ’look’ is written as ¨A£ [tQ a:Q] in Kuwaiti Gulf dialect. Éªm.&apos; [najaQl] is the stem and each one of these segments is called clitic. For more explanation see (Darwish and Magdy, 2014; Habash, 2010). Researchers usually consider five main dialects for DA, namely: Egyptian, Iraqi, Levantine, Maghribi, and Gulf (Samih et al., 2017). Although Gulf Arabic is"
L18-1620,W05-0708,0,0.743529,"Missing"
L18-1620,P06-1086,0,0.326157,"Missing"
L18-1620,habash-etal-2012-conventional,0,0.01948,"eir grammars while dialects have no strict rules. In this paper, we focus our study on GA, which is one group of dialects that share many characteristics. It is the dialect of countries surrounding the Arab Gulf, such as Saudi Arabia, Kuwait, Qatar, Bahrain, Oman, United Arab Emirates and Iraq. GA has additional characteristics that distinguish it from other dialects, for example: • Phonologically: GA maintains the pronunciation of : X [D], H [T] and [DQ ] unlike other dialects. More• Vocabulary: Arabic dialects have richer vocabulary than MSA some of which are borrowed from other languages (Habash et al., 2012a). ÈA¯ [qa:l] , ’he said’(Khalifa et al., 2016). • Morphologically: In most cases, there is no case inflection on GA words. Also, the prefix H . [ba] and • DA words are written as they are pronounced since there is no orthographic standards for dialects. This fact causes inconsistency in writing some words for  example the word Y [sQ Idq], which means ’truth’ 2 IPA is used to present Arabic words phonetically  [q] has different pronunciations e.g. ÈAg. [dZa:l] and ÈA¿ [ka:l] ] which means over, the sound • Word order: in dialects it is usually Subject-VerbObject (SVO) while it is Verb-S"
L18-1620,W12-2301,0,0.345569,"eir grammars while dialects have no strict rules. In this paper, we focus our study on GA, which is one group of dialects that share many characteristics. It is the dialect of countries surrounding the Arab Gulf, such as Saudi Arabia, Kuwait, Qatar, Bahrain, Oman, United Arab Emirates and Iraq. GA has additional characteristics that distinguish it from other dialects, for example: • Phonologically: GA maintains the pronunciation of : X [D], H [T] and [DQ ] unlike other dialects. More• Vocabulary: Arabic dialects have richer vocabulary than MSA some of which are borrowed from other languages (Habash et al., 2012a). ÈA¯ [qa:l] , ’he said’(Khalifa et al., 2016). • Morphologically: In most cases, there is no case inflection on GA words. Also, the prefix H . [ba] and • DA words are written as they are pronounced since there is no orthographic standards for dialects. This fact causes inconsistency in writing some words for  example the word Y [sQ Idq], which means ’truth’ 2 IPA is used to present Arabic words phonetically  [q] has different pronunciations e.g. ÈAg. [dZa:l] and ÈA¿ [ka:l] ] which means over, the sound • Word order: in dialects it is usually Subject-VerbObject (SVO) while it is Verb-S"
L18-1620,L16-1679,0,0.217841,"s. In this paper, we focus our study on GA, which is one group of dialects that share many characteristics. It is the dialect of countries surrounding the Arab Gulf, such as Saudi Arabia, Kuwait, Qatar, Bahrain, Oman, United Arab Emirates and Iraq. GA has additional characteristics that distinguish it from other dialects, for example: • Phonologically: GA maintains the pronunciation of : X [D], H [T] and [DQ ] unlike other dialects. More• Vocabulary: Arabic dialects have richer vocabulary than MSA some of which are borrowed from other languages (Habash et al., 2012a). ÈA¯ [qa:l] , ’he said’(Khalifa et al., 2016). • Morphologically: In most cases, there is no case inflection on GA words. Also, the prefix H . [ba] and • DA words are written as they are pronounced since there is no orthographic standards for dialects. This fact causes inconsistency in writing some words for  example the word Y [sQ Idq], which means ’truth’ 2 IPA is used to present Arabic words phonetically  [q] has different pronunciations e.g. ÈAg. [dZa:l] and ÈA¿ [ka:l] ] which means over, the sound • Word order: in dialects it is usually Subject-VerbObject (SVO) while it is Verb-Subject-Object (VSO) in MSA(Diab and Habash, 2007)"
L18-1620,D15-1176,0,0.0960929,"Missing"
L18-1620,pasha-etal-2014-madamira,0,0.305784,"Missing"
L18-1620,P16-2067,0,0.0215024,". È@ [al] determiner. We also include meta-type feature which is an additional information added about the type of clitic i.e. to specify whether it is a number, an adjective number, a prefix, a suffix, a foreign, a punctuation, an Arabic letter and twitter specific types: hashtags, URLs and mentions. 4. 4.1. Bi-LSTM Based POS Tagger Bi-LSTM is a special type of Recurrent Neural Network (RNN). It has proved to be a good choice for sequence modeling tasks (Ling et al., 2015) such as speech processing, POS tagging, phrased based chunking ... etc. It is also less sensitive to training data size (Plank et al., 2016). Moreover, Bi-LSTM can capture the context around source words up to very long sequences in both directions (previous and upfront) (Wang et al., 2015). It also does not need hand crafted features to work well. These characteristics make it a suitable fit for POS tagging of DA. Since there is not much training data available for DA – GA in this case – and since DA lacks standards to design powerful features, a model is needed that auto-fits its features and characteristics. Bi-LSTM structure differs from the classic RNN in that it adds a memory cell to the neural network architecture that lear"
L18-1620,K17-1043,1,0.910025,"ct-Object (VSO) in MSA(Diab and Habash, 2007). is written as l .  [sQ IdZ] in some Gulf dialects variants. Another result of writing words as they are pronounced is that some letters are dropped when pro nounced. For example the word Y«A¯[qa:QId], which ©ËA£ [tQ a:lIQ], which means ’look’ is written as ¨A£ [tQ a:Q] in Kuwaiti Gulf dialect. Éªm.&apos; [najaQl] is the stem and each one of these segments is called clitic. For more explanation see (Darwish and Magdy, 2014; Habash, 2010). Researchers usually consider five main dialects for DA, namely: Egyptian, Iraqi, Levantine, Maghribi, and Gulf (Samih et al., 2017). Although Gulf Arabic is the largest existing dialect in social media, there is very limited attention towards building NLP tools for it. DA is derived from MSA; nevertheless, they differ at many linguistic levels. Some notable differences are in terms of: ¨A¯ [qa:Q], and the the verb h@P [raaè] are used to indicate future tense. In addition, the words I . Ó [mub], H. ñÓ [mob], AÓ [ma:], ñëAÓ [ma:hu] and H . ñëAÓ [ma:hu:b] are used for negation (Khalifa et al., 2016). These differences emphasize the need for specially designed NLP tools for dialects to prevent the performance drop when using"
N16-3003,W11-4417,0,0.0982575,"Missing"
N16-3003,C96-1017,0,0.288953,"Missing"
N16-3003,N12-1047,0,0.014351,"BLEU scores and Time (in seconds) 202K Sentences) to train phrase-based systems. Systems: We used Moses (Koehn et al., 2007), a state-of-the-art toolkit with the the settings described in (Durrani et al., 2014a): these include a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), used at runtime, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007) using a stack size of 1,000 during tuning and 5,000 during testing. We tuned with the k-best batch MIRA (Cherry and Foster, 2012). Among other features, we used lexicalized reordering model (Galley and Manning, 2008), a 5-gram Operation Sequence Model (Durrani et al., 2011), Class-based Models (Durrani et al., 2014b)4 and other default parameters. We used an unsupervised transliteration model (Durrani et al., 2014c) to transliterate the OOV words. We used the standard tune and test set provided by the IWSLT shared task to evaluate the systems. In each experiment, we simply changed the segmentation pipeline to try different segmentation. We used ATB scheme for MADAMIRA which has shown to outperform its alternatives (S2 a"
N16-3003,darwish-etal-2014-using,1,0.880454,"Missing"
N16-3003,W02-0506,1,0.190376,"Missing"
N16-3003,P11-1105,1,0.385324,"olkit with the the settings described in (Durrani et al., 2014a): these include a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), used at runtime, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007) using a stack size of 1,000 during tuning and 5,000 during testing. We tuned with the k-best batch MIRA (Cherry and Foster, 2012). Among other features, we used lexicalized reordering model (Galley and Manning, 2008), a 5-gram Operation Sequence Model (Durrani et al., 2011), Class-based Models (Durrani et al., 2014b)4 and other default parameters. We used an unsupervised transliteration model (Durrani et al., 2014c) to transliterate the OOV words. We used the standard tune and test set provided by the IWSLT shared task to evaluate the systems. In each experiment, we simply changed the segmentation pipeline to try different segmentation. We used ATB scheme for MADAMIRA which has shown to outperform its alternatives (S2 and D3) previously (Sajjad et al., 2013). Results: Table 2 compares the Arabic-to-English SMT systems using the three segmentation tools. Farasa p"
N16-3003,W14-3309,1,0.823799,"ord and MADAMIRA3 . The comparison was done in terms of BLEU (Papineni et al., 2002) and processing times. We used concatenation of IWSLT TED talks (Cettolo et al., 2014) (containing 183K Sentences) and NEWS corpus (containing 3 Release-01292014-1.0 was used in the experiments 13 iwslt12 iwslt13 Avg Time 30.4 30.0 30.2 30.8 30.5 30.8 30.6 30.3 30.5 4074 395 80 Table 2: Arabic-to-English Machine Translation, BLEU scores and Time (in seconds) 202K Sentences) to train phrase-based systems. Systems: We used Moses (Koehn et al., 2007), a state-of-the-art toolkit with the the settings described in (Durrani et al., 2014a): these include a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), used at runtime, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007) using a stack size of 1,000 during tuning and 5,000 during testing. We tuned with the k-best batch MIRA (Cherry and Foster, 2012). Among other features, we used lexicalized reordering model (Galley and Manning, 2008), a 5-gram Operation Sequence Model (Durrani et al., 2011), Class-based Models (Durrani et al., 201"
N16-3003,E14-4029,1,0.444802,"ord and MADAMIRA3 . The comparison was done in terms of BLEU (Papineni et al., 2002) and processing times. We used concatenation of IWSLT TED talks (Cettolo et al., 2014) (containing 183K Sentences) and NEWS corpus (containing 3 Release-01292014-1.0 was used in the experiments 13 iwslt12 iwslt13 Avg Time 30.4 30.0 30.2 30.8 30.5 30.8 30.6 30.3 30.5 4074 395 80 Table 2: Arabic-to-English Machine Translation, BLEU scores and Time (in seconds) 202K Sentences) to train phrase-based systems. Systems: We used Moses (Koehn et al., 2007), a state-of-the-art toolkit with the the settings described in (Durrani et al., 2014a): these include a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), used at runtime, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007) using a stack size of 1,000 during tuning and 5,000 during testing. We tuned with the k-best batch MIRA (Cherry and Foster, 2012). Among other features, we used lexicalized reordering model (Galley and Manning, 2008), a 5-gram Operation Sequence Model (Durrani et al., 2011), Class-based Models (Durrani et al., 201"
N16-3003,N13-1073,0,0.0288181,". We used concatenation of IWSLT TED talks (Cettolo et al., 2014) (containing 183K Sentences) and NEWS corpus (containing 3 Release-01292014-1.0 was used in the experiments 13 iwslt12 iwslt13 Avg Time 30.4 30.0 30.2 30.8 30.5 30.8 30.6 30.3 30.5 4074 395 80 Table 2: Arabic-to-English Machine Translation, BLEU scores and Time (in seconds) 202K Sentences) to train phrase-based systems. Systems: We used Moses (Koehn et al., 2007), a state-of-the-art toolkit with the the settings described in (Durrani et al., 2014a): these include a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), used at runtime, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007) using a stack size of 1,000 during tuning and 5,000 during testing. We tuned with the k-best batch MIRA (Cherry and Foster, 2012). Among other features, we used lexicalized reordering model (Galley and Manning, 2008), a 5-gram Operation Sequence Model (Durrani et al., 2011), Class-based Models (Durrani et al., 2014b)4 and other default parameters. We used an unsupervised transliteration model (Durrani et al., 2014c"
N16-3003,eisele-chen-2010-multiun,0,0.0434022,"Missing"
N16-3003,D08-1089,0,0.0253633,"ems: We used Moses (Koehn et al., 2007), a state-of-the-art toolkit with the the settings described in (Durrani et al., 2014a): these include a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), used at runtime, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007) using a stack size of 1,000 during tuning and 5,000 during testing. We tuned with the k-best batch MIRA (Cherry and Foster, 2012). Among other features, we used lexicalized reordering model (Galley and Manning, 2008), a 5-gram Operation Sequence Model (Durrani et al., 2011), Class-based Models (Durrani et al., 2014b)4 and other default parameters. We used an unsupervised transliteration model (Durrani et al., 2014c) to transliterate the OOV words. We used the standard tune and test set provided by the IWSLT shared task to evaluate the systems. In each experiment, we simply changed the segmentation pipeline to try different segmentation. We used ATB scheme for MADAMIRA which has shown to outperform its alternatives (S2 and D3) previously (Sajjad et al., 2013). Results: Table 2 compares the Arabic-to-Englis"
N16-3003,P12-1016,0,0.0135218,"e that each of the segmenters took to process the entire document collection. As can be seen from the results, Farasa outperformed using words, MADAMIRA, and Stanford significantly. Farasa was an order of magnitude faster than Stanford and two orders of magnitude faster than MADAMIRA. 5 Analysis The major advantage of using Farasa is speed, without loss in accuracy. This mainly results from optimization described earlier in the Section 2 which includes caching and limiting the context used for building the features vector. Stanford segmenter uses a third-order (i.e., 4-gram) Markov CRF model (Green and DeNero, 2012) to predict the correct segmentation. On the other hand, MADAMIRA bases its segmentation on the output of a morphological analyzer which provides a list of possible analyses (independent of context) for each word. Both text and analyses are passed to a feature modeling component, which applies SVM and language models to derive predictions for the word segmentation (Pasha et al., 2014). This hierarchy could explain the slowness of MADAMIRA versus other tokenizers. 6 Conclusion In this paper we introduced Farasa, a new Arabic segmenter, which uses SVM for ranking. We compared our segmenter with"
N16-3003,W11-2123,0,0.0161867,"es) and NEWS corpus (containing 3 Release-01292014-1.0 was used in the experiments 13 iwslt12 iwslt13 Avg Time 30.4 30.0 30.2 30.8 30.5 30.8 30.6 30.3 30.5 4074 395 80 Table 2: Arabic-to-English Machine Translation, BLEU scores and Time (in seconds) 202K Sentences) to train phrase-based systems. Systems: We used Moses (Koehn et al., 2007), a state-of-the-art toolkit with the the settings described in (Durrani et al., 2014a): these include a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), used at runtime, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007) using a stack size of 1,000 during tuning and 5,000 during testing. We tuned with the k-best batch MIRA (Cherry and Foster, 2012). Among other features, we used lexicalized reordering model (Galley and Manning, 2008), a 5-gram Operation Sequence Model (Durrani et al., 2011), Class-based Models (Durrani et al., 2014b)4 and other default parameters. We used an unsupervised transliteration model (Durrani et al., 2014c) to transliterate the OOV words. We used the standard tune and test set provided by th"
N16-3003,P07-1019,0,0.0266176,"iwslt12 iwslt13 Avg Time 30.4 30.0 30.2 30.8 30.5 30.8 30.6 30.3 30.5 4074 395 80 Table 2: Arabic-to-English Machine Translation, BLEU scores and Time (in seconds) 202K Sentences) to train phrase-based systems. Systems: We used Moses (Koehn et al., 2007), a state-of-the-art toolkit with the the settings described in (Durrani et al., 2014a): these include a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), used at runtime, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007) using a stack size of 1,000 during tuning and 5,000 during testing. We tuned with the k-best batch MIRA (Cherry and Foster, 2012). Among other features, we used lexicalized reordering model (Galley and Manning, 2008), a 5-gram Operation Sequence Model (Durrani et al., 2011), Class-based Models (Durrani et al., 2014b)4 and other default parameters. We used an unsupervised transliteration model (Durrani et al., 2014c) to transliterate the OOV words. We used the standard tune and test set provided by the IWSLT shared task to evaluate the systems. In each experiment, we simply changed the segment"
N16-3003,P07-2045,0,0.0264015,"l Machine Translation (SMT) systems for Arabic↔English, to compare Farasa with Stanford and MADAMIRA3 . The comparison was done in terms of BLEU (Papineni et al., 2002) and processing times. We used concatenation of IWSLT TED talks (Cettolo et al., 2014) (containing 183K Sentences) and NEWS corpus (containing 3 Release-01292014-1.0 was used in the experiments 13 iwslt12 iwslt13 Avg Time 30.4 30.0 30.2 30.8 30.5 30.8 30.6 30.3 30.5 4074 395 80 Table 2: Arabic-to-English Machine Translation, BLEU scores and Time (in seconds) 202K Sentences) to train phrase-based systems. Systems: We used Moses (Koehn et al., 2007), a state-of-the-art toolkit with the the settings described in (Durrani et al., 2014a): these include a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), used at runtime, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007) using a stack size of 1,000 during tuning and 5,000 during testing. We tuned with the k-best batch MIRA (Cherry and Foster, 2012). Among other features, we used lexicalized reordering model (Galley and Manning, 2008), a 5-gram Ope"
N16-3003,N04-1022,0,0.0119473,"14-1.0 was used in the experiments 13 iwslt12 iwslt13 Avg Time 30.4 30.0 30.2 30.8 30.5 30.8 30.6 30.3 30.5 4074 395 80 Table 2: Arabic-to-English Machine Translation, BLEU scores and Time (in seconds) 202K Sentences) to train phrase-based systems. Systems: We used Moses (Koehn et al., 2007), a state-of-the-art toolkit with the the settings described in (Durrani et al., 2014a): these include a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), used at runtime, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007) using a stack size of 1,000 during tuning and 5,000 during testing. We tuned with the k-best batch MIRA (Cherry and Foster, 2012). Among other features, we used lexicalized reordering model (Galley and Manning, 2008), a 5-gram Operation Sequence Model (Durrani et al., 2011), Class-based Models (Durrani et al., 2014b)4 and other default parameters. We used an unsupervised transliteration model (Durrani et al., 2014c) to transliterate the OOV words. We used the standard tune and test set provided by the IWSLT shared task to evaluate the systems. In each ex"
N16-3003,P14-2034,0,0.070743,"Missing"
N16-3003,P02-1040,0,0.119887,"Missing"
N16-3003,pasha-etal-2014-madamira,0,0.166971,"Missing"
N16-3003,2014.iwslt-evaluation.6,1,\N,Missing
N16-3003,2013.iwslt-evaluation.8,1,\N,Missing
N19-1248,D15-1274,0,0.212375,"Missing"
N19-1248,D17-1151,0,0.0558737,"Missing"
N19-1248,W17-1302,1,0.866003,"Missing"
N19-1248,W02-0504,0,0.868168,"Missing"
N19-1248,P05-1071,0,0.215534,"Missing"
N19-1248,N07-2014,0,0.680665,"Missing"
N19-1248,P17-4012,0,0.0215192,"31 2.37 1.99 3.03 2.05 5.97 3.57 3.07 3.93 3.04 7.79 5.49 4.77 6.40 4.77 2.01 1.49 1.30 1.78 1.29 0.00 0.00 0.00 0.00 0.00 12 Combination ∗ 09 +† 11 1.89 2.89 4.49 1.21 0.00 Table 3: Diacritization results: *g represents ngram size e.g. 7g means 7-gram context. Experiment 09 and 11 are comparing NMT models – LSTM-based architecture with attention mechanism and Transformer model Setup and dropout rate = 0.3. The setting for the Transformer were: 6 encoder and 6 decoder layers each of size 512; number of attention heads = 8; feed forward dimension = 2048; and dropout = 0.1. We used the OpenNMT (Klein et al., 2017) implementation with tensorflow for all experiments. System Runs. We conducted a variety of experiments as follows, namely: Word-level experiments where the input is a sequence of words and the output is a sequence of diacritized words: – Baseline Word: uses the full sentences and shows the deficiency of using NMT directly. – Word 7g: uses non-overlapping windows of 7 words to compare to our best character-level model, which also uses a window of length 7. – Word 7g+overlap: uses a sliding window of 7 words. Character-level experiments where the input is represented as a sequence of character"
N19-1248,W18-2507,0,0.137075,"Missing"
N19-1248,W05-0711,0,0.22455,"Missing"
N19-1248,pasha-etal-2014-madamira,0,0.446003,"Missing"
N19-1248,W04-1612,0,0.362427,"Missing"
N19-1248,P06-1073,0,0.232166,"Missing"
N19-1248,E17-2060,0,0.0197172,"not fit in our scenario as it may create source and target segments of different lengths. In the Arabic diacritization problem, both source and target words and characters are strictly tied to each other and loosening it would result in sub-optimal performance and may generate unexpected errors. Context Window. The diacritization of Arabic words is highly sensitive to context. Character representations significantly increase the size of the source and target sequences. This leads to a well known limitation of character-based LSTM-based models, namely poor handling of long range dependencies (Sennrich, 2017). An easy fix is to split sentences greater than a certain length into multiple lines. However, boundary words may loose context in the newly created sequences. To handle this, we propose to keep a fixed size context window c for every word. Given a sentence, we use a sliding context window to split it into segments of overlapping windows of size c as in Table 1. This fixes the problems of both long range dependencies and context of neighboring words. We are further aided by the fact that local context can conclusively determine the correct diacritization in the vast majority of cases. Voting."
N19-1248,P16-1162,0,0.0460472,"split into a sequence of subword units each consisting of a letter and its diacritic(s). For example, source word “AlElm” would be represented as “A/l/E/l/m” and its diacritized target “AaloEalamu” as “Aa/lo/Ea/la/mu”. The character-level representation has several benefits, such as reducing the vocabulary size and avoiding OOV words. The splitting of diacritized 2391 words into subword units simplifies the problem as there will be identical number of source and target tokens in a parallel sentence. Later, we support our design decisions with results in the experiments section. Subwords (BPE (Sennrich et al., 2016)) have been used as a defacto standard in building NMT systems. They are a natural choice to handle unknown words. However, BPE does not fit in our scenario as it may create source and target segments of different lengths. In the Arabic diacritization problem, both source and target words and characters are strictly tied to each other and loosening it would result in sub-optimal performance and may generate unexpected errors. Context Window. The diacritization of Arabic words is highly sensitive to context. Character representations significantly increase the size of the source and target sequ"
S15-2036,P14-1023,0,0.00431346,"entence node; finally, all root sentence nodes are linked to a super root for all sentences in the question/comment. 2.1.3 Semantic Similarity We apply three approaches to build wordembedding vector representations, using (i) latent semantic analysis (Croce and Previtali, 2010), trained on the Qatar Living corpus with a word co-occurrence window of size ±3 and producing a vector of 250 dimensions with SVD (we produced a vector for each noun in the vocabulary); (ii) GloVe (Pennington et al., 2014), using a model pre-trained on Common Crawl (42B tokens), with 300 dimensions; and (iii) COMPOSES (Baroni et al., 2014), using previously-estimated predict vectors of 400 dimensions.3 We represent both q and c as a sum of the vectors corresponding to the words within them (neglecting the subject of c). We compute the cosine similarity to estimate sim(q, c). We also experimented with word2vec (Mikolov et al., 2013) vectors pre-trained with both cbow and skipgram on news data, and also with both word2vec and GloVe vectors trained on Qatar Living data, but we discarded them as they did not help us on top of all other features we had. 2.2 Context Comments are organized sequentially according to the time line of th"
S15-2036,W10-2802,0,0.0170168,"tial tree kernel (Moschitti, 2006) to calculate the similarity between the question and the comment based on their corresponding shallow syntactic trees. These trees have word lemmata as leaves, then there is a POS tag node parent for each lemma leaf, and POS tag nodes are in turn grouped under shallow parsing chunks, which are linked to a root sentence node; finally, all root sentence nodes are linked to a super root for all sentences in the question/comment. 2.1.3 Semantic Similarity We apply three approaches to build wordembedding vector representations, using (i) latent semantic analysis (Croce and Previtali, 2010), trained on the Qatar Living corpus with a word co-occurrence window of size ±3 and producing a vector of 250 dimensions with SVD (we produced a vector for each noun in the vocabulary); (ii) GloVe (Pennington et al., 2014), using a model pre-trained on Common Crawl (42B tokens), with 300 dimensions; and (iii) COMPOSES (Baroni et al., 2014), using previously-estimated predict vectors of 400 dimensions.3 We represent both q and c as a sum of the vectors corresponding to the words within them (neglecting the subject of c). We compute the cosine similarity to estimate sim(q, c). We also experimen"
S15-2036,W01-0515,0,0.415394,"rule-based. 2.1 Similarity Measures The similarity features measure the similarity sim(q, c) between the question and a target comment, assuming that high similarity signals a GOOD answer. We consider three kinds of similarity measures, which we describe below. 2.1.1 Lexical Similarity We compute the similarity between word n-gram representations (n = [1, . . . , 4]) of q and c, using the following lexical similarity measures (after stopword removal): greedy string tiling (Wise, 1996), longest common subsequences (Allison and Dix, 1986), Jaccard coefficient (Jaccard, 1901), word containment (Lyon et al., 2001), and cosine similarity. We further compute cosine on lemmata and POS tags, either including stopwords or not. We also use similarity measures, which weigh the terms using the following three formulæ: X sim(q, c) = idf (t) (1) t∈q∩c sim(q, c) = X t∈q∩c sim(q, c) = X t∈q∩c log(idf (t)) (2)   |C| log 1 + tf (t) (3) where idf (t) is the inverse document frequency (Sparck Jones, 1972) of term t in the entire Qatar Living dataset, C is the number of comments in this collection, and tf (t) is the term frequency of the term in the comment. Equations 2 and 3 are variations of idf; cf. Nallapati (200"
S15-2036,S15-2047,1,0.437926,"Missing"
S15-2036,N13-1090,0,0.0152512,"tar Living corpus with a word co-occurrence window of size ±3 and producing a vector of 250 dimensions with SVD (we produced a vector for each noun in the vocabulary); (ii) GloVe (Pennington et al., 2014), using a model pre-trained on Common Crawl (42B tokens), with 300 dimensions; and (iii) COMPOSES (Baroni et al., 2014), using previously-estimated predict vectors of 400 dimensions.3 We represent both q and c as a sum of the vectors corresponding to the words within them (neglecting the subject of c). We compute the cosine similarity to estimate sim(q, c). We also experimented with word2vec (Mikolov et al., 2013) vectors pre-trained with both cbow and skipgram on news data, and also with both word2vec and GloVe vectors trained on Qatar Living data, but we discarded them as they did not help us on top of all other features we had. 2.2 Context Comments are organized sequentially according to the time line of the comment thread. Whether a question includes further comments by the person who asked the original question or just several comments by the same user, or whether it belongs to a category in which a given kind of answer is expected, are all important factors. Therefore, we consider a set of featur"
S15-2036,S13-2053,0,0.0143676,"omments suggested visiting a Web site or contained an email address. Therefore, we included two boolean features to verify the presence of URLs or emails in c. Another feature captures the length of c, as longer (GOOD ) comments usually contain detailed information to answer a question. 2.5 Polarity These features, which we used for subtask B only, try to determine whether a comment is positive or negative, which could be associated with YES or NO answers. The polarity of a comment c is X pol(w) (5) pol(c) = w∈c where pol(w) is the polarity of word w in the NRC Hashtag Sentiment Lexicon v0.1 (Mohammad et al., 2013). We disregarded pol(w) if its absolute value was less than 1. We further use boolean features that check the existence of some keywords in the comment. Their values are set to true if c contains words like (i) yes, can, sure, wish, would, or (ii) no, not, neither. 2.6 User Profile With this set of features, we aim to model the behavior of the different participants in previous queries. Given comment c by user u, we consider the number of GOOD , BAD , POTENTIAL , and DIALOGUE comments u has produced before.4 We also consider the average word length of GOOD , BAD , POTENTIAL , and DIALOGUE comm"
S15-2036,D14-1162,0,0.0928882,"arent for each lemma leaf, and POS tag nodes are in turn grouped under shallow parsing chunks, which are linked to a root sentence node; finally, all root sentence nodes are linked to a super root for all sentences in the question/comment. 2.1.3 Semantic Similarity We apply three approaches to build wordembedding vector representations, using (i) latent semantic analysis (Croce and Previtali, 2010), trained on the Qatar Living corpus with a word co-occurrence window of size ±3 and producing a vector of 250 dimensions with SVD (we produced a vector for each noun in the vocabulary); (ii) GloVe (Pennington et al., 2014), using a model pre-trained on Common Crawl (42B tokens), with 300 dimensions; and (iii) COMPOSES (Baroni et al., 2014), using previously-estimated predict vectors of 400 dimensions.3 We represent both q and c as a sum of the vectors corresponding to the words within them (neglecting the subject of c). We compute the cosine similarity to estimate sim(q, c). We also experimented with word2vec (Mikolov et al., 2013) vectors pre-trained with both cbow and skipgram on news data, and also with both word2vec and GloVe vectors trained on Qatar Living data, but we discarded them as they did not help u"
S16-1083,S16-1128,1,0.911614,"2 MAP points over the IR baseline). They use distributed representations of words, knowledge graphs generated with BabelNet, and frames from FrameNet. Their contrastive2 run is even better, with MAP of 77.33. The second best system is that of ConvKN (Barr´on-Cede˜no et al., 2016) with MAP of 76.02; they are also first on MRR, second on AvgRec and F1 , and third on Accuracy. The third best system is KeLP (Filice et al., 2016) with MAP of 75.83; they are also first on AvgRec, F1 , and Accuracy. They have a contrastive run with MAP of 76.28, which would have ranked second. The fourth best, SLS (Mohtarami et al., 2016) is very close, with MAP of 75.55; it is also first on MRR and Accuracy, and third on AvgRec. It uses a bag-of-vectors approach with various vector- and text-based features, and different neural network approaches including CNNs and LSTMs to capture the semantic similarity between questions and answers. 6.3 Subtask C, English (Question-External Comment Similarity) The results for subtask C, English are shown in Table 5. This subtask attracted 10 teams, and 28 runs: 10 primary and 18 contrastive. Here the teams performed much better than they did for subtask B. The first three baselines were al"
S16-1083,P07-1098,1,0.303795,"cess of their creation. Section 5 explains the evaluation measures. Section 6 presents the results for all subtasks and for all participating systems. Section 7 summarizes the main approaches and features used by these systems. Finally, Section 8 offers some further discussion and presents the main conclusions. 2 Related Work Our task goes in the direction of passage reranking, where automatic classifiers are normally applied to pairs of questions and answer passages to derive a relative order between passages, e.g., see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Severyn and Moschitti, 2015; Moschitti, 2008; Tymoshenko and Moschitti, 2015; Tymoshenko et al., 2016; Surdeanu et al., 2008). In recent years, many advanced models have been developed for automating answer selection, producing a large body of work. For instance, Wang et al. (2007) proposed a probabilistic quasisynchronous grammar to learn syntactic transformations from the question to the candidate answers; Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; Wang and Manning (2010) developed a probabilistic model to learn tree"
S16-1083,S15-2047,1,0.907588,"Missing"
S16-1083,S15-2036,1,0.813812,"Missing"
S16-1083,D13-1044,1,0.636695,"n syntactic transformations from the question to the candidate answers; Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse trees; and Yao et al. (2013) applied linear chain CRFs with features derived from TED to automatically learn associations between questions and candidate answers. One interesting aspect of the above research is the need for syntactic structures; this is also corroborated in (Severyn and Moschitti, 2012; Severyn and Moschitti, 2013). Note that answer selection can use models for textual entailment, semantic similarity, and for natural language inference in general. Using information about the thread is another important direction. In the 2015 edition of the task, the top participating systems used thread-level features, in addition to the usual local features that only look at the question–answer pair. For example, the second-best team, HITSZ-ICRC, used as a feature the position of the comment in the thread, whether the answer is first, whether the answer is last (Hou et al., 2015). Similarly, the third-best team, QCRI,"
S16-1083,P08-1082,0,0.0152363,"participating systems. Section 7 summarizes the main approaches and features used by these systems. Finally, Section 8 offers some further discussion and presents the main conclusions. 2 Related Work Our task goes in the direction of passage reranking, where automatic classifiers are normally applied to pairs of questions and answer passages to derive a relative order between passages, e.g., see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Severyn and Moschitti, 2015; Moschitti, 2008; Tymoshenko and Moschitti, 2015; Tymoshenko et al., 2016; Surdeanu et al., 2008). In recent years, many advanced models have been developed for automating answer selection, producing a large body of work. For instance, Wang et al. (2007) proposed a probabilistic quasisynchronous grammar to learn syntactic transformations from the question to the candidate answers; Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse trees; and Yao et al. (2013) applied linear chain CRFs with features derived from TED to a"
S16-1083,D07-1002,0,\N,Missing
S16-1083,N10-1145,0,\N,Missing
S16-1083,S15-2035,0,\N,Missing
S16-1083,C10-1131,0,\N,Missing
S16-1083,N13-1106,0,\N,Missing
S16-1083,P15-1078,1,\N,Missing
S16-1083,P15-2113,1,\N,Missing
S16-1083,R15-1058,1,\N,Missing
S16-1083,S16-1130,1,\N,Missing
S16-1083,S16-1138,1,\N,Missing
S16-1083,S16-1126,0,\N,Missing
S16-1083,S16-1132,0,\N,Missing
S16-1083,S16-1134,0,\N,Missing
S16-1083,S16-1136,1,\N,Missing
S16-1083,S16-1133,0,\N,Missing
S16-1083,S16-1172,1,\N,Missing
S16-1083,S16-1137,1,\N,Missing
S16-1083,S16-1131,0,\N,Missing
S16-1083,N16-1084,1,\N,Missing
S16-1083,S16-1135,0,\N,Missing
S16-1083,N16-1152,1,\N,Missing
S16-1083,P16-2075,1,\N,Missing
S16-1083,P16-2065,1,\N,Missing
S16-1083,S15-2037,0,\N,Missing
S16-1083,D15-1068,1,\N,Missing
S16-1083,K15-1032,1,\N,Missing
S17-2003,S17-2044,0,0.0542572,"Missing"
S17-2003,N10-1145,0,0.016031,"es used by these systems and provides further discussion. Finally, Section 6 presents the main conclusions. 2 Question-answer similarity has been a subtask (subtask A) of our task in its two previous editions (Nakov et al., 2015, 2016b). This is a wellresearched problem in the context of general question answering. One research direction has been to try to match the syntactic structure of the question to that of the candidate answer. For example, Wang et al. (2007) proposed a probabilistic quasi-synchronous grammar to learn syntactic transformations from the question to the candidate answers. Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs. Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse trees. Yao et al. (2013) applied linear chain conditional random fields (CRFs) with features derived from TED to learn associations between questions and candidate answers. Moreover, syntactic structure was central for some of the top systems that participated in SemEval-2016 Task 3 (Filice et al., 2016; Barr´on-Cede˜no et al., 2016). Related Work The first step to automatically answer questions on"
S17-2003,C16-2001,1,0.881977,"Missing"
S17-2003,S15-2035,0,0.0226369,"andidate answer. Similarly, (Guzm´an et al., 2016a,b) ported an entire machine translation evaluation framework (Guzm´an et al., 2015) to the CQA problem. Using information about the answer thread is another important direction, which has been explored mainly to address Subtask A. In the 2015 edition of the task, the top participating systems used thread-level features, in addition to local features that only look at the question–answer pair. For example, the second-best team, HITSZ-ICRC, used as a feature the position of the comment in the thread, such as whether the answer is first or last (Hou et al., 2015). Similarly, the third-best team, QCRI, used features to model a comment in the context of the entire comment thread, focusing on user interaction (Nicosia et al., 2015). Finally, the fifth-best team, ICRC-HIT, treated the answer selection task as a sequence labeling problem and proposed recurrent convolutional neural networks to recognize good comments (Zhou et al., 2015b). In follow-up work, Zhou et al. (2015a) included long-short term memory (LSTM) units in their convolutional neural network to model the classification sequence for the thread, and Barr´on-Cede˜no et al. (2015) exploited the"
S17-2003,K15-1032,1,0.0248911,"to make more consistent global decisions about the goodness of the answers in the thread. They modeled the relations between pairs of comments at any distance in the thread, and combined the predictions of local classifiers using graph-cut and Integer Linear Programming. In follow up work, Joty et al. (2016) proposed joint learning models that integrate inference within the learning process using global normalization and an Ising-like edge potential. 5 https://github.com/tbmihailov/ semeval2016-task3-cqa 6 Using a heuristic that if several users call somebody a troll, then s/he should be one (Mihaylov et al., 2015a,b; Mihaylov and Nakov, 2016a; Mihaylov et al., 2017b). 30 Category Original Questions Train+Dev+Test from SemEval-2015 – Train(1,2)+Dev+Test from SemEval-2016 (200+67)+50+70 2,480+291+319 – – – (1,999+670)+500+700 (181+54)+59+81 (606+242)+155+152 (1,212+374)+286+467 880 24 139 717 – (19,990+6,700)+5,000+7,000 8,800 – – – (1,988+849)+345+654 (16,319+5,154)+4,061+5,943 (1,683+697)+594+403 246 8,291 263 (14,110+3,790)+2,440+3,270 2,930 (5,287+1,364)+818+1,329 (6,362+1,777)+1,209+1,485 (2,461+649)+413+456 1,523 1,407 0 Related Questions – Perfect Match – Relevant – Irrelevant Related Comments (w"
S17-2003,S17-2009,0,0.0610799,"Missing"
S17-2003,S15-2036,1,0.824235,"Missing"
S17-2003,J11-2003,0,0.0485113,"ting systems across all three subtasks. This includes fine-tuned word embeddings5 (Mihaylov and Nakov, 2016b); features modeling text complexity, veracity, and user trollness6 (Mihaylova et al., 2016); sentiment polarity features (Nicosia et al., 2015); and PMI-based goodness polarity lexicons (Balchev et al., 2016; Mihaylov et al., 2017a). Yet another research direction has been on using machine translation models as features for question-answer similarity (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015; Hoogeveen et al., 2016a; Wu and Zhang, 2016), e.g., a variation of IBM model 1 (Brown et al., 1993), to compute the probability that the question is a “translation” of the candidate answer. Similarly, (Guzm´an et al., 2016a,b) ported an entire machine translation evaluation framework (Guzm´an et al., 2015) to the CQA problem. Using information about the answer thread is another important direction, which has been explored mainly to address Subtask A. In the 2015 edition of the task, the top participating systems used thread-level features, in addition to local features tha"
S17-2003,D16-1244,0,0.0146703,"Missing"
S17-2003,S17-2059,0,0.0505879,"Missing"
W14-3601,al-sabbagh-girju-2012-yadac,0,0.189531,"Missing"
W14-3601,W12-2301,0,0.0200479,"Missing"
W14-3601,N12-1006,0,0.0434936,"g API. Bo Han et al. (2014) (Han et al., 2014) presented an integrated geolocation prediction framework and investigated what 3 Dialectal Arabic (DA) DA refers to the spoken language used for daily communication in Arab countries. There are considerable geographical distinctions between DAs within countries, across country borders, and even between cities and villages as shown in Figure 1. According to Ethnologue (http://www.ethnologue.com/browse/names), there are 34 variations of spoken Arabic or dialects in Arabic countries in addition to the Modern Standard Arabic (MSA). Some recent works (Zbib et al., 2012; Cotterell et al., 2014) are based on a coarser classification of Arabic dialects into five groups namely: Egyptian (EGY), Gulf (GLF), Maghrebi (MGR), Levantine (LEV), and Iraqi (IRQ). Other dialects are classified as OTHER. Zaidan and Callison-Burch (2014) mentioned that this is one possible breakdown but it is relatively coarse and can be further divided into more dialect groups, especially in large regions such as Maghreb. The goal of this paper is to collect a large, clean corpus for each country and study empirically if some of these dialects can be merged together. We found that there a"
W14-3601,P11-2007,0,\N,Missing
W14-3601,D14-1154,1,\N,Missing
W14-3601,J14-1006,0,\N,Missing
W14-3601,cotterell-callison-burch-2014-multi,0,\N,Missing
W14-3601,W14-3628,0,\N,Missing
W14-3601,P13-2081,0,\N,Missing
W14-3617,J03-1002,0,0.007291,"a generative model that attempts to generate all possible mappings of a source word while restricting the output to words in the target language (El-Kahki et al., 2011; Noeman and Madkour, 2010). Specifically, we used the baseline system of El-Kahky et al. (2011). To train character-level mappings, we extracted all the parallel word-pairs in the original (uncorrected) and corrected versions in the training set. If a word in the original version of the training set was actually correct, the word would be mapped to itself. We then aligned the parallel word pairs at character level using GIZA++ (Och and Ney, 2003), and symmetrized the alignments using grow-diag132 Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 132–136, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics final-and heuristic (Koehn et al., 2007). In all, we aligned a little over one million word pairs. As in the baseline of El-Kahki et al. (2011), given a possibly misspelled word worg , we produced all its possible segmentations along with their associated mappings that we learned during alignment. Valid target sequences were retained and sorted by the product of the"
W14-3617,D11-1128,1,\N,Missing
W14-3617,P07-2045,0,\N,Missing
W14-3617,W14-3605,0,\N,Missing
W14-3617,zaghouani-etal-2014-large,0,\N,Missing
W15-3201,2014.iwslt-papers.1,1,0.783454,"Missing"
W15-3201,W11-2123,0,0.123578,"s 5 Name Classification Experiments Given the 170K Namesarb and 182K Namestrans that we collected, we randomly split the set into 80/20 training and testing splits. We used word unigrams as features. We also examined giving first and last names different weights and character trigrams as a back-off for unseen words. Further, we trained two classifiers namely a Naive Bayes classifier and an SVM classifier. When using a Naive Bayes classifier and a name was not observed during training in general or for a class, we used KenLM language modeling toolkit to compute the smoothing probability of it (Heafield, 2011). Our baseline involved tagging all test items with the tag of the majority class, which means that every tweep would assigned to SA at country level and the Gulf at region level. Table 4 shows the baseline re3 http://www.geonames.org We use ”ISO 3166-1 alpha-2” for country codes 5 http://thenextweb.com/2010/01/15/twitter-geofail-023tweets-geotagged/ 4 4 Figure 2: Country Distribution for Namesarb sults in term of accuracy. Precision for the majority class would be identical to the overall accuracy and recall would be one. Precision and recall would be zero for all the other classes. Name type"
W15-3201,W14-3601,1,0.88284,"Missing"
W15-3201,N12-1006,0,\N,Missing
W15-3201,cotterell-callison-burch-2014-multi,0,\N,Missing
W15-3211,2014.iwslt-papers.1,1,0.831805,"Missing"
W15-3211,N10-1024,0,0.0336583,"Missing"
W15-3211,D14-1154,1,0.836134,"Proceedings of the Second Workshop on Arabic Natural Language Processing, pages 99–107, c Beijing, China, July 26-31, 2015. 2014 Association for Computational Linguistics work will be rejected. Sprugnoli et al. (2013) compared different automatic quality control methods for crowdsourcing speech transcription for Italian and German: Nowadays, there are many available resources for MSA such as corpora, morphological analyzers, Part Of Speech taggers, parsers, and so forth. However, there is still a need to build such resources for DA. MSA resources do not typically perform well for handling DA. Darwish et al. (2014) showed that there are differences between MSA and the Egyptian dialect of DA at almost all levels: lexical, morphological, phonological, and syntactic. Another challenge for DA is the nonstandard orthography, and words may be written in many different ways. For example, the future marker in Egyptian DA can be spelled with two different MSA characters: è or h . (For a complete overview of these issues, see Eskander et al. (2013)). There are some proposed rules for standardizing DA such as the Conventional Orthography for Dialectal Arabic (CODA) (Habash et al., 2012) which is very useful for ma"
W15-3211,N13-1066,0,0.0157116,"rt Of Speech taggers, parsers, and so forth. However, there is still a need to build such resources for DA. MSA resources do not typically perform well for handling DA. Darwish et al. (2014) showed that there are differences between MSA and the Egyptian dialect of DA at almost all levels: lexical, morphological, phonological, and syntactic. Another challenge for DA is the nonstandard orthography, and words may be written in many different ways. For example, the future marker in Egyptian DA can be spelled with two different MSA characters: è or h . (For a complete overview of these issues, see Eskander et al. (2013)). There are some proposed rules for standardizing DA such as the Conventional Orthography for Dialectal Arabic (CODA) (Habash et al., 2012) which is very useful for many applications like ASR, and natural language processing (NLP). Although these effective tools and others (such as Zribi et al. (2014)) exist for training annotators to write DA in a particular way and for automatic normalization of text after the fact, our aims are to obtain a transcribed speech corpus which exhibits natural orthographic variation among speakers, so normalization tools would not be appropriate for this task. 2"
W15-3211,W10-0708,0,0.0460815,"Missing"
W15-3211,habash-etal-2012-conventional,0,0.0551463,"form well for handling DA. Darwish et al. (2014) showed that there are differences between MSA and the Egyptian dialect of DA at almost all levels: lexical, morphological, phonological, and syntactic. Another challenge for DA is the nonstandard orthography, and words may be written in many different ways. For example, the future marker in Egyptian DA can be spelled with two different MSA characters: è or h . (For a complete overview of these issues, see Eskander et al. (2013)). There are some proposed rules for standardizing DA such as the Conventional Orthography for Dialectal Arabic (CODA) (Habash et al., 2012) which is very useful for many applications like ASR, and natural language processing (NLP). Although these effective tools and others (such as Zribi et al. (2014)) exist for training annotators to write DA in a particular way and for automatic normalization of text after the fact, our aims are to obtain a transcribed speech corpus which exhibits natural orthographic variation among speakers, so normalization tools would not be appropriate for this task. 2.2 • The iterative dual pathway method In this method, the speech segment is randomly assigned to four annotators in two independent pathway"
W15-3211,P11-1122,0,0.083073,"Missing"
W15-3211,zribi-etal-2014-conventional,0,0.0142051,"ogical, phonological, and syntactic. Another challenge for DA is the nonstandard orthography, and words may be written in many different ways. For example, the future marker in Egyptian DA can be spelled with two different MSA characters: è or h . (For a complete overview of these issues, see Eskander et al. (2013)). There are some proposed rules for standardizing DA such as the Conventional Orthography for Dialectal Arabic (CODA) (Habash et al., 2012) which is very useful for many applications like ASR, and natural language processing (NLP). Although these effective tools and others (such as Zribi et al. (2014)) exist for training annotators to write DA in a particular way and for automatic normalization of text after the fact, our aims are to obtain a transcribed speech corpus which exhibits natural orthographic variation among speakers, so normalization tools would not be appropriate for this task. 2.2 • The iterative dual pathway method In this method, the speech segment is randomly assigned to four annotators in two independent pathways. When four transcriptions, two from each pathway, match each other, the segment is considered as transcribed correctly. The key advantage of this method is to ha"
W15-3211,W10-0716,0,0.0215178,"y and it may happen in many cases that there will not be exact match between annotators (first method) nor with the gold standard (second method). Figure 1 shows real transcription outputs for the same speech segment in which there is no single match between the whole transcription among transcribers because words in colors are written differently and all are correct. Quality Control in Crowdsourcing Crowdsourcing is now considered a promising alternative to the employment of transcription experts to create large corpora of transcribed speech in languages such as English (Lee and Glass, 2011; Marge et al., 2010b; Marge et al., 2010a; H¨am¨al¨ainen et al., 2013), Spanish (Audhkhasi et al., 2011), Swahili, Amharic (Gelas et al., 2011), Korean, Hindi, and Tamil (Novotney and CallisonBurch, 2010). One of the main challenges in crowdsourcing is quality control. There is great incentive to performing automatic quality control as opposed to leaving the cleaning of data to postprocessing. Automatic quality control which issues warning messages to a user or rejects submission of spammy data reduces overall data attrition. A typical way of performing automatic quality control is the usage of a gold standard t"
W15-3218,W14-3605,0,0.0422065,"ion approach that handles specific error types such as dialectal word substitution and word splits and merges with the aid of a language model. We also applied corrections that are specific to second language learners that handle erroneous preposition selection, definiteness, and gender-number agreement. 1 Introduction 2 In This paper, we provide a system description for our submissions to the Arabic error correction shared task (QALB-2015 Shared Task on Automatic Correction of Arabic) as part of the Arabic NLP workshop. The QALB-2015 shared task is an extension of the first QALB shared task (Mohit et al., 2014) which addressed errors in comments written to Aljazeera articles by native Arabic speakers (Zaghouani et al., 2014). The current competition includes two tracks, and, in addition to errors produced by native speakers, also includes correction of texts written by learners of Arabic as a foreign language (L2) (Zaghouani et al., 2015). The native track includes Aljtrain-2014, Alj-dev-2014, Alj-test-2014 texts from QALB-2014. The L2 track includes L2-train-2015 and L2-dev-2015. This data was released for the development of the systems. The systems were scored on blind test sets Alj-test-2015 and"
W15-3218,W14-3617,1,0.791492,"Missing"
W15-3218,P12-1049,0,0.0411773,"Missing"
W15-3218,zaghouani-etal-2014-large,0,\N,Missing
W15-3218,W15-1614,0,\N,Missing
W15-3223,darwish-etal-2014-using,1,0.8246,"atching binary feature fires. We found such ranking scores to be a valuable addition in our experiments. To understand why, we note that they are able to neatly separate the different labels, with the following average scores: DIRECT 14.5, RELATED 12.3, and IRRELEVANT 10.5. 3.4 In addition to the machine learning approaches, we adapted our rule-based model, which ranked 2nd in the competition (Nicosia et al., 2015). The basic idea is to rank the comments according to their similarity and label the top ones as DIRECT . In this case our preprocessing consists of stemming, performed with QATARA (Darwish et al., 2014), and again stopword removal. In our implementation, the score of a comment is computed as 1 X score(c) = α · ω(t) + pos(t) |q |t∈q∩c Similarity This set of features measures the similarity sim(q, c) between a question and a comment, assuming that high similarity signals a DIRECT answer. We compute the similarity between word n-gram representations (n = [1, . . . , 4]) of q and c, using different lexical similarity measures: greedy string tiling (Wise, 1996), longest common subsequences (Allison and Dix, 1986), Jaccard coefficient (Jaccard, 1901), word containment (Lyon et al., 2001), and cosi"
W15-3223,N13-1090,0,0.0265105,"cal setting, first discriminating between IRRELEVANT and NON-IRRELEVANT and then between DIRECT and RELATED ; and (ii) a multiclass classification setting. Their third approach was based on an ensemble of classifiers. 3.1 Vectors Our motivation for using word vectors for this task is that they convey a soft representation of word meanings. In contrast to similarity measures that are based on words, using word vectors has the potential to bridge over lack of lexical overlap between questions and answers. We start by creating word vectors from a large corpus of raw Arabic text. We use Word2Vec (Mikolov et al., 2013b; Mikolov et al., 2013a) with default settings for creating 100-dimensional vectors. We experimented with the Arabic Gigaword (Linguistic Data Consortium, 2011), containing newswire text, and with the King Saud University Corpus of Classical Arabic (KSUCCA), containing classical Arabic text (Alrabiah et al., 2013). Table 2 provides some statistics for these corpora. We were initially expecting KSUCCA to produce better results, beFinally, Mohamed et al., (2015) applied a decision tree whose output is composed of lexical and enriched representations of q and c: the terms in the texts are expand"
W15-3223,S15-2040,0,0.0217317,"exical overlap between questions and answers. We start by creating word vectors from a large corpus of raw Arabic text. We use Word2Vec (Mikolov et al., 2013b; Mikolov et al., 2013a) with default settings for creating 100-dimensional vectors. We experimented with the Arabic Gigaword (Linguistic Data Consortium, 2011), containing newswire text, and with the King Saud University Corpus of Classical Arabic (KSUCCA), containing classical Arabic text (Alrabiah et al., 2013). Table 2 provides some statistics for these corpora. We were initially expecting KSUCCA to produce better results, beFinally, Mohamed et al., (2015) applied a decision tree whose output is composed of lexical and enriched representations of q and c: the terms in the texts are expanded on the basis of a set of Quranic ontologies. The authors do not report the 185 cause its language should be more similar to the religious texts in the Fatwa corpus. However, in practice we found vectors trained on the Arabic Gigaword to perform better, possibly thanks to its larger coverage, so we report only results with the Gigaword corpus below. We noticed in preliminary experiments that many errors are due to lack of overlap in vocabulary between answers"
W15-3223,S15-2035,0,0.027656,"easures, statistical ranking, and rule-based ranking. We describe each kind in turn. Belinkov et al., (2015)’s best submission was very similar to the one of Nicosia et al., (2015): a ranking approach based on confidence values obtained by an SVM ranker (Joachims, 2006). Their second approach consisted of a multi-class linear SVM classifier relying on three feature families: (i) lexical similarities between q and c (similar to those applied by the previous team); (ii) word vector representations of q and c; and (iii) a ranking score for c produced by the SVM ranker. The two best approaches of Hou et al., (2015) used features representing different similarities between q and c, lengths of words and sentences, and the number of named-entities in c, among others. In this case [1,2,3]-grams were also considered as features, but with two differences with respect to the other participants: only the most frequent n-grams were used and a translated version to English was also included. They explored two strategies using SVMs in their top performing submissions: (i) a hierarchical setting, first discriminating between IRRELEVANT and NON-IRRELEVANT and then between DIRECT and RELATED ; and (ii) a multiclass c"
W15-3223,S15-2047,0,0.269072,"ence Laboratory, Doha, Qatar Cambridge, MA 02139, USA {albarron, hmubarak}@qf.org.qa belinkov@csail.mit.edu Abstract ing” (Nakov et al., 2015) and focus on the Arabic language. Our approach is treating each question– comment as an instance in a supervised learning scenario. We build a support vector machine (SVM) classifier that is using different kinds of features, including vector representations, similarity measures, and rankings. Our extensive feature set allows us to achieve better results than those of the winner of the competition: 79.25 F1 compared to 78.55, obtained by Nicosia et al. (2015). The rest of the paper is organized as follows. Section 2 describes the experimental framework —composed of the Fatwa corpus and the evaluation metrics— and overviews the different models proposed at competition time. Section 3 describes our model. Experiments and results are discussed in Section 4. Related work is discussed in Section 5. We summarize our contributions in Section 6, and include an error analysis in Appendix A. The task of answer selection in community question answering consists of identifying pertinent answers from a pool of user-generated comments related to a question. The"
W15-3223,S15-2036,1,0.877926,"Missing"
W15-3223,pasha-etal-2014-madamira,0,0.1324,"Missing"
W15-3223,W01-0515,0,0.0181742,"ATARA (Darwish et al., 2014), and again stopword removal. In our implementation, the score of a comment is computed as 1 X score(c) = α · ω(t) + pos(t) |q |t∈q∩c Similarity This set of features measures the similarity sim(q, c) between a question and a comment, assuming that high similarity signals a DIRECT answer. We compute the similarity between word n-gram representations (n = [1, . . . , 4]) of q and c, using different lexical similarity measures: greedy string tiling (Wise, 1996), longest common subsequences (Allison and Dix, 1986), Jaccard coefficient (Jaccard, 1901), word containment (Lyon et al., 2001), and cosine similarity. The preprocessing in this case consists only of stopword removal. Additionally, we further compute cosine similarity on lemmas and part-of-speech tags, both including and excluding stopwords. 3.3 Rule-based Ranking where ω(t) = 1 if t is a 1-gram, 4 if it is a 2-gram, and pos(t) represents the relative position of t in the question and is estimated as the length of q minus the position of t in q. That is, we give significantly more relevance to 2-grams and to those matching n-grams at the beginning of the question. We compute this score twice: once considering the subj"
W15-3223,S15-2048,1,\N,Missing
W17-1302,D15-1274,0,0.435692,"Missing"
W17-1302,W05-0711,0,0.852256,"Missing"
W17-1302,pasha-etal-2014-madamira,0,0.231088,"Missing"
W17-1302,L16-1170,1,0.907508,"ring with systems that were trained on the ATB, some preprocessing is required, as we show later, to make sure that we are not unfairly penalizing them. 3 Training and Test Corpora 3.2 Data Preparation Given a word in the diacritized corpus, we produce multiple representations of it. To illustrate the representations, we use the word “wakitAbihimo” (and their book) as our running example. 1. diacritized surface form (“wakitAbihimo”). 2. diacritized surface form without case ending. To remove case endings, we segment each word in the corpus to its underlying clitics using the Farasa segmenter (Darwish and Mubarak, 2016). For example, given the diacritized word “wakitAbihimo” (and their book), it would be segmented to the prefix “wa”, stem “kitAbi”, and suffix “himo”. The Our Diacritizer The diacritizer has two main components. The first component recovers the diacritics for the core word (i.e. word without case ending), and the second only recovers the case ending. In this section we describe: the training and test corpora we used and how we processed them; the training of our system that diacritizes core-words and guesses 11 plates. In our example, the template “wfEAlhm” would be mapped to “wafiEAlihimo” an"
W17-1302,W02-0504,0,0.652492,"Missing"
W17-1302,W04-1612,0,0.738422,"Missing"
W17-1302,N07-2014,0,0.12851,"Missing"
W17-1302,P06-1073,0,0.923617,"Missing"
W17-1302,D11-1128,1,\N,Missing
W17-1302,P07-2045,0,\N,Missing
W17-1306,N16-3003,1,0.794973,"or testing, 75 for development and the remaining 200 for training. The concept We followed in LSTM sequence labeling is that segmentation is one-to-one mapping at the character level where each character is annotated as either beginning a segment (B), continues a previous segment (M), ends a segment (E), or is a segment by itself (S). After the labeling is complete we merge the characters and labels  together, for example @ñËñ®J K. byqwlwA is labeled as “SBMMEBE”, which means that the word is segmented as b+yqwl+wA. We compar results of our two LSTM models (BiLSTM and BiLSTMCRF) with Farasa (Abdelali et al., 2016), an open source segementer for MSA3 , and MADAMIRA for Egyptian dialect. Table 3 shows accuracy for Farasa, MADAMIRA, and both of our models. • ËQK @ñË@ AlwAyrls “the  AlHA$tAj “the hashtag”. h. AJAêË@ AlgTY “the Spelling variation: e.g. ù¢ªË@ cover”, úÎë B l&gt;hly “to Ahly”. wireless”, • Morphological inflection (imperative): e.g.  ¯ fwqwA “wake up”. ø Y $dy “pull”, @ñ¯ñ • Segmentation ambiguity: e.g. éJ Ë lyh meaning mAlnA meaning “our “why” or “to him”, AJËAÓ money” or “what we have”. • Combinations not known to MADAMIRA:  ® ® JÓ mtqflwhA$ “don’t close it”, e.g. AëñÊ @ &gt;wSflkwA “I"
W17-1306,W09-0807,0,0.049808,"instead dual and feminine plural, dropping some articles and preposition in some syntactic constructs, and using only one form yn inof noun and verb suffixes such as áK wn and stead of àð respectively. • Many words do not overlap with MSA as result of language borrowing from other lan guages (Ibrahim, 2006), such as éJ ¯A¿ kAfiyh ñKAK  mi$ “not” Ó  . balA$ “do not”. Code switching is and CK also very common in Arabic dialects (Samih et al., 2016). 2 Related Work Work on dialectal Arabic is fairly new compared to MSA. A number of research projects were devoted to dialect identification (Biadsy et al., 2009; Zbib et al., 2012; Zaidan and Callison-Burch, 2014). There are five major dialects including Egyptian, Gulf, Iraqi, Levantine and Maghribi. Few resources for these dialects are available such as the CALLHOME Egyptian Arabic Transcripts (LDC97T19), which was made available for research as early as 1997. Newly developed resources include the corpus developed by Bouamor et al. (2014), which contains 2,000 parallel sentences in multiple dialects and MSA as well as English translation. • Merging multiple words together by concatenating and dropping letters such as the word  . J J.Ó mbyjlhA$ (he"
W17-1306,bouamor-etal-2014-multidialectal,0,0.121902,"tching is and CK also very common in Arabic dialects (Samih et al., 2016). 2 Related Work Work on dialectal Arabic is fairly new compared to MSA. A number of research projects were devoted to dialect identification (Biadsy et al., 2009; Zbib et al., 2012; Zaidan and Callison-Burch, 2014). There are five major dialects including Egyptian, Gulf, Iraqi, Levantine and Maghribi. Few resources for these dialects are available such as the CALLHOME Egyptian Arabic Transcripts (LDC97T19), which was made available for research as early as 1997. Newly developed resources include the corpus developed by Bouamor et al. (2014), which contains 2,000 parallel sentences in multiple dialects and MSA as well as English translation. • Merging multiple words together by concatenating and dropping letters such as the word  . J J.Ó mbyjlhA$ (he did not go to her), AêÊj which is a concatenation of “mA byjy lhA$”. • Some affixes are altered in form from their MSA counterparts, such as the feminine second person pronoun  k → ú» ky and the second person plural pronoun wn @ð wA instead of àð • In addition, there are the regular discourse features in informal texts, such as the use of emoticons and character repetition for emp"
W17-1306,D14-1154,1,0.858589,"cters embedding and stacks them to build a matrix. This latter is then used as the input to the Bi-directional LSTM. On the last layer, an affine transformation function followed by a CRF computes the probability distribution over all labels Early Stopping We also employ early stopping (Caruana et al., 2000; Graves et al., 2013b) to mitigate overfitting by monitoring the model’s performance on development set. 4 ary respectively. The architecture of our segmentation model, shown in Figure 2, is straightforward. It comprises the following three layers: Dataset We used the dataset described in (Darwish et al., 2014). The data was used in a dialect identification task to distinguish between dialectal Egyptian and MSA. It contains 350 tweets with more than 8,000 words including 3,000 unique words written in Egyptian dialect. The tweets have much dialectal content covering most of dialectal Egyptian phonological, morphological, and syntactic phenomena. It also includes Twitter-specific aspects of the text, such as #hashtags, @mentions, emoticons and URLs. We manually annotated each word in this corpus to provide: CODA-compliant writing (Habash et al., 2012), segmentation, stem, lemma, and POS, also the corr"
W17-1306,W15-3904,0,0.0407118,"Missing"
W17-1306,W16-4828,1,0.865893,"is paper, we show how a segmenter can be trained on only 350 annotated tweets using neural networks without any normalization or reliance on lexical features or linguistic resources. We deal with segmentation as a sequence labeling problem at the character level. We show experimentally that our model can rival state-of-the-art methods that heavily depend on additional resources. 1 The advent of the social networks and the spread of smart phones, yielded the need for dialectaware smart systems and motivated the research in Dialectal Arabic such as dialectal Arabic identification for both text (Eldesouki et al., 2016) and speech (Khurana et al., 2016), morphological analysis (Habash et al., 2013) and machine translation (Sennrich et al., 2016; Sajjad et al., 2013). Due to the rich morphology in Arabic and its dialects, word segmentation is one of the most important processing steps. Word segmentation is considered an integral part for many higher Arabic NLP tasks such as part-of-speech tagging, parsing and machine translation. For example, the Egyp JºÓð “wmktbhA$” meaning: “and tian word AîD . he didn’t write it”) includes four clitics surrounding the the verb (stem) “ktb”, and is rendered after segment"
W17-1306,N16-1030,0,0.164915,"− x + W← −← − h t−1 + b← −) ht = σ(Wx← h t h h h → − ← − → h + W← − h + by yt = W− hy t hy t set of labels. In our case S ={B, M, E, S, WB}, where B is the beginning of a token, M is the middle of a token, E is the end of a token, S is a single character token, and W B is the word boundary. w ~ is the weight vector for weighting the feature vec~ Training and decoding are performed by the tor Φ. Viterbi algorithm. Note that replacing the softmax with CRF at the output layer in neural networks has proved to be very fruitful in many sequence labeling tasks (Ma and Hovy, 2016; Huang et al., 2015; Lample et al., 2016; Samih et al., 2016) More interpretations about these formulas are found in Graves et al. (2013a). A very important element of the recent success of many NLP applications, is the use of characterlevel representations in deep neural networks. This has shown to be effective for numerous NLP tasks (Collobert et al., 2011; dos Santos et al., 2015) as it can capture word morphology and reduce out-of-vocabulary. This approach has also been especially useful for handling languages with rich morphology and large character sets (Kim et al., 2016). We use pre-trained character embeddings to initialize"
W17-1306,P16-1101,0,0.269252,"(ct ) where σ is the logistic sigmoid function, and i, f , o and c are respectively the input gate, forget gate, output gate and cell activation vectors. More interpretation about this architecture can be found in (Lipton et al., 2015). Figure 1 illustrates a single LSTM memory cell (Graves and Schmidhuber, 2005) Arabic Segmentation Model In this section, we will provide a brief description of LSTM, and introduce the different components of our Arabic segmentation model. For all our work, we used the Keras toolkit (Chollet, 2015). The architecture of our model, shown in Figure 2 is similar to Ma and Hovy (2016), Huang et al. (2015), and Collobert et al. (2011) 3.1 Long Short-term Memory A recurrent neural network (RNN) belongs to a family of neural networks suited for modeling sequential data. Given an input sequence x = (x1 , ..., xn ), an RNN computes the output vector yt of each word xt by iterating the following equations from t = 1 to n: 1 Figure 1: A Long Short-Term Memory Cell. 3.2 Bi-directional LSTM Bi-LSTM networks (Schuster and Paliwal, 1997) are extensions to the single LSTM networks. They MADAMIRA release 20160516 2.1 48 are capable of learning long-term dependencies and maintain contex"
W17-1306,maamouri-etal-2014-developing,0,0.0957441,"Missing"
W17-1306,habash-etal-2012-conventional,0,0.430983,"their MSA counterparts, such as the feminine second person pronoun  k → ú» ky and the second person plural pronoun wn @ð wA instead of àð • In addition, there are the regular discourse features in informal texts, such as the use of emoticons and character repetition for emphasis, e.g. úÍððððððñ«X@ AdEwwwwwwwliy “pray for me”. tAtuw “tattoo”, or coinage, such as the negative particles Ég. @P rAjil “man” Ég. P rajul, and vowel shortening, such as AÖß X dayomA “always” from AÖß @X dAyomA. from • Lack of standard orthography. Many of the words in DA do not follow a standard orthographic system (Habash et al., 2012). “cafe” and H t or  s as in Q J» kvyr Õç' tm → ñK tw. • Some morphological patterns that do not exist in MSA, such as the passive pattern AitofaEal, such as QåºK@ Aitokasar “it broke”. 47 For segmentation, Yao and Huang (2016) successfully used a bi-directional LSTM model for segmenting Chinese text. In this paper, we build on their work and extend it in two ways, namely combining bi-LSTM with CRF and applying on Arabic, which is an alphabetic language. Mohamed et al. (2012) built a segmenter based on memory-based learning. The segmenter has been trained on a small corpus of Egyptian"
W17-1306,mohamed-etal-2012-annotating,0,0.411081,"Missing"
W17-1306,N13-1044,0,0.282692,"neural networks without any normalization or reliance on lexical features or linguistic resources. We deal with segmentation as a sequence labeling problem at the character level. We show experimentally that our model can rival state-of-the-art methods that heavily depend on additional resources. 1 The advent of the social networks and the spread of smart phones, yielded the need for dialectaware smart systems and motivated the research in Dialectal Arabic such as dialectal Arabic identification for both text (Eldesouki et al., 2016) and speech (Khurana et al., 2016), morphological analysis (Habash et al., 2013) and machine translation (Sennrich et al., 2016; Sajjad et al., 2013). Due to the rich morphology in Arabic and its dialects, word segmentation is one of the most important processing steps. Word segmentation is considered an integral part for many higher Arabic NLP tasks such as part-of-speech tagging, parsing and machine translation. For example, the Egyp JºÓð “wmktbhA$” meaning: “and tian word AîD . he didn’t write it”) includes four clitics surrounding the the verb (stem) “ktb”, and is rendered after segmentation as “w+m+ktb+hA+$”. The clitics in this word are the coordinate conjunction"
W17-1306,P14-2034,0,0.310795,"Missing"
W17-1306,pasha-etal-2014-madamira,0,0.190181,"Missing"
W17-1306,P13-2001,1,0.858088,"tures or linguistic resources. We deal with segmentation as a sequence labeling problem at the character level. We show experimentally that our model can rival state-of-the-art methods that heavily depend on additional resources. 1 The advent of the social networks and the spread of smart phones, yielded the need for dialectaware smart systems and motivated the research in Dialectal Arabic such as dialectal Arabic identification for both text (Eldesouki et al., 2016) and speech (Khurana et al., 2016), morphological analysis (Habash et al., 2013) and machine translation (Sennrich et al., 2016; Sajjad et al., 2013). Due to the rich morphology in Arabic and its dialects, word segmentation is one of the most important processing steps. Word segmentation is considered an integral part for many higher Arabic NLP tasks such as part-of-speech tagging, parsing and machine translation. For example, the Egyp JºÓð “wmktbhA$” meaning: “and tian word AîD . he didn’t write it”) includes four clitics surrounding the the verb (stem) “ktb”, and is rendered after segmentation as “w+m+ktb+hA+$”. The clitics in this word are the coordinate conjunction “w”, the negation prefix “m”, the object pronoun “hA”, and the post"
W17-1306,W16-5806,1,0.909334,"under lenition, softening of a consonant, or fortition, hardening of a consonant. • Vowel elongation, such as • The use of masculine plural or singular noun forms instead dual and feminine plural, dropping some articles and preposition in some syntactic constructs, and using only one form yn inof noun and verb suffixes such as áK wn and stead of àð respectively. • Many words do not overlap with MSA as result of language borrowing from other lan guages (Ibrahim, 2006), such as éJ ¯A¿ kAfiyh ñKAK  mi$ “not” Ó  . balA$ “do not”. Code switching is and CK also very common in Arabic dialects (Samih et al., 2016). 2 Related Work Work on dialectal Arabic is fairly new compared to MSA. A number of research projects were devoted to dialect identification (Biadsy et al., 2009; Zbib et al., 2012; Zaidan and Callison-Burch, 2014). There are five major dialects including Egyptian, Gulf, Iraqi, Levantine and Maghribi. Few resources for these dialects are available such as the CALLHOME Egyptian Arabic Transcripts (LDC97T19), which was made available for research as early as 1997. Newly developed resources include the corpus developed by Bouamor et al. (2014), which contains 2,000 parallel sentences in multiple"
W17-1306,P16-1162,0,0.049716,"reliance on lexical features or linguistic resources. We deal with segmentation as a sequence labeling problem at the character level. We show experimentally that our model can rival state-of-the-art methods that heavily depend on additional resources. 1 The advent of the social networks and the spread of smart phones, yielded the need for dialectaware smart systems and motivated the research in Dialectal Arabic such as dialectal Arabic identification for both text (Eldesouki et al., 2016) and speech (Khurana et al., 2016), morphological analysis (Habash et al., 2013) and machine translation (Sennrich et al., 2016; Sajjad et al., 2013). Due to the rich morphology in Arabic and its dialects, word segmentation is one of the most important processing steps. Word segmentation is considered an integral part for many higher Arabic NLP tasks such as part-of-speech tagging, parsing and machine translation. For example, the Egyp JºÓð “wmktbhA$” meaning: “and tian word AîD . he didn’t write it”) includes four clitics surrounding the the verb (stem) “ktb”, and is rendered after segmentation as “w+m+ktb+hA+$”. The clitics in this word are the coordinate conjunction “w”, the negation prefix “m”, the object prono"
W17-1306,J14-1006,0,0.025968,"ing some articles and preposition in some syntactic constructs, and using only one form yn inof noun and verb suffixes such as áK wn and stead of àð respectively. • Many words do not overlap with MSA as result of language borrowing from other lan guages (Ibrahim, 2006), such as éJ ¯A¿ kAfiyh ñKAK  mi$ “not” Ó  . balA$ “do not”. Code switching is and CK also very common in Arabic dialects (Samih et al., 2016). 2 Related Work Work on dialectal Arabic is fairly new compared to MSA. A number of research projects were devoted to dialect identification (Biadsy et al., 2009; Zbib et al., 2012; Zaidan and Callison-Burch, 2014). There are five major dialects including Egyptian, Gulf, Iraqi, Levantine and Maghribi. Few resources for these dialects are available such as the CALLHOME Egyptian Arabic Transcripts (LDC97T19), which was made available for research as early as 1997. Newly developed resources include the corpus developed by Bouamor et al. (2014), which contains 2,000 parallel sentences in multiple dialects and MSA as well as English translation. • Merging multiple words together by concatenating and dropping letters such as the word  . J J.Ó mbyjlhA$ (he did not go to her), AêÊj which is a concatenation of"
W17-1306,N12-1006,0,0.0419555,"inine plural, dropping some articles and preposition in some syntactic constructs, and using only one form yn inof noun and verb suffixes such as áK wn and stead of àð respectively. • Many words do not overlap with MSA as result of language borrowing from other lan guages (Ibrahim, 2006), such as éJ ¯A¿ kAfiyh ñKAK  mi$ “not” Ó  . balA$ “do not”. Code switching is and CK also very common in Arabic dialects (Samih et al., 2016). 2 Related Work Work on dialectal Arabic is fairly new compared to MSA. A number of research projects were devoted to dialect identification (Biadsy et al., 2009; Zbib et al., 2012; Zaidan and Callison-Burch, 2014). There are five major dialects including Egyptian, Gulf, Iraqi, Levantine and Maghribi. Few resources for these dialects are available such as the CALLHOME Egyptian Arabic Transcripts (LDC97T19), which was made available for research as early as 1997. Newly developed resources include the corpus developed by Bouamor et al. (2014), which contains 2,000 parallel sentences in multiple dialects and MSA as well as English translation. • Merging multiple words together by concatenating and dropping letters such as the word  . J J.Ó mbyjlhA$ (he did not go to her),"
W17-1316,darwish-etal-2014-using,1,0.942927,"engineering and word embeddigns in Arabic POS tagging. We show that feature engineering improves POS tagging significantly. • We explore the effectiveness of many features including morphological and contextual features for tagging each clitic or each word in-context. 130 Proceedings of The Third Arabic Natural Language Processing Workshop (WANLP), pages 130–137, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics tagsets such as that of the Penn Arabic Treebank (ATB), which has 70 tags (Maamouri et al., 2004). In our work, we elected to use the tagest proposed by Darwish et al. (2014) which is a simplified version of ATB tagset and uses 18 tags only. • We open-source both Arabic POS taggers, both of which are written entirely in Java. The SVMRank -based system has a load time of 5 seconds and can process about 2,000 word/second on an laptop with Intel i7 processor with 16 GB of RAM. 2 2.1 2.2 Background Arabic POS Tagging Most recent work on Arabic POS tagging has used statistical methods. Diab (2009) used an SVM classifier to ascertain the optimal POS tags. The classifier was trained on the ATB data. Essentially, they treated the problem as a sequence-labeling problem. An"
W17-1316,N12-1015,0,0.0186639,"Missing"
W17-1316,N13-1090,0,0.0296891,"tput i as follows: li = tanh(Lf Sif + Lb Sib + bl ) where Lf , Lb and bl denote the parameters for combining the forward and backward states. We experimented with a number of settings where the clitic sequence was augmented with a subset of features that includes character sequences, word meta type, stem template (Darwish et al., 2014), and also combined with 200 dimension word embeddings learned over the aforementioned collection of text containing 10 years of Al-Jazeera articles1 . To create the embeddings, we used word2vec with continuous skip-gram learning algorithm with an 8 gram window (Mikolov et al., 2013)2 . For the bi-LSTM experiments, we used the Java Neural Network Library3 , which is tuned for POS tagging(Ling et al., 2015). We extended the library to produce the additional aforementioned features. • Word context features: p(P OS|w−1 ), p(P OS|w1 ), p(P OS|w−2 , w−1 ), p(P OS|w−3 , w−2 , w−1 ), and p(P OS|w−4 , w−3 , w−2 , w−1 ) 3.1.3 OOVs and pre-Filtering For both clitic and word tagging, In case we could not compute a feature value during training (e.g., a clitic was never observed with a given POS tag), the feature value is assigned a small  value equal to 10−10 . If the clitic is a p"
W17-1316,pasha-etal-2014-madamira,0,0.226248,"Missing"
W17-1316,P16-2067,0,0.0573499,"Missing"
W17-1316,P09-2056,0,\N,Missing
W17-1316,L16-1170,1,\N,Missing
W17-3008,W16-5618,0,0.105291,"Missing"
W19-4603,J92-4003,0,0.551827,"Missing"
W19-4603,W16-5801,0,0.0290695,"Missing"
W19-4603,P13-2037,0,0.0590321,"Missing"
W19-4603,W16-5812,0,0.0243184,"is relatively less challenging for computational analysis, as each sentence still follows a monolingual model, intrasentential CS poses a bottleneck challenge. It needs a special amount of attention, because it is only this type that involves the lexical and syntactic integration and activation of two language models at the same time. NLP systems trained on monolingual data suffer significantly when trying to process this kind bilingual text or utterance. CS has proved challenging for NLP technologies, not only because current tools are geared toward the processing of one language at a time (AlGhamdi et al., 2016), but also because codeswitched data is typically associated with additional challenges such as the non-conventional orthography, non-canonicity (nonstandard or incomplete) of syntactic structures, and the large number of OOV-words (Çetino˘glu et al., 2016), which suggest the need for larger training data than what is typically used in monolingual models. Unfortunately, shortage of training data has usually been cited as the reason for the under-performance of 19 diglossic code-switching, the shift is more likely to be lexical, morphological, and structural, rather than phonological, unlike th"
W19-4603,C12-2011,1,0.885038,"Missing"
W19-4603,Q16-1026,0,0.0173957,"nsure that we get representations of all the words and reduce the number of OOVs (out of vocabulary words). We find significant improvement using FastText embedding over the traditional word2vec representation (Mikolov et al., 2013). This is probably due to the utilization of sub-word (ex. prefixes or suffixes) information in the former. Character-level CNNs. Although originally designed for image recognition, CNNs have proven effective for various NLP tasks due to their ability to encode character-level representations of words as well as extract sub-word information (Collobert et al., 2011; Chiu and Nichols, 2016; dos Santos and Guimarães, 2015). Bi-LSTM Recurrent neural networks (RNN) are well suited for modeling sequential data, achieving ground-breaking results in many NLP tasks (e.g., machine translation). BiLSTMs (Hochreiter and Schmidhuber, 1997; Schuster and Paliwal, 1997) are capable of learning long-term dependencies and maintaining contextual features from both past and future states while avoiding the vanishing/exploding gradients problem. They consist of two separate bidirectional hidden layers that feed forward to the same output layer. Figure 3: DNN Architecture. BCs on our crawled code-"
W19-4603,attia-etal-2010-automatically,1,0.834799,"Missing"
W19-4603,L18-1015,1,0.899707,"Missing"
W19-4603,W14-3901,0,0.0289644,"tagging of CS data and concluded that applying a machine learning framework as a voting mechanism on top of the output of two monolingual POS taggers achieves the best performance. Word-level CS identification for Arabic (along with Spanish–English) has been featured in a couple of shared tasks: the First Shared Task on Language Identification in CodeSwitched Data (Solorio et al., 2014) and the Second Shared Task on Language Identification in Code-Switched Data (Molina et al., 2016), of which Samih et al. (2016) was the winning system, and against which we compare our results in this project. Eskander et al. (2014) studied CS between EA written in Roman script (Arabizi) and English. Habash et al. (2008) created a standard annotation guidelines for CS between MSA and dialects. CS has also been studied in Arabic as a predictor of social influence in the collaborative writing in Wikipedia discussion pages in (Yoder et al., 2017) and it was found that CS is positively associated with the editor’s success in winning an argument. We notice from the literature that in some instances POS tagging has been used to aid with the identification of code-switching points, and in some other instances language identific"
W19-4603,Q17-1010,0,0.0325773,"Missing"
W19-4603,N13-1039,0,0.0935945,"Missing"
W19-4603,C82-1023,0,0.578981,"tactic rules of the two languages involved, sometime adding in or leaving out a determiner, or applying a system of affixation from one language and not the other. 1.2 Definition and Defining Perspectives The definition of CS has varied greatly depending on the different researchers’ attitude and perspectives of the operation involved. While some viewed it as a process where two languages are actively interacting with each other (ultimately creating a new code), other viewed the operation just as two separate languages sitting side-by-side as isolated islands. Following the first perspective, Joshi (1982) defined code-switching as the situation when two languages systematically interact with each other in the production of sentences in a framework which consists of two grammatical systems and a mechanism for switching between the two. Following the second perspective, Muysken (1995) defined CS as “the alternative use by bilinguals of two or more languages in the same conversation”, while other researchers (Auer, 1999; Nilep, 2006) defined it as the “juxtaposition” of elements from two different grammatical systems within the same speech. The juxtaposition definition has been widely cited in th"
W19-4603,N16-1030,0,0.01,"®K A£ É¯ BA« System Description Deep learning and neural nets have been used extensively in the past decade and were shown to significantly outperform traditional (linear) ML models. The proclaimed advantage of deep learning is that it eliminates the need for feature engineering. Yet, there has been a growing interest recently to augment neural nets with more and more linguistic features, which has been shown to boost performance for many tasks. We use a DNN (Deep Neural Network) model mainly suited for sequence tagging and is a variant of the bi-LSTM-CRF architecture (Ma and Hovy, 2016; Lample et al., 2016; Reimers and Gurevych, 2017; Huang et al., 2015). Our implementation is mostly inspired by the work of Reimers and Gurevych (2017). In its basic configuration, it combines a double representation of the input words by using word embeddings and a character-based representation with CNNs (convolutional Neural Networks). The input sequence is processed with bi-LSTMs, and the output layer is a linear chain CRF. We augment this model with various layers to accommodate the different features we want to incorporate. The features used in our model are explained below. 4.1 Translit. / Gloss byHbk Fine"
W19-4603,D17-1035,0,0.0140169,"tem Description Deep learning and neural nets have been used extensively in the past decade and were shown to significantly outperform traditional (linear) ML models. The proclaimed advantage of deep learning is that it eliminates the need for feature engineering. Yet, there has been a growing interest recently to augment neural nets with more and more linguistic features, which has been shown to boost performance for many tasks. We use a DNN (Deep Neural Network) model mainly suited for sequence tagging and is a variant of the bi-LSTM-CRF architecture (Ma and Hovy, 2016; Lample et al., 2016; Reimers and Gurevych, 2017; Huang et al., 2015). Our implementation is mostly inspired by the work of Reimers and Gurevych (2017). In its basic configuration, it combines a double representation of the input words by using word embeddings and a character-based representation with CNNs (convolutional Neural Networks). The input sequence is processed with bi-LSTMs, and the output layer is a linear chain CRF. We augment this model with various layers to accommodate the different features we want to incorporate. The features used in our model are explained below. 4.1 Translit. / Gloss byHbk Fine Tag prog_part Coarse Tag Ve"
W19-4603,P08-2030,0,0.104649,"Missing"
W19-4603,P16-1101,0,0.0119628,".j K. QÒªË@ð J.Ê¯ ®K A£ É¯ BA« System Description Deep learning and neural nets have been used extensively in the past decade and were shown to significantly outperform traditional (linear) ML models. The proclaimed advantage of deep learning is that it eliminates the need for feature engineering. Yet, there has been a growing interest recently to augment neural nets with more and more linguistic features, which has been shown to boost performance for many tasks. We use a DNN (Deep Neural Network) model mainly suited for sequence tagging and is a variant of the bi-LSTM-CRF architecture (Ma and Hovy, 2016; Lample et al., 2016; Reimers and Gurevych, 2017; Huang et al., 2015). Our implementation is mostly inspired by the work of Reimers and Gurevych (2017). In its basic configuration, it combines a double representation of the input words by using word embeddings and a character-based representation with CNNs (convolutional Neural Networks). The input sequence is processed with bi-LSTMs, and the output layer is a linear chain CRF. We augment this model with various layers to accommodate the different features we want to incorporate. The features used in our model are explained below. 4.1 Transli"
W19-4603,W16-5806,1,0.944761,"abels ambiguous unk lang1 lang2 mixed ne other et al. (2016) explored different technique for the POS tagging of CS data and concluded that applying a machine learning framework as a voting mechanism on top of the output of two monolingual POS taggers achieves the best performance. Word-level CS identification for Arabic (along with Spanish–English) has been featured in a couple of shared tasks: the First Shared Task on Language Identification in CodeSwitched Data (Solorio et al., 2014) and the Second Shared Task on Language Identification in Code-Switched Data (Molina et al., 2016), of which Samih et al. (2016) was the winning system, and against which we compare our results in this project. Eskander et al. (2014) studied CS between EA written in Roman script (Arabizi) and English. Habash et al. (2008) created a standard annotation guidelines for CS between MSA and dialects. CS has also been studied in Arabic as a predictor of social influence in the collaborative writing in Wikipedia discussion pages in (Yoder et al., 2017) and it was found that CS is positively associated with the editor’s success in winning an argument. We notice from the literature that in some instances POS tagging has been use"
W19-4603,W16-5805,0,0.0180421,"themselves, but our results show that words still give a stronger signal than POS tags alone. We also notice that Brown Clusters, named entity gazetteers and FastText pre-trained embeddings contribute to incrementally improve the performance of the system. Unfortunately adding information from the spelling word list did not show any improvement on the system, and this is why it is removed from the final system architecture. Now we compare our best model to the state-ofthe-art system of Samih et al. (2016), which won the 2016 Second Shared Task on Language Identification in Code-Switched Data (Molina et al., 2016) on the MSA–EA dataset. We compare the performance of the two systems in terms of f-score accuracy on both the development and test set, in Table 5 and Table 6 respectively. We also include the number of instances and the ratio percentage for each label. As the tables show, the category lang2 constitutes the majority class for both úÍ@ <ilY “to”, which can equally be used as either lang1 or lang2, depending on the context. 6 Conclusion We have presented a neural network system for conducting word-level code-switching identification. Our system outperforms the current stateof-the-art, and we sh"
W19-4603,W15-3904,0,0.0125713,"tions of all the words and reduce the number of OOVs (out of vocabulary words). We find significant improvement using FastText embedding over the traditional word2vec representation (Mikolov et al., 2013). This is probably due to the utilization of sub-word (ex. prefixes or suffixes) information in the former. Character-level CNNs. Although originally designed for image recognition, CNNs have proven effective for various NLP tasks due to their ability to encode character-level representations of words as well as extract sub-word information (Collobert et al., 2011; Chiu and Nichols, 2016; dos Santos and Guimarães, 2015). Bi-LSTM Recurrent neural networks (RNN) are well suited for modeling sequential data, achieving ground-breaking results in many NLP tasks (e.g., machine translation). BiLSTMs (Hochreiter and Schmidhuber, 1997; Schuster and Paliwal, 1997) are capable of learning long-term dependencies and maintaining contextual features from both past and future states while avoiding the vanishing/exploding gradients problem. They consist of two separate bidirectional hidden layers that feed forward to the same output layer. Figure 3: DNN Architecture. BCs on our crawled code-switched corpus of 380 million wo"
W19-4603,N16-1159,0,0.025454,"Missing"
W19-4603,D08-1102,0,0.0258644,"Missing"
W19-4603,D08-1110,0,0.107844,"Missing"
W19-4603,W15-1511,0,0.061289,"Missing"
W19-4603,W15-2902,0,0.0329637,"Missing"
W19-4603,W17-2911,0,0.0157523,"n Language Identification in CodeSwitched Data (Solorio et al., 2014) and the Second Shared Task on Language Identification in Code-Switched Data (Molina et al., 2016), of which Samih et al. (2016) was the winning system, and against which we compare our results in this project. Eskander et al. (2014) studied CS between EA written in Roman script (Arabizi) and English. Habash et al. (2008) created a standard annotation guidelines for CS between MSA and dialects. CS has also been studied in Arabic as a predictor of social influence in the collaborative writing in Wikipedia discussion pages in (Yoder et al., 2017) and it was found that CS is positively associated with the editor’s success in winning an argument. We notice from the literature that in some instances POS tagging has been used to aid with the identification of code-switching points, and in some other instances language identification has been used as an indicator or a feature for POS tagging, showing what (Çetino˘glu et al., 2016) referred to as task inter-relatedness, or the cyclic nature of task dependencies. In our work, we use a POS tagger as a predictor of CS. The POS tagger used has been trained specifically on CS data. 3 Token Count"
W19-4639,W18-3930,0,0.112038,"Missing"
W19-4639,W16-4818,0,0.0429729,"Missing"
W19-4639,E17-2068,0,0.0808269,"Missing"
W19-4639,W14-3601,1,0.945204,"Missing"
W19-4639,C18-1113,0,0.0919411,"Missing"
W19-4639,K17-1043,1,0.900152,"Missing"
W19-4639,L16-1658,1,0.898269,"Missing"
W19-4639,P11-2007,0,0.0851928,"Missing"
W19-4639,J14-1006,0,0.163523,"2016). The wide spread of dialectal use has increased the richness and diversity of the language, requiring greater complexity in dealing with it. Non-standard orthography, increased borrowing and coinage of new terms, and code switching are just a few among a long list of new challenges researchers have to deal with. Studying language varieties in particular is associated with important applications such as Dialect Identification (DID), Machine Translation (MT), and other text mining tasks. Performing DID can be achieved using a variety of features, such as character n-grams (Darwish, 2014; Zaidan and Callison-Burch, 2014; Malmasi et al., 2015), and a myriad of techniques, such as 2 System descriptions For both SubTask 1 and SubTask 2, we employed a hybrid system that incorporates different classifiers and components such DNNs and heuristics to perform sentence level dialectal Arabic identification. The classification strategy is built as a cascaded voting system that tags each sequence based on the decisions from two other underlying classifiers. DNNs: This model uses both Bidirectional Long Short Term Memory (Bi-LSTM) and Convolutional Neural Network (CNN) architectures to jointly learn both word-level and c"
