Y18-1051,Lexical Substitution is Practical for Rare Word Simplification,2018,0,0,2,0,27518,takumi maruyama,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation",0,None
Y18-1057,Dataset Construction Method for Word Reading Disambiguation,2018,0,0,2,0,27527,koki nishiyama,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation",0,None
Y18-1081,{V}iet{S}enti{L}ex: a sentiment dictionary that considers the polarity of ambiguous sentiment words,2018,0,0,2,0,27575,huynh vo,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation",0,None
L18-1072,Crowdsourced Corpus of Sentence Simplification with Core Vocabulary,2018,0,2,2,0,29581,akihiro katsuta,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1185,Simplified Corpus with Core Vocabulary,2018,0,2,2,0,27518,takumi maruyama,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
W17-5702,Controlling Target Features in Neural Machine Translation via Prefix Constraints,2017,0,3,3,1,31471,shunsuke takeno,Proceedings of the 4th Workshop on {A}sian Translation ({WAT}2017),0,"We propose \textit{prefix constraints}, a novel method to enforce constraints on target sentences in neural machine translation. It places a sequence of special tokens at the beginning of target sentence (target prefix), while side constraints places a special token at the end of source sentence (source suffix). Prefix constraints can be predicted from source sentence jointly with target sentence, while side constraints (Sennrich et al., 2016) must be provided by the user or predicted by some other methods. In both methods, special tokens are designed to encode arbitrary features on target-side or metatextual information. We show that prefix constraints are more flexible than side constraints and can be used to control the behavior of neural machine translation, in terms of output length, bidirectional decoding, domain adaptation, and unaligned target word generation."
W16-4615,Integrating empty category detection into preordering Machine Translation,2016,15,2,3,1,31471,shunsuke takeno,Proceedings of the 3rd Workshop on {A}sian Translation ({WAT}2016),0,"We propose a method for integrating Japanese empty category detection into the preordering process of Japanese-to-English statistical machine translation. First, we apply machine-learning-based empty category detection to estimate the position and the type of empty categories in the constituent tree of the source sentence. Then, we apply discriminative preordering to the augmented constituent tree in which empty categories are treated as if they are normal lexical symbols. We find that it is effective to filter empty categories based on the confidence of estimation. Our experiments show that, for the IWSLT dataset consisting of short travel conversations, the insertion of empty categories alone improves the BLEU score from 33.2 to 34.3 and the RIBES score from 76.3 to 78.7, which imply that reordering has improved For the KFTT dataset consisting of Wikipedia sentences, the proposed preordering method considering empty categories improves the BLEU score from 19.9 to 20.2 and the RIBES score from 66.2 to 66.3, which shows both translation and reordering have improved slightly."
P15-3006,Evaluation Dataset and System for {J}apanese Lexical Simplification,2015,15,7,2,1,367,tomoyuki kajiwara,Proceedings of the {ACL}-{IJCNLP} 2015 Student Research Workshop,0,"We have constructed two research resources of Japanese lexical simplification. One is a simplification system that supports reading comprehension of a wide range of readers, including children and language learners. The other is a dataset for evaluation that enables open discussions with other systems. Both the system and the dataset are made available providing the first such resources for the Japanese language."
D15-1156,Empty Category Detection using Path Features and Distributed Case Frames,2015,14,5,3,1,31471,shunsuke takeno,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"We describe an approach for machine learning-based empty category detection that is based on the phrase structure analysis of Japanese. The problem is formalized as tree node classification, and we find that the path feature, the sequence of node labels from the current node to the root, is highly effective. We also find that the set of dot products between the word embeddings for a verb and those for case particles can be used as a substitution for case frames. Experiments show that the proposed method outperforms the previous state-of the art method by 68.6% to 73.2% in terms of F-measure."
Y14-1073,Noun Paraphrasing Based on a Variety of Contexts,2014,9,0,2,1,367,tomoyuki kajiwara,"Proceedings of the 28th Pacific Asia Conference on Language, Information and Computing",0,"We paraphrase nouns along the contexts of sentence input on the basis of a variety of contexts obtained from a large-scale corpus. The proposed method only uses the number of types of context, not word frequency or co- occurrence frequency features. This method is based on the notion that paraphrase candidates appear more commonly with target words in the same context. The results of our experi- ment demonstrate that the approach can pro- duce more appropriate paraphrases than ap- proaches based on co-occurrence frequency and pointwise mutual information."
O13-1007,Selecting Proper Lexical Paraphrase for Children,2013,7,13,3,1,367,tomoyuki kajiwara,Proceedings of the 25th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2013),0,"We propose a method for acquiring plain lexical paraphrase using a Japanese dictionary in order to achieve lexical simplification for children. The proposed method extracts plain words that are the most similar to the headword from the dictionary definition. The definition statements describe the headword using plain words; therefore, paraphrasing by replacing the headword with the most similar word in the dictionary definition is expected to be an accurate means of lexical simplification. However, it is difficult to determine which word is the most appropriate for the paraphrase. The method proposed in this paper measures the similarity of each word in the definition statements against the headword and selects the one with the closest semantic match for the paraphrase. This method compares favorably with the method that acquires the target word from the end of the definition statements."
2011.mtsummit-papers.40,A Comparison of Unsupervised Bilingual Term Extraction Methods Using Phrase-Tables,2011,-1,-1,2,0,44906,masamichi ideue,Proceedings of Machine Translation Summit XIII: Papers,0,None
Y10-1074,Detecting Nasty Comments from {BBS} Posts,2010,7,1,2,0,45044,tatsuya ishisaka,"Proceedings of the 24th Pacific Asia Conference on Language, Information and Computation",0,"We propose a method to detect Japanese nasty comments from posts on bulletin board systems (BBS). Nasty comments can cause many social problem, because they express potentially harmful words and phrases. There are methods to recognize harmful words, but they are insufficient. Therefore, we present a method for detecting such comments on a BBS with many posts using an n-gram model. In addition, we compared our method with a support vector machine (SVM) that is based on nasty words. As a result, we detected nasty comments that are different to those by the SVM. We also observe higher detection accuracy by combining two methods."
Y10-1080,Generation of Summaries that Appropriately and Adequately Express the Contents of Original Documents Using Word-Association Knowledge,2010,15,0,5,0,45050,kazuki takigawa,"Proceedings of the 24th Pacific Asia Conference on Language, Information and Computation",0,"In this study, our purpose was to make a short summary for sentences. For example, we aimed to make a short summary xe2x80x9cterrorxe2x80x9d for sentences xe2x80x9cA bomb went off. Some people were killed. This was triggered by rebel campaign.xe2x80x9d In this study, we proposed a new method that generates summaries that can appropriately and adequately express the contents of their respective original documents using word-association knowledge. In this method, we assumed that a good summary comprises words that can express the contents of the original document and does not contain words that are unable to express the contents of the original document. Using statistical tests, we confirmed that the use of elements in our method was beneficial. Our method obtained 0.75 as the ratio where the top 10 summaries for each document include a correct summary and 0.45 as the mean reciprocal rank (MRR) in the xe2x80x9clenientxe2x80x9d case of experiments."
W10-3906,{E}ven Unassociated Features Can Improve Lexical Distributional Similarity,2010,11,8,1,1,27519,kazuhide yamamoto,Proceedings of the Second Workshop on {NLP} Challenges in the Information Explosion Era ({NLPIX} 2010),0,"This paper presents a new computation of lexical distributional similarity, which is a corpus-based method for computing similarity of any two words. Although the conventional method focuses on emphasizing features with which a given word is associated, we propose that even unassociated features of two input words can further improve the performance in total. We also report in addition that more than 90% of the features has no contribution and thus could be reduced in future."
W10-3501,Constructing Large-Scale Person Ontology from {W}ikipedia,2010,7,3,3,0,45230,yumi shibaki,Proceedings of the 2nd Workshop on {T}he {P}eople{'}s {W}eb {M}eets {NLP}: {C}ollaboratively {C}onstructed {S}emantic {R}esources,0,"This paper presents a method for constructing a large-scale Person Ontology with category hierarchy from Wikipedia. We first extract Wikipedia category labels which represent person (hereafter, Wikipedia Person Category, WPC) by using a machine learning classifier. We then construct a WPC hierarchy by detecting is-a relations in the Wikipedia category network. We then extract the titles of Wikipedia articles which represent person (hereafter, Wikipedia person instance, WPI). Experiments show that the accuracy of WPC extraction is 99.3% precision and 98.4% recall, while that of WPI extraction is 98.2% and 98.6%, respectively. The accuracies are significantly higher than the previous methods."
W10-3404,"Textual Entailmaint Recognition using Word Overlap, Mutual Information and Subpath Set",2010,5,2,3,0,45241,yuki muramatsu,Proceedings of the 2nd Workshop on Cognitive Aspects of the Lexicon,0,"When two texts have an inclusion relation, the relationship between them is called entailment. The task of mechanically distinguishing such a relation is called recognising textual entailment (RTE), which is basically a kind of semantic analysis. A variety of methods have been proposed for RTE. However, when the previous methods were combined, the performances were not clear. So, we utilized each method as a feature of machine learning, in order to combine methods. We have dealt with the binary classification problem of two texts exhibiting inclusion, and proposed a method that uses machine learning to judge whether the two texts present the same content. We have built a program capable to perform entailment judgment on the basis of word overlap, i.e. the matching rate of the words in the two texts, mutual information, and similarity of the respective syntax trees (Subpath Set). Word overlap was calclated by utilizing BiLingual Evaluation Understudy (BLEU). Mutual information is based on co-occurrence frequency, and the Subpath Set was determined by using the Japanise WordNet. A ConfidenceWeighted Score of 68.6% was obtained in the mutual information experiment on RTE. Mutual information and the use of three methods of SVM were shown to be effective."
W10-3302,Using Goi-Taikei as an Upper Ontology to Build a Large-Scale {J}apanese Ontology from {W}ikipedia,2010,1,4,3,0,3634,masaaki nagata,Proceedings of the 6th Workshop on {O}ntologies and {L}exical {R}esources,0,"We present a novel method for building a large-scale Japanese ontology from Wikipedia using one of the largest Japanese thesauri, Nihongo Goi-Taikei (referred to hereafter as iGoi-Taikeii) as an upper ontology. First, The leaf categories in the Goi-Taikei hierarchy are semi-automatically aligned with semantically equivalent Wikipedia categories. Then, their subcategories are created automatically by detecting is-a links in the Wikipedia category network below the junction using the knowledge dene d in Goi-Taikei above the junction. The resulting ontology has a well-dene d taxonomy in the upper level and a ne-gra ined taxonomy in the lower level with a large number of up-to-date instances. A sample evaluation shows that the precisions of the extracted categories and instances are 92.8% and 98.6%, respectively."
2009.mtsummit-posters.10,Development of a {J}apanese-{E}nglish Software Manual Parallel Corpus,2009,-1,-1,4,0,45044,tatsuya ishisaka,Proceedings of Machine Translation Summit XII: Posters,0,None
Y08-1030,Generating Story Reviews Using Phrases Expressing Emotion,2008,5,0,2,0,47641,hiroshi ota,"Proceedings of the 22nd Pacific Asia Conference on Language, Information and Computation",0,"This paper presents a method for generating reviews of stories. In this work, we focus on generating sentences that include subjective expressions. First, we constructed lexicons using emotion-emerged expressions that are thought to be the origin of the emotional content. The lexicon consists of syntactic pieces that are proposed as units of syntactic structure to represent suitable units of expression for estimating emotions. We confirmed the effectiveness of the lexicons by estimating the emotional content of blog text. Second, we confirmed the relationship between the naturalness of the sentences and the coherence of the emotion it expresses. Finally, we proposed a method of review generation using the lexicons."
Y08-1043,Extracting Troubles from Daily Reports based on Syntactic Pieces,2008,2,1,2,0,47661,yoshifumi kakimoto,"Proceedings of the 22nd Pacific Asia Conference on Language, Information and Computation",0,"It is expensive for companies to browse daily reports. Our aim is to create a system that extracts information about problems from reports. This system operates in two steps. First, it records expressions involving troubles in a dictionary from training data. Second, it expands the dictionary to include information not included in the training data. We experimentally tested this extraction system; in the tests, a two-values classifier attained an F-value of 0.772, and experimental extraction of troubles attained a precision of 0.400 and a recall of 0.827."
I08-2102,Summarization by Analogy: An Example-based Approach for News Articles,2008,8,0,2,0,48627,megumi makino,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{II},0,"Automatic summarization is an important task as a form of human support technology. We propose in this paper a new summarization method that is based on example-based approach. Using example-based approach for the summarization task has the following three advantages: high modularity, absence of the necessity to score importance for each word, and high applicability to local context. Experimental results have proven that the summarization system attains approximately 60% accuracy by human judgment."
Y07-1007,Opinion Extraction based on Syntactic Pieces,2007,0,0,2,0,48844,suguru aoki,"Proceedings of the 21st Pacific Asia Conference on Language, Information and Computation",0,None
P06-1090,A Clustered Global Phrase Reordering Model for Statistical Machine Translation,2006,11,38,3,0,3634,masaaki nagata,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"In this paper, we present a novel global reordering model that can be incorporated into standard phrase-based statistical machine translation. Unlike previous local reordering models that emphasize the reordering of adjacent phrase pairs (Till-mann and Zhang, 2005), our model explicitly models the reordering of long distances by directly estimating the parameters from the phrase alignments of bilingual training sentences. In principle, the global phrase reordering model is conditioned on the source and target phrases that are currently being translated, and the previously translated source and target phrases. To cope with sparseness, we use N-best phrase alignments and bilingual phrase clustering, and investigate a variety of combinations of conditioning factors. Through experiments, we show, that the global reordering model significantly improves the translation accuracy of a standard Japanese-English translation task."
N06-2034,Using Phrasal Patterns to Identify Discourse Relations,2006,2,33,2,0,50069,manami saito,"Proceedings of the Human Language Technology Conference of the {NAACL}, Companion Volume: Short Papers",0,"This paper describes a system which identifies discourse relations between two successive sentences in Japanese. On top of the lexical information previously proposed, we used phrasal pattern information. Adding phrasal information improves the system's accuracy 12%, from 53% to 65%."
I05-5006,Transforming a Sentence End into News Headline Style,2005,3,2,2,0,46829,satoshi ikeda,Proceedings of the Third International Workshop on Paraphrasing ({IWP}2005),0,"xe6x96xb0xe5xb9xb9xe7xb7x9axe8xa6x81xe7xb4x84, xe3x81x99xe3x81xaaxe3x82x8fxe3x81xa1xe6x96xb0xe5xb9xb9xe7xb7x9axe8xbbx8axe5x86x85xe3x82x84xe8xa1x97xe9xa0xadxe3x81xa7xe3x81xaexe9x9bxbbxe5x85x89xe6x8exb2xe7xa4xbaxe6x9dxbfxe3x81xa7xe6xb5x81xe3x82x8cxe3x82x8bxe3x83x8bxe3x83xa5xe3x83xbcxe3x82xb9xe3x81xafxe7xb0xa1xe6xbdx94xe3x81xabxe8xa1xa8xe7x8fxbexe3x81x95xe3x82x8cxe3x81xa6xe3x81x8axe3x82x8a, xe3x81x93xe3x81xaexe3x81x9fxe3x82x81xe3x81xabxe7x8bxacxe7x89xb9xe3x81xaexe8xa1xa8xe7x8fxbexe3x82x92xe3x81x97xe3x81xa6xe3x81x84xe3x82x8b.xe6x9cxacxe8xabx96xe6x96x87xe3x81xa7xe3x81xafxe3x81x93xe3x81xaexe7x89xb9xe5xbexb4xe7x9ax84xe3x81xaaxe8xa1xa8xe7x8fxbexe3x81xaexe3x81x86xe3x81xa1xe4xbdx93xe8xa8x80xe6xadxa2xe3x82x81xe3x82x84xe5x8axa9xe8xa9x9exe6xadxa2xe3x82x81xe3x81xa8xe3x81x84xe3x81xa3xe3x81x9fxe6x96x87xe6x9cxabxe8xa1xa8xe7x8fxbexe3x81xabxe7x9dx80xe7x9bxaexe3x81x97, xe4xb8x80xe8x88xacxe7x9ax84xe3x81xaaxe6x96xb0xe8x81x9exe8xa8x98xe4xbax8bxe3x81xaexe8xa1xa8xe7x8fxbexe3x82x92xe3x81x93xe3x81xaexe3x82x88xe3x81x86xe3x81xaaxe9xabx98xe5xafx86xe5xbaxa6xe8xa1xa8xe7x8fxbexe3x81xabxe5x8axa0xe5xb7xa5xe3x81x99xe3x82x8bxe6x89x8bxe6xb3x95xe3x82x92xe6x8fx90xe6xa1x88xe3x81x99xe3x82x8b.xe3x81xbexe3x81x9a, xe5xaex9fxe9x9ax9bxe3x81xab2xe4xb8x87xe8xa8x98xe4xbax8bxe3x81xabxe5x8fx8axe3x81xb6xe6x96xb0xe5xb9xb9xe7xb7x9axe8xa6x81xe7xb4x84xe3x81xaexe8xa1xa8xe7x8fxbexe3x81xaexe7x89xb9xe5xbexb4xe3x82x92xe8xaaxbfxe6x9fxbbxe3x81x97, xe6x96x87xe6x9cxabxe3x81xabxe3x81x8axe3x81x91xe3x82x8bxe3x82xb5xe5xa4x89xe5x90x8dxe8xa9x9exe3x81xa7xe3x81xaexe4xbdx93xe8xa8x80xe6xadxa2xe3x82x81xe3x81x8cxe4xb8x80xe8x88xacxe3x81xaexe6x96xb0xe8x81x9exe8xa8x98xe4xbax8bxe3x81xae8xe5x80x8d, xe6xa0xbcxe5x8axa9xe8xa9x9exe3x81xa7xe3x81xaexe5x8axa9xe8xa9x9exe6xadxa2xe3x82x81xe3x81x8cxe4xb8x80xe8x88xacxe3x81xae20xe5x80x8dxe3x81x82xe3x82x8bxe3x81x93xe3x81xa8xe3x82x92xe7xa2xbaxe8xaax8dxe3x81x97, xe6x96xb0xe5xb9xb9xe7xb7x9axe8xa6x81xe7xb4x84xe3x81xabxe3x81x8axe3x81x91xe3x82x8bxe8xa1xa8xe7x8fxbexe3x81xaexe7x89xb9xe7x95xb0xe6x80xa7xe3x82x92xe7xa2xbaxe8xaax8dxe3x81x97xe3x81x9f.xe6xacxa1xe3x81xab, xe3x81x93xe3x81xaexe3x82x88xe3x81x86xe3x81xaaxe6x96x87xe6x9cxabxe8xa1xa8xe7x8fxbexe3x82x92xe5xaex9fxe7x8fxbexe3x81x99xe3x82x8bxe3x81x9fxe3x82x81xe3x81xaexe6x8fx90xe6xa1x88xe6x89x8bxe6xb3x95xe3x82x92xe5xaex9fxe8xa3x85xe3x81x97, xe6x96xb0xe8x81x9exe8xa8x98xe4xbax8bxe3x82x92xe5x85xa5xe5x8ax9bxe3x81xa8xe3x81x97xe3x81xa6xe8xa6x81xe7xb4x84xe3x81x97xe3x81x9f.xe3x81x93xe3x81xaexe7xb5x90xe6x9ex9c, xe6x96x87xe6x9cxabxe8xa1xa8xe7x8fxbexe3x81xabxe9x99x90xe5xaex9axe3x81x97xe3x81x9fxe8xa6x81xe7xb4x84xe7x8ex87xe3x81xaf12%xe3x81xa7xe3x81x82xe3x82x8a, 1xe6x96x87xe5xbdx93xe3x81x9fxe3x82x8axe5xb9xb3xe5x9dx87xe3x81x97xe3x81xa62.5xe6x96x87xe5xadx97xe5x89x8axe9x99xa4xe3x81x99xe3x82x8bxe3x81x93xe3x81xa8xe3x81x8cxe3x81xa7xe3x81x8dxe3x81x9f.xe3x81x93xe3x81xaexe7xb5x90xe6x9ex9cxe3x82x92xe4xbaxbaxe9x96x93xe3x81x8cxe8xa1x8cxe3x81xaaxe3x81xa3xe3x81x9fxe6x96x87xe6x9cxabxe6x95xb4xe5xbdxa2xe3x81xaexe7xb5x90xe6x9ex9cxe3x81xa8xe6xafx94xe8xbcx83xe3x81x97xe3x81x9fxe3x81xa8xe3x81x93xe3x82x8d, xe8xa6x81xe7xb4x84xe7x8ex87xe3x81xafxe3x81xbbxe3x81xbcxe5x90x8cxe6xa7x98xe3x81xaexe7xb5x90xe6x9ex9cxe3x81x8cxe5xbex97xe3x82x89xe3x82x8cxe3x81x9f.xe3x81x95xe3x82x89xe3x81xab, xe5x87xbaxe5x8ax9bxe8xa1xa8xe7x8fxbexe3x81xaexe8xa9x95xe4xbexa1xe3x82x92xe8xa1x8cxe3x81xaaxe3x81xa3xe3x81x9fxe7xb5x90xe6x9ex9c, xe6xadxa3xe8xa7xa3xe7x8ex87xe3x81xaf95%xe3x81xa8xe3x81xaaxe3x81xa3xe3x81x9f."
I05-2008,A System to Solve Language Tests for Second Grade Students,2005,-1,-1,2,0,50069,manami saito,Companion Volume to the Proceedings of Conference including Posters/Demos and tutorial abstracts,0,None
2005.iwslt-1.16,{NUT}-{NTT} Statistical Machine Translation System for {IWSLT} 2005,2005,7,19,2,0,49988,kazuteru ohashi,Proceedings of the Second International Workshop on Spoken Language Translation,0,"In this paper, we present a novel distortion model for phrase-based statistical machine translation. Unlike the previous phrase distortion models whose role is to simply penalize nonmonotonic alignments[1, 2], the new model assigns the probability of relative position between two source language phrases aligned to the two adjacent target language phrases. The phrase translation probabilities and phrase distortion probabilities are calculated from the N-best phrase alignment of the training bilingual sentences. To obtain Nbest phrase alignment, we devised a novel phrase alignment algorithm based on word translation probabilities and N-best search. Experiments show that the phrase distortion model and phrase translation model improve the BLEU and NIST scores over the baseline method."
paik-etal-2004-comparison,A Comparison of Two Variant Corpora: The Same Content with Different Source,2004,2,0,3,0,51307,kyonghee paik,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"In order to investigate the effect of source language on translations, we investigate two variants of a Korean translation corpus. The first variant consists of Korean translations of 162,308 Japanese sentences from the ATR BTEC (Basic Expression Text Corpus). The second variant was made by translating the English translations of the Japanese sentences into Korean. We show that the source language text has a large influence on the target text. Even after normalizing orthographic differences, fewer than 8.3{\textbackslash}{\%} of the sentences in the two variants were identical. We describe in general which phenomena differ and then discuss how our analysis can be used in natural language processing."
C04-1102,Detecting Transliterated Orthographic Variants via Two Similarity Metrics,2004,3,4,3,1,35675,kiyonori ohtake,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,We propose a detection method for orthographic variants caused by transliteration in a large corpus. The method employs two similarities. One is string similarity based on edit distance. The other is contextual similarity by a vector space model. Experimental results show that the method performed a 0.889 F-measure in an open test.
Y03-1042,Applicability Analysis of Corpus-derived Paraphrases toward Example-based Paraphrasing,2003,0,0,2,1,35675,kiyonori ohtake,"Proceedings of the 17th Pacific Asia Conference on Language, Information and Computation",0,None
W02-1411,Acquisition of Lexical Paraphrases from Texts,2002,10,15,1,1,27519,kazuhide yamamoto,{COLING}-02: {COMPUTERM} 2002: Second International Workshop on Computational Terminology,0,"Automatic acquisition of paraphrase knowledge for content words is proposed. Using only a non-parallel text corpus, we compute the para-phrasability metrics between two words from their similarity in context. We then filter words such as proper nouns from external knowledge. Finally, we use a heuristic in further filtering to improve the accuracy of the automatic acquisition. In this paper, we report the results of acquisition experiments."
shirai-etal-2002-towards,Towards a Thesaurus of Predicates,2002,4,2,2,0,51459,satoshi shirai,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"We propose a thesaurus of predicates that can help to resolve pre-editing and/or post-editing problems in machine translation environments. It differs from earlier approaches such as conventional dictionaries in that we are aiming to link a wide range of near-synonyms and paraphrases. We are compiling such similar examples through both introspection and the use of translation data, giving us a large collection of monolingual and bilingual equivalences. This thesaurus enables the following machine translation techniques. (a) Unification of synonymous expressions in the source language (source language paraphrasing). (b) Conversion of homonymous expressions to more easily translated ones (source language rewriting). (c) Development of expressions appearing in the target language into various expressions (target language paraphrasing)."
C02-1056,Paraphrasing of {C}hinese Utterances,2002,5,13,2,0,9062,yujie zhang,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"One of the key issues in spoken language translation is how to deal with unrestricted expressions in spontaneous utterances. This research is centered on the development of a Chinese paraphraser that automatically paraphrases utterances prior to transfer in Chinese-Japanese spoken language translation. In this paper, a pattern-based approach to paraphrasing is proposed for which only morphological analysis is required. In addition, a pattern construction method is described through which paraphrasing patterns can be efficiently learned from a paraphrase corpus and human experience. Using the implemented paraphraser and the obtained patterns, a paraphrasing experiment was conducted and the results were evaluated."
C02-1163,Machine Translation by Interaction between Paraphraser and Transfer,2002,5,15,1,1,27519,kazuhide yamamoto,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"A machine translation model has been proposed where an input is translated through both source-language and target-language paraphrasing processes. We have implemented our prototype model for the Japanese-Chinese language pair. This paper. describes our core idea of translation, where a source language paraphraser and a language transfer cooperates in translation by exchanging information about the source input."
2002.tmi-papers.21,Corpus-assisted expansion of manual {MT} knowledge:,2002,6,2,3,1,42063,setsuo yamada,Proceedings of the 9th Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages: Papers,0,"Since the expansion of MT knowledge is currently being performed by humans, it is taking too long and is too expensive. This paper proposes a new procedure that expands MT knowledge efficiently by supporting human judgements with information automatically collected from any number of corpora. The new procedure uses the source knowledge present in an MT system as the key to retrieve source language information from corpora. It also uses the partial translations provided by the MT to acquire target language information. These two techniques can reduce time and labor costs. Experimental results confirm both benefits."
W99-0207,Corpus-Based Anaphora Resolution Towards Antecedent Preference,1999,13,8,2,0,12388,michael paul,Coreference and Its Applications,0,"In this paper we propose a corpus-based approach to anaphora resolution combining a machine learning method and statistical information. First, a decision tree trained on an annotated corpus determines the coreference relation of a given anaphor and antecedent candidates and is utilized as a filter in order to reduce the number of potential candidates. In the second step, preference selection is achieved by taking into account the frequency information of coreferential and non-referential pairs tagged in the training corpus as well as distance features within the current discourse. Preliminary experiments concerning the resolution of Japanese pronouns in spoken-language dialogs result in a success rate of 80.6%."
1999.mtsummit-1.34,Solutions to problems inherent in spoken-language translation: the {ATR}-{MATRIX} approach,1999,8,39,3,0,129,eiichiro sumita,Proceedings of Machine Translation Summit VII,0,"ATR has built a multi-language speech translation system called ATR-MATRIX. It consists of a spoken-language translation subsystem, which is the focus of this paper, together with a highly accurate speech recognition subsystem and a high-definition speech synthesis subsystem. This paper gives a road map of solutions to the problems inherent in spoken-language translation. Spoken-language translation systems need to tackle difficult problems such as ungrammaticality. contextual phenomena, speech recognition errors, and the high-speeds required for real-time use. We have made great strides towards solving these problems in recent years. Our approach mainly uses an example-based translation model called TDMT. We have added the use of extra-linguistic information, a decision tree learning mechanism, and methods dealing with recognition errors."
P98-2233,Feasibility Study for Ellipsis Resolution in Dialogues by Machine-Learning Technique,1998,5,8,1,1,27519,kazuhide yamamoto,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,"A method for resolving the ellipses that appear in Japanese dialogues is proposed. This method resolves not only the subject ellipsis, but also those in object and other grammatical cases. In this approach, a machine-learning algorithm is used to select the attributes necessary for a resolution. A decision tree is built, and used as the actual ellipsis resolver. The results of blind tests have shown that the proposed method was able to provide a resolution accuracy of 91.7% for indirect objects, and 78.7% for subjects with a verb predicate. By investigating the decision tree we found that topic-dependent attributes are necessary to obtain high performance resolution, and that indispensable attributes vary according to the grammatical case. The problem of data size relative to decision-tree training is also discussed."
P98-1070,Splitting Long or Ill-formed Input for Robust Spoken-language Translation,1998,3,11,3,0,49846,osamu furuse,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",1,"This paper proposes an input-splitting method for translating spoken-language which includes many long or ill-formed expressions. The proposed method splits input into well-balanced translation units based on a semantic distance calculation. The splitting is performed during left-to-right parsing, and does not degrade translation efficiency. The complete translation result is formed by concatenating the partial translation results of each split unit. The proposed method can be incorporated into frameworks like TDMT, which utilize left-to-right parsing and a score for a substructure. Experimental results show that the proposed method gives TDMT the following advantages: (1) elimination of null outputs, (2) splitting of utterances into sentences, and (3) robust translation of erroneous speech recognition results."
C98-2228,Feasibility Study for Ellipsis Resolution in Dialogues by Machine-Learning Technique,1998,5,8,1,1,27519,kazuhide yamamoto,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,"A method for resolving the ellipses that appear in Japanese dialogues is proposed. This method resolves not only the subject ellipsis, but also those in object and other grammatical cases. In this approach, a machine-learning algorithm is used to select the attributes necessary for a resolution. A decision tree is built, and used as the actual ellipsis resolver. The results of blind tests have shown that the proposed method was able to provide a resolution accuracy of 91.7% for indirect objects, and 78.7% for subjects with a verb predicate. By investigating the decision tree we found that topic-dependent attributes are necessary to obtain high performance resolution, and that indispensable attributes vary according to the grammatical case. The problem of data size relative to decision-tree training is also discussed."
C98-1067,Splitting Long or Ill-formed Input for Robust Spoken-language Translation,1998,3,11,3,0,49846,osamu furuse,{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics,0,"This paper proposes an input-splitting method for translating spoken-language which includes many long or ill-formed expressions. The proposed method splits input into well-balanced translation units based on a semantic distance calculation. The splitting is performed during left-to-right parsing, and does not degrade translation efficiency. The complete translation result is formed by concatenating the partial translation results of each split unit. The proposed method can be incorporated into frameworks like TDMT, which utilize left-to-right parsing and a score for a substructure. Experimental results show that the proposed method gives TDMT the following advantages: (1) elimination of null outputs, (2) splitting of utterances into sentences, and (3) robust translation of erroneous speech recognition results."
