2020.aacl-main.29,W05-0909,0,0.025349,"0.49 0.308 0.506 0.347 0.524 0.322 0.505 ROUGEL BLEU Extractor BLEU ROUGEL 0.335 0.557 0.655 0.708 0.746 0.757 0.71 0.764 0.489 0.618 0.8 0.6 Extractor Ratio 0.6 0.4 0.4 2 4 6 4.4 Experimental Results The performances of our KB-to-text generator and triple extractor are shown in the left and right of Table 2 respectively. Both generator and extractor of our model outperform all baseline models significantly and consistently. The comparison between our EGD model and the supervised SEG model indicates that our unsupervised EGD model 264 6 2 4 6 0.40 0.35 0.3 2 PDG 4 0.45 0.4 ton, 2002), METEOR (Banerjee and Lavie, 2005), ROUGEL (Lin, 2004), and CIDEr (Vedantam et al., 2015). These metrics are calculated with the evaluation code provided in Novikova et al. (2017). Moreover, we also evaluate the performance of the extractor with precision, recall, and F1 scores (Manning et al., 2010). In PDG, we set α = 0.8, β1 = 0.2, β2 = 0.6. We firstly pre-train the extractor and the generator in the PDG model with the data generated by PDG until convergence. All other models are fine-tuned on the PDG model. For the DL model, we train the generator for 5 steps with the txt2txt process and train the extractor with the kb2kb"
2020.aacl-main.29,N19-1071,0,0.0245563,"s very expensive to prepare. Many methods have been proposed to tackle the dataset insufficiency problem in other tasks. Fu et al. (2020c) propose to directly train the model on partially-aligned data in which the data and the text are not necessarily exactly math, and it can be built automatically. He et al. (2016); Sennrich et al. (2016); Yi et al. (2017) propose dual learning frameworks. They pre-train a weak model with parallel data and refine the model with monolingual data. This strategy has been applied in many related tasks including semantic parsing (Cao et al., 2019), summarization (Baziotis et al., 2019) and 259 pre-train Text PDG KB Eθ PDG KB split G Text KB1 … KB2 Gφ … Text2 Text1 E Textn E Eθ KB KB1 LE kb2kb KB2 Text KBn KB Gφ Data from corpus Eθ KB E/G split Gφ … Text KBn … KB2 KB1 … Text2 ARLG <latexit sha1_base64=""Y8Oe2xpmNErGDbs8P67Jf4pRbrs="">AAAB+XicbVDLSsNAFJ3UV62vVFfSzWARXJWkRZuCi4IILly0YGuhCWEynbZDJw9mJoUS8iduXCji1v9w4U6/xknbhVYPDBzOuZd75ngRo0IaxqeWW1vf2NzKbxd2dvf2D/TiYVeEMcekg0MW8p6HBGE0IB1JJSO9iBPke4zce5OrzL+fEi5oGNzJWUQcH40COqQYSSW5um77SI4xYslt6ibXacHVy0alYRm1RhUaFcu6qNbPFTHmgOaSlJtFu/TVPn5vufqHPQhx7JNAYoaE6JtGJJ0EcUkxI2nBjgWJEJ6gEekrGiCfCCeZJ0/hqVIGcBhy9QIJ5+rPjQT5Qsx8T01mOcWql"
2020.aacl-main.29,P19-1007,0,0.274549,"ship enables the design of a closed-loop learning framework in which we link KB-to-text generation and its dual task of triple extraction so as to reconstruct the unaligned KB triples and texts. The non-differentiability issue of picking words from our neural model before reconstruction makes it hard to train the extractor or generator effectively using backpropagation. To solve this issue, we apply Reinforcement Learning (RL) based on policy gradients into our dual learning framework to optimize our extractor or generator according to the rewards. Some semi-supervised works (He et al., 2016; Cao et al., 2019) have been proposed to generate 258 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 258–268 c December 4 - 7, 2020. 2020 Association for Computational Linguistics 101 Helena is discovered by James Craig Watson who was born in Canada. E <101 Helena, discoverer, James Craig Watson> 1.0 1.0 <James Craig Watson, nationality, Canada>, 0.9 <James Craig Watson, profession, Writer>, 0.5 <James Craig Watson, deathPlace, Australia> 0.5 Traditional RL G 101 Hele"
2020.aacl-main.29,2020.emnlp-main.90,1,0.674992,"pre-train the weak model. Another line of research proposes to use some extra annotations instead of using aligned data. Lample et al. (2018a,b) propose to train an unsupervised NMT system based on few annotated word pairs (Conneau et al., 2018). Luo et al. (2019) propose to generate pseudo data with a rule-based template (Li et al., 2018). However, these models cannot be directly applied in our scenario since our dataset is too complicated to make these annotations. Fu et al. (2020b) propose to utilize topic information from a dynamic topic tracker to solve the dataset insufficiency problem. Cheng et al. (2020) propose to generate better text description for a few entities by exploring the knowledge from KB and distill the useful part. In the field of computer vision, Zhu et al. (2017) propose cycleGAN which uses a cycled training method that transforms the input into another data form and then transforms it back, minimizing the recover loss. The method works well in the image domain but has some problems in text generation considering the non-differentiable discrete layer. We follow the ideas of cycleGAN to train the whole model without supervised data and adopt the RL method proposed in dual learn"
2020.aacl-main.29,D18-1426,0,0.0174228,"ise, it is sampled form the next token in Ks . This process can be expressed mathematically as:  w ∼ p(w) ri > α    i−1 X T˜i = ,  Ks [1 + 1(T˜j ∈ Ks )] otherwise   j=1 in which 1(C) = 1 if condition C is true and 0 otherwise. T˜j ∈ Ks indicates whether the word T˜j is sampled from Ks . This pseudo text data is used to solve the cold start problem when training the extractor. Pseudo KB Generator generates pseudo KB triples for each text and form a pseudo supervised training data. This data is used to solve the cold start problem when pre-training the generator. Similar with the work of Freitag and Roy (2018), for an input sequence T we randomly remove words in the input text with a probability β1 and sample new words by sampling words from a distribution ˜ with a probability β2 . The generated sequence K is the pseudo KB sequence for each text. Similar to the Pseudo Text Generator, we randomly add some words by sampling from the distribution p(w). We do not use the probability calculated from Kt since it may sample some wrong relations or wrong entity names which undermines the performance. Mathematically, it can be expressed as: 3.5 Traditional reinforcement learning for sequence generation calc"
2020.aacl-main.29,2020.coling-main.215,1,0.904227,"paper is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Codes: 14204418). “101 Helena is discovered by James Craig Watson.”. Recently, many research works have been proposed for this task. For example, Gardent et al. (2017a,b) create the WebNLG dataset to generate description for triples sampled from DBPedia (Auer et al., 2007). Lebret et al.’s (2016) method generates people’s biographies from extracted Wikipedia infobox. Novikova et al. (2017) propose to generate restaurant reviews by some given attributes and Fu et al. (2020a) create the WikiEvent dataset to generate text based on an event chain. However, the works mentioned above usually map structured triples to text via a supervised seq-to-seq (Sutskever et al., 2014) model, in which large amounts of annotated data is necessary and the annotation is very expensive and time-consuming. We aim to tackle the problem of completely unsupervised KB-to-text generation which only requires a text corpus and a KB corpus and does not assume any alignment between them. We propose a dual learning framework based on the inverse relationship between the KB-to-text generation"
2020.aacl-main.29,2020.emnlp-main.738,1,0.939481,"paper is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Codes: 14204418). “101 Helena is discovered by James Craig Watson.”. Recently, many research works have been proposed for this task. For example, Gardent et al. (2017a,b) create the WebNLG dataset to generate description for triples sampled from DBPedia (Auer et al., 2007). Lebret et al.’s (2016) method generates people’s biographies from extracted Wikipedia infobox. Novikova et al. (2017) propose to generate restaurant reviews by some given attributes and Fu et al. (2020a) create the WikiEvent dataset to generate text based on an event chain. However, the works mentioned above usually map structured triples to text via a supervised seq-to-seq (Sutskever et al., 2014) model, in which large amounts of annotated data is necessary and the annotation is very expensive and time-consuming. We aim to tackle the problem of completely unsupervised KB-to-text generation which only requires a text corpus and a KB corpus and does not assume any alignment between them. We propose a dual learning framework based on the inverse relationship between the KB-to-text generation"
2020.aacl-main.29,P17-1017,0,0.641013,"ge Base (KB)-to-text task focuses on generating plain text descriptions from given knowledge bases (KB) triples which makes them accessible to users. For instance, given a KB triple <101 Helena, discoverer, James Craig Watson>, it is expected to generate a description sentence such as ∗ The work described in this paper is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Codes: 14204418). “101 Helena is discovered by James Craig Watson.”. Recently, many research works have been proposed for this task. For example, Gardent et al. (2017a,b) create the WebNLG dataset to generate description for triples sampled from DBPedia (Auer et al., 2007). Lebret et al.’s (2016) method generates people’s biographies from extracted Wikipedia infobox. Novikova et al. (2017) propose to generate restaurant reviews by some given attributes and Fu et al. (2020a) create the WikiEvent dataset to generate text based on an event chain. However, the works mentioned above usually map structured triples to text via a supervised seq-to-seq (Sutskever et al., 2014) model, in which large amounts of annotated data is necessary and the annotation is very e"
2020.aacl-main.29,W17-3518,0,0.411061,"ge Base (KB)-to-text task focuses on generating plain text descriptions from given knowledge bases (KB) triples which makes them accessible to users. For instance, given a KB triple <101 Helena, discoverer, James Craig Watson>, it is expected to generate a description sentence such as ∗ The work described in this paper is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Codes: 14204418). “101 Helena is discovered by James Craig Watson.”. Recently, many research works have been proposed for this task. For example, Gardent et al. (2017a,b) create the WebNLG dataset to generate description for triples sampled from DBPedia (Auer et al., 2007). Lebret et al.’s (2016) method generates people’s biographies from extracted Wikipedia infobox. Novikova et al. (2017) propose to generate restaurant reviews by some given attributes and Fu et al. (2020a) create the WikiEvent dataset to generate text based on an event chain. However, the works mentioned above usually map structured triples to text via a supervised seq-to-seq (Sutskever et al., 2014) model, in which large amounts of annotated data is necessary and the annotation is very e"
2020.aacl-main.29,W18-2703,0,0.128865,"9P7LemHgtAm8bkvOg6WlDOPNhVTnHYCQbHrcNp2pmex376mQjLfu1KzgPZdPPbYiBGstDQwMz0XqwnBPKrNB9HpZe18nhqYOWQVUQyILFQpo0JFk0KxXMyXoG0tHJSrpnvZ98bBS31gvvaGPgld6inCsZRdGwWqH2GhGOF0nuqFkgaYTPGYdjX1sEtlP1qEn8MjrQzhyBf6eQou1O8bEXalnLmOnoyjyt9eLP7ldUM1Kvcj5gWhoh5ZHhqFHCofxk3AIROUKD7TBBPBdFZIJlhgonRfcQlfP4X/k1besgtWvmHnqidgiSTIgkNwDGxQAlVwAeqgCQiYgVtwDx6MG+POeDSelqMJ43MnA37AeP4AU8SXgA==</latexit> Figure 2: The extractor-generator dual (EGD) framework. It contains three processes namely a pre-train process, a kb2kb process and a txt2txt process. information narration (Sun et al., 2018). However, as indicated in Hoang et al. (2018), the dual learning approach is not easy to train. Moreover, these methods still need some aligned data to pre-train the weak model. Another line of research proposes to use some extra annotations instead of using aligned data. Lample et al. (2018a,b) propose to train an unsupervised NMT system based on few annotated word pairs (Conneau et al., 2018). Luo et al. (2019) propose to generate pseudo data with a rule-based template (Li et al., 2018). However, these models cannot be directly applied in our scenario since our dataset is too complicated to make these annotations. Fu et al. (2020b) pro"
2020.aacl-main.29,J82-2005,0,0.552672,"Missing"
2020.aacl-main.29,D16-1128,0,0.14657,"Missing"
2020.aacl-main.29,D16-1127,0,0.0138708,"the head, relation and tail entity respectively. We denote the texts corpus as T = {Ti |∀i} (i) (i) (i) in which Ti = [t1 , t2 , · · · , tni ] is the ith sen(i) tence and tj is the jth word in the sentence. In our problem, we are only given a collection of KB triples Kt ⊂ K and a collection of text Tt ⊂ T without any alignment information between them. The ultimate goal is to train a model that generates the corresponding text in T describing the given triple list from K. Reinforcement Learning (RL) has been utilized to solve the infeasibility of backpropagation through discrete tokens layer. Li et al. (2016) propose to use RL to focus on the long term target and thus improve the performance. Yu et al. (2017) propose to use the RL in generative adversarial networks to solve the discrete tokens problem. He et al. (2016); Sun et al. (2018) propose to use RL in dual training. As far as we know, no studies of RL have been conducted for KB triples in which 260 3.2 Extractor-Generator Dual Framework Our proposed Extractor-Generator Dual (EGD) framework is composed of a generator G and an extractor E that translate data in one form to another. We denote all trainable parameters in E and G as θ and φ, res"
2020.aacl-main.29,N18-1169,0,0.0666997,"Missing"
2020.aacl-main.29,P09-1011,0,0.169491,"s proposed to describe a list of triples sampled from DBPedia (Auer et al., 2007). Except for the KB triples, many other types of data have also been investigated for how to generate text from them. For example, E2E (Novikova et al., 2017) aims at generating text from some restaurants’ attributes. Wikibio (Lebret et al., 2016) proposes to generate biographies for the Wikipedia infobox while WikiEvent (Fu et al., 2020a) proposes to generate text based on an event chain. Besides, Chen and Mooney (2008); Wiseman et al. (2017) propose to generate a summarization of a match based on the scores and Liang et al. (2009) propose to generate weather reports based on the records. All these tasks require an elaborately annotated dataset which is very expensive to prepare. Many methods have been proposed to tackle the dataset insufficiency problem in other tasks. Fu et al. (2020c) propose to directly train the model on partially-aligned data in which the data and the text are not necessarily exactly math, and it can be built automatically. He et al. (2016); Sennrich et al. (2016); Yi et al. (2017) propose dual learning frameworks. They pre-train a weak model with parallel data and refine the model with monolingua"
2020.aacl-main.29,W04-1013,0,0.0400811,"ess proposed in He et al. (2016); Zhu et al. (2017). It is fine-tuned on the PDG model and iterates alternatively between txt2txt and kb2kb processes. Here, we do not use any reinforcement learning component. DL-RL1 uses the dual learning process together with an RL component. It is similar to the dual learning method proposed in He et al. (2016); Zhu et al. (2017). We use the PDG’s data to train the weak model. It uses the log-likelihood of the recover process’s output sequence as the reward. DL-RL2 follows the settings of Sun et al. (2018). Different from DL-RL1, this model uses the ROUGEL (Lin, 2004) score of the recovered sequence instead of using the log-likelihood as the reward. SEG is a Supervised Extractor-Generator using the original setting of WebNLG for both generator and extractor. It utilizes all the alignment information between KB and text and thus provides an upper bound for our experiment. 4.3 Experimental settings We evaluate the performances of the generator and the extractor with several metrics including BLEU (Papineni et al., 2002), NIST (Dodding263 PDG DL DL-RL1 DL-RL2 EGD EGD w/o ARLE EGD w/o ARLG EGD w/o PDG SEG BLEU 0.322 0.352 0.356 0.356 0.369 0.351 0.353 0.010 0."
2020.aacl-main.29,W17-5525,0,0.200505,"Missing"
2020.aacl-main.29,P02-1040,0,0.107894,"he recover process’s output sequence as the reward. DL-RL2 follows the settings of Sun et al. (2018). Different from DL-RL1, this model uses the ROUGEL (Lin, 2004) score of the recovered sequence instead of using the log-likelihood as the reward. SEG is a Supervised Extractor-Generator using the original setting of WebNLG for both generator and extractor. It utilizes all the alignment information between KB and text and thus provides an upper bound for our experiment. 4.3 Experimental settings We evaluate the performances of the generator and the extractor with several metrics including BLEU (Papineni et al., 2002), NIST (Dodding263 PDG DL DL-RL1 DL-RL2 EGD EGD w/o ARLE EGD w/o ARLG EGD w/o PDG SEG BLEU 0.322 0.352 0.356 0.356 0.369 0.351 0.353 0.010 0.406 NIST 7.06 7.71 7.73 7.75 7.77 7.72 7.77 0.82 8.31 Generator METEOR ROUGEL 0.349 0.505 0.347 0.528 0.350 0.532 0.350 0.533 0.364 0.541 0.347 0.529 0.348 0.531 0.037 0.119 0.385 0.585 CIDEr 2.63 2.96 3.00 2.99 3.13 2.97 2.99 0.02 3.66 BLEU 0.489 0.735 0.760 0.757 0.775 0.770 0.729 0.020 0.848 NIST 6.01 10.4 10.8 10.7 11.1 10.9 10.4 0.42 11.8 METEOR 0.351 0.502 0.501 0.503 0.503 0.501 0.505 0.026 0.595 Extractor ROUGEL CIDEr 0.618 3.97 0.743 5.67 0.755 5"
2020.aacl-main.29,P16-1009,0,0.0381503,"event chain. Besides, Chen and Mooney (2008); Wiseman et al. (2017) propose to generate a summarization of a match based on the scores and Liang et al. (2009) propose to generate weather reports based on the records. All these tasks require an elaborately annotated dataset which is very expensive to prepare. Many methods have been proposed to tackle the dataset insufficiency problem in other tasks. Fu et al. (2020c) propose to directly train the model on partially-aligned data in which the data and the text are not necessarily exactly math, and it can be built automatically. He et al. (2016); Sennrich et al. (2016); Yi et al. (2017) propose dual learning frameworks. They pre-train a weak model with parallel data and refine the model with monolingual data. This strategy has been applied in many related tasks including semantic parsing (Cao et al., 2019), summarization (Baziotis et al., 2019) and 259 pre-train Text PDG KB Eθ PDG KB split G Text KB1 … KB2 Gφ … Text2 Text1 E Textn E Eθ KB KB1 LE kb2kb KB2 Text KBn KB Gφ Data from corpus Eθ KB E/G split Gφ … Text KBn … KB2 KB1 … Text2 ARLG <latexit sha1_base64=""Y8Oe2xpmNErGDbs8P67Jf4pRbrs="">AAAB+XicbVDLSsNAFJ3UV62vVFfSzWARXJWkRZuCi4IILly0YGuhCWEynbZDJw9mJoUS"
2020.aacl-main.29,D18-1236,0,0.339367,"6ZVUNEDgcM593JPjhNwJhVCb0ZiZXVtfSO5mdra3tndM9P7LemHgtAm8bkvOg6WlDOPNhVTnHYCQbHrcNp2pmex376mQjLfu1KzgPZdPPbYiBGstDQwMz0XqwnBPKrNB9HpZe18nhqYOWQVUQyILFQpo0JFk0KxXMyXoG0tHJSrpnvZ98bBS31gvvaGPgld6inCsZRdGwWqH2GhGOF0nuqFkgaYTPGYdjX1sEtlP1qEn8MjrQzhyBf6eQou1O8bEXalnLmOnoyjyt9eLP7ldUM1Kvcj5gWhoh5ZHhqFHCofxk3AIROUKD7TBBPBdFZIJlhgonRfcQlfP4X/k1besgtWvmHnqidgiSTIgkNwDGxQAlVwAeqgCQiYgVtwDx6MG+POeDSelqMJ43MnA37AeP4AU8SXgA==</latexit> Figure 2: The extractor-generator dual (EGD) framework. It contains three processes namely a pre-train process, a kb2kb process and a txt2txt process. information narration (Sun et al., 2018). However, as indicated in Hoang et al. (2018), the dual learning approach is not easy to train. Moreover, these methods still need some aligned data to pre-train the weak model. Another line of research proposes to use some extra annotations instead of using aligned data. Lample et al. (2018a,b) propose to train an unsupervised NMT system based on few annotated word pairs (Conneau et al., 2018). Luo et al. (2019) propose to generate pseudo data with a rule-based template (Li et al., 2018). However, these models cannot be directly applied in our scenario since our dataset is too complicated to"
2020.acl-main.26,W18-6518,0,0.0159447,"Duan et al., 2017; Yang et al., 2017; Golub et al., 2017), collaborating QA and QG model (Tang et al., 2018, 2017), and unified learning (Xiao et al., 2018). Although question generation has been applied on other datasets, e.g., Wikipedia (Du and Cardie, 2018), most of the existing QG works treat it as a dual task of reading comprehension (Yu et al., 2018; Cui et al., 2017), namely generating a question from a piece of text where a certain text span is marked as answer, in spite of several exceptions where only sentences without answer spans are used for generating questions (Du et al., 2017; Chali and Baghaee, 2018). Such generation setting is not suitable for reviews due to the lack of (question, review) pairs and improper assumption of text span answer as aforementioned. There are works training the question generation model with the user-written QA pairs in E-commerce sites (Hu et al., 2018; Chali and Baghaee, 2018), but the practicality is limited since the questions are only generated from answers instead of reviews. Transfer learning (Pan and Yang, 2009; Tan et al., 2017; Li et al., 2020) refers to a broad scope of methods that exploit knowledge across domains for handling tasks in the target domai"
2020.acl-main.26,D13-1172,0,0.0287804,"on. To generate to the point questions about the major aspects in reviews, related features extracted in an unsupervised manner are incorporated without the burden of aspect annotation. Experiments on data from various categories of a popular E-commerce site demonstrate the effectiveness of the framework, as well as the potentials of the proposed review-based question generation task. 1 Introduction The user-written reviews for products or service have become an important information source and there are a few research areas analyzing such data, including aspect extraction (Bing et al., 2016; Chen et al., 2013), product recommendation (Chelliah and Sarkar, 2017), and sentiment analysis (Li et al., 2018; Zhao et al., 2018a). Reviews reflect certain concerns or experiences of users on products or services, and such information is valuable for other ∗ The work described in this paper is partially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14204418). † The work was done when Qian Yu was an intern at Alibaba. potential consumers. However, there are few mechanisms assisting users for efficient review digestion. It is time-consu"
2020.acl-main.26,P17-1055,0,0.0299294,"pans for facilitating the learning (Wang et al., 2019). Question generation models can be combined with its dual task, i.e., reading comprehension or question answering with various motivations, such as improving auxiliary task performance (Duan et al., 2017; Yang et al., 2017; Golub et al., 2017), collaborating QA and QG model (Tang et al., 2018, 2017), and unified learning (Xiao et al., 2018). Although question generation has been applied on other datasets, e.g., Wikipedia (Du and Cardie, 2018), most of the existing QG works treat it as a dual task of reading comprehension (Yu et al., 2018; Cui et al., 2017), namely generating a question from a piece of text where a certain text span is marked as answer, in spite of several exceptions where only sentences without answer spans are used for generating questions (Du et al., 2017; Chali and Baghaee, 2018). Such generation setting is not suitable for reviews due to the lack of (question, review) pairs and improper assumption of text span answer as aforementioned. There are works training the question generation model with the user-written QA pairs in E-commerce sites (Hu et al., 2018; Chali and Baghaee, 2018), but the practicality is limited since the"
2020.acl-main.26,D17-1219,0,0.102854,"arning is categorized into four groups (Pan and Yang, 2009), namely instance transfer, feature representation transfer, parameter transfer, and relational knowledge transfer. Our learning framework can be regarded as a case of instance transfer with iterative instance adaptation and augmentation. Related Work Question generation (QG) is an emerging research topic due to its wide application scenarios such as education (Wang et al., 2018), goal-oriented dialogue (Lee et al., 2018), and question answering (Duan et al., 2017). The preliminary neural QG models (Du et al., 2017; Zhou et al., 2017; Du and Cardie, 2017) outperform the rule-based methods relying on hand-craft features, and thereafter various models have been proposed to further improve the performance via incorporating question type (Dong et al., 2018), answer position (Sun et al., 2018), long passage modeling (Zhao et al., 2018b), question difficulty (Gao et al., 2019), and to the point context (Li et al., 2019). Some works try to find the possible answer text spans for facilitating the learning (Wang et al., 2019). Question generation models can be combined with its dual task, i.e., reading comprehension or question answering with various m"
2020.acl-main.26,P18-1177,0,0.112982,"Missing"
2020.acl-main.26,P17-1123,0,0.468063,"s or experiences of users on products or services, and such information is valuable for other ∗ The work described in this paper is partially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14204418). † The work was done when Qian Yu was an intern at Alibaba. potential consumers. However, there are few mechanisms assisting users for efficient review digestion. It is time-consuming for users to locate critical review parts that they care about, particularly in long reviews. We propose to utilize question generation (QG) (Du et al., 2017) as a new means to overcome this problem. Specifically, given a review sentence, the generated question is expected to ask about the concerned aspect of this product, from the perspective of the review writer. Such question can be regarded as a reading anchor of the review sentence, and it is easier to view and conceive due to its concise form. As an example, the review for a battery case product in Table 1 is too long to find sentences that can answer a user question such as “How long will the battery last?”. Given the generated questions in the right column, it would be much easier to find o"
2020.acl-main.26,D19-1317,1,0.843323,"to its wide application scenarios such as education (Wang et al., 2018), goal-oriented dialogue (Lee et al., 2018), and question answering (Duan et al., 2017). The preliminary neural QG models (Du et al., 2017; Zhou et al., 2017; Du and Cardie, 2017) outperform the rule-based methods relying on hand-craft features, and thereafter various models have been proposed to further improve the performance via incorporating question type (Dong et al., 2018), answer position (Sun et al., 2018), long passage modeling (Zhao et al., 2018b), question difficulty (Gao et al., 2019), and to the point context (Li et al., 2019). Some works try to find the possible answer text spans for facilitating the learning (Wang et al., 2019). Question generation models can be combined with its dual task, i.e., reading comprehension or question answering with various motivations, such as improving auxiliary task performance (Duan et al., 2017; Yang et al., 2017; Golub et al., 2017), collaborating QA and QG model (Tang et al., 2018, 2017), and unified learning (Xiao et al., 2018). Although question generation has been applied on other datasets, e.g., Wikipedia (Du and Cardie, 2018), most of the existing QG works treat it as a du"
2020.acl-main.26,P18-1087,1,0.843326,"acted in an unsupervised manner are incorporated without the burden of aspect annotation. Experiments on data from various categories of a popular E-commerce site demonstrate the effectiveness of the framework, as well as the potentials of the proposed review-based question generation task. 1 Introduction The user-written reviews for products or service have become an important information source and there are a few research areas analyzing such data, including aspect extraction (Bing et al., 2016; Chen et al., 2013), product recommendation (Chelliah and Sarkar, 2017), and sentiment analysis (Li et al., 2018; Zhao et al., 2018a). Reviews reflect certain concerns or experiences of users on products or services, and such information is valuable for other ∗ The work described in this paper is partially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14204418). † The work was done when Qian Yu was an intern at Alibaba. potential consumers. However, there are few mechanisms assisting users for efficient review digestion. It is time-consuming for users to locate critical review parts that they care about, particularly in long rev"
2020.acl-main.26,D14-1162,0,0.0844023,"Missing"
2020.acl-main.26,D17-1090,0,0.079253,"r to view and conceive due to its concise form. As an example, the review for a battery case product in Table 1 is too long to find sentences that can answer a user question such as “How long will the battery last?”. Given the generated questions in the right column, it would be much easier to find out the helpful part of the review. Recently, as a topic attracting significant research attention, question generation is regarded as a dual task of reading comprehension in most works, namely generating a question from a sentence with a fixed text segment in the sentence designated as the answer (Duan et al., 2017; Sun et al., 2018). Two unique characteristics of our review-based question generation task differentiate it from the previous question generation works. First, there is no review-question pairs available for training, thus a simple Seq2Seq-based question generation model for learning the mapping from the input (i.e. review) to the output (i.e. question) cannot be applied. Even though we can easily obtain large volumes of user-posed review sets and question sets, they are just separate datasets and cannot provide any supervision of input-output mapping (i.e. reviewquestion pair). The second o"
2020.acl-main.26,D18-1427,0,0.230602,"ive due to its concise form. As an example, the review for a battery case product in Table 1 is too long to find sentences that can answer a user question such as “How long will the battery last?”. Given the generated questions in the right column, it would be much easier to find out the helpful part of the review. Recently, as a topic attracting significant research attention, question generation is regarded as a dual task of reading comprehension in most works, namely generating a question from a sentence with a fixed text segment in the sentence designated as the answer (Duan et al., 2017; Sun et al., 2018). Two unique characteristics of our review-based question generation task differentiate it from the previous question generation works. First, there is no review-question pairs available for training, thus a simple Seq2Seq-based question generation model for learning the mapping from the input (i.e. review) to the output (i.e. question) cannot be applied. Even though we can easily obtain large volumes of user-posed review sets and question sets, they are just separate datasets and cannot provide any supervision of input-output mapping (i.e. reviewquestion pair). The second one is that differen"
2020.acl-main.26,D17-1087,0,0.0166934,"have been proposed to further improve the performance via incorporating question type (Dong et al., 2018), answer position (Sun et al., 2018), long passage modeling (Zhao et al., 2018b), question difficulty (Gao et al., 2019), and to the point context (Li et al., 2019). Some works try to find the possible answer text spans for facilitating the learning (Wang et al., 2019). Question generation models can be combined with its dual task, i.e., reading comprehension or question answering with various motivations, such as improving auxiliary task performance (Duan et al., 2017; Yang et al., 2017; Golub et al., 2017), collaborating QA and QG model (Tang et al., 2018, 2017), and unified learning (Xiao et al., 2018). Although question generation has been applied on other datasets, e.g., Wikipedia (Du and Cardie, 2018), most of the existing QG works treat it as a dual task of reading comprehension (Yu et al., 2018; Cui et al., 2017), namely generating a question from a piece of text where a certain text span is marked as answer, in spite of several exceptions where only sentences without answer spans are used for generating questions (Du et al., 2017; Chali and Baghaee, 2018). Such generation setting is not"
2020.acl-main.26,P17-1036,0,0.153488,"spect Extraction. Product aspects usually play a major role in all of product questions, answers and reviews, since they are the discussion focus of such text content. Thus, such aspects can act as connections in modeling input pairs of qa and r via the partially shared structure. To help the semantic vector hα in Eqn 3 capture salient aspects of reviews, an autoencoder module is connected to the encoding layer for reconstructing hα . Together with the matrix M, the autoencoder can be used to extract salient aspects from reviews. Note that this combined structure is similar to the ABAE model (He et al., 2017), which has been shown effective for unsupervised aspect extraction. Compared with supervised aspect detection methods, such a unsupervised module avoid the burden of aspect annotation for different product categories, and our experiments demonstrate that regularization based on this module is effective. Specifically, hα is mapped to an aspect distribution pα and then reconstructed: pα = softmax(Wp · hα + bp ) α0 h α =p ·A We adapt the Seq2Seq model for the aspect-focused generation model, which is updated gradually via the transferred and augmented instances. With the help of aspect-based var"
2020.acl-main.26,N18-2072,0,0.0266661,"questions. The second challenge, namely the issue that some verbose answers contain irrelevant content especially for subjective questions. To handle this challenge, we propose a learning framework with adaptive instance transfer and augmentation. Firstly, a pre-trained generation model based on user-posed answer-question pairs is utilized as an initial question generator. A ranker is designed to work together with the generator to improve the training instance set by distilling it via removing unsuitable answer-question pairs to avoid “negative transfer” (Pan and Yang, 2009), and augmenting (Kobayashi, 2018) it by adding suitable reviewquestion pairs. For selecting suitable reviews for question generation, the ranker considers two factors: the major aspects in a review and the review’s suitability for question generation. The two factors are captured via a reconstruction objective and a reinforcement objective with reward given by the generator. Thus, the ranker and the generator are iteratively enhanced, and the adaptively transferred answer-question pairs and the augmented reviewquestion pairs gradually relieve the data lacking problem. In accordance with the second characteristic of our task,"
2020.acl-main.26,N18-1141,0,0.0350535,"Missing"
2020.acl-main.26,P15-1060,0,0.0440336,"Missing"
2020.acl-main.26,P17-1096,0,0.0171941,"fter various models have been proposed to further improve the performance via incorporating question type (Dong et al., 2018), answer position (Sun et al., 2018), long passage modeling (Zhao et al., 2018b), question difficulty (Gao et al., 2019), and to the point context (Li et al., 2019). Some works try to find the possible answer text spans for facilitating the learning (Wang et al., 2019). Question generation models can be combined with its dual task, i.e., reading comprehension or question answering with various motivations, such as improving auxiliary task performance (Duan et al., 2017; Yang et al., 2017; Golub et al., 2017), collaborating QA and QG model (Tang et al., 2018, 2017), and unified learning (Xiao et al., 2018). Although question generation has been applied on other datasets, e.g., Wikipedia (Du and Cardie, 2018), most of the existing QG works treat it as a dual task of reading comprehension (Yu et al., 2018; Cui et al., 2017), namely generating a question from a piece of text where a certain text span is marked as answer, in spite of several exceptions where only sentences without answer spans are used for generating questions (Du et al., 2017; Chali and Baghaee, 2018). Such gener"
2020.acl-main.26,D18-1424,0,0.396601,"ervised manner are incorporated without the burden of aspect annotation. Experiments on data from various categories of a popular E-commerce site demonstrate the effectiveness of the framework, as well as the potentials of the proposed review-based question generation task. 1 Introduction The user-written reviews for products or service have become an important information source and there are a few research areas analyzing such data, including aspect extraction (Bing et al., 2016; Chen et al., 2013), product recommendation (Chelliah and Sarkar, 2017), and sentiment analysis (Li et al., 2018; Zhao et al., 2018a). Reviews reflect certain concerns or experiences of users on products or services, and such information is valuable for other ∗ The work described in this paper is partially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14204418). † The work was done when Qian Yu was an intern at Alibaba. potential consumers. However, there are few mechanisms assisting users for efficient review digestion. It is time-consuming for users to locate critical review parts that they care about, particularly in long reviews. We propose to"
2020.acl-main.523,W17-4419,0,0.0417257,"cally requires a large amount of manually labeled training data, which are not always available in low-resource languages. Training neural NER with limited labeled data can be very challenging. In this paper, we consider bridging multi-task learning (MTL) (Caruana, 1993; Ruder, 2017) and pretraining (Peters et al., 2018; Devlin et al., 2019) to leverage training signals of an auxiliary task that has a sufficiently large number of labeled data. Researchers have investigated a wide variety of auxiliary tasks and resources to boost the performance of neural NER, e.g., training coarsegrained NER (Aguilar et al., 2017), fine-tuning bilingual word embeddings (Wang et al., 2017), applying language models (Rei, 2017), integrating part-of-speech (POS) tagging (Lin et al., 2018), using cross-lingual knowledge (Feng et al., 2018), and learning paraphrases (Watanabe et al., 2019). While most of the previous studies have exploited token-level information from auxiliary tasks, a few of them have tried to use sentence-level information (Rei and Søgaard, 2018; Devlin et al., 2019). Our work is closely related to the joint labeling framework in Rei and Søgaard (2019). However, they only focused on binary classification"
2020.acl-main.523,N19-4010,0,0.0111932,"y Bag Messenger bag for Men Women Office Laptop”, the underlined elements are 3 PRODUCT and 2 CONSUMER_GROUP entities. The other reason is that in one product title, it is common to find repeated identical expressions in the same language, as well as the same entity words appearing in English. Using a VI example to illustrate: “T-Shirt - Áo thun in phản quang Ao thun Nam - Ao thun nữ - Áo thun phong cách Nam Nữ”, the underlined elements refer to the same product (t-shirt), appearing multiple times in VI and in English. 3.2 Training details We implement our model on top of the Flair framework (Akbik et al., 2019), which has recently achieved state-of-the-art results in various sequence labeling tasks. Following Lample et al. (2016), we use the IOBES tagging scheme. We use the pretrained word embeddings of fastText4 (Bojanowski et al., 2016) with de = 300 dimensions for each language and a single-layer BiLSTM with dh = 512 hidden units. We apply a locked dropout (Merity et al., 2018) with the probability of 0.5 before and after the BiLSTM layer and to the attention output before the residual connection. For the multi-head self-attention layer, we adapt the implementation of “The Annotated Transformer”"
2020.acl-main.523,C18-1139,0,0.0128372,"… Korean flexible silicon case …’’ Category: HEALTH_BEAUTY Title: COMBO Gôm Label: O Translation: combo xịt tóc Tigi Bed Head B-PRODUCT I-PRODUCT E-PRODUCT B-BRAND I-BRAND E-BRAND hairspray Tigi Bed Head ‘‘… Tigi Bed Head hairspray combo …’’ Figure 1: Examples of product titles with NER annotation in Vietnamese. Product categories are provided by sellers and can be used as sentence-level labels. Introduction Neural named entity recognition (NER) has become a mainstream approach due to its superior performance (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Akbik et al., 2018). However, neural NER typically requires a large amount of manually labeled training data, which are not always available in low-resource languages. Training neural NER with limited labeled data can be very challenging. In this paper, we consider bridging multi-task learning (MTL) (Caruana, 1993; Ruder, 2017) and pretraining (Peters et al., 2018; Devlin et al., 2019) to leverage training signals of an auxiliary task that has a sufficiently large number of labeled data. Researchers have investigated a wide variety of auxiliary tasks and resources to boost the performance of neural NER, e.g., tr"
2020.acl-main.523,Q16-1026,0,0.0372942,"le Hàn Quốc O O Korea ‘‘… Korean flexible silicon case …’’ Category: HEALTH_BEAUTY Title: COMBO Gôm Label: O Translation: combo xịt tóc Tigi Bed Head B-PRODUCT I-PRODUCT E-PRODUCT B-BRAND I-BRAND E-BRAND hairspray Tigi Bed Head ‘‘… Tigi Bed Head hairspray combo …’’ Figure 1: Examples of product titles with NER annotation in Vietnamese. Product categories are provided by sellers and can be used as sentence-level labels. Introduction Neural named entity recognition (NER) has become a mainstream approach due to its superior performance (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Akbik et al., 2018). However, neural NER typically requires a large amount of manually labeled training data, which are not always available in low-resource languages. Training neural NER with limited labeled data can be very challenging. In this paper, we consider bridging multi-task learning (MTL) (Caruana, 1993; Ruder, 2017) and pretraining (Peters et al., 2018; Devlin et al., 2019) to leverage training signals of an auxiliary task that has a sufficiently large number of labeled data. Researchers have investigated a wide variety of auxiliary tasks and resources to boost the performance of"
2020.acl-main.523,N16-1030,0,0.809852,"TERIAL S-PATTERN case dẻo silicon flexible Hàn Quốc O O Korea ‘‘… Korean flexible silicon case …’’ Category: HEALTH_BEAUTY Title: COMBO Gôm Label: O Translation: combo xịt tóc Tigi Bed Head B-PRODUCT I-PRODUCT E-PRODUCT B-BRAND I-BRAND E-BRAND hairspray Tigi Bed Head ‘‘… Tigi Bed Head hairspray combo …’’ Figure 1: Examples of product titles with NER annotation in Vietnamese. Product categories are provided by sellers and can be used as sentence-level labels. Introduction Neural named entity recognition (NER) has become a mainstream approach due to its superior performance (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Akbik et al., 2018). However, neural NER typically requires a large amount of manually labeled training data, which are not always available in low-resource languages. Training neural NER with limited labeled data can be very challenging. In this paper, we consider bridging multi-task learning (MTL) (Caruana, 1993; Ruder, 2017) and pretraining (Peters et al., 2018; Devlin et al., 2019) to leverage training signals of an auxiliary task that has a sufficiently large number of labeled data. Researchers have investigated a wide variety of auxiliary task"
2020.acl-main.523,D17-1070,0,0.0361231,", . . . , wT be an input token sequence, where wt denotes the t-th token in the sequence. We represent each wt using a pre-trained word embedding et ∈ Rde , where de is the dimensionality of word embeddings. We do not fine-tune word embeddings but project them into a new space ← ht = concat( h t , h t ) ∈ Rdh . We can either use H for both the sentence classification and NER tasks directly or apply an attention mechanism on it to help the model focus on particular tokens (detailed in §2.2). Sentence classification We create a fixed size vector by applying max-pooling (Collobert et al., 2011; Conneau et al., 2017) over H, which encourages the model to capture the most useful local features encoded in the hidden states. We feed the fixed size global feature vector to a linear layer to obtain the unnormalized predicted scores for each class. Let K be the number of target classes, sk be the k-th normalized predicted score after applying a softmax function, and t ∈ RK be the one-hot encoded true label. To train the sentence classification model, we minimize the multi-class cross-entropy loss: N K 1 X X (i) (i) tk log(sk ), LC = − N (1) i=1 k=1 where i denotes the sentence index, and N is the number of trai"
2020.acl-main.523,N19-1423,0,0.293958,"used as sentence-level labels. Introduction Neural named entity recognition (NER) has become a mainstream approach due to its superior performance (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Akbik et al., 2018). However, neural NER typically requires a large amount of manually labeled training data, which are not always available in low-resource languages. Training neural NER with limited labeled data can be very challenging. In this paper, we consider bridging multi-task learning (MTL) (Caruana, 1993; Ruder, 2017) and pretraining (Peters et al., 2018; Devlin et al., 2019) to leverage training signals of an auxiliary task that has a sufficiently large number of labeled data. Researchers have investigated a wide variety of auxiliary tasks and resources to boost the performance of neural NER, e.g., training coarsegrained NER (Aguilar et al., 2017), fine-tuning bilingual word embeddings (Wang et al., 2017), applying language models (Rei, 2017), integrating part-of-speech (POS) tagging (Lin et al., 2018), using cross-lingual knowledge (Feng et al., 2018), and learning paraphrases (Watanabe et al., 2019). While most of the previous studies have exploited token-level"
2020.acl-main.523,D17-1206,0,0.0278784,"tle texts indicate that the proposed method is effective for low-resource NER across three different languages: Vietnamese, Thai, and Indonesian. Proposed method Figure 2 shows the architecture of our joint sentence and token labeling model. Our model is based on hard parameter sharing (Ruder, 2017) in which the hidden layers are shared between two tasks. The task-specific layers include a conditional random field (CRF) layer for NER and a linear layer for sentence classification.2 Unlike the standard MTL, which trains multiple tasks at once and expects the model to perform well on all tasks (Hashimoto et al., 2017; Rei and Søgaard, 2019), the goal of our work is to improve the performance of the main task (NER) using the auxiliary task (sentence classification) for creating pre-trained representations and as a regularizer. 2.1 dh 2 ← and a backward hid← dh den state sequence H = [ h 1 , . . . , h T ] ∈ RT × 2 , where dh is the number of hidden units. We concatenate the hidden states of both directions to obtain the final hidden representation H = [h1 , . . . , hT ] ∈ RT ×dh , where Figure 2: Architecture of our joint sentence and token labeling model. The attention layer is optional, which can be skipp"
2020.acl-main.523,P18-1074,0,0.0155237,"d data can be very challenging. In this paper, we consider bridging multi-task learning (MTL) (Caruana, 1993; Ruder, 2017) and pretraining (Peters et al., 2018; Devlin et al., 2019) to leverage training signals of an auxiliary task that has a sufficiently large number of labeled data. Researchers have investigated a wide variety of auxiliary tasks and resources to boost the performance of neural NER, e.g., training coarsegrained NER (Aguilar et al., 2017), fine-tuning bilingual word embeddings (Wang et al., 2017), applying language models (Rei, 2017), integrating part-of-speech (POS) tagging (Lin et al., 2018), using cross-lingual knowledge (Feng et al., 2018), and learning paraphrases (Watanabe et al., 2019). While most of the previous studies have exploited token-level information from auxiliary tasks, a few of them have tried to use sentence-level information (Rei and Søgaard, 2018; Devlin et al., 2019). Our work is closely related to the joint labeling framework in Rei and Søgaard (2019). However, they only focused on binary classification, while we attempt to handle multi-class classification on both sentence and token levels. In this work, we focus on improving lowresource NER by exploiting l"
2020.acl-main.523,P16-1101,0,0.0598024,"dẻo silicon flexible Hàn Quốc O O Korea ‘‘… Korean flexible silicon case …’’ Category: HEALTH_BEAUTY Title: COMBO Gôm Label: O Translation: combo xịt tóc Tigi Bed Head B-PRODUCT I-PRODUCT E-PRODUCT B-BRAND I-BRAND E-BRAND hairspray Tigi Bed Head ‘‘… Tigi Bed Head hairspray combo …’’ Figure 1: Examples of product titles with NER annotation in Vietnamese. Product categories are provided by sellers and can be used as sentence-level labels. Introduction Neural named entity recognition (NER) has become a mainstream approach due to its superior performance (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Akbik et al., 2018). However, neural NER typically requires a large amount of manually labeled training data, which are not always available in low-resource languages. Training neural NER with limited labeled data can be very challenging. In this paper, we consider bridging multi-task learning (MTL) (Caruana, 1993; Ruder, 2017) and pretraining (Peters et al., 2018; Devlin et al., 2019) to leverage training signals of an auxiliary task that has a sufficiently large number of labeled data. Researchers have investigated a wide variety of auxiliary tasks and resources to"
2020.acl-main.523,N18-1202,0,0.0503169,"y sellers and can be used as sentence-level labels. Introduction Neural named entity recognition (NER) has become a mainstream approach due to its superior performance (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Akbik et al., 2018). However, neural NER typically requires a large amount of manually labeled training data, which are not always available in low-resource languages. Training neural NER with limited labeled data can be very challenging. In this paper, we consider bridging multi-task learning (MTL) (Caruana, 1993; Ruder, 2017) and pretraining (Peters et al., 2018; Devlin et al., 2019) to leverage training signals of an auxiliary task that has a sufficiently large number of labeled data. Researchers have investigated a wide variety of auxiliary tasks and resources to boost the performance of neural NER, e.g., training coarsegrained NER (Aguilar et al., 2017), fine-tuning bilingual word embeddings (Wang et al., 2017), applying language models (Rei, 2017), integrating part-of-speech (POS) tagging (Lin et al., 2018), using cross-lingual knowledge (Feng et al., 2018), and learning paraphrases (Watanabe et al., 2019). While most of the previous studies have"
2020.acl-main.523,P17-1194,0,0.0237448,"rce languages. Training neural NER with limited labeled data can be very challenging. In this paper, we consider bridging multi-task learning (MTL) (Caruana, 1993; Ruder, 2017) and pretraining (Peters et al., 2018; Devlin et al., 2019) to leverage training signals of an auxiliary task that has a sufficiently large number of labeled data. Researchers have investigated a wide variety of auxiliary tasks and resources to boost the performance of neural NER, e.g., training coarsegrained NER (Aguilar et al., 2017), fine-tuning bilingual word embeddings (Wang et al., 2017), applying language models (Rei, 2017), integrating part-of-speech (POS) tagging (Lin et al., 2018), using cross-lingual knowledge (Feng et al., 2018), and learning paraphrases (Watanabe et al., 2019). While most of the previous studies have exploited token-level information from auxiliary tasks, a few of them have tried to use sentence-level information (Rei and Søgaard, 2018; Devlin et al., 2019). Our work is closely related to the joint labeling framework in Rei and Søgaard (2019). However, they only focused on binary classification, while we attempt to handle multi-class classification on both sentence and token levels. In thi"
2020.acl-main.523,N18-1027,0,0.0627943,"led data. Researchers have investigated a wide variety of auxiliary tasks and resources to boost the performance of neural NER, e.g., training coarsegrained NER (Aguilar et al., 2017), fine-tuning bilingual word embeddings (Wang et al., 2017), applying language models (Rei, 2017), integrating part-of-speech (POS) tagging (Lin et al., 2018), using cross-lingual knowledge (Feng et al., 2018), and learning paraphrases (Watanabe et al., 2019). While most of the previous studies have exploited token-level information from auxiliary tasks, a few of them have tried to use sentence-level information (Rei and Søgaard, 2018; Devlin et al., 2019). Our work is closely related to the joint labeling framework in Rei and Søgaard (2019). However, they only focused on binary classification, while we attempt to handle multi-class classification on both sentence and token levels. In this work, we focus on improving lowresource NER by exploiting large data, only having sentence-level labels. Figure 1 shows examples of product titles on an e-commerce website in Vietnamese. While the product titles with NER annotation done by our annotators are limited, those with product categories (e.g., ELECTRONICS) labeled by sellers ar"
2020.acl-main.523,W18-2509,0,0.0154433,"which has recently achieved state-of-the-art results in various sequence labeling tasks. Following Lample et al. (2016), we use the IOBES tagging scheme. We use the pretrained word embeddings of fastText4 (Bojanowski et al., 2016) with de = 300 dimensions for each language and a single-layer BiLSTM with dh = 512 hidden units. We apply a locked dropout (Merity et al., 2018) with the probability of 0.5 before and after the BiLSTM layer and to the attention output before the residual connection. For the multi-head self-attention layer, we adapt the implementation of “The Annotated Transformer” (Rush, 2018)5 and use its default hyperparameters. We train all models using Adam (Kingma and Ba, 2015) with the batch size of 32, the learning rate of 1e-3, and the gradient clipping of 5. We initialize all model parameters by sampling from U(−0.1, 0.1). We set λ in Eq. (3) to 1. We use the same parameter setting for all languages. We apply early stopping in which the learning rate decays by 3 For TH, 941 training examples remain after removing annotation errors. 4 https://fasttext.cc/docs/en/crawl-vectors.html 5 https://nlp.seas.harvard.edu/2018/04/03/attention.html 0.5 if the F1 score on the NER develo"
2020.acl-main.523,I17-2065,0,0.0122182,"a, which are not always available in low-resource languages. Training neural NER with limited labeled data can be very challenging. In this paper, we consider bridging multi-task learning (MTL) (Caruana, 1993; Ruder, 2017) and pretraining (Peters et al., 2018; Devlin et al., 2019) to leverage training signals of an auxiliary task that has a sufficiently large number of labeled data. Researchers have investigated a wide variety of auxiliary tasks and resources to boost the performance of neural NER, e.g., training coarsegrained NER (Aguilar et al., 2017), fine-tuning bilingual word embeddings (Wang et al., 2017), applying language models (Rei, 2017), integrating part-of-speech (POS) tagging (Lin et al., 2018), using cross-lingual knowledge (Feng et al., 2018), and learning paraphrases (Watanabe et al., 2019). While most of the previous studies have exploited token-level information from auxiliary tasks, a few of them have tried to use sentence-level information (Rei and Søgaard, 2018; Devlin et al., 2019). Our work is closely related to the joint labeling framework in Rei and Søgaard (2019). However, they only focused on binary classification, while we attempt to handle multi-class classification on"
2020.coling-main.215,W05-0909,0,0.0939199,"Michigan&gt;, &lt;James Craig Watson, deathCause, Peritonitis&gt;} &lt;James Craig Watson, nationality, Canada&gt;, 0 &lt;James Craig Watson, deathPlace, Madison , Wisconsin&gt;} Figure 3: Topic distribution at each step. White stands for higher probability while black stands for lower. the number of topics is set to 100. In the T2S model, we set the number of topics to 200. In the DTT model, we set the number of topics to 500. We evaluate all the models with the same evaluation script 4 . Several metrics are evaluated, including BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), NIST L (Doddington, 2002), METEOR (Banerjee and Lavie, 2005) and CIDEr (Vedantam et al., 2015). Since some metrics are sensitive to randomness, we run each model for 5 times and report the median score with the standard deviation. 4.4 Results The experimental results are shown in Table 2. We can observe that our DTT model outperforms all comparison models significantly and consistently. It illustrates that the DTT model can capture the dynamic topic information to mitigate the off-topic problem and thus improves the overall generation performance. Besides, our DTT model not only improves performance but also improves the stability of the performance. I"
2020.coling-main.215,2016.amta-researchers.10,0,0.298795,"here is no existing dataset containing the topic annotations. Therefore, it is difficult to adopt supervised learning approaches for detecting topics. Moreover, it is even more expensive to annotate the dynamic change of topic information in one sentence, as exemplified by the topic changes from “person” to “company” in Figure 1. Therefore, we investigate the task of automatically detecting the hidden topic information and incorporating such information for the generation of sentences. Many works have been proposed to utilize the static topic information to improve the generation performance. Chen et al. (2016) and Ou et al. (2018) propose to represent the topic for each sentence as a learnable vector. The topic is predicted by the input sentence and is used to enhance the generating phase. Xing et al. (2017) and Zhang et al. (2016) detect the topic representation by applying a pre-trained LDA model on the input sequence. Moreover, Choudhary et al. (2017) and Ou et al. (2018) predict the topic representation directly from the input sequence using Recurrent Neural Networks (RNN). All the above methods make an assumption that during generation the topic does not change so as to make the problem tracta"
2020.coling-main.215,2020.emnlp-main.90,1,0.65436,"GTR-LSTM model to encode not only the triple information, but also the structure information of the entity graph into hidden semantic space. Jain et al. (2018) exploit a mixed hierarchical attention based encoder-decoder model to leverage the structure and content information. Shimorina and Gardent (2018) propose to use delexicalization and copy mechanism to enhance the performance of the sequence-to-sequence framework. Konstas and Lapata (2013) and Wiseman et al. (2018) propose to use template based methods to generate the text by using the extracted template information in the training set. Cheng et al. (2020) propose to generate text description for entities by utilizing the knowledge distilled from the existing knowledge base. However, none of the above works consider the topic information in the KB-to-text generation process and thus not directly comparable to our work proposed in this paper. Some works in text generation (Gatt and Krahmer, 2018) have been proposed to incorporate the topic information to help generate the text. These ideas can be adopted in KB-to-text generation. Tars and Fishel (2018) and Johnson et al. (2017) add an extra topic tag into the source sentence for incorporating th"
2020.coling-main.215,E17-1060,0,0.117272,"den topic representation and it calculates the most suitable local topic representation for each local topic state. The topic memory is used to memorize the previous local topic representation and computes the dynamic topic state for each generation step to guide the target sentence generation procedure. 2 Related Work Recently, various data-to-text tasks have been proposed handling different kinds of data. Gardent et al. (2017a; Gardent et al. (2017b) construct the WebNLG dataset which aims at generating text descriptions based on DBpedia (Auer et al., 2007) triples. Lebret et al. (2016) and Chisholm et al. (2017) propose to generate a person’s biography based on Wikipedia’s infobox. Fu et al. (2020a) build the WikiEvent dataset aiming at generating text based on an event chain. Novikova et al. (2017) generate restaurant reviews based on the information of restaurant attributes. Wiseman et al. (2017) generate basketball match descriptions based on the game records. Moreover, Fu et al. (2020c) propose to directly train the model on partially-aligned data called WITA while Fu et al. (2020b) propose to train a model based on purely unaligned data unsupervised with a dual learning framework. All of the abo"
2020.coling-main.215,2020.aacl-main.29,1,0.83056,"ch local topic state. The topic memory is used to memorize the previous local topic representation and computes the dynamic topic state for each generation step to guide the target sentence generation procedure. 2 Related Work Recently, various data-to-text tasks have been proposed handling different kinds of data. Gardent et al. (2017a; Gardent et al. (2017b) construct the WebNLG dataset which aims at generating text descriptions based on DBpedia (Auer et al., 2007) triples. Lebret et al. (2016) and Chisholm et al. (2017) propose to generate a person’s biography based on Wikipedia’s infobox. Fu et al. (2020a) build the WikiEvent dataset aiming at generating text based on an event chain. Novikova et al. (2017) generate restaurant reviews based on the information of restaurant attributes. Wiseman et al. (2017) generate basketball match descriptions based on the game records. Moreover, Fu et al. (2020c) propose to directly train the model on partially-aligned data called WITA while Fu et al. (2020b) propose to train a model based on purely unaligned data unsupervised with a dual learning framework. All of the above problems aim at 2370 converting some formatted data into natural language texts faci"
2020.coling-main.215,2020.emnlp-main.738,1,0.786987,"ch local topic state. The topic memory is used to memorize the previous local topic representation and computes the dynamic topic state for each generation step to guide the target sentence generation procedure. 2 Related Work Recently, various data-to-text tasks have been proposed handling different kinds of data. Gardent et al. (2017a; Gardent et al. (2017b) construct the WebNLG dataset which aims at generating text descriptions based on DBpedia (Auer et al., 2007) triples. Lebret et al. (2016) and Chisholm et al. (2017) propose to generate a person’s biography based on Wikipedia’s infobox. Fu et al. (2020a) build the WikiEvent dataset aiming at generating text based on an event chain. Novikova et al. (2017) generate restaurant reviews based on the information of restaurant attributes. Wiseman et al. (2017) generate basketball match descriptions based on the game records. Moreover, Fu et al. (2020c) propose to directly train the model on partially-aligned data called WITA while Fu et al. (2020b) propose to train a model based on purely unaligned data unsupervised with a dual learning framework. All of the above problems aim at 2370 converting some formatted data into natural language texts faci"
2020.coling-main.215,P17-1017,0,0.527677,"luding question answering and recommendation systems have benefited from KBs (Wang et al., 2017) as external knowledge sources to improve the results. Though KBs have achieved great success in supporting and improving various text mining tasks, they are still incomprehensible to humans due to the over-rigid structured format. Reading a bunch of triples always annoys people since the form is not easily understandable especially to people who have never heard about KBs. In order to address this problem, recently some researchers have proposed the KB-to-text generation task (Lebret et al., 2016; Gardent et al., 2017a; Gardent et al., 2017b) to bridge the gap between KBs and natural language. This KB-to-text generation problem aims at directly converting a group of KBs triples into human-readable sentences. For example, given a triple group ( hBill Gates, BirthPlace, Seattle i, hBill Gates, FounderOf, Microsoft i), the goal is to generate a comprehensible sentence such as “Bill Gates, the founder of Microsoft, was born in Seattle.” Some works employ the techniques in the text generation area (Gatt and Krahmer, 2018) to tackle the KB-to-text problem. Though these models have achieved some success, there ar"
2020.coling-main.215,W17-3518,0,0.45507,"luding question answering and recommendation systems have benefited from KBs (Wang et al., 2017) as external knowledge sources to improve the results. Though KBs have achieved great success in supporting and improving various text mining tasks, they are still incomprehensible to humans due to the over-rigid structured format. Reading a bunch of triples always annoys people since the form is not easily understandable especially to people who have never heard about KBs. In order to address this problem, recently some researchers have proposed the KB-to-text generation task (Lebret et al., 2016; Gardent et al., 2017a; Gardent et al., 2017b) to bridge the gap between KBs and natural language. This KB-to-text generation problem aims at directly converting a group of KBs triples into human-readable sentences. For example, given a triple group ( hBill Gates, BirthPlace, Seattle i, hBill Gates, FounderOf, Microsoft i), the goal is to generate a comprehensible sentence such as “Bill Gates, the founder of Microsoft, was born in Seattle.” Some works employ the techniques in the text generation area (Gatt and Krahmer, 2018) to tackle the KB-to-text problem. Though these models have achieved some success, there ar"
2020.coling-main.215,N18-2098,0,0.013554,"ation of the KBs. Chisholm et al. (2017) propose to directly rank the triples by relation frequency and flatten the triples to pure text. The flattened text is used as the input for a sequence-to-sequence model to generate the output text. Vougiouklis et al. (2018) propose to use a triple encoder to encode each triple into a hidden vector. The decoder input is constructed by simply concatenating all of the hidden vectors. Trisedya et al. (2018) propose a GTR-LSTM model to encode not only the triple information, but also the structure information of the entity graph into hidden semantic space. Jain et al. (2018) exploit a mixed hierarchical attention based encoder-decoder model to leverage the structure and content information. Shimorina and Gardent (2018) propose to use delexicalization and copy mechanism to enhance the performance of the sequence-to-sequence framework. Konstas and Lapata (2013) and Wiseman et al. (2018) propose to use template based methods to generate the text by using the extracted template information in the training set. Cheng et al. (2020) propose to generate text description for entities by utilizing the knowledge distilled from the existing knowledge base. However, none of t"
2020.coling-main.215,P17-4012,0,0.115144,"the above works consider the topic information in the KB-to-text generation process and thus not directly comparable to our work proposed in this paper. Some works in text generation (Gatt and Krahmer, 2018) have been proposed to incorporate the topic information to help generate the text. These ideas can be adopted in KB-to-text generation. Tars and Fishel (2018) and Johnson et al. (2017) add an extra topic tag into the source sentence for incorporating the topic information into the model. The whole model is built based on the sequence-to-sequence (Sutskever et al., 2014; Cho et al., 2014; Klein et al., 2017) framework with standard attention (Bahdanau et al., 2014; Luong et al., 2015). Mikolov and Zweig (2012) as well as Liu et al. (2015) propose to use the topic information as extra features to enhance the performance of the language model and word embedding. Chen et al. (2016) and Ou et al. (2018) use the same idea to utilize the topic feature to enhance the generation of the text. However, all these methods assume that the topic information is known in advance. Some methods investigate the problem setting that the topic information is not given and needs to be detected. For example, the topic"
2020.coling-main.215,D16-1128,0,0.455919,"2014). Many tasks including question answering and recommendation systems have benefited from KBs (Wang et al., 2017) as external knowledge sources to improve the results. Though KBs have achieved great success in supporting and improving various text mining tasks, they are still incomprehensible to humans due to the over-rigid structured format. Reading a bunch of triples always annoys people since the form is not easily understandable especially to people who have never heard about KBs. In order to address this problem, recently some researchers have proposed the KB-to-text generation task (Lebret et al., 2016; Gardent et al., 2017a; Gardent et al., 2017b) to bridge the gap between KBs and natural language. This KB-to-text generation problem aims at directly converting a group of KBs triples into human-readable sentences. For example, given a triple group ( hBill Gates, BirthPlace, Seattle i, hBill Gates, FounderOf, Microsoft i), the goal is to generate a comprehensible sentence such as “Bill Gates, the founder of Microsoft, was born in Seattle.” Some works employ the techniques in the text generation area (Gatt and Krahmer, 2018) to tackle the KB-to-text problem. Though these models have achieved"
2020.coling-main.215,W04-1013,0,0.0675852,", &lt;James Craig Watson, almaMater, University of Michigan&gt;, &lt;James Craig Watson, deathCause, Peritonitis&gt;} &lt;James Craig Watson, nationality, Canada&gt;, 0 &lt;James Craig Watson, deathPlace, Madison , Wisconsin&gt;} Figure 3: Topic distribution at each step. White stands for higher probability while black stands for lower. the number of topics is set to 100. In the T2S model, we set the number of topics to 200. In the DTT model, we set the number of topics to 500. We evaluate all the models with the same evaluation script 4 . Several metrics are evaluated, including BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), NIST L (Doddington, 2002), METEOR (Banerjee and Lavie, 2005) and CIDEr (Vedantam et al., 2015). Since some metrics are sensitive to randomness, we run each model for 5 times and report the median score with the standard deviation. 4.4 Results The experimental results are shown in Table 2. We can observe that our DTT model outperforms all comparison models significantly and consistently. It illustrates that the DTT model can capture the dynamic topic information to mitigate the off-topic problem and thus improves the overall generation performance. Besides, our DTT model not only improves per"
2020.coling-main.215,D15-1166,0,0.599629,"rocess and thus not directly comparable to our work proposed in this paper. Some works in text generation (Gatt and Krahmer, 2018) have been proposed to incorporate the topic information to help generate the text. These ideas can be adopted in KB-to-text generation. Tars and Fishel (2018) and Johnson et al. (2017) add an extra topic tag into the source sentence for incorporating the topic information into the model. The whole model is built based on the sequence-to-sequence (Sutskever et al., 2014; Cho et al., 2014; Klein et al., 2017) framework with standard attention (Bahdanau et al., 2014; Luong et al., 2015). Mikolov and Zweig (2012) as well as Liu et al. (2015) propose to use the topic information as extra features to enhance the performance of the language model and word embedding. Chen et al. (2016) and Ou et al. (2018) use the same idea to utilize the topic feature to enhance the generation of the text. However, all these methods assume that the topic information is known in advance. Some methods investigate the problem setting that the topic information is not given and needs to be detected. For example, the topic information can be detected from Latent Dirichlet Allocation (LDA). Zhang et a"
2020.coling-main.215,W17-5525,0,0.197912,"Missing"
2020.coling-main.215,P02-1040,0,0.107999,"aMater, University of Michigan&gt;, &lt;James Craig Watson, almaMater, University of Michigan&gt;, &lt;James Craig Watson, deathCause, Peritonitis&gt;} &lt;James Craig Watson, nationality, Canada&gt;, 0 &lt;James Craig Watson, deathPlace, Madison , Wisconsin&gt;} Figure 3: Topic distribution at each step. White stands for higher probability while black stands for lower. the number of topics is set to 100. In the T2S model, we set the number of topics to 200. In the DTT model, we set the number of topics to 500. We evaluate all the models with the same evaluation script 4 . Several metrics are evaluated, including BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), NIST L (Doddington, 2002), METEOR (Banerjee and Lavie, 2005) and CIDEr (Vedantam et al., 2015). Since some metrics are sensitive to randomness, we run each model for 5 times and report the median score with the standard deviation. 4.4 Results The experimental results are shown in Table 2. We can observe that our DTT model outperforms all comparison models significantly and consistently. It illustrates that the DTT model can capture the dynamic topic information to mitigate the off-topic problem and thus improves the overall generation performance. Besides, our DTT model no"
2020.coling-main.215,W18-6543,0,0.563828,"The flattened text is used as the input for a sequence-to-sequence model to generate the output text. Vougiouklis et al. (2018) propose to use a triple encoder to encode each triple into a hidden vector. The decoder input is constructed by simply concatenating all of the hidden vectors. Trisedya et al. (2018) propose a GTR-LSTM model to encode not only the triple information, but also the structure information of the entity graph into hidden semantic space. Jain et al. (2018) exploit a mixed hierarchical attention based encoder-decoder model to leverage the structure and content information. Shimorina and Gardent (2018) propose to use delexicalization and copy mechanism to enhance the performance of the sequence-to-sequence framework. Konstas and Lapata (2013) and Wiseman et al. (2018) propose to use template based methods to generate the text by using the extracted template information in the training set. Cheng et al. (2020) propose to generate text description for entities by utilizing the knowledge distilled from the existing knowledge base. However, none of the above works consider the topic information in the KB-to-text generation process and thus not directly comparable to our work proposed in this pa"
2020.coling-main.215,P18-1151,0,0.0130424,"ing some formatted data into natural language texts facilitating more understandability. Some models have proposed to solve the KB-to-text problem by utilizing various information of the KBs. Chisholm et al. (2017) propose to directly rank the triples by relation frequency and flatten the triples to pure text. The flattened text is used as the input for a sequence-to-sequence model to generate the output text. Vougiouklis et al. (2018) propose to use a triple encoder to encode each triple into a hidden vector. The decoder input is constructed by simply concatenating all of the hidden vectors. Trisedya et al. (2018) propose a GTR-LSTM model to encode not only the triple information, but also the structure information of the entity graph into hidden semantic space. Jain et al. (2018) exploit a mixed hierarchical attention based encoder-decoder model to leverage the structure and content information. Shimorina and Gardent (2018) propose to use delexicalization and copy mechanism to enhance the performance of the sequence-to-sequence framework. Konstas and Lapata (2013) and Wiseman et al. (2018) propose to use template based methods to generate the text by using the extracted template information in the tra"
2020.coling-main.215,P19-1240,0,0.0284316,"well as Liu et al. (2015) propose to use the topic information as extra features to enhance the performance of the language model and word embedding. Chen et al. (2016) and Ou et al. (2018) use the same idea to utilize the topic feature to enhance the generation of the text. However, all these methods assume that the topic information is known in advance. Some methods investigate the problem setting that the topic information is not given and needs to be detected. For example, the topic information can be detected from Latent Dirichlet Allocation (LDA). Zhang et al. (2016; Dziri et al. (2018; Wang et al. (2019) detect the topic distribution of words via topic model to enhance the translation procedure. Xing et al. (2017) propose a TA-Seq2Seq framework which uses the word topic information from LDA to generate the responses in chatbot dialog systems. Moreover, some researchers propose to directly detect the topic vector from the input sentences in the sequence-to-sequence framework. For example, Choudhary et al. (2017) propose to train a classifier to predict the topic of the source sentence and use it to help generate the dialog response. Ou et al. (2018) also propose to predict the topic vector dir"
2020.coling-main.215,D17-1239,0,0.0179987,"n procedure. 2 Related Work Recently, various data-to-text tasks have been proposed handling different kinds of data. Gardent et al. (2017a; Gardent et al. (2017b) construct the WebNLG dataset which aims at generating text descriptions based on DBpedia (Auer et al., 2007) triples. Lebret et al. (2016) and Chisholm et al. (2017) propose to generate a person’s biography based on Wikipedia’s infobox. Fu et al. (2020a) build the WikiEvent dataset aiming at generating text based on an event chain. Novikova et al. (2017) generate restaurant reviews based on the information of restaurant attributes. Wiseman et al. (2017) generate basketball match descriptions based on the game records. Moreover, Fu et al. (2020c) propose to directly train the model on partially-aligned data called WITA while Fu et al. (2020b) propose to train a model based on purely unaligned data unsupervised with a dual learning framework. All of the above problems aim at 2370 converting some formatted data into natural language texts facilitating more understandability. Some models have proposed to solve the KB-to-text problem by utilizing various information of the KBs. Chisholm et al. (2017) propose to directly rank the triples by relati"
2020.coling-main.215,D18-1356,0,0.016437,"triple into a hidden vector. The decoder input is constructed by simply concatenating all of the hidden vectors. Trisedya et al. (2018) propose a GTR-LSTM model to encode not only the triple information, but also the structure information of the entity graph into hidden semantic space. Jain et al. (2018) exploit a mixed hierarchical attention based encoder-decoder model to leverage the structure and content information. Shimorina and Gardent (2018) propose to use delexicalization and copy mechanism to enhance the performance of the sequence-to-sequence framework. Konstas and Lapata (2013) and Wiseman et al. (2018) propose to use template based methods to generate the text by using the extracted template information in the training set. Cheng et al. (2020) propose to generate text description for entities by utilizing the knowledge distilled from the existing knowledge base. However, none of the above works consider the topic information in the KB-to-text generation process and thus not directly comparable to our work proposed in this paper. Some works in text generation (Gatt and Krahmer, 2018) have been proposed to incorporate the topic information to help generate the text. These ideas can be adopted"
2020.coling-main.215,C16-1170,0,0.366443,"c information in one sentence, as exemplified by the topic changes from “person” to “company” in Figure 1. Therefore, we investigate the task of automatically detecting the hidden topic information and incorporating such information for the generation of sentences. Many works have been proposed to utilize the static topic information to improve the generation performance. Chen et al. (2016) and Ou et al. (2018) propose to represent the topic for each sentence as a learnable vector. The topic is predicted by the input sentence and is used to enhance the generating phase. Xing et al. (2017) and Zhang et al. (2016) detect the topic representation by applying a pre-trained LDA model on the input sequence. Moreover, Choudhary et al. (2017) and Ou et al. (2018) predict the topic representation directly from the input sequence using Recurrent Neural Networks (RNN). All the above methods make an assumption that during generation the topic does not change so as to make the problem tractable, which scarifies the advantage of modeling the dynamic nature of topic information. We propose a novel Dynamic Topic Tracker (DTT) neural model to tackle the problem. Different from existing models, our proposed DTT model"
2020.emnlp-main.124,S17-2001,0,0.435761,"of datasets. 4.1.1 shows the results on seven STS benchmarks, which include texts from various domains and are commonly used for evaluating general-purpose sentence representations. 4.1.2 further shows the results on the challenging Argument Facet Similarity (AFS) dataset (Misra et al., 2016), which is more suitable for evaluating model’s performance in domain-specific scenarios. For all methods compared in this subsection, Unsupervised STS Experimental Details: We evaluate our model on the STS tasks 2012-2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016), the STS benchmark (STSb for short) (Cer et al., 2017), and the SICK-Relatedness dataset (Marelli et al., 2014). The corresponding datasets consist of sentence pairs with labels from 0 to 5 indicating the semantic relatedness. As pointed out in Reimers et al. (2016) that Pearson correlation is badly suited for STS, Spearman’s rank correlation between the cosine-similarity of the sentence embeddings and the gold labels is instead used as the evaluation metric. Following SBERT which was trained on the combination of the SNLI (Bowman et al., 2015) and the Multi-Genre NLI (MultiNLI) (Williams et al., 2018) datasets with gold labels, we train IS-BERT"
2020.emnlp-main.124,D18-2029,0,0.197363,"Missing"
2020.emnlp-main.124,L18-1269,0,0.386066,"native tasks that require the models to correctly predict the contextual information , such as skip-thought (Kiros et al., 2015) and FastSent (Hill et al., 2016), or to distinguish target sentences from contrastive ones (Jernite et al., 2017; Logeswaran and Lee, 2018) for sentence embedding (Jernite et al., 2017; Logeswaran and Lee, 2018). These methods require ordered sentences or corpus with inter-sentential coherence for training, which limits their applications to domains with only short texts. We evaluate our method on two groups of tasks – Semantic Textual Similarity (STS) and SentEval (Conneau and Kiela, 2018). Empirical results show that IS-BERT significantly outperforms other unsupervised baselines on STS and SentEval tasks. In addition, we show that IS-BERT substantially outperforms SBERT in a setting where task-specific labeled data is not available. This demonstrates that IS-BERT has the flexibility to be applied to new domains without label restriction. Finally, ISBERT can achieve performance competitive with or even better than supervised learning methods in certain scenarios. 2.1 Related Work Sentence Representation Learning Prior approaches for sentence embedding include two main categorie"
2020.emnlp-main.124,S14-2010,0,0.401722,"Missing"
2020.emnlp-main.124,S16-1081,0,0.437403,"Missing"
2020.emnlp-main.124,S12-1051,0,0.548452,"Missing"
2020.emnlp-main.124,S13-1004,0,0.439483,"Missing"
2020.emnlp-main.124,D17-1070,0,0.252061,"S-BERT has the flexibility to be applied to new domains without label restriction. Finally, ISBERT can achieve performance competitive with or even better than supervised learning methods in certain scenarios. 2.1 Related Work Sentence Representation Learning Prior approaches for sentence embedding include two main categories: (1) unsupervised sentence embedding with unlabeled sentences, and (2) supervised learning with labeled sentences, while a few methods might leverage on both of them. Supervised Sentence Embedding. There have also been attempts to use labeled data for sentence embedding. Conneau et al. (2017) proposed the InferSent model that uses labeled data of the Stanford Natural Language Inference dataset (SNLI) (Bowman et al., 2015) and the Multi-Genre NLI dataset (Williams et al., 2018) to train a BiLSTM siamese network for sentence embedding. Universal Sentence Encoder (Cer et al., 2018) utilized supervised training with SNLI to augment the unsupervised training of a transformer network. SBERT (Reimers and Gurevych, 2019) also trained a siamese network on NLI to encode sentences, but it further benefits from the pretraining procedure of BERT. Though effective, those models could be 1602 pr"
2020.emnlp-main.124,C04-1051,0,0.372979,".2 4.2.1 Supervised Evaluations SentEval Experimental Details: Here we evaluate the sentence representations in IS-BERT on a set of supervised tasks. Following Reimers and Gurevych (2019), we use a set of classification tasks that covers various types of sentence classification, including sentiment analysis (CR (Hu and Liu, 2004), MR (Pang and Lee, 2005) and SST (Socher et al., 2013)), question-type classification (TREC (Li and Roth, 2002)), subjectivity classification (SUBJ (Pang and Lee, 2004)), opinion polarity classification (MPQA (Wiebe et al., 2005)) and paraphrase identification (MRPC (Dolan et al., 2004)). Since these tasks are more domain-specific, we train IS-BERT on each of the task-specific dataset (without label) to produce sentence embeddings, which are then used for training downstream classifiers. We denote this setting as IS-BERT-task. SentEval (Conneau and Kiela, 2018) toolkit is used to automate the evaluation process. It takes sentence embeddings as fixed input features to a logistic regression classifier, which is trained in a Model r Without task-specific labeled data Unigram-TFIDF 46.77 InferSent-GloVe 27.08 Avg. GloVe embeddings 32.40 Avg. BERT embeddings 35.39 SBERT-NLI 16.27"
2020.emnlp-main.124,D17-1254,0,0.0131266,"guage inference (NLI) tasks with labeled sentence pairs and achieved state-of-the-art performance on many semantic textual similarity tasks. However, such improvements are induced by high-quality supervision, and we find that their performance is degraded where labeled data of the target task is extremely scarce or the distribution of test set differs significantly from the NLI dataset used for training. Learning sentence representations in an unsupervised manner is a critical step to work with unlabeled or partially labeled dataset to address the aforementioned challenge (Kiros et al., 2015; Gan et al., 2017; Hill et al., 2016; Pagliardini et al., 2017; Yang et al., 2018). A common approach for unsupervised sentence representation learning is to leverage on self-supervision with large unlabeled corpus. For example, early methods explored various auto-encoders for sentence embedding (Socher et al., 2011; Hill et al., 2016). Recent work such as skip-thought (Kiros et al., 2015) and FastSent (Hill et al., 2016) assumed that a sentence is likely to have similar semantics to its context, and designed 1601 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 160"
2020.emnlp-main.124,N16-1162,0,0.285253,"LI) tasks with labeled sentence pairs and achieved state-of-the-art performance on many semantic textual similarity tasks. However, such improvements are induced by high-quality supervision, and we find that their performance is degraded where labeled data of the target task is extremely scarce or the distribution of test set differs significantly from the NLI dataset used for training. Learning sentence representations in an unsupervised manner is a critical step to work with unlabeled or partially labeled dataset to address the aforementioned challenge (Kiros et al., 2015; Gan et al., 2017; Hill et al., 2016; Pagliardini et al., 2017; Yang et al., 2018). A common approach for unsupervised sentence representation learning is to leverage on self-supervision with large unlabeled corpus. For example, early methods explored various auto-encoders for sentence embedding (Socher et al., 2011; Hill et al., 2016). Recent work such as skip-thought (Kiros et al., 2015) and FastSent (Hill et al., 2016) assumed that a sentence is likely to have similar semantics to its context, and designed 1601 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1601–1610, c November"
2020.emnlp-main.124,D15-1075,0,0.549997,"e with or even better than supervised learning methods in certain scenarios. 2.1 Related Work Sentence Representation Learning Prior approaches for sentence embedding include two main categories: (1) unsupervised sentence embedding with unlabeled sentences, and (2) supervised learning with labeled sentences, while a few methods might leverage on both of them. Supervised Sentence Embedding. There have also been attempts to use labeled data for sentence embedding. Conneau et al. (2017) proposed the InferSent model that uses labeled data of the Stanford Natural Language Inference dataset (SNLI) (Bowman et al., 2015) and the Multi-Genre NLI dataset (Williams et al., 2018) to train a BiLSTM siamese network for sentence embedding. Universal Sentence Encoder (Cer et al., 2018) utilized supervised training with SNLI to augment the unsupervised training of a transformer network. SBERT (Reimers and Gurevych, 2019) also trained a siamese network on NLI to encode sentences, but it further benefits from the pretraining procedure of BERT. Though effective, those models could be 1602 problematic to port to new domains where highquality labeled data is not available, or the text distribution is significantly differen"
2020.emnlp-main.124,D14-1181,0,0.0544167,"Missing"
2020.emnlp-main.124,C02-1150,0,0.0444551,"huge impact on supervised sentence embedding learning, as a result, the supervised methods are problematic to be applied to downstream tasks of domains without labeled data. 4.2 4.2.1 Supervised Evaluations SentEval Experimental Details: Here we evaluate the sentence representations in IS-BERT on a set of supervised tasks. Following Reimers and Gurevych (2019), we use a set of classification tasks that covers various types of sentence classification, including sentiment analysis (CR (Hu and Liu, 2004), MR (Pang and Lee, 2005) and SST (Socher et al., 2013)), question-type classification (TREC (Li and Roth, 2002)), subjectivity classification (SUBJ (Pang and Lee, 2004)), opinion polarity classification (MPQA (Wiebe et al., 2005)) and paraphrase identification (MRPC (Dolan et al., 2004)). Since these tasks are more domain-specific, we train IS-BERT on each of the task-specific dataset (without label) to produce sentence embeddings, which are then used for training downstream classifiers. We denote this setting as IS-BERT-task. SentEval (Conneau and Kiela, 2018) toolkit is used to automate the evaluation process. It takes sentence embeddings as fixed input features to a logistic regression classifier, w"
2020.emnlp-main.124,2021.ccl-1.108,0,0.176977,"Missing"
2020.emnlp-main.124,marelli-etal-2014-sick,0,0.128546,"nchmarks, which include texts from various domains and are commonly used for evaluating general-purpose sentence representations. 4.1.2 further shows the results on the challenging Argument Facet Similarity (AFS) dataset (Misra et al., 2016), which is more suitable for evaluating model’s performance in domain-specific scenarios. For all methods compared in this subsection, Unsupervised STS Experimental Details: We evaluate our model on the STS tasks 2012-2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016), the STS benchmark (STSb for short) (Cer et al., 2017), and the SICK-Relatedness dataset (Marelli et al., 2014). The corresponding datasets consist of sentence pairs with labels from 0 to 5 indicating the semantic relatedness. As pointed out in Reimers et al. (2016) that Pearson correlation is badly suited for STS, Spearman’s rank correlation between the cosine-similarity of the sentence embeddings and the gold labels is instead used as the evaluation metric. Following SBERT which was trained on the combination of the SNLI (Bowman et al., 2015) and the Multi-Genre NLI (MultiNLI) (Williams et al., 2018) datasets with gold labels, we train IS-BERT on the same collection of sentences, but without using th"
2020.emnlp-main.124,W16-3636,0,0.334343,"abeled NLI data including InferSent (Conneau et al., 2017), Universal Sentence Encoder (USE) (Cer et al., 2018), and sentence BERT (SBERT-NLI) (Reimers and Gurevych, 2019). uncased-BERT-base is used for all BERT-related models including IS-BERT. 4.1 Unsupervised Evaluations For STS tasks, we conduct evaluations on two types of datasets. 4.1.1 shows the results on seven STS benchmarks, which include texts from various domains and are commonly used for evaluating general-purpose sentence representations. 4.1.2 further shows the results on the challenging Argument Facet Similarity (AFS) dataset (Misra et al., 2016), which is more suitable for evaluating model’s performance in domain-specific scenarios. For all methods compared in this subsection, Unsupervised STS Experimental Details: We evaluate our model on the STS tasks 2012-2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016), the STS benchmark (STSb for short) (Cer et al., 2017), and the SICK-Relatedness dataset (Marelli et al., 2014). The corresponding datasets consist of sentence pairs with labels from 0 to 5 indicating the semantic relatedness. As pointed out in Reimers et al. (2016) that Pearson correlation is badly suited for STS, Spearman’s ran"
2020.emnlp-main.124,P05-1015,0,0.304465,"ates that the domain-relatedness between the training set and the target test set has a huge impact on supervised sentence embedding learning, as a result, the supervised methods are problematic to be applied to downstream tasks of domains without labeled data. 4.2 4.2.1 Supervised Evaluations SentEval Experimental Details: Here we evaluate the sentence representations in IS-BERT on a set of supervised tasks. Following Reimers and Gurevych (2019), we use a set of classification tasks that covers various types of sentence classification, including sentiment analysis (CR (Hu and Liu, 2004), MR (Pang and Lee, 2005) and SST (Socher et al., 2013)), question-type classification (TREC (Li and Roth, 2002)), subjectivity classification (SUBJ (Pang and Lee, 2004)), opinion polarity classification (MPQA (Wiebe et al., 2005)) and paraphrase identification (MRPC (Dolan et al., 2004)). Since these tasks are more domain-specific, we train IS-BERT on each of the task-specific dataset (without label) to produce sentence embeddings, which are then used for training downstream classifiers. We denote this setting as IS-BERT-task. SentEval (Conneau and Kiela, 2018) toolkit is used to automate the evaluation process. It t"
2020.emnlp-main.124,C16-1009,0,0.104025,"esults on the challenging Argument Facet Similarity (AFS) dataset (Misra et al., 2016), which is more suitable for evaluating model’s performance in domain-specific scenarios. For all methods compared in this subsection, Unsupervised STS Experimental Details: We evaluate our model on the STS tasks 2012-2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016), the STS benchmark (STSb for short) (Cer et al., 2017), and the SICK-Relatedness dataset (Marelli et al., 2014). The corresponding datasets consist of sentence pairs with labels from 0 to 5 indicating the semantic relatedness. As pointed out in Reimers et al. (2016) that Pearson correlation is badly suited for STS, Spearman’s rank correlation between the cosine-similarity of the sentence embeddings and the gold labels is instead used as the evaluation metric. Following SBERT which was trained on the combination of the SNLI (Bowman et al., 2015) and the Multi-Genre NLI (MultiNLI) (Williams et al., 2018) datasets with gold labels, we train IS-BERT on the same collection of sentences, but without using the label information. We denote our model in this setting as IS-BERT-NLI. SNLI contains 570,000 sentence pairs annotated with the labels contradiction, enta"
2020.emnlp-main.124,D19-1410,0,0.0906615,"mance on various downstream NLP tasks. However, they are inefficient for sentencepair regression tasks such as clustering or semantic search because they need to evaluate combinatorially many sentence pairs during inference, which will result in a massive computational overhead. ∗ Equally Contributed. This work was done when Yan Zhang was an intern at DAMO Academy, Alibaba Group. † Corresponding author. For example, finding the most similar pair in a collection  of 10k sentences requires about 50 million 10k ( 2 ) inference computations with BERT, which requires about 65 hours on a V100 GPU (Reimers and Gurevych, 2019). Much previous work attempted to address this problem by learning semantically meaningful representations for each sentence, such that similarity measures like cosine distance can be easily evaluated for sentence-pair regression tasks. The straightforward way to derive a fixed-size sentence embedding from BERT-based models is to average the token representations at the last layer or using the output of the [CLS] token. Reimers and Gurevych (2019) showed that both approaches yield rather unsatisfactory sentence embeddings. They proposed a model, Sentence-BERT (SBERT), to further fine-tune BERT"
2020.emnlp-main.124,P19-1054,0,0.0207901,"ned in a Model r Without task-specific labeled data Unigram-TFIDF 46.77 InferSent-GloVe 27.08 Avg. GloVe embeddings 32.40 Avg. BERT embeddings 35.39 SBERT-NLI 16.27 Ours: IS-BERT-AFS 49.14 ρ 42.95 26.63 34.00 35.07 15.84 45.25 Supervised: 10-fold cross-validation BERT-AFS 77.20 74.84 SBERT-AFS 76.57 74.13 Supervised: cross-topic evaluation BERT-AFS 58.49 57.23 52.34 50.65 SBERT-AFS Table 2: Average Pearson correlation r and average Spearman’s rank correlation ρ over three topics on the Argument Facet Similarity (AFS) corpus. Results of baselines are extracted from (Reimers and Gurevych, 2019; Reimers et al., 2019) 10-fold cross-validation setup and the prediction accuracy is computed for the test-fold. Results: Table 3 presents the results. Overall, supervised methods outperform unsupervised baselines. This indicates that pretraining sentence encoder with high-quality labeled data such as NLI is helpful in a supervised transfer learning setting. Note that in this task, SentEval fits a logistic regression classifier to the sentence embeddings with labels of the downstream tasks. Thus, the models that achieve good results on this task do not necessarily work well on unsupervised tasks such as clustering."
2020.emnlp-main.124,D13-1170,0,0.0177674,"ess between the training set and the target test set has a huge impact on supervised sentence embedding learning, as a result, the supervised methods are problematic to be applied to downstream tasks of domains without labeled data. 4.2 4.2.1 Supervised Evaluations SentEval Experimental Details: Here we evaluate the sentence representations in IS-BERT on a set of supervised tasks. Following Reimers and Gurevych (2019), we use a set of classification tasks that covers various types of sentence classification, including sentiment analysis (CR (Hu and Liu, 2004), MR (Pang and Lee, 2005) and SST (Socher et al., 2013)), question-type classification (TREC (Li and Roth, 2002)), subjectivity classification (SUBJ (Pang and Lee, 2004)), opinion polarity classification (MPQA (Wiebe et al., 2005)) and paraphrase identification (MRPC (Dolan et al., 2004)). Since these tasks are more domain-specific, we train IS-BERT on each of the task-specific dataset (without label) to produce sentence embeddings, which are then used for training downstream classifiers. We denote this setting as IS-BERT-task. SentEval (Conneau and Kiela, 2018) toolkit is used to automate the evaluation process. It takes sentence embeddings as fi"
2020.emnlp-main.124,N18-1101,0,0.442148,"in certain scenarios. 2.1 Related Work Sentence Representation Learning Prior approaches for sentence embedding include two main categories: (1) unsupervised sentence embedding with unlabeled sentences, and (2) supervised learning with labeled sentences, while a few methods might leverage on both of them. Supervised Sentence Embedding. There have also been attempts to use labeled data for sentence embedding. Conneau et al. (2017) proposed the InferSent model that uses labeled data of the Stanford Natural Language Inference dataset (SNLI) (Bowman et al., 2015) and the Multi-Genre NLI dataset (Williams et al., 2018) to train a BiLSTM siamese network for sentence embedding. Universal Sentence Encoder (Cer et al., 2018) utilized supervised training with SNLI to augment the unsupervised training of a transformer network. SBERT (Reimers and Gurevych, 2019) also trained a siamese network on NLI to encode sentences, but it further benefits from the pretraining procedure of BERT. Though effective, those models could be 1602 problematic to port to new domains where highquality labeled data is not available, or the text distribution is significantly different from the NLI dataset such that knowledge learned from"
2020.emnlp-main.124,W18-3022,0,0.0746441,"ieved state-of-the-art performance on many semantic textual similarity tasks. However, such improvements are induced by high-quality supervision, and we find that their performance is degraded where labeled data of the target task is extremely scarce or the distribution of test set differs significantly from the NLI dataset used for training. Learning sentence representations in an unsupervised manner is a critical step to work with unlabeled or partially labeled dataset to address the aforementioned challenge (Kiros et al., 2015; Gan et al., 2017; Hill et al., 2016; Pagliardini et al., 2017; Yang et al., 2018). A common approach for unsupervised sentence representation learning is to leverage on self-supervision with large unlabeled corpus. For example, early methods explored various auto-encoders for sentence embedding (Socher et al., 2011; Hill et al., 2016). Recent work such as skip-thought (Kiros et al., 2015) and FastSent (Hill et al., 2016) assumed that a sentence is likely to have similar semantics to its context, and designed 1601 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1601–1610, c November 16–20, 2020. 2020 Association for Computationa"
2020.emnlp-main.124,P04-1035,0,0.0619965,"s a result, the supervised methods are problematic to be applied to downstream tasks of domains without labeled data. 4.2 4.2.1 Supervised Evaluations SentEval Experimental Details: Here we evaluate the sentence representations in IS-BERT on a set of supervised tasks. Following Reimers and Gurevych (2019), we use a set of classification tasks that covers various types of sentence classification, including sentiment analysis (CR (Hu and Liu, 2004), MR (Pang and Lee, 2005) and SST (Socher et al., 2013)), question-type classification (TREC (Li and Roth, 2002)), subjectivity classification (SUBJ (Pang and Lee, 2004)), opinion polarity classification (MPQA (Wiebe et al., 2005)) and paraphrase identification (MRPC (Dolan et al., 2004)). Since these tasks are more domain-specific, we train IS-BERT on each of the task-specific dataset (without label) to produce sentence embeddings, which are then used for training downstream classifiers. We denote this setting as IS-BERT-task. SentEval (Conneau and Kiela, 2018) toolkit is used to automate the evaluation process. It takes sentence embeddings as fixed input features to a logistic regression classifier, which is trained in a Model r Without task-specific labele"
2020.emnlp-main.169,W13-2322,0,0.159262,": The concept (join-01) in vanilla GCNs is that it only captures information from its immediate neighbors (first-order), while in LDGCNs it can integrate information from neighbors of different order (e.g., second-order and third-order). In SANs, the node collects information from all other nodes, while in structured SANs it is aware of its connected nodes in the original graph. Introduction Graph structures play a pivotal role in NLP because they are able to capture particularly rich structural information. For example, Figure 1 shows a directed, labeled Abstract Meaning Representation (AMR; Banarescu et al. 2013) graph, where each node denotes a semantic concept and each edge denotes a relation between such concepts. Within ∗ Equally Contributed. Work done while Yan Zhang was an intern at DAMO Academy, Alibaba Group and Zhijiang Guo was at the University of Edinburgh. † Corresponding author. the realm of work on AMR, we focus in this paper on the problem of AMR-to-text generation, i.e. transducing AMR graphs into text that conveys the information in the AMR structure. A key challenge in this task is to efficiently learn useful representations of the AMR graphs. Early efforts (Pourdamghani et al., 2016"
2020.emnlp-main.169,P18-1026,0,0.626864,"the University of Edinburgh. † Corresponding author. the realm of work on AMR, we focus in this paper on the problem of AMR-to-text generation, i.e. transducing AMR graphs into text that conveys the information in the AMR structure. A key challenge in this task is to efficiently learn useful representations of the AMR graphs. Early efforts (Pourdamghani et al., 2016; Konstas et al., 2017) neglect a significant part of the structural information in the input graph by linearizing it. Recently, Graph Neural Networks (GNNs) have been explored to better encode structural information for this task (Beck et al., 2018; Song et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019). One type of such GNNs is Graph Convolutional Networks (GCNs; Kipf and Welling 2017). GCNs follow a local information aggregation scheme, iteratively updating the representations of nodes based on their immediate (first-order) neighbors. Intuitively, stacking more convolutional layers in GCNs helps capture more complex interactions (Xu et al., 2018; Guo et al., 2019b). However, prior efforts (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020) have shown that the locality property of existing GCNs precludes efficient nonlo"
2020.emnlp-main.169,D19-1378,0,0.0186021,"l., 2018; Li et al., 2019a). More recently, SANbased models (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020) outperform GNN-based models as they are able to capture global dependencies. Unlike previous models, our local, yet efficient model, based solely on graph convolutions, outperforms competitive structured SANs while using a significantly smaller model. Related Work Graph convolutional networks (Kipf and Welling, 2017) have been widely used as the structural encoder in various NLP applications including question answering (De Cao et al., 2019; Lin et al., 2019), semantic parsing (Bogin et al., 2019a,b) and relation extraction (Guo et al., 2019a, 2020). Conclusion In this paper, we propose LDGCNs for AMR-totext generation. Compared with existing GCNs and SANs, LDGCNs maintain a better balance between parameter efficiency and model capacity. LDGCNs outperform state-of-the-art models on AMR-to-text generation. In future work, we would like to investigate methods to stabilize the training of weight tied models and apply our model on other tasks in Natural Language Generation. Acknowledgments We would like to thank the anonymous reviewers for their constructive comments. We would also like t"
2020.emnlp-main.169,P19-1448,0,0.0224765,"l., 2018; Li et al., 2019a). More recently, SANbased models (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020) outperform GNN-based models as they are able to capture global dependencies. Unlike previous models, our local, yet efficient model, based solely on graph convolutions, outperforms competitive structured SANs while using a significantly smaller model. Related Work Graph convolutional networks (Kipf and Welling, 2017) have been widely used as the structural encoder in various NLP applications including question answering (De Cao et al., 2019; Lin et al., 2019), semantic parsing (Bogin et al., 2019a,b) and relation extraction (Guo et al., 2019a, 2020). Conclusion In this paper, we propose LDGCNs for AMR-totext generation. Compared with existing GCNs and SANs, LDGCNs maintain a better balance between parameter efficiency and model capacity. LDGCNs outperform state-of-the-art models on AMR-to-text generation. In future work, we would like to investigate methods to stabilize the training of weight tied models and apply our model on other tasks in Natural Language Generation. Acknowledgments We would like to thank the anonymous reviewers for their constructive comments. We would also like t"
2020.emnlp-main.169,2020.acl-main.640,0,0.307027,"aph Neural Networks (GNNs) have been explored to better encode structural information for this task (Beck et al., 2018; Song et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019). One type of such GNNs is Graph Convolutional Networks (GCNs; Kipf and Welling 2017). GCNs follow a local information aggregation scheme, iteratively updating the representations of nodes based on their immediate (first-order) neighbors. Intuitively, stacking more convolutional layers in GCNs helps capture more complex interactions (Xu et al., 2018; Guo et al., 2019b). However, prior efforts (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020) have shown that the locality property of existing GCNs precludes efficient nonlocal information propagation. Abu-El-Haija et al. 2162 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2162–2172, c November 16–20, 2020. 2020 Association for Computational Linguistics (2019) further proved that vanilla GCNs are unable to capture feature differences among neighbors from different orders no matter how many layers are stacked. Therefore, Self-Attention Networks (SANs; Vaswani et al. 2017) have been explored as an alternative to capture"
2020.emnlp-main.169,N19-1223,0,0.143753,"al., 2019b; Damonte and Cohen, 2019). Following Wang et al. (2020), we use a transformer as the decoder for large-scale evaluation. For fair comparisons, we use the same optimization and regularization strategies as in Guo et al. (2019b). All hyperparameters are tuned on the development set2 . For evaluation, we report BLEU scores (Papineni et al., 2002), CHRF++ (Popovic, 2017) scores and METEOR scores (Denkowski and Lavie, 2014) with additional human evaluation results. 2 Hyperparameter search; all hyperparameters are attached in the supplementary material. 2166 Model AMR2015 Type B Seq2Seq (Cao and Clark, 2019) GraphLSTM (Song et al., 2018) GGNNs (Beck et al., 2018) GCNLSTM (Damonte and Cohen, 2019) DCGCN (Guo et al., 2019b) DualGraph (Ribeiro et al., 2019) Single Single Single Single Single Single Seq2Seq (Konstas et al., 2017) GGNNs (Beck et al., 2018) DCGCN (Guo et al., 2019b) Ensemble Ensemble Ensemble Transformer (Zhu et al., 2019) GT Dual (Wang et al., 2020) GT GRU (Cai and Lam, 2020) GT SAN (Zhu et al., 2019) Single Single Single Single LDGCN WT LDGCN GC Single Single C M AMR2017 #P 23.5 23.3 24.4 23.6 25.7 0 54.5‡ 031.5‡ 018.6M‡ 24.3 053.8‡ 30.5 060.3M‡ - - B M #P 26.8 24.9 23.3 50.4 28.3M 2"
2020.emnlp-main.169,W14-3348,0,0.00949318,"and 3 for layerwise graph convolutions for the bottom and top sub-blocks, respectively. For the decoder, we employ the same attention-based LSTM as in previous work (Beck et al., 2018; Guo et al., 2019b; Damonte and Cohen, 2019). Following Wang et al. (2020), we use a transformer as the decoder for large-scale evaluation. For fair comparisons, we use the same optimization and regularization strategies as in Guo et al. (2019b). All hyperparameters are tuned on the development set2 . For evaluation, we report BLEU scores (Papineni et al., 2002), CHRF++ (Popovic, 2017) scores and METEOR scores (Denkowski and Lavie, 2014) with additional human evaluation results. 2 Hyperparameter search; all hyperparameters are attached in the supplementary material. 2166 Model AMR2015 Type B Seq2Seq (Cao and Clark, 2019) GraphLSTM (Song et al., 2018) GGNNs (Beck et al., 2018) GCNLSTM (Damonte and Cohen, 2019) DCGCN (Guo et al., 2019b) DualGraph (Ribeiro et al., 2019) Single Single Single Single Single Single Seq2Seq (Konstas et al., 2017) GGNNs (Beck et al., 2018) DCGCN (Guo et al., 2019b) Ensemble Ensemble Ensemble Transformer (Zhu et al., 2019) GT Dual (Wang et al., 2020) GT GRU (Cai and Lam, 2020) GT SAN (Zhu et al., 2019)"
2020.emnlp-main.169,E17-3017,0,0.0226855,"max( (hu WQ )(hv WK )T √ ) d (2) where WQ and WK are projection parameters. The adjacency matrix A in GCNs is given by the input AMR graph, while in SANs A is computed based on H, which neglects the structural information of the input AMR. The number of operations required by graph convolutions scales is found linearly in the input length, whereas they are quadratic for SANs. Structured SANs Zhu et al. (2019) and Cai and Lam (2020) extend SAN s by incorporating the relation ruv between node u and node v in the 1 Our implementation is based on MXNET (Chen et al., 2015) and the Sockeye toolkit (Felix et al., 2017). 2163 Hl A1 X Wl Hl+1 X Vanilla GCN Layer Hl A1 Wl X Hl A2 X X Wl X Dynamic Fusion Hl+1 LDGCN Layer Figure 2: Comparison between vanilla GCNs and LDGCNs. Hl denotes the representation of l-th layer. Wl denotes the trainable weights and × denotes matrix multiplication. Vanilla GCNs take the 1st-order adjacency matrix A1 as the input, which only captures information from one-hop neighbors. LDGCNs take k number of k-order adjacency matrix Ak as inputs, Wl is shared for all Ak . k is set to 2 here for simplification. A dynamic fusion mechanism is applied to integrate the information from 1- to k-"
2020.emnlp-main.169,N16-1087,0,0.0142735,"f the sentence, i.e., “rather than let them get even worse”, but it fails to capture the meaning of word “early” in its output, which is a critical part. DeepGCN parses both “early” and “get even worse” in the results. However, the readability of the generated sentence is not satisfactory. Compared to baselines, LDGCN is able to produce the best result, which has a correct starting phrase and captures the semantic meaning of critical words such as “early” and “get even worse” while also attaining good readability. 6 Early efforts for AMR-to-text generation mainly include grammar-based models (Flanigan et al., 2016; Song et al., 2017) and sequence-based models (Pourdamghani et al., 2016; Konstas et al., 2017; Cao and Clark, 2019), discarding crucial structural information when linearising the input AMR graph. To solve that, various GNNs including graph recurrent networks (Song et al., 2018; Ribeiro et al., 2019) and graph convolutional networks (Damonte and Cohen, 2019; Guo et al., 2019b) have been used to encode the AMR structure. Though GNNs are able to operate directly on graphs, the locality nature of them precludes efficient information propagation (Abu-El-Haija et al., 2018, 2019; Luan et al., 201"
2020.emnlp-main.169,P19-1024,1,0.166026,"information in the input graph by linearizing it. Recently, Graph Neural Networks (GNNs) have been explored to better encode structural information for this task (Beck et al., 2018; Song et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019). One type of such GNNs is Graph Convolutional Networks (GCNs; Kipf and Welling 2017). GCNs follow a local information aggregation scheme, iteratively updating the representations of nodes based on their immediate (first-order) neighbors. Intuitively, stacking more convolutional layers in GCNs helps capture more complex interactions (Xu et al., 2018; Guo et al., 2019b). However, prior efforts (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020) have shown that the locality property of existing GCNs precludes efficient nonlocal information propagation. Abu-El-Haija et al. 2162 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2162–2172, c November 16–20, 2020. 2020 Association for Computational Linguistics (2019) further proved that vanilla GCNs are unable to capture feature differences among neighbors from different orders no matter how many layers are stacked. Therefore, Self-Attention Networks (SANs; Vaswa"
2020.emnlp-main.169,Q19-1019,1,0.13775,"information in the input graph by linearizing it. Recently, Graph Neural Networks (GNNs) have been explored to better encode structural information for this task (Beck et al., 2018; Song et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019). One type of such GNNs is Graph Convolutional Networks (GCNs; Kipf and Welling 2017). GCNs follow a local information aggregation scheme, iteratively updating the representations of nodes based on their immediate (first-order) neighbors. Intuitively, stacking more convolutional layers in GCNs helps capture more complex interactions (Xu et al., 2018; Guo et al., 2019b). However, prior efforts (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020) have shown that the locality property of existing GCNs precludes efficient nonlocal information propagation. Abu-El-Haija et al. 2162 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2162–2172, c November 16–20, 2020. 2020 Association for Computational Linguistics (2019) further proved that vanilla GCNs are unable to capture feature differences among neighbors from different orders no matter how many layers are stacked. Therefore, Self-Attention Networks (SANs; Vaswa"
2020.emnlp-main.169,N19-1366,1,0.811253,"onding author. the realm of work on AMR, we focus in this paper on the problem of AMR-to-text generation, i.e. transducing AMR graphs into text that conveys the information in the AMR structure. A key challenge in this task is to efficiently learn useful representations of the AMR graphs. Early efforts (Pourdamghani et al., 2016; Konstas et al., 2017) neglect a significant part of the structural information in the input graph by linearizing it. Recently, Graph Neural Networks (GNNs) have been explored to better encode structural information for this task (Beck et al., 2018; Song et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019). One type of such GNNs is Graph Convolutional Networks (GCNs; Kipf and Welling 2017). GCNs follow a local information aggregation scheme, iteratively updating the representations of nodes based on their immediate (first-order) neighbors. Intuitively, stacking more convolutional layers in GCNs helps capture more complex interactions (Xu et al., 2018; Guo et al., 2019b). However, prior efforts (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020) have shown that the locality property of existing GCNs precludes efficient nonlocal information propagation. Abu-El-Haija et"
2020.emnlp-main.169,W04-3250,0,0.196269,"Missing"
2020.emnlp-main.169,2020.findings-emnlp.199,1,0.492865,"erly. As shown in Table 6, LDGCN GC has better human rankings in terms of both meaning similarity and readability than the state-of-the art GNN-based (DualGraph) and SAN-based model (GT SAN). DeepGCN without dynamic fusion mechanism obtains lower scores than GT SAN, which further confirms that synthesizing higher order information helps in learning better graph representations. 5.5 0-20 Additional Analysis To further reveal the source of performance gains, we perform additional analysis based on the characteristics of AMR graphs, i.e., graph size and graph reentrancy (Damonte and Cohen, 2019; Damonte et al., 2020). All experiments are conducted on the AMR2.0 test set and CHRF++ scores are reported. CHRF++ 5.4 58 64 58 0-1 2-3 4-5 Graph Reentrancies &gt;5 Figure 6: Performance against graph re-entrancies. Graph Size. As shown in Figure 5, the size of AMR graphs is partitioned into four categories ((0, 20], (20, 30], (30, 40], &gt; 40), Overall, LDGCN GC outperforms the best-reported GT SAN model across all graph sizes, and the performance gap becomes more profound with the increase of graph sizes. Although both models have sharp performance degradation for extremely large graphs (&gt; 40), the performance of LDG"
2020.emnlp-main.169,P17-1014,0,0.450535,"raph, where each node denotes a semantic concept and each edge denotes a relation between such concepts. Within ∗ Equally Contributed. Work done while Yan Zhang was an intern at DAMO Academy, Alibaba Group and Zhijiang Guo was at the University of Edinburgh. † Corresponding author. the realm of work on AMR, we focus in this paper on the problem of AMR-to-text generation, i.e. transducing AMR graphs into text that conveys the information in the AMR structure. A key challenge in this task is to efficiently learn useful representations of the AMR graphs. Early efforts (Pourdamghani et al., 2016; Konstas et al., 2017) neglect a significant part of the structural information in the input graph by linearizing it. Recently, Graph Neural Networks (GNNs) have been explored to better encode structural information for this task (Beck et al., 2018; Song et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019). One type of such GNNs is Graph Convolutional Networks (GCNs; Kipf and Welling 2017). GCNs follow a local information aggregation scheme, iteratively updating the representations of nodes based on their immediate (first-order) neighbors. Intuitively, stacking more convolutional layers in GCNs helps captur"
2020.emnlp-main.169,N19-1240,0,0.0608384,"Missing"
2020.emnlp-main.169,D19-1282,0,0.0218573,"mplex non-local interactions (Xu et al., 2018; Li et al., 2019a). More recently, SANbased models (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020) outperform GNN-based models as they are able to capture global dependencies. Unlike previous models, our local, yet efficient model, based solely on graph convolutions, outperforms competitive structured SANs while using a significantly smaller model. Related Work Graph convolutional networks (Kipf and Welling, 2017) have been widely used as the structural encoder in various NLP applications including question answering (De Cao et al., 2019; Lin et al., 2019), semantic parsing (Bogin et al., 2019a,b) and relation extraction (Guo et al., 2019a, 2020). Conclusion In this paper, we propose LDGCNs for AMR-totext generation. Compared with existing GCNs and SANs, LDGCNs maintain a better balance between parameter efficiency and model capacity. LDGCNs outperform state-of-the-art models on AMR-to-text generation. In future work, we would like to investigate methods to stabilize the training of weight tied models and apply our model on other tasks in Natural Language Generation. Acknowledgments We would like to thank the anonymous reviewers for their const"
2020.emnlp-main.169,D19-1548,0,0.0984317,"g it. Recently, Graph Neural Networks (GNNs) have been explored to better encode structural information for this task (Beck et al., 2018; Song et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019). One type of such GNNs is Graph Convolutional Networks (GCNs; Kipf and Welling 2017). GCNs follow a local information aggregation scheme, iteratively updating the representations of nodes based on their immediate (first-order) neighbors. Intuitively, stacking more convolutional layers in GCNs helps capture more complex interactions (Xu et al., 2018; Guo et al., 2019b). However, prior efforts (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020) have shown that the locality property of existing GCNs precludes efficient nonlocal information propagation. Abu-El-Haija et al. 2162 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2162–2172, c November 16–20, 2020. 2020 Association for Computational Linguistics (2019) further proved that vanilla GCNs are unable to capture feature differences among neighbors from different orders no matter how many layers are stacked. Therefore, Self-Attention Networks (SANs; Vaswani et al. 2017) have been explored as an alt"
2020.emnlp-main.169,P02-1040,0,0.106582,"dynamic fusion mechanism, N =2 for depthwise graph convolutions and M =6 and 3 for layerwise graph convolutions for the bottom and top sub-blocks, respectively. For the decoder, we employ the same attention-based LSTM as in previous work (Beck et al., 2018; Guo et al., 2019b; Damonte and Cohen, 2019). Following Wang et al. (2020), we use a transformer as the decoder for large-scale evaluation. For fair comparisons, we use the same optimization and regularization strategies as in Guo et al. (2019b). All hyperparameters are tuned on the development set2 . For evaluation, we report BLEU scores (Papineni et al., 2002), CHRF++ (Popovic, 2017) scores and METEOR scores (Denkowski and Lavie, 2014) with additional human evaluation results. 2 Hyperparameter search; all hyperparameters are attached in the supplementary material. 2166 Model AMR2015 Type B Seq2Seq (Cao and Clark, 2019) GraphLSTM (Song et al., 2018) GGNNs (Beck et al., 2018) GCNLSTM (Damonte and Cohen, 2019) DCGCN (Guo et al., 2019b) DualGraph (Ribeiro et al., 2019) Single Single Single Single Single Single Seq2Seq (Konstas et al., 2017) GGNNs (Beck et al., 2018) DCGCN (Guo et al., 2019b) Ensemble Ensemble Ensemble Transformer (Zhu et al., 2019) GT"
2020.emnlp-main.169,W17-4770,0,0.024584,"for depthwise graph convolutions and M =6 and 3 for layerwise graph convolutions for the bottom and top sub-blocks, respectively. For the decoder, we employ the same attention-based LSTM as in previous work (Beck et al., 2018; Guo et al., 2019b; Damonte and Cohen, 2019). Following Wang et al. (2020), we use a transformer as the decoder for large-scale evaluation. For fair comparisons, we use the same optimization and regularization strategies as in Guo et al. (2019b). All hyperparameters are tuned on the development set2 . For evaluation, we report BLEU scores (Papineni et al., 2002), CHRF++ (Popovic, 2017) scores and METEOR scores (Denkowski and Lavie, 2014) with additional human evaluation results. 2 Hyperparameter search; all hyperparameters are attached in the supplementary material. 2166 Model AMR2015 Type B Seq2Seq (Cao and Clark, 2019) GraphLSTM (Song et al., 2018) GGNNs (Beck et al., 2018) GCNLSTM (Damonte and Cohen, 2019) DCGCN (Guo et al., 2019b) DualGraph (Ribeiro et al., 2019) Single Single Single Single Single Single Seq2Seq (Konstas et al., 2017) GGNNs (Beck et al., 2018) DCGCN (Guo et al., 2019b) Ensemble Ensemble Ensemble Transformer (Zhu et al., 2019) GT Dual (Wang et al., 2020)"
2020.emnlp-main.169,W16-6603,0,0.247764,"R; Banarescu et al. 2013) graph, where each node denotes a semantic concept and each edge denotes a relation between such concepts. Within ∗ Equally Contributed. Work done while Yan Zhang was an intern at DAMO Academy, Alibaba Group and Zhijiang Guo was at the University of Edinburgh. † Corresponding author. the realm of work on AMR, we focus in this paper on the problem of AMR-to-text generation, i.e. transducing AMR graphs into text that conveys the information in the AMR structure. A key challenge in this task is to efficiently learn useful representations of the AMR graphs. Early efforts (Pourdamghani et al., 2016; Konstas et al., 2017) neglect a significant part of the structural information in the input graph by linearizing it. Recently, Graph Neural Networks (GNNs) have been explored to better encode structural information for this task (Beck et al., 2018; Song et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019). One type of such GNNs is Graph Convolutional Networks (GCNs; Kipf and Welling 2017). GCNs follow a local information aggregation scheme, iteratively updating the representations of nodes based on their immediate (first-order) neighbors. Intuitively, stacking more convolutional laye"
2020.emnlp-main.169,D19-1314,0,0.452439,"of work on AMR, we focus in this paper on the problem of AMR-to-text generation, i.e. transducing AMR graphs into text that conveys the information in the AMR structure. A key challenge in this task is to efficiently learn useful representations of the AMR graphs. Early efforts (Pourdamghani et al., 2016; Konstas et al., 2017) neglect a significant part of the structural information in the input graph by linearizing it. Recently, Graph Neural Networks (GNNs) have been explored to better encode structural information for this task (Beck et al., 2018; Song et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019). One type of such GNNs is Graph Convolutional Networks (GCNs; Kipf and Welling 2017). GCNs follow a local information aggregation scheme, iteratively updating the representations of nodes based on their immediate (first-order) neighbors. Intuitively, stacking more convolutional layers in GCNs helps capture more complex interactions (Xu et al., 2018; Guo et al., 2019b). However, prior efforts (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020) have shown that the locality property of existing GCNs precludes efficient nonlocal information propagation. Abu-El-Haija et al. 2162 Proceedings o"
2020.emnlp-main.169,P16-1162,0,0.0167726,"p convolutional neural networks. Formally, the Hf inal of LDGCNs which have L layer ˆ L , ..., H ˆ 1 ), where is obtained by: Hf inal = F(H F is a linear transformation. 5 5.1 Experiments Setup We evaluate our model on the LDC2015E86 (AMR1.0), LDC2017T10 (AMR2.0) and LDC2020T02 (AMR3.0) datasets, which have 16,833, 36,521 and 55,635 instances for training, respectively. Both AMR1.0 and AMR2.0 have 1,368 instances for development, and 1,371 instances for testing. AMR3.0 has 1,722 instances for development and 1,898 instances for testing. Following Zhu et al. (2019), we use byte pair encodings (Sennrich et al., 2016) to deal with rare words. Following Guo et al. (2019b), we stack 4 LDGCN blocks as the encoder of our model. Each block consists of two sub-blocks where the bottom one contains 6 layers and the top one contains 3 layers. The hidden dimension of LDGCN model is 480. Other model hyperparameters are set as λ=0.7, K=2 for dynamic fusion mechanism, N =2 for depthwise graph convolutions and M =6 and 3 for layerwise graph convolutions for the bottom and top sub-blocks, respectively. For the decoder, we employ the same attention-based LSTM as in previous work (Beck et al., 2018; Guo et al., 2019b; Damo"
2020.emnlp-main.169,P17-2002,0,0.0176185,"rather than let them get even worse”, but it fails to capture the meaning of word “early” in its output, which is a critical part. DeepGCN parses both “early” and “get even worse” in the results. However, the readability of the generated sentence is not satisfactory. Compared to baselines, LDGCN is able to produce the best result, which has a correct starting phrase and captures the semantic meaning of critical words such as “early” and “get even worse” while also attaining good readability. 6 Early efforts for AMR-to-text generation mainly include grammar-based models (Flanigan et al., 2016; Song et al., 2017) and sequence-based models (Pourdamghani et al., 2016; Konstas et al., 2017; Cao and Clark, 2019), discarding crucial structural information when linearising the input AMR graph. To solve that, various GNNs including graph recurrent networks (Song et al., 2018; Ribeiro et al., 2019) and graph convolutional networks (Damonte and Cohen, 2019; Guo et al., 2019b) have been used to encode the AMR structure. Though GNNs are able to operate directly on graphs, the locality nature of them precludes efficient information propagation (Abu-El-Haija et al., 2018, 2019; Luan et al., 2019). Larger and deepe"
2020.emnlp-main.169,P18-1150,0,0.565052,"dinburgh. † Corresponding author. the realm of work on AMR, we focus in this paper on the problem of AMR-to-text generation, i.e. transducing AMR graphs into text that conveys the information in the AMR structure. A key challenge in this task is to efficiently learn useful representations of the AMR graphs. Early efforts (Pourdamghani et al., 2016; Konstas et al., 2017) neglect a significant part of the structural information in the input graph by linearizing it. Recently, Graph Neural Networks (GNNs) have been explored to better encode structural information for this task (Beck et al., 2018; Song et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019). One type of such GNNs is Graph Convolutional Networks (GCNs; Kipf and Welling 2017). GCNs follow a local information aggregation scheme, iteratively updating the representations of nodes based on their immediate (first-order) neighbors. Intuitively, stacking more convolutional layers in GCNs helps capture more complex interactions (Xu et al., 2018; Guo et al., 2019b). However, prior efforts (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020) have shown that the locality property of existing GCNs precludes efficient nonlocal information pro"
2020.emnlp-main.169,2020.tacl-1.2,0,0.411099,"(GNNs) have been explored to better encode structural information for this task (Beck et al., 2018; Song et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019). One type of such GNNs is Graph Convolutional Networks (GCNs; Kipf and Welling 2017). GCNs follow a local information aggregation scheme, iteratively updating the representations of nodes based on their immediate (first-order) neighbors. Intuitively, stacking more convolutional layers in GCNs helps capture more complex interactions (Xu et al., 2018; Guo et al., 2019b). However, prior efforts (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020) have shown that the locality property of existing GCNs precludes efficient nonlocal information propagation. Abu-El-Haija et al. 2162 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2162–2172, c November 16–20, 2020. 2020 Association for Computational Linguistics (2019) further proved that vanilla GCNs are unable to capture feature differences among neighbors from different orders no matter how many layers are stacked. Therefore, Self-Attention Networks (SANs; Vaswani et al. 2017) have been explored as an alternative to capture global dependencies"
2020.emnlp-main.183,D17-1047,1,0.898258,"ets as well as the associated sentiment. There exist a few tasks derived from ABSA. Target extraction (Chernyshevich, 2014; San Vicente et al., 2015; Yin et al., 2016; Lample et al., 2016; Li et al., 2018b; Ma et al., 2019) is a task that focuses on recognizing all the targets which are either aspect terms or named entities. Such a task is mostly regarded as a sequence labeling problem solvable by CRF-based methods. Aspect sentiment analysis or targeted sentiment analysis is another popular task. Such a task either refers to predicting sentiment polarity for a given target (Dong et al., 2014; Chen et al., 2017; Xue and Li, 2018; Wang and Lu, 2018; Wang et al., 2018; Li et al., 2018a; Peng et al., 2018; Xu et al., 2020) or joint extraction of targets as well as sentiment associated with each target (Mitchell et al., 2013; Zhang et al., 2015; Li and Lu, 2017; Ma et al., 2018; Li and Lu, 2019; Li et al., 2019). The former mostly relies on different neural networks such as self-attention (Liu and Zhang, 2017) or memory networks (Tang et al., 2016) to generate an opinion representation for a given target for further classification. The latter mostly regards the task as a sequence labeling problem by app"
2020.emnlp-main.183,S14-2051,0,0.0261129,"worth highlighting that the ensemble models have significant improvements in terms of recall score. Note that the recall score reflects the number of gold triplets extracted. Such improvement confirms our earlier hypothesis that the two models largely complement each other. 5 Related Work ASTE is highly related to another research topic – Aspect Based Sentiment Analysis (ABSA) (Pontiki et al., 2014, 2016). Such a research topic focuses on identifying aspect categories, recognizing aspect targets as well as the associated sentiment. There exist a few tasks derived from ABSA. Target extraction (Chernyshevich, 2014; San Vicente et al., 2015; Yin et al., 2016; Lample et al., 2016; Li et al., 2018b; Ma et al., 2019) is a task that focuses on recognizing all the targets which are either aspect terms or named entities. Such a task is mostly regarded as a sequence labeling problem solvable by CRF-based methods. Aspect sentiment analysis or targeted sentiment analysis is another popular task. Such a task either refers to predicting sentiment polarity for a given target (Dong et al., 2014; Chen et al., 2017; Xue and Li, 2018; Wang and Lu, 2018; Wang et al., 2018; Li et al., 2018a; Peng et al., 2018; Xu et al.,"
2020.emnlp-main.183,P19-1520,0,0.391762,"Representation (BERT) JETt (M = 6)+ BERT 56.00 63.44 54.12 JETo (M = 6)+ BERT 56.89 70.56 55.94 Table 2: Main results on our refined dataset ASTE-Data-V2. The underlined scores indicate the best results on the dev set, and the highlighted scores are the corresponding test results. The experimental results on the previous released dataset ASTE-Data-V1 can be found in the supplementary materials. 3.2 Baselines the same method to obtain all the valid triplets as RINANTE+. Our JET approaches are compared with the following baselines using pipeline. • RINANTE+ (Peng et al., 2019) modifies RINANTE (Dai and Song, 2019) which is designed based on LSTM-CRF (Lample et al., 2016), to co-extract targets with sentiment, and opinion spans. Such an approach also fuses mined rules as weak supervision to capture dependency relations of words in a sentence at the first stage. At the second stage, it generates all the possible triplets and applies a classifier based on MLP on such triplets to determine if each triplet is valid or not. • CMLA+ (Peng et al., 2019) modifies CMLA (Wang et al., 2017) which leverages attention mechanism to capture dependencies among words, to co-extract targets with sentiment, and opinion sp"
2020.emnlp-main.183,N19-1423,0,0.0322748,"ivated by Li-unified-R to co-extract targets with sentiment, and opinion spans simultaneously. Such an approach also fuses GCN to capture dependency information to facilitate the co-extraction. At the second stage, it uses 3.3 Experimental Setup Following the previous work (Peng et al., 2019), we use pre-trained 300d GloVe (Pennington et al., 2014) to initialize the word embeddings. We use 100 as the embedding size of wr (offset embedding). We use the bi-directional LSTM with the hidden size 300. For experiments with contextualised representation, we adopt the pre-trained language model BERT (Devlin et al., 2019). Specifically, we use bert-as-service (Xiao, 2018) to generate the contextualized word embedding without fine-tuning. We use the representation from the last layer of the uncased version of BERT base model for our experiments. Before training, we discard any instance from the training data that contains triplets with offset larger than M . We train our model for a maximal of 20 epochs using Adam (Kingma and Ba, 2014) as the optimizer with batch size 1 and dropout rate 0.513 . We select the best model parameters based on the best F1 score on the development data and apply it to the test data f"
2020.emnlp-main.183,P18-1087,1,0.847518,"recall score. Note that the recall score reflects the number of gold triplets extracted. Such improvement confirms our earlier hypothesis that the two models largely complement each other. 5 Related Work ASTE is highly related to another research topic – Aspect Based Sentiment Analysis (ABSA) (Pontiki et al., 2014, 2016). Such a research topic focuses on identifying aspect categories, recognizing aspect targets as well as the associated sentiment. There exist a few tasks derived from ABSA. Target extraction (Chernyshevich, 2014; San Vicente et al., 2015; Yin et al., 2016; Lample et al., 2016; Li et al., 2018b; Ma et al., 2019) is a task that focuses on recognizing all the targets which are either aspect terms or named entities. Such a task is mostly regarded as a sequence labeling problem solvable by CRF-based methods. Aspect sentiment analysis or targeted sentiment analysis is another popular task. Such a task either refers to predicting sentiment polarity for a given target (Dong et al., 2014; Chen et al., 2017; Xue and Li, 2018; Wang and Lu, 2018; Wang et al., 2018; Li et al., 2018a; Peng et al., 2018; Xu et al., 2020) or joint extraction of targets as well as sentiment associated with each ta"
2020.emnlp-main.183,E17-2091,0,0.226331,"CRF-based methods. Aspect sentiment analysis or targeted sentiment analysis is another popular task. Such a task either refers to predicting sentiment polarity for a given target (Dong et al., 2014; Chen et al., 2017; Xue and Li, 2018; Wang and Lu, 2018; Wang et al., 2018; Li et al., 2018a; Peng et al., 2018; Xu et al., 2020) or joint extraction of targets as well as sentiment associated with each target (Mitchell et al., 2013; Zhang et al., 2015; Li and Lu, 2017; Ma et al., 2018; Li and Lu, 2019; Li et al., 2019). The former mostly relies on different neural networks such as self-attention (Liu and Zhang, 2017) or memory networks (Tang et al., 2016) to generate an opinion representation for a given target for further classification. The latter mostly regards the task as a sequence labeling problem by applying CRF-based approaches. Another related task – target and opinion span co-extraction (Qiu et al., 2011; Liu et al., 2013, 2014, 2015; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019) is also often regarded as a sequence labeling problem. 6 Conclusion In this work, we propose a novel position-aware tagging scheme by enriching label expressiveness to address a limitation associated with exis"
2020.emnlp-main.183,P14-2009,0,0.0446052,"gnizing aspect targets as well as the associated sentiment. There exist a few tasks derived from ABSA. Target extraction (Chernyshevich, 2014; San Vicente et al., 2015; Yin et al., 2016; Lample et al., 2016; Li et al., 2018b; Ma et al., 2019) is a task that focuses on recognizing all the targets which are either aspect terms or named entities. Such a task is mostly regarded as a sequence labeling problem solvable by CRF-based methods. Aspect sentiment analysis or targeted sentiment analysis is another popular task. Such a task either refers to predicting sentiment polarity for a given target (Dong et al., 2014; Chen et al., 2017; Xue and Li, 2018; Wang and Lu, 2018; Wang et al., 2018; Li et al., 2018a; Peng et al., 2018; Xu et al., 2020) or joint extraction of targets as well as sentiment associated with each target (Mitchell et al., 2013; Zhang et al., 2015; Li and Lu, 2017; Ma et al., 2018; Li and Lu, 2019; Li et al., 2019). The former mostly relies on different neural networks such as self-attention (Liu and Zhang, 2017) or memory networks (Tang et al., 2016) to generate an opinion representation for a given target for further classification. The latter mostly regards the task as a sequence labe"
2020.emnlp-main.183,P13-1172,0,0.0904361,"20) or joint extraction of targets as well as sentiment associated with each target (Mitchell et al., 2013; Zhang et al., 2015; Li and Lu, 2017; Ma et al., 2018; Li and Lu, 2019; Li et al., 2019). The former mostly relies on different neural networks such as self-attention (Liu and Zhang, 2017) or memory networks (Tang et al., 2016) to generate an opinion representation for a given target for further classification. The latter mostly regards the task as a sequence labeling problem by applying CRF-based approaches. Another related task – target and opinion span co-extraction (Qiu et al., 2011; Liu et al., 2013, 2014, 2015; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019) is also often regarded as a sequence labeling problem. 6 Conclusion In this work, we propose a novel position-aware tagging scheme by enriching label expressiveness to address a limitation associated with existing works. Such a tagging scheme is able to specify the connection among three elements – a target, the target sentiment as well as an opinion span in an aspect sentiment triplet for the ASTE task. Based on the position-aware tagging scheme, we propose a novel approach JET that is capable of jointly extracting the aspe"
2020.emnlp-main.183,W06-1643,0,0.0587744,"n span as well as the rich interactions among the three elements due to the limited expressiveness. Specifically, BIOES uses the tag B or S to represent the beginning of a target. For example, in the example sentence in Figure 1, “vegan” should be labeled with B, but the tagging scheme does not contain any information to specify the position of its corresponding opinion “excited”. Using such a tagging scheme inevitably leads to an additional step to connect each target with an opinion span as the second stage in the pipeline approach. The skip-chain sequence models (Sutton and McCallum, 2004; Galley, 2006) are able to capture interactions between given input tokens which can be far away from each other. However, they are not suitable for the ASTE task where the positions of targets and opinion spans are not explicitly provided but need to be learned. Motivated by the above observations, we present a novel approach that is capable of predicting the triplets jointly for ASTE. Specifically, we make the following contributions in this work: • We present a novel position-aware tagging scheme that is capable of specifying the structural information for a triplet – the connection among the three eleme"
2020.emnlp-main.183,P14-1030,0,0.0605334,"Missing"
2020.emnlp-main.183,D15-1168,0,0.129729,"Missing"
2020.emnlp-main.183,D18-1504,0,0.0346965,"targets which are either aspect terms or named entities. Such a task is mostly regarded as a sequence labeling problem solvable by CRF-based methods. Aspect sentiment analysis or targeted sentiment analysis is another popular task. Such a task either refers to predicting sentiment polarity for a given target (Dong et al., 2014; Chen et al., 2017; Xue and Li, 2018; Wang and Lu, 2018; Wang et al., 2018; Li et al., 2018a; Peng et al., 2018; Xu et al., 2020) or joint extraction of targets as well as sentiment associated with each target (Mitchell et al., 2013; Zhang et al., 2015; Li and Lu, 2017; Ma et al., 2018; Li and Lu, 2019; Li et al., 2019). The former mostly relies on different neural networks such as self-attention (Liu and Zhang, 2017) or memory networks (Tang et al., 2016) to generate an opinion representation for a given target for further classification. The latter mostly regards the task as a sequence labeling problem by applying CRF-based approaches. Another related task – target and opinion span co-extraction (Qiu et al., 2011; Liu et al., 2013, 2014, 2015; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019) is also often regarded as a sequence labeling problem. 6 Conclusion In thi"
2020.emnlp-main.183,P19-1344,0,0.451551,"that the recall score reflects the number of gold triplets extracted. Such improvement confirms our earlier hypothesis that the two models largely complement each other. 5 Related Work ASTE is highly related to another research topic – Aspect Based Sentiment Analysis (ABSA) (Pontiki et al., 2014, 2016). Such a research topic focuses on identifying aspect categories, recognizing aspect targets as well as the associated sentiment. There exist a few tasks derived from ABSA. Target extraction (Chernyshevich, 2014; San Vicente et al., 2015; Yin et al., 2016; Lample et al., 2016; Li et al., 2018b; Ma et al., 2019) is a task that focuses on recognizing all the targets which are either aspect terms or named entities. Such a task is mostly regarded as a sequence labeling problem solvable by CRF-based methods. Aspect sentiment analysis or targeted sentiment analysis is another popular task. Such a task either refers to predicting sentiment polarity for a given target (Dong et al., 2014; Chen et al., 2017; Xue and Li, 2018; Wang and Lu, 2018; Wang et al., 2018; Li et al., 2018a; Peng et al., 2018; Xu et al., 2020) or joint extraction of targets as well as sentiment associated with each target (Mitchell et a"
2020.emnlp-main.183,W04-3250,0,0.0820311,"Missing"
2020.emnlp-main.183,D13-1171,0,0.174703,"Missing"
2020.emnlp-main.183,N16-1030,0,0.774842,"f restaurant domain and 14Lap is of laptop domain. Such datasets were all created based on the datasets originally released by SemEval (Pontiki et al., 2014, 2015, 2016). 8 (x,y)∈D See the supplementary materials for detailed algorithm. https://github.com/xuuuluuu/ SemEval-Triplet-data 10 We also report the results on ASTE-Data-V1 in the supplementary material. 11 We also remove triplets with sentiment originally labeled as “conflict” by SemEval. 12 See the supplementary material for more statistics. 9 The overall model is analogous to that of a neural CRF (Peng et al., 2009; Do et al., 2010; Lample et al., 2016); hence the inference and decod7 We use min (j, k) since we care the offset between the starting positions of an opinion span and a target. Data 2343 Models Dev F1 CMLA+ RINANTE+ Li-unified-R Peng et al. (2019) 14Rest P. R. F1 Dev F1 14Lap P. R. F1 Dev F1 15Rest P. R. F1 Dev F1 16Rest P. R. F1 - 39.18 31.42 41.04 43.24 47.13 39.38 67.35 63.66 42.79 34.95 51.00 51.46 - 30.09 21.71 40.56 37.38 36.92 18.66 44.28 50.38 33.16 20.07 42.34 42.87 - 34.56 29.88 44.72 48.07 39.84 30.06 51.39 57.51 37.01 29.97 47.82 52.32 - 41.34 25.68 37.33 46.96 42.10 22.30 54.51 64.24 41.72 23.87 44.31 54.21 JETt (M J"
2020.emnlp-main.183,D19-1550,1,0.830666,"either aspect terms or named entities. Such a task is mostly regarded as a sequence labeling problem solvable by CRF-based methods. Aspect sentiment analysis or targeted sentiment analysis is another popular task. Such a task either refers to predicting sentiment polarity for a given target (Dong et al., 2014; Chen et al., 2017; Xue and Li, 2018; Wang and Lu, 2018; Wang et al., 2018; Li et al., 2018a; Peng et al., 2018; Xu et al., 2020) or joint extraction of targets as well as sentiment associated with each target (Mitchell et al., 2013; Zhang et al., 2015; Li and Lu, 2017; Ma et al., 2018; Li and Lu, 2019; Li et al., 2019). The former mostly relies on different neural networks such as self-attention (Liu and Zhang, 2017) or memory networks (Tang et al., 2016) to generate an opinion representation for a given target for further classification. The latter mostly regards the task as a sequence labeling problem by applying CRF-based approaches. Another related task – target and opinion span co-extraction (Qiu et al., 2011; Liu et al., 2013, 2014, 2015; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019) is also often regarded as a sequence labeling problem. 6 Conclusion In this work, we propos"
2020.emnlp-main.183,D14-1162,0,0.0843256,"Missing"
2020.emnlp-main.183,S16-1002,0,0.255302,"Missing"
2020.emnlp-main.183,S15-2082,0,0.585406,"Missing"
2020.emnlp-main.183,S14-2004,0,0.901694,"e the model effectiveness and robustness1 . 1 Figure 1: ASTE with targets in bold in solid squares, their associated sentiment on top, and opinion spans in dashed boxes. The arc indicates connection between a target and the corresponding opinion span. Introduction Designing effective algorithms that are capable of automatically performing sentiment analysis and opinion mining is a challenging and important task in the field of natural language processing (Pang and Lee, 2008; Liu, 2010; Ortigosa et al., 2014; Smailovi´c et al., 2013; Li and Wu, 2010). Recently, Aspect-based Sentiment Analysis (Pontiki et al., 2014) or Targeted Sentiment Analysis (Mitchell et al., 2013) which focuses on extracting target ∗ Equal contribution. Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. The work was done when Hao Li was a PhD student in Singapore University of Technology and Design. 1 We release our code at https://github.com/ xuuuluuu/Position-Aware-Tagging-for-ASTE phrases as well as the sentiment associated with each target, has been receiving much attention. In this work, we focus on a relatively new task – Aspect Sentiment Triplet Extraction (ASTE) proposed"
2020.emnlp-main.183,J11-1002,0,0.69422,"d above, but also the corresponding opinion spans expressing the sentiment for each target. Such three elements: a target, its sentiment and the corresponding opinion span, form a triplet to be extracted. Figure 1 presents an example sentence containing two targets in solid boxes. Each target is associated with a sentiment, where we use + to denote the positive polarity, 0 for neutral, and − for negative. Two opinion spans in dashed boxes are connected to their targets by arcs. Such opinion spans are important, since they largely explain the sentiment polarities for the corresponding targets (Qiu et al., 2011; Yang and Cardie, 2012). This ASTE problem was basically untouched before, and the only existing work that we are aware of (Peng et al., 2019) employs a 2-stage pipeline approach. At the first stage, they employ a unified tagging scheme which fuses the target tag based on the BIOES 2 tagging scheme, and sentiment tag together. Under such a unified tagging scheme, they proposed methods based on Long Short-Term Memory networks (LSTM) (Hochreiter and Schmidhuber, 1997), Conditional Random 2 BIOES is a common tagging scheme for sequence labeling tasks, and BIOES denotes “begin, inside, outside, e"
2020.emnlp-main.183,S15-2127,0,0.0217349,"he ensemble models have significant improvements in terms of recall score. Note that the recall score reflects the number of gold triplets extracted. Such improvement confirms our earlier hypothesis that the two models largely complement each other. 5 Related Work ASTE is highly related to another research topic – Aspect Based Sentiment Analysis (ABSA) (Pontiki et al., 2014, 2016). Such a research topic focuses on identifying aspect categories, recognizing aspect targets as well as the associated sentiment. There exist a few tasks derived from ABSA. Target extraction (Chernyshevich, 2014; San Vicente et al., 2015; Yin et al., 2016; Lample et al., 2016; Li et al., 2018b; Ma et al., 2019) is a task that focuses on recognizing all the targets which are either aspect terms or named entities. Such a task is mostly regarded as a sequence labeling problem solvable by CRF-based methods. Aspect sentiment analysis or targeted sentiment analysis is another popular task. Such a task either refers to predicting sentiment polarity for a given target (Dong et al., 2014; Chen et al., 2017; Xue and Li, 2018; Wang and Lu, 2018; Wang et al., 2018; Li et al., 2018a; Peng et al., 2018; Xu et al., 2020) or joint extraction"
2020.emnlp-main.183,P16-1218,0,0.0133769,"he offsets. We explain such 3 additional factorized scores appearing in Equation 6. We deploy a simple LSTM-based neural architecture for learning features. Given an input token sequence x = {x1 , x2 , · · · , xn } of length n, we first obtain the embedding sequence {e1 , e2 , · · · , en }. As illustrated in Figure 3, we then apply a bidirectional LSTM on the embedding sequence and obtain the hidden state hi for each position i, which could be represented as: → − ← − hi = [hi ; hi ] (3) → − ← − where hi and hi are the hidden states of the forward and backward LSTMs respectively. Motivated by (Wang and Chang, 2016; Stern et al., 2017), we calculate the segment representation ga,b for an opinion span with boundaries of a and b (both inclusive) as follows: → − → − ← − ← − ga,b = [ h b − h a−1 ; h a − h b+1 ] (4) → − ← − where h 0 = 0, h n+1 = 0 and 1 ≤ a ≤ b ≤ n. 2.2.2 Factorized Feature Score We explain how to compute the factorized feature scores (the second part of Equation 2) for the position-aware tagging scheme based on the neural architecture described above. Such factorized feature scores involve 4 types of scores, as illustrated in the solid boxes appearing in Figure 3 (top). Basically, we calcu"
2020.emnlp-main.183,P18-2094,0,0.273339,"timent associated with each target (Mitchell et al., 2013; Zhang et al., 2015; Li and Lu, 2017; Ma et al., 2018; Li and Lu, 2019; Li et al., 2019). The former mostly relies on different neural networks such as self-attention (Liu and Zhang, 2017) or memory networks (Tang et al., 2016) to generate an opinion representation for a given target for further classification. The latter mostly regards the task as a sequence labeling problem by applying CRF-based approaches. Another related task – target and opinion span co-extraction (Qiu et al., 2011; Liu et al., 2013, 2014, 2015; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019) is also often regarded as a sequence labeling problem. 6 Conclusion In this work, we propose a novel position-aware tagging scheme by enriching label expressiveness to address a limitation associated with existing works. Such a tagging scheme is able to specify the connection among three elements – a target, the target sentiment as well as an opinion span in an aspect sentiment triplet for the ASTE task. Based on the position-aware tagging scheme, we propose a novel approach JET that is capable of jointly extracting the aspect sentiment triplets. We also design factorized"
2020.emnlp-main.183,2020.emnlp-main.288,1,0.842257,"vich, 2014; San Vicente et al., 2015; Yin et al., 2016; Lample et al., 2016; Li et al., 2018b; Ma et al., 2019) is a task that focuses on recognizing all the targets which are either aspect terms or named entities. Such a task is mostly regarded as a sequence labeling problem solvable by CRF-based methods. Aspect sentiment analysis or targeted sentiment analysis is another popular task. Such a task either refers to predicting sentiment polarity for a given target (Dong et al., 2014; Chen et al., 2017; Xue and Li, 2018; Wang and Lu, 2018; Wang et al., 2018; Li et al., 2018a; Peng et al., 2018; Xu et al., 2020) or joint extraction of targets as well as sentiment associated with each target (Mitchell et al., 2013; Zhang et al., 2015; Li and Lu, 2017; Ma et al., 2018; Li and Lu, 2019; Li et al., 2019). The former mostly relies on different neural networks such as self-attention (Liu and Zhang, 2017) or memory networks (Tang et al., 2016) to generate an opinion representation for a given target for further classification. The latter mostly regards the task as a sequence labeling problem by applying CRF-based approaches. Another related task – target and opinion span co-extraction (Qiu et al., 2011; Liu"
2020.emnlp-main.183,P18-1234,0,0.140588,"associated sentiment. There exist a few tasks derived from ABSA. Target extraction (Chernyshevich, 2014; San Vicente et al., 2015; Yin et al., 2016; Lample et al., 2016; Li et al., 2018b; Ma et al., 2019) is a task that focuses on recognizing all the targets which are either aspect terms or named entities. Such a task is mostly regarded as a sequence labeling problem solvable by CRF-based methods. Aspect sentiment analysis or targeted sentiment analysis is another popular task. Such a task either refers to predicting sentiment polarity for a given target (Dong et al., 2014; Chen et al., 2017; Xue and Li, 2018; Wang and Lu, 2018; Wang et al., 2018; Li et al., 2018a; Peng et al., 2018; Xu et al., 2020) or joint extraction of targets as well as sentiment associated with each target (Mitchell et al., 2013; Zhang et al., 2015; Li and Lu, 2017; Ma et al., 2018; Li and Lu, 2019; Li et al., 2019). The former mostly relies on different neural networks such as self-attention (Liu and Zhang, 2017) or memory networks (Tang et al., 2016) to generate an opinion representation for a given target for further classification. The latter mostly regards the task as a sequence labeling problem by applying CRF-based ap"
2020.emnlp-main.183,D12-1122,0,0.0718184,"the corresponding opinion spans expressing the sentiment for each target. Such three elements: a target, its sentiment and the corresponding opinion span, form a triplet to be extracted. Figure 1 presents an example sentence containing two targets in solid boxes. Each target is associated with a sentiment, where we use + to denote the positive polarity, 0 for neutral, and − for negative. Two opinion spans in dashed boxes are connected to their targets by arcs. Such opinion spans are important, since they largely explain the sentiment polarities for the corresponding targets (Qiu et al., 2011; Yang and Cardie, 2012). This ASTE problem was basically untouched before, and the only existing work that we are aware of (Peng et al., 2019) employs a 2-stage pipeline approach. At the first stage, they employ a unified tagging scheme which fuses the target tag based on the BIOES 2 tagging scheme, and sentiment tag together. Under such a unified tagging scheme, they proposed methods based on Long Short-Term Memory networks (LSTM) (Hochreiter and Schmidhuber, 1997), Conditional Random 2 BIOES is a common tagging scheme for sequence labeling tasks, and BIOES denotes “begin, inside, outside, end and single” respectiv"
2020.emnlp-main.183,D15-1073,0,0.0941732,"that focuses on recognizing all the targets which are either aspect terms or named entities. Such a task is mostly regarded as a sequence labeling problem solvable by CRF-based methods. Aspect sentiment analysis or targeted sentiment analysis is another popular task. Such a task either refers to predicting sentiment polarity for a given target (Dong et al., 2014; Chen et al., 2017; Xue and Li, 2018; Wang and Lu, 2018; Wang et al., 2018; Li et al., 2018a; Peng et al., 2018; Xu et al., 2020) or joint extraction of targets as well as sentiment associated with each target (Mitchell et al., 2013; Zhang et al., 2015; Li and Lu, 2017; Ma et al., 2018; Li and Lu, 2019; Li et al., 2019). The former mostly relies on different neural networks such as self-attention (Liu and Zhang, 2017) or memory networks (Tang et al., 2016) to generate an opinion representation for a given target for further classification. The latter mostly regards the task as a sequence labeling problem by applying CRF-based approaches. Another related task – target and opinion span co-extraction (Qiu et al., 2011; Liu et al., 2013, 2014, 2015; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019) is also often regarded as a sequence lab"
2020.emnlp-main.183,P17-1076,0,0.025391,"such 3 additional factorized scores appearing in Equation 6. We deploy a simple LSTM-based neural architecture for learning features. Given an input token sequence x = {x1 , x2 , · · · , xn } of length n, we first obtain the embedding sequence {e1 , e2 , · · · , en }. As illustrated in Figure 3, we then apply a bidirectional LSTM on the embedding sequence and obtain the hidden state hi for each position i, which could be represented as: → − ← − hi = [hi ; hi ] (3) → − ← − where hi and hi are the hidden states of the forward and backward LSTMs respectively. Motivated by (Wang and Chang, 2016; Stern et al., 2017), we calculate the segment representation ga,b for an opinion span with boundaries of a and b (both inclusive) as follows: → − → − ← − ← − ga,b = [ h b − h a−1 ; h a − h b+1 ] (4) → − ← − where h 0 = 0, h n+1 = 0 and 1 ≤ a ≤ b ≤ n. 2.2.2 Factorized Feature Score We explain how to compute the factorized feature scores (the second part of Equation 2) for the position-aware tagging scheme based on the neural architecture described above. Such factorized feature scores involve 4 types of scores, as illustrated in the solid boxes appearing in Figure 3 (top). Basically, we calculate the factorized f"
2020.emnlp-main.183,D16-1021,0,0.0874345,"ysis or targeted sentiment analysis is another popular task. Such a task either refers to predicting sentiment polarity for a given target (Dong et al., 2014; Chen et al., 2017; Xue and Li, 2018; Wang and Lu, 2018; Wang et al., 2018; Li et al., 2018a; Peng et al., 2018; Xu et al., 2020) or joint extraction of targets as well as sentiment associated with each target (Mitchell et al., 2013; Zhang et al., 2015; Li and Lu, 2017; Ma et al., 2018; Li and Lu, 2019; Li et al., 2019). The former mostly relies on different neural networks such as self-attention (Liu and Zhang, 2017) or memory networks (Tang et al., 2016) to generate an opinion representation for a given target for further classification. The latter mostly regards the task as a sequence labeling problem by applying CRF-based approaches. Another related task – target and opinion span co-extraction (Qiu et al., 2011; Liu et al., 2013, 2014, 2015; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019) is also often regarded as a sequence labeling problem. 6 Conclusion In this work, we propose a novel position-aware tagging scheme by enriching label expressiveness to address a limitation associated with existing works. Such a tagging scheme is ab"
2020.emnlp-main.183,P18-1088,0,0.196648,"ew tasks derived from ABSA. Target extraction (Chernyshevich, 2014; San Vicente et al., 2015; Yin et al., 2016; Lample et al., 2016; Li et al., 2018b; Ma et al., 2019) is a task that focuses on recognizing all the targets which are either aspect terms or named entities. Such a task is mostly regarded as a sequence labeling problem solvable by CRF-based methods. Aspect sentiment analysis or targeted sentiment analysis is another popular task. Such a task either refers to predicting sentiment polarity for a given target (Dong et al., 2014; Chen et al., 2017; Xue and Li, 2018; Wang and Lu, 2018; Wang et al., 2018; Li et al., 2018a; Peng et al., 2018; Xu et al., 2020) or joint extraction of targets as well as sentiment associated with each target (Mitchell et al., 2013; Zhang et al., 2015; Li and Lu, 2017; Ma et al., 2018; Li and Lu, 2019; Li et al., 2019). The former mostly relies on different neural networks such as self-attention (Liu and Zhang, 2017) or memory networks (Tang et al., 2016) to generate an opinion representation for a given target for further classification. The latter mostly regards the task as a sequence labeling problem by applying CRF-based approaches. Another related task – targe"
2020.emnlp-main.288,D17-1047,1,0.961554,"ied based on the extracted opinion features and contextual information. The experimental results on four datasets demonstrate the effectiveness of the proposed model, and our further analysis shows that our model can capture aspect-specific opinion spans.1 1 Introduction Aspect Based Sentiment Analysis (ABSA) (Pang and Lee, 2008; Liu, 2012) is an extensively studied sentiment analysis task on a fine-grained semantic level, i.e., opinion targets explicitly mentioned in sentences. Previous ABSA studies focused on a few sub-tasks, such as Aspect Sentiment Classification (ASC) (Wang et al., 2016; Chen et al., 2017; Ma et al., 2018), Aspect Term Extraction (ATE) (Li et al., 2018b; He et al., 2017), Aspect and Opinion Co-Extraction (Liu et al., 2013; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019), E2EABSA (a joint task of ASC and ATE) (Li et al., 2019a; He et al., 2019; Li et al., 2019b), Aspect Sentiment Triplet Extraction (ASTE) (Peng et al., 2019; Xu et al., 2020), etc. ASC analyzes the sentiment polarity of given aspects/targets in a review. ∗ Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. 1 Our code is released at https://github.com/"
2020.emnlp-main.288,W14-4012,0,0.0629142,"Missing"
2020.emnlp-main.288,P19-1520,0,0.0555027,"shows that our model can capture aspect-specific opinion spans.1 1 Introduction Aspect Based Sentiment Analysis (ABSA) (Pang and Lee, 2008; Liu, 2012) is an extensively studied sentiment analysis task on a fine-grained semantic level, i.e., opinion targets explicitly mentioned in sentences. Previous ABSA studies focused on a few sub-tasks, such as Aspect Sentiment Classification (ASC) (Wang et al., 2016; Chen et al., 2017; Ma et al., 2018), Aspect Term Extraction (ATE) (Li et al., 2018b; He et al., 2017), Aspect and Opinion Co-Extraction (Liu et al., 2013; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019), E2EABSA (a joint task of ASC and ATE) (Li et al., 2019a; He et al., 2019; Li et al., 2019b), Aspect Sentiment Triplet Extraction (ASTE) (Peng et al., 2019; Xu et al., 2020), etc. ASC analyzes the sentiment polarity of given aspects/targets in a review. ∗ Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. 1 Our code is released at https://github.com/ xuuuluuu/Aspect-Sentiment-Classification For example, consider the review sentence “Food is usually very good, though occasionally I worry about freshness of raw vegetables in side orders.” Thi"
2020.emnlp-main.288,S14-2076,0,0.186602,"We set the hidden size of GRU to 32 or 64. The batch size is set to 64 or 96. The dropout rate is selected from 0.3 to 0.8, with a step size of 0.1. The dimension of the aspect indicator is selected from {50, 70, 90}. The value of γ in the position decay function is selected from {1,2,3}. The number of layer of GRU is selected from {1,2,3}. We adopt Adam (Kingma and Ba, 2014) to optimize our model with a learning rate of 0.008. All hyper-parameters are selected based on the best performance on the development set. 3.2 Baselines Our MCRF-SA model is compared with the following methods2 . SVM (Kiritchenko et al., 2014) is a support vector machine based method that integrates surface, lexicon, and parse features. ATAELSTM (Wang et al., 2016) is an LSTM (Hochreiter and Schmidhuber, 1997) based model, which has an extra attention to perform soft-selection over the context words. MemNet (Tang et al., 2016) introduces a deep memory network to implement attention mechanisms to learn the relatedness of context words towards the aspect. IAN (Ma et al., 2017) utilizes two LSTM based attention models to learn both context and aspect representations interactively. SA-LSTM-P (Wang and Lu, 2018) employs structured atten"
2020.emnlp-main.288,N16-1030,0,0.0546551,"pect, L is the maximum length of sentences across all datasets, γ is a hyper-parameter and a larger value enables more influence from the context words that are close to the aspect. Then, the decayed contextual word representation is as follows: rt = f (t) ht (4) 2.4 Multi-CRF Structured Attention We use multiple linear-chain CRFs to intensively incorporate structure dependencies to capture the corresponding opinion spans of an aspect. In particular, we create a latent label (Wang and Lu, 2018) z ∈ {Y es, N o} to indicate whether each context word belongs to part of opinion spans. Similar to (Lample et al., 2016), given the sentence representation x, the CRF is defined as: 3562 P (z|x) = P exp(score(z, x)) 0 z0 exp(score(z , x)) (5) where score(z, x) is a score function that is defined as the summation of transition scores and emission scores from the Bi-GRU: n n X X score(z, x) = Tzt ,zt+1 + Et,zt (6) t=0 Marginal Inference The latent labels introduced in the CRF layer show whether the word influences the given aspect’s sentiment. Intuitively, we can understand that the marginal probabilities on the Y es label indicate the influence of the current context word on the aspect word’s sentiment. By using"
2020.emnlp-main.288,D19-1550,1,0.70703,"a and Singapore University of Technology and Design. 1 Our code is released at https://github.com/ xuuuluuu/Aspect-Sentiment-Classification For example, consider the review sentence “Food is usually very good, though occasionally I worry about freshness of raw vegetables in side orders.” This review mentions two aspects: Food and raw vegetables, and for ASC, the objective is to give a positive sentiment on Food and a negative sentiment on raw vegetables. Most of the previous works (Wang et al., 2016; Chen et al., 2017; Liu and Zhang, 2017; Yang et al., 2017; Li et al., 2018c; He et al., 2018; Li and Lu, 2019; Hu et al., 2019) adopt attention mechanism (Bahdanau et al., 2015) to capture the semantic relatedness among the context words and the aspect, and learn aspect-specific features for sentiment classification. However, it is challenging for attention-based approaches to consider an opinion span as a whole during feature extraction because they are overreliant on neural models to learn the contextstructural information and perform feature extraction over individual hidden representations. Previous work (Wang and Lu, 2018) engage structured attention networks (Kim et al., 2017), which extend the"
2020.emnlp-main.288,P18-1087,1,0.827072,"ion. The experimental results on four datasets demonstrate the effectiveness of the proposed model, and our further analysis shows that our model can capture aspect-specific opinion spans.1 1 Introduction Aspect Based Sentiment Analysis (ABSA) (Pang and Lee, 2008; Liu, 2012) is an extensively studied sentiment analysis task on a fine-grained semantic level, i.e., opinion targets explicitly mentioned in sentences. Previous ABSA studies focused on a few sub-tasks, such as Aspect Sentiment Classification (ASC) (Wang et al., 2016; Chen et al., 2017; Ma et al., 2018), Aspect Term Extraction (ATE) (Li et al., 2018b; He et al., 2017), Aspect and Opinion Co-Extraction (Liu et al., 2013; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019), E2EABSA (a joint task of ASC and ATE) (Li et al., 2019a; He et al., 2019; Li et al., 2019b), Aspect Sentiment Triplet Extraction (ASTE) (Peng et al., 2019; Xu et al., 2020), etc. ASC analyzes the sentiment polarity of given aspects/targets in a review. ∗ Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. 1 Our code is released at https://github.com/ xuuuluuu/Aspect-Sentiment-Classification For example, consider th"
2020.emnlp-main.288,P17-1036,0,0.0264457,"tal results on four datasets demonstrate the effectiveness of the proposed model, and our further analysis shows that our model can capture aspect-specific opinion spans.1 1 Introduction Aspect Based Sentiment Analysis (ABSA) (Pang and Lee, 2008; Liu, 2012) is an extensively studied sentiment analysis task on a fine-grained semantic level, i.e., opinion targets explicitly mentioned in sentences. Previous ABSA studies focused on a few sub-tasks, such as Aspect Sentiment Classification (ASC) (Wang et al., 2016; Chen et al., 2017; Ma et al., 2018), Aspect Term Extraction (ATE) (Li et al., 2018b; He et al., 2017), Aspect and Opinion Co-Extraction (Liu et al., 2013; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019), E2EABSA (a joint task of ASC and ATE) (Li et al., 2019a; He et al., 2019; Li et al., 2019b), Aspect Sentiment Triplet Extraction (ASTE) (Peng et al., 2019; Xu et al., 2020), etc. ASC analyzes the sentiment polarity of given aspects/targets in a review. ∗ Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. 1 Our code is released at https://github.com/ xuuuluuu/Aspect-Sentiment-Classification For example, consider the review sentence “"
2020.emnlp-main.288,C18-1096,0,0.0811402,"am between Alibaba and Singapore University of Technology and Design. 1 Our code is released at https://github.com/ xuuuluuu/Aspect-Sentiment-Classification For example, consider the review sentence “Food is usually very good, though occasionally I worry about freshness of raw vegetables in side orders.” This review mentions two aspects: Food and raw vegetables, and for ASC, the objective is to give a positive sentiment on Food and a negative sentiment on raw vegetables. Most of the previous works (Wang et al., 2016; Chen et al., 2017; Liu and Zhang, 2017; Yang et al., 2017; Li et al., 2018c; He et al., 2018; Li and Lu, 2019; Hu et al., 2019) adopt attention mechanism (Bahdanau et al., 2015) to capture the semantic relatedness among the context words and the aspect, and learn aspect-specific features for sentiment classification. However, it is challenging for attention-based approaches to consider an opinion span as a whole during feature extraction because they are overreliant on neural models to learn the contextstructural information and perform feature extraction over individual hidden representations. Previous work (Wang and Lu, 2018) engage structured attention networks (Kim et al., 2017),"
2020.emnlp-main.288,D19-1466,1,0.829529,"ans.1 1 Introduction Aspect Based Sentiment Analysis (ABSA) (Pang and Lee, 2008; Liu, 2012) is an extensively studied sentiment analysis task on a fine-grained semantic level, i.e., opinion targets explicitly mentioned in sentences. Previous ABSA studies focused on a few sub-tasks, such as Aspect Sentiment Classification (ASC) (Wang et al., 2016; Chen et al., 2017; Ma et al., 2018), Aspect Term Extraction (ATE) (Li et al., 2018b; He et al., 2017), Aspect and Opinion Co-Extraction (Liu et al., 2013; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019), E2EABSA (a joint task of ASC and ATE) (Li et al., 2019a; He et al., 2019; Li et al., 2019b), Aspect Sentiment Triplet Extraction (ASTE) (Peng et al., 2019; Xu et al., 2020), etc. ASC analyzes the sentiment polarity of given aspects/targets in a review. ∗ Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. 1 Our code is released at https://github.com/ xuuuluuu/Aspect-Sentiment-Classification For example, consider the review sentence “Food is usually very good, though occasionally I worry about freshness of raw vegetables in side orders.” This review mentions two aspects: Food and raw vegetables,"
2020.emnlp-main.288,P19-1048,0,0.0158908,"on Aspect Based Sentiment Analysis (ABSA) (Pang and Lee, 2008; Liu, 2012) is an extensively studied sentiment analysis task on a fine-grained semantic level, i.e., opinion targets explicitly mentioned in sentences. Previous ABSA studies focused on a few sub-tasks, such as Aspect Sentiment Classification (ASC) (Wang et al., 2016; Chen et al., 2017; Ma et al., 2018), Aspect Term Extraction (ATE) (Li et al., 2018b; He et al., 2017), Aspect and Opinion Co-Extraction (Liu et al., 2013; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019), E2EABSA (a joint task of ASC and ATE) (Li et al., 2019a; He et al., 2019; Li et al., 2019b), Aspect Sentiment Triplet Extraction (ASTE) (Peng et al., 2019; Xu et al., 2020), etc. ASC analyzes the sentiment polarity of given aspects/targets in a review. ∗ Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. 1 Our code is released at https://github.com/ xuuuluuu/Aspect-Sentiment-Classification For example, consider the review sentence “Food is usually very good, though occasionally I worry about freshness of raw vegetables in side orders.” This review mentions two aspects: Food and raw vegetables, and for ASC, the o"
2020.emnlp-main.288,K19-1091,0,0.0151803,"niversity of Technology and Design. 1 Our code is released at https://github.com/ xuuuluuu/Aspect-Sentiment-Classification For example, consider the review sentence “Food is usually very good, though occasionally I worry about freshness of raw vegetables in side orders.” This review mentions two aspects: Food and raw vegetables, and for ASC, the objective is to give a positive sentiment on Food and a negative sentiment on raw vegetables. Most of the previous works (Wang et al., 2016; Chen et al., 2017; Liu and Zhang, 2017; Yang et al., 2017; Li et al., 2018c; He et al., 2018; Li and Lu, 2019; Hu et al., 2019) adopt attention mechanism (Bahdanau et al., 2015) to capture the semantic relatedness among the context words and the aspect, and learn aspect-specific features for sentiment classification. However, it is challenging for attention-based approaches to consider an opinion span as a whole during feature extraction because they are overreliant on neural models to learn the contextstructural information and perform feature extraction over individual hidden representations. Previous work (Wang and Lu, 2018) engage structured attention networks (Kim et al., 2017), which extend the previous attentio"
2020.emnlp-main.288,E17-2091,0,0.0842539,"/targets in a review. ∗ Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. 1 Our code is released at https://github.com/ xuuuluuu/Aspect-Sentiment-Classification For example, consider the review sentence “Food is usually very good, though occasionally I worry about freshness of raw vegetables in side orders.” This review mentions two aspects: Food and raw vegetables, and for ASC, the objective is to give a positive sentiment on Food and a negative sentiment on raw vegetables. Most of the previous works (Wang et al., 2016; Chen et al., 2017; Liu and Zhang, 2017; Yang et al., 2017; Li et al., 2018c; He et al., 2018; Li and Lu, 2019; Hu et al., 2019) adopt attention mechanism (Bahdanau et al., 2015) to capture the semantic relatedness among the context words and the aspect, and learn aspect-specific features for sentiment classification. However, it is challenging for attention-based approaches to consider an opinion span as a whole during feature extraction because they are overreliant on neural models to learn the contextstructural information and perform feature extraction over individual hidden representations. Previous work (Wang and Lu, 2018) en"
2020.emnlp-main.288,P13-1172,0,0.159276,"veness of the proposed model, and our further analysis shows that our model can capture aspect-specific opinion spans.1 1 Introduction Aspect Based Sentiment Analysis (ABSA) (Pang and Lee, 2008; Liu, 2012) is an extensively studied sentiment analysis task on a fine-grained semantic level, i.e., opinion targets explicitly mentioned in sentences. Previous ABSA studies focused on a few sub-tasks, such as Aspect Sentiment Classification (ASC) (Wang et al., 2016; Chen et al., 2017; Ma et al., 2018), Aspect Term Extraction (ATE) (Li et al., 2018b; He et al., 2017), Aspect and Opinion Co-Extraction (Liu et al., 2013; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019), E2EABSA (a joint task of ASC and ATE) (Li et al., 2019a; He et al., 2019; Li et al., 2019b), Aspect Sentiment Triplet Extraction (ASTE) (Peng et al., 2019; Xu et al., 2020), etc. ASC analyzes the sentiment polarity of given aspects/targets in a review. ∗ Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. 1 Our code is released at https://github.com/ xuuuluuu/Aspect-Sentiment-Classification For example, consider the review sentence “Food is usually very good, though occasionally I wor"
2020.emnlp-main.288,P18-2094,0,0.0301115,"further analysis shows that our model can capture aspect-specific opinion spans.1 1 Introduction Aspect Based Sentiment Analysis (ABSA) (Pang and Lee, 2008; Liu, 2012) is an extensively studied sentiment analysis task on a fine-grained semantic level, i.e., opinion targets explicitly mentioned in sentences. Previous ABSA studies focused on a few sub-tasks, such as Aspect Sentiment Classification (ASC) (Wang et al., 2016; Chen et al., 2017; Ma et al., 2018), Aspect Term Extraction (ATE) (Li et al., 2018b; He et al., 2017), Aspect and Opinion Co-Extraction (Liu et al., 2013; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019), E2EABSA (a joint task of ASC and ATE) (Li et al., 2019a; He et al., 2019; Li et al., 2019b), Aspect Sentiment Triplet Extraction (ASTE) (Peng et al., 2019; Xu et al., 2020), etc. ASC analyzes the sentiment polarity of given aspects/targets in a review. ∗ Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. 1 Our code is released at https://github.com/ xuuuluuu/Aspect-Sentiment-Classification For example, consider the review sentence “Food is usually very good, though occasionally I worry about freshness of raw vegetables"
2020.emnlp-main.288,2020.emnlp-main.183,1,0.842257,"sentiment analysis task on a fine-grained semantic level, i.e., opinion targets explicitly mentioned in sentences. Previous ABSA studies focused on a few sub-tasks, such as Aspect Sentiment Classification (ASC) (Wang et al., 2016; Chen et al., 2017; Ma et al., 2018), Aspect Term Extraction (ATE) (Li et al., 2018b; He et al., 2017), Aspect and Opinion Co-Extraction (Liu et al., 2013; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019), E2EABSA (a joint task of ASC and ATE) (Li et al., 2019a; He et al., 2019; Li et al., 2019b), Aspect Sentiment Triplet Extraction (ASTE) (Peng et al., 2019; Xu et al., 2020), etc. ASC analyzes the sentiment polarity of given aspects/targets in a review. ∗ Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. 1 Our code is released at https://github.com/ xuuuluuu/Aspect-Sentiment-Classification For example, consider the review sentence “Food is usually very good, though occasionally I worry about freshness of raw vegetables in side orders.” This review mentions two aspects: Food and raw vegetables, and for ASC, the objective is to give a positive sentiment on Food and a negative sentiment on raw vegetables. Most of"
2020.emnlp-main.288,D19-1464,0,0.313505,"lized. The input representation xt is as follows: xt = [wtword ; wtas ] (1) 2.2 Aspect-Specific Contextualized Representation We employ a bi-directional GRU (Cho et al., 2014) to generate the contextualized representation. Since the input representation has already contained the aspect information, the aspect-specific contextualized representation is obtained by concatenating the hidden states from both directions: → − ← − ht = [ht ; ht ] (2) → − where ht is the hidden state from the forward GRU ← − and ht is from the backward. 2.3 Position Decay Following the previous work (Li et al., 2018a; Zhang et al., 2019; Tang et al., 2019), we also use a position decay function to reduce the influence of Figure 1: MCRF-SA Architecture. the context words on the aspect as it goes further away from the aspect. We propose a higher-order decay function, which is more sensitive to distance, and the sensitivity can be tuned by γ on different datasets.  L−i+t γ  t&lt;i ( L ) f (t) = 1 (3) i≤t≤j   L−t+j γ ( L ) j&lt;t where i and j are the starting and ending position of an aspect, L is the maximum length of sentences across all datasets, γ is a hyper-parameter and a larger value enables more influence from the contex"
2020.emnlp-main.288,D14-1162,0,0.0869837,"r, which takes rt as input and returns a vector whose length is label size. 2.4.1 Dataset Experiments Experimental Setup Our proposed MCRF-SA model is evaluated on four benchmark datasets: SemEval 2014 Task4 (Pontiki et al., 2014), SemEval 2015 Task12 (Pontiki et al., 2015) and SemEval 2016 Task 5 (Pontiki et al., 2016). Following the previous works (Tang et al., 2016; Chen et al., 2017; Wang and Lu, 2018; Table 1: Statistics of datasets. He et al., 2018), we remove a few examples that have conflicting labels. Detailed statistics of the datasets can be found in Table 1. We use the 300d GloVe (Pennington et al., 2014) to initialize our word embeddings. One-sixth of instances are randomly selected from the original training dataset as the development dataset, and the model is only trained with the remaining data. With the development set, we tune our model hyperparameters using an open-source black-box tuner (Alberto and Giacomo, 2018). We set the hidden size of GRU to 32 or 64. The batch size is set to 64 or 96. The dropout rate is selected from 0.3 to 0.8, with a step size of 0.1. The dimension of the aspect indicator is selected from {50, 70, 90}. The value of γ in the position decay function is selected"
2020.emnlp-main.288,S15-2082,0,0.314936,"Missing"
2020.emnlp-main.288,S14-2004,0,0.101049,". #Neg. #Pos. #Neu. #Neg. #Pos. #Neu. #Neg. 1796 539 666 368 94 139 728 196 196 824 383 717 161 72 149 340 167 128 808 29 228 147 5 44 340 28 195 1106 54 406 191 9 60 474 29 127 t=1 where T is a transition matrix and Tzt ,zt+1 denotes the transition score from label zt to zt+1 . Et,zt denotes the emission score of label zt at the t-th position, and the score is obtained from a linear layer, which takes rt as input and returns a vector whose length is label size. 2.4.1 Dataset Experiments Experimental Setup Our proposed MCRF-SA model is evaluated on four benchmark datasets: SemEval 2014 Task4 (Pontiki et al., 2014), SemEval 2015 Task12 (Pontiki et al., 2015) and SemEval 2016 Task 5 (Pontiki et al., 2016). Following the previous works (Tang et al., 2016; Chen et al., 2017; Wang and Lu, 2018; Table 1: Statistics of datasets. He et al., 2018), we remove a few examples that have conflicting labels. Detailed statistics of the datasets can be found in Table 1. We use the 300d GloVe (Pennington et al., 2014) to initialize our word embeddings. One-sixth of instances are randomly selected from the original training dataset as the development dataset, and the model is only trained with the remaining data. With th"
2020.emnlp-main.288,D16-1021,0,0.584365,"5 1106 54 406 191 9 60 474 29 127 t=1 where T is a transition matrix and Tzt ,zt+1 denotes the transition score from label zt to zt+1 . Et,zt denotes the emission score of label zt at the t-th position, and the score is obtained from a linear layer, which takes rt as input and returns a vector whose length is label size. 2.4.1 Dataset Experiments Experimental Setup Our proposed MCRF-SA model is evaluated on four benchmark datasets: SemEval 2014 Task4 (Pontiki et al., 2014), SemEval 2015 Task12 (Pontiki et al., 2015) and SemEval 2016 Task 5 (Pontiki et al., 2016). Following the previous works (Tang et al., 2016; Chen et al., 2017; Wang and Lu, 2018; Table 1: Statistics of datasets. He et al., 2018), we remove a few examples that have conflicting labels. Detailed statistics of the datasets can be found in Table 1. We use the 300d GloVe (Pennington et al., 2014) to initialize our word embeddings. One-sixth of instances are randomly selected from the original training dataset as the development dataset, and the model is only trained with the remaining data. With the development set, we tune our model hyperparameters using an open-source black-box tuner (Alberto and Giacomo, 2018). We set the hidden siz"
2020.emnlp-main.288,P19-1053,0,0.124683,"Missing"
2020.emnlp-main.288,D16-1058,0,0.484669,"get is then classified based on the extracted opinion features and contextual information. The experimental results on four datasets demonstrate the effectiveness of the proposed model, and our further analysis shows that our model can capture aspect-specific opinion spans.1 1 Introduction Aspect Based Sentiment Analysis (ABSA) (Pang and Lee, 2008; Liu, 2012) is an extensively studied sentiment analysis task on a fine-grained semantic level, i.e., opinion targets explicitly mentioned in sentences. Previous ABSA studies focused on a few sub-tasks, such as Aspect Sentiment Classification (ASC) (Wang et al., 2016; Chen et al., 2017; Ma et al., 2018), Aspect Term Extraction (ATE) (Li et al., 2018b; He et al., 2017), Aspect and Opinion Co-Extraction (Liu et al., 2013; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019), E2EABSA (a joint task of ASC and ATE) (Li et al., 2019a; He et al., 2019; Li et al., 2019b), Aspect Sentiment Triplet Extraction (ASTE) (Peng et al., 2019; Xu et al., 2020), etc. ASC analyzes the sentiment polarity of given aspects/targets in a review. ∗ Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. 1 Our code is released at h"
2020.emnlp-main.488,N19-4010,0,0.02402,"Missing"
2020.emnlp-main.488,C18-1139,0,0.0275194,"Missing"
2020.emnlp-main.488,P18-1246,0,0.0265305,"Missing"
2020.emnlp-main.488,Q17-1010,0,0.0180151,"4) with initial learning rate 1e-3 and batch size 32. Learning rate is decayed by 0.5 if the performance on dev set does not improve in 3 consecutive epochs. We stop training when the learning rate drops below 1e-5 or number of epochs reaches 100. We use the pre3 Condition tag c is also in w<t , since it is a special token added to the beginning of each sentence. We write it explicitly to emphasize the conditional effect. 4 The baseline model provided in the original paper is used for evaluating end to end target based sentiment analysis task. trained 300-dimensional fastText word embeddings (Bojanowski et al., 2017) for all languages. We employ relatively simple basic models because: 1) They help to avoid the possible overfitting problems due to the small data size under the low resource setting; 2) They allow more faithful understanding on the effects of the proposed data augmentation method. 4.2 Supervised Experiments To verify the effectiveness of our data augmentation method in the supervised settings, we evaluate it on three different tagging tasks, including NER, POS and E2E-TBSA. Most of the prior works rely on additional information, so we use random deletion (rd) (Wei and Zou, 2019) as our basel"
2020.emnlp-main.488,D17-1047,1,0.833037,"Missing"
2020.emnlp-main.488,D17-1091,0,0.0158196,"t is more challenging to apply data augmentation techniques to natural language processing (NLP). Unlike computer vision and speech, where handcrafted rules (such as rotation, cropping, masking, etc.) can be easily applied to transform original data, it is difficult to generalize such rules for languages. Although simple distortion usually does not change the semantics of visual information, deleting or replacing a single word could completely change the meaning of the sentence. One successful method for data augmentation in NLP is back translation (Sennrich et al., 2016; Fadaee et al., 2017; Dong et al., 2017; Yu et al., 2018), where a translation model is used to translate monolingual sentences from target language to source language to generate synthetic parallel sentences. Other successful methods include: systematically reordering the dependents of some nodes in gold data to generate synthetic data for dependency parsing (Wang and Eisner, 2016), leveraging knowledge base for question generation (Serban et al., 2016) and using simulation-based approach to generate a set of prerequisite toy tasks for QA (Weston et al., 2015). Besides, synonym replace6045 Proceedings of the 2020 Conference on Emp"
2020.emnlp-main.488,P17-2090,0,0.0976786,"lexity of language, it is more challenging to apply data augmentation techniques to natural language processing (NLP). Unlike computer vision and speech, where handcrafted rules (such as rotation, cropping, masking, etc.) can be easily applied to transform original data, it is difficult to generalize such rules for languages. Although simple distortion usually does not change the semantics of visual information, deleting or replacing a single word could completely change the meaning of the sentence. One successful method for data augmentation in NLP is back translation (Sennrich et al., 2016; Fadaee et al., 2017; Dong et al., 2017; Yu et al., 2018), where a translation model is used to translate monolingual sentences from target language to source language to generate synthetic parallel sentences. Other successful methods include: systematically reordering the dependents of some nodes in gold data to generate synthetic data for dependency parsing (Wang and Eisner, 2016), leveraging knowledge base for question generation (Serban et al., 2016) and using simulation-based approach to generate a set of prerequisite toy tasks for QA (Weston et al., 2015). Besides, synonym replace6045 Proceedings of the 202"
2020.emnlp-main.488,P19-1553,1,0.679347,"wt |w<t ) in Eq. 2 becomes pθ (wt |w<t , c).3 A similar approach is used in CTRL (Keskar et al., 2019) to control style, task-specific behavior, etc., during text generation. 4 Experiments In this section, we present our experiments in both supervised and semi-supervised settings. In the supervised settings, only gold data are used for augmentation. In the semi-supervised settings, we also leverage unlabeled data and knowledge bases. 4.1 Basic Models Language Model We use the language model described in Section 3.2 for synthetic data generation. We modified the decoder of the LSTM-LM model in Kruengkrai (2019) to implement this language model. We set LSTM hidden state size to 512 and embedding size to 300. We use dropout rate 0.5 for the two dropout layers. All language model are trained using Stochastic gradient descent (SGD) with initial learning rate 1 and batch size 32. Learning rate will be decayed by 0.5 in the next epoch if the perplexity on dev set does not improve. We set the maximum number of epochs to 30 and stop training early if the perplexity on dev set does not improve in 3 consecutive epochs. During synthetic data generation, we use the average length of gold sentences in the traini"
2020.emnlp-main.488,2020.lifelongnlp-1.3,0,0.13528,"2016), leveraging knowledge base for question generation (Serban et al., 2016) and using simulation-based approach to generate a set of prerequisite toy tasks for QA (Weston et al., 2015). Besides, synonym replace6045 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6045–6057, c November 16–20, 2020. 2020 Association for Computational Linguistics ment, random deletion/swap/insertion, generation with VAE or pre-trained language models are also used in some NLP tasks (Kobayashi, 2018; Wei and Zou, 2019; Anaby-Tavor et al., 2020; Raille et al., 2020; Kumar et al., 2020), but mainly for translation and classification tasks. Compared with the above-mentioned downstream tasks like translation and classification, sequence tagging is more fragile when it is confronted with data augmentation noises due to the finer granularity of the (token-level) task. Annotating unlabeled data with a weak tagger, leveraging aligned bilingual corpora to induce annotation and synonym replacement are three attempted data augmentation methods for sequence tagging (Shang et al., 2018; Yarowsky et al., 2001; Mathew et al., 2019). Weakly labeled data will inevitably introduce more nois"
2020.emnlp-main.488,N16-1030,0,0.308802,"ings, our method demonstrates strong ability to exploit useful information from unlabeled data and knowledge base. 2 Background Named Entity Recognition (NER) Named entities refer to phrases that are names of persons, organizations and locations, etc. in text. For example, “[ORG U.N.] official [PER Ekeus] heads for [LOC Baghdad] ”. Named entity recognition is an important task of information extraction and it aims to locate and classify named entities in text into the predefined types (Mikheev et al., 1999; Sang and De Meulder, 2003; Li et al., 2020). It is a challenging task for two reasons (Lample et al., 2016): 1) in most languages and domains, the amount of manually labeled training data for NER is limited; 2) it is difficult to generalize from this small sample of training data due to the constraints on the kinds of words that can be names. Part-of-Speech (POS) Tagging Part-of-speech tagging consists of assigning a tag that represents a grammatical class to each word in a given sentence. It is a critical component of most NLP systems and is fundamental to facilitate downstream tasks such as syntactic parsing (Sch¨utze, 1993) and opinion analysis (Liu et al., 2015). The current state-ofthe-art POS"
2020.emnlp-main.488,D19-5505,1,0.898289,"sunaga et al., 2018). Target Based Sentiment Analysis The target based sentiment analysis is a fundamental task of sentiment analysis and it aims to detect the opinion targets in sentences and predict the sentiment polarities over the targets (Liu et al., 2015; Chen et al., 2017; Li et al., 2018, 2019a). For example, “USB3 Peripherals are noticeably less expensive than the ThunderBolt ones”. In this sentence, two opinion targets were mentioned, namely “USB3 Peripherals” and “ThunderBolt ones” and the user expresses a positive sentiment over the first, and a negative sentiment over the second. Li et al. (2019a,b) propose an end-to-end solution (E2ETBSA) of TBSA, which converts TBSA to a tagging task, and aims to solve the two subtasks (i.e. target detection and sentiment classification) in a unified manning by predicting unified tags. For example, the tag “B-POS” indicates the beginning of a target with positive sentiment. So after annotation, the above example becomes “[B-POS USB3] [E-POS Peripherals] are noticeably less expensive than the [B-NEG ThunderBolt] [E-NEG ones]”. 3 Proposed Method We propose a novel data augmentation method for sequence tagging tasks. We first linearize labeled sentenc"
2020.emnlp-main.488,D15-1168,1,0.889428,"Missing"
2020.emnlp-main.488,P14-5010,0,0.00283339,"gold data). Our method. Generate synthetic data with LM, where LM is trained on gold data and unlabeled data. Baseline method. Annotate unlabeled data with knowledge base. Our method. Generate synthetic data with LM, where LM is trained on gold data and knowledge base annotated data. Table 5: Data sources for the semi-supervised setting. 4.3.1 Only Using Unlabeled Data Dataset We use CoNLL2003 English NER data (Tjong Kim Sang and De Meulder, 2003) for evaluation. In addition to the gold NER training data, we utilize unlabeled data for semi-supervised training. The Stanford CoreNLP tokenizer (Manning et al., 2014) is used to tokenize Wikipedia sentences. Experimental Settings Similar to the above experiments, we use 1k, 2k, 4k, 6k and 8k sentences randomly sampled from NER gold data as well as the full dataset to evaluate our method. For fair comparison, we only use the same set of 10k sentences randomly sampled from Wikipedia dump in both of our and baseline methods. Let Dgold and Dunlabeled be the sampled gold NER data and the Wikipedia data, respectively. In our method, Dgold and Dunlabeled are concatenated to train language models, following the steps 6051 Method 1k 2k 4k 6k 8k all gold 58.06 67.85"
2020.emnlp-main.488,E99-1001,0,0.544131,"Missing"
2020.emnlp-main.488,P16-2067,0,0.0505893,"Missing"
2020.emnlp-main.488,S15-2082,0,0.0602815,"Missing"
2020.emnlp-main.488,S14-2004,0,0.149978,"Missing"
2020.emnlp-main.488,W03-0419,0,0.698327,"Missing"
2020.emnlp-main.488,P93-1034,0,0.295575,"Missing"
2020.emnlp-main.488,P16-1009,0,0.0477906,"owever, due to the complexity of language, it is more challenging to apply data augmentation techniques to natural language processing (NLP). Unlike computer vision and speech, where handcrafted rules (such as rotation, cropping, masking, etc.) can be easily applied to transform original data, it is difficult to generalize such rules for languages. Although simple distortion usually does not change the semantics of visual information, deleting or replacing a single word could completely change the meaning of the sentence. One successful method for data augmentation in NLP is back translation (Sennrich et al., 2016; Fadaee et al., 2017; Dong et al., 2017; Yu et al., 2018), where a translation model is used to translate monolingual sentences from target language to source language to generate synthetic parallel sentences. Other successful methods include: systematically reordering the dependents of some nodes in gold data to generate synthetic data for dependency parsing (Wang and Eisner, 2016), leveraging knowledge base for question generation (Serban et al., 2016) and using simulation-based approach to generate a set of prerequisite toy tasks for QA (Weston et al., 2015). Besides, synonym replace6045 P"
2020.emnlp-main.488,P16-1056,0,0.021012,"a single word could completely change the meaning of the sentence. One successful method for data augmentation in NLP is back translation (Sennrich et al., 2016; Fadaee et al., 2017; Dong et al., 2017; Yu et al., 2018), where a translation model is used to translate monolingual sentences from target language to source language to generate synthetic parallel sentences. Other successful methods include: systematically reordering the dependents of some nodes in gold data to generate synthetic data for dependency parsing (Wang and Eisner, 2016), leveraging knowledge base for question generation (Serban et al., 2016) and using simulation-based approach to generate a set of prerequisite toy tasks for QA (Weston et al., 2015). Besides, synonym replace6045 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6045–6057, c November 16–20, 2020. 2020 Association for Computational Linguistics ment, random deletion/swap/insertion, generation with VAE or pre-trained language models are also used in some NLP tasks (Kobayashi, 2018; Wei and Zou, 2019; Anaby-Tavor et al., 2020; Raille et al., 2020; Kumar et al., 2020), but mainly for translation and classification tasks. Compa"
2020.emnlp-main.488,W02-2024,0,0.554171,"ted. See Table 1 for the notations of the methods used in our supervised experiments. Method Description gold gen rd rd* Only use the gold data. Our method. Generate synthetic data with the language models, and oversample gold data. Baseline method. Generate synthetic data by random deletion, and oversample gold data with the same ratio as gen. Baseline method. Similar to rd, except that gold and synthetic data are equally sampled. Table 1: Data sources for the supervised setting. 4.2.1 Named Entity Recognition Dataset We evaluate our proposed methods on the CoNLL2002/2003 NER data (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003), with four languages: English, German, Dutch and Spanish. Besides, we evaluate our methods on Thai and Vietnamese NER data, which are product titles obtained from major e-commerce websites in Southeast Asian countries and annotated with 11 product attribute NER tags, including PRODUCT, BRAND, CONSUMER GROUP, MATERIAL, PATTERN, COLOR, FABRIC, OCCASION, ORIGIN, SEASON and STYLE. See Appendix for the statistics of the Thai and Vietnamese NER data used in our experiments. Experimental Settings In addition to evaluating our method on the full training data, we"
2020.emnlp-main.488,Q16-1035,0,0.0522738,"does not change the semantics of visual information, deleting or replacing a single word could completely change the meaning of the sentence. One successful method for data augmentation in NLP is back translation (Sennrich et al., 2016; Fadaee et al., 2017; Dong et al., 2017; Yu et al., 2018), where a translation model is used to translate monolingual sentences from target language to source language to generate synthetic parallel sentences. Other successful methods include: systematically reordering the dependents of some nodes in gold data to generate synthetic data for dependency parsing (Wang and Eisner, 2016), leveraging knowledge base for question generation (Serban et al., 2016) and using simulation-based approach to generate a set of prerequisite toy tasks for QA (Weston et al., 2015). Besides, synonym replace6045 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6045–6057, c November 16–20, 2020. 2020 Association for Computational Linguistics ment, random deletion/swap/insertion, generation with VAE or pre-trained language models are also used in some NLP tasks (Kobayashi, 2018; Wei and Zou, 2019; Anaby-Tavor et al., 2020; Raille et al., 2020; Kumar"
2020.emnlp-main.488,D19-1670,0,0.046969,"o generate synthetic data for dependency parsing (Wang and Eisner, 2016), leveraging knowledge base for question generation (Serban et al., 2016) and using simulation-based approach to generate a set of prerequisite toy tasks for QA (Weston et al., 2015). Besides, synonym replace6045 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6045–6057, c November 16–20, 2020. 2020 Association for Computational Linguistics ment, random deletion/swap/insertion, generation with VAE or pre-trained language models are also used in some NLP tasks (Kobayashi, 2018; Wei and Zou, 2019; Anaby-Tavor et al., 2020; Raille et al., 2020; Kumar et al., 2020), but mainly for translation and classification tasks. Compared with the above-mentioned downstream tasks like translation and classification, sequence tagging is more fragile when it is confronted with data augmentation noises due to the finer granularity of the (token-level) task. Annotating unlabeled data with a weak tagger, leveraging aligned bilingual corpora to induce annotation and synonym replacement are three attempted data augmentation methods for sequence tagging (Shang et al., 2018; Yarowsky et al., 2001; Mathew et"
2020.emnlp-main.569,D19-1291,0,0.110337,"Missing"
2020.emnlp-main.569,Q16-1026,0,0.0668326,"Missing"
2020.emnlp-main.569,N19-1423,0,0.0240504,"Missing"
2020.emnlp-main.569,N19-1129,0,0.0324957,"Missing"
2020.emnlp-main.569,W15-4631,0,0.143997,"Missing"
2020.emnlp-main.569,D19-1564,0,0.0469096,"Missing"
2020.emnlp-main.569,P17-2039,0,0.0350856,"Missing"
2020.emnlp-main.569,P11-2088,0,0.0470329,"Missing"
2020.emnlp-main.599,W19-1909,0,0.0601924,"Missing"
2020.emnlp-main.599,P07-1056,0,0.721381,"mer 3.2.1 ? ?! ? ? ??? ?????????? Transformer Embedding layer 0 ? ? ??? Following BERT (Devlin et al., 2019), most PrLMs consist of an embedding layer and several transformer layers. Suppose a PrLM has L + 1 layers, layer 0 is the embedding layer, and layer L is the last layer. Given an input sentence x = [w1 , w2 , · · · , w|x |], the embedding layer of the PrLM will encode x as: h0 = Embedding(x) ???? Figure 1: Illustration of our model architecture which includes a pre-trained language model, a feature adaptation module, and a classifier. domain-specific words to reduce domain discrepancy (Blitzer et al., 2007; Pan et al., 2010; He et al., 2011). 3 Pre-trained Language Model Preliminary In this section, we introduce the problem definition and the model architecture based on which we build our domain adaptation algorithm presented in the next section. (1) |x| where h0 = [h10 , h20 , · · · , h0 ]. After obtaining the embeddings of the input sentence, we compute the features of the sentence from the transformer blocks of PrLM. In layer l, we compute the transformer feature as: hl = Transformerl (hl−1 ) (2) |x| , hl ] where hl = [h1l , h2l , · · · and l ∈ {1, 2, · · · , L}. Using all the |x |features w"
2020.emnlp-main.599,P19-1299,0,0.0307874,"n. MMD adopts the Maximum Mean Discrepancy loss (Gretton et al., 2012) in which Gaussian Kernel is implemented. Adv (Ganin et al., 2016; Chen et al., 2018) adversarially trains a domain classifier to learn domaininvariant features by reversing the gradients from the domain classifier following Ganin et al. (2016). p is our self-training method introduced in §4.1. p+CFd is our full model that uses CFd to enhance the robustness of self-training. DAS (He et al., 2018) uses semi-supervised learning. CLDFA (Xu and Yang, 2017) is a cross-lingual baseline which uses cross-lingual resources. MAN-MoE (Chen et al., 2019) studies multi-lingual transfer which has multiple languages in the source domain. MoE learns to focus on more transferable source domains for adaptation. xlmr-10, KL, MMD, Adv, p, and p+CFd are all based on the multi-layer representations with last 10-layer features. For KL, MMD, and Adv, to minimize domain discrepancy, we use an unlabeled set of the same size in the source domain as the target domain. xlmr-tuning3 first fine-tunes XLM-R with source labeled data using the representation from the final layer [CLS] and being fed to the classifier (Devlin et al., 2019), then tests on the target."
2020.emnlp-main.599,N19-1112,0,0.301935,"ature self-distillation (CFd) to learn discriminative features from PrLMs, in which PrLM features are self-distilled into a feature adaptation module and the features from the same class are more tightly clustered. We further extend CFd to a cross-language setting, in which language discrepancy is studied. Experiments on two monolingual and multilingual Amazon review datasets show that CFd can consistently improve the performance of self-training in cross-domain and cross-language settings. 1 Introduction Pre-trained language models (PrLMs) such as BERT (Devlin et al., 2019) and its variants (Liu et al., 2019c; Yang et al., 2019) have shown significant success for various downstream NLP tasks. However, these deep neural networks are sensitive to different cross-domain distributions (QuioneroCandela et al., 2009) and their effectiveness will be much weakened in such a scenario. How to ∗ Qingyu Tan is under the Joint PhD Program between Alibaba and National University of Singapore. adapt PrLMs to new domains is important. Unlike the most recent work that fine-tunes PrLMs on the unlabeled data from the new domains (Han and Eisenstein, 2019; Gururangan et al., 2020), we are interested in how to adapt"
2020.emnlp-main.599,2021.ccl-1.108,0,0.099182,"Missing"
2020.emnlp-main.599,P19-1335,0,0.0221745,"ideal joint hypothesis (§4.3,5.4). 2 Related Work Adaptation of PrLMs. Recently, significant improvements on multiple NLP tasks have been enabled by pre-trained language models (PrLMs) (Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019c; Howard and Ruder, 2018; Peters et al., 2018). To enhance their performance on new domains, much work has been done to adapt PrLMs. Two main adaptation settings have been studied. The first is the same as what we study in this work: the PrLM provides the features based on which domain adaptation is conducted (Han and Eisenstein, 2019; Cao et al., 2019; Logeswaran et al., 2019; Ma et al., 2019; Li et al., 2020). In the second setting, the corpus for pre-training a language model has large domain discrepancy with the target domain, so in this scenario, we need the target unlabeled data to fine-tune the PrLM after which we train a task-specific model (Gururangan et al., 2020). For example, Lee et al. (2020) and Alsentzer et al. (2019) transfer PrLMs into biomedical and clinical domains. Instead of fine-tuning PrLMs with unlabeled data from the new domain as in most previous work (Rietzler et al., 2019; Han and Eisenstein, 2019; Gururangan et al., 2020), we are intere"
2020.emnlp-main.599,D15-1166,0,0.0488842,"e for classification (Hao et al., 2019; Peters et al., 2018; Liu et al., 2019b). By making a trade-off between speed and model performance, we combine the last N -layer features from the PrLM for domain adaptation, which is called the multi-layer representation of the PrLM. Our FAM consists of a feed-forward neural network (followed by a tanh activation function) and ¯ l from layer l an attention mechanism. We map h into zl with the feed-forward neural network: ¯l) zl = f (h (4) Multi-layer Representation. Since feature effectiveness differs from layer to layer, we use an attention mechanism (Luong et al., 2015) to learn to weight the features from the last N layers. We get 7388 the multi-layer representation z of the PrLM as: L X z = E(x; θ) = αi = PL ? ? Ave αi zi i=L−N +1 tanh(W att zi ) e ??????? FAM Transformer (5) Transformer tanh(Watt zj ) j=L−N +1 e in which Watt is a matrix of trainable parameters. Inspired by Berthelot et al. (2019), we want the model to focus more on the higher-weighted layers, so we further calculate the attention weight as: ℎ""! ???? ? ?????? Transformer ℎ""! Figure 2: Illustration of feature self-distillation. We take the sum of the last N -layerlayer features for distil"
2020.emnlp-main.599,D19-6109,0,0.0326124,"§4.3,5.4). 2 Related Work Adaptation of PrLMs. Recently, significant improvements on multiple NLP tasks have been enabled by pre-trained language models (PrLMs) (Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019c; Howard and Ruder, 2018; Peters et al., 2018). To enhance their performance on new domains, much work has been done to adapt PrLMs. Two main adaptation settings have been studied. The first is the same as what we study in this work: the PrLM provides the features based on which domain adaptation is conducted (Han and Eisenstein, 2019; Cao et al., 2019; Logeswaran et al., 2019; Ma et al., 2019; Li et al., 2020). In the second setting, the corpus for pre-training a language model has large domain discrepancy with the target domain, so in this scenario, we need the target unlabeled data to fine-tune the PrLM after which we train a task-specific model (Gururangan et al., 2020). For example, Lee et al. (2020) and Alsentzer et al. (2019) transfer PrLMs into biomedical and clinical domains. Instead of fine-tuning PrLMs with unlabeled data from the new domain as in most previous work (Rietzler et al., 2019; Han and Eisenstein, 2019; Gururangan et al., 2020), we are interested in the featu"
2020.emnlp-main.599,N18-1202,0,0.270565,"nd multilingual Amazon review datasets for sentiment classification: MonoAmazon for cross-domain and MultiAmazon for cross-language experiments. We demonstrate that self-training can be consistently improved by CFd in all settings (§5.3). Further empirical results indicate that the improvements come from learning lower errors of ideal joint hypothesis (§4.3,5.4). 2 Related Work Adaptation of PrLMs. Recently, significant improvements on multiple NLP tasks have been enabled by pre-trained language models (PrLMs) (Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019c; Howard and Ruder, 2018; Peters et al., 2018). To enhance their performance on new domains, much work has been done to adapt PrLMs. Two main adaptation settings have been studied. The first is the same as what we study in this work: the PrLM provides the features based on which domain adaptation is conducted (Han and Eisenstein, 2019; Cao et al., 2019; Logeswaran et al., 2019; Ma et al., 2019; Li et al., 2020). In the second setting, the corpus for pre-training a language model has large domain discrepancy with the target domain, so in this scenario, we need the target unlabeled data to fine-tune the PrLM after which we train a task-spec"
2020.emnlp-main.738,W05-0909,0,0.118945,"ength and KB number, the data are mean, median, min and max respectively. 4 4.1 Experiments Experimental Setup We split WITA into a training set, a development set, and a testing set of 50,000, 5,000, and 400 records respectively. For the purpose of evaluating the performance of the models, we ask human helpers to annotate the testing set sentences. The human helpers are asked to revise the input KB triples and the corresponding target sentences making them exactly consistent with each other. We use several evaluation metrics including BLEU (Papineni et al., 2002), ROUGEL (Lin, 2004), METEOR (Banerjee and Lavie, 2005), NIST (Doddington, 2002) and CIDEr (Vedantam et al., 2015) with the package provided by Novikova et al. (2017). We follow the default setting in ROUGEL where β is set to 1.2. We build our model based on the Transformer model (Vaswani et al., 2017; Ott et al., 2019) and use Byte Pair Encoding (BPE) (Sennrich et al., 2016) to build the subword dictionary. We use Fairseq (Ott et al., 2019) to build our model and keep all hyper-parameters for Transformer unchanged. We set κ = 0.75 from {0.1, 0.25, 0.5, 0.75, 0.9} by extracting samples and ask human helper to evaluate. We use grid search to tune h"
2020.emnlp-main.738,2020.emnlp-main.90,1,0.659943,"ully et al. (2019) propose to generate a match report according to the match briefing. All these datasets are restricted to a few domains where well-aligned data is happened to be available. No existing works are focusing on handling partiallyaligned data. To solve the dataset scarcity problem, Fu et al. (2020c) propose to use dual learning to train generation models based on unaligned text and knowledge triples. The model generates text based on input triples and then predict the input triples with a dual extraction model. The two models are trained alternatively with dual learning. Although Cheng et al. (2020) proposed the ENTDESC task aiming at generating better text description for a few entities by exploring the knowledge from KB, their focus is more on distilling the useful part from the input knowledge. Text aligning has been studied for many years. Dyer et al. (2013) propose the Fast Align model which is a log-linear reparameterization of IBM Model 2. Legrand et al. (2016) propose a new score aggregation method to improve the alignment result. Moreover, attention-based models (Bahdanau et al., 2014) can also be recognized as a kind of alignment. However, these models focus on aligning source"
2020.emnlp-main.738,D16-1128,0,0.259989,"Missing"
2020.emnlp-main.738,W16-2207,0,0.0553607,"Missing"
2020.emnlp-main.738,N13-1073,0,0.0370695,"dataset scarcity problem, Fu et al. (2020c) propose to use dual learning to train generation models based on unaligned text and knowledge triples. The model generates text based on input triples and then predict the input triples with a dual extraction model. The two models are trained alternatively with dual learning. Although Cheng et al. (2020) proposed the ENTDESC task aiming at generating better text description for a few entities by exploring the knowledge from KB, their focus is more on distilling the useful part from the input knowledge. Text aligning has been studied for many years. Dyer et al. (2013) propose the Fast Align model which is a log-linear reparameterization of IBM Model 2. Legrand et al. (2016) propose a new score aggregation method to improve the alignment result. Moreover, attention-based models (Bahdanau et al., 2014) can also be recognized as a kind of alignment. However, these models focus on aligning source words to target words, and no existing models have been proposed to directly calculate supportiveness for generation tasks. In generation systems, Fu et al. (2020b) propose to dynamically align the current generation state with topics to improve the generation perform"
2020.emnlp-main.738,D19-1052,0,0.0237787,"Missing"
2020.emnlp-main.738,2020.coling-main.215,1,0.900372,"y Frank Herberfor the Dune univer ... ” which is even not a human-readable sentence. 5 Related Works During the past few years, many tasks have been proposed to generate human-readable text from the structured data. WebNLG (Gardent et al., 2017a,b; Ferreira et al., 2019) is proposed to describe KB triples sampled from DBPedia (Auer et al., 2007). 9190 The E2E (Novikova et al., 2017; Duˇsek et al., 2020) task is proposed for generating restaurant reviews based on the given attributes. Lebret et al. (2016) propose the Wikibio task to generate people’s biography based on given Wikipedia infobox. Fu et al. (2020a) propose to generate text based on event chains. Moreover, Liang et al. (2009) propose to generate weather reports for weather records and Wiseman et al. (2017), Chen and Mooney (2008) and Puduppully et al. (2019) propose to generate a match report according to the match briefing. All these datasets are restricted to a few domains where well-aligned data is happened to be available. No existing works are focusing on handling partiallyaligned data. To solve the dataset scarcity problem, Fu et al. (2020c) propose to use dual learning to train generation models based on unaligned text and knowl"
2020.emnlp-main.738,2020.aacl-main.29,1,0.902915,"y Frank Herberfor the Dune univer ... ” which is even not a human-readable sentence. 5 Related Works During the past few years, many tasks have been proposed to generate human-readable text from the structured data. WebNLG (Gardent et al., 2017a,b; Ferreira et al., 2019) is proposed to describe KB triples sampled from DBPedia (Auer et al., 2007). 9190 The E2E (Novikova et al., 2017; Duˇsek et al., 2020) task is proposed for generating restaurant reviews based on the given attributes. Lebret et al. (2016) propose the Wikibio task to generate people’s biography based on given Wikipedia infobox. Fu et al. (2020a) propose to generate text based on event chains. Moreover, Liang et al. (2009) propose to generate weather reports for weather records and Wiseman et al. (2017), Chen and Mooney (2008) and Puduppully et al. (2019) propose to generate a match report according to the match briefing. All these datasets are restricted to a few domains where well-aligned data is happened to be available. No existing works are focusing on handling partiallyaligned data. To solve the dataset scarcity problem, Fu et al. (2020c) propose to use dual learning to train generation models based on unaligned text and knowl"
2020.emnlp-main.738,P17-1017,0,0.432533,"nding to the text “developed in Canada”. The model is likely to bind the text to existing triples incorrectly. As a result, during the testing or operational stage, the model is likely to overly generate this kind of excerpt for similar triples. given structured data. For example, given the input knowledge base (KB) triple hCompany of Heroes, developer, Relic Entertainmenti, the aim is to generate a text description such as “Company of Heroes is developed by Relic Entertainment.”. In recent years, many works have been proposed to give impetus to the Data-to-Text generation task. For instance, Gardent et al. (2017a; 2017b) proposed the WebNLG task aiming at generating description text of the given KB triples. Novikova et al. (2017) proposed the E2E task aiming at generating restaurant reviews according to the given restaurant attributes. Lebret et al. (2016) proposed the WikiBio task in which the biography of each person is generated according to the given Wikipedia infobox. Introduction The Data-to-Text generation task focuses on generating human-readable text corresponding to some 1 Train The data and source code of this paper can be obtained from https://github.com/fuzihaofzh/ distant_supervision_nl"
2020.emnlp-main.738,W17-3518,0,0.628287,"nding to the text “developed in Canada”. The model is likely to bind the text to existing triples incorrectly. As a result, during the testing or operational stage, the model is likely to overly generate this kind of excerpt for similar triples. given structured data. For example, given the input knowledge base (KB) triple hCompany of Heroes, developer, Relic Entertainmenti, the aim is to generate a text description such as “Company of Heroes is developed by Relic Entertainment.”. In recent years, many works have been proposed to give impetus to the Data-to-Text generation task. For instance, Gardent et al. (2017a; 2017b) proposed the WebNLG task aiming at generating description text of the given KB triples. Novikova et al. (2017) proposed the E2E task aiming at generating restaurant reviews according to the given restaurant attributes. Lebret et al. (2016) proposed the WikiBio task in which the biography of each person is generated according to the given Wikipedia infobox. Introduction The Data-to-Text generation task focuses on generating human-readable text corresponding to some 1 Train The data and source code of this paper can be obtained from https://github.com/fuzihaofzh/ distant_supervision_nl"
2020.emnlp-main.738,P16-1154,0,0.0932135,"Missing"
2020.emnlp-main.738,P09-1011,0,0.115782,"entence. 5 Related Works During the past few years, many tasks have been proposed to generate human-readable text from the structured data. WebNLG (Gardent et al., 2017a,b; Ferreira et al., 2019) is proposed to describe KB triples sampled from DBPedia (Auer et al., 2007). 9190 The E2E (Novikova et al., 2017; Duˇsek et al., 2020) task is proposed for generating restaurant reviews based on the given attributes. Lebret et al. (2016) propose the Wikibio task to generate people’s biography based on given Wikipedia infobox. Fu et al. (2020a) propose to generate text based on event chains. Moreover, Liang et al. (2009) propose to generate weather reports for weather records and Wiseman et al. (2017), Chen and Mooney (2008) and Puduppully et al. (2019) propose to generate a match report according to the match briefing. All these datasets are restricted to a few domains where well-aligned data is happened to be available. No existing works are focusing on handling partiallyaligned data. To solve the dataset scarcity problem, Fu et al. (2020c) propose to use dual learning to train generation models based on unaligned text and knowledge triples. The model generates text based on input triples and then predict t"
2020.emnlp-main.738,D15-1166,0,0.0442865,"Missing"
2020.emnlp-main.738,W17-5525,0,0.103702,"Missing"
2020.emnlp-main.738,N19-4009,0,0.113018,"ce of the models, we ask human helpers to annotate the testing set sentences. The human helpers are asked to revise the input KB triples and the corresponding target sentences making them exactly consistent with each other. We use several evaluation metrics including BLEU (Papineni et al., 2002), ROUGEL (Lin, 2004), METEOR (Banerjee and Lavie, 2005), NIST (Doddington, 2002) and CIDEr (Vedantam et al., 2015) with the package provided by Novikova et al. (2017). We follow the default setting in ROUGEL where β is set to 1.2. We build our model based on the Transformer model (Vaswani et al., 2017; Ott et al., 2019) and use Byte Pair Encoding (BPE) (Sennrich et al., 2016) to build the subword dictionary. We use Fairseq (Ott et al., 2019) to build our model and keep all hyper-parameters for Transformer unchanged. We set κ = 0.75 from {0.1, 0.25, 0.5, 0.75, 0.9} by extracting samples and ask human helper to evaluate. We use grid search to tune hyper-parameters on the development set and choose ωw = 0.05 from {0.02,0.05,0.1,0.2,0.5,1.0,2.0,5.0}, choose ωc = 1.0 from {0.02,0.05,0.1,0.2,0.5,1.0, 2.0,5.0} and choose α = 0.1 from {0.02,0.05,0.1,0.2,0.5,1.0}. The model has 49M parameters and it takes 2.4 hours t"
2020.emnlp-main.738,P02-1040,0,0.107014,"ble 1: Statistics of WITA and WebNLG. For the text length and KB number, the data are mean, median, min and max respectively. 4 4.1 Experiments Experimental Setup We split WITA into a training set, a development set, and a testing set of 50,000, 5,000, and 400 records respectively. For the purpose of evaluating the performance of the models, we ask human helpers to annotate the testing set sentences. The human helpers are asked to revise the input KB triples and the corresponding target sentences making them exactly consistent with each other. We use several evaluation metrics including BLEU (Papineni et al., 2002), ROUGEL (Lin, 2004), METEOR (Banerjee and Lavie, 2005), NIST (Doddington, 2002) and CIDEr (Vedantam et al., 2015) with the package provided by Novikova et al. (2017). We follow the default setting in ROUGEL where β is set to 1.2. We build our model based on the Transformer model (Vaswani et al., 2017; Ott et al., 2019) and use Byte Pair Encoding (BPE) (Sennrich et al., 2016) to build the subword dictionary. We use Fairseq (Ott et al., 2019) to build our model and keep all hyper-parameters for Transformer unchanged. We set κ = 0.75 from {0.1, 0.25, 0.5, 0.75, 0.9} by extracting samples and ask"
2020.emnlp-main.738,P19-1195,0,0.0381904,"Missing"
2020.emnlp-main.738,P16-1162,0,0.0103105,"he testing set sentences. The human helpers are asked to revise the input KB triples and the corresponding target sentences making them exactly consistent with each other. We use several evaluation metrics including BLEU (Papineni et al., 2002), ROUGEL (Lin, 2004), METEOR (Banerjee and Lavie, 2005), NIST (Doddington, 2002) and CIDEr (Vedantam et al., 2015) with the package provided by Novikova et al. (2017). We follow the default setting in ROUGEL where β is set to 1.2. We build our model based on the Transformer model (Vaswani et al., 2017; Ott et al., 2019) and use Byte Pair Encoding (BPE) (Sennrich et al., 2016) to build the subword dictionary. We use Fairseq (Ott et al., 2019) to build our model and keep all hyper-parameters for Transformer unchanged. We set κ = 0.75 from {0.1, 0.25, 0.5, 0.75, 0.9} by extracting samples and ask human helper to evaluate. We use grid search to tune hyper-parameters on the development set and choose ωw = 0.05 from {0.02,0.05,0.1,0.2,0.5,1.0,2.0,5.0}, choose ωc = 1.0 from {0.02,0.05,0.1,0.2,0.5,1.0, 2.0,5.0} and choose α = 0.1 from {0.02,0.05,0.1,0.2,0.5,1.0}. The model has 49M parameters and it takes 2.4 hours to train it on a NVIDIA TITAN RTX graphics card. 4.2 Compa"
2020.emnlp-main.738,W18-6543,0,0.0851559,"TAN RTX graphics card. 4.2 Comparison Models We compare our full DSG model with the following baselines, state-of-the-art models, and ablations. S2S utilizes the traditional S2S model (Sutskever et al., 2014; Cho et al., 2014) equipped with atten0.463 0.496 0.518 0.500 0.555 0.540 0.522 7.97 8.05 8.36 8.61 8.71 8.59 8.38 0.385 0.417 0.421 0.403 0.425 0.421 0.421 0.693 0.721 0.730 0.711 0.742 0.740 0.734 4.12 4.53 4.75 4.65 5.02 4.97 4.83 Table 2: Main results. tion (Bahdanau et al., 2014; Luong et al., 2015) and copy (Gu et al., 2016) mechanism. It is recognised as the state-of-the-art model (Shimorina and Gardent, 2018) in the WebNLG (Gardent et al., 2017b) task. S2ST utilizes the prevalent Transformer model (Vaswani et al., 2017; Ott et al., 2019) which outperforms the traditional S2S model in many generation tasks. DSG-A utilizes the attention adaptor which adapts attention as the supportiveness scores in the loss. DSG-H is almost the same as our DSG model. The only difference is that the supportiveness scores are adapted with hard adaptor while our DSG model uses the soft adaptor. DSG w/o RBS is an ablation model. It removes the Rebalanced Beam Search component from our DSG model. DSG w/o SA is an ablatio"
2020.emnlp-main.90,D17-1209,0,0.0151228,"sk is AMR-to-text generation (Konstas et al., 2017). The structure of AMR graphs is rooted and denser, which is quite different from the KG-to-text task. Researchers also studied how to generate texts from a few given entities or prompts (Li et al., 2019; Fu et al., 2020a). However, they did not explore the knowledge from a KG. Graph-to-sequence Modeling. In recent years, graph convolutional networks (GCN) have been applied to several tasks (e.g., semi-supervised node classification (Kipf and Welling, 2017), semantic role labeling (Marcheggiani and Titov, 2017) and neural machine translation (Bastings et al., 2017)) and also achieved state-of-the-art performance on graph-to-sequence modeling. In order to capture more graphical information, Velickovic et al. (2017) introduced graph attention networks (GATs) through stacking a graph attentional layer, but only allowed to learn information from adjacent nodes implicitly without considering a more global contextualization. Marcheggiani and Titov (2017) then used GCN as the encoder in order to capture more distant information in graphs. Since there are usually a large amount of labels for edges in KG, such graph-to-sequence models without graph transformatio"
2020.emnlp-main.90,P18-1026,0,0.30088,"specially when using fewer GCN layers. Our main contributions include: Our dataset is not only more practical but also more challenging due to lack of explicit alignment between the input and the output. Therefore, some knowledge is useful for generation, while others might be noise. In such a case that many different relations from the KG are involved, standard graphto-sequence models suffer from the problem of low training speed and parameter explosion, as edges are encoded in the form of parameters. Previous work deals with this problem by transforming the original graphs into Levi graphs (Beck et al., 2018). However, Levi graph transformation only explicitly represents the relations between an original node and its neighbor edges, while the relations between two original nodes are learned implicitly through graph convolutional networks (GCN). Therefore, more GCN layers are required to capture such information (Marcheggiani and Perez-Beltrachini, 2018). As more GCN layers are being stacked, it suffers from information loss from KG (Abu-ElHaija et al., 2018). In order to address these limitations, we present a multi-graph convolutional networks (MGCN) architecture by introducing multigraph transfo"
2020.emnlp-main.90,P11-2031,0,0.0173309,". 23.3 20.4 68.7 58.8 41.9 21.8 20.5 67.5 59.5 39.5 24.2 21.3 65.8 59.8 43.3 20.6 20.3 66.5 59.1 40.0 Table 2: Main results of models on ENT-DESC dataset. ↓ indicates lower is better. During decoding, we use beam search with a beam size of 10. All models are run with V100 GPU. We evaluate our models by applying both automatic and human evaluations. For automatic evaluation, we use several common evaluation metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), TER (Snover et al., 2006), ROUGE1 , ROUGE2 , ROUGEL (Lin, 2004), PARENT (Dhingra et al., 2019). We adapt MultEval (Clark et al., 2011) and Py-rouge for resampling and significance test. 6.2 Main Experimental Results We present our main experiments on ENT-DESC dataset and compare our proposed MGCN models with various aggregation methods against several strong GNN baselines (Bahdanau et al., 2014), GraphTransformer (Koncel-Kedziorski et al., 2019), GRN (Beck et al., 2018), GCN (Marcheggiani and Perez-Beltrachini, 2018) and DeepGCN (Guo et al., 2019), as well as a sequenceto-sequence (S2S) baseline. We re-implement GRN, GCN and DeepGCN using MXNET. We rearrange the order of input triples following the occurrence of entities in"
2020.emnlp-main.90,W11-2107,0,0.0226219,"7.7 23.4 26.3 24.3 E2S E2S + delex E2S-MEF E2S-MEF + delex The rows below are results of generating from entities only without exploring the KG. 23.3 20.4 68.7 58.8 41.9 21.8 20.5 67.5 59.5 39.5 24.2 21.3 65.8 59.8 43.3 20.6 20.3 66.5 59.1 40.0 Table 2: Main results of models on ENT-DESC dataset. ↓ indicates lower is better. During decoding, we use beam search with a beam size of 10. All models are run with V100 GPU. We evaluate our models by applying both automatic and human evaluations. For automatic evaluation, we use several common evaluation metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), TER (Snover et al., 2006), ROUGE1 , ROUGE2 , ROUGEL (Lin, 2004), PARENT (Dhingra et al., 2019). We adapt MultEval (Clark et al., 2011) and Py-rouge for resampling and significance test. 6.2 Main Experimental Results We present our main experiments on ENT-DESC dataset and compare our proposed MGCN models with various aggregation methods against several strong GNN baselines (Bahdanau et al., 2014), GraphTransformer (Koncel-Kedziorski et al., 2019), GRN (Beck et al., 2018), GCN (Marcheggiani and Perez-Beltrachini, 2018) and DeepGCN (Guo et al., 2019), as well as a sequenceto-sequence (S2S) base"
2020.emnlp-main.90,P19-1483,0,0.0182176,"rom entities only without exploring the KG. 23.3 20.4 68.7 58.8 41.9 21.8 20.5 67.5 59.5 39.5 24.2 21.3 65.8 59.8 43.3 20.6 20.3 66.5 59.1 40.0 Table 2: Main results of models on ENT-DESC dataset. ↓ indicates lower is better. During decoding, we use beam search with a beam size of 10. All models are run with V100 GPU. We evaluate our models by applying both automatic and human evaluations. For automatic evaluation, we use several common evaluation metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), TER (Snover et al., 2006), ROUGE1 , ROUGE2 , ROUGEL (Lin, 2004), PARENT (Dhingra et al., 2019). We adapt MultEval (Clark et al., 2011) and Py-rouge for resampling and significance test. 6.2 Main Experimental Results We present our main experiments on ENT-DESC dataset and compare our proposed MGCN models with various aggregation methods against several strong GNN baselines (Bahdanau et al., 2014), GraphTransformer (Koncel-Kedziorski et al., 2019), GRN (Beck et al., 2018), GCN (Marcheggiani and Perez-Beltrachini, 2018) and DeepGCN (Guo et al., 2019), as well as a sequenceto-sequence (S2S) baseline. We re-implement GRN, GCN and DeepGCN using MXNET. We rearrange the order of input triples"
2020.emnlp-main.90,2020.emnlp-main.738,1,0.746045,"from the KG. Extensive Dataset and Task. There is an increasing number of new datasets and tasks being proposed in recent years as more attention has been paid to data-to-text generation. Gardent et al. (2017) introduced the WebNLG challenge, which aimed to generate text from a small set of RDF knowledge triples (no more than 7) that are well-aligned with the text. To avoid the high cost of preparing such well-aligned data, researchers also studied how to leverage automatically obtained partially-aligned data in which some portion of the output text cannot be generated from the input triples (Fu et al., 2020b). Koncel-Kedziorski et al. (2019) introduced AGENDA dataset, which aimed to generate paper abstract from a title and a small KG built by information extraction system on the abstracts and has at most 7 relations. In our work, we directly create a knowledge graph for the main entities and topicrelated entities from Wikidata without looking at the relations in our output. Scale-wise, our dataset consists of 110k instances while AGENDA is 40k. Lebret et al. (2016) introduced WIKIBIO dataset that generates the first sentence of biographical articles from the key-value pairs extracted from the ar"
2020.emnlp-main.90,W17-3518,0,0.530368,"nowledge into comprehensive natural language, is an important task in natural language processing (NLP) and user interaction studies (Damljanovic et al., 2010). Specifically, the task takes as input some structured knowledge, such as resource description framework (RDF) triples of ∗ Liying Cheng is under the Joint Ph.D. Program between Alibaba and Singapore University of Technology and Design. † Dekun Wu was a visiting student at SUTD. Yan Zhang and Zhanming Jie were interns at Alibaba. 1 Our code and data are available at https://github.com/LiyingCheng95/ EntityDescriptionGeneration. WebNLG (Gardent et al., 2017), key-value pairs of WIKIBIO (Lebret et al., 2016) and E2E (Novikova et al., 2017), to generate natural text describing the input knowledge. In essence, the task can be formulated as follows: given a main entity, its one-hop attributes/relations (e.g., WIKIBIO and E2E), and/or multi-hop relations (e.g., WebNLG), the goal is to generate a text description of the main entity describing its attributes and relations. Note that these existing datasets basically have a good alignment between an input knowledge set and its output text. Obtaining such data with good alignment could be a laborious and"
2020.emnlp-main.90,Q19-1019,1,0.911671,"amount of labels for edges in KG, such graph-to-sequence models without graph transformation will incur information loss and parameter explosion. Beck et al. (2018) proposed to transform the graph into Levi graph in order to work towards the aforementioned deficiencies, together with gated graph neural network (GGNN) to build graph representation for AMR-to-text problem. However, they face some new limitations brought in by Levi graph transformation: the entityto-entity information is being ignored in Levi transformation, as also mentioned in their paper. Afterwards, deeper GCNs were stacked (Guo et al., 2019) to capture such ignored information implicitly. In contrast, we intend to use fewer GCN layers to capture more global contextualization by explicitly stating all types of graph information with different transformations. 3 Task Description WebNLG AGENDA E2E ENT-DESC # instances Input vocab Output vocab # distinct entities # distinct relations Avg. # triples per input Avg. # words per output 41K 54K 78K 297K 7 4.4 141.3 51K 120 5.2K 77 8 5.6 20.3 110K 420K 248K 691K 957 27.4 31.0 Table 1: Dataset statistics of WebNLG, AGENDA and our prepared ENT-DESC. tice, it is difficult to describe an entit"
2020.emnlp-main.90,N19-1238,0,0.299874,"ive Dataset and Task. There is an increasing number of new datasets and tasks being proposed in recent years as more attention has been paid to data-to-text generation. Gardent et al. (2017) introduced the WebNLG challenge, which aimed to generate text from a small set of RDF knowledge triples (no more than 7) that are well-aligned with the text. To avoid the high cost of preparing such well-aligned data, researchers also studied how to leverage automatically obtained partially-aligned data in which some portion of the output text cannot be generated from the input triples (Fu et al., 2020b). Koncel-Kedziorski et al. (2019) introduced AGENDA dataset, which aimed to generate paper abstract from a title and a small KG built by information extraction system on the abstracts and has at most 7 relations. In our work, we directly create a knowledge graph for the main entities and topicrelated entities from Wikidata without looking at the relations in our output. Scale-wise, our dataset consists of 110k instances while AGENDA is 40k. Lebret et al. (2016) introduced WIKIBIO dataset that generates the first sentence of biographical articles from the key-value pairs extracted from the article’s infobox. Novikova et al. (2"
2020.emnlp-main.90,P17-1014,0,0.023481,"ons associated with Levi graphs. • Experiments and analysis on our new dataset show that our proposed MGCN model incorporated with aggregation methods outperforms strong baselines by effectively capturing and aggregating multi-graph information. 2 1188 Related Work for a single domain, while ours focuses on multiple domains of over 100 categories, including people, event, location, organization, etc. Another difference is that we intend to generate the first paragraph of each Wikipedia article from a more complicated KG, but not key-value pairs. Another popular task is AMR-to-text generation (Konstas et al., 2017). The structure of AMR graphs is rooted and denser, which is quite different from the KG-to-text task. Researchers also studied how to generate texts from a few given entities or prompts (Li et al., 2019; Fu et al., 2020a). However, they did not explore the knowledge from a KG. Graph-to-sequence Modeling. In recent years, graph convolutional networks (GCN) have been applied to several tasks (e.g., semi-supervised node classification (Kipf and Welling, 2017), semantic role labeling (Marcheggiani and Titov, 2017) and neural machine translation (Bastings et al., 2017)) and also achieved state-of-"
2020.emnlp-main.90,D16-1128,0,0.439154,"important task in natural language processing (NLP) and user interaction studies (Damljanovic et al., 2010). Specifically, the task takes as input some structured knowledge, such as resource description framework (RDF) triples of ∗ Liying Cheng is under the Joint Ph.D. Program between Alibaba and Singapore University of Technology and Design. † Dekun Wu was a visiting student at SUTD. Yan Zhang and Zhanming Jie were interns at Alibaba. 1 Our code and data are available at https://github.com/LiyingCheng95/ EntityDescriptionGeneration. WebNLG (Gardent et al., 2017), key-value pairs of WIKIBIO (Lebret et al., 2016) and E2E (Novikova et al., 2017), to generate natural text describing the input knowledge. In essence, the task can be formulated as follows: given a main entity, its one-hop attributes/relations (e.g., WIKIBIO and E2E), and/or multi-hop relations (e.g., WebNLG), the goal is to generate a text description of the main entity describing its attributes and relations. Note that these existing datasets basically have a good alignment between an input knowledge set and its output text. Obtaining such data with good alignment could be a laborious and expensive annotation process. More importantly, in"
2020.emnlp-main.90,W18-6501,0,0.36384,"s from the KG are involved, standard graphto-sequence models suffer from the problem of low training speed and parameter explosion, as edges are encoded in the form of parameters. Previous work deals with this problem by transforming the original graphs into Levi graphs (Beck et al., 2018). However, Levi graph transformation only explicitly represents the relations between an original node and its neighbor edges, while the relations between two original nodes are learned implicitly through graph convolutional networks (GCN). Therefore, more GCN layers are required to capture such information (Marcheggiani and Perez-Beltrachini, 2018). As more GCN layers are being stacked, it suffers from information loss from KG (Abu-ElHaija et al., 2018). In order to address these limitations, we present a multi-graph convolutional networks (MGCN) architecture by introducing multigraph transformation incorporated with an aggregation layer. Multi-graph transformation is able to represent the original graph information more accurately, while the aggregation layer learns to extract useful information from the KG. Extensive Dataset and Task. There is an increasing number of new datasets and tasks being proposed in recent years as more attent"
2020.emnlp-main.90,D17-1159,0,0.201683,"re complicated KG, but not key-value pairs. Another popular task is AMR-to-text generation (Konstas et al., 2017). The structure of AMR graphs is rooted and denser, which is quite different from the KG-to-text task. Researchers also studied how to generate texts from a few given entities or prompts (Li et al., 2019; Fu et al., 2020a). However, they did not explore the knowledge from a KG. Graph-to-sequence Modeling. In recent years, graph convolutional networks (GCN) have been applied to several tasks (e.g., semi-supervised node classification (Kipf and Welling, 2017), semantic role labeling (Marcheggiani and Titov, 2017) and neural machine translation (Bastings et al., 2017)) and also achieved state-of-the-art performance on graph-to-sequence modeling. In order to capture more graphical information, Velickovic et al. (2017) introduced graph attention networks (GATs) through stacking a graph attentional layer, but only allowed to learn information from adjacent nodes implicitly without considering a more global contextualization. Marcheggiani and Titov (2017) then used GCN as the encoder in order to capture more distant information in graphs. Since there are usually a large amount of labels for edges in KG, su"
2020.emnlp-main.90,W17-5525,0,0.223746,"Missing"
2020.emnlp-main.90,P02-1040,0,0.106802,"31.9 31.5 58.2 59.2 60.0 59.3 27.7 23.4 26.3 24.3 E2S E2S + delex E2S-MEF E2S-MEF + delex The rows below are results of generating from entities only without exploring the KG. 23.3 20.4 68.7 58.8 41.9 21.8 20.5 67.5 59.5 39.5 24.2 21.3 65.8 59.8 43.3 20.6 20.3 66.5 59.1 40.0 Table 2: Main results of models on ENT-DESC dataset. ↓ indicates lower is better. During decoding, we use beam search with a beam size of 10. All models are run with V100 GPU. We evaluate our models by applying both automatic and human evaluations. For automatic evaluation, we use several common evaluation metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), TER (Snover et al., 2006), ROUGE1 , ROUGE2 , ROUGEL (Lin, 2004), PARENT (Dhingra et al., 2019). We adapt MultEval (Clark et al., 2011) and Py-rouge for resampling and significance test. 6.2 Main Experimental Results We present our main experiments on ENT-DESC dataset and compare our proposed MGCN models with various aggregation methods against several strong GNN baselines (Bahdanau et al., 2014), GraphTransformer (Koncel-Kedziorski et al., 2019), GRN (Beck et al., 2018), GCN (Marcheggiani and Perez-Beltrachini, 2018) and DeepGCN (Guo et al., 2019), as well"
2020.emnlp-main.90,E17-2025,0,0.0284364,"described above, we can capture the information of higher-degree neighbors by stacking multiple MGCN layers. Inspired by Xu et al. (2018), we employ a concatenation operation over h(1) , · · · , h(n) to aggregate the graph representations from all MGCN layers (Figure 3 right) to form the final layer h(f inal) , which can be written as follows:   h(f inal) = h(1) , · · · h(n) . Such a mechanism allows weight sharing across graph nodes, which helps to reduce overfitting problems. To further reduce the number of parameters and overfitting problems, we apply the softmax weight tying technique (Press and Wolf, 2017) by tying source embeddings and target embeddings with a target softmax weight matrix. 5.2 Attention-based LSTM Decoder We adopt the commonly-used standard attentionbased LSTM as our decoder, where each next word yt is generated by conditioning on the final graph representation h(f inal) and all words that have been predicted y1 , ..., yt−1 . The training objective is to minimize the negative conditional log-likelihood. Thus, the objective function can be written as: T P L=− log pθ (yt |y1 , ..., yt−1 , h(f inal) ), t=1 where T represents the length of the output sequence, and p is the probabi"
2020.emnlp-main.90,2006.amta-papers.25,0,0.0222088,"x E2S-MEF E2S-MEF + delex The rows below are results of generating from entities only without exploring the KG. 23.3 20.4 68.7 58.8 41.9 21.8 20.5 67.5 59.5 39.5 24.2 21.3 65.8 59.8 43.3 20.6 20.3 66.5 59.1 40.0 Table 2: Main results of models on ENT-DESC dataset. ↓ indicates lower is better. During decoding, we use beam search with a beam size of 10. All models are run with V100 GPU. We evaluate our models by applying both automatic and human evaluations. For automatic evaluation, we use several common evaluation metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), TER (Snover et al., 2006), ROUGE1 , ROUGE2 , ROUGEL (Lin, 2004), PARENT (Dhingra et al., 2019). We adapt MultEval (Clark et al., 2011) and Py-rouge for resampling and significance test. 6.2 Main Experimental Results We present our main experiments on ENT-DESC dataset and compare our proposed MGCN models with various aggregation methods against several strong GNN baselines (Bahdanau et al., 2014), GraphTransformer (Koncel-Kedziorski et al., 2019), GRN (Beck et al., 2018), GCN (Marcheggiani and Perez-Beltrachini, 2018) and DeepGCN (Guo et al., 2019), as well as a sequenceto-sequence (S2S) baseline. We re-implement GRN,"
2021.acl-long.172,2020.acl-main.740,0,0.0400597,"Missing"
2021.acl-long.172,D19-1424,0,0.021136,"tperform those of fine-tuning. The differences between the mean and the best values are also smaller with adapter-based tuning. The results suggest that the performance of adapters is more stable over fine-tuning along the training process. Training neural networks can be viewed as searching for a good minima in the non-convex landscape defined by the loss function. Prior work (Hochreiter and Schmidhuber, 1997; Li et al., 2018) shows that the flatness of a local minima correlates with the generalization capability. Thus, we further show the loss landscapes of the two tuning methods. Following Hao et al. (2019), we plot the loss curve by linear interpolation between θ0 and θ1 with function f (α) = L(θ0 + α · (θ1 − θ0 )), where θ0 and θ1 denote the model weights before and after tuning. L(θ) is the loss function and α is a scalar parameter. In our experiments, we set the range of α to [−2, 2] and uniformly sample 20 points. Figure 6 shows the loss landscape curves on CoLA and SST based on BERT-base. It shows that the minimas of adapter-based tuning are more wide and flat, which indicates that adapter-based tuning tends to generalize better. Compare to Mixout The focus of this paper is to answer the q"
2021.acl-long.172,P18-1031,0,0.163589,"for evaluation. Note that the same set of tokens is used for all models. Finally, we compare the representations obtained from Madapt or Mf t to those from Morg using RSA. Figure 2 plots the results on STS-2, results of other tasks demonstrate a similar trend and can be found in Appendix A.3. For both fine-tuning and adapter-based tuning, we observe that the repre1 2 Cosine similarity is used We skip [PAD], [CLS], [SEP] for token selection. sentation change generally arises in the top layers of the network, which is consistent with previous findings that higher layers are more task relevant (Howard and Ruder, 2018). It can be clearly observed that compared to fine-tuning, adapterbased tuning yields representations with less deviation from those of BERT-base at each layer, which verifies our claim that adapter-based tuning can better regularize the tuning process by mitigating the forgetting problem. Apparently, this property of adapter tuning comes from that it freezes all the parameters of PrLMs. And because of the skipconnection in the adapter, the hidden representation out of the adapter can mimic the input representation, in this way, some of the original knowledge of PrLMs (before injecting adapter"
2021.acl-long.172,2020.acl-main.197,0,0.0289874,"s a new set of weights for each task, which is parameter inefficient. Adapterbased tuning is proposed to deal with this problem (Houlsby et al., 2019). Most previous work has demonstrated that it achieves comparable performance to fine-tuning (Bapna and Firat, 2019; Pfeiffer et al., 2020b,a,c; R¨uckl´e et al., 2020; Wang et al., 2020; Guo et al., 2020). However, existing work mostly focuses on the parameter-efficient aspect while overlooks the effectiveness. 2215 Fine-tuning PrLMs in a low-resource setting has been studied for a while (Dodge et al., 2020; Lee et al., 2020; Phang et al., 2018; Jiang et al., 2020; Zhang et al., 2021). Previous work points out that with large-scale parameters, fine-tuning on a few samples can lead to overfitting and bad generalization, which causes the results unstable. Phang et al. (2018) find that pretraining on an intermediate task can improve fine-tuning outcomes. Jiang et al. (2020) improve the robustness of fine-tuning by controlling the model complexity and preventing aggressive updating. On the other hand, catastrophic forgetting can appear when transferring a pretrained neural networks (French, 1999; McCloskey and Cohen, 1989; Goodfellow et al., 2013), where t"
2021.acl-long.172,2020.blackboxnlp-1.4,0,0.0238567,"the initial weights. Since adapter-based tuning does not update the weights of PrLMs at all, we suspect that it has a similar effect of alleviating the issue of catastrophic forgetting. Since the weights of the PrLM are the same before and after adapter-based tuning, to verify this, we use Representational Similarity Analysis (RSA) (Laakso and Cottrell, 2000) to assess the similarity of tuned representations to those without tuning at each transformer layer. RSA has been widely used to analyze the similarity between two neural network outputs (Abnar et al., 2019; Chrupała and Alishahi, 2019; Merchant et al., 2020), which works by creating two comparable sets of representations by inputting a same set of n samples to the two models. For each set of representations, a n × n pairwise similarity1 matrix is calculated. The final RSA similarity score between the two representation space is computed as the Pearson correlation between the flattened upper triangulars of the two similarity matrices. We use a subset of GLUE tasks (Wang et al., 2018) for our analysis. Given a task, we first perform adapter-based tuning and fine-tuning to adapt a BERT-base model (Morg ) to the target task, which yields models Madap"
2021.acl-long.172,2020.emnlp-main.617,0,0.0602973,"Missing"
2021.acl-long.172,W18-5446,0,0.0540876,"Missing"
2021.acl-long.367,D17-1047,1,0.830963,"ean pooling Average F1 ∆F1 67.69 66.45 66.09 66.19 -1.24 -1.60 -1.53 Table 6: Ablation study on the development sets. 5 Related Work Sentiment Analysis is a major Natural Language Understanding (NLU) task (Wang et al., 2019) and has been extensively studied as a classification problem at the sentence level (Raffel et al., 2020; Lan et al., 2020; Yang et al., 2020). Aspect-Based Sentiment Analysis (ABSA) (Pontiki et al., 2014) addresses various sentiment analysis tasks at a finegrained level. As mentioned in the Section 1, the subtasks mainly include ASC (Dong et al., 2014; Zhang et al., 2016; Chen et al., 2017; He et al., 2018b; Li et al., 2018a; Peng et al., 2018; Wang and Lu, 2018; He et al., 2019; Li and Lu, 2019; Xu et al., 2020a), ATE (Qiu et al., 2011; Yin et al., 2016; Li et al., 2018b; Ma et al., 2019), OTE (Hu and Liu, 2004; Yang and Cardie, 2012; Klinger and Cimiano, 2013; Yang and Cardie, 2013). There is also another subtask named Target-oriented Opinion Words Extraction (TOWE) (Fan et al., 2019), which aim to extract the corresponding opinion words for a given target term. Another line of research focuses on addressing different subtasks together. Aspect and Opinion Term Co-Extraction ("
2021.acl-long.367,D17-1018,0,0.353032,"ntiment relation of an aspect target and opinion pair. Of course, it can also consider the single-word aspects or opinions properly. Our model explicitly generates span representations for all possible target and opinion spans, and their paired sentiment relation is independently predicted for all possible target and opinion pairs. Span-based methods have shown encouraging perof a target span with positive sentiment polarity. 3 A common tagging scheme for sequence labeling, denoting “begin, inside, outside, end and single” respectively. formance on other tasks, such as coreference resolution (Lee et al., 2017), semantic role labeling (He et al., 2018a), and relation extraction (Luan et al., 2019; Wadden et al., 2019). However, they cannot be directly applied to the ASTE task due to different task-specific characteristics. Our contribution can be summarized as follows: • We tailor a span-level approach to explicitly consider the span-to-span interactions for the ASTE task and conduct extensive analysis to demonstrate its effectiveness. Our approach significantly improves performance, especially on triplets which contain multiword targets or opinions. • We propose a dual-channel span pruning strategy"
2021.acl-long.367,P19-1344,0,0.0626,"pore University of Technology and Design. 1 We make our code publicly available at https:// github.com/chiayewken/Span-ASTE. Figure 1, the aspect targets are “Windows 8” and “touchscreen functions”. Aspect Sentiment Classification (ASC) (Dong et al., 2014; Zhang et al., 2016; Yang et al., 2017; Li et al., 2018a; Tang et al., 2019) is one of the most well-explored subtasks of ABSA and aims to predict the sentiment polarity of a given aspect target. However, it is not always practical to assume that the aspect target is provided. Aspect Term Extraction (ATE) (Yin et al., 2016; Li et al., 2018b; Ma et al., 2019) focuses on extracting aspect targets, while Opinion Term Extraction (OTE) (Yang and Cardie, 2012; Klinger and Cimiano, 2013; Yang and Cardie, 2013) aims to extract the opinion terms which largely determine the sentiment polarity of the sentence or the corresponding target term. Aspect Sentiment Triplet Extraction (ASTE) (Peng et al., 2019) is the most recently proposed subtask of ABSA, which forms a more complete picture of the sentiment information through the triplet of an aspect target term, the corresponding opinion term, and the expressed sentiment. For the example in Figure 1, there are"
2021.acl-long.367,D14-1162,0,0.0878548,"Missing"
2021.acl-long.367,S16-1002,0,0.0597644,"Missing"
2021.acl-long.367,2020.findings-emnlp.234,0,0.178059,"Natural Language Processing, pages 4755–4766 August 1–6, 2021. ©2021 Association for Computational Linguistics dard BIOES 3 tags. The second stage then couples the extracted target and opinion terms to determine their paired sentiment relation. We know that in ABSA, the aspect sentiment is mostly determined by the opinion terms expressed on the aspect target (Qiu et al., 2011; Yang and Cardie, 2012). However, this pipeline approach breaks the interaction within the triplet structure. Moreover, pipeline approaches usually suffer from the error propagation problem. Recent end-to-end approaches (Wu et al., 2020; Xu et al., 2020b; Zhang et al., 2020) can jointly extract the target and opinion terms and classify their sentiment relation. One drawback is that they heavily rely on word-to-word interactions to predict the sentiment relation for the target-opinion pair. Note that it is common for the aspect targets and opinions to contain multiple words, which accounts for roughly one-third of triplets in the benchmark datasets. However, the previous methods (Wu et al., 2020; Zhang et al., 2020) predict the sentiment polarity for each word-word pair independently, which cannot guarantee their sentiment co"
2021.acl-long.367,2020.emnlp-main.288,1,0.434223,"Processing, pages 4755–4766 August 1–6, 2021. ©2021 Association for Computational Linguistics dard BIOES 3 tags. The second stage then couples the extracted target and opinion terms to determine their paired sentiment relation. We know that in ABSA, the aspect sentiment is mostly determined by the opinion terms expressed on the aspect target (Qiu et al., 2011; Yang and Cardie, 2012). However, this pipeline approach breaks the interaction within the triplet structure. Moreover, pipeline approaches usually suffer from the error propagation problem. Recent end-to-end approaches (Wu et al., 2020; Xu et al., 2020b; Zhang et al., 2020) can jointly extract the target and opinion terms and classify their sentiment relation. One drawback is that they heavily rely on word-to-word interactions to predict the sentiment relation for the target-opinion pair. Note that it is common for the aspect targets and opinions to contain multiple words, which accounts for roughly one-third of triplets in the benchmark datasets. However, the previous methods (Wu et al., 2020; Zhang et al., 2020) predict the sentiment polarity for each word-word pair independently, which cannot guarantee their sentiment consistency when fo"
2021.acl-long.367,2020.emnlp-main.183,1,0.392224,"Processing, pages 4755–4766 August 1–6, 2021. ©2021 Association for Computational Linguistics dard BIOES 3 tags. The second stage then couples the extracted target and opinion terms to determine their paired sentiment relation. We know that in ABSA, the aspect sentiment is mostly determined by the opinion terms expressed on the aspect target (Qiu et al., 2011; Yang and Cardie, 2012). However, this pipeline approach breaks the interaction within the triplet structure. Moreover, pipeline approaches usually suffer from the error propagation problem. Recent end-to-end approaches (Wu et al., 2020; Xu et al., 2020b; Zhang et al., 2020) can jointly extract the target and opinion terms and classify their sentiment relation. One drawback is that they heavily rely on word-to-word interactions to predict the sentiment relation for the target-opinion pair. Note that it is common for the aspect targets and opinions to contain multiple words, which accounts for roughly one-third of triplets in the benchmark datasets. However, the previous methods (Wu et al., 2020; Zhang et al., 2020) predict the sentiment polarity for each word-word pair independently, which cannot guarantee their sentiment consistency when fo"
2021.acl-long.367,D12-1122,0,0.334715,"github.com/chiayewken/Span-ASTE. Figure 1, the aspect targets are “Windows 8” and “touchscreen functions”. Aspect Sentiment Classification (ASC) (Dong et al., 2014; Zhang et al., 2016; Yang et al., 2017; Li et al., 2018a; Tang et al., 2019) is one of the most well-explored subtasks of ABSA and aims to predict the sentiment polarity of a given aspect target. However, it is not always practical to assume that the aspect target is provided. Aspect Term Extraction (ATE) (Yin et al., 2016; Li et al., 2018b; Ma et al., 2019) focuses on extracting aspect targets, while Opinion Term Extraction (OTE) (Yang and Cardie, 2012; Klinger and Cimiano, 2013; Yang and Cardie, 2013) aims to extract the opinion terms which largely determine the sentiment polarity of the sentence or the corresponding target term. Aspect Sentiment Triplet Extraction (ASTE) (Peng et al., 2019) is the most recently proposed subtask of ABSA, which forms a more complete picture of the sentiment information through the triplet of an aspect target term, the corresponding opinion term, and the expressed sentiment. For the example in Figure 1, there are two triplets: (“Windows 8”, “not enjoy”, Negative) and (“touchscreen functions”, “not enjoy”, Ne"
2021.acl-long.367,S15-2082,0,0.0734923,"Missing"
2021.acl-long.367,P13-1161,0,0.096781,"ect targets are “Windows 8” and “touchscreen functions”. Aspect Sentiment Classification (ASC) (Dong et al., 2014; Zhang et al., 2016; Yang et al., 2017; Li et al., 2018a; Tang et al., 2019) is one of the most well-explored subtasks of ABSA and aims to predict the sentiment polarity of a given aspect target. However, it is not always practical to assume that the aspect target is provided. Aspect Term Extraction (ATE) (Yin et al., 2016; Li et al., 2018b; Ma et al., 2019) focuses on extracting aspect targets, while Opinion Term Extraction (OTE) (Yang and Cardie, 2012; Klinger and Cimiano, 2013; Yang and Cardie, 2013) aims to extract the opinion terms which largely determine the sentiment polarity of the sentence or the corresponding target term. Aspect Sentiment Triplet Extraction (ASTE) (Peng et al., 2019) is the most recently proposed subtask of ABSA, which forms a more complete picture of the sentiment information through the triplet of an aspect target term, the corresponding opinion term, and the expressed sentiment. For the example in Figure 1, there are two triplets: (“Windows 8”, “not enjoy”, Negative) and (“touchscreen functions”, “not enjoy”, Negative). The initial approach to ASTE (Peng et al.,"
2021.acl-long.367,S14-2004,0,0.288098,"nal efficiency but also distinguishes the opinion and target spans more properly. Our framework simultaneously achieves strong performance for the ASTE as well as ATE and OTE tasks. In particular, our analysis shows that our spanlevel approach achieves more significant improvements over the baselines on triplets with multi-word targets or opinions. 1 1 - Figure 1: An example of ASTE. The spans highlighted in orange are target terms, and the span in blue is opinion term. The “-” on top of target terms indicates negative sentiment. Introduction Aspect-Based Sentiment Analysis (ABSA) (Liu, 2012; Pontiki et al., 2014) is an aggregation of several fine-grained sentiment analysis tasks, and its various subtasks are designed with the aspect target as the fundamental item. For the example in ∗ Equal contribution. Lu Xu and Yew Ken Chia are under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. 1 We make our code publicly available at https:// github.com/chiayewken/Span-ASTE. Figure 1, the aspect targets are “Windows 8” and “touchscreen functions”. Aspect Sentiment Classification (ASC) (Dong et al., 2014; Zhang et al., 2016; Yang et al., 2017; Li et al., 2018a; Tang et al"
2021.acl-long.367,J11-1002,0,0.405283,"as well as the opinion terms with stan2 For example, the joint tag “B-POS” denotes the beginning 4755 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4755–4766 August 1–6, 2021. ©2021 Association for Computational Linguistics dard BIOES 3 tags. The second stage then couples the extracted target and opinion terms to determine their paired sentiment relation. We know that in ABSA, the aspect sentiment is mostly determined by the opinion terms expressed on the aspect target (Qiu et al., 2011; Yang and Cardie, 2012). However, this pipeline approach breaks the interaction within the triplet structure. Moreover, pipeline approaches usually suffer from the error propagation problem. Recent end-to-end approaches (Wu et al., 2020; Xu et al., 2020b; Zhang et al., 2020) can jointly extract the target and opinion terms and classify their sentiment relation. One drawback is that they heavily rely on word-to-word interactions to predict the sentiment relation for the target-opinion pair. Note that it is common for the aspect targets and opinions to contain multiple words, which accounts for"
2021.acl-long.367,2020.findings-emnlp.72,0,0.497985,"4755–4766 August 1–6, 2021. ©2021 Association for Computational Linguistics dard BIOES 3 tags. The second stage then couples the extracted target and opinion terms to determine their paired sentiment relation. We know that in ABSA, the aspect sentiment is mostly determined by the opinion terms expressed on the aspect target (Qiu et al., 2011; Yang and Cardie, 2012). However, this pipeline approach breaks the interaction within the triplet structure. Moreover, pipeline approaches usually suffer from the error propagation problem. Recent end-to-end approaches (Wu et al., 2020; Xu et al., 2020b; Zhang et al., 2020) can jointly extract the target and opinion terms and classify their sentiment relation. One drawback is that they heavily rely on word-to-word interactions to predict the sentiment relation for the target-opinion pair. Note that it is common for the aspect targets and opinions to contain multiple words, which accounts for roughly one-third of triplets in the benchmark datasets. However, the previous methods (Wu et al., 2020; Zhang et al., 2020) predict the sentiment polarity for each word-word pair independently, which cannot guarantee their sentiment consistency when forming a triplet. As a"
2021.acl-long.402,S14-2010,0,0.0652846,"Missing"
2021.acl-long.402,S16-1081,0,0.034663,"Missing"
2021.acl-long.402,S12-1051,0,0.0128858,", 2018), and sentence BERT/RoBERTa (SBERT/SRoBERTa) (Reimers and Gurevych, 2019), which are all trained on the SNLI and MultiNLI datasets. To adapt BSL to a supervised learning setting, we first train a SBERT (SRoBERTa) model and then use the learned weights to initialize the online and target networks of BSL and perform BSL training. We denote this model variant as BSL-SBERT (BSL-SRoBERTa). 4.1 Semantic Textual Similarity (STS) Hyperparameter We tune learning rate, batch size, momentum δ, and the hyperparameter k on SentEval contains a suite of STS datasets including the STS tasks 2012-2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016), the STS benchmark (STSB) (Cer et al., 2017), and the SICK-Relatedness dataset (Marelli et al., 2014). These datasets con1 We use Google translation engine. The datasets are released. 2 Hyperparameters and implementation details are attached in Appendix A 5171 Model STS-12 STS-13 STS-14 STS-15 STS-16 STS-B SICK-R Avg. Unsupervised methods Unigram-TFIDF† SDAE† SkipThought† FastSent† GloVe avg.‡ BERT avg.‡ BERT [CLS]‡ BERT-mlm IS-BERT∗ BERT-flow◦ 55.14 38.78 20.16 48.86 56.77 59.54 70.66 57.98 30.01 64.76 69.24 64.69 58.00 12.00 27.00 63.00 59.73 57.98 20.09 56.97 61.21"
2021.acl-long.402,S13-1004,0,0.0606656,"Missing"
2021.acl-long.402,D15-1075,0,0.0183507,"able to outperform strong multilingual baselines on cross-lingual STS tasks under both unsupervised and supervised settings. Detailed analysis of a few factors that could affect the model performance is provided as well to motivate future research. 2 2.1 Related Work Sentence Representation Learning Prior approaches for sentence representation learning include two main categories – supervised and unsupervised methods, while a few works might leverage on both of them. Most of the supervised methods are trained on labeled natural language inference (NLI) datasets including Stanford NLI (SNLI) (Bowman et al., 2015) and MultiNLI (Williams et al., 2018). Early methods demonstrate good performance on a wide range of tasks (Conneau et al., 2017; Cer et al., 2018). Recently, SBERT (Reimers and Gurevych, 2019) fine-tuned a pre-trained Siamese BERT network on NLI and demonstrated the state-of-the-art performance. Though effective, those methods highly rely on labeled data and could be problematic to port to new domains. Zhang et al. (2020) showed that SBERT generalizes poorly on target tasks with a data distribution significantly different from the NLI data. There are also fruitful outcomes for unsupervised me"
2021.acl-long.402,S17-2001,0,0.167326,"ate the flexibility of the proposed method, we further extend it for learning multilingual sentence representations and evaluate it on cross-lingual STS tasks. Implementation The MLP contains three linear layers. Given an input vector of dimension d, the output dimensions of the three layers are kd → kd → d, where k is a hyperparameter controlling the hidden size. Batch normalization and rectified linear units (ReLU) are applied to the intermediate linear layers. We use BERT-base or RoBERTabase to initialize the online and target networks in monolingual settings. the development set of STS-B (Cer et al., 2017). For all unsupervised experiments, we set learning rate to 5e-4, momentum to 0.999, and k to 8. Adam (Kingma and Ba, 2015) is used as the optimizer. 2 Baselines Under a unsupervised learning setting, we compare to the unigram-TFIDF model, the Sequential Denoising Auto-Encoder (SDAE) (Hill et al., 2016), the Skipthought (Kiros et al., 2015) and FastSent (Hill et al., 2016). Those models are all trained on the Toronto book corpus with 70M sentences (Zhu et al., 2015). We also compare with sentence representations obtained with the average of GloVe embeddings (GloVe avg.), the average of BERT em"
2021.acl-long.402,D18-2029,0,0.0353875,"Missing"
2021.acl-long.402,W19-4330,0,0.0157668,"sentence representations and the gold labels. ρ*100 is reported. Results of baselines are obtained from (Reimers and Gurevych, 2020). ods from (Reimers and Gurevych, 2020): mBERT/ XLM-R-nli-stsb denotes the setting where we fine-tune XLM-R and mBERT on the English NLI and the English training set of the STS benchmark (STS-B); mBERT- /XLM-R ← SBERT-nli-stsb is the knowledge-distillation method proposed in their paper where we learn mBERT and XLM-R to imitate the output of the English SBERT trained on NLI and STS-B with multilingual parallel sentence pairs. We also compared to results of mUSE (Chidambaram et al., 2019) and LaBSE (Feng et al., 2020), which use dual encoder transformer architectures. mUSE was trained on question-answer pairs, SNLI, translated SNLI data, and parallel corpora over 16 languages. LaBSE was trained on 6 billion translation pairs for 109 languages. For BSL, we initialize our online and target networks with the learned weights from XLM-R ← SBERTnli-stsb3 and then perform BSL training in a same way as described above. We denote our model in this setting as BSL-sup. Table 3 presents the results. Under the unsupervised setting, averaging the multilingual token representations yields po"
2021.acl-long.402,2020.acl-main.747,0,0.0833826,"Missing"
2021.acl-long.402,L18-1269,0,0.0597938,"imizing a predefined prediction loss. As for the target network, we apply a stop-gradient strategy (Chen and He, 2020) and update it with a weighted moving average of the online network. Hence, the outputs of the target network are iteratively bootstrapped to serve as targets, enabling enhanced representation learning of the online network while avoiding trivial solutions. Our method is evaluated through extensive experiments. Empirical results show that BSL significantly outperforms strong unsupervised baselines on a standard suite of STS and classification tasks from the SentEval benchmark (Conneau and Kiela, 2018). We also demonstrate that BSL can serve as an effective post-training approach to boost the performance of the state-of-the-art supervised SBERT model. We further extend our method for learning multilingual sentence representations and demonstrate that it is able to outperform strong multilingual baselines on cross-lingual STS tasks under both unsupervised and supervised settings. Detailed analysis of a few factors that could affect the model performance is provided as well to motivate future research. 2 2.1 Related Work Sentence Representation Learning Prior approaches for sentence represent"
2021.acl-long.402,D17-1070,0,0.114622,"tailed analysis of a few factors that could affect the model performance is provided as well to motivate future research. 2 2.1 Related Work Sentence Representation Learning Prior approaches for sentence representation learning include two main categories – supervised and unsupervised methods, while a few works might leverage on both of them. Most of the supervised methods are trained on labeled natural language inference (NLI) datasets including Stanford NLI (SNLI) (Bowman et al., 2015) and MultiNLI (Williams et al., 2018). Early methods demonstrate good performance on a wide range of tasks (Conneau et al., 2017; Cer et al., 2018). Recently, SBERT (Reimers and Gurevych, 2019) fine-tuned a pre-trained Siamese BERT network on NLI and demonstrated the state-of-the-art performance. Though effective, those methods highly rely on labeled data and could be problematic to port to new domains. Zhang et al. (2020) showed that SBERT generalizes poorly on target tasks with a data distribution significantly different from the NLI data. There are also fruitful outcomes for unsupervised methods. Some early studies attempt to learn from the internal structures within each sentence (Socher et al., 2011; Hill et al.,"
2021.acl-long.402,N19-1423,0,0.027073,") showed that SBERT generalizes poorly on target tasks with a data distribution significantly different from the NLI data. There are also fruitful outcomes for unsupervised methods. Some early studies attempt to learn from the internal structures within each sentence (Socher et al., 2011; Hill et al., 2016; Le and Mikolov, 2014) or utilize a distributional hypothesis to encode contextual information with generative (Kiros et al., 2015; Hill et al., 2016) or discriminative objectives (Jernite et al., 2017; Logeswaran and Lee, 2018). Recently, transformer-based networks attract more attentions (Devlin et al., 2019; Liu et al., 2019), however, they do not yield meaningful sentence representations directly without supervised fine-tuning. Reimers and Gurevych (2019) show that sentence embeddings obtained from BERT without fine-tuning even underperform the GloVe embeddings (Pennington et al., 2014) in terms of semantic textual similarity. More recently, a few unsupervised methods were proposed to learn sentence representations from transformer-based networks without supervised fine-tuning. Li et al. (2020) proposes to transform the representation obtained by a pre-trained language model to an isotropic Gau"
2021.acl-long.402,2020.acl-main.740,0,0.0484,"Missing"
2021.acl-long.402,N16-1162,0,0.233056,"s on multiple semantic textual similarity (STS) tasks. However, such performance is mostly induced by high-quality supervision, while labeled data are difficult and ex∗ † Equally Contributed. Corresponding author. pensive to obtain in practice. Zhang et al. (2020) showed that SBERT generalizes poorly on target tasks that differ significantly from NLI on which SBERT is fine-tuned. Many unsupervised methods learn sentence representations by optimizing over various selfsupervised learning (SSL) objectives on a largescale unlabeled corpus. Early works often use auto-encoders (Socher et al., 2011; Hill et al., 2016) or next-sentence prediction (Kiros et al., 2015) for sentence representation learning. Recently, more efforts have been devoted to representation learning with transformer-based networks using masked language modeling (MLM). However, transformer-based methods do not directly produce meaningful sentence representations. Instead, significant supervised fine-tuning steps with labeled data are commonly required to form good representations (Reimers and Gurevych, 2019). Recently, Giorgi et al. (2020) and Zhang et al. (2020) proposed novel transformer-based frameworks to directly learn sentence rep"
2021.acl-long.402,N18-1101,0,0.0144209,"gual baselines on cross-lingual STS tasks under both unsupervised and supervised settings. Detailed analysis of a few factors that could affect the model performance is provided as well to motivate future research. 2 2.1 Related Work Sentence Representation Learning Prior approaches for sentence representation learning include two main categories – supervised and unsupervised methods, while a few works might leverage on both of them. Most of the supervised methods are trained on labeled natural language inference (NLI) datasets including Stanford NLI (SNLI) (Bowman et al., 2015) and MultiNLI (Williams et al., 2018). Early methods demonstrate good performance on a wide range of tasks (Conneau et al., 2017; Cer et al., 2018). Recently, SBERT (Reimers and Gurevych, 2019) fine-tuned a pre-trained Siamese BERT network on NLI and demonstrated the state-of-the-art performance. Though effective, those methods highly rely on labeled data and could be problematic to port to new domains. Zhang et al. (2020) showed that SBERT generalizes poorly on target tasks with a data distribution significantly different from the NLI data. There are also fruitful outcomes for unsupervised methods. Some early studies attempt to"
2021.acl-long.402,2020.emnlp-main.124,1,0.237715,"Among previous approaches, supervised methods achieve state-of-the-art performance by leveraging quality sentence labels. For example, the recently proposed model Sentence-BERT (SBERT) (Reimers and Gurevych, 2019) fine-tunes a Siamese BERT network on natural language inference (NLI) tasks with labeled sentence pairs. It achieves state-of-the-art results on multiple semantic textual similarity (STS) tasks. However, such performance is mostly induced by high-quality supervision, while labeled data are difficult and ex∗ † Equally Contributed. Corresponding author. pensive to obtain in practice. Zhang et al. (2020) showed that SBERT generalizes poorly on target tasks that differ significantly from NLI on which SBERT is fine-tuned. Many unsupervised methods learn sentence representations by optimizing over various selfsupervised learning (SSL) objectives on a largescale unlabeled corpus. Early works often use auto-encoders (Socher et al., 2011; Hill et al., 2016) or next-sentence prediction (Kiros et al., 2015) for sentence representation learning. Recently, more efforts have been devoted to representation learning with transformer-based networks using masked language modeling (MLM). However, transformer"
2021.acl-long.402,2020.emnlp-main.733,0,0.39492,"l., 2017; Logeswaran and Lee, 2018). Recently, transformer-based networks attract more attentions (Devlin et al., 2019; Liu et al., 2019), however, they do not yield meaningful sentence representations directly without supervised fine-tuning. Reimers and Gurevych (2019) show that sentence embeddings obtained from BERT without fine-tuning even underperform the GloVe embeddings (Pennington et al., 2014) in terms of semantic textual similarity. More recently, a few unsupervised methods were proposed to learn sentence representations from transformer-based networks without supervised fine-tuning. Li et al. (2020) proposes to transform the representation obtained by a pre-trained language model to an isotropic Gaussian distribution. Giorgi et al. (2020) minimizes the distance between different spans sampled from the same document. However, it requires an extremely long document of 2,048 tokens as input, which limits its applications to domains with only short documents. Zhang et al. (2020) proposed IS-BERT to maximize the mutual information between the global embedding and local n-gram embeddings of a given sentence. However, IS-BERT requires careful negative sampling and the n-gram embeddings may be s"
2021.acl-long.402,2021.ccl-1.108,0,0.101849,"Missing"
2021.acl-long.402,marelli-etal-2014-sick,0,0.0100444,"NLI datasets. To adapt BSL to a supervised learning setting, we first train a SBERT (SRoBERTa) model and then use the learned weights to initialize the online and target networks of BSL and perform BSL training. We denote this model variant as BSL-SBERT (BSL-SRoBERTa). 4.1 Semantic Textual Similarity (STS) Hyperparameter We tune learning rate, batch size, momentum δ, and the hyperparameter k on SentEval contains a suite of STS datasets including the STS tasks 2012-2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016), the STS benchmark (STSB) (Cer et al., 2017), and the SICK-Relatedness dataset (Marelli et al., 2014). These datasets con1 We use Google translation engine. The datasets are released. 2 Hyperparameters and implementation details are attached in Appendix A 5171 Model STS-12 STS-13 STS-14 STS-15 STS-16 STS-B SICK-R Avg. Unsupervised methods Unigram-TFIDF† SDAE† SkipThought† FastSent† GloVe avg.‡ BERT avg.‡ BERT [CLS]‡ BERT-mlm IS-BERT∗ BERT-flow◦ 55.14 38.78 20.16 48.86 56.77 59.54 70.66 57.98 30.01 64.76 69.24 64.69 58.00 12.00 27.00 63.00 59.73 57.98 20.09 56.97 61.21 64.66 68.25 63.15 36.88 70.86 75.23 72.92 63.66 61.06 38.08 64.65 70.16 71.84 58.02 46.35 16.50 64.33 69.21 58.56 52.00 46.00"
2021.acl-long.402,W16-3636,0,0.0381762,"Missing"
2021.acl-long.402,D14-1162,0,0.0882842,"., 2011; Hill et al., 2016; Le and Mikolov, 2014) or utilize a distributional hypothesis to encode contextual information with generative (Kiros et al., 2015; Hill et al., 2016) or discriminative objectives (Jernite et al., 2017; Logeswaran and Lee, 2018). Recently, transformer-based networks attract more attentions (Devlin et al., 2019; Liu et al., 2019), however, they do not yield meaningful sentence representations directly without supervised fine-tuning. Reimers and Gurevych (2019) show that sentence embeddings obtained from BERT without fine-tuning even underperform the GloVe embeddings (Pennington et al., 2014) in terms of semantic textual similarity. More recently, a few unsupervised methods were proposed to learn sentence representations from transformer-based networks without supervised fine-tuning. Li et al. (2020) proposes to transform the representation obtained by a pre-trained language model to an isotropic Gaussian distribution. Giorgi et al. (2020) minimizes the distance between different spans sampled from the same document. However, it requires an extremely long document of 2,048 tokens as input, which limits its applications to domains with only short documents. Zhang et al. (2020) prop"
2021.acl-long.402,D19-1410,0,0.110046,"opted as a post-training procedure to boost the performance of the supervised methods. We further extend our method for learning multilingual sentence representations and demonstrate its effectiveness on cross-lingual STS tasks. Our code is available at https: //github.com/yanzhangnlp/BSL. 1 Introduction Sentence representation learning aims to map sentences into vectors that capture rich semantic information. Among previous approaches, supervised methods achieve state-of-the-art performance by leveraging quality sentence labels. For example, the recently proposed model Sentence-BERT (SBERT) (Reimers and Gurevych, 2019) fine-tunes a Siamese BERT network on natural language inference (NLI) tasks with labeled sentence pairs. It achieves state-of-the-art results on multiple semantic textual similarity (STS) tasks. However, such performance is mostly induced by high-quality supervision, while labeled data are difficult and ex∗ † Equally Contributed. Corresponding author. pensive to obtain in practice. Zhang et al. (2020) showed that SBERT generalizes poorly on target tasks that differ significantly from NLI on which SBERT is fine-tuned. Many unsupervised methods learn sentence representations by optimizing over"
2021.acl-long.402,2020.emnlp-main.365,0,0.0491899,"ed in the training process. This set of tasks is the common bechmark used to evaluate the transferability of sentence representations on downstream tasks. Table 2 presents the comparison results. On average, BSL outperforms all prior unsupervised baselines. It also outperforms supervised baselines InferSent and USE, and only slightly underperforms SBERT. BSL-SBERT can marginally improve the results of SBERT. BSL-SRoBERTa achieves the best performance. 4.3 Multilingual STS In this subsection, we show that BSL can be easily extended for learning multilingual sentence representations. Following (Reimers and Gurevych, 2020), we conduct evaluation on the multilingual STS 2017 dataset (Cer et al., 2017) which contains annotated pairs for EN-EN, AR-AR, ES-ES, ENAR, EN-ES, EN-TR, EN-DE, and EN-FR. To learn multilingual representations under the unsupervised setting, we process the NLI data as follows. We translate the English NLI sentences to AR, ES, TR, DE and FR using Google translation engine and pair the original English sentence to each of its translations. We obtain 5 pairs (ENAR/ES/TR/DE/FR) from one sentence and treat the English sentence as one view and its translation as the other view. We concatenate all"
2021.acl-long.453,2020.repl4nlp-1.1,0,0.131258,"Missing"
2021.acl-long.453,2021.acl-long.154,1,0.628573,"Bosheng Ding are under the Joint PhD Program between Alibaba and Nanyang Technological University. 1 Our code is available at https://ntunlpsg. github.io/project/mulda/. (e.g., English, German), training sets for most of the other languages are still very limited. Moreover, it is usually expensive and time-consuming to annotate such data, particularly for low-resource languages (Kruengkrai et al., 2020). Therefore, zero-shot cross-lingual NER has attracted growing interest recently, especially with the influx of deep learning methods (Mayhew et al., 2017; Joty et al., 2017; Jain et al., 2019; Bari et al., 2021). Existing approaches to cross-lingual NER can be roughly grouped into two main categories: instance-based transfer via machine translation (MT) and label projection (Mayhew et al., 2017; Jain et al., 2019), and model-based transfer with aligned cross-lingual word representations or pretrained multilingual language models (Joty et al., 2017; Baumann, 2019; Wang et al., 2020; Conneau et al., 2020; Bari et al., 2021). Recently, Wu et al. (2020) unify instance-based and model-based transfer via knowledge distillation. These recent methods have demonstrated promising zero-shot cross-lingual NER pe"
2021.acl-long.453,R19-2004,0,0.0242052,"resource languages (Kruengkrai et al., 2020). Therefore, zero-shot cross-lingual NER has attracted growing interest recently, especially with the influx of deep learning methods (Mayhew et al., 2017; Joty et al., 2017; Jain et al., 2019; Bari et al., 2021). Existing approaches to cross-lingual NER can be roughly grouped into two main categories: instance-based transfer via machine translation (MT) and label projection (Mayhew et al., 2017; Jain et al., 2019), and model-based transfer with aligned cross-lingual word representations or pretrained multilingual language models (Joty et al., 2017; Baumann, 2019; Wang et al., 2020; Conneau et al., 2020; Bari et al., 2021). Recently, Wu et al. (2020) unify instance-based and model-based transfer via knowledge distillation. These recent methods have demonstrated promising zero-shot cross-lingual NER performance. However, most of them assume the availability of a considerable amount of training data in the source language. When we reduce the size of the training data, we observe significant performance decrease. For instance-based transfer, decreasing training set size also amplifies the negative impact of the noise introduced by MT and label projection"
2021.acl-long.453,D18-1366,0,0.0226682,"projection quality 5841 with additional feature or better mapping methods (Tsai et al., 2016; Li et al., 2020). Different from these methods, our labeled sentence translation approach leverages placeholders to determine the position of entities after translation, which effectively avoids many issues during label projection, such as word order change, entity span determination, noise-sensitive similarity metrics and so on. Model-based transfer directly applies the model trained on the source language to the targetlanguage test data (T¨ackstr¨om et al., 2012; Ni et al., 2017; Joty et al., 2017; Chaudhary et al., 2018), which heavily relies on the quality of cross-lingual representations. Recent methods have achieved significant performance improvement by fine-tuning large scale pretrained multilingual LMs (Devlin et al., 2019; Keung et al., 2019; Conneau et al., 2020). Besides, there are also some approaches that combine instance-based and model-based transfer (Xu et al., 2020; Wu et al., 2020). Compared with these methods, our approach leverages MT models and LMs to add more diversity to the training data, and prevents over-fitting on language-specific features by fine-tuning NER models on multilingual da"
2021.acl-long.453,2020.acl-main.747,0,0.173144,"Missing"
2021.acl-long.453,N19-1423,0,0.0260841,"ine the position of entities after translation, which effectively avoids many issues during label projection, such as word order change, entity span determination, noise-sensitive similarity metrics and so on. Model-based transfer directly applies the model trained on the source language to the targetlanguage test data (T¨ackstr¨om et al., 2012; Ni et al., 2017; Joty et al., 2017; Chaudhary et al., 2018), which heavily relies on the quality of cross-lingual representations. Recent methods have achieved significant performance improvement by fine-tuning large scale pretrained multilingual LMs (Devlin et al., 2019; Keung et al., 2019; Conneau et al., 2020). Besides, there are also some approaches that combine instance-based and model-based transfer (Xu et al., 2020; Wu et al., 2020). Compared with these methods, our approach leverages MT models and LMs to add more diversity to the training data, and prevents over-fitting on language-specific features by fine-tuning NER models on multilingual data. Data augmentation Data augmentation (Simard et al., 1998) adds more diversity to training data to help improve model generalization, which has been widely used in many fields, such as computer vision (Zhang e"
2021.acl-long.453,2020.emnlp-main.488,1,0.922928,"e than 100 languages. Alternatively, there are also many pretrained MT models conveniently accessible, e.g., more than 1,000 MarianMT (Junczys-Dowmunt et al., 2018; Kim et al., 2019) models have been released on the Hugging Face model hub.3 Note that the instance-based transfer methods add limited semantic variety to the training set, since they only translate entities and the corresponding contexts to a different language. In contrast, data augmentation has been proven to be a successful method for tackling the data scarcity problem. Inspired by a recent monolingual data augmentation method (Ding et al., 2020), we propose a generation-based multilingual data augmentation method to increase the diversity, where LMs are trained on multilingual labeled data and then used to generate more synthetic training data. We conduct extensive experiments and analysis to verify the effectiveness of our methods. Our main contributions can be summarized as follows: augmentation method for NER, which leverages the multilingual language models to add more diversity to the training data. • Through empirical experiments, we observe that when fine-tuning pretrained multilingual LMs for low-resource cross-lingual NER, t"
2021.acl-long.453,2020.acl-main.413,0,0.0189525,"tic data and the language-independent features from multilingual synthetic data. An extensive set of experiments were conducted to demonstrate encouraging cross-lingual transfer performance of the new research on a wide variety of target languages.1 1 Introduction Named entity recognition (NER) aims to identify and classify entities in a text into predefined types, which is an essential tool for information extraction. It has also been proven to be useful in various downstream natural language processing (NLP) tasks, including information retrieval (Banerjee et al., 2019), question answering (Fabbri et al., 2020) and text summarization (Nallapati et al., 2016). However, except for some resource-rich languages ∗ Equal contribution, order decided by coin flip. Linlin Liu and Bosheng Ding are under the Joint PhD Program between Alibaba and Nanyang Technological University. 1 Our code is available at https://ntunlpsg. github.io/project/mulda/. (e.g., English, German), training sets for most of the other languages are still very limited. Moreover, it is usually expensive and time-consuming to annotate such data, particularly for low-resource languages (Kruengkrai et al., 2020). Therefore, zero-shot cross-l"
2021.acl-long.453,D19-1100,0,0.150896,"ip. Linlin Liu and Bosheng Ding are under the Joint PhD Program between Alibaba and Nanyang Technological University. 1 Our code is available at https://ntunlpsg. github.io/project/mulda/. (e.g., English, German), training sets for most of the other languages are still very limited. Moreover, it is usually expensive and time-consuming to annotate such data, particularly for low-resource languages (Kruengkrai et al., 2020). Therefore, zero-shot cross-lingual NER has attracted growing interest recently, especially with the influx of deep learning methods (Mayhew et al., 2017; Joty et al., 2017; Jain et al., 2019; Bari et al., 2021). Existing approaches to cross-lingual NER can be roughly grouped into two main categories: instance-based transfer via machine translation (MT) and label projection (Mayhew et al., 2017; Jain et al., 2019), and model-based transfer with aligned cross-lingual word representations or pretrained multilingual language models (Joty et al., 2017; Baumann, 2019; Wang et al., 2020; Conneau et al., 2020; Bari et al., 2021). Recently, Wu et al. (2020) unify instance-based and model-based transfer via knowledge distillation. These recent methods have demonstrated promising zero-shot"
2021.acl-long.453,P18-4020,0,0.024355,"Missing"
2021.acl-long.453,D19-1138,0,0.0154432,"ntities after translation, which effectively avoids many issues during label projection, such as word order change, entity span determination, noise-sensitive similarity metrics and so on. Model-based transfer directly applies the model trained on the source language to the targetlanguage test data (T¨ackstr¨om et al., 2012; Ni et al., 2017; Joty et al., 2017; Chaudhary et al., 2018), which heavily relies on the quality of cross-lingual representations. Recent methods have achieved significant performance improvement by fine-tuning large scale pretrained multilingual LMs (Devlin et al., 2019; Keung et al., 2019; Conneau et al., 2020). Besides, there are also some approaches that combine instance-based and model-based transfer (Xu et al., 2020; Wu et al., 2020). Compared with these methods, our approach leverages MT models and LMs to add more diversity to the training data, and prevents over-fitting on language-specific features by fine-tuning NER models on multilingual data. Data augmentation Data augmentation (Simard et al., 1998) adds more diversity to training data to help improve model generalization, which has been widely used in many fields, such as computer vision (Zhang et al., 2018), speech"
2021.acl-long.453,N18-2072,0,0.0234329,"n Data augmentation (Simard et al., 1998) adds more diversity to training data to help improve model generalization, which has been widely used in many fields, such as computer vision (Zhang et al., 2018), speech (Cui et al., 2015; Park et al., 2019), NLP (Wang and Eisner, 2016; Sun et al., 2020) and so on. For NLP, back translation (Sennrich et al., 2016) is one of the most successful data augmentation approaches, which translates target-language monolingual data to the source language to generate more parallel data for MT model training. Other popular approaches include synonym replacement (Kobayashi, 2018), random deletion/swap/insertion (Sun et al., 2020; Kumar et al., 2020), generation (Ding et al., 2020), etc. Data augmentation has also been proven to be useful in the cross-lingual settings (Zhang et al., 2019; Singh et al., 2020; Riabi et al., 2020; Qin et al., 2020; Bari et al., 2021; Mohiuddin et al., 2021), but most of the exiting methods overlook the better utilization of multilingual training data when such resources are available. 5 Conclusions We have proposed a multilingual data augmentation framework for low resource cross-lingual NER. Our labeled sequence translation method effect"
2021.acl-long.453,P19-1553,0,0.0173464,"English NER performance. Particularly, En + Multi-Tran achieves the best performance. Therefore, we can also use multilingual translated data to improve low-resource monolingual NER performance. 3.3 Generation-based Multilingual Data Augmentation In this section, we run experiments to verify whether applying generation-based data augmentation methods to the multilingual translated data can further improve cross-lingual performance in the low resource scenarios. Experimental settings We follow the steps described in §2.2 to implement the proposed data augmentation framework on top of LSTM-LM (Kruengkrai, 2019) and mBART (Liu et al., 2020) sep5839 500 Method 1k 2k de es nl avg de es nl avg de es nl avg En + Multi-Tran MulDA-LSTM MulDA-mBART 70.40 70.04 72.37 65.70 67.38 68.19 72.20 72.81 74.59 69.43 70.08 71.72 73.42 74.80 75.04 72.71 74.27 74.56 76.74 77.21 77.78 74.29 75.42 75.79 75.91 76.05 77.54 76.04 76.05 76.32 77.85 78.46 78.21 76.60 76.85 77.36 En + Tgt-Tran BiDA-LSTM 69.16 72.51 64.57 68.77 71.40 72.65 68.38 71.31 73.63 74.97 69.81 73.69 75.83 77.51 73.09 75.39 74.45 76.59 75.88 76.47 78.40 78.97 76.24 77.34 Table 4: Cross-lingual NER results of models trained on multilingual augmented data"
2021.acl-long.453,2020.acl-main.523,1,0.729419,"t al., 2019), question answering (Fabbri et al., 2020) and text summarization (Nallapati et al., 2016). However, except for some resource-rich languages ∗ Equal contribution, order decided by coin flip. Linlin Liu and Bosheng Ding are under the Joint PhD Program between Alibaba and Nanyang Technological University. 1 Our code is available at https://ntunlpsg. github.io/project/mulda/. (e.g., English, German), training sets for most of the other languages are still very limited. Moreover, it is usually expensive and time-consuming to annotate such data, particularly for low-resource languages (Kruengkrai et al., 2020). Therefore, zero-shot cross-lingual NER has attracted growing interest recently, especially with the influx of deep learning methods (Mayhew et al., 2017; Joty et al., 2017; Jain et al., 2019; Bari et al., 2021). Existing approaches to cross-lingual NER can be roughly grouped into two main categories: instance-based transfer via machine translation (MT) and label projection (Mayhew et al., 2017; Jain et al., 2019), and model-based transfer with aligned cross-lingual word representations or pretrained multilingual language models (Joty et al., 2017; Baumann, 2019; Wang et al., 2020; Conneau et"
2021.acl-long.453,2020.lifelongnlp-1.3,0,0.0280795,"ining data to help improve model generalization, which has been widely used in many fields, such as computer vision (Zhang et al., 2018), speech (Cui et al., 2015; Park et al., 2019), NLP (Wang and Eisner, 2016; Sun et al., 2020) and so on. For NLP, back translation (Sennrich et al., 2016) is one of the most successful data augmentation approaches, which translates target-language monolingual data to the source language to generate more parallel data for MT model training. Other popular approaches include synonym replacement (Kobayashi, 2018), random deletion/swap/insertion (Sun et al., 2020; Kumar et al., 2020), generation (Ding et al., 2020), etc. Data augmentation has also been proven to be useful in the cross-lingual settings (Zhang et al., 2019; Singh et al., 2020; Riabi et al., 2020; Qin et al., 2020; Bari et al., 2021; Mohiuddin et al., 2021), but most of the exiting methods overlook the better utilization of multilingual training data when such resources are available. 5 Conclusions We have proposed a multilingual data augmentation framework for low resource cross-lingual NER. Our labeled sequence translation method effectively avoids many label projection related problems by leveraging place"
2021.acl-long.453,2020.tacl-1.47,0,0.298836,"based transfer via knowledge distillation. These recent methods have demonstrated promising zero-shot cross-lingual NER performance. However, most of them assume the availability of a considerable amount of training data in the source language. When we reduce the size of the training data, we observe significant performance decrease. For instance-based transfer, decreasing training set size also amplifies the negative impact of the noise introduced by MT and label projection. For model-based transfer, although the large-scale pretrained multilingual language models (LM) (Conneau et al., 2020; Liu et al., 2020) have achieved state-of-the-art performance on many cross-lingual transfer tasks, simply fine-tuning them on a small training set is prone to over-fitting (Wu et al., 2018; Si et al., 2020; Kou et al., 2020). To address the above problems under the setting of low-resource cross-lingual NER, we propose a multilingual data augmentation (MulDA) framework to make better use of the cross-lingual 5834 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5834–5846 August 1–6, 2021. ©20"
2021.acl-long.453,D17-1269,0,0.046708,"Missing"
2021.acl-long.453,2021.findings-acl.267,1,0.434557,"n. For NLP, back translation (Sennrich et al., 2016) is one of the most successful data augmentation approaches, which translates target-language monolingual data to the source language to generate more parallel data for MT model training. Other popular approaches include synonym replacement (Kobayashi, 2018), random deletion/swap/insertion (Sun et al., 2020; Kumar et al., 2020), generation (Ding et al., 2020), etc. Data augmentation has also been proven to be useful in the cross-lingual settings (Zhang et al., 2019; Singh et al., 2020; Riabi et al., 2020; Qin et al., 2020; Bari et al., 2021; Mohiuddin et al., 2021), but most of the exiting methods overlook the better utilization of multilingual training data when such resources are available. 5 Conclusions We have proposed a multilingual data augmentation framework for low resource cross-lingual NER. Our labeled sequence translation method effectively avoids many label projection related problems by leveraging placeholders during MT. Our generation-based multilingual data augmentation method generates high quality synthetic training data to add more diversity. The proposed framework has demonstrated encouraging performance improvement in various low-res"
2021.acl-long.453,K16-1028,0,0.0200591,"Missing"
2021.acl-long.453,P17-1135,0,0.0321719,"Missing"
2021.acl-long.453,P17-1178,0,0.0256521,"g, while BiDA-LSTM trains one model for each target language in each setting. Therefore, we compare BiDA-LSTM with 8 https://github.com/pytorch/fairseq/blob/master/ examples/mbart/README.md En + Tgt-Tran only. As we can see, the proposed multilingual data augmentation methods further improve cross-lingual NER performance consistently. For the 1k and 2k setting, MulDA-LSTM achieves comparable average performance as BiDA-LSTM. 3.4 Evaluation on More Distant Languages We evaluate the proposed method on a wider range of target languages in this section. Experimental settings The Wikiann NER data (Pan et al., 2017) processed by Hu et al. (2020) is used in these experiments. 1k English sentences S ) are sampled from the gold train data to sim(D1k ulate the low resource scenarios. We also assume MT models are not available for all of the target languages, so we only translate the sampled English sentences to 6 target languages: ar, fr, it, ja, T tr and zh. Dtrans is used to denote the translated target-language sentences by following steps described in §2.1. The low quality translated sentences are filtered out in the same way as §3.2. To evaluate our method in the semi-supervised setting, we also sample"
2021.acl-long.453,2020.coling-main.305,0,0.0361386,"t combine instance-based and model-based transfer (Xu et al., 2020; Wu et al., 2020). Compared with these methods, our approach leverages MT models and LMs to add more diversity to the training data, and prevents over-fitting on language-specific features by fine-tuning NER models on multilingual data. Data augmentation Data augmentation (Simard et al., 1998) adds more diversity to training data to help improve model generalization, which has been widely used in many fields, such as computer vision (Zhang et al., 2018), speech (Cui et al., 2015; Park et al., 2019), NLP (Wang and Eisner, 2016; Sun et al., 2020) and so on. For NLP, back translation (Sennrich et al., 2016) is one of the most successful data augmentation approaches, which translates target-language monolingual data to the source language to generate more parallel data for MT model training. Other popular approaches include synonym replacement (Kobayashi, 2018), random deletion/swap/insertion (Sun et al., 2020; Kumar et al., 2020), generation (Ding et al., 2020), etc. Data augmentation has also been proven to be useful in the cross-lingual settings (Zhang et al., 2019; Singh et al., 2020; Riabi et al., 2020; Qin et al., 2020; Bari et al"
2021.acl-long.453,N12-1052,0,0.0914152,"Missing"
2021.acl-long.453,2021.naacl-main.282,1,0.721511,".1 Example 2 Gold EN: . . . (LOC U.S. Midwest) . . . Jain et al. (2019): . . . (LOC Mittlerer Westen) der (LOC USA) ... Li et al. (2020): . . . Mittlerer (LOC Westen) der (LOC USA) ... Ours: . . . (LOC Mittlerer Westen der USA) . . . Figure 6: Two examples that the previous methods fail to find the correct entity boundaries. Figure 7: Examples of multilingual sentences. the NER tags can be viewed as a shared vocabulary between different languages. As a result, we find that some generated sentences contain tokens from multiple languages, which are useful to help improve cross-lingual transfer (Tan and Joty, 2021). Two examples are shown in Figure 7. 4 Case Study Effectiveness in Label Projection The label projection step of the previous methods needs to locate the entities and determine their boundaries, which is vulnerable to many problems, such as word order change, long entities, etc. Our method effectively avoids these problems with placeholders. In the two examples shown in Figure 6, Jain et al. (2019) either labeled only part of the whole entity or incorrectly split the entity into two, Li et al. (2020) incorrectly split the entities into two in both examples, while our method can correctly map"
2021.acl-long.453,W14-1614,0,0.0595736,"Missing"
2021.acl-long.453,W02-2024,0,0.263791,"forward layer to the Transformer final layer for label classification. Specifically, to demonstrate that our framework can help achieve additional performance gain even on the top of the state-of-the-art multilingual LMs, the checkpoint of the pretrained XLM-R large (Conneau et al., 2020) model is used to initialize our NER models. 3.1 Labeled Sequence Translation We finetune the NER model on the translated targetlanguage data to compare our labeled sequence translation method (§2.1) with the existing instancebased transfer methods. Experimental settings The CoNLL02/03 NER dataset (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) is used for evaluation, which contains data in four different languages: English, German, Dutch and Spanish. All of the data are annotated with the same set of NER tags. We follow the steps described in §2.1 to translate En7 Similar to the token classification https://github.com/huggingface/transformers. model in glish train data to the other three languages. Following Jain et al. (2019) and Li et al. (2020), Google translation system is used in the experiments. Since our NER model is more powerful than those used by Jain et al. (2019) and Li et al. (2020"
2021.acl-long.453,K16-1022,0,0.0163074,"t language, and then apply label projection to annotate the translated data (Tiedemann et al., 2014; Jain et al., 2019). Instead of MT, some earlier approaches also use parallel corpora to construct pseudo training data in the target language (Yarowsky et al., 2001; Fu et al., 2014). To minimize resource requirement, Mayhew et al. (2017) and Xie et al. (2018) design frameworks that only rely on word-to-word/phrase-to-phrase translation with bilingual dictionaries. Besides, there are also many studies on improving label projection quality 5841 with additional feature or better mapping methods (Tsai et al., 2016; Li et al., 2020). Different from these methods, our labeled sentence translation approach leverages placeholders to determine the position of entities after translation, which effectively avoids many issues during label projection, such as word order change, entity span determination, noise-sensitive similarity metrics and so on. Model-based transfer directly applies the model trained on the source language to the targetlanguage test data (T¨ackstr¨om et al., 2012; Ni et al., 2017; Joty et al., 2017; Chaudhary et al., 2018), which heavily relies on the quality of cross-lingual representation"
2021.acl-long.453,Q16-1035,0,0.0256703,"lso some approaches that combine instance-based and model-based transfer (Xu et al., 2020; Wu et al., 2020). Compared with these methods, our approach leverages MT models and LMs to add more diversity to the training data, and prevents over-fitting on language-specific features by fine-tuning NER models on multilingual data. Data augmentation Data augmentation (Simard et al., 1998) adds more diversity to training data to help improve model generalization, which has been widely used in many fields, such as computer vision (Zhang et al., 2018), speech (Cui et al., 2015; Park et al., 2019), NLP (Wang and Eisner, 2016; Sun et al., 2020) and so on. For NLP, back translation (Sennrich et al., 2016) is one of the most successful data augmentation approaches, which translates target-language monolingual data to the source language to generate more parallel data for MT model training. Other popular approaches include synonym replacement (Kobayashi, 2018), random deletion/swap/insertion (Sun et al., 2020; Kumar et al., 2020), generation (Ding et al., 2020), etc. Data augmentation has also been proven to be useful in the cross-lingual settings (Zhang et al., 2019; Singh et al., 2020; Riabi et al., 2020; Qin et al"
2021.acl-long.453,D18-1034,0,0.473471,"anslation method to translate the source ping with alignment models or algorithms. Howtraining data to a desired language. Compared with exiting methods, our labeled sentence trans- ever, these methods suffer from a few label projeclation approach leverages placeholders for la- tion problems, such as word order change, wordspan determination (Li et al., 2020), and so on. An bel projection, which effectively avoids many alternative to avoid the label projection problems issues faced during word alignment, such as word order change, entity span determination, noise- is word-by-word translation (Xie et al., 2018), but often at the sacrifice of the translation quality. sensitive similarity metrics and so on. We address the problems identified above by • We propose a generation-based multilingual data first replacing named entities with contextual place2 holders before sentence translation, and then after https://cloud.google.com/translate 3 https://huggingface.co/transformers/model doc/marian.html translation, we replace placeholders in translated 5835 B-PER E-PER O O O S-LOC O Jamie Valentine was born in London . Labeled sentence in the source language: [PER Jamie Valentine] was born in [LOC London]."
2021.acl-long.453,D19-1092,0,0.0452794,"Missing"
2021.acl-long.453,2020.emnlp-main.410,0,0.0245173,"Missing"
2021.acl-long.453,H01-1035,0,0.13327,"k into the data generated by our multilingual data augmentation method. During LM training, Related Work Cross-lingual NER There has been growing interest in cross-lingual NER. Prior approaches can be grouped into two main categories, instancebased transfer and model-based transfer. Instancebased transfer translates source-language training data to target language, and then apply label projection to annotate the translated data (Tiedemann et al., 2014; Jain et al., 2019). Instead of MT, some earlier approaches also use parallel corpora to construct pseudo training data in the target language (Yarowsky et al., 2001; Fu et al., 2014). To minimize resource requirement, Mayhew et al. (2017) and Xie et al. (2018) design frameworks that only rely on word-to-word/phrase-to-phrase translation with bilingual dictionaries. Besides, there are also many studies on improving label projection quality 5841 with additional feature or better mapping methods (Tsai et al., 2016; Li et al., 2020). Different from these methods, our labeled sentence translation approach leverages placeholders to determine the position of entities after translation, which effectively avoids many issues during label projection, such as word o"
2021.acl-long.461,N18-2095,0,0.147027,"such valuable data, review helpfulness prediction has gained increasing interest from both academia and industry communities. Earlier review helpfulness prediction methods rely on a wide range of handcrafted features, such as semantic features (Yang et al., 2015), lexical features (Martin and Pu, 2014), and argument based features (Liu et al., 2017), to train a classifier. The success of these methods generally relies heavily on feature engineering which is labor-intensive and highlights the weakness of conventional machine learning methods. In recent years, deep neural networks such as CNN (Chen et al., 2018, 2019) and LSTM (Fan et al., 2019) have become dominant in the literature due to their powerful performance for helpfulness prediction by learning text representation automatically. Note that these existing works on review helpfulness prediction mainly focus on the pure textual data. As multimodal data become increasingly popular in online reviews, Multimodal Review Analysis (MRA) has become a valuable research direction. In this paper, we propose the Multimodal Review Helpfulness Prediction (MRHP) task which aims at exploring multimodal clues that often convey comprehensive information for r"
2021.acl-long.461,P18-1065,0,0.0201638,"review and the coherence between the target product and the review. 3 Methodology The overall architecture of our MCR method is illustrated in Figure 1. Our multi-perspective coherent reasoning consists of two perspectives of coherence: (i) the intra- and inter-modal coherence between a review and the target product and (ii) the intra-review coherence between the text content and images in the review. In the following sections, we will provide the problem definition of review helpfulness prediction and introduce each component of our MCR model in detail. 3.1 Problem Definition As mentioned by Diaz and Ng (2018), we formulate the multimodal review helpfulness prediction problem as a ranking task. Specifically, given a product item Pi consisting of product related information pi and an associated review set Ri = {ri,1 , · · · , ri,N }, where N is the number of reviews for pi . Each review has a scalar label si,j ∈ {0, · · · , S} indicating the helpfulness score of the review ri,j . The ground-truth ranking of Ri is the descending sort order determined by the helpfulness scores. The goal of review helpfulness prediction is to predict helpfulness scores for Ri which can rank the set of reviews Ri into t"
2021.acl-long.461,W06-1650,0,0.478147,"lness prediction of multimodal reviews. To facilitate research in this area, we will release the datasets and source code proposed in this paper, which would push forward the research in this field. (4) Extensive experiments on two collected datasets demonstrate that our MCR method significantly outperforms other methods. 2 Related Work Most conventional approaches on review helpfulness prediction focus solely on the text of reviews, which can be generally divided into two categories based on the way of extracting predictive features: machine learning based methods with hand-crafted features (Kim et al., 2006; Krishnamoorthy, 2015) Product Information Teflon Pans 1 Set of 3 pcs 1042-Non-stick Set of 3 Review 1 (Helpfulness Score: 2) Overall, it is quite satisfactory. Thanks to the seller. Review 2 (Helpfulness Score: 4) For that price, it is more than satisfactory, even though there are a few scratches in the pan and the small frying pan, the package is very neat, the frying pan has been used as if it‘s a little burnt, it looks like it can’t stand the heat, but overall I like it. Review 3 (Helpfulness Score: 0) Recommend for the price. Yes, the package is neat but the pan has scratched. It is unfo"
2021.acl-long.461,D14-1181,0,0.0038695,"Ir . 3.2 Feature Representation Given a text (Tp or Tr ) consisting of lT text tokens {w1 , · · · , wlT } and an image set (Ip or Ir ), we adopt a convolutional neural network to learn the contextualized text representation. Meanwhile, we use a self-attention mechanism on image region features to obtain the image representations. To prevent conceptual confusion, we use the subscripts p and r to indicate variables that are related to the product and the review, respectively. Text Representation Inspired by the great success of convolutional neural network (CNN) in natural language processing (Kim, 2014; Dai et al., 2018), we also apply CNN to learn the text representation. First, we convert each token wi in a review into an embedding vector wi ∈ Rd via an embedding layer. Then, we pass the learned word embeddings to a one-dimensional CNN so as to extract multi-gram representations. Specifically, the k-gram CNN transforms the token embedding vectors wi into k-gram representations Hk : Hk = CNNk ({w1 , · · · , wlT }), (2) where k ∈ {1, · · · , kmax } represents the kernel size. kmax represents the maximum kernel size. Hk ∈ RlT ×dT is the k-gram representation. All the k-gram representations a"
2021.acl-long.461,2020.acl-main.349,0,0.0333102,"CNN (EGCNN) (Chen et al., 2018), Convolutional Kernelbased Neural Ranking Model (Conv-KNRM) (Dai et al., 2018), the Product-aware Helpfulness Prediction Network (PRHNet) (Fan et al., 2019). We are the first to leverage images in the re2 3 http://nlp.stanford.edu/data/glove.6B.zip https://fasttext.cc/docs/en/crawl-vectors.html view for helpfulness prediction of multimodal reviews, thereby we compare our MCR model with two strong multimodal reasoning techniques: SSE-Cross (Abavisani et al., 2020) that leverages stochastic shared embedding to fuse different modality representations and D&R Net (Xu et al., 2020) that adopts a decomposition and relation network to model both cross-modality contrast and semantic association. 4.4 Evaluation Metrics In this paper, we propose a pairwise ranking loss function for review helpfulness prediction, which fully benefits from the sampling of informative negative examples. Since the output of MCR is a list of reviews ranked by their helpfulness scores, we adopt two authoritative ranking-based metrics to evaluate the model performance: Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (NDCG@N) (J¨arvelin and Kek¨al¨ainen, 2017). Here, the value"
2021.acl-long.461,P15-2007,0,0.0217832,"t encourage consumers to share their opinions and experiences. However, the user-generated reviews vary a lot in their qualities, and we are continuously bombarded with ever-growing, noise information. Therefore, it is critical to examine the quality of reviews and present consumers with useful reviews. Motivated by the demand of gleaning insights from such valuable data, review helpfulness prediction has gained increasing interest from both academia and industry communities. Earlier review helpfulness prediction methods rely on a wide range of handcrafted features, such as semantic features (Yang et al., 2015), lexical features (Martin and Pu, 2014), and argument based features (Liu et al., 2017), to train a classifier. The success of these methods generally relies heavily on feature engineering which is labor-intensive and highlights the weakness of conventional machine learning methods. In recent years, deep neural networks such as CNN (Chen et al., 2018, 2019) and LSTM (Fan et al., 2019) have become dominant in the literature due to their powerful performance for helpfulness prediction by learning text representation automatically. Note that these existing works on review helpfulness prediction"
2021.acl-long.461,D17-1142,0,0.0722464,"d reviews vary a lot in their qualities, and we are continuously bombarded with ever-growing, noise information. Therefore, it is critical to examine the quality of reviews and present consumers with useful reviews. Motivated by the demand of gleaning insights from such valuable data, review helpfulness prediction has gained increasing interest from both academia and industry communities. Earlier review helpfulness prediction methods rely on a wide range of handcrafted features, such as semantic features (Yang et al., 2015), lexical features (Martin and Pu, 2014), and argument based features (Liu et al., 2017), to train a classifier. The success of these methods generally relies heavily on feature engineering which is labor-intensive and highlights the weakness of conventional machine learning methods. In recent years, deep neural networks such as CNN (Chen et al., 2018, 2019) and LSTM (Fan et al., 2019) have become dominant in the literature due to their powerful performance for helpfulness prediction by learning text representation automatically. Note that these existing works on review helpfulness prediction mainly focus on the pure textual data. As multimodal data become increasingly popular in"
2021.acl-long.461,W02-0109,0,0.0364771,"Missing"
2021.acl-long.461,D19-1018,0,0.0512185,"Missing"
2021.acl-short.64,W18-6217,0,0.458705,"Missing"
2021.acl-short.64,D17-1047,1,0.84049,"ectively. ∗ Work done when Wenxuan Zhang was an intern at Alibaba. The work described in this paper is partially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14200719). 1 The data and code can be found at https://github. com/IsakZhang/Generative-ABSA The main research line of ABSA focuses on the identification of those sentiment elements such as extracting the aspect term (Liu et al., 2015; Yin et al., 2016; Li et al., 2018; Ma et al., 2019) or classifying the sentiment polarity for a given aspect (Wang et al., 2016; Chen et al., 2017; Jiang et al., 2019; Zhang and Qian, 2020). To provide more detailed information, many recent studies propose to jointly predict multiple elements simultaneously (Li et al., 2019a; Wan et al., 2020; Peng et al., 2020; Zhao et al., 2020). Taking the Unified ABSA (UABSA, also called End-to-End ABSA) task as an example, it tries to simultaneously predict the mentioned aspect terms and the corresponding sentiment polarities (Luo et al., 2019; He et al., 2019). In general, most ABSA tasks are formulated as either sequence-level or token-level classification problems (Li et al., 2019b). By designin"
2021.acl-short.64,2020.acl-main.582,0,0.817037,"a pre-defined category set. For example, 2.2 Generation Model R14 R15 R16 JERE-MHS† SpanMlt (Zhao et al., 2020) SDRN (Chen et al., 2020) 53.41 52.34 68.66 66.18 62.39 66.02 75.60 73.30 58.12 59.64 64.68 65.75 63.84 67.65 71.78 73.67 G AS -A NNOTATION -R G AS -E XTRACTION -R G AS -A NNOTATION G AS -E XTRACTION 68.74 67.58 69.55 68.08 72.66 73.22 75.15 74.12 65.03 65.83 67.93 67.19 73.75 74.12 75.42 74.54 Table 1: Main results of the AOPE task. The best results are in bold, second best results are underlined. Results are the average F1 scores over 5 runs. † denotes results are from Zhao et al. (2020). Input: A big disappointment, all around. The pizza was cold and the cheese wasn’t even fully melted. Target (Annotation-style): A big disappointment, all around. The [pizza |food quality |negative] was cold and the [cheese | food quality |negative] wasn’t even fully melted [null |restaurant general |negative]. Target (Extraction-style): (pizza, food quality, negative); (cheese, food quality, negative); (null, restaurant general, negative); Similarly, we pack each aspect term, the aspect category it belongs to, and its sentiment polarity into a bracket to build the target sentence for the ann"
2021.acl-short.64,2020.acl-main.340,0,0.697434,"pinion terms, we append the associated opinion modifier to each aspect term in the form of [aspect |opinion] for constructing the target sentence, as shown in the above example. The prediction of the coupled aspect and opinion term is thus achieved by including them in the same bracket. For the extraction-style paradigm, we treat the desired pairs as the target, which resembles direct extraction of the expected sentiment elements but in a generative manner. Unified ABSA (UABSA) is the task of extracting aspect terms and predicting their sentiment polarities at the same time (Li et al., 2019a; Chen and Qian, 2020). We also formulate it as an (aspect, sentiment polarity) pair extraction problem. For the same example given above, we aim to extract two pairs: (Salads, positive) and (server, positive). Similarly, we replace each aspect term as [aspect | sentiment polarity] under the annotation-style formulation and treat the desired pairs as the target output in the extraction-style paradigm to reformulate the UABSA task as a text generation problem. Aspect Sentiment Triplet Extraction (ASTE) aims to discover more complicated (aspect, opinion, sentiment polarity) triplets (Peng et al., 2020): Input: The Un"
2021.acl-short.64,N19-1259,0,0.0145666,"size of 16 and accumulate gradients every two batches. The learning rate is set to be 3e-4. The model is trained up to 20 epochs for the AOPE, UABSA, and ASTE task and 30 epochs for the TASD task. 3 3.1 Experiments Experimental Setup Datasets We evaluate the proposed G AS framework on four popular benchmark datasets including Laptop14, Rest14, Rest15, and Rest16, originally provided by the SemEval shared challenges (Pontiki et al., 2014, 2015, 2016). For each ABSA task, we use the public datasets derived from them with more sentiment annotations. Specifically, we adopt the dataset provided by Fan et al. (2019), Li et al. (2019a), Xu et al. (2020), Wan et al. (2020) for the AOPE, UABSA, ASTE, TASD task respectively. For a fair comparison, we use the same data split as previous works. Evaluation Metrics We adopt F1 scores as the main evaluation metrics for all tasks. A prediction is correct if and only if all its predicted sentiment elements in the pair or triplet are correct. Experiment Details We adopt the T5 base model from huggingface Transformer library2 for 2 https://github.com/huggingface/ transformers 3.2 Main Results The main results for the AOPE, UABSA, ASTE, TASD task are reported in Table"
2021.acl-short.64,P19-1048,0,0.400794,"l., 2015; Yin et al., 2016; Li et al., 2018; Ma et al., 2019) or classifying the sentiment polarity for a given aspect (Wang et al., 2016; Chen et al., 2017; Jiang et al., 2019; Zhang and Qian, 2020). To provide more detailed information, many recent studies propose to jointly predict multiple elements simultaneously (Li et al., 2019a; Wan et al., 2020; Peng et al., 2020; Zhao et al., 2020). Taking the Unified ABSA (UABSA, also called End-to-End ABSA) task as an example, it tries to simultaneously predict the mentioned aspect terms and the corresponding sentiment polarities (Luo et al., 2019; He et al., 2019). In general, most ABSA tasks are formulated as either sequence-level or token-level classification problems (Li et al., 2019b). By designing taskspecific classification networks, the prediction is made in a discriminative manner, using the class index as labels for training (Huang and Carley, 2018; Wan et al., 2020). However, these methods ignore the label semantics, i.e., the meaning of the natural language labels, during the training process. Intuitively, knowing the meaning of “food quality” and “restaurant ambiance”, it can be much easier to identify that the former one is more likely to"
2021.acl-short.64,P19-1051,0,0.436993,"Missing"
2021.acl-short.64,D18-1136,0,0.1029,"e elements simultaneously (Li et al., 2019a; Wan et al., 2020; Peng et al., 2020; Zhao et al., 2020). Taking the Unified ABSA (UABSA, also called End-to-End ABSA) task as an example, it tries to simultaneously predict the mentioned aspect terms and the corresponding sentiment polarities (Luo et al., 2019; He et al., 2019). In general, most ABSA tasks are formulated as either sequence-level or token-level classification problems (Li et al., 2019b). By designing taskspecific classification networks, the prediction is made in a discriminative manner, using the class index as labels for training (Huang and Carley, 2018; Wan et al., 2020). However, these methods ignore the label semantics, i.e., the meaning of the natural language labels, during the training process. Intuitively, knowing the meaning of “food quality” and “restaurant ambiance”, it can be much easier to identify that the former one is more likely to be the correct aspect category for the concerned aspect “pizza”. Such semantics of the label can be more helpful for the joint extraction of multiple sentiment elements, due to the complicated interactions of those involved elements. For example, understanding “delicious” is an adjective for descri"
2021.acl-short.64,D19-1654,0,0.0169393,"ne when Wenxuan Zhang was an intern at Alibaba. The work described in this paper is partially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14200719). 1 The data and code can be found at https://github. com/IsakZhang/Generative-ABSA The main research line of ABSA focuses on the identification of those sentiment elements such as extracting the aspect term (Liu et al., 2015; Yin et al., 2016; Li et al., 2018; Ma et al., 2019) or classifying the sentiment polarity for a given aspect (Wang et al., 2016; Chen et al., 2017; Jiang et al., 2019; Zhang and Qian, 2020). To provide more detailed information, many recent studies propose to jointly predict multiple elements simultaneously (Li et al., 2019a; Wan et al., 2020; Peng et al., 2020; Zhao et al., 2020). Taking the Unified ABSA (UABSA, also called End-to-End ABSA) task as an example, it tries to simultaneously predict the mentioned aspect terms and the corresponding sentiment polarities (Luo et al., 2019; He et al., 2019). In general, most ABSA tasks are formulated as either sequence-level or token-level classification problems (Li et al., 2019b). By designing taskspecific class"
2021.acl-short.64,D19-5505,1,0.787255,"g Special Administrative Region, China (Project Code: 14200719). 1 The data and code can be found at https://github. com/IsakZhang/Generative-ABSA The main research line of ABSA focuses on the identification of those sentiment elements such as extracting the aspect term (Liu et al., 2015; Yin et al., 2016; Li et al., 2018; Ma et al., 2019) or classifying the sentiment polarity for a given aspect (Wang et al., 2016; Chen et al., 2017; Jiang et al., 2019; Zhang and Qian, 2020). To provide more detailed information, many recent studies propose to jointly predict multiple elements simultaneously (Li et al., 2019a; Wan et al., 2020; Peng et al., 2020; Zhao et al., 2020). Taking the Unified ABSA (UABSA, also called End-to-End ABSA) task as an example, it tries to simultaneously predict the mentioned aspect terms and the corresponding sentiment polarities (Luo et al., 2019; He et al., 2019). In general, most ABSA tasks are formulated as either sequence-level or token-level classification problems (Li et al., 2019b). By designing taskspecific classification networks, the prediction is made in a discriminative manner, using the class index as labels for training (Huang and Carley, 2018; Wan et al., 2020)."
2021.acl-short.64,D15-1168,0,0.186739,"Missing"
2021.acl-short.64,P19-1056,0,0.180058,"ect term (Liu et al., 2015; Yin et al., 2016; Li et al., 2018; Ma et al., 2019) or classifying the sentiment polarity for a given aspect (Wang et al., 2016; Chen et al., 2017; Jiang et al., 2019; Zhang and Qian, 2020). To provide more detailed information, many recent studies propose to jointly predict multiple elements simultaneously (Li et al., 2019a; Wan et al., 2020; Peng et al., 2020; Zhao et al., 2020). Taking the Unified ABSA (UABSA, also called End-to-End ABSA) task as an example, it tries to simultaneously predict the mentioned aspect terms and the corresponding sentiment polarities (Luo et al., 2019; He et al., 2019). In general, most ABSA tasks are formulated as either sequence-level or token-level classification problems (Li et al., 2019b). By designing taskspecific classification networks, the prediction is made in a discriminative manner, using the class index as labels for training (Huang and Carley, 2018; Wan et al., 2020). However, these methods ignore the label semantics, i.e., the meaning of the natural language labels, during the training process. Intuitively, knowing the meaning of “food quality” and “restaurant ambiance”, it can be much easier to identify that the former one"
2021.acl-short.64,P19-1344,0,0.0615644,"us.”, the corresponding elements are “pizza”, “delicious”, “food quality” and “positive”, respectively. ∗ Work done when Wenxuan Zhang was an intern at Alibaba. The work described in this paper is partially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14200719). 1 The data and code can be found at https://github. com/IsakZhang/Generative-ABSA The main research line of ABSA focuses on the identification of those sentiment elements such as extracting the aspect term (Liu et al., 2015; Yin et al., 2016; Li et al., 2018; Ma et al., 2019) or classifying the sentiment polarity for a given aspect (Wang et al., 2016; Chen et al., 2017; Jiang et al., 2019; Zhang and Qian, 2020). To provide more detailed information, many recent studies propose to jointly predict multiple elements simultaneously (Li et al., 2019a; Wan et al., 2020; Peng et al., 2020; Zhao et al., 2020). Taking the Unified ABSA (UABSA, also called End-to-End ABSA) task as an example, it tries to simultaneously predict the mentioned aspect terms and the corresponding sentiment polarities (Luo et al., 2019; He et al., 2019). In general, most ABSA tasks are formulated"
2021.acl-short.64,S15-2082,0,0.281002,"Missing"
2021.acl-short.64,S14-2004,0,0.51721,"Missing"
2021.acl-short.64,D16-1058,0,0.0673404,"nd “positive”, respectively. ∗ Work done when Wenxuan Zhang was an intern at Alibaba. The work described in this paper is partially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14200719). 1 The data and code can be found at https://github. com/IsakZhang/Generative-ABSA The main research line of ABSA focuses on the identification of those sentiment elements such as extracting the aspect term (Liu et al., 2015; Yin et al., 2016; Li et al., 2018; Ma et al., 2019) or classifying the sentiment polarity for a given aspect (Wang et al., 2016; Chen et al., 2017; Jiang et al., 2019; Zhang and Qian, 2020). To provide more detailed information, many recent studies propose to jointly predict multiple elements simultaneously (Li et al., 2019a; Wan et al., 2020; Peng et al., 2020; Zhao et al., 2020). Taking the Unified ABSA (UABSA, also called End-to-End ABSA) task as an example, it tries to simultaneously predict the mentioned aspect terms and the corresponding sentiment polarities (Luo et al., 2019; He et al., 2019). In general, most ABSA tasks are formulated as either sequence-level or token-level classification problems (Li et al.,"
2021.acl-short.64,2020.emnlp-main.183,1,0.947028,"a pre-defined category set. For example, 2.2 Generation Model R14 R15 R16 JERE-MHS† SpanMlt (Zhao et al., 2020) SDRN (Chen et al., 2020) 53.41 52.34 68.66 66.18 62.39 66.02 75.60 73.30 58.12 59.64 64.68 65.75 63.84 67.65 71.78 73.67 G AS -A NNOTATION -R G AS -E XTRACTION -R G AS -A NNOTATION G AS -E XTRACTION 68.74 67.58 69.55 68.08 72.66 73.22 75.15 74.12 65.03 65.83 67.93 67.19 73.75 74.12 75.42 74.54 Table 1: Main results of the AOPE task. The best results are in bold, second best results are underlined. Results are the average F1 scores over 5 runs. † denotes results are from Zhao et al. (2020). Input: A big disappointment, all around. The pizza was cold and the cheese wasn’t even fully melted. Target (Annotation-style): A big disappointment, all around. The [pizza |food quality |negative] was cold and the [cheese | food quality |negative] wasn’t even fully melted [null |restaurant general |negative]. Target (Extraction-style): (pizza, food quality, negative); (cheese, food quality, negative); (null, restaurant general, negative); Similarly, we pack each aspect term, the aspect category it belongs to, and its sentiment polarity into a bracket to build the target sentence for the ann"
2021.acl-short.64,2020.emnlp-main.286,0,0.345888,"g was an intern at Alibaba. The work described in this paper is partially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14200719). 1 The data and code can be found at https://github. com/IsakZhang/Generative-ABSA The main research line of ABSA focuses on the identification of those sentiment elements such as extracting the aspect term (Liu et al., 2015; Yin et al., 2016; Li et al., 2018; Ma et al., 2019) or classifying the sentiment polarity for a given aspect (Wang et al., 2016; Chen et al., 2017; Jiang et al., 2019; Zhang and Qian, 2020). To provide more detailed information, many recent studies propose to jointly predict multiple elements simultaneously (Li et al., 2019a; Wan et al., 2020; Peng et al., 2020; Zhao et al., 2020). Taking the Unified ABSA (UABSA, also called End-to-End ABSA) task as an example, it tries to simultaneously predict the mentioned aspect terms and the corresponding sentiment polarities (Luo et al., 2019; He et al., 2019). In general, most ABSA tasks are formulated as either sequence-level or token-level classification problems (Li et al., 2019b). By designing taskspecific classification networks, the"
2021.acl-short.64,2020.acl-main.296,0,0.637006,"4200719). 1 The data and code can be found at https://github. com/IsakZhang/Generative-ABSA The main research line of ABSA focuses on the identification of those sentiment elements such as extracting the aspect term (Liu et al., 2015; Yin et al., 2016; Li et al., 2018; Ma et al., 2019) or classifying the sentiment polarity for a given aspect (Wang et al., 2016; Chen et al., 2017; Jiang et al., 2019; Zhang and Qian, 2020). To provide more detailed information, many recent studies propose to jointly predict multiple elements simultaneously (Li et al., 2019a; Wan et al., 2020; Peng et al., 2020; Zhao et al., 2020). Taking the Unified ABSA (UABSA, also called End-to-End ABSA) task as an example, it tries to simultaneously predict the mentioned aspect terms and the corresponding sentiment polarities (Luo et al., 2019; He et al., 2019). In general, most ABSA tasks are formulated as either sequence-level or token-level classification problems (Li et al., 2019b). By designing taskspecific classification networks, the prediction is made in a discriminative manner, using the class index as labels for training (Huang and Carley, 2018; Wan et al., 2020). However, these methods ignore the label semantics, i.e.,"
2021.emnlp-main.726,2020.emnlp-main.27,0,0.0354735,"s, which are usually formulated as either token-level or sequence-level classification problems, underutilize the rich semantic information of the label (i.e., the meaning of sentiment elements to be predicted) since they treat the labels as number indices during training. Intuitively, the aspect term “pasta” is unlikely to be coupled with the aspect category “service general” due to the large semantic gap between them. But such information cannot be suitably utilized in those classification-type methods. Inspired by recent success in formulating various NLP tasks as text generation problems (Athiwaratkun et al., 2020; Paolini et al., 2021; Liu et al., 2021), we propose to tackle ASQP in a sequenceto-sequence (S2S) manner in this paper. On one hand, the sentiment quads can be predicted in an end-to-end manner, alleviating the potential error propagation in the pipeline solutions. On the other hand, the rich label semantic information could be fully exploited by learning to generate the sentiment elements in the natural language form. of natural sentences, rather than directly treating the desired sentiment quad text sequence as the generation target (Zhang et al., 2021). We summarize our contributions as f"
2021.emnlp-main.726,W18-6217,0,0.0124173,"al., 2017), Li-unified-R (Li et al., 2019a), Peng-pipeline (Peng et al., 2020) which firstly extract aspect and opinion terms separately, then conduct the pairing; Two-stage (Huang et al., 2021) which proposes a two-stage method to enhance the correlation between aspects and opinions; and 2) end-to-end models including GTS (Wu et al., 2020) and Jet (Xu et al., 2020), both designing unified tagging schemes in order to solve the task in an end-to-end fashion. For the TASD task, we adopt the dataset prepared by Wan et al. (2020). We compare with a pipeline-type baseline method Baseline-1-f_lex (Brun and Nikoulina, 2018), two BERT based models including TAS-CRF and TAS-TO (Wan et al., 2020), and a recent model MEJD (Wu et al., 2021) 3 which utilizes a graph structure to model the depenThe mapping relation between the category and their indexes is pre-defined based on the entire dataset. dency among the sentiment elements. 9215 Different from previous classification-type methods for tackling ABSA problem, our PARAPHRASE modeling can take advantage of the semantics of the sentiment elements by generating the natural language labels. We conduct ablation studies to further investigate the impact of the label sema"
2021.emnlp-main.726,2021.naacl-main.167,0,0.0203398,"els in all cases. 4) The experiment also suggests that our PARAPHRASE method naturally facilitates the knowledge transfer across related tasks with the unified framework, which can be especially beneficial in the low-resource setting.1 2 Related Work ABSA has been extensively studied in recent years where the main research line is the extraction of the sentiment elements. Early studies focus on the prediction of a single element such as extracting the aspect term (Liu et al., 2015; Yin et al., 2016; Xu et al., 2018; Ma et al., 2019), detecting the mentioned aspect category (Zhou et al., 2015; Bu et al., 2021), and predicting the sentiment polarity, given either an aspect term (Wang et al., 2016; Huang and Carley, 2018; Zhang and Qian, 2020) or an aspect category (Ruder et al., 2016; Hu et al., 2019a). Some works further consider the joint detection of two sentiment elements, including the pairwise extraction of aspect and opinion term (Wang et al., 2017; Chen et al., 2020; Zhao et al., 2020); the prediction of aspect term and its corresponding sentiment polarity (Li et al., 2019a; He et al., 2019; Hu et al., 2019b; Luo et al., 2019; Chen and Qian, 2020); and the co-extraction of aspect category an"
2021.emnlp-main.726,2020.coling-main.72,0,0.122704,"entiment polarity, given either an aspect term (Wang et al., 2016; Huang and Carley, 2018; Zhang and Qian, 2020) or an aspect category (Ruder et al., 2016; Hu et al., 2019a). Some works further consider the joint detection of two sentiment elements, including the pairwise extraction of aspect and opinion term (Wang et al., 2017; Chen et al., 2020; Zhao et al., 2020); the prediction of aspect term and its corresponding sentiment polarity (Li et al., 2019a; He et al., 2019; Hu et al., 2019b; Luo et al., 2019; Chen and Qian, 2020); and the co-extraction of aspect category and sentiment polarity (Cai et al., 2020). More recently, triplet prediction tasks are proposed in ABSA, aiming to predict the sentiment elements in triplet format. Peng et al. (2020) propose the aspect sentiment triplet extraction (ASTE) task, which has received lots of attention (Xu et al., 2020; Huang et al., 2021; Mao et al., 2021; Chen Exploiting generation modeling for the ASQP task mainly faces two challenges: (i) how to linearize the desired sentiment information so as to facilitate the S2S learning? (ii) how can we utilize the pretrained models for tackling the task, which is a common practice now for solving various ABSA ta"
2021.emnlp-main.726,2020.acl-main.582,0,0.394942,"5), aspect opinionated sentence, which can reveal a more sentiment classification based on either an aspect comprehensive and complete aspect-level sencategory (Ruder et al., 2016; Hu et al., 2019a) or timent structure. We further propose a novel PARAPHRASE modeling paradigm to cast the an aspect term (Huang and Carley, 2018). More ASQP task to a paraphrase generation process. recent works propose to extract multiple associated On one hand, the generation formulation alsentiment elements at the same time (Zhang et al., lows solving ASQP in an end-to-end manner, 2021). For example, Chen et al. (2020) consider the alleviating the potential error propagation in aspect and opinion term pairwise extraction; Peng the pipeline solution. On the other hand, the seet al. (2020) propose the aspect sentiment triplet mantics of the sentiment elements can be fully extraction (ASTE) task to detect the (aspect term, exploited by learning to generate them in the natural language form. Extensive experiments opinion term, sentiment polarity) triplets; Wan et al. on benchmark datasets show the superiority of (2020) handle the target aspect sentiment detection our proposed method and the capacity of cross(TA"
2021.emnlp-main.726,P19-1048,0,0.0189274,"2016; Xu et al., 2018; Ma et al., 2019), detecting the mentioned aspect category (Zhou et al., 2015; Bu et al., 2021), and predicting the sentiment polarity, given either an aspect term (Wang et al., 2016; Huang and Carley, 2018; Zhang and Qian, 2020) or an aspect category (Ruder et al., 2016; Hu et al., 2019a). Some works further consider the joint detection of two sentiment elements, including the pairwise extraction of aspect and opinion term (Wang et al., 2017; Chen et al., 2020; Zhao et al., 2020); the prediction of aspect term and its corresponding sentiment polarity (Li et al., 2019a; He et al., 2019; Hu et al., 2019b; Luo et al., 2019; Chen and Qian, 2020); and the co-extraction of aspect category and sentiment polarity (Cai et al., 2020). More recently, triplet prediction tasks are proposed in ABSA, aiming to predict the sentiment elements in triplet format. Peng et al. (2020) propose the aspect sentiment triplet extraction (ASTE) task, which has received lots of attention (Xu et al., 2020; Huang et al., 2021; Mao et al., 2021; Chen Exploiting generation modeling for the ASQP task mainly faces two challenges: (i) how to linearize the desired sentiment information so as to facilitate the"
2021.emnlp-main.726,D19-1467,0,0.0798106,"extract those sentiment elements (Pontiki et al., ing the four elements in one shot. In this work, 2014, 2015, 2016). Early studies focus on the prewe introduce the Aspect Sentiment Quad Prediction of a single element such as aspect term diction (ASQP) task, aiming to jointly detect extraction (Liu et al., 2015; Xu et al., 2018), asall sentiment elements in quads for a given pect category detection (Zhou et al., 2015), aspect opinionated sentence, which can reveal a more sentiment classification based on either an aspect comprehensive and complete aspect-level sencategory (Ruder et al., 2016; Hu et al., 2019a) or timent structure. We further propose a novel PARAPHRASE modeling paradigm to cast the an aspect term (Huang and Carley, 2018). More ASQP task to a paraphrase generation process. recent works propose to extract multiple associated On one hand, the generation formulation alsentiment elements at the same time (Zhang et al., lows solving ASQP in an end-to-end manner, 2021). For example, Chen et al. (2020) consider the alleviating the potential error propagation in aspect and opinion term pairwise extraction; Peng the pipeline solution. On the other hand, the seet al. (2020) propose the aspec"
2021.emnlp-main.726,P19-1051,0,0.0718185,"extract those sentiment elements (Pontiki et al., ing the four elements in one shot. In this work, 2014, 2015, 2016). Early studies focus on the prewe introduce the Aspect Sentiment Quad Prediction of a single element such as aspect term diction (ASQP) task, aiming to jointly detect extraction (Liu et al., 2015; Xu et al., 2018), asall sentiment elements in quads for a given pect category detection (Zhou et al., 2015), aspect opinionated sentence, which can reveal a more sentiment classification based on either an aspect comprehensive and complete aspect-level sencategory (Ruder et al., 2016; Hu et al., 2019a) or timent structure. We further propose a novel PARAPHRASE modeling paradigm to cast the an aspect term (Huang and Carley, 2018). More ASQP task to a paraphrase generation process. recent works propose to extract multiple associated On one hand, the generation formulation alsentiment elements at the same time (Zhang et al., lows solving ASQP in an end-to-end manner, 2021). For example, Chen et al. (2020) consider the alleviating the potential error propagation in aspect and opinion term pairwise extraction; Peng the pipeline solution. On the other hand, the seet al. (2020) propose the aspec"
2021.emnlp-main.726,D18-1136,0,0.105194,"studies focus on the prewe introduce the Aspect Sentiment Quad Prediction of a single element such as aspect term diction (ASQP) task, aiming to jointly detect extraction (Liu et al., 2015; Xu et al., 2018), asall sentiment elements in quads for a given pect category detection (Zhou et al., 2015), aspect opinionated sentence, which can reveal a more sentiment classification based on either an aspect comprehensive and complete aspect-level sencategory (Ruder et al., 2016; Hu et al., 2019a) or timent structure. We further propose a novel PARAPHRASE modeling paradigm to cast the an aspect term (Huang and Carley, 2018). More ASQP task to a paraphrase generation process. recent works propose to extract multiple associated On one hand, the generation formulation alsentiment elements at the same time (Zhang et al., lows solving ASQP in an end-to-end manner, 2021). For example, Chen et al. (2020) consider the alleviating the potential error propagation in aspect and opinion term pairwise extraction; Peng the pipeline solution. On the other hand, the seet al. (2020) propose the aspect sentiment triplet mantics of the sentiment elements can be fully extraction (ASTE) task to detect the (aspect term, exploited by"
2021.emnlp-main.726,D19-5505,1,0.895374,"2015; Yin et al., 2016; Xu et al., 2018; Ma et al., 2019), detecting the mentioned aspect category (Zhou et al., 2015; Bu et al., 2021), and predicting the sentiment polarity, given either an aspect term (Wang et al., 2016; Huang and Carley, 2018; Zhang and Qian, 2020) or an aspect category (Ruder et al., 2016; Hu et al., 2019a). Some works further consider the joint detection of two sentiment elements, including the pairwise extraction of aspect and opinion term (Wang et al., 2017; Chen et al., 2020; Zhao et al., 2020); the prediction of aspect term and its corresponding sentiment polarity (Li et al., 2019a; He et al., 2019; Hu et al., 2019b; Luo et al., 2019; Chen and Qian, 2020); and the co-extraction of aspect category and sentiment polarity (Cai et al., 2020). More recently, triplet prediction tasks are proposed in ABSA, aiming to predict the sentiment elements in triplet format. Peng et al. (2020) propose the aspect sentiment triplet extraction (ASTE) task, which has received lots of attention (Xu et al., 2020; Huang et al., 2021; Mao et al., 2021; Chen Exploiting generation modeling for the ASQP task mainly faces two challenges: (i) how to linearize the desired sentiment information so as"
2021.emnlp-main.726,D15-1168,0,0.0668368,"Missing"
2021.emnlp-main.726,P19-1056,0,0.0180852,"2019), detecting the mentioned aspect category (Zhou et al., 2015; Bu et al., 2021), and predicting the sentiment polarity, given either an aspect term (Wang et al., 2016; Huang and Carley, 2018; Zhang and Qian, 2020) or an aspect category (Ruder et al., 2016; Hu et al., 2019a). Some works further consider the joint detection of two sentiment elements, including the pairwise extraction of aspect and opinion term (Wang et al., 2017; Chen et al., 2020; Zhao et al., 2020); the prediction of aspect term and its corresponding sentiment polarity (Li et al., 2019a; He et al., 2019; Hu et al., 2019b; Luo et al., 2019; Chen and Qian, 2020); and the co-extraction of aspect category and sentiment polarity (Cai et al., 2020). More recently, triplet prediction tasks are proposed in ABSA, aiming to predict the sentiment elements in triplet format. Peng et al. (2020) propose the aspect sentiment triplet extraction (ASTE) task, which has received lots of attention (Xu et al., 2020; Huang et al., 2021; Mao et al., 2021; Chen Exploiting generation modeling for the ASQP task mainly faces two challenges: (i) how to linearize the desired sentiment information so as to facilitate the S2S learning? (ii) how can we utili"
2021.emnlp-main.726,P19-1344,0,0.0160416,"P as well as other ABSA tasks, outperforming the previous state-of-the-art models in all cases. 4) The experiment also suggests that our PARAPHRASE method naturally facilitates the knowledge transfer across related tasks with the unified framework, which can be especially beneficial in the low-resource setting.1 2 Related Work ABSA has been extensively studied in recent years where the main research line is the extraction of the sentiment elements. Early studies focus on the prediction of a single element such as extracting the aspect term (Liu et al., 2015; Yin et al., 2016; Xu et al., 2018; Ma et al., 2019), detecting the mentioned aspect category (Zhou et al., 2015; Bu et al., 2021), and predicting the sentiment polarity, given either an aspect term (Wang et al., 2016; Huang and Carley, 2018; Zhang and Qian, 2020) or an aspect category (Ruder et al., 2016; Hu et al., 2019a). Some works further consider the joint detection of two sentiment elements, including the pairwise extraction of aspect and opinion term (Wang et al., 2017; Chen et al., 2020; Zhao et al., 2020); the prediction of aspect term and its corresponding sentiment polarity (Li et al., 2019a; He et al., 2019; Hu et al., 2019b; Luo e"
2021.emnlp-main.726,S15-2082,0,0.0730391,"Missing"
2021.emnlp-main.726,S14-2004,0,0.108259,"Missing"
2021.emnlp-main.726,D16-1103,0,0.0925659,"nstead of predictor extract those sentiment elements (Pontiki et al., ing the four elements in one shot. In this work, 2014, 2015, 2016). Early studies focus on the prewe introduce the Aspect Sentiment Quad Prediction of a single element such as aspect term diction (ASQP) task, aiming to jointly detect extraction (Liu et al., 2015; Xu et al., 2018), asall sentiment elements in quads for a given pect category detection (Zhou et al., 2015), aspect opinionated sentence, which can reveal a more sentiment classification based on either an aspect comprehensive and complete aspect-level sencategory (Ruder et al., 2016; Hu et al., 2019a) or timent structure. We further propose a novel PARAPHRASE modeling paradigm to cast the an aspect term (Huang and Carley, 2018). More ASQP task to a paraphrase generation process. recent works propose to extract multiple associated On one hand, the generation formulation alsentiment elements at the same time (Zhang et al., lows solving ASQP in an end-to-end manner, 2021). For example, Chen et al. (2020) consider the alleviating the potential error propagation in aspect and opinion term pairwise extraction; Peng the pipeline solution. On the other hand, the seet al. (2020)"
2021.emnlp-main.726,N19-1035,0,0.0187814,"eously, which can handle the case where the aspect term is implicit expressed in the given text (treated as “null”) (Wu et al., 2021). Built on top of those tasks, we introduce the aspect sentiment quad prediction problem, aiming to predict the four sentiment elements in one shot, which can provide a more detailed and comprehensive sentiment structure for a given text. Adopting pretrained transformer-based models such as BERT (Devlin et al., 2019) has become a common practice for tackling the ABSA problem. Especially, many ABSA tasks benefit from appropriately utilizing the pretrained models. Sun et al. (2019) transform the aspect sentiment classification task as a language inference problem by constructing an auxiliary sentence. Chen et al. (2021) and Mao et al. (2021) formulate multiple ABSA tasks as a reading comprehension task to fully utilize the knowledge of the pre-trained model. Very recently, there are some attempts on tackling ABSA problem in a S2S manner, either treating the class index (Yan et al., 2021) or the desired sentiment element sequence (Zhang et al., 2021) as the target of the generation model. In this work, we propose a PARAPHRASE modeling that can better utilize the knowledg"
2021.emnlp-main.726,D16-1058,0,0.0227744,"facilitates the knowledge transfer across related tasks with the unified framework, which can be especially beneficial in the low-resource setting.1 2 Related Work ABSA has been extensively studied in recent years where the main research line is the extraction of the sentiment elements. Early studies focus on the prediction of a single element such as extracting the aspect term (Liu et al., 2015; Yin et al., 2016; Xu et al., 2018; Ma et al., 2019), detecting the mentioned aspect category (Zhou et al., 2015; Bu et al., 2021), and predicting the sentiment polarity, given either an aspect term (Wang et al., 2016; Huang and Carley, 2018; Zhang and Qian, 2020) or an aspect category (Ruder et al., 2016; Hu et al., 2019a). Some works further consider the joint detection of two sentiment elements, including the pairwise extraction of aspect and opinion term (Wang et al., 2017; Chen et al., 2020; Zhao et al., 2020); the prediction of aspect term and its corresponding sentiment polarity (Li et al., 2019a; He et al., 2019; Hu et al., 2019b; Luo et al., 2019; Chen and Qian, 2020); and the co-extraction of aspect category and sentiment polarity (Cai et al., 2020). More recently, triplet prediction tasks are pr"
2021.emnlp-main.726,2020.emnlp-main.286,0,0.0164906,"related tasks with the unified framework, which can be especially beneficial in the low-resource setting.1 2 Related Work ABSA has been extensively studied in recent years where the main research line is the extraction of the sentiment elements. Early studies focus on the prediction of a single element such as extracting the aspect term (Liu et al., 2015; Yin et al., 2016; Xu et al., 2018; Ma et al., 2019), detecting the mentioned aspect category (Zhou et al., 2015; Bu et al., 2021), and predicting the sentiment polarity, given either an aspect term (Wang et al., 2016; Huang and Carley, 2018; Zhang and Qian, 2020) or an aspect category (Ruder et al., 2016; Hu et al., 2019a). Some works further consider the joint detection of two sentiment elements, including the pairwise extraction of aspect and opinion term (Wang et al., 2017; Chen et al., 2020; Zhao et al., 2020); the prediction of aspect term and its corresponding sentiment polarity (Li et al., 2019a; He et al., 2019; Hu et al., 2019b; Luo et al., 2019; Chen and Qian, 2020); and the co-extraction of aspect category and sentiment polarity (Cai et al., 2020). More recently, triplet prediction tasks are proposed in ABSA, aiming to predict the sentiment"
2021.emnlp-main.726,2021.acl-short.64,1,0.533825,"s as text generation problems (Athiwaratkun et al., 2020; Paolini et al., 2021; Liu et al., 2021), we propose to tackle ASQP in a sequenceto-sequence (S2S) manner in this paper. On one hand, the sentiment quads can be predicted in an end-to-end manner, alleviating the potential error propagation in the pipeline solutions. On the other hand, the rich label semantic information could be fully exploited by learning to generate the sentiment elements in the natural language form. of natural sentences, rather than directly treating the desired sentiment quad text sequence as the generation target (Zhang et al., 2021). We summarize our contributions as follows: 1) We study a new task, namely aspect sentiment quad prediction (ASQP) in this work and introduce two datasets with sentiment quad annotations for each sample, aiming to analyze more comprehensive aspect-level sentiment information. 2) We propose to tackle ASQP as a paraphrase generation problem, which can predict the sentiment quads in one shot and fully utilize the semantics information of natural language labels. 3) Extensive experiments show that the proposed PARAPHRASE modeling is effective to tackle ASQP as well as other ABSA tasks, outperform"
2021.emnlp-main.726,2020.acl-main.296,0,0.0362008,"focus on the prediction of a single element such as extracting the aspect term (Liu et al., 2015; Yin et al., 2016; Xu et al., 2018; Ma et al., 2019), detecting the mentioned aspect category (Zhou et al., 2015; Bu et al., 2021), and predicting the sentiment polarity, given either an aspect term (Wang et al., 2016; Huang and Carley, 2018; Zhang and Qian, 2020) or an aspect category (Ruder et al., 2016; Hu et al., 2019a). Some works further consider the joint detection of two sentiment elements, including the pairwise extraction of aspect and opinion term (Wang et al., 2017; Chen et al., 2020; Zhao et al., 2020); the prediction of aspect term and its corresponding sentiment polarity (Li et al., 2019a; He et al., 2019; Hu et al., 2019b; Luo et al., 2019; Chen and Qian, 2020); and the co-extraction of aspect category and sentiment polarity (Cai et al., 2020). More recently, triplet prediction tasks are proposed in ABSA, aiming to predict the sentiment elements in triplet format. Peng et al. (2020) propose the aspect sentiment triplet extraction (ASTE) task, which has received lots of attention (Xu et al., 2020; Huang et al., 2021; Mao et al., 2021; Chen Exploiting generation modeling for the ASQP task"
2021.emnlp-main.726,2020.findings-emnlp.234,0,0.0283253,"ablations on the sentiment polarity and aspect category, the model suffers more when the aspect category is projected to an indexed symbol. The possible reason is that there are only three types of sentiment polarities, which is much less than the number of types for the aspect category. Therefore, it can be easier for the model to learn the mapping between the special symbols and the polarity type during the training. 5.2 5.3 Effect of Label Semantics L14 R14 R15 R16 CMLA+ (Wang et al., 2017) Li-unified-R (Li et al., 2019a) P-pipeline (Peng et al., 2020) Jet+BERT (Xu et al., 2020) GTS+BERT (Wu et al., 2020) Two-Stage (Huang et al., 2021) 33.16 42.34 42.87 51.04 55.21 58.58 42.79 51.00 51.46 62.40 64.81 68.16 37.01 47.82 52.32 57.53 54.88 58.59 41.72 44.31 54.21 63.83 66.08 67.52 GAS (Zhang et al., 2021) PARAPHRASE 58.19 61.13 70.52 72.03 60.23 62.56 69.05 71.70 Table 3: Results of the ASTE task compared with previous state-of-the-art models. F1 scores are reported. Results on ASTE and TASD Tasks As described in Sec 3.4, the proposed PARA PHRASE modeling provides a unified framework to tackle the ABSA problem, we thus test it on the ASTE and TASD tasks, and compare with the previous state-of-the-"
2021.emnlp-main.726,P18-2094,0,0.136292,"ements, including the aspect category, aspect Due to its broad application scenarios, many reterm, opinion term, and sentiment polarity. Existing studies usually consider the detection of search efforts have been made on ABSA to predict partial sentiment elements, instead of predictor extract those sentiment elements (Pontiki et al., ing the four elements in one shot. In this work, 2014, 2015, 2016). Early studies focus on the prewe introduce the Aspect Sentiment Quad Prediction of a single element such as aspect term diction (ASQP) task, aiming to jointly detect extraction (Liu et al., 2015; Xu et al., 2018), asall sentiment elements in quads for a given pect category detection (Zhou et al., 2015), aspect opinionated sentence, which can reveal a more sentiment classification based on either an aspect comprehensive and complete aspect-level sencategory (Ruder et al., 2016; Hu et al., 2019a) or timent structure. We further propose a novel PARAPHRASE modeling paradigm to cast the an aspect term (Huang and Carley, 2018). More ASQP task to a paraphrase generation process. recent works propose to extract multiple associated On one hand, the generation formulation alsentiment elements at the same time ("
2021.emnlp-main.726,2020.emnlp-main.183,1,0.871168,"5), aspect opinionated sentence, which can reveal a more sentiment classification based on either an aspect comprehensive and complete aspect-level sencategory (Ruder et al., 2016; Hu et al., 2019a) or timent structure. We further propose a novel PARAPHRASE modeling paradigm to cast the an aspect term (Huang and Carley, 2018). More ASQP task to a paraphrase generation process. recent works propose to extract multiple associated On one hand, the generation formulation alsentiment elements at the same time (Zhang et al., lows solving ASQP in an end-to-end manner, 2021). For example, Chen et al. (2020) consider the alleviating the potential error propagation in aspect and opinion term pairwise extraction; Peng the pipeline solution. On the other hand, the seet al. (2020) propose the aspect sentiment triplet mantics of the sentiment elements can be fully extraction (ASTE) task to detect the (aspect term, exploited by learning to generate them in the natural language form. Extensive experiments opinion term, sentiment polarity) triplets; Wan et al. on benchmark datasets show the superiority of (2020) handle the target aspect sentiment detection our proposed method and the capacity of cross(TA"
2021.emnlp-main.726,2021.acl-long.188,0,0.0380688,"ls such as BERT (Devlin et al., 2019) has become a common practice for tackling the ABSA problem. Especially, many ABSA tasks benefit from appropriately utilizing the pretrained models. Sun et al. (2019) transform the aspect sentiment classification task as a language inference problem by constructing an auxiliary sentence. Chen et al. (2021) and Mao et al. (2021) formulate multiple ABSA tasks as a reading comprehension task to fully utilize the knowledge of the pre-trained model. Very recently, there are some attempts on tackling ABSA problem in a S2S manner, either treating the class index (Yan et al., 2021) or the desired sentiment element sequence (Zhang et al., 2021) as the target of the generation model. In this work, we propose a PARAPHRASE modeling that can better utilize the knowledge of the pre-trained model via casting the original task to a paraphrase generation process. ASQP The wine list yesterday … ! The wine list yesterday was excellent, but the place is too tiny for me! ASQP Figure 1: Overview of the paraphrase generation framework. The underlined task identifier in the input is only used under the cross-task transfer setting. tion problem and solve it in a sequence-to-sequence man"
2021.emnlp-main.727,L18-1344,0,0.0167474,"fic knowledge for the target languages, previous works usually first translate the source sentence with an off-the-shelf translation system, then word alignment tools such as fastAlign (Dyer et al., 2013) are used to map the token-level label from the source sentence to the translated sentence (Mayhew et al., 2017; Fei et al., 2020). Some heuristics are proposed for alleviating the alignment error, for example, by conducting a phrase-to-phrase mapping to refine the aspect boundary (Klinger and Cimiano, 2015; Li et al., 2020). However, the word or phrase alignment itself is a challenging task (Akbik and Vollgraf, 2018). The sentences of the ABSA task are usually user-generated (e.g., product reviews and tweets) and informal, which further hinders the translatethen-align method to produce satisfactory pseudolabeled target data (Lohar et al., 2019). The inaccurate pseudo labels inevitably limit the task-specific knowledge and lead to poor model performance. We propose an alignment-free label projection method for obtaining the pseudo-labeled data in the target language3 . As depicted in the upper-left portion of Figure 1, we first mark each aspect term in the sentence with a special symbol (e.g., different br"
2021.emnlp-main.727,N18-1053,0,0.12141,"imiano, 2015). Later methods make use of the task of extracting mentioned aspects from a given cross-lingual word embeddings (Ruder et al., 2019) sentence and predicting their corresponding senti- trained on large parallel corpus to allow the model ment polarities1 (Liu, 2012; Pontiki et al., 2014). to be used in a language-independent manner, by Consider the following example, “The food is great, simply switching the word embedding layer while but the service is kinda disappointing”, we can de- keeping the model unchanged (Barnes et al., 2016; tect two mentioned aspect terms “food” and “ser- Akhtar et al., 2018; Jebbara and Cimiano, 2019) vice”, and judge their corresponding sentiments when adopted for different languages. as positive and negative, respectively. Given its Recently, employing multilingual pre-trained ∗ models such as the multilingual BERT (Devlin Work done when Wenxuan Zhang was an intern at Alibaba. This work was supported by Alibaba Group through et al., 2019) and XLM-Roberta (Conneau et al., Alibaba Research Intern Program, and a grant from the Re2020) has become the de-facto approach to tackle search Grant Council of the Hong Kong Special Administrathe cross-lingual transfer for"
2021.emnlp-main.727,C16-1152,0,0.0537568,"Missing"
2021.emnlp-main.727,2020.acl-main.340,0,0.0676377,"Missing"
2021.emnlp-main.727,2020.acl-main.747,0,0.11228,"learned in the pre-training stage (Wu and Dredze, 2019; K et al., 2020). There are some challenges for adopting such a paradigm to solve the cross-lingual ABSA task. The language-specific knowledge plays an essential role in tackling the ABSA problem, since the concerned texts are often written by ordinary users with all kinds of abbreviations or slang. The aspect terms and the opinion expressions may also be language-dependent. However, the languagespecific knowledge of the zero-shot method purely comes from the pre-training process where the lowresource languages might be under-represented (Conneau et al., 2020; Pfeiffer et al., 2020). Utilizing the translated target language data with projected labels is a plausible idea to compensate the language-specific knowledge (Li et al., 2020). But the performance of such translation-based methods largely depends on the quality of the translation and label projection. The task-specific knowledge in the translated data would also be limited if the projected label quality is unsatisfactory. ACS method to the multilingual setting, assuming multiple translation engines are available. In this case, the target languages can benefit from the task-specific knowledge"
2021.emnlp-main.727,N19-1423,0,0.00855939,"od [seafood]POS . Labeled French Sentence with [Mermaid Inn]POS est un bon restaurant dans l’ ensemble avec de très translate-then-align Method bons [fruits]POS de mer . Labeled French Sentence with [Mermaid Inn]POS est un bon restaurant dans l’ ensemble avec de très alignment-free Method bons [fruits de mer]POS . Table 6: Example of different label projection methods with French as the target language. We use the bracket to highlight the aspect term, the corresponding sentiment polarities are shown as the subscript for each aspect. on large multilingual corpus, such as the multilingual BERT (Devlin et al., 2019) and XLM-Roberta (Conneau et al., 2020), have shown significant improvements for various cross-lingual NLP tasks. Thanks to the language knowledge learned in the pre-training process, fine-tuning the model on the labeled source language data and directly conducting the inference on the target data can achieve impressive cross-lingual adaptation performance (Wu and Dredze, 2019; Pires et al., 2019; K et al., 2020). Some studies further utilize the translation system together with the pre-trained models, for example, by direct data transfer (Fei et al., 2020; Hu et al., 2020), data augmentation"
2021.emnlp-main.727,N13-1073,0,0.197432,"ur moi. The [nourriture] is very fresh and delicious, but this {endroit} is too small for me. []: nourriture, positive {}: endroit, negative Figure 1: Example of the alignment-free label projection method (upper part) and the aspect code-switching strategy (lower part). Here we use English and French as the source and target language respectively. 2.2 Alignment-free Label Projection To obtain the language-specific knowledge for the target languages, previous works usually first translate the source sentence with an off-the-shelf translation system, then word alignment tools such as fastAlign (Dyer et al., 2013) are used to map the token-level label from the source sentence to the translated sentence (Mayhew et al., 2017; Fei et al., 2020). Some heuristics are proposed for alleviating the alignment error, for example, by conducting a phrase-to-phrase mapping to refine the aspect boundary (Klinger and Cimiano, 2015; Li et al., 2020). However, the word or phrase alignment itself is a challenging task (Akbik and Vollgraf, 2018). The sentences of the ABSA task are usually user-generated (e.g., product reviews and tweets) and informal, which further hinders the translatethen-align method to produce satisf"
2021.emnlp-main.727,2020.acl-main.627,0,0.0489553,"Missing"
2021.emnlp-main.727,P19-1048,1,0.844521,"-align paradigm, our method does not rely on any word alignment tool for projecting the labels from the source to the translated target sentence, which avoids the mis-alignment issue brought in by this step. The high-quality la2 Methodology beled target data thus preserves more task-specific knowledge, helping establish a strong baseline by 2.1 Problem Formulation purely training on such pseudo-labeled data. PreviWe formulate the ABSA task as a sequence labeling ous findings suggest that training on the bilingual corpus (i.e., labeled source data and translated tar- problem (Li et al., 2019b; He et al., 2019). Given L get data) often leads to better performance in cross- a sentence x = {xi }i=1 with L tokens, the model predicts a label sequence y = {yi }L lingual transfer tasks (Hu et al., 2020). Inspired i=1 where by this finding and to further enhance the interac- yi ∈ Y = {B, I, E, S}-{POS, NEU, NEG} ∪ {O} denotes the aspect boundary and its sentiment polartions between the two languages, we propose an ity for the corresponding token xi . For example, aspect code-switching (ACS) mechanism, which yi = B-POS means xi is the beginning of a positive switches the aspect terms between the source and"
2021.emnlp-main.727,P19-1051,0,0.0540098,"Missing"
2021.emnlp-main.727,N19-1257,0,0.025294,"methods make use of the task of extracting mentioned aspects from a given cross-lingual word embeddings (Ruder et al., 2019) sentence and predicting their corresponding senti- trained on large parallel corpus to allow the model ment polarities1 (Liu, 2012; Pontiki et al., 2014). to be used in a language-independent manner, by Consider the following example, “The food is great, simply switching the word embedding layer while but the service is kinda disappointing”, we can de- keeping the model unchanged (Barnes et al., 2016; tect two mentioned aspect terms “food” and “ser- Akhtar et al., 2018; Jebbara and Cimiano, 2019) vice”, and judge their corresponding sentiments when adopted for different languages. as positive and negative, respectively. Given its Recently, employing multilingual pre-trained ∗ models such as the multilingual BERT (Devlin Work done when Wenxuan Zhang was an intern at Alibaba. This work was supported by Alibaba Group through et al., 2019) and XLM-Roberta (Conneau et al., Alibaba Research Intern Program, and a grant from the Re2020) has become the de-facto approach to tackle search Grant Council of the Hong Kong Special Administrathe cross-lingual transfer for many NLP tasks (Hu tive Regi"
2021.emnlp-main.727,2020.emnlp-main.369,0,0.0230427,"020; Liang et al., 2020; Mao et al., While most existing studies focus on English 2021; Zhang et al., 2021). texts, handling ABSA in resource-poor lanThe majority of existing ABSA studies are conguages remains a challenging problem. In this paper, we consider the unsupervised crossducted on English texts. However, in real-world lingual transfer for the ABSA task, where only scenarios such as the E-commerce website, users’ labeled data in the source language is available opinions are usually expressed in different lanand we aim at transferring its knowledge to the guages (Pontiki et al., 2016; Keung et al., 2020). target language having no labeled data. To this Manually annotating a large quantity of ABSA data end, we propose an alignment-free label profor every language can be extremely costly. In this jection method to obtain high-quality pseudowork, we investigate the unsupervised cross-lingual labeled data of the target language with the transfer for the ABSA task, where we only have help of the translation system, which could preserve more accurate task-specific knowledge labeled data in the source language and aim to in the target language. For better utilizing the transfer the knowledge to targ"
2021.emnlp-main.727,K15-1016,0,0.0273519,"as the source and target language respectively. 2.2 Alignment-free Label Projection To obtain the language-specific knowledge for the target languages, previous works usually first translate the source sentence with an off-the-shelf translation system, then word alignment tools such as fastAlign (Dyer et al., 2013) are used to map the token-level label from the source sentence to the translated sentence (Mayhew et al., 2017; Fei et al., 2020). Some heuristics are proposed for alleviating the alignment error, for example, by conducting a phrase-to-phrase mapping to refine the aspect boundary (Klinger and Cimiano, 2015; Li et al., 2020). However, the word or phrase alignment itself is a challenging task (Akbik and Vollgraf, 2018). The sentences of the ABSA task are usually user-generated (e.g., product reviews and tweets) and informal, which further hinders the translatethen-align method to produce satisfactory pseudolabeled target data (Lohar et al., 2019). The inaccurate pseudo labels inevitably limit the task-specific knowledge and lead to poor model performance. We propose an alignment-free label projection method for obtaining the pseudo-labeled data in the target language3 . As depicted in the upper-l"
2021.emnlp-main.727,D17-1269,0,0.0627898,"Missing"
2021.emnlp-main.727,2020.emnlp-main.617,0,0.0157667,"ining stage (Wu and Dredze, 2019; K et al., 2020). There are some challenges for adopting such a paradigm to solve the cross-lingual ABSA task. The language-specific knowledge plays an essential role in tackling the ABSA problem, since the concerned texts are often written by ordinary users with all kinds of abbreviations or slang. The aspect terms and the opinion expressions may also be language-dependent. However, the languagespecific knowledge of the zero-shot method purely comes from the pre-training process where the lowresource languages might be under-represented (Conneau et al., 2020; Pfeiffer et al., 2020). Utilizing the translated target language data with projected labels is a plausible idea to compensate the language-specific knowledge (Li et al., 2020). But the performance of such translation-based methods largely depends on the quality of the translation and label projection. The task-specific knowledge in the translated data would also be limited if the projected label quality is unsatisfactory. ACS method to the multilingual setting, assuming multiple translation engines are available. In this case, the target languages can benefit from the task-specific knowledge contained in different"
2021.emnlp-main.727,P19-1493,0,0.0329588,"Missing"
2021.emnlp-main.727,D19-5505,1,0.930201,"ous translate-then-align paradigm, our method does not rely on any word alignment tool for projecting the labels from the source to the translated target sentence, which avoids the mis-alignment issue brought in by this step. The high-quality la2 Methodology beled target data thus preserves more task-specific knowledge, helping establish a strong baseline by 2.1 Problem Formulation purely training on such pseudo-labeled data. PreviWe formulate the ABSA task as a sequence labeling ous findings suggest that training on the bilingual corpus (i.e., labeled source data and translated tar- problem (Li et al., 2019b; He et al., 2019). Given L get data) often leads to better performance in cross- a sentence x = {xi }i=1 with L tokens, the model predicts a label sequence y = {yi }L lingual transfer tasks (Hu et al., 2020). Inspired i=1 where by this finding and to further enhance the interac- yi ∈ Y = {B, I, E, S}-{POS, NEU, NEG} ∪ {O} denotes the aspect boundary and its sentiment polartions between the two languages, we propose an ity for the corresponding token xi . For example, aspect code-switching (ACS) mechanism, which yi = B-POS means xi is the beginning of a positive switches the aspect terms betw"
2021.emnlp-main.727,S14-2004,0,0.124041,"Missing"
2021.emnlp-main.727,P15-2128,0,0.0645053,"Missing"
2021.emnlp-main.727,D19-1077,0,0.0557,"Kong Special Administrathe cross-lingual transfer for many NLP tasks (Hu tive Region, China (Project Codes: 14204418). 1 Also called End-to-End ABSA or Unified ABSA et al., 2020). Typically, the model is first fine-tuned 9220 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9220–9230 c November 7–11, 2021. 2021 Association for Computational Linguistics on labeled source language data and then can be directly used for inference on the target language data (i.e., zero-shot approach), thanks to the language knowledge learned in the pre-training stage (Wu and Dredze, 2019; K et al., 2020). There are some challenges for adopting such a paradigm to solve the cross-lingual ABSA task. The language-specific knowledge plays an essential role in tackling the ABSA problem, since the concerned texts are often written by ordinary users with all kinds of abbreviations or slang. The aspect terms and the opinion expressions may also be language-dependent. However, the languagespecific knowledge of the zero-shot method purely comes from the pre-training process where the lowresource languages might be under-represented (Conneau et al., 2020; Pfeiffer et al., 2020). Utilizin"
2021.emnlp-main.727,2021.acl-short.64,1,0.757218,"ith Aspect Term Code-Switching ∗ Wenxuan Zhang1 , Ruidan He2 , Haiyun Peng2 , Lidong Bing2 and Wai Lam1 1 The Chinese University of Hong Kong 2 DAMO Academy, Alibaba Group {wxzhang,wlam}@se.cuhk.edu.hk {ruidan.he,haiyun.p,l.bing}@alibaba-inc.com Abstract wide application scenarios, it has attracted lots of attention in the NLP community in recent years Many efforts have been made in solving the (Li et al., 2019a; He et al., 2019; Hu et al., 2019; Aspect-based sentiment analysis (ABSA) task. Chen and Qian, 2020; Liang et al., 2020; Mao et al., While most existing studies focus on English 2021; Zhang et al., 2021). texts, handling ABSA in resource-poor lanThe majority of existing ABSA studies are conguages remains a challenging problem. In this paper, we consider the unsupervised crossducted on English texts. However, in real-world lingual transfer for the ABSA task, where only scenarios such as the E-commerce website, users’ labeled data in the source language is available opinions are usually expressed in different lanand we aim at transferring its knowledge to the guages (Pontiki et al., 2016; Keung et al., 2020). target language having no labeled data. To this Manually annotating a large quantity o"
2021.findings-emnlp.237,L18-1157,0,0.0344834,"Missing"
2021.findings-emnlp.237,Q19-1038,0,0.0237917,", and ZH), we construct silver training data following Blloshmi et al. (2020). Specifically, we use OPUS-MT (Tiedemann and Thottingal, 2020)4 , 4 https://huggingface.co/transformers/ model_doc/marian.html 2782 an off-the-shelf translation tool, to translate English sentences in AMR2.0 to other foreign languages. To ensure the quality of silver data, we filter out data with less accurate translations via back-translation consistency check. That is, the translation quality is measured by the cosine similarity between the original English sentence and its back-translated counterpart using LASER (Artetxe and Schwenk, 2019). We refer readers to Blloshmi et al. (2020) for an exhaustive description of the data filtering process. Detailed statistics of our training, dev, and test sets are shown in Table 1. Language English(EN) German(DE) Spanish(ES) Italian(IT) Chinese(ZH) Test 1,371∗ 1,371∗ 1,371∗ 1,371∗ 1,371∗ • w/o KD-FT. To show the benefits from KD, we conduct an ablation experiment where the KD-FT stage (F3) is skipped. The training process becomes P1→P2→F4. • w/o Gold-FT. To validate the necessity of the fine-tuning with gold AMR graph, we also report the model results without the final GoldFT (F4) stage. Th"
2021.findings-emnlp.237,W13-2322,0,0.0493841,"arse all different languages including English. We identify that noisy input and precise output are the key to successful distillation. Together with extensive pre-training, we obtain an AMR parser whose performances surpass all previously published results on four different foreign languages, including German, Spanish, Italian, and Chinese, by large margins (up to 18.8 S MATCH points on Chinese and on average 11.3 S MATCH points). Our parser also achieves comparable performance on English to the latest state-of-the-art Englishonly parser. 1 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a broad-coverage semantic formalism that encodes the meaning of a sentence as a rooted, directed, and labeled graph, where nodes represent concepts and edges represent relations among concepts. AMR parsing is the task of translating natural language sentences into their corresponding AMR graphs, which encompasses a set of natural language understanding tasks, such as named entity recognition, semantic role labeling, and coreference resolution. AMR has proved to be beneficial to a wide range of applications such as text summarization (Liao et al., 2018), machine translation (Song et al., 20"
2021.findings-emnlp.237,S16-1176,0,0.0117734,"imple English AMR parser. Blloshmi et al. (2020) find random noise. that translating the source side of existing English Effect of Data Sizes for Knowledge Distillation AMR dataset into other target languages produces Lastly, we study the relation between model per- better silver training data. Sheth et al. (2021) foformance and the size of monolingual data used cus on improving cross-lingual word-to-node alignfor KD. Figure 3 shows that the S MATCH scores ment for training cross-lingual AMR parsers that 2785 rely on explicit alignment. Our work follows the alignment-free seq2seq formulation (Barzdins and Gosko, 2016; Konstas et al., 2017; Van Noord and Bos, 2017; Peng et al., 2017; Zhang et al., 2019a; Ge et al., 2019; Bevilacqua et al., 2021) and we alternatively study this problem from the perspective of knowledge distillation, which provides a new way to enable multilingual AMR parsing. Knowledge Distillation for Sequence Generation Knowledge distillation (KD) is a classic technique originally proposed for model compression (Buciluˇa et al., 2006; Ba and Caruana, 2014; Hinton et al., 2015). KD suggests training a (smaller) student model to mimic a (larger) teacher model, by minimizing the loss (typica"
2021.findings-emnlp.237,2020.emnlp-main.195,0,0.0582609,"d Lam, 2019, 2020) rely on the textual overlap between English words and AMR node values (i.e., concepts). include the original English test set in our evaluation. On four zero-resource languages, our single universal parser consistently outperforms the previous best results by large margins (+11.3 S MATCH points on average and up to +18.8 S MATCH points). Meanwhile, our parser achieves competitive results on English even compared with the latest state-ofthe-art English AMR parser in the literature. To sum up, our contributions are listed below: Some initial attempts (Damonte and Cohen, 2018; Blloshmi et al., 2020; Sheth et al., 2021) towards multilingual AMR parsing mainly investigated the construction of pseudo parallel data via annotation projection. In this paper, we study multilingual AMR parsing from the perspective of knowledge distillation (Buciluˇa et al., 2006; Ba and Caruana, 2014; Hinton et al., 2015; Kim and Rush, 2016), where our primary goal is to improve a multilingual AMR parser by using an existing English parser as its teacher. We focus on a strict multilingual setting for developing one AMR parser that can parse all different languages. In contrast to the language-specific (one pars"
2021.findings-emnlp.237,D19-1393,1,0.891737,"Missing"
2021.findings-emnlp.237,2020.acl-main.119,1,0.854087,"Missing"
2021.findings-emnlp.237,P13-2131,0,0.066217,"Missing"
2021.findings-emnlp.237,W14-5808,0,0.0350731,"Missing"
2021.findings-emnlp.237,2020.acl-main.747,0,0.0326833,"gued that the method is not informative in terms of the cross-lingual properties of AMR (Damonte and Cohen, 2018; Blloshmi et al., 2020). To tackle cross-lingual AMR parsing, most previous work relies on pre-trained multilingual language models and silver training data (i.e., pseudo parallel data). Pre-trained Multilingual Language Model Previous work proves that language-independent features provided by pre-trained multilingual language models can boost cross-lingual parsing performance. For example, Blloshmi et al. (2020) use mBERT (Devlin et al., 2019) and Sheth et al. (2021) employ XLM-R (Conneau et al., 2020). We present experiments on the benchmark dataset created by Damonte and Cohen (2018), covering four different languages with no training data, Silver Training Data There are two typical including German, Spanish, Italian, and Chinese. methods for creating silver training examples: (I) To cover as many languages as possible, we also Parsing English to AMR (Damonte and Cohen, 2779 2018). This approach creates silver training examples for the foreign language X through an external X-EN parallel corpus and an existing English AMR parser. The English sentences of the parallel corpus are parsed usi"
2021.findings-emnlp.237,P84-1044,0,0.399229,"Missing"
2021.findings-emnlp.237,D18-1232,0,0.0469771,"Missing"
2021.findings-emnlp.237,N18-1104,0,0.112821,"glaubt. El chico quiere que la chica le crea. Il ragazzo vuole che la ragazza gli creda. Figure 1: An example of AMR. Sentences written in English and other languages share the same meaning and therefore correspond to the same AMR graph. English sentences with the same meaning correspond to the same AMR graph. Furthermore, there are no explicit alignments between elements (nodes or edges) in the graph and words in the text. While this property leads to a distinct difficulty in AMR parsing, it also suggests the potential of AMR to work as an interlingua (Xue et al., 2014; Hajiˇc et al., 2014; Damonte and Cohen, 2018), which could be useful to multilingual applications of natural language understanding (Liang et al., 2020; Hu et al., 2020). An example is given in Figure 1, we represent the semantics of semantically-equivalent sentences in other languages using the same AMR graph. This defines the multilingual AMR parsing problem we seek to address in this paper. Multilingual AMR parsing is an extremely challenging task due to several reasons. First, AMR was initially designed for and heavily biased towards English, thus the parsing has to overcome ∗ some structural linguistic divergences among lanThis work"
2021.findings-emnlp.237,N19-1423,0,0.164842,"nte and Cohen, 2018; Uhrig et al., 2021). However, it is argued that the method is not informative in terms of the cross-lingual properties of AMR (Damonte and Cohen, 2018; Blloshmi et al., 2020). To tackle cross-lingual AMR parsing, most previous work relies on pre-trained multilingual language models and silver training data (i.e., pseudo parallel data). Pre-trained Multilingual Language Model Previous work proves that language-independent features provided by pre-trained multilingual language models can boost cross-lingual parsing performance. For example, Blloshmi et al. (2020) use mBERT (Devlin et al., 2019) and Sheth et al. (2021) employ XLM-R (Conneau et al., 2020). We present experiments on the benchmark dataset created by Damonte and Cohen (2018), covering four different languages with no training data, Silver Training Data There are two typical including German, Spanish, Italian, and Chinese. methods for creating silver training examples: (I) To cover as many languages as possible, we also Parsing English to AMR (Damonte and Cohen, 2779 2018). This approach creates silver training examples for the foreign language X through an external X-EN parallel corpus and an existing English AMR parser."
2021.findings-emnlp.237,P14-1134,0,0.0242669,"ative Research (AIR) Program. guages (Damonte and Cohen, 2018; Zhu et al., 2778 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2778–2789 November 7–11, 2021. ©2021 Association for Computational Linguistics 2019). Second, the human-annotated resources for training are only available in English and none is present in other languages. Moreover, since the AMR graph involves rich semantic labels, the AMR annotation for other languages can be laborintensive and unaffordable. Third, current modeling techniques focus mostly on English. For example, existing AMR aligners (Flanigan et al., 2014; Pourdamghani et al., 2014; Liu et al., 2018) and widely-used pointer-generator mechanisms (Zhang et al., 2019b; Cai and Lam, 2019, 2020) rely on the textual overlap between English words and AMR node values (i.e., concepts). include the original English test set in our evaluation. On four zero-resource languages, our single universal parser consistently outperforms the previous best results by large margins (+11.3 S MATCH points on average and up to +18.8 S MATCH points). Meanwhile, our parser achieves competitive results on English even compared with the latest state-ofthe-art English AMR p"
2021.findings-emnlp.237,P16-5005,0,0.0243887,"Missing"
2021.findings-emnlp.237,D16-1139,0,0.343809,"nd up to +18.8 S MATCH points). Meanwhile, our parser achieves competitive results on English even compared with the latest state-ofthe-art English AMR parser in the literature. To sum up, our contributions are listed below: Some initial attempts (Damonte and Cohen, 2018; Blloshmi et al., 2020; Sheth et al., 2021) towards multilingual AMR parsing mainly investigated the construction of pseudo parallel data via annotation projection. In this paper, we study multilingual AMR parsing from the perspective of knowledge distillation (Buciluˇa et al., 2006; Ba and Caruana, 2014; Hinton et al., 2015; Kim and Rush, 2016), where our primary goal is to improve a multilingual AMR parser by using an existing English parser as its teacher. We focus on a strict multilingual setting for developing one AMR parser that can parse all different languages. In contrast to the language-specific (one parser one language) setting, our setting is more challenging yet more appealing in practice. Intuitively, knowledge distillation is effective because the teacher’s output provides a rich training signal for the student parser. We develop both the teacher parser and the student parser with language-agnostic seq2seq design and e"
2021.findings-emnlp.237,2005.mtsummit-papers.11,0,0.0630837,"rser is trained. • Multilingual. One single parser is trained to parse all target languages. While this paper focuses on the multilingual setting, we also report the results of the languagespecific parsers in previous work (Damonte and Cohen, 2018; Blloshmi et al., 2020; Sheth et al., 2021) for comparative reference. 4.3 Dev 1,368∗ 1,319 1,325 1,322 1,311 Table 1: The number of instances per language and for each data split. ∗ marks gold quality and otherwise silver quality. Knowledge Distillation Data For the knowledge distillation stage, we use 320K English sentences in the Europarl corpus (Koehn, 2005), which contains parallel sentence pairs of En⇔DE, En⇔ES, and En⇔IT. Unless otherwise specified, we use sequence-level KD with noisy input from OPUS-MT. Note that essentially our noisy KD only requires monolingual English data. Nevertheless, we choose Europarl following Damonte and Cohen (2018); Blloshmi et al. (2020) and use the gold translations as noise-free input to demonstrate the impact of our noisy KD comparatively (§5.2). 4.2 Train 36,521∗ 34,415 34,552 34,521 33,221 Models Model Variants Our full training pipeline consists of multiple pre-training and fine-tuning stages. To study the"
2021.findings-emnlp.237,P17-1014,0,0.117256,"o vuole che la ragazza gli creda. The boy’s desire is for the girl to believe him. El chico quiere que la chica le crea. parse ( &lt;V0> want-01 :ARG0 ( &lt;V1> boy ) :ARG1 ( &lt;V2> believe-01 :ARG0 ( &lt;V3> girl ) :ARG1 &lt;V1> ) ) match Linearized AMR ( &lt;V0> want-01 :ARG0 ( &lt;V1> boy ) :ARG1 ( &lt;V2> believe-01 :ARG0 ( &lt;V3> girl ) :ARG1 &lt;V1> ) ) parallel Figure 2: Illustration of different training stages. Stage P1 is omitted for space limit. is linearized through a depth-first traversal starting from the root. For edge ordering, we use the default order in the release files of AMR datasets as suggested by Konstas et al. (2017). The bottom right of Figure 2 illustrates the linearization result of the AMR graph in Figure 1. The output sequence of our seq2seq model may produce an invalid graph. For example, the parenthesis parity may be broken, resulting in an incomplete graph. To ensure the validity of the graph produced in parsing, post-processing steps such as parenthesis parity restoration and invalid segment removal are introduced. We use the pre- and postprocessing scripts provided by Bevilacqua et al. (2021).1 3.3 Training Stages We now clarify the four different training stages. The whole training process is r"
2021.findings-emnlp.237,D16-1180,0,0.0272877,"ation, which provides a new way to enable multilingual AMR parsing. Knowledge Distillation for Sequence Generation Knowledge distillation (KD) is a classic technique originally proposed for model compression (Buciluˇa et al., 2006; Ba and Caruana, 2014; Hinton et al., 2015). KD suggests training a (smaller) student model to mimic a (larger) teacher model, by minimizing the loss (typically crossentropy) between the teacher/student predictions (Romero et al., 2015; Yim et al., 2017; Zagoruyko and Komodakis, 2017). KD has been successfully applied to various natural language understanding tasks (Kuncoro et al., 2016; Hu et al., 2018; Sanh et al., 2019). For sequence generation tasks, Kim and Rush (2016) first introduce sequence-level KD, which aims to mimic the teacher’s actions at the sequence-level. KD has been proved useful in a range of sequence generation tasks such as machine translation (Freitag et al., 2017; Tan et al., 2019), non-autoregressive text generation (Gu et al., 2017; Zhou et al., 2019), and text summarization (Liu et al., 2020a). To the best of our knowledge, our paper is the first work to investigate the potential of knowledge distillation in the context of crosslingual AMR parsing."
2021.findings-emnlp.237,2020.acl-main.703,0,0.0171654,"in different languages. Damonte and Cohen (2018) show that it is possible to use the original AMR tant ingredient for superior performance, we also conduct experiments where the reference transla- annotations devised for English as representation for equivalent sentences in other languages and retions in Europarl are used as noise-free input to the lease a cross-lingual AMR evaluation benchmark student. Also, to show that the noise from MT is (Damonte and Cohen, 2020) very recently. Crossnon-trivial, we further employ BART-style random lingual AMR parsing suffers severely from the data noise (Lewis et al., 2020) for comparison. BARTscarcity issue; there is no gold annotated training style noise masks text spans in the input and we tune the rate of word deletion. The results are pre- data for languages other than English. Damonte and Cohen (2018) propose to build silver trainsented in Table 4. We show that MT noise is indeed ing data based on external bitext resources and helpful and its role cannot be replaced by simple English AMR parser. Blloshmi et al. (2020) find random noise. that translating the source side of existing English Effect of Data Sizes for Knowledge Distillation AMR dataset into oth"
2021.findings-emnlp.237,2021.eacl-main.30,0,0.0682866,"on the textual overlap between English words and AMR node values (i.e., concepts). include the original English test set in our evaluation. On four zero-resource languages, our single universal parser consistently outperforms the previous best results by large margins (+11.3 S MATCH points on average and up to +18.8 S MATCH points). Meanwhile, our parser achieves competitive results on English even compared with the latest state-ofthe-art English AMR parser in the literature. To sum up, our contributions are listed below: Some initial attempts (Damonte and Cohen, 2018; Blloshmi et al., 2020; Sheth et al., 2021) towards multilingual AMR parsing mainly investigated the construction of pseudo parallel data via annotation projection. In this paper, we study multilingual AMR parsing from the perspective of knowledge distillation (Buciluˇa et al., 2006; Ba and Caruana, 2014; Hinton et al., 2015; Kim and Rush, 2016), where our primary goal is to improve a multilingual AMR parser by using an existing English parser as its teacher. We focus on a strict multilingual setting for developing one AMR parser that can parse all different languages. In contrast to the language-specific (one parser one language) sett"
2021.findings-emnlp.237,C18-1101,0,0.0117174,"ct Meaning Representation (AMR) (Banarescu et al., 2013) is a broad-coverage semantic formalism that encodes the meaning of a sentence as a rooted, directed, and labeled graph, where nodes represent concepts and edges represent relations among concepts. AMR parsing is the task of translating natural language sentences into their corresponding AMR graphs, which encompasses a set of natural language understanding tasks, such as named entity recognition, semantic role labeling, and coreference resolution. AMR has proved to be beneficial to a wide range of applications such as text summarization (Liao et al., 2018), machine translation (Song et al., 2019), and question answering (Kapanipathi et al., 2020; Xu et al., 2021). One most critical feature of the AMR formalism is that it abstracts away from syntactic realization and surface forms. As shown in Figure 1, different English The boy wants the girl to believe him. The boy’s desire is for the girl to believe him. The boy wants to be believed by the girl. AMR want-01 :ARG0 boy :ARG1 :ARG1 believe-01 :ARG0 girl German, Spanish, Italian, and Chinese Der Junge möchte, dass das Mädchen ihm glaubt. El chico quiere que la chica le crea. Il ragazzo vuole che"
2021.findings-emnlp.237,2021.naacl-main.56,0,0.0476053,"Missing"
2021.findings-emnlp.237,D18-1264,0,0.0141859,"Cohen, 2018; Zhu et al., 2778 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2778–2789 November 7–11, 2021. ©2021 Association for Computational Linguistics 2019). Second, the human-annotated resources for training are only available in English and none is present in other languages. Moreover, since the AMR graph involves rich semantic labels, the AMR annotation for other languages can be laborintensive and unaffordable. Third, current modeling techniques focus mostly on English. For example, existing AMR aligners (Flanigan et al., 2014; Pourdamghani et al., 2014; Liu et al., 2018) and widely-used pointer-generator mechanisms (Zhang et al., 2019b; Cai and Lam, 2019, 2020) rely on the textual overlap between English words and AMR node values (i.e., concepts). include the original English test set in our evaluation. On four zero-resource languages, our single universal parser consistently outperforms the previous best results by large margins (+11.3 S MATCH points on average and up to +18.8 S MATCH points). Meanwhile, our parser achieves competitive results on English even compared with the latest state-ofthe-art English AMR parser in the literature. To sum up, our contri"
2021.findings-emnlp.237,2020.tacl-1.47,0,0.261224,"cross-lingual language understanding tasks. For cross-lingual AMR parsing, in particular, Blloshmi et al. (2020) used mBERT2 (Devlin et al., 2019) while Sheth et al. (2021) employed XLM-R3 (Conneau et al., 1 https://github.com/SapienzaNLP/spring 2 bert-base-multilingual-cased 3 xlm-roberta-large 2020) to provide language-independent features. Unlike previous work, we argue that such encoderonly pre-trained models are not the most suitable choice for our seq2seq parser. Instead, we adopt mBART, an encoder-decoder denoising language model pre-trained with monolingual corpora in many languages (Liu et al., 2020b), to initialize both the encoder and decoder of our seq2seq parser. P2: Multilingual Machine Translation Pretraining (MMT-PT) The task of multilingual machine translation (MMT) is to learn one single model to translate between various language pairs. Essentially, natural languages can be considered as informal meaning representations compared to formal meaning representation such as AMR. On the other hand, AMR can be regarded as a special language. The above observations connect the dots between MMT and multilingual AMR parsing, both of which model the process of digesting the semantics in o"
2021.findings-emnlp.237,E17-1035,0,0.0132477,"translating the source side of existing English Effect of Data Sizes for Knowledge Distillation AMR dataset into other target languages produces Lastly, we study the relation between model per- better silver training data. Sheth et al. (2021) foformance and the size of monolingual data used cus on improving cross-lingual word-to-node alignfor KD. Figure 3 shows that the S MATCH scores ment for training cross-lingual AMR parsers that 2785 rely on explicit alignment. Our work follows the alignment-free seq2seq formulation (Barzdins and Gosko, 2016; Konstas et al., 2017; Van Noord and Bos, 2017; Peng et al., 2017; Zhang et al., 2019a; Ge et al., 2019; Bevilacqua et al., 2021) and we alternatively study this problem from the perspective of knowledge distillation, which provides a new way to enable multilingual AMR parsing. Knowledge Distillation for Sequence Generation Knowledge distillation (KD) is a classic technique originally proposed for model compression (Buciluˇa et al., 2006; Ba and Caruana, 2014; Hinton et al., 2015). KD suggests training a (smaller) student model to mimic a (larger) teacher model, by minimizing the loss (typically crossentropy) between the teacher/student predictions (Romero"
2021.findings-emnlp.237,D14-1048,0,0.0229144,"ogram. guages (Damonte and Cohen, 2018; Zhu et al., 2778 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2778–2789 November 7–11, 2021. ©2021 Association for Computational Linguistics 2019). Second, the human-annotated resources for training are only available in English and none is present in other languages. Moreover, since the AMR graph involves rich semantic labels, the AMR annotation for other languages can be laborintensive and unaffordable. Third, current modeling techniques focus mostly on English. For example, existing AMR aligners (Flanigan et al., 2014; Pourdamghani et al., 2014; Liu et al., 2018) and widely-used pointer-generator mechanisms (Zhang et al., 2019b; Cai and Lam, 2019, 2020) rely on the textual overlap between English words and AMR node values (i.e., concepts). include the original English test set in our evaluation. On four zero-resource languages, our single universal parser consistently outperforms the previous best results by large margins (+11.3 S MATCH points on average and up to +18.8 S MATCH points). Meanwhile, our parser achieves competitive results on English even compared with the latest state-ofthe-art English AMR parser in the literature. To"
2021.findings-emnlp.237,Q19-1002,0,0.0182946,"u et al., 2013) is a broad-coverage semantic formalism that encodes the meaning of a sentence as a rooted, directed, and labeled graph, where nodes represent concepts and edges represent relations among concepts. AMR parsing is the task of translating natural language sentences into their corresponding AMR graphs, which encompasses a set of natural language understanding tasks, such as named entity recognition, semantic role labeling, and coreference resolution. AMR has proved to be beneficial to a wide range of applications such as text summarization (Liao et al., 2018), machine translation (Song et al., 2019), and question answering (Kapanipathi et al., 2020; Xu et al., 2021). One most critical feature of the AMR formalism is that it abstracts away from syntactic realization and surface forms. As shown in Figure 1, different English The boy wants the girl to believe him. The boy’s desire is for the girl to believe him. The boy wants to be believed by the girl. AMR want-01 :ARG0 boy :ARG1 :ARG1 believe-01 :ARG0 girl German, Spanish, Italian, and Chinese Der Junge möchte, dass das Mädchen ihm glaubt. El chico quiere que la chica le crea. Il ragazzo vuole che la ragazza gli creda. Figure 1: An exampl"
2021.findings-emnlp.237,2020.eamt-1.61,0,0.0248388,"Missing"
2021.findings-emnlp.237,2021.iwpt-1.6,0,0.0221391,"all different languages including English. • We obtain a performant multilingual AMR parser, establishing new state-of-the-art results on multiple languages. We hope our parser can facilitate the multilingual applications of AMR. 2 2.1 Background Prior Work Cross-lingual AMR parsing is the task of mapping a sentence in any language X to the AMR graph of its English translation. To date, there is no human-annotated X-AMR parallel dataset for training. Therefore, one straightforward solution is to translate the sentences from X into English then apply an English parser (Damonte and Cohen, 2018; Uhrig et al., 2021). However, it is argued that the method is not informative in terms of the cross-lingual properties of AMR (Damonte and Cohen, 2018; Blloshmi et al., 2020). To tackle cross-lingual AMR parsing, most previous work relies on pre-trained multilingual language models and silver training data (i.e., pseudo parallel data). Pre-trained Multilingual Language Model Previous work proves that language-independent features provided by pre-trained multilingual language models can boost cross-lingual parsing performance. For example, Blloshmi et al. (2020) use mBERT (Devlin et al., 2019) and Sheth et al. (2"
2021.findings-emnlp.237,2020.emnlp-main.196,0,0.0220711,"guage pairs. Essentially, natural languages can be considered as informal meaning representations compared to formal meaning representation such as AMR. On the other hand, AMR can be regarded as a special language. The above observations connect the dots between MMT and multilingual AMR parsing, both of which model the process of digesting the semantics in one form and and conveying the same semantics in another form. Therefore, we argue that pre-training our parser using the MMT task should be helpful. In fact, the usefulness of MT pre-training has also been validated in English AMR parsing (Xu et al., 2020). In practice, we directly use the mBARTmmt checkpoint (Tang et al., 2020), an MMT model covering 50 languages that are trained from mBART. F3: Knowledge Distillation Fine-tuning (KDFT) Motivated by the fact that the parsing accuracy on English is significantly better than those 2781 on other languages, we propose to reduce the performance gap via knowledge distillation (Kim and Rush, 2016). Specifically, we first pre-train a highperformance AMR parser for English and treat it as the teacher model. By considering our multilingual AMR parser as the student model, the goal is to transfer the kno"
2021.findings-emnlp.237,2021.findings-acl.90,1,0.720625,"e meaning of a sentence as a rooted, directed, and labeled graph, where nodes represent concepts and edges represent relations among concepts. AMR parsing is the task of translating natural language sentences into their corresponding AMR graphs, which encompasses a set of natural language understanding tasks, such as named entity recognition, semantic role labeling, and coreference resolution. AMR has proved to be beneficial to a wide range of applications such as text summarization (Liao et al., 2018), machine translation (Song et al., 2019), and question answering (Kapanipathi et al., 2020; Xu et al., 2021). One most critical feature of the AMR formalism is that it abstracts away from syntactic realization and surface forms. As shown in Figure 1, different English The boy wants the girl to believe him. The boy’s desire is for the girl to believe him. The boy wants to be believed by the girl. AMR want-01 :ARG0 boy :ARG1 :ARG1 believe-01 :ARG0 girl German, Spanish, Italian, and Chinese Der Junge möchte, dass das Mädchen ihm glaubt. El chico quiere que la chica le crea. Il ragazzo vuole che la ragazza gli creda. Figure 1: An example of AMR. Sentences written in English and other languages share the"
2021.findings-emnlp.237,xue-etal-2014-interlingua,0,0.0627008,"Missing"
2021.findings-emnlp.237,P19-1009,0,0.0336454,"Missing"
2021.findings-emnlp.237,D19-1392,0,0.0214797,"Missing"
2021.findings-emnlp.237,W19-3320,0,0.0259951,"Missing"
2021.findings-emnlp.390,Q17-1010,0,0.00733991,"urs, we concatenate the question and answer sequences as their inputs for them to utilize the answer information. For our proposed model, we report the results for the following variants: Base Model, which only uses LU to train the model; Base+ATE, where the base model is augmented with the ATE task using L as the loss function; Base+QA, where the base model is augmented with the pre-training of QA pair matching; Full Model, our full model involving both auxiliary tasks.7 3.3 Experimental Settings For baseline models using pre-trained word vectors, we use cc.zh.300.vec8 trained with fastText (Bojanowski et al., 2017) for fair comparison. For BERT-based models including ours, we use the same pre-trained BERT-Base,Chinese9 in all experiments, which includes 12 transformer layers and the hidden dimension dh is 768. For our proposed model, the parameters of BERT is further fine-tuned during the training process. Regarding the network architectures, the hidden dimension of the answer encoding module da is 300, the dimension of the encoded answer vector de is 64. For the local context capturing layer, the 7 The code is publicly available at https://github. com/IsakZhang/ABSA-QA. 8 https://github.com/facebookres"
2021.findings-emnlp.390,P17-1152,0,0.251728,"ation from the obtain the related opinion information, which is answer to help extract the discussed aspect. the “value” part. Similarly, we can also obtain 4584 the question-attended answer representation as ¯ a = ATTN(H a , H q ). H The matched information from the answer can well indicate the mentioned aspects. For example, it may rephrase or simply repeat the aspect term asked in the question and then present their senti¯q ment. To combine the attended representations H and the original representations H q , a multi-layer perceptron is typically involved in solving the text matching task (Chen et al., 2017; Yang et al., 2019). However, since we are tackling a token-level prediction problem, such a fusion method would obscure the fine-grained feature representations. We propose a gated fusion approach to absorb the aspect information from the answer while also maintain the most salient information in each question token. Concretely, for the i-th word, we have: ¯ q + bg ) g = σ(W r hqi + W a h i q q ˜ ¯q h = g h + (1 − g) h i i i (2) (3) where W r and W a are trainable parameters, σ and denote the sigmoid function and the element˜q wise multiplication respectively. The resulting h t represents th"
2021.findings-emnlp.390,2020.acl-main.582,0,0.152581,"ictions are marked with 3/7 respectively. 3.4.4 Case Analysis task aims to detect the mentioned aspect (He et al., 2017; Xu et al., 2019; Tulkens and van CranenWe present some sample cases including input burgh, 2020; Li et al., 2020; Wei et al., 2020). The QA pairs and predictions given by the baseline second aspect sentiment classification (ASC) task Span-Pipeline model, our proposed base model then predicts the sentiment polarity, assuming an and the full model in Table 4. We can see that aspect is given (Sun et al., 2019; Tang et al., 2020; Span-Pipeline fails when the alignment is needed Chen et al., 2020b; Zheng et al., 2020). between the question and answer sentences. For Since separately handling these two tasks igexample, the second answer A2 only comments on nores the relations between them and leads to unthe “last long” aspect, thus Span-Pipeline just ransatisfactory performance, recent works attempt to domly assigns a sentiment polarity for the “mask solve it in a unified framework. These studies either blemishes”. Regarding the third question Q3 , its adopt a unified tagging scheme (Li et al., 2019b,a; answer expresses “okay” to both aspects mentioned Hu et al., 2019) or solving them i"
2021.findings-emnlp.390,2020.acl-main.338,0,0.158998,"ictions are marked with 3/7 respectively. 3.4.4 Case Analysis task aims to detect the mentioned aspect (He et al., 2017; Xu et al., 2019; Tulkens and van CranenWe present some sample cases including input burgh, 2020; Li et al., 2020; Wei et al., 2020). The QA pairs and predictions given by the baseline second aspect sentiment classification (ASC) task Span-Pipeline model, our proposed base model then predicts the sentiment polarity, assuming an and the full model in Table 4. We can see that aspect is given (Sun et al., 2019; Tang et al., 2020; Span-Pipeline fails when the alignment is needed Chen et al., 2020b; Zheng et al., 2020). between the question and answer sentences. For Since separately handling these two tasks igexample, the second answer A2 only comments on nores the relations between them and leads to unthe “last long” aspect, thus Span-Pipeline just ransatisfactory performance, recent works attempt to domly assigns a sentiment polarity for the “mask solve it in a unified framework. These studies either blemishes”. Regarding the third question Q3 , its adopt a unified tagging scheme (Li et al., 2019b,a; answer expresses “okay” to both aspects mentioned Hu et al., 2019) or solving them i"
2021.findings-emnlp.390,2020.acl-main.340,0,0.473455,"evaluated on three real-world datasets and the results show that our model outperforms several strong baselines adopted from related state-of-the-art models. Q: How about the screen? Is this phone’s battery life durable? Thanks in advance! A: Not as large as I thought. But the battery is quite good, I like it. TASK INPUT OUTPUT ATE-QA QA pair [screen]; [battery life] QA pair + [screen] NEG ASC-QA QA pair + [battery life] POS [screen] NEG ABSA-QA QA pair [battery life] POS Figure 1: Demonstrations of ABSA-QA task and its two sub-tasks including ATE-QA and ASC-QA. prediction (Li et al., 2019a; Chen and Qian, 2020a; Mao et al., 2021; Zhang et al., 2021) have received increasing attention in recent years. Most existing ABSA studies focus on a single opinionated sentence such as the customer review (Pontiki et al., 2014, 2015). Besides product reviews, another kind of opinion sharing platform, namely question answering (QA) forum, has been provided on many E-commerce websites, due to the rising demand for users and sellers to communicate with the former buyers to obtain their opinions towards various aspects of the concerned 1 Introduction product (Zhang et al., 2020b). Thus, investigatAspect-based senti"
2021.findings-emnlp.390,P17-1036,0,0.123915,"e sentence “The feel of the restaurant was Several attempts have been made on analyzing crowded but the food is great.”, ATE is to detect the sentiment information in QA forums. However, the mentioned aspects “feel” and “food”, whereas they either predict an overall sentiment polarity tosupposing aspects are given, ASC predicts their wards the entire QA pair (Shen et al., 2018; Hu sentiment polarities as negative and positive re- et al., 2020) or only consider partial ABSA-QA spectively. Given the broad application scenarios, problems. For example, Wang et al. (2019) tackle the two sub-tasks (He et al., 2017; Sun et al., 2019; the ASC-QA task under the assumption that the Tulkens and van Cranenburgh, 2020) and their joint targeted aspects are given. As illustrated in Figure ∗ 1, they perform aspect-level sentiment classificaThe work described in this paper is substantially supported by a grant from the Research Grant Council of the tion according to both of the QA pair and the input Hong Kong Special Administrative Region, China (Project aspect. However, obtaining the discussed aspects Code: 14200719). Work partially done when Wenxuan Zhang was an intern at Alibaba. is not a trivial task, which i"
2021.findings-emnlp.390,P19-1048,0,0.261327,"l of the tion according to both of the QA pair and the input Hong Kong Special Administrative Region, China (Project aspect. However, obtaining the discussed aspects Code: 14200719). Work partially done when Wenxuan Zhang was an intern at Alibaba. is not a trivial task, which is especially difficult 4582 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4582–4591 November 7–11, 2021. ©2021 Association for Computational Linguistics for those QA pairs involving multiple aspects. Inspired by previous success on jointly solving the two sub-tasks in review-oriented ABSA (He et al., 2019; Luo et al., 2019; Chen and Qian, 2020a), we aim to handle the ABSA-QA task in a similar unified setting in this work1 . As shown in Figure 1, given a question-answer pair, our goal is to jointly detect the discussed aspect(s) and predict their corresponding sentiment polarities. To tackle the ABSA-QA task, an intuitive idea would be concatenating the question and answer sentence, then employing the existing ABSA models to solve it. However, the question and answer sentence are two parallel sequences, therefore, simply concatenating them cannot produce a semanticfluent expression. In such a c"
2021.findings-emnlp.390,P19-1051,0,0.295664,"entiment polarities. To tackle the ABSA-QA task, an intuitive idea would be concatenating the question and answer sentence, then employing the existing ABSA models to solve it. However, the question and answer sentence are two parallel sequences, therefore, simply concatenating them cannot produce a semanticfluent expression. In such a concatenation, the aspect terms and their corresponding opinion words do not appear next or near to each other, making the position clue utilized by many ABSA models, i.e., the aspect modifier is closer to the corresponding aspect term in the sentence, invalid (Hu et al., 2019; He et al., 2019). To make matters worse, it will result in wrong proximity relation, for instance, compared with “quite good”, “not as large as” is nearer to “battery life” in the example. Meanwhile, because the opinions are expressed in an interactive manner, i.e., the question asks about one or multiple aspects and the answer expresses the opinions towards them, the aspect terms are likely to be omitted or rephrased in the answer sentence. Returning to the example in Figure 1, the aspect “battery life” is shortened to “battery” while the explicit mention of the aspect “screen” is directly"
2021.findings-emnlp.390,2020.acl-main.631,0,0.0123657,"[质量]POS 3 [quality]POS [质量]POS 7 [质量]POS 7 [质量]NEG 3 [quality]POS [quality]POS [quality]NEG None 7 Table 4: Case analysis. The “Examples” column contains sample QA pairs with gold labels where words in brackets are annotated aspect terms, the subscripts denotes their sentiment polarities. “None” in predictions denotes that no aspect terms are extracted. The correct/incorrect predictions are marked with 3/7 respectively. 3.4.4 Case Analysis task aims to detect the mentioned aspect (He et al., 2017; Xu et al., 2019; Tulkens and van CranenWe present some sample cases including input burgh, 2020; Li et al., 2020; Wei et al., 2020). The QA pairs and predictions given by the baseline second aspect sentiment classification (ASC) task Span-Pipeline model, our proposed base model then predicts the sentiment polarity, assuming an and the full model in Table 4. We can see that aspect is given (Sun et al., 2019; Tang et al., 2020; Span-Pipeline fails when the alignment is needed Chen et al., 2020b; Zheng et al., 2020). between the question and answer sentences. For Since separately handling these two tasks igexample, the second answer A2 only comments on nores the relations between them and leads to unthe “l"
2021.findings-emnlp.390,D19-5505,1,0.565803,"proposed method is evaluated on three real-world datasets and the results show that our model outperforms several strong baselines adopted from related state-of-the-art models. Q: How about the screen? Is this phone’s battery life durable? Thanks in advance! A: Not as large as I thought. But the battery is quite good, I like it. TASK INPUT OUTPUT ATE-QA QA pair [screen]; [battery life] QA pair + [screen] NEG ASC-QA QA pair + [battery life] POS [screen] NEG ABSA-QA QA pair [battery life] POS Figure 1: Demonstrations of ABSA-QA task and its two sub-tasks including ATE-QA and ASC-QA. prediction (Li et al., 2019a; Chen and Qian, 2020a; Mao et al., 2021; Zhang et al., 2021) have received increasing attention in recent years. Most existing ABSA studies focus on a single opinionated sentence such as the customer review (Pontiki et al., 2014, 2015). Besides product reviews, another kind of opinion sharing platform, namely question answering (QA) forum, has been provided on many E-commerce websites, due to the rising demand for users and sellers to communicate with the former buyers to obtain their opinions towards various aspects of the concerned 1 Introduction product (Zhang et al., 2020b). Thus, invest"
2021.findings-emnlp.390,P19-1056,0,0.0909167,"ording to both of the QA pair and the input Hong Kong Special Administrative Region, China (Project aspect. However, obtaining the discussed aspects Code: 14200719). Work partially done when Wenxuan Zhang was an intern at Alibaba. is not a trivial task, which is especially difficult 4582 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4582–4591 November 7–11, 2021. ©2021 Association for Computational Linguistics for those QA pairs involving multiple aspects. Inspired by previous success on jointly solving the two sub-tasks in review-oriented ABSA (He et al., 2019; Luo et al., 2019; Chen and Qian, 2020a), we aim to handle the ABSA-QA task in a similar unified setting in this work1 . As shown in Figure 1, given a question-answer pair, our goal is to jointly detect the discussed aspect(s) and predict their corresponding sentiment polarities. To tackle the ABSA-QA task, an intuitive idea would be concatenating the question and answer sentence, then employing the existing ABSA models to solve it. However, the question and answer sentence are two parallel sequences, therefore, simply concatenating them cannot produce a semanticfluent expression. In such a concatenation, the"
2021.findings-emnlp.390,P19-1465,0,0.127581,"in the related opinion information, which is answer to help extract the discussed aspect. the “value” part. Similarly, we can also obtain 4584 the question-attended answer representation as ¯ a = ATTN(H a , H q ). H The matched information from the answer can well indicate the mentioned aspects. For example, it may rephrase or simply repeat the aspect term asked in the question and then present their senti¯q ment. To combine the attended representations H and the original representations H q , a multi-layer perceptron is typically involved in solving the text matching task (Chen et al., 2017; Yang et al., 2019). However, since we are tackling a token-level prediction problem, such a fusion method would obscure the fine-grained feature representations. We propose a gated fusion approach to absorb the aspect information from the answer while also maintain the most salient information in each question token. Concretely, for the i-th word, we have: ¯ q + bg ) g = σ(W r hqi + W a h i q q ˜ ¯q h = g h + (1 − g) h i i i (2) (3) where W r and W a are trainable parameters, σ and denote the sigmoid function and the element˜q wise multiplication respectively. The resulting h t represents the fused representati"
2021.findings-emnlp.390,D13-1171,0,0.0871582,"Missing"
2021.findings-emnlp.390,S15-2082,0,0.0727637,"Missing"
2021.findings-emnlp.390,S14-2004,0,0.124535,"Missing"
2021.findings-emnlp.390,N19-1035,0,0.235306,"eel of the restaurant was Several attempts have been made on analyzing crowded but the food is great.”, ATE is to detect the sentiment information in QA forums. However, the mentioned aspects “feel” and “food”, whereas they either predict an overall sentiment polarity tosupposing aspects are given, ASC predicts their wards the entire QA pair (Shen et al., 2018; Hu sentiment polarities as negative and positive re- et al., 2020) or only consider partial ABSA-QA spectively. Given the broad application scenarios, problems. For example, Wang et al. (2019) tackle the two sub-tasks (He et al., 2017; Sun et al., 2019; the ASC-QA task under the assumption that the Tulkens and van Cranenburgh, 2020) and their joint targeted aspects are given. As illustrated in Figure ∗ 1, they perform aspect-level sentiment classificaThe work described in this paper is substantially supported by a grant from the Research Grant Council of the tion according to both of the QA pair and the input Hong Kong Special Administrative Region, China (Project aspect. However, obtaining the discussed aspects Code: 14200719). Work partially done when Wenxuan Zhang was an intern at Alibaba. is not a trivial task, which is especially diffi"
2021.findings-emnlp.390,2021.acl-short.64,1,0.703495,"and the results show that our model outperforms several strong baselines adopted from related state-of-the-art models. Q: How about the screen? Is this phone’s battery life durable? Thanks in advance! A: Not as large as I thought. But the battery is quite good, I like it. TASK INPUT OUTPUT ATE-QA QA pair [screen]; [battery life] QA pair + [screen] NEG ASC-QA QA pair + [battery life] POS [screen] NEG ABSA-QA QA pair [battery life] POS Figure 1: Demonstrations of ABSA-QA task and its two sub-tasks including ATE-QA and ASC-QA. prediction (Li et al., 2019a; Chen and Qian, 2020a; Mao et al., 2021; Zhang et al., 2021) have received increasing attention in recent years. Most existing ABSA studies focus on a single opinionated sentence such as the customer review (Pontiki et al., 2014, 2015). Besides product reviews, another kind of opinion sharing platform, namely question answering (QA) forum, has been provided on many E-commerce websites, due to the rising demand for users and sellers to communicate with the former buyers to obtain their opinions towards various aspects of the concerned 1 Introduction product (Zhang et al., 2020b). Thus, investigatAspect-based sentiment analysis (ABSA) usually ing the ABS"
2021.findings-emnlp.390,2020.acl-main.296,0,0.0360922,"re representaSpan-Pipeline. Our proposed model, both the base and full model successfully handle these two cases, tions (He et al., 2019; Luo et al., 2019). Recently, showing the necessity to model the interactions be- there are also some attempts of combining another tween the given QA pairs. For the last example Q4 , related task, namely opinion term extraction (OTE), with the ATE and/or ASC tasks to provide a more the answer does not provide any direct comment on the asked aspects, for instance, it does not men- complete understanding of the aspect-level user sentiment (Chen et al., 2020a; Zhao et al., 2020; tion aspect “quality” or any related opinion term Chen and Qian, 2020b; Liang et al., 2020; Zhang such as “bad” at all, making it difficult to predict et al., 2021). the sentiment polarity. Our proposed full model However, most existing studies target at cusequipped with the QA matching pre-training gives tomer reviews (Pontiki et al., 2014, 2015) or twitter correct predictions on them, which attributes to the posts (Mitchell et al., 2013). Thus the proposed pre-training that brings in some prior knowledge methods are often tailored for observations made in for identifying that the answer is"
2021.findings-emnlp.390,2020.acl-main.588,0,0.0349641,"notes that no aspect terms are extracted. The correct/incorrect predictions are marked with 3/7 respectively. 3.4.4 Case Analysis task aims to detect the mentioned aspect (He et al., 2017; Xu et al., 2019; Tulkens and van CranenWe present some sample cases including input burgh, 2020; Li et al., 2020; Wei et al., 2020). The QA pairs and predictions given by the baseline second aspect sentiment classification (ASC) task Span-Pipeline model, our proposed base model then predicts the sentiment polarity, assuming an and the full model in Table 4. We can see that aspect is given (Sun et al., 2019; Tang et al., 2020; Span-Pipeline fails when the alignment is needed Chen et al., 2020b; Zheng et al., 2020). between the question and answer sentences. For Since separately handling these two tasks igexample, the second answer A2 only comments on nores the relations between them and leads to unthe “last long” aspect, thus Span-Pipeline just ransatisfactory performance, recent works attempt to domly assigns a sentiment polarity for the “mask solve it in a unified framework. These studies either blemishes”. Regarding the third question Q3 , its adopt a unified tagging scheme (Li et al., 2019b,a; answer expresses"
2021.findings-emnlp.390,2020.acl-main.290,0,0.0312401,"Missing"
2021.findings-emnlp.390,P19-1345,0,0.071119,"ontiki et al., 2014). For an QA pairs. example sentence “The feel of the restaurant was Several attempts have been made on analyzing crowded but the food is great.”, ATE is to detect the sentiment information in QA forums. However, the mentioned aspects “feel” and “food”, whereas they either predict an overall sentiment polarity tosupposing aspects are given, ASC predicts their wards the entire QA pair (Shen et al., 2018; Hu sentiment polarities as negative and positive re- et al., 2020) or only consider partial ABSA-QA spectively. Given the broad application scenarios, problems. For example, Wang et al. (2019) tackle the two sub-tasks (He et al., 2017; Sun et al., 2019; the ASC-QA task under the assumption that the Tulkens and van Cranenburgh, 2020) and their joint targeted aspects are given. As illustrated in Figure ∗ 1, they perform aspect-level sentiment classificaThe work described in this paper is substantially supported by a grant from the Research Grant Council of the tion according to both of the QA pair and the input Hong Kong Special Administrative Region, China (Project aspect. However, obtaining the discussed aspects Code: 14200719). Work partially done when Wenxuan Zhang was an intern"
2021.findings-emnlp.390,2020.acl-main.339,0,0.0231202,"y]POS [质量]POS 7 [质量]POS 7 [质量]NEG 3 [quality]POS [quality]POS [quality]NEG None 7 Table 4: Case analysis. The “Examples” column contains sample QA pairs with gold labels where words in brackets are annotated aspect terms, the subscripts denotes their sentiment polarities. “None” in predictions denotes that no aspect terms are extracted. The correct/incorrect predictions are marked with 3/7 respectively. 3.4.4 Case Analysis task aims to detect the mentioned aspect (He et al., 2017; Xu et al., 2019; Tulkens and van CranenWe present some sample cases including input burgh, 2020; Li et al., 2020; Wei et al., 2020). The QA pairs and predictions given by the baseline second aspect sentiment classification (ASC) task Span-Pipeline model, our proposed base model then predicts the sentiment polarity, assuming an and the full model in Table 4. We can see that aspect is given (Sun et al., 2019; Tang et al., 2020; Span-Pipeline fails when the alignment is needed Chen et al., 2020b; Zheng et al., 2020). between the question and answer sentences. For Since separately handling these two tasks igexample, the second answer A2 only comments on nores the relations between them and leads to unthe “last long” aspect, t"
2021.findings-emnlp.390,N19-1242,0,0.0414769,"Missing"
2021.naacl-main.271,C18-1139,0,0.0135403,"P RODUCT entity if a model relies more on the context “begin trading with” but ignores the hidden information that “PCP” is the symbol of “Precision Castparts Corp.”. Previous research works (Li et al., 2017; Jie and 1 Introduction Lu, 2019; Wang et al., 2019) have been using the Named entity recognition (NER) is one of the parse trees (Chomsky, 1956, 1969; Sandra and Taft, most fundamental and important tasks in natu- 2014) to incorporate such structured information. ral language processing (NLP). While the litera- Figure 1 (Dependency Path) shows that the first enture (Peters et al., 2018; Akbik et al., 2018; De- tity can be connected to the second entity following vlin et al., 2019) largely focuses on training deep the dependency tree with 5 hops. Incorporating the language models to improve the contextualized dependency information can be done with graph word representations, previous studies show that neural networks (GNNs) such as graph convolutional networks (GCNs) (Kipf and Welling, 2017). ∗ Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. The However, simply stacking the LSTM and GCN work was done when Zhanming Jie was a PhD student in"
2021.naacl-main.271,Q16-1026,0,0.0927949,"r GCN. Without GCN at all, the score drops by 1.13 F1 . The original dependency contributes 0.27 F1 score. Removing the dependency relation embedding also decreases the performance by 0.27 F1 . When we remove the POS tags embedding, the result drops by 0.39 F1 . NER Early work (Sasano and Kurohashi, 2008) uses syntactic dependency features to improve the SVM performance on Japanese NER task. Liu et al. (2010) propose to construct skip-edges to link similar words or words having typed dependencies to capture long-range dependencies. The later works (Collobert et al., 2010; Lample et al., 2016; Chiu and Nichols, 2016b) focus on using neural networks to extract features and achieved the stateof-the-art performance. Jie et al. (2017) find that some relations between the dependency edges and the entities can be used to reduce the search space of their model, which significantly reduces the time complexity. Yu et al. (2020) employ pre-trained language model to encode document-level information to explore all spans with the graph-based dependency graph based ideas. The pre-trained language models (e.g., BERT (Devlin et al., 2019), ELMO (Peters et al., 2018)) further improve neuralbased approaches with a good c"
2021.naacl-main.271,C18-1161,0,0.0375604,"Missing"
2021.naacl-main.271,L18-1550,0,0.014406,"al. Following the work 3460 by Jie and Lu (2019), we transform the parse trees into the Stanford dependency trees (De Marneffe and Manning, 2008) by using Stanford CoreNLP (Manning et al., 2014). Detailed statistics of each dataset can be found in Table 1. Intuitively, longer sentences would require the model to capture more long-distance interactions in the sentences. We present the number of entities in terms of different sentence lengths to show that these datasets have a modest amount of entities in long sentences. Experimental Setup For Catalan, Spanish, and Chinese, we use the FastText (Grave et al., 2018) 300 dimensional embeddings to initialize the word embeddings. For OntoNotes 5.0 English, we adopt the publicly available GloVE (Pennington et al., 2014) 100 dimensional embeddings to initialize the word embeddings. For experiments with the contextualized representation, we adopt the pre-trained language model BERT (Devlin et al., 2019) for the four datasets. Specifically, we use bert-as-service (Xiao, 2018) to generate the contextualized word representation without fine-tuning. Following Luo et al. (2020), we use the cased version of BERT large model for the experiments on the OntoNotes 5.0 E"
2021.naacl-main.271,D19-1399,1,0.347676,"ne with graph word representations, previous studies show that neural networks (GNNs) such as graph convolutional networks (GCNs) (Kipf and Welling, 2017). ∗ Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. The However, simply stacking the LSTM and GCN work was done when Zhanming Jie was a PhD student in architectures for NER can only provide us with Singapore University of Technology and Design. 1 modest improvements; sometimes, it decreases perWe make our code publicly available at https:// github.com/xuuuluuu/SynLSTM-for-NER. formance (Jie and Lu, 2019). Based on the depen3457 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3457–3469 June 6–11, 2021. ©2021 Association for Computational Linguistics dency path in Figure 1, it requires a 5-layer GCN to capture the connections between these two entities. However, deep GCN architectures often face training difficulties, which cause a performance drop (Hamilton et al., 2017b; Kipf and Welling, 2017). Directly stacking GCN and LSTM has difficulties in modeling the interaction between dependency tre"
2021.naacl-main.271,N16-1030,0,0.472356,"he number of state ˜st to represent the cell state that corresponds words. Our model is mainly constructed with three to the graph-encoded representation separately. layers: input representation layer, bi-directional With the proposed Syn-LSTM, the structured Syn-LSTM layer, and CRF layer. The architecture information captured by the dependency trees can of our Syn-LSTM-CRF is shown in Figure 3. be passed to each cell, and the additional gate mt is able to control how much structured information Input Representation Layer Similar to the can be incorporated. The additional gate enables work by Lample et al. (2016), our input representathe model to feed the contextual and structured tion also includes the character embeddings, which information into the LSTM cell separately. Such are the hidden states of character-based BiLSTM. a mechanism allows our model to aggregate the Jie and Lu (2019) highlight that the dependency information from linear sequence and dependency relation helps to enhance the input representation. trees selectively. Furthermore, previous methods (Wang et al., 2018; 3459 Wang and Lu, 2018) use embeddings of part-ofspeech (POS) tags as additional input representation. The input repres"
2021.naacl-main.271,P18-2116,0,0.0181568,"affected by the graph-encoded representation gt . The cell state ct and hidden state ht are computed as follows: ft = σ(W (f ) xt + U (f ) ht−1 + Q(f ) gt + b(f ) ) (1) ot = σ(W (o) xt + U (o) ht−1 + Q(o) gt + b(o) ) (2) it = σ(W (i) xt + U (i) (i) ht−1 + b ) yt−1 yt yt+1 yt+2 Syn-LSTM Syn-LSTM Syn-LSTM Syn-LSTM xt-1 L gt-1 (4) 0 gt-1 gt0 = + t X (ij 0 gt+2 t X t Y (mj t X ht = ot tanh(ct ) (8) j=0 (9) fk ) ˜cj k=j+1 (6) = t Y j=0 ˜st = tanh(W (n) gt + U (n) ht−1 + b(n) ) ct = ft ct−1 + it ˜ct + mt ˜st Q(·) 0 gt+1 ct = ft ct−1 + it ˜ct + mt ˜st j=0 U (·) , L gt+2 Similar to the previous work (Levy et al., 2018), it is also possible to show that the cell state ct implicitly computes the element-wise weighted sum of the previous states by expanding Equation 7: (5) W (·) , xt+2 Figure 3: Syn-LSTM-CRF architecture. ˜ct = tanh(W (u) xt + U (u) ht−1 + b(u) ) (7) L gt+1 xt+1 Graph Convolutional Network (3) mt = σ(W (m) gt + U (m) ht−1 + b(m) ) gtL xt fk ) ˜sj (10) k=j+1 atj ˜cj + t X qtj ˜sj (11) j=0 Note that the two terms, atj and qtj , are the product of gates. The value of the two terms are in the range from 0 to 1. Since the ˜ct and ˜st represent contextual and structured features, the corresponding w"
2021.naacl-main.271,D17-1282,0,0.0187283,"However, sequence models such as bidirectional LSTM (Hochreiter and Schmidhuber, 1997) are not able to fully capture the long-range dependencies (Bengio, 2009). For instance, Figure 1 (top) shows one type of structured information in NER. The words “Precision Castparts Corp.” can be easily inferred as O RGANIZATION by its context (i.e., Corp.). However, the second entity “PCP” could be misclassified as a P RODUCT entity if a model relies more on the context “begin trading with” but ignores the hidden information that “PCP” is the symbol of “Precision Castparts Corp.”. Previous research works (Li et al., 2017; Jie and 1 Introduction Lu, 2019; Wang et al., 2019) have been using the Named entity recognition (NER) is one of the parse trees (Chomsky, 1956, 1969; Sandra and Taft, most fundamental and important tasks in natu- 2014) to incorporate such structured information. ral language processing (NLP). While the litera- Figure 1 (Dependency Path) shows that the first enture (Peters et al., 2018; Akbik et al., 2018; De- tity can be connected to the second entity following vlin et al., 2019) largely focuses on training deep the dependency tree with 5 hops. Incorporating the language models to improve t"
2021.naacl-main.271,N19-1423,0,0.638236,"on helps to enhance the input representation. trees selectively. Furthermore, previous methods (Wang et al., 2018; 3459 Wang and Lu, 2018) use embeddings of part-ofspeech (POS) tags as additional input representation. The input representation xt of our model is the concatenation of the word embedding vt , the character representation et , the dependency relation embedding rt , and the POS embedding pt : xt = [vt ; et ; rt ; pt ] (12) where both rt and pt embeddings are randomly initialized and are fine-tuned during training. For experiments with the contextualized representations (e.g., BERT (Devlin et al., 2019)), we further concatenate the contextual word representation to xt . For our task, we employ the graph convolutional network (Kipf and Welling, 2017; Zhang et al., 2018b) to get the graph-encoded representation gt . Given a graph, an adjacency matrix A of size n × n is able to represent the graph structure, where n is the number of nodes; Ai,j = 1 indicates that node i and node j are connected. We transform dependency tree into its corresponding adjacency matrix3 A, and Ai,j = 1 denotes that node i and node j have dependency relation. Note that the purpose of graph-encoded representation gt is"
2021.naacl-main.271,2020.acl-main.519,0,0.0531685,"Missing"
2021.naacl-main.271,2020.acl-main.45,0,0.0243537,"Missing"
2021.naacl-main.271,P05-1045,0,0.11316,"r further analysis demonstrates that our model can capture longer dependencies compared with strong baselines.1 Precision Castparts Corp. , Portlan , will begin trading with the symbol PCP . O RG O RG P RODUCT ? Dependency Path: Corp. begin trading with symbol PCP neighbor context Hybrid Paths: Corp. begin . PCP OR Corp. begin trading with the symbol PCP Figure 1: A sentence annotated with dependency trees and named entities. The paths to connect two entities are shown below the sentence. the structured information such as interactions between non-adjacent words can also be important for NER (Finkel et al., 2005; Jie et al., 2017; Aguilar and Solorio, 2019). However, sequence models such as bidirectional LSTM (Hochreiter and Schmidhuber, 1997) are not able to fully capture the long-range dependencies (Bengio, 2009). For instance, Figure 1 (top) shows one type of structured information in NER. The words “Precision Castparts Corp.” can be easily inferred as O RGANIZATION by its context (i.e., Corp.). However, the second entity “PCP” could be misclassified as a P RODUCT entity if a model relies more on the context “begin trading with” but ignores the hidden information that “PCP” is the symbol of “Preci"
2021.naacl-main.271,W10-1902,0,0.0338542,"on the OntoNotes 5.0 English dataset, and Table 7 presents the detailed results of our model with contextualized representation. We find that the performance drops by 0.24 F1 score when we only use 1-layer GCN. Without GCN at all, the score drops by 1.13 F1 . The original dependency contributes 0.27 F1 score. Removing the dependency relation embedding also decreases the performance by 0.27 F1 . When we remove the POS tags embedding, the result drops by 0.39 F1 . NER Early work (Sasano and Kurohashi, 2008) uses syntactic dependency features to improve the SVM performance on Japanese NER task. Liu et al. (2010) propose to construct skip-edges to link similar words or words having typed dependencies to capture long-range dependencies. The later works (Collobert et al., 2010; Lample et al., 2016; Chiu and Nichols, 2016b) focus on using neural networks to extract features and achieved the stateof-the-art performance. Jie et al. (2017) find that some relations between the dependency edges and the entities can be used to reduce the search space of their model, which significantly reduces the time complexity. Yu et al. (2020) employ pre-trained language model to encode document-level information to explor"
2021.naacl-main.271,P16-1101,0,0.0548691,"textual information well. 7 Conclusion In this paper, we propose a simple and robust SynLSTM model to better integrate the structured information leveraged from the long-range dependencies. Specifically, we introduce an additional graph6 Related Work encoded representation to each recurrent unit. Such LSTM LSTM has demonstrated its great effec- a graph-encoded representation can be obtained via tiveness in many NLP tasks and becomes a stan- GNNs. Through the newly designed gating mechdard module for many state-of-the-art models (Wen anism, the hidden states are enhanced by contexet al., 2015; Ma and Hovy, 2016; Dozat and Man- tual information captured by the linear sequence ning, 2017). However, the sequential nature of the and structured information captured by the depenLSTM makes it challenging to capture long-range dency trees. We present the Syn-LSTM-CRF for dependencies. Zhang et al. (2018a) propose the NER and adopt the GCN on dependency trees to S-LSTM model to include a sentence state to allow obtain the graph-encoded representations. Our exboth local and global information exchange simul- tensive experiments and analysis on the datasets taneously. Mogrifier LSTM (Melis et al., 2020) with f"
2021.naacl-main.271,P14-5010,0,0.0119785,"f our approach when dependency trees of from both directions. We concatenate the hidden different qualities are used. For SemEval 2010 Task → − state ht from forward Syn-LSTM and hidden state 1 datasets, there are 4 entity types: P ER, L OC and 3 O RG and M ISC. For OntoNotes 5.0 datasets, there We treat the dependency edge as undirected and add a self-loop for each node: Ai,j = Aj,i and Ai,i = 1. are 18 entity types in total. Following the work 3460 by Jie and Lu (2019), we transform the parse trees into the Stanford dependency trees (De Marneffe and Manning, 2008) by using Stanford CoreNLP (Manning et al., 2014). Detailed statistics of each dataset can be found in Table 1. Intuitively, longer sentences would require the model to capture more long-distance interactions in the sentences. We present the number of entities in terms of different sentence lengths to show that these datasets have a modest amount of entities in long sentences. Experimental Setup For Catalan, Spanish, and Chinese, we use the FastText (Grave et al., 2018) 300 dimensional embeddings to initialize the word embeddings. For OntoNotes 5.0 English, we adopt the publicly available GloVE (Pennington et al., 2014) 100 dimensional embed"
2021.naacl-main.271,D14-1162,0,0.0968802,"using Stanford CoreNLP (Manning et al., 2014). Detailed statistics of each dataset can be found in Table 1. Intuitively, longer sentences would require the model to capture more long-distance interactions in the sentences. We present the number of entities in terms of different sentence lengths to show that these datasets have a modest amount of entities in long sentences. Experimental Setup For Catalan, Spanish, and Chinese, we use the FastText (Grave et al., 2018) 300 dimensional embeddings to initialize the word embeddings. For OntoNotes 5.0 English, we adopt the publicly available GloVE (Pennington et al., 2014) 100 dimensional embeddings to initialize the word embeddings. For experiments with the contextualized representation, we adopt the pre-trained language model BERT (Devlin et al., 2019) for the four datasets. Specifically, we use bert-as-service (Xiao, 2018) to generate the contextualized word representation without fine-tuning. Following Luo et al. (2020), we use the cased version of BERT large model for the experiments on the OntoNotes 5.0 English data. We use the cased version of BERT base model for the experiments on the other three datasets. For the character embedding, we randomly initia"
2021.naacl-main.271,N18-1202,0,0.698056,"be misclassified as a P RODUCT entity if a model relies more on the context “begin trading with” but ignores the hidden information that “PCP” is the symbol of “Precision Castparts Corp.”. Previous research works (Li et al., 2017; Jie and 1 Introduction Lu, 2019; Wang et al., 2019) have been using the Named entity recognition (NER) is one of the parse trees (Chomsky, 1956, 1969; Sandra and Taft, most fundamental and important tasks in natu- 2014) to incorporate such structured information. ral language processing (NLP). While the litera- Figure 1 (Dependency Path) shows that the first enture (Peters et al., 2018; Akbik et al., 2018; De- tity can be connected to the second entity following vlin et al., 2019) largely focuses on training deep the dependency tree with 5 hops. Incorporating the language models to improve the contextualized dependency information can be done with graph word representations, previous studies show that neural networks (GNNs) such as graph convolutional networks (GCNs) (Kipf and Welling, 2017). ∗ Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. The However, simply stacking the LSTM and GCN work was done when Zhanming Jie"
2021.naacl-main.271,D15-1199,0,0.0401775,"Missing"
2021.naacl-main.271,2020.acl-main.577,0,0.0122708,"tactic dependency features to improve the SVM performance on Japanese NER task. Liu et al. (2010) propose to construct skip-edges to link similar words or words having typed dependencies to capture long-range dependencies. The later works (Collobert et al., 2010; Lample et al., 2016; Chiu and Nichols, 2016b) focus on using neural networks to extract features and achieved the stateof-the-art performance. Jie et al. (2017) find that some relations between the dependency edges and the entities can be used to reduce the search space of their model, which significantly reduces the time complexity. Yu et al. (2020) employ pre-trained language model to encode document-level information to explore all spans with the graph-based dependency graph based ideas. The pre-trained language models (e.g., BERT (Devlin et al., 2019), ELMO (Peters et al., 2018)) further improve neuralbased approaches with a good contextualized representation. However, previous works did not focus on investigating how to effectively integrate structured and contextual information well. 7 Conclusion In this paper, we propose a simple and robust SynLSTM model to better integrate the structured information leveraged from the long-range d"
2021.naacl-main.271,P18-1030,0,0.0790404,"(POS) tags as additional input representation. The input representation xt of our model is the concatenation of the word embedding vt , the character representation et , the dependency relation embedding rt , and the POS embedding pt : xt = [vt ; et ; rt ; pt ] (12) where both rt and pt embeddings are randomly initialized and are fine-tuned during training. For experiments with the contextualized representations (e.g., BERT (Devlin et al., 2019)), we further concatenate the contextual word representation to xt . For our task, we employ the graph convolutional network (Kipf and Welling, 2017; Zhang et al., 2018b) to get the graph-encoded representation gt . Given a graph, an adjacency matrix A of size n × n is able to represent the graph structure, where n is the number of nodes; Ai,j = 1 indicates that node i and node j are connected. We transform dependency tree into its corresponding adjacency matrix3 A, and Ai,j = 1 denotes that node i and node j have dependency relation. Note that the purpose of graph-encoded representation gt is to incorporate the dependency information from neighbor nodes. The input and output representations of the l-th layer GCN at t-th position are denoted as gtl−1 and gtl"
2021.naacl-main.271,P18-1144,0,0.038964,"Missing"
2021.naacl-main.271,D17-1283,0,0.0391916,"Missing"
D15-1060,P11-1055,0,0.172048,"Missing"
D15-1060,D14-1203,0,0.0265206,"Missing"
D15-1060,W12-2402,1,0.552018,"Missing"
D15-1060,D07-1111,0,0.0206166,"Missing"
D15-1060,D12-1042,0,0.0384392,"Missing"
D15-1060,P10-1013,0,0.0891479,"Missing"
D17-1047,W14-4012,0,0.0484462,"Missing"
D17-1047,S14-2004,0,0.861166,"m, it usually requires a large training corpus to capture the flexible usage of parenthesis. ∗ Corresponding author. 452 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 452–461 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics of multiple attentions with a GRU network, which has different behaviors inherited from RNNs, such as forgetting, maintaining, and non-linearly transforming, and thus allows a better prediction accuracy. We evaluate our approach on four datasets: the first two come from SemEval 2014 (Pontiki et al., 2014), containing reviews of restaurant domain and laptop domain; the third one is a collection of tweets, collected by (Dong et al., 2014); to examine whether our framework is language-insensitive (since languages show differences in quite a few aspects in expressing sentiments), we prepared a dataset of Chinese news comments with people mentions as opinion targets. The experimental results show that our model performs well for different types of data, and consistently outperforms the state-of-the-art methods. sentence structures are particularly challenging for target sentiment analysis. For exam"
D17-1047,D15-1018,0,0.0243007,"nd nonlinearly combine the attention results with a recurrent network, i.e. GRUs. Finally, we apply softmax on the output of the GRU network to predict the sentiment on the target. 2 Related Work The task of aspect sentiment classification belongs to entity-level sentiment analysis. Conventional representative methods for this task include rulebased methods (Ding et al., 2008) and statisticbased methods (Jiang et al., 2011; Zhao et al., 2010). Ganapathibhotla and Liu (2008) extracted 2-tuples of (opinion target, opinion word) from comments and then identified the sentiment of opinion targets. Deng and Wiebe (2015) adopted Probabilistic Soft Logic to handle the task. There are also statistic-based approaches which employ SVM (Jiang et al., 2011) or MaxEnt-LDA (Zhao et al., 2010). These methods need either laborious feature engineering work or massive extralinguistic resources. Neural Networks (NNs) have the capability of fusing original features to generate new representations through multiple hidden layers. Recursive NN (Rec-NN) can conduct semantic compositions on tree structures, which has been used for syntactic analysis (Socher et al., 2010) and sentence sentiment analysis (Socher et al., 2013). (D"
D17-1047,D15-1044,0,0.0130815,"nning and the tail to the target respectively to capture the information before and after it. Nevertheless, TD-LSTM might not work well when the opinion word is far from the target, because the captured feature is likely to be lost ((Cho et al., 2014) reported similar problems of LSTM-based models in machine translation). 3 Our Model (Graves et al., 2014) introduced the concept of memory for NNs and proposed a differentiable process to read and write memory, which is called Neural Turing Machine (NTM). Attention mechanism, which has been used successfully in many areas (Bahdanau et al., 2014; Rush et al., 2015), can be treated as a simplified version of NTM because the size of memory is unlimited and we only need to read from it. Single attention or multiple attentions were applied in aspect sentiment classification in some previous works (Wang et al., 2016; Tang et al., 2016). One difference between our method and (Tang et al., 2016) is that we introduce a memory module between the attention module and the input module, thus our method can synthesize features of word sequences such as The architecture of our model is shown in Figure 1, which consists of five modules: input module, memory module, po"
D17-1047,P14-2009,0,0.824101,"he 2017 Conference on Empirical Methods in Natural Language Processing, pages 452–461 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics of multiple attentions with a GRU network, which has different behaviors inherited from RNNs, such as forgetting, maintaining, and non-linearly transforming, and thus allows a better prediction accuracy. We evaluate our approach on four datasets: the first two come from SemEval 2014 (Pontiki et al., 2014), containing reviews of restaurant domain and laptop domain; the third one is a collection of tweets, collected by (Dong et al., 2014); to examine whether our framework is language-insensitive (since languages show differences in quite a few aspects in expressing sentiments), we prepared a dataset of Chinese news comments with people mentions as opinion targets. The experimental results show that our model performs well for different types of data, and consistently outperforms the state-of-the-art methods. sentence structures are particularly challenging for target sentiment analysis. For example, in “Except Patrick, all other actors don’t play well”, the word “except” and the phrase “don’t play well” produce a positive sent"
D17-1047,D11-1014,0,0.0177617,"Missing"
D17-1047,C08-1031,0,0.00626977,"t different targets from the same sentence have their own tailor-made memories. After that, we pay multiple attentions on the position-weighted memory and nonlinearly combine the attention results with a recurrent network, i.e. GRUs. Finally, we apply softmax on the output of the GRU network to predict the sentiment on the target. 2 Related Work The task of aspect sentiment classification belongs to entity-level sentiment analysis. Conventional representative methods for this task include rulebased methods (Ding et al., 2008) and statisticbased methods (Jiang et al., 2011; Zhao et al., 2010). Ganapathibhotla and Liu (2008) extracted 2-tuples of (opinion target, opinion word) from comments and then identified the sentiment of opinion targets. Deng and Wiebe (2015) adopted Probabilistic Soft Logic to handle the task. There are also statistic-based approaches which employ SVM (Jiang et al., 2011) or MaxEnt-LDA (Zhao et al., 2010). These methods need either laborious feature engineering work or massive extralinguistic resources. Neural Networks (NNs) have the capability of fusing original features to generate new representations through multiple hidden layers. Recursive NN (Rec-NN) can conduct semantic compositions"
D17-1047,D13-1170,0,0.0223175,"s. Deng and Wiebe (2015) adopted Probabilistic Soft Logic to handle the task. There are also statistic-based approaches which employ SVM (Jiang et al., 2011) or MaxEnt-LDA (Zhao et al., 2010). These methods need either laborious feature engineering work or massive extralinguistic resources. Neural Networks (NNs) have the capability of fusing original features to generate new representations through multiple hidden layers. Recursive NN (Rec-NN) can conduct semantic compositions on tree structures, which has been used for syntactic analysis (Socher et al., 2010) and sentence sentiment analysis (Socher et al., 2013). (Dong et al., 2014; Nguyen and Shirai, 2015) adopted Rec-NN for aspect sentiment classification, by converting the opinion target as the tree root and propagating the sentiment of targets depending on the context and syntactic relationships between them. However, Rec-NN needs dependency parsing which is likely ineffective on nonstandard texts such as news comments and tweets. (Chen et al., 2016) employed Convolution NNs to identify the sentiOur framework introduces a novel way of applying multiple-attention mechanism to synthesize important features in difficult sentence structures. It’s sor"
D17-1047,P11-1016,0,0.667013,"relative positions to the target, so that different targets from the same sentence have their own tailor-made memories. After that, we pay multiple attentions on the position-weighted memory and nonlinearly combine the attention results with a recurrent network, i.e. GRUs. Finally, we apply softmax on the output of the GRU network to predict the sentiment on the target. 2 Related Work The task of aspect sentiment classification belongs to entity-level sentiment analysis. Conventional representative methods for this task include rulebased methods (Ding et al., 2008) and statisticbased methods (Jiang et al., 2011; Zhao et al., 2010). Ganapathibhotla and Liu (2008) extracted 2-tuples of (opinion target, opinion word) from comments and then identified the sentiment of opinion targets. Deng and Wiebe (2015) adopted Probabilistic Soft Logic to handle the task. There are also statistic-based approaches which employ SVM (Jiang et al., 2011) or MaxEnt-LDA (Zhao et al., 2010). These methods need either laborious feature engineering work or massive extralinguistic resources. Neural Networks (NNs) have the capability of fusing original features to generate new representations through multiple hidden layers. Rec"
D17-1047,S14-2076,0,0.569478,"the code released by the authors.4 For each method, the maximum number of training iterations is 100, and the model with the minimum training error is utilized for testing. We will discuss different settings of RAM later. Compared Methods We compare our proposed framework of Recurrent Attention on Memory (RAM) with the following methods: • Average Context: There are two versions of this method. The first one, named AC-S, averages the word vectors before the target and the word vectors after the target separately. The second one, named AC, averages the word vectors of the full context. • SVM (Kiritchenko et al., 2014): The traditional state-of-the-art method using SVMs on surface features, lexicon features and parsing features, which is the best team in SemEval 2014. • Rec-NN (Dong et al., 2014): It firstly uses rules to transform the dependency tree and put the opinion target at the root, and then performs semantic composition with Recursive NNs for sentiment prediction. • TD-LSTM (Tang et al., 2015): It uses a forward LSTM and a backward LSTM to abstract the information before and after the target. Finally, it takes the hidden states of LSTM at last time step to represent the context for prediction. We r"
D17-1047,D16-1021,0,0.372607,"m the target, it needs to propagate the feature word by word to the target, in which case it’s likely to lose this feature, such as the feature “cost-effective” for “the phone” in “My overall feeling is that the phone, after using it for three months and considering its price, is really cost-effective”.1 Attention mechanism, which has been successfully used in machine translation (Bahdanau et al., 2014), can enforce a model to pay more attention to the important part of a sentence. There are already some works using attention in sentiment analysis to exploit this advantage (Wang et al., 2016; Tang et al., 2016). Another observation is that some types of We propose a novel framework based on neural networks to identify the sentiment of opinion targets in a comment/review. Our framework adopts multiple-attention mechanism to capture sentiment features separated by a long distance, so that it is more robust against irrelevant information. The results of multiple attentions are non-linearly combined with a recurrent neural network, which strengthens the expressive power of our model for handling more complications. The weightedmemory mechanism not only helps us avoid the labor-intensive feature engineer"
D17-1047,D16-1058,0,0.789213,"ent feature far from the target, it needs to propagate the feature word by word to the target, in which case it’s likely to lose this feature, such as the feature “cost-effective” for “the phone” in “My overall feeling is that the phone, after using it for three months and considering its price, is really cost-effective”.1 Attention mechanism, which has been successfully used in machine translation (Bahdanau et al., 2014), can enforce a model to pay more attention to the important part of a sentence. There are already some works using attention in sentiment analysis to exploit this advantage (Wang et al., 2016; Tang et al., 2016). Another observation is that some types of We propose a novel framework based on neural networks to identify the sentiment of opinion targets in a comment/review. Our framework adopts multiple-attention mechanism to capture sentiment features separated by a long distance, so that it is more robust against irrelevant information. The results of multiple attentions are non-linearly combined with a recurrent neural network, which strengthens the expressive power of our model for handling more complications. The weightedmemory mechanism not only helps us avoid the labor-intens"
D17-1047,D10-1006,0,0.00659165,"o the target, so that different targets from the same sentence have their own tailor-made memories. After that, we pay multiple attentions on the position-weighted memory and nonlinearly combine the attention results with a recurrent network, i.e. GRUs. Finally, we apply softmax on the output of the GRU network to predict the sentiment on the target. 2 Related Work The task of aspect sentiment classification belongs to entity-level sentiment analysis. Conventional representative methods for this task include rulebased methods (Ding et al., 2008) and statisticbased methods (Jiang et al., 2011; Zhao et al., 2010). Ganapathibhotla and Liu (2008) extracted 2-tuples of (opinion target, opinion word) from comments and then identified the sentiment of opinion targets. Deng and Wiebe (2015) adopted Probabilistic Soft Logic to handle the task. There are also statistic-based approaches which employ SVM (Jiang et al., 2011) or MaxEnt-LDA (Zhao et al., 2010). These methods need either laborious feature engineering work or massive extralinguistic resources. Neural Networks (NNs) have the capability of fusing original features to generate new representations through multiple hidden layers. Recursive NN (Rec-NN) c"
D17-1047,D15-1298,0,0.654353,"Missing"
D17-1047,D14-1162,0,\N,Missing
D17-1221,P15-1153,1,0.892052,"he word level to the clause level, which include news headers such as “BEIJING, Nov. 24 (Xinhua) –”, intra-sentential attribution such as “, police said Thursday”, “, he said”, etc. The information filtered by the rules will be processed according to the word salience score. Information with smaller salience score (&lt; ) will be removed. 2084 2.3.2 Phrase-based Optimization for Summary Construction After coarse-grained compression on each single sentence as described above, we design a unified optimization method for summary generation. We refine the phrase-based summary construction model in (Bing et al., 2015) by adjusting the goal as compressive summarization. We consider the salience information obtained by our neural attention model and the compressed sentences in the coarse-grained compression component. Based on the parsed constituency tree for each input sentence as described in Section 2.3.1, we extract the noun-phrases (NPs) and verb-phrases (VPs). The salience Si of a phrase Pi is defined as: X X Si = { tf (t)/ tf (t)} × ai (15) t∈Pi t∈T opic where ai is the salience of the sentence containing Pi . tf (t) is the frequency of the concept t (unigram/bigram) in the whole topic. Thus, Si inher"
D17-1221,P16-1046,0,0.0226281,"mpression component. Finally, the attention weights are integrated into a phrase-based optimization framework for compressive summary generation. In fact, the notion of “attention” has gained popularity recently in neural network modeling, which has improved the performance of many tasks such as machine translation (Bahdanau et al., 2015; Luong et al., 2015). However, very few previous works employ attention mechanism to tackle MDS. Rush et al. (2015) and Nallapati et al. (2016) employed attention-based sequenceto-sequence (seq2seq) framework only for sentence summarization. Gu et al. (2016), Cheng and Lapata (2016), and Nallapati et al. (2016) also utilized seq2seq based framework with attention modeling for short text or single document summarization. Different from their works, our framework aims at conducting multi-document summarization in an unsupervised manner. Our contributions are as follows: (1) We propose a cascaded attention model that captures salient information in different semantic representations. (2) The attention weights are learned automatically by an unsupervised data reconstruction framework which can capture the sentence salience. By adding sparsity constraints on the number of out"
D17-1221,W04-3247,0,0.565526,"nts on the number of output vectors, we can generate condensed information which can be treated as word salience. Fine-grained and coarse-grained sentence compression strategies are incorporated to produce compressive summaries. Experiments on some benchmark data sets show that our framework achieves better results than the state-of-the-art methods. 1 Introduction The goal of Multi-Document Summarization (MDS) is to automatically produce a succinct summary, preserving the most important information of a set of documents describing a topic1 (Luhn, 1958; Edmundson, 1969; Goldstein et al., 2000; Erkan and Radev, 2004b; Wan et al., 2007; Nenkova and McKeown, 2012). Considering the procedure of summary writing by humans, when people read, they will remember and forget part ∗ The work described in this paper is supported by grants from the Research and Development Grant of Huawei Technologies Co. Ltd (YB2015100076/TH1510257) and the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414). 1 A topic represents a real event, e.g., “AlphaGo versus Lee Sedol”. of the content. Information which is more important may make a deep impression easily. When people recall and digest"
D17-1221,W00-0405,0,0.241849,"adding sparsity constraints on the number of output vectors, we can generate condensed information which can be treated as word salience. Fine-grained and coarse-grained sentence compression strategies are incorporated to produce compressive summaries. Experiments on some benchmark data sets show that our framework achieves better results than the state-of-the-art methods. 1 Introduction The goal of Multi-Document Summarization (MDS) is to automatically produce a succinct summary, preserving the most important information of a set of documents describing a topic1 (Luhn, 1958; Edmundson, 1969; Goldstein et al., 2000; Erkan and Radev, 2004b; Wan et al., 2007; Nenkova and McKeown, 2012). Considering the procedure of summary writing by humans, when people read, they will remember and forget part ∗ The work described in this paper is supported by grants from the Research and Development Grant of Huawei Technologies Co. Ltd (YB2015100076/TH1510257) and the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414). 1 A topic represents a real event, e.g., “AlphaGo versus Lee Sedol”. of the content. Information which is more important may make a deep impression easily. When pe"
D17-1221,P16-1154,1,0.918562,"rained sentence compression component. Finally, the attention weights are integrated into a phrase-based optimization framework for compressive summary generation. In fact, the notion of “attention” has gained popularity recently in neural network modeling, which has improved the performance of many tasks such as machine translation (Bahdanau et al., 2015; Luong et al., 2015). However, very few previous works employ attention mechanism to tackle MDS. Rush et al. (2015) and Nallapati et al. (2016) employed attention-based sequenceto-sequence (seq2seq) framework only for sentence summarization. Gu et al. (2016), Cheng and Lapata (2016), and Nallapati et al. (2016) also utilized seq2seq based framework with attention modeling for short text or single document summarization. Different from their works, our framework aims at conducting multi-document summarization in an unsupervised manner. Our contributions are as follows: (1) We propose a cascaded attention model that captures salient information in different semantic representations. (2) The attention weights are learned automatically by an unsupervised data reconstruction framework which can capture the sentence salience. By adding sparsity constra"
D17-1221,D13-1047,0,0.0188099,"hat aoi = Aoi,: ∈ Rm is the attention weight vector for si . According to Equation 9, a large value in aoi conveys a meaning that the corresponding sentence should contribute more when generating si . We also use the magnitude of the columns in Ao to represent the salience of sentences. 2.3 2.3.1 Compressive Summary Generation Phase Coarse-grained Sentence Compression Using the information distillation result from the cascaded neural attention model, we conduct coarse-grained compression for each individual sentence. Such strategy has been adopted in some multi-document summarization methods (Li et al., 2013; Wang et al., 2013; Yao et al., 2015). Our coarse-grained sentence compression jointly considers word salience obtained from the neural attention model and linguistically-motivated rules. The linguistically-motivated rules are designed based on the observed obvious evidence for uncritical information from the word level to the clause level, which include news headers such as “BEIJING, Nov. 24 (Xinhua) –”, intra-sentential attribution such as “, police said Thursday”, “, he said”, etc. The information filtered by the rules will be processed according to the word salience score. Information wit"
D17-1221,W04-1013,0,0.0249676,"Missing"
D17-1221,P98-2222,0,0.0532758,"ESU4 (R-SU4) are reported. 4 R-1 0.280 0.308 0.360 0.373 0.340 0.377 0.391 0.392 0.393* R-1 0.302 0.312 0.378 0.403 0.353 0.398 0.408 0.419 0.423* R-2 0.046 0.058 0.075 0.083 0.055 0.087 0.097 0.103 0.107* R-SU4 0.088 0.102 0.130 0.144 0.112 0.137 0.150 0.156 0.161* tence compression (Section 2.3.1) show that the compression can indeed improve the sumamrization performance. 4.2 Main Results of Compressive MDS We compare our system C-Attention with several unsupervised summarization baselines and state-of-the-art models. Random baseline selects sentences randomly for each topic. Lead baseline (Wasson, 1998) ranks the news chronologically and extracts the leading sentences one by one. TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004a) estimate sentence salience by applying the PageRank algorithm to the sentence graph. PKUTM (Li et al., 2011) employs manifold-ranking for sentence scoring and selection; ABS-Phrase (Bing et al., 2015) generates abstractive summaries using phrase-based optimization framework. Three other unsupervised methods based on sparse coding are also compared, namely, DSDR (He et al., 2012), MDS-Sparse (Liu et al., 2015), and RAMDS (Li et al., 2015). As sh"
D17-1221,D12-1022,0,0.028573,"der to obtain coherent summaries with good readability, we add some constraints into the ILP framework such as sentence generation constraint: Let βk denote the selection indicator of the sentence xk . If any phrase from xk is selected, βk = 1. Otherwise, βk = 0. For generating a compressed summary sentence, it is required that if βk = 1, at least one NP and at lease one VP of the sentence should be selected. It is expressed as: ∀Pi ∈ xk , αi ≤ βk ∧ X i αi ≥ βk , Other constraints include sentence number, summary length, phrase co-occurrence, etc. For details, please refer to McDonald (2007), Woodsend and Lapata (2012), and Bing et al. (2015). The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as the simplex algorithm (Dantzig and Thapa, 2006). In the implementation, we use a package called lp solve3 . In the post-processing, the phrases and sentences in a summary are ordered according to their natural order if they come from the same document. Otherwise, they are ordered according to the timestamps of the corresponding documents. 3 Experimental Setup 3.1 Datasets DUC: Both DUC 2006 and DUC 2007 are used in our evaluation. DUC 2006 and DU"
D17-1221,D15-1166,0,0.250292,"is preserved. Precisely, the recaller outputs fewer vectors s than that of the input UHFRQVWUXFW ܺ ܿଶ ܪ ܿଶ Dec ݏଶ ܿଵ ݏଵ ܿଵ UHFDOOHU ܵ ܣ ܣ ܿ Enc UHDGHU supervised manner. Thereafter, the word salience is fed into a coarse-grained sentence compression component. Finally, the attention weights are integrated into a phrase-based optimization framework for compressive summary generation. In fact, the notion of “attention” has gained popularity recently in neural network modeling, which has improved the performance of many tasks such as machine translation (Bahdanau et al., 2015; Luong et al., 2015). However, very few previous works employ attention mechanism to tackle MDS. Rush et al. (2015) and Nallapati et al. (2016) employed attention-based sequenceto-sequence (seq2seq) framework only for sentence summarization. Gu et al. (2016), Cheng and Lapata (2016), and Nallapati et al. (2016) also utilized seq2seq based framework with attention modeling for short text or single document summarization. Different from their works, our framework aims at conducting multi-document summarization in an unsupervised manner. Our contributions are as follows: (1) We propose a cascaded attention model tha"
D17-1221,C12-1128,0,0.047742,"Missing"
D17-1221,K16-1028,0,0.165303,"ଶ ܿଵ ݏଵ ܿଵ UHFDOOHU ܵ ܣ ܣ ܿ Enc UHDGHU supervised manner. Thereafter, the word salience is fed into a coarse-grained sentence compression component. Finally, the attention weights are integrated into a phrase-based optimization framework for compressive summary generation. In fact, the notion of “attention” has gained popularity recently in neural network modeling, which has improved the performance of many tasks such as machine translation (Bahdanau et al., 2015; Luong et al., 2015). However, very few previous works employ attention mechanism to tackle MDS. Rush et al. (2015) and Nallapati et al. (2016) employed attention-based sequenceto-sequence (seq2seq) framework only for sentence summarization. Gu et al. (2016), Cheng and Lapata (2016), and Nallapati et al. (2016) also utilized seq2seq based framework with attention modeling for short text or single document summarization. Different from their works, our framework aims at conducting multi-document summarization in an unsupervised manner. Our contributions are as follows: (1) We propose a cascaded attention model that captures salient information in different semantic representations. (2) The attention weights are learned automatically b"
D17-1221,D15-1044,0,0.299409,"W ܺ ܿଶ ܪ ܿଶ Dec ݏଶ ܿଵ ݏଵ ܿଵ UHFDOOHU ܵ ܣ ܣ ܿ Enc UHDGHU supervised manner. Thereafter, the word salience is fed into a coarse-grained sentence compression component. Finally, the attention weights are integrated into a phrase-based optimization framework for compressive summary generation. In fact, the notion of “attention” has gained popularity recently in neural network modeling, which has improved the performance of many tasks such as machine translation (Bahdanau et al., 2015; Luong et al., 2015). However, very few previous works employ attention mechanism to tackle MDS. Rush et al. (2015) and Nallapati et al. (2016) employed attention-based sequenceto-sequence (seq2seq) framework only for sentence summarization. Gu et al. (2016), Cheng and Lapata (2016), and Nallapati et al. (2016) also utilized seq2seq based framework with attention modeling for short text or single document summarization. Different from their works, our framework aims at conducting multi-document summarization in an unsupervised manner. Our contributions are as follows: (1) We propose a cascaded attention model that captures salient information in different semantic representations. (2) The attention weights"
D17-1221,P13-1136,0,0.0304866,"Rm is the attention weight vector for si . According to Equation 9, a large value in aoi conveys a meaning that the corresponding sentence should contribute more when generating si . We also use the magnitude of the columns in Ao to represent the salience of sentences. 2.3 2.3.1 Compressive Summary Generation Phase Coarse-grained Sentence Compression Using the information distillation result from the cascaded neural attention model, we conduct coarse-grained compression for each individual sentence. Such strategy has been adopted in some multi-document summarization methods (Li et al., 2013; Wang et al., 2013; Yao et al., 2015). Our coarse-grained sentence compression jointly considers word salience obtained from the neural attention model and linguistically-motivated rules. The linguistically-motivated rules are designed based on the observed obvious evidence for uncritical information from the word level to the clause level, which include news headers such as “BEIJING, Nov. 24 (Xinhua) –”, intra-sentential attribution such as “, police said Thursday”, “, he said”, etc. The information filtered by the rules will be processed according to the word salience score. Information with smaller salience"
D17-1221,W04-3252,0,\N,Missing
D17-1221,C98-2217,0,\N,Missing
D17-1222,C16-1053,0,0.00399758,"maries. (3) Experimental results on some benchmark datasets in different languages show that our framework achieves better performance than the state-of-the-art models. 2 Related Works Automatic summarization is the process of automatically generating a summary that retains the most important content of the original text document (Nenkova and McKeown, 2012). Traditionally, the summarization methods can be classified into three categories: extraction-based methods (Erkan and Radev, 2004; Goldstein et al., 2000; Wan et al., 2007; Min et al., 2012; Nallapati et al., 2017; Cheng and Lapata, 2016; Cao et al., 2016; Song et al., 2017), compression-based methods (Li et al., 2013; Wang et al., 2013; Li et al., 2015, 2017), and abstraction-based methods. In fact, previous investigations show that human-written summaries are more abstractive (Barzilay and McKeown, 2005; Bing et al., 2015). Abstraction-based approaches can generate new sentences based on the facts from different source sentences. Barzilay and McKeown (2005) employed sentence fusion to generate a new sentence. Bing et al. (2015) proposed a more fine-grained fusion framework, where new sentences are generated by selecting and merging salient p"
D17-1222,P10-1084,0,0.014277,"Missing"
D17-1222,P16-1046,0,0.0456727,"s of the abstractive summaries. (3) Experimental results on some benchmark datasets in different languages show that our framework achieves better performance than the state-of-the-art models. 2 Related Works Automatic summarization is the process of automatically generating a summary that retains the most important content of the original text document (Nenkova and McKeown, 2012). Traditionally, the summarization methods can be classified into three categories: extraction-based methods (Erkan and Radev, 2004; Goldstein et al., 2000; Wan et al., 2007; Min et al., 2012; Nallapati et al., 2017; Cheng and Lapata, 2016; Cao et al., 2016; Song et al., 2017), compression-based methods (Li et al., 2013; Wang et al., 2013; Li et al., 2015, 2017), and abstraction-based methods. In fact, previous investigations show that human-written summaries are more abstractive (Barzilay and McKeown, 2005; Bing et al., 2015). Abstraction-based approaches can generate new sentences based on the facts from different source sentences. Barzilay and McKeown (2005) employed sentence fusion to generate a new sentence. Bing et al. (2015) proposed a more fine-grained fusion framework, where new sentences are generated by selecting and"
D17-1222,N16-1012,0,0.147341,"50. We also set the dimension of hidden states and latent variables to 500. The maximum length of documents and summaries is 120 and 25 respectively, and the batch size is also 256. The beam size of the decoder was set to be 10. Adadelta (Schmidhuber, 2015) with hyperparameter ρ = 0.95 and  = 1e − 6 is used for gradient based optimization. Our neural network based framework is implemented using Theano (Theano Development Team, 2016). 5 5.1 • RNN-distract (Chen et al., 2016) uses a new attention mechanism by distracting the historical attention in the decoding steps. • RAS-LSTM and RAS-Elman (Chopra et al., 2016) both consider words and word positions as input and use convolutional encoders to handle the source information. For the attention based sequence decoding process, RAS-Elman selects Elman RNN (Elman, 1990) as decoder, and RAS-LSTM selects Long Short-Term Memory architecture (Hochreiter and Schmidhuber, 1997). • LenEmb (Kikuchi et al., 2016) uses a mechanism to control the summary length by considering the length embedding vector as the input. Experimental Settings Results and Discussions ROUGE Evaluation Table 1: ROUGE-F1 on validation sets Dataset GIGA LCSTS System StanD DRGD StanD DRGD R-1"
D17-1222,W00-0405,0,0.167306,"scriminative deterministic variables are jointly considered in the generation process of the abstractive summaries. (3) Experimental results on some benchmark datasets in different languages show that our framework achieves better performance than the state-of-the-art models. 2 Related Works Automatic summarization is the process of automatically generating a summary that retains the most important content of the original text document (Nenkova and McKeown, 2012). Traditionally, the summarization methods can be classified into three categories: extraction-based methods (Erkan and Radev, 2004; Goldstein et al., 2000; Wan et al., 2007; Min et al., 2012; Nallapati et al., 2017; Cheng and Lapata, 2016; Cao et al., 2016; Song et al., 2017), compression-based methods (Li et al., 2013; Wang et al., 2013; Li et al., 2015, 2017), and abstraction-based methods. In fact, previous investigations show that human-written summaries are more abstractive (Barzilay and McKeown, 2005; Bing et al., 2015). Abstraction-based approaches can generate new sentences based on the facts from different source sentences. Barzilay and McKeown (2005) employed sentence fusion to generate a new sentence. Bing et al. (2015) proposed a mo"
D17-1222,J05-3002,0,0.170352,"rating a summary that retains the most important content of the original text document (Nenkova and McKeown, 2012). Traditionally, the summarization methods can be classified into three categories: extraction-based methods (Erkan and Radev, 2004; Goldstein et al., 2000; Wan et al., 2007; Min et al., 2012; Nallapati et al., 2017; Cheng and Lapata, 2016; Cao et al., 2016; Song et al., 2017), compression-based methods (Li et al., 2013; Wang et al., 2013; Li et al., 2015, 2017), and abstraction-based methods. In fact, previous investigations show that human-written summaries are more abstractive (Barzilay and McKeown, 2005; Bing et al., 2015). Abstraction-based approaches can generate new sentences based on the facts from different source sentences. Barzilay and McKeown (2005) employed sentence fusion to generate a new sentence. Bing et al. (2015) proposed a more fine-grained fusion framework, where new sentences are generated by selecting and merging salient phrases. These methods can be regarded as a kind of indirect abstractive summarization, and complicated constraints are used to guarantee the linguistic quality. Recently, some researchers employ neural network based framework to tackle the abstractive sum"
D17-1222,P15-1153,1,0.900844,"ntroduction Automatic summarization is the process of automatically generating a summary that retains the most important content of the original text document (Edmundson, 1969; Luhn, 1958; Nenkova and McKeown, 2012). Different from the common extraction-based and compression-based methods, abstraction-based methods aim at constructing new sentences as summaries, thus they require a deeper understanding of the text and the capability of generating new sentences, which provide an obvious advantage in improving the focus of a summary, reducing the redundancy, and keeping a good compression rate (Bing et al., 2015; Rush et al., 2015; Nallapati et al., 2016). ∗ The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414). Some previous research works show that human-written summaries are more abstractive (Jing and McKeown, 2000). Moreover, our investigation reveals that people may naturally follow some inherent structures when they write the abstractive summaries. To illustrate this observation, we show some examples in Figure 1, which are some top story summaries or headlines from the channel “Technology”"
D17-1222,P16-1154,0,0.0798899,"e-grained fusion framework, where new sentences are generated by selecting and merging salient phrases. These methods can be regarded as a kind of indirect abstractive summarization, and complicated constraints are used to guarantee the linguistic quality. Recently, some researchers employ neural network based framework to tackle the abstractive summarization problem. Rush et al. (2015) proposed a neural network based model with local attention modeling, which is trained on the Gigaword corpus, but combined with an additional loglinear extractive summarization model with handcrafted features. Gu et al. (2016) integrated a copying mechanism into a seq2seq framework to improve the quality of the generated summaries. Chen et al. (2016) proposed a new attention mechanism that not only considers the important source 2092 segments, but also distracts them in the decoding step in order to better grasp the overall meaning of input documents. Nallapati et al. (2016) utilized a trick to control the vocabulary size to improve the training efficiency. The calculations in these methods are all deterministic and the representation ability is limited. Miao and Blunsom (2016) extended the seq2seq framework and pr"
D17-1222,D15-1229,0,0.257327,"acting the first sentence from articles with the headline to form a sourcesummary pair. We directly download the prepared dataset used in (Rush et al., 2015). It roughly contains 3.8M training pairs, 190K validation pairs, and 2,000 test pairs. DUC-20042 is another English dataset only used for testing in our experiments. It contains 500 documents. Each document contains 4 model summaries written by experts. The length of the summary is limited to 75 bytes. LCSTS is a large-scale Chinese short text summarization dataset, consisting of pairs of (short text, summary) collected from Sina Weibo3 (Hu et al., 2015). We take Part-I as the training set, Part-II as the development set, and Part-III as the test set. There is a score in range 1 ∼ 5 labeled by human to indicate how relevant an article and its summary is. We only reserve those pairs with scores no less than 3. The size of the three sets are 2.4M, 8.7k, and 725 respectively. In our experiments, we only take Chinese character sequence as input, without performing word segmentation. 4.2 Evaluation Metrics We use ROUGE score (Lin, 2004) as our evaluation metric with standard options. The basic idea of ROUGE is to count the number of overlapping un"
D17-1222,A00-2024,0,0.0855086,"ethods aim at constructing new sentences as summaries, thus they require a deeper understanding of the text and the capability of generating new sentences, which provide an obvious advantage in improving the focus of a summary, reducing the redundancy, and keeping a good compression rate (Bing et al., 2015; Rush et al., 2015; Nallapati et al., 2016). ∗ The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414). Some previous research works show that human-written summaries are more abstractive (Jing and McKeown, 2000). Moreover, our investigation reveals that people may naturally follow some inherent structures when they write the abstractive summaries. To illustrate this observation, we show some examples in Figure 1, which are some top story summaries or headlines from the channel “Technology” of CNN. After analyzing the summaries carefully, we can find some common structures from them, such as “What”, “What-Happened” , “Who Action What”, etc. For example, the summary “Apple sues Qualcomm for nearly $1 billion” can be structuralized as “Who (Apple) Action (sues) What (Qualcomm)”. Similarly, the summaries"
D17-1222,D16-1140,0,0.0415869,"Our neural network based framework is implemented using Theano (Theano Development Team, 2016). 5 5.1 • RNN-distract (Chen et al., 2016) uses a new attention mechanism by distracting the historical attention in the decoding steps. • RAS-LSTM and RAS-Elman (Chopra et al., 2016) both consider words and word positions as input and use convolutional encoders to handle the source information. For the attention based sequence decoding process, RAS-Elman selects Elman RNN (Elman, 1990) as decoder, and RAS-LSTM selects Long Short-Term Memory architecture (Hochreiter and Schmidhuber, 1997). • LenEmb (Kikuchi et al., 2016) uses a mechanism to control the summary length by considering the length embedding vector as the input. Experimental Settings Results and Discussions ROUGE Evaluation Table 1: ROUGE-F1 on validation sets Dataset GIGA LCSTS System StanD DRGD StanD DRGD R-1 32.69 36.25 33.88 36.71 R-2 15.29 17.61 21.49 24.00 R-L 30.60 33.55 31.05 34.10 We first depict the performance of our model DRGD by comparing to the standard decoders (StanD) of our own implementation. The comparison results on the validation datasets of Gigawords and LCSTS are shown in Table 1. From the results we can see that our proposed"
D17-1222,koen-2004-pharaoh,0,0.0931554,"ss. To generate summaries precisely, we first integrate the recurrent generative decoding component with the discriminative deterministic decoding component, and map the latent structure variable zt and the deterministic decoding hidden state hdt 2 to a new hidden variable: d d d dz d2 ht + bhy ) ht y = tanh(Wzhy zt + Whh (12) d Given the combined decoding state ht y at the time t, the probability of generating any target word yt is given as follows: d d ht y + bdhy ) yt = ς(Why (13) d ∈ Rky ×kh and bd ∈ Rky . ς(·) is the where Why hy softmax function. Finally, we use a beam search algorithm (Koehn, 2004) for decoding and generating the best summary. 3.4 Learning Although the proposed model contains a recurrent generative decoder, the whole framework is fully differentiable. As shown in Section 3.3, both the recurrent deterministic decoder and the recurrent generative decoder are designed based on neural networks. Therefore, all the parameters in our model can be optimized in an end-to-end paradigm using back-propagation. We use {X}N and {Y }N to denote the training source and target sequence. Generally, the objective of our framework consists of two terms. One term is the negative loglikeliho"
D17-1222,K16-1028,0,0.418007,"the process of automatically generating a summary that retains the most important content of the original text document (Edmundson, 1969; Luhn, 1958; Nenkova and McKeown, 2012). Different from the common extraction-based and compression-based methods, abstraction-based methods aim at constructing new sentences as summaries, thus they require a deeper understanding of the text and the capability of generating new sentences, which provide an obvious advantage in improving the focus of a summary, reducing the redundancy, and keeping a good compression rate (Bing et al., 2015; Rush et al., 2015; Nallapati et al., 2016). ∗ The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414). Some previous research works show that human-written summaries are more abstractive (Jing and McKeown, 2000). Moreover, our investigation reveals that people may naturally follow some inherent structures when they write the abstractive summaries. To illustrate this observation, we show some examples in Figure 1, which are some top story summaries or headlines from the channel “Technology” of CNN. After analyzing the summaries carefu"
D17-1222,D15-1044,0,0.768032,"ic summarization is the process of automatically generating a summary that retains the most important content of the original text document (Edmundson, 1969; Luhn, 1958; Nenkova and McKeown, 2012). Different from the common extraction-based and compression-based methods, abstraction-based methods aim at constructing new sentences as summaries, thus they require a deeper understanding of the text and the capability of generating new sentences, which provide an obvious advantage in improving the focus of a summary, reducing the redundancy, and keeping a good compression rate (Bing et al., 2015; Rush et al., 2015; Nallapati et al., 2016). ∗ The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414). Some previous research works show that human-written summaries are more abstractive (Jing and McKeown, 2000). Moreover, our investigation reveals that people may naturally follow some inherent structures when they write the abstractive summaries. To illustrate this observation, we show some examples in Figure 1, which are some top story summaries or headlines from the channel “Technology” of CNN. After analy"
D17-1222,D13-1047,0,0.0606521,"fferent languages show that our framework achieves better performance than the state-of-the-art models. 2 Related Works Automatic summarization is the process of automatically generating a summary that retains the most important content of the original text document (Nenkova and McKeown, 2012). Traditionally, the summarization methods can be classified into three categories: extraction-based methods (Erkan and Radev, 2004; Goldstein et al., 2000; Wan et al., 2007; Min et al., 2012; Nallapati et al., 2017; Cheng and Lapata, 2016; Cao et al., 2016; Song et al., 2017), compression-based methods (Li et al., 2013; Wang et al., 2013; Li et al., 2015, 2017), and abstraction-based methods. In fact, previous investigations show that human-written summaries are more abstractive (Barzilay and McKeown, 2005; Bing et al., 2015). Abstraction-based approaches can generate new sentences based on the facts from different source sentences. Barzilay and McKeown (2005) employed sentence fusion to generate a new sentence. Bing et al. (2015) proposed a more fine-grained fusion framework, where new sentences are generated by selecting and merging salient phrases. These methods can be regarded as a kind of indirect abst"
D17-1222,W04-1013,0,0.015671,"short text summarization dataset, consisting of pairs of (short text, summary) collected from Sina Weibo3 (Hu et al., 2015). We take Part-I as the training set, Part-II as the development set, and Part-III as the test set. There is a score in range 1 ∼ 5 labeled by human to indicate how relevant an article and its summary is. We only reserve those pairs with scores no less than 3. The size of the three sets are 2.4M, 8.7k, and 725 respectively. In our experiments, we only take Chinese character sequence as input, without performing word segmentation. 4.2 Evaluation Metrics We use ROUGE score (Lin, 2004) as our evaluation metric with standard options. The basic idea of ROUGE is to count the number of overlapping units between generated summaries and the reference summaries, such as overlapped n-grams, word sequences, and word pairs. F-measures of ROUGE-1 (R-1), ROUGE-2 (R-2), ROUGE-L (RL) and ROUGE-SU4 (R-SU4) are reported. 4.3 Comparative Methods We compare our model with some baselines and state-of-the-art methods. Because the datasets are 2096 1 https://catalog.ldc.upenn.edu/ldc2012t21 http://duc.nist.gov/duc2004 3 http://www.weibo.com 2 • ASC+FSC1 (Miao and Blunsom, 2016) uses a generativ"
D17-1222,P09-2075,0,0.0173551,"all meaning of input documents. Nallapati et al. (2016) utilized a trick to control the vocabulary size to improve the training efficiency. The calculations in these methods are all deterministic and the representation ability is limited. Miao and Blunsom (2016) extended the seq2seq framework and proposed a generative model to capture the latent summary information, but they do not consider the recurrent dependencies in their generative model leading to limited representation ability. Some research works employ topic models to capture the latent information from source documents or sentences. Wang et al. (2009) proposed a new Bayesian sentence-based topic model by making use of both the term-document and termsentence associations to improve the performance of sentence selection. Celikyilmaz and HakkaniTur (2010) estimated scores for sentences based on their latent characteristics using a hierarchical topic model, and trained a regression model to extract sentences. However, they only use the latent topic information to conduct the sentence salience estimation for extractive summarization. In contrast, our purpose is to model and learn the latent structure information from the target summaries and us"
D17-1222,P13-1136,0,0.0123536,"show that our framework achieves better performance than the state-of-the-art models. 2 Related Works Automatic summarization is the process of automatically generating a summary that retains the most important content of the original text document (Nenkova and McKeown, 2012). Traditionally, the summarization methods can be classified into three categories: extraction-based methods (Erkan and Radev, 2004; Goldstein et al., 2000; Wan et al., 2007; Min et al., 2012; Nallapati et al., 2017; Cheng and Lapata, 2016; Cao et al., 2016; Song et al., 2017), compression-based methods (Li et al., 2013; Wang et al., 2013; Li et al., 2015, 2017), and abstraction-based methods. In fact, previous investigations show that human-written summaries are more abstractive (Barzilay and McKeown, 2005; Bing et al., 2015). Abstraction-based approaches can generate new sentences based on the facts from different source sentences. Barzilay and McKeown (2005) employed sentence fusion to generate a new sentence. Bing et al. (2015) proposed a more fine-grained fusion framework, where new sentences are generated by selecting and merging salient phrases. These methods can be regarded as a kind of indirect abstractive summarizati"
D17-1222,D16-1031,0,0.0846778,"of the generated summaries. However, very few existing works specifically consider the latent structure information of summaries in their summarization models. Although a very popular neural network based sequence-to-sequence (seq2seq) framework has been proposed to tackle the abstractive summarization problem (Lopyrev, 2015; Rush et al., 2015; Nallapati et al., 2016), the calculation of the internal decoding states is entirely deterministic. The deterministic transformations in these discriminative models lead to limitations on the representation ability of the latent structure information. Miao and Blunsom (2016) extended the seq2seq framework and proposed a generative model to capture the latent summary information, but they did not consider the recurrent dependencies in their generative model leading to limited representation ability. To tackle the above mentioned problems, we design a new framework based on sequenceto-sequence oriented encoder-decoder model equipped with a latent structure modeling component. We employ Variational Auto-Encoders (VAEs) (Kingma and Welling, 2013; Rezende et al., 2014) as the base model for our generative framework which can handle the inference problem associated wit"
D17-1222,C12-1128,0,0.0296115,"Missing"
D17-1222,K16-1002,0,\N,Missing
D18-1069,W10-0214,0,0.0407167,"ributions of this paper are: (1) We propose a neural attention model for (dis)agreement inference which converts this Introduction The rise of various discussion forums and online debate platforms, has given users a lot of opportunities to express themselves and argue with each other. The online argumentation and discussion are always initiated and evolved by expressions of agreement or disagreement of participants. Inferring the agreement/disagreement in online debates is crucial for many other tasks in broader analysis of social media and argumentation mining, such as stance identification (Somasundaran and Wiebe, 2010), claim/argument extraction (Hidey et al., 2017) and persuasion analysis (Tan et al., 2016). It is observed that the expression of agreement/disagreement in debates can be decomposed into two factors: 1) the self-expression of claims and 2) argumentative expressions to interact with other participants. To illustrate this observation, we show some examples in Figure 1, which is one of quote-response pair (Q-R pair) in 4forum online ∗ Corresponding author. 665 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 665–670 c Brussels, Belgium, October 31 - N"
D18-1069,P17-1021,0,0.025089,"uote and response. In other NLP tasks, the end-to-end deep learning approaches with attention mechanism have shown impressive results. The attention mechanism is proposed by Bahdanau et al. (2014) in machine translation for selecting alignment between original words and foreign words before translation. For Document Classification, Yang et al. (2016) apply a hierarchical attention from word-level to sentence-level with learnable context vector. In Natural Language Inference (NLI), Liu et al. (2016) construct an inner-attention with mean pooling vector to seize important part from text itself. Hao et al. (2017) propose an cross attention modeling mutual influence between question and answer for Question Answering (QA). But there is no neural attention model incorporating both contextual and interactive information in the scenario of (dis)agreement inference. Self Attention The first source taken into consideration should be the text sequence itself, i.e. the attention from quote to quote itself and that from response to response itself. When issuing an opinion, people tend to center on several keywords which convey the main idea. Thus in some sense, self attention is a kind of dependency parsing tha"
D18-1069,walker-etal-2012-corpus,0,0.141389,"can be represented as [r1 , r2 , · · · , rT ], which shares the same vector space with quote. To model the dependence relation of text sequence, we leverage bidirectional LSTM (BiLSTM) to encode quote and response. The BiLSTM consists of a forward −−−−→ LST M which reads the text from x1 to xT and ←−−−− a backward LST M which reads from xT to x1 : Related Work With the development of social forums, works on (dis)agreement inference have shifted to online debate. Abbott et al. (2011) utilize wordbased and dependencies-based features to recognize disagreement in Internet Argument Corpus (IAC) (Walker et al., 2012). Rosenthal and McKeown (2015) present a new corpus derived from participant information, Agreement by Create Debaters (ABCD), and investigate new features for conversational structure. Further, Menini and Tonelli (2016) develop a SVM classifier to detect disagreement, relying on three aspects including sentiment-based, semantic and surface features extracted from both whole text and topic-related part. However, the performances of all these models highly depend on the quality of hand-crafted features. And these representations cannot reflect the interaction between quote and response. → − −−−"
D18-1069,N16-1174,0,0.0277847,"tegrates the information around xt . The quote − → ← − and response are encoded as hQ = [hQ ; hQ ] ∈ − → ← − RT ×2d and hR = [hR ; hR ] ∈ RT ×2d respectively. 3.2 Attention Component After encoding the implicit word semantics, we acquire the representation of both quote and response. In other NLP tasks, the end-to-end deep learning approaches with attention mechanism have shown impressive results. The attention mechanism is proposed by Bahdanau et al. (2014) in machine translation for selecting alignment between original words and foreign words before translation. For Document Classification, Yang et al. (2016) apply a hierarchical attention from word-level to sentence-level with learnable context vector. In Natural Language Inference (NLI), Liu et al. (2016) construct an inner-attention with mean pooling vector to seize important part from text itself. Hao et al. (2017) propose an cross attention modeling mutual influence between question and answer for Question Answering (QA). But there is no neural attention model incorporating both contextual and interactive information in the scenario of (dis)agreement inference. Self Attention The first source taken into consideration should be the text sequen"
D18-1069,W17-5102,0,0.0152504,"tion model for (dis)agreement inference which converts this Introduction The rise of various discussion forums and online debate platforms, has given users a lot of opportunities to express themselves and argue with each other. The online argumentation and discussion are always initiated and evolved by expressions of agreement or disagreement of participants. Inferring the agreement/disagreement in online debates is crucial for many other tasks in broader analysis of social media and argumentation mining, such as stance identification (Somasundaran and Wiebe, 2010), claim/argument extraction (Hidey et al., 2017) and persuasion analysis (Tan et al., 2016). It is observed that the expression of agreement/disagreement in debates can be decomposed into two factors: 1) the self-expression of claims and 2) argumentative expressions to interact with other participants. To illustrate this observation, we show some examples in Figure 1, which is one of quote-response pair (Q-R pair) in 4forum online ∗ Corresponding author. 665 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 665–670 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computatio"
D18-1069,P82-1020,0,0.785571,"Missing"
D18-1069,C16-1232,0,0.199702,"GOOD all the time … Quote God IS GOOD all the time … Then why doesn&apos;t He answer prayers like He says He will in the Bible ? Response Then why doesn&apos;t He answer prayers like He says He will in the Bible ? Figure 1: Sampled Q-R Pair with topic of evolution where the words colored red deliver crucial meaning of the text itself, while the words colored blue clarify the interactive relation between users. Previous works on agreement/disagreement inference mainly focus on exploiting features to model the semantic information which only reveals author’s self-expression. (Rosenthal and McKeown, 2015; Menini and Tonelli, 2016). These existing models treat agreement/disagreement inference as a ordinary sentiment classification problem and ignore the interactions between participants in the discussion. In order to jointly leverage the semantic information of the text and interactions between Q-R pairs, we regard the (dis)agreement inference as a special case of Natural Language Inference (NLI) (Rockt¨aschel et al., 2016), and propose a hybrid neural attention model to this problem. The proposed model consists of two kinds of attention: 1) self attention locates salient parts in text of quote and response, and 2) cros"
D18-1069,D14-1162,0,0.0810211,"g from gradient vanishing results in the poor performance. It is the hybrid attention that effects. As for ABCD, compared with ME based on textual features, Our BiLSTM-hybrid also gives superior performance of average F 1 in 3-way inference. Since ABCD is a corpus annotated by meta-thread rules, the ME attaching conversational structure attains the best Experiment and Results As prior work, we concentrate on direct disagreement and agreement between quote-response (QR) pairs. Specifically, in the proposed model, the size of hidden units is 128 and all word embeddings are initialized by GloVe (Pennington et al., 2014) of 300d. Both length of quote and response are set to 64, padded where necessary. Adam is the optimizer of model whose learning rate is 1e − 3, β is (0.9, 0.999),  is 1e − 8 and weight decay is 1e − 5. All models are trained by mini-batch of 32 instances, with 5-fold cross validation. 4.1 # Neutral 72,683 • Agreement by Create Debaters (ABCD) (Rosenthal and McKeown, 2015) is developed from createdebate.com with labels of agreement, disagreement and neutral. As the original settings, the comparison experiments are conducted on a balanced training set by downsampling and the full test set. whe"
D18-1069,W15-4625,0,0.372689,"rt models. 1 Disagree God IS GOOD all the time … Quote God IS GOOD all the time … Then why doesn&apos;t He answer prayers like He says He will in the Bible ? Response Then why doesn&apos;t He answer prayers like He says He will in the Bible ? Figure 1: Sampled Q-R Pair with topic of evolution where the words colored red deliver crucial meaning of the text itself, while the words colored blue clarify the interactive relation between users. Previous works on agreement/disagreement inference mainly focus on exploiting features to model the semantic information which only reveals author’s self-expression. (Rosenthal and McKeown, 2015; Menini and Tonelli, 2016). These existing models treat agreement/disagreement inference as a ordinary sentiment classification problem and ignore the interactions between participants in the discussion. In order to jointly leverage the semantic information of the text and interactions between Q-R pairs, we regard the (dis)agreement inference as a special case of Natural Language Inference (NLI) (Rockt¨aschel et al., 2016), and propose a hybrid neural attention model to this problem. The proposed model consists of two kinds of attention: 1) self attention locates salient parts in text of quot"
D18-1322,P96-1041,0,0.44216,"ever output a given offensive phrase. Second, the marginals could be used in phrase-based information extraction, such as extracting cities by finding high-probability x’s in the phrase “cities such as x” (Soderland et al., 2004; Bhagavatula et al., 2014). Finally, we can use the phrase probabilities to train an RNNLM itself, e.g. updating the RNNLM according to n-gram statistics instead of running text (Chelba et al., 2017; Noraset et al., 2018). In our experiments, we show an example of the last application. Estimating marginals from an RNNLM is challenging. Unlike an n-gram language model (Chen and Goodman, 1996), an RNNLM does not explicitly store marginal probabilities as its parameters. Instead, previous words are recurrently combined with the RNN’s hidden state to produce a new state, which is used to compute a probability distribution of the next word (Elman, 1990; Mikolov et al., 2010). When the preceding context is absent, however, the starting state is also missing. In order to compute the marginal probability, in principle we must marginalize over all possible previous contexts or all continuous-vector states. Both options pose a severe computational challenge. In this paper, we study how to"
D18-1322,W14-4012,0,0.0917312,"Missing"
D18-1322,P82-1020,0,0.804525,"Missing"
D18-1322,P17-1099,0,0.0206905,"Finally, we show how we can use the marginal estimation to improve an RNNLM by training the marginals to match n-gram probabilities from a larger corpus. 1 Introduction Recurrent neural networks (RNNs) are the stateof-the-art architecture for statistical language modeling (Jozefowicz et al., 2016; Melis et al., 2018), the task of assigning a probability distribution to a sequence of words. The relative likelihoods of the sequences are useful in applications such as speech recognition, machine translation, automated conversation, and summarization (Mikolov et al., 2010; Bahdanau et al., 2014; See et al., 2017; Wen et al., 2017). Typically, RNN language models (RNNLMs) are trained on complete sequences (e.g., a sentence or an utterance), or long sequences (e.g. several documents), and used in the same fashion in applications or testing. ∗ thanapon.nor@mahidol.edu d-downey@northwestern.edu ‡ lyndonbig@tencent.com † Lidong Bing‡ Tencent AI Lab Shenzhen, China A question arises when we want to compute the probability of a short sequence without the preceding context. For instance, we may wish to query for how likely the RNNLM is to generate a particular phase aggregated over all contexts. We refer to"
D18-1354,K16-1002,0,0.244028,"al., 2016b; Xing et al., 2017; Zhou et al., 2017b) and modifying the architecture of existing models (Li et al., 2016a; Xu et al., 2017; Zhou et al., 2017a). Another solution to address this problem is to add stochastic latent variables in order to change the deterministic structure of Seq2Seq models. VAE (Kingma and Welling, 2013) is one of the most successful models (Serban et al., 2017; Zhao et al., 2017; Shen et al., 2017; Cao and Clark, 2017). However, VAE-based models only use a single latent variable to encode the whole response sequence, thus suffering from the model collapse problem (Bowman et al., 2016). To overcome this problem, we propose a novel model that based on the variational autoregressive decoder to better represent highly structural latent variables. 2.2 Variational Autoregressive Models Recently, some works attempted to combine VAE with autoregressive models to better process input sequences. Broadly speaking, they can be categorized into two groups. Methods in the first group leverage autoregressive models to improve the inference of traditional VAEs. The most well-known model is Inverse Autoregressive Flow (IAF), which used a series of invertible transformations based on the au"
D18-1354,E17-2029,0,0.0370672,"eneric responses, such as I don’t know (Li et al., 2016a). Various approaches have been proposed to address this problem, including adding additional information (Li et al., 2016b; Xing et al., 2017; Zhou et al., 2017b) and modifying the architecture of existing models (Li et al., 2016a; Xu et al., 2017; Zhou et al., 2017a). Another solution to address this problem is to add stochastic latent variables in order to change the deterministic structure of Seq2Seq models. VAE (Kingma and Welling, 2013) is one of the most successful models (Serban et al., 2017; Zhao et al., 2017; Shen et al., 2017; Cao and Clark, 2017). However, VAE-based models only use a single latent variable to encode the whole response sequence, thus suffering from the model collapse problem (Bowman et al., 2016). To overcome this problem, we propose a novel model that based on the variational autoregressive decoder to better represent highly structural latent variables. 2.2 Variational Autoregressive Models Recently, some works attempted to combine VAE with autoregressive models to better process input sequences. Broadly speaking, they can be categorized into two groups. Methods in the first group leverage autoregressive models to imp"
D18-1354,N16-1014,0,0.0894923,"associating latent variables to different time steps of autoregressive decoder and approximating the posterior of latent variables by augmenting the hidden states of a backward RNN. • A BOW based auxiliary objective is proposed to help preserving the diversity of generated responses. 2 2.1 Related Work Conversational Systems As neural network based models dominate the research in natural language processing, Seq2Seq models have been widely used for response generation (Sordoni et al., 2015). However, Seq2seq models suffer from the problem of generating generic responses, such as I don’t know (Li et al., 2016a). Various approaches have been proposed to address this problem, including adding additional information (Li et al., 2016b; Xing et al., 2017; Zhou et al., 2017b) and modifying the architecture of existing models (Li et al., 2016a; Xu et al., 2017; Zhou et al., 2017a). Another solution to address this problem is to add stochastic latent variables in order to change the deterministic structure of Seq2Seq models. VAE (Kingma and Welling, 2013) is one of the most successful models (Serban et al., 2017; Zhao et al., 2017; Shen et al., 2017; Cao and Clark, 2017). However, VAE-based models only us"
D18-1354,P16-1094,0,0.131353,"associating latent variables to different time steps of autoregressive decoder and approximating the posterior of latent variables by augmenting the hidden states of a backward RNN. • A BOW based auxiliary objective is proposed to help preserving the diversity of generated responses. 2 2.1 Related Work Conversational Systems As neural network based models dominate the research in natural language processing, Seq2Seq models have been widely used for response generation (Sordoni et al., 2015). However, Seq2seq models suffer from the problem of generating generic responses, such as I don’t know (Li et al., 2016a). Various approaches have been proposed to address this problem, including adding additional information (Li et al., 2016b; Xing et al., 2017; Zhou et al., 2017b) and modifying the architecture of existing models (Li et al., 2016a; Xu et al., 2017; Zhou et al., 2017a). Another solution to address this problem is to add stochastic latent variables in order to change the deterministic structure of Seq2Seq models. VAE (Kingma and Welling, 2013) is one of the most successful models (Serban et al., 2017; Zhao et al., 2017; Shen et al., 2017; Cao and Clark, 2017). However, VAE-based models only us"
D18-1354,D17-1222,1,0.881935,"Missing"
D18-1354,D16-1230,0,0.071479,"Missing"
D18-1354,N15-1020,0,0.0998218,"Missing"
D18-1354,D17-1065,0,0.0351809,"e diversity of generated responses. 2 2.1 Related Work Conversational Systems As neural network based models dominate the research in natural language processing, Seq2Seq models have been widely used for response generation (Sordoni et al., 2015). However, Seq2seq models suffer from the problem of generating generic responses, such as I don’t know (Li et al., 2016a). Various approaches have been proposed to address this problem, including adding additional information (Li et al., 2016b; Xing et al., 2017; Zhou et al., 2017b) and modifying the architecture of existing models (Li et al., 2016a; Xu et al., 2017; Zhou et al., 2017a). Another solution to address this problem is to add stochastic latent variables in order to change the deterministic structure of Seq2Seq models. VAE (Kingma and Welling, 2013) is one of the most successful models (Serban et al., 2017; Zhao et al., 2017; Shen et al., 2017; Cao and Clark, 2017). However, VAE-based models only use a single latent variable to encode the whole response sequence, thus suffering from the model collapse problem (Bowman et al., 2016). To overcome this problem, we propose a novel model that based on the variational autoregressive decoder to better"
D18-1354,P17-1061,0,0.404009,"te-of-the-art baselines. 1 Figure 1: Distributions of latent variable Introduction Recently, variational Bayesian models have shown attractive merits from both theoretical and practical perspectives (Kingma and Welling, 2013). As one of the most successful variational Bayesian models, Conditional Variational Auto-Encoder (CVAE) (Kingma et al., 2014) was proposed to improve upon the traditional Sequence-to-Sequence (Seq2Seq) dialogue models. The CVAE based models incorporate stochastic latent variables into decoders in order to generate more relevant and diverse responses (Serban et al., 2017; Zhao et al., 2017; Shen et al., 2017). However, existing CVAE ∗ Corresponding author As illustrated in Figure 1, the unimodal latent variable z used in the conventional VAE usually captures simple unimodal pattern of responses. However, in open-domain conversations, an utterance may have various responses which form complex multimodal distributions. To overcome this problem and improve the quality of generated responses, we propose a novel model, named Variational Autoregressive Decoder (VAD) to iteratively incorporate a series of latent variables into the autoregressive decoder. In particular, a distinct late"
D18-1420,K16-1002,0,0.108391,"Missing"
D18-1420,Q18-1031,0,0.0250233,"ms, producing monotonous language, and generating short common sentences (Li et al., 2017). To solve these problems, some researchers branch out into the way of post-editing (could be under some guidance, say sentiment polarity) a given message to generate text of better quality. For example, skeleton-based text generation first outlines a skeleton in the form of phrases/words, and then starts from the skeleton to generate text (Wang et al., 2017; Xiao et al., 2016). Another line of works conduct editing on an existing sentence and expect that the output will serve particular purposes better (Guu et al., 2018). Similarly in conversation, some systems post-edit the retrieval results to generate new sentences as the response (Song et al., 2016). The third type is to perform editing on the input under the guidance of specific style. For example, Shen et al. (2017) take a sentence with negative sentiment as input, and edit it to transfer its sentiment polarity into positive. In this paper, we generalize the third type of post-editing into a more general scenario, named Quantifiable Sequence Editing (QuaSE). Specifically, in the training stage, each input sentence is associated with a numeric outcome. F"
D18-1420,W17-4902,0,0.0179707,"lies in the content factors z and z 0 . Given that z and z 0 are not enforced to resemble each other when Lsim is excluded from this tuning step, Lrec and Ld−rec cannot be minimized simultaneously. Moreover, when we minimize Lsim in the second step with the weights of Lrec and Lmse fixed, we observe that Ld−rec also decreases, which complies with the above analysis. 5 Related Works Inspired by the task of image style transfer (Gatys et al., 2016; Liu and Tuzel, 2016), researchers proposed the task of text style transfer and obtained some encouraging results (Fu et al., 2018; Hu et al., 2017; Jhamtani et al., 2017; Melnyk et al., 2017; Zhang et al., 2018; Li et al., 2018; Prabhumoye et al., 2018; Niu and Bansal, 2018). Existing studies on text style transfer mainly aim at transferring text from an original style into a target style, e.g., from negative to positive, from male to female, from rude/normal to polite; from modern text to Shakespeare style, etc. In contrast, our proposed task QuaSE assumes each sentence is associated with an outcome pertaining to continues values, and the editing is under the guidance of a specific target. To transfer the style of a sentence, the paradigm of most works (Shen"
D18-1420,D17-1230,0,0.030564,"lts are reported and discussed to elaborate the peculiarities of our framework. 1 ∗ The work described in this paper was done when Yi Liao was an intern at Tencent AI Lab. The work is partially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: CUHK413510) 1 Our code and data are available at https:// bitbucket.org/leoeaton/quase/src/master/ Introduction Typical neural text generation is observed suffering from the problems of repetitions in word ngrams, producing monotonous language, and generating short common sentences (Li et al., 2017). To solve these problems, some researchers branch out into the way of post-editing (could be under some guidance, say sentiment polarity) a given message to generate text of better quality. For example, skeleton-based text generation first outlines a skeleton in the form of phrases/words, and then starts from the skeleton to generate text (Wang et al., 2017; Xiao et al., 2016). Another line of works conduct editing on an existing sentence and expect that the output will serve particular purposes better (Guu et al., 2018). Similarly in conversation, some systems post-edit the retrieval results"
D18-1420,N18-1169,0,0.0943885,"Missing"
D18-1420,P14-5010,0,0.00247152,"ry of the primary data. The vocabulary size of the dataset is 9,625. In total, our dataset contains 599K sentences, and we randomly hold 50K for test, 10K for validation, and the remaining for training. For training, we need each input sentence being associated with a rating value, and for test, we need to measure the rating of a generated sentence to check if the generated sentence satisfies the specified outcome target. Therefore, an automatic method is needed for measuring the rating values of training sentences and generated sentences. We employ the sentiment analyzer in Stanford CoreNLP (Manning et al., 2014) to do so. Specifically, we first invoke CoreNLP to output the probability of each rating in {1, 2, 3, 4, 5} for a sentence, then we take the sum of the probabilitymultiplied ratings as the sentence rating. Some statistics of the data is given in Table 2. Hereafter, we use “rating” and “outcome” interchangeably. 3859 2 https://www.yelp.com/dataset/challenge Rating interval Sentence# [1, 2) 34273 [2, 3) 231740 [3, 4) 165159 [4, 5] 167803 4.2 Table 2: Numbers of sentences in each rating interval. One may think that would it be possible to use the original rating given by Yelp users as outcome fo"
D18-1420,Q18-1027,0,0.0238259,"Lsim is excluded from this tuning step, Lrec and Ld−rec cannot be minimized simultaneously. Moreover, when we minimize Lsim in the second step with the weights of Lrec and Lmse fixed, we observe that Ld−rec also decreases, which complies with the above analysis. 5 Related Works Inspired by the task of image style transfer (Gatys et al., 2016; Liu and Tuzel, 2016), researchers proposed the task of text style transfer and obtained some encouraging results (Fu et al., 2018; Hu et al., 2017; Jhamtani et al., 2017; Melnyk et al., 2017; Zhang et al., 2018; Li et al., 2018; Prabhumoye et al., 2018; Niu and Bansal, 2018). Existing studies on text style transfer mainly aim at transferring text from an original style into a target style, e.g., from negative to positive, from male to female, from rude/normal to polite; from modern text to Shakespeare style, etc. In contrast, our proposed task QuaSE assumes each sentence is associated with an outcome pertaining to continues values, and the editing is under the guidance of a specific target. To transfer the style of a sentence, the paradigm of most works (Shen et al., 2017; Mueller et al., 2017; Prabhumoye et al., 2018) first learns the latent representation of th"
D18-1420,P18-1080,0,0.0520265,"resemble each other when Lsim is excluded from this tuning step, Lrec and Ld−rec cannot be minimized simultaneously. Moreover, when we minimize Lsim in the second step with the weights of Lrec and Lmse fixed, we observe that Ld−rec also decreases, which complies with the above analysis. 5 Related Works Inspired by the task of image style transfer (Gatys et al., 2016; Liu and Tuzel, 2016), researchers proposed the task of text style transfer and obtained some encouraging results (Fu et al., 2018; Hu et al., 2017; Jhamtani et al., 2017; Melnyk et al., 2017; Zhang et al., 2018; Li et al., 2018; Prabhumoye et al., 2018; Niu and Bansal, 2018). Existing studies on text style transfer mainly aim at transferring text from an original style into a target style, e.g., from negative to positive, from male to female, from rude/normal to polite; from modern text to Shakespeare style, etc. In contrast, our proposed task QuaSE assumes each sentence is associated with an outcome pertaining to continues values, and the editing is under the guidance of a specific target. To transfer the style of a sentence, the paradigm of most works (Shen et al., 2017; Mueller et al., 2017; Prabhumoye et al., 2018) first learns the late"
D18-1420,D17-1298,0,0.0298791,"of the style. A transferred sentence is generated from a modified latent representation. Different from the aforementioned works based on latent representations, Li et al. (2018) propose a simpler method that achieves attribute transfer by changing a few attribute marker words or phrases in the sentence that are indicative of a particular attribute, while leaving the rest of the sentence largely unchanged. The simple method is able to generate better-quality sentences than the aforementioned works. Besides style transfer, sentence editing models can be developed for other tasks. For example, Schmaltz et al. (2017) propose neural sequence-labelling models for correcting the grammatical errors of sentences. 6 Conclusions We proposed a new task namely Quantifiable Sequence Editing (QuaSE), where a model needs to edit an input sentences towards the direction of a numerical outcome target. To tackle this task, we proposed a novel framework that simultaneously exploits the single sentences and pseudo-parallel sentence pairs. For evaluation, we prepared a dataset with Yelp sentences and their ratings. Experimental results show that our framework outperforms the compared methods under the measures of sentiment"
D18-1420,N18-1138,0,0.0434783,"en that z and z 0 are not enforced to resemble each other when Lsim is excluded from this tuning step, Lrec and Ld−rec cannot be minimized simultaneously. Moreover, when we minimize Lsim in the second step with the weights of Lrec and Lmse fixed, we observe that Ld−rec also decreases, which complies with the above analysis. 5 Related Works Inspired by the task of image style transfer (Gatys et al., 2016; Liu and Tuzel, 2016), researchers proposed the task of text style transfer and obtained some encouraging results (Fu et al., 2018; Hu et al., 2017; Jhamtani et al., 2017; Melnyk et al., 2017; Zhang et al., 2018; Li et al., 2018; Prabhumoye et al., 2018; Niu and Bansal, 2018). Existing studies on text style transfer mainly aim at transferring text from an original style into a target style, e.g., from negative to positive, from male to female, from rude/normal to polite; from modern text to Shakespeare style, etc. In contrast, our proposed task QuaSE assumes each sentence is associated with an outcome pertaining to continues values, and the editing is under the guidance of a specific target. To transfer the style of a sentence, the paradigm of most works (Shen et al., 2017; Mueller et al., 2017; Prab"
D19-1019,Q18-1002,0,0.110033,"gue participants (Majumder et al., 2018). However, none of these works can do dialogue-level satisfaction classification and utterance-level sentiment classification simultaneously. Recent studies (Cerisara et al., 2018; Ma et al., 2018; Wang et al., 2018b) employing multi-task learning open a possibility to address this issue. However, these models must be trained under the supervision of both documentlevel and sentence-level sentiment labels in which the later are generally not easy to obtain. Sentiment classification based on Multiple Instance Learning (MIL) frameworks (Wang and Wan, 2018; Angelidis and Lapata, 2018) aims to perform document-level and sentence-level sentiment classification tasks simultaneously with the supervision of document labels only. Angelidis and Lapata (2018) proposed an MIL model for fine-grained sentiment analysis. Wang and Wan (2018) further applied the model to peerreviewed research papers by integrating a memory built from abstracts. However, their models are not suitable for our SSA task because they ignore 3 Context-Assisted MIL Network In order to predict service satisfaction and identify sentiments of all customer utterances with available satisfaction labels, we propose"
D19-1019,D17-1047,1,0.823046,"e staff, meanwhile locating possible sentiment reasons, i.e., sentiment identification of the customer utterances. For example, Figure 1 gives the satisfaction prediction of the service as “unsatisfied” and identifies the detailed sentiments of all customer utterances. Obviously, SSA focuses on two special cases of text classification over predefined satisfaction labels (“well satisfied/met/unsatisfied”) and predefined sentiment labels (“positive/neutral/negative”). Text classification has been widely studied for decades, such as sentiment classification on product reviews (Song et al., 2017; Chen et al., 2017; Li et al., 2018, 2019), stance classification on tweets or blogs (Du et al., 2017; Liu, 2010), emotion classification for chit-chat (Majumder et al., 2018), etc. However, all these methods cannot deal with these two classification tasks simultaneously in a unified framework. Although recent studies on multi-task learning framework suggest that closely related tasks can improve each other mutually from separated supervision information (Ma et al., 2018; Cerisara et al., 2018; Wang et al., 2018b), the acquisition of sentence (or utterance)-level sentiment labels, which is required by multi-tas"
D19-1019,P82-1020,0,0.713342,"Missing"
D19-1019,P18-1087,1,0.850541,"locating possible sentiment reasons, i.e., sentiment identification of the customer utterances. For example, Figure 1 gives the satisfaction prediction of the service as “unsatisfied” and identifies the detailed sentiments of all customer utterances. Obviously, SSA focuses on two special cases of text classification over predefined satisfaction labels (“well satisfied/met/unsatisfied”) and predefined sentiment labels (“positive/neutral/negative”). Text classification has been widely studied for decades, such as sentiment classification on product reviews (Song et al., 2017; Chen et al., 2017; Li et al., 2018, 2019), stance classification on tweets or blogs (Du et al., 2017; Liu, 2010), emotion classification for chit-chat (Majumder et al., 2018), etc. However, all these methods cannot deal with these two classification tasks simultaneously in a unified framework. Although recent studies on multi-task learning framework suggest that closely related tasks can improve each other mutually from separated supervision information (Ma et al., 2018; Cerisara et al., 2018; Wang et al., 2018b), the acquisition of sentence (or utterance)-level sentiment labels, which is required by multi-task learning, remai"
D19-1019,D18-1031,1,0.903504,"widely studied for decades, such as sentiment classification on product reviews (Song et al., 2017; Chen et al., 2017; Li et al., 2018, 2019), stance classification on tweets or blogs (Du et al., 2017; Liu, 2010), emotion classification for chit-chat (Majumder et al., 2018), etc. However, all these methods cannot deal with these two classification tasks simultaneously in a unified framework. Although recent studies on multi-task learning framework suggest that closely related tasks can improve each other mutually from separated supervision information (Ma et al., 2018; Cerisara et al., 2018; Wang et al., 2018b), the acquisition of sentence (or utterance)-level sentiment labels, which is required by multi-task learning, remains a laborious and expensive endeavor. In contrast, coarse-grained document (or dialogue)-level annotations are relatively easy to obtain due to the widespread use of opinion grading interfaces (e.g., ratings). • We introduce a new SSA task based on customer service dialogues. We thus propose a novel CAMIL model to predict the sentiment distributions of all customer utterances, and then aggregate those distributions to determine the final satisfaction polarity. • We further pro"
D19-1019,N16-1174,0,0.652404,"teractions between customers and servers. In contrast, we consider complex multi-turn interactions within dialogues and explore context clue matching between customer utterances and server utterances for multi-tasking in the SSA task. Specifically, we improve the basic MIL models by proposing a position-guided automatic context clue matching mechanism (CCMM) to conduct customer utterance and context clues alignments for better sentiment classification to boost satisfaction classification. Other related work related to sentiment analysis for subjective texts in different granularities include (Yang et al., 2016; Wu and Huang, 2016; Yang et al., 2018b; Du et al., 2017; Wang et al., 2018a; Song et al., 2019). Related Work Service satisfaction analysis (SSA) is closely related to sentiment analysis (SA), because the sentiment of the customer utterances is a basic clue signaling the customer’s satisfaction. Existing SA works aim to predict sentiment polarities (positive, neutral and negative) for subjective texts in different granularities, such as word (Feng et al., 2015; Song et al., 2016), sentence (Ma et al., 2017), short text (Song et al., 2015) and document (Yang et al., 2018a). In these studies,"
D19-1024,D15-1060,1,0.888782,"Missing"
D19-1024,E17-1104,0,0.0173039,"l., 2013), where L1 -norm is used. Finally, during the meta-training phase or the training stage of the meta-testing phase, a loss function L related to C is computed, and the metalearner is adopted to optimize L so that the framework can be easily adapted to new relations and entities. Otherwise, during the testing stage of the meta-testing phase, we collect scores of the correct triplet and other candidates, and then we compute metrics based on the rank of correct triplet within all scores for evaluation. 4.1 The core of the encoding process is a N -layer convolutional neural network (CNN) (Conneau et al., 2017), which is shown to have excellent ability of extracting information. In our CNN, the basic convolutional block consists of three consecutive operations: two 1-d convolutions, an instance normalization (Ulyanov et al., 2016), and a non-linear mapping. For the pooling strategy, max pooling with a proper stride is used to distill the key information in the previous N − 1 layers, and Description Encoder In KG, if an entity is involved in multiple relations, it is natural that different relations are more 253 mean pooling is used to gather the information in the last layer. Moreover, in Step 3, we"
D19-1024,N18-2053,0,0.0284368,"y a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14204418). 1 The implementation of our framework can be found in https://github.com/ZihaoWang/ Few-shot-KGC. 250 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 250–260, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 0.10 0.05 0.00 Relation Frequency et al., 2013; Socher et al., 2013; Wang et al., 2014; Trouillon et al., 2016; Nguyen et al., 2018) that have been proposed to learn good embeddings for entities and relations. However, embedding of uncommon relation or entities can not learn a good representation due to the data insufficiency. Some research has proposed that additional information can be introduced to enhance the learning performance. Among different types of information, textual descriptions is commonly considered by previous works (Zhong et al., 2015; Toutanova et al., 2015; Xie et al., 2016; Shi and Weninger, 2018). Recently, meta-learning is also proposed by (Xiong et al., 2018) to learn infrequent long-tailed relation"
D19-1024,D18-1223,0,0.229806,"encent AI Lab, Shenzhen, China 3 R&D Center Singapore, Machine Intelligence Technology, Alibaba DAMO Academy zihaowangbupt@gmail.com {kplai, wlam}@se.cuhk.edu.hk pijili@tencent.com l.bing@alibaba-inc.com Abstract KGs still have the issue of incomplete facts. To deal with the problem, Knowledge Graph Completion (KGC) task is introduced to automatically deduce and fill the missing facts. There exist many previous works focusing on this task and embedding-based methods (Bordes et al., 2013; Wang et al., 2014; Trouillon et al., 2016) achieves the best performance among them. Recent works such as (Xiong et al., 2018) have pointed out that relations in KGs follow a long-tailed distribution. To be more precise, a large proportion of relations have only a few facts in KGs. However, previous works of KGC usually focused on small proportions of frequent relations and ignored the remaining ones. One observation is that they often conducted experiments on small datasets such as FB15k and WN18 (Bordes et al., 2013) where a relation typically possesses thousands of facts. Moreover, after analyzing real-world KGs, we find that the more infrequently a relation appears, the entities within its facts are also more unc"
D19-1024,P15-1128,0,0.0414025,"ng stage. Experiments are conducted on two datasets from real-world KGs, and the results show that our framework 1 outperforms previous methods when dealing with infrequent relations and their accompanying uncommon entities. 1 Introduction Modern knowledge graphs (KGs)(Bollacker et al., 2008; Lehmann et al., 2015; Vrandeˇci´c and Kr¨otzsch, 2014) consist of a large number of facts, where each fact is represented as a triplet consisting of two entities and a binary relation between them. KGs provide rich information and it has been widely adopted in different tasks, such as question answering (Yih et al., 2015), information extraction (Bing et al., 2017, 2015, 2016) and image classification (Marino et al., 2017). However, Previous works such as (Xiong et al., 2018) only focused on those infrequent relations and ignored the accompanying problem of uncommon entities. When handling uncommon entities, relying only on the structural information of KGs would lead to inferior performance due to data insufficiency, ∗ The work described in this paper is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14204418). 1 The impl"
D19-1024,D15-1174,0,0.299412,"sociation for Computational Linguistics 0.10 0.05 0.00 Relation Frequency et al., 2013; Socher et al., 2013; Wang et al., 2014; Trouillon et al., 2016; Nguyen et al., 2018) that have been proposed to learn good embeddings for entities and relations. However, embedding of uncommon relation or entities can not learn a good representation due to the data insufficiency. Some research has proposed that additional information can be introduced to enhance the learning performance. Among different types of information, textual descriptions is commonly considered by previous works (Zhong et al., 2015; Toutanova et al., 2015; Xie et al., 2016; Shi and Weninger, 2018). Recently, meta-learning is also proposed by (Xiong et al., 2018) to learn infrequent long-tailed relations in KG. • We propose a novel model to extract relationspecific information from entity description for entities with multiple relations. 2.2 Meta-Learning Meta-learning (Lemke et al., 2015) aims at learning common experiences across different tasks and easy adapting the existing model to new tasks. One interesting application of meta-learning is few-shot learning problem where each task has only a few training data available. Some research focus"
D19-1024,D15-1031,0,0.188219,"r 3–7, 2019. 2019 Association for Computational Linguistics 0.10 0.05 0.00 Relation Frequency et al., 2013; Socher et al., 2013; Wang et al., 2014; Trouillon et al., 2016; Nguyen et al., 2018) that have been proposed to learn good embeddings for entities and relations. However, embedding of uncommon relation or entities can not learn a good representation due to the data insufficiency. Some research has proposed that additional information can be introduced to enhance the learning performance. Among different types of information, textual descriptions is commonly considered by previous works (Zhong et al., 2015; Toutanova et al., 2015; Xie et al., 2016; Shi and Weninger, 2018). Recently, meta-learning is also proposed by (Xiong et al., 2018) to learn infrequent long-tailed relations in KG. • We propose a novel model to extract relationspecific information from entity description for entities with multiple relations. 2.2 Meta-Learning Meta-learning (Lemke et al., 2015) aims at learning common experiences across different tasks and easy adapting the existing model to new tasks. One interesting application of meta-learning is few-shot learning problem where each task has only a few training data availa"
D19-1093,P16-1231,0,0.178772,"ical University. † Equal contribution. of shift-reduce decisions. The advantage of this method is that the parsing time is linear with respect to the sequence length. The limitation, however, is that the decisions made at each step are based on local information, disallowing the model to capture long distance dependencies and also causing error propagation to subsequent steps. Recent methods attempt to address this issue using neural networks capable of remembering long range relationships such as Stacked LSTMs (Dyer et al., 2015; Ballesteros et al., 2015) or using globally normalized models (Andor et al., 2016). The globally optimized methods, on the other hand, learn scoring functions for subtrees and perform search over all possible trees to find the most probable tree for a text. Recent graph-based methods use neural models as scoring functions (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017). Despite being more accurate than greedy parsers, these methods are generally slow having a polynomial time complexity (O(n3 ) or higher). Recently, transition-based top-down parsing with Pointer Networks (Vinyals et al., 2015) has attained state-of-the-art results in both dependency and discourse p"
D19-1093,D15-1041,0,0.0283709,"der the Joint PhD Program between Alibaba and Nanyang Technological University. † Equal contribution. of shift-reduce decisions. The advantage of this method is that the parsing time is linear with respect to the sequence length. The limitation, however, is that the decisions made at each step are based on local information, disallowing the model to capture long distance dependencies and also causing error propagation to subsequent steps. Recent methods attempt to address this issue using neural networks capable of remembering long range relationships such as Stacked LSTMs (Dyer et al., 2015; Ballesteros et al., 2015) or using globally normalized models (Andor et al., 2016). The globally optimized methods, on the other hand, learn scoring functions for subtrees and perform search over all possible trees to find the most probable tree for a text. Recent graph-based methods use neural models as scoring functions (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017). Despite being more accurate than greedy parsers, these methods are generally slow having a polynomial time complexity (O(n3 ) or higher). Recently, transition-based top-down parsing with Pointer Networks (Vinyals et al., 2015) has attained st"
D19-1093,D14-1179,0,0.033968,"Missing"
D19-1093,P15-1033,0,0.0242619,"∗ Linlin Liu is under the Joint PhD Program between Alibaba and Nanyang Technological University. † Equal contribution. of shift-reduce decisions. The advantage of this method is that the parsing time is linear with respect to the sequence length. The limitation, however, is that the decisions made at each step are based on local information, disallowing the model to capture long distance dependencies and also causing error propagation to subsequent steps. Recent methods attempt to address this issue using neural networks capable of remembering long range relationships such as Stacked LSTMs (Dyer et al., 2015; Ballesteros et al., 2015) or using globally normalized models (Andor et al., 2016). The globally optimized methods, on the other hand, learn scoring functions for subtrees and perform search over all possible trees to find the most probable tree for a text. Recent graph-based methods use neural models as scoring functions (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017). Despite being more accurate than greedy parsers, these methods are generally slow having a polynomial time complexity (O(n3 ) or higher). Recently, transition-based top-down parsing with Pointer Networks (Vinyals et"
D19-1093,C96-1058,0,0.150356,"xamples of a dependency tree and a sentence-level discourse tree that respectively represent how the words and clauses are related in a sentence. Such parse trees are directly useful in numerous NLP applications, and also serve as intermediate representations for further language processing tasks such as semantic and discourse processing. Existing approaches to parsing can be distinguished based on whether they employ a greedy transition-based algorithm (Marcu, 1999; Zhang and Nivre, 2011; Wang et al., 2017) or a globally optimized algorithm such as graph-based methods for dependency parsing (Eisner, 1996) or chart parsing for discourse (Soricut and Marcu, 2003; Joty et al., 2015). Transition-based parsers build the tree incrementally by making a series ∗ Linlin Liu is under the Joint PhD Program between Alibaba and Nanyang Technological University. † Equal contribution. of shift-reduce decisions. The advantage of this method is that the parsing time is linear with respect to the sequence length. The limitation, however, is that the decisions made at each step are based on local information, disallowing the model to capture long distance dependencies and also causing error propagation to subseq"
D19-1093,P14-1048,0,0.170523,"rence analysis in RST consists of two subtasks: (a) identifying the EDUs in a text, referred to as Discourse Segmentation, and (b) building a discourse tree by linking the EDUs hierarchically, referred to as Discourse Parsing. This work focuses on the more challenging task of discourse parsing assuming that EDUs have already been identified. In fact, state-of-the-art segmenter (Lin et al., 2019) has already achieved 95.6 F1 on RST discourse treebank, where the human agreement is 98.3 F1 . Earlier methods have mostly utilized handcrafted lexical and syntactic features (Soricut and Marcu, 2003; Feng and Hirst, 2014; Joty et al., 2015; Wang et al., 2017). Recent approaches have 1008 shown competitive results with neural models that are able to automatically learn the feature representations in an end-to-end fashion (Ji and Eisenstein, 2014; Li et al., 2014). Very recently, Lin et al. (2019) propose a parser based on pointer networks and achieve state-of-the-art performance. 3 Remark. Although related, the dependency and RST tree structures (hence the parsing tasks) are different. RST structure is similar to constituency structure. Therefore, the differences between constituency and dependency structures"
D19-1093,P14-1002,0,0.284759,". This work focuses on the more challenging task of discourse parsing assuming that EDUs have already been identified. In fact, state-of-the-art segmenter (Lin et al., 2019) has already achieved 95.6 F1 on RST discourse treebank, where the human agreement is 98.3 F1 . Earlier methods have mostly utilized handcrafted lexical and syntactic features (Soricut and Marcu, 2003; Feng and Hirst, 2014; Joty et al., 2015; Wang et al., 2017). Recent approaches have 1008 shown competitive results with neural models that are able to automatically learn the feature representations in an end-to-end fashion (Ji and Eisenstein, 2014; Li et al., 2014). Very recently, Lin et al. (2019) propose a parser based on pointer networks and achieve state-of-the-art performance. 3 Remark. Although related, the dependency and RST tree structures (hence the parsing tasks) are different. RST structure is similar to constituency structure. Therefore, the differences between constituency and dependency structures also hold here.1 First, while dependency relations can only be between words, discourse relations can be between elementary units, between larger units or both. Second, in dependency parsing, any two words can be linked, whereas"
D19-1093,D12-1083,1,0.889814,"Missing"
D19-1093,J15-3002,1,0.948703,"espectively represent how the words and clauses are related in a sentence. Such parse trees are directly useful in numerous NLP applications, and also serve as intermediate representations for further language processing tasks such as semantic and discourse processing. Existing approaches to parsing can be distinguished based on whether they employ a greedy transition-based algorithm (Marcu, 1999; Zhang and Nivre, 2011; Wang et al., 2017) or a globally optimized algorithm such as graph-based methods for dependency parsing (Eisner, 1996) or chart parsing for discourse (Soricut and Marcu, 2003; Joty et al., 2015). Transition-based parsers build the tree incrementally by making a series ∗ Linlin Liu is under the Joint PhD Program between Alibaba and Nanyang Technological University. † Equal contribution. of shift-reduce decisions. The advantage of this method is that the parsing time is linear with respect to the sequence length. The limitation, however, is that the decisions made at each step are based on local information, disallowing the model to capture long distance dependencies and also causing error propagation to subsequent steps. Recent methods attempt to address this issue using neural networ"
D19-1093,Q16-1023,0,0.0631775,"Missing"
D19-1093,P14-1003,0,0.305546,"e more challenging task of discourse parsing assuming that EDUs have already been identified. In fact, state-of-the-art segmenter (Lin et al., 2019) has already achieved 95.6 F1 on RST discourse treebank, where the human agreement is 98.3 F1 . Earlier methods have mostly utilized handcrafted lexical and syntactic features (Soricut and Marcu, 2003; Feng and Hirst, 2014; Joty et al., 2015; Wang et al., 2017). Recent approaches have 1008 shown competitive results with neural models that are able to automatically learn the feature representations in an end-to-end fashion (Ji and Eisenstein, 2014; Li et al., 2014). Very recently, Lin et al. (2019) propose a parser based on pointer networks and achieve state-of-the-art performance. 3 Remark. Although related, the dependency and RST tree structures (hence the parsing tasks) are different. RST structure is similar to constituency structure. Therefore, the differences between constituency and dependency structures also hold here.1 First, while dependency relations can only be between words, discourse relations can be between elementary units, between larger units or both. Second, in dependency parsing, any two words can be linked, whereas RST allows connec"
D19-1093,P19-1410,1,0.721475,"ctions for subtrees and perform search over all possible trees to find the most probable tree for a text. Recent graph-based methods use neural models as scoring functions (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017). Despite being more accurate than greedy parsers, these methods are generally slow having a polynomial time complexity (O(n3 ) or higher). Recently, transition-based top-down parsing with Pointer Networks (Vinyals et al., 2015) has attained state-of-the-art results in both dependency and discourse parsing tasks with the same computational efficiency (Ma et al., 2018; Lin et al., 2019); thanks to the encoder-decoder architecture that makes it possible to capture information from the whole text and the previously derived subtrees, 1007 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1007–1017, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics while limiting the number of parsing steps to linear. However, the decoder of these parsers has a sequential structure, which may not yield the most appropriate inductive bias for d"
D19-1093,N15-1142,0,0.0337556,"±0.02 ro 91.15±0.12 85.54±0.13 91.34±0.18 85.73±0.22 91.09±0.09 85.36±0.10 Table 1: Dependency parsing results on 7 UD Treebanks. StackPtr (code) denotes the experiments we rerun on our machine. H-PtrNet-PST (Gate) and H-PtrNet-PST (SGate) are H-PtrNet models with gating mechanism. we follow the standard split for training, validation and testing. It should be noted that Ma et al. (2018) used UD Treebanks 2.1, which is not the most up-to-date version. Therefore, during experiments, we rerun their codes with UD Treebanks 2.3 to match our experiments. To be specific, we use structured-skipgram (Ling et al., 2015) for English and German, while Polyglot embedding (AlRfou et al., 2013) for the other languages. Adam optimizer (Kingma and Ba, 2015) is used as the optimization algorithm. We apply 0.33 dropout rate between layers of encoder and to word embeddings as well as Eq. 8. We use beam size of 10 for English Penn Treebank, and beam size of 1 for UD Treebanks. The gold-standard POS tags is used for English Penn Treebank. We also use the universal POS tags (Petrov et al., 2011) provided in the dataset for UD Treebanks. See Appendix for a complete list of hyperparameters. Results on UD Treebanks. We eval"
D19-1093,P18-1130,0,0.0660044,"learn scoring functions for subtrees and perform search over all possible trees to find the most probable tree for a text. Recent graph-based methods use neural models as scoring functions (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017). Despite being more accurate than greedy parsers, these methods are generally slow having a polynomial time complexity (O(n3 ) or higher). Recently, transition-based top-down parsing with Pointer Networks (Vinyals et al., 2015) has attained state-of-the-art results in both dependency and discourse parsing tasks with the same computational efficiency (Ma et al., 2018; Lin et al., 2019); thanks to the encoder-decoder architecture that makes it possible to capture information from the whole text and the previously derived subtrees, 1007 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1007–1017, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics while limiting the number of parsing steps to linear. However, the decoder of these parsers has a sequential structure, which may not yield the most appropriate i"
D19-1093,J00-3005,0,0.459713,"Missing"
D19-1093,J18-2001,0,0.0661953,"d hierarchical pointer networks, we first revisit how pointer networks have been used for parsing tasks. 3.1 3.2 where σ(., .) is a scoring function for attention, which can be a neural network or an explicit formula like dot product. The model uses at to infer the output: ˆ yt = arg max(at ) = arg max p(yt |y&lt;t , X, θ) where θ is the set of parameters. To condition on yt−1 , the corresponding input xyt−1 is copied as the input to the decoder. 1 There are also studies that use dependency structure to directly represent the relations between the EDUs; see (Muller et al., 2012; Li et al., 2014; Morey et al., 2018). Hierarchical Pointer Networks Pointer Networks for Parsing. Limitations of Existing Methods One crucial limitation of the existing models is that the decoder has a linear structure, although the task is to construct a hierarchical structure. This can be noticed in the Figures 2 and 3, where the current decoder state dt is conditioned on the previous state dt−1 (see horizontal blue lines), but not on its parent’s decoder state or siblings’ decoder state, when it was pointed from its head. This can induce irrelevant information if the previous decoding state corresponds to an element that is n"
D19-1093,C12-1115,0,0.0211337,"st ) (1) Before presenting our proposed hierarchical pointer networks, we first revisit how pointer networks have been used for parsing tasks. 3.1 3.2 where σ(., .) is a scoring function for attention, which can be a neural network or an explicit formula like dot product. The model uses at to infer the output: ˆ yt = arg max(at ) = arg max p(yt |y&lt;t , X, θ) where θ is the set of parameters. To condition on yt−1 , the corresponding input xyt−1 is copied as the input to the decoder. 1 There are also studies that use dependency structure to directly represent the relations between the EDUs; see (Muller et al., 2012; Li et al., 2014; Morey et al., 2018). Hierarchical Pointer Networks Pointer Networks for Parsing. Limitations of Existing Methods One crucial limitation of the existing models is that the decoder has a linear structure, although the task is to construct a hierarchical structure. This can be noticed in the Figures 2 and 3, where the current decoder state dt is conditioned on the previous state dt−1 (see horizontal blue lines), but not on its parent’s decoder state or siblings’ decoder state, when it was pointed from its head. This can induce irrelevant information if the previous decoding sta"
D19-1093,K18-2008,0,0.017857,"o two main categories: greedy transitionbased parsing and graph-based parsing. In both paradigms, neural models have proven to be more effective than feature-based models where selecting the composition of features is a major challenge. Kiperwasser and Goldberg (2016) proposed graph-based and transition-based dependency parsers with a Bi-LSTM feature representation. Since then much work has been done to improve these two parsers. Dozat and Manning (2017) proposed a bi-affine classifier for the prediction of arcs and labels based on graph-based model, and achieved state-of-the-art performance. Nguyen and Verspoor (2018) adopted a joint modeling approach by adding a Bi-LSTM POS tagger to generate POS tags for the graph-based dependency parser. Though transition-based methods are superior in terms of time complexity, they fail in capturing the global dependency information when making decisions. To address this issue, Andor et al. (2016) proposed a globally optimized transition-based model. Recently, by incorporating a stack within a pointer network, Ma et al. (2018) proposed a transition-based model and achieved state-of-the-art performance across many languages . 2.2 Discourse Parsing Rhetorical Structure Th"
D19-1093,N18-1202,0,0.083523,"Missing"
D19-1093,L16-1376,0,0.0303939,"model with fusion function dt = f (dp(t) , ds(t) , hp(t) ), where the decoder receives the hidden states from both the parent and sibling in each decoding step. • H-PtrNet-PST: This is the full model with fusion function dt = f (dp(t) , ds(t) , dt−1 , hp(t) ) (Eq. 2). In this model, the decoder receives the hidden states from its parent, sibling and previous step in each decoding step. 4.1 Dependency Parsing Dataset. We evaluate our model on the English Penn Treebank (PTB v3.0) (Marcus et al., 1994), which is converted to Stanford Dependencies format with Stanford Dependency Converter 3.3.0 (Schuster and Manning, 2016). To make a thorough empirical comparison with previous studies, we also evaluate our system on seven (7) languages from the Universal Dependency (UD) Treebanks5 (version 2.3). Metrics. We evaluate the performance of our models with unlabeled attachment score (UAS) and labeled attachment score (LAS). We ignore punctuations in the evaluation for English. Experimental Settings. We use the same setup as Ma et al. (2018) in the experiments for English Penn Treebank and UD Treebanks. For a fair comparison, we rerun their model with the hyperparameters provided by the authors on the same machine as"
D19-1093,N03-1030,0,0.435235,"vel discourse tree that respectively represent how the words and clauses are related in a sentence. Such parse trees are directly useful in numerous NLP applications, and also serve as intermediate representations for further language processing tasks such as semantic and discourse processing. Existing approaches to parsing can be distinguished based on whether they employ a greedy transition-based algorithm (Marcu, 1999; Zhang and Nivre, 2011; Wang et al., 2017) or a globally optimized algorithm such as graph-based methods for dependency parsing (Eisner, 1996) or chart parsing for discourse (Soricut and Marcu, 2003; Joty et al., 2015). Transition-based parsers build the tree incrementally by making a series ∗ Linlin Liu is under the Joint PhD Program between Alibaba and Nanyang Technological University. † Equal contribution. of shift-reduce decisions. The advantage of this method is that the parsing time is linear with respect to the sequence length. The limitation, however, is that the decisions made at each step are based on local information, disallowing the model to capture long distance dependencies and also causing error propagation to subsequent steps. Recent methods attempt to address this issue"
D19-1093,P17-2029,0,0.255334,"bes the relationships between the tree constituents (e.g., words, phrases). For example, Figure 1 shows examples of a dependency tree and a sentence-level discourse tree that respectively represent how the words and clauses are related in a sentence. Such parse trees are directly useful in numerous NLP applications, and also serve as intermediate representations for further language processing tasks such as semantic and discourse processing. Existing approaches to parsing can be distinguished based on whether they employ a greedy transition-based algorithm (Marcu, 1999; Zhang and Nivre, 2011; Wang et al., 2017) or a globally optimized algorithm such as graph-based methods for dependency parsing (Eisner, 1996) or chart parsing for discourse (Soricut and Marcu, 2003; Joty et al., 2015). Transition-based parsers build the tree incrementally by making a series ∗ Linlin Liu is under the Joint PhD Program between Alibaba and Nanyang Technological University. † Equal contribution. of shift-reduce decisions. The advantage of this method is that the parsing time is linear with respect to the sequence length. The limitation, however, is that the decisions made at each step are based on local information, disa"
D19-1093,P11-2033,0,0.0382745,"ucture that best describes the relationships between the tree constituents (e.g., words, phrases). For example, Figure 1 shows examples of a dependency tree and a sentence-level discourse tree that respectively represent how the words and clauses are related in a sentence. Such parse trees are directly useful in numerous NLP applications, and also serve as intermediate representations for further language processing tasks such as semantic and discourse processing. Existing approaches to parsing can be distinguished based on whether they employ a greedy transition-based algorithm (Marcu, 1999; Zhang and Nivre, 2011; Wang et al., 2017) or a globally optimized algorithm such as graph-based methods for dependency parsing (Eisner, 1996) or chart parsing for discourse (Soricut and Marcu, 2003; Joty et al., 2015). Transition-based parsers build the tree incrementally by making a series ∗ Linlin Liu is under the Joint PhD Program between Alibaba and Nanyang Technological University. † Equal contribution. of shift-reduce decisions. The advantage of this method is that the parsing time is linear with respect to the sequence length. The limitation, however, is that the decisions made at each step are based on loc"
D19-1199,I17-2028,0,0.0211377,"ing for utterances, another key issue is to model multiple participants in the conversations. It is intuitive to introduce multiple user embeddings for multi-party conversations, either as persona-dependent embeddings (Li et al., 2016b), or as persona-independent embeddings (Ouchi and Tsuboi, 2016; Zhang et al., 2017; Meng et al., 2017). Recently, some researchers utilized users’ 1 To make the model practical in learning, we assume that one utterance is associated with only one addressee. information based on different roles in conversations, such as senders and recipients (Chen et al., 2017; Chi et al., 2017; Luan et al., 2016). In multi-party conversations, identifying the relationship among users is also an important task. It can be categorized into two topics, 1) predicting who will be the next speaker (Meng et al., 2017) and 2) who is the addressee (Ouchi and Tsuboi, 2016; Zhang et al., 2017). For the first topic, Meng et al. (2017) investigated a temporal-based and a content-based method to jointly model the users and context. For the second topic, which is closely related to ours, Ouchi and Tsuboi (2016) proposed to predict the addressee and utterance given a context with all available info"
D19-1199,D14-1179,0,0.00765946,"Missing"
D19-1199,C16-1073,1,0.885519,"Missing"
D19-1199,D17-1230,0,0.0155911,"oach to jointly learn the representations of users and utterances and enhance them mutually. • The proposed approach (W2W) considers both previous and subsequent information in the session while incorporating the correlation with users and utterances. For conversations with complex structures, W2W models them in a uniform way and could handle any kind of occasion even when all the addressee information is missing. 2 Related Work In this section, we briefly review recent works and progresses on multi-party conversations. Multi-party conversations, as a general case of multi-turn conversations (Li et al., 2017, 2016c; Yan et al., 2016; Serban et al., 2016) involve more than two participants. In addition to the representation of learning for utterances, another key issue is to model multiple participants in the conversations. It is intuitive to introduce multiple user embeddings for multi-party conversations, either as persona-dependent embeddings (Li et al., 2016b), or as persona-independent embeddings (Ouchi and Tsuboi, 2016; Zhang et al., 2017; Meng et al., 2017). Recently, some researchers utilized users’ 1 To make the model practical in learning, we assume that one utterance is associated with"
D19-1199,D16-1231,0,0.148673,"2 Related Work In this section, we briefly review recent works and progresses on multi-party conversations. Multi-party conversations, as a general case of multi-turn conversations (Li et al., 2017, 2016c; Yan et al., 2016; Serban et al., 2016) involve more than two participants. In addition to the representation of learning for utterances, another key issue is to model multiple participants in the conversations. It is intuitive to introduce multiple user embeddings for multi-party conversations, either as persona-dependent embeddings (Li et al., 2016b), or as persona-independent embeddings (Ouchi and Tsuboi, 2016; Zhang et al., 2017; Meng et al., 2017). Recently, some researchers utilized users’ 1 To make the model practical in learning, we assume that one utterance is associated with only one addressee. information based on different roles in conversations, such as senders and recipients (Chen et al., 2017; Chi et al., 2017; Luan et al., 2016). In multi-party conversations, identifying the relationship among users is also an important task. It can be categorized into two topics, 1) predicting who will be the next speaker (Meng et al., 2017) and 2) who is the addressee (Ouchi and Tsuboi, 2016; Zhang e"
D19-1199,D14-1162,0,0.0858018,"descending order according to the first time when they speak, and the i-th user is assigned with the i-th row of A(0) as ai(0) . The user matrix A(0) is trained as parameters along with other weight matrices in the neural network. Users of the same order in different sessions share the same initialization user embedding. Note that the user representations are independent of each personality (unique user). Such strategy guarantees the initialization user embeddings to carry position information as well as handle new users unseen in training data during addressee identification. 3 We use GloVe(Pennington et al., 2014), but it can be any word embeddings(Mikolov et al., 2013; Hu et al., 2016). 4 Such a hierarchical framework (Serban et al., 2016) takes into account the context of all previous and future sentences in the whole session, thus enables the model to learn a strong representation. 1911 ???? ????????????? Forward a?(?) a?(?) a?(?) (?1 , a???? ) 0 a?(?) a?(?) ? ????2 ) 0 a?(?) a?(?) ? ????3 ) 0 Utterance ? ? ????1 ) 2 a? a?(?) a? ????2 ) 2 (?32 , a a?(?) ????3 ) 1 (?23 , a Backward a? (?31 , a ? (?2 , a???? ) 1 a?(?) (?13 , a User ? ???? (?21 , a 1 1 ) a?(?) (?12 , a ?(0) a?(?) a? a? (?3 , a???? ) 2"
D19-1199,N16-1014,0,0.230717,"hich models users and utterances in the session jointly in an interactive way. We conduct experiments on the benchmark Ubuntu Multi-Party Conversation Corpus and the experimental results demonstrate that our model outperforms baselines with consistent improvements. 1 Speaker User 1 User 1 User 2 User 3 User 4 Introduction As an essential aspect of artificial intelligence, dialogue systems have attracted extensive attention in recent studies (Vinyals and Le, 2015; Serban et al., 2016). Researchers have paid great efforts to understand conversations between two participants, either single-turn (Li et al., 2016a; Shang et al., 2015; Vinyals and Le, 2015) or multi-turn (Zhou et al., 2016; Yan et al., 2016; Tao et al., 2019a,b), and achieved encouraging results. A more general and challenging scenario is that a conversation may involve more than two interlocutors conversing among each other (Uthus and Aha, 2013; Hu et al., 2019), which is known as multi-party conversation. Ubuntu Internet Relay Chat channel (IRC) is a multi-party conversation scenario as shown in Table 1. Generally, each utterance is associated with a speaker and one or more addressees in the conversation. Such a characteristic ∗ † Eq"
D19-1199,P15-1152,0,0.0970801,"Missing"
D19-1199,P16-1094,0,0.136522,"hich models users and utterances in the session jointly in an interactive way. We conduct experiments on the benchmark Ubuntu Multi-Party Conversation Corpus and the experimental results demonstrate that our model outperforms baselines with consistent improvements. 1 Speaker User 1 User 1 User 2 User 3 User 4 Introduction As an essential aspect of artificial intelligence, dialogue systems have attracted extensive attention in recent studies (Vinyals and Le, 2015; Serban et al., 2016). Researchers have paid great efforts to understand conversations between two participants, either single-turn (Li et al., 2016a; Shang et al., 2015; Vinyals and Le, 2015) or multi-turn (Zhou et al., 2016; Yan et al., 2016; Tao et al., 2019a,b), and achieved encouraging results. A more general and challenging scenario is that a conversation may involve more than two interlocutors conversing among each other (Uthus and Aha, 2013; Hu et al., 2019), which is known as multi-party conversation. Ubuntu Internet Relay Chat channel (IRC) is a multi-party conversation scenario as shown in Table 1. Generally, each utterance is associated with a speaker and one or more addressees in the conversation. Such a characteristic ∗ † Eq"
D19-1199,D19-1011,0,0.257771,"chmark Ubuntu Multi-Party Conversation Corpus and the experimental results demonstrate that our model outperforms baselines with consistent improvements. 1 Speaker User 1 User 1 User 2 User 3 User 4 Introduction As an essential aspect of artificial intelligence, dialogue systems have attracted extensive attention in recent studies (Vinyals and Le, 2015; Serban et al., 2016). Researchers have paid great efforts to understand conversations between two participants, either single-turn (Li et al., 2016a; Shang et al., 2015; Vinyals and Le, 2015) or multi-turn (Zhou et al., 2016; Yan et al., 2016; Tao et al., 2019a,b), and achieved encouraging results. A more general and challenging scenario is that a conversation may involve more than two interlocutors conversing among each other (Uthus and Aha, 2013; Hu et al., 2019), which is known as multi-party conversation. Ubuntu Internet Relay Chat channel (IRC) is a multi-party conversation scenario as shown in Table 1. Generally, each utterance is associated with a speaker and one or more addressees in the conversation. Such a characteristic ∗ † Equal contribution. Corresponding author. Utterance ”Good point, tmux is the thing I miss.” ”Cool thanks for ur hel"
D19-1199,D16-1127,0,0.179163,"hich models users and utterances in the session jointly in an interactive way. We conduct experiments on the benchmark Ubuntu Multi-Party Conversation Corpus and the experimental results demonstrate that our model outperforms baselines with consistent improvements. 1 Speaker User 1 User 1 User 2 User 3 User 4 Introduction As an essential aspect of artificial intelligence, dialogue systems have attracted extensive attention in recent studies (Vinyals and Le, 2015; Serban et al., 2016). Researchers have paid great efforts to understand conversations between two participants, either single-turn (Li et al., 2016a; Shang et al., 2015; Vinyals and Le, 2015) or multi-turn (Zhou et al., 2016; Yan et al., 2016; Tao et al., 2019a,b), and achieved encouraging results. A more general and challenging scenario is that a conversation may involve more than two interlocutors conversing among each other (Uthus and Aha, 2013; Hu et al., 2019), which is known as multi-party conversation. Ubuntu Internet Relay Chat channel (IRC) is a multi-party conversation scenario as shown in Table 1. Generally, each utterance is associated with a speaker and one or more addressees in the conversation. Such a characteristic ∗ † Eq"
D19-1199,D16-1036,1,0.896926,"ay. We conduct experiments on the benchmark Ubuntu Multi-Party Conversation Corpus and the experimental results demonstrate that our model outperforms baselines with consistent improvements. 1 Speaker User 1 User 1 User 2 User 3 User 4 Introduction As an essential aspect of artificial intelligence, dialogue systems have attracted extensive attention in recent studies (Vinyals and Le, 2015; Serban et al., 2016). Researchers have paid great efforts to understand conversations between two participants, either single-turn (Li et al., 2016a; Shang et al., 2015; Vinyals and Le, 2015) or multi-turn (Zhou et al., 2016; Yan et al., 2016; Tao et al., 2019a,b), and achieved encouraging results. A more general and challenging scenario is that a conversation may involve more than two interlocutors conversing among each other (Uthus and Aha, 2013; Hu et al., 2019), which is known as multi-party conversation. Ubuntu Internet Relay Chat channel (IRC) is a multi-party conversation scenario as shown in Table 1. Generally, each utterance is associated with a speaker and one or more addressees in the conversation. Such a characteristic ∗ † Equal contribution. Corresponding author. Utterance ”Good point, tmux is the th"
D19-1317,W14-3348,0,0.0157719,".28 13.98 15.32 16.20 16.27 MET 16.62 18.77 19.29 19.92 20.36 Zhou Split (Zhou et al., 2017) R-L B1 B2 B3 39.75 42.72 43.91 44.51 29.07 21.06 - 43.02 28.14 20.51 43.96 44.35 44.40 29.48 21.54 B4 MET R-L 13.29 13.91 15.82 19.67 44.24 15.64 16.17 16.37 20.68 44.73 Table 4: The main experimental results for our model and several baselines. ‘-’ means no results reported in their papers. (Bn: BLEU-n, MET: METEOR, R-L: ROUGE-L) We evaluate with all commonly-used metrics in question generation (Du et al., 2017): BLEU1 (B1), BLEU-2 (B2), BLEU-3 (B3), BLEU-4 (B4) (Papineni et al., 2002), METEOR (MET) (Denkowski and Lavie, 2014) and ROUGE-L (RL) (Lin, 2004). We use the evaluation script released by Chen et al. (2015). 3.2 Baseline Models We compare with the following models. • s2s (Du et al., 2017) proposes an attention-based sequence-to-sequence neural network for question generation. • NQG++ (Zhou et al., 2017) takes the answer position feature and linguistic features into consideration and equips the Seq2Seq model with copy mechanism. • M2S+cp (Song et al., 2018) conducts multiperspective matching between the answer and the sentence to derive an answer-aware sentence representation for question generation. • s2s+M"
D19-1317,P17-1123,0,0.758494,"onstruction. In this paper, we focus on question generation from reading comprehension materials like SQuAD (Rajpurkar et al., 2016). As shown in Figure 1, given a sentence in the reading comprehension paragraph and the text fragment (i.e., the answer) that we want to ask about, we aim to generate a question that is asked about the specified answer. Question generation for reading comprehension is firstly formalized as a declarative-tointerrogative sentence transformation problem with predefined rules or templates (Mitkov and Ha, 2003; Heilman and Smith, 2010). With the rise of neural models, Du et al. (2017) propose to model this task under the sequence-to-sequence (Seq2Seq) learning framework (Sutskever et al., 2014) with attention mechanism (Luong et al., 2015). However, question generation is a oneto-many sequence generation problem, i.e., several aspects can be asked given a sentence. Zhou et al. (2017) propose the answer-aware question generation setting which assumes the answer, a contiguous span inside the input sentence, is al3216 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing"
D19-1317,P19-1480,1,0.791263,"and the original sentence. Factoid question generation from structured text is initially investigated by Serban et al. (2016), but our focus here is leveraging structured inputs to help question generation over unstructured sentences. Our proposed model can take advantage of unstructured sentences and structured answerrelevant relations to maintain informativeness and faithfulness of generated questions. The proposed model can also be generalized in other conditional sequence generation tasks which require multiple sources of inputs, e.g., distractor generation for multiple choice questions (Gao et al., 2019b). 6 Conclusions and Future Work In this paper, we propose a question generation system which combines unstructured sentences and structured answer-relevant relations for generation. The unstructured sentences maintain the informativeness of generated questions while structured answer-relevant relations keep the faithfulness of questions. Extensive experiments demonstrate that our proposed model achieves state-ofthe-art performance across several metrics. Furthermore, our model can generate diverse questions with different structured answer-relevant relations. For future work, there are some"
D19-1317,P02-1040,0,0.105716,"019) Our model 45.66 30.21 21.82 B4 12.28 13.98 15.32 16.20 16.27 MET 16.62 18.77 19.29 19.92 20.36 Zhou Split (Zhou et al., 2017) R-L B1 B2 B3 39.75 42.72 43.91 44.51 29.07 21.06 - 43.02 28.14 20.51 43.96 44.35 44.40 29.48 21.54 B4 MET R-L 13.29 13.91 15.82 19.67 44.24 15.64 16.17 16.37 20.68 44.73 Table 4: The main experimental results for our model and several baselines. ‘-’ means no results reported in their papers. (Bn: BLEU-n, MET: METEOR, R-L: ROUGE-L) We evaluate with all commonly-used metrics in question generation (Du et al., 2017): BLEU1 (B1), BLEU-2 (B2), BLEU-3 (B3), BLEU-4 (B4) (Papineni et al., 2002), METEOR (MET) (Denkowski and Lavie, 2014) and ROUGE-L (RL) (Lin, 2004). We use the evaluation script released by Chen et al. (2015). 3.2 Baseline Models We compare with the following models. • s2s (Du et al., 2017) proposes an attention-based sequence-to-sequence neural network for question generation. • NQG++ (Zhou et al., 2017) takes the answer position feature and linguistic features into consideration and equips the Seq2Seq model with copy mechanism. • M2S+cp (Song et al., 2018) conducts multiperspective matching between the answer and the sentence to derive an answer-aware sentence repre"
D19-1317,P16-1154,0,0.0342353,"fferent from the standard pointing method, we design a dual copy mechanism to copy from two sources with two gates. The first gate is designed for determining copy tokens from two sources of inputs or generate next word from PV , which is ˜ t + bv ). The seccomputed as gtv = sigmoid(wgv h g ond gate takes charge of selecting the source (sentence or relation) to copy from, which is computed c as gtc = sigmoid(wgc [cst ; cm t ] + bg ). Finally, we (9) Experimental Setting 3.1 Dual Copy Mechanism. To deal with the rare and unknown words, the decoder applies the pointing method (See et al., 2017; Gu et al., 2016; Gulcehre et al., 2016) to allow copying a token from the input sentence at the t-th decoding step. We reuse the attention score αts and αtm to derive the copy probability over two source inputs: X X s m PS (w) = αt,i , PM (w) = αt,i . gtc )PM (w). Training and Inference. Given the answer A, sentence S and relation M , the training objective is to minimize the negative log-likelihood with regard to all parameters: X L=− log P(Q|A, S, M ; θ), (8) Q where WV and bV are parameters. i:wi =w Table 3: Dataset statistics on Du Split (Du et al., 2017) and Zhou Split (Zhou et al., 2017). (4) (5) ˜ t +"
D19-1317,D14-1162,0,0.0842024,"e encoder and a maxout pointer mechanism into the decoder. We report their sentence-level results for a fair comparison. • Hybrid (Sun et al., 2018) is a hybrid model which considers the answer embedding for the question word generation and the position of context words for modeling the relative distance between the context words and the answer. • ASs2s (Kim et al., 2019) replaces the answer in the sentence with a special token to avoid its appearance in the generated questions. 3.3 Implementation Details We take the most frequent 20k words as our vocabulary and use the GloVe word embeddings (Pennington et al., 2014) for initialization. The embedding dimensions for POS, NER, answer position are set to 20. We use two-layer LSTMs in both encoder and decoder, and the LSTMs hidden unit size is set to 600. We use dropout (Srivastava et al., 2014) with the probability p = 0.3. All trainable parameters, except word embeddings, are randomly initialized with the Xavier uniform in (−0.1, 0.1) (Glorot and Bengio, 2010). For optimization in the training, we use SGD as the optimizer with a minibatch size of 64 and an initial learning rate of 1.0. We train the model for 15 epochs and start halving the learning rate aft"
D19-1317,P16-1014,0,0.0276998,"standard pointing method, we design a dual copy mechanism to copy from two sources with two gates. The first gate is designed for determining copy tokens from two sources of inputs or generate next word from PV , which is ˜ t + bv ). The seccomputed as gtv = sigmoid(wgv h g ond gate takes charge of selecting the source (sentence or relation) to copy from, which is computed c as gtc = sigmoid(wgc [cst ; cm t ] + bg ). Finally, we (9) Experimental Setting 3.1 Dual Copy Mechanism. To deal with the rare and unknown words, the decoder applies the pointing method (See et al., 2017; Gu et al., 2016; Gulcehre et al., 2016) to allow copying a token from the input sentence at the t-th decoding step. We reuse the attention score αts and αtm to derive the copy probability over two source inputs: X X s m PS (w) = αt,i , PM (w) = αt,i . gtc )PM (w). Training and Inference. Given the answer A, sentence S and relation M , the training objective is to minimize the negative log-likelihood with regard to all parameters: X L=− log P(Q|A, S, M ; θ), (8) Q where WV and bV are parameters. i:wi =w Table 3: Dataset statistics on Du Split (Du et al., 2017) and Zhou Split (Zhou et al., 2017). (4) (5) ˜ t + bV ), PV = softmax(WV h"
D19-1317,N10-1086,0,0.858158,"ructured sentence provides the full information. Extensive experiments show that to the point context helps our question generation model achieve significant improvements on several automatic evaluation metrics. Furthermore, our model is capable of generating diverse questions for a sentence which conveys multiple relations of its answer fragment. 1 Figure 1: An example SQuAD question with the baseline’s prediction. The answer (“0.3 ◦ C”) is highlighted. Introduction Question Generation (QG) is the task of automatically creating questions from a range of inputs, such as natural language text (Heilman and Smith, 2010), knowledge base (Serban et al., 2016) and ∗ These two authors contributed equally. image (Mostafazadeh et al., 2016). QG is an increasingly important area in NLP with various application scenarios such as intelligence tutor systems, open-domain chatbots and question answering dataset construction. In this paper, we focus on question generation from reading comprehension materials like SQuAD (Rajpurkar et al., 2016). As shown in Figure 1, given a sentence in the reading comprehension paragraph and the text fragment (i.e., the answer) that we want to ask about, we aim to generate a question tha"
D19-1317,W04-1013,0,0.0173695,"9.29 19.92 20.36 Zhou Split (Zhou et al., 2017) R-L B1 B2 B3 39.75 42.72 43.91 44.51 29.07 21.06 - 43.02 28.14 20.51 43.96 44.35 44.40 29.48 21.54 B4 MET R-L 13.29 13.91 15.82 19.67 44.24 15.64 16.17 16.37 20.68 44.73 Table 4: The main experimental results for our model and several baselines. ‘-’ means no results reported in their papers. (Bn: BLEU-n, MET: METEOR, R-L: ROUGE-L) We evaluate with all commonly-used metrics in question generation (Du et al., 2017): BLEU1 (B1), BLEU-2 (B2), BLEU-3 (B3), BLEU-4 (B4) (Papineni et al., 2002), METEOR (MET) (Denkowski and Lavie, 2014) and ROUGE-L (RL) (Lin, 2004). We use the evaluation script released by Chen et al. (2015). 3.2 Baseline Models We compare with the following models. • s2s (Du et al., 2017) proposes an attention-based sequence-to-sequence neural network for question generation. • NQG++ (Zhou et al., 2017) takes the answer position feature and linguistic features into consideration and equips the Seq2Seq model with copy mechanism. • M2S+cp (Song et al., 2018) conducts multiperspective matching between the answer and the sentence to derive an answer-aware sentence representation for question generation. • s2s+MP+GSA (Zhao et al., 2018) int"
D19-1317,D15-1166,0,0.0297493,"representam m tion (hm 1 , h2 , ..., hn ). Decoder. We use an LSTM as the decoder to generate the question. The decoder predicts the word probability distribution at each decoding timestep to generate the question. At the t-th timestep, it reads the word embedding wt and the hidden state ut−1 of the previous timestep to gen3219 erate the current hidden state: ut = LSTM(ut−1 , wt ). (2) Gated Attention Mechanism. We design a gated attention mechanism to jointly attend the sentence representation and the relation representation. For sentence representation (hs1 , hs2 , ..., hsn ), we employ the Luong et al. (2015)’s attention mechanism to obtain the sentence context vector cst , s X exp(u> t Wa hi ) s ast,i = P , c = ast,i hsi , t > W hs ) exp(u a t j j i cst ct = gt + (1 − gt ) ˜ t = tanh(Wh [ut ; ct ]), h cm t , (3) where represents element-wise dot production and Wg , Wh are trainable weights. Finally, the predicted probability distribution over the vocabulary V is computed as: combine all probabilities PV , PS and PM through two soft gates gtv and gtc . The probability of predicting w as the t-th token of the question is: + gtv gtc PS (w) + gtv (1 (7) − (6) Q∈{Q} where {Q} is the set of all trainin"
D19-1317,W03-0203,0,0.253632,"telligence tutor systems, open-domain chatbots and question answering dataset construction. In this paper, we focus on question generation from reading comprehension materials like SQuAD (Rajpurkar et al., 2016). As shown in Figure 1, given a sentence in the reading comprehension paragraph and the text fragment (i.e., the answer) that we want to ask about, we aim to generate a question that is asked about the specified answer. Question generation for reading comprehension is firstly formalized as a declarative-tointerrogative sentence transformation problem with predefined rules or templates (Mitkov and Ha, 2003; Heilman and Smith, 2010). With the rise of neural models, Du et al. (2017) propose to model this task under the sequence-to-sequence (Seq2Seq) learning framework (Sutskever et al., 2014) with attention mechanism (Luong et al., 2015). However, question generation is a oneto-many sequence generation problem, i.e., several aspects can be asked given a sentence. Zhou et al. (2017) propose the answer-aware question generation setting which assumes the answer, a contiguous span inside the input sentence, is al3216 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processi"
D19-1317,P16-1170,0,0.312826,"stion generation model achieve significant improvements on several automatic evaluation metrics. Furthermore, our model is capable of generating diverse questions for a sentence which conveys multiple relations of its answer fragment. 1 Figure 1: An example SQuAD question with the baseline’s prediction. The answer (“0.3 ◦ C”) is highlighted. Introduction Question Generation (QG) is the task of automatically creating questions from a range of inputs, such as natural language text (Heilman and Smith, 2010), knowledge base (Serban et al., 2016) and ∗ These two authors contributed equally. image (Mostafazadeh et al., 2016). QG is an increasingly important area in NLP with various application scenarios such as intelligence tutor systems, open-domain chatbots and question answering dataset construction. In this paper, we focus on question generation from reading comprehension materials like SQuAD (Rajpurkar et al., 2016). As shown in Figure 1, given a sentence in the reading comprehension paragraph and the text fragment (i.e., the answer) that we want to ask about, we aim to generate a question that is asked about the specified answer. Question generation for reading comprehension is firstly formalized as a decla"
D19-1317,W10-4234,0,0.0392845,"d; in December 2007) Question 1: What game premiered in December 2007 ? Relation 2: (One of the network’s strike-replacement programs during that time; was; the game show Duel) Question 2: What was the name of an network ’s strike replacement programs ? One might think that the two subsequences should be regarded as individual sentences, however, several off-the-shelf tools do recognize them as one sentence. 5 Related Work The topic of question generation, initially motivated for educational purposes, is tackled by designing many complex rules for specific question types (Mitkov and Ha, 2003; Rus et al., 2010). Heilman and Smith (2010) improve rule-based question generation by introducing a statistical ranking model. First, they remove extraneous information in the sentence to transform it into a simpler one, which can be transformed easily into a succinct question with predefined sets of general rules. Then they adopt an overgenerate-and-rank approach to select the best candidate considering several features. With the rise of dominant neural sequence-tosequence learning models (Sutskever et al., 2014), Du et al. (2017) frame question generation as a sequence-to-sequence learning problem. Compared"
D19-1317,C18-1194,0,0.0355952,"Missing"
D19-1317,P17-1099,0,0.0382107,"; θ). 3 i:wi =w Different from the standard pointing method, we design a dual copy mechanism to copy from two sources with two gates. The first gate is designed for determining copy tokens from two sources of inputs or generate next word from PV , which is ˜ t + bv ). The seccomputed as gtv = sigmoid(wgv h g ond gate takes charge of selecting the source (sentence or relation) to copy from, which is computed c as gtc = sigmoid(wgc [cst ; cm t ] + bg ). Finally, we (9) Experimental Setting 3.1 Dual Copy Mechanism. To deal with the rare and unknown words, the decoder applies the pointing method (See et al., 2017; Gu et al., 2016; Gulcehre et al., 2016) to allow copying a token from the input sentence at the t-th decoding step. We reuse the attention score αts and αtm to derive the copy probability over two source inputs: X X s m PS (w) = αt,i , PM (w) = αt,i . gtc )PM (w). Training and Inference. Given the answer A, sentence S and relation M , the training objective is to minimize the negative log-likelihood with regard to all parameters: X L=− log P(Q|A, S, M ; θ), (8) Q where WV and bV are parameters. i:wi =w Table 3: Dataset statistics on Du Split (Du et al., 2017) and Zhou Split (Zhou et al., 201"
D19-1317,P16-1056,0,0.299652,"Missing"
D19-1317,N18-2090,0,0.114284,"l., 2017) can generate more fluent and grammatical questions. However, question generation is a one-to-many sequence generation problem, i.e., several aspects can be asked given a sentence, which confuses the model during train and prevents concrete automatic evaluation. To tackle this issue, Zhou et al. (2017) propose the answer-aware question generation setting which assumes the answer is already known 3223 and acts as a contiguous span inside the input sentence. They adopt a BIO tagging scheme to incorporate the answer position information as learned embedding features in Seq2Seq learning. Song et al. (2018) explicitly model the information between answer and sentence with a multiperspective matching model. Kim et al. (2019) also focus on the answer information and proposed an answer-separated Seq2Seq model by masking the answer with special tokens. All answer-aware neural models treat question generation as a oneto-one mapping problem, but existing models perform poorly for sentences with a complex structure (as shown in Table 1). Our work is inspired by the process of extraneous information removing in (Heilman and Smith, 2010; Cao et al., 2018). Different from Heilman and Smith (2010) which di"
D19-1317,D18-1427,0,0.433617,"sed sequence-to-sequence neural network for question generation. • NQG++ (Zhou et al., 2017) takes the answer position feature and linguistic features into consideration and equips the Seq2Seq model with copy mechanism. • M2S+cp (Song et al., 2018) conducts multiperspective matching between the answer and the sentence to derive an answer-aware sentence representation for question generation. • s2s+MP+GSA (Zhao et al., 2018) introduces a gated self-attention into the encoder and a maxout pointer mechanism into the decoder. We report their sentence-level results for a fair comparison. • Hybrid (Sun et al., 2018) is a hybrid model which considers the answer embedding for the question word generation and the position of context words for modeling the relative distance between the context words and the answer. • ASs2s (Kim et al., 2019) replaces the answer in the sentence with a special token to avoid its appearance in the generated questions. 3.3 Implementation Details We take the most frequent 20k words as our vocabulary and use the GloVe word embeddings (Pennington et al., 2014) for initialization. The embedding dimensions for POS, NER, answer position are set to 20. We use two-layer LSTMs in both en"
D19-1317,W17-2603,0,0.0187549,"ed question generation by introducing a statistical ranking model. First, they remove extraneous information in the sentence to transform it into a simpler one, which can be transformed easily into a succinct question with predefined sets of general rules. Then they adopt an overgenerate-and-rank approach to select the best candidate considering several features. With the rise of dominant neural sequence-tosequence learning models (Sutskever et al., 2014), Du et al. (2017) frame question generation as a sequence-to-sequence learning problem. Compared with rule-based approaches, neural models (Yuan et al., 2017) can generate more fluent and grammatical questions. However, question generation is a one-to-many sequence generation problem, i.e., several aspects can be asked given a sentence, which confuses the model during train and prevents concrete automatic evaluation. To tackle this issue, Zhou et al. (2017) propose the answer-aware question generation setting which assumes the answer is already known 3223 and acts as a contiguous span inside the input sentence. They adopt a BIO tagging scheme to incorporate the answer position information as learned embedding features in Seq2Seq learning. Song et a"
D19-1317,D18-1424,0,0.428628,"UGE-L (RL) (Lin, 2004). We use the evaluation script released by Chen et al. (2015). 3.2 Baseline Models We compare with the following models. • s2s (Du et al., 2017) proposes an attention-based sequence-to-sequence neural network for question generation. • NQG++ (Zhou et al., 2017) takes the answer position feature and linguistic features into consideration and equips the Seq2Seq model with copy mechanism. • M2S+cp (Song et al., 2018) conducts multiperspective matching between the answer and the sentence to derive an answer-aware sentence representation for question generation. • s2s+MP+GSA (Zhao et al., 2018) introduces a gated self-attention into the encoder and a maxout pointer mechanism into the decoder. We report their sentence-level results for a fair comparison. • Hybrid (Sun et al., 2018) is a hybrid model which considers the answer embedding for the question word generation and the position of context words for modeling the relative distance between the context words and the answer. • ASs2s (Kim et al., 2019) replaces the answer in the sentence with a special token to avoid its appearance in the generated questions. 3.3 Implementation Details We take the most frequent 20k words as our voca"
D19-1466,P07-1056,0,0.935045,"n domain supervisions, we explore an unsupervised domain adaptation setting for E2E-ABSA, which aims to leverage knowledge from a labeled source domain to improve the sequence learning in an unlabeled target domain. The challenges in fulfillment of this setting are two-fold: (1) there exists a large feature distribution shift between domains since aspect terms in different domains are usually disjoint. For example, users usually mention “pizza” in the Restaurant domain while “camera” is often discussed in the Laptop domain; (2) Unlike domain adaptation in traditional sentiment classification (Blitzer et al., 2007) that learns shared sentence or document representations, we need to learn fine-grained (word-level) representations to be domain-invariant for sequence prediction. Consider the first problem, i.e., what to transfer? Even though aspect terms from different domains behave distinctly, some association patterns between aspect and opinion words are common across domains; e.g., “The pizza is great.” from the Restaurant domain and “The camera is excellent.” from the Laptop domain. Both of them share the same syntactic pattern (as4590 Proceedings of the 2019 Conference on Empirical Methods in Natural"
D19-1466,D17-1047,1,0.869351,"be broken into two sub-tasks, namely, aspect detection and aspect sentiment classification. The aspect detection aims to extract the aspect terms mentioned in the text and it has been actively studied (Qiu et al., 2011; Liu et al., 2015; Poria et al., 2016; Wang et al., 2016a; He et al., 2017; Wang et al., 2017; Majumder et al., 2018; Li et al., 2018b; Xu et al., 2018). The aspect sentiment classification is to predict the sentiment polarities of the given aspect terms and has also received a lot of attention recently (Dong et al., 2014; Tang et al., 2016; Wang et al., 2016b; Ma et al., 2017; Chen et al., 2017; Ma et al., 2018; He et al., 2018b; Li et al., 2018a, 2019b). For practical applications, a typical way is to pipeline these two sub-tasks together, which becomes ineffective due to the accumulated errors across tasks. These two sub-tasks have strong couplings and thus a unified formulation (Mitchell et al., 2013; Zhang et al., 2015; Li et al., 2019a) to handle them together in an end-to-end manner becomes a more promising direction. Despite its importance, existing studies are only exploring the performance in a single domain, while ignoring the transferability across domains. To address thi"
D19-1466,P14-2009,0,0.0707082,"of the model for the fine-grained adaptation. 5 Related Works E2E-ABSA can be broken into two sub-tasks, namely, aspect detection and aspect sentiment classification. The aspect detection aims to extract the aspect terms mentioned in the text and it has been actively studied (Qiu et al., 2011; Liu et al., 2015; Poria et al., 2016; Wang et al., 2016a; He et al., 2017; Wang et al., 2017; Majumder et al., 2018; Li et al., 2018b; Xu et al., 2018). The aspect sentiment classification is to predict the sentiment polarities of the given aspect terms and has also received a lot of attention recently (Dong et al., 2014; Tang et al., 2016; Wang et al., 2016b; Ma et al., 2017; Chen et al., 2017; Ma et al., 2018; He et al., 2018b; Li et al., 2018a, 2019b). For practical applications, a typical way is to pipeline these two sub-tasks together, which becomes ineffective due to the accumulated errors across tasks. These two sub-tasks have strong couplings and thus a unified formulation (Mitchell et al., 2013; Zhang et al., 2015; Li et al., 2019a) to handle them together in an end-to-end manner becomes a more promising direction. Despite its importance, existing studies are only exploring the performance in a singl"
D19-1466,P17-1036,0,0.0337708,"the AD-SAL model can be used for both discriminative word tags predictions and acting as a learnable alignment weight for each word. This shows that the proposed SAL method can learn to align important aspect words to improve the transferability of the model for the fine-grained adaptation. 5 Related Works E2E-ABSA can be broken into two sub-tasks, namely, aspect detection and aspect sentiment classification. The aspect detection aims to extract the aspect terms mentioned in the text and it has been actively studied (Qiu et al., 2011; Liu et al., 2015; Poria et al., 2016; Wang et al., 2016a; He et al., 2017; Wang et al., 2017; Majumder et al., 2018; Li et al., 2018b; Xu et al., 2018). The aspect sentiment classification is to predict the sentiment polarities of the given aspect terms and has also received a lot of attention recently (Dong et al., 2014; Tang et al., 2016; Wang et al., 2016b; Ma et al., 2017; Chen et al., 2017; Ma et al., 2018; He et al., 2018b; Li et al., 2018a, 2019b). For practical applications, a typical way is to pipeline these two sub-tasks together, which becomes ineffective due to the accumulated errors across tasks. These two sub-tasks have strong couplings and thus a uni"
D19-1466,D18-1383,0,0.0299589,", aspect detection and aspect sentiment classification. The aspect detection aims to extract the aspect terms mentioned in the text and it has been actively studied (Qiu et al., 2011; Liu et al., 2015; Poria et al., 2016; Wang et al., 2016a; He et al., 2017; Wang et al., 2017; Majumder et al., 2018; Li et al., 2018b; Xu et al., 2018). The aspect sentiment classification is to predict the sentiment polarities of the given aspect terms and has also received a lot of attention recently (Dong et al., 2014; Tang et al., 2016; Wang et al., 2016b; Ma et al., 2017; Chen et al., 2017; Ma et al., 2018; He et al., 2018b; Li et al., 2018a, 2019b). For practical applications, a typical way is to pipeline these two sub-tasks together, which becomes ineffective due to the accumulated errors across tasks. These two sub-tasks have strong couplings and thus a unified formulation (Mitchell et al., 2013; Zhang et al., 2015; Li et al., 2019a) to handle them together in an end-to-end manner becomes a more promising direction. Despite its importance, existing studies are only exploring the performance in a single domain, while ignoring the transferability across domains. To address this problem, unsupervised domain ada"
D19-1466,P18-2092,0,0.0115798,", aspect detection and aspect sentiment classification. The aspect detection aims to extract the aspect terms mentioned in the text and it has been actively studied (Qiu et al., 2011; Liu et al., 2015; Poria et al., 2016; Wang et al., 2016a; He et al., 2017; Wang et al., 2017; Majumder et al., 2018; Li et al., 2018b; Xu et al., 2018). The aspect sentiment classification is to predict the sentiment polarities of the given aspect terms and has also received a lot of attention recently (Dong et al., 2014; Tang et al., 2016; Wang et al., 2016b; Ma et al., 2017; Chen et al., 2017; Ma et al., 2018; He et al., 2018b; Li et al., 2018a, 2019b). For practical applications, a typical way is to pipeline these two sub-tasks together, which becomes ineffective due to the accumulated errors across tasks. These two sub-tasks have strong couplings and thus a unified formulation (Mitchell et al., 2013; Zhang et al., 2015; Li et al., 2019a) to handle them together in an end-to-end manner becomes a more promising direction. Despite its importance, existing studies are only exploring the performance in a single domain, while ignoring the transferability across domains. To address this problem, unsupervised domain ada"
D19-1466,D10-1101,0,0.612638,"“The pizza is great.” from the Restaurant domain and “The camera is excellent.” from the Laptop domain. Both of them share the same syntactic pattern (as4590 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4590–4600, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics pect words→nsubj→opinion words). Inspired by this, existing studies use general syntactic relations as the pivot to bridge the domain gaps for cross-domain aspect extraction (Jakob and Gurevych, 2010; Ding et al., 2017), or aspect and opinion co-extraction (Li et al., 2012; Wang and Pan, 2018). Unfortunately, these methods highly rely on prior knowledge (e.g., manually-designed rules) or external linguistic resources (e.g., dependency parsers), which are inflexible and prone to bringing in knowledge errors. Instead, we introduce a multi-hop Dual Memory Interaction (DMI) mechanism to automatically capture the latent relations among aspect and opinion words. The DMI iteratively infers the correlation vectors of each word by interacting its local memory (LSTM hidden state) with both the glob"
D19-1466,J81-4005,0,0.719887,"Missing"
D19-1466,P12-1043,1,0.879672,"om the Laptop domain. Both of them share the same syntactic pattern (as4590 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4590–4600, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics pect words→nsubj→opinion words). Inspired by this, existing studies use general syntactic relations as the pivot to bridge the domain gaps for cross-domain aspect extraction (Jakob and Gurevych, 2010; Ding et al., 2017), or aspect and opinion co-extraction (Li et al., 2012; Wang and Pan, 2018). Unfortunately, these methods highly rely on prior knowledge (e.g., manually-designed rules) or external linguistic resources (e.g., dependency parsers), which are inflexible and prone to bringing in knowledge errors. Instead, we introduce a multi-hop Dual Memory Interaction (DMI) mechanism to automatically capture the latent relations among aspect and opinion words. The DMI iteratively infers the correlation vectors of each word by interacting its local memory (LSTM hidden state) with both the global aspect and opinion memories, such that the inter-correlations between a"
D19-1466,P18-1087,1,0.873024,"tags predictions and acting as a learnable alignment weight for each word. This shows that the proposed SAL method can learn to align important aspect words to improve the transferability of the model for the fine-grained adaptation. 5 Related Works E2E-ABSA can be broken into two sub-tasks, namely, aspect detection and aspect sentiment classification. The aspect detection aims to extract the aspect terms mentioned in the text and it has been actively studied (Qiu et al., 2011; Liu et al., 2015; Poria et al., 2016; Wang et al., 2016a; He et al., 2017; Wang et al., 2017; Majumder et al., 2018; Li et al., 2018b; Xu et al., 2018). The aspect sentiment classification is to predict the sentiment polarities of the given aspect terms and has also received a lot of attention recently (Dong et al., 2014; Tang et al., 2016; Wang et al., 2016b; Ma et al., 2017; Chen et al., 2017; Ma et al., 2018; He et al., 2018b; Li et al., 2018a, 2019b). For practical applications, a typical way is to pipeline these two sub-tasks together, which becomes ineffective due to the accumulated errors across tasks. These two sub-tasks have strong couplings and thus a unified formulation (Mitchell et al., 2013; Zhang et al., 2015"
D19-1466,D15-1168,0,0.0903467,"model to attend the aspect words, while the A-attention of the AD-SAL model can be used for both discriminative word tags predictions and acting as a learnable alignment weight for each word. This shows that the proposed SAL method can learn to align important aspect words to improve the transferability of the model for the fine-grained adaptation. 5 Related Works E2E-ABSA can be broken into two sub-tasks, namely, aspect detection and aspect sentiment classification. The aspect detection aims to extract the aspect terms mentioned in the text and it has been actively studied (Qiu et al., 2011; Liu et al., 2015; Poria et al., 2016; Wang et al., 2016a; He et al., 2017; Wang et al., 2017; Majumder et al., 2018; Li et al., 2018b; Xu et al., 2018). The aspect sentiment classification is to predict the sentiment polarities of the given aspect terms and has also received a lot of attention recently (Dong et al., 2014; Tang et al., 2016; Wang et al., 2016b; Ma et al., 2017; Chen et al., 2017; Ma et al., 2018; He et al., 2018b; Li et al., 2018a, 2019b). For practical applications, a typical way is to pipeline these two sub-tasks together, which becomes ineffective due to the accumulated errors across tasks."
D19-1466,D13-1171,0,0.228549,"Missing"
D19-1466,D16-1046,0,0.0253617,"ty scores zD i ∈R over the domain labels Y D as: D L L zD i = p(yi |ra,i ) = Softmax(WD Rλ (ra,i )+bD ). L at And meanwhile, the aspect attention weight αa,i the final hop serves as a selector to be a learnable alignment weight for each word. Thus, the selective domain adversarial loss is a weighted crossentropy loss ` for all the words from the labeled source data Ds and unlabeled target data Dt : LD = T X X L D αa,i `(zD i , yi ). (1) Ds ∪Dt i=1 2 The opinion lexicon (http://mpqa.cs.pitt. edu/) is used to provide opinion labels for both domains. 4593 Existing studies (Yosinski et al., 2014; Mou et al., 2016) have already shown some evidence that lowlevel neural layer features (i.e., low-level task) are more easily transferred to different tasks or domains. Thus, we choose the rL a,i from the lowlevel AD task to be aligned instead of the feature hUi from the high-level ADS task to transfer. Our ablation studies also confirm this assumption. 3.5 Alternating Training LM = X T X Q `(zQ i , yi ). (2) Ds Q∈{B,U } i=1 The auxiliary opinion detection loss is the crossentropy loss for the labeled source data Ds and unlabeled target data Dt as follows: LO = T X X O `(zO i , yi ). (3) Ds ∪Dt i=1 Traditional"
D19-1466,S16-1002,0,0.15681,"Missing"
D19-1466,S15-2082,0,0.394811,"Missing"
D19-1466,S14-2004,0,0.649619,"l Learning (SAL) method to align the inferred correlation vectors that automatically capture their latent relations. The SAL method can dynamically learn an alignment weight for each word such that more important words can possess higher alignment weights to achieve finegrained (word-level) adaptation. Empirically, extensive experiments1 demonstrate the effectiveness of the proposed SAL method. 1 Introduction End-to-End Aspect-Based Sentiment Analysis (E2E-ABSA) aims to jointly detect the aspect terms explicitly mentioned in sentences and predict the sentiment polarities over them (Liu, 2012; Pontiki et al., 2014). For example, in the sentence “The AMD Turin Processor seems to always perform much better than Intel”, the user mentions two aspect terms, i.e., “AMD Turin Processor” and “Intel”, and expresses positive and negative sentiments over them, respectively. Typically, prior work formulates E2E-ABSA as a sequence labeling problem over a unified tagging scheme (Mitchell et al., 2013; Zhang et al., 2015; Li et al., 2019a). The unified tagging scheme 1 The code is available at https://github.com/ hsqmlzno1/Transferable-E2E-ABSA connects a set of aspect boundary tags (e.g., {B, I, E, S, O} denotes the"
D19-1466,D16-1021,0,0.072647,"e fine-grained adaptation. 5 Related Works E2E-ABSA can be broken into two sub-tasks, namely, aspect detection and aspect sentiment classification. The aspect detection aims to extract the aspect terms mentioned in the text and it has been actively studied (Qiu et al., 2011; Liu et al., 2015; Poria et al., 2016; Wang et al., 2016a; He et al., 2017; Wang et al., 2017; Majumder et al., 2018; Li et al., 2018b; Xu et al., 2018). The aspect sentiment classification is to predict the sentiment polarities of the given aspect terms and has also received a lot of attention recently (Dong et al., 2014; Tang et al., 2016; Wang et al., 2016b; Ma et al., 2017; Chen et al., 2017; Ma et al., 2018; He et al., 2018b; Li et al., 2018a, 2019b). For practical applications, a typical way is to pipeline these two sub-tasks together, which becomes ineffective due to the accumulated errors across tasks. These two sub-tasks have strong couplings and thus a unified formulation (Mitchell et al., 2013; Zhang et al., 2015; Li et al., 2019a) to handle them together in an end-to-end manner becomes a more promising direction. Despite its importance, existing studies are only exploring the performance in a single domain, while ign"
D19-1466,P10-1059,0,0.162088,"he labeled source data Ds : X Dataset L R D S 4.1 Experiments Experimental Setup Datasets: Our experiments are conducted on four benchmark datasets: Laptop (L), Restaurant (R), Device (D), and Service (S). L contains reviews from the laptop domain in SemEval ABSA challenge 2014 (Pontiki et al., 2014). Following the setup in (Li et al., 2019a), R is the union set of the restaurant datasets from SemEval ABSA challenge 2014, 2015, and 2016 (Pontiki et al., 2014, 2015, 2016). D is a combination of device reviews from 5 different digital products provided by (Hu and Liu, 2004). S is introduced by (Toprak et al., 2010) and contains reviews from web services. Detailed statistics are shown in Table 1. Settings: We construct 10 transfer pairs like Ds →Dt with the four domains mentioned above, and we do not use the pairs L→D and D→L as these two domains are very similar. Note that for the unsupervised domain adaptation setting, no labels are available for the target domain. Therefore, for each transfer pair, its training dataset is the combination of the labeled training data of the source domain and the unlabeled training data of the target domain. Meanwhile, it employs the testing data of the source domain wi"
D19-1466,P18-1202,0,0.735418,"ain. Both of them share the same syntactic pattern (as4590 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4590–4600, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics pect words→nsubj→opinion words). Inspired by this, existing studies use general syntactic relations as the pivot to bridge the domain gaps for cross-domain aspect extraction (Jakob and Gurevych, 2010; Ding et al., 2017), or aspect and opinion co-extraction (Li et al., 2012; Wang and Pan, 2018). Unfortunately, these methods highly rely on prior knowledge (e.g., manually-designed rules) or external linguistic resources (e.g., dependency parsers), which are inflexible and prone to bringing in knowledge errors. Instead, we introduce a multi-hop Dual Memory Interaction (DMI) mechanism to automatically capture the latent relations among aspect and opinion words. The DMI iteratively infers the correlation vectors of each word by interacting its local memory (LSTM hidden state) with both the global aspect and opinion memories, such that the inter-correlations between aspects and opinions,"
D19-1466,D16-1059,0,0.112139,"e the A-attention of the AD-SAL model can be used for both discriminative word tags predictions and acting as a learnable alignment weight for each word. This shows that the proposed SAL method can learn to align important aspect words to improve the transferability of the model for the fine-grained adaptation. 5 Related Works E2E-ABSA can be broken into two sub-tasks, namely, aspect detection and aspect sentiment classification. The aspect detection aims to extract the aspect terms mentioned in the text and it has been actively studied (Qiu et al., 2011; Liu et al., 2015; Poria et al., 2016; Wang et al., 2016a; He et al., 2017; Wang et al., 2017; Majumder et al., 2018; Li et al., 2018b; Xu et al., 2018). The aspect sentiment classification is to predict the sentiment polarities of the given aspect terms and has also received a lot of attention recently (Dong et al., 2014; Tang et al., 2016; Wang et al., 2016b; Ma et al., 2017; Chen et al., 2017; Ma et al., 2018; He et al., 2018b; Li et al., 2018a, 2019b). For practical applications, a typical way is to pipeline these two sub-tasks together, which becomes ineffective due to the accumulated errors across tasks. These two sub-tasks have strong coupli"
D19-1466,D16-1058,0,0.115671,"e the A-attention of the AD-SAL model can be used for both discriminative word tags predictions and acting as a learnable alignment weight for each word. This shows that the proposed SAL method can learn to align important aspect words to improve the transferability of the model for the fine-grained adaptation. 5 Related Works E2E-ABSA can be broken into two sub-tasks, namely, aspect detection and aspect sentiment classification. The aspect detection aims to extract the aspect terms mentioned in the text and it has been actively studied (Qiu et al., 2011; Liu et al., 2015; Poria et al., 2016; Wang et al., 2016a; He et al., 2017; Wang et al., 2017; Majumder et al., 2018; Li et al., 2018b; Xu et al., 2018). The aspect sentiment classification is to predict the sentiment polarities of the given aspect terms and has also received a lot of attention recently (Dong et al., 2014; Tang et al., 2016; Wang et al., 2016b; Ma et al., 2017; Chen et al., 2017; Ma et al., 2018; He et al., 2018b; Li et al., 2018a, 2019b). For practical applications, a typical way is to pipeline these two sub-tasks together, which becomes ineffective due to the accumulated errors across tasks. These two sub-tasks have strong coupli"
D19-1466,P18-2094,0,0.1058,"nd acting as a learnable alignment weight for each word. This shows that the proposed SAL method can learn to align important aspect words to improve the transferability of the model for the fine-grained adaptation. 5 Related Works E2E-ABSA can be broken into two sub-tasks, namely, aspect detection and aspect sentiment classification. The aspect detection aims to extract the aspect terms mentioned in the text and it has been actively studied (Qiu et al., 2011; Liu et al., 2015; Poria et al., 2016; Wang et al., 2016a; He et al., 2017; Wang et al., 2017; Majumder et al., 2018; Li et al., 2018b; Xu et al., 2018). The aspect sentiment classification is to predict the sentiment polarities of the given aspect terms and has also received a lot of attention recently (Dong et al., 2014; Tang et al., 2016; Wang et al., 2016b; Ma et al., 2017; Chen et al., 2017; Ma et al., 2018; He et al., 2018b; Li et al., 2018a, 2019b). For practical applications, a typical way is to pipeline these two sub-tasks together, which becomes ineffective due to the accumulated errors across tasks. These two sub-tasks have strong couplings and thus a unified formulation (Mitchell et al., 2013; Zhang et al., 2015; Li et al., 2019a)"
D19-1466,D16-1023,0,0.109125,"et al., 2013; Zhang et al., 2015; Li et al., 2019a) to handle them together in an end-to-end manner becomes a more promising direction. Despite its importance, existing studies are only exploring the performance in a single domain, while ignoring the transferability across domains. To address this problem, unsupervised domain adaptation methods can be applied. While existing methods focus on traditional cross-domain sentiment classification to learn shared representations for sentences or documents, including pivot-based methods (Blitzer et al., 2007; Pan et al., 2010; Bollegala et al., 2013; Yu and Jiang, 2016), auto-encoders (Glorot et al., 2011; Chen et al., 2012; Zhou et al., 2016), domain adversarial networks (Ganin et al., 2016; Li et al., 2017, 2018c), or semi-supervised methods (He et al., 2018a). Due to the difficulties in fine-grained adaptation, there exist very few methods for cross-domain aspect extraction (Jakob and Gurevych, 2010; Ding et al., 2017), which acts as a sub-task of E2E-ABSA, or aspect and opinion co-extraction (Li et al., 2012; Wang and Pan, 2018) that focuses on detecting aspect and opinion words, while E2E-ABSA needs to analyze more complicated correspondences between th"
D19-1466,D15-1073,0,0.517663,"troduction End-to-End Aspect-Based Sentiment Analysis (E2E-ABSA) aims to jointly detect the aspect terms explicitly mentioned in sentences and predict the sentiment polarities over them (Liu, 2012; Pontiki et al., 2014). For example, in the sentence “The AMD Turin Processor seems to always perform much better than Intel”, the user mentions two aspect terms, i.e., “AMD Turin Processor” and “Intel”, and expresses positive and negative sentiments over them, respectively. Typically, prior work formulates E2E-ABSA as a sequence labeling problem over a unified tagging scheme (Mitchell et al., 2013; Zhang et al., 2015; Li et al., 2019a). The unified tagging scheme 1 The code is available at https://github.com/ hsqmlzno1/Transferable-E2E-ABSA connects a set of aspect boundary tags (e.g., {B, I, E, S, O} denotes the beginning of, inside of, end of, single-word, and no aspect term), and sentiment tags (e.g. {POS, NEG, NEU} denotes positive, negative or neutral sentiment) together to constitute a joint label space for each word. As such, “AMD Turin Processor” and “Intel” should be tagged with {B-POS, I-POS, E-POS} and {S-NEG}, respectively, while the remaining words are tagged with O. This formulation makes tw"
D19-1466,P16-1031,0,0.0206475,"in an end-to-end manner becomes a more promising direction. Despite its importance, existing studies are only exploring the performance in a single domain, while ignoring the transferability across domains. To address this problem, unsupervised domain adaptation methods can be applied. While existing methods focus on traditional cross-domain sentiment classification to learn shared representations for sentences or documents, including pivot-based methods (Blitzer et al., 2007; Pan et al., 2010; Bollegala et al., 2013; Yu and Jiang, 2016), auto-encoders (Glorot et al., 2011; Chen et al., 2012; Zhou et al., 2016), domain adversarial networks (Ganin et al., 2016; Li et al., 2017, 2018c), or semi-supervised methods (He et al., 2018a). Due to the difficulties in fine-grained adaptation, there exist very few methods for cross-domain aspect extraction (Jakob and Gurevych, 2010; Ding et al., 2017), which acts as a sub-task of E2E-ABSA, or aspect and opinion co-extraction (Li et al., 2012; Wang and Pan, 2018) that focuses on detecting aspect and opinion words, while E2E-ABSA needs to analyze more complicated correspondences between them. Besides, those methods can only rely on general syntactic relations bet"
D19-1466,J11-1002,0,0.427607,"which hinders the model to attend the aspect words, while the A-attention of the AD-SAL model can be used for both discriminative word tags predictions and acting as a learnable alignment weight for each word. This shows that the proposed SAL method can learn to align important aspect words to improve the transferability of the model for the fine-grained adaptation. 5 Related Works E2E-ABSA can be broken into two sub-tasks, namely, aspect detection and aspect sentiment classification. The aspect detection aims to extract the aspect terms mentioned in the text and it has been actively studied (Qiu et al., 2011; Liu et al., 2015; Poria et al., 2016; Wang et al., 2016a; He et al., 2017; Wang et al., 2017; Majumder et al., 2018; Li et al., 2018b; Xu et al., 2018). The aspect sentiment classification is to predict the sentiment polarities of the given aspect terms and has also received a lot of attention recently (Dong et al., 2014; Tang et al., 2016; Wang et al., 2016b; Ma et al., 2017; Chen et al., 2017; Ma et al., 2018; He et al., 2018b; Li et al., 2018a, 2019b). For practical applications, a typical way is to pipeline these two sub-tasks together, which becomes ineffective due to the accumulated er"
D19-1466,N19-1127,0,0.0554787,"Missing"
D19-1466,P19-1336,0,0.083979,"Missing"
D19-1499,N16-1012,0,0.0398371,"space of different styles and design two constraints to train it. We also introduce two other simple but effective semisupervised methods to compare with. To evaluate the performance of the proposed methods, we build and release a novel style transfer dataset that alters sentences between the style of ancient Chinese poem and the modern Chinese. 1 Introduction Recently, the natural language generation (NLG) tasks have been attracting the growing attention of researchers, including response generation (Vinyals and Le, 2015), machine translation (Bahdanau et al., 2014), automatic summarization (Chopra et al., 2016), question generation (Gao et al., 2019), etc. Among these generation tasks, one interesting but challenging problem is text style transfer (Shen et al., 2017; Fu et al., 2018; Logeswaran et al., 2018). Given a sentence from one style domain, a style transfer system is required to convert it to another style domain as well as keeping its content meaning unchanged. As a fundamental attribute of text, style can have a broad and ambiguous scope, such as ancient poetry style v.s. modern language style and positive sentiment v.s. negative sentiment. ∗ This work was done when Mingyue Shang was an in"
D19-1499,D19-1306,0,0.0616094,"Missing"
D19-1499,D14-1181,0,0.00593167,"Missing"
D19-1499,J82-2005,0,0.727621,"Missing"
D19-1499,D18-1420,1,0.850292,"gn a strategy to disentangle the variables for content and style. Shen et al. (2017) first map the text corpora belonging to different styles to their own space respectively, and leverages the alignment of latent representations from different styles to perform style transfer. Chen et al. (2019) propose to extract and control the style of the image caption through domain layer normalization. Prabhumoye et al. (2018) and Zhang et al. (2018b) employ the back-translation mechanism to ensure that the input from the source style can be reconstructed from the transferred result in the target style. Liao et al. (2018) associate the style latent variable with a numeric discrete or continues numeric value and can generate sentences controlled by this value. Among them, many use the adversarial training mechanism (Goodfellow et al., 2014) to improve the performance of the basic models (Shen et al., 2017; Zhao et al., 2018). To sum up, most of the existing unsupervised frameworks on the text style transfer focus on getting the disentangled representations of style and content. However, Lample et al. (2019) illustrated that the disentanglement not adequate to learn the style-independent representations, thus th"
D19-1499,W02-0109,0,0.189984,"by Rao and Tetreault (2018) which contains texts of formal and informal style. With the released data, we randomly sample 5,000 sentence pairs from it as the parallel corpus with limited data volume. We then use the Yahoo Answers L6 corpus7 as the source which is in the same content domain as the parallel data to construct the large-scale nonparallel data. To divide nonparallel dataset into two styles, we train a CNN-based classifier (Kim, 2014) on the parallel data with annotation of styles and use it to classify the nonparallel data. sentence as 30. For the formality datasets, we use NLTK (Loper and Bird, 2002) to tokenize the texts and set the minimum length as 5 and the maximum length as 30 for both formal and informal styles. We adopt GloVE (Pennington et al., 2014) to pretrain the embeddings, and the dimensions of the embeddings are set to 300 for all the datasets. The hidden states are set to 500 for both encoders and decoders. We adopt SGD optimizer with the learning rate as 1 for DAE models and 0.1 for S2S models. The dropout rate is 0.4. In the inference stage, the beam size is set to 5. 7.2 8.1 Experimental Settings We perform different data preprocessing on different datasets. The Chinese"
D19-1499,P15-2097,0,0.0419915,"e supervised baseline and the three semi-supervised models. Fluency 0.3575 0.4425 0.5800 0.6325 0.3050 0.5475 0.5725 0.6200 9 Table 4: The human annotation results of the S2S model and CPLS model from three aspects. as the automatic evaluation metrics to measure the content preservation degree and the style changing degree. BLEU calculates the N-gram overlap between the generated sentence and the references, thus can be used to measure the preservation of text content. Considering that text style transfer is a monolingual text generation task, we also use GLEU, a generalized BLEU proposed by (Napoles et al., 2015). To evaluate the extent to which the sentences are transferred to the target style, we follow Shen et al. (2017); Hu et al. (2017) that build a CNN-based style classifier and use it to measure the style accuracy. 8.3 Human Evaluation We also adopt human evaluations to judge the quality of the transferred sentences from three aspects, namely content, style and fluency. These aspects evaluate how well the transferred text preserve the content of the input, the style strength and the fluency of the transferred text. Take the content relevance for example, the criterion is as follows: +2: The tra"
D19-1499,D18-1138,0,0.0162566,"e the sentence with classifier favored style. Hu et al. (2017) employ variational autoencoders (Kingma and Welling, 2013) to conduct the style latent variable learning and design a strategy to disentangle the variables for content and style. Shen et al. (2017) first map the text corpora belonging to different styles to their own space respectively, and leverages the alignment of latent representations from different styles to perform style transfer. Chen et al. (2019) propose to extract and control the style of the image caption through domain layer normalization. Prabhumoye et al. (2018) and Zhang et al. (2018b) employ the back-translation mechanism to ensure that the input from the source style can be reconstructed from the transferred result in the target style. Liao et al. (2018) associate the style latent variable with a numeric discrete or continues numeric value and can generate sentences controlled by this value. Among them, many use the adversarial training mechanism (Goodfellow et al., 2014) to improve the performance of the basic models (Shen et al., 2017; Zhao et al., 2018). To sum up, most of the existing unsupervised frameworks on the text style transfer focus on getting the disentangl"
D19-1499,P02-1040,0,0.106223,"Missing"
D19-1499,D14-1162,0,0.0825285,"parallel corpus with limited data volume. We then use the Yahoo Answers L6 corpus7 as the source which is in the same content domain as the parallel data to construct the large-scale nonparallel data. To divide nonparallel dataset into two styles, we train a CNN-based classifier (Kim, 2014) on the parallel data with annotation of styles and use it to classify the nonparallel data. sentence as 30. For the formality datasets, we use NLTK (Loper and Bird, 2002) to tokenize the texts and set the minimum length as 5 and the maximum length as 30 for both formal and informal styles. We adopt GloVE (Pennington et al., 2014) to pretrain the embeddings, and the dimensions of the embeddings are set to 300 for all the datasets. The hidden states are set to 500 for both encoders and decoders. We adopt SGD optimizer with the learning rate as 1 for DAE models and 0.1 for S2S models. The dropout rate is 0.4. In the inference stage, the beam size is set to 5. 7.2 8.1 Experimental Settings We perform different data preprocessing on different datasets. The Chinese literary datasets are segmented by characters instead of word to alleviate the issue of unknown words. Our statistics show that the average length of ancient poe"
D19-1499,P18-1080,0,0.0387814,"ting the training mode between supervised and unsupervised. • We introduce another two semi-supervised methods that are simple but effective to leverage both the nonparallel and parallel data. • We build a small-scale parallel dataset that contains ancient Chinese poem style and modern Chinese style sentences. We also collect two large nonparallel datasets of these styles.1 2 Related Works Recently, text style transfer has stimulated great interests of researchers from the area of neural language processing and some encouraging results are obtained (Shen et al., 2017; Rao and Tetreault, 2018; Prabhumoye et al., 2018; Hu et al., 2017; Jin et al., 2019). In the primary stage, due to the lacking of parallel corpus, most of the methods employ unsupervised learning paradigm to conduct the semantic modeling and transfer. 1 Download link: https://tinyurl.com/yyc8zkqg Unsupervised Learning Methods. Mueller et al. (2017) modify the latent variables of sentences in a certain direction guided by a classifier to generate the sentence with classifier favored style. Hu et al. (2017) employ variational autoencoders (Kingma and Welling, 2013) to conduct the style latent variable learning and design a strategy to disenta"
D19-1499,N18-1012,0,0.358646,"el is flexible in alternating the training mode between supervised and unsupervised. • We introduce another two semi-supervised methods that are simple but effective to leverage both the nonparallel and parallel data. • We build a small-scale parallel dataset that contains ancient Chinese poem style and modern Chinese style sentences. We also collect two large nonparallel datasets of these styles.1 2 Related Works Recently, text style transfer has stimulated great interests of researchers from the area of neural language processing and some encouraging results are obtained (Shen et al., 2017; Rao and Tetreault, 2018; Prabhumoye et al., 2018; Hu et al., 2017; Jin et al., 2019). In the primary stage, due to the lacking of parallel corpus, most of the methods employ unsupervised learning paradigm to conduct the semantic modeling and transfer. 1 Download link: https://tinyurl.com/yyc8zkqg Unsupervised Learning Methods. Mueller et al. (2017) modify the latent variables of sentences in a certain direction guided by a classifier to generate the sentence with classifier favored style. Hu et al. (2017) employ variational autoencoders (Kingma and Welling, 2013) to conduct the style latent variable learning and des"
D19-1499,P16-1009,0,0.0730157,"Missing"
D19-1563,D17-1047,1,0.831698,"Missing"
D19-1563,D18-1066,0,0.112865,"(c−2 ), (c−1 ), and (c0 ). The goal of ECA is to determine which clause contains emotion cause (e.g., (c−1 )) for an emotion word (e.g., happiness in (c0 )). Previous approaches for emotion cause analysis mostly depend on rule-based methods (Lee et al., 2010; Chen et al., 2010) and machine learning algorithms (Ghazi et al., 2015; Gui et al., 2016; Xu et al., 2019). Most of them rely heavily on complicated linguistic rules or feature engineering, which is time-consuming and labor-intensive. Recent studies have focused on solving the task using neural models (Gui et al., 2017; Li et al., 2018a; Chen et al., 2018; Li et al., 2019) with well designed attention mechanism based on local text. Despite the effectiveness of neural models, there are some defects in previous studies. First, they usually consider each clause individually, i.e., ignoring the discourse context information that can impact the semantic expression among different clauses of a document. Second, prior knowledge such as sentiment lexicon and relative position information that can provide crucial emotion cause cues has not been fully exploited in neural models. To alleviate these limitations, we propose a regularized hierarchical neura"
D19-1563,C10-1021,0,0.84091,"ovided by users. Emotion cause analysis (ECA) aims to identify the reasons behind a certain emotion expression in an event text, for example: Ex.1 When the children saw the gifts I prepared carefully, (c−2 )|they cheered happily and hugged me. (c−1 ) |I was full of happiness. (c0 ) Here, Ex.1 shows a document with three clauses marked as (c−2 ), (c−1 ), and (c0 ). The goal of ECA is to determine which clause contains emotion cause (e.g., (c−1 )) for an emotion word (e.g., happiness in (c0 )). Previous approaches for emotion cause analysis mostly depend on rule-based methods (Lee et al., 2010; Chen et al., 2010) and machine learning algorithms (Ghazi et al., 2015; Gui et al., 2016; Xu et al., 2019). Most of them rely heavily on complicated linguistic rules or feature engineering, which is time-consuming and labor-intensive. Recent studies have focused on solving the task using neural models (Gui et al., 2017; Li et al., 2018a; Chen et al., 2018; Li et al., 2019) with well designed attention mechanism based on local text. Despite the effectiveness of neural models, there are some defects in previous studies. First, they usually consider each clause individually, i.e., ignoring the discourse context in"
D19-1563,D14-1179,0,0.0343443,"Missing"
D19-1563,D17-1167,1,0.818974,"cument with three clauses marked as (c−2 ), (c−1 ), and (c0 ). The goal of ECA is to determine which clause contains emotion cause (e.g., (c−1 )) for an emotion word (e.g., happiness in (c0 )). Previous approaches for emotion cause analysis mostly depend on rule-based methods (Lee et al., 2010; Chen et al., 2010) and machine learning algorithms (Ghazi et al., 2015; Gui et al., 2016; Xu et al., 2019). Most of them rely heavily on complicated linguistic rules or feature engineering, which is time-consuming and labor-intensive. Recent studies have focused on solving the task using neural models (Gui et al., 2017; Li et al., 2018a; Chen et al., 2018; Li et al., 2019) with well designed attention mechanism based on local text. Despite the effectiveness of neural models, there are some defects in previous studies. First, they usually consider each clause individually, i.e., ignoring the discourse context information that can impact the semantic expression among different clauses of a document. Second, prior knowledge such as sentiment lexicon and relative position information that can provide crucial emotion cause cues has not been fully exploited in neural models. To alleviate these limitations, we pro"
D19-1563,D16-1170,1,0.2569,"ns behind a certain emotion expression in an event text, for example: Ex.1 When the children saw the gifts I prepared carefully, (c−2 )|they cheered happily and hugged me. (c−1 ) |I was full of happiness. (c0 ) Here, Ex.1 shows a document with three clauses marked as (c−2 ), (c−1 ), and (c0 ). The goal of ECA is to determine which clause contains emotion cause (e.g., (c−1 )) for an emotion word (e.g., happiness in (c0 )). Previous approaches for emotion cause analysis mostly depend on rule-based methods (Lee et al., 2010; Chen et al., 2010) and machine learning algorithms (Ghazi et al., 2015; Gui et al., 2016; Xu et al., 2019). Most of them rely heavily on complicated linguistic rules or feature engineering, which is time-consuming and labor-intensive. Recent studies have focused on solving the task using neural models (Gui et al., 2017; Li et al., 2018a; Chen et al., 2018; Li et al., 2019) with well designed attention mechanism based on local text. Despite the effectiveness of neural models, there are some defects in previous studies. First, they usually consider each clause individually, i.e., ignoring the discourse context information that can impact the semantic expression among different clau"
D19-1563,D14-1181,0,0.00355089,"al results on the English dataset, we follow the results that are implemented in (Li et al., 2019), the only available results on this dataset (p &lt;0.001). Table 2: Experimental results on the Chinese dataset. Superscript ∗ indicates the results are reported in (Gui et al., 2017) and the rest are reprinted from the corresponding publications (p &lt;0.001). LambdaMART extracts emotion causes using learning to rank methods which based on the emotion-independent and emotiondependent features (Xu et al., 2019). • Deep learning method: CNN is a convolutional neural network for sentence classification (Kim, 2014). ConvMS-Memnet considers emotion cause analysis as a reading comprehension task and designs a multiple-slot deep memory network to model context information (Gui et al., 2017). CANN uses a co-attention neural network to identify emotion causes (Li et al., 2018a). HCS is proposed by Yu et al. (2019) using a multiplelevel hierarchical network to detect the emotion causes. MANN is the current state-ofthe-art method employing a multi-attentionbased model for emotion cause extraction (Li et al., 2019). RHNN is our proposed model. 5.1 P 0.1651 0.2757 0.7218 0.4605 0.7933 0.6901 Main Results The exp"
D19-1563,W10-0206,0,0.438743,"e emotion cause provided by users. Emotion cause analysis (ECA) aims to identify the reasons behind a certain emotion expression in an event text, for example: Ex.1 When the children saw the gifts I prepared carefully, (c−2 )|they cheered happily and hugged me. (c−1 ) |I was full of happiness. (c0 ) Here, Ex.1 shows a document with three clauses marked as (c−2 ), (c−1 ), and (c0 ). The goal of ECA is to determine which clause contains emotion cause (e.g., (c−1 )) for an emotion word (e.g., happiness in (c0 )). Previous approaches for emotion cause analysis mostly depend on rule-based methods (Lee et al., 2010; Chen et al., 2010) and machine learning algorithms (Ghazi et al., 2015; Gui et al., 2016; Xu et al., 2019). Most of them rely heavily on complicated linguistic rules or feature engineering, which is time-consuming and labor-intensive. Recent studies have focused on solving the task using neural models (Gui et al., 2017; Li et al., 2018a; Chen et al., 2018; Li et al., 2019) with well designed attention mechanism based on local text. Despite the effectiveness of neural models, there are some defects in previous studies. First, they usually consider each clause individually, i.e., ignoring the"
D19-1563,P15-1101,0,0.0484834,"Missing"
D19-1563,D18-1506,0,0.178681,"clauses marked as (c−2 ), (c−1 ), and (c0 ). The goal of ECA is to determine which clause contains emotion cause (e.g., (c−1 )) for an emotion word (e.g., happiness in (c0 )). Previous approaches for emotion cause analysis mostly depend on rule-based methods (Lee et al., 2010; Chen et al., 2010) and machine learning algorithms (Ghazi et al., 2015; Gui et al., 2016; Xu et al., 2019). Most of them rely heavily on complicated linguistic rules or feature engineering, which is time-consuming and labor-intensive. Recent studies have focused on solving the task using neural models (Gui et al., 2017; Li et al., 2018a; Chen et al., 2018; Li et al., 2019) with well designed attention mechanism based on local text. Despite the effectiveness of neural models, there are some defects in previous studies. First, they usually consider each clause individually, i.e., ignoring the discourse context information that can impact the semantic expression among different clauses of a document. Second, prior knowledge such as sentiment lexicon and relative position information that can provide crucial emotion cause cues has not been fully exploited in neural models. To alleviate these limitations, we propose a regularize"
D19-1563,P18-1087,1,0.899852,"Missing"
D19-1563,P17-1154,0,0.0573365,"sing multiple-user structures. Besides, Yu et al. (2019) proposed a multiple-level hierarchical network-based clause selection strategy. Li et al. (2019) proposed a multi-attention-based neural model to capture the mutual influences between the emotion clause and each candidate clause, and then generate the representations for the above two clauses separately. This method achieves the current best performance. However, the existing approaches usually focus on the local textural information, ignoring the discourse structure (Zubiaga et al., 2018), and prior knowledge such as sentiment lexicon (Qian et al., 2017) and relative position information, which can provide important emotion cues for emotion cause analysis task. 7 Conclusion and Future Work In this paper, we provide a regularized hierarchical neural network (RHNN) for emotion cause analysis. The proposed model aggregates discourse context information through a hierarchical learning structure and restrains the parameters with knowledge-based regularizations. We evaluate the proposed model on two public datasets in different languages. The experimental results demonstrate that our proposed method achieves the stateof-the-art performance on both"
D19-1563,D18-1137,0,0.0294088,"Missing"
D19-1563,D16-1061,0,0.0694939,"Missing"
D19-1563,W11-1720,0,0.811794,"Missing"
D19-1563,H05-1044,0,0.0637208,"lected from English novels. Each document of both datasets has only one emotion word and one or more emotion causes. It has been ensured that the emotion and the causes are relevant. The documents are segmented into several clauses manually for emotion cause analysis. The details about the two datasets are summarized in Table 1. 4.2 lected from HowNet (Dong et al., 2006) sentiment analysis lexicon set4 and the second part comes from NTUSD (Ku et al., 2006). The combination of the two parts serves as the Chinese sentiment lexicon of this research. The English sentiment lexicon comes from MPQA (Wilson et al., 2005) and we only select the words with high sentiment polarity, because they are less sensitive to contextual information and usually express consistence sentiment polarities from their prior polarity. For both sentiment lexica, we filter out the words that are not in the datasets. Ultimately, 2022 and 1348 sentiment words are selected for the Chinese and English dataset respectively. Online learning is performed with the Adam optimizer (Kingma and Ba, 2015) and initial learning rate 0.001 is adopted. The number of layers in Bi-GRU is set to 2 and dropout rate 0.5 is used to avoid overfitting. The"
D19-1563,N16-1174,0,0.117078,"Missing"
D19-5505,N19-1423,0,0.205326,"agnostic embedding layer, usually a linear layer initialized with Word2Vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014), only provides context-independent word-level features, which is insufficient for capturing the complex semantic dependencies in the sentence. Meanwhile, the size of existing datasets is too small to train sophisticated task-specific architectures. Thus, introducing a context-aware word embedding3 layer pre-trained on large-scale datasets with deep LSTM (McCann et al., 2017; Peters et al., 2018; Howard and Ruder, 2018) or Transformer (Radford et al., 2018, 2019; Devlin et al., 2019; Lample ∗ The work described in this paper is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14204418). 1 Our code is open-source and available at: https:// github.com/lixin4ever/BERT-E2E-ABSA 2 Due to the limited space, we can not list all of the existing works here, please refer to the survey (Zhou et al., 2019) for more related papers. 3 In this paper, we generalize the concept of “word embedding” as a mapping between the word and the lowdimensional word representations. In this paper, we investigate t"
D19-5505,N19-1259,0,0.0378825,"al., 2016), from user-generated natural language texts (Liu, 2012). The most popular ABSA benchmark datasets are from SemEval ABSA challenges (Pontiki et al., 2014, 2015, 2016) where a few thousand review sentences with gold standard aspect sentiment annotations are provided. Table 1 summarizes three existing research problems related to ABSA. The first one is the original ABSA, aiming at predicting the sentiment polarity of the sentence towards the given aspect. Compared to this classification problem, the second one and the third one, namely, Aspectoriented Opinion Words Extraction (AOWE) (Fan et al., 2019) and End-to-End Aspect-based Sentiment Analysis (E2E-ABSA) (Ma et al., 2018a; Schmitt et al., 2018; Li et al., 2019a; Li and Lu, 2017, 2019), are related to a sequence tagging problem. Precisely, the goal of AOWE is to extract the aspect-specific opinion words from the sentence given the aspect. The goal of E2E-ABSA is to jointly detect aspect terms/categories and the corresponding aspect sentiments. Many neural models composed of a taskagnostic pre-trained word embedding layer and task-specific neural architecture have been proposed for the original ABSA task (i.e. the aspectlevel sentiment c"
D19-5505,P19-1048,0,0.453897,"RF layer on top of the BERT embedding layer. Different from the above mentioned neural models maximizing the token-level likelihood p(yt |xt ), the CRF-based model aims to find the globally most   rt T = σ(L N(Wx hL t ) + L N (Wh ht−1 )) zt T nt = tanh(L N(Wxn hL t ) + rt ∗ L N (Whn ht−1 )) (4) (3) hTt = (1 − zt ) ∗ nt + zt ∗ hTt−1 where σ is the sigmoid activation function and rt , zt , nt respectively denote the reset gate, update gate and new gate. Wx , Wh ∈ R2dimh ×dimh , Wxn , Whn ∈ Rdimh ×dimh are the parameters of 36 Model 2019a) LSTM-CRF BERT Models (Li et al., (Luo et al., 2019) (He et al., 2019) (Lample et al., 2016)] (Ma and Hovy, 2016)] (Liu et al., 2018)] BERT+Linear BERT+GRU BERT+SAN BERT+TFM BERT+CRF Table 2: Main results. The symbol retrieved from Li et al. (2019a). Dataset # sent LAPTOP # aspect # sent REST # aspect Train 2741 2041 3490 3893  Dev 304 256 387 413 LAPTOP R 54.89 50.47 51.26 59.40 58.90 60.47 58.71 58.64 59.49 F1 57.90 60.35 58.37 54.24 54.71 56.19 60.43 61.12 60.49 60.80 60.78 P 68.64 66.10 61.56 68.46 71.42 70.61 72.92 72.39 71.88 REST R 71.01 66.30 67.26 64.43 75.25 76.20 76.72 76.64 76.48 F1 69.80 72.78 66.20 64.29 66.38 73.22 73.24 74.72 74.41 74.06 denote"
D19-5505,P18-1031,0,0.0318527,"y or F1 score has reached a bottleneck. One reason is that the task-agnostic embedding layer, usually a linear layer initialized with Word2Vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014), only provides context-independent word-level features, which is insufficient for capturing the complex semantic dependencies in the sentence. Meanwhile, the size of existing datasets is too small to train sophisticated task-specific architectures. Thus, introducing a context-aware word embedding3 layer pre-trained on large-scale datasets with deep LSTM (McCann et al., 2017; Peters et al., 2018; Howard and Ruder, 2018) or Transformer (Radford et al., 2018, 2019; Devlin et al., 2019; Lample ∗ The work described in this paper is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14204418). 1 Our code is open-source and available at: https:// github.com/lixin4ever/BERT-E2E-ABSA 2 Due to the limited space, we can not list all of the existing works here, please refer to the survey (Zhou et al., 2019) for more related papers. 3 In this paper, we generalize the concept of “word embedding” as a mapping between the word and the lowd"
D19-5505,P19-1051,0,0.40859,"itional Word2Vec- or GloVebased embedding layer which only provides a single context-independent representation for each token, the BERT embedding layer takes the sentence as input and calculates the token-level representations using the information from the entire sentence. First of all, we pack the input features as H 0 = {e1 , · · · , eT }, where et (t ∈ [1, T ]) is 4 Both of ABSA and AOWE assume that the aspects in a sentence are given. Such setting makes them less practical in real-world scenarios since manual annotation of the finegrained aspect mentions/categories is quite expensive. 5 Hu et al. (2019) introduce BERT to handle the E2EABSA problem but their focus is to design a task-specific architecture rather than exploring the potential of BERT. 35 the combination of the token embedding, position embedding and segment embedding corresponding to the input token xt . Then L transformer layers are introduced to refine the token-level features layer by layer. Specifically, the representations H l = {hl1 , · · · , hlT } at the l-th (l ∈ [1, L]) layer are calculated below: GRU. Since directly applying RNN on the output of transformer, namely, the BERT representation hL t , may lead to unstable"
D19-5505,D18-1136,0,0.0504621,"aspect-specific opinion words from the sentence given the aspect. The goal of E2E-ABSA is to jointly detect aspect terms/categories and the corresponding aspect sentiments. Many neural models composed of a taskagnostic pre-trained word embedding layer and task-specific neural architecture have been proposed for the original ABSA task (i.e. the aspectlevel sentiment classification) (Tang et al., 2016; Wang et al., 2016; Chen et al., 2017; Liu and Zhang, 2017; Ma et al., 2017, 2018b; Majumder et al., 2018; Li et al., 2018; He et al., 2018; Xue and Li, 2018; Wang et al., 2018; Fan et al., 2018; Huang and Carley, 2018; Lei et al., 2019; Li et al., 2019b)2 , but the improvement of these models measured by the accuracy or F1 score has reached a bottleneck. One reason is that the task-agnostic embedding layer, usually a linear layer initialized with Word2Vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014), only provides context-independent word-level features, which is insufficient for capturing the complex semantic dependencies in the sentence. Meanwhile, the size of existing datasets is too small to train sophisticated task-specific architectures. Thus, introducing a context-aware word embedding3"
D19-5505,E17-2091,0,0.0203864,"18a; Schmitt et al., 2018; Li et al., 2019a; Li and Lu, 2017, 2019), are related to a sequence tagging problem. Precisely, the goal of AOWE is to extract the aspect-specific opinion words from the sentence given the aspect. The goal of E2E-ABSA is to jointly detect aspect terms/categories and the corresponding aspect sentiments. Many neural models composed of a taskagnostic pre-trained word embedding layer and task-specific neural architecture have been proposed for the original ABSA task (i.e. the aspectlevel sentiment classification) (Tang et al., 2016; Wang et al., 2016; Chen et al., 2017; Liu and Zhang, 2017; Ma et al., 2017, 2018b; Majumder et al., 2018; Li et al., 2018; He et al., 2018; Xue and Li, 2018; Wang et al., 2018; Fan et al., 2018; Huang and Carley, 2018; Lei et al., 2019; Li et al., 2019b)2 , but the improvement of these models measured by the accuracy or F1 score has reached a bottleneck. One reason is that the task-agnostic embedding layer, usually a linear layer initialized with Word2Vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014), only provides context-independent word-level features, which is insufficient for capturing the complex semantic dependencies in the senten"
D19-5505,D19-1549,0,0.0374129,"Missing"
D19-5505,P19-1056,0,0.685331,"uce a linear-chain CRF layer on top of the BERT embedding layer. Different from the above mentioned neural models maximizing the token-level likelihood p(yt |xt ), the CRF-based model aims to find the globally most   rt T = σ(L N(Wx hL t ) + L N (Wh ht−1 )) zt T nt = tanh(L N(Wxn hL t ) + rt ∗ L N (Whn ht−1 )) (4) (3) hTt = (1 − zt ) ∗ nt + zt ∗ hTt−1 where σ is the sigmoid activation function and rt , zt , nt respectively denote the reset gate, update gate and new gate. Wx , Wh ∈ R2dimh ×dimh , Wxn , Whn ∈ Rdimh ×dimh are the parameters of 36 Model 2019a) LSTM-CRF BERT Models (Li et al., (Luo et al., 2019) (He et al., 2019) (Lample et al., 2016)] (Ma and Hovy, 2016)] (Liu et al., 2018)] BERT+Linear BERT+GRU BERT+SAN BERT+TFM BERT+CRF Table 2: Main results. The symbol retrieved from Li et al. (2019a). Dataset # sent LAPTOP # aspect # sent REST # aspect Train 2741 2041 3490 3893  Dev 304 256 387 413 LAPTOP R 54.89 50.47 51.26 59.40 58.90 60.47 58.71 58.64 59.49 F1 57.90 60.35 58.37 54.24 54.71 56.19 60.43 61.12 60.49 60.80 60.78 P 68.64 66.10 61.56 68.46 71.42 70.61 72.92 72.39 71.88 REST R 71.01 66.30 67.26 64.43 75.25 76.20 76.72 76.64 76.48 F1 69.80 72.78 66.20 64.29 66.38 73.22 73.24 74.72"
D19-5505,D18-1504,0,0.120861,"popular ABSA benchmark datasets are from SemEval ABSA challenges (Pontiki et al., 2014, 2015, 2016) where a few thousand review sentences with gold standard aspect sentiment annotations are provided. Table 1 summarizes three existing research problems related to ABSA. The first one is the original ABSA, aiming at predicting the sentiment polarity of the sentence towards the given aspect. Compared to this classification problem, the second one and the third one, namely, Aspectoriented Opinion Words Extraction (AOWE) (Fan et al., 2019) and End-to-End Aspect-based Sentiment Analysis (E2E-ABSA) (Ma et al., 2018a; Schmitt et al., 2018; Li et al., 2019a; Li and Lu, 2017, 2019), are related to a sequence tagging problem. Precisely, the goal of AOWE is to extract the aspect-specific opinion words from the sentence given the aspect. The goal of E2E-ABSA is to jointly detect aspect terms/categories and the corresponding aspect sentiments. Many neural models composed of a taskagnostic pre-trained word embedding layer and task-specific neural architecture have been proposed for the original ABSA task (i.e. the aspectlevel sentiment classification) (Tang et al., 2016; Wang et al., 2016; Chen et al., 2017; Li"
D19-5505,N16-1030,0,0.485322,"fowicz et al. (2015). The computational formula of the task-specific hidden representation hTt ∈ Rdimh at the t-th time step is shown below: (6) where F FN refers to the point-wise feed-forward networks (Vaswani et al., 2017). Again, a linear layer with softmax activation is stacked on the designed SAN/TFM layer to output the predictions (same with that in Eq(4)). Conditional Random Fields Conditional Random Fields (CRF) (Lafferty et al., 2001) is effective in sequence modeling and has been widely adopted for solving the sequence labeling tasks together with neural models (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016). In this paper, we introduce a linear-chain CRF layer on top of the BERT embedding layer. Different from the above mentioned neural models maximizing the token-level likelihood p(yt |xt ), the CRF-based model aims to find the globally most   rt T = σ(L N(Wx hL t ) + L N (Wh ht−1 )) zt T nt = tanh(L N(Wxn hL t ) + rt ∗ L N (Whn ht−1 )) (4) (3) hTt = (1 − zt ) ∗ nt + zt ∗ hTt−1 where σ is the sigmoid activation function and rt , zt , nt respectively denote the reset gate, update gate and new gate. Wx , Wh ∈ R2dimh ×dimh , Wxn , Whn ∈ Rdimh ×dimh are the parameters of 36 Mo"
D19-5505,P16-1101,0,0.225002,"The computational formula of the task-specific hidden representation hTt ∈ Rdimh at the t-th time step is shown below: (6) where F FN refers to the point-wise feed-forward networks (Vaswani et al., 2017). Again, a linear layer with softmax activation is stacked on the designed SAN/TFM layer to output the predictions (same with that in Eq(4)). Conditional Random Fields Conditional Random Fields (CRF) (Lafferty et al., 2001) is effective in sequence modeling and has been widely adopted for solving the sequence labeling tasks together with neural models (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016). In this paper, we introduce a linear-chain CRF layer on top of the BERT embedding layer. Different from the above mentioned neural models maximizing the token-level likelihood p(yt |xt ), the CRF-based model aims to find the globally most   rt T = σ(L N(Wx hL t ) + L N (Wh ht−1 )) zt T nt = tanh(L N(Wxn hL t ) + rt ∗ L N (Whn ht−1 )) (4) (3) hTt = (1 − zt ) ∗ nt + zt ∗ hTt−1 where σ is the sigmoid activation function and rt , zt , nt respectively denote the reset gate, update gate and new gate. Wx , Wh ∈ R2dimh ×dimh , Wxn , Whn ∈ Rdimh ×dimh are the parameters of 36 Model 2019a) LSTM-CRF"
D19-5505,D18-1139,0,0.0318697,"hmark datasets are from SemEval ABSA challenges (Pontiki et al., 2014, 2015, 2016) where a few thousand review sentences with gold standard aspect sentiment annotations are provided. Table 1 summarizes three existing research problems related to ABSA. The first one is the original ABSA, aiming at predicting the sentiment polarity of the sentence towards the given aspect. Compared to this classification problem, the second one and the third one, namely, Aspectoriented Opinion Words Extraction (AOWE) (Fan et al., 2019) and End-to-End Aspect-based Sentiment Analysis (E2E-ABSA) (Ma et al., 2018a; Schmitt et al., 2018; Li et al., 2019a; Li and Lu, 2017, 2019), are related to a sequence tagging problem. Precisely, the goal of AOWE is to extract the aspect-specific opinion words from the sentence given the aspect. The goal of E2E-ABSA is to jointly detect aspect terms/categories and the corresponding aspect sentiments. Many neural models composed of a taskagnostic pre-trained word embedding layer and task-specific neural architecture have been proposed for the original ABSA task (i.e. the aspectlevel sentiment classification) (Tang et al., 2016; Wang et al., 2016; Chen et al., 2017; Liu and Zhang, 2017; Ma e"
D19-5505,D13-1171,0,0.300659,"Missing"
D19-5505,N19-1035,0,0.0404263,"L L h h 5 6 L h 7 L h 8 L h 9 L h 10 BERT L-th Transformer Layer Table 1: Different problem settings in ABSA. Gold standard aspects and opinions are wrapped in [] and &lt;> respectively. The subscripts N and P refer to aspect sentiment. Underline :* or * indicates the association between the aspect and the opinion. ⋯ 1-st Transformer Layer Segment Embedding Position Embedding Token Embedding and Conneau, 2019; Yang et al., 2019; Dong et al., 2019) for fine-tuning a lightweight task-specific network using the labeled data has good potential for further enhancing the performance. Xu et al. (2019); Sun et al. (2019); Song et al. (2019); Yu and Jiang (2019); Rietzler et al. (2019); Huang and Carley (2019) have conducted some initial attempts to couple the deep contextualized word embedding layer with downstream neural models for the original ABSA task and establish the new state-of-the-art results. It encourages us to explore the potential of using such contextualized embeddings to the more difficult but practical task, i.e. E2E-ABSA (the third setting in Table 1).4 Note that we are not aiming at developing a task-specific architecture, instead, our focus is to examine the potential of contextualized embe"
D19-5505,D14-1162,0,0.0900216,"en proposed for the original ABSA task (i.e. the aspectlevel sentiment classification) (Tang et al., 2016; Wang et al., 2016; Chen et al., 2017; Liu and Zhang, 2017; Ma et al., 2017, 2018b; Majumder et al., 2018; Li et al., 2018; He et al., 2018; Xue and Li, 2018; Wang et al., 2018; Fan et al., 2018; Huang and Carley, 2018; Lei et al., 2019; Li et al., 2019b)2 , but the improvement of these models measured by the accuracy or F1 score has reached a bottleneck. One reason is that the task-agnostic embedding layer, usually a linear layer initialized with Word2Vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014), only provides context-independent word-level features, which is insufficient for capturing the complex semantic dependencies in the sentence. Meanwhile, the size of existing datasets is too small to train sophisticated task-specific architectures. Thus, introducing a context-aware word embedding3 layer pre-trained on large-scale datasets with deep LSTM (McCann et al., 2017; Peters et al., 2018; Howard and Ruder, 2018) or Transformer (Radford et al., 2018, 2019; Devlin et al., 2019; Lample ∗ The work described in this paper is substantially supported by a grant from the Research Grant Council"
D19-5505,D16-1021,0,0.03627,"Aspect-based Sentiment Analysis (E2E-ABSA) (Ma et al., 2018a; Schmitt et al., 2018; Li et al., 2019a; Li and Lu, 2017, 2019), are related to a sequence tagging problem. Precisely, the goal of AOWE is to extract the aspect-specific opinion words from the sentence given the aspect. The goal of E2E-ABSA is to jointly detect aspect terms/categories and the corresponding aspect sentiments. Many neural models composed of a taskagnostic pre-trained word embedding layer and task-specific neural architecture have been proposed for the original ABSA task (i.e. the aspectlevel sentiment classification) (Tang et al., 2016; Wang et al., 2016; Chen et al., 2017; Liu and Zhang, 2017; Ma et al., 2017, 2018b; Majumder et al., 2018; Li et al., 2018; He et al., 2018; Xue and Li, 2018; Wang et al., 2018; Fan et al., 2018; Huang and Carley, 2018; Lei et al., 2019; Li et al., 2019b)2 , but the improvement of these models measured by the accuracy or F1 score has reached a bottleneck. One reason is that the task-agnostic embedding layer, usually a linear layer initialized with Word2Vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014), only provides context-independent word-level features, which is insufficient fo"
D19-5505,N18-1202,0,0.0426857,"asured by the accuracy or F1 score has reached a bottleneck. One reason is that the task-agnostic embedding layer, usually a linear layer initialized with Word2Vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014), only provides context-independent word-level features, which is insufficient for capturing the complex semantic dependencies in the sentence. Meanwhile, the size of existing datasets is too small to train sophisticated task-specific architectures. Thus, introducing a context-aware word embedding3 layer pre-trained on large-scale datasets with deep LSTM (McCann et al., 2017; Peters et al., 2018; Howard and Ruder, 2018) or Transformer (Radford et al., 2018, 2019; Devlin et al., 2019; Lample ∗ The work described in this paper is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14204418). 1 Our code is open-source and available at: https:// github.com/lixin4ever/BERT-E2E-ABSA 2 Due to the limited space, we can not list all of the existing works here, please refer to the survey (Zhou et al., 2019) for more related papers. 3 In this paper, we generalize the concept of “word embedding” as a mapping betw"
D19-5505,P18-1088,0,0.036111,"y, the goal of AOWE is to extract the aspect-specific opinion words from the sentence given the aspect. The goal of E2E-ABSA is to jointly detect aspect terms/categories and the corresponding aspect sentiments. Many neural models composed of a taskagnostic pre-trained word embedding layer and task-specific neural architecture have been proposed for the original ABSA task (i.e. the aspectlevel sentiment classification) (Tang et al., 2016; Wang et al., 2016; Chen et al., 2017; Liu and Zhang, 2017; Ma et al., 2017, 2018b; Majumder et al., 2018; Li et al., 2018; He et al., 2018; Xue and Li, 2018; Wang et al., 2018; Fan et al., 2018; Huang and Carley, 2018; Lei et al., 2019; Li et al., 2019b)2 , but the improvement of these models measured by the accuracy or F1 score has reached a bottleneck. One reason is that the task-agnostic embedding layer, usually a linear layer initialized with Word2Vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014), only provides context-independent word-level features, which is insufficient for capturing the complex semantic dependencies in the sentence. Meanwhile, the size of existing datasets is too small to train sophisticated task-specific architectures. Thus, in"
D19-5505,S15-2082,0,0.474457,"Missing"
D19-5505,D16-1058,0,0.340573,"BERT for End-to-End Aspect-based Sentiment Analysis∗ Xin Li1 , Lidong Bing2 , Wenxuan Zhang1 and Wai Lam1 1 Department of Systems Engineering and Engineering Management The Chinese University of Hong Kong, Hong Kong 2 R&D Center Singapore, Machine Intelligence Technology, Alibaba DAMO Academy {lixin,wxzhang,wlam}@se.cuhk.edu.hk l.bing@alibaba-inc.com Abstract Aspect-based sentiment analysis (ABSA) is to discover the users’ sentiment or opinion towards an aspect, usually in the form of explicitly mentioned aspect terms (Mitchell et al., 2013; Zhang et al., 2015) or implicit aspect categories (Wang et al., 2016), from user-generated natural language texts (Liu, 2012). The most popular ABSA benchmark datasets are from SemEval ABSA challenges (Pontiki et al., 2014, 2015, 2016) where a few thousand review sentences with gold standard aspect sentiment annotations are provided. Table 1 summarizes three existing research problems related to ABSA. The first one is the original ABSA, aiming at predicting the sentiment polarity of the sentence towards the given aspect. Compared to this classification problem, the second one and the third one, namely, Aspectoriented Opinion Words Extraction (AOWE) (Fan et al.,"
D19-5505,S14-2004,0,0.704498,"neering Management The Chinese University of Hong Kong, Hong Kong 2 R&D Center Singapore, Machine Intelligence Technology, Alibaba DAMO Academy {lixin,wxzhang,wlam}@se.cuhk.edu.hk l.bing@alibaba-inc.com Abstract Aspect-based sentiment analysis (ABSA) is to discover the users’ sentiment or opinion towards an aspect, usually in the form of explicitly mentioned aspect terms (Mitchell et al., 2013; Zhang et al., 2015) or implicit aspect categories (Wang et al., 2016), from user-generated natural language texts (Liu, 2012). The most popular ABSA benchmark datasets are from SemEval ABSA challenges (Pontiki et al., 2014, 2015, 2016) where a few thousand review sentences with gold standard aspect sentiment annotations are provided. Table 1 summarizes three existing research problems related to ABSA. The first one is the original ABSA, aiming at predicting the sentiment polarity of the sentence towards the given aspect. Compared to this classification problem, the second one and the third one, namely, Aspectoriented Opinion Words Extraction (AOWE) (Fan et al., 2019) and End-to-End Aspect-based Sentiment Analysis (E2E-ABSA) (Ma et al., 2018a; Schmitt et al., 2018; Li et al., 2019a; Li and Lu, 2017, 2019), are r"
D19-5505,N19-1242,0,0.0347135,"L h 2 L h 3 L h 4 L L h h 5 6 L h 7 L h 8 L h 9 L h 10 BERT L-th Transformer Layer Table 1: Different problem settings in ABSA. Gold standard aspects and opinions are wrapped in [] and &lt;> respectively. The subscripts N and P refer to aspect sentiment. Underline :* or * indicates the association between the aspect and the opinion. ⋯ 1-st Transformer Layer Segment Embedding Position Embedding Token Embedding and Conneau, 2019; Yang et al., 2019; Dong et al., 2019) for fine-tuning a lightweight task-specific network using the labeled data has good potential for further enhancing the performance. Xu et al. (2019); Sun et al. (2019); Song et al. (2019); Yu and Jiang (2019); Rietzler et al. (2019); Huang and Carley (2019) have conducted some initial attempts to couple the deep contextualized word embedding layer with downstream neural models for the original ABSA task and establish the new state-of-the-art results. It encourages us to explore the potential of using such contextualized embeddings to the more difficult but practical task, i.e. E2E-ABSA (the third setting in Table 1).4 Note that we are not aiming at developing a task-specific architecture, instead, our focus is to examine the potential of"
D19-5505,P18-1234,0,0.0213883,"problem. Precisely, the goal of AOWE is to extract the aspect-specific opinion words from the sentence given the aspect. The goal of E2E-ABSA is to jointly detect aspect terms/categories and the corresponding aspect sentiments. Many neural models composed of a taskagnostic pre-trained word embedding layer and task-specific neural architecture have been proposed for the original ABSA task (i.e. the aspectlevel sentiment classification) (Tang et al., 2016; Wang et al., 2016; Chen et al., 2017; Liu and Zhang, 2017; Ma et al., 2017, 2018b; Majumder et al., 2018; Li et al., 2018; He et al., 2018; Xue and Li, 2018; Wang et al., 2018; Fan et al., 2018; Huang and Carley, 2018; Lei et al., 2019; Li et al., 2019b)2 , but the improvement of these models measured by the accuracy or F1 score has reached a bottleneck. One reason is that the task-agnostic embedding layer, usually a linear layer initialized with Word2Vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014), only provides context-independent word-level features, which is insufficient for capturing the complex semantic dependencies in the sentence. Meanwhile, the size of existing datasets is too small to train sophisticated task-specific arch"
D19-5505,D15-1073,0,0.215456,"Missing"
D19-5505,D14-1179,0,\N,Missing
D19-5505,D16-1053,0,\N,Missing
D19-5505,D17-1047,1,\N,Missing
D19-5505,D18-1380,0,\N,Missing
D19-5505,D18-1377,0,\N,Missing
D19-5505,P18-1008,0,\N,Missing
D19-5505,D19-1550,0,\N,Missing
D19-5505,K19-1091,0,\N,Missing
N19-1292,I11-1130,0,0.463417,"f keyphrase generation and retrieval. The present keyphrases are bold. Introduction Keyphrases are short text pieces that can quickly express the key ideas of a given document. The keyphrase generation task aims at automatically generating a set of keyphrases given a document. As shown in the upper part of Figure 1, the input is a document and the output is a set of keyphrases. Due to the concise and precise expression, keyphrases are beneficial to extensive downstream applications such as text summarization (Zhang et al., 2004; Wang and Cardie, 2013), sentiment analysis (Wilson et al., 2005; Berend, 2011), and document clustering (Hulth and Megyesi, 2006; Hammouda et al., 2005). Existing methods on keyphrase generation can be divided into two categories: extractive and generative. Extractive methods (Medelyan et al., 2009; Mihalcea and Tarau, 2004; Zhang et al., 2016; Luan et al., 2017) identify present keyphrases that appear in the source text like “parameter control” in Figure 1. Although extractive methods are simple to implement, they cannot predict absent keyphrases which are not in the document like “optimization” in Figure 1. Generative methods (Meng et al., 2017; Chen et al., 2018a; Ye"
N19-1292,P18-1015,0,0.0204324,"., 2014) with copy mechanism (Gu et al., 2016) to generate keyphrases. CorrRNN (Chen et al., 2018a), an extension of CopyRNN, was proposed to model the correlations among keyphrases. This model utilizes hidden states and attention vectors of previously generated keyphrases to avoid generating repetitive keyphrases. The title information of the source text was explicitly exploited by Ye and Wang (2018) and Chen et al. (2018b) to further improve the performance. Ye and Wang (2018) first considered a semi-supervised setting for keyphrase generation. In contrast, inspired by Hsu et al. (2018) and Cao et al. (2018), we enhance existing generative methods by adopting an extractive model to assist the copy mechanism and exploiting external knowledge from retrieved keyphrases to help the generation. Furthermore, we also design a merging module to combine the predictions from different components. 3 Our Methodology As shown in Figure 2, our integrated framework consists of a retriever, two encoders, an extractor, a decoder, and a merging module. Given a document x, the retriever returns the keyphrases r retrieved from the training corpus. In addition to acting as keyphrase candidates, these retrieved keyphr"
N19-1292,D18-1439,0,0.503196,"., 2005; Berend, 2011), and document clustering (Hulth and Megyesi, 2006; Hammouda et al., 2005). Existing methods on keyphrase generation can be divided into two categories: extractive and generative. Extractive methods (Medelyan et al., 2009; Mihalcea and Tarau, 2004; Zhang et al., 2016; Luan et al., 2017) identify present keyphrases that appear in the source text like “parameter control” in Figure 1. Although extractive methods are simple to implement, they cannot predict absent keyphrases which are not in the document like “optimization” in Figure 1. Generative methods (Meng et al., 2017; Chen et al., 2018a; Ye and Wang, 2018; Yuan et al., 2018) adopt the wellknown encoder-decoder generative model (Luong et al., 2015; Bahdanau et al., 2014) with copy mechanism (Gu et al., 2016; See et al., 2017) to produce keyphrases. In a generative model, the decoder generates keyphrases word by word through either selecting from a predefined vocabulary according to a language model or copying from the source text according to the copy probability distribution computed by a copy mechanism. Thus, these generative methods are capable of generating both present and absent keyphrases. From a high-level perspectiv"
N19-1292,D14-1179,0,0.0181283,"Missing"
N19-1292,P16-1154,0,0.418088,"xtractive and generative. Extractive methods (Medelyan et al., 2009; Mihalcea and Tarau, 2004; Zhang et al., 2016; Luan et al., 2017) identify present keyphrases that appear in the source text like “parameter control” in Figure 1. Although extractive methods are simple to implement, they cannot predict absent keyphrases which are not in the document like “optimization” in Figure 1. Generative methods (Meng et al., 2017; Chen et al., 2018a; Ye and Wang, 2018; Yuan et al., 2018) adopt the wellknown encoder-decoder generative model (Luong et al., 2015; Bahdanau et al., 2014) with copy mechanism (Gu et al., 2016; See et al., 2017) to produce keyphrases. In a generative model, the decoder generates keyphrases word by word through either selecting from a predefined vocabulary according to a language model or copying from the source text according to the copy probability distribution computed by a copy mechanism. Thus, these generative methods are capable of generating both present and absent keyphrases. From a high-level perspective, extractive methods directly locate essential phrases in the docu2846 Proceedings of NAACL-HLT 2019, pages 2846–2856 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 A"
N19-1292,P18-1013,0,0.0189687,"model (Bahdanau et al., 2014) with copy mechanism (Gu et al., 2016) to generate keyphrases. CorrRNN (Chen et al., 2018a), an extension of CopyRNN, was proposed to model the correlations among keyphrases. This model utilizes hidden states and attention vectors of previously generated keyphrases to avoid generating repetitive keyphrases. The title information of the source text was explicitly exploited by Ye and Wang (2018) and Chen et al. (2018b) to further improve the performance. Ye and Wang (2018) first considered a semi-supervised setting for keyphrase generation. In contrast, inspired by Hsu et al. (2018) and Cao et al. (2018), we enhance existing generative methods by adopting an extractive model to assist the copy mechanism and exploiting external knowledge from retrieved keyphrases to help the generation. Furthermore, we also design a merging module to combine the predictions from different components. 3 Our Methodology As shown in Figure 2, our integrated framework consists of a retriever, two encoders, an extractor, a decoder, and a merging module. Given a document x, the retriever returns the keyphrases r retrieved from the training corpus. In addition to acting as keyphrase candidates,"
N19-1292,W03-1028,0,0.955872,"extraction approaches and sequence labeling models. Two-step extraction approaches first identify a set of candidate phrases from the document using different heuristics, such as the phrases that match specific part-of-speech (POS) tags (Liu et al., 2011; Wang et al., 2016; Le et al., 2016). Then, they learn a score for each candidate and select the top-ranked candidates as predicted keyphrases. The scores can be learned by 2847 Final Predictions either supervised methods with hand-crafted textual features (Medelyan et al., 2009; Witten et al., 1999; Nguyen and Kan, 2007; Frank et al., 1999; Hulth, 2003) or unsupervised graph ranking methods (Mihalcea and Tarau, 2004; Grineva et al., 2009; Wan and Xiao, 2008). Sequence labeling models are built on a recurrent neural network to sequentially go through a source text and learn the likelihood of each word in the source text to be a keyphrase word (Zhang et al., 2016; Luan et al., 2017; Gollapalli et al., 2017). In contrast to these extractive methods, our approach can generate both absent and present keyphrases. 2.2 Automatic Keyphrase Generation Keyphrase generation aims at predicting both present and absent keyphrases for a source text. Meng et"
N19-1292,P06-1068,0,0.758908,"e present keyphrases are bold. Introduction Keyphrases are short text pieces that can quickly express the key ideas of a given document. The keyphrase generation task aims at automatically generating a set of keyphrases given a document. As shown in the upper part of Figure 1, the input is a document and the output is a set of keyphrases. Due to the concise and precise expression, keyphrases are beneficial to extensive downstream applications such as text summarization (Zhang et al., 2004; Wang and Cardie, 2013), sentiment analysis (Wilson et al., 2005; Berend, 2011), and document clustering (Hulth and Megyesi, 2006; Hammouda et al., 2005). Existing methods on keyphrase generation can be divided into two categories: extractive and generative. Extractive methods (Medelyan et al., 2009; Mihalcea and Tarau, 2004; Zhang et al., 2016; Luan et al., 2017) identify present keyphrases that appear in the source text like “parameter control” in Figure 1. Although extractive methods are simple to implement, they cannot predict absent keyphrases which are not in the document like “optimization” in Figure 1. Generative methods (Meng et al., 2017; Chen et al., 2018a; Ye and Wang, 2018; Yuan et al., 2018) adopt the well"
N19-1292,P17-4012,0,0.0966559,"Missing"
N19-1292,W11-0316,0,0.347528,"m extractive, generative, and retrieval methods to further improve the performance; and (3) the new state-of-the-art performance on five real-world benchmarks. 2 2.1 Related Work Automatic Keyphrase Extraction Keyphrase extraction focuses on predicting the keyphrases that are present in the source text. Existing methods can mainly be categorized into two-step extraction approaches and sequence labeling models. Two-step extraction approaches first identify a set of candidate phrases from the document using different heuristics, such as the phrases that match specific part-of-speech (POS) tags (Liu et al., 2011; Wang et al., 2016; Le et al., 2016). Then, they learn a score for each candidate and select the top-ranked candidates as predicted keyphrases. The scores can be learned by 2847 Final Predictions either supervised methods with hand-crafted textual features (Medelyan et al., 2009; Witten et al., 1999; Nguyen and Kan, 2007; Frank et al., 1999; Hulth, 2003) or unsupervised graph ranking methods (Mihalcea and Tarau, 2004; Grineva et al., 2009; Wan and Xiao, 2008). Sequence labeling models are built on a recurrent neural network to sequentially go through a source text and learn the likelihood of"
N19-1292,D17-1279,0,0.304191,"hown in the upper part of Figure 1, the input is a document and the output is a set of keyphrases. Due to the concise and precise expression, keyphrases are beneficial to extensive downstream applications such as text summarization (Zhang et al., 2004; Wang and Cardie, 2013), sentiment analysis (Wilson et al., 2005; Berend, 2011), and document clustering (Hulth and Megyesi, 2006; Hammouda et al., 2005). Existing methods on keyphrase generation can be divided into two categories: extractive and generative. Extractive methods (Medelyan et al., 2009; Mihalcea and Tarau, 2004; Zhang et al., 2016; Luan et al., 2017) identify present keyphrases that appear in the source text like “parameter control” in Figure 1. Although extractive methods are simple to implement, they cannot predict absent keyphrases which are not in the document like “optimization” in Figure 1. Generative methods (Meng et al., 2017; Chen et al., 2018a; Ye and Wang, 2018; Yuan et al., 2018) adopt the wellknown encoder-decoder generative model (Luong et al., 2015; Bahdanau et al., 2014) with copy mechanism (Gu et al., 2016; See et al., 2017) to produce keyphrases. In a generative model, the decoder generates keyphrases word by word throug"
N19-1292,D15-1166,0,0.207695,"ds on keyphrase generation can be divided into two categories: extractive and generative. Extractive methods (Medelyan et al., 2009; Mihalcea and Tarau, 2004; Zhang et al., 2016; Luan et al., 2017) identify present keyphrases that appear in the source text like “parameter control” in Figure 1. Although extractive methods are simple to implement, they cannot predict absent keyphrases which are not in the document like “optimization” in Figure 1. Generative methods (Meng et al., 2017; Chen et al., 2018a; Ye and Wang, 2018; Yuan et al., 2018) adopt the wellknown encoder-decoder generative model (Luong et al., 2015; Bahdanau et al., 2014) with copy mechanism (Gu et al., 2016; See et al., 2017) to produce keyphrases. In a generative model, the decoder generates keyphrases word by word through either selecting from a predefined vocabulary according to a language model or copying from the source text according to the copy probability distribution computed by a copy mechanism. Thus, these generative methods are capable of generating both present and absent keyphrases. From a high-level perspective, extractive methods directly locate essential phrases in the docu2846 Proceedings of NAACL-HLT 2019, pages 2846"
N19-1292,D09-1137,0,0.817688,"t automatically generating a set of keyphrases given a document. As shown in the upper part of Figure 1, the input is a document and the output is a set of keyphrases. Due to the concise and precise expression, keyphrases are beneficial to extensive downstream applications such as text summarization (Zhang et al., 2004; Wang and Cardie, 2013), sentiment analysis (Wilson et al., 2005; Berend, 2011), and document clustering (Hulth and Megyesi, 2006; Hammouda et al., 2005). Existing methods on keyphrase generation can be divided into two categories: extractive and generative. Extractive methods (Medelyan et al., 2009; Mihalcea and Tarau, 2004; Zhang et al., 2016; Luan et al., 2017) identify present keyphrases that appear in the source text like “parameter control” in Figure 1. Although extractive methods are simple to implement, they cannot predict absent keyphrases which are not in the document like “optimization” in Figure 1. Generative methods (Meng et al., 2017; Chen et al., 2018a; Ye and Wang, 2018; Yuan et al., 2018) adopt the wellknown encoder-decoder generative model (Luong et al., 2015; Bahdanau et al., 2014) with copy mechanism (Gu et al., 2016; See et al., 2017) to produce keyphrases. In a gene"
N19-1292,P17-1054,0,0.657313,"lysis (Wilson et al., 2005; Berend, 2011), and document clustering (Hulth and Megyesi, 2006; Hammouda et al., 2005). Existing methods on keyphrase generation can be divided into two categories: extractive and generative. Extractive methods (Medelyan et al., 2009; Mihalcea and Tarau, 2004; Zhang et al., 2016; Luan et al., 2017) identify present keyphrases that appear in the source text like “parameter control” in Figure 1. Although extractive methods are simple to implement, they cannot predict absent keyphrases which are not in the document like “optimization” in Figure 1. Generative methods (Meng et al., 2017; Chen et al., 2018a; Ye and Wang, 2018; Yuan et al., 2018) adopt the wellknown encoder-decoder generative model (Luong et al., 2015; Bahdanau et al., 2014) with copy mechanism (Gu et al., 2016; See et al., 2017) to produce keyphrases. In a generative model, the decoder generates keyphrases word by word through either selecting from a predefined vocabulary according to a language model or copying from the source text according to the copy probability distribution computed by a copy mechanism. Thus, these generative methods are capable of generating both present and absent keyphrases. From a hi"
N19-1292,W04-3252,0,0.977193,"ing a set of keyphrases given a document. As shown in the upper part of Figure 1, the input is a document and the output is a set of keyphrases. Due to the concise and precise expression, keyphrases are beneficial to extensive downstream applications such as text summarization (Zhang et al., 2004; Wang and Cardie, 2013), sentiment analysis (Wilson et al., 2005; Berend, 2011), and document clustering (Hulth and Megyesi, 2006; Hammouda et al., 2005). Existing methods on keyphrase generation can be divided into two categories: extractive and generative. Extractive methods (Medelyan et al., 2009; Mihalcea and Tarau, 2004; Zhang et al., 2016; Luan et al., 2017) identify present keyphrases that appear in the source text like “parameter control” in Figure 1. Although extractive methods are simple to implement, they cannot predict absent keyphrases which are not in the document like “optimization” in Figure 1. Generative methods (Meng et al., 2017; Chen et al., 2018a; Ye and Wang, 2018; Yuan et al., 2018) adopt the wellknown encoder-decoder generative model (Luong et al., 2015; Bahdanau et al., 2014) with copy mechanism (Gu et al., 2016; See et al., 2017) to produce keyphrases. In a generative model, the decoder"
N19-1292,D16-1244,0,0.0701476,"Missing"
N19-1292,P17-1099,0,0.513047,"erative. Extractive methods (Medelyan et al., 2009; Mihalcea and Tarau, 2004; Zhang et al., 2016; Luan et al., 2017) identify present keyphrases that appear in the source text like “parameter control” in Figure 1. Although extractive methods are simple to implement, they cannot predict absent keyphrases which are not in the document like “optimization” in Figure 1. Generative methods (Meng et al., 2017; Chen et al., 2018a; Ye and Wang, 2018; Yuan et al., 2018) adopt the wellknown encoder-decoder generative model (Luong et al., 2015; Bahdanau et al., 2014) with copy mechanism (Gu et al., 2016; See et al., 2017) to produce keyphrases. In a generative model, the decoder generates keyphrases word by word through either selecting from a predefined vocabulary according to a language model or copying from the source text according to the copy probability distribution computed by a copy mechanism. Thus, these generative methods are capable of generating both present and absent keyphrases. From a high-level perspective, extractive methods directly locate essential phrases in the docu2846 Proceedings of NAACL-HLT 2019, pages 2846–2856 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Comp"
N19-1292,P13-1137,0,0.378229,"Missing"
N19-1292,H05-1044,0,0.0187342,"igure 1: An example of keyphrase generation and retrieval. The present keyphrases are bold. Introduction Keyphrases are short text pieces that can quickly express the key ideas of a given document. The keyphrase generation task aims at automatically generating a set of keyphrases given a document. As shown in the upper part of Figure 1, the input is a document and the output is a set of keyphrases. Due to the concise and precise expression, keyphrases are beneficial to extensive downstream applications such as text summarization (Zhang et al., 2004; Wang and Cardie, 2013), sentiment analysis (Wilson et al., 2005; Berend, 2011), and document clustering (Hulth and Megyesi, 2006; Hammouda et al., 2005). Existing methods on keyphrase generation can be divided into two categories: extractive and generative. Extractive methods (Medelyan et al., 2009; Mihalcea and Tarau, 2004; Zhang et al., 2016; Luan et al., 2017) identify present keyphrases that appear in the source text like “parameter control” in Figure 1. Although extractive methods are simple to implement, they cannot predict absent keyphrases which are not in the document like “optimization” in Figure 1. Generative methods (Meng et al., 2017; Chen et"
N19-1292,D18-1447,0,0.695877,"1), and document clustering (Hulth and Megyesi, 2006; Hammouda et al., 2005). Existing methods on keyphrase generation can be divided into two categories: extractive and generative. Extractive methods (Medelyan et al., 2009; Mihalcea and Tarau, 2004; Zhang et al., 2016; Luan et al., 2017) identify present keyphrases that appear in the source text like “parameter control” in Figure 1. Although extractive methods are simple to implement, they cannot predict absent keyphrases which are not in the document like “optimization” in Figure 1. Generative methods (Meng et al., 2017; Chen et al., 2018a; Ye and Wang, 2018; Yuan et al., 2018) adopt the wellknown encoder-decoder generative model (Luong et al., 2015; Bahdanau et al., 2014) with copy mechanism (Gu et al., 2016; See et al., 2017) to produce keyphrases. In a generative model, the decoder generates keyphrases word by word through either selecting from a predefined vocabulary according to a language model or copying from the source text according to the copy probability distribution computed by a copy mechanism. Thus, these generative methods are capable of generating both present and absent keyphrases. From a high-level perspective, extractive method"
N19-1292,D16-1080,0,0.371743,"ven a document. As shown in the upper part of Figure 1, the input is a document and the output is a set of keyphrases. Due to the concise and precise expression, keyphrases are beneficial to extensive downstream applications such as text summarization (Zhang et al., 2004; Wang and Cardie, 2013), sentiment analysis (Wilson et al., 2005; Berend, 2011), and document clustering (Hulth and Megyesi, 2006; Hammouda et al., 2005). Existing methods on keyphrase generation can be divided into two categories: extractive and generative. Extractive methods (Medelyan et al., 2009; Mihalcea and Tarau, 2004; Zhang et al., 2016; Luan et al., 2017) identify present keyphrases that appear in the source text like “parameter control” in Figure 1. Although extractive methods are simple to implement, they cannot predict absent keyphrases which are not in the document like “optimization” in Figure 1. Generative methods (Meng et al., 2017; Chen et al., 2018a; Ye and Wang, 2018; Yuan et al., 2018) adopt the wellknown encoder-decoder generative model (Luong et al., 2015; Bahdanau et al., 2014) with copy mechanism (Gu et al., 2016; See et al., 2017) to produce keyphrases. In a generative model, the decoder generates keyphrases"
P15-1153,P13-1020,0,0.0682145,"s. Thus, the selected sentences may still contain unimportant or redundant phrases. Compression-based approaches have been investigated to alleviate the above limitation. As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 2009). The first step selects the sentences, and the second step removes the unimportant or redundant units from the sentences. Recently, integrated models have been proposed that jointly conduct sentence extraction and compression (Martins and Smith, 2009; Woodsend and Lapata, 2010; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011; Li et al., 2015). Note that our model also jointly conducts phrase selection and phrase merging (new sentence generation). Nonetheless, compressive methods are unable to merge the related facts from different sentences. On the other hand, abstraction-based approaches can generate new sentences based on the facts from different source sentences. In addition to the previously mentioned sentence fusion work, new directions have been explored. Researchers developed an information extraction based approach that extracts information items (Genest and Lapalme, 2011) o"
P15-1153,J05-3002,0,0.423913,"compression-based method. Yet, these compressive summarization models cannot merge facts from different source sentences, because all the words in a summary sentence are solely from one source sentence. In fact, previous investigations show that human-written summaries are more abstractive, which can be regarded as a result of sentence aggregation and fusion (Cheung and Penn, 2013; Jing and McKeown, 2000). Some works, albeit less popular, have studied abstraction-based approach that can construct a sentence whose fragments come from different source sentences. One important work developed by Barzilay and McKeown (2005) employed sentence fusion, followed by (Filippova and Strube, 2008; Filippova, 2010). These works first conduct clustering on sentences to compute the salience of topical themes. Then, sentence fusion is applied within each cluster of related sentences to generate a new sentence containing common information units of the sentences. The abstractive-based approaches gather information across sentence boundary, and hence have the potential to cover more content in a more concise manner. In this paper, we propose an abstractive MDS framework that can construct new sentences by 1587 Proceedings of"
P15-1153,P14-1086,0,0.0209135,"mmary by rewriting the noun phrases or people references in the summaries (Nenkova, 2008; Siddharthan et al., 2011). Sentence generation with word graph was applied for summarizing customer opinions and chat conversations (Ganesan et al., 2010; Mehdad et al., 2014). Recently, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011). Researchers also explored some variants of the typical MDS setting, such as query-chain focused summarization that combines aspects of update summarization and query-focused summarization (Baumel et al., 2014), and hierarchical summarization that scales up MDS to summarize a large set of documents (Christensen et al., 2014). A data-driven method for mining sentence structures on large news archive was proposed and utilized to summarize unseen news events (Pighin et al., 2014). Moreover, some works (Liu et al., 2012; K˚ageb¨ack et al., 2014; Denil et al., 2014; Cao et al., 2015) utilized deep learning techniques to tackle some summarization tasks. 6 Conclusions and Future Work We propose an abstractive MDS framework that constructs new sentences by exploring more finegrained syntactic units, namely,"
P15-1153,P11-1049,0,0.225126,"nces may still contain unimportant or redundant phrases. Compression-based approaches have been investigated to alleviate the above limitation. As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 2009). The first step selects the sentences, and the second step removes the unimportant or redundant units from the sentences. Recently, integrated models have been proposed that jointly conduct sentence extraction and compression (Martins and Smith, 2009; Woodsend and Lapata, 2010; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011; Li et al., 2015). Note that our model also jointly conducts phrase selection and phrase merging (new sentence generation). Nonetheless, compressive methods are unable to merge the related facts from different sentences. On the other hand, abstraction-based approaches can generate new sentences based on the facts from different source sentences. In addition to the previously mentioned sentence fusion work, new directions have been explored. Researchers developed an information extraction based approach that extracts information items (Genest and Lapalme, 2011) or abstraction schemes (Genest a"
P15-1153,P13-1121,0,0.0381358,"n in the summary. To this end, some researchers apply compression on the selected sentences by deleting words or phrases (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2006; Harabagiu and Lacatusu, 2010; Li et al., 2015), which is the compression-based method. Yet, these compressive summarization models cannot merge facts from different source sentences, because all the words in a summary sentence are solely from one source sentence. In fact, previous investigations show that human-written summaries are more abstractive, which can be regarded as a result of sentence aggregation and fusion (Cheung and Penn, 2013; Jing and McKeown, 2000). Some works, albeit less popular, have studied abstraction-based approach that can construct a sentence whose fragments come from different source sentences. One important work developed by Barzilay and McKeown (2005) employed sentence fusion, followed by (Filippova and Strube, 2008; Filippova, 2010). These works first conduct clustering on sentences to compute the salience of topical themes. Then, sentence fusion is applied within each cluster of related sentences to generate a new sentence containing common information units of the sentences. The abstractive-based a"
P15-1153,P14-1085,0,0.00660705,"011). Sentence generation with word graph was applied for summarizing customer opinions and chat conversations (Ganesan et al., 2010; Mehdad et al., 2014). Recently, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011). Researchers also explored some variants of the typical MDS setting, such as query-chain focused summarization that combines aspects of update summarization and query-focused summarization (Baumel et al., 2014), and hierarchical summarization that scales up MDS to summarize a large set of documents (Christensen et al., 2014). A data-driven method for mining sentence structures on large news archive was proposed and utilized to summarize unseen news events (Pighin et al., 2014). Moreover, some works (Liu et al., 2012; K˚ageb¨ack et al., 2014; Denil et al., 2014; Cao et al., 2015) utilized deep learning techniques to tackle some summarization tasks. 6 Conclusions and Future Work We propose an abstractive MDS framework that constructs new sentences by exploring more finegrained syntactic units, namely, noun phrases and verb phrases. The designed optimization framework operates on the summary level so that more compl"
P15-1153,C04-1057,0,0.0172214,"011; Goldstein et al., 2000; Wan et al., 2007). Each sentence in the documents is firstly assigned a salience score. Then, sentence selection is performed by greedily selecting the sentence with the largest salience score among the remaining ones. The redundancy is controlled during the selection by penalizing the remaining ones according to their similarity with the selected sentences. An obvious drawback of such greedy strategy is that it is easily trapped in local optima. Later, unified models are proposed to conduct sentence selection and redundancy control simultaneously (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Yih et al., 2007; Gillick et al., 2007; Lin and Bilmes, 2010; Lin and Bilmes, 2012; Sipos et al., 2012). However, extraction-based approaches are unable to evaluate the salience and control the redundancy on the granularity finer than sentences. Thus, the selected sentences may still contain unimportant or redundant phrases. Compression-based approaches have been investigated to alleviate the above limitation. As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 2009). The first step selects the senten"
P15-1153,D08-1019,0,0.237574,"els cannot merge facts from different source sentences, because all the words in a summary sentence are solely from one source sentence. In fact, previous investigations show that human-written summaries are more abstractive, which can be regarded as a result of sentence aggregation and fusion (Cheung and Penn, 2013; Jing and McKeown, 2000). Some works, albeit less popular, have studied abstraction-based approach that can construct a sentence whose fragments come from different source sentences. One important work developed by Barzilay and McKeown (2005) employed sentence fusion, followed by (Filippova and Strube, 2008; Filippova, 2010). These works first conduct clustering on sentences to compute the salience of topical themes. Then, sentence fusion is applied within each cluster of related sentences to generate a new sentence containing common information units of the sentences. The abstractive-based approaches gather information across sentence boundary, and hence have the potential to cover more content in a more concise manner. In this paper, we propose an abstractive MDS framework that can construct new sentences by 1587 Proceedings of the 53rd Annual Meeting of the Association for Computational Lingu"
P15-1153,C10-1037,0,0.00615543,"different source sentences, because all the words in a summary sentence are solely from one source sentence. In fact, previous investigations show that human-written summaries are more abstractive, which can be regarded as a result of sentence aggregation and fusion (Cheung and Penn, 2013; Jing and McKeown, 2000). Some works, albeit less popular, have studied abstraction-based approach that can construct a sentence whose fragments come from different source sentences. One important work developed by Barzilay and McKeown (2005) employed sentence fusion, followed by (Filippova and Strube, 2008; Filippova, 2010). These works first conduct clustering on sentences to compute the salience of topical themes. Then, sentence fusion is applied within each cluster of related sentences to generate a new sentence containing common information units of the sentences. The abstractive-based approaches gather information across sentence boundary, and hence have the potential to cover more content in a more concise manner. In this paper, we propose an abstractive MDS framework that can construct new sentences by 1587 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th"
P15-1153,C10-1039,0,0.0485824,"on to the previously mentioned sentence fusion work, new directions have been explored. Researchers developed an information extraction based approach that extracts information items (Genest and Lapalme, 2011) or abstraction schemes (Genest and Lapalme, 2012) as components for generating sentences. Summary revision was also investigated to improve the quality of automatic summary by rewriting the noun phrases or people references in the summaries (Nenkova, 2008; Siddharthan et al., 2011). Sentence generation with word graph was applied for summarizing customer opinions and chat conversations (Ganesan et al., 2010; Mehdad et al., 2014). Recently, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011). Researchers also explored some variants of the typical MDS setting, such as query-chain focused summarization that combines aspects of update summarization and query-focused summarization (Baumel et al., 2014), and hierarchical summarization that scales up MDS to summarize a large set of documents (Christensen et al., 2014). A data-driven method for mining sentence structures on large news archive was proposed and utilized to su"
P15-1153,W11-1608,0,0.254617,"; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011; Li et al., 2015). Note that our model also jointly conducts phrase selection and phrase merging (new sentence generation). Nonetheless, compressive methods are unable to merge the related facts from different sentences. On the other hand, abstraction-based approaches can generate new sentences based on the facts from different source sentences. In addition to the previously mentioned sentence fusion work, new directions have been explored. Researchers developed an information extraction based approach that extracts information items (Genest and Lapalme, 2011) or abstraction schemes (Genest and Lapalme, 2012) as components for generating sentences. Summary revision was also investigated to improve the quality of automatic summary by rewriting the noun phrases or people references in the summaries (Nenkova, 2008; Siddharthan et al., 2011). Sentence generation with word graph was applied for summarizing customer opinions and chat conversations (Ganesan et al., 2010; Mehdad et al., 2014). Recently, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011). Researchers also expl"
P15-1153,P12-2069,0,0.0121625,"l., 2011; Li et al., 2015). Note that our model also jointly conducts phrase selection and phrase merging (new sentence generation). Nonetheless, compressive methods are unable to merge the related facts from different sentences. On the other hand, abstraction-based approaches can generate new sentences based on the facts from different source sentences. In addition to the previously mentioned sentence fusion work, new directions have been explored. Researchers developed an information extraction based approach that extracts information items (Genest and Lapalme, 2011) or abstraction schemes (Genest and Lapalme, 2012) as components for generating sentences. Summary revision was also investigated to improve the quality of automatic summary by rewriting the noun phrases or people references in the summaries (Nenkova, 2008; Siddharthan et al., 2011). Sentence generation with word graph was applied for summarizing customer opinions and chat conversations (Ganesan et al., 2010; Mehdad et al., 2014). Recently, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011). Researchers also explored some variants of the typical MDS setting, suc"
P15-1153,W09-1802,0,0.203135,"eously (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Yih et al., 2007; Gillick et al., 2007; Lin and Bilmes, 2010; Lin and Bilmes, 2012; Sipos et al., 2012). However, extraction-based approaches are unable to evaluate the salience and control the redundancy on the granularity finer than sentences. Thus, the selected sentences may still contain unimportant or redundant phrases. Compression-based approaches have been investigated to alleviate the above limitation. As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 2009). The first step selects the sentences, and the second step removes the unimportant or redundant units from the sentences. Recently, integrated models have been proposed that jointly conduct sentence extraction and compression (Martins and Smith, 2009; Woodsend and Lapata, 2010; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011; Li et al., 2015). Note that our model also jointly conducts phrase selection and phrase merging (new sentence generation). Nonetheless, compressive methods are unable to merge the related facts from different sentences. On the other hand, abstraction-based appro"
P15-1153,W00-0405,0,0.425057,"ch as “sent the boys outside”, “authorities said”, etc. In addition, the VP “killing five girls” of the original sentence with ID 64 is also excluded since it has significant redundancy with the summary sentence with ID 1. 5 Related Work Existing multi-document summarization (MDS) works can be classified into three categories: 1594 extraction-based approaches, compression-based approaches, and abstraction-based approaches. Extraction-based approaches are the most studied of the three. Early studies mainly followed a greedy strategy in sentence selection (C ¸ elikyilmaz and Hakkani-T¨ur, 2011; Goldstein et al., 2000; Wan et al., 2007). Each sentence in the documents is firstly assigned a salience score. Then, sentence selection is performed by greedily selecting the sentence with the largest salience score among the remaining ones. The redundancy is controlled during the selection by penalizing the remaining ones according to their similarity with the selected sentences. An obvious drawback of such greedy strategy is that it is easily trapped in local optima. Later, unified models are proposed to conduct sentence selection and redundancy control simultaneously (McDonald, 2007; Filatova and Hatzivassilogl"
P15-1153,P12-1091,1,0.836203,"rkshops such as TAC, the pyramid is used as the major metric. Since manual pyramid evaluation is timeconsuming, and the exact evaluation scores are not reproducible especially when the assessors for our results are different from those of TAC, we employ the automated version of pyramid proposed in (Passonneau et al., 2013). The automated pyramid scoring procedure relies on distributional semantics to assign SCUs to a target summary. Specifically, all n-grams within sentence bounds are extracted, and converted into 100 dimension latent topical vectors via a weighted matrix factorization model (Guo and Diab, 2012). Similarly, the contributors and the label of an SCU are transformed into 100 dimensional vector representations. An SCU is assigned to a summary if there exists an n-gram such that the similarity score between the SCU low dimensional vector and the n-gram low dimensional vector exceeds a threshold. Passonneau et al. (2013) showed that the distributional similarity based method produces automated scores that correlate well with manual pyramid scores, yielding more accurate pyramid scores than string matching based automated methods (Harnly et al., 2005). In this paper, we adopt the same setti"
P15-1153,A00-2024,0,0.188299,"is end, some researchers apply compression on the selected sentences by deleting words or phrases (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2006; Harabagiu and Lacatusu, 2010; Li et al., 2015), which is the compression-based method. Yet, these compressive summarization models cannot merge facts from different source sentences, because all the words in a summary sentence are solely from one source sentence. In fact, previous investigations show that human-written summaries are more abstractive, which can be regarded as a result of sentence aggregation and fusion (Cheung and Penn, 2013; Jing and McKeown, 2000). Some works, albeit less popular, have studied abstraction-based approach that can construct a sentence whose fragments come from different source sentences. One important work developed by Barzilay and McKeown (2005) employed sentence fusion, followed by (Filippova and Strube, 2008; Filippova, 2010). These works first conduct clustering on sentences to compute the salience of topical themes. Then, sentence fusion is applied within each cluster of related sentences to generate a new sentence containing common information units of the sentences. The abstractive-based approaches gather informat"
P15-1153,W14-1504,0,0.00804706,"Missing"
P15-1153,P03-1054,0,0.0033611,"en we formulate the sentence generation task as an optimization problem, and design constraints. In the end, we perform several post-processing steps to improve the order and the readability of the generated sentences. 2.1 Phrase Salience Calculation The first component decomposes the sentences in documents into a set of noun phrases (NPs) derived from the subject parts of a constituency tree and a set of verb-object phrases (VPs), representing potential key concepts and key facts, respectively. These phrases will serve as the basic elements for sentence generation. We employ Stanford parser (Klein and Manning, 2003) to obtain a constituency tree for each input sentence. After that, we extract NPs and VPs from the tree as follows: (1) The NPs and VPs that are the direct children of the sentence node (repre1588 sented by the S node) are extracted. (2) VPs (NPs) in a path on which all the nodes are VPs (NPs) are also recursively extracted and regarded as having the same parent node S. Recursive operation in the second step will only be carried out in two levels since the phrases in the lower levels may not be able to convey a complete fact. Take the tree in Figure 1 as an example, the corresponding sentence"
P15-1153,J13-4004,0,0.00850382,"ts are jointly considered. 2.2.1 Compatibility Relation Compatibility relation is designed to indicate whether an NP and a VP can be used to form a new sentence. For example, the NP “Police” from another sentence should not be the subject of the VP “sent the boys outside” extracted from Figure 1. We use some heuristics to find compatibility, and then expand the compatibility relation to more phrases by extracting coreference. To find coreference NPs (different mentions for the same entity), we first conduct coreference resolution for each document with Stanford coreference resolution package (Lee et al., 2013). We adopt those resolution rules that are able to achieve high quality and address our need for summarization. In particular, Sieve 1, 2, 3, 4, 5, 9, and 10 in the package are used. A set of clusters are obtained and each cluster contains the mentions that refer to the same entity in a document. The clusters from different documents in the same topic are merged by matching the named entities. After merging, the mentions that are not NPs extracted in the phrase extraction step are removed in each cluster. Two NPs in the same cluster are determined as alternative of each other. To find alternat"
P15-1153,N10-1134,0,0.119631,"s is firstly assigned a salience score. Then, sentence selection is performed by greedily selecting the sentence with the largest salience score among the remaining ones. The redundancy is controlled during the selection by penalizing the remaining ones according to their similarity with the selected sentences. An obvious drawback of such greedy strategy is that it is easily trapped in local optima. Later, unified models are proposed to conduct sentence selection and redundancy control simultaneously (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Yih et al., 2007; Gillick et al., 2007; Lin and Bilmes, 2010; Lin and Bilmes, 2012; Sipos et al., 2012). However, extraction-based approaches are unable to evaluate the salience and control the redundancy on the granularity finer than sentences. Thus, the selected sentences may still contain unimportant or redundant phrases. Compression-based approaches have been investigated to alleviate the above limitation. As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 2009). The first step selects the sentences, and the second step removes the unimportant or redundant"
P15-1153,N03-1020,0,0.168226,"as to recognize alternate realizations of the same meaning. Different weights are assigned to SCUs based on their frequency in model summaries. A weighted inventory of SCUs named a pyramid is created, which constitutes a resource for investigating alternate realizations of the same meaning. Such property makes pyramid method more suitable to evaluSystem Our 22 43 17 Auto-pyr (Th: .6) 0.905 0.878 0.875 0.860 Auto-pyr (Th: .65) 0.793 0.775 0.756 0.741 Rank in TAC 2011 NA 1 2 3 Table 2: Comparison with the top 3 systems in TAC 2011. ate summaries. Another widely used evaluation metric is ROUGE (Lin and Hovy, 2003) and it evaluates summaries from word overlapping perspective. Because of the strict string matching, it ignores the semantic content units and performs better when larger sets of model summaries are available. In contrast to ROUGE, pyramid scoring is robust with as few as four model summaries (Nenkova and Passonneau, 2004). Therefore, in recent summarization evaluation workshops such as TAC, the pyramid is used as the major metric. Since manual pyramid evaluation is timeconsuming, and the exact evaluation scores are not reproducible especially when the assessors for our results are different"
P15-1153,W03-1101,0,0.416943,"the Hong Kong Special Administrative Region, China (Project Codes: 413510 and 14203414). The work was done when Weiwei Guo was in Columbia University summarization systems adopt the extractionbased approach which selects some original sentences from the source documents to create a short summary (Erkan and Radev, 2004; Wan et al., 2007). However, the restriction that the whole sentence should be selected potentially yields some overlapping information in the summary. To this end, some researchers apply compression on the selected sentences by deleting words or phrases (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2006; Harabagiu and Lacatusu, 2010; Li et al., 2015), which is the compression-based method. Yet, these compressive summarization models cannot merge facts from different source sentences, because all the words in a summary sentence are solely from one source sentence. In fact, previous investigations show that human-written summaries are more abstractive, which can be regarded as a result of sentence aggregation and fusion (Cheung and Penn, 2013; Jing and McKeown, 2000). Some works, albeit less popular, have studied abstraction-based approach that can construct a sentence whos"
P15-1153,W09-1801,0,0.0671521,"he redundancy on the granularity finer than sentences. Thus, the selected sentences may still contain unimportant or redundant phrases. Compression-based approaches have been investigated to alleviate the above limitation. As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 2009). The first step selects the sentences, and the second step removes the unimportant or redundant units from the sentences. Recently, integrated models have been proposed that jointly conduct sentence extraction and compression (Martins and Smith, 2009; Woodsend and Lapata, 2010; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011; Li et al., 2015). Note that our model also jointly conducts phrase selection and phrase merging (new sentence generation). Nonetheless, compressive methods are unable to merge the related facts from different sentences. On the other hand, abstraction-based approaches can generate new sentences based on the facts from different source sentences. In addition to the previously mentioned sentence fusion work, new directions have been explored. Researchers developed an information extraction based approach that e"
P15-1153,P14-1115,0,0.0357106,"Missing"
P15-1153,N04-1019,1,0.536063,"f concepts and facts represented by NPs and VPs from the input documents. A salience score is computed for each phrase by exploiting redundancy of the document content in a global manner. The second component constructs new sentences by selecting and merging phrases based on their salience scores, and ensures the validity of new sentences using a integer linear optimization model. The contribution of this paper is two folds. (1) We extract NPs/VPs from constituency trees to represent key concepts/facts, and merge them to construct new sentences, which allows more summary content units (SCUs) (Nenkova and Passonneau, 2004) to be included in a sentence by breaking the original sentence boundaries. (2) The designed optimization framework for addressing the problem is unique and effective. Our optimization algorithm simultaneously selects and merges a set of phrases that maximize the number of covered SCUs in a summary. Meanwhile, since the basic unit is phrases, we design compatibility relations among NPs and VPs, as well as other optimization constraints, to ensure that the generated sentences contain correct facts. Compared with the sentence fusion approaches that compute salience scores of sentence clusters, o"
P15-1153,P14-1084,0,0.113882,"ntly, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011). Researchers also explored some variants of the typical MDS setting, such as query-chain focused summarization that combines aspects of update summarization and query-focused summarization (Baumel et al., 2014), and hierarchical summarization that scales up MDS to summarize a large set of documents (Christensen et al., 2014). A data-driven method for mining sentence structures on large news archive was proposed and utilized to summarize unseen news events (Pighin et al., 2014). Moreover, some works (Liu et al., 2012; K˚ageb¨ack et al., 2014; Denil et al., 2014; Cao et al., 2015) utilized deep learning techniques to tackle some summarization tasks. 6 Conclusions and Future Work We propose an abstractive MDS framework that constructs new sentences by exploring more finegrained syntactic units, namely, noun phrases and verb phrases. The designed optimization framework operates on the summary level so that more complementary semantic content units can be incorporated. The phrase selection and merging is done simultaneously to achieve global optimal. Meanwhile, the cons"
P15-1153,J11-4007,0,0.0159621,"the other hand, abstraction-based approaches can generate new sentences based on the facts from different source sentences. In addition to the previously mentioned sentence fusion work, new directions have been explored. Researchers developed an information extraction based approach that extracts information items (Genest and Lapalme, 2011) or abstraction schemes (Genest and Lapalme, 2012) as components for generating sentences. Summary revision was also investigated to improve the quality of automatic summary by rewriting the noun phrases or people references in the summaries (Nenkova, 2008; Siddharthan et al., 2011). Sentence generation with word graph was applied for summarizing customer opinions and chat conversations (Ganesan et al., 2010; Mehdad et al., 2014). Recently, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011). Researchers also explored some variants of the typical MDS setting, such as query-chain focused summarization that combines aspects of update summarization and query-focused summarization (Baumel et al., 2014), and hierarchical summarization that scales up MDS to summarize a large set of documents (Chri"
P15-1153,E12-1023,0,0.123518,", sentence selection is performed by greedily selecting the sentence with the largest salience score among the remaining ones. The redundancy is controlled during the selection by penalizing the remaining ones according to their similarity with the selected sentences. An obvious drawback of such greedy strategy is that it is easily trapped in local optima. Later, unified models are proposed to conduct sentence selection and redundancy control simultaneously (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Yih et al., 2007; Gillick et al., 2007; Lin and Bilmes, 2010; Lin and Bilmes, 2012; Sipos et al., 2012). However, extraction-based approaches are unable to evaluate the salience and control the redundancy on the granularity finer than sentences. Thus, the selected sentences may still contain unimportant or redundant phrases. Compression-based approaches have been investigated to alleviate the above limitation. As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 2009). The first step selects the sentences, and the second step removes the unimportant or redundant units from the sentences. Recently, integra"
P15-1153,P10-1058,0,0.0225702,"ularity finer than sentences. Thus, the selected sentences may still contain unimportant or redundant phrases. Compression-based approaches have been investigated to alleviate the above limitation. As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 2009). The first step selects the sentences, and the second step removes the unimportant or redundant units from the sentences. Recently, integrated models have been proposed that jointly conduct sentence extraction and compression (Martins and Smith, 2009; Woodsend and Lapata, 2010; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011; Li et al., 2015). Note that our model also jointly conducts phrase selection and phrase merging (new sentence generation). Nonetheless, compressive methods are unable to merge the related facts from different sentences. On the other hand, abstraction-based approaches can generate new sentences based on the facts from different source sentences. In addition to the previously mentioned sentence fusion work, new directions have been explored. Researchers developed an information extraction based approach that extracts information items ("
P15-1153,D12-1022,0,0.286119,"n”, “walked into an Amish school, sent the boys outside and tied up and shot the girls, killing three of them”, “walked into an Amish school”, “sent the boys outside”, and “tied up and shot the girls, killing three of them”. 1 Because of the recursive operation, the extracted phrases may have overlaps. Later, we will show how to avoid such overlapping in phrase selection. A salience score is calculated for each phrase to indicate its importance. Different types of salience can be incorporated in our framework, such as position-based method (Yih et al., 2007), statistical feature based method (Woodsend and Lapata, 2012), concept-based method (Li et al., 2011), etc. One key characteristic of our approach is that the considered basic units are phrases instead of sentences. Such finer granularity leaves more room for better global salience score by potentially covering more distinct facts. In our implementation, we adopt a concept-based weight incorporating the position information. The concept set is designated to be the union set of unigrams, bigrams, and named entities in the documents. We remove stopwords and perform lemmatization before extracting unigrams and bigrams. The position-based term frequency is"
P15-1153,I08-1016,0,0.0590528,"sentences. On the other hand, abstraction-based approaches can generate new sentences based on the facts from different source sentences. In addition to the previously mentioned sentence fusion work, new directions have been explored. Researchers developed an information extraction based approach that extracts information items (Genest and Lapalme, 2011) or abstraction schemes (Genest and Lapalme, 2012) as components for generating sentences. Summary revision was also investigated to improve the quality of automatic summary by rewriting the noun phrases or people references in the summaries (Nenkova, 2008; Siddharthan et al., 2011). Sentence generation with word graph was applied for summarizing customer opinions and chat conversations (Ganesan et al., 2010; Mehdad et al., 2014). Recently, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011). Researchers also explored some variants of the typical MDS setting, such as query-chain focused summarization that combines aspects of update summarization and query-focused summarization (Baumel et al., 2014), and hierarchical summarization that scales up MDS to summarize a l"
P15-1153,P14-1087,0,0.0220839,"tion based approach that extracts information items (Genest and Lapalme, 2011) or abstraction schemes (Genest and Lapalme, 2012) as components for generating sentences. Summary revision was also investigated to improve the quality of automatic summary by rewriting the noun phrases or people references in the summaries (Nenkova, 2008; Siddharthan et al., 2011). Sentence generation with word graph was applied for summarizing customer opinions and chat conversations (Ganesan et al., 2010; Mehdad et al., 2014). Recently, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011). Researchers also explored some variants of the typical MDS setting, such as query-chain focused summarization that combines aspects of update summarization and query-focused summarization (Baumel et al., 2014), and hierarchical summarization that scales up MDS to summarize a large set of documents (Christensen et al., 2014). A data-driven method for mining sentence structures on large news archive was proposed and utilized to summarize unseen news events (Pighin et al., 2014). Moreover, some works (Liu et al., 2012; K˚ageb¨ack et al., 2014; Denil et al"
P15-1153,P13-2026,1,\N,Missing
P16-1064,D10-1005,0,0.0262438,"ics at the vocabulary level, which are different from that of Zhang et al. (2010) and ours. The model in Zhang et al. (2010) adds the constraints of word translation pairs into PLSA. These constraints cannot handle the original word co-occurrences well. In contrast, we consider the language gap by incorporating word distributions from the other language, capturing the common semantics on the topic level. Moreover, we use a fully Bayesian paradigm with a prior distribution. Some existing topic methods conduct crosslingual sentiment analysis (Lu et al., 2011; Guo et al., 2010; Lin et al., 2014; Boyd-Graber and Resnik, 2010). These models are not suitable for our CTD task because they mainly detect common elements related to product aspects. Moreover some works focus more on detecting sentiments. 3 3.1 θde zne α θdc znc wne ϕe ηe e Ndw Nde wnc β ϕc ηc c Ndw Ndc K Figure 1: Our proposed graphical model butions ηe , with dimension Λe , and ηc , with dimension Λc , to help the generation of ϕek and ϕck . Precisely, we generate ηe and ηc from the Dirichlet prior distributions Dir(β ·1|Λe |) and Dir(β ·1|Λc |) respectively, where 1D denotes a D-dimensional vector whose components are 1. Then we draw ϕek from the mixtu"
P16-1064,P11-1033,0,0.0155455,"um´e (2010) focus on mining the correspondence of topics at the vocabulary level, which are different from that of Zhang et al. (2010) and ours. The model in Zhang et al. (2010) adds the constraints of word translation pairs into PLSA. These constraints cannot handle the original word co-occurrences well. In contrast, we consider the language gap by incorporating word distributions from the other language, capturing the common semantics on the topic level. Moreover, we use a fully Bayesian paradigm with a prior distribution. Some existing topic methods conduct crosslingual sentiment analysis (Lu et al., 2011; Guo et al., 2010; Lin et al., 2014; Boyd-Graber and Resnik, 2010). These models are not suitable for our CTD task because they mainly detect common elements related to product aspects. Moreover some works focus more on detecting sentiments. 3 3.1 θde zne α θdc znc wne ϕe ηe e Ndw Nde wnc β ϕc ηc c Ndw Ndc K Figure 1: Our proposed graphical model butions ηe , with dimension Λe , and ηc , with dimension Λc , to help the generation of ϕek and ϕck . Precisely, we generate ηe and ηc from the Dirichlet prior distributions Dir(β ·1|Λe |) and Dir(β ·1|Λc |) respectively, where 1D denotes a D-dimensi"
P16-1064,P10-1115,0,0.379116,"s “plane:飞 机 ocean:海洋 . . . ”. The assumption of one-toone mapping of words has some drawbacks. One drawback is that the correspondence of identified common topics is restricted to the vocabulary level. Another drawback is that the one-toone mapping of words cannot fit the original word occurrences well. For example, the English term “plane” appears in the English documents frequently while the Chinese translation “飞机” appears less. It is not reasonable that “plane” and “飞 机” share the same probability mass in common topics. Another closely related existing work is the PCLSA model proposed by Zhang et al. (2010). PCLSA employs a mixture of English words and Chinese words to represent common topics. It incorporates bilingual constraints into the Probabilistic Latent Semantic Analysis (PLSA) model (Hofmann, 2001) and assumes that word pairs in the dictionary share similar probability in a common topic. However, similar to one-to-one mapping of words, such bilingual constraints cannot handle well the original word co-occurrence in each language resulting in a degradation of the co2 Related Work Prasojo et al. (2015) and Biyani et al. (2015) organized news reader comments via identified entities or aspec"
P16-1064,D09-1092,0,0.446979,"Missing"
P16-1064,N10-1012,0,0.0209176,"The word-to-word bilingual constraints in PCLSA are not as effective. On the other hand, our MTCA model incorporates the bilingual translations using auxiliary distributions which incorporate word distributions from the other language on the topic level and can capture common semantics of multilingual reader comments. λ = 0.2 λ = 0.5 λ = 0.8 50 Topic Coherence Evaluation P M I(wi , wj ) = log 5 0 MCTA 0.138 0.158 0.120 0.169 0.152 0.152 0.111 0.124 0.096 0.154 0.137 We also evaluate the coherence of topics generated by PCLSA and MCTA, which indicates the interpretability of topics. Following Newman et al. (2010), we use a pointwise mutual information (PMI) score to measure the topic coherence. We compute the average PMI score of top 20 topic word pairs using Eq. 15. Newman et al. (2010) observed that it is important to use an external data set to evaluate PMI. Therefore, we use a 20word sliding window in Wikipedia (Shaoul, 2010) to identify the co-occurrence of word pairs. Determining Number of Topics 1840 1820 1800 1780 1760 1740 1720 1700 1680 PCLSA 0.117 0.126 0.117 0.138 0.109 0.138 0.108 0.099 0.085 0.133 0.117 Table 4: Topic coherence evaluation tified by MCTA have better topic commonality beca"
P16-1064,D09-1146,0,0.18643,"aints cannot handle well the original word co-occurrence in each language resulting in a degradation of the co2 Related Work Prasojo et al. (2015) and Biyani et al. (2015) organized news reader comments via identified entities or aspects. Such kind of organization via entities or aspects cannot capture common topics discussed by readers. Digesting merely based on entities fails to work in multilingual settings due to the fact that the common entities have distinct mentions in different languages. Zhai et al. (2004) discovered common topics from comparable texts via a PLSA based mixture model. Paul and Girju (2009) proposed a MixedCollection Topic Model for finding common topics from different collections. Despite the fact that the above models can find a kind of common topic, they only deal with a single language setting without considering the language gap. Some works discover common latent topics from multilingual corpora. For aligned corpora, they assume that the topic distribution in each document is the same (Vuli´c et al., 2011; Vuli´c and Moens, 2014; Erosheva et al., 2004; Fukumasu et al., 2012; Mimno et al., 2009; Ni et al., 2009; Zhang et al., 2013; Peng et al., 2014). However, aligned corpor"
P16-1064,P14-2110,0,0.0240109,"based mixture model. Paul and Girju (2009) proposed a MixedCollection Topic Model for finding common topics from different collections. Despite the fact that the above models can find a kind of common topic, they only deal with a single language setting without considering the language gap. Some works discover common latent topics from multilingual corpora. For aligned corpora, they assume that the topic distribution in each document is the same (Vuli´c et al., 2011; Vuli´c and Moens, 2014; Erosheva et al., 2004; Fukumasu et al., 2012; Mimno et al., 2009; Ni et al., 2009; Zhang et al., 2013; Peng et al., 2014). However, aligned corpora are often unavailable for most domains. For unaligned corpora, cross-lingual topic models use some language resources, such 677 as a bilingual dictionary or a bilingual knowledge base to bridge the language gap (Boyd-Graber and Blei, 2009; Zhang et al., 2010; Jagarlamudi and Daum´e III, 2010). As mentioned above, the goals of Boyd-Graber and Blei (2009) as well as Jagarlamud and Daum´e (2010) focus on mining the correspondence of topics at the vocabulary level, which are different from that of Zhang et al. (2010) and ours. The model in Zhang et al. (2010) adds the co"
P16-1064,D14-1040,0,0.0348108,"Missing"
P16-1064,P11-2084,0,0.0513094,"Missing"
P18-1087,E17-2091,0,0.366632,"neering and Engineering Management The Chinese University of Hong Kong, Hong Kong 2 Tencent AI Lab, Shenzhen, China {lixin,wlam,bshi}@se.cuhk.edu.hk lyndonbing@tencent.com Abstract “log on” and “better life”, and expresses positive sentiments over them. The task is usually formulated as predicting a sentiment category for a (target, sentence) pair. Recurrent Neural Networks (RNNs) with attention mechanism, firstly proposed in machine translation (Bahdanau et al., 2014), is the most commonly-used technique for this task. For example, Wang et al. (2016); Tang et al. (2016b); Yang et al. (2017); Liu and Zhang (2017); Ma et al. (2017) and Chen et al. (2017) employ attention to measure the semantic relatedness between each context word and the target, and then use the induced attention scores to aggregate contextual features for prediction. In these works, the attention weight based combination of word-level features for classification may introduce noise and downgrade the prediction accuracy. For example, in “This dish is my favorite and I always get it and never get tired of it.”, these approaches tend to involve irrelevant words such as “never” and “tired” when they highlight the opinion modifier “favor"
P18-1087,D15-1166,0,0.075596,"word and the target, and then use the induced attention scores to aggregate contextual features for prediction. In these works, the attention weight based combination of word-level features for classification may introduce noise and downgrade the prediction accuracy. For example, in “This dish is my favorite and I always get it and never get tired of it.”, these approaches tend to involve irrelevant words such as “never” and “tired” when they highlight the opinion modifier “favorite”. To some extent, this drawback is rooted in the attention mechanism, as also observed in machine translation (Luong et al., 2015) and image captioning (Xu et al., 2015). Another observation is that the sentiment of a target is usually determined by key phrases such as “is my favorite”. By this token, Convolutional Neural Networks (CNNs)—whose capability for extracting the informative n-gram features (also called “active local features”) as sentence representations has been verified in (Kim, 2014; Johnson and Zhang, 2015)— should be a suitable model for this classification problem. However, CNN likely fails in cases where a sentence expresses different sentiments over multiple targets, such as “great food but the service"
P18-1087,D17-1047,1,0.853645,"ese University of Hong Kong, Hong Kong 2 Tencent AI Lab, Shenzhen, China {lixin,wlam,bshi}@se.cuhk.edu.hk lyndonbing@tencent.com Abstract “log on” and “better life”, and expresses positive sentiments over them. The task is usually formulated as predicting a sentiment category for a (target, sentence) pair. Recurrent Neural Networks (RNNs) with attention mechanism, firstly proposed in machine translation (Bahdanau et al., 2014), is the most commonly-used technique for this task. For example, Wang et al. (2016); Tang et al. (2016b); Yang et al. (2017); Liu and Zhang (2017); Ma et al. (2017) and Chen et al. (2017) employ attention to measure the semantic relatedness between each context word and the target, and then use the induced attention scores to aggregate contextual features for prediction. In these works, the attention weight based combination of word-level features for classification may introduce noise and downgrade the prediction accuracy. For example, in “This dish is my favorite and I always get it and never get tired of it.”, these approaches tend to involve irrelevant words such as “never” and “tired” when they highlight the opinion modifier “favorite”. To some extent, this drawback is ro"
P18-1087,P14-2009,0,0.370583,"ˆ (l) = h(l) ∗ vi , i ∈ [1, n], l ∈ [1, L]. h i i • SVM (Kiritchenko et al., 2014): It is a traditional support vector machine based model with extensive feature engineering; (11) Based on Eq. 10 and Eq. 11, the words close to the target will be highlighted and those far away will be downgraded. v is also applied on the intermediate output to introduce the position information into each CPT layer. Then we feed the weighted h(L) to the convolutional layer, i.e., the top-most layer in Fig. 1, to generate the feature map c ∈ Rn−s+1 as follows: (L) ci = ReLU(w&gt; conv hi:i+s−1 + bconv ), • AdaRNN (Dong et al., 2014): It learns the sentence representation toward target for sentiment prediction via semantic composition over dependency tree; • AE-LSTM, and ATAE-LSTM (Wang et al., 2016): AE-LSTM is a simple LSTM model incorporating the target embedding as input, while ATAE-LSTM extends AE-LSTM with attention; (12) (L) where hi:i+s−1 ∈ Rs·dimh is the concatenated vecˆ (L) , · · · , h ˆ (L) , and s is the kernel size. tor of h i i+s−1 wconv ∈ Rs·dimh and bconv ∈ R are learnable weights of the convolutional kernel. To capture the most informative features, we apply max pooling (Kim, 2014) and obtain the sentenc"
P18-1087,D14-1162,0,0.0839652,"g two attention-based LSTMs and introduces gates to measure the importance of left context, right context, and the entire sentence for the prediction; • RAM (Chen et al., 2017): RAM is a multilayer architecture where each layer consists of attention-based aggregation of word features and a GRU cell to learn the sentence representation. 3.2 We run the released codes of TD-LSTM and BILSTM-ATT-G to generate results, since their papers only reported results on TWITTER. We also rerun MemNet on our datasets and evaluate it with both accuracy and Macro-Averaged F1.5 We use pre-trained GloVe vectors (Pennington et al., 2014) to initialize the word embeddings and the dimension is 300 (i.e., dimw = 300). For out-of-vocabulary words, we randomly sample their embeddings from the uniform distribution U(−0.25, 0.25), as done in (Kim, 2014). We only use one convolutional kernel size because it was observed that CNN with single optimal kernel size is comparable with CNN having multiple kernel sizes on small datasets (Zhang and Wallace, 2017). To alleviate overfitting, we apply dropout on the input word embeddings of the LSTM and the ultimate sentence representation z. All weight matrices are initialized with the uniform"
P18-1087,S14-2004,0,0.451917,"t phrase interactively; • CNN-ASP: It is a CNN-based model implemented by us which directly concatenates target representation to each word embedding; (13) • TD-LSTM (Tang et al., 2016a): It employs two LSTMs to model the left and right contexts of the target separately, then performs predictions based on concatenated context representations; Finally, we pass z to a fully connected layer for sentiment prediction: p(y|wτ , w) = Softmax(Wf z + bf ). Experimental Setup As shown in Table 1, we evaluate the proposed TNet on three benchmark datasets: LAPTOP and REST are from SemEval ABSA challenge (Pontiki et al., 2014), containing user reviews in laptop domain and restaurant domain respectively. We also remove a few examples having the “conflict label” as done in (Chen et al., 2017); TWITTER is built by Dong et al. (2014), containing twitter posts. All tokens are lowercased without removal of stop words, symbols or digits, and sentences are zero-padded to the length of the longest sentence in the dataset. Evaluation metrics are Accuracy and Macro-Averaged F1 where the latter is more appropriate for datasets with unbalanced classes. We also conduct pairwise t-test on both Accuracy and Macro-Averaged F1 to ve"
P18-1087,P11-1016,0,0.728867,"he performance of TNet consistently dominates previous state-of-the-art methods on different types of data. The ablation studies show the efficacy of its different modules, and thus verify the rationality of TNet’s architecture. Apart from sentence level sentiment classification (Kim, 2014; Shi et al., 2018), aspect/target level sentiment classification is also an important research topic in the field of sentiment analysis. The early methods mostly adopted supervised learning approach with extensive hand-coded features (Blair-Goldensohn et al., 2008; Titov and McDonald, 2008; Yu et al., 2011; Jiang et al., 2011; Kiritchenko et al., 2014; Wagner et al., 2014; Vo and Zhang, 2015), and they fail to model the semantic relatedness between a target and its context which is critical for target sentiment analysis. Dong et al. (2014) incorporate the target information into the feature learning using dependency trees. As observed in previous works, the performance heavily relies on the quality of dependency parsing. Tang et al. (2016a) propose to split the context into two parts and associate target with contextual features separately. Similar to (Tang et al., 2016a), Zhang et al. (2016) develop a three-way g"
P18-1087,P18-1232,1,0.835326,"ific transformation component to better integrate target information into the word representation. Moreover, we employ CNN as the feature extractor for this classification problem, and rely on the contextpreserving and position relevance mechanisms to maintain the advantages of previous LSTM-based models. The performance of TNet consistently dominates previous state-of-the-art methods on different types of data. The ablation studies show the efficacy of its different modules, and thus verify the rationality of TNet’s architecture. Apart from sentence level sentiment classification (Kim, 2014; Shi et al., 2018), aspect/target level sentiment classification is also an important research topic in the field of sentiment analysis. The early methods mostly adopted supervised learning approach with extensive hand-coded features (Blair-Goldensohn et al., 2008; Titov and McDonald, 2008; Yu et al., 2011; Jiang et al., 2011; Kiritchenko et al., 2014; Wagner et al., 2014; Vo and Zhang, 2015), and they fail to model the semantic relatedness between a target and its context which is critical for target sentiment analysis. Dong et al. (2014) incorporate the target information into the feature learning using depen"
P18-1087,C16-1311,0,0.412731,"and Bei Shi1 1 Department of Systems Engineering and Engineering Management The Chinese University of Hong Kong, Hong Kong 2 Tencent AI Lab, Shenzhen, China {lixin,wlam,bshi}@se.cuhk.edu.hk lyndonbing@tencent.com Abstract “log on” and “better life”, and expresses positive sentiments over them. The task is usually formulated as predicting a sentiment category for a (target, sentence) pair. Recurrent Neural Networks (RNNs) with attention mechanism, firstly proposed in machine translation (Bahdanau et al., 2014), is the most commonly-used technique for this task. For example, Wang et al. (2016); Tang et al. (2016b); Yang et al. (2017); Liu and Zhang (2017); Ma et al. (2017) and Chen et al. (2017) employ attention to measure the semantic relatedness between each context word and the target, and then use the induced attention scores to aggregate contextual features for prediction. In these works, the attention weight based combination of word-level features for classification may introduce noise and downgrade the prediction accuracy. For example, in “This dish is my favorite and I always get it and never get tired of it.”, these approaches tend to involve irrelevant words such as “never” and “tired” whe"
P18-1087,D14-1181,0,0.274961,"to involve irrelevant words such as “never” and “tired” when they highlight the opinion modifier “favorite”. To some extent, this drawback is rooted in the attention mechanism, as also observed in machine translation (Luong et al., 2015) and image captioning (Xu et al., 2015). Another observation is that the sentiment of a target is usually determined by key phrases such as “is my favorite”. By this token, Convolutional Neural Networks (CNNs)—whose capability for extracting the informative n-gram features (also called “active local features”) as sentence representations has been verified in (Kim, 2014; Johnson and Zhang, 2015)— should be a suitable model for this classification problem. However, CNN likely fails in cases where a sentence expresses different sentiments over multiple targets, such as “great food but the service was dreadful!”. One reason is that CNN cannot fully explore the target information as done by RNN-based methTarget-oriented sentiment classification aims at classifying sentiment polarities over individual opinion targets in a sentence. RNN with attention seems a good fit for the characteristics of this task, and indeed it achieves the state-of-the-art performance. Af"
P18-1087,D16-1021,0,0.416934,"and Bei Shi1 1 Department of Systems Engineering and Engineering Management The Chinese University of Hong Kong, Hong Kong 2 Tencent AI Lab, Shenzhen, China {lixin,wlam,bshi}@se.cuhk.edu.hk lyndonbing@tencent.com Abstract “log on” and “better life”, and expresses positive sentiments over them. The task is usually formulated as predicting a sentiment category for a (target, sentence) pair. Recurrent Neural Networks (RNNs) with attention mechanism, firstly proposed in machine translation (Bahdanau et al., 2014), is the most commonly-used technique for this task. For example, Wang et al. (2016); Tang et al. (2016b); Yang et al. (2017); Liu and Zhang (2017); Ma et al. (2017) and Chen et al. (2017) employ attention to measure the semantic relatedness between each context word and the target, and then use the induced attention scores to aggregate contextual features for prediction. In these works, the attention weight based combination of word-level features for classification may introduce noise and downgrade the prediction accuracy. For example, in “This dish is my favorite and I always get it and never get tired of it.”, these approaches tend to involve irrelevant words such as “never” and “tired” whe"
P18-1087,S14-2076,0,0.69114,"anged during the feature combination. 949 Train Test Train Test Train Test LAPTOP REST TWITTER # Positive 980 340 2159 730 1567 174 # Negative 858 128 800 195 1563 174 3 # Neutral 454 171 632 196 3127 346 3.1 Specifically, we first calculate the position relevance vi between the i-th word and the target4 : (k+m−i) C i−k C i&lt;k+m k+m≤i≤n i&gt;n (10) where k is the index of the first target word, C is a pre-specified constant, and m is the length of the target wτ . Then, we use v to help CNN locate the correct opinion w.r.t. the given target: ˆ (l) = h(l) ∗ vi , i ∈ [1, n], l ∈ [1, L]. h i i • SVM (Kiritchenko et al., 2014): It is a traditional support vector machine based model with extensive feature engineering; (11) Based on Eq. 10 and Eq. 11, the words close to the target will be highlighted and those far away will be downgraded. v is also applied on the intermediate output to introduce the position information into each CPT layer. Then we feed the weighted h(L) to the convolutional layer, i.e., the top-most layer in Fig. 1, to generate the feature map c ∈ Rn−s+1 as follows: (L) ci = ReLU(w&gt; conv hi:i+s−1 + bconv ), • AdaRNN (Dong et al., 2014): It learns the sentence representation toward target for sentime"
P18-1087,D16-1076,0,0.053622,"Missing"
P18-1087,S14-2036,0,0.169243,"Missing"
P18-1087,D16-1058,0,0.480203,"ng Bing2 , Wai Lam1 and Bei Shi1 1 Department of Systems Engineering and Engineering Management The Chinese University of Hong Kong, Hong Kong 2 Tencent AI Lab, Shenzhen, China {lixin,wlam,bshi}@se.cuhk.edu.hk lyndonbing@tencent.com Abstract “log on” and “better life”, and expresses positive sentiments over them. The task is usually formulated as predicting a sentiment category for a (target, sentence) pair. Recurrent Neural Networks (RNNs) with attention mechanism, firstly proposed in machine translation (Bahdanau et al., 2014), is the most commonly-used technique for this task. For example, Wang et al. (2016); Tang et al. (2016b); Yang et al. (2017); Liu and Zhang (2017); Ma et al. (2017) and Chen et al. (2017) employ attention to measure the semantic relatedness between each context word and the target, and then use the induced attention scores to aggregate contextual features for prediction. In these works, the attention weight based combination of word-level features for classification may introduce noise and downgrade the prediction accuracy. For example, in “This dish is my favorite and I always get it and never get tired of it.”, these approaches tend to involve irrelevant words such as “nev"
P18-1087,P11-1150,0,0.0499508,"M-based models. The performance of TNet consistently dominates previous state-of-the-art methods on different types of data. The ablation studies show the efficacy of its different modules, and thus verify the rationality of TNet’s architecture. Apart from sentence level sentiment classification (Kim, 2014; Shi et al., 2018), aspect/target level sentiment classification is also an important research topic in the field of sentiment analysis. The early methods mostly adopted supervised learning approach with extensive hand-coded features (Blair-Goldensohn et al., 2008; Titov and McDonald, 2008; Yu et al., 2011; Jiang et al., 2011; Kiritchenko et al., 2014; Wagner et al., 2014; Vo and Zhang, 2015), and they fail to model the semantic relatedness between a target and its context which is critical for target sentiment analysis. Dong et al. (2014) incorporate the target information into the feature learning using dependency trees. As observed in previous works, the performance heavily relies on the quality of dependency parsing. Tang et al. (2016a) propose to split the context into two parts and associate target with contextual features separately. Similar to (Tang et al., 2016a), Zhang et al. (2016) d"
P18-1087,I17-1026,0,0.0291095,"since their papers only reported results on TWITTER. We also rerun MemNet on our datasets and evaluate it with both accuracy and Macro-Averaged F1.5 We use pre-trained GloVe vectors (Pennington et al., 2014) to initialize the word embeddings and the dimension is 300 (i.e., dimw = 300). For out-of-vocabulary words, we randomly sample their embeddings from the uniform distribution U(−0.25, 0.25), as done in (Kim, 2014). We only use one convolutional kernel size because it was observed that CNN with single optimal kernel size is comparable with CNN having multiple kernel sizes on small datasets (Zhang and Wallace, 2017). To alleviate overfitting, we apply dropout on the input word embeddings of the LSTM and the ultimate sentence representation z. All weight matrices are initialized with the uniform distribution U(−0.01, 0.01) and the biases are initialized Main Results As shown in Table 3, both TNet-LF and TNet-AS consistently achieve the best performance on all datasets, which verifies the efficacy of our whole TNet model. Moreover, TNet can perform well for different kinds of user generated content, such as product reviews with relatively formal sentences in LAPTOP and REST, and tweets with more ungrammati"
P18-1087,I17-1006,0,0.0135571,"preferred. The input targets are wrapped in brackets with the true labels given as subscripts. 7 indicates incorrect prediction. teraction between the target and its surrounding contexts. Despite the advantages of jointly modeling target and context, they are not capable of capturing long-range information when some critical context information is far from the target. To overcome this limitation, researchers bring in the attention mechanism to model target-context association (Tang et al., 2016a,b; Wang et al., 2016; Yang et al., 2017; Liu and Zhang, 2017; Ma et al., 2017; Chen et al., 2017; Zhang et al., 2017; Tay et al., 2017). Compared with these methods, our TNet avoids using attention for feature extraction so as to alleviate the attended noise. “long” in the fifth sentence is negative for “startup time”, while it could be positive for other targets such as “battery life” in the sixth sentence. The sentiment of target-specific opinion word is conditioned on the given target. Our TNet variants, armed with the word-level feature transformation w.r.t. the target, is capable of handling such case. We also find that all these models cannot give correct prediction for the last sentence, a commonly u"
P18-1087,D17-1310,1,\N,Missing
P18-1232,P07-1056,0,0.805243,"to predict the target word. The skip-gram model uses pairwise training examples which are much easier to integrate with sentiment information. Note that our model can be easily extended to more than two domains. Similarly, we use a domain-specific vector for each word in each domain and each word is also associated with a domain-common vector. We just need to extend the probability distribution of zw from Bernoulli distribution to Multinomial distribution according to the number of domains. 4 4.1 Experiment Experimental Setup We conducted experiments on the Amazon product reviews collected by Blitzer et al. (2007). We use four product categories: books (B), DVDs (D), electronic items (E), and kitchen appliances (K). A category corresponds to a domain. For each domain, there are 17,457 unlabeled reviews on average associated with rating scores from 1.0 to 5.0 for each domain. We use unlabeled reviews with rating score higher than 3.0 as positive reviews and unlabeled reviews with rating score lower than 3.0 Yang’s Work Yang et al. (2017) have proposed a method2 to learn domain-sensitive word embeddings. They choose pivot words and add a regularization item into the original skipgram objective function e"
P18-1232,P15-1071,0,0.24436,"ht” usually connotes a positive sentiment in the electronics domain since a lightweight device is easier to carry. In contrast, in the movie 2494 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2494–2504 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics domain, the word “lightweight” usually connotes a negative opinion describing movies that do not invoke deep thoughts among the audience. This observation motivates the study of learning domainsensitive word representations (Yang et al., 2017; Bollegala et al., 2015, 2014). They basically learn separate embeddings of the same word for different domains. To bridge the semantics of individual embedding spaces, they select a subset of words that are likely to be domain-insensitive and align the dimensions of their embeddings. However, the sentiment information is not exploited in these methods although they intend to tackle the task of sentiment classification. In this paper, we aim at learning word embeddings that are both domain-sensitive and sentiment-aware. Our proposed method can jointly model the sentiment semantics and domain specificity of words, ex"
P18-1232,P14-1058,0,0.0621552,"Missing"
P18-1232,D17-1047,1,0.936057,"ve Region, China (Project Code: 14203414). Introduction Sentiment classification aims to predict the sentiment polarity, such as “positive” or “negative”, over a piece of review. It has been a long-standing research topic because of its importance for many applications such as social media analysis, ecommerce, and marketing (Liu, 2012; Pang et al., 2008). Deep learning has brought in progress in various NLP tasks, including sentiment classification. Some researchers focus on designing RNN or CNN based models for predicting sentence level (Kim, 2014) or aspect level sentiment (Li et al., 2018; Chen et al., 2017; Wang et al., 2016). These works directly take the word embeddings pre-trained for general purpose as initial word representations and may conduct fine tuning in the training process. Some other researchers look into the problem of learning taskspecific word embeddings for sentiment classification aiming at solving some limitations of applying general pre-trained word embeddings. For example, Tang et al. (2014b) develop a neural network model to convey sentiment information in the word embeddings. As a result, the learned embeddings are sentiment-aware and able to distinguish words with simil"
P18-1232,W16-1620,0,0.0245813,"s for generating such representations have been investigated. For example, Bengio et al. propose a neural network architecture for this purpose (Bengio et al., 2003; Bengio, 2009). Later, Mikolov et al. (2013) propose two methods that are considerably more efficient, namely skip-gram and CBOW. This work has made it possible to learn word embeddings from large data sets, which has led to the current popularity of word embed2495 dings. Word embedding models have been applied to many tasks, such as named entity recognition (Turian et al., 2010), word sense disambiguation (Collobert et al., 2011; Iacobacci et al., 2016; Zhang and Hasan, 2017; Dave et al., 2018), parsing (Roth and Lapata, 2016), and document classification (Tang et al., 2014a,b; Shi et al., 2017). Sentiment classification has been a longstanding research topic (Liu, 2012; Pang et al., 2008; Chen et al., 2017; Moraes et al., 2013). Given a review, the task aims at predicting the sentiment polarity on the sentence level (Kim, 2014) or the aspect level (Li et al., 2018; Chen et al., 2017). Supervised learning algorithms have been widely used in sentiment classification (Pang et al., 2002). People usually use different expressions of sentiment s"
P18-1232,P18-1087,1,0.921412,"cial Administrative Region, China (Project Code: 14203414). Introduction Sentiment classification aims to predict the sentiment polarity, such as “positive” or “negative”, over a piece of review. It has been a long-standing research topic because of its importance for many applications such as social media analysis, ecommerce, and marketing (Liu, 2012; Pang et al., 2008). Deep learning has brought in progress in various NLP tasks, including sentiment classification. Some researchers focus on designing RNN or CNN based models for predicting sentence level (Kim, 2014) or aspect level sentiment (Li et al., 2018; Chen et al., 2017; Wang et al., 2016). These works directly take the word embeddings pre-trained for general purpose as initial word representations and may conduct fine tuning in the training process. Some other researchers look into the problem of learning taskspecific word embeddings for sentiment classification aiming at solving some limitations of applying general pre-trained word embeddings. For example, Tang et al. (2014b) develop a neural network model to convey sentiment information in the word embeddings. As a result, the learned embeddings are sentiment-aware and able to distingui"
P18-1232,W02-1011,0,0.021239,", word sense disambiguation (Collobert et al., 2011; Iacobacci et al., 2016; Zhang and Hasan, 2017; Dave et al., 2018), parsing (Roth and Lapata, 2016), and document classification (Tang et al., 2014a,b; Shi et al., 2017). Sentiment classification has been a longstanding research topic (Liu, 2012; Pang et al., 2008; Chen et al., 2017; Moraes et al., 2013). Given a review, the task aims at predicting the sentiment polarity on the sentence level (Kim, 2014) or the aspect level (Li et al., 2018; Chen et al., 2017). Supervised learning algorithms have been widely used in sentiment classification (Pang et al., 2002). People usually use different expressions of sentiment semantics in different domains. Due to the mismatch between domainspecific words, a sentiment classifier trained in one domain may not work well when it is directly applied to other domains. Thus cross-domain sentiment classification algorithms have been explored (Pan et al., 2010; Li et al., 2009; Glorot et al., 2011). These works usually find common feature spaces across domains and then share learned parameters from the source domain to the target domain. For example, Pan et al. (2010) propose a spectral feature alignment algorithm to"
P18-1232,P16-1113,0,0.0326032,"engio et al. propose a neural network architecture for this purpose (Bengio et al., 2003; Bengio, 2009). Later, Mikolov et al. (2013) propose two methods that are considerably more efficient, namely skip-gram and CBOW. This work has made it possible to learn word embeddings from large data sets, which has led to the current popularity of word embed2495 dings. Word embedding models have been applied to many tasks, such as named entity recognition (Turian et al., 2010), word sense disambiguation (Collobert et al., 2011; Iacobacci et al., 2016; Zhang and Hasan, 2017; Dave et al., 2018), parsing (Roth and Lapata, 2016), and document classification (Tang et al., 2014a,b; Shi et al., 2017). Sentiment classification has been a longstanding research topic (Liu, 2012; Pang et al., 2008; Chen et al., 2017; Moraes et al., 2013). Given a review, the task aims at predicting the sentiment polarity on the sentence level (Kim, 2014) or the aspect level (Li et al., 2018; Chen et al., 2017). Supervised learning algorithms have been widely used in sentiment classification (Pang et al., 2002). People usually use different expressions of sentiment semantics in different domains. Due to the mismatch between domainspecific wo"
P18-1232,C14-1018,0,0.363619,"ks, including sentiment classification. Some researchers focus on designing RNN or CNN based models for predicting sentence level (Kim, 2014) or aspect level sentiment (Li et al., 2018; Chen et al., 2017; Wang et al., 2016). These works directly take the word embeddings pre-trained for general purpose as initial word representations and may conduct fine tuning in the training process. Some other researchers look into the problem of learning taskspecific word embeddings for sentiment classification aiming at solving some limitations of applying general pre-trained word embeddings. For example, Tang et al. (2014b) develop a neural network model to convey sentiment information in the word embeddings. As a result, the learned embeddings are sentiment-aware and able to distinguish words with similar syntactic context but opposite sentiment polarity, such as the words “good” and “bad”. In fact, sentiment information can be easily obtained or derived in large scale from some data sources (e.g., the ratings provided by users), which allows reliable learning of such sentiment-aware embeddings. Apart from these words (e.g. “good” and “bad”) with consistent sentiment polarity in different contexts, the polari"
P18-1232,P14-1146,0,0.53574,"ks, including sentiment classification. Some researchers focus on designing RNN or CNN based models for predicting sentence level (Kim, 2014) or aspect level sentiment (Li et al., 2018; Chen et al., 2017; Wang et al., 2016). These works directly take the word embeddings pre-trained for general purpose as initial word representations and may conduct fine tuning in the training process. Some other researchers look into the problem of learning taskspecific word embeddings for sentiment classification aiming at solving some limitations of applying general pre-trained word embeddings. For example, Tang et al. (2014b) develop a neural network model to convey sentiment information in the word embeddings. As a result, the learned embeddings are sentiment-aware and able to distinguish words with similar syntactic context but opposite sentiment polarity, such as the words “good” and “bad”. In fact, sentiment information can be easily obtained or derived in large scale from some data sources (e.g., the ratings provided by users), which allows reliable learning of such sentiment-aware embeddings. Apart from these words (e.g. “good” and “bad”) with consistent sentiment polarity in different contexts, the polari"
P18-1232,P10-1040,0,0.0756154,"ed word representation instead, called word embeddings. Several techniques for generating such representations have been investigated. For example, Bengio et al. propose a neural network architecture for this purpose (Bengio et al., 2003; Bengio, 2009). Later, Mikolov et al. (2013) propose two methods that are considerably more efficient, namely skip-gram and CBOW. This work has made it possible to learn word embeddings from large data sets, which has led to the current popularity of word embed2495 dings. Word embedding models have been applied to many tasks, such as named entity recognition (Turian et al., 2010), word sense disambiguation (Collobert et al., 2011; Iacobacci et al., 2016; Zhang and Hasan, 2017; Dave et al., 2018), parsing (Roth and Lapata, 2016), and document classification (Tang et al., 2014a,b; Shi et al., 2017). Sentiment classification has been a longstanding research topic (Liu, 2012; Pang et al., 2008; Chen et al., 2017; Moraes et al., 2013). Given a review, the task aims at predicting the sentiment polarity on the sentence level (Kim, 2014) or the aspect level (Li et al., 2018; Chen et al., 2017). Supervised learning algorithms have been widely used in sentiment classification ("
P18-1232,D16-1058,0,0.10183,"Missing"
P18-1232,H05-1044,0,0.0490706,"ve because both of them consider the sentiment information in the embeddings. Our DSE model outperforms other methods which do not consider sentiments such as Yang, EmbeddingCat and EmbeddingAll. Note that the advantage of domain-sensitive embeddings would be insufficient for this task because the sentiment lexicons are not domain-specific. Lexicon Term Sentiment Classification To further evaluate the quality of the sentiment semantics of the learned word embeddings, we also conduct lexicon term sentiment classification on two popular sentiment lexicons, namely HL (Hu and Liu, 2004) and MPQA (Wilson et al., 2005). The words with neutral sentiment and phrases are removed. The statistics of HL and MPQA are shown in Table 3. We conduct multiple trials by selecting every possible two domains from books (B), DVDs (D), electronics items (E) and kitchen appliances (K). 5 Case Study Table 4 shows the probabilities of “lightweight”, “die”, “mysterious”, and “great” to be domaincommon for different domain combinations. For “lightweight”, its domain-common probability for the books domain and the DVDs domain (“B & D”) is quite high, i.e. p(z = 1) = 0.999, and the review examples in the last column show that the"
P18-1232,D17-1312,0,0.34591,"the word “lightweight” usually connotes a positive sentiment in the electronics domain since a lightweight device is easier to carry. In contrast, in the movie 2494 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2494–2504 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics domain, the word “lightweight” usually connotes a negative opinion describing movies that do not invoke deep thoughts among the audience. This observation motivates the study of learning domainsensitive word representations (Yang et al., 2017; Bollegala et al., 2015, 2014). They basically learn separate embeddings of the same word for different domains. To bridge the semantics of individual embedding spaces, they select a subset of words that are likely to be domain-insensitive and align the dimensions of their embeddings. However, the sentiment information is not exploited in these methods although they intend to tackle the task of sentiment classification. In this paper, we aim at learning word embeddings that are both domain-sensitive and sentiment-aware. Our proposed method can jointly model the sentiment semantics and domain"
P18-1232,P16-1085,0,\N,Missing
W16-1301,D15-1060,1,0.812529,"g examples. If the subject of a triple matches with a drug or disease title entity in a corpus and its object value also appears in that document, it is extracted. In total, we get 2022, 2453, 905, 753, and 164 triples for 5 disease relations respectively, and 3112, 315, and 265 triples for 3 drug relations, respectively. Each triple is used to label the document whose subject entity is the same as the triple subject. For instance, triple sideEffects(Aspirin,heartburn) will label a mention “heartburn” from the Aspirin article as an example of sideEffects relation. This raw data is very noisy (Bing et al., 2015; Bing et al., 2016), so we add a distillation step. We first distantly label these relations in two small structured corpora, namely, WebMD for drug and MayoClinic for disease.2 They have well-defined section information, which can be matched with target relations. We only label usedToTreat and conditionsThisMayPrevent from the “Uses” section, and label sideEffects from “Side Effects” section of WebMD. Similarly, the disease relations are labeled from “Treatments and drugs”,“Symptoms”, “Risk factors”, “Causes”, and “Prevention” sections of MayoClinic. After that we build a graph containing ex"
W16-1301,P11-1055,0,0.249315,"nt predictions at unlabeled points, and many graph-based SSL approaches require that the instances associated with the endpoints of an edge have similar labels (Zhu et al., 2003; Talukdar and Crammer, 2009). Other weakly-supervised methods also can be viewed as imposing constraints predictions made by a classifier: for instance, in distantlysupervised information extraction, constraints sometimes are imposed which requires that the classifier, when applied to the set S of mentions of an entity pair that is a member of relation r, classify at least one mention in S as a positive instance of r (Hoffmann et al., 2011). Different constraints (and different assumptions about the loss function for the learner) lead to different SSL algorithms. In this paper, we propose a general approach to modeling such constraints. In particular, we show that many types of constraints can be modeled by specifying the desired behavior of random walks through a graph of classifiers. In the graph, nodes correspond to relational conditions on small subsets of the data, and edges are annotated by feature vectors. Feature weights, combined with the feature vector at each edge and a non-linear postprocessing step, define a weighti"
W16-1301,P09-1113,0,0.0405789,"consider some combinations of these: SSL sd; SSL st; SSL dt; and SSL sdt. All the SSL pipelines employ the evaluation pages as unlabeled data for those constraints (i.e., they are used transductively). As one baseline, we compare to a standard supervised learning pipeline, SL, which learns a classifier with no constraints using ProPPR. We also compare against three existing methods: MultiR, (Hoffmann et al., 2011) which models each relation mention separately and aggregates their labels using a deterministic OR; Mintz++ from (Surdeanu et al., 2012), which improves on the original model from (Mintz et al., 2009) by training multiple classifiers, and allowing multiple labels per entity pair; and MIML-RE (Surdeanu et al., 2012) which has a similar structure to MultiR, but uses a classifier to aggregate the mention level predictions into an entity pair prediction. We used the publicly available code from the authors 3 for the experiments. Since these methods do not distinguish between structured and unstructured corpora, we used the union of these corpora in our experiments. We found that the performance of these methods varies significantly with the number of negative examples used during training, and"
W16-1301,D12-1042,0,0.0221559,"ument constraint; SSL t, only the section title constraint. We also consider some combinations of these: SSL sd; SSL st; SSL dt; and SSL sdt. All the SSL pipelines employ the evaluation pages as unlabeled data for those constraints (i.e., they are used transductively). As one baseline, we compare to a standard supervised learning pipeline, SL, which learns a classifier with no constraints using ProPPR. We also compare against three existing methods: MultiR, (Hoffmann et al., 2011) which models each relation mention separately and aggregates their labels using a deterministic OR; Mintz++ from (Surdeanu et al., 2012), which improves on the original model from (Mintz et al., 2009) by training multiple classifiers, and allowing multiple labels per entity pair; and MIML-RE (Surdeanu et al., 2012) which has a similar structure to MultiR, but uses a classifier to aggregate the mention level predictions into an entity pair prediction. We used the publicly available code from the authors 3 for the experiments. Since these methods do not distinguish between structured and unstructured corpora, we used the union of these corpora in our experiments. We found that the performance of these methods varies significantl"
W17-4512,P15-1153,1,0.93793,"the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in inforIntroduction The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Bing et al., 2015; Li et al., 2017). In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents. With the development of social media and mobile equipments, more and more user generated ∗ The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414). 1 http://www.se.cuhk.edu.hk/˜textmine/ dataset/ra-mds/ 2 91 https://goo.gl/DdU0vL Proceedings of the W"
W17-4512,D15-1166,0,0.0126449,"u(Wzh sz + bzh ) sx = sigmoid(Whx sh + bhx ) where x denotes a general sentence, and it can be a news sentence xd or a comment sentnece xc . By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound L(θ, ϕ; x) (3) VAESum (Li et al., 2017) employs an alignment mechanism (Bahdanau et al., 2015; Luong et al., 2015) to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state sih , we align it with each news encoder hidden state hjd 93 by an alignment vector ad ∈ Rnd . We also align it with each comments encoder hidden state hjc by an alignment vector ac ∈ Rnc . In order to filter the noisy information from the comments, we again employ the comment weight ρ to adjust the alignment vector of comments: the news content, then it cont"
W17-4512,C12-1128,0,0.0727713,"Missing"
W17-4512,W00-0405,0,0.181996,"he summaries. One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in inforIntroduction The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Bing et al., 2015; Li et al., 2017). In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents. With the development of social media and mobile equipments, more and more user generated ∗ The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414"
W17-4512,P98-2222,0,0.140521,"n dataset are depicted in Section 3.3. We use ROUGE score as our evaluation metric (Lin, 2004) with standard options8 . Fmeasures of ROUGE-1, ROUGE-2 and ROUGESU4 are reported. 4.2 Experimental Settings Comparative Methods To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods: • RA-Sparse (Li et al., 2015): It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments. 5 5.1 • Lead (Wasson, 1998) : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit. Results and Discussions Results on Our Dataset The results of our framework as well as the baseline methods are depicted in Table 1. It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly (p < 0.05), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the"
W17-4512,D12-1022,0,0.0241417,"rall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: X X max{ αi Si − αij (Si + Sj )Rij }, (12) i i<j where αi is the selection indicator for the phrase Pi , Si is the salience scores of Pi , αij and Rij is co-occurrence indicator and the similarity a pair of phrases (Pi , Pj ) respectively. The similarity is 94 calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to Woodsend and Lapata (2012), Bing et al. (2015), and Li et al. (2015). The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms (Dantzig and Thapa, 2006). In the implementation, we use a package called lp solve5 . 3 “about an hour into its flight from Kuala Lumpur”, etc. Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document. 3.2 The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find"
