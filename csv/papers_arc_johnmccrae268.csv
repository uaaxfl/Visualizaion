2021.smm4h-1.33,{ULD}-{NUIG} at Social Media Mining for Health Applications ({\\#}{SMM}4{H}) Shared Task 2021,2021,-1,-1,5,0.694444,1252,atul ojha,Proceedings of the Sixth Social Media Mining for Health ({\\#}SMM4H) Workshop and Shared Task,0,"Social media platforms such as Twitter and Facebook have been utilised for various research studies, from the cohort-level discussion to community-driven approaches to address the challenges in utilizing social media data for health, clinical and biomedical information. Detection of medical jargon{'}s, named entity recognition, multi-word expression becomes the primary, fundamental steps in solving those challenges. In this paper, we enumerate the ULD-NUIG team{'}s system, designed as part of Social Media Mining for Health Applications ({\#}SMM4H) Shared Task 2021. The team conducted a series of experiments to explore the challenges of task 6 and task 5. The submitted systems achieve F-1 0.84 and 0.53 score for task 6 and 5 respectively."
2021.nllp-1.10,Few-shot and Zero-shot Approaches to Legal Text Classification: A Case Study in the Financial Sector,2021,-1,-1,6,1,3060,rajdeep sarkar,Proceedings of the Natural Legal Language Processing Workshop 2021,0,"The application of predictive coding techniques to legal texts has the potential to greatly reduce the cost of legal review of documents, however, there is such a wide array of legal tasks and continuously evolving legislation that it is hard to construct sufficient training data to cover all cases. In this paper, we investigate few-shot and zero-shot approaches that require substantially less training data and introduce a triplet architecture, which for promissory statements produces performance close to that of a supervised system. This method allows predictive coding methods to be rapidly developed for new regulations and markets."
2021.gwc-1.9,Monolingual Word Sense Alignment as a Classification Problem,2021,-1,-1,2,0.520833,6137,sina ahmadi,Proceedings of the 11th Global Wordnet Conference,0,"Words are defined based on their meanings in various ways in different resources. Aligning word senses across monolingual lexicographic resources increases domain coverage and enables integration and incorporation of data. In this paper, we explore the application of classification methods using manually-extracted features along with representation learning techniques in the task of word sense alignment and semantic relationship detection. We demonstrate that the performance of classification methods dramatically varies based on the type of semantic relationships due to the nature of the task but outperforms the previous experiments."
2021.gwc-1.11,The {G}lobal{W}ord{N}et Formats: Updates for 2020,2021,-1,-1,1,1,1255,john mccrae,Proceedings of the 11th Global Wordnet Conference,0,"The Global Wordnet Formats have been introduced to enable wordnets to have a common representation that can be integrated through the Global WordNet Grid. As a result of their adoption, a number of shortcomings of the format were identified, and in this paper we describe the extensions to the formats that address these issues. These include: ordering of senses, dependencies between wordnets, pronunciation, syntactic modelling, relations, sense keys, metadata and RDF support. Furthermore, we provide some perspectives on how these changes help in the integration of wordnets."
2021.gwc-1.29,Towards a Linking between {W}ord{N}et and {W}ikidata,2021,-1,-1,1,1,1255,john mccrae,Proceedings of the 11th Global Wordnet Conference,0,"WordNet is the most widely used lexical resource for English, while Wikidata is one of the largest knowledge graphs of entity and concepts available. While, there is a clear difference in the focus of these two resources, there is also a significant overlap and as such a complete linking of these resources would have many uses. We propose the development of such a linking, first by means of the hapax legomenon links and secondly by the use of natural language processing techniques. We show that these can be done with high accuracy but that human validation is still necessary. This has resulted in over 9,000 links being added between these two resources."
2021.emnlp-main.716,Cross-lingual Sentence Embedding using Multi-Task Learning,2021,-1,-1,5,1,1254,koustava goswami,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Multilingual sentence embeddings capture rich semantic information not only for measuring similarity between texts but also for catering to a broad range of downstream cross-lingual NLP tasks. State-of-the-art multilingual sentence embedding models require large parallel corpora to learn efficiently, which confines the scope of these models. In this paper, we propose a novel sentence embedding framework based on an unsupervised loss function for generating effective multilingual sentence embeddings, eliminating the need for parallel corpora. We capture semantic similarity and relatedness between sentences using a multi-task loss function for training a dual encoder model mapping different languages onto the same vector space. We demonstrate the efficacy of an unsupervised as well as a weakly supervised variant of our framework on STS, BUCC and Tatoeba benchmark tasks. The proposed unsupervised sentence embedding framework outperforms even supervised state-of-the-art methods for certain under-resourced languages on the Tatoeba dataset and on a monolingual benchmark. Further, we show enhanced zero-shot learning capabilities for more than 30 languages, with the model being trained on only 13 languages. Our model can be extended to a wide range of languages from any language family, as it overcomes the requirement of parallel corpora for training."
2021.dravidianlangtech-1.15,Findings of the Shared Task on Machine Translation in {D}ravidian languages,2021,-1,-1,5,1,613,bharathi chakravarthi,Proceedings of the First Workshop on Speech and Language Technologies for Dravidian Languages,0,"This paper presents an overview of the shared task on machine translation of Dravidian languages. We presented the shared task results at the EACL 2021 workshop on Speech and Language Technologies for Dravidian Languages. This paper describes the datasets used, the methodology used for the evaluation of participants, and the experiments{'} overall results. As a part of this shared task, we organized four sub-tasks corresponding to machine translation of the following language pairs: English to Tamil, English to Malayalam, English to Telugu and Tamil to Telugu which are available at https://competitions.codalab.org/competitions/27650. We provided the participants with training and development datasets to perform experiments, and the results were evaluated on unseen test data. In total, 46 research groups participated in the shared task and 7 experimental runs were submitted for evaluation. We used BLEU scores for assessment of the translations."
2021.dravidianlangtech-1.17,"Findings of the Shared Task on Offensive Language Identification in {T}amil, {M}alayalam, and {K}annada",2021,-1,-1,9,1,613,bharathi chakravarthi,Proceedings of the First Workshop on Speech and Language Technologies for Dravidian Languages,0,"Detecting offensive language in social media in local languages is critical for moderating user-generated content. Thus, the field of offensive language identification in under-resourced Tamil, Malayalam and Kannada languages are essential. As the user-generated content is more code-mixed and not well studied for under-resourced languages, it is imperative to create resources and conduct benchmarking studies to encourage research in under-resourced Dravidian languages. We created a shared task on offensive language detection in Dravidian languages. We summarize here the dataset for this challenge which are openly available at https://competitions.codalab.org/competitions/27654, and present an overview of the methods and the results of the competing systems."
2020.wmt-1.49,{NUIG}-Panlingua-{KMI} {H}indi-{M}arathi {MT} Systems for Similar Language Translation Task @ {WMT} 2020,2020,-1,-1,6,0.694444,1252,atul ojha,Proceedings of the Fifth Conference on Machine Translation,0,"NUIG-Panlingua-KMI submission to WMT 2020 seeks to push the state-of-the-art in Similar Language Translation Task for HindiâMarathi language pair. As part of these efforts, we conducteda series of experiments to address the challenges for translation between similar languages. Among the 4 MT systems prepared under this task, 1 PBSMT systems were prepared for HindiâMarathi each and 1 NMT systems were developed for HindiâMarathi using Byte PairEn-coding (BPE) into subwords. The results show that different architectures NMT could be an effective method for developing MT systems for closely related languages. Our Hindi-Marathi NMT system was ranked 8th among the 14 teams that participated and our Marathi-Hindi NMT system was ranked 8th among the 11 teams participated for the task."
2020.wildre-1.2,A Dataset for Troll Classification of {T}amil{M}emes,2020,-1,-1,5,0,11157,shardul suryawanshi,Proceedings of the WILDRE5{--} 5th Workshop on Indian Language Data: Resources and Evaluation,0,"Social media are interactive platforms that facilitate the creation or sharing of information, ideas or other forms of expression among people. This exchange is not free from offensive, trolling or malicious contents targeting users or communities. One way of trolling is by making memes, which in most cases combines an image with a concept or catchphrase. The challenge of dealing with memes is that they are region-specific and their meaning is often obscured in humour or sarcasm. To facilitate the computational modelling of trolling in the memes for Indian languages, we created a meme dataset for Tamil (TamilMemes). We annotated and released the dataset containing suspected trolls and not-troll memes. In this paper, we use the a image classification to address the difficulties involved in the classification of troll memes with the existing methods. We found that the identification of a troll meme with such an image classifier is not feasible which has been corroborated with precision, recall and F1-score."
2020.vardial-1.6,Bilingual Lexicon Induction across Orthographically-distinct Under-Resourced {D}ravidian Languages,2020,-1,-1,6,1,613,bharathi chakravarthi,"Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects",0,"Bilingual lexicons are a vital tool for under-resourced languages and recent state-of-the-art approaches to this leverage pretrained monolingual word embeddings using supervised or semi-supervised approaches. However, these approaches require cross-lingual information such as seed dictionaries to train the model and find a linear transformation between the word embedding spaces. Especially in the case of low-resourced languages, seed dictionaries are not readily available, and as such, these methods produce extremely weak results on these languages. In this work, we focus on the Dravidian languages, namely Tamil, Telugu, Kannada, and Malayalam, which are even more challenging as they are written in unique scripts. To take advantage of orthographic information and cognates in these languages, we bring the related languages into a single script. Previous approaches have used linguistically sub-optimal measures such as the Levenshtein edit distance to detect cognates, whereby we demonstrate that the longest common sub-sequence is linguistically more sound and improves the performance of bilingual lexicon induction. We show that our approach can increase the accuracy of bilingual lexicon induction methods on these languages many times, making bilingual lexicon induction approaches feasible for such under-resourced languages."
2020.trac-1.7,A Comparative Study of Different State-of-the-Art Hate Speech Detection Methods in {H}indi-{E}nglish Code-Mixed Data,2020,-1,-1,6,1,1253,priya rani,"Proceedings of the Second Workshop on Trolling, Aggression and Cyberbullying",0,"Hate speech detection in social media communication has become one of the primary concerns to avoid conflicts and curb undesired activities. In an environment where multilingual speakers switch among multiple languages, hate speech detection becomes a challenging task using methods that are designed for monolingual corpora. In our work, we attempt to analyze, detect and provide a comparative study of hate speech in a code-mixed social media text. We also provide a Hindi-English code-mixed data set consisting of Facebook and Twitter posts and comments. Our experiments show that deep learning models trained on this code-mixed corpus perform better."
2020.sltu-1.25,A Sentiment Analysis Dataset for Code-Mixed {M}alayalam-{E}nglish,2020,-1,-1,5,1,613,bharathi chakravarthi,Proceedings of the 1st Joint Workshop on Spoken Language Technologies for Under-resourced languages (SLTU) and Collaboration and Computing for Under-Resourced Languages (CCURL),0,"There is an increasing demand for sentiment analysis of text from social media which are mostly code-mixed. Systems trained on monolingual data fail for code-mixed data due to the complexity of mixing at different levels of the text. However, very few resources are available for code-mixed data to create models specific for this data. Although much research in multilingual and cross-lingual sentiment analysis has used semi-supervised or unsupervised methods, supervised methods still performs better. Only a few datasets for popular languages such as English-Spanish, English-Hindi, and English-Chinese are available. There are no resources available for Malayalam-English code-mixed data. This paper presents a new gold standard corpus for sentiment analysis of code-mixed text in Malayalam-English annotated by voluntary annotators. This gold standard corpus obtained a Krippendorff{'}s alpha above 0.8 for the dataset. We use this new corpus to provide the benchmark for sentiment analysis in Malayalam-English code-mixed texts."
2020.sltu-1.28,Corpus Creation for Sentiment Analysis in Code-Mixed {T}amil-{E}nglish Text,2020,-1,-1,4,1,613,bharathi chakravarthi,Proceedings of the 1st Joint Workshop on Spoken Language Technologies for Under-resourced languages (SLTU) and Collaboration and Computing for Under-Resourced Languages (CCURL),0,"Understanding the sentiment of a comment from a video or an image is an essential task in many applications. Sentiment analysis of a text can be useful for various decision-making processes. One such application is to analyse the popular sentiments of videos on social media based on viewer comments. However, comments from social media do not follow strict rules of grammar, and they contain mixing of more than one language, often written in non-native scripts. Non-availability of annotated code-mixed data for a low-resourced language like Tamil also adds difficulty to this problem. To overcome this, we created a gold standard Tamil-English code-switched, sentiment-annotated corpus containing 15,744 comment posts from YouTube. In this paper, we describe the process of creating the corpus and assigning polarities. We present inter-annotator agreement and show the results of sentiment analysis trained on this corpus as a benchmark."
2020.semeval-1.125,{ULD}@{NUIG} at {S}em{E}val-2020 Task 9: Generative Morphemes with an Attention Model for Sentiment Analysis in Code-Mixed Text,2020,-1,-1,5,1,1254,koustava goswami,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"Code mixing is a common phenomena in multilingual societies where people switch from one language to another for various reasons. Recent advances in public communication over different social media sites have led to an increase in the frequency of code-mixed usage in written language. In this paper, we present the Generative Morphemes with Attention (GenMA) Model sentiment analysis system contributed to SemEval 2020 Task 9 SentiMix. The system aims to predict the sentiments of the given English-Hindi code-mixed tweets without using word-level language tags instead inferring this automatically using a morphological model. The system is based on a novel deep neural network (DNN) architecture, which has outperformed the baseline F1-score on the test data-set as well as the validation data-set. Our results can be found under the user name {``}koustava{''} on the {``}Sentimix Hindi English{''} page."
2020.mmw-1.3,{E}nglish {W}ord{N}et 2020: Improving and Extending a {W}ord{N}et for {E}nglish using an Open-Source Methodology,2020,-1,-1,1,1,1255,john mccrae,Proceedings of the LREC 2020 Workshop on Multimodal Wordnets (MMW2020),0,"WordNet, while one of the most widely used resources for NLP, has not been updated for a long time, and as such a new project English WordNet has arisen to continue the development of the model under an open-source paradigm. In this paper, we detail the second release of this resource entitled {``}English WordNet 2020{''}. The work has focused firstly, on the introduction of new synsets and senses and developing guidelines for this and secondly, on the integration of contributions from other projects. We present the changes in this edition, which total over 15,000 changes over the previous release."
2020.lrec-1.390,Some Issues with Building a Multilingual {W}ordnet,2020,-1,-1,4,0,6126,francis bond,Proceedings of the 12th Language Resources and Evaluation Conference,0,"In this paper we discuss the experience of bringing together over 40 different wordnets. We introduce some extensions to the GWA wordnet LMF format proposed in Vossen et al. (2016) and look at how this new information can be displayed. Notable extensions include: confidence, corpus frequency, orthographic variants, lexicalized and non-lexicalized synsets and lemmas, new parts of speech, and more. Many of these extensions already exist in multiple wordnets {--} the challenge was to find a compatible representation. To this end, we introduce a new version of the Open Multilingual Wordnet (Bond and Foster, 2013), that integrates a new set of tools that tests the extensions introduced by this new format, while also ensuring the integrity of the Collaborative Interlingual Index (CILI: Bond et al., 2016), avoiding the same new concept to be introduced through multiple projects."
2020.lrec-1.395,A Multilingual Evaluation Dataset for Monolingual Word Sense Alignment,2020,-1,-1,2,0.520833,6137,sina ahmadi,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Aligning senses across resources and languages is a challenging task with beneficial applications in the field of natural language processing and electronic lexicography. In this paper, we describe our efforts in manually aligning monolingual dictionaries. The alignment is carried out at sense-level for various resources in 15 languages. Moreover, senses are annotated with possible semantic relationships such as broadness, narrowness, relatedness, and equivalence. In comparison to previous datasets for this task, this dataset covers a wide range of languages and resources and focuses on the more challenging task of linking general-purpose language. We believe that our data will pave the way for further advances in alignment and evaluation of word senses by creating new solutions, particularly those notoriously requiring data such as neural networks. Our resources are publicly available at https://github.com/elexis-eu/MWSA."
2020.lrec-1.695,Recent Developments for the Linguistic Linked Open Data Infrastructure,2020,-1,-1,2,0,2109,thierry declerck,Proceedings of the 12th Language Resources and Evaluation Conference,0,"In this paper we describe the contributions made by the European H2020 project {``}Pr{\^e}t-{\`a}-LLOD{''} ({`}Ready-to-use Multilingual Linked Language Data for Knowledge Services across Sectors{'}) to the further development of the Linguistic Linked Open Data (LLOD) infrastructure. Pr{\^e}t-{\`a}-LLOD aims to develop a new methodology for building data value chains applicable to a wide range of sectors and applications and based around language resources and language technologies that can be integrated by means of semantic technologies. We describe the methods implemented for increasing the number of language data sets in the LLOD. We also present the approach for ensuring interoperability and for porting LLOD data sets and services to other infrastructures, as well as the contribution of the projects to existing standards."
2020.lrec-1.712,Figure Me Out: A Gold Standard Dataset for Metaphor Interpretation,2020,-1,-1,2,1,18055,omnia zayed,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Metaphor comprehension and understanding is a complex cognitive task that requires interpreting metaphors by grasping the interaction between the meaning of their target and source concepts. This is very challenging for humans, let alone computers. Thus, automatic metaphor interpretation is understudied in part due to the lack of publicly available datasets. The creation and manual annotation of such datasets is a demanding task which requires huge cognitive effort and time. Moreover, there will always be a question of accuracy and consistency of the annotated data due to the subjective nature of the problem. This work addresses these issues by presenting an annotation scheme to interpret verb-noun metaphoric expressions in text. The proposed approach is designed with the goal of reducing the workload on annotators and maintain consistency. Our methodology employs an automatic retrieval approach which utilises external lexical resources, word embeddings and semantic similarity to generate possible interpretations of identified metaphors in order to enable quick and accurate annotation. We validate our proposed approach by annotating around 1,500 metaphors in tweets which were annotated by six native English speakers. As a result of this work, we publish as linked data the first gold standard dataset for metaphor interpretation which will facilitate research in this area."
2020.ldl-1.7,Challenges of Word Sense Alignment: {P}ortuguese Language Resources,2020,-1,-1,4,0,17442,ana salgado,Proceedings of the 7th Workshop on Linked Data in Linguistics (LDL-2020),0,"This paper reports on an ongoing task of monolingual word sense alignment in which a comparative study between the Portuguese Academy of Sciences Dictionary and the Dicion{\'a}rio Aberto is carried out in the context of the ELEXIS (European Lexicographic Infrastructure) project. Word sense alignment involves searching for matching senses within dictionary entries of different lexical resources and linking them, which poses significant challenges. The lexicographic criteria are not always entirely consistent within individual dictionaries and even less so across different projects where different options may have been assumed in terms of structure and especially wording techniques of lexicographic glosses. This hinders the task of matching senses. We aim to present our annotation workflow in Portuguese using the Semantic Web technologies. The results obtained are useful for the discussion within the community."
2020.iwltp-1.2,On the Linguistic Linked Open Data Infrastructure,2020,-1,-1,5,0,2108,christian chiarcos,Proceedings of the 1st International Workshop on Language Technology Platforms,0,"In this paper we describe the current state of development of the Linguistic Linked Open Data (LLOD) infrastructure, an LOD(sub-)cloud of linguistic resources, which covers various linguistic data bases, lexicons, corpora, terminology and metadata repositories.We give in some details an overview of the contributions made by the European H2020 projects {``}Pr{\^e}t-{\`a}-LLOD{''} ({`}Ready-to-useMultilingual Linked Language Data for Knowledge Services across Sectors{'}) and {``}ELEXIS{''} ({`}European Lexicographic Infrastructure{'}) to the further development of the LLOD."
2020.iwltp-1.15,Towards an Interoperable Ecosystem of {AI} and {LT} Platforms: A Roadmap for the Implementation of Different Levels of Interoperability,2020,20,2,17,0,60,georg rehm,Proceedings of the 1st International Workshop on Language Technology Platforms,0,"With regard to the wider area of AI/LT platform interoperability, we concentrate on two core aspects: (1) cross-platform search and discovery of resources and services; (2) composition of cross-platform service workflows. We devise five different levels (of increasing complexity) of platform interoperability that we suggest to implement in a wider federation of AI/LT platforms. We illustrate the approach using the five emerging AI/LT platforms AI4EU, ELG, Lynx, QURATOR and SPEAKER."
2020.globalex-1.1,Modelling Frequency and Attestations for {O}nto{L}ex-Lemon,2020,-1,-1,8,0,2108,christian chiarcos,Proceedings of the 2020 Globalex Workshop on Linked Lexicography,0,"The OntoLex vocabulary enjoys increasing popularity as a means of publishing lexical resources with RDF and as Linked Data. The recent publication of a new OntoLex module for lexicography, lexicog, reflects its increasing importance for digital lexicography. However, not all aspects of digital lexicography have been covered to the same extent. In particular, supplementary information drawn from corpora such as frequency information, links to attestations, and collocation data were considered to be beyond the scope of lexicog. Therefore, the OntoLex community has put forward the proposal for a novel module for frequency, attestation and corpus information (FrAC), that not only covers the requirements of digital lexicography, but also accommodates essential data structures for lexical information in natural language processing. This paper introduces the current state of the OntoLex-FrAC vocabulary, describes its structure, some selected use cases, elementary concepts and fundamental definitions, with a focus on frequency and attestations."
2020.globalex-1.15,{NUIG} at {TIAD}: Combining Unsupervised {NLP} and Graph Metrics for Translation Inference,2020,-1,-1,1,1,1255,john mccrae,Proceedings of the 2020 Globalex Workshop on Linked Lexicography,0,"In this paper, we present the NUIG system at the TIAD shard task. This system includes graph-based metrics calculated using novel algorithms, with an unsupervised document embedding tool called ONETA and an unsupervised multi-way neural machine translation method. The results are an improvement over our previous system and produce the highest precision among all systems in the task as well as very competitive F-Measure results. Incorporating features from other systems should be easy in the framework we describe in this paper, suggesting this could very easily be extended to an even stronger result."
2020.findings-emnlp.36,Contextual Modulation for Relation-Level Metaphor Identification,2020,-1,-1,2,1,18055,omnia zayed,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"Identifying metaphors in text is very challenging and requires comprehending the underlying comparison. The automation of this cognitive process has gained wide attention lately. However, the majority of existing approaches concentrate on word-level identification by treating the task as either single-word classification or sequential labelling without explicitly modelling the interaction between the metaphor components. On the other hand, while existing relation-level approaches implicitly model this interaction, they ignore the context where the metaphor occurs. In this work, we address these limitations by introducing a novel architecture for identifying relation-level metaphoric expressions of certain grammatical relations based on contextual modulation. In a methodology inspired by works in visual reasoning, our approach is based on conditioning the neural network computation on the deep contextualised features of the candidate expressions using feature-wise linear modulation. We demonstrate that the proposed architecture achieves state-of-the-art results on benchmark datasets. The proposed methodology is generic and could be applied to other textual classification problems that benefit from contextual interaction."
2020.figlang-1.22,Adaptation of Word-Level Benchmark Datasets for Relation-Level Metaphor Identification,2020,-1,-1,2,1,18055,omnia zayed,Proceedings of the Second Workshop on Figurative Language Processing,0,"Metaphor processing and understanding has attracted the attention of many researchers recently with an increasing number of computational approaches. A common factor among these approaches is utilising existing benchmark datasets for evaluation and comparisons. The availability, quality and size of the annotated data are among the main difficulties facing the growing research area of metaphor processing. The majority of current approaches pertaining to metaphor processing concentrate on word-level processing due to data availability. On the other hand, approaches that process metaphors on the relation-level ignore the context where the metaphoric expression. This is due to the nature and format of the available data. Word-level annotation is poorly grounded theoretically and is harder to use in downstream tasks such as metaphor interpretation. The conversion from word-level to relation-level annotation is non-trivial. In this work, we attempt to fill this research gap by adapting three benchmark datasets, namely the VU Amsterdam metaphor corpus, the TroFi dataset and the TSV dataset, to suit relation-level metaphor identification. We publish the adapted datasets to facilitate future research in relation-level metaphor processing."
2020.coling-main.141,Unsupervised Deep Language and Dialect Identification for Short Texts,2020,-1,-1,5,1,1254,koustava goswami,Proceedings of the 28th International Conference on Computational Linguistics,0,"Automatic Language Identification (LI) or Dialect Identification (DI) of short texts of closely related languages or dialects, is one of the primary steps in many natural language processing pipelines. Language identification is considered a solved task in many cases; however, in the case of very closely related languages, or in an unsupervised scenario (where the languages are not known in advance), performance is still poor. In this paper, we propose the Unsupervised Deep Language and Dialect Identification (UDLDI) method, which can simultaneously learn sentence embeddings and cluster assignments from short texts. The UDLDI model understands the sentence constructions of languages by applying attention to character relations which helps to optimize the clustering of languages. We have performed our experiments on three short-text datasets for different language families, each consisting of closely related languages or dialects, with very minimal training sets. Our experimental evaluations on these datasets have shown significant improvement over state-of-the-art unsupervised methods and our model has outperformed state-of-the-art LI and DI systems in supervised settings."
2020.coling-main.369,Suggest me a movie for tonight: Leveraging Knowledge Graphs for Conversational Recommendation,2020,-1,-1,4,1,3060,rajdeep sarkar,Proceedings of the 28th International Conference on Computational Linguistics,0,"Conversational recommender systems focus on the task of suggesting products to users based on the conversation flow. Recently, the use of external knowledge in the form of knowledge graphs has shown to improve the performance in recommendation and dialogue systems. Information from knowledge graphs aids in enriching those systems by providing additional information such as closely related products and textual descriptions of the items. However, knowledge graphs are incomplete since they do not contain all factual information present on the web. Furthermore, when working on a specific domain, knowledge graphs in its entirety contribute towards extraneous information and noise. In this work, we study several subgraph construction methods and compare their performance across the recommendation task. We incorporate pre-trained embeddings from the subgraphs along with positional embeddings in our models. Extensive experiments show that our method has a relative improvement of at least 5.62{\%} compared to the state-of-the-art on multiple metrics on the recommendation task."
2020.cogalex-1.8,{C}og{AL}ex-{VI} Shared Task: Bidirectional Transformer based Identification of Semantic Relations,2020,-1,-1,2,0,21791,saurav karmakar,Proceedings of the Workshop on the Cognitive Aspects of the Lexicon,0,This paper presents a bidirectional transformer based approach for recognising semantic relationships between a pair of words as proposed by CogALex VI shared task in 2020. The system presented here works by employing BERT embeddings of the words and passing the same over tuned neural network to produce a learning model for the pair of words and their relationships. Afterwards the very same model is used for the relationship between unknown words from the test set. CogALex VI provided Subtask 1 as the identification of relationship of three specific categories amongst English pair of words and the presented system opts to work on that. The resulted relationships of the unknown words are analysed here which shows a balanced performance in overall characteristics with some scope for improvement.
W19-7101,{W}ord{N}et Gloss Translation for Under-resourced Languages using Multilingual Neural Machine Translation,2019,0,2,3,1,613,bharathi chakravarthi,Proceedings of the Second Workshop on Multilingualism at the Intersection of Knowledge Bases and Machine Translation,0,"This work was supported by the Spanish Ministry of Economy andn Competitiveness (MINECO) FPI grant numbern BES-2017-081045, and projects BigKnowledge (BBVA foundation grant 2018), DOMINOn (PGC2018-102041-B-I00, MCIU/AEI/FEDER,n UE) and PROSA-MED (TIN2016-77820-C3-n 1-R, MCIU/AEI/FEDER, UE). We thank Uxoan Inurrieta for helping us with the glosses."
W19-6907,Adapting Term Recognition to an Under-Resourced Language: the Case of {I}rish,2019,-1,-1,1,1,1255,john mccrae,Proceedings of the Celtic Language Technology Workshop,0,None
W19-6910,"A Character-Level {LSTM} Network Model for Tokenizing the {O}ld {I}rish text of the {W}{\\\u}rzburg Glosses on the {P}auline Epistles""",2019,-1,-1,2,0,23552,adrian doyle,Proceedings of the Celtic Language Technology Workshop,0,None
W19-6809,Multilingual Multimodal Machine Translation for {D}ravidian Languages utilizing Phonetic Transcription,2019,-1,-1,8,1,613,bharathi chakravarthi,Proceedings of the 2nd Workshop on Technologies for MT of Low Resource Languages,0,None
W19-5116,Identification of Adjective-Noun Neologisms using Pretrained Language Models,2019,0,0,1,1,1255,john mccrae,Proceedings of the Joint Workshop on Multiword Expressions and WordNet (MWE-WN 2019),0,"Neologism detection is a key task in the constructing of lexical resources and has wider implications for NLP, however the identification of multiword neologisms has received little attention. In this paper, we show that we can effectively identify the distinction between compositional and non-compositional adjective-noun pairs by using pretrained language models and comparing this with individual word embeddings. Our results show that the use of these models significantly improves over baseline linguistic features, however the combination with linguistic features still further improves the results, suggesting the strength of a hybrid approach."
2019.gwc-1.31,{E}nglish {W}ord{N}et 2019 {--} An Open-Source {W}ord{N}et for {E}nglish,2019,-1,-1,1,1,1255,john mccrae,Proceedings of the 10th Global Wordnet Conference,0,"We describe the release of a new wordnet for English based on the Princeton WordNet, but now developed under an open-source model. In particular, this version of WordNet, which we call English WordNet 2019, which has been developed by multiple people around the world through GitHub, fixes many errors in previous wordnets for English. We give some details of the changes that have been made in this version and give some perspectives about likely future changes that will be made as this project continues to evolve."
W18-4921,Constructing an Annotated Corpus of Verbal {MWE}s for {E}nglish,2018,0,1,4,0,14283,abigail walsh,"Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions ({LAW}-{MWE}-{C}x{G}-2018)",0,"This paper describes the construction and annotation of a corpus of verbal MWEs for English, as part of the PARSEME Shared Task 1.1 on automatic identification of verbal MWEs. The criteria for corpus selection, the categories of MWEs used, and the training process are discussed, along with the particular issues that led to revisions in edition 1.1 of the annotation guidelines. Finally, an overview of the characteristics of the final annotated corpus is presented, as well as some discussion on inter-annotator agreement."
W18-0910,Phrase-Level Metaphor Identification Using Distributed Representations of Word Meaning,2018,0,1,2,1,18055,omnia zayed,Proceedings of the Workshop on Figurative Language Processing,0,"Metaphor is an essential element of human cognition which is often used to express ideas and emotions that might be difficult to express using literal language. Processing metaphoric language is a challenging task for a wide range of applications ranging from text simplification to psychotherapy. Despite the variety of approaches that are trying to process metaphor, there is still a need for better models that mimic the human cognition while exploiting fewer resources. In this paper, we present an approach based on distributional semantics to identify metaphors on the phrase-level. We investigated the use of different word embeddings models to identify verb-noun pairs where the verb is used metaphorically. Several experiments are conducted to show the performance of the proposed approach on benchmark datasets."
L18-1149,Automatic Enrichment of Terminological Resources: the {IATE} {RDF} Example,2018,0,0,3,0.69117,6275,mihael arcan,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"This publication is supported by a research grant from Science Foundation Ireland, SFI/12/RC/2289 (Insight), a research visit grant from Universidad Politecnica de Madrid, xc2xb4n by the Spanish Datos4.0 project (TIN2016-78011-C4-4-R)n and by the EUxe2x80x99s Lynx project (H2020 Research and Innovation Programme under GA num 780602)."
L18-1192,A Comparison Of Emotion Annotation Schemes And A New Annotated Data Set,2018,0,4,2,0,4046,ian wood,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,We would like to thank volunteers from the Insight Centren for Data Analytics for their efforts in pilot study annotations.n This work was supported in part by the Science Foundationn Ireland under Grant Number 16/IFB/4336 and Grantn Number SFI/12/RC/2289 (Insight). The research leadingn to these results has received funding from the Europeann Unionxe2x80x99s Horizon 2020 research and innovation programmen under grant agreements No. 644632 (MixedEmotions).
L18-1324,A supervised approach to taxonomy extraction using word embeddings,2018,0,1,2,1,3060,rajdeep sarkar,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1383,{T}eanga: A Linked Data based platform for Natural Language Processing,2018,0,3,2,0,29939,housam ziad,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
2018.gwc-1.8,Mapping {W}ord{N}et Instances to {W}ikipedia,2018,-1,-1,1,1,1255,john mccrae,Proceedings of the 9th Global Wordnet Conference,0,"Lexical resource differ from encyclopaedic resources and represent two distinct types of resource covering general language and named entities respectively. However, many lexical resources, including Princeton WordNet, contain many proper nouns, referring to named entities in the world yet it is not possible or desirable for a lexical resource to cover all named entities that may reasonably occur in a text. In this paper, we propose that instead of including synsets for instance concepts PWN should instead provide links to Wikipedia articles describing the concept. In order to enable this we have created a gold-quality mapping between all of the 7,742 instances in PWN and Wikipedia (where such a mapping is possible). As such, this resource aims to provide a gold standard for link discovery, while also allowing PWN to distinguish itself from other resources such as DBpedia or BabelNet. Moreover, this linking connects PWN to the Linguistic Linked Open Data cloud, thus creating a richer, more usable resource for natural language processing."
2018.gwc-1.10,Improving Wordnets for Under-Resourced Languages Using Machine Translation,2018,23,2,3,1,613,bharathi chakravarthi,Proceedings of the 9th Global Wordnet Conference,0,"Wordnets are extensively used in natural language processing, but the current approaches for manually building a wordnet from scratch involves large research groups for a long period of time, which are typically not available for under-resourced languages. Even if wordnet-like resources are available for under-resourced languages, they are often not easily accessible, which can alter the results of applications using these resources. Our proposed method presents an expand approach for improving and generating wordnets with the help of machine translation. We apply our methods to improve and extend wordnets for the Dravidian languages, i.e., Tamil, Telugu, Kannada, which are severly under-resourced languages. We report evaluation results of the generated wordnet senses in term of precision for these languages. In addition to that, we carried out a manual evaluation of the translations for the Tamil language, where we demonstrate that our approach can aid in improving wordnet resources for under-resourced Dravidian languages."
2018.gwc-1.40,{ELEXIS} - a {E}uropean infrastructure fostering cooperation and information exchange among lexicographical research communities,2018,-1,-1,2,0,6194,bolette pedersen,Proceedings of the 9th Global Wordnet Conference,0,"The paper describes objectives, concept and methodology for ELEXIS, a European infrastructure fostering cooperation and information exchange among lexicographical research communities. The infrastructure is a newly granted project under the Horizon 2020 INFRAIA call, with the topic Integrating Activities for Starting Communities. The project is planned to start in January 2018."
2018.gwc-1.51,Towards a Crowd-Sourced {W}ord{N}et for Colloquial {E}nglish,2018,-1,-1,1,1,1255,john mccrae,Proceedings of the 9th Global Wordnet Conference,0,"Princeton WordNet is one of the most widely-used resources for natural language processing, but is updated only infrequently and cannot keep up with the fast-changing usage of the English language on social media platforms such as Twitter. The Colloquial WordNet aims to provide an open platform whereby anyone can contribute, while still following the structure of WordNet. Many crowd-sourced lexical resources often have significant quality issues, and as such care must be taken in the design of the interface to ensure quality. In this paper, we present the development of a platform that can be opened on the Web to any lexicographer who wishes to contribute to this resource and the lexicographic methodology applied by this interface."
S16-1110,{NUIG}-{UNLP} at {S}em{E}val-2016 Task 1: Soft Alignment and Deep Learning for Semantic Textual Similarity,2016,10,0,1,1,1255,john mccrae,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,None
L16-1386,The Open Linguistics Working Group: Developing the Linguistic Linked Open Data Cloud,2016,16,11,1,1,1255,john mccrae,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"The Open Linguistics Working Group (OWLG) brings together researchers from various fields of linguistics, natural language processing, and information technology to present and discuss principles, case studies, and best practices for representing, publishing and linking linguistic data collections. A major outcome of our work is the Linguistic Linked Open Data (LLOD) cloud, an LOD (sub-)cloud of linguistic resources, which covers various linguistic databases, lexicons, corpora, terminologies, and metadata repositories. We present and summarize five years of progress on the development of the cloud and of advancements in open data in linguistics, and we describe recent community activities. The paper aims to serve as a guideline to orient and involve researchers with the community and/or Linguistic Linked Open Data."
C16-1010,Expanding wordnets to new languages with multilingual sense disambiguation,2016,33,5,2,0.628274,6275,mihael arcan,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Princeton WordNet is one of the most important resources for natural language processing, but is only available for English. While it has been translated using the expand approach to many other languages, this is an expensive manual process. Therefore it would be beneficial to have a high-quality automatic translation approach that would support NLP techniques, which rely on WordNet in new languages. The translation of wordnets is fundamentally complex because of the need to translate all senses of a word including low frequency senses, which is very challenging for current machine translation approaches. For this reason we leverage existing translations of WordNet in other languages to identify contextual information for wordnet senses from a large set of generic parallel corpora. We evaluate our approach using 10 translated wordnets for European languages. Our experiment shows a significant improvement over translation without any contextual information. Furthermore, we evaluate how the choice of pivot languages affects performance of multilingual word sense disambiguation."
2016.gwc-1.9,{CILI}: the Collaborative Interlingual Index,2016,0,15,3,0,6126,francis bond,Proceedings of the 8th Global WordNet Conference (GWC),0,"This paper introduces the motivation for and design of the Collaborative InterLingual Index (CILI). It is designed to make possible coordination between multiple loosely coupled wordnet projects. The structure of the CILI is based on the Interlingual index first proposed in the EuroWordNet project with several pragmatic extensions: an explicit open license, definitions in English and links to wordnets in the Global Wordnet Grid."
2016.gwc-1.59,Toward a truly multilingual {G}lobal{W}ordnet Grid,2016,-1,-1,3,0,5469,piek vossen,Proceedings of the 8th Global WordNet Conference (GWC),0,"In this paper, we describe a new and improved Global Wordnet Grid that takes advantage of the Collaborative InterLingual Index (CILI). Currently, the Open Multilingal Wordnet has made many wordnets accessible as a single linked wordnet, but as it used the Princeton Wordnet of English (PWN) as a pivot, it loses concepts that are not part of PWN. The technical solution to this, a central registry of concepts, as proposed in the EuroWordnet project through the InterLingual Index, has been known for many years. However, the practical issues of how to host this index and who decides what goes in remained unsolved. Inspired by current practice in the Semantic Web and the Linked Open Data community, we propose a way to solve this issue. In this paper we define the principles and protocols for contributing to the Grid. We tested them on two use cases, adding version 3.1 of the Princeton WordNet to a CILI based on 3.0 and adding the Open Dutch Wordnet, to validate the current set up. This paper aims to be a call for action that we hope will be further discussed and ultimately taken up by the whole wordnet community."
W15-4205,Reconciling Heterogeneous Descriptions of Language Resources,2015,19,3,1,1,1255,john mccrae,Proceedings of the 4th Workshop on Linked Data in Linguistics: Resources and Applications,0,"Language resources are a cornerstone of linguistic research and for the development of natural language processing tools, but the discovery of relevant resources remains a challenging task. This is due to the fact that relevant metadata records are spread among different repositories and it is currently impossible to query all these repositories in an integrated fashion, as they use different data models and vocabularies. In this paper we present a first attempt to collect and harmonize the metadata of different repositories, thus making them queriable and browsable in an integrated way. We make use of RDF and linked data technologies for this and provide a first level of harmonization of the vocabularies used in the different resources by mapping them to standard RDF vocabularies including Dublin Core and DCAT. Further, we present an approach that relies on NLP and in particular word sense disambiguation techniques to harmonize resources by mapping values of attributes xe2x80x90 such as the type, license or intended use of a resource xe2x80x90 into normalized values. Finally, as there are duplicate entries within the same repository as well as across different repositories, we also report results of detection of these duplicates."
W15-4207,Linking Four Heterogeneous Language Resources as Linked Data,2015,8,2,2,0,36722,benjamin siemoneit,Proceedings of the 4th Workshop on Linked Data in Linguistics: Resources and Applications,0,"The interest in publishing language resources as linked data is increasing, as clearly corroborated by the recent growth of the Linguistic Linked Data cloud. However, the actual value of data published as linked data is the fact that it is linked across datasets, supporting integration and discovery of data. As the manual creation of links between datasets is costly and therefore does not scale well, automatic linking approaches are of great importance to increase the quality and degree of linking of the Linguistic Linked Data cloud. In this paper we examine an automatic approach to link four different datasets to each other: two terminologies, the European Migration Network (EMN) glossary as well as the Interactive Terminology for Europe (IATE), BabelNet, and the Manually Annotated Subcorpus (MASC) of the American National Corpus. We describe our methodology, present some results on the quality of the links and summarize our experiences with this small linking exercise We will make sure that the resources are added to the linguistic linked data cloud."
W14-4718,Default Physical Measurements in {SUMO},2014,7,0,3,0,38374,francesca quattri,Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex),0,"The following paper presents a further extension of the Suggested Upper Merged Ontology (SUMO), i. e. the development of default physical measurements for most of its classes (Artifacts, Devices, Objects) and respective children. The extension represents an arbitrary, computable and reproducible approximation of defaults for upper and middle-level concepts. The paper illustrates advantages of such extension, challenges encountered during the compilation, related work and future research."
W14-4724,Modelling the Semantics of Adjectives in the Ontology-Lexicon Interface,2014,0,6,1,1,1255,john mccrae,Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex),0,None
S14-2016,Bielefeld {SC}: Orthonormal Topic Modelling for Grammar Induction,2014,5,0,1,1,1255,john mccrae,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"In this paper, we consider the application of topic modelling to the task of inducting grammar rules. In particular, we look at the use of a recently developed method called orthonormal explicit topic analysis, which combines explicit and latent models of semantics. Although, it remains unclear how topic model may be applied to the case of grammar induction, we show that it is not impossible and that this may allow the capture of subtle semantic distinctions that are not captured by other methods."
ehrmann-etal-2014-representing,Representing Multilingual Data as Linked Data: the Case of {B}abel{N}et 2.0,2014,36,40,4,0,16860,maud ehrmann,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Recent years have witnessed a surge in the amount of semantic information published on the Web. Indeed, the Web of Data, a subset of the Semantic Web, has been increasing steadily in both volume and variety, transforming the Web into a {`}global database{'} in which resources are linked across sites. Linguistic fields -- in a broad sense -- have not been left behind, and we observe a similar trend with the growth of linguistic data collections on the so-called {`}Linguistic Linked Open Data (LLOD) cloud{'}. While both Semantic Web and Natural Language Processing communities can obviously take advantage of this growing and distributed linguistic knowledge base, they are today faced with a new challenge, i.e., that of facilitating multilingual access to the Web of data. In this paper we present the publication of BabelNet 2.0, a wide-coverage multilingual encyclopedic dictionary and ontology, as Linked Data. The conversion made use of lemon, a lexicon model for ontologies particularly well-suited for this enterprise. The result is an interlinked multilingual (lexical) resource which can not only be accessed on the LOD, but also be used to enrich existing datasets with linguistic information, or to support the process of mapping datasets across languages."
W13-5501,Linguistic Linked Open Data ({LLOD}). Introduction and Overview,2013,13,2,4,0,2108,christian chiarcos,"Proceedings of the 2nd Workshop on Linked Data in Linguistics ({LDL}-2013): Representing and linking lexicons, terminologies and other language data",0,None
W13-5507,Releasing multimodal data as Linguistic Linked Open Data: An experience report,2013,13,2,2,0,40532,peter menke,"Proceedings of the 2nd Workshop on Linked Data in Linguistics ({LDL}-2013): Representing and linking lexicons, terminologies and other language data",0,"In this paper we describe an implemented framework for releasing multimodal corpora as Linked Data. In particular, we describe our experiences in releasing a multimodal corpus based on an online chat game as Linked Data. Building on an internal multimodal data model we call FiESTA, we have implemented a library that enhances existing libraries and classes by functionality allowing to convert the data to RDF. Our framework is implemented on the Rails web application framework. We argue that this work can be highly useful for further contributions to the Linked Data community, especially from the fields of spoken dialogue and multimodal communication."
W13-5203,Mining translations from the web of open linked data,2013,15,6,1,1,1255,john mccrae,"Proceedings of the Joint Workshop on {NLP}{\\&}{LOD} and {SWAIE}: Semantic Web, Linked Open Data and Information Extraction",0,"In this paper we consider the prospect of extracting translations for words from the web of linked data. By searching for entities that have labels in both English and German we extract 665,000 translations. We then also consider a linguistic linked data resource, lemonUby, from which we extract a further 115,000 translations. We combine these translations with the Moses statistical machine translation, and we show that the translations extracted from the linked data can be used to improve the translation of unknown words."
D13-1179,Orthonormal Explicit Topic Analysis for Cross-Lingual Document Matching,2013,18,9,1,1,1255,john mccrae,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"Cross-lingual topic modelling has applications in machine translation, word sense disambiguation and terminology alignment. Multilingual extensions of approaches based on latent (LSI), generative (LDA, PLSI) as well as explicit (ESA) topic modelling can induce an interlingual topic space allowing documents in different languages to be mapped into the same space and thus to be compared across languages. In this paper, we present a novel approach that combines latent and explicit topic modelling approaches in the sense that it builds on a set of explicitly defined topics, but then computes latent relations between these. Thus, the method combines the benefits of both explicit and latent topic modelling approaches. We show that on a crosslingual mate retrieval task, our model significantly outperforms LDA, LSI, and ESA, as well as a baseline that translates every word in a document into the target language."
mccrae-etal-2012-collaborative,Collaborative semantic editing of linked data lexica,2012,23,12,1,1,1255,john mccrae,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"The creation of language resources is a time-consuming process requiring the efforts of many people. The use of resources collaboratively created by non-linguistists can potentially ameliorate this situation. However, such resources often contain more errors compared to resources created by experts. For the particular case of lexica, we analyse the case of Wiktionary, a resource created along wiki principles and argue that through the use of a principled lexicon model, namely Lemon, the resulting data could be better understandable to machines. We then present a platform called Lemon Source that supports the creation of linked lexical data along the Lemon model. This tool builds on the concept of a semantic wiki to enable collaborative editing of the resources by many users concurrently. In this paper, we describe the model, the tool and present an evaluation of its usability based on a small group of users."
W11-1013,Combining statistical and semantic approaches to the translation of ontologies and taxonomies,2011,30,19,1,1,1255,john mccrae,"Proceedings of Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation",0,"Ontologies and taxonomies are widely used to organize concepts providing the basis for activities such as indexing, and as background knowledge for NLP tasks. As such, translation of these resources would prove useful to adapt these systems to new languages. However, we show that the nature of these resources is significantly different from the free-text paradigm used to train most statistical machine translation systems. In particular, we see significant differences in the linguistic nature of these resources and such resources have rich additional semantics. We demonstrate that as a result of these linguistic differences, standard SMT methods, in particular evaluation metrics, can produce poor performance. We then look to the task of leveraging these semantics for translation, which we approach in three ways: by adapting the translation system to the domain of the resource; by examining if semantics can help to predict the syntactic structure used in translation; and by evaluating if we can use existing translated taxonomies to disambiguate translations. We present some early results from these experiments, which shed light on the degree of success we may have with each approach."
C10-1025,An ontology-driven system for detecting global health events,2010,19,20,3,0,219,nigel collier,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,Text mining for global health surveillance is an emerging technology that is gaining increased attention from public health organisations and governments. The lack of multilingual resources such as WordNets specifically targeted at this task have so far been a major bottleneck. This paper reports on a major upgrade to the BioCaster Web monitoring system and its freely available multilingual ontology; improving its original design and extending its coverage of diseases from 70 to 336 in 12 languages.
