2019.gwc-1.34,P13-1133,0,0.041303,"o other types of lexical data, such as decomposition results, exemplified for German data, and inflectional phenomena, here outlined for English data. 1 Introduction In the following sections we first describe OdeNet, before presenting the main characteristics of OntoLex-Lemon. In Section 4 we present the current state of the mapping from OdeNet to OntoLex-Lemon, before finally discussing the potential added-value of having WordNets represented in OntoLex-Lemon. 2 OdeNet OdeNet combines two existing resources: The OpenThesaurus German synonym lexicon6 and the Open Multilingual WordNet (OMW)7 (Bond and Foster, 2013). In terms of English resources, it includes the Princeton WordNet of English (PWN) (Fellbaum, 1998). Integrating OpenThesaurus in OdeNet means making use of a large resource for German that is generated and updated by the crowd. A consequence of this approach is that OdeNet needs to be curated, as the authors of the resource mention. We downloaded the most recent version8 and first analyzed its content. OdeNet is in an XML format and shares its Document Type Definition (DTD)9 with other WordNets in the OMW initiative. Lexical entries provide information on different senses of a lexeme, such a"
2019.gwc-1.34,E12-1059,0,0.034471,"ever, its access is restricted by its current license. Without such open data access, reuse of GermaNet in global initiatives, such as Open Multilingual WordNet (OMW) (Bond and Paik, 2012), is inhibited. One of the objectives of this paper is to represent lexical semantics data openly linked to other OMW datasets in the LLOD. Two alternatives compliant with WordNet specifications and available under an open-source license are the lemonUby set of resources (EckleKohler et al., 2015) and the Open-de-WordNet (OdeNet) effort.3 lemonUby is an export of lexical data from the large-scale linked UBY (Gurevych et al., 2012)4 , which unites collaboratively and expert-developed resources (e.g. FrameNet and Wiktionary) in English and German, to lemon. lemonUby contains the German version of OmegaWiki5 , which encodes WordNet compliant descriptions of German words. OdeNet provides a German resource to the OMW initiative. A mapping from lemonUby to OntoLex-Lemon can be expected to be straightforward due to a high compliance between both models. Thus, this publication concentrates on mapping the WordNetcompliant XML code of OdeNet to OntoLexLemon, while in the long run a cross-linking or, where possible, a merging of"
2019.gwc-1.34,W97-0802,0,0.56509,"Missing"
2019.gwc-1.34,L16-1386,1,0.717522,"Missing"
2019.gwc-1.34,P79-1022,0,0.438632,"Missing"
2020.globalex-1.1,2020.lrec-1.695,1,0.81461,"Missing"
2020.globalex-1.5,2016.gwc-1.43,0,0.0183031,"Missing"
2020.globalex-1.5,vossen-etal-2008-integrating,0,0.10643,"Missing"
2020.globalex-1.5,P13-1133,0,0.0117494,"sed in the Dutch edition. When discovering discrepancies between the two, we check manually if a corresponding entry is included in the “Algemeen Nederlands Woordenboek”,7 as a referential point for taking a decision on which data source is to be selected. Work on interlinking or merging language data for Italian, Spanish and French included in wordnets on the one side and morphological data sets on the other side is documented in (Racioppa and Declerck, 2019). The authors accessed for this experiment Wordnet data that are available at the Open Multilingual Wordnet (OMW, (Bond and Paik, 2012; Bond and Foster, 2013)) portal.1 OMW brings together wordnets in different languages, harmonizing them in a uniform tabular format that lists synsets IDs and the associated lemmas. OMW is linking those Wordnets to the original Princeton WordNet (PWN, (Miller, 1995; Fellbaum, 1998)). Additionally, XML versions of LMF and lemon representations2 of the data are provided. The morphological data used in those experiments were taken from updated versions of the MMorph data sets.3 (Declerck et al., 2019) describe a similar experiment conducted for combining the German data from MMorph with an emerging lexical semantics re"
2020.globalex-1.5,2019.gwc-1.34,1,0.764199,"cessed for this experiment Wordnet data that are available at the Open Multilingual Wordnet (OMW, (Bond and Paik, 2012; Bond and Foster, 2013)) portal.1 OMW brings together wordnets in different languages, harmonizing them in a uniform tabular format that lists synsets IDs and the associated lemmas. OMW is linking those Wordnets to the original Princeton WordNet (PWN, (Miller, 1995; Fellbaum, 1998)). Additionally, XML versions of LMF and lemon representations2 of the data are provided. The morphological data used in those experiments were taken from updated versions of the MMorph data sets.3 (Declerck et al., 2019) describe a similar experiment conducted for combining the German data from MMorph with an emerging lexical semantics resource for German. In all those experiments, the OntoLex-Lemon model (Cimiano et al., 2016)4 was used for representing the linking and merging of the language data originating from both the Wordnet and the MMorph frameworks. In our current work we expand this kind of experiments beyond the use of morphologies and consider also full lexical resources. 1 See http://compling.hss.ntu.edu.sg/omw/ for downloading the resources. 2 LMF stands for“Lexical Markup Framework”, an ISO sta"
2020.globalex-1.5,francopoulo-etal-2006-lexical,0,0.374994,"similar experiment conducted for combining the German data from MMorph with an emerging lexical semantics resource for German. In all those experiments, the OntoLex-Lemon model (Cimiano et al., 2016)4 was used for representing the linking and merging of the language data originating from both the Wordnet and the MMorph frameworks. In our current work we expand this kind of experiments beyond the use of morphologies and consider also full lexical resources. 1 See http://compling.hss.ntu.edu.sg/omw/ for downloading the resources. 2 LMF stands for“Lexical Markup Framework”, an ISO standard. See (Francopoulo et al., 2006) and http://www. lexicalmarkupframework.org/ for more details. lemon stands for “LExicon MOdel for oNtologies”. See (McCrae et al., 2012a) and https://lemon-model.net/ for more details. 3 See (Petitpierre and Russell, 1995). 4 OntoLex-Lemon is a further development of the lemon model. See also https://www.w3.org/2016/05/ ontolex/ for more details on the model. 5 See (Navigli and Ponzetto, 2010) and https: //babelnet.org/. 6 As far as we are aware of, BabelNet integrates only the English edition of Wiktionary, but includes all the languages covered by this edition. 7 See http://anw.inl.nl/ and"
2020.globalex-1.5,L16-1498,0,0.0259504,"relevant number of complex lexical units, we are aiming at expanding it and proposing a new representational framework for the encoding of the interlinked and integrated data. The longer term goal of the work is to investigate if and on how senses can be restricted to particular morphological variations of Dutch lexical entries, and how to represent this information in a Linguistic Linked Open Data compliant format. Keywords: Dutch WordNet, Lexicography, Linking, OntoLex-Lemon 1. Introduction It has been shown that the access and use of Wiktionary can be helpful in a series of applications. (Kirov et al., 2016), for example, describe work to extract and standardize the data in Wiktionary and to make it available for a range of NLP applications, while the authors focus on extracting and normalizing a huge number of inflectional paradigms across a large selection of languages. This effort contributed to the creation of the UniMorph data (http://unimorph.org/). BabelNet5 is integrating Witkionary data6 with a focus on sense information, in order to support, among others, word sense disambiguation and tasks dealing with word similarity and sense clustering (Camacho-Collados et al., 2016). (McCrae et al."
2020.globalex-1.5,P10-1023,0,0.493229,"Missing"
2020.iwltp-1.2,L16-1707,1,0.790295,"e applications tailored to the needs of linguists, lexicographers, researchers in NLP and knowledge engineering. Promising approaches in this direction do exist: Existing tools can be complemented with an RDF layer to facilitate their interoperability. Likewise, LLOD-native applications are possible, e.g., to use RDFa (RDF in attributes) (Herman et al., 2015) to complement an XML workflow with SPARQL-based semantic search by means of web services (Sabine Tittel and Chiarcos, 2018), to provide aggregation, enrichment and search routines for language resource metadata (McCrae and Cimiano, 2015; Chiarcos et al., 2016), to use RDF as a formalism for annotation integration and data management (Burchardt et al., 2008; Chiarcos et al., 2017), or to use RDF and SPARQL for manipulating and evaluating linguistic annotations (Chiarcos et al., 2018b; Chiarcos et al., 2018a). While these applications demonstrate the potential of LOD technology in linguistics, they come with a considerable entry barrier and they address the advanced user of RDF technology rather than a typical linguist. Even though concrete applications to exist, a long way is still to go to achieve the level of user-friendliness expected by occasion"
2020.iwltp-1.2,L18-1717,1,0.852139,"e their interoperability. Likewise, LLOD-native applications are possible, e.g., to use RDFa (RDF in attributes) (Herman et al., 2015) to complement an XML workflow with SPARQL-based semantic search by means of web services (Sabine Tittel and Chiarcos, 2018), to provide aggregation, enrichment and search routines for language resource metadata (McCrae and Cimiano, 2015; Chiarcos et al., 2016), to use RDF as a formalism for annotation integration and data management (Burchardt et al., 2008; Chiarcos et al., 2017), or to use RDF and SPARQL for manipulating and evaluating linguistic annotations (Chiarcos et al., 2018b; Chiarcos et al., 2018a). While these applications demonstrate the potential of LOD technology in linguistics, they come with a considerable entry barrier and they address the advanced user of RDF technology rather than a typical linguist. Even though concrete applications to exist, a long way is still to go to achieve the level of user-friendliness expected by occasional users of this technology. A notable exception in this regard is LexO (Bellandi et al., 2017), which is a graphical tool for the collaborative editing of lexical and ontological resources natively building on the OntoLex voc"
2020.iwltp-1.2,2020.lrec-1.395,1,0.811257,"Missing"
2020.iwltp-1.2,W17-7010,0,0.0379115,"t (Burchardt et al., 2008; Chiarcos et al., 2017), or to use RDF and SPARQL for manipulating and evaluating linguistic annotations (Chiarcos et al., 2018b; Chiarcos et al., 2018a). While these applications demonstrate the potential of LOD technology in linguistics, they come with a considerable entry barrier and they address the advanced user of RDF technology rather than a typical linguist. Even though concrete applications to exist, a long way is still to go to achieve the level of user-friendliness expected by occasional users of this technology. A notable exception in this regard is LexO (Bellandi et al., 2017), which is a graphical tool for the collaborative editing of lexical and ontological resources natively building on the OntoLex vocabulary and RDF, designed to conduct lexicographical work in a philological context (i.e., creating the Dictionnaire des Termes Médico-botaniques de l’Ancien Occitan). Other projects whose objective is to provide LLOD-based tools for specific areas of application have been recently approved, so that progress in this direction is to be expected within the next years. Ten years after the formation of the OWLG, the situation of linked data in language technology and l"
2020.iwltp-1.2,I08-1051,0,0.0564092,"e engineering. Promising approaches in this direction do exist: Existing tools can be complemented with an RDF layer to facilitate their interoperability. Likewise, LLOD-native applications are possible, e.g., to use RDFa (RDF in attributes) (Herman et al., 2015) to complement an XML workflow with SPARQL-based semantic search by means of web services (Sabine Tittel and Chiarcos, 2018), to provide aggregation, enrichment and search routines for language resource metadata (McCrae and Cimiano, 2015; Chiarcos et al., 2016), to use RDF as a formalism for annotation integration and data management (Burchardt et al., 2008; Chiarcos et al., 2017), or to use RDF and SPARQL for manipulating and evaluating linguistic annotations (Chiarcos et al., 2018b; Chiarcos et al., 2018a). While these applications demonstrate the potential of LOD technology in linguistics, they come with a considerable entry barrier and they address the advanced user of RDF technology rather than a typical linguist. Even though concrete applications to exist, a long way is still to go to achieve the level of user-friendliness expected by occasional users of this technology. A notable exception in this regard is LexO (Bellandi et al., 2017), w"
2020.iwltp-1.2,W07-1501,0,0.0696464,"ersion should be provided. Ecosystem RDF as a data exchange framework is maintained by an interdisciplinary, large and active community, and it comes with a developed infrastructure that provides APIs, database implementations, technical support and validators for various RDF-based languages, e.g., reasoners for OWL. For developers of linguistic resources, this ecosystem can provide technological support or off-the-shelf implementations for common problems, e.g., the development of a database that is capable of support flexible, graphbased data structures as necessary for multi-layer corpora (Ide and Suderman, 2007). Linked (Open) Data for Language Resources Publishing Linked Data allows resources to be globally and uniquely identified such that they can be retrieved through standard Web protocols. Moreover, resources can be easily linked to one another in a uniform fashion and thus become structurally interoperable. (Chiarcos et al., 2013) identified the five main benefits of Linked Data for Linguistics and NLP: Conceptual Interoperability Semantic Web technologies allow to provide, to maintain and to share centralized, but freely accessible terminology repositories. Reference to such terminology reposi"
2020.iwltp-1.2,wright-2004-global,0,0.123506,"ly referenced from any other resource on the Web through URIs. Similar to hyperlinks in the HTML web, the web of data created by these links allows navigation along these connections, and thereby to freely integrate information from different resources in the cloud. Dynamic Import When linguistic resources are interlinked by references to resolvable URIs instead of system-defined IDs (or static copies of parts from another resource), we always provide access to the most recent version of a resource. For communitymaintained terminology repositories like the ISO TC37/SC4 Data Category Registry (Wright, 2004; Windhouwer and Wright, 2012), for example, new categories, definitions or examples can be introduced occasionally, and this information is available immediately to anyone whose resources refer to ISOcat URIs. In order to preserve link consistency among Linguistic Linked Open Data resources, however, it is strongly advised to apply a proper versioning system such that backward-compatibility can be preserved: Adding concepts or examples is unproblematic, but when concepts are deleted, renamed or redefined, a new version should be provided. Ecosystem RDF as a data exchange framework is maintain"
2020.lrec-1.395,I13-1057,0,0.0253544,"reated dynamically for semantic relationship annotation. scribed our methodology in Section 3, we further elaborate on the challenges of sense annotation in Section 4. We evaluate the datasets in Section 5 and finally, conclude the paper in Section 6. 2. Related work Aligning senses across lexical resources has been attempted in several lexicographical milieus over the recent years. Such resources mainly include open-source dictionaries, WordNet and collaboratively-curated resources, such as Wikipedia. The latter has been shown to be reliable resources to construct accurate sense classifiers (Dandala et al., 2013). There has been a significant body of research in aligning English resources, particularly, Princeton WordNet with Wikipedia (including (Ruiz-Casado et al., 2005; Ponzetto and Navigli, 2010; Niemann and Gurevych, 2011; McCrae, 2018)), with the Longman Dictionary of Contemporary English and with Roget’s thesaurus (Kwong, 1998), with Wiktionary3 (Meyer and Gurevych, 2011) or with the Oxford Dictionary of English (Navigli, 2006). Meyer and Gurevych (2011) also present a manually-annotated dataset for WSA between the English WordNet and Wiktionary. On the other hand, there are a fewer number of m"
2020.lrec-1.395,erjavec-fiser-2006-building,0,0.137609,"Missing"
2020.lrec-1.395,E12-1059,0,0.155275,"geneity in content, which makes aligning information across resources and languages a challenging task. Word sense alignment (WSA) is a more specific task of linking dictionary content at sense level which has been proved to be beneficial in various NLP tasks, such as wordsense disambiguation (Navigli and Ponzetto, 2012), semantic role labeling (Palmer, 2009) and information extraction (Moro et al., 2013). Moreover, combining LSRs can enhance domain coverage in terms of the number of lexical items and types of lexical-semantic information (Shi and 1 Mihalcea, 2005; Ponzetto and Navigli, 2010; Gurevych et al., 2012). Given the current progress of artificial intelligence and the usage of data to train neural networks, annotated data with specific features play a crucial role to tackle data-driven challenges, particularly in NLP. In recent years, a few efforts have been made to create gold-standard dataset, i.e., a dataset of instances used for learning and fitting parameters, for aligning senses across monolingual resources including collaboratively-curated ones such as Wikipedia2 , and expert-made ones such as WordNet. However, the previous work is limited to a handful of languages and much of it is not"
2020.lrec-1.395,W97-0800,0,0.569498,"2011; McCrae, 2018)), with the Longman Dictionary of Contemporary English and with Roget’s thesaurus (Kwong, 1998), with Wiktionary3 (Meyer and Gurevych, 2011) or with the Oxford Dictionary of English (Navigli, 2006). Meyer and Gurevych (2011) also present a manually-annotated dataset for WSA between the English WordNet and Wiktionary. On the other hand, there are a fewer number of manually aligned monolingual resources in other languages. For instance, there have been considerable efforts in aligning lexical semantic resources (LSRs) in German, particularly, the GermaNet–the German Wordnet (Hamp and Feldweg, 1997) with the German Wiktionary (Henrich et al., 2011), with the German Wikipedia (Henrich et al., 2012) and with the Digital Dictionary of the German Language (Digitales W¨orterbuch der Deutschen Sprache (Klein and Geyken, 2010)) (Henrich et al., 2014). Gurevych et al. (2012) present UKB–a large-scale lexical-semantic resource containing pairwise sense alignments between a subset of nine resources in English and German which are mapped to a uniform representation. For Danish, aligning senses across modern lexical resources has been carried out in several projects in recent years (Pedersen et al.,"
2020.lrec-1.395,W14-0109,0,0.0200074,"resent a manually-annotated dataset for WSA between the English WordNet and Wiktionary. On the other hand, there are a fewer number of manually aligned monolingual resources in other languages. For instance, there have been considerable efforts in aligning lexical semantic resources (LSRs) in German, particularly, the GermaNet–the German Wordnet (Hamp and Feldweg, 1997) with the German Wiktionary (Henrich et al., 2011), with the German Wikipedia (Henrich et al., 2012) and with the Digital Dictionary of the German Language (Digitales W¨orterbuch der Deutschen Sprache (Klein and Geyken, 2010)) (Henrich et al., 2014). Gurevych et al. (2012) present UKB–a large-scale lexical-semantic resource containing pairwise sense alignments between a subset of nine resources in English and German which are mapped to a uniform representation. For Danish, aligning senses across modern lexical resources has been carried out in several projects in recent years (Pedersen et al., 2018), and a next natural step is to link these to historical Danish dictionaries. 3 https://www.wiktionary.org/ Pedersen et al. (2009) describe the semi-automatic compilation of a WordNet for Danish, DanNet, based on a monolingual dictionary, the"
2020.lrec-1.395,bel-etal-2000-simple,1,0.672745,"Missing"
2020.lrec-1.395,Q13-1013,0,0.110061,"nment of LSRs and applied it to the production of a three-way alignment of the English WordNet, Wikipedia and Wiktionary. Niemann and Gurevych (2011) propose a threshold-based Personalized PageRank method for extracting a set of Wikipedia articles as alignment candidates and automatically aligning them with WordNet synsets. This method yields a sense inventory of higher coverage in comparison to taxonomy mapping techniques where Wikipedia categories are aligned to WordNet synsets (Ponzetto and Navigli, 2009). Matuschek and Gurevych present the Dijkstra-WSA algorithm as a graph-based approach (Matuschek and Gurevych, 2013) and a machine learning approach where features such as sense distances and gloss similarities are used for the task of WSA (Matuschek and Gurevych, 2014). It should be noted that all of these approaches produce results that are of lower reliability than gold standard datasets such as the ones presented in this paper. 3233 3. Methodology The main goal of the current study is to provide semantic relationships between two sets of senses for the same lemmas in two monolingual dictionaries. As an example, Figure 1 illustrates the senses for the entry “clog” (verb) in the English WordNet (Miller, 1"
2020.lrec-1.395,C14-1025,0,0.31488,"ose a threshold-based Personalized PageRank method for extracting a set of Wikipedia articles as alignment candidates and automatically aligning them with WordNet synsets. This method yields a sense inventory of higher coverage in comparison to taxonomy mapping techniques where Wikipedia categories are aligned to WordNet synsets (Ponzetto and Navigli, 2009). Matuschek and Gurevych present the Dijkstra-WSA algorithm as a graph-based approach (Matuschek and Gurevych, 2013) and a machine learning approach where features such as sense distances and gloss similarities are used for the task of WSA (Matuschek and Gurevych, 2014). It should be noted that all of these approaches produce results that are of lower reliability than gold standard datasets such as the ones presented in this paper. 3233 3. Methodology The main goal of the current study is to provide semantic relationships between two sets of senses for the same lemmas in two monolingual dictionaries. As an example, Figure 1 illustrates the senses for the entry “clog” (verb) in the English WordNet (Miller, 1995) (left) and the Webster’s Dictionary 1913 (Webster and Slater, 1828) (right). For further clarification, we provide two case studies of Danish and Ita"
2020.lrec-1.395,2018.gwc-1.8,1,0.745977,"Section 6. 2. Related work Aligning senses across lexical resources has been attempted in several lexicographical milieus over the recent years. Such resources mainly include open-source dictionaries, WordNet and collaboratively-curated resources, such as Wikipedia. The latter has been shown to be reliable resources to construct accurate sense classifiers (Dandala et al., 2013). There has been a significant body of research in aligning English resources, particularly, Princeton WordNet with Wikipedia (including (Ruiz-Casado et al., 2005; Ponzetto and Navigli, 2010; Niemann and Gurevych, 2011; McCrae, 2018)), with the Longman Dictionary of Contemporary English and with Roget’s thesaurus (Kwong, 1998), with Wiktionary3 (Meyer and Gurevych, 2011) or with the Oxford Dictionary of English (Navigli, 2006). Meyer and Gurevych (2011) also present a manually-annotated dataset for WSA between the English WordNet and Wiktionary. On the other hand, there are a fewer number of manually aligned monolingual resources in other languages. For instance, there have been considerable efforts in aligning lexical semantic resources (LSRs) in German, particularly, the GermaNet–the German Wordnet (Hamp and Feldweg, 19"
2020.lrec-1.395,I11-1099,0,0.152048,"he recent years. Such resources mainly include open-source dictionaries, WordNet and collaboratively-curated resources, such as Wikipedia. The latter has been shown to be reliable resources to construct accurate sense classifiers (Dandala et al., 2013). There has been a significant body of research in aligning English resources, particularly, Princeton WordNet with Wikipedia (including (Ruiz-Casado et al., 2005; Ponzetto and Navigli, 2010; Niemann and Gurevych, 2011; McCrae, 2018)), with the Longman Dictionary of Contemporary English and with Roget’s thesaurus (Kwong, 1998), with Wiktionary3 (Meyer and Gurevych, 2011) or with the Oxford Dictionary of English (Navigli, 2006). Meyer and Gurevych (2011) also present a manually-annotated dataset for WSA between the English WordNet and Wiktionary. On the other hand, there are a fewer number of manually aligned monolingual resources in other languages. For instance, there have been considerable efforts in aligning lexical semantic resources (LSRs) in German, particularly, the GermaNet–the German Wordnet (Hamp and Feldweg, 1997) with the German Wiktionary (Henrich et al., 2011), with the German Wikipedia (Henrich et al., 2012) and with the Digital Dictionary of t"
2020.lrec-1.395,miller-gurevych-2014-wordnet,0,0.125981,"Missing"
2020.lrec-1.395,P06-1014,0,0.254458,"ies, WordNet and collaboratively-curated resources, such as Wikipedia. The latter has been shown to be reliable resources to construct accurate sense classifiers (Dandala et al., 2013). There has been a significant body of research in aligning English resources, particularly, Princeton WordNet with Wikipedia (including (Ruiz-Casado et al., 2005; Ponzetto and Navigli, 2010; Niemann and Gurevych, 2011; McCrae, 2018)), with the Longman Dictionary of Contemporary English and with Roget’s thesaurus (Kwong, 1998), with Wiktionary3 (Meyer and Gurevych, 2011) or with the Oxford Dictionary of English (Navigli, 2006). Meyer and Gurevych (2011) also present a manually-annotated dataset for WSA between the English WordNet and Wiktionary. On the other hand, there are a fewer number of manually aligned monolingual resources in other languages. For instance, there have been considerable efforts in aligning lexical semantic resources (LSRs) in German, particularly, the GermaNet–the German Wordnet (Hamp and Feldweg, 1997) with the German Wiktionary (Henrich et al., 2011), with the German Wikipedia (Henrich et al., 2012) and with the Digital Dictionary of the German Language (Digitales W¨orterbuch der Deutschen S"
2020.lrec-1.395,W11-0122,0,0.627617,"ally, conclude the paper in Section 6. 2. Related work Aligning senses across lexical resources has been attempted in several lexicographical milieus over the recent years. Such resources mainly include open-source dictionaries, WordNet and collaboratively-curated resources, such as Wikipedia. The latter has been shown to be reliable resources to construct accurate sense classifiers (Dandala et al., 2013). There has been a significant body of research in aligning English resources, particularly, Princeton WordNet with Wikipedia (including (Ruiz-Casado et al., 2005; Ponzetto and Navigli, 2010; Niemann and Gurevych, 2011; McCrae, 2018)), with the Longman Dictionary of Contemporary English and with Roget’s thesaurus (Kwong, 1998), with Wiktionary3 (Meyer and Gurevych, 2011) or with the Oxford Dictionary of English (Navigli, 2006). Meyer and Gurevych (2011) also present a manually-annotated dataset for WSA between the English WordNet and Wiktionary. On the other hand, there are a fewer number of manually aligned monolingual resources in other languages. For instance, there have been considerable efforts in aligning lexical semantic resources (LSRs) in German, particularly, the GermaNet–the German Wordnet (Hamp"
2020.lrec-1.395,P10-1154,0,0.289699,"nces in structure and heterogeneity in content, which makes aligning information across resources and languages a challenging task. Word sense alignment (WSA) is a more specific task of linking dictionary content at sense level which has been proved to be beneficial in various NLP tasks, such as wordsense disambiguation (Navigli and Ponzetto, 2012), semantic role labeling (Palmer, 2009) and information extraction (Moro et al., 2013). Moreover, combining LSRs can enhance domain coverage in terms of the number of lexical items and types of lexical-semantic information (Shi and 1 Mihalcea, 2005; Ponzetto and Navigli, 2010; Gurevych et al., 2012). Given the current progress of artificial intelligence and the usage of data to train neural networks, annotated data with specific features play a crucial role to tackle data-driven challenges, particularly in NLP. In recent years, a few efforts have been made to create gold-standard dataset, i.e., a dataset of instances used for learning and fitting parameters, for aligning senses across monolingual resources including collaboratively-curated ones such as Wikipedia2 , and expert-made ones such as WordNet. However, the previous work is limited to a handful of language"
2020.lrec-1.395,roventini-ruimy-2008-mapping,0,0.0409542,"the semantic level of a quadripartite Italian lexicon. Its structure is inspired by Generative Lexicon theory (Pustejovsky, 1995) and in particular the notion of qualia structure which is used to organise the Semantic Units (SemUs) which constitute the basic structures representing word-sense. SIMPLE contains 20,000 SemUs and we used the definitions of these SemUs for the task. Both lexicons share a set of common “base concepts” that provided the basis of a previous (semi-)automatic mapping of the two lexicons on the basis of their respective ontological organisations (Roventini et al., 2007; Roventini and Ruimy, 2008). Although this mapping did not make the five-fold distinction, i.e., exact, narrower, broader, related, and none, it did constitute a useful starting point and a basis for comparison for the task. The teams that had originally compiled IWN and SIMPLE shared many members in common and so, the definitions for corresponding senses across the two lexicons are sometimes very similar or differ solely on the basis of an extra clause. This made it easy to determine, in many cases, if two senses were ‘exact’ matches or if one was ‘broader’ or ‘narrower’ than the other by just comparing strings. The ap"
2020.lrec-1.395,roventini-etal-2000-italwordnet,0,0.0791879,"Missing"
2020.lrec-1.395,roventini-etal-2002-integrating,0,0.0935961,"n incapacitated adult). An opposite case where the historical sense is ‘narrower’ than the modern one can be illustrated by the adjective spids (‘sharp’) where ODS describes two specific senses, one about sound and another one about smell, while DDO merges the two senses into one: ‘pungent in an unpleasant way (about smell, taste or sound)’. 4.2. ItalWordNet and SIMPLE Regarding Italian, the team at ILC-CNR chose ItalWordNet (IWN) and SIMPLE, two Italian language lexical resources which had been previously developed in the institute. The former, IWN, is a lexical semantic network for Italian (Roventini et al., 2002) which is part of the WordNet family (Miller, 1995). As such it is organised around the notion of a synset of word senses and the network structure based on lexical-semantic relations which hold between senses across synsets. The 50,000 Italian synsets contained in IWN are linked to the Princeton Wordnet. The latter resource, SIMPLE, constitutes the semantic level of a quadripartite Italian lexicon. Its structure is inspired by Generative Lexicon theory (Pustejovsky, 1995) and in particular the notion of qualia structure which is used to organise the Semantic Units (SemUs) which constitute the"
2020.lrec-1.395,P07-2041,0,0.0611602,"ce, SIMPLE, constitutes the semantic level of a quadripartite Italian lexicon. Its structure is inspired by Generative Lexicon theory (Pustejovsky, 1995) and in particular the notion of qualia structure which is used to organise the Semantic Units (SemUs) which constitute the basic structures representing word-sense. SIMPLE contains 20,000 SemUs and we used the definitions of these SemUs for the task. Both lexicons share a set of common “base concepts” that provided the basis of a previous (semi-)automatic mapping of the two lexicons on the basis of their respective ontological organisations (Roventini et al., 2007; Roventini and Ruimy, 2008). Although this mapping did not make the five-fold distinction, i.e., exact, narrower, broader, related, and none, it did constitute a useful starting point and a basis for comparison for the task. The teams that had originally compiled IWN and SIMPLE shared many members in common and so, the definitions for corresponding senses across the two lexicons are sometimes very similar or differ solely on the basis of an extra clause. This made it easy to determine, in many cases, if two senses were ‘exact’ matches or if one was ‘broader’ or ‘narrower’ than the other by ju"
2020.lrec-1.395,2019.gwc-1.37,1,0.782709,"onary, the Danish Dictionary (Den Danske Ordbog (DDO)). Later, the semantic links between these two resources facilitated the compilation of a comprehensive thesaurus (Den Danske Begrebsordbog) (Nimb et al., 2014). The semantic links between thesaurus and dictionary made it possible to combine verb groups and dictionary valency information, used as input for the compilation of the Danish FrameNet Lexicon (Nimb, 2018). Furthermore, they constitute the basis for the automatically integrated information on related words in DDO, on the fly for each dictionary sense (Nimb et al., 2018). Similarly, Simov et al. (2019) report the manual mapping of the Bulgarian Word-Net BTB-WN with the Bulgarian Wikipedia. Given the amount of the effort required to construct and maintain expert-made resources, various solutions have been proposed to automatically link and merge existing LSRs at different levels. LSRs being very diverse in domain coverage (Meyer, 2010; Burgun and Bodenreider, 2001), previous works have focused on methods to increase domain coverage, enrich sense representations and decrease sense granularity (Miller, 2016). Miller and Gurevych (2014) describe a technique for constructing an n-way alignment o"
2020.lrec-1.422,L18-1213,1,0.891813,"Missing"
2020.lrec-1.422,L18-1205,1,0.791884,"Missing"
2020.lrec-1.422,hinrichs-krauwer-2014-clarin,0,0.16717,"ering legal guidance to language data providers, as in many cases their data was not foreseen to be delivered to a repository or to be used for training MT systems. As a consequence, information about the type of licenses that can best suit the purpose of re-using the data is needed . While from the outside it can seem that ELRC shares similarities with other initiatives like CLARIN 8, there are several differences. First, CLARIN is a research infrastructure which aims to support the sharing, use and sustainability of language data and tools for research in the humanities and social sciences (Hinrichs & Krauwer, 2014). In this sense, CLARIN’s scope is broader than that of ELRC, which focuses mainly on language resources and technologies for multilinguality. While CLARIN addresses mainly the research community, ELRC targets data owners from the public sector. This difference with regard to the targeted audience, entails different operations for data identification, processing, sharing and re-purposing. Since ELRC is mainly dealing with language data that reside in public organisations, it is heavily engaged in efforts to “unlock” these data through raising awareness of their value and potential for language"
2020.lrec-1.695,P18-1073,0,0.016647,"ersion of 5 terminological resources in TBX to RDF. 5.3. Linking Finally, the project is developing (semi-)automated linking mechanisms. This concerns both the conceptual level of language descriptions as also the lexical data. We are working both in a mono- and in a cross-lingual set up. Since this work is still in progress, at this time we can only report on preliminary approaches. In the context of cross-lingual concept matching, we are updating an already existent ontology matching tool, CIDERCL (Gracia and Asooja, 2013) with contemporary techniques based on cross-lingual word embeddings (Artetxe et al., 2018). Regarding linking cross-lingual lexical data, the project has laid the groundwork for research in the topic of ”translation inference across dictionaries” by organising the TIAD’19 shared task (Gracia et al., 2019), in which a benchmark and evaluation framework were provided to allow for systematic comparisons between systems. Such systems were able to infer indirect translations between language pairs that were initially disconnected in the Apertium RDF graph (Gracia et al., 2018), showing promising results but also the need of further research. Ontology lexicalisation aims at developing te"
2020.lrec-1.695,2019.gwc-1.34,1,0.826147,"Missing"
2020.lrec-1.695,E09-2008,0,0.0376885,"ons for morphology (Klimek et al., 2019) and the representation of frequency, attestation and corpus information (Chiarcos and Ionov, 2019). The morphology module is specifically important for the cross-linguistic applicability of OntoLex-Lemon, as it aims to support languages with a lot of stem internal alternations: By using regular expressions to represent morphology generation rules, it provides implementation-independent means to generate inflected forms from lemma information, that can be subsequently incorporated in conventional morphology frameworks such as XFST (Ranta, 1998) or FOMA (Hulden, 2009). Specifications for phonological processes and mor3 See https://www.w3.org/2016/05/ontolex/ See (McCrae et al., 2012) and (Cimiano et al., 2016). 5 SKOS stands for “Simple Knowledge Organization System”. SKOS provides “a model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabulary” (https://www. w3.org/TR/skos-primer/). 6 See http://www.elex.is/ for more detail. 4 Figure 2: The core Modules of OntoLex-Lemon. Graphic taken from https://www.w3.or"
2020.lrec-1.695,L16-1386,1,0.78537,"Missing"
2020.lrec-1.695,W98-1308,0,0.29918,"d emerging specifications for morphology (Klimek et al., 2019) and the representation of frequency, attestation and corpus information (Chiarcos and Ionov, 2019). The morphology module is specifically important for the cross-linguistic applicability of OntoLex-Lemon, as it aims to support languages with a lot of stem internal alternations: By using regular expressions to represent morphology generation rules, it provides implementation-independent means to generate inflected forms from lemma information, that can be subsequently incorporated in conventional morphology frameworks such as XFST (Ranta, 1998) or FOMA (Hulden, 2009). Specifications for phonological processes and mor3 See https://www.w3.org/2016/05/ontolex/ See (McCrae et al., 2012) and (Cimiano et al., 2016). 5 SKOS stands for “Simple Knowledge Organization System”. SKOS provides “a model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabulary” (https://www. w3.org/TR/skos-primer/). 6 See http://www.elex.is/ for more detail. 4 Figure 2: The core Modules of OntoLex-Lemon. Graphic taken"
2020.mmw-1.7,francopoulo-etal-2006-lexical,0,0.727154,"lso called heteronymy, can be relevant for a variety of speech-based ap2 This information is retrieved from the PWN Web interface, accessible at http://wordnetweb.princeton. edu/perl/webwn. 3 See https://www.wiktionary.org/ and for the German edition https://de.wiktionary.org/wiki/ Wiktionary:Hauptseite. 4 IPA stands for “International Phonetic Alphabet”. See https://www. internationalphoneticassociation.org/ content/ipa-chart for more details. 5 The pronunciation information is taken from https://de. wiktionary.org/wiki/Boot. 1 LMF stands for “Lexical Markup Framework”, an ISO standard. See (Francopoulo et al., 2006) and http://www. lexicalmarkupframework.org/ for more details. lemon stands for “LExicon MOdel for oNtologies”. See (McCrae et al., 2012) and https://lemon-model.net/ for more details. 39 plications. Therefore, this type of information should be added to wordnets, so that they can help to disambiguate words in spoken utterances. We need to make this linking of Wordnet entries to pronunciation information explicit, and for this we are adapting the approach described in (Racioppa and Declerck, 2019), and which is dealing with the linking of Wordnet lemmas to morphological information. We thus ag"
2020.mmw-1.7,L16-1498,0,0.0148107,"ry can deliver a relevant source of data for our experiment consisting in equipping wordnets with pronunciation information. 4. Extracting Pronunciation Information from the German Edition of Wiktionary We display in Figure 1 below as an example the pronunciation information for the German substantive “Januar” (january) as represented in the XML dump of the German edition of Wiktionary.10 As the reader can see, the Extracting Pronunciation Data from Wiktionary It has been shown that the access and use of Wiktionary can be helpful in a series of Natural Language Processing (NLP) applications. (Kirov et al., 2016), for example, describe work to extract and standardize data contained in Wiktionary and to make it available for a range of NLP tasks, while the authors focus on extracting and normalizing a huge number of inflectional paradigms across a large selection of languages. This effort contributed to the creation of the UniMorph data (http://unimorph. org/). The UniMorph project was focusing on (scraping) the HTML representation of Wiktionary (mostly the English version, but also looking at other language editions). (Metheniti and Neumann, 2018) and (Metheniti and Neumann, 2020) describe a related a"
2020.mmw-1.7,P79-1022,0,0.182728,"Missing"
2020.mmw-1.7,2016.gwc-1.9,0,0.0903345,"up and maintained by hand, especially the original Princeton WordNet of English (PWN) (Miller, 1995; Fellbaum, 1998). In recent years, there have been increasing activities in which open wordnets for different languages have been automatically extracted from various resources and enriched with lexical semantics information, building the so-called Open Multilingual Wordnet (OMW) (Bond and Paik, 2012). OMW brings together wordnets in different languages, harmonizing them in a uniform tabular format that lists synsets IDs and the associated lemmas, and linking them to PWN (Bond and Foster, 2013; Bond et al., 2016). Additionally, XML versions of LMF and lemon representations1 of the data are provided. A starting motivation for our work was to investigate if and how specific Wordnet senses can be restricted to what appears to be morphological variations of a lexical entry. The question touched also the issue on how to encode this information. (Gromann and Declerck, 2019) describe a first experiment done for English, looking at specific Princeton WordNet senses associated with word forms that look like regular plural forms of a lexical entry, but which rather need to be considered as separate lexical entr"
2020.mmw-1.7,2020.lrec-1.481,0,0.0182197,"ocessing (NLP) applications. (Kirov et al., 2016), for example, describe work to extract and standardize data contained in Wiktionary and to make it available for a range of NLP tasks, while the authors focus on extracting and normalizing a huge number of inflectional paradigms across a large selection of languages. This effort contributed to the creation of the UniMorph data (http://unimorph. org/). The UniMorph project was focusing on (scraping) the HTML representation of Wiktionary (mostly the English version, but also looking at other language editions). (Metheniti and Neumann, 2018) and (Metheniti and Neumann, 2020) describe a related approach, but making use of a combination of the HTML pages and the underlying XML dump of the English edition of Wiktionary, which is covering also 4,050 other languages, some of them with a very low number of entries.7 The English edition of Wiktionary has of today a number of 6,262,000 pages, whereas 734,130 pages are dealing with English words. BabelNet8 is also integrating Witkionary data,9 with a focus on sense information, in order to support, among others, word sense disambiguation and tasks dealing with word similarity and sense clustering (Camacho-Collados et al.,"
2020.mmw-1.7,P10-1023,0,0.167929,"” (adjective, [mo""dEKn], modern) versus “modern” (verb, [""mo:d5n], moulder). But this cross-categories extension is less relevant, as wordnets would anyway introduce different lemmas for a word belonging to distinct categories. An example of a German substantive having two different pronunciations is “Vollzug”, with the stress put either at the 6 See also https://www.w3.org/2016/05/ ontolex/ for more details. 7 A possibly tentative list of entries in the different languages contained in the English Wiktionary is given here: https://en.wiktionary.org/wiki/Special: Statistics?action=raw. 8 See (Navigli and Ponzetto, 2010) and https: //babelnet.org/. 9 As far as we are aware of, BabelNet integrates only the English edition of Wiktionary, but includes all the languages covered by this edition. 10 XML dumps of the various editions of Wiktionary are available at https://dumps.wikimedia.org/ backup-index.html. 11 In parallel, we are extracting a list of German substantives that have different genders (502 entries detected) or different plural forms (440 entries detected), each with specific senses. 40 beginning or at the end of the word, as shown in Figure 2 and Figure 3, which are displaying screen shots from the"
2020.mmw-1.7,2016.gwc-1.43,0,0.0556377,"Missing"
2020.mmw-1.7,vossen-etal-2008-integrating,0,0.0910914,"Missing"
2021.gwc-1.33,2020.globalex-1.5,1,0.785639,"ary is an extension and a refinement of a first experiment dealing with the German version of Wiktionary, with the aim of enriching a new open WordNet for German with pronunciation information (Declerck et al., 2020). In both cases, we make use of the OntoLexLemon community standard for encoding the heteronyms (and other entries). Our current development is aiming at including the results into various integrated or interlinked lexical databases. We are also aiming at automatically adding pronunciation information to derived terms, on the base of senselinking algorithms. The work presented in (Declerck, 2020) describes an approach for linking the Open Dutch WordNet to external lexical resources, including the Dutch version of Wiktionary, with the goal of enriching the lemmas in the WordNet entries with morphological variants. But the work was not dealing with pronunciation information. (Schlippe et al., 2010) assess the quality of pronunciation information in Wiktionary for four languages (English, French, German, and Spanish) and come to satisfying results, especially in the case of French, when it comes to the evaluation of the coverage and also to the impact on automatic speech recognition (ASR"
2021.gwc-1.33,2020.mmw-1.7,1,0.901277,"en ; ontolex : isLexicalizedSenseOf : LexicalConcept 1 ; ontolex : isSenseOf : lex lead 1 ; ontolex : reference < h t t p s : / / www. w i k i d a t a . o r g / w i k i / Q708> ; ## o n t o l e x : u s a g e l e x i n f o : s i n g u l a r ; . effective to just duplicate the lexical forms along the line of their pronunciation (even if they have the same gender and number features), and to point to those from the lexical sense via the corresponding lexical entry. 5 Sense Linking Related conclusive experiments were also done for encoding lexical information extracted from the German Wiktionary (Declerck et al., 2020). It is suggested in (Declerck et al., 2020) that one could link specific senses of an entry to a lexical form carrying a specific pronunciation (by the use of the ontolex:phoneticRep) by applying restrictions that are defined in the lexicog module ((Bosque-Gil et al., 2019)16 of OntoLex-Lemon. However, in our current experiment, we think that it might be more In the following phase of our work, we plan to connect the extracted information with the correct WordNet synsets. After extracting the pronunciation information from Wiktionary, the subsequent step of our work lies in sense disambiguati"
2021.gwc-1.33,francopoulo-etal-2006-lexical,0,0.0934069,"Fellbaum, 1998)), but aiming at an open source development policy. This makes this version of WordNet a good candidate for testing in a near future the addition of pronunciation information in a collaborative manner, using the corresponding GitHub platform.7 The Open English WordNet (OEW) data can be downloaded in various formats, including XML, LMF8 and RDF. 6 See also https://babelnet.org/. Open English WordNet is accessible at https:// github.com/globalwordnet/english-wordnet It is also accessible via a GUI: https://en-word.net/. 8 LMF stands for “Lexical Markup Language”, an ISO standard (Francopoulo et al., 2006), which has also be employed for encoding WordNet, as this is described for example in (Henrich and Hinrichs, 2010). 7 2.2 BabelNet While BabelNet already combines wordnets and wiktionaries, as well as many other resources, it does not yet provide the phonetic transcription that it has extracted from various language versions of Wiktionary. Although BabelNet provides sound files in its word entries, those pronunciations are given by an external library that do not read from IPA codes. This library seems to be connected to the text-to-speech modules of the browser accessing the server, and util"
2021.gwc-1.33,C10-1052,0,0.0387844,"idate for testing in a near future the addition of pronunciation information in a collaborative manner, using the corresponding GitHub platform.7 The Open English WordNet (OEW) data can be downloaded in various formats, including XML, LMF8 and RDF. 6 See also https://babelnet.org/. Open English WordNet is accessible at https:// github.com/globalwordnet/english-wordnet It is also accessible via a GUI: https://en-word.net/. 8 LMF stands for “Lexical Markup Language”, an ISO standard (Francopoulo et al., 2006), which has also be employed for encoding WordNet, as this is described for example in (Henrich and Hinrichs, 2010). 7 2.2 BabelNet While BabelNet already combines wordnets and wiktionaries, as well as many other resources, it does not yet provide the phonetic transcription that it has extracted from various language versions of Wiktionary. Although BabelNet provides sound files in its word entries, those pronunciations are given by an external library that do not read from IPA codes. This library seems to be connected to the text-to-speech modules of the browser accessing the server, and utilises it to add pronunciation to some textual information on the BabelNet pages, like the entry and its associated d"
2021.gwc-1.33,2019.gwc-1.31,0,0.0109222,"ion information to lexical semantic resources, with a focus on open wordnets. Our goal is not only to add a new modality to those semantic networks, but also to mark heteronyms listed in them with the pronunciation information associated with their different meanings. This work could contribute in the longer term to the disambiguation of multi-modal resources, which are combining text and speech. 1 Introduction The work described in this paper aims at enriching lexical semantic databases by adding the modality of pronunciation, primarily targeting in our current work the Open English WordNet (McCrae et al., 2019a, 2020).1 Pronunciation information is typically not associated with WordNet, but can be particularly relevant within the vision of contributing directly or indirectly to integrated lexical resources and architectures, like the ELEXIS Dictionary Matrix (McCrae et al., 2019b) or BabelNet (Navigli and Ponzetto, 2010), as well as text-to-speech systems which use WordNet or WordNet-based lexical resources or tools. In a number of cases, homographs with different meanings are also characterised by different pronunciations. This can be the case across syntactic categories, but also within one categ"
2021.gwc-1.33,2020.mmw-1.3,0,0.0349859,"Missing"
2021.gwc-1.33,P10-1023,0,0.0687467,"term to the disambiguation of multi-modal resources, which are combining text and speech. 1 Introduction The work described in this paper aims at enriching lexical semantic databases by adding the modality of pronunciation, primarily targeting in our current work the Open English WordNet (McCrae et al., 2019a, 2020).1 Pronunciation information is typically not associated with WordNet, but can be particularly relevant within the vision of contributing directly or indirectly to integrated lexical resources and architectures, like the ELEXIS Dictionary Matrix (McCrae et al., 2019b) or BabelNet (Navigli and Ponzetto, 2010), as well as text-to-speech systems which use WordNet or WordNet-based lexical resources or tools. In a number of cases, homographs with different meanings are also characterised by different pronunciations. This can be the case across syntactic categories, but also within one category, like for example for the noun “lead”,2 which is having a different pronunciation per sense, as this is exem1 See also https://github.com/ globalwordnet/english-wordnet. 2 The two pronunciation and definition pairs for the noun “lead” displayed here are taken from the XML dump of the English edition of Wiktionar"
2021.semdeep-1.3,C18-1139,0,0.0237652,"efines k. For the GloVe example, a lemma (lexical entry) embedding can be represented as follows: : frak a ontolex : LexicalEntry ; ontolex : canonicalForm / o n t o l e x : w r i t t e n R e p ” f r a k ”@en ; f r a c : embedding [ a frac : FixedSizeVector ; r d f : value ”0.015246 . . . ” ; dct : source <h t t p s : / / c a t a l o g . l d c . . . . > ; d c t : e x t e n t 5 0 ˆ ˆ ˆ xsd : i n t ; d c t : d e s c r i p t i o n ” GloVe v . 1 . 1 , . . . ” @en . ] . 4.2 Contextualized Embeddings Above, we mentioned contextualized embeddings, and more recent methods such as ELMo [16], Flair NLP [1], or BERT [7] have been shown to be remarkably effective at many NLP problems. In the context of lexical semantics, contextual embeddings can prove beneficial for inducing or distinguishing word senses, and in extension of the classical Lesk algorithm, for example, a lexical sense can be described by means of the contextualized word embeddings for the examples associated with that particular lexical sense, and words for which word sense disambiguation is to be performed can then just be compared with these. These examples then serve a similar function as attestations in a dictionary, and indee"
2021.semdeep-1.3,2016.gwc-1.9,0,0.060729,"conventions in the field, such mistakes will very likely go unnoticed, thus leading to highly unexpected results in applications developed on this basis. Our suggestion here is to use resolvable URIs as concept identifiers, and if they provide machine-readable lexical data, lexical information about concept embeddings can be more easily verified and (this is another application) integrated with predictions from distributional semantics. Indeed, the WordNet community has adopted OntoLex-Lemon as an RDF-based representation schema and developed the Collaborative Interlingual Index (ILI or CILI) [3] to establish sense mappings across a large number of WordNets. Reference to ILI URIs would allow to retrieve the lexical information behind a particular concept embedding, as the WordNet can be queried for the lexemes this concept (synset) is associated with. A versioning mismatch can then be easily spotted by comparing the cosine distance between the word embeddings of these lexemes and the embedding of the concept presumably derived from them. 3 See ili https://github.com/globalwordnet/ 2.2 Organizing Contextualized Embeddings A related challenge is the organization of contextualized embedd"
2021.semdeep-1.3,2020.globalex-1.1,1,0.879106,"eddings generated with different algorithms that can be freely used in various NLP applications. With this paper, we present the current state of an effort to connect these embeddings with lexical knowledge graphs. This effort is a part of an extension of a widely used community standard for representing, linking and publishing lexical resources on the web, OntoLex-Lemon1 . Our work aims to complement the emerging OntoLex module for representing Frequency, Attestation and Corpus Information (FrAC) which is currently being developed by the W3C Community Group “Ontology-Lexica”, as presented in [4]. There we addressed only frequency and attestations, whereas core aspects of corpus-based information such as embeddings were 1 https://www.w3.org/2016/05/ontolex/ identified as a topic for future developments. Here, we describe possible use-cases for the latter and present our current model for this. 2 Sharing Embeddings on the Web Although word embeddings are often calculated on the fly, the community recognizes the importance of pre-trained embeddings as these are readily available (it saves time), and cover large quantities of text (their replication would be energy- and timeintense). Fin"
2021.semdeep-1.3,N19-1423,0,0.0230632,"the GloVe example, a lemma (lexical entry) embedding can be represented as follows: : frak a ontolex : LexicalEntry ; ontolex : canonicalForm / o n t o l e x : w r i t t e n R e p ” f r a k ”@en ; f r a c : embedding [ a frac : FixedSizeVector ; r d f : value ”0.015246 . . . ” ; dct : source <h t t p s : / / c a t a l o g . l d c . . . . > ; d c t : e x t e n t 5 0 ˆ ˆ ˆ xsd : i n t ; d c t : d e s c r i p t i o n ” GloVe v . 1 . 1 , . . . ” @en . ] . 4.2 Contextualized Embeddings Above, we mentioned contextualized embeddings, and more recent methods such as ELMo [16], Flair NLP [1], or BERT [7] have been shown to be remarkably effective at many NLP problems. In the context of lexical semantics, contextual embeddings can prove beneficial for inducing or distinguishing word senses, and in extension of the classical Lesk algorithm, for example, a lexical sense can be described by means of the contextualized word embeddings for the examples associated with that particular lexical sense, and words for which word sense disambiguation is to be performed can then just be compared with these. These examples then serve a similar function as attestations in a dictionary, and indeed, the link h"
2021.semdeep-1.3,D14-1162,0,0.10149,"calculated from plain strings, but from normalized strings, e.g., lemmatized text. For such data, we model every individual lemma as an ontolex:LexicalEntry. Moreover, as argued in Sec. 2, embeddings are equally relevant for lexical senses and lexical concepts; the embedding property that associates a lexical entity with an embedding is thus applicable to every Observable. 4.1 Word Embeddings Pre-trained word embeddings are often distributed as text files consisting of the label (token) and a sequence of whitespace-separated numbers. E.g. the entry for the word frak from the GloVe embeddings [15]: frak 0.015246 -0.30472 0.68107 ... Since our focus on publishing and sharing embeddings, we propose to provide the value of an embedding as a literal rdf:value. If necessary, more elaborate representations, e.g., using rdf:List, may subsequently be generated from these literals. A natural and effort-less modelling choice is to represent embedding values as string literals with whitespace-separated numbers. For decoding and verification, such a representation benefits from metadata about the length of the vector. For a fixed-size vector, this should be provided by dc:extent. An alternative is"
2021.semdeep-1.3,N18-1202,0,0.0502005,"erty dc:extent defines k. For the GloVe example, a lemma (lexical entry) embedding can be represented as follows: : frak a ontolex : LexicalEntry ; ontolex : canonicalForm / o n t o l e x : w r i t t e n R e p ” f r a k ”@en ; f r a c : embedding [ a frac : FixedSizeVector ; r d f : value ”0.015246 . . . ” ; dct : source <h t t p s : / / c a t a l o g . l d c . . . . > ; d c t : e x t e n t 5 0 ˆ ˆ ˆ xsd : i n t ; d c t : d e s c r i p t i o n ” GloVe v . 1 . 1 , . . . ” @en . ] . 4.2 Contextualized Embeddings Above, we mentioned contextualized embeddings, and more recent methods such as ELMo [16], Flair NLP [1], or BERT [7] have been shown to be remarkably effective at many NLP problems. In the context of lexical semantics, contextual embeddings can prove beneficial for inducing or distinguishing word senses, and in extension of the classical Lesk algorithm, for example, a lexical sense can be described by means of the contextualized word embeddings for the examples associated with that particular lexical sense, and words for which word sense disambiguation is to be performed can then just be compared with these. These examples then serve a similar function as attestations in a dictio"
2021.semdeep-1.3,J17-3004,0,0.0142652,"ut rather documented in human-readable form or given implicitly as part of file names.2 2.1 Concept Embeddings It is to be noted, however, that our focus is not so much on word embeddings, since lexical information in this context is apparently trivial – plain 2 See the ISO-639-1 code ‘en’ in FastText/MUSE files such as https://dl.fbaipublicfiles.com/arrival/ vectors/wiki.multi.en.vec. tokens without any lexical information do not seem to require a structured approach to lexical semantics. This changes drastically for embeddings of more abstract lexical entities, e.g., word senses or concepts [17], that need to be synchronized between the embedding store and the lexical knowledge graph by which they are defined. WordNet [14] synset identifiers are a notorious example for the instability of concepts between different versions: Synset 00019837-a means ‘incapable of being put up with’ in WordNet 1.71, but ‘established by authority’ in version 2.1. In WordNet 3.0, the first synset has the ID 02435671-a, the second 00179035-s.3 The precompiled synset embeddings provided with AutoExtend [17] illustrate the consequences: The synset IDs seem to refer to WordNet 2.1 (wn-2.1-00001742-n), but use"
A97-1006,A97-1031,0,0.0491737,"Missing"
A97-1006,C96-2120,1,0.420671,"Missing"
A97-1006,C94-1072,1,0.883729,"Missing"
A97-1006,W96-0411,1,0.545341,"e goes on along the same lines. 7 Generation Client systems usually want to express in NL a cooperation primitive and a date expression. Hence NL generation is based on a semantic template filled by the client. Depending on its content the template is unified with a prefabricated structure specifying linguistic-oriented input to the generator. The same holds for failure messages, such as (02), and for specifications of free time slots, as in (07), where simple rules of aggregation take care not to repeat the full date specification for each clock time mentioned. The production system T G / 2 (Busemann, 1996) ~If the client is not satisfied with such an expression, backtracking will pass the next-bcst structure etc. 30 proved to be sufficiently flexible to accomplish this task by its ability to generate preferred formulations first. For instance, COSMA clients can parameterize T G / 2 so as to refer to their owner by a first person pronoun or by a full name, or to use formal or informal form of addressing the human hearer, or to prefer deictic time descriptions over anaphorical ones. 8 A novel is carried out by the workflow manager, which also manages interdependencies between these activities whi"
broeder-etal-2002-lrep,A97-1035,0,\N,Missing
buitelaar-etal-2004-towards,A00-1031,0,\N,Missing
buitelaar-etal-2004-towards,buitelaar-etal-2004-evaluation,1,\N,Missing
C96-1049,C90-2064,0,0.0770935,"Missing"
C98-2190,C96-1049,1,0.575046,"Missing"
capstick-etal-2002-collate,W01-1506,0,\N,Missing
capstick-etal-2002-collate,W01-1502,1,\N,Missing
capstick-etal-2002-collate,declerck-etal-2000-new,1,\N,Missing
declerck-2008-framework,W07-1501,0,\N,Missing
declerck-2008-framework,ide-romary-2004-registry,0,\N,Missing
declerck-2008-framework,ide-romary-2006-representing,0,\N,Missing
declerck-etal-2004-towards,W03-1905,1,\N,Missing
declerck-etal-2004-towards,buitelaar-etal-2004-towards,1,\N,Missing
declerck-etal-2004-towards,broeder-etal-2004-large,1,\N,Missing
declerck-etal-2004-towards,capstick-etal-2002-collate,1,\N,Missing
declerck-etal-2012-accessing,zesch-etal-2008-extracting,0,\N,Missing
declerck-etal-2014-skos,W13-5501,1,\N,Missing
declerck-krieger-2014-harmonization,remus-etal-2010-sentiws,0,\N,Missing
declerck-krieger-2014-harmonization,waltinger-2010-germanpolarityclues,0,\N,Missing
declerck-krieger-2014-harmonization,clematide-etal-2012-mlsa,0,\N,Missing
declerck-krieger-2014-harmonization,W13-5502,0,\N,Missing
declerck-lendvai-2010-towards,W07-1501,0,\N,Missing
declerck-lendvai-2010-towards,ide-romary-2006-representing,0,\N,Missing
declerck-vela-2006-generic,W03-1314,0,\N,Missing
declerck-vela-2006-generic,W02-0308,0,\N,Missing
E03-2014,J98-3005,0,0.0411612,"e definitions; and template generation and filling component that uses the domain lexicon and linguistic output of the first step as a guidance to fill-in the templates. The systems takes advantage of the information extracted from formal texts (e.g., lists of players) in order to carry out the analysis of tickers. 3 Merging or Cross-document Event Coreference The merging component in MUMIS combines the partial information as extracted from various sources, such that more complete annotations can be obtained. Information extraction and merging from multiple sources has been tried in the past (Radev and McKeown, 1998) but only for single events, the novelty of our approach consists on applying merging to multiple-events extracted from multiple sources. As an example consider the following situation (Netherlands-Yugoslavia match): One of the IE components extracted from document A that in the 30th minute of the match a free-kick was taken, but did not discover who took it. It did find the names of two players, though: Mihajlovic (a Yugoslavian player) and Van der Sar (the Dutch keeper). From document B a save in the 31st minute was extracted by the IE component, and the names of the same two players were re"
gavrilidou-etal-2012-meta,piperidis-2012-meta,1,\N,Missing
gavrilidou-etal-2012-meta,broeder-etal-2010-data,0,\N,Missing
gavrilidou-etal-2012-meta,federmann-etal-2012-meta,0,\N,Missing
gromann-declerck-2017-hashtag,P14-1023,0,\N,Missing
gromann-declerck-2017-hashtag,R15-1015,1,\N,Missing
hayashi-etal-2010-laf,W07-1501,0,\N,Missing
hayashi-etal-2010-laf,W02-2016,0,\N,Missing
hayashi-etal-2010-laf,W08-1301,0,\N,Missing
hayashi-etal-2010-laf,kountz-etal-2008-laf,0,\N,Missing
hayashi-etal-2010-laf,ide-romary-2006-representing,0,\N,Missing
hayashi-etal-2010-laf,declerck-2008-framework,1,\N,Missing
krieger-declerck-2014-tmo,krieger-2010-general,1,\N,Missing
L16-1386,2016.gwc-1.9,1,0.821562,"gy, since they are used as domain specific ontologies. Additionally, other supporting ontologies have been added, such as GeoNames for the named entities; PROTON as an upper ontology; SKOS as a mapper between ontologies and terminological lexicons; Dublin Core as a metadata ontology. Also, for the purposes of search, Web Interface Querying EUCases Linking Platform was designed. For its Web Interface, the EUCases Linking Platform relies on a customized version of the GraphDB Workbench27 , developed by Ontotext AD. 5.2. Wordnet Interlingual Index (ILI) A recent development (Vossen et al., 2016; Bond et al., 2016) has been the adoption of LLOD technology by the wordnet community, with a new plan that uses LLOD as the basic mechanism for the creation of links between wordnets in different languages. This Collaborative InterLingual Index enables wordnets to share and link their resources for concepts lexicalized in any of the group’s languages. This was supported directly by a workshop at the 2016 Global WordNet Conference and will lead to the adoption of LLOD technology by a new community. In addition, the open multilingual wordnet (Bond et al., 2014) provides all open wordnets for download using the le"
L16-1386,calzolari-etal-2012-lre,0,0.0712075,"Missing"
L16-1386,ehrmann-etal-2014-representing,1,0.804428,"lable by attempting to download it and discarding all resources that are no longer available. We have attempted to notify the authors of resources that no longer meet the criteria for inclusion in the cloud. However, our experience has been that this did not motivate many authors to update their resources. 2.5. Vocabularies The Linguistic Linked Open Data Cloud has grown significantly in the last few years and most notably, unlike the non-linguistic LOD Cloud, is not centered around one nucleus but instead has used many different vocabularies and datasets to link to. Among these are BabelNet (Ehrmann et al., 2014), LexInfo (Cimiano et al., 2011), and Lexvo (de Melo, 2015). In addition, a number of new vocabularies have emerged including the OntoLex model,13 the NLP Interchange format NIF (Hellmann et al., 2013), the WordNet Interlingual Index (Sect. 5.2.), and the FrameBase schema (Rouces et al., 2015a) (Sect. 5.3.). These vocabularies have increased the power of linked data to represent the complete spectrum of language resources and show that new resources can be created that use the power of linked data to link across different types of languages resources, such as terminologies and dictionaries (Si"
L16-1386,federmann-etal-2012-meta,0,0.0601026,"Missing"
L16-1386,W15-4205,1,0.920405,"not necessarily created for this purpose, e.g., large collections of texts such as news articles, terminological or encyclopedic and general-purpose knowledge bases such as DBpedia (Bizer et al., 2009), or metadata collections. 2.2. Infrastructure and Metadata The OWLG provides guidelines to data publishers on how to include their resources in the LLOD cloud.6 The cloud diagram is currently generated from metadata maintained at DataHub7 and hence contains only resources described in DataHub. An alternative metadata repository specialized for linguistic resources is under development: Linghub (McCrae et al., 2015a).8 It aims to provide a search engine and index for linguistic resources and attempts to harmonize metadata from a number of different sources, including Metashare (Federmann et al., 2012), CLARIN VLO (Van Uytvanck et al., 2012), DataHub and LRE Map (Calzolari et al., 2012). It will soon replace DataHub in the generation of the cloud diagram. LingHub, 4 http://lod2.eu/ http://qtleap.eu/ 6 http://wiki.okfn.org/Working_Groups/ Linguistics/How_to_contribute 7 http://datahub.io 8 http://linghub.org 5 Datasets Links 28 53 103 126 128 41 78 167 203 209 February 2012 September 2013 November 2014 Ma"
L16-1386,W15-4201,0,0.02663,"Missing"
L16-1386,W15-4207,1,0.813122,"4), LexInfo (Cimiano et al., 2011), and Lexvo (de Melo, 2015). In addition, a number of new vocabularies have emerged including the OntoLex model,13 the NLP Interchange format NIF (Hellmann et al., 2013), the WordNet Interlingual Index (Sect. 5.2.), and the FrameBase schema (Rouces et al., 2015a) (Sect. 5.3.). These vocabularies have increased the power of linked data to represent the complete spectrum of language resources and show that new resources can be created that use the power of linked data to link across different types of languages resources, such as terminologies and dictionaries (Siemoneit et al., 2015) and corpora and dictionaries (McGovern et al., 2015). 3. OWLG members have been very active in promoting the development and adoption of linguistic linked data, which had an effect not only in the growth of the LLOD cloud but in the development of representation models, guidelines, and best practices. These activities have been developed in the context of a number of W3C groups and projects, as it is detailed in the rest of this section. 13 12 http://lodvader.aksw.org/ Other Community Group Efforts http://cimiano.github.io/ontolex/ specification.html 2437 3.1. OntoLex 8. LLOD aware services 1"
L16-1386,van-uytvanck-etal-2012-semantic,0,0.0695217,"Missing"
L16-1729,S16-1063,1,0.862706,"Missing"
L16-1729,bentivogli-etal-2010-building,0,0.0187668,"ortable across languages and domains, but 11 To assess the PHEME RTE pilot dataset, we use the MaxEnt classifier-based model (Wang and Neumann, 2007) distributed with the Excitement Open Platform (EOP, Pado et train http://hltfbk.github.io/Excitement-Open-Platform/ generated by the sklearn metric classification report, see http://scikit-learn.org 13 http://nlp.stanford.edu/RTE3-pilot/RTE3 dev 3class.xml 4604 12 requires event and claim annotations. The manual effort spent to create such annotations is feasible to replace by automatic means which are currently being implemented in the project. Bentivogli et al. (2010) stress the importance of creating specialized data sets for RTE, in order to facilitate more targeted assessment and decomposition of the RTE task’s complexity. In our resource, the text snippets that form a RTE pair deliberately keep reoccurring across all three judgement labels in systematically varied pairings, allowing to investigate, model and evaluate linguistic and extralinguistic phenomena that underly semantic inference in the misinformation detection scenario. Previous RTE research has mainly focused on achieving good performance on the Entailment relation, whereas our method is mot"
L16-1729,D15-1075,0,0.0337061,"neither of those. A bottleneck for this task is obtaining training data. The creation of natural language data annotated for inference phenomena is so far a nontrivial and largely manual procedure, yielding expensive resources that are nonetheless problematically portable to new text genres and application domains. Existing initiatives have often created RTE data by syntactic and lexical transformations with predictable effects asking annotators to (re)write sentences taken from gold standards for other tasks such as question answering (Bar-Haim et al., 2006) and image and video description (Bowman et al., 2015; Marelli et al., 2014). RTE tasks may involve 2-way or 3-way inference judgements. In case of a 2-way judgement, the class to guess is either Entailment or Nonentailment. On the 3-way judgement scheme the Nonentailment class is further differentiated into Contradiction and Unknown. The presence of contradictory statements in social media can be indicative 1 for mis-/disinformation, controversy or speculation, which are important triggers in veracity checking procedures. Our present contribution therefore addresses the transformation of a project-internal corpus of annotated microblog texts in"
L16-1729,P08-1118,0,0.593236,"Missing"
L16-1729,N16-1138,0,0.160666,"Missing"
L16-1729,marelli-etal-2014-sick,0,0.035106,"bottleneck for this task is obtaining training data. The creation of natural language data annotated for inference phenomena is so far a nontrivial and largely manual procedure, yielding expensive resources that are nonetheless problematically portable to new text genres and application domains. Existing initiatives have often created RTE data by syntactic and lexical transformations with predictable effects asking annotators to (re)write sentences taken from gold standards for other tasks such as question answering (Bar-Haim et al., 2006) and image and video description (Bowman et al., 2015; Marelli et al., 2014). RTE tasks may involve 2-way or 3-way inference judgements. In case of a 2-way judgement, the class to guess is either Entailment or Nonentailment. On the 3-way judgement scheme the Nonentailment class is further differentiated into Contradiction and Unknown. The presence of contradictory statements in social media can be indicative 1 for mis-/disinformation, controversy or speculation, which are important triggers in veracity checking procedures. Our present contribution therefore addresses the transformation of a project-internal corpus of annotated microblog texts into a 3-way RTE dataset."
L16-1729,S16-1003,0,0.0544845,"t is centered on contradictory claims present in the data, and is extended to the other two classes to a limited extent. The resulting pilot dataset is balanced across the three classes. In future work we will investigate and evaluate the relevance of our data and compilation approach with respect to the RTE-5 Entailment Search pilot task14 and the RTE-6 Entailment Summarisation task15 , in which RTE systems are required to find all sentences in a document or a set that entail a given Hypothesis. RTE and its resources also tend to be utilized in the recently emerging task of stance detection (Mohammad et al., 2016), i.e. classification of the standpoint of an expression such as ”Climate change is a real concern” towards a piece of (social media) text as either supportive, denying, or neutral (Augenstein et al., 2016; Ferreira and Vlachos, 2016). It remains to be evaluated if the approaches built for stance detection are reusable or need specific adaptation to our goal of RTE in social media verification. Our current efforts include further development of the reported approach and the curation of project-internal data in other languages, in order to release16 several monolingual RTE benchmark resources."
L16-1729,D11-1147,0,0.0721362,"hod for the special purpose of information verification, which draws on manually established rumourous claims reported during crisis events. From about 500 English tweets related to 70 unique claims we compile and evaluate 5.4k RTE pairs, while continue automatizing the workflow to generate similar-sized datasets in other languages. Keywords: textual entailment, social media, verification 1. Introduction In this paper, we report on building a special-purpose Recognizing Textual Entailment (RTE) dataset in the context of information verification in user-generated content (Mendoza et al., 2010; Qazvinian et al., 2011; Procter et al., 2013) for the PHEME project1 . The dataset is compiled based on naturally occurring contradiction in manually labeled claims in crisis events discussed on Twitter, and to our knowledge is the first resource for RTE in the social media and verification domain. The detection of semantic inference phenomena between natural language text snippets, such as contradiction, entailment, and stance, is targeted by a number of research communities. Its most focused interest group formalizes inference tasks in the generic framework of RTE2 . RTE is applied to benefit several Natural Lang"
L16-1729,W07-1401,0,\N,Missing
L16-1729,N16-1170,0,\N,Missing
L18-1034,W13-3520,0,0.193178,"reasons. First, it allows to consider synonyms and terminological variants. For instance, “Schuhe”@de (shoes) should have a lower similarity when compared to “Schule”@de (school) than compared to its actual synonym “Fußbekleidung”@de (footwear). Second, shortened strings, such as abbreviations, co-occur in the same context as their full form but differ strongly in their surface form. However, the computational cost of training word embeddings increases proportionally with a rising number of languages considered. In this paper, we evaluate three existing embedding libraries, that is, Polyglot (Al-Rfou et al., 2013), a FastText (Bojanowski et al., 2016), and a word2vec embedding repository (Park, 2017) on an ontology alignment task in four languages: English, Spanish, German, and Italian. We compare the embedding libraries to a Jaccard baseline, which is a string-matching technique that has been shown to perform well on multilingual ontology alignment (Cheatham and Hitzler, 2013). For this task, two multilingual ontologies that exist in all four languages with enough overlap to allow for an alignment are needed and the structure of each ontology should be the same in all its languages, whereas the struct"
L18-1034,P14-1023,0,0.300646,"nt, multilingual resources, distributional semantics, comparison, evaluation 1. Introduction Word embeddings constitute a distributed word representation to leverage the semantics of words by mapping them to vectors of real numbers, where each dimension of the embedding represents a latent feature of the word (Turian et al., 2010). They have been shown to be very successful in many NLP tasks (Mikolov et al., 2013; CamachoCollados et al., 2016) and also ontology alignment (Zhang et al., 2014). Evaluations of embeddings mostly focus on English standard datasets with high frequency single words (Baroni et al., 2014; Riedl and Biemann, 2017) and the few available multilingual comparisons, such as by CamachoCollados et al. (2016), usually focus on one type of embedding. This paper proposes the use of a non-standard, domain-specific and multilingual dataset with multi-word expressions to compare three different pretrained embedding repositories. Re-use of existing embeddings is attractive since no training time or expertise in learning embeddings is required. Embeddings abstract away from the string’s surface form, which is not the case with syntactic similarity metrics, such as the Levenshtein edit distan"
L18-1034,S16-1124,0,0.0542181,"Missing"
L18-1034,P16-1176,0,0.0277153,"however, as can be seen from the good results obtained by FastText, this difference in similarity relation does not seem to be detrimental to the overall application of word embeddings to a label-centric ontology alignment task. In our results, English embeddings obtain better results than embeddings used in other languages to align ontology labels. Both ontologies were originally produced by Englishspeaking companies in English and then translated to the other languages. It has been shown that non-native language and translations are closer to each other than they are to the native language (Rabinovich et al., 2016). Thus, this difference in accuracy cannot necessarily be attributed to the embeddings but more likely to the input labels. For this purpose it would be interesting to repeat the experiment on a multilingual and structurally parallel standard dataset. We believe that this method can also be applied to other interesting scenarios. The similarity between the labels also hints at the similarity between the entire resources. Thus, this method could potentially be used to find similar resources in a repository of ontologies, which is offset in comparison to simple string matching by embeddings and"
L18-1034,W17-6933,0,0.111293,"urces, distributional semantics, comparison, evaluation 1. Introduction Word embeddings constitute a distributed word representation to leverage the semantics of words by mapping them to vectors of real numbers, where each dimension of the embedding represents a latent feature of the word (Turian et al., 2010). They have been shown to be very successful in many NLP tasks (Mikolov et al., 2013; CamachoCollados et al., 2016) and also ontology alignment (Zhang et al., 2014). Evaluations of embeddings mostly focus on English standard datasets with high frequency single words (Baroni et al., 2014; Riedl and Biemann, 2017) and the few available multilingual comparisons, such as by CamachoCollados et al. (2016), usually focus on one type of embedding. This paper proposes the use of a non-standard, domain-specific and multilingual dataset with multi-word expressions to compare three different pretrained embedding repositories. Re-use of existing embeddings is attractive since no training time or expertise in learning embeddings is required. Embeddings abstract away from the string’s surface form, which is not the case with syntactic similarity metrics, such as the Levenshtein edit distance (Levenshtein, 1966), co"
L18-1034,P10-1040,0,0.0772982,"d approach produces a number of correct alignments on a non-standard data set based on embeddings from the three repositories, where FastText embeddings performed best on all four languages and clearly outperformed the string-matching baseline. Keywords: word embeddings, ontology alignment, multilingual resources, distributional semantics, comparison, evaluation 1. Introduction Word embeddings constitute a distributed word representation to leverage the semantics of words by mapping them to vectors of real numbers, where each dimension of the embedding represents a latent feature of the word (Turian et al., 2010). They have been shown to be very successful in many NLP tasks (Mikolov et al., 2013; CamachoCollados et al., 2016) and also ontology alignment (Zhang et al., 2014). Evaluations of embeddings mostly focus on English standard datasets with high frequency single words (Baroni et al., 2014; Riedl and Biemann, 2017) and the few available multilingual comparisons, such as by CamachoCollados et al. (2016), usually focus on one type of embedding. This paper proposes the use of a non-standard, domain-specific and multilingual dataset with multi-word expressions to compare three different pretrained em"
L18-1094,W16-2017,1,0.874667,"Missing"
L18-1213,L18-1599,1,0.756254,"/Licence Ouverte, for France). LRs provided can be classified and viewed depending on the licence available: public domain, open under PSI, open licenses, standard licenses, and non-standard licenses. Validation of Language Resources within ELRC Validation Guidelines Implementation Validation can be understood as the quality control of a LR against a list of relevant criteria (Schneller et al., 2017). Due to the high number of LRs required within the project, the ELRC consortium decided to complement the donated LRs with additional LRs produced from scratch through a website crawling process (Papavassiliou et al, 2018). Web crawling was conducted using ILSP-FC, a comprehensive end-to-end solution for the acquisition of domain-specific monolingual and bilingual corpora from the web. Data Processing Each LR is analysed and processed by ELRC experts to ensure compliance with the Language Resources Data 12 https://www.maxprograms.com/products/tmxvalidator.html https://www.microsoft.com/en-us/download/ details.aspx?id=52608 14 https://github.com/aboSamoor/pycld2 13 10 11 http://lr-coordination.eu/helpdesk http://helpdesk.lr-coordination.eu/overview 1341 The ELRC partners (the National Anchor Points) initially id"
L18-1213,W12-0102,0,0.040596,"Missing"
L18-1213,L18-1391,1,0.797927,"eHealth Business Registers Interconnection System Safer Internet Cybersecurity Public Open Data Europeana Domain Consumers’ rights Social security, insurance Public procurement, contractual agreements Justice, Law Health, Medicine Business, market ICT ICT Multiple domains Culture Table 1: CEF Digital Service Infrastructures (DSIs) and their domains Even though the PSI Directive is an important instrument to open up public sector data, there are many challenges in collecting LRs from public services, such as lack of awareness, lack of technical or legal competence, poor data management, etc. (Vasiļjevs et al., 2018). 1.3 Setting up a European Language Resource Coordination (ELRC) European, national and regional public administrations deal with a huge amount of multilingual textual information in original and translated form. By sharing this linguistic data and turning it into language resources (LRs), they can improve the quality, coverage and performance of CEF eTranslation that needs multilingual LRs to train MT systems. In April 2015, the ELRC Consortium was set up through EC’s Connecting Europe Facility SMART 2014/1074 programme to initiate a number of actions with the aim to support the collection o"
lee-etal-2004-towards,callmeier-etal-2004-deepthought,0,\N,Missing
lee-etal-2004-towards,E95-1025,0,\N,Missing
lee-etal-2004-towards,W03-0802,0,\N,Missing
lee-etal-2004-towards,P03-1014,0,\N,Missing
lee-etal-2004-towards,C94-2144,0,\N,Missing
lee-etal-2004-towards,P03-2019,0,\N,Missing
lee-etal-2004-towards,kasper-etal-2004-integrated,0,\N,Missing
lee-etal-2004-towards,2002.jeptalnrecital-long.6,1,\N,Missing
P98-2195,C96-1049,1,0.586531,"Missing"
R15-1015,llewellyn-etal-2014-using,0,0.015974,"cs carried by hashtags. Our observations are based on a large Twitter corpus dedicated to riots in the UK in the summer of 20111.Variants for hashtags that refer to the same topic abound, e.g. “#LondonRiots”, “#londonriots”, “#RiotsInLondon”, “londonriot”, 1 This corpus was built on behalf of the newspaper „The Guardian“, and its first objective was to gather data for tracking the emergence of rumours in social media. See http://www.theguardian.com/news/datablog/2011/dec/08/tw itter-riots-interactive . An example usage of this corpus with NLP approaches for argumentation research is given in (Llewellyn et al., 2014). Piroska Lendvai Dept. of Computational Linguistics, Saarland University, Saarbrücken, Germany piroska.r@gmail.com “#LONDONRIOTS”, and so on. We hypothesize that consolidating variants to a preferred hashtag form would benefit further tasks that draw on semantic similarity, such as the recently organized Semantic Textual Similarity Shared Task on Twitter data2. We have implemented a set of scripts in order to normalize the surface forms of hashtags. This includes case normalization, lemmatization and syntactic segmentation. We first describe related work, then our approach, and finally displa"
R15-1015,buitelaar-etal-2004-towards,1,0.760878,"Missing"
R19-1027,L16-1386,1,0.89523,"Missing"
W01-1017,W00-1503,1,0.871131,"Missing"
W01-1017,M98-1007,0,0.07732,"Missing"
W01-1017,C96-2116,0,0.0408392,"Missing"
W01-1017,P98-2143,0,0.0201447,"Missing"
W01-1017,A97-1031,0,0.04498,"Missing"
W01-1017,C96-1021,0,\N,Missing
W01-1017,A00-1033,0,\N,Missing
W01-1017,C94-2144,0,\N,Missing
W01-1017,M95-1017,0,\N,Missing
W01-1017,C98-2138,0,\N,Missing
W01-1502,W01-1506,0,0.197751,"freely available morphological analyzer for Spanish running on a specific platform. Products can be listed in distinct sections. In order to know in which sections a product is to be found, the user can submit a standard query to the Registry Database. The underlying classification of the actual version of the ACL Registry is largely based on the book (Varile and Zampolli, 1996). But this taxonomy will probably have to be further specialized and extended in order to satisfy the majority of the visitors of the NLSR. Therefore the classification can be enriched by the products submitted 1 2 As (Bird and Simons, 2001) names it. See http://registry.dfki.de/ and/or by comments made by the visitors, introducing thus a bottom-up, developer and/or user oriented classification. A general goal of the most recent editions of the NLSR was the simplification of the registration procedure, providing a short form to be filled by the customer. We do not request anymore an exhaustive description of the submitted product, but concentrate on few points providing a guiding for the visitor, who will have to consult the home page of the institutions or authors having submitted their product for getting more detailed informat"
W01-1502,W01-1507,0,0.0198787,"nt for the systematic connection of the descriptions of both NLP tools and language resources. 6 Connection with Metadata-Descriptions for (Multimedia/Mltimodal) Language Resources Catalogue and repositories for Natural Language data resources have already been working on the topic of metadata description for their entries (See for example LDC and ELRA). One can see OLAC as a natural extension of the LDC, enlarging the resources catalogue to a real infrastructure for language resource identification. From the side of the Language Engineering there are initiatives for describing standards and (Calzolari et al., 2001) present such an initiative, the ISLE project, which is the continuation of the EAGLES initiative. The main objective of ISLE is to promote “widely agreed and urgently demanded standards and guidelines for infrastructural language resources ..., tools that exploit them and LE products”. The ongoing discussions within this project are thus important for the intended extension of NLP tools repositories. While (Calzolari et al., 2001) concentrate on the description of the task of the ISLE computational lexicon working group and address the topic of metadata for encoding multilingual lexical resou"
W01-1502,chaudiron-etal-2000-repository,0,0.146389,"repositories. In this paper we briefly address one of the central discussion point of the workshop: how to achieve a close interlinking between NLP tools and NL resources repositories. We will base this discussion on the ACL Natural Language Software Registry (see (Declerck et al., 2000)) and some papers printed in these proceedings (see the list of papers in the bibliography). The necessity of having repositories for NLP tools has already been clearly recognized in the past, and recently this topic has also been addressed within the broader context of a conference on Language Resources (see (Chaudiron et al., 2000) and (Declerck et al., 2000)). (Chaudiron et al., 2000) is essentially concerned with the question of identifying the NLP supply according to its different uses, and thus is describing a useroriented approach to NLP tools repositories. (Declerck et al., 2000) is mainly describing the functionalities of the new version of the ACL Natural Language Software Registry, also showing how this version can overcome some of the practical problems encountered by former repositories (a summarized presentation of the ACL Registry is given below in section 2). Both papers are also discussing the problem of"
W01-1502,declerck-etal-2000-new,1,0.455869,"Missing"
W01-1502,W01-1505,0,0.0306361,"osing a discussion on how to integrate in the description of the tools the particular relation to a specific corpus. Nevertheless this should be a common task to be tackled by all providers of tools repositories. Probably it would be the best strategy to start with specialized repositories, where the problems to solve can appear earlier. 5 Metadata for NLP Tools As we saw above, the sole conformance to standards (XML) for document description and interchange is not enough in the context of OLAC. But the use of metadata descriptions for tools seems to make sense not only for such initiatives. (Lavelli et al., 2001) show the use of metadata description for tools in the context of an infrastructure for NLP application development. The role of metadata there is to specify the “level of analysis accomplished by the source processor”. Thus the metadata descriptions are useful for the communication between processes within an NLP chain, and also allow to mark and identify the document produced by such a process. In any cases, the use of metadata description for tools (or processes triggered by those tools) is probably a key-issue in the modular design of complex NLP environment. And one can see in the SiSSA a"
W01-1502,W01-1503,0,\N,Missing
W09-3741,declerck-vela-2006-generic,1,0.899934,"Missing"
W13-2712,declerck-lendvai-2010-towards,1,0.846256,"t :NN ; lemon:leaf drop-out_comp ] ]; ]. in a NLP Before applying the (possibly extended) terminological material of TheSoz for supporting the semantic annotation of running texts, it has to be submitted to pre-processing steps, in order to ensure as a minimum a possible matching to morpho-syntactic variations of (elements of) the terms that are to be expected in external text. For this, we need to lexicalize the labels of the thesaurus, transforming the terms to linguistic data that can be used for matching linguistically processed text. A first sketch of this approach has been described in (Declerck & Lendvai, 2010) and a more elaborated methodology, encoding the linguistic data in RDF is presented in (McCrae et al, 2012). And for ensuring a linking of linguistic data in text to the conceptual elements of the thesaurus (or other knowledge sources), the development of an information extraction grammar is needed. We present in section 3.2 below an automatized approach for this. For both steps we are using the NooJ platform 8 , whose finite states engine supports the flexible implementation of lexicons, morphological, syntactic and semantic grammars. 4.1 For the sake of simplicity we do not display the lemo"
W13-5501,I08-1051,0,0.0753902,"esent typologically relevant phenomena, along with examples for their illustration and annotations (glosses) and translations applied to these examples (structurally comparable to corpus data), or word lists (structurally comparable to lexical-semantic resources). RDF as a generic representation formalism is thus particularly appealing for this class of resources. Finally, for linguistic corpora (Fig. 1, corpora), the potential of the Linked Data paradigm for modeling, processing and querying of corpora is immense, and RDF conversions of semantically annotated corpora have been proposed early [3]. RDF provides a graph-based data model as required for the interoperable representation of arbitrary kinds of annotation [2, 15], and this flexibility makes it a promising candidate for a general means of representation for corpora with complex and heterogeneous annotations. RDF does not only establish interoperability between annotations within a corpus, but also between corpora and other linguistic resources [4]. In comparison to other types of linguistic resources, corpora are currently underrepresented in the LLOD cloud, but the development of schemes for corpora and/or NLP annotations re"
W13-5501,chiarcos-2012-ontologies,1,0.897695,"to establish conceptual interoperability between language resources. If resourcespecific annotations or abbreviations are expanded into references to repositories of linguistic terminology and/or metadata categories, linguistic annotations, grammatical features and metadata specifications become more easily comparable. Important repositories developed by different communities include GOLD [9] and ISOcat [20, 19], yet, only recently these terminology repositories were put in relation with each other using Linked Data principles and with linguistic resources, e.g., within the OLiA architecture [5]. Linguistic databases are a particularly heterogeneous group of linguistic resources; they contain complex and manifold types of information, e.g., feature structures that represent typologically relevant phenomena, along with examples for their illustration and annotations (glosses) and translations applied to these examples (structurally comparable to corpus data), or word lists (structurally comparable to lexical-semantic resources). RDF as a generic representation formalism is thus particularly appealing for this class of resources. Finally, for linguistic corpora (Fig. 1, corpora), the p"
W13-5501,W07-1501,0,0.0927928,"s refer to ISOcat URIs. Ecosystem RDF as a data exchange framework is maintained by an interdisciplinary, large and active community, and it comes with a developed infrastructure that provides APIs, database implementations, technical support and validators for various RDF-based languages, e.g., reasoners for OWL. For developers of linguistic resources, this ecosystem can provide technological support or off-the-shelf implementations for common problems, e.g., the development of a database that is capable of supporting flexible, graph-based data structures as necessary for multi-layer corpora [15]. Beyond this, another advantage warrants a mention: The distributed approach of the Linked Data paradigm facilitates the distributed development of a web of resources and collaboration between researchers that provide and use this data and that employ a shared set of technologies. One consequence is the emergence of interdisciplinary efforts to create large and interconnected sets of resources in linguistics and beyond. LDL-2013 aims to provide a forum to discuss and to facilitate such on-going developments. LLOD: Building the Cloud Recent years have seen not only a number of approaches to pr"
W13-5501,wright-2004-global,0,0.0894745,"able URIs, it is possible to combine information from physically separated repositories in a single query at runtime. Information from different resources in the cloud can then be integrated freely. Dynamic Import If cross-references between linguistic resources are represented by resolvable URIs instead of system-defined ID references or static copies of parts from another resource, it is not only possible to resolve them at runtime, but also to have access to the most recent version of a resource. For community-maintained terminology repositories like the ISO TC37/SC4 Data Category Registry [20, 19, ISOcat], for example, new categories, definitions or examples can be introduced occasionally, and this information is available immediately to anyone whose resources refer to ISOcat URIs. Ecosystem RDF as a data exchange framework is maintained by an interdisciplinary, large and active community, and it comes with a developed infrastructure that provides APIs, database implementations, technical support and validators for various RDF-based languages, e.g., reasoners for OWL. For developers of linguistic resources, this ecosystem can provide technological support or off-the-shelf implementations for c"
W14-5803,clematide-etal-2012-mlsa,0,0.0339928,"Missing"
W14-5803,remus-etal-2010-sentiws,0,0.0645962,"Missing"
W14-5803,P05-1017,0,0.0246343,"Missing"
W14-5803,W13-5502,0,\N,Missing
W14-5805,W13-5502,0,0.0608932,"Missing"
W14-5805,krieger-declerck-2014-tmo,1,0.771348,"(2010) discusses this problem from the point of view of crowdsourcing, where there are multiple expert views and no certain ground truth - but we can equally apply this in the context of sentiment analysis, viewing each source as an expert. However, unlike their approach, our algorithm does not directly produce a classifier, but rather a newly labelled resource. Confronted with a multiplicity of data sources, some researchers have opted to link resources together (Eckle-Kohler and Gurevych, 2013). Indeed, the lexicons we consider in section 3 have already been compiled into a common format by Declerck and Krieger (2014). However, while linking resources makes it easier to access a larger amount of data, it does not solve the problem of how best to process it. To the best of our knowledge, there has not been a previous attempt to use a probabilistic model to merge a number of sentiment lexicons into a single resource. 3 Data Sources In the following subsections, we first describe four existing sentiment lexicons for German. These four lexicons represent the data we have merged into a single resource, with a size comparison given in table 1, where we count the number of distinct lemmas, not considering parts o"
W14-5805,W97-0802,0,0.121011,"ons into a single resource. 3 Data Sources In the following subsections, we first describe four existing sentiment lexicons for German. These four lexicons represent the data we have merged into a single resource, with a size comparison given in table 1, where we count the number of distinct lemmas, not considering parts of speech. Finally, in section 3.5, we describe the manually annotated MLSA corpus, which we use for evaluation. 3.1 Clematide and Klenner Clematide and Klenner (2010) manually curated a lexicon1 of around 8000 words, based on the synsets in GermaNet, a WordNet-like database (Hamp and Feldweg, 1997). A semi-automatic approach was used to extend the lexicon, first generating candidate polar words by searching in a corpus for coordination with known polar words, and then presenting these words to human annotators. We will refer to this resource as the C&K lexicon. 3.2 SentimentWortschatz Remus et al. (2010) compiled a sentiment lexicon2 from three data sources: a German translation of Stone et al. (1966)’s General Inquirer lexicon, a set of rated product reviews, and a German collocation dictionary. At this stage, words have binary polarity: positive or negative. To assign polarity weights"
W14-5805,remus-etal-2010-sentiws,0,0.12224,"parts of speech. Finally, in section 3.5, we describe the manually annotated MLSA corpus, which we use for evaluation. 3.1 Clematide and Klenner Clematide and Klenner (2010) manually curated a lexicon1 of around 8000 words, based on the synsets in GermaNet, a WordNet-like database (Hamp and Feldweg, 1997). A semi-automatic approach was used to extend the lexicon, first generating candidate polar words by searching in a corpus for coordination with known polar words, and then presenting these words to human annotators. We will refer to this resource as the C&K lexicon. 3.2 SentimentWortschatz Remus et al. (2010) compiled a sentiment lexicon2 from three data sources: a German translation of Stone et al. (1966)’s General Inquirer lexicon, a set of rated product reviews, and a German collocation dictionary. At this stage, words have binary polarity: positive or negative. To assign polarity weights, they use a corpus to calculate the mutual information of a target word with a small set of seed words. 1 2 http://bics.sentimental.li/index.php/downloads http://asv.informatik.uni-leipzig.de/download/sentiws.html 31 3.3 GermanSentiSpin Takamura et al. (2005) produced SentiSpin, a sentiment lexicon for English"
W14-5805,P05-1017,0,0.0995826,"s resource as the C&K lexicon. 3.2 SentimentWortschatz Remus et al. (2010) compiled a sentiment lexicon2 from three data sources: a German translation of Stone et al. (1966)’s General Inquirer lexicon, a set of rated product reviews, and a German collocation dictionary. At this stage, words have binary polarity: positive or negative. To assign polarity weights, they use a corpus to calculate the mutual information of a target word with a small set of seed words. 1 2 http://bics.sentimental.li/index.php/downloads http://asv.informatik.uni-leipzig.de/download/sentiws.html 31 3.3 GermanSentiSpin Takamura et al. (2005) produced SentiSpin, a sentiment lexicon for English. It is so named becaused it applies the Ising Model of electron spins. The lexicon is modelled as an undirected graph, with each word type represented by a single node. A dictionary is used to define edges: two nodes are connected if one word appears in the other’s definition. Each word is modelled as having either positive or negative sentiment, analogous to electrons being spin up or spin down. An energy function is defined across the whole graph, which prefers words to have the same sentiment if they are linked together. By using a small"
W14-5805,waltinger-2010-germanpolarityclues,0,0.36242,"ry is used to define edges: two nodes are connected if one word appears in the other’s definition. Each word is modelled as having either positive or negative sentiment, analogous to electrons being spin up or spin down. An energy function is defined across the whole graph, which prefers words to have the same sentiment if they are linked together. By using a small seed set of words which are manually assigned positive or negative sentiment, this energy function allows us to propagate sentiment across the entire graph, assigning each word a real-valued sentiment score in the interval [−1, 1]. Waltinger (2010b) translated the SentiSpin resource into German3 using an online dictionary, taking at most three translations of each English word. 3.4 GermanPolarityClues Waltinger (2010a) utilised automatic translations of two English resources: the SentiSpin lexicon, described in section 3.3 above; and the Subjectivity Clues lexicon, a manually annotated lexicon produced by Wilson et al. (2005). The sentiment orientations of the German translations were then manually assessed and corrected where necessary, to produce a new resource.4 3.5 MLSA To evaluate a sentiment lexicon, separately from the general t"
W14-5805,H05-1044,0,0.0783295,"ords which are manually assigned positive or negative sentiment, this energy function allows us to propagate sentiment across the entire graph, assigning each word a real-valued sentiment score in the interval [−1, 1]. Waltinger (2010b) translated the SentiSpin resource into German3 using an online dictionary, taking at most three translations of each English word. 3.4 GermanPolarityClues Waltinger (2010a) utilised automatic translations of two English resources: the SentiSpin lexicon, described in section 3.3 above; and the Subjectivity Clues lexicon, a manually annotated lexicon produced by Wilson et al. (2005). The sentiment orientations of the German translations were then manually assessed and corrected where necessary, to produce a new resource.4 3.5 MLSA To evaluate a sentiment lexicon, separately from the general task of judging the sentiment of an entire sentence, we relied on the MLSA (Multi-Layered reference corpus for German Sentiment Analysis). This corpus was produced by Clematide et al. (2012), independently of the above four lexicons, and consists of 270 sentences annotated at three levels of granularity. In the first layer, annotators judged the sentiment of whole sentences; in the se"
W14-5805,clematide-etal-2012-mlsa,0,\N,Missing
W15-5504,declerck-lendvai-2010-towards,1,0.890716,"Missing"
W15-5504,R15-1015,1,0.895455,"h the hashtags and their elements by linking them to existing semantic and lexical LOD resources: DBpedia and Wiktionary. 1 Introduction Applying term clustering methods to hashtags in social media posts is an emerging research thread in language and semantic web technologies. Hashtags often denote named entities and events, as exemplified by an entry from our reference corpus that includes Twitter 1 posts ('tweets') about the Ferguson unrest 2: &quot;#foxnews #FergusonShooting is in a long line of questionable acts by the police. Because some acted out does not excuse the police.&quot; In recent work (Declerck and Lendvai, 2015) we have applied string and pattern matching to address lexical variation in hashtags with the goal of normalizing, and subsequently contextualizing hashtagged strings. Types of contexts for a hashtag can be derived from e.g. hashtag cooccurrence and semantic relations between hahstags; representing such contexts necessitates 1 2 3 (Declerck and Lendvai, 2010) discussed already the possible benefits of the linguistic annotation of this type of language data. 4 A more technical definition of Linked Data is given at http://www.w3.org/standards/semanticweb/data 5 http://linguistics.okfn.org/ 6 ht"
W16-2017,W15-0122,0,\N,Missing
W16-2017,dima-etal-2014-tell,0,\N,Missing
W16-2017,R11-1058,0,\N,Missing
W16-2017,kunze-lemnitzer-2002-germanet,0,\N,Missing
W16-2017,francopoulo-etal-2006-lexical,0,\N,Missing
W19-5104,P13-1133,0,0.0152553,"odenet-49-n"" ili=""i51746"" partOfSpeech=""n"" confidenceScore=""1.0""> <Definition> Eine Sitzgelegenheit fuer eine Person, mit einer Lehne im Ruecken. </Definition> <SynsetRelation target=’odenet-11251-n’ relType=’hypernym’/> <SynsetRelation target=’odenet-8518-n 9 12 In a next step we will also consider the resource lemonUby (Eckle-Kohler et al., 2015), which contains a lemon representation of the German version of Omega-Wiki. A dump of this resource can be downloaded at https: //lemon-model.net/lexica/uby/ow_deu/. 10 https://www.openthesaurus.de/ 11 http://compling.hss.ntu.edu.sg/omw/. See also (Bond and Foster, 2013). https://github.com/ hdaSprachtechnologie/odenet. 13 See https://github.com/globalwordnet/ schemas/blob/master/WN-LMF.dtd for more details. 14 These are automatically generated and not yet curated entries that got their synset definition from an automatic linking to PWN. 23 relType=’hyponym’/> <SynsetRelation target=’odenet-20127-n’ relType=’hyponym’/> <SynsetRelation target=’odenet-34983-n’ relType=’hyponym’/> <Example> Sie sitzt auf dem Stuhl. </Example> </Synset> Access to the lemma information for hypernyms and hyponyms is also possible, so for the odenet49-n synset for “Stuhl”: Figure 1:"
W19-5104,W97-0802,0,0.614394,"Missing"
