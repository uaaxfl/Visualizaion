2001.mtsummit-papers.3,C96-1070,0,0.0318176,"Missing"
2001.mtsummit-papers.3,C92-2067,0,0.862792,"translated sentences have been manually ranked by native speakers of target languages. Such subjective evaluation by ranking, however, is taxing on both time and resources (King, 1996). The developers of TDMT would like to evaluate their MT system under development more frequently; therefore if automatic evaluation methods are inexpensive, fast, sufficiently accurate for them to assess whether or not the current version of their MT system is improved, then these automatic evaluation methods will prove beneficial. Conventional approaches to automatic evaluation include methods (Thompson, 1991; Su, 1992; Takezawa et al., 1999; Sugaya et al., 1999; Yasuda et al., 2000; Yasuda et al., 2001) that automatically assign one of several ranks (Sumita et al., 1999; Nagao & Tsujii, 1985) such as A, B, C, and D to MT output according to a single edit distance between an MT output and a correct translation example. The single edit distance can be differently designed, but changing its design makes assigning a certain rank more accurate, but another rank less accurate. For examples, EDi (i = 1, 5, 9, or 13) in Figure 1 differ from each other in its design. For ED1, the combination of edit operators: eith"
2001.mtsummit-papers.3,1999.mtsummit-1.34,1,0.851374,"Missing"
2001.mtsummit-papers.3,1999.mtsummit-1.44,0,0.027287,"Missing"
2001.mtsummit-papers.3,2001.mtsummit-papers.67,0,0.619385,"Missing"
2002.tmi-papers.20,J93-2003,0,0.170703,"by applying hierarchical phrase alignment. The hierarchical phrase alignment is a method to align bilingual sentences phrase-by-phrase employing the partial parse results. Based on the hierarchical phrase alignment, a translation model is trained on a chunked corpus by converting hierarchically aligned phrases into a sequence of chunks. The second method transforms the bilingual correspondence of the phrase alignments into that of translation model. Both of our approaches yield better quality of the translaiton model. 1 Introduction A statistical machine translation (SMT), first introduced by Brown et al. (1993), represents a translation process as a noisy channel model that consists of a source-channel model, a translation model and a prior, language model of target language texts. This transformed the problem of machine translation into a maximum posteriori solution to the source-channel paradigm. The translation model is based on word-for-word translation and limited to allow only one channel source word to be aligned from a channel target word. Although phrasal correspondence is implicitly implemented into some translation models by means of distortion, careful parameter training is required. In"
2002.tmi-papers.20,C92-2101,0,0.0830307,"nger input length. The results above demonstrate that the translation model parameters derived from the hierarchical phrase alignment were better than those acquired only by EM-training on a given corpus. One of the advantages of using hierarchical phrase alignment is that it is neutral to language pairs: They can share the same parsing system with a simple algorithm for aligning texts. The next advantage is the robustness to the input, since the phrase alignments are extracted from partial parse results. These advantages were not available in alignment methods of Yamamoto & Matsumoto (2000); Kaji et al. (1992). In addition, hierarchical phrase alignment is different from other chunking methods bacause it can preserve the correspondence in bilingual texts. Although the proposed method here did not use the higher level structures in the hierarchically aligned phrases, it will be challenging to incorporate those alignments restricted by non-terminal correspondences. The quality of translation is expected to be improved by including the restricted alignments into training steps. This idea is based on pegging (Brown et al. 1993) or from the work of Och & Ney (2000), in which a subset of all the alignmen"
2002.tmi-papers.20,J99-4005,0,0.129977,"he search problem is a critical issue for the success of statistical machine translation. The decoder, or the search system, should induce the source string from a sequence of target words by utilizing clues from a large numbers of parameters. Basically, if the vocabulary size is 10,000 and the output sentence length is 10, then 1000010 possible candidates must be enumerated. In addition, since the source sentence length is unknown to the decoder, the search system should also infer the total length of output at the same time. For details of the search problem, refer to Germann et al. (2001); Knight (1999); Och et al. (2001). 3 Hierarchical Phrase Alignment Hierarchical phrase alignment, proposed by Imamura (2001), computes the correspondence of sub-trees between source language and target language parse trees based on partial parse results. A phrase alignment is defined as an equivalent sequence of words between bilingual sentences, and it may be a sequence of words representing noun phrases and/or verb phrases etc. For instance, the sentence pairs, E: J: I have just arrived in Kyoto . kyoto ni tsui ta bakari desu . consists of three phrase alignments: in Kyoto arrived in Kyoto have just arriv"
2002.tmi-papers.20,W01-1405,0,0.0146108,"t word, B( f ), relative to the center of the previous source word, k. – d&gt;1 ( j − j0 |B( f )) : Distortion probability for non-head words. The position of a non-head word j is determined by the word class and relative to the previous target word generated from the same source word ( j0 ). • NULL Translation Model — p1 : A fixed probability of inserting a NULL word after determining each target word f (p0 = 1 − p1 ). For details, refer to Brown et al. (1993). 2.2 Problems in Statistical Machine Translation In statistical machine translation, there exists three key problems as described below (Ney 2001): Modeling Problem As this model suggests, a target word can be aligned to only a single source word. This restriction prohibits, for instance in Figure 1, “teitadake” from being mapped to both “could” and “you”, but allows only “could” to be mapped, and the other remaining source word, “you”, is treated as a zero fertility word. Och et al. (1999) introduced the concept of a translation template that could capture the phrase level correspondence, though the model relied on the HMM based translation model and could not be directly applied to fertility models such as the IBM Model 4. Training Pr"
2002.tmi-papers.20,P00-1056,0,0.826996,"el target word. Although phrasal correspondence is implicitly implemented into some translation models by means of distortion, careful parameter training is required. In addition, the training procedure relies on the EM algorithm, which can converge to an optimal solution but does not assured the global maximum parameter assignment. Furthermore, the translation models are represented by the numbers of parameters, so that easily suffered from the overfitting problem. In order to overcome these problems, simpler models, such as word-for-word translation models (Brown et al. 1993) or HMM models (Och & Ney 2000), have been introduced to determine the initial parameters and to bootstrap the training. This paper describes two methods to overcome the above problems by using hierarchical phrase alignment (Imamura 2001). Hierarchical phrase alignment (HPA) is a method to align bilingual texts phrase-by-phrase from partial parse results. One method converts the hierarchically aligned phrasal texts into a pair of sequences of chunks of words, treating the word-forword translation model as a chunk-for-chunk translation model. The second method computes the parameters for the translation model from the comput"
2002.tmi-papers.20,W99-0604,0,0.0797365,"y of inserting a NULL word after determining each target word f (p0 = 1 − p1 ). For details, refer to Brown et al. (1993). 2.2 Problems in Statistical Machine Translation In statistical machine translation, there exists three key problems as described below (Ney 2001): Modeling Problem As this model suggests, a target word can be aligned to only a single source word. This restriction prohibits, for instance in Figure 1, “teitadake” from being mapped to both “could” and “you”, but allows only “could” to be mapped, and the other remaining source word, “you”, is treated as a zero fertility word. Och et al. (1999) introduced the concept of a translation template that could capture the phrase level correspondence, though the model relied on the HMM based translation model and could not be directly applied to fertility models such as the IBM Model 4. Training Problem Training for the various parameters, t, n, p1 , d1 , d&gt;1 relies on the EM algorithm, which optimizes the log-likelihood of the model over a given bilingual corpus. The EM algorithm can find an optimal solution, although it cannot assure finding the globally best one. As the number of parameters is larger than those of speech it will become e"
2002.tmi-papers.20,W01-1408,0,0.209859,"em is a critical issue for the success of statistical machine translation. The decoder, or the search system, should induce the source string from a sequence of target words by utilizing clues from a large numbers of parameters. Basically, if the vocabulary size is 10,000 and the output sentence length is 10, then 1000010 possible candidates must be enumerated. In addition, since the source sentence length is unknown to the decoder, the search system should also infer the total length of output at the same time. For details of the search problem, refer to Germann et al. (2001); Knight (1999); Och et al. (2001). 3 Hierarchical Phrase Alignment Hierarchical phrase alignment, proposed by Imamura (2001), computes the correspondence of sub-trees between source language and target language parse trees based on partial parse results. A phrase alignment is defined as an equivalent sequence of words between bilingual sentences, and it may be a sequence of words representing noun phrases and/or verb phrases etc. For instance, the sentence pairs, E: J: I have just arrived in Kyoto . kyoto ni tsui ta bakari desu . consists of three phrase alignments: in Kyoto arrived in Kyoto have just arrived in Kyoto — — — k"
2002.tmi-papers.20,1999.mtsummit-1.34,1,0.807335,"Missing"
2002.tmi-papers.20,C00-2135,0,0.102903,"+train model is better for longer input length. The results above demonstrate that the translation model parameters derived from the hierarchical phrase alignment were better than those acquired only by EM-training on a given corpus. One of the advantages of using hierarchical phrase alignment is that it is neutral to language pairs: They can share the same parsing system with a simple algorithm for aligning texts. The next advantage is the robustness to the input, since the phrase alignments are extracted from partial parse results. These advantages were not available in alignment methods of Yamamoto & Matsumoto (2000); Kaji et al. (1992). In addition, hierarchical phrase alignment is different from other chunking methods bacause it can preserve the correspondence in bilingual texts. Although the proposed method here did not use the higher level structures in the hierarchically aligned phrases, it will be challenging to incorporate those alignments restricted by non-terminal correspondences. The quality of translation is expected to be improved by including the restricted alignments into training steps. This idea is based on pegging (Brown et al. 1993) or from the work of Och & Ney (2000), in which a subset"
2002.tmi-papers.20,P01-1030,0,\N,Missing
2002.tmi-papers.21,J00-1004,0,0.024364,"Missing"
2002.tmi-papers.21,P99-1009,0,0.0235605,"Missing"
2002.tmi-papers.21,1999.mtsummit-1.34,1,0.854328,"Missing"
2002.tmi-papers.21,C94-2116,0,0.0276097,"ful rules in a shorter period of time on a small training set compared to the best systems available. That is, current automatic acquisition methods in some tasks are no more efficient than humans. Consequently, it is important to consider the cooperation of humans and computers from the viewpoint of practicality. Streiter et al. (1999) proposed a strategy that rates manual MT rules by counting occurrence frequencies in a corpus. Some researchers expect this approach to contribute to the disambiguation of languages, but the approach is not applicable to the expansion and maintenance of rules. Tanaka (1994), in contrast, proposed an acquisition model for English case frames given by machine learning. This approach allows optimality to be maintained by confining the human descriptions of rules, but it still suffers from the problem of low knowledge construction efficiency, since it requires a bilingual tagged corpus of high quality. Given the above review, we have been considering what information is useful to humans. As one solution, Shirai et al. (1995) examined methods of making semantic structure dictionaries for Japanese-to-English MT. The results indicated that the most efficient method was"
2002.tmi-papers.21,1993.tmi-1.25,0,0.0444177,"adequate body of knowledge. The current approach to knowledge acquisition demands that a human rule writer, someone who is familiar with general linguistic knowledge and a framework of MT knowledge, manually check and correct the translation examples output by an MT system. This incurs high time and labor costs. While this is reasonable for creating prototype systems, its efficiency is too low to create practical MT systems. One engineering solution to this problem appears to be automatic knowledge acquisition (e.g., Almuallim et al. (1994); Alshawi et al. (2000); Kitamura & Matsumoto (1995); Watanabe (1993)). Such systems are capable of detecting simple rules directly from corpora through automatic learning. This approach is becoming more attractive due to the wide variety of corpora available. The current performance offered by automatic knowledge acquisition is rather suspect; the problem is that low-frequency phenomena are not well handled. Our basic approach is to combine human language skills with the automatic extraction of source and target language information. This is eminently practical since the number of corpora suitable for MT development continues to increase. The voluminous inform"
2002.tmi-papers.21,C94-1006,0,\N,Missing
2002.tmi-papers.9,J93-2003,0,0.00787485,"ed. The knowledge needs to be cleaned, since the corpus contains various translations and the phrase alignment contains errors. Various cleaning methods are applied in this paper. The results indicate that when the best cleaning method is used, the knowledge acquired by hierarchical phrase alignment is comparable to manually acquired knowledge. 1 Introduction Translation knowledge is necessary for machine translation (MT) systems. Automatic translation knowledge construction is an eﬀective way to reduce costs when applying a system to other task domains. Statistical translation methods (e.g., Brown et al. 1993) automatically acquire statistical models, which are considered elements of translation knowledge, so little cost is necessary. However, in most cases, these methods are applied to the same language families, such as English and French. In the case of diﬀerent families, the translation quality is still unclear. A hierarchical phrase alignment method has been proposed (Imamura 2001). This method hierarchically extracts equivalent phrases from a sentence-aligned bilingual corpus even though they belong to diﬀerent language families. Kaji et al. (1992), Yamamoto & Matsumoto (2000), and Meyers et"
2002.tmi-papers.9,C94-1015,0,0.313667,"ase and the target phrase are in the same syntactic category, the resulting synthesized sentence is appropriate. On the other hand, if they are in diﬀerent categories, the source or target sentence becomes grammatically inappropriate. The syntactic constraint suppresses such replacement. This is a particular advantage for translation between diﬀerent language families, since this phenomenon appears more frequently in such case than in translation between languages of the same language family. 3 Transfer Driven Machine Translation (TDMT) The Transfer Driven Machine Translation system, or TDMT (Furuse & Iida 1994; Sumita et al. 1999), used here is an MT system based on the syntactic transfer method. The following sections describe the abstract focusing of the transfer module. 3.1 Transfer Patterns Transfer patterns represent the correspondence between source language expressions and target language expressions. They are the most important kinds of knowledge in TDMT. Examples are shown in Figure 2 that include the preposition ‘at.’ In this pattern, source language information is constructed by a source pattern and its syntactic category. The source pattern is a sequence of instantiate-able variables an"
2002.tmi-papers.9,H91-1026,0,0.145172,"Missing"
2002.tmi-papers.9,C92-2101,0,0.0782787,"Missing"
2002.tmi-papers.9,P93-1004,0,0.0717435,"Missing"
2002.tmi-papers.9,J00-2004,0,0.0520015,"Missing"
2002.tmi-papers.9,W01-1406,0,0.0463296,"ent families, the translation quality is still unclear. A hierarchical phrase alignment method has been proposed (Imamura 2001). This method hierarchically extracts equivalent phrases from a sentence-aligned bilingual corpus even though they belong to diﬀerent language families. Kaji et al. (1992), Yamamoto & Matsumoto (2000), and Meyers et al. (2000) have also proposed methods to acquire translation knowledge automatically. They have evaluated the knowledge, but there are few examples in which the translation quality was evaluated when the entire knowledge was applied to translation systems (Menezes & Richardson 2001). This comprehensive level of quality should be measured on an actual translation system to judge whether the acquired knowledge is useful from a practical point of view. In this paper, translation knowledge is acquired automatically by hierarchical phrase alignment and integrated into a pattern-based MT system, and then the resulting translation quality is evaluated. Through the integration, the problem of ungeneralized patterns contained within the knowledge became clear. Because this problem caused bad translations or increased ambiguities, it became obvious that the knowledge needed to be"
2002.tmi-papers.9,C00-1078,0,0.133168,"Missing"
2002.tmi-papers.9,C94-1032,0,0.0269581,"ted by counting the phrases that satisfy the above two conditions, and the parsing candidate that has the maximum score is selected. Combination of Partial Trees: Partial parsing is an eﬀective way to avoid a lack of grammar or to parse ungrammatical sentences. It is used to combine partial candidates in the parser. Therefore, a criterion as to whether the part is valid or not is necessary for the combining process. We utilize the phrase score as the criterion, and a partial tree sequence that maximizes the sum of the phrase scores is searched for. The forward DP backward A* search algorithm (Nagata 1994) is employed to speed up the combination. 2.3 Placement in Translation Knowledge Acquisition The phrase alignment result by this method maintains correspondent parse trees and hierarchical information, so it is especially suitable for MT systems using syntactic transfer methods. Moreover, a characteristic of this method is the introduction of a syntactic constraint (Condition 2). 1 There are two eﬀects of the syntactic constraint. One is that few unnatural short phrases are extracted, as described above. The other is that it is easy to construct translation patterns because the phrases can be"
2002.tmi-papers.9,P91-1024,0,0.413143,"from training sentences. For instance, the ﬁrst rule of Figure 2 means that the English phrase “present at (the) conference” was translated into the Japanese phrase “kaigi “conference” de happyo-suru “present”.” 3.2 Translation Process At the time of translation, the source sentence is parsed using source patterns. Then, the target structure, which is mapped by target patterns, is generated (Figure 3). However, as shown in Figure 2, one transfer pattern has multiple target patterns. In order to select an appropriate target pattern, semantic distances (node distances on the thesaurus; refer to Sumita & Iida 1991) are calculated between the examples and the daughter headwords of the input sentence, and the target pattern that has the nearest example is selected. Therefore, each pattern also has head information. 1 The methods of Yamamoto & Matsumoto (2000) and Meyers et al. (2000) do not use syntactic categories. Alternatively, dependency structures are utilized. Chunks and relationships may be substituted Syn. Cat. VP Source Pattern XVP at YNP ⇒ NP XNP at YNP ⇒ Target Pattern Y’ de X’ Y’ ni X’ Y’ wo X’ Y’ no X’ Example ((present, conference) ...) ((stay, hotel), (arrive, p.m) ...) ((look, it) ...) ((m"
2002.tmi-papers.9,1999.mtsummit-1.34,0,0.276335,"Missing"
2002.tmi-papers.9,C00-2135,0,0.087238,"anslation methods (e.g., Brown et al. 1993) automatically acquire statistical models, which are considered elements of translation knowledge, so little cost is necessary. However, in most cases, these methods are applied to the same language families, such as English and French. In the case of diﬀerent families, the translation quality is still unclear. A hierarchical phrase alignment method has been proposed (Imamura 2001). This method hierarchically extracts equivalent phrases from a sentence-aligned bilingual corpus even though they belong to diﬀerent language families. Kaji et al. (1992), Yamamoto & Matsumoto (2000), and Meyers et al. (2000) have also proposed methods to acquire translation knowledge automatically. They have evaluated the knowledge, but there are few examples in which the translation quality was evaluated when the entire knowledge was applied to translation systems (Menezes & Richardson 2001). This comprehensive level of quality should be measured on an actual translation system to judge whether the acquired knowledge is useful from a practical point of view. In this paper, translation knowledge is acquired automatically by hierarchical phrase alignment and integrated into a pattern-base"
2002.tmi-papers.9,2001.mtsummit-ebmt.4,0,\N,Missing
2002.tmi-tutorials.1,W99-0905,0,\N,Missing
2002.tmi-tutorials.1,1987.mtsummit-1.11,0,\N,Missing
2002.tmi-tutorials.1,1995.tmi-1.28,0,\N,Missing
2002.tmi-tutorials.1,P98-1069,0,\N,Missing
2002.tmi-tutorials.1,C98-1066,0,\N,Missing
2002.tmi-tutorials.1,C92-2101,0,\N,Missing
2002.tmi-tutorials.1,C00-2131,0,\N,Missing
2002.tmi-tutorials.1,P95-1033,0,\N,Missing
2002.tmi-tutorials.1,C96-1078,0,\N,Missing
2002.tmi-tutorials.1,J97-2004,0,\N,Missing
2002.tmi-tutorials.1,1993.tmi-1.5,0,\N,Missing
2002.tmi-tutorials.1,C96-1030,0,\N,Missing
2004.iwslt-evaluation.2,J90-2002,0,0.724612,"imal solution due to the enormous search space. However, SMT can sort translations in the order of their quality according to its statistical models. We show two different EBMT systems here, briefly explain each system, and then compare them. Finally, we ex1. Introduction There are two main strategies used in corpus-based translation: 1. Example-Based Machine Translation (EBMT) [1]: EBMT uses the corpus directly. EBMT retrieves the translation examples that are best matched to an input expression and then adjusts the examples to obtain the translation. 2. Statistical Machine Translation (SMT) [2]: SMT learns statistical models for translation from corpora and dictionaries and then searches for the best translation at run-time according to the statistical models for language and translation. By using the IWSLT04 task, this paper describes two endeavors that are independent at this moment: (a) a hybridization of EBMT and statistical models, and (b) a new approach for SMT, phrase-based HMM. (a) is used in the “unrestricted” Japanese-to-English track (Section 2), and (b) is used in “supplied” Japanese-to-English and Chinese-toEnglish tracks (Section 3). In addition, paraphrasing technolog"
2004.iwslt-evaluation.2,W01-1401,1,0.863919,"on is performed. Finally, Imamura [8] proposed a feedback cleaning method that utilizes automatic evaluation to remove incorrect/redundant translation rules. BLEU was utilized to measure translation quality for the feedback process, and the hillclimbing algorithm was applied in searching for the combinatorial optimization. Utilizing the features of this task, incorrect/redundant rules were removed from the initial solution, which contains all rules acquired from the training corpus. Our experiments showed a considerable improvement in MT quality. 2.2. Two EBMTs 2.2.1. D3, DP-based EBMT Sumita [3] proposed D3 (Dp-match Driven transDucer), which exploits DP-matching between word sequences. Let’s illustrate the process with a simple sample below. Suppose we are translating a Japanese sentence into English. The Japanese input sentence (1-j) is translated into the English sentence (1-e) by utilizing the English sentence (2-e), whose source sentence (2-j) is similar to (1-j). The common parts are unchanged, and the different portions, shown in bold face, are substituted by consulting a bilingual dictionary. ;;; A Japanese input (1-j) iro/ga/ki/ni/iri/masen ;;; the most similar example in co"
2004.iwslt-evaluation.2,P91-1024,1,0.754066,"source sentence of examples from a bilingual corpus. For this, we use DP-matching, which tells us the edit distance between word sequences while giving us the matched portions between the input and the example. The edit distance is calculated as follows. The count of the inserted words, the count of the deleted words, and the semantic distance of the substituted words are summed. Then, this total is normalized by the sum of the lengths of the input and the source part of translation example. The semantic distance between two substituted words is calculated by using the hierachy of a thesaurus[4]. Our language resources in addition to a bilingual corpus are a bilingual dictionary, which is used for generating target sentences, and thesauri of both languages, which are used for incorporating the semantic distance between words into the distance between word sequences. Furthermore, lexical resources are also used for word alignment. Table 1: Resources used for two EBMTs in IWSLT04 unresticted Japanese-to-English track. bilingual corpus bilingual dictionary thesaurus grammar D3 travel domain (20K) in-house in-house N.A. HPAT travel domain (20K) in-house in-house in-house D3 achieves a go"
2004.iwslt-evaluation.2,C94-1015,0,0.0154509,"is used in “supplied” Japanese-to-English and Chinese-toEnglish tracks (Section 3). In addition, paraphrasing technologies, which are not used in the IWSLT04 task but boost translation performance, are also introduced in Section 4. 13 plain the selector used to determine the best from multiple translations based on SMT models. of the same syntactic category. Imamura [6] subsequently proposed HPA-based translation (HPAT). HPAed bilingual trees include all information necessary to automatically generate transfer patterns. Translation is done according to transfer patterns using the TDMT engine [7]. First, the source part of transfer patterns are utilized, and source structure is obtained. Second, structural changes are performed by mapping source patterns to target patterns. Finally, lexical items are inserted by referring to a bilingual dictionary, and then a conventional generation is performed. Finally, Imamura [8] proposed a feedback cleaning method that utilizes automatic evaluation to remove incorrect/redundant translation rules. BLEU was utilized to measure translation quality for the feedback process, and the hillclimbing algorithm was applied in searching for the combinatorial"
2004.iwslt-evaluation.2,P03-1057,1,0.787621,"s. of the same syntactic category. Imamura [6] subsequently proposed HPA-based translation (HPAT). HPAed bilingual trees include all information necessary to automatically generate transfer patterns. Translation is done according to transfer patterns using the TDMT engine [7]. First, the source part of transfer patterns are utilized, and source structure is obtained. Second, structural changes are performed by mapping source patterns to target patterns. Finally, lexical items are inserted by referring to a bilingual dictionary, and then a conventional generation is performed. Finally, Imamura [8] proposed a feedback cleaning method that utilizes automatic evaluation to remove incorrect/redundant translation rules. BLEU was utilized to measure translation quality for the feedback process, and the hillclimbing algorithm was applied in searching for the combinatorial optimization. Utilizing the features of this task, incorrect/redundant rules were removed from the initial solution, which contains all rules acquired from the training corpus. Our experiments showed a considerable improvement in MT quality. 2.2. Two EBMTs 2.2.1. D3, DP-based EBMT Sumita [3] proposed D3 (Dp-match Driven tran"
2004.iwslt-evaluation.2,C02-1076,1,0.866736,"7.00 70.00 77.60 83.40 16.60 HPAT 38.60 59.80 77.40 83.40 16.60 SELECT 59.80 73.00 82.40 87.80 12.20 DIFF. +2.80 +3.00 +4.80 +4.40 -4.40 Next, the relationship between translation quality of element systems and gain by the selector was analyzed. Table 5 shows that the proposed selector reduces the number of low-quality translations (ranked “D”) while it increases the number of high-quality translations (ranked “S” to “B”). 2.3. SMT-based Selector We proposed an SMT-based method of automatically selecting the best translation among outputs generated by multiple machine translation (MT) systems [9]. Conventional approaches to the selection problem include a method that automatically selects the output to which the highest probability is assigned according to a language model (LM). [10] These existing methods have two problems. First, they do not check whether information on source sentences is adequately translated into MT outputs, although they do check the fluency of MT outputs. Second, they do not take the statistical behavior of assigned scores into consideration. The proposed approach scores MT outputs by using not only the language but also a translation model (TM). To conduct a s"
2004.iwslt-evaluation.2,hogan-frederking-1998-evaluation,0,0.0713962,"e drastic reduction in mWER has been demonstrated (Table 6). However, the quality with the small corpus is not so bad in the subjective evaluation shown in Table 7. We conjecture that adequacy is not low even with the supplied corpus, and the translation become similar to native English, that is, its fluency improves as the size of corpus increases. 2.4. Results 2.4.1. Selecting Effect 2.5. Discussion As shown in Table 4, all of the metrics taken together show that the proposed selector outperforms both element transRelated works have proposed ways to merge MT outputs from multiple MT systems [12] in order to output better translations. When the source language and the target language have similar sentence structures, this merging apGood: easy to understand, with either some unimportant information missing or flawed grammar; (C) Fair: broken, but understandable with effort; (D) Unacceptable: important information has been translated incorrectly. 15 translations with additional constraints [17, 18, 19]:  P (¯fi |¯ ea i ) P (f |e) ≈ Table 7: ATR’s Overall Subjective Evaluation - IWSLT supplied corpus. S S,A S,A,B S,A,B,C D D3 34.80 47.40 62.60 73.40 26.60 HPAT 25.20 44.20 70.40 80.40 19"
2004.iwslt-evaluation.2,P01-1050,0,0.0620593,"age and the target language have different sentence structures, such as English and Japanese, we often have translations whose structures are different from each other for a single input sentences. Thus, the authors regard the merging approach as less suitable than the approach of selecting. Hybridization can be implemented in several arichitectures, for example, SMT followed by EBMT, SMT and EBMT in parallel, and so on. Which archtecture is best is still an interesting open question. In addition to the merging and selecting approaches, a modification approach can be taken. For example, Marcu [14] proposed a method in which initial translations are constructed by combining bilingual phrases from translation memory, which is followed by modifying the translations by greedy decoding [15]. Watanabe et al. [16] proposed a decoding algorithm in which translations that are similar to the input sentence are retrieved from bilingual corpora and then modified by greedy decoding. ¯|e) is further decomposed into three terms: The term P (f , ¯f , e ¯ ¯|e) = P (f |¯f , e ¯, e)P (¯f |¯ P (f , f , e e, e)P (¯ e|e) (5) The first term of Equation 5 represents the probability that a segmented input sent"
2004.iwslt-evaluation.2,P01-1030,0,0.0810071,"nces. Thus, the authors regard the merging approach as less suitable than the approach of selecting. Hybridization can be implemented in several arichitectures, for example, SMT followed by EBMT, SMT and EBMT in parallel, and so on. Which archtecture is best is still an interesting open question. In addition to the merging and selecting approaches, a modification approach can be taken. For example, Marcu [14] proposed a method in which initial translations are constructed by combining bilingual phrases from translation memory, which is followed by modifying the translations by greedy decoding [15]. Watanabe et al. [16] proposed a decoding algorithm in which translations that are similar to the input sentence are retrieved from bilingual corpora and then modified by greedy decoding. ¯|e) is further decomposed into three terms: The term P (f , ¯f , e ¯ ¯|e) = P (f |¯f , e ¯, e)P (¯f |¯ P (f , f , e e, e)P (¯ e|e) (5) The first term of Equation 5 represents the probability that a segmented input sentence ¯f can be reordered and generated as the input text of f . The second term indicates the ¯ and translation probability of the two phrase sequences of e ¯f . The last term is the likelihoo"
2004.iwslt-evaluation.2,2003.mtsummit-papers.54,1,0.856638,"s regard the merging approach as less suitable than the approach of selecting. Hybridization can be implemented in several arichitectures, for example, SMT followed by EBMT, SMT and EBMT in parallel, and so on. Which archtecture is best is still an interesting open question. In addition to the merging and selecting approaches, a modification approach can be taken. For example, Marcu [14] proposed a method in which initial translations are constructed by combining bilingual phrases from translation memory, which is followed by modifying the translations by greedy decoding [15]. Watanabe et al. [16] proposed a decoding algorithm in which translations that are similar to the input sentence are retrieved from bilingual corpora and then modified by greedy decoding. ¯|e) is further decomposed into three terms: The term P (f , ¯f , e ¯ ¯|e) = P (f |¯f , e ¯, e)P (¯f |¯ P (f , f , e e, e)P (¯ e|e) (5) The first term of Equation 5 represents the probability that a segmented input sentence ¯f can be reordered and generated as the input text of f . The second term indicates the ¯ and translation probability of the two phrase sequences of e ¯f . The last term is the likelihood of the phrase-segmen"
2004.iwslt-evaluation.2,N03-1017,0,0.00448749,"ndling long Japanese input. The latter was attributed to the fact that we tuned our parameter to mWER and we exploited phrase models as well. Table 8: Evaluation - IWSLT Chinese-to-English supplied task. System Top Our Bottom (15) where count(¯ e, f¯) is the cooccurrence frequency of the two phrases e¯ and f¯. The basic idea of Equation 15 is to capture the bilingual correspondence while considering two directions. Additional phrases were exhaustively induced based on the intersection/union of the viterbi word alignments of the two directional models, P (e|f ) and P (f |e), computed by GIZA++ [17]. After the extraction of phrase translation pairs, their monolingual phrase lexicons were extracted and used as the possible segmentation for the source and target sentences. mWER 45.59 46.99 61.69 Fluency 38.20 38.20 25.04 Adequacy 33.38 29.50 29.06 4. Other Features of C3 This section introduces another feature of C3: paraphrasing and filtering corpora, which are not used in the IWSLT04 task but are useful for boosting MT performance. The large variety of possible translations in a corpus causes difficulty in building machine translation on the corpus. Specifically, theis variety makes it m"
2004.iwslt-evaluation.2,2003.mtsummit-papers.53,0,0.0727491,"Missing"
2004.iwslt-evaluation.2,W03-1001,0,0.0212046,"Missing"
2004.iwslt-evaluation.2,W04-3216,0,0.0140311,"regarded as the distortion tion 5, the term P (f |¯f , e probability of how a phrase segmented sentence ¯f will be reordered to form the source sentence f . Instead, we model this as the likelihood of a particular phrase segment ¯fj observed in f : ¯, e) ∝ P (f |¯f , e P (¯f |f )  P (¯fj |f ) ≈ Equation 12 can be regarded as a Hidden Markov Model in ¯ j in the lattice F ¯ is treated as an which each source phrase F ¯ observation emitted from a state Ei , a target phrase, in the ¯ as shown in Figure 2. lattice E, (8) (9) j The use of the phrase-based HMM structure has already been proposed in [20] in the context of aligning documents and abstracts. In their approach, jump probabilities were explicitly encoded as the state transitions that roughly corresponded to the alignment probabilities in the context of the word-based statistical translation model. The use of the explicit jump or alignment probabilities served for the completeness of the translation modeling at the cost of the enormous search space needed to train the phrase-based HMM structure. The segmentation model is realized as the unigram posterior probability of the phrase ngram model presented in Section 3.1. To briefly sum"
2004.iwslt-evaluation.2,J03-1002,0,0.00549884,"rocedure generates the word-graph, or the lattice, of translations for an input sentence by using a beam search. On the first pass, the submodels of all phrase-based HMM translation models were integrated with the wordbased trigram language model and the class 5-gram model. The second pass uses A* strategy to search for the best path of translation on the generated word-graph. j fj2 ∩fj 2 =∅ 1  1 2 j  j ×P (eii2 +1 |eii1 )P (fj 2 |eii2 +1 )P (fj 2 |f ) 1 1 (14) To overcome the problem of local convergence often observed in the EM algorithm [21], we use the lexicon model from the GIZA++ [22] training as the initial parameters for the phrase translation model. In addition, the phrase ngram model and the phrase segmentation models are individually trained over the monolingual corpus and remained fixed during the HMM iterations. 3.8. Results The results appear strange in two points: (1) Our proposal didn’t work well for the Japanese-to-English track but did work well for the Chinese-to-English track; (2) Our proposal achieved high fluency but marked low adequacy. 3.6. Phrase Segment Induction Equations 13 and 14 involve summation over all possible contexts, either in its left-hand-s"
2004.iwslt-evaluation.2,P02-1038,0,0.0308911,"e segmentation model, and the phrase translation model – Equation 4 can be rewritten as  P (¯fj |f )P (¯fj |¯ ei )P (¯ ei |¯ ei ) (12) P (f |e) ≈ ¯,¯ e f j,i ¯ and ¯f are expanded If the phrase segmented sentences e ¯ and F, ¯ then into the corresponding lattice structures of E Therefore, the Forward-Backward algorithm can be for17 where P rj (e, f ) are the subcomponents of translation models, such as the phrase ngram model or the language model, and λj is the weight for each model. The weighting parameters, λj , can be efficiently computed based either on the maximum likelihood criterion [23] by IIS or GIS algorithms or on the minimum error rate criterion [24] by some unconstrained optimization algorithms, such as the Downhill Simplex Method [25]. mulated to solve the recursions α(eii21 , fjj12 ) = i 1 −2 i =1  j j α(eii1 −1 , fj 2 ) 1 j fj2 ∩fj 2 =∅ 1 1 ×P (eii21 |eii1 −1 )P (fjj12 |eii21 )P (fjj12 |f ) β(eii21 , fjj12 ) = l   i =i2 +2 j (13) j  β(eii2 +1 , fj 2 ) 1 The decoder is taken after the word-graph-based decoder [26], which allows the multi-pass decoding strategies to incorporate complicated submodel structures. The first pass of the decoding procedure ge"
2004.iwslt-evaluation.2,P03-1021,0,0.00794031,"an be rewritten as  P (¯fj |f )P (¯fj |¯ ei )P (¯ ei |¯ ei ) (12) P (f |e) ≈ ¯,¯ e f j,i ¯ and ¯f are expanded If the phrase segmented sentences e ¯ and F, ¯ then into the corresponding lattice structures of E Therefore, the Forward-Backward algorithm can be for17 where P rj (e, f ) are the subcomponents of translation models, such as the phrase ngram model or the language model, and λj is the weight for each model. The weighting parameters, λj , can be efficiently computed based either on the maximum likelihood criterion [23] by IIS or GIS algorithms or on the minimum error rate criterion [24] by some unconstrained optimization algorithms, such as the Downhill Simplex Method [25]. mulated to solve the recursions α(eii21 , fjj12 ) = i 1 −2 i =1  j j α(eii1 −1 , fj 2 ) 1 j fj2 ∩fj 2 =∅ 1 1 ×P (eii21 |eii1 −1 )P (fjj12 |eii21 )P (fjj12 |f ) β(eii21 , fjj12 ) = l   i =i2 +2 j (13) j  β(eii2 +1 , fj 2 ) 1 The decoder is taken after the word-graph-based decoder [26], which allows the multi-pass decoding strategies to incorporate complicated submodel structures. The first pass of the decoding procedure generates the word-graph, or the lattice, of translations for an input"
2004.iwslt-evaluation.2,W02-1021,0,0.0253988,"λj is the weight for each model. The weighting parameters, λj , can be efficiently computed based either on the maximum likelihood criterion [23] by IIS or GIS algorithms or on the minimum error rate criterion [24] by some unconstrained optimization algorithms, such as the Downhill Simplex Method [25]. mulated to solve the recursions α(eii21 , fjj12 ) = i 1 −2 i =1  j j α(eii1 −1 , fj 2 ) 1 j fj2 ∩fj 2 =∅ 1 1 ×P (eii21 |eii1 −1 )P (fjj12 |eii21 )P (fjj12 |f ) β(eii21 , fjj12 ) = l   i =i2 +2 j (13) j  β(eii2 +1 , fj 2 ) 1 The decoder is taken after the word-graph-based decoder [26], which allows the multi-pass decoding strategies to incorporate complicated submodel structures. The first pass of the decoding procedure generates the word-graph, or the lattice, of translations for an input sentence by using a beam search. On the first pass, the submodels of all phrase-based HMM translation models were integrated with the wordbased trigram language model and the class 5-gram model. The second pass uses A* strategy to search for the best path of translation on the generated word-graph. j fj2 ∩fj 2 =∅ 1  1 2 j  j ×P (eii2 +1 |eii1 )P (fj 2 |eii2 +1 )P (fj 2 |f ) 1 1 (1"
2004.iwslt-evaluation.2,shimohata-sumita-2002-automatic,1,0.850547,"xtract good transfer patterns for HPAT, and to estimate the parameters for SMT. 3.7. Decoder The decision rule to compute the best translation is based on the log-linear combinations of all subcomponents of translation models as presented in [23]. 1  ˆ = argmax λj log P rj (e, f ) (16) e Z(f ) j e We propose ways to overcome these problems by paraphrasing corpora through automated processes or filtering corpora by abandoning inappropriate expressions. 18 4.1. Paraphrasing for providing the Ruigo-Shin-Jiten. Three methods have been investigated for automatic paraphrasing. (1) Shimohata et al. [27] grouped sentences by the equivalence of the translation and extract rules of paraphrasing by DP-matching. (2) Finch et al. [28] clustered sentences in a paraphrase corpus to obtain pairs that are similar to each other for training SMT models. Then by using the models, the decoder generates a paraphrase. (3) Finch et al. [29] developed a paraphraser based on data-oriented parsing, which utilizes synatactic information within an examplebased framework. The experimental results indicate that the EBMT based on normalization of the source side had increased coverage [30] and that the SMT created o"
2004.iwslt-evaluation.2,2002.tmi-tutorials.2,0,0.02481,"st translation is based on the log-linear combinations of all subcomponents of translation models as presented in [23]. 1  ˆ = argmax λj log P rj (e, f ) (16) e Z(f ) j e We propose ways to overcome these problems by paraphrasing corpora through automated processes or filtering corpora by abandoning inappropriate expressions. 18 4.1. Paraphrasing for providing the Ruigo-Shin-Jiten. Three methods have been investigated for automatic paraphrasing. (1) Shimohata et al. [27] grouped sentences by the equivalence of the translation and extract rules of paraphrasing by DP-matching. (2) Finch et al. [28] clustered sentences in a paraphrase corpus to obtain pairs that are similar to each other for training SMT models. Then by using the models, the decoder generates a paraphrase. (3) Finch et al. [29] developed a paraphraser based on data-oriented parsing, which utilizes synatactic information within an examplebased framework. The experimental results indicate that the EBMT based on normalization of the source side had increased coverage [30] and that the SMT created on the normalized target sentences had a reduced word-error rate [31]. Finch et al. [32] demonstrated that the expansion of refer"
2004.iwslt-evaluation.2,C04-1017,1,0.875016,"Missing"
2004.iwslt-evaluation.2,E03-1029,1,0.895696,"Missing"
2004.iwslt-evaluation.2,W02-1611,1,\N,Missing
2004.iwslt-evaluation.2,watanabe-etal-2002-statistical,1,\N,Missing
2005.iwslt-1.5,J90-2002,0,0.320474,"aining data conditions showed the potential of the proposed hybrid approach and revealed new directions in how to improve the current system performance. 1. Introduction Corpus-based approaches to machine translation (MT) have achieved much progress over the last decades. There are two main strategies used in corpus-based translation: 1. Example-Based Machine Translation (EBMT) [1]: EBMT uses the corpus directly. EBMT retrieves the translation examples that are best matched to an input expression and then adjusts the examples to obtain the translation. 2. Statistical Machine Translation (SMT) [2]: SMT learns statistical models for translation from corpora and dictionaries and then searches for the best translation at run-time according to the statistical models for language and translation. Despite a high performance on average, these approaches can often produce translations with severe errors. However, different MT engines not always do the same error. Due to the particular output characteristics of each MT engine, quite different translation hypotheses are produced. Thus, combining multiple MT systems can boost the system performance by exploiting the strengths of each MT engine. W"
2005.iwslt-1.5,2003.mtsummit-papers.54,1,0.79351,"-based SMT engine [MSEP] (cf. Section 2.1.3), an SMT engine based on syntactic transfer [HPATR2] (cf. Section 2.1.4), an EBMT engine that incorporates word-level SMT methods [HPATR] (cf. Section 2.1.5), an EBMT engine based on hierarchical phrase alignments [HPAT] (cf. Section 2.1.6), an DP-match-driven EBMT engine [D3 ] (cf. Section 2.1.7), and a translation memory system [EM] (cf. Section 2.1.8). The translation knowledge of the eight MT systems is automatically acquired from a parallel corpus. The characteristics of the element MTs are summarized in Table 1. 2.1.1. SAT SAT is an SMT system [3]. The decoder searches for an optimal translation by using SMT models starting from a decoder seed, i.e., the source language input paired with an initial translation hypothesis. In SAT, the search is initiated from Preprocessing Translation Tagger Selection MT1 Hypothesis MTm Hypothesis 1 Input SELECTOR Chunker Parser Resources m TM LM 1 Output TM LM n Statistical Models (LM, TM) Thesaurus Dictionary Corpus (monolingual) Corpus (bilingual) Figure 1: System outline Table 1: Features of element MT engines U nit Coverage Quality Speed Resources SAT SMT PBHMTM sentence&word wide excellent modest"
2005.iwslt-1.5,P01-1030,0,0.0372404,"1: Features of element MT engines U nit Coverage Quality Speed Resources SAT SMT PBHMTM sentence&word wide excellent modest corpus phrase wide good slow corpus MSEP HPATR2 HPATR phrase wide good slow corpus, chunker phrase wide good modest corpus, parser phrase wide good modest corpus, parser similar translation examples retrieved from a parallel corpus. The similarity measure used here is a combination of an editdistance and tf/idf criteria as seen in the information retrieval framework [4]. The retrieved translations are modified by using a greedy search approach to find better translations [5]. 2.1.2. PBHMTM PBHMTM is a statistical MT system that is based on a phrase-based HMM translation model [6]. The model directly structures the phrase-based SMT approach in a Hidden Markov structure. The probability P (f |e) of translating a foreign source sentence f into a target language sentence e using noisy channel modeling is approximated by introduc¯, to explicitly capture ing two new hidden variables, ¯f and e the phrase translation relationship: X P(f |¯ f, ¯ e, e)P(¯ f |¯ e, e)P(¯ e|e) (1) P (f |e) = ¯ f ,¯ e The first term represents the probability that a phrasesegmented source lang"
2005.iwslt-1.5,2004.iwslt-evaluation.2,1,0.800582,"excellent modest corpus phrase wide good slow corpus MSEP HPATR2 HPATR phrase wide good slow corpus, chunker phrase wide good modest corpus, parser phrase wide good modest corpus, parser similar translation examples retrieved from a parallel corpus. The similarity measure used here is a combination of an editdistance and tf/idf criteria as seen in the information retrieval framework [4]. The retrieved translations are modified by using a greedy search approach to find better translations [5]. 2.1.2. PBHMTM PBHMTM is a statistical MT system that is based on a phrase-based HMM translation model [6]. The model directly structures the phrase-based SMT approach in a Hidden Markov structure. The probability P (f |e) of translating a foreign source sentence f into a target language sentence e using noisy channel modeling is approximated by introduc¯, to explicitly capture ing two new hidden variables, ¯f and e the phrase translation relationship: X P(f |¯ f, ¯ e, e)P(¯ f |¯ e, e)P(¯ e|e) (1) P (f |e) = ¯ f ,¯ e The first term represents the probability that a phrasesegmented source language sentence ¯f can be reordered and generated as the source text of f (Phrase Segmentation Model). The se"
2005.iwslt-1.5,W02-1021,0,0.0245308,"nted target language sentence e (Phrase Ngram Model). ¯ and ¯f are expanded If the phrase segmented sentences e ¯ ¯ then the apinto corresponding lattice structures E and F, proximation of the proposed models can be regarded as a Hidden Markov Model in which each source phrase in the EBMT HPAT D3 phrase wide good fast corpus, parser, thesaurus sentence narrow excellent fast corpus, thesaurus, bilingual dictionary EM sentence narrow excellent fast corpus ¯ is treated as an observation emitted from a state, a lattice F ¯ target phrase, in the lattice E. The decoder is a word-graph-based decoder [7], which allows the multi-pass decoding strategies to incorporate complicated submodel structures. The first pass of the decoding procedure generates the word-graph, or the lattice, of translations for an input sentence by using a beam search. On the first pass, the submodels of all phrase-based HMM translation models were integrated with the word-based trigram language model and the class 5-gram model. The second pass uses the A* strategy to search for the best path for translation on the generated word-graph. 2.1.3. MSEP MSEP is a phrase-based SMT system that utilizes morphosyntactic informat"
2005.iwslt-1.5,2005.mtsummit-papers.35,1,0.73862,"p1 ), we incorporate the following features into the loglinear translation model: • Class-based n-gram Q model: P r(eI1 ) = i Pr(ei |ci )P r(ci |ci−1 1 ) • Length model: P r(l|eI1 , f1J ), whereby l is the length (number of words) of a translated target sentence. • Phrase matching score: The translated target sentence is matched with phrase translation examples that are extracted from a parallel corpus based on bidirectional word alignment of phrase translation pairs. A score is derived based on the number of matches. 2.1.4. HPATR2 HPATR2 is a statistical MT system based on syntactic transfer [9]. The translation model of HPATR2 is defined as an inside probability of two parse trees, which is used to create probabilistic context-free grammar rules. The system searches for the best translation that maximizes the product of the following probabilities, where F, E are a source and a target parse trees, and θ, π are context-free grammar rules of the source and the target language, respectively. • Probability of Source Tree Model Y P (F) = P (θ) (3) θ:θ∈F • Probability of Target Tree Model Y P (E) = P (π) 2.1.6. HPAT HPAT is an example-based MT system based on syntactic transfer [11]. The"
2005.iwslt-1.5,C04-1015,1,0.7436,"π:π∈E A characteristic of HPATR2 is that not only word translations but also the translation of multi-word sequences is carried out by the syntactic transfer. Parsing hypotheses, which are multi-word sequences connected by context-free grammar rules, are created. The best hypothesis (parse tree and translation) is selected based on the above models. Therefore, HPATR2 is an MT system that contains features of phrase-based SMT as well as syntax-based SMT. 2.1.5. HPATR HPATR is an extension of the example-based HPAT system (cf. Section 2.1.6) that incorporates a word-based statistical MT system [10]. Similar to HPAT, an EBMT module based on syntactic transfer is used to generate translation candidates that have minimum semantic distances. However, word D3 (DP-match Driven transDucer) is an EBMT system that exploits DP-matching between word sequences [13]. In the translation process, D3 retrieves the most similar source sentence from a parallel corpus for an input sentence. The similarity is calculated based on the counts of insertion, deletion, and substitution operations, where the total is normalized by the sum of the lengths of the word sequences. Substitution considers the semantic d"
2005.iwslt-1.5,2002.tmi-papers.9,1,0.840155,"ansfer [9]. The translation model of HPATR2 is defined as an inside probability of two parse trees, which is used to create probabilistic context-free grammar rules. The system searches for the best translation that maximizes the product of the following probabilities, where F, E are a source and a target parse trees, and θ, π are context-free grammar rules of the source and the target language, respectively. • Probability of Source Tree Model Y P (F) = P (θ) (3) θ:θ∈F • Probability of Target Tree Model Y P (E) = P (π) 2.1.6. HPAT HPAT is an example-based MT system based on syntactic transfer [11]. The most important knowledge in HPAT are transfer rules, which define the correspondences between source and target patterns. The transfer rules can be regarded as synchronized context-free grammar rules. When the system translates an input sentence, the sentence is first parsed by using the source side of the transfer rules. Next, a tree structure of the target language is generated by mapping the source grammar rules to the corresponding target rules. When non-terminal symbols remain in the target tree, target words are inserted by referring to a translation dictionary. Ambiguities, which"
2005.iwslt-1.5,P03-1057,1,0.847084,"the target language is generated by mapping the source grammar rules to the corresponding target rules. When non-terminal symbols remain in the target tree, target words are inserted by referring to a translation dictionary. Ambiguities, which occur during parsing or mapping, are resolved by selecting the rules that minimize the semantic distance between the input words and source examples of the transfer rules. In general, the automatic acquisition process generates many redundant rules. To avoid this problem, HPAT optimizes the transfer rules by removing redundant rules (feedback cleaning, [12]) in order to increase an automatic evaluation score. 2.1.7. D3 (4) π:π∈E • Probability of Tree-mapping Model Y P (F|E)P (E|F) = P (θ|π)P (π|θ) selection is not performed during transfer, but all possible word translation candidates are generated. In a second step, an SMT module using a lexicon model and an n-gram language model is exploited to search for the best translation that maximizes the product of the probabilities. Therefore, HPATR selects the best translation among the output of example-based MT using models of statistical MT from the viewpoints of adequacy of word translation and fl"
2005.iwslt-1.5,W01-1401,1,0.85314,"ted. The best hypothesis (parse tree and translation) is selected based on the above models. Therefore, HPATR2 is an MT system that contains features of phrase-based SMT as well as syntax-based SMT. 2.1.5. HPATR HPATR is an extension of the example-based HPAT system (cf. Section 2.1.6) that incorporates a word-based statistical MT system [10]. Similar to HPAT, an EBMT module based on syntactic transfer is used to generate translation candidates that have minimum semantic distances. However, word D3 (DP-match Driven transDucer) is an EBMT system that exploits DP-matching between word sequences [13]. In the translation process, D3 retrieves the most similar source sentence from a parallel corpus for an input sentence. The similarity is calculated based on the counts of insertion, deletion, and substitution operations, where the total is normalized by the sum of the lengths of the word sequences. Substitution considers the semantic distance between two substituted words and is defined as the division of K, the level of the least common abstraction in the thesaurus of two words, by N, the height of the thesaurus [14]. According to the difference between the input sentence and the retrieved"
2005.iwslt-1.5,P91-1024,1,0.660049,"Ducer) is an EBMT system that exploits DP-matching between word sequences [13]. In the translation process, D3 retrieves the most similar source sentence from a parallel corpus for an input sentence. The similarity is calculated based on the counts of insertion, deletion, and substitution operations, where the total is normalized by the sum of the lengths of the word sequences. Substitution considers the semantic distance between two substituted words and is defined as the division of K, the level of the least common abstraction in the thesaurus of two words, by N, the height of the thesaurus [14]. According to the difference between the input sentence and the retrieved source sentence, the translation of the retrieved source sentence is modified by using dictionaries. 2.1.8. EM EM is a translation memory system that matches a given source sentence against the source language parts of translation examples extracted from a parallel corpus. In case an exact match can be achieved, the corresponding target language sentence will be used. Otherwise, the system fails to output a translation. 2.2. Selection of the Best MT Engine Output We use an SMT-based method of automatically selecting the"
2005.iwslt-1.5,C02-1076,1,0.892551,"retrieved source sentence, the translation of the retrieved source sentence is modified by using dictionaries. 2.1.8. EM EM is a translation memory system that matches a given source sentence against the source language parts of translation examples extracted from a parallel corpus. In case an exact match can be achieved, the corresponding target language sentence will be used. Otherwise, the system fails to output a translation. 2.2. Selection of the Best MT Engine Output We use an SMT-based method of automatically selecting the best translation among outputs generated by multiple MT systems [15]. This approach scores MT outputs by using multiple language (LM) and translation model (TM) pairs trained on different subsets of the training data. It uses a statistical test to check whether the average TM·LM score of one MT output is significantly higher than those of another MT output. The SELECTOR algorithm is summarized in Figure 2. (1) proc SELECTOR( Input, Corpus, n, M T 1 , . . . , M T m ) ; (2) begin (3) (∗ initalize statistical models ∗) (4) for each i in {1, . . . , n} do (5) Corpusi ← subset(Corpus) ; (6) T Mi ← translation-model(Corpusi ) ; (7) LMi ← language-model(Corpusi ) ; ("
2005.mtsummit-papers.35,2004.iwslt-evaluation.1,0,0.0116257,"hose products of the probabilities are the highest. 6 Experiments We evaluate the proposed method through Japanese to English translation. 6.1 Experimental Settings Corpora: The corpus used here is the Basic Travel Expression Corpus (BTEC) (Takezawa et al., 2002; Kikui et al., 2003). This is a collection of Japanese sentences and their English translations based on expressions that are usually found in phrasebooks for foreign tourists. The corpus size is shown in Table 2. IWSLT in Table 2 is a corpus used in the evaluation campaign of the International Workshop on Spoken Language Translation (Akiba et al., 2004), which is a subset of BTEC. The test set is the same as that of IWSLT. 2 Training: Word alignment was acquired from the Viterbi alignment of IBM model 4 using GIZA++ (Och and Ney, 2003). Charniak (2000)’s parser was used for English parsing, and a rule-based phrase structure parser developed in-house was used for Japanese parsing in the training phase. Word bigram and trigram models learned by CMU-Cambridge Statistical Language Modeling Toolkit (Clarkson and Rosenfeld, 1997) formed the language model. Evaluation Metrics: We used BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and mWER"
2005.mtsummit-papers.35,J93-2003,0,0.0122806,"by combining phrase (multiword sequence) translation and phrase reordering without syntax. On the other hand, SMT based on tree-to-tree mapping, which involves syntactic information, is theoretical, so its features remain unclear from the viewpoint of a practical system. The SMT proposed in this paper translates phrases with hierarchical reordering based on the bilingual parse tree. In our experiments, the best translation was obtained when both phrases and syntactic information were used for the translation process. 1 Introduction Statistical machine translation (SMT), originally proposed by Brown et al. (1993), has evolved from word-level translation to phrase-level (multi-word, i.e., flat phrases in this paper) translation (Koehn et al., 2003; Vogel et al., 2003; Zens and Ney, 2004). In phrase-based SMT, the cost of reordering words is reduced because the word order in a phrase is locally changed before translation. However, reordering phrases is also necessary for accurate translation. Most phrase-based SMT systems reorder phrases on a flat structure. Another approach, statistical machine translation based on tree-to-tree mapping, explicitly involves syntactic information and hierarchically reord"
2005.mtsummit-papers.35,2003.mtsummit-papers.6,0,0.0195235,"Reduce parsing failure is a task that must be accomplished to improve translation quality. 6.2.3 Translation Quality According to N -best Size Figure 6 shows the changes in the multiple word error rates according to the n-best size in the case of IWSLT. The translation quality by Pharaoh improved along with the expansion of the beam width. In the proposed methods, the quality was nearly fixed to the n-best size. Generally, beam search decoders decode from the 273 Related Work Statistical machine translation that employs syntax has been proposed as outlined below. Yamada and Knight (2001) and Charniak et al. (2003) proposed translation and language models in which the input sentence is mapped to the output parse tree. Even though they only used parse trees for one side, while we use both sides, a syntax-based language model would improve the fluency of translation. Graehl and Knight (2004) and Melamed (2004) proposed theoretical models that employ parse trees of source and target languages. Our proposed method is a realization of these methods. On the other hand, Koehn et al. (2003), Vogel et al. (2003), and Zens and Ney (2004) proposed phrase-based statistical MTs that do not use syntactic information."
2005.mtsummit-papers.35,A00-2018,0,0.0157509,"Travel Expression Corpus (BTEC) (Takezawa et al., 2002; Kikui et al., 2003). This is a collection of Japanese sentences and their English translations based on expressions that are usually found in phrasebooks for foreign tourists. The corpus size is shown in Table 2. IWSLT in Table 2 is a corpus used in the evaluation campaign of the International Workshop on Spoken Language Translation (Akiba et al., 2004), which is a subset of BTEC. The test set is the same as that of IWSLT. 2 Training: Word alignment was acquired from the Viterbi alignment of IBM model 4 using GIZA++ (Och and Ney, 2003). Charniak (2000)’s parser was used for English parsing, and a rule-based phrase structure parser developed in-house was used for Japanese parsing in the training phase. Word bigram and trigram models learned by CMU-Cambridge Statistical Language Modeling Toolkit (Clarkson and Rosenfeld, 1997) formed the language model. Evaluation Metrics: We used BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and mWER (multiple Word Error Rate, (Nießen et al., 2000)) 2 In these experiments, we arranged numerical words into numbers (e.g., “fifty/CD one/CD” → ‘51/CD’), so the number of words is different from that of IW"
2005.mtsummit-papers.35,N04-1014,0,0.145709,"d from word-level translation to phrase-level (multi-word, i.e., flat phrases in this paper) translation (Koehn et al., 2003; Vogel et al., 2003; Zens and Ney, 2004). In phrase-based SMT, the cost of reordering words is reduced because the word order in a phrase is locally changed before translation. However, reordering phrases is also necessary for accurate translation. Most phrase-based SMT systems reorder phrases on a flat structure. Another approach, statistical machine translation based on tree-to-tree mapping, explicitly involves syntactic information and hierarchically reordered words (Graehl and Knight, 2004; Melamed, 2004). However, these proposals are theoretical, and thus their features on a practical system remain unclear. This paper presents a practical method of statistical MT based on syntactic transfer, which is a kind of tree-to-tree mapping. Syntactic transfer has been widely used in machine translation, and it is suitable for a language pair whose respective structures are different (e.g., languages formed by SVO and SOV). An advantage of our method is that not only hierarchical reordering but also the flat phrases han267 dled in phrase-based SMT can be directly applied to the translat"
2005.mtsummit-papers.35,N03-1017,0,0.0394759,"Missing"
2005.mtsummit-papers.35,P04-1083,0,0.231027,"tion to phrase-level (multi-word, i.e., flat phrases in this paper) translation (Koehn et al., 2003; Vogel et al., 2003; Zens and Ney, 2004). In phrase-based SMT, the cost of reordering words is reduced because the word order in a phrase is locally changed before translation. However, reordering phrases is also necessary for accurate translation. Most phrase-based SMT systems reorder phrases on a flat structure. Another approach, statistical machine translation based on tree-to-tree mapping, explicitly involves syntactic information and hierarchically reordered words (Graehl and Knight, 2004; Melamed, 2004). However, these proposals are theoretical, and thus their features on a practical system remain unclear. This paper presents a practical method of statistical MT based on syntactic transfer, which is a kind of tree-to-tree mapping. Syntactic transfer has been widely used in machine translation, and it is suitable for a language pair whose respective structures are different (e.g., languages formed by SVO and SOV). An advantage of our method is that not only hierarchical reordering but also the flat phrases han267 dled in phrase-based SMT can be directly applied to the translation. The rest of"
2005.mtsummit-papers.35,niessen-etal-2000-evaluation,0,0.0182495,"The test set is the same as that of IWSLT. 2 Training: Word alignment was acquired from the Viterbi alignment of IBM model 4 using GIZA++ (Och and Ney, 2003). Charniak (2000)’s parser was used for English parsing, and a rule-based phrase structure parser developed in-house was used for Japanese parsing in the training phase. Word bigram and trigram models learned by CMU-Cambridge Statistical Language Modeling Toolkit (Clarkson and Rosenfeld, 1997) formed the language model. Evaluation Metrics: We used BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and mWER (multiple Word Error Rate, (Nießen et al., 2000)) 2 In these experiments, we arranged numerical words into numbers (e.g., “fifty/CD one/CD” → ‘51/CD’), so the number of words is different from that of IWSLT. Step 1 Step 2 VP X PP 12 ji ni VP Y V Y VB de leave exit start VP X Y NP ni V 12 ji Step 3 de VP X PP at 12 o’clock at noon to 12 o’clock leave at 12 o’clock leave at noon exit at 12 o’clock : VP Y VB leave exit start Step 4 VP X NP 12 o’clock noon VP VP 12 ji ni de leave at 12 o’clock leave 12 o’clock leave noon exit 12 o’clock : VP leave at 12 o’clock leave at noon leave 12 o’clock exit at 12 o’clock leave noon : (pruning) VP leave at"
2005.mtsummit-papers.35,P02-1038,0,0.0461785,"argmax P (e|f )2 e = argmax P (e)P (f |e)P (e|f ). e (4) According to the above representation, the final score of the translation model is given by multiplying the translation model from source to target (P (e|f ), forward translation model) and the translation model from target to source (P (f |e), backward translation model). Then, we obtain the score of the bidirectional translation model by applying Equation 3 to these models as follows. P (f |e)P (e|f ) =  P (F  |E  )P (E  |e) F  ,E  ·  P (E  |F  )P (F  |f ) E  ,F  ≈  Equation 5 is nearly equal to the log linear model (Och and Ney, 2002), in which the feature functions are probabilities of source/target tree models and tree mapping models, and the weights of the models are uniform. 3.3 Inside Probability The source and target tree models can be regarded as probabilistic context-free grammar (PCFG). Namely, nodes in the tree are generated independently of each other, and the probability of the tree is computed by the product of the probabilities of a parent node generating a child node sequence (i.e., the inside probability). P (F|f ) = P (E|e) = • Since the model includes the source tree model, a correct parse tree of the sou"
2005.mtsummit-papers.35,J03-1002,0,0.0284406,"here is a similar approach to acquisition of alignment templates (Och and Ney, 2004), which extracts phrases based on the continuity of word alignment. The phrase alignment in this paper uses not only the continuity of word alignment but also the constraints of parse trees. Namely, only phrases that are a part of the source and target trees are extracted. Figure 3 shows an example of this phrase alignment. The phrases are extracted as follows. 1. First, perform word alignment in both directions (source to target and target to source). We use Viterbi alignment of IBM model 4 learned by GIZA++ (Och and Ney, 2003). 2. Next, extract sure alignments, i.e., those that agree with Viterbi alignments in both directions. The alignments that do not agree in both directions are regarded as possible alignments. 270 For example, by focusing on the sure alignments (2) and (3) in Figure 3, the pair (NP → 12 o’clock) and (NP → 12 ji) is extracted as a bilingual phrase because it only contains (2) and (3) (i.e., it does not contain the sure alignments (1), (4), (5), and (6)). However, focusing on the sure alignments (4) and (5), there are no sub-trees that contain only them and thus do not contain (1), (2), (3), and"
2005.mtsummit-papers.35,J04-4002,0,0.0269972,"ional translation model can be computed in a bottom-up manner, so it can also naturally be applied to a bottom-up parser that parses two languages. 4 Training In the training phase, we assume that parse trees are given in advance in order to reduce complexity. Therefore, the problems in the training phase are (1) extracting corresponding nodes between bilingual trees (phrase alignment) and (2) estimating probabilities of the source and target tree models and the tree-mapping model. 4.1 Phrase Alignment The phrase alignment used here is a similar approach to acquisition of alignment templates (Och and Ney, 2004), which extracts phrases based on the continuity of word alignment. The phrase alignment in this paper uses not only the continuity of word alignment but also the constraints of parse trees. Namely, only phrases that are a part of the source and target trees are extracted. Figure 3 shows an example of this phrase alignment. The phrases are extracted as follows. 1. First, perform word alignment in both directions (source to target and target to source). We use Viterbi alignment of IBM model 4 learned by GIZA++ (Och and Ney, 2003). 2. Next, extract sure alignments, i.e., those that agree with Vi"
2005.mtsummit-papers.35,P02-1040,0,0.0732214,"rkshop on Spoken Language Translation (Akiba et al., 2004), which is a subset of BTEC. The test set is the same as that of IWSLT. 2 Training: Word alignment was acquired from the Viterbi alignment of IBM model 4 using GIZA++ (Och and Ney, 2003). Charniak (2000)’s parser was used for English parsing, and a rule-based phrase structure parser developed in-house was used for Japanese parsing in the training phase. Word bigram and trigram models learned by CMU-Cambridge Statistical Language Modeling Toolkit (Clarkson and Rosenfeld, 1997) formed the language model. Evaluation Metrics: We used BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and mWER (multiple Word Error Rate, (Nießen et al., 2000)) 2 In these experiments, we arranged numerical words into numbers (e.g., “fifty/CD one/CD” → ‘51/CD’), so the number of words is different from that of IWSLT. Step 1 Step 2 VP X PP 12 ji ni VP Y V Y VB de leave exit start VP X Y NP ni V 12 ji Step 3 de VP X PP at 12 o’clock at noon to 12 o’clock leave at 12 o’clock leave at noon exit at 12 o’clock : VP Y VB leave exit start Step 4 VP X NP 12 o’clock noon VP VP 12 ji ni de leave at 12 o’clock leave 12 o’clock leave noon exit 12 o’clock : VP leave at 12 o’clock"
2005.mtsummit-papers.35,1999.mtsummit-1.34,1,0.790879,"it at 12 o’clock leave noon : (pruning) VP leave at 12 o’clock Figure 4: Example of Decoding Set Name BTEC (Training) IWSLT (Training) Test Item # of Sentences # of Words # of Diff. Words # of Sentences # of Words # of Diff. Words # of Sentences # of Words # of Diff. Words Japanese English 152,170 1,178,419 1,103,600 16,686 10,669 20,000 188,533 182,018 8,652 6,133 500 — 4,018 — 888 — Table 2: Corpus Size metrics for the automatic evaluation. For the subjective evaluation, an English native classified the translations into the four ranks of A: Perfect, B: Fair, C: Acceptable, and D: Nonsense (Sumita et al., 1999). Note that a lower score denotes a better translation in the mWER metric. 6.2 Results 6.2.1 Translation Quality First, we measured the translation quality of the proposed method. The results are shown in Table 3. In order to measure the effect of syntactic transfer and flat phrases independently, two alternative methods were applied: • Flat phrases were excluded from the translation model (w/o phrases). Only the most primitive rules were applied to decoding. • Decoding was performed without syntactic information. We used the phrase-based beam 272 search decoder, Pharaoh, developed by USC ISI"
2005.mtsummit-papers.35,takezawa-etal-2002-toward,1,0.781689,"merged in Step 4. Through this process, not only the output sequence but also 271 the syntactic labels of the input and output are acquired, so the decoder can parse and transfer the higher structure. If the parsing of the input (or output) sentence fails, the decoder extracts partial translations from its agenda and sequentially outputs the translations whose products of the probabilities are the highest. 6 Experiments We evaluate the proposed method through Japanese to English translation. 6.1 Experimental Settings Corpora: The corpus used here is the Basic Travel Expression Corpus (BTEC) (Takezawa et al., 2002; Kikui et al., 2003). This is a collection of Japanese sentences and their English translations based on expressions that are usually found in phrasebooks for foreign tourists. The corpus size is shown in Table 2. IWSLT in Table 2 is a corpus used in the evaluation campaign of the International Workshop on Spoken Language Translation (Akiba et al., 2004), which is a subset of BTEC. The test set is the same as that of IWSLT. 2 Training: Word alignment was acquired from the Viterbi alignment of IBM model 4 using GIZA++ (Och and Ney, 2003). Charniak (2000)’s parser was used for English parsing,"
2005.mtsummit-papers.35,2003.mtsummit-papers.53,0,0.0914521,"es syntactic information, is theoretical, so its features remain unclear from the viewpoint of a practical system. The SMT proposed in this paper translates phrases with hierarchical reordering based on the bilingual parse tree. In our experiments, the best translation was obtained when both phrases and syntactic information were used for the translation process. 1 Introduction Statistical machine translation (SMT), originally proposed by Brown et al. (1993), has evolved from word-level translation to phrase-level (multi-word, i.e., flat phrases in this paper) translation (Koehn et al., 2003; Vogel et al., 2003; Zens and Ney, 2004). In phrase-based SMT, the cost of reordering words is reduced because the word order in a phrase is locally changed before translation. However, reordering phrases is also necessary for accurate translation. Most phrase-based SMT systems reorder phrases on a flat structure. Another approach, statistical machine translation based on tree-to-tree mapping, explicitly involves syntactic information and hierarchically reordered words (Graehl and Knight, 2004; Melamed, 2004). However, these proposals are theoretical, and thus their features on a practical system remain unclear."
2005.mtsummit-papers.35,P01-1067,0,0.13895,"ce can be applied to parsing. Reduce parsing failure is a task that must be accomplished to improve translation quality. 6.2.3 Translation Quality According to N -best Size Figure 6 shows the changes in the multiple word error rates according to the n-best size in the case of IWSLT. The translation quality by Pharaoh improved along with the expansion of the beam width. In the proposed methods, the quality was nearly fixed to the n-best size. Generally, beam search decoders decode from the 273 Related Work Statistical machine translation that employs syntax has been proposed as outlined below. Yamada and Knight (2001) and Charniak et al. (2003) proposed translation and language models in which the input sentence is mapped to the output parse tree. Even though they only used parse trees for one side, while we use both sides, a syntax-based language model would improve the fluency of translation. Graehl and Knight (2004) and Melamed (2004) proposed theoretical models that employ parse trees of source and target languages. Our proposed method is a realization of these methods. On the other hand, Koehn et al. (2003), Vogel et al. (2003), and Zens and Ney (2004) proposed phrase-based statistical MTs that do not"
2005.mtsummit-papers.35,N04-1033,0,0.0826958,"tion, is theoretical, so its features remain unclear from the viewpoint of a practical system. The SMT proposed in this paper translates phrases with hierarchical reordering based on the bilingual parse tree. In our experiments, the best translation was obtained when both phrases and syntactic information were used for the translation process. 1 Introduction Statistical machine translation (SMT), originally proposed by Brown et al. (1993), has evolved from word-level translation to phrase-level (multi-word, i.e., flat phrases in this paper) translation (Koehn et al., 2003; Vogel et al., 2003; Zens and Ney, 2004). In phrase-based SMT, the cost of reordering words is reduced because the word order in a phrase is locally changed before translation. However, reordering phrases is also necessary for accurate translation. Most phrase-based SMT systems reorder phrases on a flat structure. Another approach, statistical machine translation based on tree-to-tree mapping, explicitly involves syntactic information and hierarchically reordered words (Graehl and Knight, 2004; Melamed, 2004). However, these proposals are theoretical, and thus their features on a practical system remain unclear. This paper presents"
2016.amta-researchers.7,D11-1033,0,0.501687,"ction Machine translation is used for translating a variety of text types, including speech. However, it remains challenging to appropriately translate texts across all domains and only a limited number of domains have been targeted. The most promising approach to improve translation quality is to train the translator on massive bilingual corpora. However, collecting such corpora is challenging and expensive in several domains. Domain adaptation, which improves target domain quality by using data from another domain, has been proposed as a solution (Foster and Kuhn, 2007; Foster et al., 2010; Axelrod et al., 2011; Bisazza et al., 2011; Sennrich, 2012; Sennrich et al., 2013). This technique is important when applying machine translation to practical tasks. This paper presents methods of domain adaptation for statistical machine translation (SMT) that assume multiple domains. The proposed methods combine multiple models using log-linear interpolation. These are simple yet effective approaches to take advantage of multiple domains based on feature augmentation (Daum´e, 2007), a domain adaptation technique used in machine learning. We propose the following two methods. 1. Simultaneous optimization of mult"
2016.amta-researchers.7,W07-0702,0,0.0173464,"development corpus2 . 3.3 Optimization 3.3.1 Joint Optimization One merit of feature augmentation in machine learning is that conventional algorithms can be used for optimization because feature augmentation operates only in the feature space. Machine translation uses optimization algorithms such as MERT (Och, 2003), pairwise ranking optimization (PRO) (Hopkins and May, 2011), and k-best batch MIRA (KBMIRA) (Cherry and Foster, 2012). We employ KBMIRA in this paper because it is appropriate for high-dimensional optimization3 . 2 Moses assigns -100 as the empty value (Koehn and Schroeder, 2007; Birch et al., 2007). As we describe in Section 4.2, this is extremely small and produces low BLEU scores. 3 Another reason is that the BLEU score of a baseline system was the highest in our preliminary experiments. 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S A major difference between general machine learning and optimization in machine translation is in the loss functions. The loss functions of machine learning algorithms use likelihood output by decoders. In contrast, the optimization algorithms employed in machine translation use both likelihood and automati"
2016.amta-researchers.7,2011.iwslt-evaluation.18,0,0.549189,"ion is used for translating a variety of text types, including speech. However, it remains challenging to appropriately translate texts across all domains and only a limited number of domains have been targeted. The most promising approach to improve translation quality is to train the translator on massive bilingual corpora. However, collecting such corpora is challenging and expensive in several domains. Domain adaptation, which improves target domain quality by using data from another domain, has been proposed as a solution (Foster and Kuhn, 2007; Foster et al., 2010; Axelrod et al., 2011; Bisazza et al., 2011; Sennrich, 2012; Sennrich et al., 2013). This technique is important when applying machine translation to practical tasks. This paper presents methods of domain adaptation for statistical machine translation (SMT) that assume multiple domains. The proposed methods combine multiple models using log-linear interpolation. These are simple yet effective approaches to take advantage of multiple domains based on feature augmentation (Daum´e, 2007), a domain adaptation technique used in machine learning. We propose the following two methods. 1. Simultaneous optimization of multiple domains: this met"
2016.amta-researchers.7,N12-1047,0,0.183885,"should be computed from the probability distribution of the phrases, we treat it as a hyper-parameter. In other words, empty values are set experimentally to maximize the BLEU score of a development corpus2 . 3.3 Optimization 3.3.1 Joint Optimization One merit of feature augmentation in machine learning is that conventional algorithms can be used for optimization because feature augmentation operates only in the feature space. Machine translation uses optimization algorithms such as MERT (Och, 2003), pairwise ranking optimization (PRO) (Hopkins and May, 2011), and k-best batch MIRA (KBMIRA) (Cherry and Foster, 2012). We employ KBMIRA in this paper because it is appropriate for high-dimensional optimization3 . 2 Moses assigns -100 as the empty value (Koehn and Schroeder, 2007; Birch et al., 2007). As we describe in Section 4.2, this is extremely small and produces low BLEU scores. 3 Another reason is that the BLEU score of a baseline system was the highest in our preliminary experiments. 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S A major difference between general machine learning and optimization in machine translation is in the loss functions. The los"
2016.amta-researchers.7,P11-2031,0,0.0242386,"a-En translation. If we treat these values as probabilities, they are exp(−7) ≈ 0.0009 and exp(−6) ≈ 0.0025, respectively. Evaluation Metrics We used word BLEU, the translation edit rate (TER) (Snover et al., 2006), Meteor (English only) (Denkowski and Lavie, 2014), and the rank-based intuitive bilingual evaluation score (RIBES; Japanese only) (Isozaki et al., 2010) as the evaluation metrics. 5 http://research.nii.ac.jp/ntcir/index-ja.html 6 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S The MultEval tool (Clark et al., 2011)7 was used for statistical testing8 with the signiﬁcance level set to p &lt; 0.05. The mean scores of ﬁve runs were used to reduce instability in optimization. Although we used multiple metrics, for simplicity we will describe the results using BLEU. Comparison Methods We compared various methods using the single-domain model as the baseline. We used the following conventional methods, which have been described in Section 2. • Corpus Concatenation: A corpus-concatenated model was constructed using all domain data. Optimization and testing were performed using the development and test sets of each"
2016.amta-researchers.7,2012.amta-papers.4,0,0.359509,"translated them while changing the weight vectors of the linear and log-linear interpolations. Although they used perplexities as objective functions to estimate the weights, optimization algorithms, such as minimum error rate training (MERT) (Och, 2003), have been used recently to estimate weight vectors (Foster et al., 2010). 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S Feature augmentation (Daum´e, 2007) is a domain adaptation method used in machine learning that simultaneously optimizes the weight vector of each domain (cf., Section 3.1). Clark et al. (2012) applied it to machine translation as a type of log-linear interpolation; however, they only adapted the weight vectors of a model. Model Adaptation There are basically two approaches to achieve domain adaptation by changing the feature vector h(e, f ). The ﬁrst is model adaptation, which modiﬁes trained submodels, and the second is corpus ﬁltering, which trains models using adapted corpora. The ﬁll-up method (Bisazza et al., 2011), translation model combination (Sennrich, 2012), and instance weighting (Foster et al., 2010; Matsoukas et al., 2009) are well-known model adaptation methods. The ﬁ"
2016.amta-researchers.7,P07-1033,0,0.247024,"Missing"
2016.amta-researchers.7,W14-3348,0,0.0410445,"Missing"
2016.amta-researchers.7,W08-0334,1,0.802845,"in Adaptation for Statistical Machine Translation Domain adaptation is applied when the target domain (in-domain) data are insufﬁcient but data from another domain (out-domain) are available in sufﬁcient quantities. Domain adaptation in machine translation aims to improve the translation quality of in-domain texts using both in-domain and out-domain data. There are two types of domains: those that are predeﬁned, such as “News” and “Web,” and those that are artiﬁcially created via automatic clustering. Even when using automatic clustering, the translation quality can be improved in some cases (Finch and Sumita, 2008; Sennrich et al., 2013). However, in this study, we have used predeﬁned domains. Corpus Concatenation The simplest approach to achieving domain adaptation for SMT is training the model using a concatenated corpus of in- and out-domain data. We refer to this method as corpus concatenation. The trained model is optimized using development (held-out) data of the in-domain. In machine learning, a model trained on a concatenated corpus has features that are intermediate between the in- and the out-domains. Therefore, model accuracy is also generally intermediate between models trained individually"
2016.amta-researchers.7,D10-1044,0,0.200207,"ear models. 1 Introduction Machine translation is used for translating a variety of text types, including speech. However, it remains challenging to appropriately translate texts across all domains and only a limited number of domains have been targeted. The most promising approach to improve translation quality is to train the translator on massive bilingual corpora. However, collecting such corpora is challenging and expensive in several domains. Domain adaptation, which improves target domain quality by using data from another domain, has been proposed as a solution (Foster and Kuhn, 2007; Foster et al., 2010; Axelrod et al., 2011; Bisazza et al., 2011; Sennrich, 2012; Sennrich et al., 2013). This technique is important when applying machine translation to practical tasks. This paper presents methods of domain adaptation for statistical machine translation (SMT) that assume multiple domains. The proposed methods combine multiple models using log-linear interpolation. These are simple yet effective approaches to take advantage of multiple domains based on feature augmentation (Daum´e, 2007), a domain adaptation technique used in machine learning. We propose the following two methods. 1. Simultaneou"
2016.amta-researchers.7,W07-0717,0,0.230586,"using standard log-linear models. 1 Introduction Machine translation is used for translating a variety of text types, including speech. However, it remains challenging to appropriately translate texts across all domains and only a limited number of domains have been targeted. The most promising approach to improve translation quality is to train the translator on massive bilingual corpora. However, collecting such corpora is challenging and expensive in several domains. Domain adaptation, which improves target domain quality by using data from another domain, has been proposed as a solution (Foster and Kuhn, 2007; Foster et al., 2010; Axelrod et al., 2011; Bisazza et al., 2011; Sennrich, 2012; Sennrich et al., 2013). This technique is important when applying machine translation to practical tasks. This paper presents methods of domain adaptation for statistical machine translation (SMT) that assume multiple domains. The proposed methods combine multiple models using log-linear interpolation. These are simple yet effective approaches to take advantage of multiple domains based on feature augmentation (Daum´e, 2007), a domain adaptation technique used in machine learning. We propose the following two me"
2016.amta-researchers.7,P13-2121,0,0.0344768,"rom ASPEC-JE. Translation System Each source sentence was preordered using an in-house preordering system (Section 4.5 of Goto et al. (2015)) trained for general-purpose. The same preordering system was applied to all domains. In addition, all Japanese sentences, including the test sets, were segmented into words in advance using the MeCab morphological analyzer (Kudo et al., 2004). The phrase tables and lexicalized reordering models were trained using the default settings in the Moses toolkit. The 5-gram language models were learned from the target side of the training sentences using KenLM (Heaﬁeld et al., 2013). Multi-domain KBMIRA, described in Section 3.3.1, was used for optimization. A clone of the Moses decoder was used for decoding. The settings were the same as the default values in Moses, i.e., phrase table limit = 20, distortion limit = 6, and the beam width was 200. When the decoder selected phrase pair candidates, 1) phrase pairs were ﬁrst obtained from all phrase tables, 2) a likelihood of each phrase was computed in accordance with the augmented feature space, and 3) the highest 20 pairs were selected. Empty Value The empty value described in Section 3.2.1 was set empirically. From integ"
2016.amta-researchers.7,D11-1125,0,0.0266107,"ough an empty value is a type of unknown probability and should be computed from the probability distribution of the phrases, we treat it as a hyper-parameter. In other words, empty values are set experimentally to maximize the BLEU score of a development corpus2 . 3.3 Optimization 3.3.1 Joint Optimization One merit of feature augmentation in machine learning is that conventional algorithms can be used for optimization because feature augmentation operates only in the feature space. Machine translation uses optimization algorithms such as MERT (Och, 2003), pairwise ranking optimization (PRO) (Hopkins and May, 2011), and k-best batch MIRA (KBMIRA) (Cherry and Foster, 2012). We employ KBMIRA in this paper because it is appropriate for high-dimensional optimization3 . 2 Moses assigns -100 as the empty value (Koehn and Schroeder, 2007; Birch et al., 2007). As we describe in Section 4.2, this is extremely small and produces low BLEU scores. 3 Another reason is that the BLEU score of a baseline system was the highest in our preliminary experiments. 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S A major difference between general machine learning and optimizatio"
2016.amta-researchers.7,Q13-1035,0,0.205305,"Missing"
2016.amta-researchers.7,D10-1092,0,0.0488247,"Missing"
2016.amta-researchers.7,W14-3627,0,0.0154271,"domain from the outdomain corpora on the basis of cross-entropy difference (i.e., modiﬁed Moore-Lewis ﬁltering). Then, they trained the models using the in-domain corpus with additional sentences. Corpus ﬁltering adapts not only phrase tables but also all submodels used in the translator. However, the ideal number of additional sentences cannot be estimated in advance. Another Approach Another approach that does not require changing the likelihoods is connecting two translators in series. A translation result generated by the out-domain translator is re-translated by the in-domain translator (Jeblee et al., 2014). This method treats the generation of domain-speciﬁc translation as error correction. 3 Multi-domain Adaptation 3.1 Feature Augmentation Feature augmentation is used to adapt feature weights to domains in machine learning. The feature space is segmented into the following subspaces: common, out-domain (source domain), and in-domain (target domain). In-domain features are copied to the in-domain and common spaces, and out-domain features are copied to the out-domain and common spaces. The adapted weight vector is obtained by optimizing the entire space. The in- and out-domain features deployed"
2016.amta-researchers.7,P07-2045,0,0.0137197,"corporating Corpus-Concatenated Model and SingleDomain Models where hc and hi denote the feature vectors of the common and the domain-speciﬁc spaces, respectively. All features are deployed to the common space, but only features that match the domain are copied to the domain space. hc hi = = Φ(f, e)  Φ(f, e) ∅ (3) if domain(f ) = i otherwise (4) where Φ(f, e) denotes the subvector that stores the model scores and so on. It is equal to h(f, e) if no feature augmentation is applied. We obtain the weight vector by optimizing this feature matrix. We use the default features of the Moses toolkit (Koehn et al., 2007) (15 dimensions) in the experiments reported in Section 4. The number of dimensions in the augmented feature space is 15 in the common space and 14 in each of the domain spaces1 . Clark et al. (2012) applied feature augmentation to machine translation from Arabic to English (with News and Web domains) and Czech to English (six domains, e.g., Fiction). Only a corpus-concatenated model was used to obtain features so that feature functions were not changed to reﬂect the different domains. 3.2 Core of Proposed Methods 3.2.1 Corpus-Concatenated Model and Single-domain Models In machine translation,"
2016.amta-researchers.7,N03-1017,0,0.0634641,"_S this space as that used in the standard log-linear model. This can be realized via a slight modiﬁcation of existing translation systems. Both methods use a corpus-concatenated model, which covers multiple domains and contains few unknown words, and single-domain models, which are accurate in their speciﬁc domains. In addition, we tune the hyper-parameter of the multiple-model combination. With appropriate settings, state-of-the-art domain adaptation can be realized even when using standard log-linear models. In this study, we use phrase-based statistical machine translation (PBSMT) (Koehn et al., 2003, 2007) with preordering. The remainder of this paper is organized as follows. Section 2 brieﬂy reviews domain adaptation in machine translation. Section 3 explains our proposed methods in detail. Section 4 discusses the characteristics of our methods through experiments, and Section 5 concludes the paper. 2 Domain Adaptation for Statistical Machine Translation Domain adaptation is applied when the target domain (in-domain) data are insufﬁcient but data from another domain (out-domain) are available in sufﬁcient quantities. Domain adaptation in machine translation aims to improve the translati"
2016.amta-researchers.7,W07-0733,0,0.0412336,"ximize the BLEU score of a development corpus2 . 3.3 Optimization 3.3.1 Joint Optimization One merit of feature augmentation in machine learning is that conventional algorithms can be used for optimization because feature augmentation operates only in the feature space. Machine translation uses optimization algorithms such as MERT (Och, 2003), pairwise ranking optimization (PRO) (Hopkins and May, 2011), and k-best batch MIRA (KBMIRA) (Cherry and Foster, 2012). We employ KBMIRA in this paper because it is appropriate for high-dimensional optimization3 . 2 Moses assigns -100 as the empty value (Koehn and Schroeder, 2007; Birch et al., 2007). As we describe in Section 4.2, this is extremely small and produces low BLEU scores. 3 Another reason is that the BLEU score of a baseline system was the highest in our preliminary experiments. 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S A major difference between general machine learning and optimization in machine translation is in the loss functions. The loss functions of machine learning algorithms use likelihood output by decoders. In contrast, the optimization algorithms employed in machine translation use both li"
2016.amta-researchers.7,W04-3230,0,0.0271748,"were provided by the international conference NTCIR-8, and the test set was provided by NTCIR-95 . • ASPEC: An Asian scientiﬁc paper excerpt corpus (Nakazawa et al., 2016)6 . We used a million sentences of high-conﬁdence translation from ASPEC-JE. Translation System Each source sentence was preordered using an in-house preordering system (Section 4.5 of Goto et al. (2015)) trained for general-purpose. The same preordering system was applied to all domains. In addition, all Japanese sentences, including the test sets, were segmented into words in advance using the MeCab morphological analyzer (Kudo et al., 2004). The phrase tables and lexicalized reordering models were trained using the default settings in the Moses toolkit. The 5-gram language models were learned from the target side of the training sentences using KenLM (Heaﬁeld et al., 2013). Multi-domain KBMIRA, described in Section 3.3.1, was used for optimization. A clone of the Moses decoder was used for decoding. The settings were the same as the default values in Moses, i.e., phrase table limit = 20, distortion limit = 6, and the beam width was 200. When the decoder selected phrase pair candidates, 1) phrase pairs were ﬁrst obtained from all"
2016.amta-researchers.7,D09-1074,0,0.0775839,"e weight vector of each domain (cf., Section 3.1). Clark et al. (2012) applied it to machine translation as a type of log-linear interpolation; however, they only adapted the weight vectors of a model. Model Adaptation There are basically two approaches to achieve domain adaptation by changing the feature vector h(e, f ). The ﬁrst is model adaptation, which modiﬁes trained submodels, and the second is corpus ﬁltering, which trains models using adapted corpora. The ﬁll-up method (Bisazza et al., 2011), translation model combination (Sennrich, 2012), and instance weighting (Foster et al., 2010; Matsoukas et al., 2009) are well-known model adaptation methods. The ﬁll-up method changes feature values. If a phrase is contained in an in-domain phrase table, the feature values in that table are used. Otherwise, the feature values in the out-domain phrase table are used. Translation model combination generates a new phrase table by combining two translation probabilities of in- and out- domains. The weights of the combination are determined using each feature function to minimize the perplexity on a development set. Instance weighting modiﬁes each model parameter in the phrase table to discriminate between the i"
2016.amta-researchers.7,P03-1021,0,0.104288,"els. The overall likelihood is computed by the following equation: log P (e|f ) ∝ w · h(e, f ) (1) where h(e, f ) is a feature vector and w is a weight vector of the feature functions. Then, a domain-speciﬁc translation is generated by changing the weight vector w of each domain. For example, Foster and Kuhn (2007) trained single-domain PBSMT models and translated them while changing the weight vectors of the linear and log-linear interpolations. Although they used perplexities as objective functions to estimate the weights, optimization algorithms, such as minimum error rate training (MERT) (Och, 2003), have been used recently to estimate weight vectors (Foster et al., 2010). 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S Feature augmentation (Daum´e, 2007) is a domain adaptation method used in machine learning that simultaneously optimizes the weight vector of each domain (cf., Section 3.1). Clark et al. (2012) applied it to machine translation as a type of log-linear interpolation; however, they only adapted the weight vectors of a model. Model Adaptation There are basically two approaches to achieve domain adaptation by changing the featur"
2016.amta-researchers.7,P02-1040,0,0.0965662,"is is extremely small and produces low BLEU scores. 3 Another reason is that the BLEU score of a baseline system was the highest in our preliminary experiments. 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S A major difference between general machine learning and optimization in machine translation is in the loss functions. The loss functions of machine learning algorithms use likelihood output by decoders. In contrast, the optimization algorithms employed in machine translation use both likelihood and automatic evaluation scores, such as BLEU (Papineni et al., 2002). Automatic evaluation scores are computed by comparing system outputs with their reference translations over the entire document. In fact, MERT and KBMIRA contain BLEU scores of the development set in their loss functions4 . This means that BLEU scores must be computed for each domain to optimize multiple domains. To solve this problem, we modify the KBMIRA algorithm. The modiﬁcations to Algorithm 1 proposed by Cherry and Foster (2012) are as follows. 1. The variable BG that maintains BLEU statistics (such as the number of n-gram matches) is extended to the D-dimensional array, where D denote"
2016.amta-researchers.7,E12-1055,0,0.194504,"ating a variety of text types, including speech. However, it remains challenging to appropriately translate texts across all domains and only a limited number of domains have been targeted. The most promising approach to improve translation quality is to train the translator on massive bilingual corpora. However, collecting such corpora is challenging and expensive in several domains. Domain adaptation, which improves target domain quality by using data from another domain, has been proposed as a solution (Foster and Kuhn, 2007; Foster et al., 2010; Axelrod et al., 2011; Bisazza et al., 2011; Sennrich, 2012; Sennrich et al., 2013). This technique is important when applying machine translation to practical tasks. This paper presents methods of domain adaptation for statistical machine translation (SMT) that assume multiple domains. The proposed methods combine multiple models using log-linear interpolation. These are simple yet effective approaches to take advantage of multiple domains based on feature augmentation (Daum´e, 2007), a domain adaptation technique used in machine learning. We propose the following two methods. 1. Simultaneous optimization of multiple domains: this method uses an opti"
2016.amta-researchers.7,P13-1082,0,0.272476,"of text types, including speech. However, it remains challenging to appropriately translate texts across all domains and only a limited number of domains have been targeted. The most promising approach to improve translation quality is to train the translator on massive bilingual corpora. However, collecting such corpora is challenging and expensive in several domains. Domain adaptation, which improves target domain quality by using data from another domain, has been proposed as a solution (Foster and Kuhn, 2007; Foster et al., 2010; Axelrod et al., 2011; Bisazza et al., 2011; Sennrich, 2012; Sennrich et al., 2013). This technique is important when applying machine translation to practical tasks. This paper presents methods of domain adaptation for statistical machine translation (SMT) that assume multiple domains. The proposed methods combine multiple models using log-linear interpolation. These are simple yet effective approaches to take advantage of multiple domains based on feature augmentation (Daum´e, 2007), a domain adaptation technique used in machine learning. We propose the following two methods. 1. Simultaneous optimization of multiple domains: this method uses an optimizer extended to multip"
2016.amta-researchers.7,2006.amta-papers.25,0,0.039511,"Missing"
2016.amta-researchers.7,D13-1112,0,0.0512636,"Missing"
2020.wat-1.3,P11-2031,0,0.127481,"Missing"
2020.wat-1.3,N19-1423,0,0.0206702,"aster than the self-attention mechanism because they depend only on the last state. Similar to our method, which is described in this paper, Zhou et al. (2019) proposed a model that simultaneously decodes two tokens from the head and tail of a 2.2 Non-autoregressive Decoding Non-autoregressive decoding generates all tokens simultaneously, utilizing the parallelism of Transformer (Gu et al., 2017; Lee et al., 2018; Ghazvininejad et al., 2019). For example, the Mask-Predict method (Ghazvininejad et al., 2019) recovers masked tokens ([mask]) using the left and right contexts, like BERT encoders (Devlin et al., 2019). The initial tokens are all masks. Because of the parallel generation, the generation lengths must be determined in advance. The Mask-Predict method predicts these lengths from the encoder output. Although non-autoregressive decoding performs fast generation, the translation quality in one step is relatively low. We can improve the quality by iteratively applying the parallel decoding. However, iterative decoding causes the following problems. • The decoding speed reduces as the number of iterations increases. This is a trade-off between quality and speed. • Iterative non-autoregressive decod"
2020.wat-1.3,D19-1633,0,0.16977,"n systems are based on an encoder–decoder architecture. Although there are some frameworks, such as recurrent neural network-based translation (Sutskever et al., 2014; Bahdanau et al., 2014) and Transformerbased translation (Vaswani et al., 2017), they employ autoregressive decoding for high-quality translation. However, autoregressive decoding requires a decoding time that depends on sentence length because it generates a single token in each step. To solve this problem, non-autoregressive decoding, which generates all tokens in one step, has been proposed (Gu et al., 2017; Lee et al., 2018; Ghazvininejad et al., 2019). However, the translation quality of non-autoregressive decoding has not yet matched the quality of autoregressive decoding. To improve the quality, some methods, such as Mask-Predict (Ghazvininejad et al., 2019), apply non-autoregressive decoding iteratively. This is a trade-off between quality and – All tokens can be learned in parallel in the training phase. – In contrast to non-autoregressive decoders, our method does not determine generation lengths in advance. In the following sections, we first briefly review autoregressive and non-autoregressive decoding. We then explain the proposed"
2020.wat-1.3,W17-4123,0,0.10563,"tion Most neural machine translation systems are based on an encoder–decoder architecture. Although there are some frameworks, such as recurrent neural network-based translation (Sutskever et al., 2014; Bahdanau et al., 2014) and Transformerbased translation (Vaswani et al., 2017), they employ autoregressive decoding for high-quality translation. However, autoregressive decoding requires a decoding time that depends on sentence length because it generates a single token in each step. To solve this problem, non-autoregressive decoding, which generates all tokens in one step, has been proposed (Gu et al., 2017; Lee et al., 2018; Ghazvininejad et al., 2019). However, the translation quality of non-autoregressive decoding has not yet matched the quality of autoregressive decoding. To improve the quality, some methods, such as Mask-Predict (Ghazvininejad et al., 2019), apply non-autoregressive decoding iteratively. This is a trade-off between quality and – All tokens can be learned in parallel in the training phase. – In contrast to non-autoregressive decoders, our method does not determine generation lengths in advance. In the following sections, we first briefly review autoregressive and non-autoreg"
2020.wat-1.3,Q19-1042,0,0.0301868,"Missing"
2020.wat-1.3,W18-2716,0,0.0339688,"Missing"
2020.wat-1.3,P18-1166,0,0.0718059,"o compute the current state hℓt . However, the previous states have already been computed while predicting the previous tokens. Therefore, only the state hℓ−1 needs to be computed if t we preserve the previous states. We call this inner state preservation in this paper. When training, all tokens can be learned in parallel using a mask of the triangular matrix, which restricts the tokens for the self-attention mechanism (Figure 1(a)). Another strategy for speeding up autoregressive decoding is to substitute the self-attention mechanism with the other units. The Average Attention Network (AAN) (Zhang et al., 2018; JunczysDowmunt et al., 2018) and Simpler Simple Recurrent Unit (SSRU) (Kim et al., 2019) are faster than the self-attention mechanism because they depend only on the last state. Similar to our method, which is described in this paper, Zhou et al. (2019) proposed a model that simultaneously decodes two tokens from the head and tail of a 2.2 Non-autoregressive Decoding Non-autoregressive decoding generates all tokens simultaneously, utilizing the parallelism of Transformer (Gu et al., 2017; Lee et al., 2018; Ghazvininejad et al., 2019). For example, the Mask-Predict method (Ghazvininejad et al"
2020.wat-1.3,D18-1149,0,0.0691345,"machine translation systems are based on an encoder–decoder architecture. Although there are some frameworks, such as recurrent neural network-based translation (Sutskever et al., 2014; Bahdanau et al., 2014) and Transformerbased translation (Vaswani et al., 2017), they employ autoregressive decoding for high-quality translation. However, autoregressive decoding requires a decoding time that depends on sentence length because it generates a single token in each step. To solve this problem, non-autoregressive decoding, which generates all tokens in one step, has been proposed (Gu et al., 2017; Lee et al., 2018; Ghazvininejad et al., 2019). However, the translation quality of non-autoregressive decoding has not yet matched the quality of autoregressive decoding. To improve the quality, some methods, such as Mask-Predict (Ghazvininejad et al., 2019), apply non-autoregressive decoding iteratively. This is a trade-off between quality and – All tokens can be learned in parallel in the training phase. – In contrast to non-autoregressive decoders, our method does not determine generation lengths in advance. In the following sections, we first briefly review autoregressive and non-autoregressive decoding."
2020.wat-1.3,N19-4009,0,0.0242604,"used as the validation sets. All corpora were segmented into subwords (Sennrich et al., 2016). We used 37K shared vocabulary in WMT-14 and 16K vocabularies for the source and target languages in ASPEC. 1. Divide the sequence of tokens into left and right halves, reverse the right half, and alternately fold the halves. 2. Supply one or two EOD tokens, to make the total number of tokens even. The self-attention mask for training is a triangular matrix in which the unit is a pair of tokens (Figure 1(b)). 4 Experiments 4.1 Experimental Settings Systems: We modified the fairseq translation system (Ott et al., 2019)1 for the proposed method. For comparison, we considered the following three system types. Models and Hyperparameters: We used two model types: the Transformer base model (six layers, eight heads, 512 model dimensions, and 2,048 FFN dimensions) and the Transformer big model (six layers, 16 heads, 1,024 model dimensions, and 4,096 FFN dimensions). Table 1 shows the details of the hyperparameters. All models were trained using almost the same settings, except for the learning rates and stopping criteria. In the test phase, we used a beam width of 10 for autoregressive decoding. For Mask-Predict,"
2020.wat-1.3,P02-1040,0,0.106409,"Missing"
2020.wat-1.3,P16-1162,0,0.0524334,"ds.3 Corpora: We used two corpora: the English– German (en-de) corpus of WMT-14 (4.5M sentences) (Bojar et al., 2014) and the Japanese– English (ja-en) corpus of ASPEC (3M sentences) (Nakazawa et al., 2016). To make the evaluation stable, we concatenated all test sets in the corpora, except for validation sets. That is, we used 19,666 sentences (newstest2010-2016) for the WMT-14 corpus and 3,596 sentences (devtest and test) for the ASPEC corpus, as the test sets. The newstest2009 set in WMT-14 and the dev set in ASPEC were used as the validation sets. All corpora were segmented into subwords (Sennrich et al., 2016). We used 37K shared vocabulary in WMT-14 and 16K vocabularies for the source and target languages in ASPEC. 1. Divide the sequence of tokens into left and right halves, reverse the right half, and alternately fold the halves. 2. Supply one or two EOD tokens, to make the total number of tokens even. The self-attention mask for training is a triangular matrix in which the unit is a pair of tokens (Figure 1(b)). 4 Experiments 4.1 Experimental Settings Systems: We modified the fairseq translation system (Ott et al., 2019)1 for the proposed method. For comparison, we considered the following three"
2021.wat-1.8,2020.acl-main.703,0,0.0355193,"ally pretrained the model on the ﬁve languages in the NICT-SAP task. The corpus for additional pretraining was extracted from Wikipedia dump ﬁles as follows. Unlike the XLM models (Lample and Conneau, 2019), which were also pretrained using Wikipedia corpora, we divided each article into sentences in our corpus, to train the sentence permutation task. Additionally, we applied sentence ﬁltering to clean each language. In this section, we brieﬂy review the pretrained mBART model (Liu et al., 2020). The mBART model is a multilingual model of bidirectional and auto-regressive Transformers (BART; (Lewis et al., 2020)). The model is based on the encoder-decoder Transformer (Vaswani et al., 2017), in which the decoder uses an autoregressive method (Figure 1). Two tasks of BART are trained in the mBART model. One is the token masking task, which restores masked tokens in input sentences. The other is the sentence permutation task, which predicts the original order of permuted sentences. Both tasks learn using monolingual corpora. To build multilingual models based on BART, mBART supplies language tags (as special tokens) at the tail of the encoder input and head of the decoder input. Using these language tag"
2021.wat-1.8,W19-6601,1,0.847731,"Missing"
2021.wat-1.8,2020.tacl-1.47,0,0.262755,"roduction In this paper, we present the NICT system (NICT2) that we submitted to the NICT-SAP shared task at the 8th Workshop on Asian Translation (WAT2021) (Nakazawa et al., 2021). Because the NICTSAP task expects to perform translations with little parallel data, we developed a system to improve translation quality by applying the following models and techniques. Pretrained model: An encoder-decoder model pretrained using huge monolingual corpora was used. We used a multilingual bidirectional auto-regressive Transformer (mBART) (i.e., multilingual sequence-to-sequence denoising autoencoder (Liu et al., 2020)) model, which supports 25 languages. Because it includes English and Hindi, but does not include Indonesian, Malay, and Thai, we expanded it to include the unsupported languages and additionally pretrained it on these ﬁve languages.1 2 NICT-SAP Shared Task The NICT-SAP shared task was to translate text between English and four languages, that is, Hindi (Hi), Indonesian (Id), Malay (Ms), and Thai (Th), for which the amount of data in parallel corpora is relatively low. The task contained two domains. The data in the Asian Language Translation (ALT) domain (Thu et al., 2016) consisted of transl"
2021.wat-1.8,N19-4009,0,0.0546528,"Missing"
2021.wat-1.8,P02-1040,0,0.11082,"Therefore, we created as many domain models as the number of domains. Other Options 5 Experiments We ﬁne-tuned the pretrained model using the NICT-SAP parallel corpora shown in Table 1. We also used Transformer base models (six layers, the model dimension of 512 on 8 heads) for comparison without the pretrained model. In addition to the effect of the pretrained models, we investigated the effects of multilingual models and domain adaptation. 4.2.1 The models and methods described above were ﬁne-tuned and tested using the hyperparameters in Table 4. Tables 5 and 6 show the ofﬁcial BLEU scores (Papineni et al., 2002) for the test set in the ALT and IT domains, respectively. Similar results were obtained on the development sets, but they were omitted in this paper. We submitted the results using the pretrained mBART model, which were good on the development sets, on average. The results are summarized as follows; Multilingual Models Similar to the multilingual training of mBART, the multilingual model translated all the language pairs using one model by supplying source and target language tags to parallel sentences. By contrast, bilingual models were trained using the corpora of each language pair. When w"
2021.wat-1.8,P16-1162,0,0.0180532,"ger than the target/source sentences if they had over 20 tokens. 3 #Sentences 7,000,000 (*1) 1,968,984 6,997,907 2,723,230 2,233,566 (*2) model can learn multiple languages. The published pretrained mBART model2 consists of a 12-layer encoder and decoder with a model dimension of 1,024 on 16 heads. This model was trained on 25 languages in the Common Crawl corpus (Wenzek et al., 2019). Of the languages for the NICT-SAP task, English and Hindi are supported by the published mBART model, but Indonesian, Malay, and Thai are not supported. The tokenizer for the mBART model uses bytepair encoding (Sennrich et al., 2016) of the SentencePiece model (Kudo and Richardson, 2018)3 . The vocabulary size is 250K subwords. 4 Our System 4.1 Language Expansion/Additional Pretraining of mBART mBART Model As described above, the published mBART model does not support Indonesian, Malay, and Thai. We expanded the mBART model to support these three languages, and additionally pretrained the model on the ﬁve languages in the NICT-SAP task. The corpus for additional pretraining was extracted from Wikipedia dump ﬁles as follows. Unlike the XLM models (Lample and Conneau, 2019), which were also pretrained using Wikipedia corpor"
2021.wat-1.8,L16-1249,1,0.829096,"oising autoencoder (Liu et al., 2020)) model, which supports 25 languages. Because it includes English and Hindi, but does not include Indonesian, Malay, and Thai, we expanded it to include the unsupported languages and additionally pretrained it on these ﬁve languages.1 2 NICT-SAP Shared Task The NICT-SAP shared task was to translate text between English and four languages, that is, Hindi (Hi), Indonesian (Id), Malay (Ms), and Thai (Th), for which the amount of data in parallel corpora is relatively low. The task contained two domains. The data in the Asian Language Translation (ALT) domain (Thu et al., 2016) consisted of translations obtained from WikiNews. The ALT is not supported by either the mBART model or mBART-50 models. Therefore, we applied additional pretraining to the mBART model. 1 The mBART-50 model (Tang et al., 2020) supports 50 languages including Indonesian and Thai. However, Malay 90 Proceedings of the 8th Workshop on Asian Translation, pages 90–95 Bangkok, Thailand (online), August 5-6, 2021. ©2021 Association for Computational Linguistics Domain ALT IT Set Train Dev Test Train Dev Test En-Hi 18,088 252,715 2,016 2,073 En-Id En-Ms 18,087 18,088 1,000 1,018 158,200 504,856 2,023"
2021.wat-1.8,tiedemann-2012-parallel,0,0.0561721,"aximum number of the other languages. (*2) Sentences in Thai were detected using an in-house sentence splitter. Table 1: Data sizes for the NICT-SAP task after ﬁltering. data is a multilingual parallel corpus, that is, it contains the same sentences in all languages. The training, development, and test sets were provided from the WAT organizers. The data in the IT domain consisted of translations of software documents. The WAT organizers provided the development and test sets (Buschbeck and Exel, 2020). For the training set, we obtained GNOME, KDE, and Ubuntu sub-corpora from the OPUS corpus (Tiedemann, 2012). Therefore, the domains for the training and dev/test sets were not identical. The data sizes are shown in Table 1. There were fewer than 20K training sentences in the ALT domain. Between 73K and 504K training sentences were in the IT domain. Note that there were inadequate sentences in the training sets. We ﬁltered out translations that were longer than 512 tokens, or where source/target sentences were three times longer than the target/source sentences if they had over 20 tokens. 3 #Sentences 7,000,000 (*1) 1,968,984 6,997,907 2,723,230 2,233,566 (*2) model can learn multiple languages. The"
C00-1083,J86-3001,0,0.103124,"Towns et al., 1998), planning is used in generating multimodal presentations including graphics and animations. They are similar to MID-3D in that they use planning mechanisms in content planning. However, in presentation systems, unlike dialogue systems, the user just watches the presentation without changing her/his view. Therefore, these studies are not concerned with changing the content of the discourse to match the user's view. In some studies of dialogue management (Rich and Sidner, 1998; Stent et al., 1999), the state of the dialogue is represented using Grosz and Sidner's framework (Grosz and Sidner, 1986). We also adopt this theory in our dialogue management mechanism. However, they do not keep track of the user's viewpoint information as a part of the dialogue state because they were not concerned with dialogue management in virtual environments. Studies on pedagogical agents have goals closer to ours. In (Rickel and Johnson, 1999), a pedagogical agent demonstrates the sequential operation of complex machinery and answers some follow up questions from the student. Lester et al. (1999) proposes a lifelike pedagogical agent that supports problemsolving activities. Although these studies are con"
C00-1083,J95-3007,0,0.0412249,"ent should aim to realize conversations which are performed in the real world. It would also be very useful for education, where it is necessary to learn in near real-life situations. One of the most signi cant characteristics of 3D virtual environments is that the user can select her/his own view from which to observe the virtual world. Thus, the multimodal instruction dialogue system should be able to set the course of the dialogue by considering the user's current view. However, previous works on multimodal presentation generation and instruction dialogue generation (Wahlster et al., 1993; Moore, 1995; Cawsey, 1992) do not achieve this goal because they were not designed to handle dialogues performed in 3D virtual environments. This paper proposes a method that ensures that the course of the dialogue matches the user's view in the virtual environment. More speci cally, we focus on (1) how to select the contents of the dialogue since it is essential that the instruction dialogue system form a sequence of dialogue contents that is coherent and comprehensible, and (2) how to control mixed-initiative instruction dialogues smoothly, especially how to manage interruptive subdialogues. These two"
C00-1083,P99-1024,0,0.024596,"of which use 3D graphics and 3D animations. In some of them (Maybury, 1993; Wahlster et al., 1993; Towns et al., 1998), planning is used in generating multimodal presentations including graphics and animations. They are similar to MID-3D in that they use planning mechanisms in content planning. However, in presentation systems, unlike dialogue systems, the user just watches the presentation without changing her/his view. Therefore, these studies are not concerned with changing the content of the discourse to match the user's view. In some studies of dialogue management (Rich and Sidner, 1998; Stent et al., 1999), the state of the dialogue is represented using Grosz and Sidner's framework (Grosz and Sidner, 1986). We also adopt this theory in our dialogue management mechanism. However, they do not keep track of the user's viewpoint information as a part of the dialogue state because they were not concerned with dialogue management in virtual environments. Studies on pedagogical agents have goals closer to ours. In (Rickel and Johnson, 1999), a pedagogical agent demonstrates the sequential operation of complex machinery and answers some follow up questions from the student. Lester et al. (1999) propose"
C04-1015,C02-1076,1,0.882262,"own et al., 1993) translates an input sentence by the combination of word transfer and word re-ordering. Therefore, when it is applied to a language pair in which the word order is quite different (e.g., English and Japanese, Figure 1), it becomes difficult to find a globally optimal solution due to the enormous search space (Watanabe and Sumita, 2003). Statistical MT could generate high-quality translations if it succeeded in finding a globally optimal solution. Therefore, the models employed by statistical MT are superior indicators of the quality of machine translation. Using this feature, Akiba et al. (2002) achieved selection of the best translation among those output by multiple MT engines. This paper presents an example-based MT method based on syntactic transfer, which selects the best translation by using models of statistical MT. This method is roughly structured using two modules (Figure 2). One is an example-based syntactic transfer module. This module constructs Transfer Rules Example-based Syntactic Transfer Statistical Generation Thesaurus Translation Dictionary Preprocessing Postprocessing Input Sentence Output Sentence Translation Model Language Model Figure 2: Structure of Proposed"
C04-1015,J93-2003,0,0.00358261,"g case relations or idiomatic expressions. However, when some examples conflict during reE = NULL0 show1 J= A= ( me2 the3 one4 in5 the6 window7 uindo1 no2 shinamono3 o4 mise5 telidasai6 7 0 4 0 1 1 ) Figure 1: Example of Word Alignment between English and Japanese (Watanabe and Sumita, 2003) trieval, example-based MT selects the best example scored by the similarity between the input and the source part of the example. This implies that example-based MT does not check whether the translation of the given input sentence is correct or not. On the other hand, statistical MT employing IBM models (Brown et al., 1993) translates an input sentence by the combination of word transfer and word re-ordering. Therefore, when it is applied to a language pair in which the word order is quite different (e.g., English and Japanese, Figure 1), it becomes difficult to find a globally optimal solution due to the enormous search space (Watanabe and Sumita, 2003). Statistical MT could generate high-quality translations if it succeeded in finding a globally optimal solution. Therefore, the models employed by statistical MT are superior indicators of the quality of machine translation. Using this feature, Akiba et al. (200"
C04-1015,P01-1030,0,0.0451743,"shared nodes of the target tree, so it can improve translation speed. Therefore, bottom-up generation is suitable for tasks that require real-time processing, such as spoken dialogue translation. 5 Discussion We incorporated example-based MT in models of statistical MT. However, some methods to obtain initial solutions of statistical MT by examplebased MT have already been proposed. For example, Marcu (2001) proposed a method in which initial translations are constructed by combining bilingual phrases from translation memory, which is followed by modifying the translations by greedy decoding (Germann et al., 2001). Watanabe and Sumita (2003) proposed a decoding algorithm in which translations that are similar to the input sentence are retrieved from bilingual corpora and then modified by greedy decoding. The difference between our method and these methods involves whether modification is applied. Our approach simply selects the best translation from candidates that are output from examplebased MT. Even though example-based MT can output appropriate translations to some degree, our method assumes that the candidates contain a globally optimal solution. This means that the upper bound of MT quality is li"
C04-1015,P03-1057,1,0.874225,"ds involves whether modification is applied. Our approach simply selects the best translation from candidates that are output from examplebased MT. Even though example-based MT can output appropriate translations to some degree, our method assumes that the candidates contain a globally optimal solution. This means that the upper bound of MT quality is limited by the example-based transfer, so we have to improve this stage in order to further improve MT quality. For instance, example-based MT can be improved by applying an optimization algorithm that uses an automatic evaluation of MT quality (Imamura et al., 2003). 6 Conclusions This paper demonstrated that example-based MT can be improved by incorporating it in models of statistical MT. The example-based MT used in this paper is based on syntactic transfer, so word reordering is achieved in the transfer module. Using this feature, the best translation was selected by using only a lexicon model and an n-gram language model. In addition, bottom-up generation achieved faster translation speed by using the tree structure of the target sentence. Acknowledgements The authors would like to thank Kadokawa Publishers, who permitted us to use the hierarchy of R"
C04-1015,2002.tmi-papers.9,1,0.88153,"because the example-based transfer generates syntactically correct candidates for the most appropriate translation. The rest of this paper is organized as follows: Section 2 describes the example-based syntactic transfer, Section 3 describes the statistical generation, Section 4 evaluates an experimental system that uses this method, and Section 5 compares other hybrid methods of example-based and statistical MT. 2 Example-based Syntactic Transfer The example-based syntactic transfer used in this paper is a revised version of the Hierarchical Phrase Alignment-based Translator (HPAT, refer to (Imamura, 2002)). This section gives an overview with an example of Japanese-to-English machine translation. 2.1 Transfer Rules Transfer rules are automatically acquired from bilingual corpora by using hierarchical phrase alignment (HPA; (Imamura, 2001)). HPA parses bilingual sentences and acquires corresponding syntactic nodes of the source and target sentences. The transfer rules are created from their node correspondences. Figure 3 shows an example of the transfer rules. Variables, such as X and Y in Figure 3, denote non-terminal symbols that correspond between source and target grammar. The set of transf"
C04-1015,P01-1050,0,0.0174084,"essary for improving MT quality. Finally, focusing on translation speed, the worst time for Bottom-up generation was dramatically faster than that for All Search. Bottom-up generation effectively uses shared nodes of the target tree, so it can improve translation speed. Therefore, bottom-up generation is suitable for tasks that require real-time processing, such as spoken dialogue translation. 5 Discussion We incorporated example-based MT in models of statistical MT. However, some methods to obtain initial solutions of statistical MT by examplebased MT have already been proposed. For example, Marcu (2001) proposed a method in which initial translations are constructed by combining bilingual phrases from translation memory, which is followed by modifying the translations by greedy decoding (Germann et al., 2001). Watanabe and Sumita (2003) proposed a decoding algorithm in which translations that are similar to the input sentence are retrieved from bilingual corpora and then modified by greedy decoding. The difference between our method and these methods involves whether modification is applied. Our approach simply selects the best translation from candidates that are output from examplebased MT"
C04-1015,J03-1002,0,0.00490061,"sic Travel Expression Corpus (Takezawa et al., 2002; Kikui et al., 2003). This is a collection of Japanese sentences and their English translations based on expressions that are usually found in phrasebooks for foreign tourists. We divided it into subsets for training and testing as shown in Table 1. Transfer Rules Transfer rules were acquired from the training set using hierarchical phrase alignment, and low-frequency rules that appeared less than twice were removed. The number of rules was 24,310. Translation Model and Language Model We used a lexicon model of IBM Model 4 learned by GIZA++ (Och and Ney, 2003) and word bigram and trigram models learned by CMU-Cambridge Statistical Language Modeling Toolkit (Clarkson and Rosenfeld, 1997). Compared Methods We compared the following four methods. • Baseline (Example-based Transfer only) The best translation that had the same semantic distance was randomly selected from the S &lt;s&gt; bus will leave at 11 o’clock TM: -7.13 LM: -14.30 bus will start at 11 o’clock TM: -8.03 LM: -13.84 &lt;/s&gt; the bus will leave at 11 o’clock TM: -7.13 LM: -13.54 XNP bus TM: -0.07 LM: -0.0 the bus TM: -0.07 LM: -1.94 a bus TM: -0.07 LM: -2.11 n-best YVP leaves at 11 o’clock TM: -"
C04-1015,P02-1040,0,0.0702122,"Missing"
C04-1015,P91-1024,1,0.748447,"ock NP -&gt; X6 11 11 Figure 4: Example of Syntactic Transfer Process (Bold frames are syntactic nodes mentioned in text) that do not correspond between the source and target sentences (e.g., the determiner ‘a’ or ‘the’) are automatically inserted or eliminated by the target grammar (cf. NP node represented by a bold frame). Namely, transfer rules work in a manner similar to the functions of distortion, fertility, and NULL in IBM models. 2.3 Usage of Source Examples Example-based transfer utilizes the source examples for disambiguation of mapping and parsing. Specifically, the semantic distance (Sumita and Iida, 1991) is calculated between the source examples and the headwords of the input sentence, and the transfer rules that contain the nearest example are used to construct the target tree structure. The semantic distance between words is defined as the distance from the leaf node to the most specific common abstraction (MSCA) in a thesaurus (Ohno and Hamanishi, 1984). For example, if the input phrase “ie (home) ni kaeru (return)” is given, Rules 1 to 3 in Figure 3 are used for the syntactic transfer, and three target nodes are generated without any disambiguation. However, when we compare the source exa"
C04-1015,takezawa-etal-2002-toward,1,0.73883,"sentence and end-of-sentence, and the n-best list is re-sorted. As a result, the translation “The bus will leave at 11 o’clock” is obtained from the tree of Figure 4. Bottom-up generation calculates the probabilities of shared nodes only once, so it effectively uses tree information. 4 Evaluation In order to evaluate the effect when models of statistical MT are integrated into example-based MT, we compared various methods that changed the statistical generation module. 4.1 Experimental Setting Bilingual Corpus The corpus used in the following experiments is the Basic Travel Expression Corpus (Takezawa et al., 2002; Kikui et al., 2003). This is a collection of Japanese sentences and their English translations based on expressions that are usually found in phrasebooks for foreign tourists. We divided it into subsets for training and testing as shown in Table 1. Transfer Rules Transfer rules were acquired from the training set using hierarchical phrase alignment, and low-frequency rules that appeared less than twice were removed. The number of rules was 24,310. Translation Model and Language Model We used a lexicon model of IBM Model 4 learned by GIZA++ (Och and Ney, 2003) and word bigram and trigram mode"
C04-1015,2003.mtsummit-papers.53,0,0.0163217,"is determined from the product of the translation model and the language model in the same manner as statistical MT. In other words, when F and E denote the channel target and channel source sequence, ˆ that satrespectively, the output word sequence E isfies the following equation is searched for. Set Name Training Test ˆ = argmax P (E|F ) E Item # of Sentences # of Words # of Sentences # of Words English Japanese 152,170 886,708 1,007,484 510 2,973 3,340 E = argmax P (E)P (F |E). E (1) We only utilize the lexicon model as the translation model in this paper, similar to the models proposed by Vogel et al. (2003). Namely, when f and e denote the channel target and channel source word, respectively, the translation probability is computed by the following equation. P (F |E) =  j t(fj |ei ). (2) i The IBM models include other models, such as fertility, NULL, and distortion models. As we described in Section 2.2, the quality of machine translation is maintained using only the lexicon model because syntactical correctness is already preserved by example-based transfer. For the language model, we utilize a standard word n-gram model. 3.2 Bottom-up Generation We can construct word graphs by serializing th"
C04-1015,2003.mtsummit-papers.54,1,0.938164,"ngual corpus as a database and retrieves examples that are similar to an input sentence. Then, a translation is generated by modifying the target part of the examples while referring to translation dictionaries. Most example-based MT systems employ phrases or sentences as the unit for examples, so they can translate while considering case relations or idiomatic expressions. However, when some examples conflict during reE = NULL0 show1 J= A= ( me2 the3 one4 in5 the6 window7 uindo1 no2 shinamono3 o4 mise5 telidasai6 7 0 4 0 1 1 ) Figure 1: Example of Word Alignment between English and Japanese (Watanabe and Sumita, 2003) trieval, example-based MT selects the best example scored by the similarity between the input and the source part of the example. This implies that example-based MT does not check whether the translation of the given input sentence is correct or not. On the other hand, statistical MT employing IBM models (Brown et al., 1993) translates an input sentence by the combination of word transfer and word re-ordering. Therefore, when it is applied to a language pair in which the word order is quite different (e.g., English and Japanese, Figure 1), it becomes difficult to find a globally optimal solut"
C14-1077,W04-2412,0,0.0257716,"Missing"
C14-1077,W05-0620,0,0.0339116,"Missing"
C14-1077,P07-1033,0,0.0380565,"Missing"
C14-1077,J12-4003,0,0.0161288,"Journal), the shared tasks were evaluated on newspaper articles. M`arquez et al. (2008) provides a review of SRL. OntoNotes Corpus (Hovy et al., 2006) contains multiple genres such as newswire, broadcast news, broadcast conversation. The annotation to OntoNotes includes semantic role labels compliant with the Proposition Bank. It is currently used for coreference resolution (Pradhan et al., 2012), and is expected to be applied to dialogue analysis. A few SRL studies have focused on not only verbal predicates (e.g., ‘decide’) but also nominal predicates (e.g., ‘decision’) (Jiang and Ng, 2006; Gerber and Chai, 2012; Laparra and Rigau, 2013). Because the subject and object of nominal predicates are frequently omitted (e.g., the object in the phrase “the decision” is omitted), problems similar to the Japanese zero-pronouns have to be resolved in the SRL of nominal predicates. 2.2 Predicate-Argument Structure Analyses in Japanese Japanese material includes the NAIST Text Corpus (Iida et al., 2007)2 , which is an annotated corpus of predicate-argument structures and coreference information for newspaper articles. Argument noun phrases of the nominative, accusative, and dative cases are assigned to each pred"
C14-1077,J02-3001,0,0.521269,"owledge, this is the first paper to describe a PASA for dialogues that include many zero-pronouns. The paper is organized as follows. Section 2 briefly reviews SRL/PASA in English and Japanese. Section 3 discusses characteristics of chat dialogues by comparing two annotated corpora, newspaper articles and dialogues. Section 4 describes the basic strategy of our PASA, and Section 5 shows how it was adapted for dialogues. Experiments are presented in Section 6, and Section 7 concludes the paper. 2 Related Work 2.1 Semantic Role Labeling in English The advent of the supervised method proposed by Gildea and Jurafsky (2002) has led to the creation of annotated corpora for semantic role labeling. In the CoNLL-2004 and 2005 shared task (Carreras and M`arquez, 2004; Carreras and M`arquez, 2005), evaluations were carried out using the Proposition Bank (Palmer et al., 2005). Because the Proposition Bank was annotated to the Penn Treebank (i.e., the source texts were from the Wall Street Journal), the shared tasks were evaluated on newspaper articles. M`arquez et al. (2008) provides a review of SRL. OntoNotes Corpus (Hovy et al., 2006) contains multiple genres such as newswire, broadcast news, broadcast conversation."
C14-1077,C14-1088,1,0.87563,"Missing"
C14-1077,N06-2015,0,0.0227242,"ic Role Labeling in English The advent of the supervised method proposed by Gildea and Jurafsky (2002) has led to the creation of annotated corpora for semantic role labeling. In the CoNLL-2004 and 2005 shared task (Carreras and M`arquez, 2004; Carreras and M`arquez, 2005), evaluations were carried out using the Proposition Bank (Palmer et al., 2005). Because the Proposition Bank was annotated to the Penn Treebank (i.e., the source texts were from the Wall Street Journal), the shared tasks were evaluated on newspaper articles. M`arquez et al. (2008) provides a review of SRL. OntoNotes Corpus (Hovy et al., 2006) contains multiple genres such as newswire, broadcast news, broadcast conversation. The annotation to OntoNotes includes semantic role labels compliant with the Proposition Bank. It is currently used for coreference resolution (Pradhan et al., 2012), and is expected to be applied to dialogue analysis. A few SRL studies have focused on not only verbal predicates (e.g., ‘decide’) but also nominal predicates (e.g., ‘decision’) (Jiang and Ng, 2006; Gerber and Chai, 2012; Laparra and Rigau, 2013). Because the subject and object of nominal predicates are frequently omitted (e.g., the object in the p"
C14-1077,W07-1522,0,0.0293426,"al., 2012), and is expected to be applied to dialogue analysis. A few SRL studies have focused on not only verbal predicates (e.g., ‘decide’) but also nominal predicates (e.g., ‘decision’) (Jiang and Ng, 2006; Gerber and Chai, 2012; Laparra and Rigau, 2013). Because the subject and object of nominal predicates are frequently omitted (e.g., the object in the phrase “the decision” is omitted), problems similar to the Japanese zero-pronouns have to be resolved in the SRL of nominal predicates. 2.2 Predicate-Argument Structure Analyses in Japanese Japanese material includes the NAIST Text Corpus (Iida et al., 2007)2 , which is an annotated corpus of predicate-argument structures and coreference information for newspaper articles. Argument noun phrases of the nominative, accusative, and dative cases are assigned to each predicate. The predicate and the noun phrases are not limited to the same sentence. If arguments of the predicate are represented as zero-pronouns, the antecedent noun phrases in other sentences are assigned as the arguments. Many PASA methods have been studied on the NAIST Text Corpus (Komachi et al., 2007; Taira et al., 2008; Imamura et al., 2009; Yoshikawa et al., 2011). In Japanese, s"
C14-1077,P09-2022,1,0.942058,"e material includes the NAIST Text Corpus (Iida et al., 2007)2 , which is an annotated corpus of predicate-argument structures and coreference information for newspaper articles. Argument noun phrases of the nominative, accusative, and dative cases are assigned to each predicate. The predicate and the noun phrases are not limited to the same sentence. If arguments of the predicate are represented as zero-pronouns, the antecedent noun phrases in other sentences are assigned as the arguments. Many PASA methods have been studied on the NAIST Text Corpus (Komachi et al., 2007; Taira et al., 2008; Imamura et al., 2009; Yoshikawa et al., 2011). In Japanese, some of them simultaneously resolve the zero-anaphora caused by zero-pronouns. Most English SRL and Japanese PASA currently target newspaper articles, and it is unclear whether the methods for newspapers can be applied to dialogue conversations. 3 Characteristics of Chat Dialogues We first collected chat dialogues of two speakers and annotated them with the predicate-argument structure. The participants chatted via keyboard input. Therefore, fillers and repetitions, which are frequent in speech dialogues, were rare. The theme was one of 20 topics, such a"
C14-1077,W06-1617,0,0.0269758,"from the Wall Street Journal), the shared tasks were evaluated on newspaper articles. M`arquez et al. (2008) provides a review of SRL. OntoNotes Corpus (Hovy et al., 2006) contains multiple genres such as newswire, broadcast news, broadcast conversation. The annotation to OntoNotes includes semantic role labels compliant with the Proposition Bank. It is currently used for coreference resolution (Pradhan et al., 2012), and is expected to be applied to dialogue analysis. A few SRL studies have focused on not only verbal predicates (e.g., ‘decide’) but also nominal predicates (e.g., ‘decision’) (Jiang and Ng, 2006; Gerber and Chai, 2012; Laparra and Rigau, 2013). Because the subject and object of nominal predicates are frequently omitted (e.g., the object in the phrase “the decision” is omitted), problems similar to the Japanese zero-pronouns have to be resolved in the SRL of nominal predicates. 2.2 Predicate-Argument Structure Analyses in Japanese Japanese material includes the NAIST Text Corpus (Iida et al., 2007)2 , which is an annotated corpus of predicate-argument structures and coreference information for newspaper articles. Argument noun phrases of the nominative, accusative, and dative cases ar"
C14-1077,C02-1122,0,0.0526739,"the weights in the respective space, source or target, are emphasized. 4. When the argument is identified, the selectors use only the features in the common and target spaces. The parameters in the spaces are optimized to the target domain, plus we can utilize the features that appear only in the source domain data. 5.2 Weak Knowledge Acquisition from Very Large Resources In this paper, we use two types of knowledge to reduce the harmful effect of out-of-vocabulary in the training corpus. Both types are constructed by automatically analyzing, summing up, and filtering large 811 text corpora (Kawahara and Kurohashi, 2002; Sasano et al., 2008; Sasano et al., 2013). They provide information about unknown words with some confidence but they do contain some errors. We use them as the features of the models, and parameters are optimized by the discriminative learning of the selectors. 5.2.1 Obligatory Case Information (Frame Feature) Case frames are important clues for SRL and PASA. The obligatory case information (OCI) comprises subsets of the case frames that only clarify whether the cases of each predicate are necessary or not. The OCI dictionary is automatically constructed from large text corpora as follows."
C14-1077,W02-2016,0,0.0852998,"aper articles as the base PASA in this paper. It can simultaneously identify arguments of a predicate in the sentence, those in other sentences, and exophoric arguments. The analyzer receives the entire article (dialogue) and performs the following steps for each sentence (utterance). 1. The input sentences are tagged and parsed. During parsing, the base phrases and their headwords are also identified. At this time, the part-of-speech tags and the parse trees of the Dialogue Corpus are supplied by applying the morphological analyzer MeCab (Kudo et al., 2004) and the dependency parser CaboCha (Kudo and Matsumoto, 2002). The NAIST Corpus version 1.5 already includes the part-of-speech tags and the parse trees. 2. Predicate phrases are identified from the sentences. We use the correct predicates in the corpora for the evaluation. When we build dialogue systems on PASA, predicate phrases will be identified using part-of-speech patterns that include verbs, adjectives, and copular verbs. 3. For each predicate, candidate arguments are acquired from the sentence that includes the predicate (called the current sentence) and the past sentences. Concretely, the following base phrases are regarded as candidates. • All"
C14-1077,W04-3230,0,0.0634939,"use Imamura et al. (2009)’s method developed for newspaper articles as the base PASA in this paper. It can simultaneously identify arguments of a predicate in the sentence, those in other sentences, and exophoric arguments. The analyzer receives the entire article (dialogue) and performs the following steps for each sentence (utterance). 1. The input sentences are tagged and parsed. During parsing, the base phrases and their headwords are also identified. At this time, the part-of-speech tags and the parse trees of the Dialogue Corpus are supplied by applying the morphological analyzer MeCab (Kudo et al., 2004) and the dependency parser CaboCha (Kudo and Matsumoto, 2002). The NAIST Corpus version 1.5 already includes the part-of-speech tags and the parse trees. 2. Predicate phrases are identified from the sentences. We use the correct predicates in the corpora for the evaluation. When we build dialogue systems on PASA, predicate phrases will be identified using part-of-speech patterns that include verbs, adjectives, and copular verbs. 3. For each predicate, candidate arguments are acquired from the sentence that includes the predicate (called the current sentence) and the past sentences. Concretely,"
C14-1077,P13-1116,0,0.012307,"asks were evaluated on newspaper articles. M`arquez et al. (2008) provides a review of SRL. OntoNotes Corpus (Hovy et al., 2006) contains multiple genres such as newswire, broadcast news, broadcast conversation. The annotation to OntoNotes includes semantic role labels compliant with the Proposition Bank. It is currently used for coreference resolution (Pradhan et al., 2012), and is expected to be applied to dialogue analysis. A few SRL studies have focused on not only verbal predicates (e.g., ‘decide’) but also nominal predicates (e.g., ‘decision’) (Jiang and Ng, 2006; Gerber and Chai, 2012; Laparra and Rigau, 2013). Because the subject and object of nominal predicates are frequently omitted (e.g., the object in the phrase “the decision” is omitted), problems similar to the Japanese zero-pronouns have to be resolved in the SRL of nominal predicates. 2.2 Predicate-Argument Structure Analyses in Japanese Japanese material includes the NAIST Text Corpus (Iida et al., 2007)2 , which is an annotated corpus of predicate-argument structures and coreference information for newspaper articles. Argument noun phrases of the nominative, accusative, and dative cases are assigned to each predicate. The predicate and t"
C14-1077,J08-2001,0,0.0260886,"Missing"
C14-1077,J05-1004,0,0.0124007,"ring two annotated corpora, newspaper articles and dialogues. Section 4 describes the basic strategy of our PASA, and Section 5 shows how it was adapted for dialogues. Experiments are presented in Section 6, and Section 7 concludes the paper. 2 Related Work 2.1 Semantic Role Labeling in English The advent of the supervised method proposed by Gildea and Jurafsky (2002) has led to the creation of annotated corpora for semantic role labeling. In the CoNLL-2004 and 2005 shared task (Carreras and M`arquez, 2004; Carreras and M`arquez, 2005), evaluations were carried out using the Proposition Bank (Palmer et al., 2005). Because the Proposition Bank was annotated to the Penn Treebank (i.e., the source texts were from the Wall Street Journal), the shared tasks were evaluated on newspaper articles. M`arquez et al. (2008) provides a review of SRL. OntoNotes Corpus (Hovy et al., 2006) contains multiple genres such as newswire, broadcast news, broadcast conversation. The annotation to OntoNotes includes semantic role labels compliant with the Proposition Bank. It is currently used for coreference resolution (Pradhan et al., 2012), and is expected to be applied to dialogue analysis. A few SRL studies have focused"
C14-1077,J08-2006,0,0.0414033,"equently, SRL and PASA are very similar tasks. We use the term predicate-argument structure analysis in this paper because most Japanese analyzers use this term. 806 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 806–815, Dublin, Ireland, August 23-29 2014. This paper presents predicate-argument structure analysis with zero-anaphora resolution for Japanese chat dialogues. Here, we regard the task of constructing PASA for dialogues as a kind of domain adaptation from newspaper articles to dialogues. M`arquez et al. (2008) and Pradhan et al. (2008) indicated that the tuning of parameter distribution and reducing the out-of-vocabulary are important for the domain adaptation of SRL. We also focus on parameter distribution and out-of-vocabulary to construct a PASA adapted to dialogues. To the best of our knowledge, this is the first paper to describe a PASA for dialogues that include many zero-pronouns. The paper is organized as follows. Section 2 briefly reviews SRL/PASA in English and Japanese. Section 3 discusses characteristics of chat dialogues by comparing two annotated corpora, newspaper articles and dialogues. Section 4 describes t"
C14-1077,C08-1097,0,0.024882,"space, source or target, are emphasized. 4. When the argument is identified, the selectors use only the features in the common and target spaces. The parameters in the spaces are optimized to the target domain, plus we can utilize the features that appear only in the source domain data. 5.2 Weak Knowledge Acquisition from Very Large Resources In this paper, we use two types of knowledge to reduce the harmful effect of out-of-vocabulary in the training corpus. Both types are constructed by automatically analyzing, summing up, and filtering large 811 text corpora (Kawahara and Kurohashi, 2002; Sasano et al., 2008; Sasano et al., 2013). They provide information about unknown words with some confidence but they do contain some errors. We use them as the features of the models, and parameters are optimized by the discriminative learning of the selectors. 5.2.1 Obligatory Case Information (Frame Feature) Case frames are important clues for SRL and PASA. The obligatory case information (OCI) comprises subsets of the case frames that only clarify whether the cases of each predicate are necessary or not. The OCI dictionary is automatically constructed from large text corpora as follows. The process assumes t"
C14-1077,D13-1121,0,0.0119466,"get, are emphasized. 4. When the argument is identified, the selectors use only the features in the common and target spaces. The parameters in the spaces are optimized to the target domain, plus we can utilize the features that appear only in the source domain data. 5.2 Weak Knowledge Acquisition from Very Large Resources In this paper, we use two types of knowledge to reduce the harmful effect of out-of-vocabulary in the training corpus. Both types are constructed by automatically analyzing, summing up, and filtering large 811 text corpora (Kawahara and Kurohashi, 2002; Sasano et al., 2008; Sasano et al., 2013). They provide information about unknown words with some confidence but they do contain some errors. We use them as the features of the models, and parameters are optimized by the discriminative learning of the selectors. 5.2.1 Obligatory Case Information (Frame Feature) Case frames are important clues for SRL and PASA. The obligatory case information (OCI) comprises subsets of the case frames that only clarify whether the cases of each predicate are necessary or not. The OCI dictionary is automatically constructed from large text corpora as follows. The process assumes that 1) most of the cas"
C14-1077,D08-1055,0,0.0205172,"in Japanese Japanese material includes the NAIST Text Corpus (Iida et al., 2007)2 , which is an annotated corpus of predicate-argument structures and coreference information for newspaper articles. Argument noun phrases of the nominative, accusative, and dative cases are assigned to each predicate. The predicate and the noun phrases are not limited to the same sentence. If arguments of the predicate are represented as zero-pronouns, the antecedent noun phrases in other sentences are assigned as the arguments. Many PASA methods have been studied on the NAIST Text Corpus (Komachi et al., 2007; Taira et al., 2008; Imamura et al., 2009; Yoshikawa et al., 2011). In Japanese, some of them simultaneously resolve the zero-anaphora caused by zero-pronouns. Most English SRL and Japanese PASA currently target newspaper articles, and it is unclear whether the methods for newspapers can be applied to dialogue conversations. 3 Characteristics of Chat Dialogues We first collected chat dialogues of two speakers and annotated them with the predicate-argument structure. The participants chatted via keyboard input. Therefore, fillers and repetitions, which are frequent in speech dialogues, were rare. The theme was on"
C14-1077,I11-1126,0,0.0128023,"e NAIST Text Corpus (Iida et al., 2007)2 , which is an annotated corpus of predicate-argument structures and coreference information for newspaper articles. Argument noun phrases of the nominative, accusative, and dative cases are assigned to each predicate. The predicate and the noun phrases are not limited to the same sentence. If arguments of the predicate are represented as zero-pronouns, the antecedent noun phrases in other sentences are assigned as the arguments. Many PASA methods have been studied on the NAIST Text Corpus (Komachi et al., 2007; Taira et al., 2008; Imamura et al., 2009; Yoshikawa et al., 2011). In Japanese, some of them simultaneously resolve the zero-anaphora caused by zero-pronouns. Most English SRL and Japanese PASA currently target newspaper articles, and it is unclear whether the methods for newspapers can be applied to dialogue conversations. 3 Characteristics of Chat Dialogues We first collected chat dialogues of two speakers and annotated them with the predicate-argument structure. The participants chatted via keyboard input. Therefore, fillers and repetitions, which are frequent in speech dialogues, were rare. The theme was one of 20 topics, such as meals, travel, hobbies,"
C14-1077,N07-1070,0,\N,Missing
C14-1088,J08-1001,0,0.0105208,"his is probably the cause. The reason could be the inconsistency of linguistic styles in Twitter or the noise that could not be suppressed by the ﬁltering. Since Twitter sentences surely augment diversity, we would like to consider ways to make better use of them, for example, by normalizing the linguistic style and applying stricter ﬁlters. There is a slight tendency for Prop (tri) to be preferred to Prop (bi), which is reasonable because it uses more context for deciding the next utterance. In the future, we would like to pursue methods that can exploit longer context, such as entity grids (Barzilay and Lapata, 2008) and co-reference structures (Swanson and Gordon, 2012). We performed a brief analysis of the collected dialogues. Table 2 shows, for each system, the number of unique utterances, unique words, utterances, words, words per utterance, and perplexity. It can be seen that the utterances of the rule-based system are very rigid: the perplexity is very low (23.46) and there are only 353 unique utterances, which is about half of that of the other systems. It is interesting that, despite this fact, the rule-based system was perceived to produce the most diverse utterances by questionnaire. Since the r"
C14-1088,W12-1631,0,0.0178617,"es. Experimental results using human subjects show that our architecture achieves signiﬁcantly better naturalness than a retrieval-based baseline and that its naturalness is close to that of a rule-based system using 149K hand-crafted rules. 1 Introduction Although task-oriented dialogue systems have been extensively researched over the decades (Walker et al., 2001; Williams et al., 2013), it is only recently that non-task-oriented dialogue, open-domain conversation, or chat has been attracting attention for its social and entertainment aspects (Bickmore and Picard, 2005; Ritter et al., 2011; Bessho et al., 2012). Creating an open-domain conversational system is a challenging problem. In task-oriented dialogue systems, it is possible to prepare knowledge for a domain and create understanding and generation modules for that domain (Nakano et al., 2000). However, for open-domain conversation, such preparation cannot be performed. Since it is difﬁcult to handle users’ open-domain utterances, to create workable systems, conventional approaches have used hand-crafted rules (Wallace, 2004). Although elaborate rules may work well, the problem with the rulebased approach is the high cost and the dependence on"
C14-1088,P98-1068,0,0.0459333,"Missing"
C14-1088,J86-3001,0,0.216438,"t and extendable with other modalities. In Section 2, we describe the architecture and its underlying modules. In Section 3, we describe the rule-based and retrieval-based systems that we use for comparison. In Section 4, we describe the experiment we performed to evaluate our system. Section 5 summarizes the paper. 2 Architecture and System Description Figure 1 shows the architecture we propose for an open-domain conversational system. The architecture has three main components: utterance understanding, dialogue control, and utterance generation. Following the literature on discourse theory (Grosz and Sidner, 1986), we regard intention (intentional structure), topic (attention state), and content (linguistic structure) as three important elements in conversation, and seek to create a system that can understand and generate on the basis of them in a general way. The dialogue control component works by ranking utterance candidates using a general coherence criterion (Hovy, 1991). Note that the overall architecture is roughly the same as conventional dialogue systems; however, the internal architecture is different so as to allow open-domain conversation. To give a rough idea of how the system works, Figur"
C14-1088,C92-2082,0,0.0734472,"Missing"
C14-1088,D08-1040,0,0.0676352,"Missing"
C14-1088,P09-2022,1,0.290908,"Missing"
C14-1088,C14-1077,1,0.84913,"Missing"
C14-1088,P03-1069,0,0.029232,"Missing"
C14-1088,W00-1017,0,0.0838856,"Introduction Although task-oriented dialogue systems have been extensively researched over the decades (Walker et al., 2001; Williams et al., 2013), it is only recently that non-task-oriented dialogue, open-domain conversation, or chat has been attracting attention for its social and entertainment aspects (Bickmore and Picard, 2005; Ritter et al., 2011; Bessho et al., 2012). Creating an open-domain conversational system is a challenging problem. In task-oriented dialogue systems, it is possible to prepare knowledge for a domain and create understanding and generation modules for that domain (Nakano et al., 2000). However, for open-domain conversation, such preparation cannot be performed. Since it is difﬁcult to handle users’ open-domain utterances, to create workable systems, conventional approaches have used hand-crafted rules (Wallace, 2004). Although elaborate rules may work well, the problem with the rulebased approach is the high cost and the dependence on individual skills of developers, which hinders systematic development. Another problem with the rule-based approach is its low coverage; that is, the inability to handle unexpected utterances. The recent increase of web data has propelled the"
C14-1088,P08-2043,0,0.00712674,"ones by a cut-off threshold of ten occurrences, we obtained 22K utterance pairs. The input to this module is the user utterance string, and the module outputs utterances from matched utterance pairs. User PAS: This module uses the PASs of the user utterance and the next dialogue-act. It performs the same operation as the PAS-based generation and returns the converted sentences. The merit of this module is that the system can use the user’s content in its utterance, which has been found to be useful in casual conversation for showing understanding (Ivey et al., 2013) and entraining with users (Nenkova et al., 2008). 3 Rule-based and Retrieval-based Systems For comparison, we prepared a rule-based system and a retrieval-based one. Since there is no offthe-shelf rule-based system in Japanese, we created one on our own. Because we wanted to compare our system with a state-of-the-art rule-based system, we put a great deal of effort in its development. Remember that creating rules is still the standard way of creating an open-domain conversational system. Last year’s Loebner Prize (a chatbot contest) winner, Mitsuku, was based on rules written in artiﬁcial intelligence markup language (AIML) (Wallace, 2004)."
C14-1088,D11-1054,0,0.150994,"e processing techniques. Experimental results using human subjects show that our architecture achieves signiﬁcantly better naturalness than a retrieval-based baseline and that its naturalness is close to that of a rule-based system using 149K hand-crafted rules. 1 Introduction Although task-oriented dialogue systems have been extensively researched over the decades (Walker et al., 2001; Williams et al., 2013), it is only recently that non-task-oriented dialogue, open-domain conversation, or chat has been attracting attention for its social and entertainment aspects (Bickmore and Picard, 2005; Ritter et al., 2011; Bessho et al., 2012). Creating an open-domain conversational system is a challenging problem. In task-oriented dialogue systems, it is possible to prepare knowledge for a domain and create understanding and generation modules for that domain (Nakano et al., 2000). However, for open-domain conversation, such preparation cannot be performed. Since it is difﬁcult to handle users’ open-domain utterances, to create workable systems, conventional approaches have used hand-crafted rules (Wallace, 2004). Although elaborate rules may work well, the problem with the rulebased approach is the high cost"
C14-1088,W13-4051,1,0.434919,"Missing"
C14-1088,voorhees-tice-2000-trec,0,0.241798,"Missing"
C14-1088,P01-1066,0,0.130789,"suo.yoshihiro}@lab.ntt.co.jp Abstract This paper proposes an architecture for an open-domain conversational system and evaluates an implemented system. The proposed architecture is fully composed of modules based on natural language processing techniques. Experimental results using human subjects show that our architecture achieves signiﬁcantly better naturalness than a retrieval-based baseline and that its naturalness is close to that of a rule-based system using 149K hand-crafted rules. 1 Introduction Although task-oriented dialogue systems have been extensively researched over the decades (Walker et al., 2001; Williams et al., 2013), it is only recently that non-task-oriented dialogue, open-domain conversation, or chat has been attracting attention for its social and entertainment aspects (Bickmore and Picard, 2005; Ritter et al., 2011; Bessho et al., 2012). Creating an open-domain conversational system is a challenging problem. In task-oriented dialogue systems, it is possible to prepare knowledge for a domain and create understanding and generation modules for that domain (Nakano et al., 2000). However, for open-domain conversation, such preparation cannot be performed. Since it is difﬁcult to h"
C14-1088,W13-4065,0,0.00506369,"t.co.jp Abstract This paper proposes an architecture for an open-domain conversational system and evaluates an implemented system. The proposed architecture is fully composed of modules based on natural language processing techniques. Experimental results using human subjects show that our architecture achieves signiﬁcantly better naturalness than a retrieval-based baseline and that its naturalness is close to that of a rule-based system using 149K hand-crafted rules. 1 Introduction Although task-oriented dialogue systems have been extensively researched over the decades (Walker et al., 2001; Williams et al., 2013), it is only recently that non-task-oriented dialogue, open-domain conversation, or chat has been attracting attention for its social and entertainment aspects (Bickmore and Picard, 2005; Ritter et al., 2011; Bessho et al., 2012). Creating an open-domain conversational system is a challenging problem. In task-oriented dialogue systems, it is possible to prepare knowledge for a domain and create understanding and generation modules for that domain (Nakano et al., 2000). However, for open-domain conversation, such preparation cannot be performed. Since it is difﬁcult to handle users’ open-domain"
C14-1088,J94-2003,0,\N,Missing
C14-1088,C98-1065,0,\N,Missing
D19-5217,W17-5711,1,0.866816,"Missing"
D19-5217,W04-3230,0,0.142547,"Missing"
D19-5217,W18-2713,1,0.848795,"n is that the pseudo-parallel sentences become less varied than those created manually, because of machine translation. This characteristic makes it difficult for the back-translation method to enhance the encoder, in contrast to the decoder. To solve this problem, Imamura et al. (2018) proposed a method that combines the following two methods. 4.3 Static and Dynamic Self-Training There are two types of self-training based on backtranslation, depending on the pseudo-parallel sentences and the structure of mini-batches: the static self-training (Imamura et al., 2018) and dynamic self-training (Imamura and Sumita, 2018). Steps 3 and 4 of Section 4.2 are different for each type. Static self-training constructs a training set by combining Kstatic pseudo-parallel sentences with each of the original sentences. In this paper, we set Kstatic = 4. During training, the training set is fixed. In static self-training, the number of pseudoparallel sentences is Kstatic times larger than the number of original sentences. If we simply mix these sentences, the ratio of pseudo-parallel sentences to original sentences would be too high. To avoid this problem, we oversample the original sentences by a factor of Kstatic , inst"
D19-5217,P07-2045,0,0.0188997,"ing; the back-translation of additional monolingual corpora has different features. Results of ASPEC and TDDC Tasks The results of ASPEC and TDDC are shown in Tables 3 and 4, respectively. Both tables show the translation quality (BLEU) and the perplexity of the development set (Dev. PPL), depending on the model type and training method. The effect of the long warm-up has already been shown in Section 3. 5.1 Notes of Experimental Settings The BLEU scores (Papineni et al., 2002) in the tables were computed based on the tokenizers MeCab (for Japanese (Kudo et al., 2004)) and Moses (for English (Koehn et al., 2007)). We trained four models with different random seeds. The single model rows of the tables show the average score of four models, and the ensemble rows show the score of the ensemble of four models. The length penalty for testing was set to maximize the BLEU score of the development set. However, in the TDDC task, we used different penalties for the items and texts sets, and independently optimized according to the set. Finally, we submitted the ensemble models for which the BLEU scores of the development set (in the single model cases) were the highest. 6 Conclusions This paper explained the"
D19-5217,P02-1040,0,0.105856,"nvestigating the conditions that influence translation quality is our future work. Note that this phenomenon is only observed for self-training; the back-translation of additional monolingual corpora has different features. Results of ASPEC and TDDC Tasks The results of ASPEC and TDDC are shown in Tables 3 and 4, respectively. Both tables show the translation quality (BLEU) and the perplexity of the development set (Dev. PPL), depending on the model type and training method. The effect of the long warm-up has already been shown in Section 3. 5.1 Notes of Experimental Settings The BLEU scores (Papineni et al., 2002) in the tables were computed based on the tokenizers MeCab (for Japanese (Kudo et al., 2004)) and Moses (for English (Koehn et al., 2007)). We trained four models with different random seeds. The single model rows of the tables show the average score of four models, and the ensemble rows show the score of the ensemble of four models. The length penalty for testing was set to maximize the BLEU score of the development set. However, in the TDDC task, we used different penalties for the items and texts sets, and independently optimized according to the set. Finally, we submitted the ensemble mode"
D19-5217,P16-1009,0,0.169362,"of ASPEC (abbreviated to ASPEC.en-ja and ASPEC.ja-en, respectively), and Japanese-to-English of the TDDC (TDDC.ja-en). The corpus used in the ASPEC tasks is Asian Scientific Paper Excerpt Corpus (Nakazawa et al., 2016), which is a collection of scientific papers. In the TDDC task, the Timely Disclosure Documents Corpus (TDDC) was used. The development and test sets of TDDC are divided into items and texts sets, which are collections of titles and body texts, respectively. The sizes of the corpora are shown in Table 1. All corpora were divided into sub-words using the byte-pair encoding rules (Sennrich et al., 2016b) acquired from the training sets of each corpus. The rules were independently acquired from the source and target languages, to give a vocabulary size around 16K. • We investigated the relationship between the learning rate, warm-up, and model perplexity, and found that a long warm-up allows high learning rates, and consequently the translation quality improves. According to this finding, we applied the long warm-up. • We applied the self-training strategy, which uses multiple back-translations generated by sampling (Imamura et al., 2018) to increase the robustness of the encoder and improve"
D19-5217,P16-1162,0,0.47311,"of ASPEC (abbreviated to ASPEC.en-ja and ASPEC.ja-en, respectively), and Japanese-to-English of the TDDC (TDDC.ja-en). The corpus used in the ASPEC tasks is Asian Scientific Paper Excerpt Corpus (Nakazawa et al., 2016), which is a collection of scientific papers. In the TDDC task, the Timely Disclosure Documents Corpus (TDDC) was used. The development and test sets of TDDC are divided into items and texts sets, which are collections of titles and body texts, respectively. The sizes of the corpora are shown in Table 1. All corpora were divided into sub-words using the byte-pair encoding rules (Sennrich et al., 2016b) acquired from the training sets of each corpus. The rules were independently acquired from the source and target languages, to give a vocabulary size around 16K. • We investigated the relationship between the learning rate, warm-up, and model perplexity, and found that a long warm-up allows high learning rates, and consequently the translation quality improves. According to this finding, we applied the long warm-up. • We applied the self-training strategy, which uses multiple back-translations generated by sampling (Imamura et al., 2018) to increase the robustness of the encoder and improve"
D19-5603,D18-1045,0,0.0207437,"sizes. BERT encoders themselves do not need parallel corpora for training. They can be applied to low-resource language pairs for which large parallel corpora are difficult to obtain. However, huge monolingual corpora are necessary to train BERT encoders. Therefore, they are suitable for translation from resource-rich languages (e.g., English) to low-resource languages. By contrast, back-translation requires a certain size of parallel corpora to translate back from the target to the source languages. This is because back-translated results are not confident if the parallel corpora are small (Edunov et al., 2018). Therefore, back-translation is suitable for translating middle-resource language pairs. Note that unsupervised machine translation can be realized using the XLM described in Section 2.2 by connecting two autoencoders as an encoderdecoder. Those autoencoders are trained to encode source-target-source and target-source-target using monolingual corpora. Therefore, this approach can be regarded as including back-translation. Because back-translation was originally developed to enhance decoders, it is reasonable to incorporate it into pre-training. simultaneously trains a translation model and tw"
D19-5603,P02-1040,0,0.103539,"Missing"
D19-5603,P18-2124,0,0.0854278,"Missing"
D19-5603,P16-1009,0,0.104341,"Missing"
D19-5603,P16-1162,0,0.19246,"Missing"
D19-5603,P16-1185,0,0.0269904,"Missing"
D19-5603,P11-2031,0,0.121927,"Missing"
D19-5603,N19-1423,0,0.227764,"ostage optimization, which first trains only the unlearned parameters by freezing the BERT model, and then fine-tunes all the sub-models. In our experiments, stable two-stage optimization was achieved, in contrast the BLEU scores of direct fine-tuning were extremely low. Consequently, the BLEU scores of the proposed method were better than those of the Transformer base model and the same model without pre-training. Additionally, we confirmed that NMT with the BERT encoder is more effective in low-resource settings. 1 Introduction Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) is a language representation model trained in advance on a very large monolingual dataset. We adapt this model to our own tasks after fine-tuning (Freitag and Al-Onaizan, 2016; Servan et al., 2016) using task-specific data. Systems using BERT have achieved high accuracy in various tasks, such as the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019) and the reading comprehension benchmark using the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2018). However, most tasks using BERT are monolingual because it was originally developed for natural lang"
D19-5603,D16-1160,0,0.0537589,"Missing"
E03-1029,2002.tmi-papers.9,1,0.900479,"rce sentence. For example, the Japanese sentence &quot;Kono toraberaazu chekku wo genkin ni shite kudasai&quot; can be translated into English any of the following sentences. • I&apos;d like to cash these traveler&apos;s checks. • Could you change these traveler&apos;s checks into cash? • Please cash these traveler&apos;s checks. These translations are all correct. Actually, the corpus of Takezawa et al. (2002) contains ten different translations of this source sentence. When we construct MT knowledge from corpora that contain such variety, redundant rules are acquired. For instance, a pattern-based MT system described in Imamura (2002) acquires different transfer rules from each multiple translations, although only one rule is necessary for translating a sentence. Redundant rules increase ambiguity or decrease translation speed (Meyers et al., 2000). 156 3 Appropriate Translation for MTs 3.1 Controlled Translation Controlled language (Mitamura et al., 1991; Mitamura and Nyberg, 1995) is proposed for monolingual processing in order to reduce variety. This method allows monolingual texts within a restricted vocabulary and a restricted grammar. Texts written by the controlled language method have fewer semantic and syntactic a"
E03-1029,J00-2004,0,0.0227829,"k up words in the dictionary by target words. Tt denotes the number of target words found in the definition parts of the dictionary. 3. If there is an entry that includes both the source and target word, the word pair is regarded as the word link L denotes the number of word links. 4.1 Literalness Measure A literal translation means that source words are translated one by one to target words. Therefore, a bilingual sentence that has many word correspondences is literal. The word correspondences can be acquired by referring to translation dictionaries or using statistical word aligners (e.g., (Melamed, 2000)). However, not all source words always have an exact corresponding target word. For example, in 158 4. Calculate the literalness with the following equation, which we call the Translation Correspondence Rate (TCR) in this paper. TCR = 2L Ts + Tt (1) The TCR denotes the portion of the directly translated words among the words that should be translated. This definition is bi-directional, Ts,Tt TCR Word Links and Words in the Dictionary Target 1 (English) Source (Japanese: 4111M va Target 2 (English) wo is Cihfferet from what dei muse 0 ordere Figure 2: Example of Measuring Literalness Using Tra"
E03-1029,W01-1406,0,0.0360482,"Missing"
E03-1029,C00-1078,0,0.0154717,"ould you change these traveler&apos;s checks into cash? • Please cash these traveler&apos;s checks. These translations are all correct. Actually, the corpus of Takezawa et al. (2002) contains ten different translations of this source sentence. When we construct MT knowledge from corpora that contain such variety, redundant rules are acquired. For instance, a pattern-based MT system described in Imamura (2002) acquires different transfer rules from each multiple translations, although only one rule is necessary for translating a sentence. Redundant rules increase ambiguity or decrease translation speed (Meyers et al., 2000). 156 3 Appropriate Translation for MTs 3.1 Controlled Translation Controlled language (Mitamura et al., 1991; Mitamura and Nyberg, 1995) is proposed for monolingual processing in order to reduce variety. This method allows monolingual texts within a restricted vocabulary and a restricted grammar. Texts written by the controlled language method have fewer semantic and syntactic ambiguities when they are read by a human or analyzed by a computer. A similar idea can be applied to bilingual corpora. Namely, the expressions in bilingual corpora should be restricted, and &quot;translations that are appr"
E03-1029,1995.tmi-1.12,0,0.0357637,"lly, the corpus of Takezawa et al. (2002) contains ten different translations of this source sentence. When we construct MT knowledge from corpora that contain such variety, redundant rules are acquired. For instance, a pattern-based MT system described in Imamura (2002) acquires different transfer rules from each multiple translations, although only one rule is necessary for translating a sentence. Redundant rules increase ambiguity or decrease translation speed (Meyers et al., 2000). 156 3 Appropriate Translation for MTs 3.1 Controlled Translation Controlled language (Mitamura et al., 1991; Mitamura and Nyberg, 1995) is proposed for monolingual processing in order to reduce variety. This method allows monolingual texts within a restricted vocabulary and a restricted grammar. Texts written by the controlled language method have fewer semantic and syntactic ambiguities when they are read by a human or analyzed by a computer. A similar idea can be applied to bilingual corpora. Namely, the expressions in bilingual corpora should be restricted, and &quot;translations that are appropriate for the MT&quot; should be used in knowledge construction. This approach assumes that context/situation-dependent translations should"
E03-1029,1991.mtsummit-papers.9,0,0.022503,"are all correct. Actually, the corpus of Takezawa et al. (2002) contains ten different translations of this source sentence. When we construct MT knowledge from corpora that contain such variety, redundant rules are acquired. For instance, a pattern-based MT system described in Imamura (2002) acquires different transfer rules from each multiple translations, although only one rule is necessary for translating a sentence. Redundant rules increase ambiguity or decrease translation speed (Meyers et al., 2000). 156 3 Appropriate Translation for MTs 3.1 Controlled Translation Controlled language (Mitamura et al., 1991; Mitamura and Nyberg, 1995) is proposed for monolingual processing in order to reduce variety. This method allows monolingual texts within a restricted vocabulary and a restricted grammar. Texts written by the controlled language method have fewer semantic and syntactic ambiguities when they are read by a human or analyzed by a computer. A similar idea can be applied to bilingual corpora. Namely, the expressions in bilingual corpora should be restricted, and &quot;translations that are appropriate for the MT&quot; should be used in knowledge construction. This approach assumes that context/situation-de"
E03-1029,J93-2003,0,0.00415245,"rce sentence agrees substantially with that of a target sentence. This measure ensures that the cost of word order adjustment is small. • Word Translation Stability: A source word is better translated into the same target word through the corpus. For example, the Japanese adjectival verb `hitstiyoo-da&apos; can be translated into the English adjective &apos;necessary,&apos; the verb &apos;need,&apos; or the verb &apos;require.&apos; It is better for an MT system to always translate this word into &apos;necessary,&apos; if possible. Effective measures of controlled translation depend on MT methods. For example, word-level statistical MT (Brown et al., 1993) translates a source sentence with a combination of word transfer and word order adjustment. Thus, wordorder agreement is an important measure. On the other hand, this is not important for transfer-based MTs because the word order can be significantly changed through syntactic transfer. A transferbased MT method using the phrase structure is studied here. 3.2 Base MT System We use Hierarchical Phrase Alignment-based Translator (HPAT) (Imamura, 2002) as the target transfer-based MT system. HPAT is an new version of Transfer Driven Machine Translator (TDMT) (Furuse and Iida, 1994). Transfer rule"
E03-1029,P02-1040,0,0.0821353,"Missing"
E03-1029,C94-1015,0,0.114985,"l statistical MT (Brown et al., 1993) translates a source sentence with a combination of word transfer and word order adjustment. Thus, wordorder agreement is an important measure. On the other hand, this is not important for transfer-based MTs because the word order can be significantly changed through syntactic transfer. A transferbased MT method using the phrase structure is studied here. 3.2 Base MT System We use Hierarchical Phrase Alignment-based Translator (HPAT) (Imamura, 2002) as the target transfer-based MT system. HPAT is an new version of Transfer Driven Machine Translator (TDMT) (Furuse and Iida, 1994). Transfer rules of HPAT are automatically acquired from a parallel corpus, but those of TDMT were constructed manually. The procedure of HPAT is briefly described as follows (Figure 1). First, phrasal correspondences are hierarchically extracted from a parallel corpus using Hierarchical Phrase Alignment (Imamura, 2001). Next, the hierarchical correspondences are transferred into patterns, and transfer rules are generated. At the time of translation, the input sentence is parsed by using source patterns in the transfer rules. The MT result is generated by mapping the source patterns to the tar"
E03-1029,takezawa-etal-2002-toward,1,0.802206,"r situation. 2.2 Multiple Translations Generally speaking, a single source expression can be translated into multiple target expressions. Therefore, a corpus contains multiple translations even though they are translated from the same source sentence. For example, the Japanese sentence &quot;Kono toraberaazu chekku wo genkin ni shite kudasai&quot; can be translated into English any of the following sentences. • I&apos;d like to cash these traveler&apos;s checks. • Could you change these traveler&apos;s checks into cash? • Please cash these traveler&apos;s checks. These translations are all correct. Actually, the corpus of Takezawa et al. (2002) contains ten different translations of this source sentence. When we construct MT knowledge from corpora that contain such variety, redundant rules are acquired. For instance, a pattern-based MT system described in Imamura (2002) acquires different transfer rules from each multiple translations, although only one rule is necessary for translating a sentence. Redundant rules increase ambiguity or decrease translation speed (Meyers et al., 2000). 156 3 Appropriate Translation for MTs 3.1 Controlled Translation Controlled language (Mitamura et al., 1991; Mitamura and Nyberg, 1995) is proposed fo"
E03-1029,2001.mtsummit-ebmt.4,0,\N,Missing
E03-1048,P02-1040,0,0.0780651,"ere an SMT system scored highest. We are studying these interesting contradictory observations. Let&apos;s consider the relationships among the HUMAN rank, the RED rank, and the BLEU score. While RED accords with HUMAN, BLEU fails to agree with HUMAN in the EJ evaluation. One reason for this is that the BLEU score favors SAT translations in that they are more similar to the reference translation from the viewpoint of Ngrams. Table 1 Quality Evaluation of Three MTs 5 (2) BLEU score: The MT translations are scored based on the precision of N-grams in an entire set of multiple reference translations (Papineni et al., 2002). It ranges from 1.0 (best) down to 0.0 (worst). (3) Estimated TOEIC score: It is important to interpret MT performance from the viewpoint of a language proficiency test such as TOEIC 4 . A translator compared MT translations with human ones, then, MT&apos;s proficiency is estimated by regression analysis (Sugaya et al., 2000). It ranges from 10 (lowest) to 990 points (perfect). 3.3 Results Table 1 wraps up the results. So far, SMT has been applied mainly to language pairs of similar European languages. Skeptical opinions dominate about Average is calculated: A, B, C, and D are assigned values of 4"
E03-1048,2001.mtsummit-papers.3,1,0.832851,"e and the RED rank are measured by referring to the test corpus, i.e., a set of input sentences and their multiple reference translations; the HUMAN rank and the estimated TOEIC score are judged by bilingual translators. (1) Average of Ranks 2 : HUMAN rank: In our evaluation, 9 translators who are native speakers of the target language ranked the MT translations into 4 ranks: A, B, C, and D, from good to bad (Sumita et al., 1999). 3 RED rank: An automatic ranker is learned as a decision tree from HUMAN-ranked examples. It exploits edit-distances between MT and multiple reference translations (Akiba et al., 2001). the effectiveness or applicability of SMT to dissimilar language pairs. However, we implemented SMT for translation between Japanese and English. They are dissimilar in many points, such as word order and lexical systems. We found that SAT, which is an SMT, worked in both J-to-E and Eto-J directions. The EBMT systems, HPAT and D 3 , surpassed SAT in the HUMAN rank. This is the reverse result obtained in a Verbmobil experiment (Ney, 2001) where an SMT system scored highest. We are studying these interesting contradictory observations. Let&apos;s consider the relationships among the HUMAN rank, the"
E03-1048,shimohata-sumita-2002-automatic,1,0.89347,"Missing"
E03-1048,C02-1076,1,0.819515,"ging infeasible. Methods using N-gram statistics of a target language corpus have been proposed before (Brown and Frederking, 1995; Callison-Burch et al., 2001). They are based on the assumptions that (1) the naturalness of the translations is effective for selecting good translations because they are sensitive to the broken target sentences due to errors in translation processes, and (2) the source and target correspondences from the semantic point of view are maintained in a state-of-the-art translation system. However, the second assumption does not necessarily hold. To solve this problem, Akiba et al. (2002) used not only a language model but also a translation model of SMT derived from a corpus, and Sumita et al. (2002) exploited a corpus whose sentences are converted into semantic class sequences. These two selectors outperformed conventional selectors using the target N-gram in our experiments. 5 Paraphrasing and Filtering This section introduces another feature of C3 : paraphrasing and filtering corpora. The large variety of possible translations in a corpus causes difficulty in building machine translation on the corpus. For example, the variety makes it harder to estimate the parameters for"
E03-1048,1995.tmi-1.17,0,0.00827994,"w the HUMAN rank, as described above. Table 2. Sample of Translation Variety [B] Is the payment cash? Or is it the credit card? [A] Would you like to pay in cash or with a credit card? [C] Could you cash or credit card? In our experiment, while D3 , HPAT, and SAT for the E-to-J direction have A-ratios of 0.62, 0.55, and 0.53, respectively, the ideal selection would have an interestingly high A-ratio of 0.79. Thus, we could obtain a large increase in accuracy if it were possible to select the best one of the three different translations for each input sentence. Unlike other approaches such as (Brown and Frederking, 1995), we do not merge multiple results into a single one but we select the best one because the large difference between multiple translations for distant language pairs such as Japanese and English makes merging infeasible. Methods using N-gram statistics of a target language corpus have been proposed before (Brown and Frederking, 1995; Callison-Burch et al., 2001). They are based on the assumptions that (1) the naturalness of the translations is effective for selecting good translations because they are sensitive to the broken target sentences due to errors in translation processes, and (2) the"
E03-1048,2001.mtsummit-papers.12,0,0.0767102,"Missing"
E03-1048,W02-1611,1,0.895885,"Missing"
E03-1048,suyaga-etal-2002-proposal,0,0.0182927,"or example, the variety makes it harder to estimate the parameters for SAT, to find appropriate translation examples for D3 , to extract good transfer patterns for HPAT. We propose ways to overcome these problems by paraphrasing corpora through automated processes or filtering corpora by abandoning inappropriate expressions. Two methods have been investigated for automatic paraphrasing. (1) Shimohata et al. (2002a) group sentences by the equivalence of the translation and extract rules of paraphrasing by DPmatching. (2) Finch et al. (2002) cluster sentences in a handcrafted paraphrase corpus (Sugaya et al., 2002) to obtain pairs that are similar to each other for training SMT models, then by using the models the decoder generates a paraphrase. The experimental results indicate that (i) the EBMT based on normalization had increased coverage (Shimohata et al., 2002b) and (ii) the SMT created on the normalized sentences had a reduced word-error-rate (Watanabe et al., 2002a). Imamura et al. (2003) proposed a calculation that measures the literalness of a translation pair and called it TCR. After the word alignment of a translation pair, TCR is calculated as the rate of the aligned word count over the coun"
E03-1048,2002.tmi-tutorials.2,0,0.0456358,"corpus causes difficulty in building machine translation on the corpus. For example, the variety makes it harder to estimate the parameters for SAT, to find appropriate translation examples for D3 , to extract good transfer patterns for HPAT. We propose ways to overcome these problems by paraphrasing corpora through automated processes or filtering corpora by abandoning inappropriate expressions. Two methods have been investigated for automatic paraphrasing. (1) Shimohata et al. (2002a) group sentences by the equivalence of the translation and extract rules of paraphrasing by DPmatching. (2) Finch et al. (2002) cluster sentences in a handcrafted paraphrase corpus (Sugaya et al., 2002) to obtain pairs that are similar to each other for training SMT models, then by using the models the decoder generates a paraphrase. The experimental results indicate that (i) the EBMT based on normalization had increased coverage (Shimohata et al., 2002b) and (ii) the SMT created on the normalized sentences had a reduced word-error-rate (Watanabe et al., 2002a). Imamura et al. (2003) proposed a calculation that measures the literalness of a translation pair and called it TCR. After the word alignment of a translation"
E03-1048,W01-1401,1,0.84948,"stical Machine Translation (SMT; Brown et al., 1993; Knight, 1997; Ney, 2001; Alshawi et al., 2000). C3 is developing both technologies in parallel and blending them. In this paper, we introduce three different machine translation systems: Di, HPAT, and SAT. The three MT systems are characterized by different translation units. D3 , HPAT, and SAT use sentences, phrases, and words, respectively. D3 (Sentence-based EBMT): It retrieves the most similar example by DP-matching of the input and example sentences and adjusts the gap between the input and the retrieved example by using dictionaries. (Sumita 2001) HPAT (Phrase-based EBMT): Based on phrasealigned bilingual trees, transfer patterns are generated. According to the patterns, the source phrase structure is obtained and converted to generate target sentences (Imamura 2002) SAT (Word-based SMT): Watanabe et al. (2002b) implemented SAT dealing with Japanese and English on top of a word-based SMT framework (Brown et al. 1993). 3 Competition on the Same Corpus 3.1 Resources In our competitive evaluation of the MT systems, we used the BTEC corpus &apos;, which is a collection of Japanese sentences and their English translations typically found in phra"
E03-1048,2002.tmi-papers.9,1,0.841258,"slation systems: Di, HPAT, and SAT. The three MT systems are characterized by different translation units. D3 , HPAT, and SAT use sentences, phrases, and words, respectively. D3 (Sentence-based EBMT): It retrieves the most similar example by DP-matching of the input and example sentences and adjusts the gap between the input and the retrieved example by using dictionaries. (Sumita 2001) HPAT (Phrase-based EBMT): Based on phrasealigned bilingual trees, transfer patterns are generated. According to the patterns, the source phrase structure is obtained and converted to generate target sentences (Imamura 2002) SAT (Word-based SMT): Watanabe et al. (2002b) implemented SAT dealing with Japanese and English on top of a word-based SMT framework (Brown et al. 1993). 3 Competition on the Same Corpus 3.1 Resources In our competitive evaluation of the MT systems, we used the BTEC corpus &apos;, which is a collection of Japanese sentences and their English translations typically found in phrasebooks for tourists. The size is about 150 thousand sentence pairs. A quality evaluation was done using a test set consisting of 345 sentences selected randomly from the above corpus, and the remaining sentences were used f"
E03-1048,1983.tc-1.13,0,0.350529,"Missing"
E03-1048,E03-1029,1,0.834211,"g. (1) Shimohata et al. (2002a) group sentences by the equivalence of the translation and extract rules of paraphrasing by DPmatching. (2) Finch et al. (2002) cluster sentences in a handcrafted paraphrase corpus (Sugaya et al., 2002) to obtain pairs that are similar to each other for training SMT models, then by using the models the decoder generates a paraphrase. The experimental results indicate that (i) the EBMT based on normalization had increased coverage (Shimohata et al., 2002b) and (ii) the SMT created on the normalized sentences had a reduced word-error-rate (Watanabe et al., 2002a). Imamura et al. (2003) proposed a calculation that measures the literalness of a translation pair and called it TCR. After the word alignment of a translation pair, TCR is calculated as the rate of the aligned word count over the count of words in the translation pair. After abandoning the non-literal parts of the corpus, the acquisition of HPAT transfer patterns is done. The effect has been confirmed by an improvement in translation quality. 6 Conclusion Our project, called C3 , places corpora at the center of speech-to-speech technology. Good performance in translation components is demonstrated in the experiment"
E03-1048,1999.mtsummit-1.34,1,0.836105,"ezawa et al., 2002). 171 We used bilingual dictionaries and thesauri of about fifty thousand words for the travel domain. 3.2 Evaluation Measures We used the measures below. The BLEU score and the RED rank are measured by referring to the test corpus, i.e., a set of input sentences and their multiple reference translations; the HUMAN rank and the estimated TOEIC score are judged by bilingual translators. (1) Average of Ranks 2 : HUMAN rank: In our evaluation, 9 translators who are native speakers of the target language ranked the MT translations into 4 ranks: A, B, C, and D, from good to bad (Sumita et al., 1999). 3 RED rank: An automatic ranker is learned as a decision tree from HUMAN-ranked examples. It exploits edit-distances between MT and multiple reference translations (Akiba et al., 2001). the effectiveness or applicability of SMT to dissimilar language pairs. However, we implemented SMT for translation between Japanese and English. They are dissimilar in many points, such as word order and lexical systems. We found that SAT, which is an SMT, worked in both J-to-E and Eto-J directions. The EBMT systems, HPAT and D 3 , surpassed SAT in the HUMAN rank. This is the reverse result obtained in a Ver"
E03-1048,W01-1405,0,0.048248,"g a high-quality translation subsystem for a speechto-speech translation system. This paper introduces recent progress in C3 . Sections 2 and 3 demonstrate a competition between multiple machine translation systems developed in our project, and Sections 4 and 5 explain the features that differentiate our project from other corpus-based projects. 2 Three Corpus-based MT Systems There are two main strategies in corpus-based machine translation: (i) Example-Based Machine Translation (EBMT; Nagao, 1984; Somers, 1999) and (ii) Statistical Machine Translation (SMT; Brown et al., 1993; Knight, 1997; Ney, 2001; Alshawi et al., 2000). C3 is developing both technologies in parallel and blending them. In this paper, we introduce three different machine translation systems: Di, HPAT, and SAT. The three MT systems are characterized by different translation units. D3 , HPAT, and SAT use sentences, phrases, and words, respectively. D3 (Sentence-based EBMT): It retrieves the most similar example by DP-matching of the input and example sentences and adjusts the gap between the input and the retrieved example by using dictionaries. (Sumita 2001) HPAT (Phrase-based EBMT): Based on phrasealigned bilingual tree"
E03-1048,C02-1050,1,0.897796,"Missing"
E03-1048,J90-2002,0,\N,Missing
E03-1048,takezawa-etal-2002-toward,1,\N,Missing
I13-1184,J96-1002,0,0.0303091,"995 Jan.,2005 - Nov.,2005 Dec.,1995 Jan.,1996 - Dec.,1996 Jan.,2006 - Dec.,2006 # of Data 102,454 88,202 9,043 114,116 95,761 Table 1: Statistics of Data Used Machine Learning Original Dataset Adaptation Additional Dataset Figure 2: Relationship between Developers and Users of NLP Tools tured learning, which extends the classification, we select the linear classification task. In this paper, we investigate the combination of the following learning methods and additional datasets. • We test two learning methods, batch and online learning. In batch learning, we use a maximum entropy classifier (Berger et al., 1996; Chen and Rosenfeld, 2000) and adapt the model using transfer learning. In online learning, we select soft confidence-weighted learning (Wang et al., 2012). • We test two kinds of additional datasets. One is that all data are used for adaptation. The other is that only the data that failed to predict correct categories by the original model are used. We consider the active learning strategy for the second dataset. The remainder of this paper is organized as follows. In Section 2, we detail the task, datasets, and learning methods (batch and online learning). Section 3 describes the experiment"
I13-1184,P07-1033,0,0.16775,"Missing"
I13-1184,W07-1522,0,0.0133022,"adaptation (Evgeniou and Pontil, 2004; Xiao and Bilmes, 2006) is a variant of model-based domain adaptation. As the regularizer, it uses the differences 1 The task of this study is category classification of Japanese newspaper articles. We selected articles from Mainichi Shinbun newspapers for the years of 1995, 1996, 2005, and 2006. A part of the 1995 data is widely used in the Japanese Dependency structures are published as Kyoto University Text Corpus (http://nlp.ist.i.kyotou.ac.jp/EN/index.php?Kyoto University Text Corpus). Predicate-argument structures are published as NAIST Text Corpus (Iida et al., 2007) (http://cl.naist.jp/nldata/corpus/). Note that the texts of the articles must be purchased from the newspaper company. 1293 1995 Original Dataset Data Year 1996 ・・・ Test set 1 2005 2006 Additional Dataset Test set 2 Learning Model Original Model Adapted Model Adaptation Figure 3: Datasets and Models in parameters between the adapted model and the original model, not adapted parameters. This is done to minimize the differences between the original model and the adapted model. Although Evgeniou and Pontil (2004) proposed regularized adaptation for SVMs, and Xiao and Bilmes (2006) proposed the s"
I13-1184,W04-3230,0,0.0340692,"Missing"
I13-1184,D09-1052,0,\N,Missing
L18-1545,L16-1502,0,0.0153575,"structed. Furthermore, it can also be applied in comparative studies of pivot translation (Utiyama and Isahara, 2007; Cohn and Lapata, 2007) and zero-shot translation (Johnson et al., 2016). 2. It covers four domains, namely, medical care, disaster prevention, shopping, and tourism. It can be applied to domain adaptation studies (e.g., (Imamura and Sumita, 2016)). 3. It consists of pseudo-dialogues. Therefore, it can also be applied to discourse studies that consider longdistance contexts. Note that such contexts are simpler than those of real dialogues because the dialogue never breaks down (Higashinaka et al., 2016). In this paper, we focus on the first characteristic. We use the GCP Corpus to confirm MT qualities between Japanese and other languages. Furthermore, we compare the qualities of direct, pivot, and zero-shot translations. Europarl (Koehn, 2005), a collection of European Parliament proceedings, is a well-known multilingual parallel corpus. The characteristics of the GCP Corpus are similar to those of the Europarl. However, the GCP Corpus has different applications because it includes Asian languages and pseudo-dialogues that are being developed for use in speech translation systems. The remind"
L18-1545,2005.mtsummit-papers.11,0,0.109897,"n, shopping, and tourism. It can be applied to domain adaptation studies (e.g., (Imamura and Sumita, 2016)). 3. It consists of pseudo-dialogues. Therefore, it can also be applied to discourse studies that consider longdistance contexts. Note that such contexts are simpler than those of real dialogues because the dialogue never breaks down (Higashinaka et al., 2016). In this paper, we focus on the first characteristic. We use the GCP Corpus to confirm MT qualities between Japanese and other languages. Furthermore, we compare the qualities of direct, pivot, and zero-shot translations. Europarl (Koehn, 2005), a collection of European Parliament proceedings, is a well-known multilingual parallel corpus. The characteristics of the GCP Corpus are similar to those of the Europarl. However, the GCP Corpus has different applications because it includes Asian languages and pseudo-dialogues that are being developed for use in speech translation systems. The reminder of this paper is organized as follows. Sections 2. and 3. summarize the GCP and discuss the current status of the GCP Corpus, respectively. In Section 4., we construct a neural machine translation system using the GCP Corpus and evaluate the"
L18-1545,P07-1092,0,0.187897,"eted by the GCP. Therefore, the corpus is sentence-aligned. The target domains are medical care, disaster prevention, shopping, and tourism. Although the GCP Corpus is being developed for use in speech translation systems, it could also be used in various other research fields because it has the following characteristics. 1. It is a multilingual sentence-aligned corpus that covers ten languages, including Asian languages. Therefore, this allows 90 different MT systems to be constructed. Furthermore, it can also be applied in comparative studies of pivot translation (Utiyama and Isahara, 2007; Cohn and Lapata, 2007) and zero-shot translation (Johnson et al., 2016). 2. It covers four domains, namely, medical care, disaster prevention, shopping, and tourism. It can be applied to domain adaptation studies (e.g., (Imamura and Sumita, 2016)). 3. It consists of pseudo-dialogues. Therefore, it can also be applied to discourse studies that consider longdistance contexts. Note that such contexts are simpler than those of real dialogues because the dialogue never breaks down (Higashinaka et al., 2016). In this paper, we focus on the first characteristic. We use the GCP Corpus to confirm MT qualities between Japane"
L18-1545,2016.amta-researchers.7,1,0.674954,", it could also be used in various other research fields because it has the following characteristics. 1. It is a multilingual sentence-aligned corpus that covers ten languages, including Asian languages. Therefore, this allows 90 different MT systems to be constructed. Furthermore, it can also be applied in comparative studies of pivot translation (Utiyama and Isahara, 2007; Cohn and Lapata, 2007) and zero-shot translation (Johnson et al., 2016). 2. It covers four domains, namely, medical care, disaster prevention, shopping, and tourism. It can be applied to domain adaptation studies (e.g., (Imamura and Sumita, 2016)). 3. It consists of pseudo-dialogues. Therefore, it can also be applied to discourse studies that consider longdistance contexts. Note that such contexts are simpler than those of real dialogues because the dialogue never breaks down (Higashinaka et al., 2016). In this paper, we focus on the first characteristic. We use the GCP Corpus to confirm MT qualities between Japanese and other languages. Furthermore, we compare the qualities of direct, pivot, and zero-shot translations. Europarl (Koehn, 2005), a collection of European Parliament proceedings, is a well-known multilingual parallel corpu"
L18-1545,Q17-1024,0,0.112679,"Missing"
L18-1545,P17-4012,0,0.0314013,"Missing"
L18-1545,W17-5706,0,0.134396,"Missing"
L18-1545,W17-5712,1,0.79518,"Missing"
L18-1545,P02-1040,0,0.101218,"nsiders translation length, the first term of the right side denotes the log-likelihood, W P denotes a word penalty (W P ≥ 0), and T denotes the word number of the translation. Equation 1 corrects a translation length using the word penalty because NMTs typically generate short translations. The word penalty is optimized using a development set to make the translation length and reference length nearly equal. By correcting the translation length, we can compute the BLEU scores regardless of the brevity penalty. 4.2. Translation Quality Table 3 shows the quality of the MTs as measured by BLEU (Papineni et al., 2002). 3 http://opennmt.net/ First, the BLEU scores are significantly different for each language, ranging from 22.05 to 52.87 for translation from Japanese and from 23.39 to 58.13 for translation to Japanese. However, the score tended to increase with an increasing number of training sentences. Next, by comparing translations from Japanese with translations to Japanese, it was found that the scores when translating to Japanese were higher than those when translating from Japanese for all language pairs. This phenomenon shows that translating from Japanese was more difficult than translating to Jap"
L18-1545,P16-1162,0,0.0108932,"tween Japanese and the other languages (Ja ↔ X; a total of 18 systems) due to resource limitations. Datasets The corpora (Table 2) were divided into training, development, and test sets. Initially, we set aside some sentences from each corpus (held-out data) and used the remaining sentences as the training set. From the held-out data, we uniformly selected two 2,000 sentence sets as development and test sets. MT System The training, development, and test sets were segmented into words using in-house word segmenters, and the words were further segmented into subwords using a byte-pair encoder (Sennrich et al., 2016). 3454 Language Japanese English Chinese Korean Thai Vietnamese Indonesian Myanmar Spanish French Abbr. Ja En Zh Ko Th Vi Id My Es Fr Total 2,029,111 (25.2 chars. / sent.) 2,029,111 (11.2 words / sent.) 2,026,608 2,026,608 1,150,070 1,150,070 1,150,070 1,150,070 337,654 340,499 No. of Sentences (Utterances) Medical Care Disaster Prevention 420,270 249,495 420,270 249,495 420,270 249,495 420,270 249,495 145,054 117,636 145,054 117,636 145,054 117,636 145,054 117,636 145,054 117,636 145,054 117,636 Shopping 355,429 355,429 355,429 355,429 180,843 180,843 180,843 180,843 9,512 9,867 Tourism 527,0"
L18-1545,N07-1061,0,0.166814,"s (including Japanese) targeted by the GCP. Therefore, the corpus is sentence-aligned. The target domains are medical care, disaster prevention, shopping, and tourism. Although the GCP Corpus is being developed for use in speech translation systems, it could also be used in various other research fields because it has the following characteristics. 1. It is a multilingual sentence-aligned corpus that covers ten languages, including Asian languages. Therefore, this allows 90 different MT systems to be constructed. Furthermore, it can also be applied in comparative studies of pivot translation (Utiyama and Isahara, 2007; Cohn and Lapata, 2007) and zero-shot translation (Johnson et al., 2016). 2. It covers four domains, namely, medical care, disaster prevention, shopping, and tourism. It can be applied to domain adaptation studies (e.g., (Imamura and Sumita, 2016)). 3. It consists of pseudo-dialogues. Therefore, it can also be applied to discourse studies that consider longdistance contexts. Note that such contexts are simpler than those of real dialogues because the dialogue never breaks down (Higashinaka et al., 2016). In this paper, we focus on the first characteristic. We use the GCP Corpus to confirm MT"
N03-2013,2001.mtsummit-papers.3,1,0.836719,"ences are regarded as bilingual sentences, and simplified machine translation is carried out. Paraphrasing by our method has the following characteristics. • Not only lexical paraphrasing but also phrasal paraphrasing can be generated because our method is based on structural substitution. • Equivalent phrases extracted by HPA are not only semantically but also grammatically equivalent. Thus, our method rarely generates ungrammatical sentences by substitution. Expansion of the equivalent sentence set can be applied to automatic evaluation of machine translation quality (Papineni et al., 2002; Akiba et al., 2001), for example. These methods evaluate the quality of the translation by measuring the similarity between machine translation results and translations done by humans (called references). However, the accuracy increases when multiple references are applied because one source sentence can be translated into multiple target expressions. Our method generates multiple sentences that are suitable for this purpose. 2 Acquisition of Paraphrasing Rules: Hierarchical Phrase Alignment Hierarchical Phrase Alignment is based on the assumption that an “equivalent phrase pair has the same information and the"
N03-2013,P01-1008,0,0.0368033,"meaning (called an equivalent sentence set). This task is regarded as paraphrasing. The features of our method are: 1) The paraphrasing rules are dynamically acquired by Hierarchical Phrase Alignment from the equivalent sentence set, and 2) A large equivalent sentence set is generated by substituting source syntactic structures. Our experiments show that 561 sentences on average are correctly generated from 8.48 equivalent sentences. 1 Introduction Sentences can be represented by various expressions even though they have the same meaning. Paraphrasing that transfer from sentence to sentence (Barzilay and McKeown, 2001) is a technique that generates such various expressions. In this paper, we propose an automatic quantitative expansion method for a sentence set that contains sentences of the same meaning (called an equivalent sentence set), as a paraphrasing technique. Our method is roughly structured from the following two phases. 1. Extract phrasal correspondences that have the same meaning (called equivalent phrases) from the source equivalent sentence set (acquisition phase). 2. Based on the parse tree of the sentence selected from the source set, generate target sentences by recursively substituting the"
N03-2013,P02-1040,0,0.0978924,"ly, two equivalent sentences are regarded as bilingual sentences, and simplified machine translation is carried out. Paraphrasing by our method has the following characteristics. • Not only lexical paraphrasing but also phrasal paraphrasing can be generated because our method is based on structural substitution. • Equivalent phrases extracted by HPA are not only semantically but also grammatically equivalent. Thus, our method rarely generates ungrammatical sentences by substitution. Expansion of the equivalent sentence set can be applied to automatic evaluation of machine translation quality (Papineni et al., 2002; Akiba et al., 2001), for example. These methods evaluate the quality of the translation by measuring the similarity between machine translation results and translations done by humans (called references). However, the accuracy increases when multiple references are applied because one source sentence can be translated into multiple target expressions. Our method generates multiple sentences that are suitable for this purpose. 2 Acquisition of Paraphrasing Rules: Hierarchical Phrase Alignment Hierarchical Phrase Alignment is based on the assumption that an “equivalent phrase pair has the same"
P03-1057,2001.mtsummit-papers.3,1,0.436254,"02)). We employ the second approach in this paper. The cutoff by frequency (Menezes and Richardson, 2001) and the hypothesis test (Imamura, 2002) have been applied to clean the rules. The cutoff by frequency can slightly improve MT quality, but the improvement is still insufficient from the viewpoint of the large number of redundant rules. The hypothesis test requires very large corpora in order to obtain a sufficient number of rules that are statistically confident. Another current topic of machine translation is automatic evaluation of MT quality (Papineni et al., 2002; Yasuda et al., 2001; Akiba et al., 2001). These methods aim to replace subjective evaluation in order to speed up the development cycle of MT systems. However, they can be utilized not only as developers’ aids but also for automatic tuning of MT systems (Su et al., 1992). We propose feedback cleaning that utilizes an automatic evaluation for removing incorrect/redundant translation rules as a tuning method Feedback Cleaning Evaluation Corpus Training Corpus Automatic Acquisition Translation Rules MT Engine MT Results Rule Selection/Deletion Automatic Evaluation Figure 1: Structure of Feedback Cleaning (Figure 1). Our method evaluate"
P03-1057,C94-1015,0,0.0544641,"en the system translates an input sentence, the sentence is first parsed by using source patterns of the transfer rules. Next, a tree structure of the target language is generated by mapping the source patterns to the corresponding target patterns. When non-terminal symbols remain in the target tree, target words are inserted by referring to a translation dictionary. Ambiguities, which occur during parsing or mapping, are resolved by selecting the rules that minimize the semantic distance between the input words and source examples (real examples in the training corpus) of the transfer rules (Furuse and Iida, 1994). For instance, when the input phrase “leave at 11 a.m.” is translated into Japanese, Rule 2 in Figure 2 is selected because the semantic distance from the source example (arrive, p.m.) is the shortest to the head words of the input phrase (leave, a.m.). 2.2 Problems of Automatic Acquisition HPAT automatically acquires its transfer rules from parallel corpora by using Hierarchical Phrase Alignment (Imamura, 2001). However, the rule set contains many incorrect/redundant rules. The reasons for this problem are roughly classified as follows. • Errors in automatic rule acquisition • Translation va"
P03-1057,2002.tmi-papers.9,1,0.934691,"corpora. Such rules conflict with other existing rules and cause implausible Yuji Matsumoto Nara Institute of Science and Technology Ikoma-shi, Nara, Japan matsu@is.aist-nara.ac.jp MT results or increase ambiguity. If incorrect rules could be avoided, MT quality would necessarily improve. There are two approaches to overcoming incorrect/redundant rules: • Selecting appropriate rules in a disambiguation process during the translation (on-line processing, (Meyers et al., 2000)). • Cleaning incorrect/redundant rules after automatic acquisition (off-line processing, (Menezes and Richardson, 2001; Imamura, 2002)). We employ the second approach in this paper. The cutoff by frequency (Menezes and Richardson, 2001) and the hypothesis test (Imamura, 2002) have been applied to clean the rules. The cutoff by frequency can slightly improve MT quality, but the improvement is still insufficient from the viewpoint of the large number of redundant rules. The hypothesis test requires very large corpora in order to obtain a sufficient number of rules that are statistically confident. Another current topic of machine translation is automatic evaluation of MT quality (Papineni et al., 2002; Yasuda et al., 2001; Aki"
P03-1057,W01-1406,0,0.0360282,"or translation variety in the corpora. Such rules conflict with other existing rules and cause implausible Yuji Matsumoto Nara Institute of Science and Technology Ikoma-shi, Nara, Japan matsu@is.aist-nara.ac.jp MT results or increase ambiguity. If incorrect rules could be avoided, MT quality would necessarily improve. There are two approaches to overcoming incorrect/redundant rules: • Selecting appropriate rules in a disambiguation process during the translation (on-line processing, (Meyers et al., 2000)). • Cleaning incorrect/redundant rules after automatic acquisition (off-line processing, (Menezes and Richardson, 2001; Imamura, 2002)). We employ the second approach in this paper. The cutoff by frequency (Menezes and Richardson, 2001) and the hypothesis test (Imamura, 2002) have been applied to clean the rules. The cutoff by frequency can slightly improve MT quality, but the improvement is still insufficient from the viewpoint of the large number of redundant rules. The hypothesis test requires very large corpora in order to obtain a sufficient number of rules that are statistically confident. Another current topic of machine translation is automatic evaluation of MT quality (Papineni et al., 2002; Yasuda e"
P03-1057,C00-1078,0,0.0461382,"e for transferbased MT acquired from corpora contains many incorrect/redundant rules due to acquisition errors or translation variety in the corpora. Such rules conflict with other existing rules and cause implausible Yuji Matsumoto Nara Institute of Science and Technology Ikoma-shi, Nara, Japan matsu@is.aist-nara.ac.jp MT results or increase ambiguity. If incorrect rules could be avoided, MT quality would necessarily improve. There are two approaches to overcoming incorrect/redundant rules: • Selecting appropriate rules in a disambiguation process during the translation (on-line processing, (Meyers et al., 2000)). • Cleaning incorrect/redundant rules after automatic acquisition (off-line processing, (Menezes and Richardson, 2001; Imamura, 2002)). We employ the second approach in this paper. The cutoff by frequency (Menezes and Richardson, 2001) and the hypothesis test (Imamura, 2002) have been applied to clean the rules. The cutoff by frequency can slightly improve MT quality, but the improvement is still insufficient from the viewpoint of the large number of redundant rules. The hypothesis test requires very large corpora in order to obtain a sufficient number of rules that are statistically confide"
P03-1057,P02-1040,0,0.124322,", (Menezes and Richardson, 2001; Imamura, 2002)). We employ the second approach in this paper. The cutoff by frequency (Menezes and Richardson, 2001) and the hypothesis test (Imamura, 2002) have been applied to clean the rules. The cutoff by frequency can slightly improve MT quality, but the improvement is still insufficient from the viewpoint of the large number of redundant rules. The hypothesis test requires very large corpora in order to obtain a sufficient number of rules that are statistically confident. Another current topic of machine translation is automatic evaluation of MT quality (Papineni et al., 2002; Yasuda et al., 2001; Akiba et al., 2001). These methods aim to replace subjective evaluation in order to speed up the development cycle of MT systems. However, they can be utilized not only as developers’ aids but also for automatic tuning of MT systems (Su et al., 1992). We propose feedback cleaning that utilizes an automatic evaluation for removing incorrect/redundant translation rules as a tuning method Feedback Cleaning Evaluation Corpus Training Corpus Automatic Acquisition Translation Rules MT Engine MT Results Rule Selection/Deletion Automatic Evaluation Figure 1: Structure of Feedbac"
P03-1057,C92-2067,0,0.186427,"ality, but the improvement is still insufficient from the viewpoint of the large number of redundant rules. The hypothesis test requires very large corpora in order to obtain a sufficient number of rules that are statistically confident. Another current topic of machine translation is automatic evaluation of MT quality (Papineni et al., 2002; Yasuda et al., 2001; Akiba et al., 2001). These methods aim to replace subjective evaluation in order to speed up the development cycle of MT systems. However, they can be utilized not only as developers’ aids but also for automatic tuning of MT systems (Su et al., 1992). We propose feedback cleaning that utilizes an automatic evaluation for removing incorrect/redundant translation rules as a tuning method Feedback Cleaning Evaluation Corpus Training Corpus Automatic Acquisition Translation Rules MT Engine MT Results Rule Selection/Deletion Automatic Evaluation Figure 1: Structure of Feedback Cleaning (Figure 1). Our method evaluates the contribution of each rule to the MT results and removes inappropriate rules as a way to increase the evaluation scores. Since the automatic evaluation correlates with a subjective evaluation, MT quality will improve after cle"
P03-1057,takezawa-etal-2002-toward,1,0.054083,"rule set is a subset of the base rule set. 4. Apply the feedback cleaning algorithm to each of the N pairs and record the rule contributions even if the rules are removed. The purpose of this step is to obtain the rule contributions. 5. For each rule in the base rule set, sum up the rule contributions obtained from the rule subsets. If the sum is negative, remove the rule from the base rule set. The major difference of this method from crossvalidation is Step 5. In the case of cross-cleaning, Bilingual Corpora The corpus used in the following experiments is the Basic Travel Expression Corpus (Takezawa et al., 2002). This is a collection of Japanese sentences and their English translations based on expressions that are usually found in phrasebooks for foreign tourists. We divided it into sub-corpora for training, evaluation, and test as shown in Table 1. The number of rules acquired from the training corpus (the base rule set size) was 105,588. Evaluation Methods of MT Quality We used the following two methods to evaluate MT quality. 1. Test Corpus BLEU Score The BLUE score was calculated with the test corpus. The number of references was one for each sentence, in the same way used for the feedback clean"
P03-1057,2001.mtsummit-papers.67,0,0.0955976,"on, 2001; Imamura, 2002)). We employ the second approach in this paper. The cutoff by frequency (Menezes and Richardson, 2001) and the hypothesis test (Imamura, 2002) have been applied to clean the rules. The cutoff by frequency can slightly improve MT quality, but the improvement is still insufficient from the viewpoint of the large number of redundant rules. The hypothesis test requires very large corpora in order to obtain a sufficient number of rules that are statistically confident. Another current topic of machine translation is automatic evaluation of MT quality (Papineni et al., 2002; Yasuda et al., 2001; Akiba et al., 2001). These methods aim to replace subjective evaluation in order to speed up the development cycle of MT systems. However, they can be utilized not only as developers’ aids but also for automatic tuning of MT systems (Su et al., 1992). We propose feedback cleaning that utilizes an automatic evaluation for removing incorrect/redundant translation rules as a tuning method Feedback Cleaning Evaluation Corpus Training Corpus Automatic Acquisition Translation Rules MT Engine MT Results Rule Selection/Deletion Automatic Evaluation Figure 1: Structure of Feedback Cleaning (Figure 1)"
P03-1057,E03-1010,0,0.0111989,"output scores are applicable to feedback cleaning. The characteristics common to these methods, including BLEU, is that the similarity to references are measured for each sentence, and the evaluation score of an MT system is calculated by aggregating the similarities. Therefore, MT results of the evaluation corpus are necessary to evaluate the system, and reducing the number of sentence translations is an important technique for all of these methods. The effects of feedback cleaning depend on the characteristics of objective measures. DP-based measures and BLEU have different characteristics (Yasuda et al., 2003). The exploration of several measures for feedback cleaning remains an interesting future work. 7.2 Domain Adaptation When applying corpus-based machine translation to a different domain, bilingual corpora of the new domain are necessary. However, the sizes of the new corpora are generally smaller than that of the original corpus because the collection of bilingual sentences requires a high cost. The feedback cleaning proposed in this paper can be interpreted as adapting the translation rules so that the MT results become similar to the evaluation corpus. Therefore, if we regard the bilingual"
P03-1057,2001.mtsummit-ebmt.4,0,\N,Missing
P07-2057,A00-2018,0,0.0337142,"tructurally analyzing a sentence from the viewpoint of modification. In Japanese, relationships of modification between phrasal units called bunsetsu segments are analyzed. A number of studies have focused on parsing of Japanese as well as of other languages. Popular parsers are CaboCha (Kudo and Matsumoto, 2002) and KNP (Kurohashi and Nagao, 1994), which were developed to analyze formal written language expressions such as that in newspaper articles. Generally, the syntactic structure of a sentence is represented as a tree, and parsing is carried out by maximizing the likelihood of the tree (Charniak, 2000; Uchimoto et al., 1999). Units that do not modify any other units, such as fillers, are difficult to place in the tree structure. Conventional parsers have forced such independent units to modify other units. Documents published by end users (e.g., blogs) are increasing on the Internet along with the growth 225 of Web 2.0. Such documents do not use controlled written language and contain fillers and emoticons. This implies that analyzing such documents is difficult for conventional parsers. This paper presents a new method of Japanese dependency parsing that utilizes sequential labeling based"
P07-2057,W02-2016,0,0.40304,"ifficult to analyze using conventional parsers. This paper presents dependency parsing whose goal is to analyze Japanese semi-spoken expressions. One characteristic of our method is that it can parse selfdependent (independent) segments using sequential labeling. 1 Introduction Dependency parsing is a way of structurally analyzing a sentence from the viewpoint of modification. In Japanese, relationships of modification between phrasal units called bunsetsu segments are analyzed. A number of studies have focused on parsing of Japanese as well as of other languages. Popular parsers are CaboCha (Kudo and Matsumoto, 2002) and KNP (Kurohashi and Nagao, 1994), which were developed to analyze formal written language expressions such as that in newspaper articles. Generally, the syntactic structure of a sentence is represented as a tree, and parsing is carried out by maximizing the likelihood of the tree (Charniak, 2000; Uchimoto et al., 1999). Units that do not modify any other units, such as fillers, are difficult to place in the tree structure. Conventional parsers have forced such independent units to modify other units. Documents published by end users (e.g., blogs) are increasing on the Internet along with t"
P07-2057,J94-4001,0,\N,Missing
P07-2057,E99-1026,0,\N,Missing
P09-2022,W07-1522,0,0.121103,"also happen in English noun predicates, in which arguments of noun predicates sometimes do not exist in the sentence due to things such as ellipses (Jiang and Ng, 2006). To correctly extract the structures from such sentences, it is necessary to resolve what zero pronouns refer to by using other information such as context. Although predicate-argument structure analysis and zero-anaphora resolution are closely related, it was not until recently that these two tasks were lumped together. Due to the developments of large annotated corpora with predicate-argument and coreference relations (e.g.,(Iida et al., 2007)) 2.1 Procedure and Models The procedure of our predicate-argument structure analyzer is as follows. The input to the analyzer is an article (multiple sentences) because our target is to identify arguments spread across sentences. 1. First, each sentence is individually analyzed and segmented into base phrases by a morphological analyzer and a base phrase chunker. In Japanese, a base phrase is usually constructed by one or more content words (such as base noun phrases) and function words (such as case particles). In addition, dependency relations among base phrases are parsed by a dependency p"
P09-2022,W06-1617,0,0.0186604,"re Analyzer Predicate-argument structure analysis is a type of semantic role labeling, which is an important module to extract event information such as “who did what to whom” from a sentence. There are many arguments called zero pronouns that do not appear in the surface of a sentence in Japanese. In this case, predicate-argument structures cannot be constructed if we only rely on the syntactic information of a single sentence. Similar phenomena also happen in English noun predicates, in which arguments of noun predicates sometimes do not exist in the sentence due to things such as ellipses (Jiang and Ng, 2006). To correctly extract the structures from such sentences, it is necessary to resolve what zero pronouns refer to by using other information such as context. Although predicate-argument structure analysis and zero-anaphora resolution are closely related, it was not until recently that these two tasks were lumped together. Due to the developments of large annotated corpora with predicate-argument and coreference relations (e.g.,(Iida et al., 2007)) 2.1 Procedure and Models The procedure of our predicate-argument structure analyzer is as follows. The input to the analyzer is an article (multiple"
P09-2022,2002.tmi-papers.15,0,0.0874106,"e annotated to almost the same newspaper articles. We divided them into training, development, and test sets as shown in Table 2. 2.3 Usage of Context Centering theory claims that noun phrases that have been used once tend to be used again within the same context. We adopt this claim and add two different kinds of features. One is the feature that indicates whether a candidate has been used as an argument of predicates in the preceding sentences (‘Used’ features). However, the Used features are affected by the accuracy of the previous analyses. Thus, we also adopt the Salience Reference List (Nariyama, 2002), which only uses explicit surface case markers or a topic marker, and added Argument Identification Models: Maximum entropy models were trained using the training set. In these experiments, we used the Gaussian prior, and the variance was tuned using the development set. Candidate argument restrictions were applied during both training and decoding. Language Models: Language models were trained from twelve years of newspaper articles (Mainichi Shinbun newspaper 1991-2002, about 87 Case Nom. Acc. Dat. Type Dep. Zero-Intra Zero-Inter Total Dep. Zero-Intra Zero-Inter Total Dep. Zero-Intra Zero-I"
P09-2022,C08-1097,0,0.165628,"Missing"
P09-2022,D08-1055,0,0.592952,"Missing"
P11-2128,P98-1068,0,0.642164,"Missing"
P11-2128,D08-1106,0,0.0463252,"Missing"
P11-2128,D10-1022,0,0.0257846,"ods that are effective in terms of extraction, even though their clustering target is only the surrounding context. Ritter and Etzioni (2010) proposed a generative approach to use extended LDA to model selectional preferences. Although their approach is similar to ours, our approach is discriminative and so can treat arbitrary features; it is applicable to bootstrapping methods. The accurate selection of negative examples is a major problem for positive and unlabeled learning methods or general bootstrapping methods and some previous works have attempted to reach a solution (Liu et al., 2002; Li et al., 2010). However, their methods are hard to apply to the Bootstrapping algorithms because the positive seed set is too small to accurately select negative examples. Our method uses topic information to efficiently solve both the problem of extracting global information and the problem of selecting negative examples. 6 Conclusion We proposed an approach to set expansion that uses topic information in three modules and showed that it can improve expansion accuracy. The remaining problem is that the grain size of topic models is not always the same as the target domain. To resolve this problem, we will"
P11-2128,P09-1113,0,0.0233032,"s. In Section 2, we illustrate discriminative bootstrapping algorithms and describe their problems. Our proposal is described in Section 3 and experimental results are shown in Section 4. Related works are described in Section 5. Finally, Section 6 provides our conclusion and describes future works. 2 Problems of the previous Discriminative Bootstrapping method Some previous works introduced discriminative methods based on the logistic sigmoid classifier, which can utilize arbitrary features for the relation extraction task instead of a scoring function such as Espresso (Bellare et al., 2006; Mintz et al., 2009). Bellare et al. reported that the discriminative approach achieves better accuracy than Espresso when the number of extracted pairs is increased because 726 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 726–731, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics multiple features are used to support the evidence. However, three problems exist in their methods. First, they use only local context features. The discriminative approach is useful for using arbitrary features, however, they did not identi"
P11-2128,P08-1003,0,0.133341,"Missing"
P11-2128,P06-1015,0,0.17644,"1 Introduction The task of this paper is entity set expansion in which the lexicons are expanded from just a few seed entities (Pantel et al., 2009). For example, the user inputs a few words “Apple”, “Google” and “IBM” , and the system outputs “Microsoft”, “Facebook” and “Intel”. Many set expansion algorithms are based on bootstrapping algorithms, which iteratively acquire new entities. These algorithms suffer from the general problem of “semantic drift”. Semantic drift moves the extraction criteria away from the initial criteria demanded by the user and so reduces the accuracy of extraction. Pantel and Pennacchiotti (2006) proposed Espresso, a relation extraction method based on the co-training bootstrapping algorithm with entities and attributes. Espresso alleviates semanticdrift by a sophisticated scoring system based on ∗ Presently with Okayama Prefectural University pointwise mutual information (PMI). Thelen and Riloff (2002), Ghahramani and Heller (2005) and Sarmento et al. (2007) also proposed original score functions with the goal of reducing semantic-drift. Our purpose is also to reduce semantic drift. For achieving this goal, we use a discriminative method instead of a scoring function and incorporate"
P11-2128,P10-1044,0,0.0402157,"(Japanese musicians) toritomento (treatment), keana (pore), hoshitsu (moisture retention) Table 2: The characteristic words belonging to three topics, zh , zl and ze . zh is the nearest topic and zl is the farthest topic for positive entity-attribute seed pairs. ze is an effective negative topic for eliminating “drifted entities” extracted by the baseline system. are clustering methods based on probabilistic measures. By contrast, Pas¸ca and Durme (2008) proposed clustering methods that are effective in terms of extraction, even though their clustering target is only the surrounding context. Ritter and Etzioni (2010) proposed a generative approach to use extended LDA to model selectional preferences. Although their approach is similar to ours, our approach is discriminative and so can treat arbitrary features; it is applicable to bootstrapping methods. The accurate selection of negative examples is a major problem for positive and unlabeled learning methods or general bootstrapping methods and some previous works have attempted to reach a solution (Liu et al., 2002; Li et al., 2010). However, their methods are hard to apply to the Bootstrapping algorithms because the positive seed set is too small to accu"
P11-2128,P06-1028,0,0.117094,"up as many negative documents as there are positive documents with each selected negative topic being equally represented. 3.4 Candidate Pruning Previous works discriminate all candidates for extracting new entities. Our basic system can constrain 1 z is a random variable whose sample space is represented as a discrete variable, not explicit words. 728 4.1 Experimental Settings We use 30M Japanese blog articles crawled in May 2008. The documents were tokenized by JTAG (Fuchi and Takagi, 1998), chunked, and labeled with IREX 8 Named Entity types by CRFs using Minimum Classification Error rate (Suzuki et al., 2006), and transformed into features. The context features were defined using the template “(head) entity (mid.) attribute (tail)”. The words included in each part were used as surface, part-of-speech and Named Entity label features added position information. Maximum word number of each part was set at 2 words. The features have to appear in both the positive and negative training data at least 5 times. In the experiments, we used three domains, car (“CAR”), broadcast program (“PRG”) and sports organization (“SPT”). The adjustment numbers for basic settings are Ns = 10, Na = 10, Nn = 100. After ru"
P11-2128,W02-1028,0,0.144401,"on bootstrapping algorithms, which iteratively acquire new entities. These algorithms suffer from the general problem of “semantic drift”. Semantic drift moves the extraction criteria away from the initial criteria demanded by the user and so reduces the accuracy of extraction. Pantel and Pennacchiotti (2006) proposed Espresso, a relation extraction method based on the co-training bootstrapping algorithm with entities and attributes. Espresso alleviates semanticdrift by a sophisticated scoring system based on ∗ Presently with Okayama Prefectural University pointwise mutual information (PMI). Thelen and Riloff (2002), Ghahramani and Heller (2005) and Sarmento et al. (2007) also proposed original score functions with the goal of reducing semantic-drift. Our purpose is also to reduce semantic drift. For achieving this goal, we use a discriminative method instead of a scoring function and incorporate topic information into it. Topic information means the genre of each document as estimated by statistical topic models. In this paper, we effectively utilize topic information in three modules: the first generates the features of the discriminative models; the second selects negative examples; the third prunes i"
P11-2128,C98-1065,0,\N,Missing
P11-2128,D09-1098,0,\N,Missing
P12-2076,P07-1033,0,0.0243983,"gainst new sentences because the common features acquired from the source domain can be used even when they do not appear in the target domain. 4 Experiments 4.1 Experimental Settings Although the error generation probabilities are computed from the real-error corpus, the error distribution that results may be inappropriate. To better ﬁt the pseudo-errors to the real-errors, we apply a domain adaptation technique. Namely, we regard the pseudo-error corpus as the source domain and the real-error corpus as the target domain, and models are learnt that ﬁt the target domain. In this paper, we use Daume (2007)’s feature augmentation method for the domain adaptation, which eliminates the need to change the learning algorithm. This method regards the models for the source domain as the prior distribution and learns the models for the target domain. 390 Real-error Corpus: We collected learner’s sentences written by Chinese native speakers. The sentences were created from English Linux manuals and ﬁgures, and Japanese native speakers revised them. From these sentences, only particle errors were retained; the other errors were corrected. As a result, we obtained 2,770 paired sentences. The number of inc"
P12-2076,N10-1019,0,0.0493801,"nted by postpositional particles in Japanese. Incorrect usage of the particles causes serious communication errors because the cases become unclear. For example, in the following sentence, it is unclear what must be deleted. mail o todoi tara sakujo onegai-shi-masu mail ACC. arrive when delete please “When φ has arrived an e-mail, please delete it.” corresponds to preposition/article error correction in English. For English error correction, many studies employ classiﬁers, which select the appropriate prepositions/articles, by restricting the error types to articles and frequent prepositions (Gamon, 2010; Han et al., 2010; Rozovskaya and Roth, 2011). On the contrary, Mizumoto et al. (2011) proposed translator-based error correction. This approach can handle all error types by converting the learner’s sentences into the correct ones. Although the target of this paper is particle error, we employ a similar approach based on sequence conversion (Imamura et al., 2011) since this offers excellent scalability. The conversion approach requires pairs of the learner’s and the correct sentences. However, collecting a sufﬁcient number of pairs is expensive. To avoid this problem, we use additional corpu"
P12-2076,han-etal-2010-using,0,0.0337121,"ositional particles in Japanese. Incorrect usage of the particles causes serious communication errors because the cases become unclear. For example, in the following sentence, it is unclear what must be deleted. mail o todoi tara sakujo onegai-shi-masu mail ACC. arrive when delete please “When φ has arrived an e-mail, please delete it.” corresponds to preposition/article error correction in English. For English error correction, many studies employ classiﬁers, which select the appropriate prepositions/articles, by restricting the error types to articles and frequent prepositions (Gamon, 2010; Han et al., 2010; Rozovskaya and Roth, 2011). On the contrary, Mizumoto et al. (2011) proposed translator-based error correction. This approach can handle all error types by converting the learner’s sentences into the correct ones. Although the target of this paper is particle error, we employ a similar approach based on sequence conversion (Imamura et al., 2011) since this offers excellent scalability. The conversion approach requires pairs of the learner’s and the correct sentences. However, collecting a sufﬁcient number of pairs is expensive. To avoid this problem, we use additional corpus consisting of ps"
P12-2076,I11-1017,0,0.116131,"es causes serious communication errors because the cases become unclear. For example, in the following sentence, it is unclear what must be deleted. mail o todoi tara sakujo onegai-shi-masu mail ACC. arrive when delete please “When φ has arrived an e-mail, please delete it.” corresponds to preposition/article error correction in English. For English error correction, many studies employ classiﬁers, which select the appropriate prepositions/articles, by restricting the error types to articles and frequent prepositions (Gamon, 2010; Han et al., 2010; Rozovskaya and Roth, 2011). On the contrary, Mizumoto et al. (2011) proposed translator-based error correction. This approach can handle all error types by converting the learner’s sentences into the correct ones. Although the target of this paper is particle error, we employ a similar approach based on sequence conversion (Imamura et al., 2011) since this offers excellent scalability. The conversion approach requires pairs of the learner’s and the correct sentences. However, collecting a sufﬁcient number of pairs is expensive. To avoid this problem, we use additional corpus consisting of pseudo-error sentences automatically generated from correct sentences t"
P12-2076,D10-1094,0,0.706454,"error correction. This approach can handle all error types by converting the learner’s sentences into the correct ones. Although the target of this paper is particle error, we employ a similar approach based on sequence conversion (Imamura et al., 2011) since this offers excellent scalability. The conversion approach requires pairs of the learner’s and the correct sentences. However, collecting a sufﬁcient number of pairs is expensive. To avoid this problem, we use additional corpus consisting of pseudo-error sentences automatically generated from correct sentences that mimic the realerrors (Rozovskaya and Roth, 2010b). Furthermore, we apply a domain adaptation technique that regards the pseudo-errors and the real-errors as the source and the target domain, respectively, so that the pseudo-errors better match the real-errors. 2 Error Correction by Discriminative Sequence Conversion If the accusative particle o is replaced by a nominative one ga, it becomes clear that the writer wants to delete the e-mail (“When the e-mail has arrived, please delete it.”). Such particle errors frequently occur in sentences written by non-native Japanese speakers. This paper presents a method that can automatically correct"
P12-2076,N10-1018,0,0.425203,"error correction. This approach can handle all error types by converting the learner’s sentences into the correct ones. Although the target of this paper is particle error, we employ a similar approach based on sequence conversion (Imamura et al., 2011) since this offers excellent scalability. The conversion approach requires pairs of the learner’s and the correct sentences. However, collecting a sufﬁcient number of pairs is expensive. To avoid this problem, we use additional corpus consisting of pseudo-error sentences automatically generated from correct sentences that mimic the realerrors (Rozovskaya and Roth, 2010b). Furthermore, we apply a domain adaptation technique that regards the pseudo-errors and the real-errors as the source and the target domain, respectively, so that the pseudo-errors better match the real-errors. 2 Error Correction by Discriminative Sequence Conversion If the accusative particle o is replaced by a nominative one ga, it becomes clear that the writer wants to delete the e-mail (“When the e-mail has arrived, please delete it.”). Such particle errors frequently occur in sentences written by non-native Japanese speakers. This paper presents a method that can automatically correct"
P12-2076,P11-1093,0,0.104164,"s in Japanese. Incorrect usage of the particles causes serious communication errors because the cases become unclear. For example, in the following sentence, it is unclear what must be deleted. mail o todoi tara sakujo onegai-shi-masu mail ACC. arrive when delete please “When φ has arrived an e-mail, please delete it.” corresponds to preposition/article error correction in English. For English error correction, many studies employ classiﬁers, which select the appropriate prepositions/articles, by restricting the error types to articles and frequent prepositions (Gamon, 2010; Han et al., 2010; Rozovskaya and Roth, 2011). On the contrary, Mizumoto et al. (2011) proposed translator-based error correction. This approach can handle all error types by converting the learner’s sentences into the correct ones. Although the target of this paper is particle error, we employ a similar approach based on sequence conversion (Imamura et al., 2011) since this offers excellent scalability. The conversion approach requires pairs of the learner’s and the correct sentences. However, collecting a sufﬁcient number of pairs is expensive. To avoid this problem, we use additional corpus consisting of pseudo-error sentences automat"
P12-2076,P08-1076,0,0.0197657,"rom a training corpus (paired sentences). Since the feature weights are optimized considering the entire feature space, ﬁne-tuning can be achieved. The accuracy becomes almost perfect on the training corpus. • Language model probability: This is a logarithmic value (real value) of the n-gram probability of the output word sequence. One feature weight is assigned. The n-gram language model can be constructed from a large sentence set because it does not need the learner’s sentences. Incorporating binary and real features yields a rough approximation of generative models in semisupervised CRFs (Suzuki and Isozaki, 2008). It can appropriately correct new sentences while maintaining high accuracy on the training corpus. 3 Pseudo-error Sentences and Domain Adaptation The error corrector described in Section 2 requires paired sentences. However, it is expensive to collect them. We resolve this problem by using pseudoerror sentences and domain adaptation. 3.1 Pseudo-Error Generation Correct sentences, which are halves of the paired sentences, can be easily acquired from corpora such as newspaper articles. Pseudo-errors are generated from them by the substitution, insertion, and deletion functions according to the"
sadamitsu-etal-2012-constructing,W10-3302,0,\N,Missing
sadamitsu-etal-2012-constructing,P98-1068,0,\N,Missing
sadamitsu-etal-2012-constructing,C98-1065,0,\N,Missing
sadamitsu-etal-2012-constructing,P11-1026,0,\N,Missing
sadamitsu-etal-2012-constructing,P06-1028,0,\N,Missing
W09-3535,P06-1028,0,0.0301841,"scheme (Sang and De Meulder, 1999) combined with eight Japanese NE types defined in the IREX workshop (IREX 1999) as shown in Table 1. For example, “ 東 京 (Tokyo)/ 都 (City)/ に (in)” is labeled like this: “東京/B-<LOC&gt; 都/I-<LOC&gt; に/O”. This task is regarded as the sequential tagging problem, i.e., assigning NE tag sequences T = t1 Ltn to word sequences W = w1 L wn . Recently, discriminative models such as Conditional Random Fields (CRFs) have been successfully applied to this task (Lafferty et al., 2001). In this paper, we use linear-chain CRFs based on the Minimum Classification Error framework (Suzuki et al., 2006). The posterior probability of a tag sequence is calculated as follows: P (T |W ) = n 1 exp{ (Σ λ a ⋅ f a (t i , wi ) a Z (W ) i =1 (1) + Σ λ b ⋅ f b (t i −1 , t i ))}, Σ b where wi and ti are the i-th word and its corresponding NE tag, respectively. f a (t i , wi ) and f b (t i −1 , t i ) is a feature function 3 . λa and λb is a parameter to be estimated from the training data. Z(W) is a normalization factor over all candidate paths expressed as follows: Z (W ) = T A base model is the initial model trained with the initial annotated corpora. Σ (Σ λ i =1 a a ⋅ f a (t i , wi ) + Σ λb ⋅ f b (t i"
W09-3535,P01-1005,0,0.0326254,"Missing"
W09-3535,P95-1026,0,0.225773,"Missing"
W09-3535,W00-1306,0,0.0938364,"Missing"
W09-3535,C08-1059,0,0.0911436,"ive resource for information retrieval and information extraction tasks. CGM has two distinctive features; enormous quantities of new texts are generated day after day, and new vocabularies and topics come and go rapidly. The most effective approach to keep up with new linguistic phenomena is creating new annotated corpora for model re-training at short intervals. However, it is difficult to build new corpora expeditiously because of the high manual costs imposed by traditional schemes. To reduce the manual labor and costs, various learning methods, such as active learning (Shen et al., 2004, Laws and Schütze, 2008), semi-supervised learning (Suzuki and Isozaki, 2008) and bootstrapping (Etzioni, 2005) have been proposed. Active learning automatically selects effective texts to be annotated from huge raw-text corpora. The correct answers are then manually annotated, and the model is re-trained. In active learning, one major issue is data selection, namely, determining which sample data is most effective. The data units used in conventional methods are sentences. Automatically creating annotated corpora would dramatically decrease the manual costs. In fact, there always are some recognition errors in any a"
W09-3535,P06-1015,0,0.0212555,"Missing"
W09-3535,E99-1023,0,0.10773,"Missing"
W09-3535,P04-1075,0,0.177351,"ntly as an informative resource for information retrieval and information extraction tasks. CGM has two distinctive features; enormous quantities of new texts are generated day after day, and new vocabularies and topics come and go rapidly. The most effective approach to keep up with new linguistic phenomena is creating new annotated corpora for model re-training at short intervals. However, it is difficult to build new corpora expeditiously because of the high manual costs imposed by traditional schemes. To reduce the manual labor and costs, various learning methods, such as active learning (Shen et al., 2004, Laws and Schütze, 2008), semi-supervised learning (Suzuki and Isozaki, 2008) and bootstrapping (Etzioni, 2005) have been proposed. Active learning automatically selects effective texts to be annotated from huge raw-text corpora. The correct answers are then manually annotated, and the model is re-trained. In active learning, one major issue is data selection, namely, determining which sample data is most effective. The data units used in conventional methods are sentences. Automatically creating annotated corpora would dramatically decrease the manual costs. In fact, there always are some re"
W09-3535,P08-1076,0,0.0298329,"Missing"
W10-3709,I08-2094,1,0.883832,"Missing"
W10-3709,W04-0405,0,\N,Missing
W16-4611,N12-1047,0,0.0149631,"ulti-threaded GIZA++ for word alignment. For the language models of the corpus-concatenated and single-domain models, we constructed 5gram models from the target side of the bilingual corpora using KenLM (Heafield et al., 2013). In addition, we included the Google n-gram language models for Japanese and English as the external knowledge. These are back-off models estimated using maximum likelihood. The Japanese model was constructed from Web Japanese N-gram Version 1,3 and the English model was constructed from Web 1T 5-gram Version 1 (LDC2006T13). For optimization, we used k-best batch MIRA (Cherry and Foster, 2012). 3.3 Translation The decoder used here is a clone of the Moses PBSMT decoder. It accepts feature augmentation, i.e., it can use multiple submodels and set an empty value. 2 This preorderer modifies word order based on parse trees output by the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007). 3 http://www.gsk.or.jp/catalog/gsk2007-c/ 129 Method Single-Domain Model Corpus Concatenation Domain Adaptation Ja-En 34.58 35.64 35.68 JPC En-Ja Ja-Zh 38.06 33.35 38.61 34.27 39.03 34.64 Zh-Ja 39.54 40.96 41.09 Table 3: BLEU Scores on JPO Corpus (official scores) Method Single-Domain Model"
W16-4611,P11-2031,0,0.0123879,"34.64 Zh-Ja 39.54 40.96 41.09 Table 3: BLEU Scores on JPO Corpus (official scores) Method Single-Domain Model Corpus Concatenation Domain Adaptation Ja-En 35.12(-) 36.22 36.29 JPC En-Ja Ja-Zh 37.40(-) 31.96(-) 38.03(-) 32.92(-) 38.48 33.36 Zh-Ja 38.15(-) 39.68(-) 39.85 Table 4: BLEU Scores on JPO Corpus (MultEval scores) 4 Experimental Results For evaluation, we used two toolkits based on BLEU (Papineni et al., 2002). One is the official BLEU scores provided by the WAT2016 committee. Because the official tool cannot measure a significance level of two systems, we also used the MultEval tool (Clark et al., 2011), which can measure significance levels based on bootstrap resampling. Since we represent the mean scores of three optimizations, the MultEval scores differ from the official scores. 4.1 JPO Corpus (without External Knowledge) For JPO corpus experiments, we did not use external knowledge and compared translations of the singledomain model, corpus concatenation, and domain adaptation. The JPO corpus was divided into four domains (chemistry, electricity, machine, and physics). Tables 3 and 4 show the results evaluated by the official scorer and MultEval tools, respectively. The symbol (-) indica"
W16-4611,P07-1033,0,0.175276,"Missing"
W16-4611,P13-2121,0,0.0452534,"n Grammar (TDBTG) trained by the JPO corpus as the preorderer without external knowledge. For the preorderer with external knowledge, we used the one developed in-house (Chapter 4.5 of Goto et al. (2015)),2 which was tuned for patent translation. 3.2 Training and Optimization We used the Moses toolkit (Koehn et al., 2007) to train the phrase tables and lexicalized reordering models. We used multi-threaded GIZA++ for word alignment. For the language models of the corpus-concatenated and single-domain models, we constructed 5gram models from the target side of the bilingual corpora using KenLM (Heafield et al., 2013). In addition, we included the Google n-gram language models for Japanese and English as the external knowledge. These are back-off models estimated using maximum likelihood. The Japanese model was constructed from Web Japanese N-gram Version 1,3 and the English model was constructed from Web 1T 5-gram Version 1 (LDC2006T13). For optimization, we used k-best batch MIRA (Cherry and Foster, 2012). 3.3 Translation The decoder used here is a clone of the Moses PBSMT decoder. It accepts feature augmentation, i.e., it can use multiple submodels and set an empty value. 2 This preorderer modifies word"
W16-4611,2016.amta-researchers.7,1,0.899683,"tem employs a domain adaptation method based on feature augmentation. We regarded the Japan Patent Office Corpus as a mixture of four domain corpora and improved the translation quality of each domain. In addition, we incorporated language models constructed from Google n-grams as external knowledge. Our domain adaptation method can naturally incorporate such external knowledge that contributes to translation quality. 1 Introduction In this paper, we describe the NICT-2 translation system for the 3rd Workshop on Asian Translation (WAT2016) (Nakazawa et al., 2016a). The proposed system employs Imamura and Sumita (2016)’s domain adaptation technique, which improves translation quality using other domain data when the target domain data is insufficient. The method employed in this paper assumes multiple domains and improves the quality inside the domains (cf., Section 2). For WAT2016, the Japan Patent Office (JPO) Corpus can be regarded as multi-domain data because it includes chemistry, electricity, machine, and physics patents with their domain ID, and thus it is suitable for observing the effects of domain adaptation. WAT2016 provides the JPO corpora in Japanese and English (Ja-En), Japanese and Chinese (J"
W16-4611,N03-1017,0,0.0372434,"wc and wi denote subvectors of the weight vector w. Φc (e, f ) and Φi (e, f ) are feature functions that return feature subvectors (cf., Section 2.2). 2 Domain Adaptation We used the domain adaptation method proposed by Imamura and Sumita (2016). This method adapts a weight vector by feature augmentation (Daum´e, 2007) and a feature vector using a corpus-concatenated model. Since this method only operates in feature space, it can be applied to various translation strategies, such as tree-to-tree translation. In this study, we applied it to phrase-based statistical machine translation (PBSMT) (Koehn et al., 2003; Koehn et al., 2007). 2.1 Adaptation of Weight Vector by Feature Augmentation Most statistical machine translation employs log-linear models that interpolate feature function values obtained from various submodels, such as phrase tables and language models (LMs). The likelihood of a translation is computed as follows: log P (e|f ) ∝ w · h(e, f ), (1) where h(e, f ) denotes a feature vector and w denotes its weight vector. Figure 1 shows a feature space structure of feature augmentation. When we translate texts of D domains, the feature space is segmented into D + 1 subspaces: common, domain 1"
W16-4611,P07-2045,0,0.0275413,"vectors of the weight vector w. Φc (e, f ) and Φi (e, f ) are feature functions that return feature subvectors (cf., Section 2.2). 2 Domain Adaptation We used the domain adaptation method proposed by Imamura and Sumita (2016). This method adapts a weight vector by feature augmentation (Daum´e, 2007) and a feature vector using a corpus-concatenated model. Since this method only operates in feature space, it can be applied to various translation strategies, such as tree-to-tree translation. In this study, we applied it to phrase-based statistical machine translation (PBSMT) (Koehn et al., 2003; Koehn et al., 2007). 2.1 Adaptation of Weight Vector by Feature Augmentation Most statistical machine translation employs log-linear models that interpolate feature function values obtained from various submodels, such as phrase tables and language models (LMs). The likelihood of a translation is computed as follows: log P (e|f ) ∝ w · h(e, f ), (1) where h(e, f ) denotes a feature vector and w denotes its weight vector. Figure 1 shows a feature space structure of feature augmentation. When we translate texts of D domains, the feature space is segmented into D + 1 subspaces: common, domain 1, · · · domain D. A f"
W16-4611,P15-1021,0,0.0169397,"Moses Toolkit Moses Toolkit - Table 2: Summary of Preprocessing, Training, and Translation Optimization Imamura and Sumita (2016) proposed joint optimization and independent optimization. We employ independent optimization, which can use existing optimizers. 3 System Description In this section, we describe the preprocessing, training, and translation components of the proposed system (Table 2). 3.1 Preprocessing Preprocessing is nearly the same as the baseline system provided by the WAT2016 committee. However, preorderers are added because our system is phrase-based with preordering. We used Nakagawa (2015)’s Top-Down Bracketing Transduction Grammar (TDBTG) trained by the JPO corpus as the preorderer without external knowledge. For the preorderer with external knowledge, we used the one developed in-house (Chapter 4.5 of Goto et al. (2015)),2 which was tuned for patent translation. 3.2 Training and Optimization We used the Moses toolkit (Koehn et al., 2007) to train the phrase tables and lexicalized reordering models. We used multi-threaded GIZA++ for word alignment. For the language models of the corpus-concatenated and single-domain models, we constructed 5gram models from the target side of t"
W16-4611,W16-4601,0,0.0611743,"Missing"
W16-4611,L16-1350,1,0.872835,"Missing"
W16-4611,P02-1040,0,0.0945046,"v and Klein, 2007). 3 http://www.gsk.or.jp/catalog/gsk2007-c/ 129 Method Single-Domain Model Corpus Concatenation Domain Adaptation Ja-En 34.58 35.64 35.68 JPC En-Ja Ja-Zh 38.06 33.35 38.61 34.27 39.03 34.64 Zh-Ja 39.54 40.96 41.09 Table 3: BLEU Scores on JPO Corpus (official scores) Method Single-Domain Model Corpus Concatenation Domain Adaptation Ja-En 35.12(-) 36.22 36.29 JPC En-Ja Ja-Zh 37.40(-) 31.96(-) 38.03(-) 32.92(-) 38.48 33.36 Zh-Ja 38.15(-) 39.68(-) 39.85 Table 4: BLEU Scores on JPO Corpus (MultEval scores) 4 Experimental Results For evaluation, we used two toolkits based on BLEU (Papineni et al., 2002). One is the official BLEU scores provided by the WAT2016 committee. Because the official tool cannot measure a significance level of two systems, we also used the MultEval tool (Clark et al., 2011), which can measure significance levels based on bootstrap resampling. Since we represent the mean scores of three optimizations, the MultEval scores differ from the official scores. 4.1 JPO Corpus (without External Knowledge) For JPO corpus experiments, we did not use external knowledge and compared translations of the singledomain model, corpus concatenation, and domain adaptation. The JPO corpus"
W16-4611,P06-1055,0,0.0716582,"Missing"
W16-4611,N07-1051,0,\N,Missing
W17-5711,W08-0336,0,0.0134164,"enabled right-to-left decoding in the trainer and translator. bi-directional reranking 15.0 0 5 10 15 20 N-best Size (=Beam Width) Figure 2: BLEU Scores According to N-best Size The n-best size for the reranking was determined by the experiment in Section 3.2. Evaluation: Of the WAT official evaluation metrics, we employ BLEU (Papineni et al., 2002) for the evaluation. WAT official scores are changed by word segmenters. In this paper, we use JUMAN (Kurohashi et al., 1994) for Japanese, Moses tokenizer (Koehn et al., 2007) for English, and Stanford Word Segmenter (Chinese Penn Treebank Model) (Chang et al., 2008) for Chinese evaluation. In all methods, the BLEU scores changed according to the size of the n-best list. For left-toright and right-to-left decoding, the BLEU scores were highest when the n-best size was 4, and the scores decreased when the n-best size increased above 4. After the bi-directional reranking, the BLEU score was the highest when the n-best size was 5, and slowly decreased when the size increased above 5. 3.2 Optimal Size of N-best List To output n-best translations using the beam search, beam width is better to set equal or more than n. In our experiments, we set the beam width"
W17-5711,P07-2045,0,0.0174661,"6.5 16.0 left-to-right right-to-left 15.5 • We enabled the ensemble in the translator. • We enabled right-to-left decoding in the trainer and translator. bi-directional reranking 15.0 0 5 10 15 20 N-best Size (=Beam Width) Figure 2: BLEU Scores According to N-best Size The n-best size for the reranking was determined by the experiment in Section 3.2. Evaluation: Of the WAT official evaluation metrics, we employ BLEU (Papineni et al., 2002) for the evaluation. WAT official scores are changed by word segmenters. In this paper, we use JUMAN (Kurohashi et al., 1994) for Japanese, Moses tokenizer (Koehn et al., 2007) for English, and Stanford Word Segmenter (Chinese Penn Treebank Model) (Chang et al., 2008) for Chinese evaluation. In all methods, the BLEU scores changed according to the size of the n-best list. For left-toright and right-to-left decoding, the BLEU scores were highest when the n-best size was 4, and the scores decreased when the n-best size increased above 4. After the bi-directional reranking, the BLEU score was the highest when the n-best size was 5, and slowly decreased when the size increased above 5. 3.2 Optimal Size of N-best List To output n-best translations using the beam search,"
W17-5711,P17-4012,0,0.017969,"Japanese and then translated into English. The byte-pair encoding (Sennrich et al., 2016) rules were acquired from a training set of each corpus, and they were applied to the training, development, and test sets. The number of sub-word types is 34–35k in the JIJI Corpus and 20–21k in the MED Corpus. We used sentences with 80 or fewer sub-words for training. Preprocessing, Postprocessing: Table 3 shows a summary of our system. As shown in the table, we used the same preprocessing and postprocessing steps as the WAT baseline systems (Nakazawa et al., 2016a). Translation System: We used OpenNMT (Klein et al., 2017)1 as the base translation system. The encoder comprises a two-layer bi-directional LSTM (long short-term memory), in which the number of units is 500 each. The decoder comprises a two-layer LSTM (1000 Experiments Using Small Data Sets We perform Japanese-English translation experiments using small data (with approximately 200k sentences) to clarify characteristics of the ensemble and the bi-directional reranking approaches. 1 129 http://opennmt.net/ Preprocessing Training and Translation Postprocessing Character Normalization Tokenizer TrueCaser Byte Pair Encoding System Encoder Decoder Attent"
W17-5711,W04-3230,0,0.251116,"Missing"
W17-5711,N16-1046,1,0.898576,"Missing"
W17-5711,D15-1166,0,0.0281317,"Tokenizer Japanese English Chinese NFKC Normalization of Unicode MeCab (Kudo Moses Toolkit Stanford Segmenter (CTB) et al., 2004) – Moses Toolkit – In-house Encoder OpenNMT (modified for right-to-left decoding and the ensemble method) Word embedding: 500 units, two-layer Bi-LSTM (500 + 500 units) Word embedding: 500 units, two-layer LSTM (1,000 units) Global Attention Mini Batch Size:64, SGD Optimization (10+6 epochs), Dropout:0.3 Beam Width:5 (c.f., Sec. 3.2) – Moses Toolkit – WAT Official’s Moses Toolkit WAT Official’s Table 3: Summary of the NICT-2 NMT System 18.0 units). Global Attention (Luong et al., 2015) was utilized. We used the stochastic gradient descent (SGD) method for the optimization. The learning rate was 1.0 for the first ten epochs, and then annealing was performed for six epochs while decreasing the learning rate by half. To implement the methods described in Section 2.3, we modified OpenNMT as follows. 17.5 BLEU Score 17.0 16.5 16.0 left-to-right right-to-left 15.5 • We enabled the ensemble in the translator. • We enabled right-to-left decoding in the trainer and translator. bi-directional reranking 15.0 0 5 10 15 20 N-best Size (=Beam Width) Figure 2: BLEU Scores According to N-b"
W17-5711,W16-4601,0,0.0305515,"Missing"
W17-5711,L16-1350,1,0.890132,"Missing"
W17-5711,N04-1021,0,0.141948,"Missing"
W17-5711,P02-1040,0,0.0980273,"was performed for six epochs while decreasing the learning rate by half. To implement the methods described in Section 2.3, we modified OpenNMT as follows. 17.5 BLEU Score 17.0 16.5 16.0 left-to-right right-to-left 15.5 • We enabled the ensemble in the translator. • We enabled right-to-left decoding in the trainer and translator. bi-directional reranking 15.0 0 5 10 15 20 N-best Size (=Beam Width) Figure 2: BLEU Scores According to N-best Size The n-best size for the reranking was determined by the experiment in Section 3.2. Evaluation: Of the WAT official evaluation metrics, we employ BLEU (Papineni et al., 2002) for the evaluation. WAT official scores are changed by word segmenters. In this paper, we use JUMAN (Kurohashi et al., 1994) for Japanese, Moses tokenizer (Koehn et al., 2007) for English, and Stanford Word Segmenter (Chinese Penn Treebank Model) (Chang et al., 2008) for Chinese evaluation. In all methods, the BLEU scores changed according to the size of the n-best list. For left-toright and right-to-left decoding, the BLEU scores were highest when the n-best size was 4, and the scores decreased when the n-best size increased above 4. After the bi-directional reranking, the BLEU score was the"
W17-5711,P16-1162,0,0.0416405,"as small data sets. The first is the JIJI Corpus, which consists of newswires. Japanese and English articles were automatically aligned sentence by sentence. Note that the translations are sometimes not literal because the original articles were not translated sentence by sentence. The second is the corpus of pseudo-dialogues at hospitals (MED Corpus). This corpus is a collection of conversations between patients and hospital staffs, which were created by writers (developed in-house). The pseudo-dialogues were first written in Japanese and then translated into English. The byte-pair encoding (Sennrich et al., 2016) rules were acquired from a training set of each corpus, and they were applied to the training, development, and test sets. The number of sub-word types is 34–35k in the JIJI Corpus and 20–21k in the MED Corpus. We used sentences with 80 or fewer sub-words for training. Preprocessing, Postprocessing: Table 3 shows a summary of our system. As shown in the table, we used the same preprocessing and postprocessing steps as the WAT baseline systems (Nakazawa et al., 2016a). Translation System: We used OpenNMT (Klein et al., 2017)1 as the base translation system. The encoder comprises a two-layer bi"
W18-2707,P16-1185,0,0.0450977,"17) generated synthetic parallel sentences by copying target sentences to the source. This method utilizes a feature in which some words, such as named entities, are often identical across the source and target languages and do not require translation. However, this method provides no benefits to language pairs having different character sets, such as English and Japanese. On the other hand, the basis of source monolingual corpora, a pre-training method based on an autoencoder has been proposed to enhance the encoder (Zhang and Zong, 2016). However, the decoder is not enhanced by this method. Cheng et al. (2016) trained two autoencoders using source and target monolingual corpora, while translation models are trained using a parallel corpus. This method enhances both the encoder and decoder, but it requires two monolingual corpora, respectively. Our proposed method enhances not only the decoder but also the encoder and attention using target monolingual corpora. 3 Output Word Output Word Sampling Sampling Word Distribution Generator & Attention Mech. Word Distribution Generator & Attention Mech. Contexts Contexts LSTM States LSTM Figure 2: Decoding Process of Back-Translator by the back-translator to"
W18-2707,P11-2031,0,0.181239,"Missing"
W18-2707,P16-1009,0,0.612859,"ro Sumita National Institute of Information and Communications Technology 3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289, Japan {kenji.imamura,atsushi.fujita,eiichiro.sumita}@nict.go.jp Abstract coder is accurately trained because the target side of the synthetic parallel texts consists of manually created (correct) sentences. Consequently, this method provides steady improvements. However, this approach may not contribute to the improvement of the encoder because the source side of the synthetic parallel texts are automatically generated. In this paper, we extend the method proposed by Sennrich et al. (2016a) to enhance the encoder and attention using target monolingual corpora. Our proposed method generates multiple source sentences by sampling when each target sentence is translated back. By using multiple source sentences, we aim to achieve the following. A large-scale parallel corpus is required to train encoder-decoder neural machine translation. The method of using synthetic parallel texts, in which target monolingual corpora are automatically translated into source sentences, is effective in improving the decoder, but is unreliable for enhancing the encoder. In this paper, we propose a me"
W18-2707,W17-4715,0,0.0716399,"lator Target → Source Synthetic Source Sentences Base Parallel Corpus Training (Forward) Translator Source → Target Training Filter Test Sentence Synthetic Parallel Corpus Translation Figure 1: Flow of Our Approach methods only enhance the decoder and require a modification of the NMT. Another approach of using monolingual corpora of the target language is to learn models using synthetic parallel sentences. The method of Sennrich et al. (2016a) generates synthetic parallel corpora through back-translation and learns models from such corpora. Our proposed method is an extension of this method. Currey et al. (2017) generated synthetic parallel sentences by copying target sentences to the source. This method utilizes a feature in which some words, such as named entities, are often identical across the source and target languages and do not require translation. However, this method provides no benefits to language pairs having different character sets, such as English and Japanese. On the other hand, the basis of source monolingual corpora, a pre-training method based on an autoencoder has been proposed to enhance the encoder (Zhang and Zong, 2016). However, the decoder is not enhanced by this method. Che"
W18-2707,P16-1162,0,0.855078,"ro Sumita National Institute of Information and Communications Technology 3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289, Japan {kenji.imamura,atsushi.fujita,eiichiro.sumita}@nict.go.jp Abstract coder is accurately trained because the target side of the synthetic parallel texts consists of manually created (correct) sentences. Consequently, this method provides steady improvements. However, this approach may not contribute to the improvement of the encoder because the source side of the synthetic parallel texts are automatically generated. In this paper, we extend the method proposed by Sennrich et al. (2016a) to enhance the encoder and attention using target monolingual corpora. Our proposed method generates multiple source sentences by sampling when each target sentence is translated back. By using multiple source sentences, we aim to achieve the following. A large-scale parallel corpus is required to train encoder-decoder neural machine translation. The method of using synthetic parallel texts, in which target monolingual corpora are automatically translated into source sentences, is effective in improving the decoder, but is unreliable for enhancing the encoder. In this paper, we propose a me"
W18-2707,D17-1158,0,0.0425771,"nd, monolingual corpora are readily available in large quantities. Sennrich et al. (2016a) proposed a method using synthetic parallel texts, in which target monolingual corpora are translated back into the source language (Figure 1). The advantage of this method is that the de2 Related Work One approach of using target monolingual corpora is to construct a recurrent neural network language model and combine the model with the decoder (G¨ulc¸ehere et al., 2015; Sriram et al., 2017). Similarly, there is a method of training language models, jointly with the translator, using multitask learning (Domhan and Hieber, 2017). These 55 Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 55–63 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics Target Monolingual Corpus Back-Translator Target → Source Synthetic Source Sentences Base Parallel Corpus Training (Forward) Translator Source → Target Training Filter Test Sentence Synthetic Parallel Corpus Translation Figure 1: Flow of Our Approach methods only enhance the decoder and require a modification of the NMT. Another approach of using monolingual corpora of the target language is to learn models us"
W18-2707,W16-2392,0,0.0290266,"r not. We train the support vector machines (SVMs) on the sentence-level data and regard the classifier’s score as the confidence score. The features of the SVM classifier include the 17 basic features of QuEst++ (Specia et al., 2015).4 They are roughly categorized into the following two types. • Language model features of each of the source and target sentences. • Features based on the parallel sentences such as the average number of translation hypotheses per word. In addition, we add the source and target word embeddings. The sentence features are computed by averaging all word embeddings (Shah et al., 2016). The hyperparameters for the training are set using the grid search on the development set. In the expriments of Section 4, features are extracted from the base parallel corpus. 4 Translation System The translation system used in this study was OpenNMT (Klein et al., 2017). We modified it to accept Sections 3.1 and 3.2. 5 http://www.quest.dcs.shef.ac.uk/ 58 http://pj.ninjal.ac.jp/corpus center/bccwj/en/ The encoder was comprised of a two-layer BiLSTM (500 + 500 units), the decoder included a two-layer LSTM (1,000 units), and the stochastic gradient descent was used for optimization. The learn"
W18-2707,W17-5705,1,0.838981,"nces, each of which contains less than 1024 characters. We assume practical situations in which the domains of parallel and monolingual corpora are not identical. All sentences were segmented into words using an in-house word segmenter. The words were further segmented into 16K sub-words based on the byte-pair encoding rules (Sennrich et al., 2016b) acquired from the base parallel corpus for each language independently. 3.3.2 Confidence Filtering The second method involves filtering with the confidence of translation used in the translation quality estimation task. We use the data provided by Fujita and Sumita (2017), which is a collection of manual labels indicating whether the translation is acceptable or not. We train the support vector machines (SVMs) on the sentence-level data and regard the classifier’s score as the confidence score. The features of the SVM classifier include the 17 basic features of QuEst++ (Specia et al., 2015).4 They are roughly categorized into the following two types. • Language model features of each of the source and target sentences. • Features based on the parallel sentences such as the average number of translation hypotheses per word. In addition, we add the source and ta"
W18-2707,P16-1159,0,0.0282914,"t changed. It must be noted that if the domains of the base parallel and the target monolingual corpora are different, it is better to perform “further training” using the base parallel corpus for domain adaptation (Freitag and Al-Onaizan, 2016; Servan et al., 2016). 3 (2) yt where samplingy (P ) denotes the sampling operation of y based on the probability distribution P . The decoding continues until the end-of-sentence symbol is generated.2 We repeat the above process to generate multiple synthetic sentences. Note that this generation method is the same as that of the minimum risk training (Shen et al., 2016). In NMT, even if a low-probability word is selected by the sampling, the subsequent word would become fluent because it is conditioned by the history. Table 1 presents examples of the synthetic source sentences produced by the back-translator. Most of the synthetic source sentences are identical, or close to, the manual backtranslation (i.e., the reference translation). On the other hand, the last example is quite different from the perspective of word order because the clauses are inverted. Such a synthetic sentence is usually not produced by the n-best translation because of the low likelih"
W18-2707,P15-4020,0,0.0128306,"ennrich et al., 2016b) acquired from the base parallel corpus for each language independently. 3.3.2 Confidence Filtering The second method involves filtering with the confidence of translation used in the translation quality estimation task. We use the data provided by Fujita and Sumita (2017), which is a collection of manual labels indicating whether the translation is acceptable or not. We train the support vector machines (SVMs) on the sentence-level data and regard the classifier’s score as the confidence score. The features of the SVM classifier include the 17 basic features of QuEst++ (Specia et al., 2015).4 They are roughly categorized into the following two types. • Language model features of each of the source and target sentences. • Features based on the parallel sentences such as the average number of translation hypotheses per word. In addition, we add the source and target word embeddings. The sentence features are computed by averaging all word embeddings (Shah et al., 2016). The hyperparameters for the training are set using the grid search on the development set. In the expriments of Section 4, features are extracted from the base parallel corpus. 4 Translation System The translation"
W18-2707,L18-1545,1,0.800428,"h an appropriate value, we can obtain synthetic sentences that are almost of the same length as the manual back-translation. We set the word penalty such that the lengths of the translation and reference translation on the development set are approximately equal, using line search. 3.3.3 Random Filtering The third method is random filtering. This is identical to the reduction of the number of synthetic source sentences to be generated. 4 Experiments 4.1 Experimental Settings Corpora The corpus sizes used here are shown in Table 2. We used the global communication plan corpus (the GCP corpus, (Imamura and Sumita, 2018)), which is an in-house parallel corpus of daily life conversations and consists of Japanese (Ja), English (En), and Chinese (Zh). The experiments were performed on Englishto-Japanese and Chinese-to-Japanese translation tasks. We randomly selected 400K sentences for the base parallel corpus, and the remaining (1.55M sentences) were used as the Japanese monolingual corpus. The reason for dividing the parallel corpus into two corpora is to measure the upper-bound of quality improvement by using existing parallel texts on the same domain as the manual backtranslation. We also used the Balanced Co"
W18-2707,P17-4012,0,0.0440349,"e following two types. • Language model features of each of the source and target sentences. • Features based on the parallel sentences such as the average number of translation hypotheses per word. In addition, we add the source and target word embeddings. The sentence features are computed by averaging all word embeddings (Shah et al., 2016). The hyperparameters for the training are set using the grid search on the development set. In the expriments of Section 4, features are extracted from the base parallel corpus. 4 Translation System The translation system used in this study was OpenNMT (Klein et al., 2017). We modified it to accept Sections 3.1 and 3.2. 5 http://www.quest.dcs.shef.ac.uk/ 58 http://pj.ninjal.ac.jp/corpus center/bccwj/en/ The encoder was comprised of a two-layer BiLSTM (500 + 500 units), the decoder included a two-layer LSTM (1,000 units), and the stochastic gradient descent was used for optimization. The learning rate for the base parallel corpus was 1.0 for the first 14 epochs, followed by the annealing of 6 epochs while decreasing the learning rate by half. The mini-batch size was 64. At the translation stage, we generated 10-best translations and selected the best among them"
W18-2707,W17-5706,0,0.157005,"Development Test Monolingual GCP Corpus (Japanese) BCCWJ out. Note that the likelihood is corrected with the length of the synthetic source sentence. We call this the length biased log-likelihood lllen (Oda et al., 2017). lllen (y|x) = ∑ Parallel log Pr(yt |x, y&lt;t )+W P ·T, (3) # Sentences 400,000 2,000 2,000 1,552,475 4,791,336 Table 2: Corpus Statistics t where the first term on the right-hand side is the log-likelihood, W P denotes the word penalty (W P ≥ 0), and T denotes the number of words in the synthetic source sentence. NMTs tend to generate shorter translations than the expectation (Morishita et al., 2017). The word penalty works to increase the likelihood of long hypotheses when it is set to a positive value. With an appropriate value, we can obtain synthetic sentences that are almost of the same length as the manual back-translation. We set the word penalty such that the lengths of the translation and reference translation on the development set are approximately equal, using line search. 3.3.3 Random Filtering The third method is random filtering. This is identical to the reduction of the number of synthetic source sentences to be generated. 4 Experiments 4.1 Experimental Settings Corpora Th"
W18-2707,D16-1160,0,0.0383788,"rpora. Our proposed method is an extension of this method. Currey et al. (2017) generated synthetic parallel sentences by copying target sentences to the source. This method utilizes a feature in which some words, such as named entities, are often identical across the source and target languages and do not require translation. However, this method provides no benefits to language pairs having different character sets, such as English and Japanese. On the other hand, the basis of source monolingual corpora, a pre-training method based on an autoencoder has been proposed to enhance the encoder (Zhang and Zong, 2016). However, the decoder is not enhanced by this method. Cheng et al. (2016) trained two autoencoders using source and target monolingual corpora, while translation models are trained using a parallel corpus. This method enhances both the encoder and decoder, but it requires two monolingual corpora, respectively. Our proposed method enhances not only the decoder but also the encoder and attention using target monolingual corpora. 3 Output Word Output Word Sampling Sampling Word Distribution Generator & Attention Mech. Word Distribution Generator & Attention Mech. Contexts Contexts LSTM States LS"
W18-2707,W17-5712,1,0.825262,"hood Filtering The first method is filtering by the likelihood output from the back-translator. We consider the likelihood as an indicator of translation quality, and low-likelihood synthetic sentences are filtered 2 The back-translator does not use the beam search because the sampling is independently performed for each word. 3 57 We did not perform “further training” in this paper. Type Base Development Test Monolingual GCP Corpus (Japanese) BCCWJ out. Note that the likelihood is corrected with the length of the synthetic source sentence. We call this the length biased log-likelihood lllen (Oda et al., 2017). lllen (y|x) = ∑ Parallel log Pr(yt |x, y&lt;t )+W P ·T, (3) # Sentences 400,000 2,000 2,000 1,552,475 4,791,336 Table 2: Corpus Statistics t where the first term on the right-hand side is the log-likelihood, W P denotes the word penalty (W P ≥ 0), and T denotes the number of words in the synthetic source sentence. NMTs tend to generate shorter translations than the expectation (Morishita et al., 2017). The word penalty works to increase the likelihood of long hypotheses when it is set to a positive value. With an appropriate value, we can obtain synthetic sentences that are almost of the same l"
W18-2707,D16-1163,0,0.0229488,"ic source sentences and the BLEU score on the GCP corpus of En-Ja and Zh-Ja translation tasks, respectively. The graphs and tables in the figures present the same data for overviews and for analyzing the data in detail. Note that the method of Sennrich et al. (2016a) corresponds to the case of one synthetic source 6 4.3 Results with BCCWJ Table 3 shows the results using BCCWJ as a monolingual corpus (the results of the GCP cor7 Unfortunately, it is unknown in this experiment whether the encoder or attention were enhanced. We plan to investigate which module is enhanced by freezing parameters (Zoph et al., 2016) of the encoder and attention through the training. https://github.com/jhclark/multeval 59 32.0 Manual Back-Translation 31.0 BLEU 30.0 29.0 28.0 Sennrich et al. (2016) Likelihood Filtering Confidence Filtering Random Filtering N-best Generation 27.0 26.0 Base Corpus Only 25.0 0 2 4 6 8 10 Number of Synthetic Source Sentences # of Synthetic Sentences Base Corpus Only Sennrich et al. (2016a) 1 2 4 6 8 10 Manual Back-Translation En-Ja Confidence Random Filtering Filtering 26.19 28.61 (+2.42) 28.49 (+2.30) 28.85 (+2.66) 29.26 (+3.07) 30.30 (+4.11) 30.26 (+4.07) 30.08 (+3.89) 30.59 (+4.40) 30.27 (+"
W18-2707,P02-1040,0,0.100513,"Missing"
W18-2713,1983.tc-1.13,0,0.75701,"Missing"
W18-2713,W17-5706,0,0.0169257,"eal that the length ratio should be constant for fair comparison when we compare different systems because they generate translations of different lengths. Therefore, we compare different models and settings by tuning the word penalty to maintain the stable length ratio on the development set (newstest2013). In this experiment, we show results of the two length ratios based on the “original parallel corpus only” of the transformer model. Note that the submitted system employs the first setting. 4.1 Word Penalty / Length Ratio The BLEU score significantly changes due to the translation length (Morishita et al., 2017). For instance, Figure 2 shows BLEU scores of our submitted system (a) when the word penalty was changed from 0.0 to 2.0 and (b) on various length ratios (LRs), which indicate the ratios of the number of words of the system outputs to the reference translations (sys/ref). As shown in Figure 2 (a), the BLEU scores change over 0.5 when we change the word penalty. The penalties of the peaks are different among the development/test sets. The BLEU score peaks were at W P = 1.2, 0.2, and 0.5 in the newstest2013, newstest2014, and newstest2015 sets, respectively. Therefore, the BLEU scores significan"
W18-2713,P11-2031,0,0.115317,"Missing"
W18-2713,W17-5712,1,0.804157,"ynthetic parallel set, and train the model using the mixture of the synthetic and original parallel sets. 3 3.3 Forward Translator 2: Seq-to-Seq Model Applied Systems The other forward translator used herein is OpenNMT based on the seq-to-seq model. The settings were almost the same as the back-translator. SGD was used for the optimization, but the learning rate was set to 0.5 because all target sentences appear twice in an epoch. At the translation, we translated the source sentence into 10-best, and the best hypothesis was selected using the length reranking based on the following equation (Oda et al., 2017). In this paper, we apply the proposed self-training approach to two translator types; the seq-to-seq model (Sutskever et al., 2014; Bahdanau et al., 2014) implemented by OpenNMT (LUA version) (Klein et al., 2017) and the transformer model (Vaswani et al., 2017) implemented by Marian NMT (Junczys-Dowmunt et al., 2018). Table 1 summerizes the system description. 3.1 Back-Translator The back-translator used herein is OpenNMT, which employs an RNN-based seq-to-seq model. The training corpus for the back-translation is preprocessed using the byte-pair encoding (BPE) (Sennrich et al., 2016b). For e"
W18-2713,W18-2707,1,0.924678,"d approach. Section 3 describes the details of our system. Section 4 explains the results of experiments, and Section 5 concludes the paper. Introduction In this study, we introduce the NICT neural translation system at the Second Workshop on Neural Machine Translation and Generation (NMT-2018) (Birch et al., 2018). A characteristic of the system is that translation qualities are improved by introducing self-training, using open-source neural translation systems and defined training data. The self-training method discussed herein is based on the methods proposed by Sennrich et al. (2016a) and Imamura et al. (2018), and they are applied to a self training strategy. It extends only the source side of the training data to increase variety. The merit of the proposed self-training strategy is that it does not influence the efficiency of the translation, such as the translation speed, because it does not change the model structure. (However, the training time increases due to an increase in the training data size.) The proposed approach can be applied to any translation method. However, we want to confirm on which model our approach is practically effective. This paper verifies the effect of our selftraining"
W18-2713,P16-1009,0,0.232298,"tion 2 describes the proposed approach. Section 3 describes the details of our system. Section 4 explains the results of experiments, and Section 5 concludes the paper. Introduction In this study, we introduce the NICT neural translation system at the Second Workshop on Neural Machine Translation and Generation (NMT-2018) (Birch et al., 2018). A characteristic of the system is that translation qualities are improved by introducing self-training, using open-source neural translation systems and defined training data. The self-training method discussed herein is based on the methods proposed by Sennrich et al. (2016a) and Imamura et al. (2018), and they are applied to a self training strategy. It extends only the source side of the training data to increase variety. The merit of the proposed self-training strategy is that it does not influence the efficiency of the translation, such as the translation speed, because it does not change the model structure. (However, the training time increases due to an increase in the training data size.) The proposed approach can be applied to any translation method. However, we want to confirm on which model our approach is practically effective. This paper verifies th"
W18-2713,P18-4020,0,0.0347549,"Missing"
W18-2713,P16-1162,0,0.251699,"tion 2 describes the proposed approach. Section 3 describes the details of our system. Section 4 explains the results of experiments, and Section 5 concludes the paper. Introduction In this study, we introduce the NICT neural translation system at the Second Workshop on Neural Machine Translation and Generation (NMT-2018) (Birch et al., 2018). A characteristic of the system is that translation qualities are improved by introducing self-training, using open-source neural translation systems and defined training data. The self-training method discussed herein is based on the methods proposed by Sennrich et al. (2016a) and Imamura et al. (2018), and they are applied to a self training strategy. It extends only the source side of the training data to increase variety. The merit of the proposed self-training strategy is that it does not influence the efficiency of the translation, such as the translation speed, because it does not change the model structure. (However, the training time increases due to an increase in the training data size.) The proposed approach can be applied to any translation method. However, we want to confirm on which model our approach is practically effective. This paper verifies th"
W18-2713,P17-4012,0,0.0300075,"OpenNMT based on the seq-to-seq model. The settings were almost the same as the back-translator. SGD was used for the optimization, but the learning rate was set to 0.5 because all target sentences appear twice in an epoch. At the translation, we translated the source sentence into 10-best, and the best hypothesis was selected using the length reranking based on the following equation (Oda et al., 2017). In this paper, we apply the proposed self-training approach to two translator types; the seq-to-seq model (Sutskever et al., 2014; Bahdanau et al., 2014) implemented by OpenNMT (LUA version) (Klein et al., 2017) and the transformer model (Vaswani et al., 2017) implemented by Marian NMT (Junczys-Dowmunt et al., 2018). Table 1 summerizes the system description. 3.1 Back-Translator The back-translator used herein is OpenNMT, which employs an RNN-based seq-to-seq model. The training corpus for the back-translation is preprocessed using the byte-pair encoding (BPE) (Sennrich et al., 2016b). For each language, 16K subword types were independently computed. The model was optimized using the stochastic gradient descent (SGD) whose learning rate was 1.0. For the back-translation, we modified OpenNMT to genera"
W18-2713,P18-1007,0,0.0240338,"Self-training they proposed such self-training strategy and confirmed the effect on their own corpus. Figure 1 shows the flow of self-training. The procedure is summarized as follows. 2.3 Dynamic Generation 1. First, train the back-translator that translates the target language into the source using original parallel corpus. A problem in the research proposed by Imamura et al. (2018) is that the training time increases (N + 1)-times with an increase in the training data size. To alleviate this problem, we introduce dynamic generation that uses different synthetic parallel sets for each epoch (Kudo, 2018). Specifically, a synthetic parallel sentence set, which contains one synthetic source sentence per target sentence, is used for an epoch of the training. By changing the synthetic parallel sentence set for each epoch, we expect a similar effect to using multiple source sentences in the training. For implementation, we do not embed the dynamic generation in the training program but perform it offline. Multiple synthetic source sentences were generated in advance, whose number N is 20 this time, and N synthetic parallel sets are constructed. During training, a synthetic set is selected for each"
W19-6613,D18-1399,0,0.237594,"lel corpus is available. This restricts one of the source or the target languages to be English which, in our case, is not possible. Approaches involving bi-directional NMT modeling is shown to drasti1 News domain translation is also the most competitive tasks in WMT indicating its importance. 2 http://opus.nlpl.eu/News-Commentary-v11. php 3 https://github.com/aizhanti/JaRuNC Dublin, Aug. 19-23, 2019 |p. 129 cally improve low-resource translation (Niu et al., 2018). However, like most other, this work focuses on translation from and into English. Remaining options include (a) unsupervised MT (Artetxe et al., 2018; Lample et al., 2018; Marie and Fujita, 2018), (b) parallel sentence mining from non-parallel or comparable corpora (Utiyama and Isahara, 2003; Tillmann and Xu, 2009), (c) generating pseudo-parallel data (Sennrich et al., 2016), and (d) MT based on pivot languages (Utiyama and Isahara, 2007). The linguistic distance between Japanese and Russian makes it extremely difficult to learn bilingual knowledge, such as bilingual lexicons and bilingual word embeddings. Unsupervised MT is thus not promising yet, due to its heavy reliance on accurate bilingual word embeddings. Neither does parallel sente"
W19-6613,N12-1047,0,0.0289999,"s) = P exp(β cos(emb(t),emb(s)) , where emb(·) stands 0 t0 exp(β cos(emb(t ),emb(s)) for a bilingual embedding of a given phrase, obtained through averaging bilingual embeddings of constituent words learned from the two monolingual data using fastText17 and vecmap.18 For each of the retained phrase pair, p(s|t) was computed analogously. We also computed lexical translation probabilities relying on those learned from the given small parallel corpus. Up to four phrase tables were jointly exploited by the multiple decoding path ability of Moses. Weights for the features were tuned using KB-MIRA (Cherry and Foster, 2012) on the development set; we took the best weights after 15 iterations. Two hyper-parameters, namely, k for the number of pivot-based phrase pairs per source phrase and d for distortion limit, were determined by a grid search on k ∈ {10, 20, 40, 60, 80, 100} and d ∈ {8, 10, 12, 14, 16, 18, 20}. In contrast, we used predetermined hyper-parameters for phrase table induction from monolingual data, following 15 https://github.com/facebookresearch/ UnsupervisedMT 16 https://code.google.com/archive/p/ word2vec/ 17 https://fasttext.cc/ 18 https://github.com/artetxem/vecmap Proceedings of MT Summit XVI"
W19-6613,P17-2061,1,0.92136,"odel trained on a resource-rich language pair like French↔English (parent), which is to be fine-tuned for a low-resource language pair like Uzbek↔English (child). On the other hand, multilingual approaches (Johnson et al., 2017) propose to train a single model to translate multiple language pairs. However, these approaches are effective only when the parent target or source language is relatively resource-rich like English (En). Furthermore, the parents and children models should be trained on similar domains; otherwise, one has to take into account an additional problem of domain adaptation (Chu et al., 2017). In this paper, we work on a linguistically distant and thus challenging language pair Japanese↔Russian (Ja↔Ru) which has only 12k lines of news domain parallel corpus and hence is extremely resource-poor. Furthermore, the amount of indirect in-domain parallel corpora, i.e., Ja↔En and Ru↔En, are also small. As we demonstrate in Section 4, this severely limits the performance of prominent low-resource techniques, such as multilingual modeling, back-translation, and pivotbased PBSMT. To remedy this, we propose a novel multistage fine-tuning method for NMT that combines multilingual modeling (Jo"
W19-6613,J82-2005,0,0.634941,"Missing"
W19-6613,N16-1101,0,0.0717709,"system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). Although NMT can be significantly better than PBSMT in resourcerich scenarios, PBSMT performs better in lowresource scenarios (Koehn and Knowles, 2017). c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 Only by exploiting cross-lingual transfer learning techniques (Firat et al., 2016; Zoph et al., 2016; Kocmi and Bojar, 2018), can the NMT performance approach PBSMT performance in lowresource scenarios. However, such methods usually require an NMT model trained on a resource-rich language pair like French↔English (parent), which is to be fine-tuned for a low-resource language pair like Uzbek↔English (child). On the other hand, multilingual approaches (Johnson et al., 2017) propose to train a single model to translate multiple language pairs. However, these approaches are effective only when the parent target or source language is relatively resource-rich like English (En)."
W19-6613,D07-1103,0,0.0524556,"u and Ru→Ja. Cascade: 2-step decoding using the source-toEnglish and English-to-target systems. Synthesize: Obtain a new phrase table from synthetic parallel data generated by translating English side of the target–English training parallel data to the source language with the English-to-source system. Triangulate: Compile a new phrase table combining those for the source-to-English and English-to-target systems. Among these three, triangulation is the most computationally expensive method. Although we had filtered the component phrase tables using the statistical significance pruning method (Johnson et al., 2007), triangulation can generate an enormous number of phrase pairs. To reduce the computational cost during decoding and the negative effects of potentially noisy phrase pairs, we retained for each source phrase s only the k-best translations t according to the forward translation probability φ(t|s) calculated from the conditional probabilities in the component models as defined in Utiyama and Isahara (2007). For each of the retained phrase pairs, we also calculated the backward translation probability, φ(s|t), and lexical translation probabilities, φlex (t|s) and φlex (s|t), in the same manner a"
W19-6613,Q17-1024,0,0.0503006,"Missing"
W19-6613,W18-6325,0,0.124224,"rd alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). Although NMT can be significantly better than PBSMT in resourcerich scenarios, PBSMT performs better in lowresource scenarios (Koehn and Knowles, 2017). c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 Only by exploiting cross-lingual transfer learning techniques (Firat et al., 2016; Zoph et al., 2016; Kocmi and Bojar, 2018), can the NMT performance approach PBSMT performance in lowresource scenarios. However, such methods usually require an NMT model trained on a resource-rich language pair like French↔English (parent), which is to be fine-tuned for a low-resource language pair like Uzbek↔English (child). On the other hand, multilingual approaches (Johnson et al., 2017) propose to train a single model to translate multiple language pairs. However, these approaches are effective only when the parent target or source language is relatively resource-rich like English (En). Furthermore, the parents and children mode"
W19-6613,W17-3204,0,0.502618,"more than 3.7 BLEU points, over a strong baseline, for this extremely low-resource scenario. 1 Introduction Neural machine translation (NMT) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) has enabled end-to-end training of a translation system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). Although NMT can be significantly better than PBSMT in resourcerich scenarios, PBSMT performs better in lowresource scenarios (Koehn and Knowles, 2017). c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 Only by exploiting cross-lingual transfer learning techniques (Firat et al., 2016; Zoph et al., 2016; Kocmi and Bojar, 2018), can the NMT performance approach PBSMT performance in lowresource scenarios. However, such methods usually require an NMT model trained on a resource-rich language pair like French↔English (parent), which is to be fine-tuned for a low-resource language pair like Uzbek↔English (child). On the other hand, m"
W19-6613,P07-2045,0,0.0227712,"eudo-parallel data. Our approach, which combines domain adaptation, multilingualism, and back-translation, helps improve the translation quality by more than 3.7 BLEU points, over a strong baseline, for this extremely low-resource scenario. 1 Introduction Neural machine translation (NMT) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) has enabled end-to-end training of a translation system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). Although NMT can be significantly better than PBSMT in resourcerich scenarios, PBSMT performs better in lowresource scenarios (Koehn and Knowles, 2017). c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 Only by exploiting cross-lingual transfer learning techniques (Firat et al., 2016; Zoph et al., 2016; Kocmi and Bojar, 2018), can the NMT performance approach PBSMT performance in lowresource scenarios. However, such methods usually require an NMT model trained on a resource-ric"
W19-6613,C18-1054,0,0.0296836,"the poor quality of the bilingual word embeddings used to extract the phrase table, as envisaged in Section 3. None of pivot-based approaches with unidirectional NMT models could even remotely rival the M2M Transformer NMT model (b3). 4.4 Augmentation with Back-translation Given that the M2M Transformer NMT model (b3) achieved best results for most of the translation directions and competitive results for the rest, we further explored it through back-translation. We examined the utility of pseudo-parallel data for all the six translation directions, unlike the work of Lakew et al. (2017) and Lakew et al. (2018), which concentrate only on the zero-shot language pair, and the work of Niu et al. (2018), which compares only uni- or bi-directional models. We investigated whether each translation direction in M2M models will benefit from pseudoparallel data and if so, what kind of improvement takes place. Dublin, Aug. 19-23, 2019 |p. 133 ID System #1–#10 Ja∗→Ru and/or Ru∗→Ja Ja∗→En and/or En∗→Ja Ru∗→En and/or En∗→Ru All Pseudo 12k→82k 47k→82k 82k All of the above Parallel data Ja↔Ru Ja↔En 12k→82k 47k→82k×2 12k→82k×2 47k→82k 12k→82k×2 47k→82k×2 12k→82k 47k→82k Ru↔En 82k×2 82k×2 82k 82k Total size of traini"
W19-6613,P10-2041,0,0.0550434,"• 22.85 22.77 • 23.09 En→Ru 16.92 17.30 17.20 • 17.89 16.76 16.68 16.80 • 17.73 17.13 17.26 17.30 Table 8: BLEU scores of M2M Transformer NMT systems trained on the mixture of given parallel corpus and pseudo-parallel data generated by back-translation using (b3). Six “X∗→Y” columns show whether the pseudo-parallel data for each translation direction is involved. Bold indicates the scores higher than (b3) and “• ” indicates statistical significance of the improvement. First, we selected sentences to be backtranslated from in-domain monolingual data (Table 3), relying on the score proposed by Moore and Lewis (2010) via the following procedure. 1. For each language, train two 4-gram language models, using KenLM: an in-domain one on all the Global Voices data, i.e., both parallel and monolingual data, and a general-domain one on the concatenation of Global Voices, IWSLT, and Tatoeba data. 2. For each language, discard sentences containing OOVs according to the in-domain language model. 3. For each translation direction, select the T best monolingual sentences in the news domain, according to the difference between cross-entropy scores given by the in-domain and general-domain language models. Whereas Niu"
W19-6613,W18-2710,0,0.413744,"s opposed to PBSMT. Transfer learning approaches (Firat et al., 2016; Zoph et al., 2016; Kocmi and Bojar, 2018) work well when a large helping parallel corpus is available. This restricts one of the source or the target languages to be English which, in our case, is not possible. Approaches involving bi-directional NMT modeling is shown to drasti1 News domain translation is also the most competitive tasks in WMT indicating its importance. 2 http://opus.nlpl.eu/News-Commentary-v11. php 3 https://github.com/aizhanti/JaRuNC Dublin, Aug. 19-23, 2019 |p. 129 cally improve low-resource translation (Niu et al., 2018). However, like most other, this work focuses on translation from and into English. Remaining options include (a) unsupervised MT (Artetxe et al., 2018; Lample et al., 2018; Marie and Fujita, 2018), (b) parallel sentence mining from non-parallel or comparable corpora (Utiyama and Isahara, 2003; Tillmann and Xu, 2009), (c) generating pseudo-parallel data (Sennrich et al., 2016), and (d) MT based on pivot languages (Utiyama and Isahara, 2007). The linguistic distance between Japanese and Russian makes it extremely difficult to learn bilingual knowledge, such as bilingual lexicons and bilingual w"
W19-6613,P02-1040,0,0.104547,".70 0.19 3.72 2.02 Ru→Ja 1.86 1.61 4.29 1.96 0.87 8.35 4.45 Ja→En 2.41 6.18 5.15 4.36 6.48 10.24 8.19 En→Ja 7.83 8.81 7.55 7.97 10.63 12.43 10.27 Ru→En 18.42 19.60 14.24 20.70 22.25 22.10 22.37 En→Ru 13.64 15.11 10.86 16.24 16.03 16.92 16.52 Table 5: BLEU scores of baseline systems. Bold indicates the best BLEU score for each translation direction. tuned by a linear search on the BLEU score for the development set. Similarly to PBSMT, we also evaluated “Cascade” and “Synthesize” methods with unidirectional NMT models. 4.3 Results We evaluated MT models using case-sensitive and tokenized BLEU (Papineni et al., 2002) on test sets, using Moses’s multi-bleu.perl. Statistical significance (p &lt; 0.05) on the difference of BLEU scores was tested by Moses’s bootstraphypothesis-difference-significance.pl. Tables 5 and 6 show BLEU scores of all the models, except the NMT systems augmented with back-translations. Whereas some models achieved reasonable BLEU scores for Ja↔En and Ru↔En translation, all the results for Ja↔Ru, which is our main concern, were abysmal. Among the NMT models, Transformer models (b∗) were proven to be better than RNMT models (a∗). RNMT models could not even outperform the uni-directional PB"
W19-6613,P16-1009,0,0.406018,"is also the most competitive tasks in WMT indicating its importance. 2 http://opus.nlpl.eu/News-Commentary-v11. php 3 https://github.com/aizhanti/JaRuNC Dublin, Aug. 19-23, 2019 |p. 129 cally improve low-resource translation (Niu et al., 2018). However, like most other, this work focuses on translation from and into English. Remaining options include (a) unsupervised MT (Artetxe et al., 2018; Lample et al., 2018; Marie and Fujita, 2018), (b) parallel sentence mining from non-parallel or comparable corpora (Utiyama and Isahara, 2003; Tillmann and Xu, 2009), (c) generating pseudo-parallel data (Sennrich et al., 2016), and (d) MT based on pivot languages (Utiyama and Isahara, 2007). The linguistic distance between Japanese and Russian makes it extremely difficult to learn bilingual knowledge, such as bilingual lexicons and bilingual word embeddings. Unsupervised MT is thus not promising yet, due to its heavy reliance on accurate bilingual word embeddings. Neither does parallel sentence mining, due to the difficulty of obtaining accurate bilingual lexicons. Pseudo-parallel data can be used to augment existing parallel corpora for training, and previous work has reported that such data generated by so-called"
W19-6613,N09-2024,0,0.0303021,"nal NMT modeling is shown to drasti1 News domain translation is also the most competitive tasks in WMT indicating its importance. 2 http://opus.nlpl.eu/News-Commentary-v11. php 3 https://github.com/aizhanti/JaRuNC Dublin, Aug. 19-23, 2019 |p. 129 cally improve low-resource translation (Niu et al., 2018). However, like most other, this work focuses on translation from and into English. Remaining options include (a) unsupervised MT (Artetxe et al., 2018; Lample et al., 2018; Marie and Fujita, 2018), (b) parallel sentence mining from non-parallel or comparable corpora (Utiyama and Isahara, 2003; Tillmann and Xu, 2009), (c) generating pseudo-parallel data (Sennrich et al., 2016), and (d) MT based on pivot languages (Utiyama and Isahara, 2007). The linguistic distance between Japanese and Russian makes it extremely difficult to learn bilingual knowledge, such as bilingual lexicons and bilingual word embeddings. Unsupervised MT is thus not promising yet, due to its heavy reliance on accurate bilingual word embeddings. Neither does parallel sentence mining, due to the difficulty of obtaining accurate bilingual lexicons. Pseudo-parallel data can be used to augment existing parallel corpora for training, and pre"
W19-6613,N07-1061,0,0.47734,"rtance of sharing news between different language speakers. Moreover, news domain is one of the most challenging tasks, Proceedings of MT Summit XVII, volume 1 Ru Ja En #sent. X X X X X X 913 173 276 0 4 287 1 1,654 X X X Total X X X test 600 - Usage development 313 173 276 - Table 1: Manually aligned News Commentary data. due to large presence of out-of-vocabulary (OOV) tokens and long sentences.1 To establish and evaluate existing methods, we also involved English as the third language. As direct parallel corpora are scarce, involving a language such as English for pivoting is quite common (Utiyama and Isahara, 2007). There has been no clean held-out parallel data for Ja↔Ru and Ja↔En news translation. Therefore, we manually compiled development and test sets using News Commentary data2 as a source. Since the given Ja↔Ru and Ja↔En data share many lines in the Japanese side, we first compiled tri-text data. Then, from each line, corresponding parts across languages were manually identified, and unaligned parts were split off into a new line. Note that we have never merged two or more lines. As a result, we obtained 1,654 lines of data comprising trilingual, bilingual, and monolingual segments (mainly senten"
W19-6613,D16-1163,0,0.0944142,"ing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). Although NMT can be significantly better than PBSMT in resourcerich scenarios, PBSMT performs better in lowresource scenarios (Koehn and Knowles, 2017). c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 Only by exploiting cross-lingual transfer learning techniques (Firat et al., 2016; Zoph et al., 2016; Kocmi and Bojar, 2018), can the NMT performance approach PBSMT performance in lowresource scenarios. However, such methods usually require an NMT model trained on a resource-rich language pair like French↔English (parent), which is to be fine-tuned for a low-resource language pair like Uzbek↔English (child). On the other hand, multilingual approaches (Johnson et al., 2017) propose to train a single model to translate multiple language pairs. However, these approaches are effective only when the parent target or source language is relatively resource-rich like English (En). Furthermore, the p"
W19-6613,P03-1010,0,\N,Missing
W19-6613,P07-1092,0,\N,Missing
W19-6613,L16-1350,0,\N,Missing
Y12-1011,P98-1068,0,0.046516,"racy is improved since an improvement is achieved with unsupervised topic information. Figure 3: Results for the three classes “car”, “dorama” and “soccer team”. Bold font indicates that the difference in accuracy between proposal and best of baseline is significant by binomial test with P < 0.05 and italic font indicates P < 0.1. 4 Experiments 4.1 Experimental Settings The experimental parameters follow those of the experiments in Sadamitsu et al. (2011). We used 30M Japanese blog articles crawled in May 2008. The documents were tokenized, chunked, and labeled with IREX 8 named entity types (Fuchi and Takagi, 1998; Suzuki et al., 2006), and transformed into context features. The context features were defined using the template “(head) ent. (mid.) attr. (tail)”. The words included in each part were used as surface, part-of-speech, and named entity label features with added position information. Maximum word number of each part was set at 2. The features have to appear in both the positive and negative training data at least 5 times. In the experiments, we used three classes, “car”, “dorama” and “soccer team” since they often suffer semantic drift. The adjustment numbers for the basic setting are Nent ="
Y12-1011,P11-1026,0,0.0348205,"ion. Even when we use seed entities for modeling the semisupervised topic models, as in (Andrzejewski et al., 2009), estimating the appropriate grain size is difficult because of a lack of information about other topics and contra-examples. In order to control grain size in topic models, this section introduces interactive topic models that permit free control via human interaction. This interaction also includes some negative examples which are very effective in modifying the topic models. Topic model modification is now possible with the recent proposal of the Interactive Topic model (ITM) (Hu and Boyd-graber, 2011), which is based on LDA with the Dirichlet Forest prior (Andrzejewski et al., 2009). ITM makes it possible to accept the alterations input by users and to revise the topic model accordingly. Although ITM can modify a topic model, the calculation cost is high because it uses Gibbs sampling. The factor of processing overhead is very important because the user must wait for system feedback before interaction is possible. If userinteractivity is to be well accepted, we need to raise the response speed. 3.2 Interactive Unigram Mixtures To obtain faster response, we utilize interactive Unigram Mixtu"
Y12-1011,P09-1045,0,0.0152064,"he initial parameters are given at random, the model might converge on an inadequate local minima. To avoid this, the initial parameters are set to the parent topic model parameters. 3.3 Applying interactive Unigram Mixtures to set expansion In this section we describe how to apply IUMs to set expansion in agreement with user’s intuition. Our system’s diagram is shown in Figure 2. After the preliminary standard set expansion (“I” in Figure 2) outputs some entities, we can choose interactive negative entities “EIN ” (e.g. “Harley, Vespa” in previous sections) found by either automatic methods (McIntosh and Curran, 2009) or manual selection (“II” in Figure 2). Because this paper focuses on interactive control, it is out of scope as to which approach, automatic method or manual selection, should be used. In this paper, we choose few negative entities manually (in our experiments, we select two entities for each negative class). We choose not only EIN but also their class names “CIN ” (e.g. “motorcycle” in previous sections) and treat them as negative “attributes” in the same way as seed attributes. IUMs are modeled using very little interactive information (EIN , CIN ) as well as initial positive seed entities"
Y12-1011,D10-1035,0,0.012219,"calculated by IUMs (“IV” in Figure 2). The system uses the trained discriminative model in the second stage to re-score the selected candidates from the first stage. Although the single step discriminative approach can be utilized by using TIN in the first stage as the supervised data, this would degrade discrimination performance. The discriminative models could not train fine and coarse grain simultaneously as same as UMs. In preliminary experiments on the one stage method, we confirmed that the system outputs many inadequate entities belonging to wrong topics in the sense of coarse grain. McIntosh (2010) proposed the method most similar to ours. In McIntosh (2010), only negative entities are clustered based on distributional similarity. We cluster not only the entities themselves but also their topic information. Vyas and Pantel proposed an interactive method for entities refinement and improved accuracy of set expansion (Vyas and Pantel, 2009). They utilized the similarity method (SIM) and feature modification method (FMM) for refinement of entities and their local context features. As far as we know, our proposal represents the first interactive method designed for the set expansion task wi"
Y12-1011,P08-1003,0,0.0263875,"Missing"
Y12-1011,P06-1015,0,0.0185395,"ctive topic information into a two-stage discriminative system for stable set expansion. Experiments confirm that the proposal raises the accuracy of the set expansion system from the baselines examined. 1 Introduction The task of this paper is entity set expansion in which the lexicons are expanded from just a few seed entities (Pantel et al., 2009). For example, the user inputs the words “Apple”, “Google” and “IBM”, and the system outputs “Microsoft”, “Facebook” and “Intel”. Many set expansion and relation extraction algorithms are based on bootstrapping algorithms (Thelen and Riloff, 2002; Pantel and Pennacchiotti, 2006), which iteratively acquire new entities from corpora. These algorithms suffer from the general problem of “semantic drift”. Semantic 108 drift moves the extraction criteria away from the initial criteria demanded by the user and so reduces the accuracy of extraction. Recently, topic information is being used to alleviate semantic drift. Topic information means the genre of each document as estimated by statistical topic models. Sadamitsu et al. (2011) proposed a bootstrapping method that uses unsupervised topic information estimated by Latent Dirichlet Allocation (LDA) (Blei et al., 2003) to"
Y12-1011,P10-1044,0,0.0543268,"Missing"
Y12-1011,P11-2128,1,0.594735,"ook” and “Intel”. Many set expansion and relation extraction algorithms are based on bootstrapping algorithms (Thelen and Riloff, 2002; Pantel and Pennacchiotti, 2006), which iteratively acquire new entities from corpora. These algorithms suffer from the general problem of “semantic drift”. Semantic 108 drift moves the extraction criteria away from the initial criteria demanded by the user and so reduces the accuracy of extraction. Recently, topic information is being used to alleviate semantic drift. Topic information means the genre of each document as estimated by statistical topic models. Sadamitsu et al. (2011) proposed a bootstrapping method that uses unsupervised topic information estimated by Latent Dirichlet Allocation (LDA) (Blei et al., 2003) to alleviate semantic drift. They use a discriminative method (Bellare et al., 2006) in order to incorporates topic information. They showed that the use of topic information improves the accuracy of the extracted entities. Although unsupervised topic information has been confirmed to be effective, the topic models and target entity sometimes demonstrate grain mismatch. To avoid this mismatch, we refine the topic models to match the target entity grain. D"
Y12-1011,sadamitsu-etal-2012-constructing,1,0.851182,"ol the topic models using not only positive seed entities but also a very small number of negative entities as distinguished from the output of the preliminary set expansion system. To implement this approach, we need topic models that offer conCopyright 2012 by Kugatsu Sadamitsu Sadamitsu, Kuniko Saito, Kenji Imamura, and Yoshihiro Matsuo 26th Pacific Asia Conference on Language,Information and Computation pages 108–116 trollability through the addition of negative words and high response speed for re-training. We utilize a variation of interactive topic models: interactive Unigram Mixtures (Sadamitsu et al., 2012). In a later section, we show that proposed method improves the accuracy of a set expansion system. 2 Set expansion using Topic information 2.1 Basic bootstrapping methods with discriminative models In this section, we describe the basic method adopted from Bellare et al. (2006) since it offers easy handling of arbitrary features including topic information. At first, Nent positive seed entities and Nattr seed attributes are given. The set of positive entity-attribute tuple, TP , is obtained by taking the cross product of seed entity lists and attribute lists. Tuples TP are used as queries for"
Y12-1011,P06-1028,0,0.0179037,"n improvement is achieved with unsupervised topic information. Figure 3: Results for the three classes “car”, “dorama” and “soccer team”. Bold font indicates that the difference in accuracy between proposal and best of baseline is significant by binomial test with P < 0.05 and italic font indicates P < 0.1. 4 Experiments 4.1 Experimental Settings The experimental parameters follow those of the experiments in Sadamitsu et al. (2011). We used 30M Japanese blog articles crawled in May 2008. The documents were tokenized, chunked, and labeled with IREX 8 named entity types (Fuchi and Takagi, 1998; Suzuki et al., 2006), and transformed into context features. The context features were defined using the template “(head) ent. (mid.) attr. (tail)”. The words included in each part were used as surface, part-of-speech, and named entity label features with added position information. Maximum word number of each part was set at 2. The features have to appear in both the positive and negative training data at least 5 times. In the experiments, we used three classes, “car”, “dorama” and “soccer team” since they often suffer semantic drift. The adjustment numbers for the basic setting are Nent = 10, Nattr = 10, Nnew ="
Y12-1011,W02-1028,0,0.039079,"We incorporate the interactive topic information into a two-stage discriminative system for stable set expansion. Experiments confirm that the proposal raises the accuracy of the set expansion system from the baselines examined. 1 Introduction The task of this paper is entity set expansion in which the lexicons are expanded from just a few seed entities (Pantel et al., 2009). For example, the user inputs the words “Apple”, “Google” and “IBM”, and the system outputs “Microsoft”, “Facebook” and “Intel”. Many set expansion and relation extraction algorithms are based on bootstrapping algorithms (Thelen and Riloff, 2002; Pantel and Pennacchiotti, 2006), which iteratively acquire new entities from corpora. These algorithms suffer from the general problem of “semantic drift”. Semantic 108 drift moves the extraction criteria away from the initial criteria demanded by the user and so reduces the accuracy of extraction. Recently, topic information is being used to alleviate semantic drift. Topic information means the genre of each document as estimated by statistical topic models. Sadamitsu et al. (2011) proposed a bootstrapping method that uses unsupervised topic information estimated by Latent Dirichlet Allocat"
Y12-1011,N09-1033,0,0.0249455,"iscriminative models could not train fine and coarse grain simultaneously as same as UMs. In preliminary experiments on the one stage method, we confirmed that the system outputs many inadequate entities belonging to wrong topics in the sense of coarse grain. McIntosh (2010) proposed the method most similar to ours. In McIntosh (2010), only negative entities are clustered based on distributional similarity. We cluster not only the entities themselves but also their topic information. Vyas and Pantel proposed an interactive method for entities refinement and improved accuracy of set expansion (Vyas and Pantel, 2009). They utilized the similarity method (SIM) and feature modification method (FMM) for refinement of entities and their local context features. As far as we know, our proposal represents the first interactive method designed for the set expansion task with topic information. By incorporating interactive topic information, we can expect that the accuracy is improved since an improvement is achieved with unsupervised topic information. Figure 3: Results for the three classes “car”, “dorama” and “soccer team”. Bold font indicates that the difference in accuracy between proposal and best of baselin"
Y12-1011,D09-1098,0,\N,Missing
Y12-1011,C98-1065,0,\N,Missing
