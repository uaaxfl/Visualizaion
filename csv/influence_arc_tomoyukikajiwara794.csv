2020.aacl-srw.22,N19-1423,0,0.0146258,"ch as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), as rewards. Although reinforcement learning based text simplification models (Zhang and Lapata, 2017; Zhao et al., 2020) have used rewards metrics such as SARI (Xu et al., 2016) and FKGL (Kincaid et al., 1975), these metrics do not align with human-perspectives, i.e., human evaluation results (Xu et al., 2016; Sulem et al., 2018; Alva-Manchego et al., 2020). In this study, we train a text simplification model based on reinforcement learning with rewards that highly agree with human-perspectives. Specifically, we apply a BERT regressor (Devlin et al., 2019) on grammaticality, meaning preservation, and simplicity, respectively, as shown in Figure 1. Experiments on the Newsela dataset (Xu et al., 2015) have shown that reinforcement learning with our rewards balances meaning preservation and simplicity. Further, manual evaluation has shown that 153 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop, pages 153–159 c December 4 - 7, 2020. 2020 Association for Computational Linguistics our ou"
2020.aacl-srw.22,N19-1315,0,0.0280522,"Missing"
2020.aacl-srw.22,P14-2029,0,0.0659069,"Missing"
2020.aacl-srw.22,2020.acl-main.424,0,0.0201035,"g has been employed in text-to-text generation tasks, such as machine translation (Ranzato et al., 2016) and abstractive summarization (Paulus et al., 2018). These studies use metrics suitable for each task, such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), as rewards. Although reinforcement learning based text simplification models (Zhang and Lapata, 2017; Zhao et al., 2020) have used rewards metrics such as SARI (Xu et al., 2016) and FKGL (Kincaid et al., 1975), these metrics do not align with human-perspectives, i.e., human evaluation results (Xu et al., 2016; Sulem et al., 2018; Alva-Manchego et al., 2020). In this study, we train a text simplification model based on reinforcement learning with rewards that highly agree with human-perspectives. Specifically, we apply a BERT regressor (Devlin et al., 2019) on grammaticality, meaning preservation, and simplicity, respectively, as shown in Figure 1. Experiments on the Newsela dataset (Xu et al., 2015) have shown that reinforcement learning with our rewards balances meaning preservation and simplicity. Further, manual evaluation has shown that 153 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Lin"
2020.aacl-srw.22,E99-1042,0,0.375693,"n Simplicity Reward Calculator Figure 1: Overview of the reinforcement learning for text simplification. Introduction Text simplification is one of the text-to-text generation tasks that rewrites complex sentences into simpler ones. Text simplification is useful for preprocessing of NLP tasks such as semantic role labeling (Vickrey and Koller, 2008; Woodsend and ˇ Lapata, 2014) and machine translation (Stajner and Popovi´c, 2016, 2018). It also has valuable applications such as assisting language learning (Inui et al., 2003; Petersen and Ostendorf, 2007) and helping language-impaired readers (Carroll et al., 1999). There are two problems in text-to-text generation with an encoder-decoder model: exposure bias and loss-evaluation mismatch (Ranzato et al., 2016; Wiseman and Rush, 2016). The former is that the model is not exposed to its own errors during training. The latter is that while the generated sentence is evaluated as a whole sentence during inference, it is evaluated at the token-level during training. To address these problems, reinforcement learning has been employed in text-to-text generation tasks, such as machine translation (Ranzato et al., 2016) and abstractive summarization (Paulus et al"
2020.aacl-srw.22,W04-1013,0,0.0840608,"aluation mismatch (Ranzato et al., 2016; Wiseman and Rush, 2016). The former is that the model is not exposed to its own errors during training. The latter is that while the generated sentence is evaluated as a whole sentence during inference, it is evaluated at the token-level during training. To address these problems, reinforcement learning has been employed in text-to-text generation tasks, such as machine translation (Ranzato et al., 2016) and abstractive summarization (Paulus et al., 2018). These studies use metrics suitable for each task, such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), as rewards. Although reinforcement learning based text simplification models (Zhang and Lapata, 2017; Zhao et al., 2020) have used rewards metrics such as SARI (Xu et al., 2016) and FKGL (Kincaid et al., 1975), these metrics do not align with human-perspectives, i.e., human evaluation results (Xu et al., 2016; Sulem et al., 2018; Alva-Manchego et al., 2020). In this study, we train a text simplification model based on reinforcement learning with rewards that highly agree with human-perspectives. Specifically, we apply a BERT regressor (Devlin et al., 2019) on grammaticality, meaning preserva"
2020.aacl-srw.22,D15-1166,0,0.0431759,"wards from the following three perspectives, based on the standards in manual evaluation for text simplification. Background: Reinforcement Learning for Text Simplification Reinforcement learning in text-to-text generation tasks is performed as additional training for pretrained text-to-text generation models. It is a common technique to linearly interpolate a reward of reinforcement learning and the cross-entropy loss to avoid misleading training because of a large action space (Ranzato et al., 2016; Zhang and Lapata, 2017). We first explain an attention based encoderdecoder model (EncDecA) (Luong et al., 2015) in Section 2.1 and then reinforcement learning for text simplification in Section 2.2. 2.1 • Grammaticality: This reward assesses the grammatical acceptability of the generated sentence Yˆ . Previous studies used an neural language model implemented using Long short-term memory (Mikolov et al., 2010; Hochreiter and Schmidhuber, 1997). • Meaning Preservation: This reward assesses the semantic similarity between the source sentence X and the generated sentence Yˆ . Zhang and Lapata (2017) used cosine similarity of the sentence representations from a sequence auto-encoder (Dai and Le, 2015). Zha"
2020.aacl-srw.22,P08-1040,0,0.0550111,"nt learning with our rewards balances meaning preservation and simplicity. Additionally, human evaluation confirmed that simplified texts by our method are preferred by humans compared to previous studies. 1 Encoder Decoder Simple Sentence Grammaticality Meaning Preservation Simplicity Reward Calculator Figure 1: Overview of the reinforcement learning for text simplification. Introduction Text simplification is one of the text-to-text generation tasks that rewrites complex sentences into simpler ones. Text simplification is useful for preprocessing of NLP tasks such as semantic role labeling (Vickrey and Koller, 2008; Woodsend and ˇ Lapata, 2014) and machine translation (Stajner and Popovi´c, 2016, 2018). It also has valuable applications such as assisting language learning (Inui et al., 2003; Petersen and Ostendorf, 2007) and helping language-impaired readers (Carroll et al., 1999). There are two problems in text-to-text generation with an encoder-decoder model: exposure bias and loss-evaluation mismatch (Ranzato et al., 2016; Wiseman and Rush, 2016). The former is that the model is not exposed to its own errors during training. The latter is that while the generated sentence is evaluated as a whole sent"
2020.aacl-srw.22,N18-2013,0,0.0179016,"imit the vocabulary S size to 20, 000 in addition to the pre-processing by Zhang and Lapata (2017). The EncDecA model was pre-trained by cross entropy loss with Adam optimizer ahead of reinforcement learning. The batch size was 32 sentences. We created a checkpoint for the model at every 100 steps. In the pre-training, training was stopped after 10 epochs without improvement of SARI score measured on the validation set. However, as the SARI is not stable at the beginning of the training, we ignored checkpoints whose BLEU scores measured on the validation set were less than 21, as suggested by Vu et al. (2018). End-to-End Evaluation on Text Simplification 4.1 M Table 2: Pearson correlation of each sub-reward estimator. Note that G, M, S correspond to grammaticality, meaning preservation, and simplicity, respectively. Results Table 2 shows the evaluation results of each sub-reward estimator. In all perspectives, existing unsupervised sub-reward estimators have little or no correlation with human annotations. As expected, fine-tuning BERT for each task significantly improved the Pearson correlations. 4 G 4.3 Results of Automatic Evaluation The performance of each method is automatically evaluated usi"
2020.aacl-srw.22,P02-1040,0,0.112184,"r model: exposure bias and loss-evaluation mismatch (Ranzato et al., 2016; Wiseman and Rush, 2016). The former is that the model is not exposed to its own errors during training. The latter is that while the generated sentence is evaluated as a whole sentence during inference, it is evaluated at the token-level during training. To address these problems, reinforcement learning has been employed in text-to-text generation tasks, such as machine translation (Ranzato et al., 2016) and abstractive summarization (Paulus et al., 2018). These studies use metrics suitable for each task, such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), as rewards. Although reinforcement learning based text simplification models (Zhang and Lapata, 2017; Zhao et al., 2020) have used rewards metrics such as SARI (Xu et al., 2016) and FKGL (Kincaid et al., 1975), these metrics do not align with human-perspectives, i.e., human evaluation results (Xu et al., 2016; Sulem et al., 2018; Alva-Manchego et al., 2020). In this study, we train a text simplification model based on reinforcement learning with rewards that highly agree with human-perspectives. Specifically, we apply a BERT regressor (Devlin et al., 2019) on grammatica"
2020.aacl-srw.22,P16-1162,0,0.0207283,"the Newsela dataset6 shown in Table 1. We implemented and pre-trained the EncDecA model as a common base to add reinforcement learning with rewards of ours and previous studies (Zhang and Lapata, 2017; Zhao et al., 2020). The EncDecA model has a 2-layer LSTM of 256 hidden dimensions for both the encoder and decoder, and attention mechanism by multi-layer perceptron with a layer size of 256. It has word embedding layers of 300 dimensions tying the source, target, and the output layer’s weight matrices. Dropout of 0.2 was applied to all embeddings and hidden layers. We used byte-pair encoding7 (Sennrich et al., 2016) to limit the vocabulary S size to 20, 000 in addition to the pre-processing by Zhang and Lapata (2017). The EncDecA model was pre-trained by cross entropy loss with Adam optimizer ahead of reinforcement learning. The batch size was 32 sentences. We created a checkpoint for the model at every 100 steps. In the pre-training, training was stopped after 10 epochs without improvement of SARI score measured on the validation set. However, as the SARI is not stable at the beginning of the training, we ignored checkpoints whose BLEU scores measured on the validation set were less than 21, as suggeste"
2020.aacl-srw.22,W16-3411,0,0.0579649,"Missing"
2020.aacl-srw.22,W18-7006,0,0.0305222,"Missing"
2020.aacl-srw.22,D18-1081,0,0.0467401,"Missing"
2020.aacl-srw.22,D16-1137,0,0.0570611,"Missing"
2020.aacl-srw.22,2020.emnlp-demos.6,0,0.0551372,"Missing"
2020.aacl-srw.22,D18-1397,0,0.0216993,"baseline estimator b(ht ) calculated from the hidden state at time step t. r(ht ) = R(·) − b(ht ). (3) 154 Following (Ranzato et al., 2016), the baseline estimator is optimised by minimizing kbt − R(·)k2 . Hashimoto and Tsuruoka (2019) discussed problems in text-to-text generation by reinforcement learning; the expected future reward estimation is unstable due to the huge action space, which hinders convergence. This is because the action space of text-to-text generation corresponds to the entire target vocabulary, where many words are rarely used for prediction. Therefore, previous studies (Wu et al., 2018; Paulus et al., 2018; Hashimoto and Tsuruoka, 2019) proposed to stabilize the training in reinforcement learning by first pre-training a model with cross-entropy loss, and then adding weighted REINFORCE loss: L = λLR + (1 − λ)LC . 3 GUG STS-B Newsela (5) Grammaticality We use the GUG dataset2 (Heilman et al., 2014) for estimating the grammaticality 747 1, 500 1, 129 754 1, 379 1, 077 Simplicity We use the Newsela dataset4 (Xu et al., 2015) for estimating the simplicity of a sentence. The Newsela dataset is a parallel dataset of complex and simple sentences. Each sentence is assigned a U.S. el"
2020.aacl-srw.22,D17-1062,0,0.211118,"odel is not exposed to its own errors during training. The latter is that while the generated sentence is evaluated as a whole sentence during inference, it is evaluated at the token-level during training. To address these problems, reinforcement learning has been employed in text-to-text generation tasks, such as machine translation (Ranzato et al., 2016) and abstractive summarization (Paulus et al., 2018). These studies use metrics suitable for each task, such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), as rewards. Although reinforcement learning based text simplification models (Zhang and Lapata, 2017; Zhao et al., 2020) have used rewards metrics such as SARI (Xu et al., 2016) and FKGL (Kincaid et al., 1975), these metrics do not align with human-perspectives, i.e., human evaluation results (Xu et al., 2016; Sulem et al., 2018; Alva-Manchego et al., 2020). In this study, we train a text simplification model based on reinforcement learning with rewards that highly agree with human-perspectives. Specifically, we apply a BERT regressor (Devlin et al., 2019) on grammaticality, meaning preservation, and simplicity, respectively, as shown in Figure 1. Experiments on the Newsela dataset (Xu et al"
2020.acl-main.33,E17-2068,0,0.0319338,"! I caught a cold! – Cold Cold Cold Table 1: Examples of BERT classification for labelling a disease contracted by a writer. Both sentences are about the common cold. Only the second example indicates that the writer had a cold. BERT misclassified the first sentence. Introduction Text classification generally consists of two processes: an encoder that converts texts to numerical representations and a classifier that estimates hidden relations between the representations and class labels. The text representations are generated using N -gram statistics (Wang and Manning, 2012), word embeddings (Joulin et al., 2017; Wang et al., 2018), convolutional neural networks (Kalchbrenner et al., 2014; Zhang et al., 2015; Shen et al., 2018), and recurrent neural networks (Yang et al., 2016, 2018). Recently, powerful pre-trained models for text representations, e.g. Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019), have shown stateof-the-art performance on text classification tasks using only the simple classifier of a fully connected layer. However, a problem occurs when a classification task is adversarial to text encoders. Encoders aim to represent the meanings of texts; henc"
2020.acl-main.33,P14-1062,0,0.0613231,"tion for labelling a disease contracted by a writer. Both sentences are about the common cold. Only the second example indicates that the writer had a cold. BERT misclassified the first sentence. Introduction Text classification generally consists of two processes: an encoder that converts texts to numerical representations and a classifier that estimates hidden relations between the representations and class labels. The text representations are generated using N -gram statistics (Wang and Manning, 2012), word embeddings (Joulin et al., 2017; Wang et al., 2018), convolutional neural networks (Kalchbrenner et al., 2014; Zhang et al., 2015; Shen et al., 2018), and recurrent neural networks (Yang et al., 2016, 2018). Recently, powerful pre-trained models for text representations, e.g. Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019), have shown stateof-the-art performance on text classification tasks using only the simple classifier of a fully connected layer. However, a problem occurs when a classification task is adversarial to text encoders. Encoders aim to represent the meanings of texts; hence, semantically similar texts tend to have closer representations. Meanwhile,"
2020.acl-main.33,D19-1542,1,0.827149,"N treat all negative examples equally, disregarding variables, such as relations between class labels. Future work should focus on the semantic relations among class labels in the auxiliary task. 354 4 Related Work Multitask learning has been employed to improve the performance of text classification (Liu et al., 2019; Xiao et al., 2018). Previous studies aimed to improve multiple tasks; hence, they required multiple sets of annotated datasets. In contrast, our method does not require any extra labelled datasets and is easily applicable to various classification tasks. The methods proposed in Arase and Tsujii (2019) and Phang et al. (2018) improved the BERT classification performance by further training the pre-trained model using natural language inference and paraphrase recognition. Similar to multitask learning, both methods require an additional largescale labelled dataset. Furthermore, these previous studies revealed that the similarity of tasks in training affects the models’ final performance (Xiao et al., 2018; Arase and Tsujii, 2019). Our method achieved consistent improvements across tasks, indicating its wider applicability. 5 Conclusion In this paper, we proposed a simple multitask learning m"
2020.acl-main.33,P18-1216,0,0.0185967,"Cold Cold Cold Table 1: Examples of BERT classification for labelling a disease contracted by a writer. Both sentences are about the common cold. Only the second example indicates that the writer had a cold. BERT misclassified the first sentence. Introduction Text classification generally consists of two processes: an encoder that converts texts to numerical representations and a classifier that estimates hidden relations between the representations and class labels. The text representations are generated using N -gram statistics (Wang and Manning, 2012), word embeddings (Joulin et al., 2017; Wang et al., 2018), convolutional neural networks (Kalchbrenner et al., 2014; Zhang et al., 2015; Shen et al., 2018), and recurrent neural networks (Yang et al., 2016, 2018). Recently, powerful pre-trained models for text representations, e.g. Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019), have shown stateof-the-art performance on text classification tasks using only the simple classifier of a fully connected layer. However, a problem occurs when a classification task is adversarial to text encoders. Encoders aim to represent the meanings of texts; hence, semantically simi"
2020.acl-main.33,P12-2018,0,0.058188,"BERT A cold is a legit disease. Oh my god! I caught a cold! – Cold Cold Cold Table 1: Examples of BERT classification for labelling a disease contracted by a writer. Both sentences are about the common cold. Only the second example indicates that the writer had a cold. BERT misclassified the first sentence. Introduction Text classification generally consists of two processes: an encoder that converts texts to numerical representations and a classifier that estimates hidden relations between the representations and class labels. The text representations are generated using N -gram statistics (Wang and Manning, 2012), word embeddings (Joulin et al., 2017; Wang et al., 2018), convolutional neural networks (Kalchbrenner et al., 2014; Zhang et al., 2015; Shen et al., 2018), and recurrent neural networks (Yang et al., 2016, 2018). Recently, powerful pre-trained models for text representations, e.g. Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019), have shown stateof-the-art performance on text classification tasks using only the simple classifier of a fully connected layer. However, a problem occurs when a classification task is adversarial to text encoders. Encoders aim to"
2020.acl-main.33,C18-1175,0,0.0208288,"using negative supervision is crucial. SST-5 is an exception wherein our models degraded the performance of the Baseline. We hypothesise that this is because its class labels are gradational, e.g. Somewhat Negative is closer to Negative rather than Positive sentences. AM and AAN treat all negative examples equally, disregarding variables, such as relations between class labels. Future work should focus on the semantic relations among class labels in the auxiliary task. 354 4 Related Work Multitask learning has been employed to improve the performance of text classification (Liu et al., 2019; Xiao et al., 2018). Previous studies aimed to improve multiple tasks; hence, they required multiple sets of annotated datasets. In contrast, our method does not require any extra labelled datasets and is easily applicable to various classification tasks. The methods proposed in Arase and Tsujii (2019) and Phang et al. (2018) improved the BERT classification performance by further training the pre-trained model using natural language inference and paraphrase recognition. Similar to multitask learning, both methods require an additional largescale labelled dataset. Furthermore, these previous studies revealed tha"
2020.acl-main.33,C18-1330,0,0.0310524,"Missing"
2020.acl-main.33,N16-1174,0,0.38251,"y the second example indicates that the writer had a cold. BERT misclassified the first sentence. Introduction Text classification generally consists of two processes: an encoder that converts texts to numerical representations and a classifier that estimates hidden relations between the representations and class labels. The text representations are generated using N -gram statistics (Wang and Manning, 2012), word embeddings (Joulin et al., 2017; Wang et al., 2018), convolutional neural networks (Kalchbrenner et al., 2014; Zhang et al., 2015; Shen et al., 2018), and recurrent neural networks (Yang et al., 2016, 2018). Recently, powerful pre-trained models for text representations, e.g. Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019), have shown stateof-the-art performance on text classification tasks using only the simple classifier of a fully connected layer. However, a problem occurs when a classification task is adversarial to text encoders. Encoders aim to represent the meanings of texts; hence, semantically similar texts tend to have closer representations. Meanwhile, a classifier should distinguish subtle differences that lead to different label assignment"
2020.acl-main.33,C16-1329,0,0.0180987,"dWeb arXiv Ja En Zh SOTA 83.5 86.3 52.4 96.4 95.5 82.5 79.5 80.9 - Baseline ACE 86.5 86.3 89.2 88.8 54.0 53.2 97.0 97.0 96.5 96.5 86.1 86.2 83.1 82.8 86.9 86.8 36.0 35.8 AM AAN 86.4 86.8 89.1 89.4 52.9 53.0 97.2 96.9 96.3 96.6 86.5 87.1 83.2 83.6 87.1 86.4 36.3 36.4 Table 3: Evaluation results. The best scores are presented in the bold font, and scores higher than the Baseline are underlined. Our models consistently outperform the baseline and ACE, which indicates the effectiveness of negative supervision through the auxiliary task. Previous SOTA results are reported by Du et al. (2019) (MR), Zhou et al. (2016) (CR, SST-5), Howard and Ruder (2018) (TREC), Zhao et al. (2015) (SUBJ) and Iso et al. (2017) (MedWeb). the auxiliary task, we compared our model to one that predicts a sentence with the same label. Accurately, this model conducts classification given cosine similarities using cross entropy loss (referred to as ACE (the auxiliary task with cross entropy loss)). Furthermore, we evaluated two variations of our model. The first purely gives negative supervision, i.e., the auxiliary task only encourages the generation of distinct representation to negative examples, as described in Section 2.2 (re"
2020.coling-main.103,Q17-1010,0,0.264892,"lar meanings having similar embeddings, our reconstruction training learns the global relationships among words, which can be employed in various models for word embedding reconstruction. Experimental results on word similarity benchmarks show that the proposed method improves the performance of the all subword-based reconstruction models. 1 Introduction Word embeddings form the basis for many natural language processing (NLP) applications, e.g., text classification (Shen et al., 2018) and machine translation (Qi et al., 2018). However, widely used pretrained word embeddings such as fastText (Bojanowski et al., 2017) are considerably large, thereby making it difficult to develop NLP applications in limited memory environments such as mobile devices. For example, fastText1 (crawl-300d-2M-subword) requires approximately 2 GB of memory. In previous studies, the model size has been reduced by reconstructing word embeddings from characters (Pinter et al., 2017; Kim et al., 2018) and character N-grams (Zhao et al., 2018; Sasaki et al., 2019). As the number of characters or character N-grams, is significantly smaller than that of words, reconstructing word embeddings with accuracy from these subwords can reduce"
2020.coling-main.103,P12-1015,0,0.0419486,"ning batch. Finally, we minimize the loss function that combines Equations (1) and (2), as given below: L = Llocal + Lglobal . 3 (3) Experiment We evaluate the effectiveness of the globally informed reconstruction of word embeddings using word similarity tasks4 (Faruqui and Dyer, 2014). Our experiment employs the following five datasets: Rubenstein-Goodenough dataset (RG, 65 word-pairs) (Rubenstein and Goodenough, 1965), MillerCharles dataset (MC, 30 word-pairs) (Miller and Chales, 1991), WordSim-353 (WS, 353 wordpairs) (Finkelstein et al., 2002), MEN test collection (MEN, 3, 000 word-pairs) (Bruni et al., 2012), and Stanford Rare Word Similarity dataset (RW, 2, 034 word-pairs) (Luong et al., 2013). The performance of each method is evaluated using micro averaged Spearman’s rank correlation coefficient between the cosine similarities of the word embeddings and gold-standard similarities. 3 4 We also tried the mean squared error, but the loss function based on the cosine similarity achieved higher performance. https://github.com/mfaruqui/eval-word-vectors/ 1200 Character ρ Character RNN + Global Loss Character CNN + Global Loss 0.534 0.540 0.594 0.602 Small ρ Size 14 25 Bag of N-gram + Global Loss N-g"
2020.coling-main.103,P14-5004,0,0.0190022,"follows: 1 X Lglobal = (cos(ˆ ew , eg ) − cos(ew , eg ))2 . (2) n g∈W In this study, we sample n words from W in each training batch. To balance the similarity distribution among the selected n words, we first select the top-n/2 words that have a high cosine similarity to the target word, and then randomly select the rest from the training batch. Finally, we minimize the loss function that combines Equations (1) and (2), as given below: L = Llocal + Lglobal . 3 (3) Experiment We evaluate the effectiveness of the globally informed reconstruction of word embeddings using word similarity tasks4 (Faruqui and Dyer, 2014). Our experiment employs the following five datasets: Rubenstein-Goodenough dataset (RG, 65 word-pairs) (Rubenstein and Goodenough, 1965), MillerCharles dataset (MC, 30 word-pairs) (Miller and Chales, 1991), WordSim-353 (WS, 353 wordpairs) (Finkelstein et al., 2002), MEN test collection (MEN, 3, 000 word-pairs) (Bruni et al., 2012), and Stanford Rare Word Similarity dataset (RW, 2, 034 word-pairs) (Luong et al., 2013). The performance of each method is evaluated using micro averaged Spearman’s rank correlation coefficient between the cosine similarities of the word embeddings and gold-standard"
2020.coling-main.103,C18-1216,0,0.0736944,"s form the basis for many natural language processing (NLP) applications, e.g., text classification (Shen et al., 2018) and machine translation (Qi et al., 2018). However, widely used pretrained word embeddings such as fastText (Bojanowski et al., 2017) are considerably large, thereby making it difficult to develop NLP applications in limited memory environments such as mobile devices. For example, fastText1 (crawl-300d-2M-subword) requires approximately 2 GB of memory. In previous studies, the model size has been reduced by reconstructing word embeddings from characters (Pinter et al., 2017; Kim et al., 2018) and character N-grams (Zhao et al., 2018; Sasaki et al., 2019). As the number of characters or character N-grams, is significantly smaller than that of words, reconstructing word embeddings with accuracy from these subwords can reduce the model size while preserving the performance of applications.2 As shown in Figure 1, existing methods reconstruct word embeddings from subword embeddings and mimic the corresponding pre-trained word embeddings. These methods rely only on local information of subwords and pre-trained word embeddings. To improve the performance of word embedding reconstruction,"
2020.coling-main.103,W13-3512,0,0.0523873,"as given below: L = Llocal + Lglobal . 3 (3) Experiment We evaluate the effectiveness of the globally informed reconstruction of word embeddings using word similarity tasks4 (Faruqui and Dyer, 2014). Our experiment employs the following five datasets: Rubenstein-Goodenough dataset (RG, 65 word-pairs) (Rubenstein and Goodenough, 1965), MillerCharles dataset (MC, 30 word-pairs) (Miller and Chales, 1991), WordSim-353 (WS, 353 wordpairs) (Finkelstein et al., 2002), MEN test collection (MEN, 3, 000 word-pairs) (Bruni et al., 2012), and Stanford Rare Word Similarity dataset (RW, 2, 034 word-pairs) (Luong et al., 2013). The performance of each method is evaluated using micro averaged Spearman’s rank correlation coefficient between the cosine similarities of the word embeddings and gold-standard similarities. 3 4 We also tried the mean squared error, but the loss function based on the cosine similarity achieved higher performance. https://github.com/mfaruqui/eval-word-vectors/ 1200 Character ρ Character RNN + Global Loss Character CNN + Global Loss 0.534 0.540 0.594 0.602 Small ρ Size 14 25 Bag of N-gram + Global Loss N-gram SAM + Global Loss 0.191 0.210 0.494 0.618 Medium Size 12 12 ρ 0.597 0.605 0.684 0.69"
2020.coling-main.103,D17-1010,0,0.121556,"uction Word embeddings form the basis for many natural language processing (NLP) applications, e.g., text classification (Shen et al., 2018) and machine translation (Qi et al., 2018). However, widely used pretrained word embeddings such as fastText (Bojanowski et al., 2017) are considerably large, thereby making it difficult to develop NLP applications in limited memory environments such as mobile devices. For example, fastText1 (crawl-300d-2M-subword) requires approximately 2 GB of memory. In previous studies, the model size has been reduced by reconstructing word embeddings from characters (Pinter et al., 2017; Kim et al., 2018) and character N-grams (Zhao et al., 2018; Sasaki et al., 2019). As the number of characters or character N-grams, is significantly smaller than that of words, reconstructing word embeddings with accuracy from these subwords can reduce the model size while preserving the performance of applications.2 As shown in Figure 1, existing methods reconstruct word embeddings from subword embeddings and mimic the corresponding pre-trained word embeddings. These methods rely only on local information of subwords and pre-trained word embeddings. To improve the performance of word embedd"
2020.coling-main.103,N18-2084,0,0.0576056,"Missing"
2020.coling-main.103,N19-1353,0,0.07127,"applications, e.g., text classification (Shen et al., 2018) and machine translation (Qi et al., 2018). However, widely used pretrained word embeddings such as fastText (Bojanowski et al., 2017) are considerably large, thereby making it difficult to develop NLP applications in limited memory environments such as mobile devices. For example, fastText1 (crawl-300d-2M-subword) requires approximately 2 GB of memory. In previous studies, the model size has been reduced by reconstructing word embeddings from characters (Pinter et al., 2017; Kim et al., 2018) and character N-grams (Zhao et al., 2018; Sasaki et al., 2019). As the number of characters or character N-grams, is significantly smaller than that of words, reconstructing word embeddings with accuracy from these subwords can reduce the model size while preserving the performance of applications.2 As shown in Figure 1, existing methods reconstruct word embeddings from subword embeddings and mimic the corresponding pre-trained word embeddings. These methods rely only on local information of subwords and pre-trained word embeddings. To improve the performance of word embedding reconstruction, we propose a global loss function that uses words other than t"
2020.coling-main.103,P18-1041,0,0.0633877,"Missing"
2020.coling-main.103,D18-1059,0,0.0727754,"e processing (NLP) applications, e.g., text classification (Shen et al., 2018) and machine translation (Qi et al., 2018). However, widely used pretrained word embeddings such as fastText (Bojanowski et al., 2017) are considerably large, thereby making it difficult to develop NLP applications in limited memory environments such as mobile devices. For example, fastText1 (crawl-300d-2M-subword) requires approximately 2 GB of memory. In previous studies, the model size has been reduced by reconstructing word embeddings from characters (Pinter et al., 2017; Kim et al., 2018) and character N-grams (Zhao et al., 2018; Sasaki et al., 2019). As the number of characters or character N-grams, is significantly smaller than that of words, reconstructing word embeddings with accuracy from these subwords can reduce the model size while preserving the performance of applications.2 As shown in Figure 1, existing methods reconstruct word embeddings from subword embeddings and mimic the corresponding pre-trained word embeddings. These methods rely only on local information of subwords and pre-trained word embeddings. To improve the performance of word embedding reconstruction, we propose a global loss function that u"
2020.coling-main.573,I17-2058,0,0.0844909,"Missing"
2020.coling-main.573,W19-4406,0,0.0257381,"Missing"
2020.coling-main.573,S17-2001,0,0.0233278,"Furthermore, to verify the effectiveness of our dataset based on the GEC systems, we compared our metric with a BERT-based metric fine-tuned on datasets not based on the GEC systems. 5.1 Fine-tuning BERT SOME (BERT w/ existing data) The existing datasets described in Section 2 were used for grammaticality2 and fluency3 sub-metrics for fine-tuning BERT in the baseline method. For fluency, the dataset from the BNC was used for training the fluency, whereas the dataset from Wikipedia was used for development. For meaning preservation, we used the dataset7 of the Semantic Textual Similarity task (Cer et al., 2017), which evaluates the semantic similarity between two sentences using continuous values in [0.0, 5.0]. SOME (BERT w/ our data) Our dataset, introduced in Section 3, was divided into train/dev/test with 3,376/422/423 sentences and used for fine-tuning BERT8 , hyperparameter tuning, and intrinsic evaluation of each sub-metric, respectively. Refer to Appendix C for the hyperparameter settings. 5.2 Meta-Evaluation System-level meta-evaluation In the system level meta-evaluation, the average of the sentence scores was used as the system score , and the correlation coefficients with the manual evalu"
2020.coling-main.573,P18-1059,0,0.0117377,"e task of automatically correcting grammatically incorrect sentences, especially those written by language learners. To develop GEC systems efficiently, we construct an evaluation metric that has a high correlation with manual evaluations. Reference-based metrics such as Max Match (M2 ) (Dahlmeier and Ng, 2012) and GLEU (Napoles et al., 2015) are commonly used for automatic evaluation in the GEC task. However, these metrics penalize sentences whose words or phrases are not included in the reference, even if they are correct expressions because it is difficult to cover all possible references (Choshen and Abend, 2018). In contrast, reference-less metrics (Napoles et al., 2016; Asano et al., 2017) do not suffer from this limitation. Among them, Asano et al. (2017) achieved a higher correlation with manual evaluations than reference-based metrics by integrating sub-metrics from the three perspectives of (i) grammaticality, (ii) fluency, and (iii) meaning preservation. However, the correlation with the manual evaluation of system output can be further improved because they are not considered for optimizing each sub-metric. To achieve a better correlation with manual evaluation, we create a dataset to optimize"
2020.coling-main.573,N12-1067,0,0.0229837,"error correction systems to optimize the metrics. Experimental results show that the proposed metric improves the correlation with manual evaluation in both systemand sentence-level meta-evaluation. Our dataset and metric will be made publicly available.1 1 Introduction Grammatical error correction (GEC) is the task of automatically correcting grammatically incorrect sentences, especially those written by language learners. To develop GEC systems efficiently, we construct an evaluation metric that has a high correlation with manual evaluations. Reference-based metrics such as Max Match (M2 ) (Dahlmeier and Ng, 2012) and GLEU (Napoles et al., 2015) are commonly used for automatic evaluation in the GEC task. However, these metrics penalize sentences whose words or phrases are not included in the reference, even if they are correct expressions because it is difficult to cover all possible references (Choshen and Abend, 2018). In contrast, reference-less metrics (Napoles et al., 2016; Asano et al., 2017) do not suffer from this limitation. Among them, Asano et al. (2017) achieved a higher correlation with manual evaluations than reference-based metrics by integrating sub-metrics from the three perspectives o"
2020.coling-main.573,W14-3348,0,0.0189197,"mmaticaly: 3.8 Fluency: 3.8 Meaning: 1.6 Source text: The increasing longevity is due to fast development of the society so as the living pressure. System output: The increase in longevity is due to the fast development of society so as the living pressure. Grammaticaly: 2.6 Fluency: 2.4 Meaning: 3.8 Figure 1: Histogram of each manual evaluation and examples of annotation. of grammaticality. Although the GUG is a dataset annotated for grammaticality to sentences written by language learners, our target is the learner sentence corrected by the GEC system. They used a language model and METEOR (Denkowski and Lavie, 2014) as sub-metric for fluency and meaning preservation, respectively; yet these sub-metrics are not optimized for manual evaluation. The weighted linear sum of each evaluation score was used as the final score. Although our metric follows Asano et al. (2017), each sub-metric is trained on our dataset, thus achieving a higher correlation with manual evaluation. Apart from the GUG dataset, a fluency annotated dataset3 (Lau et al., 2015) exists with manual evaluations of acceptability for pseudo-error sentences generated by round-trip translation of English sentences from the British National Corpus"
2020.coling-main.573,N19-1423,0,0.0097229,"ram of manual evaluations and examples of annotation. Ratings of 2 or lower generally exhibited a low frequency; the majority of the meaning preservation ratings were 3 or higher. 4 Automatic Evaluation of GEC using BERT Using our dataset introduced in the previous section, we trained regression models corresponding to each sub-metric of (Asano et al., 2017). For grammaticality and fluency, the manual evaluations were estimated only from the system outputs, whereas, for meaning preservation, the manual evaluations were estimated from pairs of source sentences and system outputs. We used BERT (Devlin et al., 2019) for the regression models. BERT is a sentence encoder pre-trained with large-scale corpora, such as Wikipedia, based on both masked language modeling and next sentence prediction, which can achieve high performance in various natural language processing tasks by fine-tuning on a small dataset of the target task. We fine-tuned three BERT models for each perspective of grammaticality, fluency, and meaning preservation, and constructed sub-metrics that were optimized for manual evaluations of each perspective. The final evaluation score is calculated by the weighted linear sum of each evaluation"
2020.coling-main.573,D14-1020,0,0.061153,"Missing"
2020.coling-main.573,W14-3333,0,0.0730499,"Missing"
2020.coling-main.573,N18-2046,0,0.0144674,"stems, and thus we collected manual evaluations of the output of the GEC systems to train the metrics. In this study, these datasets are referred to as existing data. 3 Manual Evaluation of GEC System Outputs Data and GEC systems We collected manual evaluations for the grammaticality, fluency, and meaning preservation of the system outputs of 1,381 sentences from CoNLL 2013,4 which are often used to evaluate GEC systems. To collect the manual evaluations for various system outputs, each source sentence was corrected by the following five typical systems: statistical machine translation (SMT) (Grundkiewicz and Junczys-Dowmunt, 2018), recurrent neural network (RNN) (Luong et al., 2015), convolutional neural network (CNN) (Chollampatt and Ng, 2018), self-attention network (SAN) (Vaswani et al., 2017), and SAN with copy mechanism (SAN+Copy) (Zhao et al., 2019). More details can be found in Appendix A. Annotation By excluding duplicate corrected sentences, manual evaluation for the grammaticality, fluency, and meaning preservation were assigned to a total of 4,223 sentences, as follows: Grammaticality: Annotators evaluated the grammatical correctness of the system output. We followed the five-point scale evaluation criteria"
2020.coling-main.573,D15-1052,0,0.303965,"for fine-tuning BERT8 , hyperparameter tuning, and intrinsic evaluation of each sub-metric, respectively. Refer to Appendix C for the hyperparameter settings. 5.2 Meta-Evaluation System-level meta-evaluation In the system level meta-evaluation, the average of the sentence scores was used as the system score , and the correlation coefficients with the manual evaluations were calculated. Following Asano et al. (2017), system-level meta-evaluation was performed using Pearson’s correlation coefficient and Spearman’s rank correlation coefficient with the manual ranking of 12 systems described in (Grundkiewicz et al., 2015). The weights of the evaluation score (↵, , and ) were tuned on the JFLEG dataset (Napoles et al., 2017), following Asano et al. (2017). To perform a comprehensive evaluation considering all perspectives, we performed a grid search in increments of 0.01 in the range of 0.01 to 0.98 for each weight and maximized Pearson’s correlation coefficient. Following the recommendation of Graham and Baldwin (2014), we used Williams significance test to identify differences in correlation that are statistically significant. 6 Incomplete or unclear sentences. http://ixa2.si.ehu.es/stswiki/index.php/STSbench"
2020.coling-main.573,P14-2029,0,0.54856,"lysis reveals that optimization for both the manual evaluation and the output of GEC systems contribute to improvement. 2 Related Work Napoles et al. (2016) pioneered the reference-less GEC metric. They presented a metric based on grammatical error detection tools and linguistic features such as language models, and demonstrated that its performance was close to that of reference-based metrics. Asano et al. (2017) combined three submetrics: grammaticality, fluency, and meaning preservation, and outperformed reference-based metrics. They trained a logistic regression model on the GUG dataset2 (Heilman et al., 2014) for the sub-metric This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. 1 https://github.com/kokeman/SOME 2 https://github.com/EducationalTestingService/gug-data License details: http:// 6516 Proceedings of the 28th International Conference on Computational Linguistics, pages 6516–6522 Barcelona, Spain (Online), December 8-13, 2020 Source text: This will inversely improve the sale of the shop. System output: This will deﬁnitely improve the sales of the shop. Grammaticaly: 3.8 Fluency: 3.8 Meaning: 1.6 Source text: The incr"
2020.coling-main.573,P15-1156,0,0.0358276,"aticality to sentences written by language learners, our target is the learner sentence corrected by the GEC system. They used a language model and METEOR (Denkowski and Lavie, 2014) as sub-metric for fluency and meaning preservation, respectively; yet these sub-metrics are not optimized for manual evaluation. The weighted linear sum of each evaluation score was used as the final score. Although our metric follows Asano et al. (2017), each sub-metric is trained on our dataset, thus achieving a higher correlation with manual evaluation. Apart from the GUG dataset, a fluency annotated dataset3 (Lau et al., 2015) exists with manual evaluations of acceptability for pseudo-error sentences generated by round-trip translation of English sentences from the British National Corpus (BNC) and Wikipedia using Google Translate. We assume that sentences written by learners or translated by systems have different properties from those generated by GEC systems, and thus we collected manual evaluations of the output of the GEC systems to train the metrics. In this study, these datasets are referred to as existing data. 3 Manual Evaluation of GEC System Outputs Data and GEC systems We collected manual evaluations fo"
2020.coling-main.573,D15-1166,0,0.0222827,"systems to train the metrics. In this study, these datasets are referred to as existing data. 3 Manual Evaluation of GEC System Outputs Data and GEC systems We collected manual evaluations for the grammaticality, fluency, and meaning preservation of the system outputs of 1,381 sentences from CoNLL 2013,4 which are often used to evaluate GEC systems. To collect the manual evaluations for various system outputs, each source sentence was corrected by the following five typical systems: statistical machine translation (SMT) (Grundkiewicz and Junczys-Dowmunt, 2018), recurrent neural network (RNN) (Luong et al., 2015), convolutional neural network (CNN) (Chollampatt and Ng, 2018), self-attention network (SAN) (Vaswani et al., 2017), and SAN with copy mechanism (SAN+Copy) (Zhao et al., 2019). More details can be found in Appendix A. Annotation By excluding duplicate corrected sentences, manual evaluation for the grammaticality, fluency, and meaning preservation were assigned to a total of 4,223 sentences, as follows: Grammaticality: Annotators evaluated the grammatical correctness of the system output. We followed the five-point scale evaluation criteria (4: Perfect, 3: Comprehensible, 2: Somewhat comprehen"
2020.coling-main.573,N19-1132,1,0.887866,"Missing"
2020.coling-main.573,P15-2097,0,0.0331299,"ze the metrics. Experimental results show that the proposed metric improves the correlation with manual evaluation in both systemand sentence-level meta-evaluation. Our dataset and metric will be made publicly available.1 1 Introduction Grammatical error correction (GEC) is the task of automatically correcting grammatically incorrect sentences, especially those written by language learners. To develop GEC systems efficiently, we construct an evaluation metric that has a high correlation with manual evaluations. Reference-based metrics such as Max Match (M2 ) (Dahlmeier and Ng, 2012) and GLEU (Napoles et al., 2015) are commonly used for automatic evaluation in the GEC task. However, these metrics penalize sentences whose words or phrases are not included in the reference, even if they are correct expressions because it is difficult to cover all possible references (Choshen and Abend, 2018). In contrast, reference-less metrics (Napoles et al., 2016; Asano et al., 2017) do not suffer from this limitation. Among them, Asano et al. (2017) achieved a higher correlation with manual evaluations than reference-based metrics by integrating sub-metrics from the three perspectives of (i) grammaticality, (ii) fluen"
2020.coling-main.573,D16-1228,0,0.10179,"ences, especially those written by language learners. To develop GEC systems efficiently, we construct an evaluation metric that has a high correlation with manual evaluations. Reference-based metrics such as Max Match (M2 ) (Dahlmeier and Ng, 2012) and GLEU (Napoles et al., 2015) are commonly used for automatic evaluation in the GEC task. However, these metrics penalize sentences whose words or phrases are not included in the reference, even if they are correct expressions because it is difficult to cover all possible references (Choshen and Abend, 2018). In contrast, reference-less metrics (Napoles et al., 2016; Asano et al., 2017) do not suffer from this limitation. Among them, Asano et al. (2017) achieved a higher correlation with manual evaluations than reference-based metrics by integrating sub-metrics from the three perspectives of (i) grammaticality, (ii) fluency, and (iii) meaning preservation. However, the correlation with the manual evaluation of system output can be further improved because they are not considered for optimizing each sub-metric. To achieve a better correlation with manual evaluation, we create a dataset to optimize each submetric to the manual evaluation of GEC systems. Ou"
2020.coling-main.573,E17-2037,0,0.186197,"Missing"
2020.coling-main.573,Q16-1029,0,0.0258589,"2: Somewhat comprehensible, 1: Incomprehensible, and 0: Other) proposed by Heilman et al. (2014). Fluency: Annotators evaluated how natural the sentence sounds for native speakers. We followed the criteria (4: Extremely natural, 3: Somewhat natural, 2: Somewhat unnatural, and 1: Extremely unnatural) proposed by Lau et al. (2015). Meaning preservation: Annotators evaluated the extent to which the meaning of source sentences is preserved in system output. We followed the criteria (4: Identical, 3: Minor differences, 2: Moderate differences, 1: Substantially different, and 0: Other) proposed by Xu et al. (2016). We used Amazon Mechanical Turk5 and recruited five native English annotators. The average of the ratings excluding “0: Other” was used as the final sentence score. For more details, refer to Appendix B. Finally, we created a dataset with manual evaluations for a total of 4,221 sentences, excluding sentences in which three or more annotators 3 https://clasp.gu.se/about/people/shalom-lappin/smog/experiments-and-datasets https://www.comp.nus.edu.sg/˜nlp/conll13st.html 5 https://www.mturk.com/ 4 6517 answered “0: Other.”6 Figure 1 presents a histogram of manual evaluations and examples of annota"
2020.coling-main.573,N19-1014,0,0.0187269,"evaluations for the grammaticality, fluency, and meaning preservation of the system outputs of 1,381 sentences from CoNLL 2013,4 which are often used to evaluate GEC systems. To collect the manual evaluations for various system outputs, each source sentence was corrected by the following five typical systems: statistical machine translation (SMT) (Grundkiewicz and Junczys-Dowmunt, 2018), recurrent neural network (RNN) (Luong et al., 2015), convolutional neural network (CNN) (Chollampatt and Ng, 2018), self-attention network (SAN) (Vaswani et al., 2017), and SAN with copy mechanism (SAN+Copy) (Zhao et al., 2019). More details can be found in Appendix A. Annotation By excluding duplicate corrected sentences, manual evaluation for the grammaticality, fluency, and meaning preservation were assigned to a total of 4,223 sentences, as follows: Grammaticality: Annotators evaluated the grammatical correctness of the system output. We followed the five-point scale evaluation criteria (4: Perfect, 3: Comprehensible, 2: Somewhat comprehensible, 1: Incomprehensible, and 0: Other) proposed by Heilman et al. (2014). Fluency: Annotators evaluated how natural the sentence sounds for native speakers. We followed the"
2020.eamt-1.12,W18-6402,0,0.305768,"lication of semantic image regions for MNMT by integrating visual and textual features using two individual attention mechanisms (double attention). We conducted experiments on the Multi30k dataset and achieved an improvement of 0.5 and 0.9 BLEU points for English→German and English→French translation tasks, compared with the MNMT with grid visual features. We also demonstrated concrete improvements on translation performance benefited from semantic image regions. Object detection Abstract attention Figure 1: Overview of our MNMT model. many studies (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018) have been increasingly focusing on incorporating multimodal contents, particularly images, to improve translations. Hence, researchers in this field have established a shared task called multimodal machine translation (MMT), which consists of translating a target sentence from a source language description into another language using information from the image described by the source sentence. The first MMT study by (Elliott et al., 2015) demonstrated the potential of improving the translation quality by using image. To effectively use an image, several subsequent studies (Gao et al., 2015; H"
2020.eamt-1.12,W16-2358,0,0.0423569,"Missing"
2020.eamt-1.12,J82-2005,0,0.576686,"Missing"
2020.eamt-1.12,W17-4746,0,0.0849293,"Missing"
2020.eamt-1.12,W18-6438,0,0.0342887,"Missing"
2020.eamt-1.12,D17-1105,0,0.0955703,"ngly focusing on incorporating multimodal contents, particularly images, to improve translations. Hence, researchers in this field have established a shared task called multimodal machine translation (MMT), which consists of translating a target sentence from a source language description into another language using information from the image described by the source sentence. The first MMT study by (Elliott et al., 2015) demonstrated the potential of improving the translation quality by using image. To effectively use an image, several subsequent studies (Gao et al., 2015; Huang et al., 2016; Calixto and Liu, 2017) incorporated global visual features extracted from the entire image by convolutional neural networks (CNNs) into a source word sequence or hidden states of a recurrent neural network (RNN). Furthermore, other studies started using local visual features in the context of an attention-based NMT. These features were extracted from equally-sized grids in an image by a CNN. For instance, multimodal attention (Caglayan et al., 2016b) has been designed for a mix of text and local visual features. Additionally, double attention mechanisms (Calixto et al., 2017) have been proposed for text homme en De"
2020.eamt-1.12,W16-2359,0,0.018129,"r hidden states using elementwise multiplication. Delbrouck and Dupont (2018) proposed a variation of the conditional gated recurrent unit decoder, which receives the global visual features as input. Calixto et al. (2019) incorporated global visual features through latent variables. Although their results surpassed the performance of the NMT baseline, the visual features of an entire image are complex and non-specific, so that the effect of the image is not fully exerted. computing the multimodal context vector, wherein the local visual features were extracted by the ResNet-50 CNN. Similarly, Calixto et al. (2016) incorporated multiple multimodal attention mechanisms into decoder using grid visual features by VGG-19 CNN. Because the grid regions do not contain semantic visual features, the multimodal attention mechanism can not capture useful information with grid visual features. Therefore, instead of multimodal attention, Calixto, Liu, and Campbell (2017) proposed two individual attention mechanisms focusing on two modalities. Similarly, Libovick´y and Helcl (2017) proposed two attention strategies that can be applied to all hidden layers or context vectors of each modality. But they still used grid"
2020.eamt-1.12,P17-1175,0,0.0528235,"Missing"
2020.eamt-1.12,P19-1642,0,0.0192037,"features into source sentence vectors and encoder/decoder hidden states. Elliott and K´ad´ar (2017) utilized global visual features to learn both machine translation and visually grounding task simultaneously. As for the best system in WMT 2017,7 Caglayan et al. (2017) proposed different methods to incorporate global visual features based on attention-based NMT model such as initial encoder/decoder hidden states using elementwise multiplication. Delbrouck and Dupont (2018) proposed a variation of the conditional gated recurrent unit decoder, which receives the global visual features as input. Calixto et al. (2019) incorporated global visual features through latent variables. Although their results surpassed the performance of the NMT baseline, the visual features of an entire image are complex and non-specific, so that the effect of the image is not fully exerted. computing the multimodal context vector, wherein the local visual features were extracted by the ResNet-50 CNN. Similarly, Calixto et al. (2016) incorporated multiple multimodal attention mechanisms into decoder using grid visual features by VGG-19 CNN. Because the grid regions do not contain semantic visual features, the multimodal attention"
2020.eamt-1.12,D17-1095,0,0.0549949,"s. Additionally, double attention mechanisms (Calixto et al., 2017) have been proposed for text homme en Decoder yt-1 yt chemise rouge st-1 st &lt;eos&gt; zt ct Att_text wp (max) rp Bi-directional GRU encoder 100 semantic image region feature vectores Att_img hi Faster R-CNN + ResNet-101 Man Source image in a red xi shirt &lt;eos&gt; Source sentence Figure 2: Our model of double attention-based MNMT with semantic image regions. and local visual features, respectively. Although previous studies improved the use of local visual features and the text modality, these improvements were minor. As discussed in (Delbrouck and Dupont, 2017), these local visual features may not be suitable to attention-based NMT, because the attention mechanism cannot understand complex relationships between textual objects and visual concepts. Other studies utilized richer local visual features to MNMT such as dense captioning features (Delbrouck et al., 2017). However, their efforts have not convincingly demonstrated that visual features can improve the translation quality. Caglayan et al. (2019) demonstrated that, when the textual context is limited, visual features can assist in generating better translations. MMT models disregard visual feat"
2020.eamt-1.12,W18-6439,0,0.113891,"studies have fused either global or local visual image features into MMT. 6.1 Global visual feature Calixto and Liu (2017) incorporated global visual features into source sentence vectors and encoder/decoder hidden states. Elliott and K´ad´ar (2017) utilized global visual features to learn both machine translation and visually grounding task simultaneously. As for the best system in WMT 2017,7 Caglayan et al. (2017) proposed different methods to incorporate global visual features based on attention-based NMT model such as initial encoder/decoder hidden states using elementwise multiplication. Delbrouck and Dupont (2018) proposed a variation of the conditional gated recurrent unit decoder, which receives the global visual features as input. Calixto et al. (2019) incorporated global visual features through latent variables. Although their results surpassed the performance of the NMT baseline, the visual features of an entire image are complex and non-specific, so that the effect of the image is not fully exerted. computing the multimodal context vector, wherein the local visual features were extracted by the ResNet-50 CNN. Similarly, Calixto et al. (2016) incorporated multiple multimodal attention mechanisms i"
2020.eamt-1.12,W14-3348,0,0.0161004,"testset of Multi30k. All scores are averages of three runs. We present the results using the mean and the standard deviation. † and ‡ indicate that the result is significantly better than OpenNMT and double-attentive MNMT at p-value &lt; 0.01, respectively. Additionally, we report the best results of using grid and global visual features on Multi30k dataset according to (Caglayan et al., 2017), which is the state-of-the-art system for En→De translation on this dataset. 3.3 Evaluation We evaluated the quality of the translation according to the token level BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) metrics. We trained all models (baselines and proposed model) three times and calculated the BLEU and METEOR scores, respectively. Based on the calculation results, we report the mean and standard deviation over three runs. Moreover, we report the statistical significance with bootstrap resampling (Koehn, 2004) using the merger of three test translation results. We defined the threshold for the statistical significance test as 0.01, and report only if the p-value was less than the threshold. 4 Results In Table 1, we present the results for the OpenNMT, doubly-attentive MNMT and our model on M"
2020.eamt-1.12,P17-2031,0,0.0369791,"Missing"
2020.eamt-1.12,I17-1014,0,0.0393147,"Missing"
2020.eamt-1.12,P02-1040,0,0.106569,"models on the En→De and En→Fr 2016 testset of Multi30k. All scores are averages of three runs. We present the results using the mean and the standard deviation. † and ‡ indicate that the result is significantly better than OpenNMT and double-attentive MNMT at p-value &lt; 0.01, respectively. Additionally, we report the best results of using grid and global visual features on Multi30k dataset according to (Caglayan et al., 2017), which is the state-of-the-art system for En→De translation on this dataset. 3.3 Evaluation We evaluated the quality of the translation according to the token level BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) metrics. We trained all models (baselines and proposed model) three times and calculated the BLEU and METEOR scores, respectively. Based on the calculation results, we report the mean and standard deviation over three runs. Moreover, we report the statistical significance with bootstrap resampling (Koehn, 2004) using the merger of three test translation results. We defined the threshold for the statistical significance test as 0.01, and report only if the p-value was less than the threshold. 4 Results In Table 1, we present the results for the OpenNMT, d"
2020.eamt-1.12,W16-3210,0,0.111543,"Missing"
2020.eamt-1.12,W17-4718,0,0.0640345,"st, we propose the application of semantic image regions for MNMT by integrating visual and textual features using two individual attention mechanisms (double attention). We conducted experiments on the Multi30k dataset and achieved an improvement of 0.5 and 0.9 BLEU points for English→German and English→French translation tasks, compared with the MNMT with grid visual features. We also demonstrated concrete improvements on translation performance benefited from semantic image regions. Object detection Abstract attention Figure 1: Overview of our MNMT model. many studies (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018) have been increasingly focusing on incorporating multimodal contents, particularly images, to improve translations. Hence, researchers in this field have established a shared task called multimodal machine translation (MMT), which consists of translating a target sentence from a source language description into another language using information from the image described by the source sentence. The first MMT study by (Elliott et al., 2015) demonstrated the potential of improving the translation quality by using image. To effectively use an image, several subsequent stud"
2020.eamt-1.12,W16-2346,0,0.39395,"Missing"
2020.eamt-1.12,D16-1044,0,0.0377199,"eline NMT for the English–German task. Helcl, Libovick´y, and Variˇs (2018) set an additional attention sub-layer after the self-attention based on the Transformer architecture, and integrated grid visual features extracted by a pretrained CNN. Caglayan et al. (2018) enhanced the multimodal attention into the filtered attention, which filters out grid regions irrelevant to translation and focuses on the most important part of the grid visual features. They made efforts to integrate a stronger attention function, but the considered regions were still grid visual features. Grid visual features. Fukui et al. (2016) applied multimodal compact bilinear pooling to combine the grid visual features and text vectors, but their model does not convincingly surpass an attentionbased NMT baseline. Caglayan et al. (2016a) integrated local visual features extracted by ResNet-50 and source text vectors into an NMT decoder using shared transformation. They reported that the results obtained by their method did not surpass the results obtained by NMT systems. Caglayan, Barrault, and Bougares (2016b) proposed a multimodal attention mechanism based on (Caglayan et al., 2016a). They integrated two modalities by Image reg"
2020.eamt-1.12,W18-6441,0,0.0269252,"Missing"
2020.eamt-1.12,W16-2360,0,0.0519306,") have been increasingly focusing on incorporating multimodal contents, particularly images, to improve translations. Hence, researchers in this field have established a shared task called multimodal machine translation (MMT), which consists of translating a target sentence from a source language description into another language using information from the image described by the source sentence. The first MMT study by (Elliott et al., 2015) demonstrated the potential of improving the translation quality by using image. To effectively use an image, several subsequent studies (Gao et al., 2015; Huang et al., 2016; Calixto and Liu, 2017) incorporated global visual features extracted from the entire image by convolutional neural networks (CNNs) into a source word sequence or hidden states of a recurrent neural network (RNN). Furthermore, other studies started using local visual features in the context of an attention-based NMT. These features were extracted from equally-sized grids in an image by a CNN. For instance, multimodal attention (Caglayan et al., 2016b) has been designed for a mix of text and local visual features. Additionally, double attention mechanisms (Calixto et al., 2017) have been propo"
2020.eamt-1.12,W04-3250,0,0.0908902,"features on Multi30k dataset according to (Caglayan et al., 2017), which is the state-of-the-art system for En→De translation on this dataset. 3.3 Evaluation We evaluated the quality of the translation according to the token level BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) metrics. We trained all models (baselines and proposed model) three times and calculated the BLEU and METEOR scores, respectively. Based on the calculation results, we report the mean and standard deviation over three runs. Moreover, we report the statistical significance with bootstrap resampling (Koehn, 2004) using the merger of three test translation results. We defined the threshold for the statistical significance test as 0.01, and report only if the p-value was less than the threshold. 4 Results In Table 1, we present the results for the OpenNMT, doubly-attentive MNMT and our model on Multi30k dataset. Additionally, we also compared with Caglayan et al. (2017), which achieved the best performance under the same condition with our experiments. Comparing the baselines, the doubly-attentive MNMT outperformed OpenNMT. Because there did not exist a big difference amongst the three image feature ext"
2020.lrec-1.381,P11-2087,0,0.033932,"laced the most frequently used word with the target word. Many subsequent systems consist of similar pipelines: a candidate acquisition step (lexical substitution task in Figure 1) and a ranking step (word complexity estimation task in Figure 1).8 Approaches for acquiring substitution candidates include lexicon-based methods and distributional similarity-based methods. Devlin and Tait (1998), a pioneer in lexiconbased methods, has acquired synonyms from WordNet. Recent studies (Pavlick and Callison-Burch, 2016; Maddela and Xu, 2018) uses Simple PPDB, which can handle phrases as well as words. Biran et al. (2011) used distributional similarity for the ﬁrst time to select semantically close substitutions from synonyms obtained ˇ 2015; from WordNet. Recent studies (Glavaˇs and Stajner, Paetzold and Specia, 2016) uses word embeddings such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). 4 https://github.com/mounicam/lexical simpliﬁcation https://catalog.ldc.upenn.edu/LDC2006T13 6 http://www7a.biglobe.ne.jp/nifongo/data/ 7 http://jhlee.sakura.ne.jp/JEV.html 8 Since the target words are given in the evaluation dataset, many methods do not implement the CWI subtask in Figure 1. 5 3. C"
2020.lrec-1.381,N19-1423,0,0.0173356,"Missing"
2020.lrec-1.381,N13-1092,0,0.0378583,"nguage resources for Japanese lexical simpliﬁcation.1 Our contributions are the construction of the following three language resources in Japanese: • a large-scale word complexity lexicon, • a simpliﬁed synonym lexicon from complex words into simpler ones, • and a toolkit for developing and benchmarking a lexical simpliﬁcation system. 2. Related Work This section outlines the lexica used for lexical simpliﬁcation and the main approaches in lexical simpliﬁcation. 2.1. Lexica for Lexical Simpliﬁcation Simple PPDB (Pavlick and Callison-Burch, 2016), a subset of a large-scale paraphrase database (Ganitkevitch et al., 2013; Pavlick et al., 2015), is a language resource that extracts synonymous phrase pairs from complex phrases into simpler ones for lexical simpliﬁcation. Pavlick and Callison-Burch (2016) trained a classiﬁer to estimate which phrase in paraphrase pair is simpler based on a logistic regression model with features such as number of characters, number of words, frequency, part of speech, and word embeddings. 4.5 million simpliﬁed synonymous phrase pairs2 are publicly available which were extracted by applying this complexity estimator to the paraphrase database.3 Substitution ranking: Task of ranki"
2020.lrec-1.381,W11-2123,0,0.0153779,", 1-gram and 5-gram frequency. According to a survey paper on lexical simpliﬁcation (Paetzold and Specia, 2017a), L IGHT-LS achieves the highest accuracy in the English benchmark dataset. Our method acquired substitution candidates from the simpliﬁed synonym lexicon constructed in Section 4.4 We used the 5-gram language model and ˇ Glavaˇs and Stajner (2015)’s average ranking for the candidate ranking. The experimental settings are the same as in Section 3.2 and Section 4.2 We used the Skip-gram model (Mikolov et al., 2013) of the Asahi Shimbun word vector14 as word embeddings. We used KenLM (Heaﬁeld, 2011) to train a 5-gram language model on Japanese Wikipedia10 for candidate ranking. The 1-gram frequency was also calculated from Japanese Wikipedia. Table 7 shows that our method based on the simpliﬁed synonym lexicon outperformed L IGHT-LS ˇ 2015) in all evaluation metrics. (Glavaˇs and Stajner, As described in Section 4.3, since the pointwise method achieves higher performance than the pairwise method for estimating word pair complexity, the simpliﬁed synonym lexicon constructed using the pointwise method achieves higher precision and accuracy in this experiment. Since L IGHT-LS cannot rank hi"
2020.lrec-1.381,P14-2075,0,0.025626,"similarity, 1-gram and 5-gram frequency. The candidate with the highest average ranking is the generated output. 5.3. Evaluations We evaluated the performance of each method using an evaluation dataset (Kodaira et al., 2016) for Japanese lexical simpliﬁcation. This dataset contains one complex word in all 2, 010 sentences extracted from BCCWJ (Maekawa et al., 2010). Five annotators provide an average of 4.3 words of simple substitutions for each complex word. We used three evaluation metrics: accuracy, precision, and changed proportion, which are commonly used in English lexical simpliﬁcation(Horn et al., 2014; ˇ Glavaˇs and Stajner, 2015; Paetzold and Specia, 2017a). Accuracy: The ratio with which the highest ranking candidate is not the target word itself and is in the goldstandard. Precision: The ratio with which the highest ranking candidate is either the target word itself or is in the goldstandard. Changed Proportion: The ratio with which the highest ranking candidate is not the target word itself. A comparison method is L IGHT-LS ˇ (Glavaˇs and Stajner, 2015), which can extract features from raw corpus. L IGHT-LS acquires the top 10 words with the highest cosine similarity between word embed"
2020.lrec-1.381,W18-0521,1,0.801974,"imation. Training Development Test OOV Total Words 先生 (teacher) 信用 (conﬁdence) 胎盤 (placenta) 3.2. Experimental Settings 10 was used to count the freJapanese Wikipedia quency of characters and words. We used WikiExtractor11 to extract text from Wikipedia, and MeCab (Kudo et al., 2004) with IPADIC-2.7.0 for tokenization. In addition to Wikipedia, we used the word frequencies of the Tsukuba Web Corpus (TWC)12 and the Balanced Corpus of Contemporary Written Japanese (BCCWJ)13 (Maekawa et al., 2010) because counting word frequencies from multiple corpora improves the estimation of word complexity (Kajiwara and Komachi, 2018). Since the BCCWJ tags each word with a part-of-speech, we use it as a part-of-speech feature. We used a 300dimensional Skip-gram model (Mikolov et al., 2013) of the Asahi Shimbun word vector14 as our word embedding feature. This word embedding model is trained on approximately 8 million newspaper articles. Note that the performance of the Skip-gram model was better than the CBOW model (Mikolov et al., 2013) and the GloVe model (Pennington et al., 2014) included with the Skipgram model. As shown in Table 1, JEV was divided into a training set, development set and test set at a ratio of 8:1:1 f"
2020.lrec-1.381,L18-1072,0,0.0885669,"Missing"
2020.lrec-1.381,P16-3001,1,0.886177,"ubstitutions by counting the 1-gram frequencies. A recent study (Paetzold and Specia, 2016) uses the 5-gram frequency that considers context words around ˇ the target word. Glavaˇs and Stajner (2015) proposed an average ranking approach that ranks substitutions by word similarity, context similarity, 1-gram frequency, and 5-gram frequency. In English, a toolkit (Paetzold and Specia, 2015) for developing and benchmarking a lexical simpliﬁcation system is available that implements each of these methods.9 In this study, we release a Japanese version of the toolkit based on an evaluation dataset (Kodaira et al., 2016) for Japanese lexical simpliﬁcation. 2.2. Lexical Simpliﬁcation Approaches The Japanese Educational Vocabulary List (JEV)7 is a highquality word complexity lexicon constructed by Japanese language teachers. Because it is constructed manually, however, the number of recorded vocabularies is small. We construct a large-scale Japanese word complexity lexicon based on JEV for use in natural language processing applications such as lexical simpliﬁcation. Early studies of lexical simpliﬁcation (Devlin and Tait, 1998; Belder and Moens, 2010) obtained synonyms of the target word from WordNet (Miller,"
2020.lrec-1.381,W04-3230,0,0.217969,"mbun word vector. Ours w/o POS Ours w/o CF Ours w/o WF Ours w/o WE 0.757 0.735 0.716 0.671 Katakana characters is an easy word even if it has a large number of characters; on the contrary, a word composed of Kanji characters may be a difﬁcult word even if it has a small number of characters. Table 2: Accuracy for word complexity estimation. Training Development Test OOV Total Words 先生 (teacher) 信用 (conﬁdence) 胎盤 (placenta) 3.2. Experimental Settings 10 was used to count the freJapanese Wikipedia quency of characters and words. We used WikiExtractor11 to extract text from Wikipedia, and MeCab (Kudo et al., 2004) with IPADIC-2.7.0 for tokenization. In addition to Wikipedia, we used the word frequencies of the Tsukuba Web Corpus (TWC)12 and the Balanced Corpus of Contemporary Written Japanese (BCCWJ)13 (Maekawa et al., 2010) because counting word frequencies from multiple corpora improves the estimation of word complexity (Kajiwara and Komachi, 2018). Since the BCCWJ tags each word with a part-of-speech, we use it as a part-of-speech feature. We used a 300dimensional Skip-gram model (Mikolov et al., 2013) of the Asahi Shimbun word vector14 as our word embedding feature. This word embedding model is tra"
2020.lrec-1.381,D18-1410,0,0.312558,"and Specia, 2017a), which paraphrases complex words into simpler ones according to the context while maintaining the syntactic structure of the input sentence, is being actively researched (mainly in English). The lexical simpliﬁcation system is useful not only for assisting the reading comprehension of language learners and children but also for the preprocessing of natural language processing applications such as maˇ chine translation (Stajner and Popovi´c, 2016). Lexical simpliﬁcation has been studied mainly in English, which is rich in language resources such as a word complexity lexicon (Maddela and Xu, 2018), a paraphrase database from complex phrases to simpler ones (Pavlick and Callison-Burch, 2016) and a toolkit (Paetzold and Specia, 2015). Such language resources are not available for other languages, including Japanese. Lexical simpliﬁcation consists of the following four subtasks (Shardlow, 2014; Paetzold and Specia, 2017a): Complex word identiﬁcation: Task of deciding which words of a given sentence may not be understood by a given target audience and hence must be simpliﬁed. Substitution generation: Task of ﬁnding words or expressions that could replace the target complex word. Substituti"
2020.lrec-1.381,maekawa-etal-2010-design,0,0.036287,"i characters may be a difﬁcult word even if it has a small number of characters. Table 2: Accuracy for word complexity estimation. Training Development Test OOV Total Words 先生 (teacher) 信用 (conﬁdence) 胎盤 (placenta) 3.2. Experimental Settings 10 was used to count the freJapanese Wikipedia quency of characters and words. We used WikiExtractor11 to extract text from Wikipedia, and MeCab (Kudo et al., 2004) with IPADIC-2.7.0 for tokenization. In addition to Wikipedia, we used the word frequencies of the Tsukuba Web Corpus (TWC)12 and the Balanced Corpus of Contemporary Written Japanese (BCCWJ)13 (Maekawa et al., 2010) because counting word frequencies from multiple corpora improves the estimation of word complexity (Kajiwara and Komachi, 2018). Since the BCCWJ tags each word with a part-of-speech, we use it as a part-of-speech feature. We used a 300dimensional Skip-gram model (Mikolov et al., 2013) of the Asahi Shimbun word vector14 as our word embedding feature. This word embedding model is trained on approximately 8 million newspaper articles. Note that the performance of the Skip-gram model was better than the CBOW model (Mikolov et al., 2013) and the GloVe model (Pennington et al., 2014) included with"
2020.lrec-1.381,L18-1185,0,0.0378065,"Missing"
2020.lrec-1.381,P15-4015,0,0.335679,"ture of the input sentence, is being actively researched (mainly in English). The lexical simpliﬁcation system is useful not only for assisting the reading comprehension of language learners and children but also for the preprocessing of natural language processing applications such as maˇ chine translation (Stajner and Popovi´c, 2016). Lexical simpliﬁcation has been studied mainly in English, which is rich in language resources such as a word complexity lexicon (Maddela and Xu, 2018), a paraphrase database from complex phrases to simpler ones (Pavlick and Callison-Burch, 2016) and a toolkit (Paetzold and Specia, 2015). Such language resources are not available for other languages, including Japanese. Lexical simpliﬁcation consists of the following four subtasks (Shardlow, 2014; Paetzold and Specia, 2017a): Complex word identiﬁcation: Task of deciding which words of a given sentence may not be understood by a given target audience and hence must be simpliﬁed. Substitution generation: Task of ﬁnding words or expressions that could replace the target complex word. Substitution selection: Task of deciding which of the generated candidate substitutions can replace the complex word without compromising the sente"
2020.lrec-1.381,E17-2006,0,0.0937117,"of Japanese lexical simpliﬁcation. The current lexical simpliﬁcation is mainly studied in English, which is rich in language resources such as lexicons and toolkits. The language resources constructed in this study will help advance the lexical simpliﬁcation system in Japanese. Keywords: Lexical simpliﬁcation, word complexity, lexicon 1. Introduction Text simpliﬁcation (Shardlow, 2014) that rewrites a sentence into an easy-to-understand form for language learners (Petersen and Ostendorf, 2007) and children (Belder and Moens, 2010) is attracting attention. In particular, lexical simpliﬁcation (Paetzold and Specia, 2017a), which paraphrases complex words into simpler ones according to the context while maintaining the syntactic structure of the input sentence, is being actively researched (mainly in English). The lexical simpliﬁcation system is useful not only for assisting the reading comprehension of language learners and children but also for the preprocessing of natural language processing applications such as maˇ chine translation (Stajner and Popovi´c, 2016). Lexical simpliﬁcation has been studied mainly in English, which is rich in language resources such as a word complexity lexicon (Maddela and Xu,"
2020.lrec-1.381,P16-2024,0,0.171371,"the context while maintaining the syntactic structure of the input sentence, is being actively researched (mainly in English). The lexical simpliﬁcation system is useful not only for assisting the reading comprehension of language learners and children but also for the preprocessing of natural language processing applications such as maˇ chine translation (Stajner and Popovi´c, 2016). Lexical simpliﬁcation has been studied mainly in English, which is rich in language resources such as a word complexity lexicon (Maddela and Xu, 2018), a paraphrase database from complex phrases to simpler ones (Pavlick and Callison-Burch, 2016) and a toolkit (Paetzold and Specia, 2015). Such language resources are not available for other languages, including Japanese. Lexical simpliﬁcation consists of the following four subtasks (Shardlow, 2014; Paetzold and Specia, 2017a): Complex word identiﬁcation: Task of deciding which words of a given sentence may not be understood by a given target audience and hence must be simpliﬁed. Substitution generation: Task of ﬁnding words or expressions that could replace the target complex word. Substitution selection: Task of deciding which of the generated candidate substitutions can replace the c"
2020.lrec-1.381,N15-1023,0,0.0179605,"part of Table 2 shows that all the features contribute to the performance improvement as the performance decreases even if a feature is removed. 3.4. Word Complexity Lexicon for Japanese We estimated the word complexity of 40, 605 words that are shared vocabularies of Wikipedia, TWC, BCCWJ, and Asahi Shimbun word vector; these sources were used for feature extraction in our experiment and constructed a word complexity lexicon was constructed. Table 3 shows examples of the word complexity lexicon. 4. Constructing a Simpliﬁed Synonym Lexicon for Lexical Simpliﬁcation Following the simple PPDB (Pavlick and Nenkova, 2015; Pavlick and Callison-Burch, 2016), we automatically construct a simpliﬁed synonym lexicon for lexical simpliﬁcation that extracts complex-to-simple synonyms from a large-scale paraphrase database. In this study, we only convert from complex words to simpler synonyms because our complexity estimator is for words. Estimating a phrasal complexity is reserved for future work. 4.1. Proposed Method We construct a synonym lexicon for lexical simpliﬁcation by translating synonym pairs from complex words into simpler ones found in the paraphrase database. In order to identify complex and simple word"
2020.lrec-1.381,P15-2070,0,0.0768611,"se lexical simpliﬁcation.1 Our contributions are the construction of the following three language resources in Japanese: • a large-scale word complexity lexicon, • a simpliﬁed synonym lexicon from complex words into simpler ones, • and a toolkit for developing and benchmarking a lexical simpliﬁcation system. 2. Related Work This section outlines the lexica used for lexical simpliﬁcation and the main approaches in lexical simpliﬁcation. 2.1. Lexica for Lexical Simpliﬁcation Simple PPDB (Pavlick and Callison-Burch, 2016), a subset of a large-scale paraphrase database (Ganitkevitch et al., 2013; Pavlick et al., 2015), is a language resource that extracts synonymous phrase pairs from complex phrases into simpler ones for lexical simpliﬁcation. Pavlick and Callison-Burch (2016) trained a classiﬁer to estimate which phrase in paraphrase pair is simpler based on a logistic regression model with features such as number of characters, number of words, frequency, part of speech, and word embeddings. 4.5 million simpliﬁed synonymous phrase pairs2 are publicly available which were extracted by applying this complexity estimator to the paraphrase database.3 Substitution ranking: Task of ranking the remaining candid"
2020.lrec-1.381,D14-1162,0,0.0920571,"Missing"
2020.lrec-1.381,W16-3411,0,0.258385,"Missing"
2020.lrec-1.836,W19-5036,0,0.0131161,"ses both challenges, as it annotates weblogs written in colloquial style, where adverse drug reactions are described not only as phrases, but also as sentences or even longer spans. Recent studies show that a pre-trained model for text representation achieves impressive performances on various downstream tasks, such as automatic question answering and natural language inference (Devlin et al., 2019). Alsentzer et al. (2019) adapted the pre-trained model to the medical domain. Further, sentiments in texts are useful clues to identify adverse effects (Sarker and Gonzalez, 2015; Wu et al., 2018; Alhuzali and Ananiadou, 2019), which is reasonable because adverse effects are firmly negative events for patients. These approaches are promising for adverse effect mining on patient-generated texts, from online forums to weblogs including our dataset. 6. Potential Task Designs with Our Dataset Our annotation dataset is a valuable resource to advance research on the automatic detection of drug effects. More broadly, it is useful for research on knowledge extraction from patient-generated texts. Various types of tasks can be designed using our dataset. To name a few: 1. Automatic linking of texts describing drug reactions"
2020.lrec-1.836,W19-1909,0,0.0116813,"ern mining (Stilo et al., 2013) and applied deep neural networks (Limsopatham and Collier, 2016) to map colloquial expressions in posts onto formal writing used in medical concepts. Our dataset poses both challenges, as it annotates weblogs written in colloquial style, where adverse drug reactions are described not only as phrases, but also as sentences or even longer spans. Recent studies show that a pre-trained model for text representation achieves impressive performances on various downstream tasks, such as automatic question answering and natural language inference (Devlin et al., 2019). Alsentzer et al. (2019) adapted the pre-trained model to the medical domain. Further, sentiments in texts are useful clues to identify adverse effects (Sarker and Gonzalez, 2015; Wu et al., 2018; Alhuzali and Ananiadou, 2019), which is reasonable because adverse effects are firmly negative events for patients. These approaches are promising for adverse effect mining on patient-generated texts, from online forums to weblogs including our dataset. 6. Potential Task Designs with Our Dataset Our annotation dataset is a valuable resource to advance research on the automatic detection of drug effects. More broadly, it is"
2020.lrec-1.836,N19-1423,0,0.00602232,"ious studies used pattern mining (Stilo et al., 2013) and applied deep neural networks (Limsopatham and Collier, 2016) to map colloquial expressions in posts onto formal writing used in medical concepts. Our dataset poses both challenges, as it annotates weblogs written in colloquial style, where adverse drug reactions are described not only as phrases, but also as sentences or even longer spans. Recent studies show that a pre-trained model for text representation achieves impressive performances on various downstream tasks, such as automatic question answering and natural language inference (Devlin et al., 2019). Alsentzer et al. (2019) adapted the pre-trained model to the medical domain. Further, sentiments in texts are useful clues to identify adverse effects (Sarker and Gonzalez, 2015; Wu et al., 2018; Alhuzali and Ananiadou, 2019), which is reasonable because adverse effects are firmly negative events for patients. These approaches are promising for adverse effect mining on patient-generated texts, from online forums to weblogs including our dataset. 6. Potential Task Designs with Our Dataset Our annotation dataset is a valuable resource to advance research on the automatic detection of drug effe"
2020.lrec-1.836,W10-1915,0,0.291478,"of drugs, we focused on a third source of information—patient weblogs. There is a world-wide trend of sharing fight experiences against diseases on the internet. A typical example is PatientsLikeMe.3 Patients write about their real experiences in their weblog articles, exchange comments, and share information that they curated, to help each other fight and survive diseases. Because of their nature, patient weblogs provide lively descriptions of their physical conditions. Previous studies confirmed the usefulness of such patient self-reporting notes for finding information on adverse effects (Leaman et al., 2010; Nikfarjam and Gonzalez, 2011; Yang et al., 2012). Nikfarjam et al. (2015) annotated adverse effects on posts mined from a health-related online forum and Twitter,4 where shortness of content is typical. In contrast with their dataset, we targeted weblogs, 2 https://www.pmda.go.jp/safety/ info-services/drugs/adr-info/suspected-adr/ 0006.html (in Japanese) 3 https://www.patientslikeme.com/ 4 https://twitter.com/ 6769 タグリッソ服用丸２か月が経ちました。今のとこ ろの副作用は、相変わらずの舌炎症。はじめは、 米や小麦料理がザラザラした舌触りで不味いって 感じでした。 (It has been two months since the start of Tagrisso. Its adverse effect is glossitis, as I have always"
2020.lrec-1.836,P16-1096,0,0.0620381,"ose in general domains. To identify spans describing adverse drug reactions, previous studies used lexicons in the medical domain (Leaman et al., 2010; Yang et al., 2012), lexical patterns identified by association rule mining (Nikfarjam and Gonzalez, 2011), and word embedding (Nikfarjam et al., 2015). Differences in style of the texts is another issue. People write posts to forums and Twitter in a colloquial style, which is significantly different from the formal style used to define medical concepts. Previous studies used pattern mining (Stilo et al., 2013) and applied deep neural networks (Limsopatham and Collier, 2016) to map colloquial expressions in posts onto formal writing used in medical concepts. Our dataset poses both challenges, as it annotates weblogs written in colloquial style, where adverse drug reactions are described not only as phrases, but also as sentences or even longer spans. Recent studies show that a pre-trained model for text representation achieves impressive performances on various downstream tasks, such as automatic question answering and natural language inference (Devlin et al., 2019). Alsentzer et al. (2019) adapted the pre-trained model to the medical domain. Further, sentiments"
2020.lrec-1.836,N18-1100,0,0.0283833,"nowledge extraction from patient-generated texts. Various types of tasks can be designed using our dataset. To name a few: 1. Automatic linking of texts describing drug reactions to concept IDs in a medical ontology 2. Prediction of effect type given an article and drug names discussed in the article 3. Span identification describing drug effects given an article 4. Span identification and labelling of drug names, corresponding ICD-10 codes, and effect types The first task automatically identifies spans describing drug effects and maps onto standard codes or IDs defined in a medical ontology (Mullenbach et al., 2018; Limsopatham and Collier, 2016). The second task is a variant of the aspect-based sentiment analysis in the domain of drug reaction detection. The third task is a kind of sequential labelling problems, but with a number of labels as large as related drug names. The fourth task is an advanced sequential labelling problem with multiple types of labels with dependent relations. This task conforms to a more practical setting on drug effect mining. As discussed in Sec. 4.2., our annotation dataset consists of challenging examples that (a) have many-to-many correspondences between drugs and drug re"
2020.lrec-1.836,W95-0107,0,0.680353,"g these questions. The guideline was immediately updated whenever necessary then shared with annotators to maintain a consistent standard. 4. Analysis of Annotation Results Over the cumulative total of 1, 500 annotated articles, the number of identified drug names was 108, and the number of identified ICD-10 codes was 104. In this section, we discuss the quality and consolidate the annotation results. 4.1. Agreement Rates of Annotations To examine the agreement level of the annotations, we formatted the annotation results with the character-level Inside–Outside–Beginning (IOB) tagging scheme (Ramshaw and Marcus, 1995) and calculated Fleiss’ kappa. The IOB tagging scheme is common for evaluating named entity recognition, which is a sequential tagging task of named 10 9 4,147 27 693 The dictionary of Japanese drug names will be released at our web site. 6772 IOB (span only) IOB+drug name IOB+ICD-10 IOB+effect type IOB+all labels Fleiss’ kappa # of unique tags 0.635 0.615 0.565 0.589 0.520 3 287 211 59 1,020 Table 6: Frequency of types of effects Table 4: Agreement rates of annotations. Drug name ICD-10 code 169 677 87 78 8.1 328.5 2.5 1.6 40 30 20 10 0 10.0 Table 5: Statistics of consolidated annotations. en"
2020.lrec-1.836,R13-1084,0,0.0600326,"Missing"
2020.lrec-1.836,W18-5909,0,0.0131146,"s. Our dataset poses both challenges, as it annotates weblogs written in colloquial style, where adverse drug reactions are described not only as phrases, but also as sentences or even longer spans. Recent studies show that a pre-trained model for text representation achieves impressive performances on various downstream tasks, such as automatic question answering and natural language inference (Devlin et al., 2019). Alsentzer et al. (2019) adapted the pre-trained model to the medical domain. Further, sentiments in texts are useful clues to identify adverse effects (Sarker and Gonzalez, 2015; Wu et al., 2018; Alhuzali and Ananiadou, 2019), which is reasonable because adverse effects are firmly negative events for patients. These approaches are promising for adverse effect mining on patient-generated texts, from online forums to weblogs including our dataset. 6. Potential Task Designs with Our Dataset Our annotation dataset is a valuable resource to advance research on the automatic detection of drug effects. More broadly, it is useful for research on knowledge extraction from patient-generated texts. Various types of tasks can be designed using our dataset. To name a few: 1. Automatic linking of"
2020.lrec-1.847,D17-1001,1,0.892833,"Missing"
2020.lrec-1.847,Q17-1010,0,0.0652895,"ments by measuring the quality of word alignments inside (Yao et al., 2013; Ouyang and McKeown, 2019). We also use the precision, recall, and F-measure of word alignments as evaluation metrics following the previous studies. For our phrase alignment results, we regard that every word pair in a phrase alignment has an alignment, following Ouyang and McKeown (2019). 3.3. . ↔. both enterprises . ↔ either case . 3.2. Implementation Details As word embedding models, we investigate the effects of static and dynamic embeddings. As the static word embeddings, we use the pre-trained model of fastText (Bojanowski et al., 2017).3 As the dynamic word embeddings, we use the pre-trained model of BERT (Devlin et al., 2019) that generates embeddings from the contexts.4 Specifically, we use the output of the last layer of each token. Table 1 summarizes the hyperparameters in SAPPHIRE. We tuned these hyperparameters by a grid search to maximize the F-measure score at the development set. We searched settings of λ and δ from the range of [0.5, 0.9] with 0.1 intervals, and α from [0.05, 0.10] with 0.01 intervals. 3.4. Results Table 2 shows the results of the phrase alignment evaluation on the test set of MSR RTE corpus. The"
2020.lrec-1.847,N19-1423,0,0.0153139,"019). We also use the precision, recall, and F-measure of word alignments as evaluation metrics following the previous studies. For our phrase alignment results, we regard that every word pair in a phrase alignment has an alignment, following Ouyang and McKeown (2019). 3.3. . ↔. both enterprises . ↔ either case . 3.2. Implementation Details As word embedding models, we investigate the effects of static and dynamic embeddings. As the static word embeddings, we use the pre-trained model of fastText (Bojanowski et al., 2017).3 As the dynamic word embeddings, we use the pre-trained model of BERT (Devlin et al., 2019) that generates embeddings from the contexts.4 Specifically, we use the output of the last layer of each token. Table 1 summarizes the hyperparameters in SAPPHIRE. We tuned these hyperparameters by a grid search to maximize the F-measure score at the development set. We searched settings of λ and δ from the range of [0.5, 0.9] with 0.1 intervals, and α from [0.05, 0.10] with 0.01 intervals. 3.4. Results Table 2 shows the results of the phrase alignment evaluation on the test set of MSR RTE corpus. The scores of the method proposed by Ouyang and McKeown (2019) are borrowed from their paper, whi"
2020.lrec-1.847,D19-1006,0,0.0616378,"ic) The ROE printed here were issued by General Jean Cot , then commander of U.N. forces , and were intended to establish the conditions under which the forces could use their weapons as they carry out the U.N. peacekeeping mission in Bosnia . U.N. peacekeeping forces withdrew from Bosnia . (d) Phrase alignment output by SAPPHIRE on a sentence pair with a large length difference Figure 3: Examples of phrase alignments on MSR RTE corpus score than BERT. This result is from the side-effect of contextualized word embeddings; words in semantically similar sentences tend to have closer embeddings (Ethayarajh, 2019). Because most sentence pairs in MSR RTE corpus are semantically relevant due to its purpose of RTE, the side-effect of contextualized word embeddings should be pronounced. When we compare word alignment methods of the growdiag-final heuristic and the Hungarian algorithm, the former has a slightly higher recall but lower precision, and the latter has higher precision but lower recall. These characteristics are more noticeable when λ is small. Figure 3 shows a couple of examples of the phrase alignment by SAPPHIRE with fastText embedding. Figure 3 (a) and 3 (b) show that SAPPHIRE correctly iden"
2020.lrec-1.847,N13-1092,0,0.137332,"Missing"
2020.lrec-1.847,C16-1109,1,0.838969,"lingual phrase alignment is one of the fundamental tasks in natural language understanding. It identifies the most plausible phrase alignments that are semantically equivalent in a monolingual sentence pair. The applications of monolingual phrase alignment are diverse. The most relevant application is the sentence pair modeling tasks (Lan and Xu, 2018), such as recognizing textual entailment (Dagan et al., 2006) and assessing semantic textual similarity (Sultan et al., 2014). Besides, monolingual phrase alignment is useful to identify parallel sentences automatically from a comparable corpus (Kajiwara and Komachi, 2016) and to generate paraphrases (Li et al., 2019). Previous studies on monolingual phrase alignment (MacCartney et al., 2008; Yao et al., 2013; Arase and Tsujii, 2017; Ouyang and McKeown, 2019) depend on largescale paraphrase dictionaries or assume the availability of high-quality parsers or chunkers, which severely restricts the applicability of alignment methods to corpora of specific domains and different languages. On the other hand, bilingual phrase extraction that has been widely studied in the field of statistical machine translation (SMT) only assumes the availability of a large-scale par"
2020.lrec-1.847,N03-1017,0,0.0611213,"gnments from both directions. Next, the grow-diag-final heuristic adds alignments from the union set, considering its association matrix if they meet the following conditions. • Alignments whose words are adjacent in the vertical, horizontal, or diagonal directions of alignments in the initial candidate set. Embedding-based Word Alignment We regard a phrase of null does not have a span, and hence it does not overlap with any phrases. (2) k • Alignments whose words have no alignment in the initial candidate set. SAPPHIRE obtains word alignment candidates based on the grow-diag-final heuristic (Koehn et al., 2003) designed 2 (1) k Overview of Alignment Process Figure 1 shows the overview of the phrase alignment process of SAPPHIRE. First, SAPPHIRE obtains word alignments based on cosine similarities between every pair of word embeddings in the sentence pair. Next, it extends the word alignments to phrase alignments using methods for bilingual phrase extraction. Finally, SAPPHIRE searches a set of consistent phrase alignments on a lattice constructed from phrase alignment candidates. 2.3. for bilingual phrase alignment and the extended Hungarian algorithm for rectangle matrices (Bourgeois and Lassalle,"
2020.lrec-1.847,P07-2045,0,0.0105781,"o-one word alignments by the Hungarian algorithm. The cost of each word pair (xi , yj ) is cost(xi , yj ) = 1 − cos(exi , eyj ). (3) The Hungarian algorithm minimizes the cost in the cost matrix as XX min Ci,j Zi,j , (4) i j where Ci,j is the cost of between xi and yj , and Z is the final word alignment matrix. Zi,j = 1 if row i and column j is assigned, i.e., xi is aligned to yj , and Zi,j = 0, otherwise. 2.4. Extraction of Phrase Alignment Candidates SAPPHIRE obtains phrase alignment candidates by expanding the word alignments based on the bilingual phrase alignment heuristic used by Moses (Koehn et al., 2007). Algorithm 1 presents the algorithm of phrase alignment candidate extraction. It generates a phrase alignment that covers an arbitrary pair of word alignments (lines 5 to 7). It expands the phrase alignment if there are adjacent word alignments (lines 8 to 14). As Algorithm 1 shows, a phrase means a word n-gram, which can be a single word to the entire sentence. However, we can easily adapt to align grammatical phrases by restricting phrase alignment candidates to conform to predetermined spans of phrases. For each pair of phrase alignment candidates, SAPPHIRE computes a score to estimate the"
2020.lrec-1.847,C18-1328,0,0.0155305,"ignment score. Experimental results using the standard dataset for phrase alignment evaluation show that SAPPHIRE outperforms the previous method and establishes the state-of-the-art performance. Keywords: phrase alignment, phrasal paraphrase 1. Introduction Monolingual phrase alignment is one of the fundamental tasks in natural language understanding. It identifies the most plausible phrase alignments that are semantically equivalent in a monolingual sentence pair. The applications of monolingual phrase alignment are diverse. The most relevant application is the sentence pair modeling tasks (Lan and Xu, 2018), such as recognizing textual entailment (Dagan et al., 2006) and assessing semantic textual similarity (Sultan et al., 2014). Besides, monolingual phrase alignment is useful to identify parallel sentences automatically from a comparable corpus (Kajiwara and Komachi, 2016) and to generate paraphrases (Li et al., 2019). Previous studies on monolingual phrase alignment (MacCartney et al., 2008; Yao et al., 2013; Arase and Tsujii, 2017; Ouyang and McKeown, 2019) depend on largescale paraphrase dictionaries or assume the availability of high-quality parsers or chunkers, which severely restricts th"
2020.lrec-1.847,P19-1332,0,0.0150792,"in natural language understanding. It identifies the most plausible phrase alignments that are semantically equivalent in a monolingual sentence pair. The applications of monolingual phrase alignment are diverse. The most relevant application is the sentence pair modeling tasks (Lan and Xu, 2018), such as recognizing textual entailment (Dagan et al., 2006) and assessing semantic textual similarity (Sultan et al., 2014). Besides, monolingual phrase alignment is useful to identify parallel sentences automatically from a comparable corpus (Kajiwara and Komachi, 2016) and to generate paraphrases (Li et al., 2019). Previous studies on monolingual phrase alignment (MacCartney et al., 2008; Yao et al., 2013; Arase and Tsujii, 2017; Ouyang and McKeown, 2019) depend on largescale paraphrase dictionaries or assume the availability of high-quality parsers or chunkers, which severely restricts the applicability of alignment methods to corpora of specific domains and different languages. On the other hand, bilingual phrase extraction that has been widely studied in the field of statistical machine translation (SMT) only assumes the availability of a large-scale parallel corpus. The standard approach is first i"
2020.lrec-1.847,D08-1084,0,0.130053,"phrase alignments that are semantically equivalent in a monolingual sentence pair. The applications of monolingual phrase alignment are diverse. The most relevant application is the sentence pair modeling tasks (Lan and Xu, 2018), such as recognizing textual entailment (Dagan et al., 2006) and assessing semantic textual similarity (Sultan et al., 2014). Besides, monolingual phrase alignment is useful to identify parallel sentences automatically from a comparable corpus (Kajiwara and Komachi, 2016) and to generate paraphrases (Li et al., 2019). Previous studies on monolingual phrase alignment (MacCartney et al., 2008; Yao et al., 2013; Arase and Tsujii, 2017; Ouyang and McKeown, 2019) depend on largescale paraphrase dictionaries or assume the availability of high-quality parsers or chunkers, which severely restricts the applicability of alignment methods to corpora of specific domains and different languages. On the other hand, bilingual phrase extraction that has been widely studied in the field of statistical machine translation (SMT) only assumes the availability of a large-scale parallel corpus. The standard approach is first identifying word alignment and then composing phrase alignments from the ide"
2020.lrec-1.847,J03-1002,0,0.0367869,"these resources assume to be applied to problems in the general domain, their performances are likely degraded in domain-specific areas. Besides, such resources are unlikely available other than in English. Different from these previous methods, SAPPHIRE is easily adaptable to any domains or languages because it requires only a raw corpus to train word embedding models. Furthermore, SAPPHIRE can handle both types of phrases with or without grammatical constraints. Bilingual phrase pair extraction is a common technique in SMT. The typical approach is first obtaining word alignments by GIZA++ (Och and Ney, 2003) and then composing phrase alignment pairs. Different from the monolingual setting, bilingual word alignment can assume that an abundant parallel corpus is available. SAPPHIRE, on the other hand, requires only a raw corpus to train word embeddings, which is far easier to collect than monolingual parallel (i.e., paraphrase) corpora. 5. Conclusion We proposed SAPPHIRE, a simple phrase aligner that depends only on a monolingual corpus. Experiment results showed that SAPPHIRE outperformed the previous method and achieved the state-of-the-art phrase alignment F-measure score on the MSR RTE corpus."
2020.lrec-1.847,P19-1467,0,0.285734,"sentence pair. The applications of monolingual phrase alignment are diverse. The most relevant application is the sentence pair modeling tasks (Lan and Xu, 2018), such as recognizing textual entailment (Dagan et al., 2006) and assessing semantic textual similarity (Sultan et al., 2014). Besides, monolingual phrase alignment is useful to identify parallel sentences automatically from a comparable corpus (Kajiwara and Komachi, 2016) and to generate paraphrases (Li et al., 2019). Previous studies on monolingual phrase alignment (MacCartney et al., 2008; Yao et al., 2013; Arase and Tsujii, 2017; Ouyang and McKeown, 2019) depend on largescale paraphrase dictionaries or assume the availability of high-quality parsers or chunkers, which severely restricts the applicability of alignment methods to corpora of specific domains and different languages. On the other hand, bilingual phrase extraction that has been widely studied in the field of statistical machine translation (SMT) only assumes the availability of a large-scale parallel corpus. The standard approach is first identifying word alignment and then composing phrase alignments from the identified word alignments based on heuristics. However, the purpose of"
2020.lrec-1.847,N15-1138,0,0.0287894,"bilingual phrase extraction. Finally, SAPPHIRE searches a set of consistent phrase alignments on a lattice constructed from phrase alignment candidates. 2.3. for bilingual phrase alignment and the extended Hungarian algorithm for rectangle matrices (Bourgeois and Lassalle, 1971) using cosine similarities between word embeddings. Word alignments obtained by the grow-diag-final heuristic and the Hungarian algorithm might be unreliable. Hence, SAPPHIRE selects final word alignments among the obtained alignment candidates whose cosine similarities are greater than or equal to a threshold λ, like Song and Roth (2015). Note that the final set of word alignment candidates have many-to-many alignments. 6862 Algorithm 1 Extraction of Phrase Alignment Candidates Input: Index pairs of word alignments W = {(i, j)}, a sentence pair of X and Y Output: Phrase alignment candidates U = {(xqp , yrs )} 1: Initialization: U ← ∅, M ← 0 2: for (i, j) in W do 3: Mi,j ← 1 . Create a word alignment matrix 4: for (i, j), (i0 , j 0 ) in W do 5: is = min(i, i0 ), ie = max(i, i0 ) 6: js = min(j, j 0 ), je = max(j, j 0 ) 7: u ← (xiies , yjjse ) 8: while do 9: if u has adjacent word alignments Ms in vertical and horizontal directi"
2020.lrec-1.847,S14-2039,0,0.139084,"rms the previous method and establishes the state-of-the-art performance. Keywords: phrase alignment, phrasal paraphrase 1. Introduction Monolingual phrase alignment is one of the fundamental tasks in natural language understanding. It identifies the most plausible phrase alignments that are semantically equivalent in a monolingual sentence pair. The applications of monolingual phrase alignment are diverse. The most relevant application is the sentence pair modeling tasks (Lan and Xu, 2018), such as recognizing textual entailment (Dagan et al., 2006) and assessing semantic textual similarity (Sultan et al., 2014). Besides, monolingual phrase alignment is useful to identify parallel sentences automatically from a comparable corpus (Kajiwara and Komachi, 2016) and to generate paraphrases (Li et al., 2019). Previous studies on monolingual phrase alignment (MacCartney et al., 2008; Yao et al., 2013; Arase and Tsujii, 2017; Ouyang and McKeown, 2019) depend on largescale paraphrase dictionaries or assume the availability of high-quality parsers or chunkers, which severely restricts the applicability of alignment methods to corpora of specific domains and different languages. On the other hand, bilingual phr"
2020.lrec-1.847,P11-2044,0,0.0300088,"r phrases of U.N. forces and U.N. peacekeepingto the phrase of U.N. peacekeeping forces in the second sentence. For such a challenging sentence pair, SAPPHIRE correctly identifies the alignment U.N. forces ↔ U.N. peacekeeping forces. 4. Related Work There have been two approaches in monolingual phrase alignment; one aligns arbitrary phrases without grammat6865 ical constraints, and the other aligns phrases defined by a grammar. In any case, previous methods of monolingual phrase alignment are resource-intensive. As the first approach, MANLI (MacCartney et al., 2008) and the following studies (Thadani and McKeown, 2011; Thadani et al., 2012), as well as Yao et al. (2013), depend on lexical database of WordNet (Miller, 1995) or paraphrase database of PPDB (Ganitkevitch et al., 2013) for feature extraction from arbitrary phrases that are simply n-grams. The second approach, on the other hand, needs reliable parsers or chunkers to identify phrase boundaries. Phrase alignment methods proposed by Ouyang and McKeown (2019) uses chunkers while the method proposed by Sultan et al. (2014) and Arase and Tsujii (2017) depend on the syntactic parser to obtain phrase structures. Although these lexical and paraphrase dic"
2020.lrec-1.847,C12-2120,0,0.018069,"d U.N. peacekeepingto the phrase of U.N. peacekeeping forces in the second sentence. For such a challenging sentence pair, SAPPHIRE correctly identifies the alignment U.N. forces ↔ U.N. peacekeeping forces. 4. Related Work There have been two approaches in monolingual phrase alignment; one aligns arbitrary phrases without grammat6865 ical constraints, and the other aligns phrases defined by a grammar. In any case, previous methods of monolingual phrase alignment are resource-intensive. As the first approach, MANLI (MacCartney et al., 2008) and the following studies (Thadani and McKeown, 2011; Thadani et al., 2012), as well as Yao et al. (2013), depend on lexical database of WordNet (Miller, 1995) or paraphrase database of PPDB (Ganitkevitch et al., 2013) for feature extraction from arbitrary phrases that are simply n-grams. The second approach, on the other hand, needs reliable parsers or chunkers to identify phrase boundaries. Phrase alignment methods proposed by Ouyang and McKeown (2019) uses chunkers while the method proposed by Sultan et al. (2014) and Arase and Tsujii (2017) depend on the syntactic parser to obtain phrase structures. Although these lexical and paraphrase dictionaries, chunkers, an"
2020.lrec-1.847,D13-1056,0,0.0265161,"Missing"
2020.wmt-1.120,C04-1046,0,0.134588,"ac.jp shimanaka-hiroki@ed.tmu.ac.jp Tomoyuki Kajiwara Osaka University Mamoru Komachi Tokyo Metropolitan University kajiwara@ids.osaka-u.ac.jp komachi@tmu.ac.jp Abstract We introduce the TMUOU1 submission for the WMT20 Quality Estimation Shared Task 1: Sentence-Level Direct Assessment. Our system is an ensemble model of four regression models based on XLM-RoBERTa with language tags. We ranked 4th in Pearson and 2nd in MAE and RMSE on a multilingual track. 1 Introduction Quality Estimation (QE) is a task of estimating translation quality without reference sentences (Gandrabur and Foster, 2003; Blatz et al., 2004; Specia et al., 2018). Automatic evaluation metrics based on reference sentences, such as BLEU (Papineni et al., 2002), have contributed to improving translation quality on benchmark datasets. However, in situations where machine translation (MT) is actually used, these metrics are sometimes unable to assess the translation quality owing to the lack of reference sentences. The development of QE methods that are well correlated with manual evaluations enable users to decide whether to use the translation results as is, post-edit the results, or employ other machine translations. At the Confere"
2020.wmt-1.120,W17-4755,0,0.0851111,"anslation results as is, post-edit the results, or employ other machine translations. At the Conference on Machine Translation (WMT), there have been conducted several QErelated competitions such as the QE task (Fonseca et al., 2019) for estimating post-edit rate HTER (Snover et al., 2006) and the QE as a Metric task (Ma et al., 2019) for relative evaluations of translation quality. This year, the WMT QE task held a new competition (Specia et al., 2020) on absolute evaluations of translation quality. In task 1, sentences are annotated with direct assessment (DA) scores as in the metrics task (Bojar et al., 2017). 1 We have been working on the metrics task with an approach that uses pre-trained sentence encoders (Shimanaka et al., 2018, 2019). Shimanaka et al. (2018) employed InferSent (Conneau et al., 2017), Quick-Thought (Logeswaran and Lee, 2018), and Universal Sentence Encoder (Cer et al., 2018) as encoders, and achieved the highest performance in all to-English language pairs of WMT18 metrics shared task (Ma et al., 2018). Subsequently, Shimanaka et al. (2019) employed BERT (Devlin et al., 2019) as an encoder to further improve the correlation with manual evaluations. In this study, we apply simi"
2020.wmt-1.120,D18-2029,0,0.0765928,"Missing"
2020.wmt-1.120,2020.acl-main.747,0,0.114035,"Missing"
2020.wmt-1.120,D17-1070,0,0.0257736,"s the QE task (Fonseca et al., 2019) for estimating post-edit rate HTER (Snover et al., 2006) and the QE as a Metric task (Ma et al., 2019) for relative evaluations of translation quality. This year, the WMT QE task held a new competition (Specia et al., 2020) on absolute evaluations of translation quality. In task 1, sentences are annotated with direct assessment (DA) scores as in the metrics task (Bojar et al., 2017). 1 We have been working on the metrics task with an approach that uses pre-trained sentence encoders (Shimanaka et al., 2018, 2019). Shimanaka et al. (2018) employed InferSent (Conneau et al., 2017), Quick-Thought (Logeswaran and Lee, 2018), and Universal Sentence Encoder (Cer et al., 2018) as encoders, and achieved the highest performance in all to-English language pairs of WMT18 metrics shared task (Ma et al., 2018). Subsequently, Shimanaka et al. (2019) employed BERT (Devlin et al., 2019) as an encoder to further improve the correlation with manual evaluations. In this study, we apply similar approaches to the QE task. However, to support both source and target languages, we employ XLM-RoBERTa2 (Conneau et al., 2020), a pre-trained multilingual sentence encoder. 2 WMT20 QE Shared Task"
2020.wmt-1.120,N19-1423,0,0.205736,"quality. In task 1, sentences are annotated with direct assessment (DA) scores as in the metrics task (Bojar et al., 2017). 1 We have been working on the metrics task with an approach that uses pre-trained sentence encoders (Shimanaka et al., 2018, 2019). Shimanaka et al. (2018) employed InferSent (Conneau et al., 2017), Quick-Thought (Logeswaran and Lee, 2018), and Universal Sentence Encoder (Cer et al., 2018) as encoders, and achieved the highest performance in all to-English language pairs of WMT18 metrics shared task (Ma et al., 2018). Subsequently, Shimanaka et al. (2019) employed BERT (Devlin et al., 2019) as an encoder to further improve the correlation with manual evaluations. In this study, we apply similar approaches to the QE task. However, to support both source and target languages, we employ XLM-RoBERTa2 (Conneau et al., 2020), a pre-trained multilingual sentence encoder. 2 WMT20 QE Shared Task 1 In the WMT20 QE task 1 (Sentence-Level Direct Assessment), participants predict translation quality at the sentence level from pairs of source and MT output sentences. This task provides datasets for seven language pairs and sets up a multilingual track for a language-independent approach. 2.1"
2020.wmt-1.120,W03-0413,0,0.168434,"kamachi.akifumi@ist.osaka-u.ac.jp shimanaka-hiroki@ed.tmu.ac.jp Tomoyuki Kajiwara Osaka University Mamoru Komachi Tokyo Metropolitan University kajiwara@ids.osaka-u.ac.jp komachi@tmu.ac.jp Abstract We introduce the TMUOU1 submission for the WMT20 Quality Estimation Shared Task 1: Sentence-Level Direct Assessment. Our system is an ensemble model of four regression models based on XLM-RoBERTa with language tags. We ranked 4th in Pearson and 2nd in MAE and RMSE on a multilingual track. 1 Introduction Quality Estimation (QE) is a task of estimating translation quality without reference sentences (Gandrabur and Foster, 2003; Blatz et al., 2004; Specia et al., 2018). Automatic evaluation metrics based on reference sentences, such as BLEU (Papineni et al., 2002), have contributed to improving translation quality on benchmark datasets. However, in situations where machine translation (MT) is actually used, these metrics are sometimes unable to assess the translation quality owing to the lack of reference sentences. The development of QE methods that are well correlated with manual evaluations enable users to decide whether to use the translation results as is, post-edit the results, or employ other machine translat"
2020.wmt-1.120,D19-1632,0,0.0757872,"Missing"
2020.wmt-1.120,P19-3020,0,0.0904359,"Missing"
2020.wmt-1.120,W17-4763,0,0.0600966,"of English-German dataset. Three or more professional translators annotated DA scores in the range of 0-100 points for each pair of source and MT output sentences. These annotations are following the FLORES setup (Guzm´an et al., 2019). The dataset consists of pairs of source and MT output sentences, z-standardized DA scores, and MT model score (log probabilities for words). Table 1 shows examples of the dataset. For each language pair, 7,000 training sets, 1,000 development sets, and 1,000 test sets are provided. 2.2 Baseline and Evaluation The baseline system is a Predictor-Estimator model (Kim et al., 2017) implemented in OpenKiwi3 (Kepler et al., 2019). The predictor is trained on a parallel corpus used to train the MT model, and predicts each target token from source and target contexts. And the estimator predicts the QE score from features produced by the predictor. Participants are evaluated by Pearson’s correlation metric (Pearson), mean absolute error (MAE), and root mean squared error (RMSE). A z-standardized DA score is used as a gold label. 3 TMUOU System Our system is an ensemble model of four regression models based on XLM-RoBERTa (Conneau et al., 2020) with language tags. We first ex"
2020.wmt-1.120,W18-6450,0,0.0117523,"ompetition (Specia et al., 2020) on absolute evaluations of translation quality. In task 1, sentences are annotated with direct assessment (DA) scores as in the metrics task (Bojar et al., 2017). 1 We have been working on the metrics task with an approach that uses pre-trained sentence encoders (Shimanaka et al., 2018, 2019). Shimanaka et al. (2018) employed InferSent (Conneau et al., 2017), Quick-Thought (Logeswaran and Lee, 2018), and Universal Sentence Encoder (Cer et al., 2018) as encoders, and achieved the highest performance in all to-English language pairs of WMT18 metrics shared task (Ma et al., 2018). Subsequently, Shimanaka et al. (2019) employed BERT (Devlin et al., 2019) as an encoder to further improve the correlation with manual evaluations. In this study, we apply similar approaches to the QE task. However, to support both source and target languages, we employ XLM-RoBERTa2 (Conneau et al., 2020), a pre-trained multilingual sentence encoder. 2 WMT20 QE Shared Task 1 In the WMT20 QE task 1 (Sentence-Level Direct Assessment), participants predict translation quality at the sentence level from pairs of source and MT output sentences. This task provides datasets for seven language pairs"
2020.wmt-1.120,W19-5302,0,0.0137512,"re machine translation (MT) is actually used, these metrics are sometimes unable to assess the translation quality owing to the lack of reference sentences. The development of QE methods that are well correlated with manual evaluations enable users to decide whether to use the translation results as is, post-edit the results, or employ other machine translations. At the Conference on Machine Translation (WMT), there have been conducted several QErelated competitions such as the QE task (Fonseca et al., 2019) for estimating post-edit rate HTER (Snover et al., 2006) and the QE as a Metric task (Ma et al., 2019) for relative evaluations of translation quality. This year, the WMT QE task held a new competition (Specia et al., 2020) on absolute evaluations of translation quality. In task 1, sentences are annotated with direct assessment (DA) scores as in the metrics task (Bojar et al., 2017). 1 We have been working on the metrics task with an approach that uses pre-trained sentence encoders (Shimanaka et al., 2018, 2019). Shimanaka et al. (2018) employed InferSent (Conneau et al., 2017), Quick-Thought (Logeswaran and Lee, 2018), and Universal Sentence Encoder (Cer et al., 2018) as encoders, and achieve"
2020.wmt-1.120,N19-4009,0,0.09205,"Missing"
2020.wmt-1.120,P02-1040,0,0.109431,"kajiwara@ids.osaka-u.ac.jp komachi@tmu.ac.jp Abstract We introduce the TMUOU1 submission for the WMT20 Quality Estimation Shared Task 1: Sentence-Level Direct Assessment. Our system is an ensemble model of four regression models based on XLM-RoBERTa with language tags. We ranked 4th in Pearson and 2nd in MAE and RMSE on a multilingual track. 1 Introduction Quality Estimation (QE) is a task of estimating translation quality without reference sentences (Gandrabur and Foster, 2003; Blatz et al., 2004; Specia et al., 2018). Automatic evaluation metrics based on reference sentences, such as BLEU (Papineni et al., 2002), have contributed to improving translation quality on benchmark datasets. However, in situations where machine translation (MT) is actually used, these metrics are sometimes unable to assess the translation quality owing to the lack of reference sentences. The development of QE methods that are well correlated with manual evaluations enable users to decide whether to use the translation results as is, post-edit the results, or employ other machine translations. At the Conference on Machine Translation (WMT), there have been conducted several QErelated competitions such as the QE task (Fonseca"
2020.wmt-1.120,D19-1410,0,0.0342386,"sentence to estimate the QE score: &lt;s&gt; source &lt;/s&gt; &lt;s&gt; MT output &lt;/s&gt;. E0+LangTag Model To make it clear to the XLM-RoBERTa which language each sentence is in, we add a special token (LangTag) for language identification, such as &lt;en&gt;, at the beginning of each sentence. We have expanded the tokenizer and vocabulary and added the following eight LangTags: &lt;en&gt; &lt;et&gt; &lt;de&gt; &lt;ne&gt; &lt;ro&gt; &lt;ru&gt; &lt;si&gt; &lt;zh&gt;. An example of input to the model is as follows: &lt;s&gt; &lt;en&gt; source &lt;/s&gt; &lt;s&gt; &lt;de&gt; MT output &lt;/s&gt;. E0+AVG Model Averaged token vector is as fruitful as the &lt;s&gt; vector at the beginning of the first sentence (Reimers and Gurevych, 2019). We concatenate the averaged token vector and the &lt;s&gt; vector to get richer information from sentence pairs. E0+AVG+LangTag Model This model is a combination of the above models. As shown in Figure 1, we add LangTag at the beginning of each sentence and concatenate the &lt;s&gt; vector with the averaged token vector to estimate the QE score. 3.2 Ensemble Model We ensemble four models described above to make prediction stable. A Gradient Boosting Tree (Fried1038 QE Score QE Regression Layer f(・) E0 Encoded Token Average E0 E1 E2 E3 E4 E5 E6 E7 E8 E9 E10 E11 E12 E13 E14 E15 XLM-RoBERTa Position Embedd"
2020.wmt-1.120,W18-6456,1,0.842135,"tion (WMT), there have been conducted several QErelated competitions such as the QE task (Fonseca et al., 2019) for estimating post-edit rate HTER (Snover et al., 2006) and the QE as a Metric task (Ma et al., 2019) for relative evaluations of translation quality. This year, the WMT QE task held a new competition (Specia et al., 2020) on absolute evaluations of translation quality. In task 1, sentences are annotated with direct assessment (DA) scores as in the metrics task (Bojar et al., 2017). 1 We have been working on the metrics task with an approach that uses pre-trained sentence encoders (Shimanaka et al., 2018, 2019). Shimanaka et al. (2018) employed InferSent (Conneau et al., 2017), Quick-Thought (Logeswaran and Lee, 2018), and Universal Sentence Encoder (Cer et al., 2018) as encoders, and achieved the highest performance in all to-English language pairs of WMT18 metrics shared task (Ma et al., 2018). Subsequently, Shimanaka et al. (2019) employed BERT (Devlin et al., 2019) as an encoder to further improve the correlation with manual evaluations. In this study, we apply similar approaches to the QE task. However, to support both source and target languages, we employ XLM-RoBERTa2 (Conneau et al.,"
2020.wmt-1.120,2006.amta-papers.25,0,0.0434531,"on benchmark datasets. However, in situations where machine translation (MT) is actually used, these metrics are sometimes unable to assess the translation quality owing to the lack of reference sentences. The development of QE methods that are well correlated with manual evaluations enable users to decide whether to use the translation results as is, post-edit the results, or employ other machine translations. At the Conference on Machine Translation (WMT), there have been conducted several QErelated competitions such as the QE task (Fonseca et al., 2019) for estimating post-edit rate HTER (Snover et al., 2006) and the QE as a Metric task (Ma et al., 2019) for relative evaluations of translation quality. This year, the WMT QE task held a new competition (Specia et al., 2020) on absolute evaluations of translation quality. In task 1, sentences are annotated with direct assessment (DA) scores as in the metrics task (Bojar et al., 2017). 1 We have been working on the metrics task with an approach that uses pre-trained sentence encoders (Shimanaka et al., 2018, 2019). Shimanaka et al. (2018) employed InferSent (Conneau et al., 2017), Quick-Thought (Logeswaran and Lee, 2018), and Universal Sentence Encod"
2020.wnut-1.62,W19-1909,0,0.0434744,"Missing"
2020.wnut-1.62,D19-1371,0,0.0194133,"et al., 2020) The denoising autoencoder based on a bidirectional transformer encoder and a left-to-right transformer decoder. We employ two types of pre-trained models, BART-base14 and BART-large.15 BART-base consists of 12 transformer layers, 16 selfattention heads per layer, and a hidden size of 768. BART-large consists of 24 transformer layers, 16 self-attention heads per layer, and a hidden size of 1,024. The language models mentioned above are pretrained on corpora in the general domain such as the BookCorpus (Zhu et al., 2015) and English Wikipedia. Recent studies (Lee and Hsiang, 2019; Beltagy et al., 2019) have revealed that language models pre-trained on a domain-specific corpus achieve better performance in that domain. We employ the following three types of BERT models pre-trained on large-scale corpora of the medical domain and Twitter domain to build a classifier suitable for COVID-19 English Tweets. ALBERT (Lan et al., 2020) The transformer encoder pre-trained by multitask learning of masked language modeling and sentence order prediction. ALBERT has significantly fewer parameters than the traditional BERT architecture due to two parameter reduction techniques, factorized embedding parame"
2020.wnut-1.62,Q17-1010,0,0.0292819,"man, 2001). 2 Dev Test Informative Uninformative 3,303 3,697 472 528 944 1,056 Total 7,000 1,000 2,000 Table 1: Statistics of the dataset. Introduction 1 Train WNUT-2020 Shared Task 2 In the shared task (Nguyen et al., 2020), systems are required to classify whether a COVID-19 English Tweet is informative or not. Such informative Tweets provide information about recovered, suspected, confirmed and death cases as well as location or travel history of the cases. The 10,000 COVID-19 English Tweets3 shown in Table 1 have been released for the shared task. The baseline system is based on fastText (Bojanowski et al., 2017). Systems are evaluated by accuracy, precision, recall and F1 score, and are ranked by F1 score, which is the main metric. Note that the latter three metrics are calculated for the informative class only. 3 IDSOU System We first introduce each base model in Section 3.1 and each loss function in Section 3.2. We then introduce the ensemble model in Section 3.3. Finally, Section 3.4 describes the implementation details. Institute for Datability Science, Osaka University http://noisy-text.github.io/2020/ 3.1 Base Models Recently, the fine-tuning approach for pre-trained language models (Devlin et"
2020.wnut-1.62,2020.acl-main.747,0,0.0936734,"Missing"
2020.wnut-1.62,N19-1423,0,0.117509,"he F1 score. 1 2 The spread of the COVID-19 is causing fear and panic to people around the world. To monitor the COVID-19 outbreaks in real-time, SNS analysis such as Twitter is attracting much attention. Although there are 4 million COVID-19 English Tweets posted daily on Twitter (Lamsal, 2020), most of them are uninformative. Against this background, WNUT-2020 held a shared task2 (Nguyen et al., 2020) to automatically identify whether a COVID-19 English Tweet is informative or not. Our system employs an ensemble approach based on pre-trained language models. Such pretrained language models (Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019; Lan et al., 2020; Conneau et al., 2020; Lewis et al., 2020) have achieved high performance in various text classification tasks (Wang et al., 2019). In addition, we employ domain-specific pre-trained language models (Lee et al., 2019; Alsentzer et al., 2019; M¨uller et al., 2020) to build models suitable for COVID-19 and Twitter domains. Each model is optimized for three types of loss functions, cross-entropy, negative supervision (Ohashi et al., 2020), and Dice similarity coefficient (Li et al., 2020), which are useful for various text classification tas"
2020.wnut-1.62,2020.acl-main.703,0,0.219204,"To monitor the COVID-19 outbreaks in real-time, SNS analysis such as Twitter is attracting much attention. Although there are 4 million COVID-19 English Tweets posted daily on Twitter (Lamsal, 2020), most of them are uninformative. Against this background, WNUT-2020 held a shared task2 (Nguyen et al., 2020) to automatically identify whether a COVID-19 English Tweet is informative or not. Our system employs an ensemble approach based on pre-trained language models. Such pretrained language models (Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019; Lan et al., 2020; Conneau et al., 2020; Lewis et al., 2020) have achieved high performance in various text classification tasks (Wang et al., 2019). In addition, we employ domain-specific pre-trained language models (Lee et al., 2019; Alsentzer et al., 2019; M¨uller et al., 2020) to build models suitable for COVID-19 and Twitter domains. Each model is optimized for three types of loss functions, cross-entropy, negative supervision (Ohashi et al., 2020), and Dice similarity coefficient (Li et al., 2020), which are useful for various text classification tasks. Finally, we ensemble 48 classifiers based on 16 pre-trained language models and 3 loss functio"
2020.wnut-1.62,2020.acl-main.45,0,0.227755,"language models. Such pretrained language models (Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019; Lan et al., 2020; Conneau et al., 2020; Lewis et al., 2020) have achieved high performance in various text classification tasks (Wang et al., 2019). In addition, we employ domain-specific pre-trained language models (Lee et al., 2019; Alsentzer et al., 2019; M¨uller et al., 2020) to build models suitable for COVID-19 and Twitter domains. Each model is optimized for three types of loss functions, cross-entropy, negative supervision (Ohashi et al., 2020), and Dice similarity coefficient (Li et al., 2020), which are useful for various text classification tasks. Finally, we ensemble 48 classifiers based on 16 pre-trained language models and 3 loss functions with a random forest classifier (Breiman, 2001). 2 Dev Test Informative Uninformative 3,303 3,697 472 528 944 1,056 Total 7,000 1,000 2,000 Table 1: Statistics of the dataset. Introduction 1 Train WNUT-2020 Shared Task 2 In the shared task (Nguyen et al., 2020), systems are required to classify whether a COVID-19 English Tweet is informative or not. Such informative Tweets provide information about recovered, suspected, confirmed and death c"
2020.wnut-1.62,2021.ccl-1.108,0,0.0881054,"Missing"
2020.wnut-1.62,2020.acl-main.33,1,0.877017,"ystem employs an ensemble approach based on pre-trained language models. Such pretrained language models (Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019; Lan et al., 2020; Conneau et al., 2020; Lewis et al., 2020) have achieved high performance in various text classification tasks (Wang et al., 2019). In addition, we employ domain-specific pre-trained language models (Lee et al., 2019; Alsentzer et al., 2019; M¨uller et al., 2020) to build models suitable for COVID-19 and Twitter domains. Each model is optimized for three types of loss functions, cross-entropy, negative supervision (Ohashi et al., 2020), and Dice similarity coefficient (Li et al., 2020), which are useful for various text classification tasks. Finally, we ensemble 48 classifiers based on 16 pre-trained language models and 3 loss functions with a random forest classifier (Breiman, 2001). 2 Dev Test Informative Uninformative 3,303 3,697 472 528 944 1,056 Total 7,000 1,000 2,000 Table 1: Statistics of the dataset. Introduction 1 Train WNUT-2020 Shared Task 2 In the shared task (Nguyen et al., 2020), systems are required to classify whether a COVID-19 English Tweet is informative or not. Such informative Tweets provide informatio"
2021.acl-short.105,2020.acl-main.673,0,0.0999322,"Missing"
2021.acl-short.105,N19-1423,0,0.194641,"f a model is measured using the macro-averaged accuracy of all episodes. 2.2 eR(li ,qj ) p(i|l1 , · · · , lN , qj ) = P R(l ,q ) . i j ie R(·) can be any metrics for estimating similarity. In natural language processing, cosine similarity is a standard choice. As a loss function Lc , negative log-likelihood is commonly used: M 1 X Lc = − log p(yj ), M  xpl = E xpl , qj = E(qj ). where yj is the gold-standard label of qj . 3 E(·) can be any text encoder, such as recurrent neural networks (Yang et al., 2016), convolutional neural networks (Kim, 2014), and pre-trained language models like BERT (Devlin et al., 2019). Second, the classification model generates a label representation for l. Let C(·) be the function that generates the label representation l ∈ Rd :  l = C x1l , x2l , · · · , xK . l Proposed Method Figure 1 shows the overview of our method. It adds a mechanism for learning to generate distinctive label representations into conventional few-shot classification models by converting its training into multi-task learning. Our method adds a difference extractor (Section 3.1) and a loss function based on mutual information (Section 3.2) to an arbitrary few-shot classification model. 3.1 (1) (2) (4"
2021.acl-short.105,D18-1514,0,0.0241512,"relevant labels. Table 1 shows examples with labels sampled from Huffpost (Misra, 2018). The label pair of TECH and BUSINESS is semantically relevant, for which the classifiers are easily confused. To address this problem, we propose a mechanism that compares label representations to derive distinctive representations. It learns semantic differences between labels and generates representations that embed information specific to each label. Our method can be easily applied to existing few-shot classification models. We evaluated our method using the standard benchmarks of Huffpost and FewRel (Han et al., 2018). Experimental results showed that our method significantly improved the performance of previous few-shot classifiers across models and datasets, and achieved the state-of-the-art accuracy. 831 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 831–836 August 1–6, 2021. ©2021 Association for Computational Linguistics 2 Few-Shot Text Classification C(·) is typically a pooling function, such as average pooling and max pooling. Finally, the model calculates the sim"
2021.acl-short.105,2020.acl-main.244,0,0.0525153,"Missing"
2021.acl-short.105,D14-1181,0,0.00394947,"has not been exposed during training. The performance of a model is measured using the macro-averaged accuracy of all episodes. 2.2 eR(li ,qj ) p(i|l1 , · · · , lN , qj ) = P R(l ,q ) . i j ie R(·) can be any metrics for estimating similarity. In natural language processing, cosine similarity is a standard choice. As a loss function Lc , negative log-likelihood is commonly used: M 1 X Lc = − log p(yj ), M  xpl = E xpl , qj = E(qj ). where yj is the gold-standard label of qj . 3 E(·) can be any text encoder, such as recurrent neural networks (Yang et al., 2016), convolutional neural networks (Kim, 2014), and pre-trained language models like BERT (Devlin et al., 2019). Second, the classification model generates a label representation for l. Let C(·) be the function that generates the label representation l ∈ Rd :  l = C x1l , x2l , · · · , xK . l Proposed Method Figure 1 shows the overview of our method. It adds a mechanism for learning to generate distinctive label representations into conventional few-shot classification models by converting its training into multi-task learning. Our method adds a difference extractor (Section 3.1) and a loss function based on mutual information (Section 3"
2021.acl-short.105,D19-1045,0,0.0703009,"el. Our method is widely applicable to conventional few-shot classification models. Experimental results show that our method significantly improved the performance of few-shot text classification across models and datasets. 1 BIZ Apple confirms it slows down old iPhones as their batteries age Self-driving cars may be coming sooner than you thought Apple apologizes for slowed iPhones, drops price of battery replacements Wall Street isn’t too worried about first selfdriving Tesla death Table 1: Examples from Huffpost (BIZ: BUSINESS) Introduction Few-shot text classification (Ye and Ling, 2019; Sun et al., 2019; Gao et al., 2019; Bao et al., 2020) has been actively studied aiming to classify texts whose labels have only a few examples. Such infrequent labels are pervasive in datasets in practice, which are headaches for text classifiers because of the lack of training examples. Snell et al. (2017) showed that the conventional text classifiers are annoyed by the over-fitting problem when the distribution of labels is skewed in a dataset. Few-shot classification has two approaches: metric-based and meta-learning based methods. The metric-based methods conduct classification based on distances estimate"
2021.acl-short.105,N16-1174,0,0.0353156,"e classifier is required to predict labels that it has not been exposed during training. The performance of a model is measured using the macro-averaged accuracy of all episodes. 2.2 eR(li ,qj ) p(i|l1 , · · · , lN , qj ) = P R(l ,q ) . i j ie R(·) can be any metrics for estimating similarity. In natural language processing, cosine similarity is a standard choice. As a loss function Lc , negative log-likelihood is commonly used: M 1 X Lc = − log p(yj ), M  xpl = E xpl , qj = E(qj ). where yj is the gold-standard label of qj . 3 E(·) can be any text encoder, such as recurrent neural networks (Yang et al., 2016), convolutional neural networks (Kim, 2014), and pre-trained language models like BERT (Devlin et al., 2019). Second, the classification model generates a label representation for l. Let C(·) be the function that generates the label representation l ∈ Rd :  l = C x1l , x2l , · · · , xK . l Proposed Method Figure 1 shows the overview of our method. It adds a mechanism for learning to generate distinctive label representations into conventional few-shot classification models by converting its training into multi-task learning. Our method adds a difference extractor (Section 3.1) and a loss func"
2021.acl-short.105,P19-1277,0,0.335834,"pecific to each label. Our method is widely applicable to conventional few-shot classification models. Experimental results show that our method significantly improved the performance of few-shot text classification across models and datasets. 1 BIZ Apple confirms it slows down old iPhones as their batteries age Self-driving cars may be coming sooner than you thought Apple apologizes for slowed iPhones, drops price of battery replacements Wall Street isn’t too worried about first selfdriving Tesla death Table 1: Examples from Huffpost (BIZ: BUSINESS) Introduction Few-shot text classification (Ye and Ling, 2019; Sun et al., 2019; Gao et al., 2019; Bao et al., 2020) has been actively studied aiming to classify texts whose labels have only a few examples. Such infrequent labels are pervasive in datasets in practice, which are headaches for text classifiers because of the lack of training examples. Snell et al. (2017) showed that the conventional text classifiers are annoyed by the over-fitting problem when the distribution of labels is skewed in a dataset. Few-shot classification has two approaches: metric-based and meta-learning based methods. The metric-based methods conduct classification based on"
2021.acl-srw.24,2020.findings-emnlp.212,0,0.0132989,"fficult because the input sentence requires replacement and reordering of clauses besides lexical and phrasal paraphrasing. Because of this feature in paraphrase generation, difficulty in transformations requires to consider both source and target contexts. Introduction Paraphrase generation is a task that transforms expressions of an input sentence while retaining its meaning. While there are various subtasks in paraphrase generation, formality transfer (Rao and Tetreault, 2018; Niu et al., 2018; Kajiwara, 2019; Wang et al., 2019; Kajiwara et al., 2020; Zhang et al., 2020; Wang et al., 2020; Chawla and Yang, 2020) has been extensively studied. As paraphrase generation can be regarded as a machine translation task (Finch et al., 2004; Specia, 2010) within the same language, the same models (Bahdanau et al., 2015; Vaswani et al., 2017) have been applied to a monolingual parallel corpus. Recent studies (Platanios et al., 2019; Liu et al., 2020) have shown that curriculum learning (Bengio et al., 2009) achieves faster convergence and improved translation quality on neural machine To address this problem, we propose to use an edit distance between a paraphrased sentence pair as a difficulty metric that appr"
2021.acl-srw.24,N19-1423,0,0.0140643,"sed for training throughout curriculum learning. CL-SL and CL-WR degraded the BLEU scores on Medium class, and even deteriorated the baseline transformer on the Difficult 5 Summary and Future Work In this study, we applied the edit distance to curriculum learning for paraphrase generation. Experiment results on an informal to formal style transfer task confirmed the effectiveness of our method, particularly for paraphrasing difficult sentences. Curriculum learning can be applied to any task when reasonable metrics for task difficulty are available. Transfer learning using a pre-trained model (Devlin et al., 2019; Lewis et al., 2020) has significantly improved the performance of various natural language processing tasks. In transfer learning, fine-tuning samples similar to the ones in the pre-training corpus should be easier to learn. We plan to apply our edit-distance based curriculum learning to transfer learning. 232 Acknowledgments Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, pages 177– 180. This work was supported by JST, ACT-X Grant Number JPMJAX1907,"
2021.acl-srw.24,P19-1607,1,0.845896,"generated by copying almost all the input sentence’s words. For the latter, transformation is difficult because the input sentence requires replacement and reordering of clauses besides lexical and phrasal paraphrasing. Because of this feature in paraphrase generation, difficulty in transformations requires to consider both source and target contexts. Introduction Paraphrase generation is a task that transforms expressions of an input sentence while retaining its meaning. While there are various subtasks in paraphrase generation, formality transfer (Rao and Tetreault, 2018; Niu et al., 2018; Kajiwara, 2019; Wang et al., 2019; Kajiwara et al., 2020; Zhang et al., 2020; Wang et al., 2020; Chawla and Yang, 2020) has been extensively studied. As paraphrase generation can be regarded as a machine translation task (Finch et al., 2004; Specia, 2010) within the same language, the same models (Bahdanau et al., 2015; Vaswani et al., 2017) have been applied to a monolingual parallel corpus. Recent studies (Platanios et al., 2019; Liu et al., 2020) have shown that curriculum learning (Bengio et al., 2009) achieves faster convergence and improved translation quality on neural machine To address this problem"
2021.acl-srw.24,D19-3019,0,0.0452785,"Missing"
2021.acl-srw.24,2020.acl-main.703,0,0.0875958,"Missing"
2021.acl-srw.24,2020.acl-main.41,0,0.159962,"put sentence while retaining its meaning. While there are various subtasks in paraphrase generation, formality transfer (Rao and Tetreault, 2018; Niu et al., 2018; Kajiwara, 2019; Wang et al., 2019; Kajiwara et al., 2020; Zhang et al., 2020; Wang et al., 2020; Chawla and Yang, 2020) has been extensively studied. As paraphrase generation can be regarded as a machine translation task (Finch et al., 2004; Specia, 2010) within the same language, the same models (Bahdanau et al., 2015; Vaswani et al., 2017) have been applied to a monolingual parallel corpus. Recent studies (Platanios et al., 2019; Liu et al., 2020) have shown that curriculum learning (Bengio et al., 2009) achieves faster convergence and improved translation quality on neural machine To address this problem, we propose to use an edit distance between a paraphrased sentence pair as a difficulty metric that approximates necessary amounts of transformations. We evaluate our method on a formality transfer task using Grammarly’s Yahoo Answers Formality Corpus (GYAFC) (Rao and Tetreault, 2018). The result of paraphrase generation from informal English to formal English confirmed the effectiveness of curriculum learning based on the edit distan"
2021.acl-srw.24,C18-1086,0,0.0447496,"Missing"
2021.acl-srw.24,P02-1040,0,0.109083,"slation model θ. Output: Trained neural machine translation model θ. 1: List of difficulty values L ← ∅ 2: for i = 1, ..., M do: 3: L ← L ∪ {ddistance (si , ti )}. 4: end for 5: Compute a cumulative distribution function from difficulty values in L 6: for i = 1, ..., M do: 7: Compute the difficulty score d¯i 8: 9: 10: 11: 12: 13: Dev Test 52, 595 51, 967 209, 124 209, 124 2, 877 2, 788 1, 416 1, 332 byte-pair encoding3 (Sennrich et al., 2016) to limit the number of token types to 16, 000. On GYAFC, Rao and Tetreault (2018) reported that a correlation exists between manual annotation and BLEU (Papineni et al., 2002) scores for the task of informal to formal English transfer. Hence, we used BLEU as an evaluation metric. end for for t = 1, ..., T do: . Curriculum learning Compute the model competence c(t). Sample a data batch Bt uniformly from all si ∈ D, such that d¯i ≤ c(t). Train neural machine translation model θ using Bt as input. end for 4.2 Experiment We evaluate the performance of edit-distance based curriculum learning on a style transfer task: paraphrase generation from informal English to formal English using GYAFC1 (Rao and Tetreault, 2018). 4.1 Train* Table 3: Statistics of GYAFC (Train* indic"
2021.acl-srw.24,N19-1119,0,0.306684,"rms expressions of an input sentence while retaining its meaning. While there are various subtasks in paraphrase generation, formality transfer (Rao and Tetreault, 2018; Niu et al., 2018; Kajiwara, 2019; Wang et al., 2019; Kajiwara et al., 2020; Zhang et al., 2020; Wang et al., 2020; Chawla and Yang, 2020) has been extensively studied. As paraphrase generation can be regarded as a machine translation task (Finch et al., 2004; Specia, 2010) within the same language, the same models (Bahdanau et al., 2015; Vaswani et al., 2017) have been applied to a monolingual parallel corpus. Recent studies (Platanios et al., 2019; Liu et al., 2020) have shown that curriculum learning (Bengio et al., 2009) achieves faster convergence and improved translation quality on neural machine To address this problem, we propose to use an edit distance between a paraphrased sentence pair as a difficulty metric that approximates necessary amounts of transformations. We evaluate our method on a formality transfer task using Grammarly’s Yahoo Answers Formality Corpus (GYAFC) (Rao and Tetreault, 2018). The result of paraphrase generation from informal English to formal English confirmed the effectiveness of curriculum learning based"
2021.acl-srw.24,N18-1012,0,0.122538,"is easy because the target sentence can be generated by copying almost all the input sentence’s words. For the latter, transformation is difficult because the input sentence requires replacement and reordering of clauses besides lexical and phrasal paraphrasing. Because of this feature in paraphrase generation, difficulty in transformations requires to consider both source and target contexts. Introduction Paraphrase generation is a task that transforms expressions of an input sentence while retaining its meaning. While there are various subtasks in paraphrase generation, formality transfer (Rao and Tetreault, 2018; Niu et al., 2018; Kajiwara, 2019; Wang et al., 2019; Kajiwara et al., 2020; Zhang et al., 2020; Wang et al., 2020; Chawla and Yang, 2020) has been extensively studied. As paraphrase generation can be regarded as a machine translation task (Finch et al., 2004; Specia, 2010) within the same language, the same models (Bahdanau et al., 2015; Vaswani et al., 2017) have been applied to a monolingual parallel corpus. Recent studies (Platanios et al., 2019; Liu et al., 2020) have shown that curriculum learning (Bengio et al., 2009) achieves faster convergence and improved translation quality on neur"
2021.acl-srw.24,kocmi-bojar-2017-curriculum,0,0.0172004,"s no where there is no such thing they just got a little agressive ;) What is the title of this song. Unsure, appreciate the pair of points. That does not exist. Suddenly they became angrier. Table 1: Examples with simple transformations (bold fonts indicate words that should be rewritten) Table 2: Examples with drastic transformations (bold fonts indicate words that should be rewritten) 2 adopted word rarity: Preliminary: Curriculum Learning for Neural Machine Translation Initial curriculum learning methods for neural machine translation considered only the difficulty of the training sample (Kocmi and Bojar, 2017; Zhang et al., 2018). These methods achieved faster convergence; however, they could not improve machine translation quality after convergence. Following these studies, Platanios et al. (2019) and Liu et al. (2020) proposed a method that considers both the difficulty of the training samples and the model competence, which achieved both of faster convergence and improvement in the translation quality. This study bases on the model proposed by Platanios et al. (2019), who introduced the model competence in machine translation. Their method defines d¯i ∈ [0, 1] that is the difficulty score of th"
2021.acl-srw.24,P16-1162,0,0.0292548,"learns more dynamic transformations. 230 Algorithm 1 Edit-distance based curriculum learning Input: Dataset D = {(si , ti )}M i=1 , consisting of M samples, neural machine translation model θ. Output: Trained neural machine translation model θ. 1: List of difficulty values L ← ∅ 2: for i = 1, ..., M do: 3: L ← L ∪ {ddistance (si , ti )}. 4: end for 5: Compute a cumulative distribution function from difficulty values in L 6: for i = 1, ..., M do: 7: Compute the difficulty score d¯i 8: 9: 10: 11: 12: 13: Dev Test 52, 595 51, 967 209, 124 209, 124 2, 877 2, 788 1, 416 1, 332 byte-pair encoding3 (Sennrich et al., 2016) to limit the number of token types to 16, 000. On GYAFC, Rao and Tetreault (2018) reported that a correlation exists between manual annotation and BLEU (Papineni et al., 2002) scores for the task of informal to formal English transfer. Hence, we used BLEU as an evaluation metric. end for for t = 1, ..., T do: . Curriculum learning Compute the model competence c(t). Sample a data batch Bt uniformly from all si ∈ D, such that d¯i ≤ c(t). Train neural machine translation model θ using Bt as input. end for 4.2 Experiment We evaluate the performance of edit-distance based curriculum learning on a"
2021.acl-srw.24,D19-1365,0,0.0111757,"pying almost all the input sentence’s words. For the latter, transformation is difficult because the input sentence requires replacement and reordering of clauses besides lexical and phrasal paraphrasing. Because of this feature in paraphrase generation, difficulty in transformations requires to consider both source and target contexts. Introduction Paraphrase generation is a task that transforms expressions of an input sentence while retaining its meaning. While there are various subtasks in paraphrase generation, formality transfer (Rao and Tetreault, 2018; Niu et al., 2018; Kajiwara, 2019; Wang et al., 2019; Kajiwara et al., 2020; Zhang et al., 2020; Wang et al., 2020; Chawla and Yang, 2020) has been extensively studied. As paraphrase generation can be regarded as a machine translation task (Finch et al., 2004; Specia, 2010) within the same language, the same models (Bahdanau et al., 2015; Vaswani et al., 2017) have been applied to a monolingual parallel corpus. Recent studies (Platanios et al., 2019; Liu et al., 2020) have shown that curriculum learning (Bengio et al., 2009) achieves faster convergence and improved translation quality on neural machine To address this problem, we propose to use"
2021.acl-srw.24,2020.coling-main.203,0,0.033627,"ransformation is difficult because the input sentence requires replacement and reordering of clauses besides lexical and phrasal paraphrasing. Because of this feature in paraphrase generation, difficulty in transformations requires to consider both source and target contexts. Introduction Paraphrase generation is a task that transforms expressions of an input sentence while retaining its meaning. While there are various subtasks in paraphrase generation, formality transfer (Rao and Tetreault, 2018; Niu et al., 2018; Kajiwara, 2019; Wang et al., 2019; Kajiwara et al., 2020; Zhang et al., 2020; Wang et al., 2020; Chawla and Yang, 2020) has been extensively studied. As paraphrase generation can be regarded as a machine translation task (Finch et al., 2004; Specia, 2010) within the same language, the same models (Bahdanau et al., 2015; Vaswani et al., 2017) have been applied to a monolingual parallel corpus. Recent studies (Platanios et al., 2019; Liu et al., 2020) have shown that curriculum learning (Bengio et al., 2009) achieves faster convergence and improved translation quality on neural machine To address this problem, we propose to use an edit distance between a paraphrased sentence pair as a dif"
2021.acl-srw.24,2020.acl-main.294,0,0.0174482,"s. For the latter, transformation is difficult because the input sentence requires replacement and reordering of clauses besides lexical and phrasal paraphrasing. Because of this feature in paraphrase generation, difficulty in transformations requires to consider both source and target contexts. Introduction Paraphrase generation is a task that transforms expressions of an input sentence while retaining its meaning. While there are various subtasks in paraphrase generation, formality transfer (Rao and Tetreault, 2018; Niu et al., 2018; Kajiwara, 2019; Wang et al., 2019; Kajiwara et al., 2020; Zhang et al., 2020; Wang et al., 2020; Chawla and Yang, 2020) has been extensively studied. As paraphrase generation can be regarded as a machine translation task (Finch et al., 2004; Specia, 2010) within the same language, the same models (Bahdanau et al., 2015; Vaswani et al., 2017) have been applied to a monolingual parallel corpus. Recent studies (Platanios et al., 2019; Liu et al., 2020) have shown that curriculum learning (Bengio et al., 2009) achieves faster convergence and improved translation quality on neural machine To address this problem, we propose to use an edit distance between a paraphrased sen"
2021.emnlp-main.194,2020.emnlp-main.585,0,0.211131,"Furthermore, Yang et al. (2020) established a transformer-based model for generating Chinese definitions, followed by Mickus et al. (2019), who use the attention-based model with GloVe vectors (Pennington et al., 2014) in English definition modelling. Nevertheless, all these studies struggle with the OOV problem. Moreover, the encoder-decoder models used in these studies were trained on relatively small corpora for definition modelling. Therefore, these previous studies often result in OOV definitions, i.e., ‘a target is hunki,’ particularly for non-standard languages (e.g., internet slang). Bevilacqua et al. (2020) employed the pretrained BART (Lewis et al., 2020) for definition generation to address the problem. Furthermore, these studies do not have any mechanism to consider the specificity of the generated definitions. Although these models succeeded in generating definitions without OOV, the generated definitions are often too general or too specific. To this end, we employ the T5 model pre-trained on a large-scale corpora, which effectively address the OOV problem. Furthermore, we address the over/under-specific definition problem using the re-ranking mechanism. Related Work An early study on defin"
2021.emnlp-main.194,P18-2043,0,0.185776,"ctionaries is crucial but labour-intensive and timeconsuming. Such definitions are also useful for computer-aided language learning (CALL), which helps language learners learn a target word or phrase (Shardlow, 2014; Srikanth and Li, 2021). A definition generation technique aims to automatically generate a textual definition for a target word or phrase (referred to as ‘target’ herein) in a given sentence containing the target (referred to as ‘local context’ herein). Noraset et al. (2017) employed a static word embedding that models the usage of a target word or phrase, and Ni and Wang (2017), Gadetsky et al. (2018) and Ishiwatari et al. (2019) used an encoder-decoder model to generate a definition for a given sentence containing the target. However, these previous studies are limited by two problems: out-of-vocabulary (OOV) and overand under-specificity (Noraset et al., 2017; Mickus et al., 2019; Li et al., 2020), as shown in Table 1. An under-specific definition denotes a general definition wherein part of the meaning of the target word in context is lost. In Table 1, the target word hammer2 means attack or criticize forcefully and relentlessly, but the definition generated in the previous study failed"
2021.emnlp-main.194,N19-1350,0,0.105703,"tioned problems by leveraging a pre-trained encoderdecoder model, namely Text-to-Text Transfer Transformer, and introducing a re-ranking mechanism to model specificity in definitions.1 Experimental results on standard evaluation datasets indicate that our method significantly outperforms the previous state-of-theart method. Moreover, manual evaluation confirms that our method effectively addresses the over/under-specificity problems. Local Context Health professionals are mobilising to condemn the government , propose major structural reforms , and hammer the ineffectual minister . Reference (Ishiwatari et al., 2019) Proposed method attack or criticize forcefully and relentlessly a person who hits something attack or criticize severely Target Bang Local Context Young andrew wilson , until a bang on the head necessitated his withdrawal , again played very well . Reference (Ishiwatari et al., 2019) a sudden painful blow ( of a person ) strike or strike ( something ) with a sudden sharp noise a sudden sharp blow Introduction Proposed method Table 1: Examples of generated definitions by a previous study and our method; the previous study struggles with under- and over-specific generations. The usage of a word"
2021.emnlp-main.194,D18-2012,0,0.0132212,"trained on a large-scale corpus scraped from the web combined with corpora for supervised tasks of translation, summarisation, classification, and reading comprehension. T5 can handle various text-based language problems in natural language processing after fine-tuning. We follow the fine-tuning procedure described in Raffel et al. (2020), as shown in Figure 2. First, we prepare the pairs of targets and the corresponding local contexts. Second, we concatenate them with the labels, ‘word:’ and ‘context:’. Then, we input them into the encoder of T5 after sub-word segmentation by SentencePiece (Kudo and Richardson, 2018) and train the model to generate definitions using the cross-entropy loss. Through this fine-tuning, T5 learns to generate the definition of the target conditioned in the local context. (2) The lower the score, the corresponding definition is more likely to be generated. 3.2 Re-Ranking Models To identify a definition with appropriate specificity, we use two estimators: one evaluates the level of over-specificity of a definition and the other evaluates the level of under-specificity. In the quality estimation of machine translation, force-decoding has been used to estimate the likelihoods of ma"
2021.emnlp-main.194,2020.acl-main.703,0,0.0229499,"mer-based model for generating Chinese definitions, followed by Mickus et al. (2019), who use the attention-based model with GloVe vectors (Pennington et al., 2014) in English definition modelling. Nevertheless, all these studies struggle with the OOV problem. Moreover, the encoder-decoder models used in these studies were trained on relatively small corpora for definition modelling. Therefore, these previous studies often result in OOV definitions, i.e., ‘a target is hunki,’ particularly for non-standard languages (e.g., internet slang). Bevilacqua et al. (2020) employed the pretrained BART (Lewis et al., 2020) for definition generation to address the problem. Furthermore, these studies do not have any mechanism to consider the specificity of the generated definitions. Although these models succeeded in generating definitions without OOV, the generated definitions are often too general or too specific. To this end, we employ the T5 model pre-trained on a large-scale corpora, which effectively address the OOV problem. Furthermore, we address the over/under-specific definition problem using the re-ranking mechanism. Related Work An early study on definition generation (Noraset et al., 2017) proposed a"
2021.emnlp-main.194,2020.acl-main.65,0,0.15212,"xtual definition for a target word or phrase (referred to as ‘target’ herein) in a given sentence containing the target (referred to as ‘local context’ herein). Noraset et al. (2017) employed a static word embedding that models the usage of a target word or phrase, and Ni and Wang (2017), Gadetsky et al. (2018) and Ishiwatari et al. (2019) used an encoder-decoder model to generate a definition for a given sentence containing the target. However, these previous studies are limited by two problems: out-of-vocabulary (OOV) and overand under-specificity (Noraset et al., 2017; Mickus et al., 2019; Li et al., 2020), as shown in Table 1. An under-specific definition denotes a general definition wherein part of the meaning of the target word in context is lost. In Table 1, the target word hammer2 means attack or criticize forcefully and relentlessly, but the definition generated in the previous study failed to capture the meaning of attacking or criticizing. 1 An over-specific definition represents a definition Code is available at https://github.com/ amanotaiga/Definition_Modeling_Project that contains too many details, which narrow down 2499 Proceedings of the 2021 Conference on Empirical Methods in Nat"
2021.emnlp-main.194,W19-6201,0,0.0173564,"tically generate a textual definition for a target word or phrase (referred to as ‘target’ herein) in a given sentence containing the target (referred to as ‘local context’ herein). Noraset et al. (2017) employed a static word embedding that models the usage of a target word or phrase, and Ni and Wang (2017), Gadetsky et al. (2018) and Ishiwatari et al. (2019) used an encoder-decoder model to generate a definition for a given sentence containing the target. However, these previous studies are limited by two problems: out-of-vocabulary (OOV) and overand under-specificity (Noraset et al., 2017; Mickus et al., 2019; Li et al., 2020), as shown in Table 1. An under-specific definition denotes a general definition wherein part of the meaning of the target word in context is lost. In Table 1, the target word hammer2 means attack or criticize forcefully and relentlessly, but the definition generated in the previous study failed to capture the meaning of attacking or criticizing. 1 An over-specific definition represents a definition Code is available at https://github.com/ amanotaiga/Definition_Modeling_Project that contains too many details, which narrow down 2499 Proceedings of the 2021 Conference on Empiri"
2021.emnlp-main.194,I17-2070,0,0.343823,"their meanings in dictionaries is crucial but labour-intensive and timeconsuming. Such definitions are also useful for computer-aided language learning (CALL), which helps language learners learn a target word or phrase (Shardlow, 2014; Srikanth and Li, 2021). A definition generation technique aims to automatically generate a textual definition for a target word or phrase (referred to as ‘target’ herein) in a given sentence containing the target (referred to as ‘local context’ herein). Noraset et al. (2017) employed a static word embedding that models the usage of a target word or phrase, and Ni and Wang (2017), Gadetsky et al. (2018) and Ishiwatari et al. (2019) used an encoder-decoder model to generate a definition for a given sentence containing the target. However, these previous studies are limited by two problems: out-of-vocabulary (OOV) and overand under-specificity (Noraset et al., 2017; Mickus et al., 2019; Li et al., 2020), as shown in Table 1. An under-specific definition denotes a general definition wherein part of the meaning of the target word in context is lost. In Table 1, the target word hammer2 means attack or criticize forcefully and relentlessly, but the definition generated in t"
2021.emnlp-main.194,P02-1040,0,0.112247,"contains erroneous entries whose definitions are single Arabic numerals or part-of-speech tags. We excluded these erroneous entries from evaluation using a simple heuristic. 6 https://developer.oxforddictionaries. com/ 7 https://www.urbandictionary.com/ Wikipedia dataset The Wikipedia dataset was collected from Wikipedia8 and Wikidata9 by Ishiwatari et al. (2019). The Wikipedia dataset also provides phrases as targets, but their domains are across different fields, whereas phrases in the Urban dataset are all online slangs. 4.2 Evaluation Metrics Following the previous studies, we used BLEU (Papineni et al., 2002) as an automatic evaluation metric. However, BLEU is vulnerable to the evaluation of definition generation because the references are short (less than 12 words as shown in Table 2) and many of them have prototypical expressions, such as ‘the quality of being something’. Moreover, we found that definitions generated by previous studies have high OOV rates, which is critical in definition generation. Although definitions of high OOV rates, such as ‘the quality of being hunki’, are inefficient, BLEU evaluates them highly because 8 https://dumps.wikimedia.org/enwiki/ 20170720/ 9 https://dumps.wiki"
2021.emnlp-main.194,D14-1162,0,0.0853785,"s produced by the T5 model, respectively. 2 approach and proposed a method that models local and global contexts with multiple encoders and gate mechanisms. Washio et al. (2019) exploited lexical semantic relations between the target and words in definitions. Following Ishiwatari et al. (2019), Li et al. (2020) further introduced a module to decompose the meanings of words as discrete latent variables. Furthermore, Yang et al. (2020) established a transformer-based model for generating Chinese definitions, followed by Mickus et al. (2019), who use the attention-based model with GloVe vectors (Pennington et al., 2014) in English definition modelling. Nevertheless, all these studies struggle with the OOV problem. Moreover, the encoder-decoder models used in these studies were trained on relatively small corpora for definition modelling. Therefore, these previous studies often result in OOV definitions, i.e., ‘a target is hunki,’ particularly for non-standard languages (e.g., internet slang). Bevilacqua et al. (2020) employed the pretrained BART (Lewis et al., 2020) for definition generation to address the problem. Furthermore, these studies do not have any mechanism to consider the specificity of the genera"
2021.emnlp-main.194,D19-1357,0,0.0159523,"alues are dataset dependent, setting α in the range of [0.4, 0.8] and β in the range of [0.1, 0.4] consistently performed well. The values of α and β for all cases are listed in the appendices. 5 Experimental Results and Analyses We present the results of the automatic evaluation and further conduct quantitative analyses to declare the performance of the proposed method under different conditions.13 Furthermore, we conduct a manual analysis to investigate whether the over/under-specific definition problems are addressed. 13 We further present experimental results for relevant comparisons with Washio et al. (2019), Li et al. (2020), and Bevilacqua et al. (2020) in the appendices. 2504 Error type Word Definition (1) Over-specified (2) Self-reference (3) Wrong part-of-speech (4) Under-specified (5) Opposite (6) Similar semantics (7) Incorrect (8) Correct waft self-consciousness red-hot forerunner hollow machine first winery ( of an unpleasant smell ) spread through the air the state of being self-conscious of the most recent interest or importance a thing that precedes another a cavity that is felt by food a device with automatic functions the next after all others in a set of items a factory or business"
2021.emnlp-main.194,W18-6319,0,0.0303726,"Missing"
2021.emnlp-main.194,2021.findings-acl.455,0,0.0319115,"( something ) with a sudden sharp noise a sudden sharp blow Introduction Proposed method Table 1: Examples of generated definitions by a previous study and our method; the previous study struggles with under- and over-specific generations. The usage of a word or phrase changes over time and new words and phrases emerge every day; therefore, the maintenance of their meanings in dictionaries is crucial but labour-intensive and timeconsuming. Such definitions are also useful for computer-aided language learning (CALL), which helps language learners learn a target word or phrase (Shardlow, 2014; Srikanth and Li, 2021). A definition generation technique aims to automatically generate a textual definition for a target word or phrase (referred to as ‘target’ herein) in a given sentence containing the target (referred to as ‘local context’ herein). Noraset et al. (2017) employed a static word embedding that models the usage of a target word or phrase, and Ni and Wang (2017), Gadetsky et al. (2018) and Ishiwatari et al. (2019) used an encoder-decoder model to generate a definition for a given sentence containing the target. However, these previous studies are limited by two problems: out-of-vocabulary (OOV) and"
2021.emnlp-main.194,2020.emnlp-main.8,0,0.0162501,"y loss. Through this fine-tuning, T5 learns to generate the definition of the target conditioned in the local context. (2) The lower the score, the corresponding definition is more likely to be generated. 3.2 Re-Ranking Models To identify a definition with appropriate specificity, we use two estimators: one evaluates the level of over-specificity of a definition and the other evaluates the level of under-specificity. In the quality estimation of machine translation, force-decoding has been used to estimate the likelihoods of machine translation outputs, achieving state-of-the-art performance (Thompson and Post, 2020). Inspired by this approach, we fine-tune other T5 models and use force-decoding for estimating the levels of over/under-specificity. Over-Specificity We observed that over-specific definitions are generated when a generation model Generation Likelihood For re-ranking, we con- is overly affected by local contexts, i.e., the gensider the generation likelihood of each definition. erated definitions tend to contain words that are Given a target w∗ and corresponding local context relevant to those in the local context. For example, 2501 WordNet Phrases Entries Context length Desc. length Oxford Ur"
2021.emnlp-main.612,P19-1309,0,0.154319,"a parallel corpus and map embeddings in other languages into the space of English. BERTScore (Zhang et al., 2020) estimates the semantic similarity between sentences by matching token embeddings from BERT (Devlin et al., 2019). Although the BERTScore of its original form is a reference-based automatic evaluation method, it 2 Related Work can be applied to an unsupervised cross-lingual sim2.1 Multilingual Sentence Encoders ilarity estimation by using multilingual sentence encoders instead of BERT. Early multilingual sentence encoders, such as D-TP and D-Lex-Sim (Fomicheva et al., 2020b) LASER (Artetxe and Schwenk, 2019a,b), were are unsupervised QE methods; however, they use encoder-decoder models based on recurrent neural neural machine translation (NMT) systems that are networks. Similar to the evolution of monolingual the targets of QE. D-TP uses a sequence-level transsentence encoders (Kiros et al., 2015; Logeswaran lation probability normalised by sentence length. and Lee, 2018; Cer et al., 2018; Reimers and D-Lex-Sim calculates the METEOR score (BanerGurevych, 2019), multilingual sentence encoders jee and Lavie, 2005) based on the lexical variation have now been replaced by encoder-only models based o"
2021.emnlp-main.612,W05-0909,0,0.18912,"Missing"
2021.emnlp-main.612,D15-1075,0,0.115413,"Missing"
2021.emnlp-main.612,D18-2029,0,0.0516217,"Missing"
2021.emnlp-main.612,2020.acl-main.747,0,0.224074,"ych, 2019), multilingual sentence encoders jee and Lavie, 2005) based on the lexical variation have now been replaced by encoder-only models based on self-attention networks (Vaswani et al., between the translation hypotheses. These methods are useful for white-box machine translation 2017) for computational efficiency and improved performance in downstream tasks. Recent multi- systems; however, in general, users can access only lingual sentence encoders, such as mBERT (De- the output sentences. vlin et al., 2019) and XLM-R (Conneau and LamPrism (Thompson and Post, 2020) and BGT (Wiple, 2019; Conneau et al., 2020), are single self- eting et al., 2020) are state-of-the-art unsupervised attention networks pre-trained on monolingual cor- methods for QE and STS, respectively. These are pora in over 100 languages for the objective func- NMT models that train encoder-decoder structures tion of masked language modelling (Devlin et al., of SANs on bilingual corpora. Prism uses the gen2019; Liu et al., 2019). LaBSE (Feng et al., 2020) eration probability of force-decoding a target senis a state-of-the-art multilingual sentence encoder tence as the QE score. BGT disentangles languagefor parallel text retrieval t"
2021.emnlp-main.612,N19-1423,0,0.529297,"rmance of any pre-trained multilingual sentence encoder, even in low-resource language pairs, where only tens of thousands of parallel sentence pairs are available.1 en zh ne Figure 1: Visualisation of 1, 000 mBERT embeddings of parallel sentences in three languages: English (en), Chinese (zh), and Nepalese (ne) In the latest QE competitions at the conference on machine translation (WMT) (Specia et al., 2020), all top-ranked systems (Ranasinghe et al., 2020; Fomicheva et al., 2020a; Nakamachi et al., 2020) employed pre-trained multilingual sentence encoders, such as multilingual BERT (mBERT) (Devlin et al., 2019) and XLM-RoBERTa 1 Introduction (XLM-R) (Conneau and Lample, 2019; Conneau Pre-trained sentence encoders (Kiros et al., 2015; et al., 2020). These multilingual sentence encoders Logeswaran and Lee, 2018; Cer et al., 2018; De- form a single self-attention network pre-trained on vlin et al., 2019; Liu et al., 2019; Lan et al., monolingual corpora in over 100 languages with the objective function of masked language mod2020) boost the performance of various natural language understanding (NLU) tasks (Wang et al., elling. Fine-tuning with a human-annotated corpus 2018). Among them, the combination"
2021.emnlp-main.612,2020.wmt-1.116,0,0.0462641,"Missing"
2021.emnlp-main.612,2020.findings-emnlp.150,0,0.0203199,"ders. Our embeddings allow efficient cross-lingual sentence similarity estimation using simple cosine similarity. Our method does not require human annotations specific to the target task and is based solely on the bilingual corpora. Experimental results on both the WMT20 QE task (Specia et al., 2020) and the SemEval-2017 cross-lingual STS task (Cer et al., 2017) in unsupervised settings revealed that our method consistently outperformed the strong baselines using the existing pre-trained multilingual sentence encoders. 2.2 Unsupervised Methods for Cross-lingual Sentence Similarity Estimation Libovický et al. (2020) extracts language-neutral embeddings (centered and projection) from pretrained multilingual sentence encoders. The centered method subtracts the mean embedding for each language from the sentence embedding. The projection method involves bilingual projections using a parallel corpus and map embeddings in other languages into the space of English. BERTScore (Zhang et al., 2020) estimates the semantic similarity between sentences by matching token embeddings from BERT (Devlin et al., 2019). Although the BERTScore of its original form is a reference-based automatic evaluation method, it 2 Relate"
2021.emnlp-main.612,2021.ccl-1.108,0,0.0451315,"Missing"
2021.emnlp-main.612,2020.wmt-1.120,1,0.790147,"the strong baselines using the original multilingual embeddings. The method also consistently improves the performance of any pre-trained multilingual sentence encoder, even in low-resource language pairs, where only tens of thousands of parallel sentence pairs are available.1 en zh ne Figure 1: Visualisation of 1, 000 mBERT embeddings of parallel sentences in three languages: English (en), Chinese (zh), and Nepalese (ne) In the latest QE competitions at the conference on machine translation (WMT) (Specia et al., 2020), all top-ranked systems (Ranasinghe et al., 2020; Fomicheva et al., 2020a; Nakamachi et al., 2020) employed pre-trained multilingual sentence encoders, such as multilingual BERT (mBERT) (Devlin et al., 2019) and XLM-RoBERTa 1 Introduction (XLM-R) (Conneau and Lample, 2019; Conneau Pre-trained sentence encoders (Kiros et al., 2015; et al., 2020). These multilingual sentence encoders Logeswaran and Lee, 2018; Cer et al., 2018; De- form a single self-attention network pre-trained on vlin et al., 2019; Liu et al., 2019; Lan et al., monolingual corpora in over 100 languages with the objective function of masked language mod2020) boost the performance of various natural language understanding (N"
2021.emnlp-main.612,N19-4009,0,0.0300719,"Missing"
2021.emnlp-main.612,2020.coling-main.445,0,0.01657,"s reveal that our method consistently outperforms the strong baselines using the original multilingual embeddings. The method also consistently improves the performance of any pre-trained multilingual sentence encoder, even in low-resource language pairs, where only tens of thousands of parallel sentence pairs are available.1 en zh ne Figure 1: Visualisation of 1, 000 mBERT embeddings of parallel sentences in three languages: English (en), Chinese (zh), and Nepalese (ne) In the latest QE competitions at the conference on machine translation (WMT) (Specia et al., 2020), all top-ranked systems (Ranasinghe et al., 2020; Fomicheva et al., 2020a; Nakamachi et al., 2020) employed pre-trained multilingual sentence encoders, such as multilingual BERT (mBERT) (Devlin et al., 2019) and XLM-RoBERTa 1 Introduction (XLM-R) (Conneau and Lample, 2019; Conneau Pre-trained sentence encoders (Kiros et al., 2015; et al., 2020). These multilingual sentence encoders Logeswaran and Lee, 2018; Cer et al., 2018; De- form a single self-attention network pre-trained on vlin et al., 2019; Liu et al., 2019; Lan et al., monolingual corpora in over 100 languages with the objective function of masked language mod2020) boost the perfor"
2021.emnlp-main.612,D19-1410,0,0.0156824,"The tence pairs of bilingual corpora. We extended these need for large-scale bilingual corpora to train NMT SAN-based multilingual sentence encoders for un- models limits the language pairs that these models supervised cross-lingual similarity estimation. can support. While multilingual sentence encoders The multilingual version of Sentence-BERT cover over 100 languages, Prism covers only 39 (SBERT) (Reimers and Gurevych, 2020) was ob- languages. Although we extract both languagetained by knowledge distillation from the English specific and language-agnostic embeddings, the version of SBERT (Reimers and Gurevych, 2019). decoder-free architecture of our model supports Although this model achieves the best performance to support low-resource language pairs. In other in cross-lingual STS tasks, it is not fully unsuper- words, our method is sufficiently efficient to supvised because SBERT is fine-tuned for STS tasks. port the massively multilingual scenario. 7765 Language identification MLPI (a) Source embedding MLPL MLPM Language embedding Meaning embedding Reconstruction loss (b) Target embedding Meaning embedding loss (Similar) (c) Random source embedding MLPL Meaning embedding Language embedding Language id"
2021.emnlp-main.612,2020.emnlp-main.365,0,0.334764,"betrained using a maximum of 100 million sentence tween such language-agnostic embeddings, BGT pairs in each language, with a total of 6 billion sen- estimates cross-lingual sentence similarity. The tence pairs of bilingual corpora. We extended these need for large-scale bilingual corpora to train NMT SAN-based multilingual sentence encoders for un- models limits the language pairs that these models supervised cross-lingual similarity estimation. can support. While multilingual sentence encoders The multilingual version of Sentence-BERT cover over 100 languages, Prism covers only 39 (SBERT) (Reimers and Gurevych, 2020) was ob- languages. Although we extract both languagetained by knowledge distillation from the English specific and language-agnostic embeddings, the version of SBERT (Reimers and Gurevych, 2019). decoder-free architecture of our model supports Although this model achieves the best performance to support low-resource language pairs. In other in cross-lingual STS tasks, it is not fully unsuper- words, our method is sufficiently efficient to supvised because SBERT is fine-tuned for STS tasks. port the massively multilingual scenario. 7765 Language identification MLPI (a) Source embedding MLPL ML"
2021.emnlp-main.612,N18-1101,0,0.0144323,"ual but also for 4.2.2 Result monolingual tasks. The first and second sets of rows in Tables 4 and The last sets of rows in Tables 4 and 5 show the 5 show the Pearson correlation coefficients of the performance of state-of-the-art models: the multioriginal multilingual sentence encoders, the meanlingual version of SBERT (Reimers and Gurevych, ing embeddings by our model, and baselines, mea2020). It uses knowledge distillation by setting sured on the cross-lingual STS task, respectively. SBERT trained with AllNLI (SNLI (Bowman Similar to the evaluations of the QE task, our et al., 2015), MNLI (Williams et al., 2018)), and 16 https://tatoeba.org STSB (Cer et al., 2017) as a teacher and training 7770 ro en ro en mBERT (Disentangled) Meaning Embedding mBERT ro en mBERT (Disentangled) Language Embedding Figure 5: Visualisation of embeddings from 1, 000 sentence pairs in ro-en parallel corpus. Our Method w/o Language Loss w/o Meaning Loss mBERT XLM-R LaBSE 0.335 0.322 0.008 0.289 0.286 0.026 0.458 0.449 0.274 mance. We conjecture that this is because meaning loss allows learning semantic equivalence and inequivalence, which is useful for conducting QE. 5.2 Table 6: Pearson correlation coefficients of WMT20 QE"
2021.emnlp-main.612,2020.emnlp-main.8,0,0.077401,"Lex-Sim calculates the METEOR score (BanerGurevych, 2019), multilingual sentence encoders jee and Lavie, 2005) based on the lexical variation have now been replaced by encoder-only models based on self-attention networks (Vaswani et al., between the translation hypotheses. These methods are useful for white-box machine translation 2017) for computational efficiency and improved performance in downstream tasks. Recent multi- systems; however, in general, users can access only lingual sentence encoders, such as mBERT (De- the output sentences. vlin et al., 2019) and XLM-R (Conneau and LamPrism (Thompson and Post, 2020) and BGT (Wiple, 2019; Conneau et al., 2020), are single self- eting et al., 2020) are state-of-the-art unsupervised attention networks pre-trained on monolingual cor- methods for QE and STS, respectively. These are pora in over 100 languages for the objective func- NMT models that train encoder-decoder structures tion of masked language modelling (Devlin et al., of SANs on bilingual corpora. Prism uses the gen2019; Liu et al., 2019). LaBSE (Feng et al., 2020) eration probability of force-decoding a target senis a state-of-the-art multilingual sentence encoder tence as the QE score. BGT disent"
2021.emnlp-main.612,W18-5446,0,0.0481124,"Missing"
2021.emnlp-main.612,2020.emnlp-main.122,0,0.130751,"h that meaning and language-specific information are clearly separated. Such non-parallel sentences are written in the same language, but their meanings are different. The constraint of our meaning embedding loss makes these non-parallel sentences distant, while the constraint of our language embedding loss makes language embeddings come closer. In other words, meaning and language embedding losses operate in opposite directions for non-parallel sentences. We expect that this training helps clearly separate the meaning and language embeddings. In contrast, language-specific embeddings in BGT (Wieting et al., 2020) are trained with only parallel sentences, which may allow meaning information to leak into the language-specific embeddings. LiL computes the loss for language identification. We conduct language identification using an MLP: Lm sM , sˆ0M ))+max(0, φ(tˆM , tˆ0M )). M = max(0, φ(ˆ (7) 7767 y ˆ = softmax(MLPI (ˆ eL )), (10) High Resource Medium Resource Low Resource Model en-de en-zh ro-en et-en ne-en si-en Avg. mBERT mBERT (Meaning) XLM-R XLM-R (Meaning) LaBSE LaBSE (Meaning) 0.071 0.125 0.061 0.093 0.084 0.151 0.010 0.131 0.007 0.120 0.036 0.156 0.182 0.663 0.151 0.647 0.705 0.711 0.009 0.354"
2021.findings-emnlp.170,D15-1075,0,0.0853222,"Missing"
2021.findings-emnlp.170,D18-1547,0,0.0586416,"Missing"
2021.findings-emnlp.170,N19-1423,0,0.0932554,"e Wizard-of-Oz method, in which the user and system speak alternately. For each user’s utterance, we use crowdsourcing to collect ‘an utterance that is more indirect than the original utterance’ and ‘an utterance that is more direct than the original utterance’. Hence, DIRECT provides triples of paraphrases: original utterances, indirect utterances, and direct utterances. We designed three benchmark tasks using this corpus to evaluate the model’s ability to recognize and generate pragmatic paraphrases. As baselines, we investigated the performance of state-of-the-art pre-trained models, BERT (Devlin et al., 2019) and BART (Lewis et al., 2020), for benchmark tasks. 2 Related Work mine whether a text entails a hypothesis. In contrast, context is a crucial element in determining paraphrasal relationships in pragmatic paraphrases. Our DIRECT is the first corpus that provides largescale pragmatic paraphrases. It would be a valuable resource also for research on paraphrase identification and generation to make a step forward from literal paraphrases. 3 DIRECT Corpus A pragmatic paraphrase is a pair of texts that have equivalent outcomes in a given context, which frequently emerge in conversations. Expanding"
2021.findings-emnlp.170,I05-5002,0,0.51706,"nswers written by crowdsourcsourcing service, to expand MultiWoZ with praging workers; however, it is limited to context-free Yes/No questions. In contrast to these studies, DI- matic paraphrases. The workers first received inRECT provides natural utterances written by hu- structions, as presented in Table 1, and some examples of the task. Then, the workers were shown mans with rich dialogue histories. Furthermore, it dialogue histories extracted from MultiWoZ, as covers various types of utterances. 3 While there are several paraphrase cor- illustrated in Figure 2. Based on the given conpora (Dolan and Brockett, 2005; Lan et al., 2017), versation histories, the workers input indirect and all have focused on context-free paraphrases. direct responses that have the same intent as the specified user response in the dialogue (written in Hence, none provide pragmatic paraphrases that red in Figure 2) into the input forms at the bottom. emerge in contexts. Corpora for natural language 2 inference are also relevant to our study (Giampichttps://www.mturk.com/ 3 colo et al., 2007; Marelli et al., 2014; Bowman The original MultiWoZ data contains dialogues between et al., 2015). Similar to the paraphrase corpora, a"
2021.findings-emnlp.170,2020.acl-main.60,0,0.0151636,"ories. Specifically, we employed MultiWoZ2.1 (Budzianowski et al., 2018; Eric et al., 2020) and collected pragmatic paraphrases using crowdsourcing. We describe how we collected pragmatic paraphrases in Section 3.1 with careful quality control as described in Section 3.2. Section 3.3 describes the statistics of the collected corpus. Section 3.4 presents a comparative analysis between our corpus and existing paraphrase corpora using conventional paraphrase identification models. Paraphrases have been applied in a dialogue system’s research in the context of data augmentation (Hou et al., 2018; Gao et al., 2020). Despite its importance in understanding users’ intentions, the pragmatic paraphrases have been overlooked. Only a few recent studies have focused on pragmatic paraphrases to advance the understanding of users’ intentions. Pragst and Ultes (2018) proposed a rule-based approach to automatically construct a corpus consisting of pairs of indirect and direct utterances. They demonstrated that the neural con3.1 Direct and Indirect Response Collection versation model could accurately extract utterances with opposing directness. Because of their rule- MultiWoZ is a multi-domain, task-oriented diabas"
2021.findings-emnlp.170,W07-1401,0,0.345989,"Missing"
2021.findings-emnlp.170,C18-1105,0,0.0225479,"conversation histories. Specifically, we employed MultiWoZ2.1 (Budzianowski et al., 2018; Eric et al., 2020) and collected pragmatic paraphrases using crowdsourcing. We describe how we collected pragmatic paraphrases in Section 3.1 with careful quality control as described in Section 3.2. Section 3.3 describes the statistics of the collected corpus. Section 3.4 presents a comparative analysis between our corpus and existing paraphrase corpora using conventional paraphrase identification models. Paraphrases have been applied in a dialogue system’s research in the context of data augmentation (Hou et al., 2018; Gao et al., 2020). Despite its importance in understanding users’ intentions, the pragmatic paraphrases have been overlooked. Only a few recent studies have focused on pragmatic paraphrases to advance the understanding of users’ intentions. Pragst and Ultes (2018) proposed a rule-based approach to automatically construct a corpus consisting of pairs of indirect and direct utterances. They demonstrated that the neural con3.1 Direct and Indirect Response Collection versation model could accurately extract utterances with opposing directness. Because of their rule- MultiWoZ is a multi-domain, t"
2021.findings-emnlp.170,D17-1126,0,0.0534313,"Missing"
2021.findings-emnlp.170,2020.acl-main.703,0,0.154264,"the user and system speak alternately. For each user’s utterance, we use crowdsourcing to collect ‘an utterance that is more indirect than the original utterance’ and ‘an utterance that is more direct than the original utterance’. Hence, DIRECT provides triples of paraphrases: original utterances, indirect utterances, and direct utterances. We designed three benchmark tasks using this corpus to evaluate the model’s ability to recognize and generate pragmatic paraphrases. As baselines, we investigated the performance of state-of-the-art pre-trained models, BERT (Devlin et al., 2019) and BART (Lewis et al., 2020), for benchmark tasks. 2 Related Work mine whether a text entails a hypothesis. In contrast, context is a crucial element in determining paraphrasal relationships in pragmatic paraphrases. Our DIRECT is the first corpus that provides largescale pragmatic paraphrases. It would be a valuable resource also for research on paraphrase identification and generation to make a step forward from literal paraphrases. 3 DIRECT Corpus A pragmatic paraphrase is a pair of texts that have equivalent outcomes in a given context, which frequently emerge in conversations. Expanding a dialogue corpus is a promis"
2021.findings-emnlp.170,2020.emnlp-main.273,0,0.0127671,"us of direct and indirect responses in conversational text, DIRECT, which contains 71, 498 pairs of indirect and direct responses. We expand the commonly used dialogue corpus of MultiWoZ (Eric et al., 2020), a multi-domain and multi-turn task-oriented dialogue We create a large-scale dialogue corpus that discloses users’ hidden intentions to advance techniques for natural language understanding in dialogue systems. Neural conversation models have been able to generate high-quality responses (Zhao et al., 2020; Zhang et al., 2020) and achieve dialogue state tracking (Hosseini-Asl et al., 2020; Lin et al., 2020). These previous studies have been based on the literal meanings of user utterances. Little attention has been given to the implied intention of the utterances considered. However, during conversation, humans often respond to others with indirect expressions, without directly telling them their requests or intentions (Searle, 1979; Brown et al., 1987). When humans receive an indirect response, they infer the inten1 tion implied in the response based on context, such https://github.com/junya-takayama/ as dialogue history. For example, in the example DIRECT 1980 Findings of the Association for C"
2021.findings-emnlp.170,2020.emnlp-main.601,0,0.0366059,"udies have focused on pragmatic paraphrases to advance the understanding of users’ intentions. Pragst and Ultes (2018) proposed a rule-based approach to automatically construct a corpus consisting of pairs of indirect and direct utterances. They demonstrated that the neural con3.1 Direct and Indirect Response Collection versation model could accurately extract utterances with opposing directness. Because of their rule- MultiWoZ is a multi-domain, task-oriented diabased approach, patterns of indirect/direct utter- logue corpus annotated with dialogue act tags ances in their corpus are limited. Louis et al. (2020) and dialogue states, comprising 10, 438 dialogues. Each dialogue involves alternate utterances by a used crowdsourcing to build a corpus comprising user and system; the total number of utterances is indirect answers to Yes/No questions, annotating 71, 524. whether the answers were Yes or No. This corpus We used Amazon Mechanical Turk2 , a crowdprovides natural answers written by crowdsourcsourcing service, to expand MultiWoZ with praging workers; however, it is limited to context-free Yes/No questions. In contrast to these studies, DI- matic paraphrases. The workers first received inRECT prov"
2021.findings-emnlp.170,S14-2001,0,0.0331492,"of utterances. 3 While there are several paraphrase cor- illustrated in Figure 2. Based on the given conpora (Dolan and Brockett, 2005; Lan et al., 2017), versation histories, the workers input indirect and all have focused on context-free paraphrases. direct responses that have the same intent as the specified user response in the dialogue (written in Hence, none provide pragmatic paraphrases that red in Figure 2) into the input forms at the bottom. emerge in contexts. Corpora for natural language 2 inference are also relevant to our study (Giampichttps://www.mturk.com/ 3 colo et al., 2007; Marelli et al., 2014; Bowman The original MultiWoZ data contains dialogues between et al., 2015). Similar to the paraphrase corpora, a person acting as the ‘system’ and another acting as the ‘user.’ We presented the former as the ‘operator’ in our user they do not provide contexts. This means that interfaces to prevent the workers from misunderstanding the these corpora rely on world knowledge to deter- corresponding utterances that were automatically generated. 1981 Instructions Read the following dialogue between the USER and the OPERATOR, please rephrase the USER’s response written in red letters into two diff"
2021.findings-emnlp.170,P02-1040,0,0.109636,"ap hotel in the West. USER Do they have free internet? SYSTEM Yes, they have internet would you like me to book it for you? indirect response i dont think so on the booking. address and phone number though. BART dont need to book it. address and phone number for them though. - w/o history address and phone number isnt needed. no...just give me the address and phone transformer number. reference address and phone number is all i need right now. SYSTEM Table 7: Examples of generated direct responses in the indirect-to-direct transfer task. Results and Discussion Table 6 presents the BLEU score (Papineni et al., 2002) and Perplexity of each model. For the BART-based models, the model using dialogue history has a higher BLEU score, as expected, because pragmatic paraphrases are context-dependent. Comparing BART and transformer with dialogue history, the former largely outperformed the latter. This result confirms that pre-training is also crucial in this task. Model Transformer w/ history BART w/o history BART w/ history BLEU 19.84 27.12 26.52 Perplexity 3.06 2.39 2.34 Table 8: Evaluation results of the direct-to-indirect task (omitted) Thanks! I’m also looking for the Curry Prince restaurant, do you know w"
2021.findings-emnlp.170,2020.acl-demos.30,0,0.0359466,"hrases to address the true intentions of the user. In this study, we release1 a corpus of direct and indirect responses in conversational text, DIRECT, which contains 71, 498 pairs of indirect and direct responses. We expand the commonly used dialogue corpus of MultiWoZ (Eric et al., 2020), a multi-domain and multi-turn task-oriented dialogue We create a large-scale dialogue corpus that discloses users’ hidden intentions to advance techniques for natural language understanding in dialogue systems. Neural conversation models have been able to generate high-quality responses (Zhao et al., 2020; Zhang et al., 2020) and achieve dialogue state tracking (Hosseini-Asl et al., 2020; Lin et al., 2020). These previous studies have been based on the literal meanings of user utterances. Little attention has been given to the implied intention of the utterances considered. However, during conversation, humans often respond to others with indirect expressions, without directly telling them their requests or intentions (Searle, 1979; Brown et al., 1987). When humans receive an indirect response, they infer the inten1 tion implied in the response based on context, such https://github.com/junya-takayama/ as dialogue"
2021.findings-emnlp.170,N19-1131,0,0.049735,"Missing"
2021.findings-emnlp.170,2020.emnlp-main.279,0,0.0181441,"the pragmatic paraphrases to address the true intentions of the user. In this study, we release1 a corpus of direct and indirect responses in conversational text, DIRECT, which contains 71, 498 pairs of indirect and direct responses. We expand the commonly used dialogue corpus of MultiWoZ (Eric et al., 2020), a multi-domain and multi-turn task-oriented dialogue We create a large-scale dialogue corpus that discloses users’ hidden intentions to advance techniques for natural language understanding in dialogue systems. Neural conversation models have been able to generate high-quality responses (Zhao et al., 2020; Zhang et al., 2020) and achieve dialogue state tracking (Hosseini-Asl et al., 2020; Lin et al., 2020). These previous studies have been based on the literal meanings of user utterances. Little attention has been given to the implied intention of the utterances considered. However, during conversation, humans often respond to others with indirect expressions, without directly telling them their requests or intentions (Searle, 1979; Brown et al., 1987). When humans receive an indirect response, they infer the inten1 tion implied in the response based on context, such https://github.com/junya-t"
2021.findings-emnlp.170,W18-5002,0,0.0274204,"ol as described in Section 3.2. Section 3.3 describes the statistics of the collected corpus. Section 3.4 presents a comparative analysis between our corpus and existing paraphrase corpora using conventional paraphrase identification models. Paraphrases have been applied in a dialogue system’s research in the context of data augmentation (Hou et al., 2018; Gao et al., 2020). Despite its importance in understanding users’ intentions, the pragmatic paraphrases have been overlooked. Only a few recent studies have focused on pragmatic paraphrases to advance the understanding of users’ intentions. Pragst and Ultes (2018) proposed a rule-based approach to automatically construct a corpus consisting of pairs of indirect and direct utterances. They demonstrated that the neural con3.1 Direct and Indirect Response Collection versation model could accurately extract utterances with opposing directness. Because of their rule- MultiWoZ is a multi-domain, task-oriented diabased approach, patterns of indirect/direct utter- logue corpus annotated with dialogue act tags ances in their corpus are limited. Louis et al. (2020) and dialogue states, comprising 10, 438 dialogues. Each dialogue involves alternate utterances by"
2021.findings-emnlp.170,D19-1410,0,0.0161828,"e-BERT) for paraphrase pairs. number of words that need to be deleted. Table 4 demonstrates that ‘Keep’ is smaller than ‘Add’ and ‘Delete.’ This indicates that more than half of the words need to be replaced to transfer an indirect response into a direct response. In this section, we investigate how the DIRECT corpus differs from existing paraphrase corpora using state-of-the-art paraphrase identification models. First, we compute the cosine similarity between paraphrase pairs in DIRECT, MRPC (Dolan and Brockett, 2005), and Twitter URL Paraphrase corpus (Lan et al., 2017) using SentenceBERT5 (Reimers and Gurevych, 2019). Figure 4 shows the histograms, which confirms DIRECT provides more paraphrase pairs with lower cosine similarities than the MRPC and Twitter URL Paraphrase corpus. Sentence-BERT is expected to address the literal meaning of a sentence through its pre-training via STSBenchmark (Cer et al., 2017). The large volume of paraphrases with lower similarities confirms that DIRECT provides paraphrases beyond literal similarity. Next, we investigate whether a paraphrase identification model trained on existing paraphrase corpora transfers to DIRECT. Specifically, we calculated the percentage of paraphr"
2021.findings-emnlp.49,S14-2010,0,0.0643419,"Missing"
2021.findings-emnlp.49,S16-1081,0,0.0630249,"Missing"
2021.findings-emnlp.49,S12-1051,0,0.0160764,"STS Tasks We also evaluated the proposed method on STS tasks. Cosine similarity is commonly used to estimate the similarity between two text representations. In this experiment, we also used cosine similarity because such a primitive measure is sensible to characteristics of different representations. We generated a sentence representation by simply averaging representations of sub-words in a sentence excluding representations for special tokens preserved in BERT, i.e., [CLS] and [SEP]. We then computed cosine similarities between them. We evaluated the 2012-to-2016 SemEval STS shared tasks (Agirre et al., 2012, 2013, 2014, 2015, 2016), where the goal is to predict human scores that indicate the degree of semantic similarity between two sentences. The Pearson’s r between model predictions and human scores was used as an evaluation metric. Each STS corpus is divided by data sources. Hence, the corpus level score is the average of the Pearson’s r for each sub-corpus. We downloaded and pre-processed STS 2012 to 2016 corpora using the SentEval toolkit (Conneau and Kiela, 2018). The right-side columns of Table 2 show the number of sentence pairs in these corpora. 5.3 Training Corpus Preparation To prepar"
2021.findings-emnlp.49,S13-1004,0,0.021079,"Missing"
2021.findings-emnlp.49,N19-1078,0,0.0214256,"s 534–546 November 7–11, 2021. ©2021 Association for Computational Linguistics https:// � ?? art method that transforms contextualised representations for context-aware lexical semantics. Furthermore, the results confirm that our representations are more effective for composing sentence representations, which contributes to unsupervised STS estimation. 2 2.1 Transformation of Word Representations Previous studies have proposed transformations of contextualised word representations for various purposes. Pooling aggregates multiple representations to perform one of the simplest transformations. Akbik et al. (2019) complement underspecified contexts for named entity recognition, while Bommasani et al. (2020) investigate information captured in layers of pre-trained models. Wang et al. (2019) transform contextualised word representations by inserting them into Skip-gram (Mikolov et al., 2013) to generate static word representations for context-free lexical semantic tasks such as word similarity and analogy prediction. Transformation has also been used to adjust excessive effects of context that dominate representations. Shi et al. (2019) add a transformation matrix on top of the embedding layer of ELMo ("
2021.findings-emnlp.49,2020.lrec-1.720,0,0.0469685,"Missing"
2021.findings-emnlp.49,D19-1662,0,0.0183858,"n et al. (2019) and Cheng et al. (2020) disentangled content and style. Apart from style-transfer, Chen et al. (2019) disentangled semantics and syntax to estimate semantic and syntactic similarities between sentences, and Wieting et al. (2020) disentangled language-dependent styles and sentence meanings for STS estimations. The removal of specific attributes from representations is also relevant. Previous studies have proposed methods for removing predetermined attributes instead of disentangling for multi-linguality (Chen et al., 2018; Lample et al., 2018) and debiasing (Zemel et al., 2013; Barrett et al., 2019). These previous studies assume that disentangled attributes are distinctive, e.g., language-dependent styles and meanings are supposed to be independent of one another. Similarly, studies on attribute removal assume that the removed attributes are independent of the information remaining in the output representations. In contrast, the distillation of word meaning in context requires a subtle balance to the extent that context information is present in the meaning representations. In this study, we design a self-supervision framework to achieve this challenging goal. Disentanglement techniques"
2021.findings-emnlp.49,Q17-1010,0,0.0482082,"exical information and contextspecific information are captured in different layers of masked language models. They argued that a sophisticated mechanism is required to derive representations of word meaning in context from them. Although contextualised word representations have shown considerable promise, how best to compose the outputs of different layers of masked language models to effectively represent word meaning in context remains an open question. Liu et al. (2020) improved contextualised word representations by transforming their space towards static word embeddings, e.g., fastText (Bojanowski et al., 2017). Although this transformation is computationally efficient, the process is monotonic, weakening the effect of context in representations. As an orthogonal approach, pre-trained masked language models should fit themselves to generate representations of word meaning in context with supervised fine-tuning. However, annotating word meanings in context is non-trivial, and no such resource is abundantly available. To address these challenges, we propose a method that distils representations of word meaning in context from masked language models via self-supervised learning.1 Specifically, our mode"
2021.findings-emnlp.49,2020.acl-main.431,0,0.198245,"? art method that transforms contextualised representations for context-aware lexical semantics. Furthermore, the results confirm that our representations are more effective for composing sentence representations, which contributes to unsupervised STS estimation. 2 2.1 Transformation of Word Representations Previous studies have proposed transformations of contextualised word representations for various purposes. Pooling aggregates multiple representations to perform one of the simplest transformations. Akbik et al. (2019) complement underspecified contexts for named entity recognition, while Bommasani et al. (2020) investigate information captured in layers of pre-trained models. Wang et al. (2019) transform contextualised word representations by inserting them into Skip-gram (Mikolov et al., 2013) to generate static word representations for context-free lexical semantic tasks such as word similarity and analogy prediction. Transformation has also been used to adjust excessive effects of context that dominate representations. Shi et al. (2019) add a transformation matrix on top of the embedding layer of ELMo (Peters et al., 2018). Their approach derives the matrix such that final representations of the"
2021.findings-emnlp.49,S17-2002,0,0.0264602,"utperformed it on STS tasks. In a future work, we plan to investigate correspondences of the context representations. We had assumed that these representations preserve the sentence-level meaning; however, the STS results confirmed that this assumption was incorrect. Another possibility is that context representations may retain syntactic information. We intend to conduct in-depth investigations using syntactic tasks. Moreover, we will expand our method to support multilingual masked language models to contribute to cross-lingual processing, e.g., cross-lingual word in context disambiguation (Camacho-Collados et al., 2017), word alignment (Nagata et al., 2020), and quality estimation and post-editing for machine translation (Fomicheva et al., 2020). The meaning and context representations trained with negative samples as well as the context representations without negative samples preserve these characteristics; in other words, they have noticeable distinction between common and different words and words in paraphrases and non-paraphrases. In Acknowledgments contrast, the meaning representations generated without negative samples have high cosine simi- We appreciate the anonymous reviewers for their inlarities"
2021.findings-emnlp.49,N19-1254,0,0.018422,"combining different layers of a pre-trained model while preserving useful context information intact. 2.2 ???? Related Work Representation Disentanglement Reconstruction Mean-pooling ???? Meaning distiller Context distiller Mean-pooling Mean-pooling Transformer Transformer … ⋮ … They ??ℓ ⋮ ????+1 ???? BERT ⋮ ⋮ … promised ???? him … Figure 1: Distillation of word meaning in context via autoencoder tions. In style-transfer research, Shen et al. (2017) disentangled content and sentiment, whereas John et al. (2019) and Cheng et al. (2020) disentangled content and style. Apart from style-transfer, Chen et al. (2019) disentangled semantics and syntax to estimate semantic and syntactic similarities between sentences, and Wieting et al. (2020) disentangled language-dependent styles and sentence meanings for STS estimations. The removal of specific attributes from representations is also relevant. Previous studies have proposed methods for removing predetermined attributes instead of disentangling for multi-linguality (Chen et al., 2018; Lample et al., 2018) and debiasing (Zemel et al., 2013; Barrett et al., 2019). These previous studies assume that disentangled attributes are distinctive, e.g., language-dep"
2021.findings-emnlp.49,Q18-1039,0,0.0152357,"esearch, Shen et al. (2017) disentangled content and sentiment, whereas John et al. (2019) and Cheng et al. (2020) disentangled content and style. Apart from style-transfer, Chen et al. (2019) disentangled semantics and syntax to estimate semantic and syntactic similarities between sentences, and Wieting et al. (2020) disentangled language-dependent styles and sentence meanings for STS estimations. The removal of specific attributes from representations is also relevant. Previous studies have proposed methods for removing predetermined attributes instead of disentangling for multi-linguality (Chen et al., 2018; Lample et al., 2018) and debiasing (Zemel et al., 2013; Barrett et al., 2019). These previous studies assume that disentangled attributes are distinctive, e.g., language-dependent styles and meanings are supposed to be independent of one another. Similarly, studies on attribute removal assume that the removed attributes are independent of the information remaining in the output representations. In contrast, the distillation of word meaning in context requires a subtle balance to the extent that context information is present in the meaning representations. In this study, we design a self-sup"
2021.findings-emnlp.49,2020.acl-main.673,0,0.0373824,". We adopt an orthogonal approach to derive word in context representations by combining different layers of a pre-trained model while preserving useful context information intact. 2.2 ???? Related Work Representation Disentanglement Reconstruction Mean-pooling ???? Meaning distiller Context distiller Mean-pooling Mean-pooling Transformer Transformer … ⋮ … They ??ℓ ⋮ ????+1 ???? BERT ⋮ ⋮ … promised ???? him … Figure 1: Distillation of word meaning in context via autoencoder tions. In style-transfer research, Shen et al. (2017) disentangled content and sentiment, whereas John et al. (2019) and Cheng et al. (2020) disentangled content and style. Apart from style-transfer, Chen et al. (2019) disentangled semantics and syntax to estimate semantic and syntactic similarities between sentences, and Wieting et al. (2020) disentangled language-dependent styles and sentence meanings for STS estimations. The removal of specific attributes from representations is also relevant. Previous studies have proposed methods for removing predetermined attributes instead of disentangling for multi-linguality (Chen et al., 2018; Lample et al., 2018) and debiasing (Zemel et al., 2013; Barrett et al., 2019). These previous s"
2021.findings-emnlp.49,L18-1269,0,0.0205346,"RT, i.e., [CLS] and [SEP]. We then computed cosine similarities between them. We evaluated the 2012-to-2016 SemEval STS shared tasks (Agirre et al., 2012, 2013, 2014, 2015, 2016), where the goal is to predict human scores that indicate the degree of semantic similarity between two sentences. The Pearson’s r between model predictions and human scores was used as an evaluation metric. Each STS corpus is divided by data sources. Hence, the corpus level score is the average of the Pearson’s r for each sub-corpus. We downloaded and pre-processed STS 2012 to 2016 corpora using the SentEval toolkit (Conneau and Kiela, 2018). The right-side columns of Table 2 show the number of sentence pairs in these corpora. 5.3 Training Corpus Preparation To prepare a training corpus for self-supervised learning as described in Section 4.2, we used English Wikipedia dumps distributed for the WMT20 competition, the texts of which were extracted using WikiExtractor. As a pre-processing step, we first identified the language of each text using the 539 langdetect toolkit and discarded all non-English texts. We then conducted sentence segmentation and tokenization using Stanza (Qi et al., 2020) and extracted sentences of 15 to 50 w"
2021.findings-emnlp.49,N19-1423,0,0.0198896,"ibited a performance competitive with that of the state-of-theWord representations are the basis for various natural language processing tasks. Particularly, they are crucial as a component in context-aware lexical semantics and in the estimation of unsupervised semantic textual similarity (STS) (Arora et al., 2017; Ethayarajh, 2018; Yokoi et al., 2020). Word representations are desired to represent word meaning in context to improve these downstream tasks. Large-scale masked language models pre-trained on massive corpora, e.g., bi-directional encoder representations from transformers (BERT) (Devlin et al., 2019), embed both the context and mean1 ing of a word; thus, word-level representations Code and training corpus are available at generated by such masked language models are github.com/yukiar/distil_wic 534 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 534–546 November 7–11, 2021. ©2021 Association for Computational Linguistics https:// � ?? art method that transforms contextualised representations for context-aware lexical semantics. Furthermore, the results confirm that our representations are more effective for composing sentence representations, which contributes"
2021.findings-emnlp.49,D18-1027,0,0.023627,"mation has also been used to adjust excessive effects of context that dominate representations. Shi et al. (2019) add a transformation matrix on top of the embedding layer of ELMo (Peters et al., 2018). Their approach derives the matrix such that final representations of the same words in paraphrased sentences become similar, whereas those of non-paraphrases become distant. The study most relevant to the present work was conducted by Liu et al. (2020). They transform the space of word representations towards the rotated space of static word embeddings using a crosslingual alignment technique (Doval et al., 2018) for context-aware lexical semantic tasks. In principle, these previous studies aim to make contextualised representations less sensitive to contexts through transformation and prevent them from dominating the representations. We adopt an orthogonal approach to derive word in context representations by combining different layers of a pre-trained model while preserving useful context information intact. 2.2 ???? Related Work Representation Disentanglement Reconstruction Mean-pooling ???? Meaning distiller Context distiller Mean-pooling Mean-pooling Transformer Transformer … ⋮ … They ??ℓ ⋮ ????+"
2021.findings-emnlp.49,J13-3003,0,0.0336865,"ation corpora parison. They categorised context-aware lexical semantic tasks into Within-word and Inter-word tasks. The former evaluates the diversity of word representations for different meanings of the same word associated with different contexts. In contrast, the latter evaluates the similarity of word representations for different words when they have the same meaning. The left-side columns of Table 2 show the number of word pairs in the evaluation corpora. Within-word Tasks The within-word evaluation was divided into three tasks. The first is based on the Usage Similarity (Usim) corpus (Erk et al., 2013), which provides graded similarity between the meanings of the same word in a pair of different contexts. The second task uses the Word in Context (WiC) corpus (Pilehvar and CamachoCollados, 2019), which provides binary judgements as to whether the meaning of a given word varies in different contexts. Following the standard setting recommended in the original work, we tuned the threshold for cosine similarity between word representations to make binary judgments. Specifically, we searched the threshold in the range of [0, 1.0] with 0.01 intervals to maximise the accuracy of the development set"
2021.findings-emnlp.49,W18-3012,0,0.0192266,"construct original representations with an automatically generated training corpus. In contrast to the transformation-based approach, our representations preserve useful context information intact. Experimental results on a range of benchmark tasks show that our representations exhibited a performance competitive with that of the state-of-theWord representations are the basis for various natural language processing tasks. Particularly, they are crucial as a component in context-aware lexical semantics and in the estimation of unsupervised semantic textual similarity (STS) (Arora et al., 2017; Ethayarajh, 2018; Yokoi et al., 2020). Word representations are desired to represent word meaning in context to improve these downstream tasks. Large-scale masked language models pre-trained on massive corpora, e.g., bi-directional encoder representations from transformers (BERT) (Devlin et al., 2019), embed both the context and mean1 ing of a word; thus, word-level representations Code and training corpus are available at generated by such masked language models are github.com/yukiar/distil_wic 534 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 534–546 November 7–11, 2021. ©2021"
2021.findings-emnlp.49,D19-1006,0,0.0176386,"ne outputs of different hidden layers using selfattention through self-supervised learning with an automatically generated training corpus. To evaluate the performance of the proposed approach, we performed comparative experiments using a range of benchmark tasks. The results confirm that our representations exhibited a competitive performance compared to that of the state-of-the-art method transforming contextualised representations for the context-aware lexical semantic tasks and outperformed it for STS estimation. 1 Introduction called contextualised word representations. Previous studies (Ethayarajh, 2019; Vuli´c et al., 2020) have revealed that lexical information and contextspecific information are captured in different layers of masked language models. They argued that a sophisticated mechanism is required to derive representations of word meaning in context from them. Although contextualised word representations have shown considerable promise, how best to compose the outputs of different layers of masked language models to effectively represent word meaning in context remains an open question. Liu et al. (2020) improved contextualised word representations by transforming their space towar"
2021.findings-emnlp.49,2020.emnlp-main.333,0,0.297598,"n. 1 Introduction called contextualised word representations. Previous studies (Ethayarajh, 2019; Vuli´c et al., 2020) have revealed that lexical information and contextspecific information are captured in different layers of masked language models. They argued that a sophisticated mechanism is required to derive representations of word meaning in context from them. Although contextualised word representations have shown considerable promise, how best to compose the outputs of different layers of masked language models to effectively represent word meaning in context remains an open question. Liu et al. (2020) improved contextualised word representations by transforming their space towards static word embeddings, e.g., fastText (Bojanowski et al., 2017). Although this transformation is computationally efficient, the process is monotonic, weakening the effect of context in representations. As an orthogonal approach, pre-trained masked language models should fit themselves to generate representations of word meaning in context with supervised fine-tuning. However, annotating word meanings in context is non-trivial, and no such resource is abundantly available. To address these challenges, we propose"
2021.findings-emnlp.49,N13-1092,0,0.0904731,"Missing"
2021.findings-emnlp.49,2020.emnlp-main.41,0,0.0227329,"e plan to investigate correspondences of the context representations. We had assumed that these representations preserve the sentence-level meaning; however, the STS results confirmed that this assumption was incorrect. Another possibility is that context representations may retain syntactic information. We intend to conduct in-depth investigations using syntactic tasks. Moreover, we will expand our method to support multilingual masked language models to contribute to cross-lingual processing, e.g., cross-lingual word in context disambiguation (Camacho-Collados et al., 2017), word alignment (Nagata et al., 2020), and quality estimation and post-editing for machine translation (Fomicheva et al., 2020). The meaning and context representations trained with negative samples as well as the context representations without negative samples preserve these characteristics; in other words, they have noticeable distinction between common and different words and words in paraphrases and non-paraphrases. In Acknowledgments contrast, the meaning representations generated without negative samples have high cosine simi- We appreciate the anonymous reviewers for their inlarities among all words, regardless of word an"
2021.findings-emnlp.49,2020.semeval-1.18,0,0.0603289,"Missing"
2021.findings-emnlp.49,N18-1202,0,0.0433863,"complement underspecified contexts for named entity recognition, while Bommasani et al. (2020) investigate information captured in layers of pre-trained models. Wang et al. (2019) transform contextualised word representations by inserting them into Skip-gram (Mikolov et al., 2013) to generate static word representations for context-free lexical semantic tasks such as word similarity and analogy prediction. Transformation has also been used to adjust excessive effects of context that dominate representations. Shi et al. (2019) add a transformation matrix on top of the embedding layer of ELMo (Peters et al., 2018). Their approach derives the matrix such that final representations of the same words in paraphrased sentences become similar, whereas those of non-paraphrases become distant. The study most relevant to the present work was conducted by Liu et al. (2020). They transform the space of word representations towards the rotated space of static word embeddings using a crosslingual alignment technique (Doval et al., 2018) for context-aware lexical semantic tasks. In principle, these previous studies aim to make contextualised representations less sensitive to contexts through transformation and preve"
2021.findings-emnlp.49,J15-4004,0,0.0466847,"different contexts. Following the standard setting recommended in the original work, we tuned the threshold for cosine similarity between word representations to make binary judgments. Specifically, we searched the threshold in the range of [0, 1.0] with 0.01 intervals to maximise the accuracy of the development set. The performance of the test set was measured on the CodaLab server.5 The third task is the subtask-1 of CoSimlex (Armendariz et al., 2020) (denoted as CoSimlex-I). The CoSimlex provides a pair of contexts consisting of a few sentences for each word pair extracted from SimLex-999 (Hill et al., 2015). It annotates the graded similarity in each context. CoSimlex-I requires the estimation of the change in similarities between the same word pair in different contexts. Hence, it evaluates whether representations can change for different word meanings according to context. 5 https://competitions.codalab.org/ competitions/20010 Inter-word Tasks The inter-word evaluation consisted of two tasks. The first was the subtask2 of CoSimlex (denoted as CoSimlex-II), which required estimating the similarity between different word pairs in the same context. The second task used the Stanford Contextual Wor"
2021.findings-emnlp.49,P12-1092,0,0.0613326,"milarity in each context. CoSimlex-I requires the estimation of the change in similarities between the same word pair in different contexts. Hence, it evaluates whether representations can change for different word meanings according to context. 5 https://competitions.codalab.org/ competitions/20010 Inter-word Tasks The inter-word evaluation consisted of two tasks. The first was the subtask2 of CoSimlex (denoted as CoSimlex-II), which required estimating the similarity between different word pairs in the same context. The second task used the Stanford Contextual Word Similarity (SCWS) corpus (Huang et al., 2012), which provides graded similarity between word pairs in a pair of different contexts. The contexts of CoSimlex and SCWS consist of several sentences. We input all the sentences as a single context. Evaluation Metrics We estimated the similarity between words using cosine similarity between their representations. We used evaluation metrics determined by each corpus. Namely, we evaluated WiC using accuracy, CoSimlex-I using Pearson’s r, and others using Spearman’s ρ. 5.2 STS Tasks We also evaluated the proposed method on STS tasks. Cosine similarity is commonly used to estimate the similarity b"
2021.findings-emnlp.49,N19-1128,0,0.0182695,"different meanings of the same word associated with different contexts. In contrast, the latter evaluates the similarity of word representations for different words when they have the same meaning. The left-side columns of Table 2 show the number of word pairs in the evaluation corpora. Within-word Tasks The within-word evaluation was divided into three tasks. The first is based on the Usage Similarity (Usim) corpus (Erk et al., 2013), which provides graded similarity between the meanings of the same word in a pair of different contexts. The second task uses the Word in Context (WiC) corpus (Pilehvar and CamachoCollados, 2019), which provides binary judgements as to whether the meaning of a given word varies in different contexts. Following the standard setting recommended in the original work, we tuned the threshold for cosine similarity between word representations to make binary judgments. Specifically, we searched the threshold in the range of [0, 1.0] with 0.01 intervals to maximise the accuracy of the development set. The performance of the test set was measured on the CodaLab server.5 The third task is the subtask-1 of CoSimlex (Armendariz et al., 2020) (denoted as CoSimlex-I). The CoSimlex provides a pair o"
2021.findings-emnlp.49,P19-1041,0,0.156206,"ing the representations. We adopt an orthogonal approach to derive word in context representations by combining different layers of a pre-trained model while preserving useful context information intact. 2.2 ???? Related Work Representation Disentanglement Reconstruction Mean-pooling ???? Meaning distiller Context distiller Mean-pooling Mean-pooling Transformer Transformer … ⋮ … They ??ℓ ⋮ ????+1 ???? BERT ⋮ ⋮ … promised ???? him … Figure 1: Distillation of word meaning in context via autoencoder tions. In style-transfer research, Shen et al. (2017) disentangled content and sentiment, whereas John et al. (2019) and Cheng et al. (2020) disentangled content and style. Apart from style-transfer, Chen et al. (2019) disentangled semantics and syntax to estimate semantic and syntactic similarities between sentences, and Wieting et al. (2020) disentangled language-dependent styles and sentence meanings for STS estimations. The removal of specific attributes from representations is also relevant. Previous studies have proposed methods for removing predetermined attributes instead of disentangling for multi-linguality (Chen et al., 2018; Lample et al., 2018) and debiasing (Zemel et al., 2013; Barrett et al.,"
2021.findings-emnlp.49,2020.acl-demos.14,0,0.01247,"a using the SentEval toolkit (Conneau and Kiela, 2018). The right-side columns of Table 2 show the number of sentence pairs in these corpora. 5.3 Training Corpus Preparation To prepare a training corpus for self-supervised learning as described in Section 4.2, we used English Wikipedia dumps distributed for the WMT20 competition, the texts of which were extracted using WikiExtractor. As a pre-processing step, we first identified the language of each text using the 539 langdetect toolkit and discarded all non-English texts. We then conducted sentence segmentation and tokenization using Stanza (Qi et al., 2020) and extracted sentences of 15 to 50 words. As candidate target words, we extracted the top50k frequent words6 following Liu et al. (2020). We then sampled 1M sentences containing these words from the pre-processed Wikipedia corpus. Using these 1M sentences, we generated positive and negative samples via round-trip translation and masked token prediction. For round-trip translation, we trained translators using exactly the same settings as Kajiwara et al. (2020). For convenience, we used fastText as a static word embedding model in Algorithm 4.1. However, other word embeddings or paraphrase le"
2021.findings-emnlp.49,D19-1113,0,0.0749117,"e representations to perform one of the simplest transformations. Akbik et al. (2019) complement underspecified contexts for named entity recognition, while Bommasani et al. (2020) investigate information captured in layers of pre-trained models. Wang et al. (2019) transform contextualised word representations by inserting them into Skip-gram (Mikolov et al., 2013) to generate static word representations for context-free lexical semantic tasks such as word similarity and analogy prediction. Transformation has also been used to adjust excessive effects of context that dominate representations. Shi et al. (2019) add a transformation matrix on top of the embedding layer of ELMo (Peters et al., 2018). Their approach derives the matrix such that final representations of the same words in paraphrased sentences become similar, whereas those of non-paraphrases become distant. The study most relevant to the present work was conducted by Liu et al. (2020). They transform the space of word representations towards the rotated space of static word embeddings using a crosslingual alignment technique (Doval et al., 2018) for context-aware lexical semantic tasks. In principle, these previous studies aim to make co"
2021.findings-emnlp.49,N07-4000,0,0.316157,"Missing"
2021.findings-emnlp.49,2020.emnlp-main.122,0,0.0220137,"Representation Disentanglement Reconstruction Mean-pooling ???? Meaning distiller Context distiller Mean-pooling Mean-pooling Transformer Transformer … ⋮ … They ??ℓ ⋮ ????+1 ???? BERT ⋮ ⋮ … promised ???? him … Figure 1: Distillation of word meaning in context via autoencoder tions. In style-transfer research, Shen et al. (2017) disentangled content and sentiment, whereas John et al. (2019) and Cheng et al. (2020) disentangled content and style. Apart from style-transfer, Chen et al. (2019) disentangled semantics and syntax to estimate semantic and syntactic similarities between sentences, and Wieting et al. (2020) disentangled language-dependent styles and sentence meanings for STS estimations. The removal of specific attributes from representations is also relevant. Previous studies have proposed methods for removing predetermined attributes instead of disentangling for multi-linguality (Chen et al., 2018; Lample et al., 2018) and debiasing (Zemel et al., 2013; Barrett et al., 2019). These previous studies assume that disentangled attributes are distinctive, e.g., language-dependent styles and meanings are supposed to be independent of one another. Similarly, studies on attribute removal assume that t"
2021.findings-emnlp.49,2020.lrec-1.847,1,0.759785,"static word embedding model M , similarity threshold λ Output: Lexical paraphrase wp of wt 1: M ← ∅, A ← ∅, wp ← ∅ 2: for all wi ∈ S and wj ∈ Sp do 3: M [i][j] ← CosineSim(M (wi ), M (wj )) . Compute cosine similarity of embeddings 4: for all wi ∈ S  wt do . Identify alignments of words other than wt 5: if j = argmax M [i] and i = argmax M [j] then 6: A ← A ∪ {j} 7: for all j ∈ argsort(M [t]) do . Sort indices in descending order of M [t] 8: if j 6∈ A and M [t][j] ≥ λ then 9: wp ← wj 10: break; 11: return wp in Sp corresponds to wt non-trivial. Following the trends on monolingual alignment (Yoshinaka et al., 2020) that use static word embeddings, we designed an alignment method based on a simple heuristic using cosine similarities between the embeddings of words in S and Sp , as depicted in Algorithm 4.1. Specifically, we first identify an alignment between word wi ∈ S wt and wj ∈ Sp if and only if they have highest cosine similarities to each other (line 5). We then determine wp as a word that has the highest cosine similarity to wt satisfying that it is higher or equal to a pre-determined threshold λ and has not been aligned to others (line 9). a descending order of Q and identify wn the word embedd"
2021.naacl-main.169,Q17-1010,0,0.010028,"04) to tokenize Japanese text. The performance of the emotional intensity estimation models is evaluated by the mean absolute error (MAE) and the quadratic weighted kappa (QWK). We evaluated the model using both the emotional intensity labels given by the subjective annotators (subjective labels) and the average of the emotional intensity labels given by the three objective annotators (objective labels). 12 Each writer provided 500 posts for the training set and 100 posts for the validation and test sets. 13 https://taku910.github.io/mecab/ • fastText+SVM vectorizes each word with fastText14 (Bojanowski et al., 2017) and estimates the emotional intensity with a Support Vector Machine based on their average vector. • BERT is a model that fine-tunes the pretrained BERT15 (Devlin et al., 2019) and estimates the emotional intensity as y = softmax(hW ), where h is a feature vector obtained for the [CLS] token of BERT. We investigate the performance of both BERT trained with subjective labels (Subj. BERT) and BERT trained with objective labels (Obj. 14 https://dl.fbaipublicfiles.com/ fasttext/vectors-crawl/cc.ja.300.bin.gz 15 https://huggingface.co/cl-tohoku/ bert-base-japanese-whole-word-masking 2100 MAE Subje"
2021.naacl-main.169,2020.lrec-1.194,0,0.145658,"(Valence, Arousal, and Dominance) by Russell (1980). Table 1 lists datasets with emotional intensity.1 1 In this paper, the emotions of the text writers themselves are called subjective emotions, and the emotions that the readers receive from the text are called objective emotions. These existing emotion analysis datasets include subjective emotional intensity labels by the writers (Scherer and Wallbott, 1994) and objective ones by the readers (Aman and Szpakowicz, 2007; Strapparava and Mihalcea, 2007; Buechel and Hahn, 2017; Mohammad and Bravo-Marquez, 2017a; Mohammad and Kiritchenko, 2018; Bostan et al., 2020), whereas the latter is mainly done by, e.g., expert or crowdsourcing annotators. It depends on the applications whether the writer’s emotions or the reader’s ones to be estimated in NLP-based emotion analysis. For example, in a dialogue system, it is important to estimate the reader’s emotion because we want to know how the user feels in response to the system’s utterance. On the other hand, in applications such as social media mining, we want to estimate the writer’s emotion. In other applications such as story generation, it is worth considering the difference between the emotions the write"
2021.naacl-main.169,C18-1179,0,0.0379437,"Missing"
2021.naacl-main.169,E17-2092,0,0.324587,"t, fear, joy, sadness, surprise, trust, and anticipation) by Plutchik (1980), and VAD model (Valence, Arousal, and Dominance) by Russell (1980). Table 1 lists datasets with emotional intensity.1 1 In this paper, the emotions of the text writers themselves are called subjective emotions, and the emotions that the readers receive from the text are called objective emotions. These existing emotion analysis datasets include subjective emotional intensity labels by the writers (Scherer and Wallbott, 1994) and objective ones by the readers (Aman and Szpakowicz, 2007; Strapparava and Mihalcea, 2007; Buechel and Hahn, 2017; Mohammad and Bravo-Marquez, 2017a; Mohammad and Kiritchenko, 2018; Bostan et al., 2020), whereas the latter is mainly done by, e.g., expert or crowdsourcing annotators. It depends on the applications whether the writer’s emotions or the reader’s ones to be estimated in NLP-based emotion analysis. For example, in a dialogue system, it is important to estimate the reader’s emotion because we want to know how the user feels in response to the system’s utterance. On the other hand, in applications such as social media mining, we want to estimate the writer’s emotion. In other applications such a"
2021.naacl-main.169,N19-1423,0,0.11642,"e given in a four-point scale (no, weak, medium, and strong). Our comparative study over subjective and objective labels demonstrates that readers may not well infer the emotions of the writers, especially of anger and trust. For example, even for posts written by the writer with a strong anger emotion, our readers (i.e., the annotators) did not assign the anger label at all to more than half of the posts with the subjective anger label. Overall, readers may tend to underestimate the writers’ emotional intensities. In addition, experimental results on emotional intensity estimation with BERT (Devlin et al., 2019) show that predicting the subjective labels is a more difficult task than predicting the objective ones. This large gap between the subjective and objective annotations implies the challenge in predicting the subjective emotional intensity for a machine learning model, which can be viewed as a “reader” of the posts. 2 Related Work To estimate the emotional intensity of the text, datasets labeled with Ekman’s six emotions (Ekman, 1992) and Plutchik’s eight emotions (Plutchik, 1980) has been constructed for languages such as English, as shown in Table 1. EmoBank4 (Buechel and Hahn, 2017), which"
2021.naacl-main.169,P06-2059,0,0.0515809,"1 vs. Reader 2 Reader 1 vs. Reader 3 Reader 2 vs. Reader 3 0.697 0.662 0.700 0.607 0.545 0.597 0.594 0.567 0.632 0.342 0.443 0.415 0.627 0.581 0.630 0.359 0.429 0.476 0.527 0.455 0.512 0.203 0.196 0.295 0.547 0.549 0.585 Writer vs. Writer vs. Writer vs. Writer vs. 0.622 0.633 0.624 0.683 0.461 0.526 0.450 0.536 0.423 0.432 0.459 0.498 0.348 0.339 0.396 0.441 0.363 0.386 0.374 0.401 0.333 0.361 0.380 0.433 0.394 0.442 0.467 0.514 0.089 0.153 0.134 0.132 0.439 0.465 0.463 0.515 Reader 1 Reader 2 Reader 3 Avg. Readers Table 2: Inter-annotator agreement by quadratic weighted kappa. Some datasets (Kaji and Kitsuregawa, 2006; Suzuki, 2019) are available in Japanese. However, these are sentences with sentiment polarity, and do not cover the various emotions dealt with in this study. Our study is the first to label Japanese texts with various emotional intensity. • 3: I fully agree with the label given. 3 • 0: I do not think the annotator seriously engaged for this post. • 2: I can find the relevance between the post and label. • 1: I hardly find the relevance between the post and label. Emotional Intensity Annotation 3.1 Annotating Subjective Labels We hired 50 participants via crowdsourcing service Lancers.8 Thos"
2021.naacl-main.169,W04-3230,0,0.263034,"owing the standard emotional intensity estimation models (Acheampong et al., 2020), we train the following three types of four-class classification models for each emotion. • BoW+LogReg employs Bag-of-Words to extract features and Logistic Regression to the estimate emotional intensity. Experimental Settings In this experiment, we divided the dataset12 into training set of 15,000 posts from 30 writers, validation set of 1,000 posts from 10 writers, and evaluation set of 1,000 posts from 10 writers. That is, there is no duplication of writers between the splits. We used MeCab (IPADIC-2.7.0)13 (Kudo et al., 2004) to tokenize Japanese text. The performance of the emotional intensity estimation models is evaluated by the mean absolute error (MAE) and the quadratic weighted kappa (QWK). We evaluated the model using both the emotional intensity labels given by the subjective annotators (subjective labels) and the average of the emotional intensity labels given by the three objective annotators (objective labels). 12 Each writer provided 500 posts for the training set and 100 posts for the validation and test sets. 13 https://taku910.github.io/mecab/ • fastText+SVM vectorizes each word with fastText14 (Boj"
2021.naacl-main.169,S17-1007,0,0.308327,"urprise, trust, and anticipation) by Plutchik (1980), and VAD model (Valence, Arousal, and Dominance) by Russell (1980). Table 1 lists datasets with emotional intensity.1 1 In this paper, the emotions of the text writers themselves are called subjective emotions, and the emotions that the readers receive from the text are called objective emotions. These existing emotion analysis datasets include subjective emotional intensity labels by the writers (Scherer and Wallbott, 1994) and objective ones by the readers (Aman and Szpakowicz, 2007; Strapparava and Mihalcea, 2007; Buechel and Hahn, 2017; Mohammad and Bravo-Marquez, 2017a; Mohammad and Kiritchenko, 2018; Bostan et al., 2020), whereas the latter is mainly done by, e.g., expert or crowdsourcing annotators. It depends on the applications whether the writer’s emotions or the reader’s ones to be estimated in NLP-based emotion analysis. For example, in a dialogue system, it is important to estimate the reader’s emotion because we want to know how the user feels in response to the system’s utterance. On the other hand, in applications such as social media mining, we want to estimate the writer’s emotion. In other applications such as story generation, it is worth co"
2021.naacl-main.169,W17-5205,0,0.0389243,"Missing"
2021.naacl-main.169,S18-1001,0,0.0278752,"Missing"
2021.naacl-main.169,L18-1030,0,0.0711005,"y Plutchik (1980), and VAD model (Valence, Arousal, and Dominance) by Russell (1980). Table 1 lists datasets with emotional intensity.1 1 In this paper, the emotions of the text writers themselves are called subjective emotions, and the emotions that the readers receive from the text are called objective emotions. These existing emotion analysis datasets include subjective emotional intensity labels by the writers (Scherer and Wallbott, 1994) and objective ones by the readers (Aman and Szpakowicz, 2007; Strapparava and Mihalcea, 2007; Buechel and Hahn, 2017; Mohammad and Bravo-Marquez, 2017a; Mohammad and Kiritchenko, 2018; Bostan et al., 2020), whereas the latter is mainly done by, e.g., expert or crowdsourcing annotators. It depends on the applications whether the writer’s emotions or the reader’s ones to be estimated in NLP-based emotion analysis. For example, in a dialogue system, it is important to estimate the reader’s emotion because we want to know how the user feels in response to the system’s utterance. On the other hand, in applications such as social media mining, we want to estimate the writer’s emotion. In other applications such as story generation, it is worth considering the difference between"
2021.naacl-main.169,D13-1170,0,0.00413074,"bjective labels than the readers’. The large gap between the subjective and objective emotions implies the complexity of the mapping from a post to the subjective emotional intensities, which also leads to a lower performance with machine learning models. 1 Introduction Emotion analysis is one of the major NLP tasks with a wide range of applications, such as a dialogue system (Tokuhisa et al., 2008) and social media mining (Stieglitz and Dang-Xuan, 2013). Since emotion analysis has been actively studied, not only the classification of the sentiment polarity (positive or negative) of the text (Socher et al., 2013), but also more detailed emotion detection and emotional intensity estimation (Bostan and Klinger, 2018) have been attempted in recent years. Previous studies on emotion analysis use six emotions (anger, disgust, fear, joy, sadness, and surprise) by Ekman (1992), eight emotions (anger, disgust, fear, joy, sadness, surprise, trust, and anticipation) by Plutchik (1980), and VAD model (Valence, Arousal, and Dominance) by Russell (1980). Table 1 lists datasets with emotional intensity.1 1 In this paper, the emotions of the text writers themselves are called subjective emotions, and the emotions th"
2021.naacl-main.169,S07-1013,0,0.730795,"), eight emotions (anger, disgust, fear, joy, sadness, surprise, trust, and anticipation) by Plutchik (1980), and VAD model (Valence, Arousal, and Dominance) by Russell (1980). Table 1 lists datasets with emotional intensity.1 1 In this paper, the emotions of the text writers themselves are called subjective emotions, and the emotions that the readers receive from the text are called objective emotions. These existing emotion analysis datasets include subjective emotional intensity labels by the writers (Scherer and Wallbott, 1994) and objective ones by the readers (Aman and Szpakowicz, 2007; Strapparava and Mihalcea, 2007; Buechel and Hahn, 2017; Mohammad and Bravo-Marquez, 2017a; Mohammad and Kiritchenko, 2018; Bostan et al., 2020), whereas the latter is mainly done by, e.g., expert or crowdsourcing annotators. It depends on the applications whether the writer’s emotions or the reader’s ones to be estimated in NLP-based emotion analysis. For example, in a dialogue system, it is important to estimate the reader’s emotion because we want to know how the user feels in response to the system’s utterance. On the other hand, in applications such as social media mining, we want to estimate the writer’s emotion. In o"
2021.naacl-main.169,C08-1111,0,0.0603374,"at the reader cannot fully detect the emotions of the writer, especially anger and trust. In addition, experimental results in estimating the emotional intensity show that it is more difficult to estimate the writer’s subjective labels than the readers’. The large gap between the subjective and objective emotions implies the complexity of the mapping from a post to the subjective emotional intensities, which also leads to a lower performance with machine learning models. 1 Introduction Emotion analysis is one of the major NLP tasks with a wide range of applications, such as a dialogue system (Tokuhisa et al., 2008) and social media mining (Stieglitz and Dang-Xuan, 2013). Since emotion analysis has been actively studied, not only the classification of the sentiment polarity (positive or negative) of the text (Socher et al., 2013), but also more detailed emotion detection and emotional intensity estimation (Bostan and Klinger, 2018) have been attempted in recent years. Previous studies on emotion analysis use six emotions (anger, disgust, fear, joy, sadness, and surprise) by Ekman (1992), eight emotions (anger, disgust, fear, joy, sadness, surprise, trust, and anticipation) by Plutchik (1980), and VAD mod"
2021.wat-1.20,W18-6402,0,0.0223804,"isual modalities is the key to performance enhancement in our TMEKU system, which leads to better visual information use. 1 Introduction Neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015) has achieved state-of-the-art translation performance. However, there remain numerous situations where textual context alone is insufficient for correct translation, such as in the presence of ambiguous words and grammatical gender. Therefore, researchers in this field have established multimodal neural machine translation (MNMT) tasks (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018), which translates sentences paired with images into a target language. Due to the lack of multimodal datasets, multimodal tasks on the English→Japanese (En→Ja) language pair have not been paid attention to. Since the year 2020, as the multimodal dataset on the En→Ja language pair has been made publicly available, the multimodal machine translation (MMT) tasks on the En→Ja were held at the WAT 2020 (Nakazawa et al., 2020) for the first time. Some studies (Tamura et al., 2020) have started to focus on incorporating multimodal contents, particularly images, to improve the translation performance"
2021.wat-1.20,Q17-1010,0,0.0161645,"ject class instead of only the object class. We take these visual concepts to represent the image regions. We set each image labeled with 36 visual concepts of image regions, which are space-separated phrases. For the words, we lowercase and tokenize the source English sentences via the Moses toolkit.3 The soft alignment is a similarity matrix filled with the cosine similarity between source words and visual concepts. To avoid unknown words, we convert the words and concepts into subword units using the byte pair encoding (BPE) model (Sennrich et al., 2016). Subsequently, we utilize fastText (Bojanowski et al., 2017) to learn subword embeddings. We use a pre-trained model4 containing two million word vectors trained with subword information on Common Crawl (600B tokens). The source subword embeddings can be generated directly, whereas the generation of visual concept embeddings should take an average of the embeddings of all constituent subwords because they are phrases. As shown in Figure 1, source subwords are represented by W = {w1 , w2 , w3 , · · · , wn }, and the visual concepts are represented by C = {c1 , c2 , c3 , · · · , c36 }. These embeddings provide a mapping function from a subword to a 300-d"
2021.wat-1.20,W18-6439,0,0.118929,"e the |R |and |H |represent the length of source words and the numbers of image regions: n and 36; the CONCAT is a concatenation operator. 175 H Atxt h1 Bi-directional RNN man in red shirt watches dog on an agility course . polo (yt-1) GRU (3) GRU (2) hn GRU (1) RoI r3 en zt hn-1 rouge (yt) Aimg Halign agilité ct CONCAT r2 homme Text-attention a3txt h3 R r1 un h2 . Image-attention a3img &lt;eos&gt; r35 r36 Figure 2: The TMEKU system. 2.3 (1) img T eimg ) tanh(U img st t,j = (V Decoder To generate target word yt at time step t, a hidden (1) state proposal st is computed in the first cell of deepGRU (Delbrouck and Dupont, 2018) (GRU (1)) by function fgru1 (yt−1 , st−1 ). The function considers the previously emitted target word yt−1 and generated hidden state st−1 as follows. (1) st = (1 − ξˆt ) s˙ t + ξˆt st−1 ˆ γt = σ(Wγ EY [yt−1 ] + Uγ st−1 ) ξˆt = σ(Wξ EY [yt−1 ] + Uξ st−1 ) where Wξ , Uξ , Wγ , Uγ , W , and U are training parameters, and EY is the target word embedding. 2.3.1 Text-Attention At time step t, the text-attention focuses on every textual annotation atxt in Atxt and assigns an ati tention weight. The textual context vector zt is generated as follows. (1) img αt,j = softmax(eimg t,j ) ct = img img αt,"
2021.wat-1.20,W17-4718,0,0.0171891,"ween the textual and visual modalities is the key to performance enhancement in our TMEKU system, which leads to better visual information use. 1 Introduction Neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015) has achieved state-of-the-art translation performance. However, there remain numerous situations where textual context alone is insufficient for correct translation, such as in the presence of ambiguous words and grammatical gender. Therefore, researchers in this field have established multimodal neural machine translation (MNMT) tasks (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018), which translates sentences paired with images into a target language. Due to the lack of multimodal datasets, multimodal tasks on the English→Japanese (En→Ja) language pair have not been paid attention to. Since the year 2020, as the multimodal dataset on the En→Ja language pair has been made publicly available, the multimodal machine translation (MMT) tasks on the En→Ja were held at the WAT 2020 (Nakazawa et al., 2020) for the first time. Some studies (Tamura et al., 2020) have started to focus on incorporating multimodal contents, particularly images, to improve the"
2021.wat-1.20,2020.lrec-1.518,0,0.035343,"K, K 0 ∈ Rn×m and b, b0 ∈ Rn are the training parameters. 5 https://github.com/nyu-dl/ dl4mt-tutorial/blob/master/docs/cgru.pdf 176 To ensure that both representations have their own projections to compute the candidate probabilities, a textual GRU block and visual GRU block (Delbrouck and Dupont, 2018) obtained as below. bvt = fght (Wbv st ) (2) btt = fght (Wbt st ) t v yt ∼ pt = softmax(Wproj btt + Wproj bvt ), t ,Wv where Wbv , Wbt , Wproj proj are training parameters. 3 Experiments 3.1 Dataset Firstly, we conducted experiments for the En→Ja task using the official Flickr30kEnt-JP dataset (Nakayama et al., 2020), which was extended from the Flickr30k (Young et al., 2014) and Flickr30k Entities (Plummer et al., 2017) datasets, where manual Japanese translations were newly added. For training and validation, we used the Flickr30kEnt-JP dataset6 for Japanese sentences, the Flickr30k Entities dataset7 for English sentences, and the Flickr30k dataset8 for images. They were sharing the same splits of training and validation data made in Flickr30k Entities. For test data, we used the officially provided data of the Flickr30kEnt-JP task, and their corresponding images were in the Flickr30k dataset. Note that"
2021.wat-1.20,P02-1040,0,0.109413,"m; word embedding to 200dim; batch size to 32; beam size to 12; text dropout to 0.3; image region dropout to 0.5; dropout of source RNN hidden states to 0.5; and blocks btt and bvt to 0.5. Specifically, the textual annotation Atxt was 800dim, which was consistent with H. Further, the visual annotation Aimg was 4,096-dim by a concatenation of R and Halign , where R was 2,048-dim and Halign was 2,048-dim by a linear transformation from 800-dim. We trained the model using stochastic gradient descent with ADAM (Kingma and Ba, 2015) and a learning rate of 0.0004. We stopped training when the BLEU (Papineni et al., 2002) score did not improve for 20 evaluations on the validation set, 11 177 https://taku910.github.io/mecab/ Model Baseline NMT Baseline MNMT TMEKU System v.s. baseline NMT v.s. baseline MNMT Ensemble (top 10 models) Test NMT baseline by BLEU scores of 0.86 and outperformed the MNMT baseline by BLEU scores of 0.69 on the official test set. Our TMEKU system achieved significant improvement over both the NMT and MNMT baselines. Moreover, the result of ensembling the top 10 models has achieved the first place in the ranking of this task. We also participated in the Ambiguous MSCOCO task on the En→Ja"
2021.wat-1.20,P16-1162,0,0.0113637,"concept consisting of an attribute class followed by an object class instead of only the object class. We take these visual concepts to represent the image regions. We set each image labeled with 36 visual concepts of image regions, which are space-separated phrases. For the words, we lowercase and tokenize the source English sentences via the Moses toolkit.3 The soft alignment is a similarity matrix filled with the cosine similarity between source words and visual concepts. To avoid unknown words, we convert the words and concepts into subword units using the byte pair encoding (BPE) model (Sennrich et al., 2016). Subsequently, we utilize fastText (Bojanowski et al., 2017) to learn subword embeddings. We use a pre-trained model4 containing two million word vectors trained with subword information on Common Crawl (600B tokens). The source subword embeddings can be generated directly, whereas the generation of visual concept embeddings should take an average of the embeddings of all constituent subwords because they are phrases. As shown in Figure 1, source subwords are represented by W = {w1 , w2 , w3 , · · · , wn }, and the visual concepts are represented by C = {c1 , c2 , c3 , · · · , c36 }. These em"
2021.wat-1.20,W16-2346,0,0.0443301,"Missing"
2021.wat-1.20,2020.wat-1.7,1,0.745616,"field have established multimodal neural machine translation (MNMT) tasks (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018), which translates sentences paired with images into a target language. Due to the lack of multimodal datasets, multimodal tasks on the English→Japanese (En→Ja) language pair have not been paid attention to. Since the year 2020, as the multimodal dataset on the En→Ja language pair has been made publicly available, the multimodal machine translation (MMT) tasks on the En→Ja were held at the WAT 2020 (Nakazawa et al., 2020) for the first time. Some studies (Tamura et al., 2020) have started to focus on incorporating multimodal contents, particularly images, to improve the translation performance on the En→Ja task. In this study, we apply our system (Zhao et al., 2021) for the MMT task on the En→Ja language pair, which is called TMEKU system. This system is designed to translate a source word into a target word, focusing on a relevant image region. To guide the model to translate certain words based on certain image regions, explicit alignment over source words and image regions is needed. We propose to generate soft alignment of word-region based on cosine similarit"
2021.wat-1.20,Q14-1006,0,0.0188477,"ps://github.com/nyu-dl/ dl4mt-tutorial/blob/master/docs/cgru.pdf 176 To ensure that both representations have their own projections to compute the candidate probabilities, a textual GRU block and visual GRU block (Delbrouck and Dupont, 2018) obtained as below. bvt = fght (Wbv st ) (2) btt = fght (Wbt st ) t v yt ∼ pt = softmax(Wproj btt + Wproj bvt ), t ,Wv where Wbv , Wbt , Wproj proj are training parameters. 3 Experiments 3.1 Dataset Firstly, we conducted experiments for the En→Ja task using the official Flickr30kEnt-JP dataset (Nakayama et al., 2020), which was extended from the Flickr30k (Young et al., 2014) and Flickr30k Entities (Plummer et al., 2017) datasets, where manual Japanese translations were newly added. For training and validation, we used the Flickr30kEnt-JP dataset6 for Japanese sentences, the Flickr30k Entities dataset7 for English sentences, and the Flickr30k dataset8 for images. They were sharing the same splits of training and validation data made in Flickr30k Entities. For test data, we used the officially provided data of the Flickr30kEnt-JP task, and their corresponding images were in the Flickr30k dataset. Note that the Japanese training data size is originally 148,915 sente"
2021.wat-1.20,2020.eamt-1.12,1,0.81952,"abulary sizes of En→Ja were 9,578→22,274 tokens. For image regions, we used Faster-RCNN (Ren et al., 2015) in Anderson et al. (2018) to detect up to 36 salient visual objects per image and extracted their corresponding 2,048-dim image region features and attribute-object combined concepts. 3.3 Settings (i) NMT: the baseline NMT system (Bahdanau et al., 2015) is the architecture comprised a 2-layer bidirectional GRU encoder and a 2-layer cGRU decoder with attention mechanism, which only encodes the source sentence as the input. (ii) MNMT: the baseline MNMT system without word-region alignment (Zhao et al., 2020). This architecture comprised a 2-layer bidirectional GRU encoder and a 2-layer cGRU decoder with double attentions to integrate visual and textual features. (iii) TMEKU system: our proposed MNMT system with word-region alignment. We conducted all experiments on Nmtpy toolkit (Caglayan et al., 2017). 3.3.1 Parameters We ensured that the parameters were consistent in all the settings. We set the encoder and decoder hidden state to 400-dim; word embedding to 200dim; batch size to 32; beam size to 12; text dropout to 0.3; image region dropout to 0.5; dropout of source RNN hidden states to 0.5; an"
C16-1109,S12-1051,0,0.0216001,"tistical machine translation and showed that sentence pairs with a moderate level of similarity are effective for training text simplification models. Therefore, we use the sentence similarity method to accurately measure the moderate level of 3 https://www.ukp.tu-darmstadt.de/data/sentence-simplification/ simple-complex-sentence-pairs/ 4 http://www.cs.pomona.edu/˜dkauchak/simplification/ 1149 similarity. To address the challenge of computing the similarity between sentences containing different words with similar meanings, many methods have been proposed. In semantic textual similarity task (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015), sentence similarity is computed on the basis of word similarity following the success of word embeddings such as word2vec (Mikolov et al., 2013a). For example, a supervised approach using word embeddings when obtaining a word alignment achieved the best performance in SemEval-2015 Task 2 (Sultan et al., 2015). Word embeddings have also been used in unsupervised sentence similarity metrics (Mikolov et al., 2013b; Song and Roth, 2015; Kusner et al., 2015). These unsupervised sentence similarity metrics can be applied to the automa"
C16-1109,S13-1004,0,0.0173112,"slation and showed that sentence pairs with a moderate level of similarity are effective for training text simplification models. Therefore, we use the sentence similarity method to accurately measure the moderate level of 3 https://www.ukp.tu-darmstadt.de/data/sentence-simplification/ simple-complex-sentence-pairs/ 4 http://www.cs.pomona.edu/˜dkauchak/simplification/ 1149 similarity. To address the challenge of computing the similarity between sentences containing different words with similar meanings, many methods have been proposed. In semantic textual similarity task (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015), sentence similarity is computed on the basis of word similarity following the success of word embeddings such as word2vec (Mikolov et al., 2013a). For example, a supervised approach using word embeddings when obtaining a word alignment achieved the best performance in SemEval-2015 Task 2 (Sultan et al., 2015). Word embeddings have also been used in unsupervised sentence similarity metrics (Mikolov et al., 2013b; Song and Roth, 2015; Kusner et al., 2015). These unsupervised sentence similarity metrics can be applied to the automatic construction of a"
C16-1109,S15-2045,0,0.158019,"15). These unsupervised sentence similarity metrics can be applied to the automatic construction of a monolingual parallel corpus for text simplification, without requiring the data to be labeled. 3 Sentence Similarity based on Alignment between Word Embeddings We propose four types of sentence similarity measures for building a monolingual parallel corpus for text simplification, based on alignments between word embeddings that have achieved outstanding performance on different NLP tasks. The methods discussed in Sections 3.1-3.3 are the sentence similarity measures proposed by Song and Roth (2015) for a short text similarity task. The Word Mover’s Distance (Kusner et al., 2015) discussed in Section 3.4 is another sentence similarity measure based on alignment between word embeddings that is known to achieve good performance on a document classification task. 3.1 Average Alignment The sentence similarity STSave (x, y) between sentence x and sentence y is computed by averaging the similarities between all pairs of words taken from the two sentences, as follows: |x ||y| 1 XX STSave (x, y) = φ(xi , yj ) |x||y| (1) i=1 j=1 Here, xi denotes the i-th word in the sentence x (x = (x1 , x2 , . ."
C16-1109,W11-1603,0,0.0188976,"015b; Goto et al., 2015). However, unlike statistical machine translation, which uses bilingual parallel corpora, text simplification requires a monolingual parallel corpus for training. While bilingual parallel data are available in large quantities, monolingual parallel data are hard to obtain because simplification of a complex text is not a by-product of other tasks. Monolingual parallel corpora for text simplification are available in only seven languages—English (Zhu et al., 2010; Coster and Kauchak, 2011b; Hwang et al., 2015; Xu et al., 2015), Portuguese (Caseli et al., 2009), Spanish (Bott and Saggion, 2011), Danish (Klerke and Søgaard, 2012), German (Klaper et al., 2013), Italian (Brunato et al., 2015), and Japanese (Goto et al., 2015). In addition, only the English corpora are open to the public. We therefore propose an unsupervised method 1 that automatically builds monolingual parallel corpora for text simplification without using any external resources for computing sentence similarity. In this study, a monolingual parallel corpus for text simplification is built from a comparable corpus comprising complex and simple texts. This was done in two steps. First, we compute the similarity for all"
C16-1109,W15-1604,0,0.0612777,"Missing"
C16-1109,W11-1601,0,0.304794,"tistical machine translation trained using the corpus built by the proposed method to that using the existing corpora. 1 Introduction Text simplification is the process of rewriting a complex text into a simpler form while preserving its meaning. The purpose of text simplification is to assist the comprehension of readers, especially language learners and children. Recent studies have treated text simplification as a monolingual machine translation problem in which a simple synonymous sentence is generated using the framework of statistical machine translation (Specia, 2010; Zhu et al., 2010; Coster and Kauchak, 2011a; Coster and ˇ ˇ Kauchak, 2011b; Wubben et al., 2012; Stajner et al., 2015a; Stajner et al., 2015b; Goto et al., 2015). However, unlike statistical machine translation, which uses bilingual parallel corpora, text simplification requires a monolingual parallel corpus for training. While bilingual parallel data are available in large quantities, monolingual parallel data are hard to obtain because simplification of a complex text is not a by-product of other tasks. Monolingual parallel corpora for text simplification are available in only seven languages—English (Zhu et al., 2010; Coster and Ka"
C16-1109,P11-2117,0,0.220372,"tistical machine translation trained using the corpus built by the proposed method to that using the existing corpora. 1 Introduction Text simplification is the process of rewriting a complex text into a simpler form while preserving its meaning. The purpose of text simplification is to assist the comprehension of readers, especially language learners and children. Recent studies have treated text simplification as a monolingual machine translation problem in which a simple synonymous sentence is generated using the framework of statistical machine translation (Specia, 2010; Zhu et al., 2010; Coster and Kauchak, 2011a; Coster and ˇ ˇ Kauchak, 2011b; Wubben et al., 2012; Stajner et al., 2015a; Stajner et al., 2015b; Goto et al., 2015). However, unlike statistical machine translation, which uses bilingual parallel corpora, text simplification requires a monolingual parallel corpus for training. While bilingual parallel data are available in large quantities, monolingual parallel data are hard to obtain because simplification of a complex text is not a by-product of other tasks. Monolingual parallel corpora for text simplification are available in only seven languages—English (Zhu et al., 2010; Coster and Ka"
C16-1109,2015.mtsummit-papers.2,0,0.569469,"oduction Text simplification is the process of rewriting a complex text into a simpler form while preserving its meaning. The purpose of text simplification is to assist the comprehension of readers, especially language learners and children. Recent studies have treated text simplification as a monolingual machine translation problem in which a simple synonymous sentence is generated using the framework of statistical machine translation (Specia, 2010; Zhu et al., 2010; Coster and Kauchak, 2011a; Coster and ˇ ˇ Kauchak, 2011b; Wubben et al., 2012; Stajner et al., 2015a; Stajner et al., 2015b; Goto et al., 2015). However, unlike statistical machine translation, which uses bilingual parallel corpora, text simplification requires a monolingual parallel corpus for training. While bilingual parallel data are available in large quantities, monolingual parallel data are hard to obtain because simplification of a complex text is not a by-product of other tasks. Monolingual parallel corpora for text simplification are available in only seven languages—English (Zhu et al., 2010; Coster and Kauchak, 2011b; Hwang et al., 2015; Xu et al., 2015), Portuguese (Caseli et al., 2009), Spanish (Bott and Saggion, 2011),"
C16-1109,W11-2123,0,0.0106434,"implification corpora (Zhu et al., 2010; Coster and Kauchak, 2011b; Hwang et al., 2015). The results were compared to evaluate the effectiveness of our text simplification corpus. We treated text simplification as a translation problem from the normal sentence to the simple one and modeled it using a phrase-based SMT trained as a log linear model. In each corpus, we randomly sampled 500 sentence pairs for tuning with MERT (Och, 2003) and used the remainder for training. Moses was used as the phrase-based SMT tool. We employed GIZA++ (Och and Ney, 2003) to obtain the word alignment, and KenLM (Heafield, 2011) to build the 5-gram language model from the entire Simple English Wikipedia 7 . As test data, we used 277 sentence pairs labeled G and 281 sentence pairs labeled G + GP from the Hwang et al. (2015) dataset and evaluated the accuracy using BLEU. Table 3 shows the number of sentences, range of vocabulary, average number of words per sentence, and BLEU scores of the text simplification models trained on each corpus. The text simplification model trained on our corpus achieved the best BLEU score. To compare the learning curves of our corpus with that from Hwang et al. (2015), we recorded the BLE"
C16-1109,N15-1022,0,0.144244,"ˇ ˇ Kauchak, 2011b; Wubben et al., 2012; Stajner et al., 2015a; Stajner et al., 2015b; Goto et al., 2015). However, unlike statistical machine translation, which uses bilingual parallel corpora, text simplification requires a monolingual parallel corpus for training. While bilingual parallel data are available in large quantities, monolingual parallel data are hard to obtain because simplification of a complex text is not a by-product of other tasks. Monolingual parallel corpora for text simplification are available in only seven languages—English (Zhu et al., 2010; Coster and Kauchak, 2011b; Hwang et al., 2015; Xu et al., 2015), Portuguese (Caseli et al., 2009), Spanish (Bott and Saggion, 2011), Danish (Klerke and Søgaard, 2012), German (Klaper et al., 2013), Italian (Brunato et al., 2015), and Japanese (Goto et al., 2015). In addition, only the English corpora are open to the public. We therefore propose an unsupervised method 1 that automatically builds monolingual parallel corpora for text simplification without using any external resources for computing sentence similarity. In this study, a monolingual parallel corpus for text simplification is built from a comparable corpus comprising complex"
C16-1109,W13-2902,0,0.0486956,"ation, which uses bilingual parallel corpora, text simplification requires a monolingual parallel corpus for training. While bilingual parallel data are available in large quantities, monolingual parallel data are hard to obtain because simplification of a complex text is not a by-product of other tasks. Monolingual parallel corpora for text simplification are available in only seven languages—English (Zhu et al., 2010; Coster and Kauchak, 2011b; Hwang et al., 2015; Xu et al., 2015), Portuguese (Caseli et al., 2009), Spanish (Bott and Saggion, 2011), Danish (Klerke and Søgaard, 2012), German (Klaper et al., 2013), Italian (Brunato et al., 2015), and Japanese (Goto et al., 2015). In addition, only the English corpora are open to the public. We therefore propose an unsupervised method 1 that automatically builds monolingual parallel corpora for text simplification without using any external resources for computing sentence similarity. In this study, a monolingual parallel corpus for text simplification is built from a comparable corpus comprising complex and simple texts. This was done in two steps. First, we compute the similarity for all combinations of complex and simple sentences using the alignment"
C16-1109,klerke-sogaard-2012-dsim,0,0.0310404,", unlike statistical machine translation, which uses bilingual parallel corpora, text simplification requires a monolingual parallel corpus for training. While bilingual parallel data are available in large quantities, monolingual parallel data are hard to obtain because simplification of a complex text is not a by-product of other tasks. Monolingual parallel corpora for text simplification are available in only seven languages—English (Zhu et al., 2010; Coster and Kauchak, 2011b; Hwang et al., 2015; Xu et al., 2015), Portuguese (Caseli et al., 2009), Spanish (Bott and Saggion, 2011), Danish (Klerke and Søgaard, 2012), German (Klaper et al., 2013), Italian (Brunato et al., 2015), and Japanese (Goto et al., 2015). In addition, only the English corpora are open to the public. We therefore propose an unsupervised method 1 that automatically builds monolingual parallel corpora for text simplification without using any external resources for computing sentence similarity. In this study, a monolingual parallel corpus for text simplification is built from a comparable corpus comprising complex and simple texts. This was done in two steps. First, we compute the similarity for all combinations of complex and simple"
C16-1109,P07-2045,0,0.0109755,"l corpus. • The proposed method can build a monolingual parallel corpus for text simplification at low cost because it does not require any external resources such as labeled data or dictionaries when computing sentence similarity. 2 Related Work The statistical machine translation framework has become widely used in text simplification. In English, text simplification using a monolingual parallel corpus extracted from the English Wikipedia and Simple English Wikipedia has been actively studied. Coster and Kauchak (2011b) simplified sentences using the standard phrase-based SMT toolkit Moses (Koehn et al., 2007) and evaluated it using the standard automatic MT evaluation metric BLEU (Papineni et al., 2002). In addition to generic SMT translation models, specialized translation models such as targeting phrasal deletion have been proposed (Zhu et al., 2010; Coster and Kauchak, 2011a; Wubben et al., 2012). These studies reported that models specialized 2 http://ssli.ee.washington.edu/tial/projects/simplification/ 1148 Figure 2: Readability score distribution of English Wikipedia and Simple English Wikipedia. A higher score in Flesch Reading Ease indicates simpler sentences. in text simplification improv"
C16-1109,J03-1002,0,0.00740768,"t simplification models using our corpus and existing text simplification corpora (Zhu et al., 2010; Coster and Kauchak, 2011b; Hwang et al., 2015). The results were compared to evaluate the effectiveness of our text simplification corpus. We treated text simplification as a translation problem from the normal sentence to the simple one and modeled it using a phrase-based SMT trained as a log linear model. In each corpus, we randomly sampled 500 sentence pairs for tuning with MERT (Och, 2003) and used the remainder for training. Moses was used as the phrase-based SMT tool. We employed GIZA++ (Och and Ney, 2003) to obtain the word alignment, and KenLM (Heafield, 2011) to build the 5-gram language model from the entire Simple English Wikipedia 7 . As test data, we used 277 sentence pairs labeled G and 281 sentence pairs labeled G + GP from the Hwang et al. (2015) dataset and evaluated the accuracy using BLEU. Table 3 shows the number of sentences, range of vocabulary, average number of words per sentence, and BLEU scores of the text simplification models trained on each corpus. The text simplification model trained on our corpus achieved the best BLEU score. To compare the learning curves of our corpu"
C16-1109,P03-1021,0,0.0105773,"rom our text simplification corpus ranked by similarity. 4.3 English Text Simplification We trained SMT-based text simplification models using our corpus and existing text simplification corpora (Zhu et al., 2010; Coster and Kauchak, 2011b; Hwang et al., 2015). The results were compared to evaluate the effectiveness of our text simplification corpus. We treated text simplification as a translation problem from the normal sentence to the simple one and modeled it using a phrase-based SMT trained as a log linear model. In each corpus, we randomly sampled 500 sentence pairs for tuning with MERT (Och, 2003) and used the remainder for training. Moses was used as the phrase-based SMT tool. We employed GIZA++ (Och and Ney, 2003) to obtain the word alignment, and KenLM (Heafield, 2011) to build the 5-gram language model from the entire Simple English Wikipedia 7 . As test data, we used 277 sentence pairs labeled G and 281 sentence pairs labeled G + GP from the Hwang et al. (2015) dataset and evaluated the accuracy using BLEU. Table 3 shows the number of sentences, range of vocabulary, average number of words per sentence, and BLEU scores of the text simplification models trained on each corpus. The"
C16-1109,P02-1040,0,0.0958609,"on at low cost because it does not require any external resources such as labeled data or dictionaries when computing sentence similarity. 2 Related Work The statistical machine translation framework has become widely used in text simplification. In English, text simplification using a monolingual parallel corpus extracted from the English Wikipedia and Simple English Wikipedia has been actively studied. Coster and Kauchak (2011b) simplified sentences using the standard phrase-based SMT toolkit Moses (Koehn et al., 2007) and evaluated it using the standard automatic MT evaluation metric BLEU (Papineni et al., 2002). In addition to generic SMT translation models, specialized translation models such as targeting phrasal deletion have been proposed (Zhu et al., 2010; Coster and Kauchak, 2011a; Wubben et al., 2012). These studies reported that models specialized 2 http://ssli.ee.washington.edu/tial/projects/simplification/ 1148 Figure 2: Readability score distribution of English Wikipedia and Simple English Wikipedia. A higher score in Flesch Reading Ease indicates simpler sentences. in text simplification improved readability and the BLEU score. In languages other than English, text ˇ simplification using"
C16-1109,N15-1138,0,0.078013,"words with similar meanings, many methods have been proposed. In semantic textual similarity task (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015), sentence similarity is computed on the basis of word similarity following the success of word embeddings such as word2vec (Mikolov et al., 2013a). For example, a supervised approach using word embeddings when obtaining a word alignment achieved the best performance in SemEval-2015 Task 2 (Sultan et al., 2015). Word embeddings have also been used in unsupervised sentence similarity metrics (Mikolov et al., 2013b; Song and Roth, 2015; Kusner et al., 2015). These unsupervised sentence similarity metrics can be applied to the automatic construction of a monolingual parallel corpus for text simplification, without requiring the data to be labeled. 3 Sentence Similarity based on Alignment between Word Embeddings We propose four types of sentence similarity measures for building a monolingual parallel corpus for text simplification, based on alignments between word embeddings that have achieved outstanding performance on different NLP tasks. The methods discussed in Sections 3.1-3.3 are the sentence similarity measures propose"
C16-1109,S15-2027,0,0.0260286,"/simplification/ 1149 similarity. To address the challenge of computing the similarity between sentences containing different words with similar meanings, many methods have been proposed. In semantic textual similarity task (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015), sentence similarity is computed on the basis of word similarity following the success of word embeddings such as word2vec (Mikolov et al., 2013a). For example, a supervised approach using word embeddings when obtaining a word alignment achieved the best performance in SemEval-2015 Task 2 (Sultan et al., 2015). Word embeddings have also been used in unsupervised sentence similarity metrics (Mikolov et al., 2013b; Song and Roth, 2015; Kusner et al., 2015). These unsupervised sentence similarity metrics can be applied to the automatic construction of a monolingual parallel corpus for text simplification, without requiring the data to be labeled. 3 Sentence Similarity based on Alignment between Word Embeddings We propose four types of sentence similarity measures for building a monolingual parallel corpus for text simplification, based on alignments between word embeddings that have achieved outstandi"
C16-1109,P15-2135,0,0.0605519,"hod to that using the existing corpora. 1 Introduction Text simplification is the process of rewriting a complex text into a simpler form while preserving its meaning. The purpose of text simplification is to assist the comprehension of readers, especially language learners and children. Recent studies have treated text simplification as a monolingual machine translation problem in which a simple synonymous sentence is generated using the framework of statistical machine translation (Specia, 2010; Zhu et al., 2010; Coster and Kauchak, 2011a; Coster and ˇ ˇ Kauchak, 2011b; Wubben et al., 2012; Stajner et al., 2015a; Stajner et al., 2015b; Goto et al., 2015). However, unlike statistical machine translation, which uses bilingual parallel corpora, text simplification requires a monolingual parallel corpus for training. While bilingual parallel data are available in large quantities, monolingual parallel data are hard to obtain because simplification of a complex text is not a by-product of other tasks. Monolingual parallel corpora for text simplification are available in only seven languages—English (Zhu et al., 2010; Coster and Kauchak, 2011b; Hwang et al., 2015; Xu et al., 2015), Portuguese (Caseli et a"
C16-1109,R15-1080,0,0.0623022,"hod to that using the existing corpora. 1 Introduction Text simplification is the process of rewriting a complex text into a simpler form while preserving its meaning. The purpose of text simplification is to assist the comprehension of readers, especially language learners and children. Recent studies have treated text simplification as a monolingual machine translation problem in which a simple synonymous sentence is generated using the framework of statistical machine translation (Specia, 2010; Zhu et al., 2010; Coster and Kauchak, 2011a; Coster and ˇ ˇ Kauchak, 2011b; Wubben et al., 2012; Stajner et al., 2015a; Stajner et al., 2015b; Goto et al., 2015). However, unlike statistical machine translation, which uses bilingual parallel corpora, text simplification requires a monolingual parallel corpus for training. While bilingual parallel data are available in large quantities, monolingual parallel data are hard to obtain because simplification of a complex text is not a by-product of other tasks. Monolingual parallel corpora for text simplification are available in only seven languages—English (Zhu et al., 2010; Coster and Kauchak, 2011b; Hwang et al., 2015; Xu et al., 2015), Portuguese (Caseli et a"
C16-1109,P12-1107,0,0.144611,"Missing"
C16-1109,C10-1152,0,0.605214,"e framework of statistical machine translation trained using the corpus built by the proposed method to that using the existing corpora. 1 Introduction Text simplification is the process of rewriting a complex text into a simpler form while preserving its meaning. The purpose of text simplification is to assist the comprehension of readers, especially language learners and children. Recent studies have treated text simplification as a monolingual machine translation problem in which a simple synonymous sentence is generated using the framework of statistical machine translation (Specia, 2010; Zhu et al., 2010; Coster and Kauchak, 2011a; Coster and ˇ ˇ Kauchak, 2011b; Wubben et al., 2012; Stajner et al., 2015a; Stajner et al., 2015b; Goto et al., 2015). However, unlike statistical machine translation, which uses bilingual parallel corpora, text simplification requires a monolingual parallel corpus for training. While bilingual parallel data are available in large quantities, monolingual parallel data are hard to obtain because simplification of a complex text is not a by-product of other tasks. Monolingual parallel corpora for text simplification are available in only seven languages—English (Zhu e"
D19-5552,P17-2070,0,0.0192232,"eves a state-of-the-art performance on lexical substitution tasks. • Creation and release2 of CEFR-LP, which is a new evaluation dataset for lexical substitution with an expanded coverage of substitution candidates and English proficiency levels. 2 Related Work There are two major approaches to lexical substitution. One approach generates contextualized word embeddings by assigning multiple embeddings to one word. Paetzold and Specia (2016) generated word embeddings per part-of-speech of the same word assuming that words with the same surface have different senses for different part-ofspeech. Fadaee et al. (2017) also generated multiple word embeddings per topic represented in a sentence. For example, the word soft may have embeddings for topics of food when used like soft cheese and that for music when used like soft voice. To adequately distinguish these word senses, both methods assign embeddings that are too coarse. For example, the phrases soft cheese and soft drink both use soft as an adjective and are related to the food topic. The former has the sense of tender while the latter represents the sense of non-alcoholic. To solve this problem, DMSE generates finer-grained word 3 Proposed Method We"
D19-5552,Y18-1004,1,0.838788,"efing . go start (4), proceed (1), move (1) ... Table 1: Example of the lexical substitution tasks weights. The key technology to solve lexical substitution tasks is to precisely capture word senses in a context. There are mainly two approaches for lexical substitution: (1) generating contextualized word embeddings by assigning multiple embeddings to one word and (2) generating context embeddings using the sentence. The former realizes static embeddings as it pre-computes word embeddings. One example of the first approach is DMSE (Dependency-based Multi-Sense Embedding), which was proposed by Ashihara et al. (2018) to contextualize word embeddings using words with dependency relations as a clue to distinguish senses. As an example of the second approach, context2vec (Melamud et al., 2016) generates a context embedding by inputting the sentence into bidirectional recurrent neural networks. It combines context embedding and a simple word embedding to generate a dynamic embedding. These two methods are current state-ofthe-arts among methods of each approach. We focus on the fact that these two methods have a complementary nature. DMSE considers only a single word as context, while context2vec uses a simple"
D19-5552,E14-1057,0,0.0697646,"Missing"
D19-5552,P14-5010,0,0.0112493,"naturally less frequent than others in general documents. The distribution reflects this tendency. We believe that these CEFR levels are useful when applying lexical substitution technologies to educational applications. 5 Evaluation Settings This section describes the evaluation settings used to investigate the performance of our method on lexical substitution tasks. 5.1 Training of Our Method To train contextualized word embeddings by using our method, we used 61.6M sentences7 extracted from the main contents of English Wikipedia8 articles. We lemmatized each word using the Stanford Parser (Manning et al., 2014) and replaced words less than or equal to ten frequency to ⟨unk⟩ tag to reduce the size of the vocabulary. Pre-training used the same hyper-parameter settings of context2vec (Table 5). These settings achieved the best performance on lexical substitution tasks in Melamud et al. (2016). 4.3 Analysis of CEFR-LP Table 3 shows the basic statistics for CEFR-LP compared to those in LS-SE and LS-CIC. CEFRLP provides 14, 259 substitution candidates for 863 target words. The average number of paraphrasable candidates per word is 10.0, which is larger than 3.48 of LS-SE and 6.65 of LS-CIC. Here, a paraph"
D19-5552,W02-0816,0,0.170416,"es of both contextualized word embeddings and context embeddings. Specifically, we apply a contextualized word embedding generated by DMSE to replace the word embedding used in context2vec. Introduction Lexical substitution (McCarthy and Navigli, 2007) is the finest-level paraphrase problem. It determines if a word in a sentence can be replaced by other words while preserving the same meaning. It is important not only as a fundamental paraphrase problem but also as a practical application for language learning support such as lexical simplification (Paetzold and Specia, 2017) and acquisition (McCarthy, 2002). Table 1 shows an example of the lexical substitution task with a sentence,1 the target word to replace, and words of substitution candidates. The numbers in parentheses represent the paraphrasability of each candidate, where a larger value means the corresponding word is more appropriate to substitute the target word. The lexical substitution task ranks these candidates according to assigning 1 In this paper, the terms context and sentence are used interchangeably wherever the context for the target refers to the sentence. 397 Proceedings of the 2019 EMNLP Workshop W-NUT: The 5th Workshop on"
D19-5552,S07-1009,0,0.350563,"xt embedding and a simple word embedding to generate a dynamic embedding. These two methods are current state-ofthe-arts among methods of each approach. We focus on the fact that these two methods have a complementary nature. DMSE considers only a single word as context, while context2vec uses a simple word embedding. Herein we combine DMSE and context2vec to take advantages of both contextualized word embeddings and context embeddings. Specifically, we apply a contextualized word embedding generated by DMSE to replace the word embedding used in context2vec. Introduction Lexical substitution (McCarthy and Navigli, 2007) is the finest-level paraphrase problem. It determines if a word in a sentence can be replaced by other words while preserving the same meaning. It is important not only as a fundamental paraphrase problem but also as a practical application for language learning support such as lexical simplification (Paetzold and Specia, 2017) and acquisition (McCarthy, 2002). Table 1 shows an example of the lexical substitution task with a sentence,1 the target word to replace, and words of substitution candidates. The numbers in parentheses represent the paraphrasability of each candidate, where a larger v"
D19-5552,N15-1050,0,0.0179194,"on to generate a unified context embedding for the target word. On the other hand, the target word is represented by a word embedding that has the same dimensions as the conRanking Method As shown in Table 1, lexical substitution ranks substitution candidates of the target word based on their paraphrasabilities under a given context. We use the same ranking method with context2vec, which assumes not only that a good substitution candidate is semantically similar to the target word but also is suitable for a given context. This assumption is commonly used in recent lexical substitution models (Melamud et al., 2015; Roller and Erk, 2016). Here we have target word t and its dependencyword d. The contextualized word embedding of t is noted as vtd and the word embedding of a substitution candidate s contextualized by d is vsd . Finally, the context embedding is denoted as vc . The 399 following scores are calculated for each substitution candidate and ranked them in descending order. they have limited annotation coverage because the annotators provide substitution candidates manually. Specifically, each annotator provides up to three substitution candidates for LS-SE and up to five substitution candidates"
D19-5552,K16-1006,0,0.30083,"capture word senses in a context. There are mainly two approaches for lexical substitution: (1) generating contextualized word embeddings by assigning multiple embeddings to one word and (2) generating context embeddings using the sentence. The former realizes static embeddings as it pre-computes word embeddings. One example of the first approach is DMSE (Dependency-based Multi-Sense Embedding), which was proposed by Ashihara et al. (2018) to contextualize word embeddings using words with dependency relations as a clue to distinguish senses. As an example of the second approach, context2vec (Melamud et al., 2016) generates a context embedding by inputting the sentence into bidirectional recurrent neural networks. It combines context embedding and a simple word embedding to generate a dynamic embedding. These two methods are current state-ofthe-arts among methods of each approach. We focus on the fact that these two methods have a complementary nature. DMSE considers only a single word as context, while context2vec uses a simple word embedding. Herein we combine DMSE and context2vec to take advantages of both contextualized word embeddings and context embeddings. Specifically, we apply a contextualized"
D19-5552,W16-4912,0,0.0510228,"text2vec outperforms ELMo in Section 6. • A method that takes advantages of contextualized word embedding and dynamic embedding generation from contexts is proposed. This method achieves a state-of-the-art performance on lexical substitution tasks. • Creation and release2 of CEFR-LP, which is a new evaluation dataset for lexical substitution with an expanded coverage of substitution candidates and English proficiency levels. 2 Related Work There are two major approaches to lexical substitution. One approach generates contextualized word embeddings by assigning multiple embeddings to one word. Paetzold and Specia (2016) generated word embeddings per part-of-speech of the same word assuming that words with the same surface have different senses for different part-ofspeech. Fadaee et al. (2017) also generated multiple word embeddings per topic represented in a sentence. For example, the word soft may have embeddings for topics of food when used like soft cheese and that for music when used like soft voice. To adequately distinguish these word senses, both methods assign embeddings that are too coarse. For example, the phrases soft cheese and soft drink both use soft as an adjective and are related to the food"
D19-5552,N18-1202,0,0.0158751,"embedding using bidirectional long short-term memory (biLSTM) networks (Schuster and Paliwal, 1997). Then it combines the context embedding with a simple word embedding. Context2vec is the current stateof-the-art method for representative lexical substitution tasks. Its advantage is that it can consider the entire sentence as the context, while DMSE is bounded by a window size. However, DMSE can use contextualized word embeddings, whereas context2vec just uses a simple word embedding for each word. The complementary nature of these two methods inspired us to combine them. More recently, ELMo (Peters et al., 2018) showed a language modeling using biLSTM networks produces contextualized word embeddings, which are effective for various NLP tasks such as named entity recognition. Context2vec differs from ELMo when explicitly considering word embeddings of substitution targets. Our experiments empirically confirm that context2vec outperforms ELMo in Section 6. • A method that takes advantages of contextualized word embedding and dynamic embedding generation from contexts is proposed. This method achieves a state-of-the-art performance on lexical substitution tasks. • Creation and release2 of CEFR-LP, which"
D19-5552,N16-1131,0,0.0148923,"ed context embedding for the target word. On the other hand, the target word is represented by a word embedding that has the same dimensions as the conRanking Method As shown in Table 1, lexical substitution ranks substitution candidates of the target word based on their paraphrasabilities under a given context. We use the same ranking method with context2vec, which assumes not only that a good substitution candidate is semantically similar to the target word but also is suitable for a given context. This assumption is commonly used in recent lexical substitution models (Melamud et al., 2015; Roller and Erk, 2016). Here we have target word t and its dependencyword d. The contextualized word embedding of t is noted as vtd and the word embedding of a substitution candidate s contextualized by d is vsd . Finally, the context embedding is denoted as vc . The 399 following scores are calculated for each substitution candidate and ranked them in descending order. they have limited annotation coverage because the annotators provide substitution candidates manually. Specifically, each annotator provides up to three substitution candidates for LS-SE and up to five substitution candidates for LS-CIC. These candi"
D19-5552,L18-1514,1,0.898985,"value means the corresponding word is more appropriate to substitute the target word. The lexical substitution task ranks these candidates according to assigning 1 In this paper, the terms context and sentence are used interchangeably wherever the context for the target refers to the sentence. 397 Proceedings of the 2019 EMNLP Workshop W-NUT: The 5th Workshop on Noisy User-generated Text, pages 397–406 c Hong Kong, Nov 4, 2019. 2019 Association for Computational Linguistics In addition, we create a new evaluation dataset for lexical substitution, named CEFR-LP. It is an extension of CEFR-LS (Uchida et al., 2018) and is created for lexical simplification to support substitution tasks. The benefits of CEFR-LP are that it expands the coverage of substitution candidates and provides English proficiency levels. These features are unavailable in previous evaluation datasets such as LS-SE (McCarthy and Navigli, 2007) and LS-CIC (Kremer et al., 2014). The evaluation results on CEFR-LP, LS-SE, and LS-CIC confirm that our method effectively strengthens DMSE and context2vec. Additionally, our proposed method outperforms the current state-of-the-art methods. The contributions of this paper are twofold: embedding"
I17-1009,ganitkevitch-callison-burch-2014-multilingual,0,0.0552687,". (2011) reranked paraphrase pairs acquired via bilingual pivoting using distributional similarity. The main idea of reranking paraphrase pairs using information from a monolingual corpus is similar to ours, but Chan et al.’s method failed to acquire semantically similar paraphrases. We succeeded in acquiring semantically similar paraphrases because we effectively combined information from a bilingual corpus and a monolingual corpus by using weighted PMI. In addition to English, paraphrase databases are constructed in many languages using bilingual pivoting (Bannard and Callison-Burch, 2005). Ganitkevitch and Callison-Burch (2014) constructed paraphrase databases8 in 23 languages, including European languages and Chinese. 7 Conclusion We proposed a new approach for formalizing lexical paraphrasability based on weighted PMI and acquired paraphrase pairs using information from both a bilingual corpus and a monolingual corpus. Our proposed method, MIPA, uses bilingual pivoting weighted by distributional similarity to acquire paraphrase pairs robustly, as each of the methods complements the other. Experimental results using manually annotated datasets for lexical paraphrase showed that the proposed method outperformed bili"
I17-1009,N13-1092,0,0.0730301,"Missing"
I17-1009,P15-2011,0,0.0689292,"Missing"
I17-1009,W11-2123,0,0.0228054,"Missing"
I17-1009,2005.mtsummit-papers.11,0,0.0823385,"Missing"
I17-1009,D15-1163,0,0.019982,"roach in terms of MRR (Figure 9) and MAP (Figure 10). Furthermore, Mizukami et al. (2014) constructed the Japanese version9 . In this study, we improved bilingual pivoting using a monolingual corpus. Since large-scale monolingual corpora are easily available for many languages, our proposed method may improve paraphrase databases in each of these languages. PPDB (Ganitkevitch et al., 2013) constructed by bilingual pivoting is used in many NLP applications, such as learning word embeddings (Yu and Dredze, 2014), semantic textual similarity (Sultan et al., 2015), machine translation (Mehdizadeh Seraj et al., 2015), sentence compression (Napoles et al., 2016), question answering (Sultan et al., 2016), and text simplification (Xu et al., 2016). Our proposed method may improve the performance of many of these NLP applications supported by PPDB. 6 Related Work Levy and Goldberg (2014) explained a wellknown representation learning method for word embeddings, the skip-gram with negativesampling (SGNS) (Mikolov et al., 2013a,b), as a matrix factorization of a word-context co-occurrence matrix with shifted positive PMI. In this paper, we explained a well-known method for paraphrase acquisition, bilingual pivot"
I17-1009,P05-1074,0,0.558871,"man’s correlation. The contributions of our study are as follows. Introduction Paraphrases are useful for flexible language understanding in many NLP applications. For example, the usefulness of the paraphrase database PPDB (Ganitkevitch et al., 2013; Pavlick et al., 2015), a publicly available largescale resource for lexical paraphrasing, has been reported for tasks such as learning word embeddings (Yu and Dredze, 2014) and semantic textual similarity (Sultan et al., 2015). In PPDB, paraphrase pairs are acquired via word alignment on a bilingual corpus by a process called bilingual pivoting (Bannard and Callison-Burch, 2005). Figure 1 shows an example of English language paraphrase acquisition using the German language as a pivot. Although bilingual pivoting is widely used for paraphrase acquisition, it always includes noise 80 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 80–89, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP without loss of generality, we set2 λ1 = λ2 = −1. • Bilingual pivoting-based lexical paraphrase acquisition is generalized using PMI. sbp (e1 , e2 ) = log p(e2 |e1 ) + log p(e1 |e2 ) • Lexical paraphrases are acquired robustly us"
I17-1009,W11-2504,0,0.0218909,"Missing"
I17-1009,N16-3013,0,0.0146782,"igure 10). Furthermore, Mizukami et al. (2014) constructed the Japanese version9 . In this study, we improved bilingual pivoting using a monolingual corpus. Since large-scale monolingual corpora are easily available for many languages, our proposed method may improve paraphrase databases in each of these languages. PPDB (Ganitkevitch et al., 2013) constructed by bilingual pivoting is used in many NLP applications, such as learning word embeddings (Yu and Dredze, 2014), semantic textual similarity (Sultan et al., 2015), machine translation (Mehdizadeh Seraj et al., 2015), sentence compression (Napoles et al., 2016), question answering (Sultan et al., 2016), and text simplification (Xu et al., 2016). Our proposed method may improve the performance of many of these NLP applications supported by PPDB. 6 Related Work Levy and Goldberg (2014) explained a wellknown representation learning method for word embeddings, the skip-gram with negativesampling (SGNS) (Mikolov et al., 2013a,b), as a matrix factorization of a word-context co-occurrence matrix with shifted positive PMI. In this paper, we explained a well-known method for paraphrase acquisition, bilingual pivoting (Bannard and Callison-Burch, 2005; Ganitk"
I17-1009,J03-1002,0,0.0142727,"Missing"
I17-1009,P15-2070,0,0.0343085,"Missing"
I17-1009,Q14-1018,0,0.0202178,"ownstream applications. The semantic textual similarity task deals with calculating the semantic similarity between two sentences. In this study, we conducted the evaluation by applying Pearson’s correlation coefficient with a five-step manual evaluation using five datasets constructed by SemEval (Agirre et al., 2012, 2013, 2014, 2015, 2016). We applied the acquired paraphrase pairs to the unsupervised method of DLC@CU (Sultan et al., 2015), which achieved excellent results using PPDB in the semantic textual similarity task of SemEval2015 (Agirre et al., 2015). DLS@CU performs word alignment (Sultan et al., 2014) using PPDB, and calculates sentence similarity according to the ratio of aligned words: sts(s1 , s2 ) = na (s1 ) + na (s2 ) n(s1 ) + n(s2 ) 5.3 Reranking PPDB 2.0 Finally, we reranked paraphrase pairs from a publicly available state-of-the-art paraphrase database.8 PPDB 2.0 (Pavlick et al., 2015) scores paraphrase pairs using supervised learning with (12) 8 86 http://paraphrase.org/ Figure 9: Reranking PPDB 2.0 in MRR. Figure 10: Reranking PPDB 2.0 in MAP. 26,455 labeled data and 209 features. We sorted the paraphrase pairs from PPDB 2.0 using the MIPA instead of the PPDB 2.0 score and used t"
I17-1009,S15-2027,0,0.186232,"distributional similarity themselves in terms of metrics such as mean reciprocal rank (MRR), mean average precision (MAP), coverage, and Spearman’s correlation. The contributions of our study are as follows. Introduction Paraphrases are useful for flexible language understanding in many NLP applications. For example, the usefulness of the paraphrase database PPDB (Ganitkevitch et al., 2013; Pavlick et al., 2015), a publicly available largescale resource for lexical paraphrasing, has been reported for tasks such as learning word embeddings (Yu and Dredze, 2014) and semantic textual similarity (Sultan et al., 2015). In PPDB, paraphrase pairs are acquired via word alignment on a bilingual corpus by a process called bilingual pivoting (Bannard and Callison-Burch, 2005). Figure 1 shows an example of English language paraphrase acquisition using the German language as a pivot. Although bilingual pivoting is widely used for paraphrase acquisition, it always includes noise 80 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 80–89, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP without loss of generality, we set2 λ1 = λ2 = −1. • Bilingual pivoting-bas"
I17-1009,Q16-1009,0,0.0115207,"14) constructed the Japanese version9 . In this study, we improved bilingual pivoting using a monolingual corpus. Since large-scale monolingual corpora are easily available for many languages, our proposed method may improve paraphrase databases in each of these languages. PPDB (Ganitkevitch et al., 2013) constructed by bilingual pivoting is used in many NLP applications, such as learning word embeddings (Yu and Dredze, 2014), semantic textual similarity (Sultan et al., 2015), machine translation (Mehdizadeh Seraj et al., 2015), sentence compression (Napoles et al., 2016), question answering (Sultan et al., 2016), and text simplification (Xu et al., 2016). Our proposed method may improve the performance of many of these NLP applications supported by PPDB. 6 Related Work Levy and Goldberg (2014) explained a wellknown representation learning method for word embeddings, the skip-gram with negativesampling (SGNS) (Mikolov et al., 2013a,b), as a matrix factorization of a word-context co-occurrence matrix with shifted positive PMI. In this paper, we explained a well-known method for paraphrase acquisition, bilingual pivoting (Bannard and Callison-Burch, 2005; Ganitkevitch et al., 2013), as a (weighted) PMI."
I17-1009,Q16-1029,0,0.019607,"study, we improved bilingual pivoting using a monolingual corpus. Since large-scale monolingual corpora are easily available for many languages, our proposed method may improve paraphrase databases in each of these languages. PPDB (Ganitkevitch et al., 2013) constructed by bilingual pivoting is used in many NLP applications, such as learning word embeddings (Yu and Dredze, 2014), semantic textual similarity (Sultan et al., 2015), machine translation (Mehdizadeh Seraj et al., 2015), sentence compression (Napoles et al., 2016), question answering (Sultan et al., 2016), and text simplification (Xu et al., 2016). Our proposed method may improve the performance of many of these NLP applications supported by PPDB. 6 Related Work Levy and Goldberg (2014) explained a wellknown representation learning method for word embeddings, the skip-gram with negativesampling (SGNS) (Mikolov et al., 2013a,b), as a matrix factorization of a word-context co-occurrence matrix with shifted positive PMI. In this paper, we explained a well-known method for paraphrase acquisition, bilingual pivoting (Bannard and Callison-Burch, 2005; Ganitkevitch et al., 2013), as a (weighted) PMI. Chan et al. (2011) reranked paraphrase pai"
I17-1009,P14-2089,0,0.150119,"lts show that MIPA outperforms bilingual pivoting and distributional similarity themselves in terms of metrics such as mean reciprocal rank (MRR), mean average precision (MAP), coverage, and Spearman’s correlation. The contributions of our study are as follows. Introduction Paraphrases are useful for flexible language understanding in many NLP applications. For example, the usefulness of the paraphrase database PPDB (Ganitkevitch et al., 2013; Pavlick et al., 2015), a publicly available largescale resource for lexical paraphrasing, has been reported for tasks such as learning word embeddings (Yu and Dredze, 2014) and semantic textual similarity (Sultan et al., 2015). In PPDB, paraphrase pairs are acquired via word alignment on a bilingual corpus by a process called bilingual pivoting (Bannard and Callison-Burch, 2005). Figure 1 shows an example of English language paraphrase acquisition using the German language as a pivot. Although bilingual pivoting is widely used for paraphrase acquisition, it always includes noise 80 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 80–89, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP without loss of gene"
I17-1009,S13-1004,0,\N,Missing
I17-1009,S12-1051,0,\N,Missing
I17-2019,D16-1215,0,0.0358178,"Missing"
I17-2019,P11-2087,0,0.072296,"Missing"
I17-2019,W15-1501,0,0.0598011,"Missing"
I17-2019,P11-2117,0,0.144051,"tence pair, while the other is to classify given sentence pair into one of the three classes (good, ok, and bad). In the classification task of the QATS workshop, systems based on deep neural networks (Paetzold and Specia, ˇ 2016a) and MT metrics (Stajner et al., 2016a) have achieved the best performance. However, deep neural networks are rather unstable because of the difficulty of training on a limited amount of data; for instance, the QATS dataset offers only 505 sentence pairs for training. MT metrics are incapable of properly capturing deletions that are prevalent in text simplification (Coster and Kauchak, 2011), as they are originally designed to gauge semantic equivalence. In fact, as shown in Table 1, well-known MT metrics are strongly biased by the length difference between original and simple sentences, even though it is rather unrelated with the quality of text simplification assessed by humans. In order to properly account for the surfacelevel inequivalency occurring in text simplification, we examine semantic similarity features based on word embeddings and paraphrase lexicons. Unlike end-to-end training with deep neural networks, we quantify word-level semantic correIntroduction Text simplif"
I17-2019,W16-4912,0,0.0354931,"a sentence, we used the averaged word embeddings (Adi et al., 2017). |x| |y| i=1 j=1 1 ∑ 1 ∑ xi − yj DWE(x, y) = |x| |y| 3.3 Baseline Systems As the baseline, we employed four types of sysˇ tems from the QATS workshop (Stajner et al., 2016b): two typical baselines and two top-ranked systems. “Majority-class” labels all the sentence pairs with the most frequent class in the training data. “MT-baseline” combines BLEU (Papineni et al., 2002), METEOR (Lavie and Denkowski, 2009), TER (Snover et al., 2006), and WER (Levenshtein, 1966), using a support vector machine (SVM) classifier. SimpleNets (Paetzold and Specia, 2016a) has two different forms of deep neural network architectures: multi-layer perceptron (SimpleNetsMLP) and recurrent neural network (SimpleNetsRNN). SimpleNets-MLP uses seven features of each sentence: the number of characters, tokens, and word types, 5-gram language model probabilities estimated on the basis of either SUBTLEX (Brysbaert and New, 2009), SubIMDB (Paetzold and Specia, 2016b), Wikipedia, and Simple Wikipedia (Kauchak, 2013). SimpleNets-RNN, which does not require such feature engineering, uses embeddings of word N -grams. ˇ SMH (Stajner et al., 2016a) has two types of classifier"
I17-2019,N15-1022,0,0.0128603,"on such features achieves good performance in the classification task. 2 noise by considering only the best word alignment for each word in one sentence as follows. |x| 1 ∑ MAS(x, y) = max cos(xi , yj ) j |x| As MAS is asymmetric, we calculate it for each direction, i.e., MAS(x, y) and MAS(y, x), unlike Kajiwara and Komachi (2016) who has averaged these two values. Semantic Features Based on Word Alignments We bring a total of seven types of features that are proven useful for the similar task, i.e., finding corresponding sentence pairs within English Wikipedia and Simple English Wikipedia (Hwang et al., 2015; Kajiwara and Komachi, 2016). Specifically, we assume that some of these features are useful to capture inequivalency between original sentence and its simplified version introduced during simplification, such as lexical paraphrases and deletion of words and phrases. Throughout this section, original sentence and its simplified version are referred to as x and y, respectively. 2.4 HAS: Hungarian Alignment Similarity AAS and MAS deal with many-to-many and oneto-many word alignments, respectively. On the other hand, HAS (Song and Roth, 2015) is based on one-to-one word alignments. The task of i"
I17-2019,P02-1040,0,0.098149,"nce of Word Embeddings We also introduce the difference between sentence embeddings so as to gauge their differences in terms of meaning and simplicity. As the representation of a sentence, we used the averaged word embeddings (Adi et al., 2017). |x| |y| i=1 j=1 1 ∑ 1 ∑ xi − yj DWE(x, y) = |x| |y| 3.3 Baseline Systems As the baseline, we employed four types of sysˇ tems from the QATS workshop (Stajner et al., 2016b): two typical baselines and two top-ranked systems. “Majority-class” labels all the sentence pairs with the most frequent class in the training data. “MT-baseline” combines BLEU (Papineni et al., 2002), METEOR (Lavie and Denkowski, 2009), TER (Snover et al., 2006), and WER (Levenshtein, 1966), using a support vector machine (SVM) classifier. SimpleNets (Paetzold and Specia, 2016a) has two different forms of deep neural network architectures: multi-layer perceptron (SimpleNetsMLP) and recurrent neural network (SimpleNetsRNN). SimpleNets-MLP uses seven features of each sentence: the number of characters, tokens, and word types, 5-gram language model probabilities estimated on the basis of either SUBTLEX (Brysbaert and New, 2009), SubIMDB (Paetzold and Specia, 2016b), Wikipedia, and Simple Wik"
I17-2019,C16-1109,1,0.887469,"Missing"
I17-2019,P15-2070,0,0.0621332,"Missing"
I17-2019,P13-1151,0,0.0192105,"(Lavie and Denkowski, 2009), TER (Snover et al., 2006), and WER (Levenshtein, 1966), using a support vector machine (SVM) classifier. SimpleNets (Paetzold and Specia, 2016a) has two different forms of deep neural network architectures: multi-layer perceptron (SimpleNetsMLP) and recurrent neural network (SimpleNetsRNN). SimpleNets-MLP uses seven features of each sentence: the number of characters, tokens, and word types, 5-gram language model probabilities estimated on the basis of either SUBTLEX (Brysbaert and New, 2009), SubIMDB (Paetzold and Specia, 2016b), Wikipedia, and Simple Wikipedia (Kauchak, 2013). SimpleNets-RNN, which does not require such feature engineering, uses embeddings of word N -grams. ˇ SMH (Stajner et al., 2016a) has two types of classifiers: logistic classifier (SMH-IBk/Logistic) and random forest classifier (SMH-RandForest, SMH-RandForest-b). Both are trained relying on the automatic evaluation metrics for MT, such as BLEU, METEOR, and TER, in combination with the QE features for MT (Specia et al., 2013). Instead of reimplementing the above baseline systems, we excerpted their performance scores ˇ from (Stajner et al., 2016b). (6) 2.7 PAS: Paraphrase Alignment Similarity"
I17-2019,N16-1131,0,0.0395215,"Missing"
I17-2019,2006.amta-papers.25,0,0.060982,"entence embeddings so as to gauge their differences in terms of meaning and simplicity. As the representation of a sentence, we used the averaged word embeddings (Adi et al., 2017). |x| |y| i=1 j=1 1 ∑ 1 ∑ xi − yj DWE(x, y) = |x| |y| 3.3 Baseline Systems As the baseline, we employed four types of sysˇ tems from the QATS workshop (Stajner et al., 2016b): two typical baselines and two top-ranked systems. “Majority-class” labels all the sentence pairs with the most frequent class in the training data. “MT-baseline” combines BLEU (Papineni et al., 2002), METEOR (Lavie and Denkowski, 2009), TER (Snover et al., 2006), and WER (Levenshtein, 1966), using a support vector machine (SVM) classifier. SimpleNets (Paetzold and Specia, 2016a) has two different forms of deep neural network architectures: multi-layer perceptron (SimpleNetsMLP) and recurrent neural network (SimpleNetsRNN). SimpleNets-MLP uses seven features of each sentence: the number of characters, tokens, and word types, 5-gram language model probabilities estimated on the basis of either SUBTLEX (Brysbaert and New, 2009), SubIMDB (Paetzold and Specia, 2016b), Wikipedia, and Simple Wikipedia (Kauchak, 2013). SimpleNets-RNN, which does not require"
I17-2019,N15-1138,0,0.171418,"within English Wikipedia and Simple English Wikipedia (Hwang et al., 2015; Kajiwara and Komachi, 2016). Specifically, we assume that some of these features are useful to capture inequivalency between original sentence and its simplified version introduced during simplification, such as lexical paraphrases and deletion of words and phrases. Throughout this section, original sentence and its simplified version are referred to as x and y, respectively. 2.4 HAS: Hungarian Alignment Similarity AAS and MAS deal with many-to-many and oneto-many word alignments, respectively. On the other hand, HAS (Song and Roth, 2015) is based on one-to-one word alignments. The task of identifying the best one-to-one word alignments H is regarded as a problem of bipartite graph matching, where the two sets of vertices respectively comprise words within each sentence x and y, and the weight of a edge between xi and yj is given by the cosine similarity calculated over their word embeddings. Given H identified using the Hungarian algorithm (Kuhn, 1955), HAS is computed by averaging the similarities between embeddings of the aligned pairs of words. 2.1 AES: Additive Embeddings Similarity Given two sentences, x and y, AES betwe"
I17-2019,P13-4014,0,0.0664536,"Missing"
I17-2019,Q14-1018,0,0.0247557,"pleNets-RNN, which does not require such feature engineering, uses embeddings of word N -grams. ˇ SMH (Stajner et al., 2016a) has two types of classifiers: logistic classifier (SMH-IBk/Logistic) and random forest classifier (SMH-RandForest, SMH-RandForest-b). Both are trained relying on the automatic evaluation metrics for MT, such as BLEU, METEOR, and TER, in combination with the QE features for MT (Specia et al., 2013). Instead of reimplementing the above baseline systems, we excerpted their performance scores ˇ from (Stajner et al., 2016b). (6) 2.7 PAS: Paraphrase Alignment Similarity PAS (Sultan et al., 2014, 2015) is computed based on lexical paraphrases. This feature has been proven useful in the semantic textual similarity task of SemEval-2015 (Agirre et al., 2015). PA(x, y) + PA(y, x) (7) |x |+ |y| { |x| ∑ 1 ∃j : xi ⇔ yj ∈ y PA(x, y) = 0 otherwise i=1 PAS(x, y) = where xi ⇔ yj holds if and only if the word pair (xi , yj ) is included in a given paraphrase lexicon. 3 Experiment The usefulness of the above features was evaluated through an empirical experiment using the QATS ˇ dataset (Stajner et al., 2016b). 3.1 Data The QATS dataset consists of 505 and 126 sentence pairs for training and test"
I17-2019,S15-2027,0,0.0429715,"Missing"
I17-2019,W14-1201,0,0.0511045,"Missing"
I17-2019,W16-3411,0,0.0470369,"Missing"
I17-2019,P12-1107,0,0.0809342,"Missing"
I17-2019,Q16-1029,0,0.0200772,"uch as learners (Petersen and Ostendorf, 2007) and children (Belder and Moens, 2010). Such systems would also improve the performance of other natural language processing tasks, such as information extraction (Evans, 2011) and machine ˇ translation (MT) (Stajner and Popovi´c, 2016). Similarly to other text-to-text generation tasks, such as MT and summarization, the outputs of text simplification systems have been evaluated subˇ jectively by humans (Wubben et al., 2012; Stajner et al., 2014) or automatically by comparing with handcrafted reference texts (Specia, 2010; Coster and Kauchak, 2011; Xu et al., 2016). However, the former is costly and not replicable, and the latter has achieved only a low correlation with human evaluation. On the basis of this backdrop, Quality Estimation (QE) (Specia et al., 2010), i.e., automatic evaluation without reference, has been drawing much attention in the research community. In the shared 1 http://qats2016.github.io/shared.html 109 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 109–115, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP spondences using two pre-compiled external resources: (a) word embed"
N18-4015,D15-1075,0,0.185995,"ons trained using three consecutive sentences, such as si−1 , si , and si+1 . It is an encoderdecoder model that encodes sentence si and predicts previous and next sentences si−1 and si+1 from its sentence representation si (Figure 1). As a result of training, this encoder can produce sentence representations. Skip-Thought demonstrates high performance, especially when applied to document classification tasks. Second, InferSent6 (Conneau et al., 2017) constructs a supervised model computing universal sentence representations trained using Stanford Natural Language Inference (SNLI) datasets7 (Bowman et al., 2015). The Natural Language Inference task is a classification task of sentence pairs with three labels, entailment, contradiction and neutral; thus, InferSent can train sentence representations that are sensitive to differences in meaning. This model encodes sentence pairs u and v and generates features by sentence representations u and v with a bi-directional LSTM architecture with max pooling (Figure 2). InferSent demonstrates high performance across various document classification and semantic textual similarity tasks. ing three matching methods to extract relations between t and r (Figure 3)"
N18-4015,D18-2029,0,0.0534779,"Missing"
N18-4015,D17-1070,0,0.0302421,"arity, and we call them universal sentence representations. First, Skip-Thought5 (Kiros et al., 2015) builds an unsupervised model of universal sentence representations trained using three consecutive sentences, such as si−1 , si , and si+1 . It is an encoderdecoder model that encodes sentence si and predicts previous and next sentences si−1 and si+1 from its sentence representation si (Figure 1). As a result of training, this encoder can produce sentence representations. Skip-Thought demonstrates high performance, especially when applied to document classification tasks. Second, InferSent6 (Conneau et al., 2017) constructs a supervised model computing universal sentence representations trained using Stanford Natural Language Inference (SNLI) datasets7 (Bowman et al., 2015). The Natural Language Inference task is a classification task of sentence pairs with three labels, entailment, contradiction and neutral; thus, InferSent can train sentence representations that are sensitive to differences in meaning. This model encodes sentence pairs u and v and generates features by sentence representations u and v with a bi-directional LSTM architecture with max pooling (Figure 2). InferSent demonstrates high"
N18-4015,D15-1124,0,0.236777,"Missing"
N18-4015,W15-3047,0,0.339521,"Missing"
N18-4015,W17-4755,0,0.153277,"n and deployment of a machine translation (MT) system. Various MTE metrics have been proposed in the metrics task of the Workshops on Statistical Machine Translation (WMT) that was started in 2008. However, most MTE metrics are obtained by computing the similarity between an MT hypothesis and a reference translation based on character N-grams or word N-grams, such as SentBLEU (Lin and Och, 2004), which is a smoothed version of BLEU (Papineni et al., 2002), Blend (Ma et al., 2017), MEANT 2.0 (Lo, 2017), and chrF++ (Popovi´c, 2017), which achieved excellent results in the WMT-2017 Metrics task (Bojar et al., 2017). Therefore, they can exploit only limited information for segment-level MTE. In other words, MTE metrics based on character N-grams or word N-grams cannot make full use of sentence representations; they only check for word matches. 2 Related Work DPMFcomb (Yu et al., 2015a) achieved the best performance in the WMT-2016 Metrics task (Bojar et al., 2016). It incorporates 55 default metrics provided by the Asiya MT evaluation toolkit1 (Gim´enez and M`arquez, 2010), as well as three other metrics, namely, DPMF (Yu et al., 2015b), REDp (Yu et al., 2015a), and ENTFp (Yu et al., 2015a), using rankin"
N18-4015,W17-4768,0,0.36281,"c machine translation evaluation (MTE). MTE metrics having a high correlation with human evaluation enable the continuous integration and deployment of a machine translation (MT) system. Various MTE metrics have been proposed in the metrics task of the Workshops on Statistical Machine Translation (WMT) that was started in 2008. However, most MTE metrics are obtained by computing the similarity between an MT hypothesis and a reference translation based on character N-grams or word N-grams, such as SentBLEU (Lin and Och, 2004), which is a smoothed version of BLEU (Papineni et al., 2002), Blend (Ma et al., 2017), MEANT 2.0 (Lo, 2017), and chrF++ (Popovi´c, 2017), which achieved excellent results in the WMT-2017 Metrics task (Bojar et al., 2017). Therefore, they can exploit only limited information for segment-level MTE. In other words, MTE metrics based on character N-grams or word N-grams cannot make full use of sentence representations; they only check for word matches. 2 Related Work DPMFcomb (Yu et al., 2015a) achieved the best performance in the WMT-2016 Metrics task (Bojar et al., 2016). It incorporates 55 default metrics provided by the Asiya MT evaluation toolkit1 (Gim´enez and M`arquez, 2010"
N18-4015,W17-4770,0,0.0416477,"Missing"
N18-4015,W15-3031,0,0.108361,"Missing"
N18-4015,W15-3050,0,0.0950926,"Missing"
N18-4015,W16-2342,0,0.0447906,"trained using largescale data obtained in other tasks. Therefore, the proposed approach avoids the problem of using a small dataset for training sentence representations. After the success of DPMFcomb , Blend2 (Ma et al., 2017) achieved the best performance in the WMT-2017 Metrics task (Bojar et al., 2017). Similar to DPMFcomb , Blend is essentially an SVR (RBF kernel) model that uses the scores of various metrics as features. It incorporates 25 lexical metrics provided by the Asiya MT evaluation toolkit, as well as four other metrics, namely, BEER (Stanojevi´c and Sima’an, 2015), CharacTER (Wang et al., 2016), DPMF and ENTFp. BEER (Stanojevi´c and Sima’an, 2015) is a linear model based on character N-grams and replacement trees. CharacTER (Wang et al., 2016) evaluates an MT hypothesis based on character-level edit distance. DPMFcomb is trained through relative ranking of human evaluation data in terms of relative ranking (RR). The quality of five MT hypotheses of the same source segment are ranked from 1 to 5 via comparison with the reference translation. In contrast, Blend is trained through direct assessment (DA) of human evaluation data. DA provides the absolute quality scores of hypotheses, by"
N18-4015,P02-1040,0,\N,Missing
N18-4015,C04-1072,0,\N,Missing
N18-4015,W16-2302,0,\N,Missing
O13-1007,P02-1028,0,0.131298,"of newspaper article through paraphrasing based on the use of a Japanese dictionary. Fujita et al. [1] and Mino and Tanaka [9] paraphrased the headword of a noun in a dictionary as the headword of another noun by assessing the similarity of the definitions for the two. Yet, as also reported by Mino and Tanaka, the target words acquired by this method are not simpler than the original words. We paraphrase by taking advantage of Japanese dictionary characteristics, namely that “The definition statements are simpler than the headwords” [9], because our aim is lexical simplification. Kaji et al. [3] assumed that the definition statement has an inflectable word as a nominative if the headword is inflectable, and the nominative is placed at the end of the definition statement. Then, they proposed a method for paraphrasing inflectable words. Mino and Tanaka assumed that the last segment of the main sentence in the definition statement represents the meaning of the headword, and they proposed a method for paraphrasing nouns. Kajiwara and Yamamoto [4] assumed that the target word is the same part-of-speech as headword and is placed at the end of the definition statement. They proposed a metho"
O13-1007,W02-1030,0,0.0921216,"Missing"
O13-1007,P01-1046,0,0.0180239,"phrased as the end portion of the definition statement 60 Proceedings of the Twenty-Fifth Conference on Computational Linguistics and Speech Processing (ROCLING 2013) Multiple target word candidates can be acquired by making use of the entire definition statement. Therefore, a process is needed for selecting the most appropriate target words. In the study of the selection of target words, researchers employ various methods such as assessing semantic similarity based on data from a thesaurus [7] or using the statistical information from large resources based on the distributional hypothesis [3][6]. Thesauruses provide hierarchical semantic classifications of words. By measuring the semantic distance between words in the thesaurus, it is possible to measure the proximity of meaning between words. Furthermore, according to the distributional hypothesis [2], words with similar meanings are often used in similar contexts. Based on this hypothesis, Lapata et al. and Keller et al. reported that the plausibility determination of the expression can be achieved by utilizing co-occurrence frequency and n-gram. In this paper, in order to maintain as much of the original meaning as possible in the"
O13-1007,W10-1308,0,0.163054,"inition：【大詰め】芝居の最 後 の場面 paraphrase：大 詰 め の大一番 → 最 後 の大一番 Figure 1: Example of a word that cannot be paraphrased as the end portion of the definition statement 60 Proceedings of the Twenty-Fifth Conference on Computational Linguistics and Speech Processing (ROCLING 2013) Multiple target word candidates can be acquired by making use of the entire definition statement. Therefore, a process is needed for selecting the most appropriate target words. In the study of the selection of target words, researchers employ various methods such as assessing semantic similarity based on data from a thesaurus [7] or using the statistical information from large resources based on the distributional hypothesis [3][6]. Thesauruses provide hierarchical semantic classifications of words. By measuring the semantic distance between words in the thesaurus, it is possible to measure the proximity of meaning between words. Furthermore, according to the distributional hypothesis [2], words with similar meanings are often used in similar contexts. Based on this hypothesis, Lapata et al. and Keller et al. reported that the plausibility determination of the expression can be achieved by utilizing co-occurrence freq"
O13-1007,W12-5811,0,0.0813293,"cquires the target word from the end of the definition statements. Keywords: Lexical Simplification, Lexical Paraphrase. 1. Introduction In the current information age, a various readers have easy access to diverse text data. To achieve information transmission and gathering effectively, we must address the gap in readers’ linguistic skills. The gap of linguistic skills results from differences in age, such as between children and adults, as well as from differences in expert knowledge. In the effort to bridge this gap, and also to facilitate better communication with foreign language speakers[8] and people with disabilities, technology can play an important role. To investigate how technology can be applied toward bridging the gap in readers’ linguistic skills, we simplify the text of newspaper articles containing words that pose difficulties in communication, especially for elementary school students. Children are still developing their language skills, and as such, they have smaller vocabularies than adults. In this paper, we perform text simplification for children by paraphrasing selected newspaper articles using only words found in Basic Vocabulary to Learn (BVL)(3). BVL is a co"
O13-1007,W02-1411,1,0.712847,"(BVL)(3). BVL is a collection of words selected based on a lexical analysis of elementary school textbooks. It contains 5,404 words that can help children write expressively. We define words not included in BVL as Difficult Words (DWs) and those in BVL paraphrased from DW as Simple Words (SWs). Paraphrasing newspaper articles using words that children can understand makes a great contribution to reading assistance for young students. 59 Proceedings of the Twenty-Fifth Conference on Computational Linguistics and Speech Processing (ROCLING 2013) 2. Related Works Although there are some methods [10] proposed for automatically acquiring paraphrasable expressions from Web pages, the quality of the results are still unsatisfactory. Hence typical methods use thesauri or dictionaries. Thesaurus is a language resource that contains semantically classified vocabulary words. Methods that utilize a thesaurus have an advantage in that they can measure the semantic relatedness between words (i.e., the distance between meanings). Japanese dictionaries are another language resource that provides the definition of a given lemma. Methods that utilize a dictionary have an advantage in that they are able"
P15-3006,S07-1009,0,0.385377,"n system that supports reading comprehension of a wide range of readers, including children and language learners. The other is a dataset for evaluation that enables open discussions with other systems. Both the system and the dataset are made available providing the first such resources for the Japanese language. 1 http://www.jnlp.org/SNOW 2 Previous Work Two datasets for evaluation of English lexical simplification have been published. Both were constructed by transforming a lexical substitution dataset, which was constructed in an English lexical substitution task of SemEval-2007 workshop (McCarthy and Navigli, 2007). Introduction Lexical simplification is a technique that substitutes a complex word or phrase in a sentence with a simpler synonym. This technique supports the reading comprehension of a wide range of readers, including children (Belder and Moens, 2010; Kajiwara et al., 2013) and language learners (Eom et al., 2012; Moku et al., 2012). The recent years have seen a great activity in this field of inquiry, especially for English: At the SemEval-2012 workshop, many systems were participating in the English lexical simplification task (Specia et al., 2012), for which also an evaluation dataset wa"
P15-3006,W12-5811,0,0.126157,"revious Work Two datasets for evaluation of English lexical simplification have been published. Both were constructed by transforming a lexical substitution dataset, which was constructed in an English lexical substitution task of SemEval-2007 workshop (McCarthy and Navigli, 2007). Introduction Lexical simplification is a technique that substitutes a complex word or phrase in a sentence with a simpler synonym. This technique supports the reading comprehension of a wide range of readers, including children (Belder and Moens, 2010; Kajiwara et al., 2013) and language learners (Eom et al., 2012; Moku et al., 2012). The recent years have seen a great activity in this field of inquiry, especially for English: At the SemEval-2012 workshop, many systems were participating in the English lexical simplification task (Specia et al., 2012), for which also an evaluation dataset was constructed. Other resources for statistical learning of simplified rules were built, drawing on the Simple English Wikipedia (Zhu et al., 2010; Horn et al., 2014), e.g. several parallel corpora aligning standard and simple English (Zhu et al., 2010; Kauchak, 2013)1,2 and evaluation datasets (Specia et al., 2012; Belder and Moens, 20"
P15-3006,W12-2038,0,0.0232923,".jnlp.org/SNOW 2 Previous Work Two datasets for evaluation of English lexical simplification have been published. Both were constructed by transforming a lexical substitution dataset, which was constructed in an English lexical substitution task of SemEval-2007 workshop (McCarthy and Navigli, 2007). Introduction Lexical simplification is a technique that substitutes a complex word or phrase in a sentence with a simpler synonym. This technique supports the reading comprehension of a wide range of readers, including children (Belder and Moens, 2010; Kajiwara et al., 2013) and language learners (Eom et al., 2012; Moku et al., 2012). The recent years have seen a great activity in this field of inquiry, especially for English: At the SemEval-2012 workshop, many systems were participating in the English lexical simplification task (Specia et al., 2012), for which also an evaluation dataset was constructed. Other resources for statistical learning of simplified rules were built, drawing on the Simple English Wikipedia (Zhu et al., 2010; Horn et al., 2014), e.g. several parallel corpora aligning standard and simple English (Zhu et al., 2010; Kauchak, 2013)1,2 and evaluation datasets (Specia et al., 2012;"
P15-3006,P14-2075,0,0.095193,"supports the reading comprehension of a wide range of readers, including children (Belder and Moens, 2010; Kajiwara et al., 2013) and language learners (Eom et al., 2012; Moku et al., 2012). The recent years have seen a great activity in this field of inquiry, especially for English: At the SemEval-2012 workshop, many systems were participating in the English lexical simplification task (Specia et al., 2012), for which also an evaluation dataset was constructed. Other resources for statistical learning of simplified rules were built, drawing on the Simple English Wikipedia (Zhu et al., 2010; Horn et al., 2014), e.g. several parallel corpora aligning standard and simple English (Zhu et al., 2010; Kauchak, 2013)1,2 and evaluation datasets (Specia et al., 2012; Belder and Moens, 2012)3,4 . On the other hand, there have been no published resources on Japanese lexical simplification so far. 2.1 McCarthy Substitution Dataset The English lexical substitution task of SemEval2007 requires that the system finds words or phrases that one can substitute for the given target word in the given content. These target words are content words, and their details are shown in Table 1. These contexts are selected from"
P15-3006,S12-1046,0,0.0417308,"on task of SemEval-2007 workshop (McCarthy and Navigli, 2007). Introduction Lexical simplification is a technique that substitutes a complex word or phrase in a sentence with a simpler synonym. This technique supports the reading comprehension of a wide range of readers, including children (Belder and Moens, 2010; Kajiwara et al., 2013) and language learners (Eom et al., 2012; Moku et al., 2012). The recent years have seen a great activity in this field of inquiry, especially for English: At the SemEval-2012 workshop, many systems were participating in the English lexical simplification task (Specia et al., 2012), for which also an evaluation dataset was constructed. Other resources for statistical learning of simplified rules were built, drawing on the Simple English Wikipedia (Zhu et al., 2010; Horn et al., 2014), e.g. several parallel corpora aligning standard and simple English (Zhu et al., 2010; Kauchak, 2013)1,2 and evaluation datasets (Specia et al., 2012; Belder and Moens, 2012)3,4 . On the other hand, there have been no published resources on Japanese lexical simplification so far. 2.1 McCarthy Substitution Dataset The English lexical substitution task of SemEval2007 requires that the system"
P15-3006,C10-1152,0,0.0797595,"ym. This technique supports the reading comprehension of a wide range of readers, including children (Belder and Moens, 2010; Kajiwara et al., 2013) and language learners (Eom et al., 2012; Moku et al., 2012). The recent years have seen a great activity in this field of inquiry, especially for English: At the SemEval-2012 workshop, many systems were participating in the English lexical simplification task (Specia et al., 2012), for which also an evaluation dataset was constructed. Other resources for statistical learning of simplified rules were built, drawing on the Simple English Wikipedia (Zhu et al., 2010; Horn et al., 2014), e.g. several parallel corpora aligning standard and simple English (Zhu et al., 2010; Kauchak, 2013)1,2 and evaluation datasets (Specia et al., 2012; Belder and Moens, 2012)3,4 . On the other hand, there have been no published resources on Japanese lexical simplification so far. 2.1 McCarthy Substitution Dataset The English lexical substitution task of SemEval2007 requires that the system finds words or phrases that one can substitute for the given target word in the given content. These target words are content words, and their details are shown in Table 1. These context"
P15-3006,O13-1007,1,0.853157,"ources for the Japanese language. 1 http://www.jnlp.org/SNOW 2 Previous Work Two datasets for evaluation of English lexical simplification have been published. Both were constructed by transforming a lexical substitution dataset, which was constructed in an English lexical substitution task of SemEval-2007 workshop (McCarthy and Navigli, 2007). Introduction Lexical simplification is a technique that substitutes a complex word or phrase in a sentence with a simpler synonym. This technique supports the reading comprehension of a wide range of readers, including children (Belder and Moens, 2010; Kajiwara et al., 2013) and language learners (Eom et al., 2012; Moku et al., 2012). The recent years have seen a great activity in this field of inquiry, especially for English: At the SemEval-2012 workshop, many systems were participating in the English lexical simplification task (Specia et al., 2012), for which also an evaluation dataset was constructed. Other resources for statistical learning of simplified rules were built, drawing on the Simple English Wikipedia (Zhu et al., 2010; Horn et al., 2014), e.g. several parallel corpora aligning standard and simple English (Zhu et al., 2010; Kauchak, 2013)1,2 and ev"
P15-3006,P13-1151,0,0.0458915,"; Kajiwara et al., 2013) and language learners (Eom et al., 2012; Moku et al., 2012). The recent years have seen a great activity in this field of inquiry, especially for English: At the SemEval-2012 workshop, many systems were participating in the English lexical simplification task (Specia et al., 2012), for which also an evaluation dataset was constructed. Other resources for statistical learning of simplified rules were built, drawing on the Simple English Wikipedia (Zhu et al., 2010; Horn et al., 2014), e.g. several parallel corpora aligning standard and simple English (Zhu et al., 2010; Kauchak, 2013)1,2 and evaluation datasets (Specia et al., 2012; Belder and Moens, 2012)3,4 . On the other hand, there have been no published resources on Japanese lexical simplification so far. 2.1 McCarthy Substitution Dataset The English lexical substitution task of SemEval2007 requires that the system finds words or phrases that one can substitute for the given target word in the given content. These target words are content words, and their details are shown in Table 1. These contexts are selected from the English Internet Corpus, which is a balanced and web-based corpus of English (Sharoff, 2006). This"
P15-3006,N06-1023,0,0.0111353,"ist of sets of the form {predicate, relation, argument}, where the candidate substitutions are used instead of the complex word (so there will be as many of these sets as there are candidate substitutions). These new sets are checked against the Kyoto University Case Frame18 . If the set is found there, the candidate substitution counts as a legitimate substitution; if the set is not found, the candidate substitution is not counted as a legitimate substitution. Kyoto University Case Frame is the list of predicate and argument pairs that have a case relationship, and it is built automatically (Kawahara and Kurohashi, 2006) from Web texts. 5.4 Adjective 3 Table 4: POS of the simplified target words Japanese WordNet Synonyms Database15 , Verb Entailment Database16 , and Case Base for Basic Semantic Relations16 , following previous research (Kajiwara and Yamamoto, 2014). 5.3 Verb 65 • It is {distributed –> dealt} to a {caller –> visitor} from foreign countries. • {Principal –> President} Takagi of the bank presented an idea. 6 Final Remarks We built a Japanese lexical simplification system and a dataset for evaluation of Japanese lexical simplification. Subsequently, we have published these resources on the Web. T"
P15-3006,W04-3230,0,\N,Missing
P15-3006,P11-1081,0,\N,Missing
P16-3001,I13-1110,0,0.0154535,"d which size is 21,700 ((2010 + 2330) × 5) rankings. Then, we calculate correlation between the accuracies of annotated data and either those of Kajiwara and Yamamoto (2015) or those of our dataset. 5.2.1 Lexical simplification systems We used several metrics for these experiments: Frequency Because it is said that a high frequent word is simple, most frequent word is selected as a simplification candidate from substitutes using uni-gram frequency of Japanese Web N-gram (Kudo and Kazawa, 2007). This uni-gram frequency is counted from two billion sentences in Japanese Web text. Number of Users Aramaki et al. (2013) claimed that a word used by many people is simple, so we pick the word used by the most of users. Number of Users were estimated from the Twitter corpus created by Aramaki et al. (2013). The corpus contains 250 million tweets from 100,000 users. 5.2 Extrinsic evaluation In this section, we evaluate our dataset using five simple lexical simplification methods. We calcu5 References Familiarity Assuming that a word which is known by many people is simple, replace a target word with substitutes according to the familiarity score using familiarity data constructed by Amano and Kondo (2000). The fa"
P16-3001,P14-2075,0,0.102896,"Missing"
P16-3001,P15-3006,1,0.653284,"Missing"
P16-3001,S07-1009,0,0.0312672,"ion for Computational Linguistics – Student Research Workshop, pages 1–7, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics sentence paraphrase list 「技を出し合い、気分が高揚するのがたまらない」とはいえ、技量で相手を上回りたい気持ちも強い。 Although using their techniques makes you feel exalted, I strongly feel I want to outrank my competitors in terms of skill. 盛り上がる 高まる 高ぶる 上がる 高揚する 興奮する 熱を帯びる 活性化する come alive raised, excited up exalted excited heated revitalized Figure 1: A part of the dataset of Kajiwara and Yamamoto (2015). notated on top of the evaluation dataset for English lexical substitution (McCarthy and Navigli, 2007). They asked university students to rerank substitutes according to simplification ranking. Sentences in their dataset do not always contain complex words, and it is not appropriate to evaluate simplification systems if a test sentence does not include any complex words. In addition, De Belder and Moens (2012) built an evaluation dataset for English lexical simplification based on that developed by McCarthy and Navigli (2007). They used Amazon’s Mechanical Turk to rank substitutes and employed the reliability of annotators to remove outlier annotators and/or downweight unreliable annotators. T"
P16-3001,S12-1046,0,0.19961,"vious datasets for evaluating systems with respect to correlation with human judgment. Introduction Lexical simplification is the task to find and substitute a complex word or phrase in a sentence with its simpler synonymous expression. We define complex word as a word that has lexical and subjective difficulty in a sentence. It can help in reading comprehension for children and language learners (De Belder and Moens, 2010). This task is a rather easier task which prepare a pair of complex and simple representations than a challenging task which changes the substitute pair in a given context (Specia et al., 2012; Kajiwara and Yamamoto, 2015). Construction of a benchmark dataset is important to ensure the reliability and reproducibility of evaluation. However, few resources are available for the automatic evaluation of lexical simplification. Specia et al. (2012) and De Belder and Moens (2010) created benchmark datasets for evaluating English lexical simplifica• The consistency of simplification ranking is greatly improved by allowing candidates to have ties and by considering the reliability of annotators. Our dataset is available at GitHub2 . 2 Related work The evaluation dataset for the English Lex"
P17-3007,D15-1075,0,0.0151488,"hod that inexpensively generates translations using machine translation and Quality Estimation. Ganitkevitch et al. (2013) and Pavlick et al. (2015) also use a bilingual parallel corpora to build a paraphrase database using bilingual pivoting (Bannard and Callison-Burch, 2005). Their methods differ from ours in that they aim to acquire phrase level paraphrase rules and carry out word alignment instead of machine translation. There are also many studies on building a large scale corpora utilizing crowdsourcing in related tasks such as Recognizing Textual Entailment (RTE) (Marelli et al., 2014; Bowman et al., 2015) and Lexical Simplification (De Belder and Moens, 2012; Xu et al., 2016). Moreover, there are studies collecting paraphrases from captions to videos (Chen and Dolan, 2011) and images (Chen et al., 2015). One advantage of leveraging crowdsourcing is that annotation is done inexpensively, but it requires careful task design to gather valid data from non-expert annotators. In our study, we collect sentential paraphrase pairs, but we presume that it is difficult for nonexpert annotators to provide well-balanced sentential paraphrase pairs, unlike lexical simplification, which only replaces content"
P17-3007,N15-1053,0,0.015866,"rase database using bilingual pivoting (Bannard and Callison-Burch, 2005). Their methods differ from ours in that they aim to acquire phrase level paraphrase rules and carry out word alignment instead of machine translation. There are also many studies on building a large scale corpora utilizing crowdsourcing in related tasks such as Recognizing Textual Entailment (RTE) (Marelli et al., 2014; Bowman et al., 2015) and Lexical Simplification (De Belder and Moens, 2012; Xu et al., 2016). Moreover, there are studies collecting paraphrases from captions to videos (Chen and Dolan, 2011) and images (Chen et al., 2015). One advantage of leveraging crowdsourcing is that annotation is done inexpensively, but it requires careful task design to gather valid data from non-expert annotators. In our study, we collect sentential paraphrase pairs, but we presume that it is difficult for nonexpert annotators to provide well-balanced sentential paraphrase pairs, unlike lexical simplification, which only replaces content words. For this reason, annotators classify paraphrase candidate pairs in our study similar to the method used in the TPC and previous studies on RTE. • Generated paraphrases using multiple machine tra"
P17-3007,P05-1074,0,0.357992,"bilingual parallel corpus are similar to our method. In fact, our method is an extension of previous studies that acquire paraphrases using manual translations of the same documents (Barzilay and McKeown, 2001; Pang et al., 2003). However, it is expensive to manually translate sentences to create large numbers of translation pairs. Thus, we propose a method that inexpensively generates translations using machine translation and Quality Estimation. Ganitkevitch et al. (2013) and Pavlick et al. (2015) also use a bilingual parallel corpora to build a paraphrase database using bilingual pivoting (Bannard and Callison-Burch, 2005). Their methods differ from ours in that they aim to acquire phrase level paraphrase rules and carry out word alignment instead of machine translation. There are also many studies on building a large scale corpora utilizing crowdsourcing in related tasks such as Recognizing Textual Entailment (RTE) (Marelli et al., 2014; Bowman et al., 2015) and Lexical Simplification (De Belder and Moens, 2012; Xu et al., 2016). Moreover, there are studies collecting paraphrases from captions to videos (Chen and Dolan, 2011) and images (Chen et al., 2015). One advantage of leveraging crowdsourcing is that ann"
P17-3007,P01-1008,0,0.0991419,". To actively acquire negative instances, we use Wikipedia to randomly extract sentences. In general, it is rare for sentences to become paraphrase when sentence pairs are collected randomly, so it is effective to acquire negative instances in this regard. Our contributions are summarized as follows: candidates from any sentences, and this allows us to choose any domain required by an application. Methods using a bilingual parallel corpus are similar to our method. In fact, our method is an extension of previous studies that acquire paraphrases using manual translations of the same documents (Barzilay and McKeown, 2001; Pang et al., 2003). However, it is expensive to manually translate sentences to create large numbers of translation pairs. Thus, we propose a method that inexpensively generates translations using machine translation and Quality Estimation. Ganitkevitch et al. (2013) and Pavlick et al. (2015) also use a bilingual parallel corpora to build a paraphrase database using bilingual pivoting (Bannard and Callison-Burch, 2005). Their methods differ from ours in that they aim to acquire phrase level paraphrase rules and carry out word alignment instead of machine translation. There are also many stud"
P17-3007,I05-5002,0,0.448344,"Missing"
P17-3007,D16-1025,0,0.0138668,"judge whether the candidate pairs are paraphrases. In this paper, we focus on the Japanese PI task and build a monolingual parallel corpus for its evaluation as there is no Japanese sentential paraphrase corpus available. As Figure 1 shows, we use phrase-based machine translation (PBMT) and neural machine translation (NMT) to generate two different Japanese sentences from one English sentence. We expect the two systems provide widely different translations with regard to surface form such as lexical variation and word order difference because they are known to have different characteristics (Bentivogli et al., 2016); for instance, PBMT produces more literal translations, whereas NMT produces more fluent translations. We believe that when the translation succeeds, the two Japanese sentences have the same meaning but different expressions, which is a positive instance. On the other hand, translated candidates can be negative instances when they include fluent mistranslations. This occurs since adequacy is not checked during an annotation phase. Thus, we can also acquire some negative instances in this manner. To actively acquire negative instances, we use Wikipedia to randomly extract sentences. In general"
P17-3007,P13-1158,0,0.0397279,"andidate sentence pairs using multiple machine translation systems. In the random extraction part, we extract candidate sentence pairs from a monolingual corpus. To collect both trivial and non-trivial instances, we sample candidate pairs Introduction When two sentences share the same meaning but are written using different expressions, they are deemed to be a sentential paraphrase pair. Paraphrase Identification (PI) is a task that recognizes whether a pair of sentences is a paraphrase. PI is useful in many applications such as information retrieval (Wang et al., 2013) or question answering (Fader et al., 2013). Despite this usefulness, there are only a few corpora that can be used to develop and evaluate PI systems. Moreover, such corpora are unavailable in many languages other than English. This is because manual paraphrase generation tends to cost a lot. Furthermore, unlike a bilingual parallel corpus for machine translation, a monolingual parallel corpus for PI cannot be spontaneously built. Even though some paraphrase corpora are available, there are some limitations on them. For example, the Microsoft Research Paraphrase Corpus 1 Non-trivial positive instances are difficult to identify as sema"
P17-3007,N13-1092,0,0.158714,"Missing"
P17-3007,P15-2070,0,0.0502663,"Missing"
P17-3007,P11-1109,0,0.0619616,"Missing"
P17-3007,rus-etal-2014-paraphrase,0,0.0305828,"use multiple machine translation systems to generate positive candidates and a monolingual corpus to extract negative candidates. To collect nontrivial instances, the candidates are uniformly sampled by word overlap rate. Finally, annotators judge whether the candidates are either positive or negative. Using this method, we built and released the first evaluation corpus for Japanese paraphrase identification, which comprises 655 sentence pairs. 1 Figure 1: Overview of candidate pair generation. (MSRP) (Dolan and Brockett, 2005) is a standardized corpus in English for the PI task. However, as Rus et al. (2014) pointed out, MSRP collects candidate pairs using short edit distance, but this approach is limited to collecting positive instances with a low word overlap rate (WOR) (non-trivial positive instances, hereafter)1 . In contrast, the Twitter Paraphrase Corpus (TPC) (Xu et al., 2014) comprises short noisy user-generated texts; hence, it is difficult to acquire negative instances with a high WOR (non-trivial negative instances, hereafter)2 . To develop a more robust PI model, it is important to collect both “non-trivial” positive and negative instances for the evaluation corpus. To create a useful"
P17-3007,W11-2123,0,0.0112219,"anding of sentences and that there are some ambiguous instances without context (e.g., good child and good kid), the score is considered to be sufficiently high. There were 89 disagreements, and the final label was decided by discussion. As a result, we acquired 363 positive and 102 negative machinetranslated pairs. We built the first evaluation corpus for Japanese PI using our method. We used Google Translate PBMT4 and NMT5 (Wu et al., 2016) to translate English sentences extracted from English Wikipedia 6 into Japanese sentences7 . We calculated the language model probabilities using KenLM (Heafield, 2011), and built a 5-gram language model from the English Gigaword Fifth Edition (LDC2011T07). Then we translated the top 500,000 sentences and sampled 200 pairs in the descending order of machine translation output quality for each range, except for the exact match pairs (Table 1). Although the machine translation part of our method successfully collected non-trivial positive instances, it acquired only a few non-trivial negative instances as we expected. To fill the gap between positive and negative in higher WOR, we randomly collected sentence pairs from Japanese Wikipedia8 and added 190 non-tri"
P17-3007,P15-3006,1,0.762812,"Missing"
P17-3007,P13-2008,0,0.0198718,"he machine translation part, we generate candidate sentence pairs using multiple machine translation systems. In the random extraction part, we extract candidate sentence pairs from a monolingual corpus. To collect both trivial and non-trivial instances, we sample candidate pairs Introduction When two sentences share the same meaning but are written using different expressions, they are deemed to be a sentential paraphrase pair. Paraphrase Identification (PI) is a task that recognizes whether a pair of sentences is a paraphrase. PI is useful in many applications such as information retrieval (Wang et al., 2013) or question answering (Fader et al., 2013). Despite this usefulness, there are only a few corpora that can be used to develop and evaluate PI systems. Moreover, such corpora are unavailable in many languages other than English. This is because manual paraphrase generation tends to cost a lot. Furthermore, unlike a bilingual parallel corpus for machine translation, a monolingual parallel corpus for PI cannot be spontaneously built. Even though some paraphrase corpora are available, there are some limitations on them. For example, the Microsoft Research Paraphrase Corpus 1 Non-trivial positive"
P17-3007,P16-3001,1,0.811346,"tent words. For this reason, annotators classify paraphrase candidate pairs in our study similar to the method used in the TPC and previous studies on RTE. • Generated paraphrases using multiple machine translation systems for the first time • Adjusted for a balance from two viewpoints: positive/negative and trivial/non-trivial • Released3 the first evaluation corpus for the Japanese PI task 2 Related Work As for Japanese, there exists a paraphrase database (Mizukami et al., 2014) and an evaluation dataset that includes some paraphrases for lexical simplification (Kajiwara and Yamamoto, 2015; Kodaira et al., 2016). They provide either lexical or phrase-level paraphrases, but we focus on collecting sentence-level paraphrases for PI evaluation. There is also an evaluation dataset for RTE (Watanabe et al., 2013) containing 70 sentential paraphrase pairs; however, as there is a limitation in terms of size, we aim to build a larger corpus. Paraphrase acquisition has been actively studied. For instance, paraphrases have been acquired from monolingual comparable corpora such as news articles regarding the same event (Shinyama et al., 2002) and multiple definitions of the same concept (Hashimoto et al., 2011)."
P17-3007,W16-2385,0,0.0156499,"on Systems 3.2 Non-Paraphrase Extraction from a Monolingual Corpus We use different types of machine translation systems (PBMT and NMT) to translate source sentences extracted from a monolingual corpus into a target language. This means that each source sentence has two versions in the target language, and we use the sentences as a pair. To avoid collecting ungrammatical sentences as much as possible, we use Quality Estimation and eliminate inappropriate sentences for paraphrase candidate pairs. At WMT2016 (Bojar et al., 2016) in the Shared Task on Quality Estimation, the winning system YSDA (Kozlova et al., 2016) shows that it is effective for Quality Estimation to employ language model probabilities of source and target sentences, and BLEU scores between the source sentence and back-translation. Therefore, we calculate the language model probabilities of source sentences and translate them in the order of their probabilities. To further obtain better translations, we select sentence pairs in the descending order of machine translation output quality, which is defined as follows: QEi = SBLEU(ei , BTPBMT (ei )) × SBLEU(ei , BTNMT (ei )) This extraction part of our method is for acquiring non-trivial ne"
P17-3007,marelli-etal-2014-sick,0,0.0321766,"Thus, we propose a method that inexpensively generates translations using machine translation and Quality Estimation. Ganitkevitch et al. (2013) and Pavlick et al. (2015) also use a bilingual parallel corpora to build a paraphrase database using bilingual pivoting (Bannard and Callison-Burch, 2005). Their methods differ from ours in that they aim to acquire phrase level paraphrase rules and carry out word alignment instead of machine translation. There are also many studies on building a large scale corpora utilizing crowdsourcing in related tasks such as Recognizing Textual Entailment (RTE) (Marelli et al., 2014; Bowman et al., 2015) and Lexical Simplification (De Belder and Moens, 2012; Xu et al., 2016). Moreover, there are studies collecting paraphrases from captions to videos (Chen and Dolan, 2011) and images (Chen et al., 2015). One advantage of leveraging crowdsourcing is that annotation is done inexpensively, but it requires careful task design to gather valid data from non-expert annotators. In our study, we collect sentential paraphrase pairs, but we presume that it is difficult for nonexpert annotators to provide well-balanced sentential paraphrase pairs, unlike lexical simplification, which"
P17-3007,Q16-1029,0,0.0254615,"Quality Estimation. Ganitkevitch et al. (2013) and Pavlick et al. (2015) also use a bilingual parallel corpora to build a paraphrase database using bilingual pivoting (Bannard and Callison-Burch, 2005). Their methods differ from ours in that they aim to acquire phrase level paraphrase rules and carry out word alignment instead of machine translation. There are also many studies on building a large scale corpora utilizing crowdsourcing in related tasks such as Recognizing Textual Entailment (RTE) (Marelli et al., 2014; Bowman et al., 2015) and Lexical Simplification (De Belder and Moens, 2012; Xu et al., 2016). Moreover, there are studies collecting paraphrases from captions to videos (Chen and Dolan, 2011) and images (Chen et al., 2015). One advantage of leveraging crowdsourcing is that annotation is done inexpensively, but it requires careful task design to gather valid data from non-expert annotators. In our study, we collect sentential paraphrase pairs, but we presume that it is difficult for nonexpert annotators to provide well-balanced sentential paraphrase pairs, unlike lexical simplification, which only replaces content words. For this reason, annotators classify paraphrase candidate pairs"
P17-3007,C12-1121,0,0.0238424,"ans the negative instances are not distinguishable, so this does not affect the balance of the corpus. 3.3 Balanced Sampling using Word Overlap Rate To collect both trivial and non-trivial instances, we carefully sample candidate pairs. We classify the pairs into eleven ranges depending on the WOR and sample pairs uniformly for each range, except for the exact match pairs. The WOR is calculated as follows: (1) Here, ei denotes the i-th source sentence, BTPBMT denotes the back-translation using PBMT, BTNMT denotes the back-translation using NMT, and SBLEU denotes the sentence-level BLEU score (Nakov et al., 2012). When this score is high, it indicates that the difference in sentence 38 Jaccard(TPBMT (ei ), TNMT (ei )) TPBMT (ei ) ∩ TNMT (ei ) (2) = TPBMT (ei ) ∪ TNMT (ei ) Label Positive Negative Unnatural Other Example Input: PBMT: NMT: Input: PBMT: NMT: Input: PBMT: NMT: Input: PBMT: NMT: My father was a very strong man. 私の父は非常に強い男でした。 父はとても強い男だった。 It is available as a generic medication. これは、一般的な薬として利用可能です。 ジェネリック医薬品として入手できます。 I want to wake up in the morning 私は午前中に目を覚ますしたいです* 私は朝起きたい Academy of Country Music Awards : アカデミーオブカントリーミュージックアワード： アカデミー・オブ・カントリー・ミュージック賞： My father was a very strong man."
P17-3007,Q14-1034,0,0.0214984,"positive or negative. Using this method, we built and released the first evaluation corpus for Japanese paraphrase identification, which comprises 655 sentence pairs. 1 Figure 1: Overview of candidate pair generation. (MSRP) (Dolan and Brockett, 2005) is a standardized corpus in English for the PI task. However, as Rus et al. (2014) pointed out, MSRP collects candidate pairs using short edit distance, but this approach is limited to collecting positive instances with a low word overlap rate (WOR) (non-trivial positive instances, hereafter)1 . In contrast, the Twitter Paraphrase Corpus (TPC) (Xu et al., 2014) comprises short noisy user-generated texts; hence, it is difficult to acquire negative instances with a high WOR (non-trivial negative instances, hereafter)2 . To develop a more robust PI model, it is important to collect both “non-trivial” positive and negative instances for the evaluation corpus. To create a useful evaluation corpus, we propose a novel paraphrase acquisition method that has two viewpoints of balancing the corpus: positive/negative and trivial/non-trivial. To balance between positive and negative, our method has a machine translation part collecting mainly positive instances"
P17-3007,N03-1024,0,0.561922,"Missing"
P17-3007,P11-1020,0,\N,Missing
P19-1607,D17-1098,0,0.0189812,"eneration. We evaluate the performance of the proposed method on two major paraphrase generation tasks. We conduct experiments on text simplification and formality transfer using datasets shown in Table 1. For text simplification, we identify complex words in the input sentence and generate simple paraphrase sentence without using these complex words. Similarly, for formality transfer, we identify informal words in the input sentence and generate formal paraphrase sentence without using these informal words. 2.2 Negative Lexically Constrained Decoding 3.1 Setup Lexically constrained decoding (Anderson et al., 2017; Hokamp and Liu, 2017; Post and Vilar, 2018) adds constraints to the beam search to force the output text to include certain words. The effectiveness of these methods are demonstrated in image captioning using given image tags (Anderson et al., 2017) and in the post-editing of machine translation (Hokamp and Liu, 2017). In paraphrase generation, there is no situation that words to be included in the output sentence are given. Therefore, positive lexical constraints used in the image captioning and post-editing of machine translation cannot be applied to this task For text simplification, we u"
P19-1607,P11-2117,0,0.032505,"tion tasks include subtasks such as text simplification to control complexity, formality transfer to control formality, grammatical error correction to control fluency, and sentence compression to control sentence length. These paraphrase generation applications not only support communication and language learning but also contribute to the performance improvement of other natural language processing applications (Evans, 2011; ˇ Stajner and Popovi´c, 2016). Paraphrase generation can be considered as a monolingual machine translation problem. Sentential paraphrases with different complexities (Coster and Kauchak, 2011; Xu et al., 2015) and formalities (Rao and Tetreault, 2018) were created manually, and parallel corpora specialized for each subtask were constructed. As in the field of machine translation, phrasebased (Coster and Kauchak, 2011; Xu et al., 2012) and syntax-based (Zhu et al., 2010; Xu et al., 2016) methods were proposed early. In recent years, the encode-decoder model based on the attention mechanism (Nisioi et al., 2017; Zhang and Lapata, 2017; Jhamtani et al., 2017; Niu et al., 2018) has been studied, inspired by the success of neural machine translation (Bahdanau et al., 2015). In machine"
P19-1607,N13-1092,0,0.119796,"Missing"
P19-1607,E17-3017,0,0.0241759,"asing from an informal style to formal style (Rao and Tetreault, 2018). Therefore, we will only experiment with this setting. For lexical constraints, we identified words with a PMI score above the threshold θ. We selected a threshold θ ∈ {0.0, 0.1, 0.2, ..., 0.7} that maximizes the BLEU score between the output sentence and the reference sentence in the development dataset. We calculated PMI scores using each training dataset shown in Table 1. As a paraphrase generation model, we constructed the recurrent neural network (RNN) and self-attention network (SAN) models using the Sockeye toolkit (Hieber et al., 2017).3 Our RNN model uses a single LSTM with a layer size of 512 for both the encoder and decoder, and MLP attention with a layer size of 512. Our SAN model uses a six-layer transformer with a model size of 512 and a single attention head. We used word embeddings in 512 dimensions tying the source, target, and the output layer’s weight matrix. We added dropout to the embeddings and hidden layers with probability 0.2. In addition, we used layer-normalization and label-smoothing for regularization. We trained using the Adam optimizer (Kingma and Ba, 2014) with a batch size of 4,096 tokens and checkp"
P19-1607,P17-1141,0,0.0565981,"the performance of the proposed method on two major paraphrase generation tasks. We conduct experiments on text simplification and formality transfer using datasets shown in Table 1. For text simplification, we identify complex words in the input sentence and generate simple paraphrase sentence without using these complex words. Similarly, for formality transfer, we identify informal words in the input sentence and generate formal paraphrase sentence without using these informal words. 2.2 Negative Lexically Constrained Decoding 3.1 Setup Lexically constrained decoding (Anderson et al., 2017; Hokamp and Liu, 2017; Post and Vilar, 2018) adds constraints to the beam search to force the output text to include certain words. The effectiveness of these methods are demonstrated in image captioning using given image tags (Anderson et al., 2017) and in the post-editing of machine translation (Hokamp and Liu, 2017). In paraphrase generation, there is no situation that words to be included in the output sentence are given. Therefore, positive lexical constraints used in the image captioning and post-editing of machine translation cannot be applied to this task For text simplification, we used the Newsela datase"
P19-1607,N19-1090,0,0.082193,"Missing"
P19-1607,W17-4902,0,0.0166236,"neration can be considered as a monolingual machine translation problem. Sentential paraphrases with different complexities (Coster and Kauchak, 2011; Xu et al., 2015) and formalities (Rao and Tetreault, 2018) were created manually, and parallel corpora specialized for each subtask were constructed. As in the field of machine translation, phrasebased (Coster and Kauchak, 2011; Xu et al., 2012) and syntax-based (Zhu et al., 2010; Xu et al., 2016) methods were proposed early. In recent years, the encode-decoder model based on the attention mechanism (Nisioi et al., 2017; Zhang and Lapata, 2017; Jhamtani et al., 2017; Niu et al., 2018) has been studied, inspired by the success of neural machine translation (Bahdanau et al., 2015). In machine translation, all words appearing in an input sentence must be rewritten in the target language. However, paraphrase generation does not require rewriting of all words. When some criteria are provided, words not satisfying the criteria in the input sentence are identified and rewritten. For example, the criterion for text simplification is the textual complexity, and rewrites complex words to simpler synonymous words. Owing to the characteristics of the task where only"
P19-1607,P16-1162,0,0.0384654,"Hokamp and Liu, 2017). In paraphrase generation, there is no situation that words to be included in the output sentence are given. Therefore, positive lexical constraints used in the image captioning and post-editing of machine translation cannot be applied to this task For text simplification, we used the Newsela dataset (Xu et al., 2015) split and tokenized with the same settings as the previous study (Zhang and Lapata, 2017). For formality transfer, we used the GYAFC dataset (Rao and Tetreault, 2018) normalized and tokenized using Moses toolkit.1 For each task, we used byte-pair encoding2 (Sennrich et al., 2016) to limit the number of token types to 16, 000. In the GYAFC dataset, it is reported that a correlation exists between manual evaluation We define the vocabulary Vi to be paraphrased using the threshold θ as follows. Vi = {w |w ∈ si ∧ PMI(w, x) ≥ θ} (2) 6048 1 2 https://github.com/moses-smt/mosesdecoder https://github.com/rsennrich/subword-nmt Newsela GYAFC-E&M GYAFC-F&R Add Keep Del BLEU SARI Add Keep Del BLEU Add Keep Del BLEU RNN-Base RNN-PMI RNN-Oracle 1.8 2.8 10.4 60.8 61.1 82.9 22.3 36.5 89.9 24.1 24.7 36.4 17.4 22.8 40.0 31.9 33.5 34.8 90.0 90.0 92.7 57.5 59.9 72.4 71.2 71.7 75.2 32.9 3"
P19-1607,W16-3411,0,0.060736,"Missing"
P19-1607,P17-2014,0,0.0289243,"ˇ Stajner and Popovi´c, 2016). Paraphrase generation can be considered as a monolingual machine translation problem. Sentential paraphrases with different complexities (Coster and Kauchak, 2011; Xu et al., 2015) and formalities (Rao and Tetreault, 2018) were created manually, and parallel corpora specialized for each subtask were constructed. As in the field of machine translation, phrasebased (Coster and Kauchak, 2011; Xu et al., 2012) and syntax-based (Zhu et al., 2010; Xu et al., 2016) methods were proposed early. In recent years, the encode-decoder model based on the attention mechanism (Nisioi et al., 2017; Zhang and Lapata, 2017; Jhamtani et al., 2017; Niu et al., 2018) has been studied, inspired by the success of neural machine translation (Bahdanau et al., 2015). In machine translation, all words appearing in an input sentence must be rewritten in the target language. However, paraphrase generation does not require rewriting of all words. When some criteria are provided, words not satisfying the criteria in the input sentence are identified and rewritten. For example, the criterion for text simplification is the textual complexity, and rewrites complex words to simpler synonymous words. Owin"
P19-1607,P18-1042,0,0.0435603,"these previous studies, we have identified words that are strongly related to a particular style. Furthermore, we used these words to control the neural paraphrase generation model and improved the performance of sentential paraphrase generation. 4.2 Lexically Constrained Paraphrasing Hu et al. (2019b) automatically constructed a large-scale paraphrase corpus5 via lexically constrained machine translation. In a Czech–English bilingual corpus, sentence pairs of a Czech-toEnglish machine translation and an English reference can be regarded as automatically generated sentential paraphrase pairs (Wieting and Gimpel, 2018). They used words in reference sentences as positive or negative constraints and succeeded in generating diverse paraphrases via machine translation. In addition, recent work (Hu et al., 2019a) has used lexically constrained paraphrase generation for data augmentation and improve performance in some NLP applications. Unlike these previous studies, we focused on the paraphrase generation as an application. Furthermore, we have shown that negative lexical constraints consistently improve the performance of paraphrase generation applications such as text simplification and formality transfer. 5 C"
P19-1607,C18-1086,0,0.067247,"Missing"
P19-1607,Q15-1021,0,0.0710828,"s such as text simplification to control complexity, formality transfer to control formality, grammatical error correction to control fluency, and sentence compression to control sentence length. These paraphrase generation applications not only support communication and language learning but also contribute to the performance improvement of other natural language processing applications (Evans, 2011; ˇ Stajner and Popovi´c, 2016). Paraphrase generation can be considered as a monolingual machine translation problem. Sentential paraphrases with different complexities (Coster and Kauchak, 2011; Xu et al., 2015) and formalities (Rao and Tetreault, 2018) were created manually, and parallel corpora specialized for each subtask were constructed. As in the field of machine translation, phrasebased (Coster and Kauchak, 2011; Xu et al., 2012) and syntax-based (Zhu et al., 2010; Xu et al., 2016) methods were proposed early. In recent years, the encode-decoder model based on the attention mechanism (Nisioi et al., 2017; Zhang and Lapata, 2017; Jhamtani et al., 2017; Niu et al., 2018) has been studied, inspired by the success of neural machine translation (Bahdanau et al., 2015). In machine translation, all w"
P19-1607,P02-1040,0,0.104592,"f 512. Our SAN model uses a six-layer transformer with a model size of 512 and a single attention head. We used word embeddings in 512 dimensions tying the source, target, and the output layer’s weight matrix. We added dropout to the embeddings and hidden layers with probability 0.2. In addition, we used layer-normalization and label-smoothing for regularization. We trained using the Adam optimizer (Kingma and Ba, 2014) with a batch size of 4,096 tokens and checkpoint the model every 1,000 updates. The training stopped after five checkpoints without improvement in validation perplexity. BLEU (Papineni et al., 2002) is primarily used for our evaluation metrics; SARI (Xu et al., 2016) is also used for text simplification. For a more detailed comparison of the models, we evaluated the F1 score of the words that are added (Add), kept 3 https://github.com/awslabs/sockeye (Keep), and deleted (Del) by the models.4 Our proposed method is compared with previous methods trained only on the dataset shown in Table 1. For detailed analysis, we chose the methods whose model outputs are published. Among these, Dress-LS (Zhang and Lapata, 2017) and BiFT-Ens (Niu et al., 2018) with the highest BLEU score in each task ar"
P19-1607,P16-2024,0,0.0130035,"e text simplification task, we used a threshold of θ = 0.2. 60 55 50 RNN (E&M) RNN (F&R) SAN (E&M) SAN (F&R) 45 40 0.1 0.2 0.3 0.4 0.5 0.6 0.7 θ Figure 1: Thresholds of PMI and quality of generated paraphrases on the development dataset. 4 Related Work 4.1 Style-Sensitive Paraphrase Acquisition Pavlick and Nenkova (2015) worked on a stylesensitive paraphrase acquisition. They used a large-scale raw corpus in each style to calculate PMI scores for each word or phrase and assigned style scores to paraphrase pairs in the paraphrase database (Ganitkevitch et al., 2013; Pavlick et al., 6050 2015). Pavlick and Callison-Burch (2016) further improved style-sensitive paraphrase acquisition based on supervised learning with additional features such as frequency and word embeddings. In this study, as in these previous studies, we have identified words that are strongly related to a particular style. Furthermore, we used these words to control the neural paraphrase generation model and improved the performance of sentential paraphrase generation. 4.2 Lexically Constrained Paraphrasing Hu et al. (2019b) automatically constructed a large-scale paraphrase corpus5 via lexically constrained machine translation. In a Czech–English"
P19-1607,C12-1177,0,0.0212641,"cations not only support communication and language learning but also contribute to the performance improvement of other natural language processing applications (Evans, 2011; ˇ Stajner and Popovi´c, 2016). Paraphrase generation can be considered as a monolingual machine translation problem. Sentential paraphrases with different complexities (Coster and Kauchak, 2011; Xu et al., 2015) and formalities (Rao and Tetreault, 2018) were created manually, and parallel corpora specialized for each subtask were constructed. As in the field of machine translation, phrasebased (Coster and Kauchak, 2011; Xu et al., 2012) and syntax-based (Zhu et al., 2010; Xu et al., 2016) methods were proposed early. In recent years, the encode-decoder model based on the attention mechanism (Nisioi et al., 2017; Zhang and Lapata, 2017; Jhamtani et al., 2017; Niu et al., 2018) has been studied, inspired by the success of neural machine translation (Bahdanau et al., 2015). In machine translation, all words appearing in an input sentence must be rewritten in the target language. However, paraphrase generation does not require rewriting of all words. When some criteria are provided, words not satisfying the criteria in the input"
P19-1607,N15-1023,0,0.111867,"rained paraphrase generation model. Here, we select sentences not including those words by adding negative lexically constrained decoding to the beam search (Section 2.2). Because our method only changes the beam search, it can be applied to various paraphrase generation models and model retraining is not necessary. 2.1 Identification of Word to be Paraphrased We extract words strongly related to the source style included in the input sentence si as vocabulary Vi to be paraphrased. Point-wise mutual information is used to estimate the relatedness between each word w ∈ si and style z ∈ {x, y} (Pavlick and Nenkova, 2015). Here, x and y are the source style (e.g. informal) and the target style (e.g. formal), respectively. PMI(w, z) = log p(w, z) p(w|z) = log (1) p(w)p(z) p(w) Newsela GYAFC-E&M GYAFC-F&R Train Dev Test 94,208 52,595 51,967 1,129 2,877 2,788 1,077 1,416 1,332 Table 1: Number of sentence pairs for each dataset. as they are. Meanwhile, negative lexical constraints that are forced to not include certain words in output sentence are promising for paraphrase generation. This is because, for example, text simplification is a task of generating sentential paraphrase without using complex words that app"
P19-1607,D17-1062,0,0.340585,"´c, 2016). Paraphrase generation can be considered as a monolingual machine translation problem. Sentential paraphrases with different complexities (Coster and Kauchak, 2011; Xu et al., 2015) and formalities (Rao and Tetreault, 2018) were created manually, and parallel corpora specialized for each subtask were constructed. As in the field of machine translation, phrasebased (Coster and Kauchak, 2011; Xu et al., 2012) and syntax-based (Zhu et al., 2010; Xu et al., 2016) methods were proposed early. In recent years, the encode-decoder model based on the attention mechanism (Nisioi et al., 2017; Zhang and Lapata, 2017; Jhamtani et al., 2017; Niu et al., 2018) has been studied, inspired by the success of neural machine translation (Bahdanau et al., 2015). In machine translation, all words appearing in an input sentence must be rewritten in the target language. However, paraphrase generation does not require rewriting of all words. When some criteria are provided, words not satisfying the criteria in the input sentence are identified and rewritten. For example, the criterion for text simplification is the textual complexity, and rewrites complex words to simpler synonymous words. Owing to the characteristics"
P19-1607,P15-2070,0,0.0307504,"Missing"
P19-1607,C10-1152,0,0.0430488,"ion and language learning but also contribute to the performance improvement of other natural language processing applications (Evans, 2011; ˇ Stajner and Popovi´c, 2016). Paraphrase generation can be considered as a monolingual machine translation problem. Sentential paraphrases with different complexities (Coster and Kauchak, 2011; Xu et al., 2015) and formalities (Rao and Tetreault, 2018) were created manually, and parallel corpora specialized for each subtask were constructed. As in the field of machine translation, phrasebased (Coster and Kauchak, 2011; Xu et al., 2012) and syntax-based (Zhu et al., 2010; Xu et al., 2016) methods were proposed early. In recent years, the encode-decoder model based on the attention mechanism (Nisioi et al., 2017; Zhang and Lapata, 2017; Jhamtani et al., 2017; Niu et al., 2018) has been studied, inspired by the success of neural machine translation (Bahdanau et al., 2015). In machine translation, all words appearing in an input sentence must be rewritten in the target language. However, paraphrase generation does not require rewriting of all words. When some criteria are provided, words not satisfying the criteria in the input sentence are identified and rewrit"
P19-1607,N18-1119,0,0.0680471,") p(w|z) = log (1) p(w)p(z) p(w) Newsela GYAFC-E&M GYAFC-F&R Train Dev Test 94,208 52,595 51,967 1,129 2,877 2,788 1,077 1,416 1,332 Table 1: Number of sentence pairs for each dataset. as they are. Meanwhile, negative lexical constraints that are forced to not include certain words in output sentence are promising for paraphrase generation. This is because, for example, text simplification is a task of generating sentential paraphrase without using complex words that appear in the source sentence. In this study, we add negative lexical constraints to beam search using dynamic beam allocation (Post and Vilar, 2018), which is the fastest lexically constrained decoding algorithm. In negative lexical constraints, we exclude hypotheses including the given words during beam search. Consequently, the words identified in Section 2.1 will not appear in our generated sentences. 3 Experiment After extracting the vocabulary Vi to be paraphrased for each input sentence si , we generate paraphrase sentences using it as a hard constraints. Note that PMI score is calculated using a training parallel corpus for paraphrase generation. We evaluate the performance of the proposed method on two major paraphrase generation"
P19-1607,N18-1012,0,0.243308,"ontrol complexity, formality transfer to control formality, grammatical error correction to control fluency, and sentence compression to control sentence length. These paraphrase generation applications not only support communication and language learning but also contribute to the performance improvement of other natural language processing applications (Evans, 2011; ˇ Stajner and Popovi´c, 2016). Paraphrase generation can be considered as a monolingual machine translation problem. Sentential paraphrases with different complexities (Coster and Kauchak, 2011; Xu et al., 2015) and formalities (Rao and Tetreault, 2018) were created manually, and parallel corpora specialized for each subtask were constructed. As in the field of machine translation, phrasebased (Coster and Kauchak, 2011; Xu et al., 2012) and syntax-based (Zhu et al., 2010; Xu et al., 2016) methods were proposed early. In recent years, the encode-decoder model based on the attention mechanism (Nisioi et al., 2017; Zhang and Lapata, 2017; Jhamtani et al., 2017; Niu et al., 2018) has been studied, inspired by the success of neural machine translation (Bahdanau et al., 2015). In machine translation, all words appearing in an input sentence must b"
P19-2036,P11-2117,0,0.0592613,"2) and SARI (Xu et al., 2016) compared to a baseline model (Nisioi et al., 2017) that does not consider the target level at all. This model allows the syntactic complexity to be controlled; however, it tends to output overly difficult words beyond the target grade level. Related Work 2.1 Text Simplification 3 Loss Function with Word Level Text simplification can be regarded as a monolingual machine translation problem. Previous studies have trained a model to translate complex sentences into simpler sentences on parallel corpora between Wikipedia and Simple Wikipedia (W-SW) (Zhu et al., 2010; Coster and Kauchak, 2011). As in the field of machine translation, early studies (Specia, 2010; Wubben et al., 2012; Xu et al., 2016) were mainly based on a statistical machine translation (Koehn et al., 2007; Post et al., 2013). Inspired by the success of neural machine translation (Bahdanau et al., 2015), recent studies (Nisioi et al., 2017; Zhang and Lapata, 2017; Vu et al., 2018; Guo et al., 2018; Zhao et al., 2018) use the encoder-decoder model with the attention mechanism. These studies do not consider the level of each sentence. To control the lexical complexity, our model weighs a training loss of a text simpl"
P19-2036,C18-1039,0,0.287479,"for each user. According to the input hypothesis (Krashen, 1985), educational materials slightly beyond the learner’s level effectively improve their reading abilities. On the contrary, materials that are too difficult for learners deteriorate their learning motivation. In the context of language education, teachers manually simplify 1 In this study, we use grades K-12. 260 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 260–266 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Guo et al., 2018; Zhao et al., 2018) in text simplification have trained machine translators on a monolingual parallel corpus consisting of complex-simple sentence pairs without considering the level of each sentence. Therefore, these text simplification models are ignorant regarding the sentence level. Scarton and Specia (2018) developed a pioneering text simplification model that can control the sentence level. They trained a text simplification model on a parallel corpus by attaching tags specifying 11 grade levels to each sentence (Xu et al., 2015). The trained model allows the generation of a sentence of"
P19-2036,Q17-1024,0,0.0487676,"Missing"
P19-2036,P18-4020,0,0.0221791,"Missing"
P19-2036,D18-1410,0,0.0280389,"Missing"
P19-2036,P17-2014,0,0.590628,"the syntactic and lexical complexities. When simplifying a sentence of grade level 12 into grade level 71 , paraphrasing “According to ∼ ,” to “∼ says” reduces the syntactic complexity. In addition, when simplifying the sentence from the grade levels 12 to 5, paraphrasing “Pentagon” to “military” reduces the lexical complexity. Assuming an application to language education, we aim at automatically rewriting the input sentence to accommodate the level of difficulty appropriate for each grade level, as shown in Table 1. Many previous studies (Specia, 2010; Wubben et al., 2012; Xu et al., 2016; Nisioi et al., 2017; Zhang and Lapata, 2017; Vu et al., 2018; Introduction Text simplification (Shardlow, 2014) is the task of rewriting a complex text into a simpler form while preserving its meaning. Its applications include reading comprehension assistance and language education support. Because each target user has different reading abilities and/or knowledge, we need a text simplification system that translates an input sentence into a sentence of an appropriate difficulty level for each user. According to the input hypothesis (Krashen, 1985), educational materials slightly beyond the learner’s level effect"
P19-2036,Q15-1021,0,0.210616,"t 2, 2019. 2019 Association for Computational Linguistics Guo et al., 2018; Zhao et al., 2018) in text simplification have trained machine translators on a monolingual parallel corpus consisting of complex-simple sentence pairs without considering the level of each sentence. Therefore, these text simplification models are ignorant regarding the sentence level. Scarton and Specia (2018) developed a pioneering text simplification model that can control the sentence level. They trained a text simplification model on a parallel corpus by attaching tags specifying 11 grade levels to each sentence (Xu et al., 2015). The trained model allows the generation of a sentence of a desired level specified by a tag attached to the input. This model may control the syntactic complexity such as the sentence length; however, it often outputs overly difficult words beyond the target grade level. To control the lexical complexity in text simplification, we propose a method for add weights to a training loss according to levels of words on top of (Scarton and Specia, 2018), and thus output only words under the desired level. Experiment results indicate that the proposed method improves the BLEU and SARI scores by 1.04"
P19-2036,C18-1086,0,0.0321888,"Missing"
P19-2036,P02-1040,0,0.10636,"text simplification. Newsela is a parallel corpus with 11 grade levels. Scarton and Specia (2018) trained a levelcontrollable text simplification model on Newsela. Although their model is a standard attentional encoder-decoder model similar to (Nisioi et al., 2017), a special token <grade> indicating the grade level of the target sentence is attached to the beginning of the input sentence. This is a promising approach that has been successful in similar tasks (Johnson et al., 2017; Niu et al., 2018). As expected regarding the task of text simplification, this approach has improved both BLEU (Papineni et al., 2002) and SARI (Xu et al., 2016) compared to a baseline model (Nisioi et al., 2017) that does not consider the target level at all. This model allows the syntactic complexity to be controlled; however, it tends to output overly difficult words beyond the target grade level. Related Work 2.1 Text Simplification 3 Loss Function with Word Level Text simplification can be regarded as a monolingual machine translation problem. Previous studies have trained a model to translate complex sentences into simpler sentences on parallel corpora between Wikipedia and Simple Wikipedia (W-SW) (Zhu et al., 2010; Co"
P19-2036,D17-1062,0,0.588426,"xical complexities. When simplifying a sentence of grade level 12 into grade level 71 , paraphrasing “According to ∼ ,” to “∼ says” reduces the syntactic complexity. In addition, when simplifying the sentence from the grade levels 12 to 5, paraphrasing “Pentagon” to “military” reduces the lexical complexity. Assuming an application to language education, we aim at automatically rewriting the input sentence to accommodate the level of difficulty appropriate for each grade level, as shown in Table 1. Many previous studies (Specia, 2010; Wubben et al., 2012; Xu et al., 2016; Nisioi et al., 2017; Zhang and Lapata, 2017; Vu et al., 2018; Introduction Text simplification (Shardlow, 2014) is the task of rewriting a complex text into a simpler form while preserving its meaning. Its applications include reading comprehension assistance and language education support. Because each target user has different reading abilities and/or knowledge, we need a text simplification system that translates an input sentence into a sentence of an appropriate difficulty level for each user. According to the input hypothesis (Krashen, 1985), educational materials slightly beyond the learner’s level effectively improve their read"
P19-2036,P16-2024,0,0.277889,"Missing"
P19-2036,D18-1355,0,0.387545,"cording to the input hypothesis (Krashen, 1985), educational materials slightly beyond the learner’s level effectively improve their reading abilities. On the contrary, materials that are too difficult for learners deteriorate their learning motivation. In the context of language education, teachers manually simplify 1 In this study, we use grades K-12. 260 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 260–266 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Guo et al., 2018; Zhao et al., 2018) in text simplification have trained machine translators on a monolingual parallel corpus consisting of complex-simple sentence pairs without considering the level of each sentence. Therefore, these text simplification models are ignorant regarding the sentence level. Scarton and Specia (2018) developed a pioneering text simplification model that can control the sentence level. They trained a text simplification model on a parallel corpus by attaching tags specifying 11 grade levels to each sentence (Xu et al., 2015). The trained model allows the generation of a sentence of a desired level spe"
P19-2036,C10-1152,0,0.128881,"pineni et al., 2002) and SARI (Xu et al., 2016) compared to a baseline model (Nisioi et al., 2017) that does not consider the target level at all. This model allows the syntactic complexity to be controlled; however, it tends to output overly difficult words beyond the target grade level. Related Work 2.1 Text Simplification 3 Loss Function with Word Level Text simplification can be regarded as a monolingual machine translation problem. Previous studies have trained a model to translate complex sentences into simpler sentences on parallel corpora between Wikipedia and Simple Wikipedia (W-SW) (Zhu et al., 2010; Coster and Kauchak, 2011). As in the field of machine translation, early studies (Specia, 2010; Wubben et al., 2012; Xu et al., 2016) were mainly based on a statistical machine translation (Koehn et al., 2007; Post et al., 2013). Inspired by the success of neural machine translation (Bahdanau et al., 2015), recent studies (Nisioi et al., 2017; Zhang and Lapata, 2017; Vu et al., 2018; Guo et al., 2018; Zhao et al., 2018) use the encoder-decoder model with the attention mechanism. These studies do not consider the level of each sentence. To control the lexical complexity, our model weighs a tr"
P19-2036,W13-2226,0,0.017783,"output overly difficult words beyond the target grade level. Related Work 2.1 Text Simplification 3 Loss Function with Word Level Text simplification can be regarded as a monolingual machine translation problem. Previous studies have trained a model to translate complex sentences into simpler sentences on parallel corpora between Wikipedia and Simple Wikipedia (W-SW) (Zhu et al., 2010; Coster and Kauchak, 2011). As in the field of machine translation, early studies (Specia, 2010; Wubben et al., 2012; Xu et al., 2016) were mainly based on a statistical machine translation (Koehn et al., 2007; Post et al., 2013). Inspired by the success of neural machine translation (Bahdanau et al., 2015), recent studies (Nisioi et al., 2017; Zhang and Lapata, 2017; Vu et al., 2018; Guo et al., 2018; Zhao et al., 2018) use the encoder-decoder model with the attention mechanism. These studies do not consider the level of each sentence. To control the lexical complexity, our model weighs a training loss of a text simplification model considering words that frequently appear in the sentences of a specific grade level, as shown in Figure 1. Here, the weight f (w, l) corresponds to the relevance of the word w at grade le"
P19-2036,P18-2113,0,0.673055,"ion, teachers manually simplify 1 In this study, we use grades K-12. 260 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 260–266 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Guo et al., 2018; Zhao et al., 2018) in text simplification have trained machine translators on a monolingual parallel corpus consisting of complex-simple sentence pairs without considering the level of each sentence. Therefore, these text simplification models are ignorant regarding the sentence level. Scarton and Specia (2018) developed a pioneering text simplification model that can control the sentence level. They trained a text simplification model on a parallel corpus by attaching tags specifying 11 grade levels to each sentence (Xu et al., 2015). The trained model allows the generation of a sentence of a desired level specified by a tag attached to the input. This model may control the syntactic complexity such as the sentence length; however, it often outputs overly difficult words beyond the target grade level. To control the lexical complexity in text simplification, we propose a method for add weights to a"
P19-2036,N18-2013,0,0.653613,"simplifying a sentence of grade level 12 into grade level 71 , paraphrasing “According to ∼ ,” to “∼ says” reduces the syntactic complexity. In addition, when simplifying the sentence from the grade levels 12 to 5, paraphrasing “Pentagon” to “military” reduces the lexical complexity. Assuming an application to language education, we aim at automatically rewriting the input sentence to accommodate the level of difficulty appropriate for each grade level, as shown in Table 1. Many previous studies (Specia, 2010; Wubben et al., 2012; Xu et al., 2016; Nisioi et al., 2017; Zhang and Lapata, 2017; Vu et al., 2018; Introduction Text simplification (Shardlow, 2014) is the task of rewriting a complex text into a simpler form while preserving its meaning. Its applications include reading comprehension assistance and language education support. Because each target user has different reading abilities and/or knowledge, we need a text simplification system that translates an input sentence into a sentence of an appropriate difficulty level for each user. According to the input hypothesis (Krashen, 1985), educational materials slightly beyond the learner’s level effectively improve their reading abilities. On"
P19-2036,P12-1107,0,0.501622,"Missing"
W17-5703,W15-3014,0,0.019616,"re have been studies on improving translation accuracy by reducing the OOV rate using pre- and post-processing for machine translation. Luong et al. (2015) proposed a post-processing method that translates OOV words with a corresponding word in the source sentence using a translation dictionary. This method needs to align training sentence pairs before training to learn correspondences between OOV words and their translations. In the method described in this paper, we need no word alignment, and we retain the meaning of the original word by paraphrasing the target side of the training corpus. Jean et al. (2015) proposed another post-processing method that translates each OOV word with the word that has the largest attention weight in the source sentence using a translation dictionary. Their method does not need word alignment, but it still does not necessarily consider the meaning in the target language, unlike our paraphrasing approach. Sennrich et al. (2016) applied byte pair encoding (BPE) to source and target corpora to split OOV words into units of frequent substrings to reduce the OOV rate. Their method splits words greedily without considering their meaning. Since we use lexical paraphrasing"
W17-5703,P02-1040,0,0.0989452,"on (SMT). However, NMT has a problem of high computational cost because it addresses the output generation task by solving a classification problem in vocabulary dimension. Typically, NMT has to restrict the size of the vocabulary to reduce the computational cost. Therefore, the target language vocabulary includes only high-frequency words • We propose a paraphrasing-based preprocessing method for Japanese-to-English NMT to improve translation accuracy with regard to OOV words. Our method can be combined with any NMT system. • We show that our method achieved a statistically significant BLEU (Kishore et al., 2002) score improvement of 0.58 and a METEOR (Lavie and Agarwal, 2007) score improvement of 0.52 over the previous method (Li et al., 2016) and reduced the OOV rate in output sentences by approximately 0.20. 64 Proceedings of the 4th Workshop on Asian Translation, pages 64–69, c Taipei, Taiwan, November 27, 2017. 2017 AFNLP 2 Related Work There have been studies on improving translation accuracy by reducing the OOV rate using pre- and post-processing for machine translation. Luong et al. (2015) proposed a post-processing method that translates OOV words with a corresponding word in the source sente"
W17-5703,P17-4012,0,0.0410569,"to build a 2gram language model trained with all sentences from ASPEC. We utilized the XXXL-size PPDB 2.0 (Pavlick et al., 2015) as the English paraphrase dictionary and PPDB:Japanese (Mizukami et al., 2014) as the Japanese paraphrase dictionary. Neither of these dictionaries contains the ASPEC corpus. We paraphrased either the target side of the training corpus only or both the source and target sides of the training corpus to conduct a fair comparison. We experimented with λ = 0.0, 0.25, 0.50, 0.75, and 1.0. We used OpenNMT-py5 as the NMT system, which is a Python implementation of OpenNMT (Klein et al., 2017). We built a model with settings as described below. We used bi-recurrentneural-network, batch size 64, epoch 20, embedding size 500, vocabulary size of source and target 30,000, dropout rate 0.3, optimizer SGD with learning rate 1.0, and number of RNN layers 2 with an RNN size of 500. Our baseline was trained with these settings without any paraphrasing. We re-implemented previous methods described in this paper (Luong et al., 2015; Li et al., 2016; Sennrich et al., 2016) using the underlying NMT with the abovementioned settings. We Experiment 4.1 Settings In this study, we used the Japanese–"
W17-5703,W07-0734,0,0.371531,"t because it addresses the output generation task by solving a classification problem in vocabulary dimension. Typically, NMT has to restrict the size of the vocabulary to reduce the computational cost. Therefore, the target language vocabulary includes only high-frequency words • We propose a paraphrasing-based preprocessing method for Japanese-to-English NMT to improve translation accuracy with regard to OOV words. Our method can be combined with any NMT system. • We show that our method achieved a statistically significant BLEU (Kishore et al., 2002) score improvement of 0.58 and a METEOR (Lavie and Agarwal, 2007) score improvement of 0.52 over the previous method (Li et al., 2016) and reduced the OOV rate in output sentences by approximately 0.20. 64 Proceedings of the 4th Workshop on Asian Translation, pages 64–69, c Taipei, Taiwan, November 27, 2017. 2017 AFNLP 2 Related Work There have been studies on improving translation accuracy by reducing the OOV rate using pre- and post-processing for machine translation. Luong et al. (2015) proposed a post-processing method that translates OOV words with a corresponding word in the source sentence using a translation dictionary. This method needs to align tr"
W17-5703,P15-1002,0,0.114721,"n be combined with any NMT system. • We show that our method achieved a statistically significant BLEU (Kishore et al., 2002) score improvement of 0.58 and a METEOR (Lavie and Agarwal, 2007) score improvement of 0.52 over the previous method (Li et al., 2016) and reduced the OOV rate in output sentences by approximately 0.20. 64 Proceedings of the 4th Workshop on Asian Translation, pages 64–69, c Taipei, Taiwan, November 27, 2017. 2017 AFNLP 2 Related Work There have been studies on improving translation accuracy by reducing the OOV rate using pre- and post-processing for machine translation. Luong et al. (2015) proposed a post-processing method that translates OOV words with a corresponding word in the source sentence using a translation dictionary. This method needs to align training sentence pairs before training to learn correspondences between OOV words and their translations. In the method described in this paper, we need no word alignment, and we retain the meaning of the original word by paraphrasing the target side of the training corpus. Jean et al. (2015) proposed another post-processing method that translates each OOV word with the word that has the largest attention weight in the source"
W17-5703,P15-2070,0,0.0413359,"Missing"
W17-5703,W16-3411,0,0.023253,"source and target corpora to split OOV words into units of frequent substrings to reduce the OOV rate. Their method splits words greedily without considering their meaning. Since we use lexical paraphrasing in the training data, we hope to reduce the OOV rate in the translation output while retaining the meaning. Additionally, since ours is a preprocessing method, it can be combined with a postprocessing method. On the other hand, there are methods similar to ours that paraphrase corpora as a preprocessing step of machine translation to reduce the complexity of source and/or target sentences. Sanja and Maja (2016) paraphrased source sentence vocabulary with a simple grammar as a preprocessing step for machine translation. We attempt to improve translation quality by reducing the OOV rate in the target language using paraphrasing without simplifying the source input sentences. Li et al. (2016) substituted OOV words in training corpora with a similar in-vocabulary word as pre- and post-processing steps. They replaced OOV words with frequent words using cosine similarity and a language model. They obtained word alignment between an OOV word and its counterpart in training corpora. In addition, they delete"
W17-5703,P16-1162,0,0.275476,"re training to learn correspondences between OOV words and their translations. In the method described in this paper, we need no word alignment, and we retain the meaning of the original word by paraphrasing the target side of the training corpus. Jean et al. (2015) proposed another post-processing method that translates each OOV word with the word that has the largest attention weight in the source sentence using a translation dictionary. Their method does not need word alignment, but it still does not necessarily consider the meaning in the target language, unlike our paraphrasing approach. Sennrich et al. (2016) applied byte pair encoding (BPE) to source and target corpora to split OOV words into units of frequent substrings to reduce the OOV rate. Their method splits words greedily without considering their meaning. Since we use lexical paraphrasing in the training data, we hope to reduce the OOV rate in the translation output while retaining the meaning. Additionally, since ours is a preprocessing method, it can be combined with a postprocessing method. On the other hand, there are methods similar to ours that paraphrase corpora as a preprocessing step of machine translation to reduce the complexit"
W18-0521,S16-1151,0,0.0638048,"Missing"
W18-0521,S16-1157,0,0.132156,"Missing"
W18-0521,S16-1162,0,0.0995403,"Missing"
W18-0521,P13-3015,0,0.146511,"s. This shared task was divided into two tasks (binary and probabilistic classification) and the following four tracks: Introduction Lexical simplification (Paetzold and Specia, 2017) is one of the approaches for text simplification (Shardlow, 2014), which facilitates children and language learners ’ reading comprehension. Lexical simplification comprises the following steps: 1. Complex word identification 2. Substitution generation 3. Substitution selection • English monolingual CWI 4. Substitution ranking • Spanish monolingual CWI In this study, we work on complex word identification (CWI) (Shardlow, 2013), a subtask of lexical simplification. Previous studies (Specia et al., 2012; Paetzold and Specia, 2016a) concluded that the most effective way to estimate word difficulty is to count the word frequency in a corpus. However, they counted the word frequency in corpora written by native speakers, such as Wikipedia. Language learners tend to use simple words as compared to native speakers. Therefore, we expect the word frequency in the learner corpus to be a useful feature for CWI. • German monolingual CWI • Multilingual CWI with a French test set The English dataset contained a mixture of profes"
W18-0521,S16-1154,0,0.0697223,"Missing"
W18-0521,S12-1046,0,0.190121,"classification) and the following four tracks: Introduction Lexical simplification (Paetzold and Specia, 2017) is one of the approaches for text simplification (Shardlow, 2014), which facilitates children and language learners ’ reading comprehension. Lexical simplification comprises the following steps: 1. Complex word identification 2. Substitution generation 3. Substitution selection • English monolingual CWI 4. Substitution ranking • Spanish monolingual CWI In this study, we work on complex word identification (CWI) (Shardlow, 2013), a subtask of lexical simplification. Previous studies (Specia et al., 2012; Paetzold and Specia, 2016a) concluded that the most effective way to estimate word difficulty is to count the word frequency in a corpus. However, they counted the word frequency in corpora written by native speakers, such as Wikipedia. Language learners tend to use simple words as compared to native speakers. Therefore, we expect the word frequency in the learner corpus to be a useful feature for CWI. • German monolingual CWI • Multilingual CWI with a French test set The English dataset contained a mixture of professionally written news, non-professionally written news (WikiNews), and Wikip"
W18-0521,S16-1153,0,0.194208,"Missing"
W18-0521,S16-1146,0,0.0382937,"Missing"
W18-0521,I11-1017,1,0.85983,"ses tend to be less frequent, we used the number of words as the second feature. Others features (3-8) are based on the frequency of targets in a corpus. We counted frequencies from texts written by native speakers and language learners. Language learners are more likely to use simple words than native speakers. Therefore, we expected word frequency in the learner corpus to be a useful feature for CWI. As a text written by native speakers, we counted the frequency from Wikipedia and WikiNews. By contrast, as a text written by language learners, we counted the frequency from the Lang-8 corpus (Mizumoto et al., 2011). The Lang-8 corpus contains texts before and after corrections written by learners and native speakers, respectively. We use the former. 2.2 Probabilistic Classification Task Labels in the probabilistic classification task were assigned as the proportion of annotators identifying the target as complex. Systems were evaluated using the MAE (mean absolute error). TMU Systems According to previous studies (Specia et al., 2012; Paetzold and Specia, 2016a), we estimated the word difficulty by counting word frequency. 3.1 Classifiers We used random forest classifiers and random forest regressors fo"
W18-0521,W18-0507,0,0.245705,". 1 Mamoru Komachi‡ ‡ Graduate School of Systems Design Tokyo Metropolitan University Tokyo, Japan komachi@tmu.ac.jp 2 CWI Shared Task 2018 In CWI shared tasks, systems predict whether words in a given context are complex or noncomplex for a non-native speaker. The first CWI shared task (Paetzold and Specia, 2016a; Zampieri et al., 2017) contained only English data designed for non-native English speakers. Totally, 20 annotators were assigned to each instance in the training set. However, in the test set, only one annotator was assigned to each instance. By contrast, the CWI shared task 2018 (Yimam et al., 2018) used a multilingual dataset (Yimam et al., 2017a,b) having all instances annotated by multiple annotators. This shared task was divided into two tasks (binary and probabilistic classification) and the following four tracks: Introduction Lexical simplification (Paetzold and Specia, 2017) is one of the approaches for text simplification (Shardlow, 2014), which facilitates children and language learners ’ reading comprehension. Lexical simplification comprises the following steps: 1. Complex word identification 2. Substitution generation 3. Substitution selection • English monolingual CWI 4. Sub"
W18-0521,S16-1152,0,0.3754,"Missing"
W18-0521,I17-2068,0,0.120278,"Design Tokyo Metropolitan University Tokyo, Japan komachi@tmu.ac.jp 2 CWI Shared Task 2018 In CWI shared tasks, systems predict whether words in a given context are complex or noncomplex for a non-native speaker. The first CWI shared task (Paetzold and Specia, 2016a; Zampieri et al., 2017) contained only English data designed for non-native English speakers. Totally, 20 annotators were assigned to each instance in the training set. However, in the test set, only one annotator was assigned to each instance. By contrast, the CWI shared task 2018 (Yimam et al., 2018) used a multilingual dataset (Yimam et al., 2017a,b) having all instances annotated by multiple annotators. This shared task was divided into two tasks (binary and probabilistic classification) and the following four tracks: Introduction Lexical simplification (Paetzold and Specia, 2017) is one of the approaches for text simplification (Shardlow, 2014), which facilitates children and language learners ’ reading comprehension. Lexical simplification comprises the following steps: 1. Complex word identification 2. Substitution generation 3. Substitution selection • English monolingual CWI 4. Substitution ranking • Spanish monolingual CWI In t"
W18-0521,yimam-etal-2017-multilingual,0,0.0757314,"Missing"
W18-0521,S16-1149,0,0.453088,"task 2018. TMU systems use random forest classifiers and regressors whose features are the number of characters and words and the frequency of target words in various corpora. Our simple systems performed best on 5 of the 12 tracks. Ablation analysis confirmed the usefulness of a learner corpus for a CWI task. 1 Mamoru Komachi‡ ‡ Graduate School of Systems Design Tokyo Metropolitan University Tokyo, Japan komachi@tmu.ac.jp 2 CWI Shared Task 2018 In CWI shared tasks, systems predict whether words in a given context are complex or noncomplex for a non-native speaker. The first CWI shared task (Paetzold and Specia, 2016a; Zampieri et al., 2017) contained only English data designed for non-native English speakers. Totally, 20 annotators were assigned to each instance in the training set. However, in the test set, only one annotator was assigned to each instance. By contrast, the CWI shared task 2018 (Yimam et al., 2018) used a multilingual dataset (Yimam et al., 2017a,b) having all instances annotated by multiple annotators. This shared task was divided into two tasks (binary and probabilistic classification) and the following four tracks: Introduction Lexical simplification (Paetzold and Specia, 2017) is one"
W18-0521,W17-5910,0,0.463178,"random forest classifiers and regressors whose features are the number of characters and words and the frequency of target words in various corpora. Our simple systems performed best on 5 of the 12 tracks. Ablation analysis confirmed the usefulness of a learner corpus for a CWI task. 1 Mamoru Komachi‡ ‡ Graduate School of Systems Design Tokyo Metropolitan University Tokyo, Japan komachi@tmu.ac.jp 2 CWI Shared Task 2018 In CWI shared tasks, systems predict whether words in a given context are complex or noncomplex for a non-native speaker. The first CWI shared task (Paetzold and Specia, 2016a; Zampieri et al., 2017) contained only English data designed for non-native English speakers. Totally, 20 annotators were assigned to each instance in the training set. However, in the test set, only one annotator was assigned to each instance. By contrast, the CWI shared task 2018 (Yimam et al., 2018) used a multilingual dataset (Yimam et al., 2017a,b) having all instances annotated by multiple annotators. This shared task was divided into two tasks (binary and probabilistic classification) and the following four tracks: Introduction Lexical simplification (Paetzold and Specia, 2017) is one of the approaches for te"
W18-0521,S16-1158,0,0.135728,"Missing"
W18-0521,S16-1155,0,0.259473,"Missing"
W18-0521,S16-1161,0,0.118343,"Missing"
W18-0544,W18-0521,1,0.737654,"lexrich 0.843 zz 0.839 TMU 0.834 Cam 0.822 btomosch 0.815 LambdaLearning 0.813 Grotoco 0.811 nihalnayak 0.808 jilljenn 0.808 ymatusevich 0.806 caseykennington 0.795 renhk 0.770 SLAM baseline Table 4: SLAM official evaluation results. Systems are ranked by AUROC. Model W/ History Model W/O History Model AUROC 0.834 0.648 In this work, we have not used any languagespecific information. As future work, we plan to exploit additional data for each language, such as pre-trained word representations, ngrams, and character-based features. Additionally, we hope to incorporate word difficulty features (Kajiwara and Komachi, 2018). In particular, the more complex a word is, the more difficult it likely is to be learned. Table 5: The history model has an effect to improve AUROC on English subtask. does not consider history (W/O History Model) on the dev set for English. The W/O History Model used only the Prediction Bi-LSTM component which does not use the history feature. For experiments using this model, we used a single model trained only on the English corpus. The default split of training set and dev set was 824,012 exercises and 115,770 exercises, respectively. Both aforementioned models used the same parameters a"
W18-0544,W18-0506,0,0.0370715,"Missing"
W18-6456,W17-4767,0,0.100146,"lation evaluation (MTE). The MTE metrics with a high correlation with human evaluation enable the continuous integration and deployment of a machine translation (MT) system. Various MTE metrics have been proposed in the metrics task of the Workshops on Statistical Machine Translation (WMT) that was started in 2008. However, most MTE metrics are obtained by computing the similarity between an MT hypothesis and a reference based on the character or word N-grams, such as SentBLEU (Lin and Och, 2004), which is a smoothed version of BLEU (Papineni et al., 2002), Blend (Ma et al., 2017), MEANT 2.0 (Lo, 2017), and chrF++ (Popovi´c, 2017). Therefore, they can exploit only limited information for the segment-level MTE. In other words, the MTE metrics based on character or word N-grams cannot make full use of sentence embeddings. They only check for word matches. 1 • We propose a novel supervised regression model for the segment-level MTE based on universal sentence embeddings. • We achieved a state-of-the-art performance in segment- and system-level metrics tasks on the WNT16 and WMT17 datasets for to-English language pairs without using any complex features. https://github.com/Shi-ma/RUSE 751 Proce"
W18-6456,W17-4755,0,0.270568,"as well as three other metrics, namely DPMF (Yu et al., 2015b), REDp (Yu et al., 2015a), and ENTFp (Yu et al., 2015a), using ranking SVM to train parameters of each metric score. DPMF evaluates the syntactic similarity between an MT hypothesis and a reference translation. REDp evaluates an MT hypothesis based on the dependency tree of the reference translation that comprises both lexical and syntactic information. ENTFp (Yu et al., 2015a) evaluates the fluency of an MT hypothesis. After the success of DPMFcomb , Blend3 (Ma et al., 2017) achieved the best performance in the WMT17 metrics task (Bojar et al., 2017). Similar to DPMFcomb , Blend is essentially an SVR model with RBF kernel that uses the scores of various metrics as features. It incorporates 25 lexical metrics provided by the Asiya MT evaluation toolkit, as well as four other metrics, namely BEER (Stanojevi´c and Sima’an, 2015), CharacTER (Wang et al., 2016), DPMF, and ENTFp. BEER is a linear model based on character Ngrams and replacement trees. CharacTER evaluates an MT hypothesis based on character-level edit distance. DPMFcomb is trained through relative ranking (RR) of human evaluation data in terms of relative 2 3 4 http://asiya.lsi.u"
W18-6456,W16-2302,0,0.0681552,"Missing"
W18-6456,W17-4768,0,0.633165,"c for automatic machine translation evaluation (MTE). The MTE metrics with a high correlation with human evaluation enable the continuous integration and deployment of a machine translation (MT) system. Various MTE metrics have been proposed in the metrics task of the Workshops on Statistical Machine Translation (WMT) that was started in 2008. However, most MTE metrics are obtained by computing the similarity between an MT hypothesis and a reference based on the character or word N-grams, such as SentBLEU (Lin and Och, 2004), which is a smoothed version of BLEU (Papineni et al., 2002), Blend (Ma et al., 2017), MEANT 2.0 (Lo, 2017), and chrF++ (Popovi´c, 2017). Therefore, they can exploit only limited information for the segment-level MTE. In other words, the MTE metrics based on character or word N-grams cannot make full use of sentence embeddings. They only check for word matches. 1 • We propose a novel supervised regression model for the segment-level MTE based on universal sentence embeddings. • We achieved a state-of-the-art performance in segment- and system-level metrics tasks on the WNT16 and WMT17 datasets for to-English language pairs without using any complex features. https://github.com"
W18-6456,D15-1075,0,0.0998304,"tion 3.2. 3.1 Universal Sentence Embeddings Several approaches have been proposed to learn sentence embeddings. These sentence embeddings are learned through large-scale data such that they constitute potentially useful features for MTE. These have been proven effective in various NLP tasks, such as document classification and measurement of semantic textual similarity, and we call them universal sentence embeddings. First, InferSent6 (Conneau et al., 2017) constructs a supervised model computing universal sentence embeddings trained using Stanford Natural Language Inference (SNLI) datasets7 (Bowman et al., 2015). The Natural Language Inference task is a classification task of sentence pairs with three labels, namely entailment, contradiction, and neutral; thus, InferSent can train sentence embeddings that are sensitive to differences in meaning. This model encodes a sentence pair u and v and generates features by sentence embeddings u and v with a bi-directional 6 7 8 https://github.com/lajanugen/S2V https://www.tensorflow.org/hub/modules/google/universalsentence-encoder-large/2 10 en: English, cs: Czech, de: German, fi: Finnish, ro: Romanian, ru: Russian, tr: Turkish, lv: Latvian, zh: Chinese 9 ht"
W18-6456,P02-1040,0,0.106657,"describes a segment-level metric for automatic machine translation evaluation (MTE). The MTE metrics with a high correlation with human evaluation enable the continuous integration and deployment of a machine translation (MT) system. Various MTE metrics have been proposed in the metrics task of the Workshops on Statistical Machine Translation (WMT) that was started in 2008. However, most MTE metrics are obtained by computing the similarity between an MT hypothesis and a reference based on the character or word N-grams, such as SentBLEU (Lin and Och, 2004), which is a smoothed version of BLEU (Papineni et al., 2002), Blend (Ma et al., 2017), MEANT 2.0 (Lo, 2017), and chrF++ (Popovi´c, 2017). Therefore, they can exploit only limited information for the segment-level MTE. In other words, the MTE metrics based on character or word N-grams cannot make full use of sentence embeddings. They only check for word matches. 1 • We propose a novel supervised regression model for the segment-level MTE based on universal sentence embeddings. • We achieved a state-of-the-art performance in segment- and system-level metrics tasks on the WNT16 and WMT17 datasets for to-English language pairs without using any complex fea"
W18-6456,D18-2029,0,0.0548051,"Missing"
W18-6456,W17-4770,0,0.071977,"Missing"
W18-6456,D17-1070,0,0.0340089,"ree types of sentence embeddings used in the proposed metric in Section 3.1. We then explain the proposed regression model and feature extraction for MTE in Section 3.2. 3.1 Universal Sentence Embeddings Several approaches have been proposed to learn sentence embeddings. These sentence embeddings are learned through large-scale data such that they constitute potentially useful features for MTE. These have been proven effective in various NLP tasks, such as document classification and measurement of semantic textual similarity, and we call them universal sentence embeddings. First, InferSent6 (Conneau et al., 2017) constructs a supervised model computing universal sentence embeddings trained using Stanford Natural Language Inference (SNLI) datasets7 (Bowman et al., 2015). The Natural Language Inference task is a classification task of sentence pairs with three labels, namely entailment, contradiction, and neutral; thus, InferSent can train sentence embeddings that are sensitive to differences in meaning. This model encodes a sentence pair u and v and generates features by sentence embeddings u and v with a bi-directional 6 7 8 https://github.com/lajanugen/S2V https://www.tensorflow.org/hub/modules/goo"
W18-6456,N18-4015,1,0.11916,"raining sentence embeddings using small-scale translation datasets with manual evaluation is difficult, sentence embeddings trained from large-scale data in other tasks can improve the automatic evaluation of machine translation. We use a multi-layer perceptron regressor based on three types of sentence embeddings. The experimental results of the WMT16 and WMT17 datasets show that the RUSE metric achieves a state-of-the-art performance in both segment- and system-level metrics tasks with embedding features only. 1 Figure 1: Outline of the RUSE metric. Introduction We extend our previous work (Shimanaka et al., 2018) and propose a segment-level MTE metric using universal sentence embeddings capable of capturing global information that cannot be captured by local features based on character or word N-grams. The experimental results in both segment- and system-level metrics tasks conducted using the datasets for to-English language pairs on WMT16 and WMT17 indicated that the proposed regression model using sentence embeddings, RUSE, achieves the best performance. The main contributions of the study are summarized below: This study describes a segment-level metric for automatic machine translation evaluation"
W18-6456,W15-3031,0,0.0584875,"Missing"
W18-6456,D15-1124,0,0.245194,"Missing"
W18-6456,W15-3050,0,0.175092,"Missing"
W18-6456,W15-3047,0,0.0401563,"Missing"
W18-6456,P15-1150,0,0.11408,"Missing"
W18-6456,S13-1005,0,0.0312385,"yers. Features. Publicly available pre-trained sentence embeddings, such as InferSent6 , QuickThought8 , and Universal Sentence Encoder9 , were used as the features mentioned in Section 3. InferSent is a collection of 4096-dimensional sentence embeddings trained on both 560,000 sentences of the SNLI dataset (Bowman et al., 2015) and 433,000 sentences of the MultiNLI dataset (Williams et al., 2018). Quick-Thought is a collection of 4800-dimensional sentence embeddings trained on both 45 million sentences of the BookCorpus dataset (Zhu et al., 2015) and 129 million sentences of the UMBC corpus (Han et al., 2013). Universal Sentence Encoder is a collection of 512-dimensional sentence embeddings trained on many sentences from a variety of web Sources, such as Wikipedia, web news, web question-answer pages, and discussion forums. • Number of layers ∈ {1, 2, 3} • Batch size ∈ {64, 128, 256, 512, 1024} • Dropout rate ∈ {0.1, 0.3, 0.5} • Optimizer ∈ {Adam} • C ∈ {0.1, 1.0, 10} • ϵ ∈ {0.01, 0.1, 1.0} • γ ∈ {0.001, 0.01, 0.1} Baseline Metrics. We compared the proposed metric with the four baseline metrics for each dataset. One is BLEU, which is the de facto standard metric for machine translation evaluation."
W18-6456,W16-2342,0,0.0779174,"the dependency tree of the reference translation that comprises both lexical and syntactic information. ENTFp (Yu et al., 2015a) evaluates the fluency of an MT hypothesis. After the success of DPMFcomb , Blend3 (Ma et al., 2017) achieved the best performance in the WMT17 metrics task (Bojar et al., 2017). Similar to DPMFcomb , Blend is essentially an SVR model with RBF kernel that uses the scores of various metrics as features. It incorporates 25 lexical metrics provided by the Asiya MT evaluation toolkit, as well as four other metrics, namely BEER (Stanojevi´c and Sima’an, 2015), CharacTER (Wang et al., 2016), DPMF, and ENTFp. BEER is a linear model based on character Ngrams and replacement trees. CharacTER evaluates an MT hypothesis based on character-level edit distance. DPMFcomb is trained through relative ranking (RR) of human evaluation data in terms of relative 2 3 4 http://asiya.lsi.upc.edu/ http://github.com/qingsongma/blend 5 752 https://github.com/rohitguptacs/ReVal http://clic.cimec.unitn.it/composes/sick.html WMT15 WMT16 WMT17 cs-en de-en fi-en lv-en ro-en ru-en tr-en zh-en 500 560 560 500 560 560 500 560 560 560 560 - 500 560 560 560 560 560 Table 1: Number of segment-level DA human e"
W18-6456,C04-1072,0,0.0850149,"tributions of the study are summarized below: This study describes a segment-level metric for automatic machine translation evaluation (MTE). The MTE metrics with a high correlation with human evaluation enable the continuous integration and deployment of a machine translation (MT) system. Various MTE metrics have been proposed in the metrics task of the Workshops on Statistical Machine Translation (WMT) that was started in 2008. However, most MTE metrics are obtained by computing the similarity between an MT hypothesis and a reference based on the character or word N-grams, such as SentBLEU (Lin and Och, 2004), which is a smoothed version of BLEU (Papineni et al., 2002), Blend (Ma et al., 2017), MEANT 2.0 (Lo, 2017), and chrF++ (Popovi´c, 2017). Therefore, they can exploit only limited information for the segment-level MTE. In other words, the MTE metrics based on character or word N-grams cannot make full use of sentence embeddings. They only check for word matches. 1 • We propose a novel supervised regression model for the segment-level MTE based on universal sentence embeddings. • We achieved a state-of-the-art performance in segment- and system-level metrics tasks on the WNT16 and WMT17 dataset"
W18-6456,N18-1101,0,0.0181493,"uation scores for to-English language pairs in WMT17. IS: InferSent; QT: Quick-Thought; and USE: Universal Sentence Encoder. man scores. rameters using the development data. We used ReLU as an activation function in all layers. Features. Publicly available pre-trained sentence embeddings, such as InferSent6 , QuickThought8 , and Universal Sentence Encoder9 , were used as the features mentioned in Section 3. InferSent is a collection of 4096-dimensional sentence embeddings trained on both 560,000 sentences of the SNLI dataset (Bowman et al., 2015) and 433,000 sentences of the MultiNLI dataset (Williams et al., 2018). Quick-Thought is a collection of 4800-dimensional sentence embeddings trained on both 45 million sentences of the BookCorpus dataset (Zhu et al., 2015) and 129 million sentences of the UMBC corpus (Han et al., 2013). Universal Sentence Encoder is a collection of 512-dimensional sentence embeddings trained on many sentences from a variety of web Sources, such as Wikipedia, web news, web question-answer pages, and discussion forums. • Number of layers ∈ {1, 2, 3} • Batch size ∈ {64, 128, 256, 512, 1024} • Dropout rate ∈ {0.1, 0.3, 0.5} • Optimizer ∈ {Adam} • C ∈ {0.1, 1.0, 10} • ϵ ∈ {0.01, 0.1"
Y14-1073,P01-1008,0,0.102882,"aphrase the headword. We propose lexical paraphrasing based on a variety of contexts obtained from a large corpus without depending on existing lexical resources from such a background. The proposed method is not dependent on language, thus it can perform lexical paraphrases using a corpus of arbitrary languages. In this paper we examine and report on Japanese nouns. 2 Related Works As paraphrase acquisition from a corpus, a study with a parallel corpus and comparable corpus has been performed. Barzilay and McKeown paraphrase text using plural English translations made from the same document (Barzilay and McKeown, 2001). In addition, Shinyama and Sekine paraphrase using plural newspaper articles that report the same event (Shinyama and Sekine, 2003). In a text simCopyright 2014 by Tomoyuki Kajiwara and Kazuhide Yamamoto 28th Pacific Asia Conference on Language, Information and Computation pages 644–649 !644 PACLIC 28 plification task, Coster and Kauchak create a parallel corpus that matches English Wikipedia and Simple English Wikipedia, and they perform text simplification using the framework of statistical machine translation (Coster and Kauchak, 2011). However, the technique of using these parallel corpor"
Y14-1073,P08-1077,0,0.430871,"rate paraphrases using only a single-language corpus so as not to come under these influences. In their research with paraphrasing based on the similarity of the context obtained from a nonparallel corpus, Marton et al. propose a method for paraphrasing unknown words to improve machine translation systems (Marton et al., 2009). They select candidate words with a context common to the subject. Moreover, they calculate cosine similarities of their feature vectors based on the co-occurrence frequency of subjects. Bhagat and Ravichandran extract paraphrases from a massive, 25-billion word corpus (Bhagat and Ravichandran, 2008). They regard English word 5-gram as one phrase, and they generate feature vectors using pointwise mutual information (PMI) scores. They then select the best phrase-paraphrase pairs based on their cosine similarity. Our proposed method is different from these methods in that it does not use co-occurrence frequency or word frequency of conventional features. We focus on the variety of context. Assuming that successful paraphrases have context that is common with their subject, we select paraphrases based only on the number of types of context. 3 Proposed Method In this paper, noun paraphrasing"
Y14-1073,P11-2117,0,0.0200816,"al English translations made from the same document (Barzilay and McKeown, 2001). In addition, Shinyama and Sekine paraphrase using plural newspaper articles that report the same event (Shinyama and Sekine, 2003). In a text simCopyright 2014 by Tomoyuki Kajiwara and Kazuhide Yamamoto 28th Pacific Asia Conference on Language, Information and Computation pages 644–649 !644 PACLIC 28 plification task, Coster and Kauchak create a parallel corpus that matches English Wikipedia and Simple English Wikipedia, and they perform text simplification using the framework of statistical machine translation (Coster and Kauchak, 2011). However, the technique of using these parallel corpora and comparable corpora is problematic in terms of the accuracy of alignment of corresponding expressions and quantity of the corpora that can be used. For example, for Japanese, there is no large-scale parallel corpus in which simplification is possible for use in the framework of statistical machine translation. In this paper, we generate paraphrases using only a single-language corpus so as not to come under these influences. In their research with paraphrasing based on the similarity of the context obtained from a nonparallel corpus,"
Y14-1073,O13-1007,1,0.832845,"exts for children. We believe that vocabulary simplification for children can be realized by paraphrasing text according to Basic Vocabulary to Learn (BVL) (Kai and Matsukawa, 2002) . BVL is a collection of words selected on the basis on a lexical analysis of elementary school textbooks. It contains 5,404 words that can help children write expressively. Kazuhide Yamamoto Department of Electrical Engineering Nagaoka University of Technology Nagaoka City, Niigata, Japan yamamoto@jnlp.org As previous work indicated, there are lexical paraphrases that define statements from a Japanese dictionary (Kajiwara et al., 2013). The definition statements from the Japanese dictionary explain a given headword in several easy words. Therefore, lexical simplification and paraphrasing that conserves a particular meaning are expected by paraphrasing the headword with the words in the definitions. However, definition statements are short sentences that consist of several words. Consequently, there are few paraphrase candidates, and natural paraphrasing is difficult even if we use certain dictionaries together. In addition, the definition statement as a whole is equivalent to the headword; there is no guarantee that any ind"
Y14-1073,W03-1609,0,0.0352864,"n existing lexical resources from such a background. The proposed method is not dependent on language, thus it can perform lexical paraphrases using a corpus of arbitrary languages. In this paper we examine and report on Japanese nouns. 2 Related Works As paraphrase acquisition from a corpus, a study with a parallel corpus and comparable corpus has been performed. Barzilay and McKeown paraphrase text using plural English translations made from the same document (Barzilay and McKeown, 2001). In addition, Shinyama and Sekine paraphrase using plural newspaper articles that report the same event (Shinyama and Sekine, 2003). In a text simCopyright 2014 by Tomoyuki Kajiwara and Kazuhide Yamamoto 28th Pacific Asia Conference on Language, Information and Computation pages 644–649 !644 PACLIC 28 plification task, Coster and Kauchak create a parallel corpus that matches English Wikipedia and Simple English Wikipedia, and they perform text simplification using the framework of statistical machine translation (Coster and Kauchak, 2011). However, the technique of using these parallel corpora and comparable corpora is problematic in terms of the accuracy of alignment of corresponding expressions and quantity of the corpo"
Y14-1073,W04-3230,0,\N,Missing
Y14-1073,D09-1040,0,\N,Missing
Y18-1004,P17-2070,0,0.50077,"processing tasks, such as machine translation (Sutskever et al., 2014), text classification (Mikolov and Com, 2014) and lexical substitution (Melamud et al., 2015). Widely used methods for word embedding including CBOW (Continuous Bag-of-Words) (Mikolov et al., 2013b) and SGNS (Skip-gram with Negative Sampling) (Mikolov et al., 2013a) generate a single word representation per word. Hence several meanings of the word are mixed in the representation, which significantly affect the downstream usability. To solve this problem, several methods (Neelakantan et al., 2014; Paetzold and Specia, 2016; Fadaee et al., 2017; Athiwaratkun and Wilson, 2017) to generate multi-sense embeddings, i.e., representations for each word sense are proposed. Because word sense disambiguation itself is still an open problem, these studies use approximate approaches. Paetzold and Specia (2016) use parts of speech to distinguish functionally different word senses, while Fadaee et al. (2017) propose to use topics. However, either parts of speech or topics are too coarse to distinguish word senses. In the following examples, both sentences have the topic of food, and the parts of speech of the words soft are both adjective.1 ex.1"
Y18-1004,P12-1092,0,0.410771,"le that in ex.2) has soft drink. In our approach, data sparseness is a challenge because a word has as many representations as the number of its context words. To deal with the data sparseness problem, we lemmatize each word and conduct pre-training of word representations using a conventional algorithm and use them as initial weights of the representations. It is expected that the representations of low frequency word pairs will remain in the vicinity of the initial value, which will in turn reduce the negative effects of data sparseness. Evaluation on the Context-Aware Word Similarity Task (Huang et al., 2012) and Lexical Substitution Task (McCarthy and Navigli, 2007; Kremer et al., 2014) are conducted to investigate the effects of the generated word representations in downstream tasks. The results show that our method significantly outperforms the state-of-the-art methods for multi-sense embeddings. In addition, detailed analysis confirms that the data sparseness problem has been effectively resolved by our methods due to pretraining. 2 Related Work Li and Jurafsky (2015) show that generating multiple word representations per word contributes to improving the performance of downstream tasks such a"
Y18-1004,E14-1057,0,0.250113,"Missing"
Y18-1004,P14-2050,0,0.0409833,"representations. It is expected that the representations of low frequency words will remain in the vicinity of the initial value, which will in turn reduce the negative effects of data sparseness. Extensive evaluation results confirm the effectiveness of our methods that significantly outperformed state-of-the-art methods for multi-sense embeddings. Detailed analysis of our method shows that the data sparseness problem is resolved due to the pre-training. 1 Introduction Distributed word representations by neural networks (Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014; Bojanowski et al., 2017), which are one of the implementations of the distributional hypothesis (Harris, 1954), have shown surprising effectiveness in representing semantic or syntactic information of words by dense vectors. They have become the standard language resource for many natural language processing tasks, such as machine translation (Sutskever et al., 2014), text classification (Mikolov and Com, 2014) and lexical substitution (Melamud et al., 2015). Widely used methods for word embedding including CBOW (Continuous Bag-of-Words) (Mikolov et al., 2013b) and SGNS (Skip-gram with Negat"
Y18-1004,D15-1200,0,0.0294543,"al value, which will in turn reduce the negative effects of data sparseness. Evaluation on the Context-Aware Word Similarity Task (Huang et al., 2012) and Lexical Substitution Task (McCarthy and Navigli, 2007; Kremer et al., 2014) are conducted to investigate the effects of the generated word representations in downstream tasks. The results show that our method significantly outperforms the state-of-the-art methods for multi-sense embeddings. In addition, detailed analysis confirms that the data sparseness problem has been effectively resolved by our methods due to pretraining. 2 Related Work Li and Jurafsky (2015) show that generating multiple word representations per word contributes to improving the performance of downstream tasks such as parts of speech tagging and semantic relation identification. There are several previous studies (Neelakantan et al., 2014; Paetzold and Specia, 2016; Fadaee et al., 2017; Athiwaratkun and Wilson, 2017) that generate multiple word representations per word. Athiwaratkun and Wilson (2017) assume that all words have a predetermined number of word senses Vector space non-alcoholic Vector space non-alcoholic soft_drink tender tender soft_cheese soft Pre-training soft sof"
Y18-1004,P14-5010,0,0.00447792,"the effects of the proposed method in downstream tasks, we conducted experiments in Context-Aware Word Similarity Task and Lexical Substitution Task. Both tasks require considering word senses, hence it is especially important to handle word representations for sense disambiguation. 30 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 4.1 Preprocessing snippet1 We use 988M sentences extracted from main contents of English Wikipedia2 articles as training data. We lemmatize each word using Stanford Parser (Manning et al., 2014) and replace words with frequency of 200 or less to ⟨unk⟩ tag to reduce the size of the vocabulary, which results in 112, 087 words. Dependency relations are also extracted using Stanford Parser. In order to avoid the data sparseness problem, parts of speech of contextword is limited to content word (i.e. noun, verb, adjective and adverb). For pre-training and posttraining of word representations, we set the window size to 5 and dimensions of representation to 300 in CBOW algorithm. 4.2 Baseline Models We compared our method to a baseline that generates a single word representation as well as"
Y18-1004,W15-1501,0,0.17362,"roduction Distributed word representations by neural networks (Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014; Bojanowski et al., 2017), which are one of the implementations of the distributional hypothesis (Harris, 1954), have shown surprising effectiveness in representing semantic or syntactic information of words by dense vectors. They have become the standard language resource for many natural language processing tasks, such as machine translation (Sutskever et al., 2014), text classification (Mikolov and Com, 2014) and lexical substitution (Melamud et al., 2015). Widely used methods for word embedding including CBOW (Continuous Bag-of-Words) (Mikolov et al., 2013b) and SGNS (Skip-gram with Negative Sampling) (Mikolov et al., 2013a) generate a single word representation per word. Hence several meanings of the word are mixed in the representation, which significantly affect the downstream usability. To solve this problem, several methods (Neelakantan et al., 2014; Paetzold and Specia, 2016; Fadaee et al., 2017; Athiwaratkun and Wilson, 2017) to generate multi-sense embeddings, i.e., representations for each word sense are proposed. Because word sense d"
Y18-1004,D14-1113,0,0.347753,"standard language resource for many natural language processing tasks, such as machine translation (Sutskever et al., 2014), text classification (Mikolov and Com, 2014) and lexical substitution (Melamud et al., 2015). Widely used methods for word embedding including CBOW (Continuous Bag-of-Words) (Mikolov et al., 2013b) and SGNS (Skip-gram with Negative Sampling) (Mikolov et al., 2013a) generate a single word representation per word. Hence several meanings of the word are mixed in the representation, which significantly affect the downstream usability. To solve this problem, several methods (Neelakantan et al., 2014; Paetzold and Specia, 2016; Fadaee et al., 2017; Athiwaratkun and Wilson, 2017) to generate multi-sense embeddings, i.e., representations for each word sense are proposed. Because word sense disambiguation itself is still an open problem, these studies use approximate approaches. Paetzold and Specia (2016) use parts of speech to distinguish functionally different word senses, while Fadaee et al. (2017) propose to use topics. However, either parts of speech or topics are too coarse to distinguish word senses. In the following examples, both sentences have the topic of food, and the parts of sp"
Y18-1004,W16-4912,0,0.23947,"e for many natural language processing tasks, such as machine translation (Sutskever et al., 2014), text classification (Mikolov and Com, 2014) and lexical substitution (Melamud et al., 2015). Widely used methods for word embedding including CBOW (Continuous Bag-of-Words) (Mikolov et al., 2013b) and SGNS (Skip-gram with Negative Sampling) (Mikolov et al., 2013a) generate a single word representation per word. Hence several meanings of the word are mixed in the representation, which significantly affect the downstream usability. To solve this problem, several methods (Neelakantan et al., 2014; Paetzold and Specia, 2016; Fadaee et al., 2017; Athiwaratkun and Wilson, 2017) to generate multi-sense embeddings, i.e., representations for each word sense are proposed. Because word sense disambiguation itself is still an open problem, these studies use approximate approaches. Paetzold and Specia (2016) use parts of speech to distinguish functionally different word senses, while Fadaee et al. (2017) propose to use topics. However, either parts of speech or topics are too coarse to distinguish word senses. In the following examples, both sentences have the topic of food, and the parts of speech of the words soft are"
Y18-1004,D14-1162,0,0.0811086,"ed using pre-trained word representations. It is expected that the representations of low frequency words will remain in the vicinity of the initial value, which will in turn reduce the negative effects of data sparseness. Extensive evaluation results confirm the effectiveness of our methods that significantly outperformed state-of-the-art methods for multi-sense embeddings. Detailed analysis of our method shows that the data sparseness problem is resolved due to the pre-training. 1 Introduction Distributed word representations by neural networks (Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014; Bojanowski et al., 2017), which are one of the implementations of the distributional hypothesis (Harris, 1954), have shown surprising effectiveness in representing semantic or syntactic information of words by dense vectors. They have become the standard language resource for many natural language processing tasks, such as machine translation (Sutskever et al., 2014), text classification (Mikolov and Com, 2014) and lexical substitution (Melamud et al., 2015). Widely used methods for word embedding including CBOW (Continuous Bag-of-Words) (Mikolov et al., 2013b) and S"
Y18-1004,N16-1131,0,0.47529,"ns in 10 different contexts. LS-CIC This is a large-scale dataset (Kremer et al., 2014) for Lexical Substitution Task. For 4 https://github.com/stephenroller/ naacl2016 32 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 15, 629 target words, six annotators produce up to five types of substitutions under given context. Unlike LS-SE, three sentences are given as context; a sentence containing the target as well as its preceding and following sentences. Following the previous studies (Melamud et al., 2015; Roller and Erk, 2016; Fadaee et al., 2017), substitution candidates are the union set of substitutions in each context with same target word. Therefore, these candidates are semantically and syntactically close to the target word in a certain context. Hence, it is important for this task to distinguish the meaning of target word using a given context. We use Generalized Average Precision (GAP) (Kishida, 2005) as an evaluation metric. GAP is commonly used in evaluation of Lexical Substitution Task that calculates ranking accuracy by considering the weight of correct examples. 6.1 Evaluating Appropriateness of Lexi"
