2020.figlang-1.3,A Report on the 2020 {VUA} and {TOEFL} Metaphor Detection Shared Task,2020,-1,-1,2,0.965912,19992,chee leong,Proceedings of the Second Workshop on Figurative Language Processing,0,"In this paper, we report on the shared task on metaphor identification on VU Amsterdam Metaphor Corpus and on a subset of the TOEFL Native Language Identification Corpus. The shared task was conducted as apart of the ACL 2020 Workshop on Processing Figurative Language."
2020.figlang-1.32,Go Figure! Multi-task transformer-based architecture for metaphor detection using idioms: {ETS} team in 2020 metaphor shared task,2020,-1,-1,4,0,15977,xianyang chen,Proceedings of the Second Workshop on Figurative Language Processing,0,"This paper describes the ETS entry to the 2020 Metaphor Detection shared task. Our contribution consists of a sequence of experiments using BERT, starting with a baseline, strengthening it by spell-correcting the TOEFL corpus, followed by a multi-task learning setting, where one of the tasks is the token-level metaphor classification as per the shared task, while the other is meant to provide additional training that we hypothesized to be relevant to the main task. In one case, out-of-domain data manually annotated for metaphor is used for the auxiliary task; in the other case, in-domain data automatically annotated for idioms is used for the auxiliary task. Both multi-task experiments yield promising results."
2020.bea-1.14,An Exploratory Study of Argumentative Writing by Young Students: A transformer-based Approach,2020,-1,-1,2,0,8209,debanjan ghosh,Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications,0,"We present a computational exploration of argument critique writing by young students. Middle school students were asked to criticize an argument presented in the prompt, focusing on identifying and explaining the reasoning flaws. This task resembles an established college-level argument critique task. Lexical and discourse features that utilize detailed domain knowledge to identify critiques exist for the college task but do not perform well on the young students{'} data. Instead, transformer-based architecture (e.g., BERT) fine-tuned on a large corpus of critique essays from the college task performs much better (over 20{\%} improvement in F1 score). Analysis of the performance of various configurations of the system suggests that while children{'}s writing does not exhibit the standard discourse structure of an argumentative essay, it does share basic local sequential structures with the more mature writers."
2020.acl-main.697,Automated Evaluation of Writing {--} 50 Years and Counting,2020,-1,-1,1,1,19993,beata klebanov,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"In this theme paper, we focus on Automated Writing Evaluation (AWE), using Ellis Page{'}s seminal 1966 paper to frame the presentation. We discuss some of the current frontiers in the field and offer some thoughts on the emergent uses of this technology."
P19-3024,My Turn To Read: An Interleaved {E}-book Reading Tool for Developing and Struggling Readers,2019,0,1,2,0,16057,nitin madnani,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"Literacy is crucial for functioning in modern society. It underpins everything from educational attainment and employment opportunities to health outcomes. We describe My Turn To Read, an app that uses interleaved reading to help developing and struggling readers improve reading skills while reading for meaning and pleasure. We hypothesize that the longer-term impact of the app will be to help users become better, more confident readers with an increased stamina for extended reading. We describe the technology and present preliminary evidence in support of this hypothesis."
W18-0905,Catching Idiomatic Expressions in {EFL} Essays,2018,-1,-1,2,0.481672,15978,michael flor,Proceedings of the Workshop on Figurative Language Processing,0,"This paper presents an exploratory study on large-scale detection of idiomatic expressions in essays written by non-native speakers of English. We describe a computational search procedure for automatic detection of idiom-candidate phrases in essay texts. The study used a corpus of essays written during a standardized examination of English language proficiency. Automatically-flagged candidate expressions were manually annotated for idiomaticity. The study found that idioms are widely used in EFL essays. The study also showed that a search algorithm that accommodates the syntactic and lexical exibility of idioms can increase the recall of idiom instances by 30{\%}, but it also increases the amount of false positives."
W18-0907,A Report on the 2018 {VUA} Metaphor Detection Shared Task,2018,-1,-1,2,0.965912,19992,chee leong,Proceedings of the Workshop on Figurative Language Processing,0,"As the community working on computational approaches to figurative language is growing and as methods and data become increasingly diverse, it is important to create widely shared empirical knowledge of the level of system performance in a range of contexts, thus facilitating progress in this area. One way of creating such shared knowledge is through benchmarking multiple systems on a common dataset. We report on the shared task on metaphor identification on the VU Amsterdam Metaphor Corpus conducted at the NAACL 2018 Workshop on Figurative Language Processing."
W18-0501,Using exemplar responses for training and evaluating automated speech scoring systems,2018,0,1,4,0,16058,anastassia loukina,Proceedings of the Thirteenth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"Automated scoring engines are usually trained and evaluated against human scores and compared to the benchmark of human-human agreement. In this paper we compare the performance of an automated speech scoring engine using two corpora: a corpus of almost 700,000 randomly sampled spoken responses with scores assigned by one or two raters during operational scoring, and a corpus of 16,500 exemplar responses with scores reviewed by multiple expert raters. We show that the choice of corpus used for model evaluation has a major effect on estimates of system performance with r varying between 0.64 and 0.80. Surprisingly, this is not the case for the choice of corpus for model training: when the training corpus is sufficiently large, the systems trained on different corpora showed almost identical performance when evaluated on the same corpus. We show that this effect is consistent across several learning algorithms. We conclude that evaluating the model on a corpus of exemplar responses if one is available provides additional evidence about system validity; at the same time, investing effort into creating a corpus of exemplar responses for model training is unlikely to lead to a substantial gain in model performance."
N18-2014,A Corpus of Non-Native Written {E}nglish Annotated for Metaphor,2018,0,0,1,1,19993,beata klebanov,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",0,"We present a corpus of 240 argumentative essays written by non-native speakers of English annotated for metaphor. The corpus is made publicly available. We provide benchmark performance of state-of-the-art systems on this new corpus, and explore the relationship between writing proficiency and metaphor use."
N18-1195,Towards Understanding Text Factors in Oral Reading,2018,0,2,3,0,16058,anastassia loukina,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",0,"Using a case study, we show that variation in oral reading rate across passages for professional narrators is consistent across readers and much of it can be explained using features of the texts being read. While text complexity is a poor predictor of the reading rate, a substantial share of variability can be explained by timing and story-based factors with performance reaching r=0.75 for unseen passages and narrator."
C18-2025,Writing Mentor: Self-Regulated Writing Feedback for Struggling Writers,2018,0,1,4,0,16057,nitin madnani,Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations,0,"Writing Mentor is a free Google Docs add-on designed to provide feedback to struggling writers and help them improve their writing in a self-paced and self-regulated fashion. Writing Mentor uses natural language processing (NLP) methods and resources to generate feedback in terms of features that research into post-secondary struggling writers has classified as developmental (Burstein et al., 2016b). These features span many writing sub-constructs (use of sources, claims, and evidence; topic development; coherence; and knowledge of English conventions). Prelimi- nary analysis indicates that users have a largely positive impression of Writing Mentor in terms of usability and potential impact on their writing."
W17-5003,Continuous fluency tracking and the challenges of varying text complexity,2017,0,2,1,1,19993,beata klebanov,Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"This paper is a preliminary report on using text complexity measurement in the service of a new educational application. We describe a reading intervention where a child takes turns reading a book aloud with a virtual reading partner. Our ultimate goal is to provide meaningful feedback to the parent or the teacher by continuously tracking the child{'}s improvement in reading fluency. We show that this would not be a simple endeavor, due to an intricate relationship between text complexity from the point of view of comprehension and reading rate."
W17-5011,Exploring Relationships Between Writing {\\&} Broader Outcomes With Automated Writing Evaluation,2017,16,0,3,0,30726,jill burstein,Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"Writing is a challenge, especially for at-risk students who may lack the prerequisite writing skills required to persist in U.S. 4-year postsecondary (college) institutions. Educators teaching postsecondary courses requiring writing could benefit from a better understanding of writing achievement and its role in postsecondary success. In this paper, novel exploratory work examined how automated writing evaluation (AWE) can inform our understanding of the relationship between postsecondary writing skill and broader success outcomes. An exploratory study was conducted using test-taker essays from a standardized writing assessment of postsecondary student learning outcomes. Findings showed that for the essays, AWE features were found to be predictors of broader outcomes measures: college success and learning outcomes measures. Study findings illustrate AWE{'}s potential to support educational analytics {--} i.e., relationships between writing skill and broader outcomes {--} taking a step toward moving AWE beyond writing assessment and instructional use cases."
P17-2038,Detecting Good Arguments in a Non-Topic-Specific Way: An Oxymoron?,2017,0,0,1,1,19993,beata klebanov,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Automatic identification of good arguments on a controversial topic has applications in civics and education, to name a few. While in the civics context it might be acceptable to create separate models for each topic, in the context of scoring of students{'} writing there is a preference for a single model that applies to all responses. Given that good arguments for one topic are likely to be irrelevant for another, is a single model for detecting good arguments a contradiction in terms? We investigate the extent to which it is possible to close the performance gap between topic-specific and across-topics models for identification of good arguments."
W16-2808,"{A}rgumentation: Content, Structure, and Relationship with Essay Quality",2016,18,4,1,1,19993,beata klebanov,Proceedings of the Third Workshop on Argument Mining ({A}rg{M}ining2016),0,"In this paper, we investigate the relationship between argumentation structures and (a) argument content, and (b) the holistic quality of an argumentative essay. Our results suggest that structure-based approaches hold promise for automated evaluation of argumentative writing."
W16-0507,Topicality-Based Indices for Essay Scoring,2016,20,1,1,1,19993,beata klebanov,Proceedings of the 11th Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"In this paper, we address the problem of quantifying the overall extent to which a testtakerxe2x80x99s essay deals with the topic it is assigned (prompt). We experiment with a number of models for word topicality, and a number of approaches for aggregating word-level indices into text-level ones. All models are evaluated for their ability to predict the holistic quality of essays. We show that the best texttopicality model provides a significant improvement in a state-of-art essay scoring system. We also show that the findings of the relative merits of different models generalize well across three different datasets."
W16-0522,Enhancing {STEM} Motivation through Personal and Communal Values: {NLP} for Assessment of Utility Value in Student Writing,2016,25,0,1,1,19993,beata klebanov,Proceedings of the 11th Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"We present, to our knowledge, the first experiments on using NLP to measure the extent to which a writing sample expresses the writerxe2x80x99s utility value from studying a STEM subject. Studies in social psychology have shown that a writing intervention where a STEM student is asked to reflect on the value of the STEM subject in their personal and social life is effective for improving motivation and retention of students in STEM in college. Automated assessment of UV in student writing would allow scaling the intervention up, opening access to its benefits to multitudes of college students. Our results on biology data suggest that expression of utility value can be measured with reasonable accuracy using automated means, especially in personal essays."
P16-2017,Semantic classifications for detection of verb metaphors,2016,20,5,1,1,19993,beata klebanov,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We investigate the effectiveness of semantic generalizations/classifications for capturing the regularities of the behavior of verbs in terms of their metaphoricity. Starting from orthographic word unigrams, we experiment with various ways of defining semantic classes for verbs (grammatical, resource-based, distributional) and measure the effectiveness of these classes for classifying all verbs in a running text as metaphor or non metaphor."
W15-1402,Supervised Word-Level Metaphor Detection: Experiments with Concreteness and Reweighting of Examples,2015,29,11,1,1,19993,beata klebanov,Proceedings of the Third Workshop on Metaphor in {NLP},0,"We present a supervised machine learning system for word-level classification of all content words in a running text as being metaphorical or non-metaphorical. The system provides a substantial improvement upon a previously published baseline, using re-weighting of the training examples and using features derived from a concreteness database. We observe that while the first manipulation was very effective, the second was only slightly so. Possible reasons for these observations are discussed."
W14-4705,{ETS} Lexical Associations System for the {COGALEX}-4 Shared Task,2014,16,0,2,0.833333,15978,michael flor,Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex),0,"We present an automated system that computes multi-cue associations and generates associated-word suggestions, using lexical co-occurrence data from a large corpus of English texts. The system performs expansion of cue words to their inflectional variants, retrieves candidate words from corpus data, finds maximal associations between candidates and cues, computes an aggregate score for each candidate, and outputs an n-best list of candidates. We present experiments using several measures of statistical association, two methods of score aggregation, ablation of resources and applying additional filters on retrieved candidates. The system achieves 18.6% precision on the COGALEX-4 shared task data. Results with additional evaluation methods are presented. We also describe an annotation experiment which suggests that the shared task may underestimate the appropriateness of candidate words produced by the corpus-based system."
W14-2302,"Different Texts, Same Metaphors: Unigrams and Beyond",2014,30,19,1,1,19993,beata klebanov,Proceedings of the Second Workshop on Metaphor in {NLP},0,"Current approaches to supervised learning of metaphor tend to use sophisticated features and restrict their attention to constructions and contexts where these features apply. In this paper, we describe the development of a supervised learning system to classify all content words in a running text as either being used metaphorically or not. We start by examining the performance of a simple unigram baseline that achieves surprisingly good results for some of the datasets. We then show how the recall of the system can be improved over this strong baseline."
W14-2110,Applying Argumentation Schemes for Essay Scoring,2014,22,23,3,0,22302,yi song,Proceedings of the First Workshop on Argumentation Mining,0,"Under the framework of the argumentation scheme theory (Walton, 1996), we developed annotation protocols for an argumentative writing task to support identification and classification of the arguments being made in essays. Each annotation protocol defined argumentation schemes (i.e., reasoning patterns) in a given writing prompt and listed questions to help evaluate an argument based on these schemes, to make the argument structure in a text explicit and classifiable. We report findings based on an annotation of 600 essays. Most annotation categories were applied reliably by human annotators, and some categories significantly contributed to essay score. An NLP system to identify sentences containing scheme-relevant critical questions was developed based on the human annotations."
P14-2041,Content Importance Models for Scoring Writing From Sources,2014,14,12,1,1,19993,beata klebanov,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,Selection of information from external sources is an important skill assessed in educational measurement. We address an integrative summarization task used in an assessment of English proficiency for nonnative speakers applying to higher education institutions in the USA. We evaluate a variety of content importance models that help predict which parts of the source material should be selected by the test-taker in order to succeed on this task.
P14-2064,"Difficult Cases: From Data to Learning, and Back",2014,36,3,1,1,19993,beata klebanov,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"This article contributes to the ongoing discussion in the computational linguistics community regarding instances that are difficult to annotate reliably. Is it worthwhile to identify those? What information can be inferred from them regarding the nature of the task? What should be done with them when building supervised machine learning systems? We address these questions in the context of a subjective semantic task. In this setting, we show that the presence of such instances in training data misleads a machine learner into misclassifying clear-cut cases. We also show that considering machine learning outcomes with and without the difficult cases, it is possible to identify specific weaknesses of the problem representation."
W13-3304,Associative Texture Is Lost In Translation,2013,6,9,1,1,19993,beata klebanov,Proceedings of the Workshop on Discourse in Machine Translation,0,"We present a suggestive finding regarding the loss of associative texture in the process of machine translation, using comparisons between (a) original and backtranslated texts, (b) reference and system translations, and (c) better and worse MT systems. We represent the amount of association in a text using word association profile xe2x80x90 a distribution of pointwise mutual information between all pairs of content word types in a text. We use the average of the distribution, which we term lexical tightness, as a single measure of the amount of association in a text. We show that the lexical tightness of humancomposed texts is higher than that of the machine translated materials; human references are tighter than machine translations, and better MT systems produce lexically tighter translations. While the phenomenon of the loss of associative texture has been theoretically predicted by translation scholars, we present a measure capable of quantifying the extent of this phenomenon."
W13-1504,Lexical Tightness and Text Complexity,2013,33,17,2,0.833333,15978,michael flor,Proceedings of the Workshop on Natural Language Processing for Improving Textual Accessibility,0,"We present a computational notion of Lexical Tightness that measures global cohesion of content words in a text. Lexical tightness represents the degree to which a text tends to use words that are highly inter-associated in the language. We demonstrate the utility of this measure for estimating text complexity as measured by US school grade level designations of texts. Lexical tightness strongly correlates with grade level in a collection of expertly rated reading materials. Lexical tightness captures aspects of prose complexity that are not covered by classic readability indexes, especially for literary texts. We also present initial findings on the utility of this measure for automated estimation of complexity for poetry."
W13-0902,Argumentation-Relevant Metaphors in Test-Taker Essays,2013,25,11,1,1,19993,beata klebanov,Proceedings of the First Workshop on Metaphor in {NLP},0,"This article discusses metaphor annotation in a corpus of argumentative essays written by test-takers during a standardized examination for graduate school admission. The quality of argumentation being the focus of the project, we developed a metaphor annotation protocol that targets metaphors that are relevant for the writerxe2x80x99s arguments. The reliability of the protocol is =0.58, on a set of 116 essays (the total of about 30K content-word tokens). We found a moderate-to-strong correlation (r=0.51-0.57) between the percentage of metaphorically used words in an essay and the writing quality score. We also describe encouraging findings regarding the potential of metaphor identification to contribute to automated scoring of essays."
Q13-1009,Using Pivot-Based Paraphrasing and Sentiment Profiles to Improve a Subjectivity Lexicon for Essay Data,2013,39,6,1,1,19993,beata klebanov,Transactions of the Association for Computational Linguistics,0,We demonstrate a method of improving a seed sentiment lexicon developed on essay data by using a pivot-based paraphrasing system for lexical expansion coupled with sentiment profile enrichment using crowdsourcing. Profile enrichment alone yields up to 15{\%} improvement in the accuracy of the seed lexicon on 3-way sentence-level sentiment polarity classification of essay data. Using lexical expansion in addition to sentiment profiles provides a further 7{\%} improvement in performance. Additional experiments show that the proposed method is also effective with other subjectivity lexicons and in a different domain of application (product reviews).
P13-1113,Word Association Profiles and their Use for Automated Scoring of Essays,2013,48,13,1,1,19993,beata klebanov,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We describe a new representation of the content vocabulary of a text we call word association profile that captures the proportions of highly associated, mildly associated, unassociated, and dis-associated pairs of words that co-exist in the given text. We illustrate the shape of the distirbution and observe variation with genre and target audience. We present a study of the relationship between quality of writing and word association profiles. For a set of essays written by college graduates on a number of general topics, we show that the higher scoring essays tend to have higher percentages of both highly associated and dis-associated pairs, and lower percentages of mildly associated pairs of words. Finally, we use word association profiles to improve a system for automated scoring of essays."
W12-2007,Measuring the Use of Factual Information in Test-Taker Essays,2012,30,4,1,1,19993,beata klebanov,Proceedings of the Seventh Workshop on Building Educational Applications Using {NLP},0,"We describe a study aimed at measuring the use of factual information in test-taker essays and assessing its effectiveness for predicting essay scores. We found medium correlations with the proposed measures, that remained significant after the effect of essay length was factored out. The correlations did not differ substantionally between a simple, relatively robust measure vs a more sophisticated measure with better construct validity. Implications for development of automated essay scoring systems are discussed."
P10-2047,Vocabulary Choice as an Indicator of Perspective,2010,17,19,1,1,19993,beata klebanov,Proceedings of the {ACL} 2010 Conference Short Papers,0,"We establish the following characteristics of the task of perspective classification: (a) using term frequencies in a document does not improve classification achieved with absence/presence features; (b) for datasets allowing the relevant comparisons, a small number of top features is found to be as effective as the full feature set and indispensable for the best achieved performance, testifying to the existence of perspective-specific keywords. We relate our findings to research on word frequency distributions and to discourse analytic studies of perspective."
P10-1072,A Game-Theoretic Model of Metaphorical Bargaining,2010,60,5,1,1,19993,beata klebanov,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"We present a game-theoretic model of bargaining over a metaphor in the context of political communication, find its equilibrium, and use it to rationalize observed linguistic behavior. We argue that game theory is well suited for modeling discourse as a dynamic resulting from a number of conflicting pressures, and suggest applications of interest to computational linguists."
N10-1067,Some Empirical Evidence for Annotation Noise in a Benchmarked Dataset,2010,17,5,1,1,19993,beata klebanov,Human Language Technologies: The 2010 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"A number of recent articles in computational linguistics venues called for a closer examination of the type of noise present in annotated datasets used for benchmarking (Reidsma and Carletta, 2008; Beigman Klebanov and Beigman, 2009). In particular, Beigman Klebanov and Beigman articulated a type of noise they call annotation noise and showed that in worst case such noise can severely degrade the generalization ability of a linear classifier (Beigman and Beigman Klebanov, 2009). In this paper, we provide quantitative empirical evidence for the existence of this type of noise in a recently benchmarked dataset. The proposed methodology can be used to zero in on unreliable instances, facilitating generation of cleaner gold standards for benchmarking."
W09-2001,Discourse Topics and Metaphors,2009,16,5,1,1,19993,beata klebanov,Proceedings of the Workshop on Computational Approaches to Linguistic Creativity,0,"Using metaphor-annotated material that is sufficiently representative of the topical composition of a similar-length document in a large background corpus, we show that words expressing a discourse-wide topic of discussion are less likely to be metaphorical than other words in a document. Our results suggest that to harvest metaphors more effectively, one is advised to consider words that do not represent a discourse topic."
P09-1032,Learning with Annotation Noise,2009,32,43,2,0,39144,eyal beigman,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"It is usually assumed that the kind of noise existing in annotated data is random classification noise. Yet there is evidence that differences between annotators are not always random attention slips but could result from different biases towards the classification categories, at least for the harder-to-decide cases. Under an annotation generation model that takes this into account, there is a hazard that some of the training instances are actually hard cases with unreliable annotations. We show that these are relatively unproblematic for an algorithm operating under the 0--1 loss model, whereas for the commonly used voted perceptron algorithm, hard training cases could result in incorrect prediction on the uncontroversial cases at test time."
J09-4005,{S}quibs: From Annotator Agreement to Noise Models,2009,30,41,1,1,19993,beata klebanov,Computational Linguistics,0,"This article discusses the transition from annotated data to a gold standard, that is, a subset that is sufficiently noise-free with high confidence. Unless appropriately reinterpreted, agreement coefficients do not indicate the quality of the data set as a benchmarking resource: High overall agreement is neither sufficient nor necessary to distill some amount of highly reliable data from the annotated material. A mathematical framework is developed that allows estimation of the noise level of the agreed subset of annotated data, which helps promote cautious benchmarking."
W08-1202,Analyzing Disagreements,2008,5,16,1,1,19993,beata klebanov,Coling 2008: Proceedings of the workshop on Human Judgements in Computational Linguistics,0,"We address the problem of distinguishing between two sources of disagreement in annotations: genuine subjectivity and slip of attention. The latter is especially likely when the classification task has a default class, as in tasks where annotators need to find instances of the phenomenon of interest, such as in a metaphor detection task discussed here. We apply and extend a data analysis technique proposed by Beigman Klebanov and Shamir (2006) to first distill reliably deliberate (non-chance) annotations and then to estimate the amount of attention slips vs genuine disagreement in the reliably deliberate annotations."
N06-2004,Measuring Semantic Relatedness Using People and {W}ord{N}et,2006,15,17,1,1,19993,beata klebanov,"Proceedings of the Human Language Technology Conference of the {NAACL}, Companion Volume: Short Papers",0,"In this paper, we (1) propose a new dataset for testing the degree of relatedness between pairs of words; (2) propose a new WordNet-based measure of relatedness, and evaluate it on the new dataset."
P05-2010,Using Readers to Identify Lexical Cohesive Structures in Texts,2005,18,5,1,1,19993,beata klebanov,Proceedings of the {ACL} Student Research Workshop,0,"This paper describes a reader-based experiment on lexical cohesion, detailing the task given to readers and the analysis of the experimental data. We conclude with discussion of the usefulness of the data in future research on lexical cohesion."
