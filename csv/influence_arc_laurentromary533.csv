2003.mtsummit-papers.45,2001.mtsummit-papers.56,1,0.732828,"Missing"
2003.mtsummit-papers.45,2001.mtsummit-papers.39,0,\N,Missing
2004.jeptalnrecital-long.31,brants-hansen-2002-developments,0,0.0201066,"Missing"
2004.jeptalnrecital-long.31,salmon-alt-romary-2004-towards,1,0.831119,"Missing"
2020.acl-main.156,C18-1139,0,0.0272196,"and its surrounding context, so that a single word can have multiple representations, each one depending on how it is used. In practice, most fixed embeddings are used as feature-based models. The most notable examples are word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014) and fastText (Mikolov et al., 2018). All of them are extensively used in a variety of applications nowadays. On the other hand, contextualized word representations and language models have been developed using both featurebased architectures, the most notable examples being ELMo and Flair (Peters et al., 2018; Akbik et al., 2018), and transformer based architectures, that are commonly used in a fine-tune setting, as is the case of GPT-1, GPT-2 (Radford et al., 2018, 2019), BERT and its derivatives (Devlin et al., 2018; Liu et al., 2019; Lan et al., 2019) and more recently T5 (Raffel et al., 2019). All of them have repeatedly improved the state-of-the art in many downstream NLP tasks over the last year. In general, the main advantage of using language models is that they are mostly built in an unsupervised manner and they can be trained with raw, unannotated plain text. Their main drawback is that enormous quantities o"
2020.acl-main.156,W13-3520,0,0.188146,"main drawback is that enormous quantities of data seem to be required to properly train them especially in the case of contextualized models, for which larger corpora are thought to be needed to properly address polysemy and cover the wide range of uses that commonly exist within languages. For gathering data in a wide range of languages, 1703 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1703–1714 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Wikipedia is a commonly used option. It has been used to train fixed embeddings (Al-Rfou et al., 2013; Bojanowski et al., 2017) and more recently the multilingual BERT (Devlin et al., 2018), hereafter mBERT. However, for some languages, Wikipedia might not be large enough to train good quality contextualized word embeddings. Moreover, Wikipedia data all belong to the same specific genre and style. To address this problem, one can resort to crawled text from the internet; the largest and most widespread dataset of crawled text being Common Crawl.1 Such an approach generally solves the quantity and genre/style coverage problems but might introduce noise in the data, an issue which has earned th"
2020.acl-main.156,P18-1246,0,0.0456448,"Missing"
2020.acl-main.156,L18-1550,0,0.301063,"ng Common Crawl also leads to data management challenges as the corpus is distributed in the form of a large set of plain text each containing a large quantity of unclassified multilingual documents from different websites. In this paper we study the trade-off between quantity and quality of data for training contextualized representations. To this end, we use the OSCAR corpus (Ortiz Suárez et al., 2019), a freely available2 multilingual dataset obtained by performing language classification, filtering and cleaning of the whole Common Crawl corpus.3 OSCAR was created following the approach of Grave et al. (2018) but proposing a simple improvement on their filtering method. We then train OSCARbased and Wikipedia-based ELMo contextualized word embeddings (Peters et al., 2018) for 5 languages: Bulgarian, Catalan, Danish, Finnish and Indonesian. We evaluate the models by attaching them to the to UDPipe 2.0 architecture (Straka, 2018; Straka et al., 2019) for dependency parsing and part-of-speech (POS) tagging. We show that the models using the OSCAR-based ELMo embeddings consistently outperform the Wikipediabased ones, suggesting that big high-coverage noisy corpora might be better than small high-qualit"
2020.acl-main.156,P82-1020,0,0.523599,"Missing"
2020.acl-main.156,Q17-1010,0,0.801985,"enormous quantities of data seem to be required to properly train them especially in the case of contextualized models, for which larger corpora are thought to be needed to properly address polysemy and cover the wide range of uses that commonly exist within languages. For gathering data in a wide range of languages, 1703 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1703–1714 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Wikipedia is a commonly used option. It has been used to train fixed embeddings (Al-Rfou et al., 2013; Bojanowski et al., 2017) and more recently the multilingual BERT (Devlin et al., 2018), hereafter mBERT. However, for some languages, Wikipedia might not be large enough to train good quality contextualized word embeddings. Moreover, Wikipedia data all belong to the same specific genre and style. To address this problem, one can resort to crawled text from the internet; the largest and most widespread dataset of crawled text being Common Crawl.1 Such an approach generally solves the quantity and genre/style coverage problems but might introduce noise in the data, an issue which has earned the corpus some criticism, m"
2020.acl-main.156,E17-2068,0,0.10044,"Missing"
2020.acl-main.156,K18-2005,0,0.0774715,"antly increased the performance of the embeddings especially for mid- to low-resource languages. Regarding contextualized models, the most notable non-English contribution has been that of the mBERT (Devlin et al., 2018), which is distributed as (i) a single multilingual model for 100 different languages trained on Wikipedia data, and as (ii) a single multilingual model for both Simplified and Traditional Chinese. Four monolingual fully trained ELMo models have been distributed for Japanese, Portuguese, German and Basque5 ; 44 monolingual ELMo models6 where also released by the HIT-SCIR team (Che et al., 2018) during the CoNLL 2018 Shared Task (Zeman et al., 2018), but their training sets where capped at 20 million words. A German BERT (Chan et al., 2019) as well as a French BERT model (called CamemBERT) (Martin et al., 2019) have also been released. In general no particular effort in creating a set of highquality monolingual contextualized representations has been shown yet, or at least not on a scale that 5 https://allennlp.org/elmo https://github.com/HIT-SCIR/ ELMoForManyLangs 1704 6 is comparable with what was done for fixed word embeddings. For dependency parsing and POS tagging the most notab"
2020.acl-main.156,2021.ccl-1.108,0,0.102417,"Missing"
2020.acl-main.156,2020.acl-main.645,1,0.850749,"Missing"
2020.acl-main.156,K17-3002,0,0.019878,"ffort in creating a set of highquality monolingual contextualized representations has been shown yet, or at least not on a scale that 5 https://allennlp.org/elmo https://github.com/HIT-SCIR/ ELMoForManyLangs 1704 6 is comparable with what was done for fixed word embeddings. For dependency parsing and POS tagging the most notable non-English specific contribution is that of the CoNLL 2018 Shared Task (Zeman et al., 2018), where the 1st place (LAS Ranking) was awarded to the HIT-SCIR team (Che et al., 2018) who used Dozat and Manning (2017)’s Deep Biaffine parser and its extension described in (Dozat et al., 2017), coupled with deep contextualized ELMo embeddings (Peters et al., 2018) (capping the training set at 20 million words). The 1st place in universal POS tagging was awarded to Smith et al. (2018) who used two separate instances of Bohnet et al. (2018)’s tagger. More recent developments in POS tagging and parsing include those of Straka et al. (2019) which couples another CoNLL 2018 shared task participant, UDPipe 2.0 (Straka, 2018), with mBERT greatly improving the scores of the original model, and UDify (Kondratyuk and Straka, 2019), which adds an extra attention layer on top of mBERT plus a D"
2020.acl-main.156,L18-1008,0,0.0299702,"k-specific parameters, and instead copy the weights from a pre-trained Embeddings or language models can be divided into fixed, meaning that they generate a single representation for each word in the vocabulary; and contextualized, meaning that a representation is generated based on both the word and its surrounding context, so that a single word can have multiple representations, each one depending on how it is used. In practice, most fixed embeddings are used as feature-based models. The most notable examples are word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014) and fastText (Mikolov et al., 2018). All of them are extensively used in a variety of applications nowadays. On the other hand, contextualized word representations and language models have been developed using both featurebased architectures, the most notable examples being ELMo and Flair (Peters et al., 2018; Akbik et al., 2018), and transformer based architectures, that are commonly used in a fine-tune setting, as is the case of GPT-1, GPT-2 (Radford et al., 2018, 2019), BERT and its derivatives (Devlin et al., 2018; Liu et al., 2019; Lan et al., 2019) and more recently T5 (Raffel et al., 2019). All of them have repeatedly im"
2020.acl-main.156,D14-1162,0,0.083274,"which introduce a minimal number of task-specific parameters, and instead copy the weights from a pre-trained Embeddings or language models can be divided into fixed, meaning that they generate a single representation for each word in the vocabulary; and contextualized, meaning that a representation is generated based on both the word and its surrounding context, so that a single word can have multiple representations, each one depending on how it is used. In practice, most fixed embeddings are used as feature-based models. The most notable examples are word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014) and fastText (Mikolov et al., 2018). All of them are extensively used in a variety of applications nowadays. On the other hand, contextualized word representations and language models have been developed using both featurebased architectures, the most notable examples being ELMo and Flair (Peters et al., 2018; Akbik et al., 2018), and transformer based architectures, that are commonly used in a fine-tune setting, as is the case of GPT-1, GPT-2 (Radford et al., 2018, 2019), BERT and its derivatives (Devlin et al., 2018; Liu et al., 2019; Lan et al., 2019) and more recently T5 (Raffel et al., 2"
2020.acl-main.156,N18-1202,0,0.498403,"sed on both the word and its surrounding context, so that a single word can have multiple representations, each one depending on how it is used. In practice, most fixed embeddings are used as feature-based models. The most notable examples are word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014) and fastText (Mikolov et al., 2018). All of them are extensively used in a variety of applications nowadays. On the other hand, contextualized word representations and language models have been developed using both featurebased architectures, the most notable examples being ELMo and Flair (Peters et al., 2018; Akbik et al., 2018), and transformer based architectures, that are commonly used in a fine-tune setting, as is the case of GPT-1, GPT-2 (Radford et al., 2018, 2019), BERT and its derivatives (Devlin et al., 2018; Liu et al., 2019; Lan et al., 2019) and more recently T5 (Raffel et al., 2019). All of them have repeatedly improved the state-of-the art in many downstream NLP tasks over the last year. In general, the main advantage of using language models is that they are mostly built in an unsupervised manner and they can be trained with raw, unannotated plain text. Their main drawback is that"
2020.acl-main.156,petrov-etal-2012-universal,0,0.0544696,"Ms, with Softmax classifiers on top, which process its output and generate UPOS, XPOS, UFeats and Lemmas. The lemma classifier also takes the character-level word embeddings as input. • The parser-specific bidirectional LSTM layer, whose output is then passed to a bi-affine attention layer (Dozat and Manning, 2017) producing labeled dependency trees. 4.3 Treebanks To train the selected parser and tagger (cf. Section 4.2) and evaluate the pre-trained language models in our 5 languages, we run our experiments using the Universal Dependencies (UD)14 paradigm and its corresponding UD POS tag set (Petrov et al., 2012). We use all the treebanks available for our five languages in the UD treebank collection version 2.2 (Nivre et al., 2018), which was used for the CoNLL 2018 shared task, thus we perform our evaluation tasks in 6 different treebanks (see Table 4 for treebank size information). After the CoNLL 2018 Shared Task, the UDPipe 2.0 authors added the option to concatenate contextualized representations to the embedding 12 https://github.com/allenai/bilm-tf/ issues/135 13 https://github.com/CoNLL-UD-2018/ UDPipe-Future • Bulgarian BTB: Created at the Institute of Information and Communication Technolog"
2020.acl-main.156,K18-2011,0,0.0254792,"Missing"
2020.acl-main.156,P10-1013,0,0.0096547,"n articles in 301 different languages. Because articles are curated by language and written in an Language Size Bulgarian Catalan Danish Finnish Indonesian 609M 1.1G 338M 669M 488M #Ktokens #Kwords #Ksentences 64,190 211,627 60,644 89,580 80,809 54,748 179,108 52,538 76,035 68,955 3,685 8,293 3,226 6,847 4,298 Table 1: Size of Wikipedia corpora, measured in bytes, thousands of tokens, words and sentences. open collaboration model, its text tends to be of very high-quality in comparison to other free online resources. This is why Wikipedia has been extensively used in various NLP applications (Wu and Weld, 2010; Mihalcea, 2007; Al-Rfou et al., 2013; Bojanowski et al., 2017). We downloaded the XML Wikipedia dumps7 and extracted the plaintext from them using the wikiextractor.py script8 from Giuseppe Attardi. We present the number of words and tokens available for each of our 5 languages in Table 1. We decided against deduplicating the Wikipedia data as the corpora are already quite small. We tokenize the 5 corpora using UDPipe (Straka and Straková, 2017). 3.2 OSCAR Common Crawl is a non-profit organization that produces and maintains an open, freely available repository of crawled data from the web."
2020.acl-main.156,K18-2001,0,0.0265257,"Missing"
2020.acl-main.156,K17-3009,0,0.029716,"t tends to be of very high-quality in comparison to other free online resources. This is why Wikipedia has been extensively used in various NLP applications (Wu and Weld, 2010; Mihalcea, 2007; Al-Rfou et al., 2013; Bojanowski et al., 2017). We downloaded the XML Wikipedia dumps7 and extracted the plaintext from them using the wikiextractor.py script8 from Giuseppe Attardi. We present the number of words and tokens available for each of our 5 languages in Table 1. We decided against deduplicating the Wikipedia data as the corpora are already quite small. We tokenize the 5 corpora using UDPipe (Straka and Straková, 2017). 3.2 OSCAR Common Crawl is a non-profit organization that produces and maintains an open, freely available repository of crawled data from the web. Common Crawl’s complete archive consists of petabytes of monthly snapshots collected since 2011. Common Crawl snapshots are not classified by language, and contain a certain level of noise (e.g. one-word “sentences” such as “OK” and “Cancel” are unsurprisingly very frequent). This is what motivated the creation of the freely available multilingual OSCAR corpus (Ortiz Suárez et al., 2019), extracted from the November 2018 snapshot, which amounts to"
2020.acl-main.156,P19-1355,0,0.0476506,"that all the UDPipe 2.0 + ELMoOSCAR(1) perform better than the UDPipe 2.0 + ELMoWikipedia(1) models across all metrics. Thus we believe that talking in terms of training steps as opposed to training epochs might be a more transparent way of comparing two pretrained models. 5.3 Computational cost and carbon footprint Considering the discussion above, we believe an interesting follow-up to our experiments would be training the ELMo models for more of the languages included in the OSCAR corpus. However training ELMo is computationally costly, and one way to estimate this cost, as pointed out by Strubell et al. (2019), is by using the training times of each model to compute both power consumption and CO2 emissions. In our set-up we used two different machines, each one having 4 NVIDIA GeForce GTX 1080 Ti graphic cards and 128GB of RAM, the difference between the machines being that one uses a single Intel Xeon Gold 5118 processor, while the other uses two Intel Xeon E5-2630 v4 processors. One GeForce GTX 1080 Ti card is rated at around Language Hours Days KWh·PUE CO2 e OSCAR-Based ELMos Bulgarian 1183 515.00 Catalan 1118 199.98 Danish 1183 200.89 Finnish 1118 591.25 Indonesian 1183 694.26 Power 21.45 8.33"
2020.acl-main.156,taule-etal-2008-ancora,0,0.0184647,"different treebanks (see Table 4 for treebank size information). After the CoNLL 2018 Shared Task, the UDPipe 2.0 authors added the option to concatenate contextualized representations to the embedding 12 https://github.com/allenai/bilm-tf/ issues/135 13 https://github.com/CoNLL-UD-2018/ UDPipe-Future • Bulgarian BTB: Created at the Institute of Information and Communication Technologies, Bulgarian Academy of Sciences, it consists of legal documents, news articles and fiction pieces. 14 1707 https://universaldependencies.org • Catalan-AnCora: Built on top of the SpanishCatalan AnCora corpus (Taulé et al., 2008), it contains mainly news articles. • Danish-DDT: Converted from the Danish Dependency Treebank (Buch-Kromann, 2003). It includes news articles, fiction and non fiction texts and oral transcriptions. • Finnish-FTB: Consists of manually annotated grammatical examples from VISK15 (The Web Version of the Large Grammar of Finnish). • Finnish-TDT: Based on the Turku Dependency Treebank (TDT). Contains texts from Wikipedia, Wikinews, news articles, blog entries, magazine articles, grammar examples, Europarl speeches, legal texts and fiction. Treebank Model UPOS UAS LAS Bulgarian BTB UDify UDPipe 2.0"
2020.acl-main.645,C18-1139,0,0.188832,"e of web crawled data is preferable to the use of Wikipedia data. More surprisingly, we show that a relatively small web crawled dataset (4GB) leads to results that are as good as those obtained using larger datasets (130+GB). Our best performing model CamemBERT reaches or improves the state of the art in all four downstream tasks. 1 Introduction Pretrained word representations have a long history in Natural Language Processing (NLP), from noncontextual (Brown et al., 1992; Ando and Zhang, 2005; Mikolov et al., 2013; Pennington et al., 2014) to contextual word embeddings (Peters et al., 2018; Akbik et al., 2018). Word representations are usually obtained by training language model architectures on large amounts of textual data and then fed as an input to more complex task-specific architectures. More recently, these specialized architectures have been replaced altogether by large-scale pretrained language models which are fine-tuned for each application considered. This shift has resulted in large improvements in performance over a wide ∗ Equal contribution. Order determined alphabetically. range of tasks (Devlin et al., 2019; Radford et al., 2019; Liu et al., 2019; Raffel et al., 2019). These transf"
2020.acl-main.645,bawden-etal-2014-correcting,0,0.0112223,"ncy parsing consists in predicting the labeled syntactic tree in order to capture the syntactic relations between words. For both of these tasks we run our experiments using the Universal Dependencies (UD)3 framework and its corresponding UD POS tag set (Petrov et al., 2012) and UD treebank collection (Nivre et al., 2018), which was used for the CoNLL 2018 shared task (Seker et al., 2018). We perform our evaluations on the four freely available French UD treebanks in UD v2.2: GSD (McDonald et al., 2013), Sequoia4 (Candito and Seddah, 2012; Candito et al., 2014), Spoken (Lacheret et al., 2014; Bawden et al., 2014)5 , and ParTUT (Sanguinetti and Bosco, 2015). A brief overview of the size and content of each treebank can be found in Table 1. Treebank #Tokens #Sentences GSD 389,363 16,342 68,615 3,099 Spoken 34,972 ParTUT 27,658 ···················· FTB 350,930 2,786 1,020 27,658 ···················· Sequoia ···················· ···················· sion of the Multi-Genre NLI (MultiNLI) corpus (Williams et al., 2018) to 15 languages by translating the validation and test sets manually into each of those languages. The English training set is machine translated for all languages other than English. The da"
2020.acl-main.645,J92-4003,0,0.338164,"Missing"
2020.acl-main.645,W09-3821,0,0.0439594,"l model based on mBERT, UDify is trained simultaneously on 124 different UD treebanks, creating a single POS tagging and dependency parsing model that works across 75 different languages. We report the scores from Kondratyuk (2019) paper. Table 1: Statistics on the treebanks used in POS tagging, dependency parsing, and NER (FTB). We also evaluate our model in NER, which is a sequence labeling task predicting which words refer to real-world objects, such as people, locations, artifacts and organisations. We use the French Treebank6 (FTB) (Abeillé et al., 2003) in its 2008 version introduced by Candito and Crabbé (2009) and with NER annotations by Sagot et al. (2012). The FTB contains more than 11 thousand entity mentions distributed among 7 different entity types. A brief overview of the FTB can also be found in Table 1. Finally, we evaluate our model on NLI, using the French part of the XNLI dataset (Conneau et al., 2018). NLI consists in predicting whether a hypothesis sentence is entailed, neutral or contradicts a premise sentence. The XNLI dataset is the exten3 https://universaldependencies.org https://deep-sequoia.inria.fr 5 Speech transcript uncased that includes annotated disfluencies without punctua"
2020.acl-main.645,2020.acl-main.747,0,0.0712291,"Missing"
2020.acl-main.645,D18-1269,0,0.021825,"g, and NER (FTB). We also evaluate our model in NER, which is a sequence labeling task predicting which words refer to real-world objects, such as people, locations, artifacts and organisations. We use the French Treebank6 (FTB) (Abeillé et al., 2003) in its 2008 version introduced by Candito and Crabbé (2009) and with NER annotations by Sagot et al. (2012). The FTB contains more than 11 thousand entity mentions distributed among 7 different entity types. A brief overview of the FTB can also be found in Table 1. Finally, we evaluate our model on NLI, using the French part of the XNLI dataset (Conneau et al., 2018). NLI consists in predicting whether a hypothesis sentence is entailed, neutral or contradicts a premise sentence. The XNLI dataset is the exten3 https://universaldependencies.org https://deep-sequoia.inria.fr 5 Speech transcript uncased that includes annotated disfluencies without punctuation 6 This dataset has only been stored and used on Inria’s servers after signing the research-only agreement. 4 • UDPipe Future (Straka, 2018): An LSTMbased model ranked 3rd in dependency parsing and 6th in POS tagging at the CoNLL 2018 shared task (Seker et al., 2018). We report the scores from Kondratyuk"
2020.acl-main.645,2020.findings-emnlp.292,0,0.0196656,"B model (81.88 vs. 81.55). This might be due to the random seed used for pretraining, as each model is pretrained only once. 7210 language understanding tasks. However, even with a BASE architecture and 4GB of training data, the validation loss is still decreasing beyond 100k steps (and 400 epochs). This suggests that we are still under-fitting the 4GB pretraining dataset, training longer might increase downstream performance. 7 Discussion Since the pre-publication of this work (Martin et al., 2019), many monolingual language models have appeared, e.g. (Le et al., 2019; Virtanen et al., 2019; Delobelle et al., 2020), for as much as 30 languages (Nozza et al., 2020). In almost all tested configurations they displayed better results than multilingual language models such as mBERT (Pires et al., 2019). Interestingly, Le et al. (2019) showed that using their FlauBert, a RoBERTa-based language model for French, which was trained on less but more edited data, in conjunction to CamemBERT in an ensemble system could improve the performance of a parsing model and establish a new state-of-the-art in constituency parsing of French, highlighting thus the complementarity of both models.18 As it was the case for Engli"
2020.acl-main.645,N19-1423,0,0.496677,"ennington et al., 2014) to contextual word embeddings (Peters et al., 2018; Akbik et al., 2018). Word representations are usually obtained by training language model architectures on large amounts of textual data and then fed as an input to more complex task-specific architectures. More recently, these specialized architectures have been replaced altogether by large-scale pretrained language models which are fine-tuned for each application considered. This shift has resulted in large improvements in performance over a wide ∗ Equal contribution. Order determined alphabetically. range of tasks (Devlin et al., 2019; Radford et al., 2019; Liu et al., 2019; Raffel et al., 2019). These transfer learning methods exhibit clear advantages over more traditional task-specific approaches. In particular, they can be trained in an unsupervized manner, thereby taking advantage of the information contained in large amounts of raw text. Yet they come with implementation challenges, namely the amount of data and computational resources needed for pretraining, which can reach hundreds of gigabytes of text and require hundreds of GPUs (Yang et al., 2019; Liu et al., 2019). This has limited the availability of these stat"
2020.acl-main.645,E17-2068,0,0.0593732,"training data, architecture, training objective and optimisation setup we use for CamemBERT. 4.1 Training data Pretrained language models benefits from being trained on large datasets (Devlin et al., 2018; Liu et al., 2019; Raffel et al., 2019). We therefore use the French part of the OSCAR corpus (Ortiz Suárez et al., 2019), a pre-filtered and pre-classified version of Common Crawl.7 OSCAR is a set of monolingual corpora extracted from Common Crawl snapshots. It follows the same approach as (Grave et al., 2018) by using a language classification model based on the fastText linear classifier (Grave et al., 2017; Joulin et al., 2016) pretrained on Wikipedia, Tatoeba and SETimes, which supports 176 languages. No other filtering is done. We use a non-shuffled version of the French data, which amounts to 138GB of raw text and 32.7B tokens after subword tokenization. 4.2 Pre-processing We segment the input text data into subword units using SentencePiece (Kudo and Richardson, 2018). SentencePiece is an extension of Byte-Pair encoding (BPE) (Sennrich et al., 2016) and WordPiece (Kudo, 2018) that does not require pre-tokenization (at the word or token level), thus removing the need for language-specific to"
2020.acl-main.645,P19-1356,1,0.885465,"Missing"
2020.acl-main.645,D14-1162,0,0.0850598,"entity recognition and natural language inference tasks. We show that the use of web crawled data is preferable to the use of Wikipedia data. More surprisingly, we show that a relatively small web crawled dataset (4GB) leads to results that are as good as those obtained using larger datasets (130+GB). Our best performing model CamemBERT reaches or improves the state of the art in all four downstream tasks. 1 Introduction Pretrained word representations have a long history in Natural Language Processing (NLP), from noncontextual (Brown et al., 1992; Ando and Zhang, 2005; Mikolov et al., 2013; Pennington et al., 2014) to contextual word embeddings (Peters et al., 2018; Akbik et al., 2018). Word representations are usually obtained by training language model architectures on large amounts of textual data and then fed as an input to more complex task-specific architectures. More recently, these specialized architectures have been replaced altogether by large-scale pretrained language models which are fine-tuned for each application considered. This shift has resulted in large improvements in performance over a wide ∗ Equal contribution. Order determined alphabetically. range of tasks (Devlin et al., 2019; Ra"
2020.acl-main.645,N18-1202,0,0.351202,". We show that the use of web crawled data is preferable to the use of Wikipedia data. More surprisingly, we show that a relatively small web crawled dataset (4GB) leads to results that are as good as those obtained using larger datasets (130+GB). Our best performing model CamemBERT reaches or improves the state of the art in all four downstream tasks. 1 Introduction Pretrained word representations have a long history in Natural Language Processing (NLP), from noncontextual (Brown et al., 1992; Ando and Zhang, 2005; Mikolov et al., 2013; Pennington et al., 2014) to contextual word embeddings (Peters et al., 2018; Akbik et al., 2018). Word representations are usually obtained by training language model architectures on large amounts of textual data and then fed as an input to more complex task-specific architectures. More recently, these specialized architectures have been replaced altogether by large-scale pretrained language models which are fine-tuned for each application considered. This shift has resulted in large improvements in performance over a wide ∗ Equal contribution. Order determined alphabetically. range of tasks (Devlin et al., 2019; Radford et al., 2019; Liu et al., 2019; Raffel et al."
2020.acl-main.645,petrov-etal-2012-universal,0,0.0660713,"Natural Language Inference (NLI). We also present the baselines that we will use for comparison. 1 Released at: https://camembert-model.fr under the MIT open-source license. 7204 2 https://allennlp.org/elmo Tasks POS tagging is a low-level syntactic task, which consists in assigning to each word its corresponding grammatical category. Dependency parsing consists in predicting the labeled syntactic tree in order to capture the syntactic relations between words. For both of these tasks we run our experiments using the Universal Dependencies (UD)3 framework and its corresponding UD POS tag set (Petrov et al., 2012) and UD treebank collection (Nivre et al., 2018), which was used for the CoNLL 2018 shared task (Seker et al., 2018). We perform our evaluations on the four freely available French UD treebanks in UD v2.2: GSD (McDonald et al., 2013), Sequoia4 (Candito and Seddah, 2012; Candito et al., 2014), Spoken (Lacheret et al., 2014; Bawden et al., 2014)5 , and ParTUT (Sanguinetti and Bosco, 2015). A brief overview of the size and content of each treebank can be found in Table 1. Treebank #Tokens #Sentences GSD 389,363 16,342 68,615 3,099 Spoken 34,972 ParTUT 27,658 ···················· FTB 350,930 2,786"
2020.acl-main.645,P19-1493,0,0.0210785,"itecture and 4GB of training data, the validation loss is still decreasing beyond 100k steps (and 400 epochs). This suggests that we are still under-fitting the 4GB pretraining dataset, training longer might increase downstream performance. 7 Discussion Since the pre-publication of this work (Martin et al., 2019), many monolingual language models have appeared, e.g. (Le et al., 2019; Virtanen et al., 2019; Delobelle et al., 2020), for as much as 30 languages (Nozza et al., 2020). In almost all tested configurations they displayed better results than multilingual language models such as mBERT (Pires et al., 2019). Interestingly, Le et al. (2019) showed that using their FlauBert, a RoBERTa-based language model for French, which was trained on less but more edited data, in conjunction to CamemBERT in an ensemble system could improve the performance of a parsing model and establish a new state-of-the-art in constituency parsing of French, highlighting thus the complementarity of both models.18 As it was the case for English when BERT was first released, the availability of similar scale language models for French enabled interesting applications, such as large scale anonymization of legal texts, where Ca"
2020.acl-main.645,P16-1162,0,0.0173054,"l snapshots. It follows the same approach as (Grave et al., 2018) by using a language classification model based on the fastText linear classifier (Grave et al., 2017; Joulin et al., 2016) pretrained on Wikipedia, Tatoeba and SETimes, which supports 176 languages. No other filtering is done. We use a non-shuffled version of the French data, which amounts to 138GB of raw text and 32.7B tokens after subword tokenization. 4.2 Pre-processing We segment the input text data into subword units using SentencePiece (Kudo and Richardson, 2018). SentencePiece is an extension of Byte-Pair encoding (BPE) (Sennrich et al., 2016) and WordPiece (Kudo, 2018) that does not require pre-tokenization (at the word or token level), thus removing the need for language-specific tokenisers. We use a vocabulary size of 32k subword tokens. These subwords are learned on 107 sentences sampled randomly from the pretraining dataset. We do not use subword regularisation (i.e. sampling from multiple possible segmentations) for the sake of simplicity. 7 https://commoncrawl.org/about/ 4.3 Language Modeling Transformer Similar to RoBERTa and BERT, CamemBERT is a multi-layer bidirectional Transformer (Vaswani et al., 2017). Given the widesp"
2020.acl-main.645,K18-2020,0,0.0902039,"ent entity types. A brief overview of the FTB can also be found in Table 1. Finally, we evaluate our model on NLI, using the French part of the XNLI dataset (Conneau et al., 2018). NLI consists in predicting whether a hypothesis sentence is entailed, neutral or contradicts a premise sentence. The XNLI dataset is the exten3 https://universaldependencies.org https://deep-sequoia.inria.fr 5 Speech transcript uncased that includes annotated disfluencies without punctuation 6 This dataset has only been stored and used on Inria’s servers after signing the research-only agreement. 4 • UDPipe Future (Straka, 2018): An LSTMbased model ranked 3rd in dependency parsing and 6th in POS tagging at the CoNLL 2018 shared task (Seker et al., 2018). We report the scores from Kondratyuk (2019) paper. • UDPipe Future + mBERT + Flair (Straka et al., 2019): The original UDPipe Future implementation using mBERT and Flair as feature-based contextualized word embeddings. We report the scores from Straka et al. (2019) paper. In French, no extensive work has been done on NER due to the limited availability of annotated corpora. Thus we compare our model with the only recent available baselines set by Dupont (2017), who t"
2020.acl-main.645,P81-1022,0,0.325042,"Missing"
2020.acl-main.645,P19-1527,0,0.185418,"ining set is machine translated for all languages other than English. The dataset is composed of 122k train, 2490 development and 5010 test examples for each language. As usual, NLI performance is evaluated using accuracy. Baselines In dependency parsing and POStagging we compare our model with: • mBERT: The multilingual cased version of BERT (see Section 2.1). We fine-tune mBERT on each of the treebanks with an additional layer for POS-tagging and dependency parsing, in the same conditions as our CamemBERT model. • XLMMLM-TLM : A multilingual pretrained language model from Lample and Conneau (2019), which showed better performance than mBERT on NLI. We use the version available in the Hugging’s Face transformer library (Wolf et al., 2019); like mBERT, we fine-tune it in the same conditions as our model. Genres Blogs, News Reviews, Wiki Medical, News Non-fiction, Wiki Spoken Legal, News, Wikis News • UDify (Kondratyuk, 2019): A multitask and multilingual model based on mBERT, UDify is trained simultaneously on 124 different UD treebanks, creating a single POS tagging and dependency parsing model that works across 75 different languages. We report the scores from Kondratyuk (2019) paper."
2020.acl-main.645,P19-1452,0,0.028162,"Missing"
2020.acl-main.645,N18-1101,0,0.0873351,"Missing"
2020.acl-main.645,D19-1077,0,0.0177636,"Straka et al. (2019) paper. In French, no extensive work has been done on NER due to the limited availability of annotated corpora. Thus we compare our model with the only recent available baselines set by Dupont (2017), who trained both CRF (Lafferty et al., 2001) and 7205 BiLSTM-CRF (Lample et al., 2016) architectures on the FTB and enhanced them using heuristics and pretrained word embeddings. Additionally, as for POS and dependency parsing, we compare our model to a fine-tuned version of mBERT for the NER task. For XNLI, we provide the scores of mBERT which has been reported for French by Wu and Dredze (2019). We report scores from XLMMLM-TLM (described above), the best model from Lample and Conneau (2019). We also report the results of XLM-R (Conneau et al., 2019). 4 CamemBERT: a French Language Model In this section, we describe the pretraining data, architecture, training objective and optimisation setup we use for CamemBERT. 4.1 Training data Pretrained language models benefits from being trained on large datasets (Devlin et al., 2018; Liu et al., 2019; Raffel et al., 2019). We therefore use the French part of the OSCAR corpus (Ortiz Suárez et al., 2019), a pre-filtered and pre-classified vers"
2020.jeptalnrecital-taln.5,2020.findings-emnlp.292,0,0.0264053,"Missing"
2020.jeptalnrecital-taln.5,N19-1423,0,0.0614451,"Missing"
2020.jeptalnrecital-taln.5,L18-1550,0,0.0310337,"Missing"
2020.jeptalnrecital-taln.5,E17-2068,0,0.0612565,"Missing"
2020.jeptalnrecital-taln.5,P18-1031,0,0.0436608,"Missing"
2020.jeptalnrecital-taln.5,P19-1356,1,0.883155,"Missing"
2020.jeptalnrecital-taln.5,D14-1162,0,0.0850524,"Missing"
2020.jeptalnrecital-taln.5,N18-1202,0,0.122933,"Missing"
2020.jeptalnrecital-taln.5,P19-1493,0,0.0268051,"Missing"
2020.jeptalnrecital-taln.5,F12-2050,1,0.854707,"Missing"
2020.jeptalnrecital-taln.5,P19-1527,0,0.0505485,"Missing"
2020.jeptalnrecital-taln.5,P19-1452,0,0.0305722,"Missing"
2020.jeptalnrecital-taln.5,N18-1101,0,0.0357062,"Missing"
2020.lrec-1.569,C18-1139,0,0.0420735,"cular with the CoNLL 2003 (Tjong Kim Sang and De Meulder, 2003) and Ontonotes v5 (Pradhan et al., 2012; Pradhan et al., 2013) corpora. In recent years, NER was traditionally tackled using Conditional Random Fields (CRF) (Lafferty et al., 2001) which are quite suited for NER; CRFs were later used as decoding layers for Bi-LSTM architectures (Huang et al., 2015; Lample et al., 2016) showing considerable improvements over CRFs alone. These Bi-LSTM-CRF architectures were later enhanced with contextualized word embeddings which yet again brought major improvements to the task (Peters et al., 2018; Akbik et al., 2018). Finally, large pre-trained architectures settled the current state of the art showing a small yet important improvement over previous NER-specific architectures (Devlin et al., 2019; Baevski et al., 2019). For French, rule-based system have been developed until relatively recently, due to the lack of proper training data (Sekine and Nobata, 2004; Rosset et al., 2005; Stern and Sagot, 2010; Nouvel et al., 2011). The limited availability of a few annotated corpora (cf. Section 1.) made it possible to apply statistical machine learning techniques (Bechet and Charton, 2010; Dupont and Tellier, 2"
2020.lrec-1.569,D19-1539,0,0.0122762,"om Fields (CRF) (Lafferty et al., 2001) which are quite suited for NER; CRFs were later used as decoding layers for Bi-LSTM architectures (Huang et al., 2015; Lample et al., 2016) showing considerable improvements over CRFs alone. These Bi-LSTM-CRF architectures were later enhanced with contextualized word embeddings which yet again brought major improvements to the task (Peters et al., 2018; Akbik et al., 2018). Finally, large pre-trained architectures settled the current state of the art showing a small yet important improvement over previous NER-specific architectures (Devlin et al., 2019; Baevski et al., 2019). For French, rule-based system have been developed until relatively recently, due to the lack of proper training data (Sekine and Nobata, 2004; Rosset et al., 2005; Stern and Sagot, 2010; Nouvel et al., 2011). The limited availability of a few annotated corpora (cf. Section 1.) made it possible to apply statistical machine learning techniques (Bechet and Charton, 2010; Dupont and Tellier, 2014; Dupont, 2017) as well as hybrid techniques combining handcrafted grammars and machine learning (Béchet et al., 2011). To the best of our knowledge, the best results previously published on FTB NER are"
2020.lrec-1.569,Q17-1010,0,0.0748939,"n the FTB, the named entity annotations could only be provided to people having signed the FTB license, which prevented them from being made freely downloadable online. The goal of this paper is to establish a new state of the art for French NER by (i) providing a new, easy-to-use UDaligned version of the named entity annotation layer in the FTB and (ii) using this corpus as a training and evaluation dataset for carrying out NER experiments using state-ofthe-art architectures, thereby improving over the previous state of the art in French NER. In particular, by using both FastText embeddings (Bojanowski et al., 2017) and one of the versions of the CamemBERT French neural contextual language model (Martin et al., 2019) within an LSTM-CRF architecture, we can reach an F1-score of 90.25, a 6.5point improvement over the previously state-of-the-art system SEM (Dupont, 2017). 2. A named entity annotation layer for the UD version of the French TreeBank In this section, we describe the process whereby we realigned the named entity FTB annotations by Sagot et al. (2012) with the UD version of the FTB. This makes it possible to share these annotations in the form of a set of additional columns that can easily be pa"
2020.lrec-1.569,N19-1423,0,0.0326386,"Missing"
2020.lrec-1.569,doddington-etal-2004-automatic,0,0.0864256,"gst others). Importantly, it has long been recognised that the type of named entities can be defined in two ways, which underlies the notion of metonymy: the intrinsic type (France is always a location) and the contextual type (in la France a signé un traité ‘France signed a treaty’, France denotes an organization). NER has been an important task in natural language processing for quite some time. It was already the focus of the MUC conferences and associated shared tasks (Marsh and Perzanowski, 1998), and later that of the CoNLL 2003 and ACE shared tasks (Tjong Kim Sang and De Meulder, 2003; Doddington et al., 2004). Traditionally, as for instance was the case for the MUC shared tasks, only person names, location names, organization names, and sometimes “other proper names” are considered. However, the notion of named entity mention is sometimes extended to cover any text span that does not follow the general grammar of the language at hand, but a type- and often culturespecific grammar, thereby including entities ranging from product and brand names to dates and from URLs to monetary amounts and other types of numbers. As for many other tasks, NER was first addressed using rule-based approaches, followe"
2020.lrec-1.569,W11-0411,0,0.023484,"er tasks, NER was first addressed using rule-based approaches, followed by statistical and now neural machine learning techniques (see Section 3.1. for a brief discussion on NER approaches). Of course, evaluating NER systems as well as training machine-learningbased NER systems, statistical or neural, require namedentity-annotated corpora. Unfortunately, most named entity annotated French corpora are oral transcripts, and they are not always freely available. The ESTER and ESTER2 corpora (60 plus 150 hours of NER-annotated broadcast transcripts) (Galliano et al., 2009), as well as the Quaero (Grouin et al., 2011) corpus are based on oral transcripts (radio broadcasts). Interestingly, the Quaero corpus relies on an original, very rich and structured definition of the notion of named entity (Rosset et al., 2011). It contains both the intrinsic and the contextual types of each mention, whereas the ESTER and ESTER2 corpora only provide the contextual type. Sagot et al. (2012) describe the addition to the French Treebank (FTB) (Abeillé et al., 2003) in its FTB-UC version (Candito and Crabbé, 2009) of a new, freely available annotation layer providing named entity information in terms of span and type (NER)"
2020.lrec-1.569,N16-1030,0,0.00755553,"y statistical and now neural machine learning techniques. In addition, many systems use a lexicon of named entity mentions, usually called a “gazetteer” in this context. Most of the advances in NER have been achieved on English, in particular with the CoNLL 2003 (Tjong Kim Sang and De Meulder, 2003) and Ontonotes v5 (Pradhan et al., 2012; Pradhan et al., 2013) corpora. In recent years, NER was traditionally tackled using Conditional Random Fields (CRF) (Lafferty et al., 2001) which are quite suited for NER; CRFs were later used as decoding layers for Bi-LSTM architectures (Huang et al., 2015; Lample et al., 2016) showing considerable improvements over CRFs alone. These Bi-LSTM-CRF architectures were later enhanced with contextualized word embeddings which yet again brought major improvements to the task (Peters et al., 2018; Akbik et al., 2018). Finally, large pre-trained architectures settled the current state of the art showing a small yet important improvement over previous NER-specific architectures (Devlin et al., 2019; Baevski et al., 2019). For French, rule-based system have been developed until relatively recently, due to the lack of proper training data (Sekine and Nobata, 2004; Rosset et al."
2020.lrec-1.569,P10-1052,0,0.011033,"res were averaged over a 10-fold cross validation. To see why this is important for FTB-NE, see section 3.2.4.. In this section, we will compare our strong baseline with a series of neural models. We will use the two current stateof-the-art neural architectures for NER, namely seq2seq and LSTM-CRFs models. We will use various pre-trained embeddings in said architectures: fastText, CamemBERT (a French BERT-like model) and FrELMo (a French ELMo model) embeddings. 3.2.1. SEM SEM (Dupont, 2017) is a tool that relies on linear-chain CRFs (Lafferty et al., 2001) to perform tagging. SEM uses Wapiti (Lavergne et al., 2010) v1.5.0 as linear-chain CRFs implementation. SEM uses the following features for NER: • token, prefix/suffix from 1 to 5 and a Boolean isDigit features in a [-2, 2] window; • previous/next common noun in sentence; • 10 gazetteers (including NE lists and trigger words for NEs) applied with some priority rules in a [-2, 2] window; 9 https://github.com/kermitt2/grobid-ner# corpus-lemonde-ftb-french 4633 M ODEL P RECISION R ECALL F1-S CORE SEM (CRF) 87.18 80.48 83.70 LSTM-seq2seq + FastText + FastText + FrELMo + FastText + CamemBERTOSCAR-BASE-WWM + FastText + CamemBERTOSCAR-BASE-WWM + FrELMo + Fas"
2020.lrec-1.569,M98-1002,0,0.414906,"wing for instance to distinguish between the telecommunication company Orange and the town Orange in southern France (amongst others). Importantly, it has long been recognised that the type of named entities can be defined in two ways, which underlies the notion of metonymy: the intrinsic type (France is always a location) and the contextual type (in la France a signé un traité ‘France signed a treaty’, France denotes an organization). NER has been an important task in natural language processing for quite some time. It was already the focus of the MUC conferences and associated shared tasks (Marsh and Perzanowski, 1998), and later that of the CoNLL 2003 and ACE shared tasks (Tjong Kim Sang and De Meulder, 2003; Doddington et al., 2004). Traditionally, as for instance was the case for the MUC shared tasks, only person names, location names, organization names, and sometimes “other proper names” are considered. However, the notion of named entity mention is sometimes extended to cover any text span that does not follow the general grammar of the language at hand, but a type- and often culturespecific grammar, thereby including entities ranging from product and brand names to dates and from URLs to monetary amo"
2020.lrec-1.569,N18-1202,0,0.0129468,"on English, in particular with the CoNLL 2003 (Tjong Kim Sang and De Meulder, 2003) and Ontonotes v5 (Pradhan et al., 2012; Pradhan et al., 2013) corpora. In recent years, NER was traditionally tackled using Conditional Random Fields (CRF) (Lafferty et al., 2001) which are quite suited for NER; CRFs were later used as decoding layers for Bi-LSTM architectures (Huang et al., 2015; Lample et al., 2016) showing considerable improvements over CRFs alone. These Bi-LSTM-CRF architectures were later enhanced with contextualized word embeddings which yet again brought major improvements to the task (Peters et al., 2018; Akbik et al., 2018). Finally, large pre-trained architectures settled the current state of the art showing a small yet important improvement over previous NER-specific architectures (Devlin et al., 2019; Baevski et al., 2019). For French, rule-based system have been developed until relatively recently, due to the lack of proper training data (Sekine and Nobata, 2004; Rosset et al., 2005; Stern and Sagot, 2010; Nouvel et al., 2011). The limited availability of a few annotated corpora (cf. Section 1.) made it possible to apply statistical machine learning techniques (Bechet and Charton, 2010;"
2020.lrec-1.569,W12-4501,0,0.0334694,"unhandled ""&"", for instance) or slightly altered text (for example, a missing comma). Errors in FTB-UD were probably some XML artifacts. 3. 3.1. Benchmarking NER Models Brief state of the art of NER As mentioned above, NER was first addressed using rulebased approaches, followed by statistical and now neural machine learning techniques. In addition, many systems use a lexicon of named entity mentions, usually called a “gazetteer” in this context. Most of the advances in NER have been achieved on English, in particular with the CoNLL 2003 (Tjong Kim Sang and De Meulder, 2003) and Ontonotes v5 (Pradhan et al., 2012; Pradhan et al., 2013) corpora. In recent years, NER was traditionally tackled using Conditional Random Fields (CRF) (Lafferty et al., 2001) which are quite suited for NER; CRFs were later used as decoding layers for Bi-LSTM architectures (Huang et al., 2015; Lample et al., 2016) showing considerable improvements over CRFs alone. These Bi-LSTM-CRF architectures were later enhanced with contextualized word embeddings which yet again brought major improvements to the task (Peters et al., 2018; Akbik et al., 2018). Finally, large pre-trained architectures settled the current state of the art sho"
2020.lrec-1.569,W13-3516,0,0.0300601,"xperiments We used SEM (Dupont, 2017) as our strong baseline because, to the best of our knowledge, it was the previous state-of-the-art for named entity recognition on the FTBNE corpus. Other French NER systems are available, such as the one given by SpaCy. However, it was trained on another corpus called WikiNER, making the results noncomparable. We can also cite the system of (Stern et al., 2012). This system was trained on another newswire (AFP) using the same annotation guidelines, so the results given in this article are not directly comparable. This model was trained on FTB-NE in Stern (2013) (table C.7, page 303), but the article is written in French. The model yielded an F1-score of 0.7564, which makes it a weaker baseline than SEM. We can cite yet another NER system, namely grobidner.9 It was trained on the FTB-NE and yields an F1-score of 0.8739. Two things are to be taken into consideration: the tagset was slightly modified and scores were averaged over a 10-fold cross validation. To see why this is important for FTB-NE, see section 3.2.4.. In this section, we will compare our strong baseline with a series of neural models. We will use the two current stateof-the-art neural a"
2020.lrec-1.569,sagot-stern-2012-aleda,1,0.657457,"ly, the Quaero corpus relies on an original, very rich and structured definition of the notion of named entity (Rosset et al., 2011). It contains both the intrinsic and the contextual types of each mention, whereas the ESTER and ESTER2 corpora only provide the contextual type. Sagot et al. (2012) describe the addition to the French Treebank (FTB) (Abeillé et al., 2003) in its FTB-UC version (Candito and Crabbé, 2009) of a new, freely available annotation layer providing named entity information in terms of span and type (NER) as well as reference (NE linking), using the Wikipedia-based Aleda (Sagot and Stern, 2012) as a reference entity database. This was the first freely available French corpus annotated with referential named entity information and the first freely available such corpus for the written journalistic genre. However, this annotation is provided in the form of an XML-annotated text with sentence boundaries but no tokenization. This corpus will be referred to as FTB-NE in the rest of the article. Since the publication of that named entity FTB annotation layer, the field has evolved in many ways. Firstly, most treebanks are now available as part of the Universal Dependencies (UD)1 treebank"
2020.lrec-1.569,sekine-nobata-2004-definition,0,0.114687,"ang et al., 2015; Lample et al., 2016) showing considerable improvements over CRFs alone. These Bi-LSTM-CRF architectures were later enhanced with contextualized word embeddings which yet again brought major improvements to the task (Peters et al., 2018; Akbik et al., 2018). Finally, large pre-trained architectures settled the current state of the art showing a small yet important improvement over previous NER-specific architectures (Devlin et al., 2019; Baevski et al., 2019). For French, rule-based system have been developed until relatively recently, due to the lack of proper training data (Sekine and Nobata, 2004; Rosset et al., 2005; Stern and Sagot, 2010; Nouvel et al., 2011). The limited availability of a few annotated corpora (cf. Section 1.) made it possible to apply statistical machine learning techniques (Bechet and Charton, 2010; Dupont and Tellier, 2014; Dupont, 2017) as well as hybrid techniques combining handcrafted grammars and machine learning (Béchet et al., 2011). To the best of our knowledge, the best results previously published on FTB NER are those obtained by Dupont (2017), who trained both CRF and BiLSTM-CRF architectures and improved them using heuristics and pre-trained word embe"
2020.lrec-1.569,W12-0508,1,0.797554,"task included a German corpus (Tjong Kim Sang and De Meulder, 2003). The recent efforts by Straková et al. (2019) settled the state of the art for Spanish and Dutch, while Akbik et al. (2018) did so for German. 3.2. Experiments We used SEM (Dupont, 2017) as our strong baseline because, to the best of our knowledge, it was the previous state-of-the-art for named entity recognition on the FTBNE corpus. Other French NER systems are available, such as the one given by SpaCy. However, it was trained on another corpus called WikiNER, making the results noncomparable. We can also cite the system of (Stern et al., 2012). This system was trained on another newswire (AFP) using the same annotation guidelines, so the results given in this article are not directly comparable. This model was trained on FTB-NE in Stern (2013) (table C.7, page 303), but the article is written in French. The model yielded an F1-score of 0.7564, which makes it a weaker baseline than SEM. We can cite yet another NER system, namely grobidner.9 It was trained on the FTB-NE and yields an F1-score of 0.8739. Two things are to be taken into consideration: the tagset was slightly modified and scores were averaged over a 10-fold cross valida"
2020.lrec-1.569,P19-1527,0,0.0477841,"ing handcrafted grammars and machine learning (Béchet et al., 2011). To the best of our knowledge, the best results previously published on FTB NER are those obtained by Dupont (2017), who trained both CRF and BiLSTM-CRF architectures and improved them using heuristics and pre-trained word embeddings. We use this system as our strong baseline. Leaving aside French and English, the CoNLL 2002 shared task included NER corpora for Spanish and Dutch corpora (Tjong Kim Sang, 2002) while the CoNLL 2003 shared task included a German corpus (Tjong Kim Sang and De Meulder, 2003). The recent efforts by Straková et al. (2019) settled the state of the art for Spanish and Dutch, while Akbik et al. (2018) did so for German. 3.2. Experiments We used SEM (Dupont, 2017) as our strong baseline because, to the best of our knowledge, it was the previous state-of-the-art for named entity recognition on the FTBNE corpus. Other French NER systems are available, such as the one given by SpaCy. However, it was trained on another corpus called WikiNER, making the results noncomparable. We can also cite the system of (Stern et al., 2012). This system was trained on another newswire (AFP) using the same annotation guidelines, so t"
2020.lrec-1.569,W03-0419,0,0.416548,"Missing"
2020.lrec-1.569,W02-2024,0,0.222248,"Missing"
broeder-etal-2002-lrep,A97-1035,0,\N,Missing
bunt-etal-2010-towards,bunt-2006-dimensions,1,\N,Missing
bunt-etal-2010-towards,N09-2050,1,\N,Missing
bunt-etal-2010-towards,W03-0804,1,\N,Missing
bunt-romary-2004-standardization,ide-romary-2004-registry,1,\N,Missing
C98-1044,P87-1022,0,0.607207,"Missing"
C98-1044,J86-3001,0,0.521933,"Missing"
C98-1044,J96-3006,0,\N,Missing
C98-1044,P97-1012,1,\N,Missing
C98-1044,J95-2003,0,\N,Missing
C98-1044,J92-4007,0,\N,Missing
C98-1044,P83-1007,0,\N,Missing
C98-1044,P96-1036,0,\N,Missing
C98-1044,J96-2005,0,\N,Missing
devillers-etal-2004-french,H92-1003,0,\N,Missing
devillers-etal-2004-french,P01-1066,0,\N,Missing
devillers-etal-2004-french,antoine-etal-2002-predictive,1,\N,Missing
devillers-etal-2004-french,antoine-etal-2000-obtaining,1,\N,Missing
E12-2003,P07-2045,0,0.00303578,"Table 1 presents statistics of these in-domain data. The data extracted from HAL were used to adapt a generic system to the scientific literature domain. The generic system was mostly trained on data provided for the shared task of Sixth Workshop on Statistical Machine Translation6 (WMT 2011), described in Table 2. Table 3 presents results showing, in the English–French direction, the impact on the statistical engine of introducing the resources extracted from HAL, as well as the impact of domain adaptation techniques. The baseline statistical engine is a standard PBSMT system based on Moses (Koehn et al., 2007) and the SRILM tookit (Stolcke, 2002). Is was trained and tuned only on WMT11 data (out-of-domain). Incorporating the HAL data into the language model and tuning the system on the HAL development set, Domain Lg Monolingual data Train cs En 2.5 M Fr 761 k phys En 2.1 M Fr 662 k 54 M 19 M 50 M 17 M 457 k 274 k 646 k 292 k Table 1: Statistics for the parallel training, development, and test data sets extracted from thesis abstracts contained in HAL, as well as monolingual data extracted from all documents in HAL, in computer science (cs) and physics (phys). The following statistics are given for"
E12-2003,2008.iwslt-papers.6,1,0.861276,"f the corpus: the number of sentences, the number of running words (after tokenisation) and the number of words in the vocabulary (M and k stand for millions and thousands, respectively). yielded a gain of more than 7 BLEU points, in both domains (computer science and physics). Including the theses abstracts in the parallel training corpus, a further gain of 2.3 BLEU points is observed for computer science, and 3.1 points for physics. The last experiment performed aims at increasing the amount of in-domain parallel texts by translating automatically in-domain monolingual data, as suggested by Schwenk (2008). The synthesised bitext does not bring new words into the system, but increases the probability of indomain bilingual phrases. By adding a synthetic bitext of 12 million words to the parallel training data, we observed a gain of 0.5 BLEU point for computer science, and 0.7 points for physics. Although not shown here, similar results were obtained in the French–English direction. The French–English system is actually slightly better than the English–French one as it is an easier translation direction. 13 Translation Model Language Model Tuning Domain wmt11 wmt11+hal wmt11+hal wmt11+hal wmt11 h"
ide-etal-2000-xces,W98-1102,1,\N,Missing
ide-etal-2000-xces,erjavec-etal-2000-concede,1,\N,Missing
ide-romary-2004-registry,W03-1901,1,\N,Missing
ide-romary-2004-registry,W03-0804,1,\N,Missing
ide-romary-2006-representing,W03-1901,1,\N,Missing
ide-romary-2006-representing,ide-romary-2004-registry,1,\N,Missing
ide-romary-2006-representing,W03-0804,1,\N,Missing
L16-1304,P14-1119,0,0.0204875,"is appropriate for the evaluation of automatic keyphrase extraction methods. Keywords: TermITH-Eval, structured resource, automatic evaluation, keyphrase extraction. 1. Introduction and Motivation Keyphrases are textual units (words and phrases) that represent the most important topics of a document. Keyphrase extraction is the task of automatically detecting those topics in the content of a document. The common practice to evaluate the performance of keyphrase extraction systems is to compute the number of exact matches between extracted keyphrases and (human assigned) reference keyphrases (Hasan and Ng, 2014). However, this leads to overly pessimistic scores since variations in the extracted keyphrases that might be judged as correct cannot be taken into account (Zesch and Gurevych, 2009). Producing a more reliable estimate of system performance is not an easy task as assessing whether a textual unit is a keyphrase is highly subjective (Kim et al., 2010). Yet, a handful of attempts have been made in this direction (Zesch and Gurevych, 2009; Kim et al., 2010) but with limited success. The initiating work of Zesch and Gurevych (2009) stated the need for partial matching instead of exact matching but"
L16-1304,C10-1065,0,0.123835,"etecting those topics in the content of a document. The common practice to evaluate the performance of keyphrase extraction systems is to compute the number of exact matches between extracted keyphrases and (human assigned) reference keyphrases (Hasan and Ng, 2014). However, this leads to overly pessimistic scores since variations in the extracted keyphrases that might be judged as correct cannot be taken into account (Zesch and Gurevych, 2009). Producing a more reliable estimate of system performance is not an easy task as assessing whether a textual unit is a keyphrase is highly subjective (Kim et al., 2010). Yet, a handful of attempts have been made in this direction (Zesch and Gurevych, 2009; Kim et al., 2010) but with limited success. The initiating work of Zesch and Gurevych (2009) stated the need for partial matching instead of exact matching but did not show the effectiveness of their measure compared with a human evaluation. Kim et al. (2010) improved the measure of Zesch and Gurevych (2009) and evaluated the correlation of both the original and improved measures with human evaluations. Computing the correlation between an automatic evaluation measure and human evaluators is an effective w"
L16-1304,R09-1086,0,0.171152,"ction and Motivation Keyphrases are textual units (words and phrases) that represent the most important topics of a document. Keyphrase extraction is the task of automatically detecting those topics in the content of a document. The common practice to evaluate the performance of keyphrase extraction systems is to compute the number of exact matches between extracted keyphrases and (human assigned) reference keyphrases (Hasan and Ng, 2014). However, this leads to overly pessimistic scores since variations in the extracted keyphrases that might be judged as correct cannot be taken into account (Zesch and Gurevych, 2009). Producing a more reliable estimate of system performance is not an easy task as assessing whether a textual unit is a keyphrase is highly subjective (Kim et al., 2010). Yet, a handful of attempts have been made in this direction (Zesch and Gurevych, 2009; Kim et al., 2010) but with limited success. The initiating work of Zesch and Gurevych (2009) stated the need for partial matching instead of exact matching but did not show the effectiveness of their measure compared with a human evaluation. Kim et al. (2010) improved the measure of Zesch and Gurevych (2009) and evaluated the correlation of"
L16-1304,I13-1062,1,0.873881,"enriched. The pre-indexing system relies on pattern matching between text and predefined expressions related to potential keyphrases. The predefined expressions requires constant updating in order to generate appropriate keyphrases. 2.2. Automatic Keyphrase Extraction We selected three keyphrase extraction methods to extract 30 keyphrases (10 each) per bibliographic record. The methods cover the main techniques used for automatic keyphrase extraction: the statistical method TF-IDF (Salton et al., 1975), the classification method KEA (Witten et al., 1999) and the graph-based method TopicRank (Bougouin et al., 2013). TF-IDF is a simple and common keyphrase extraction method that ranks the textual units of a document according to their TF-IDF score, frequently used in Information Retrieval. The idea is to give a high importance score to textual units which are both frequent in the document and specific to it. The specificity of a textual unit regarding a document is obtained using a collection of documents. The lower the number of documents in which a textual unit occurs, the more specific this textual unit is. KEA also relies on simple statistics. According to KEA, a keyphrase can be recognized by its im"
L16-1304,Y09-1013,0,0.0155089,"h topic is its textual unit that appears first within the document. For comparison purposes, we implemented each method and integrated them on top of the same preprocessing tools. Every document is first segmented into sentences, sentences are tokenized into words and words are labelled according their morphological class (Part-of-Speech tagging — POS tagging). We performed sentence segmentation with the PunktSentenceTokenizer provided by the Python Natural Language ToolKit (NLTK)(Bird et al., 2009), word tokenization using the French tokenizer Bonsai included with the French POS tagger MElt (Denis and Sagot, 2009), which we use for POS tagging. 2.3. Manual Evaluation Guidelines Four evaluators took part in the manual evaluation. Being chosen for their indexing experience and their expertise in the selected scientific disciplines, evaluators have been asked to follow the guidelines described below. After reading the title and the abstract of a bibliographic record, evaluators needed to assess if the automatically extracted keyphrases were relevant to the bibliographic record. This assessment is made regarding two aspects: appropriateness and silence. 2.3.1. Appropriateness Appropriateness is a property"
landragin-etal-2004-multimodal,ide-romary-2002-standards,1,\N,Missing
le-etal-2006-lexicalized,C88-2147,0,\N,Missing
le-etal-2006-lexicalized,C96-2120,0,\N,Missing
le-etal-2006-lexicalized,nguyen-etal-2004-developping,1,\N,Missing
lee-etal-2004-towards,callmeier-etal-2004-deepthought,0,\N,Missing
lee-etal-2004-towards,E95-1025,0,\N,Missing
lee-etal-2004-towards,W03-0802,0,\N,Missing
lee-etal-2004-towards,P03-1014,0,\N,Missing
lee-etal-2004-towards,C94-2144,0,\N,Missing
lee-etal-2004-towards,P03-2019,0,\N,Missing
lee-etal-2004-towards,kasper-etal-2004-integrated,0,\N,Missing
lee-etal-2004-towards,2002.jeptalnrecital-long.6,1,\N,Missing
lopez-romary-2010-grisp,W04-2214,0,\N,Missing
lopez-romary-2010-grisp,magnini-cavaglia-2000-integrating,0,\N,Missing
nguyen-etal-2004-developping,ide-romary-2002-standards,1,\N,Missing
P01-1040,ide-etal-2000-xces,1,0.822453,"actic tagging, syntactic annotation, co-reference annotation, etc.), which can be instantiated in different ways depending on the annotators approach and goals. We have implemented both the abstract model and various instantiations using XML schemas (Thompson, et al., 2000), the Resource Definition Framework (RDF) (Lassila and Swick, 2000) and RDF schemas (Brickley and Guha, 2000), which enable description and definition of abstract data models together with means to interpret, via the model, information encoded according to different conventions. The results have been incorporated into XCES (Ide, et al., 2000a), part of the EAGLES Guidelines developed by the Expert Advisory Group on Language Engineering Standards (EAGLES)1. The XCES provides a ready-made, standard encoding format together with a data architecture designed specifically for linguistically annotated corpora. In this paper we provide an overview of our representation framework and demonstrate its applicability to syntactic annotation. The framework has been applied to the representation of terminology (Terminological Markup Framework2, ISO project n.16642) and computational lexicons (Ide, et al., 2000b), thus demonstrating its general"
P01-1040,A97-1011,0,0.0573664,"Missing"
P98-1044,P87-1022,0,0.607371,"Missing"
P98-1044,J95-2003,0,0.646943,"Missing"
P98-1044,J86-3001,0,0.499813,"Missing"
P98-1044,J96-3006,0,\N,Missing
P98-1044,P97-1012,1,\N,Missing
P98-1044,J92-4007,0,\N,Missing
P98-1044,P83-1007,0,\N,Missing
P98-1044,P96-1036,0,\N,Missing
P98-1044,J96-2005,0,\N,Missing
popescu-belis-etal-2004-online,M95-1005,0,\N,Missing
popescu-belis-etal-2004-online,J00-4005,0,\N,Missing
popescu-belis-etal-2004-online,W01-1612,0,\N,Missing
popescu-belis-etal-2004-online,J96-2004,0,\N,Missing
popescu-belis-etal-2004-online,W99-0309,0,\N,Missing
popescu-belis-etal-2004-online,salmon-alt-romary-2004-towards,1,\N,Missing
R15-1003,C92-3150,0,0.346343,"rid approach. What distinguishes the approaches mentioned, is not the type of information considered, but their acquisition and handling. The linguistic approach is based on human intuition, with the manual construction of analysis models, usually in the form of contextual rules. It requires a thorough study of the types of terms, but it has a success for the extraction of NE and terms. In fact, precision is more important for symbolic systems. In previous work on non scientific and technical documents, there are those who used linguistic methods based on syntactic analysis (see for instance (Bourigault, 1992) and (Bourigault, 1 2 3 Characteristics of Arabic Scientific and Technical Terms Our study corpus contains 60 Arabic documents: 50 patents, 5 scientific papers and 5 manuals and installation documents of elevators collected from multiple resources: manuals from the websites of elevator manufacturers, patents from multiple Arabic intellectual property offices and scientific papers from some Arabic journals. All of these documents are text files and contain a total number of 619k tokens. This corpus will allow us to construct the necessary resources such as dictionaries, trigger words and extrac"
R15-1003,A94-1006,0,0.160073,"ecent Advances in Natural Language Processing, pages 17–23, Hissar, Bulgaria, Sep 7–9 2015. This paper is organized as follows. Section 2 is devoted to the presentation of the previous work. We present, in section 3, the characteristics of Arabic scientific and technical terms. In section 4, we argue the choice of terminology model. In section 5, we present our approach. Section 6 is devoted to experimentation and evaluation and we conclude and enunciate some perspectives in section 7. 2 1994)). But the most used approach is the hybrid approach combining statistical and linguistic techniques (Dagan and Church, 1994). The most recent work on scientific and technical documents were mainly based on purely statistical approaches. They used standard techniques of information retrieval and data extraction. Some of them use machine learning tools to extract header metadata using support vector machines (SVM) (Do et al., 2013), hidden markov models (HMM) (Binge, 2009), or conditional random fields (CRF) (Lopez, 2009). Others use machine learning tools to extract metadata of citations (Hetzner, 2008), tables (Liu et al., 2007), figures (Choudhury et al., 2013) or to identify concepts (Rao et al., 2013). All these"
R15-1003,lopez-romary-2010-grisp,1,\N,Missing
romary-etal-2004-experiments,rapp-strube-2002-iterative,0,\N,Missing
romary-etal-2004-experiments,W00-2003,0,\N,Missing
S10-1055,lopez-romary-2010-grisp,1,0.807445,"e selected candidates as a whole for improving the results, while in the previous step, each candidate was selected independently from the other. If we have a ranked list of term T1−N , each having a score s(Ti ), the new score s0 for the term Ti is obtained as follow: Lexical/Semantic features GRISP is a large scale terminological database for technical and scientific domains resulting from the fusion of terminological resources (MeSH, the Gene Ontology, etc.), linguistic resources (part of WordNet) and part of Wikipedia. It has been created for improving patent retrieval and classification (Lopez and Romary, 2010). The assumption is that a phrase which has been identified as controlled term in these resources tend to be a more important keyphrase. A binary feature is used to indicate if the term is part of GRISP or not. We use Wikipedia similarly as the Wikipedia keyphraseness in Maui (Medelyan, 2009). The Wikipedia keyphraseness of a term T is the probability of an appearance of T in a document being an anchor (Medelyan, 2009). We use Wikipedia Miner4 for obtaining this value. Finally we introduced an additional feature commonly used in keyword extraction, the length of the term candidate, i.e. its nu"
S10-1055,C02-1142,0,0.0115046,"his work tries to captures distributional properties of a term relatively to the overall textual content of the document where the term appears or the collection. Phraseness The phraseness measures the lexical cohesion of a sequence of words in a given document, i.e. the degree to which it can be considered as a phrase. This measure is classically used for term extraction and can rely on different techniques, usually evaluating the ability of a sequence of words to appear as a stable phrase more often than just by chance. We applied here the Generalized Dice Coeficient (GDC) as introduced by (Park et al., 2002), applicable to any arbitrary ngram of words (n ≥ 2). For a given term T , |T | being the number of words in T , f req(T ) the frequency of occurrence of T and f req(wi ) the frequency of occurrence of the word wi , we have: Features 3.1 Content features Structural features One of the goals of GROBID is to realize reliable conversions of technical and scientific documents in PDF to fully compliant TEI3 documents. This conversion implies first the recognition of the different sections of the document, then the extraction of all header metadata and references. The analysis is realized in GROBID"
S10-1055,N04-1042,0,0.0817361,"Missing"
salmon-alt-romary-2004-towards,J98-2001,0,\N,Missing
salmon-alt-romary-2004-towards,W01-1612,0,\N,Missing
salmon-alt-romary-2004-towards,W03-0804,1,\N,Missing
salmon-alt-romary-2004-towards,M98-1029,0,\N,Missing
salmon-alt-romary-2004-towards,ide-romary-2002-standards,1,\N,Missing
todirascu-etal-2002-towards,gamback-olsson-2000-experiences,0,\N,Missing
todirascu-etal-2002-towards,ide-romary-2002-standards,1,\N,Missing
W03-0804,P01-1040,1,0.753817,"e semantics of data categories included in annotations (whether they exist in the Registry or not) are well-defined and understood. The data model that will define the pivot format must be capable of representing all of the information contained in diverse annotation types. The model we assume is a feature structure graph for annotation information, capable of referencing n-dimensional regions of primary data as well as other annotations. The choice of this model is indicated by its almost universal use in defining general-purpose annotation formats, including the Generic Modeling Tool (GMT) (Ide & Romary, 2001, 2002) and Annotation Graphs (Bird & Liberman, 2001). The XML-based GMT could serve as a starting point for defining the pivot format; its applicability to diverse annotation types, including terminology, dictionaries and other lexical data (Ide, et al., 2000), morphological annotation (Ide & Romary, 2001a; 2003) and syntactic annotation (Ide & Romary, 2001b) demonstrates its generality. As specified by the LAF architecture, the GMT implements a feature structure graph, and exploits the hierarchical structure of XML elements and XML’s powerful interand intra-document pointing and linkage mech"
W03-0804,ide-romary-2002-standards,1,\N,Missing
W03-1901,P01-1040,1,0.907049,"n the primary data) 3 o super- and sub-segments, where groups of segments will comprise the parts of a larger segment (e.g., a contiguous word segments typically comprise a sentence segment) o discontinuous segments (linked continuous segments) LAF development has proceeded by first identifying an abstract data model that can formally describe linguistic annotations, distinct from any particular representation (as defined in the previous section). Development of this model has been discussed extensively within the language engineering community and tested on a variety of annotation types (see Ide and Romary, 2001a, 2001b, 2002). The data model forms the core of the framework by serving as the reference point for all annotation representation schemes. landmarks (e.g. time stamps) that note a point in the primary data In current practice, segmental information may or may not appear in the document containing the primary data itself. Documents considered to be read-only, for example, might be segmented by specifying byte offsets into the o LAF overview The overall design of LAF is illustrated in Figure 1. The fundamental principle is that the user controls the representation format for linguistic annotat"
W03-1901,ide-romary-2002-standards,1,0.31302,". Alternatively, users may define their own data categories or establish variants of categories in the registry; in such cases, the newly defined data categories will be formalized using the same format as definitions available in the registry. 5 5.1 Implementation Dump format The dump format is instantiated in XML. Structural nodes are represented as XML elements. The XML-based GMT will serve as a starting point for defining the dump format. Its applicability to diverse annotation types, including terminology, dictionaries and other lexical data (Ide, et al., 2000), morphological annotation (Ide and Romary, 2002) and syntactic annotation (Ide and Romary, 2001b, 2003) demonstrates its generality. As specified by the LAF architecture, the GMT implements a feature structure graph. Structural nodes in the graph are represented with the XML element <struct>. <brack> and <alt> elements are provided as grouping tags to handle aggregation (grouping) and alternatives (disjunction), as described above. A <feat> element is used to express category/value pairs. All of these elements are recursively nestable. Therefore, hierarchical relations among annotations and annotation components can be expressed via XML syn"
W03-1901,W03-0804,1,0.435409,"terminologies (which have already been treated in ISO/TC 37). The worldwide use of ISO/TC 37/SC 4 standards should improve information management within industrial, technical and scientific environments, and increase efficiency in computersupported language communication. Within ISO/TC 37/SC 4, a working group (WG11) has been established to develop a Linguistic Annotation Framework (LAF) that can serve as a basis for harmonizing existing language resources as well as developing new ones. The overall design of the architecture and the data model that it will instantiate have been described in Ide et al., 2003. In this paper we provide a description of the data model and its instantiations in LAF, in order to enable annotators to begin to explore how their schemes will map into the framework. 2 Terms and definitions The following terms and definitions are used in the discussion that follows: Annotation: The process of adding linguistic information to language data (“annotation of a corpus”) or the linguistic information itself (“an annotation”), independent of its representation. For example, one may annotate a document for syntax using a LISP-like representation, an XML representation, etc. Repres"
W03-1901,W03-1905,1,\N,Missing
W04-2104,W03-1901,1,0.569611,"rently being developed in ISO committee TC 37/SC 4, is conceived as a generic platform for the specification of lexical structures at any level of linguistic description. As such, it does not provide one single model, but rather a mechanism by which implementers combine elementary lexical subsystems to design models that can be both as close as possible to their needs and comparable to any other lexical models based on the same principles and, possibly, on the same components. The underlying data model for LMF follows the general principles of the linguistic annotation scheme design stated in Ide & Romary, 2003 and implemented in the context of ISO standard 16642 for the representation of terminological data (Romary, 2001). Those principles provide a mechanism for combining a given structural 6 7 http://www2.crl.go.jp/kk/e416/EDR/index.html http://www.papillon-dictionary.org/ metamodel that informs the general organization of a certain level of linguistic information (morphology, syntax, etc.) with elementary descriptors (socalled data categories). Data categories reflect basic linguistic concepts (e.g. /part of speech/, /grammatical number/, /paucal number/, etc.) and allow for recording language-s"
W04-2104,1995.mtsummit-1.1,0,0.0265286,"Missing"
W04-2104,C92-2089,0,0.0372688,"Missing"
